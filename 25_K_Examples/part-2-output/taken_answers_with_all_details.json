[
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 107,
            "user_id": 12727272,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-LCcjuGkLQlg/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reAHyQ8aGSfsRX67Z5Ulk3q6DZbxg/photo.jpg?sz=128",
            "display_name": "Leaderboard281923",
            "link": "https://stackoverflow.com/users/12727272/leaderboard281923"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 64110298,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1601331153,
        "creation_date": 1601330274,
        "question_id": 64110166,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64110166/how-to-create-bins-for-a-dataframe-column-if-the-range-is-given",
        "title": "How to create bins for a dataframe column if the range is given",
        "body": "<pre><code>data = {'Age':[18, 22,29,32,35,38,42,54,47]} \ndf = pd.DataFrame(data)\ndf\n</code></pre>\n<p>This is an example data frame that I want to play with</p>\n<pre><code>pd.cut(df['Age'],bins=5)\n</code></pre>\n<p>If I do this, I get the output as:</p>\n<pre><code>0    (17.964, 25.2]\n1    (17.964, 25.2]\n2      (25.2, 32.4]\n3      (25.2, 32.4]\n4      (32.4, 39.6]\n5      (32.4, 39.6]\n6      (39.6, 46.8]\n7      (46.8, 54.0]\n8      (46.8, 54.0]\n</code></pre>\n<p>Here's the twist: Let's say the age columns can take values between 18 to 58(the range of the column) and I want the bins(or the output) as:</p>\n<pre><code>0    (18.0, 26.0]\n1    (18.0, 26.0]\n2    (26.0, 34.0]\n3    (26.0, 34.0]\n4    (34.0, 42.0]\n5    (34.0, 42.0]\n6    (34.0, 42.0]\n7    (50.0, 58.0]\n8    (42.0, 50.0]\n</code></pre>\n<p>How can I do that? because 'cut' takes the values which are in the column. I got the desired result by doing it manually but if the values of bins were say 100 - how can I do it?</p>\n",
        "answer_body": "<p>You can pass an array to <code>bins</code>:</p>\n<pre><code>pd.cut(df['Age'], bins=np.linspace(18, 58, 100), include_lowest=True)\n</code></pre>\n<p>Output:</p>\n<pre><code>0    (17.999, 18.404]\n1     (21.636, 22.04]\n2    (28.909, 29.313]\n3    (31.737, 32.141]\n4     (34.97, 35.374]\n5    (37.798, 38.202]\n6    (41.838, 42.242]\n7     (53.96, 54.364]\n8    (46.687, 47.091]\n</code></pre>\n",
        "question_body": "<pre><code>data = {'Age':[18, 22,29,32,35,38,42,54,47]} \ndf = pd.DataFrame(data)\ndf\n</code></pre>\n<p>This is an example data frame that I want to play with</p>\n<pre><code>pd.cut(df['Age'],bins=5)\n</code></pre>\n<p>If I do this, I get the output as:</p>\n<pre><code>0    (17.964, 25.2]\n1    (17.964, 25.2]\n2      (25.2, 32.4]\n3      (25.2, 32.4]\n4      (32.4, 39.6]\n5      (32.4, 39.6]\n6      (39.6, 46.8]\n7      (46.8, 54.0]\n8      (46.8, 54.0]\n</code></pre>\n<p>Here's the twist: Let's say the age columns can take values between 18 to 58(the range of the column) and I want the bins(or the output) as:</p>\n<pre><code>0    (18.0, 26.0]\n1    (18.0, 26.0]\n2    (26.0, 34.0]\n3    (26.0, 34.0]\n4    (34.0, 42.0]\n5    (34.0, 42.0]\n6    (34.0, 42.0]\n7    (50.0, 58.0]\n8    (42.0, 50.0]\n</code></pre>\n<p>How can I do that? because 'cut' takes the values which are in the column. I got the desired result by doing it manually but if the values of bins were say 100 - how can I do it?</p>\n",
        "formatted_input": {
            "qid": 64110166,
            "link": "https://stackoverflow.com/questions/64110166/how-to-create-bins-for-a-dataframe-column-if-the-range-is-given",
            "question": {
                "title": "How to create bins for a dataframe column if the range is given",
                "ques_desc": " This is an example data frame that I want to play with If I do this, I get the output as: Here's the twist: Let's say the age columns can take values between 18 to 58(the range of the column) and I want the bins(or the output) as: How can I do that? because 'cut' takes the values which are in the column. I got the desired result by doing it manually but if the values of bins were say 100 - how can I do it? "
            },
            "io": [
                "0    (17.964, 25.2]\n1    (17.964, 25.2]\n2      (25.2, 32.4]\n3      (25.2, 32.4]\n4      (32.4, 39.6]\n5      (32.4, 39.6]\n6      (39.6, 46.8]\n7      (46.8, 54.0]\n8      (46.8, 54.0]\n",
                "0    (18.0, 26.0]\n1    (18.0, 26.0]\n2    (26.0, 34.0]\n3    (26.0, 34.0]\n4    (34.0, 42.0]\n5    (34.0, 42.0]\n6    (34.0, 42.0]\n7    (50.0, 58.0]\n8    (42.0, 50.0]\n"
            ],
            "answer": {
                "ans_desc": "You can pass an array to : Output: ",
                "code": [
                    "pd.cut(df['Age'], bins=np.linspace(18, 58, 100), include_lowest=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "text",
            "dataframe"
        ],
        "owner": {
            "reputation": 857,
            "user_id": 5232723,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://graph.facebook.com/597110223780051/picture?type=large",
            "display_name": "Amal Kostali Targhi",
            "link": "https://stackoverflow.com/users/5232723/amal-kostali-targhi"
        },
        "is_answered": true,
        "view_count": 83557,
        "accepted_answer_id": 41514539,
        "answer_count": 2,
        "score": 18,
        "last_activity_date": 1600932194,
        "creation_date": 1483366776,
        "last_edit_date": 1483743855,
        "question_id": 41428539,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/41428539/data-frame-to-file-txt-python",
        "title": "data frame to file.txt python",
        "body": "<p>I have this dataframe </p>\n\n<pre><code>       X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n</code></pre>\n\n<p>I want to save it as a text file with this format</p>\n\n<pre><code>   X    Y  Z    Value \n   18   55  1      70   \n   18   55  2      67 \n   18   57  2      75     \n   18   58  1      35  \n   19   54  2      70   \n</code></pre>\n\n<p>I tried this code but is not working:</p>\n\n<pre><code>np.savetxt('xgboost.txt', a.values, delimiter ='\\t')\n\nTypeError: Mismatch between array dtype ('object') and format specifier ('%.18e %.18e %.18e')\n</code></pre>\n",
        "answer_body": "<p>This is an almost exact duplicate of the following:<br />\n<a href=\"https://stackoverflow.com/questions/31247198/python-pandas-write-content-of-dataframe-into-text-file\">Python, Pandas : write content of DataFrame into text File</a></p>\n<p>I report again here the answer from the cited SO question with some very small modifications to fit this case.<br />\nYou can use two methods.</p>\n<p><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html\" rel=\"nofollow noreferrer\">np.savetxt()</a>, in which case you should have something like the following:</p>\n<pre><code>np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter=&quot;\\t&quot;, header=&quot;X\\tY\\tZ\\tValue&quot;)  \n</code></pre>\n<p>assuming <code>a</code> is the dataframe. Of course you can change the delimiter you want (tab, comma, space,etc.).<br />\nThe other option, as mentioned in the answer I attached and in the answer here from @MYGz, is to use the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv\" rel=\"nofollow noreferrer\">to_csv</a> method, i.e.:</p>\n<pre><code>a.to_csv('xgboost.txt', header=True, index=False, sep='\\t', mode='a')\n</code></pre>\n",
        "question_body": "<p>I have this dataframe </p>\n\n<pre><code>       X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n</code></pre>\n\n<p>I want to save it as a text file with this format</p>\n\n<pre><code>   X    Y  Z    Value \n   18   55  1      70   \n   18   55  2      67 \n   18   57  2      75     \n   18   58  1      35  \n   19   54  2      70   \n</code></pre>\n\n<p>I tried this code but is not working:</p>\n\n<pre><code>np.savetxt('xgboost.txt', a.values, delimiter ='\\t')\n\nTypeError: Mismatch between array dtype ('object') and format specifier ('%.18e %.18e %.18e')\n</code></pre>\n",
        "formatted_input": {
            "qid": 41428539,
            "link": "https://stackoverflow.com/questions/41428539/data-frame-to-file-txt-python",
            "question": {
                "title": "data frame to file.txt python",
                "ques_desc": "I have this dataframe I want to save it as a text file with this format I tried this code but is not working: "
            },
            "io": [
                "       X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n",
                "   X    Y  Z    Value \n   18   55  1      70   \n   18   55  2      67 \n   18   57  2      75     \n   18   58  1      35  \n   19   54  2      70   \n"
            ],
            "answer": {
                "ans_desc": "This is an almost exact duplicate of the following: Python, Pandas : write content of DataFrame into text File I report again here the answer from the cited SO question with some very small modifications to fit this case. You can use two methods. np.savetxt(), in which case you should have something like the following: assuming is the dataframe. Of course you can change the delimiter you want (tab, comma, space,etc.). The other option, as mentioned in the answer I attached and in the answer here from @MYGz, is to use the to_csv method, i.e.: ",
                "code": [
                    "np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter=\"\\t\", header=\"X\\tY\\tZ\\tValue\")  \n",
                    "a.to_csv('xgboost.txt', header=True, index=False, sep='\\t', mode='a')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 65,
            "user_id": 14080705,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6874f5ee6483d08899d251801e0527a2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dendoniseden",
            "link": "https://stackoverflow.com/users/14080705/dendoniseden"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 64033059,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1600881236,
        "creation_date": 1600879821,
        "last_edit_date": 1600881236,
        "question_id": 64032701,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64032701/get-last-purchase-year-from-sales-data-pivot-in-pandas",
        "title": "Get &quot;Last Purchase Year&quot; from Sales Data Pivot in Pandas",
        "body": "<p>I have pivoted the Customer ID against their year of purchase, so that I know how many times each customer purchased in different years:</p>\n<pre><code>Customer ID      1996  1997 ... 2019  2020\n100000000000001     7     7 ...  NaN   NaN\n100000000000002     8     8 ...  NaN   NaN\n100000000000003     7     4 ...  NaN   NaN\n100000000000004   NaN   NaN ...   21    24              \n100000000000005    17    11 ...   18   NaN\n</code></pre>\n<p>My desired result is to append the column names with the latest year of purchase, and thus the number of years since their last purchase:</p>\n<pre><code>Customer ID      1996  1997 ... 2019  2020   Last   Recency\n100000000000001     7     7 ...  NaN   NaN   1997        23 \n100000000000002     8     8 ...  NaN   NaN   1997        23\n100000000000003     7     4 ...  NaN   NaN   1997        23\n100000000000004   NaN   NaN ...   21    24   2020         0 \n100000000000005    17    11 ...   18   NaN   2019         1\n</code></pre>\n<p>Here is what I tried:</p>\n<pre><code>df_pivot[&quot;Last&quot;] = 2020\nk = 2020\nwhile math.isnan(df_pivot2[k]):\n    df_pivot[&quot;Last&quot;] = k-1\n    k = k-1\n\ndf_pivot[&quot;Recency&quot;] = 2020 - df_pivot[&quot;Last&quot;]\n</code></pre>\n<p>However what I got is &quot;TypeError: cannot convert the series to &lt;class 'float'&gt;&quot;</p>\n<p>Could anyone help me to get the result I need?</p>\n<p>Thanks a lot!</p>\n<p>Dennis</p>\n",
        "answer_body": "<p>You can get last year of purchase using <code>notna</code> + <code>cumsum</code> and <code>idxmax</code> along <code>axis=1</code> then subtract this last year of purchase from the max year to compute <code>Recency</code>:</p>\n<pre><code>c = df.filter(regex=r'\\d+').columns\ndf['Last'] = df[c].notna().cumsum(1).idxmax(1)\ndf['Recency'] = c.max() - df['Last']\n</code></pre>\n<hr />\n<pre><code>       Customer ID  1996  1997  2019  2020  Last  Recency\n0  100000000000001   7.0   7.0   NaN   NaN  1997       23\n1  100000000000002   8.0   8.0   NaN   NaN  1997       23\n2  100000000000003   7.0   4.0   NaN   NaN  1997       23\n3  100000000000004   NaN   NaN  21.0  24.0  2020        0\n4  100000000000005  17.0  11.0  18.0   NaN  2019        1\n</code></pre>\n",
        "question_body": "<p>I have pivoted the Customer ID against their year of purchase, so that I know how many times each customer purchased in different years:</p>\n<pre><code>Customer ID      1996  1997 ... 2019  2020\n100000000000001     7     7 ...  NaN   NaN\n100000000000002     8     8 ...  NaN   NaN\n100000000000003     7     4 ...  NaN   NaN\n100000000000004   NaN   NaN ...   21    24              \n100000000000005    17    11 ...   18   NaN\n</code></pre>\n<p>My desired result is to append the column names with the latest year of purchase, and thus the number of years since their last purchase:</p>\n<pre><code>Customer ID      1996  1997 ... 2019  2020   Last   Recency\n100000000000001     7     7 ...  NaN   NaN   1997        23 \n100000000000002     8     8 ...  NaN   NaN   1997        23\n100000000000003     7     4 ...  NaN   NaN   1997        23\n100000000000004   NaN   NaN ...   21    24   2020         0 \n100000000000005    17    11 ...   18   NaN   2019         1\n</code></pre>\n<p>Here is what I tried:</p>\n<pre><code>df_pivot[&quot;Last&quot;] = 2020\nk = 2020\nwhile math.isnan(df_pivot2[k]):\n    df_pivot[&quot;Last&quot;] = k-1\n    k = k-1\n\ndf_pivot[&quot;Recency&quot;] = 2020 - df_pivot[&quot;Last&quot;]\n</code></pre>\n<p>However what I got is &quot;TypeError: cannot convert the series to &lt;class 'float'&gt;&quot;</p>\n<p>Could anyone help me to get the result I need?</p>\n<p>Thanks a lot!</p>\n<p>Dennis</p>\n",
        "formatted_input": {
            "qid": 64032701,
            "link": "https://stackoverflow.com/questions/64032701/get-last-purchase-year-from-sales-data-pivot-in-pandas",
            "question": {
                "title": "Get &quot;Last Purchase Year&quot; from Sales Data Pivot in Pandas",
                "ques_desc": "I have pivoted the Customer ID against their year of purchase, so that I know how many times each customer purchased in different years: My desired result is to append the column names with the latest year of purchase, and thus the number of years since their last purchase: Here is what I tried: However what I got is \"TypeError: cannot convert the series to <class 'float'>\" Could anyone help me to get the result I need? Thanks a lot! Dennis "
            },
            "io": [
                "Customer ID      1996  1997 ... 2019  2020\n100000000000001     7     7 ...  NaN   NaN\n100000000000002     8     8 ...  NaN   NaN\n100000000000003     7     4 ...  NaN   NaN\n100000000000004   NaN   NaN ...   21    24              \n100000000000005    17    11 ...   18   NaN\n",
                "Customer ID      1996  1997 ... 2019  2020   Last   Recency\n100000000000001     7     7 ...  NaN   NaN   1997        23 \n100000000000002     8     8 ...  NaN   NaN   1997        23\n100000000000003     7     4 ...  NaN   NaN   1997        23\n100000000000004   NaN   NaN ...   21    24   2020         0 \n100000000000005    17    11 ...   18   NaN   2019         1\n"
            ],
            "answer": {
                "ans_desc": "You can get last year of purchase using + and along then subtract this last year of purchase from the max year to compute : ",
                "code": [
                    "c = df.filter(regex=r'\\d+').columns\ndf['Last'] = df[c].notna().cumsum(1).idxmax(1)\ndf['Recency'] = c.max() - df['Last']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 832,
            "user_id": 4463825,
            "user_type": "registered",
            "accept_rate": 46,
            "profile_image": "https://www.gravatar.com/avatar/1d305404404727e4c1dc80b8e020547a?s=128&d=identicon&r=PG&f=1",
            "display_name": "Jesh Kundem",
            "link": "https://stackoverflow.com/users/4463825/jesh-kundem"
        },
        "is_answered": true,
        "view_count": 104,
        "accepted_answer_id": 63974929,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1600568856,
        "creation_date": 1600566600,
        "question_id": 63974864,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63974864/dataframes-average-columns",
        "title": "DataFrames - Average Columns",
        "body": "<p>I have the following dataframe in pandas</p>\n<pre><code>Column 1   Column 2  Column3   Column 4\n   2           2         2         4\n   1           2         2         3\n</code></pre>\n<p>I am looking to create a dataframe which contains averages of columns 1&amp; 2, Columns 3 &amp;4, and so on.</p>\n<pre><code>  ColumnAvg(12)      ColumnAvg(34)\n      2                   3\n      1.5                 1.5\n</code></pre>\n<p>I was using this, but it is averaging everything.</p>\n<pre><code>df.mean(axis=1)\n</code></pre>\n<p>Is there a way, that I can add the column headers, when averaging each row.\nIf not, another way would be to create two arrays, average them and then create a new dataframe.</p>\n",
        "answer_body": "<p>you can use <code>groupby</code> and group it with `//2' th index, which will group it by exactly 2 columns</p>\n<pre><code>  df.groupby((np.arange(len(df.columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')\n</code></pre>\n<p>to start with index 2, as you mentioned in comment</p>\n<pre><code> dfgrp=  df.iloc[:,2:].groupby((np.arange(len(df.iloc[:,2:].columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')\n dfnew = pd.concat([df.iloc[:,2:],dfgrp])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe in pandas</p>\n<pre><code>Column 1   Column 2  Column3   Column 4\n   2           2         2         4\n   1           2         2         3\n</code></pre>\n<p>I am looking to create a dataframe which contains averages of columns 1&amp; 2, Columns 3 &amp;4, and so on.</p>\n<pre><code>  ColumnAvg(12)      ColumnAvg(34)\n      2                   3\n      1.5                 1.5\n</code></pre>\n<p>I was using this, but it is averaging everything.</p>\n<pre><code>df.mean(axis=1)\n</code></pre>\n<p>Is there a way, that I can add the column headers, when averaging each row.\nIf not, another way would be to create two arrays, average them and then create a new dataframe.</p>\n",
        "formatted_input": {
            "qid": 63974864,
            "link": "https://stackoverflow.com/questions/63974864/dataframes-average-columns",
            "question": {
                "title": "DataFrames - Average Columns",
                "ques_desc": "I have the following dataframe in pandas I am looking to create a dataframe which contains averages of columns 1& 2, Columns 3 &4, and so on. I was using this, but it is averaging everything. Is there a way, that I can add the column headers, when averaging each row. If not, another way would be to create two arrays, average them and then create a new dataframe. "
            },
            "io": [
                "Column 1   Column 2  Column3   Column 4\n   2           2         2         4\n   1           2         2         3\n",
                "  ColumnAvg(12)      ColumnAvg(34)\n      2                   3\n      1.5                 1.5\n"
            ],
            "answer": {
                "ans_desc": "you can use and group it with `//2' th index, which will group it by exactly 2 columns to start with index 2, as you mentioned in comment ",
                "code": [
                    "  df.groupby((np.arange(len(df.columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')\n",
                    " dfgrp=  df.iloc[:,2:].groupby((np.arange(len(df.iloc[:,2:].columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')\n dfnew = pd.concat([df.iloc[:,2:],dfgrp])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 14300012,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-_l_wYQ97Ra8/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucnr-cDGVjGMQjPYIQTwHB4nomXw0g/photo.jpg?sz=128",
            "display_name": "Hope ",
            "link": "https://stackoverflow.com/users/14300012/hope"
        },
        "is_answered": true,
        "view_count": 126,
        "accepted_answer_id": 63970492,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1600530178,
        "creation_date": 1600527917,
        "question_id": 63970182,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63970182/concat-list-of-dataframes-containing-empty-dataframes",
        "title": "Concat list of dataframes containing empty dataframes",
        "body": "<p>I have this list of dataframes and some of them are Empty. I do not want to discard them as it would change my number of rows when I concatenate it. I want to convert them into NA values so that these 5 dataframes are converted to 5 rows.</p>\n<p>List of dataframes:</p>\n<pre><code>[                 0              1              2      3\n 0  102,000,000.00    2,000,000.00  1,400,000.00   0.00 , Empty DataFrame\n Columns: []\n Index: [], Empty DataFrame\n Columns: []\n Index: [], Empty DataFrame\n Columns: []\n Index: [],                 0              1      2      3\n 0  60,900,000.00    1,300,000.00  0.00   0.00 ]\n</code></pre>\n<p>Code:</p>\n<pre><code>data = pd.concat(dataframes)\ndata = pd.DataFrame(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>0   102,000,000.00  2,000,000.00    1,400,000.00    0.00\n0   60,900,000.00   1,300,000.00    0.00            0.00\n</code></pre>\n<p>Desired Output:</p>\n<pre><code>0   102,000,000.00  2,000,000.00    1,400,000.00    0.00\n0   NA                   NA           NA             NA\n0   NA                   NA           NA             NA\n0   NA                   NA           NA             NA\n0   60,900,000.00   1,300,000.00    0.00            0.00\n</code></pre>\n",
        "answer_body": "<p>I would recommend a list comprehension approach where you update the empty data frames with a template data frame:</p>\n<pre><code>template = pd.DataFrame(data = [[pd.NA, pd.NA, pd.NA, pd.NA]], columns = [0,1,2,3])\ndataframes= [i if not i.empty else template for i in dataframes]\n</code></pre>\n<p>This updates data frames where <code>i.empty == True</code> with the data frame defined in <code>template</code>.  A working example:</p>\n<pre><code>\ndf1 = pd.DataFrame(data = [[102000000,2000000,1400000,0]], columns = [0,1,2,3])\ndf2 = pd.DataFrame()\ndf3 = pd.DataFrame()\ndf4 = pd.DataFrame()\ndf5 = pd.DataFrame(data = [[102000000,2000000,1400000,0]], columns = [0,1,2,3])\n\ndataframes = [df1, df2, df3,df4,df5]\n\ntemplate = pd.DataFrame(data = [[pd.NA, pd.NA, pd.NA, pd.NA]], columns = [0,1,2,3])\n\ndataframes= [i if not i.empty else template for i in dataframes]\n\npd.concat(dataframes)\n\n           0        1        2     3\n0  102000000  2000000  1400000     0\n0       &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;\n0       &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;\n0       &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;\n0  102000000  2000000  1400000     0\n\n</code></pre>\n",
        "question_body": "<p>I have this list of dataframes and some of them are Empty. I do not want to discard them as it would change my number of rows when I concatenate it. I want to convert them into NA values so that these 5 dataframes are converted to 5 rows.</p>\n<p>List of dataframes:</p>\n<pre><code>[                 0              1              2      3\n 0  102,000,000.00    2,000,000.00  1,400,000.00   0.00 , Empty DataFrame\n Columns: []\n Index: [], Empty DataFrame\n Columns: []\n Index: [], Empty DataFrame\n Columns: []\n Index: [],                 0              1      2      3\n 0  60,900,000.00    1,300,000.00  0.00   0.00 ]\n</code></pre>\n<p>Code:</p>\n<pre><code>data = pd.concat(dataframes)\ndata = pd.DataFrame(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>0   102,000,000.00  2,000,000.00    1,400,000.00    0.00\n0   60,900,000.00   1,300,000.00    0.00            0.00\n</code></pre>\n<p>Desired Output:</p>\n<pre><code>0   102,000,000.00  2,000,000.00    1,400,000.00    0.00\n0   NA                   NA           NA             NA\n0   NA                   NA           NA             NA\n0   NA                   NA           NA             NA\n0   60,900,000.00   1,300,000.00    0.00            0.00\n</code></pre>\n",
        "formatted_input": {
            "qid": 63970182,
            "link": "https://stackoverflow.com/questions/63970182/concat-list-of-dataframes-containing-empty-dataframes",
            "question": {
                "title": "Concat list of dataframes containing empty dataframes",
                "ques_desc": "I have this list of dataframes and some of them are Empty. I do not want to discard them as it would change my number of rows when I concatenate it. I want to convert them into NA values so that these 5 dataframes are converted to 5 rows. List of dataframes: Code: Output: Desired Output: "
            },
            "io": [
                "0   102,000,000.00  2,000,000.00    1,400,000.00    0.00\n0   60,900,000.00   1,300,000.00    0.00            0.00\n",
                "0   102,000,000.00  2,000,000.00    1,400,000.00    0.00\n0   NA                   NA           NA             NA\n0   NA                   NA           NA             NA\n0   NA                   NA           NA             NA\n0   60,900,000.00   1,300,000.00    0.00            0.00\n"
            ],
            "answer": {
                "ans_desc": "I would recommend a list comprehension approach where you update the empty data frames with a template data frame: This updates data frames where with the data frame defined in . A working example: ",
                "code": [
                    "template = pd.DataFrame(data = [[pd.NA, pd.NA, pd.NA, pd.NA]], columns = [0,1,2,3])\ndataframes= [i if not i.empty else template for i in dataframes]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "missing-data"
        ],
        "owner": {
            "reputation": 168,
            "user_id": 12155246,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mAdYrQbmhpCnXzv5r07Qr5y12eIUmy3TGWHfk9u=k-s128",
            "display_name": "Dandal",
            "link": "https://stackoverflow.com/users/12155246/dandal"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 63867640,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1600151318,
        "creation_date": 1599972618,
        "last_edit_date": 1600151318,
        "question_id": 63867276,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63867276/interpolates-one-series-and-outputs-constant-for-the-second-constant-series",
        "title": "Interpolates one series, and outputs constant for the second (constant) series",
        "body": "<p>I'm trying to create a function that fills in missing numbers in multiple series, with different numerical scales, and at the same time generates a constant column for each of the series.</p>\n<pre><code>from tika import parser\nimport pandas as pd\nimport numpy as np\nimport io\n\nrawtext = parser.from_file('D:\\Selenium\\Texto.txt')\ntext = rawtext['content']\nf = io.StringIO(text)\nf.readline()\ndata = f.read()\nf.readline()\ndef fill(d):\n    idx = range(d['col1'].min(), d['col1'].max() + 1)\n    return d.set_index('col1').reindex(idx, method='ffill').reset_index()\ng = df['col1'].lt(df['col1'].shift()).cumsum()\ndf = pd.concat([fill(g) for k, g in df.groupby(g)], ignore_index=True)\nprint(df)\n</code></pre>\n<p>Is it possible to create the following function with Pandas?</p>\n<p><strong>Sample dataframe:</strong></p>\n<pre><code>1029 400\n1035 400\n1031 340\n1039 340\n1020 503\n1025 503\n</code></pre>\n<p><strong>Expected output:</strong></p>\n<pre><code>1029 400\n1030 400\n1031 400\n1032 400\n1033 400\n1034 400\n1035 400\n1031 340\n1032 340\n1033 340\n1034 340\n1035 340\n1036 340\n1037 340\n1038 340\n1039 340\n1020 503\n1021 503\n1022 503\n1023 503\n1024 503\n1025 503\n</code></pre>\n",
        "answer_body": "<hr />\n<pre><code>def fill(d):\n    idx = range(d['col1'].min(), d['col1'].max() + 1)\n    return d.set_index('col1').reindex(idx, method='ffill').reset_index()\n\n\ng = df['col1'].lt(df['col1'].shift()).cumsum()\ndf = pd.concat([fill(g) for k, g in df.groupby(g)], ignore_index=True)\n</code></pre>\n<h3>Details:</h3>\n<p>Identify all the monotonically increasing sections in the <code>col1</code> where there are missing values. This can be done with the help of <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.lt.html\" rel=\"nofollow noreferrer\"><code>Series.lt</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html\" rel=\"nofollow noreferrer\"><code>Series.shift</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html\" rel=\"nofollow noreferrer\"><code>Series.cumsum</code></a> to create a grouper <code>g</code>:</p>\n<pre><code>print(g)\n0    0\n1    0\n2    1\n3    1\n4    2\n5    2\nName: col1, dtype: int64\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> the dataframe on this grouper and for each grouped frame <code>fill</code> the gaps using a custom define function which makes the use of <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html\" rel=\"nofollow noreferrer\"><code>reindex</code></a> method of the dataframe to fill the missing values.</p>\n<pre><code>print(fill(g)) # sample filled values for first group\n   col1  col2\n0  1029   400\n1  1030   400\n2  1031   400\n3  1032   400\n4  1033   400\n5  1034   400\n6  1035   400\n</code></pre>\n<p>Finally using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>pd.concat</code></a>, concatenate all these grouped frames after filling the missing values.</p>\n<pre><code>print(df)\n    col1  col2\n0   1029   400\n1   1030   400\n2   1031   400\n3   1032   400\n4   1033   400\n5   1034   400\n6   1035   400\n7   1031   340\n8   1032   340\n9   1033   340\n10  1034   340\n11  1035   340\n12  1036   340\n13  1037   340\n14  1038   340\n15  1039   340\n16  1020   503\n17  1021   503\n18  1022   503\n19  1023   503\n20  1024   503\n21  1025   503\n</code></pre>\n<hr />\n",
        "question_body": "<p>I'm trying to create a function that fills in missing numbers in multiple series, with different numerical scales, and at the same time generates a constant column for each of the series.</p>\n<pre><code>from tika import parser\nimport pandas as pd\nimport numpy as np\nimport io\n\nrawtext = parser.from_file('D:\\Selenium\\Texto.txt')\ntext = rawtext['content']\nf = io.StringIO(text)\nf.readline()\ndata = f.read()\nf.readline()\ndef fill(d):\n    idx = range(d['col1'].min(), d['col1'].max() + 1)\n    return d.set_index('col1').reindex(idx, method='ffill').reset_index()\ng = df['col1'].lt(df['col1'].shift()).cumsum()\ndf = pd.concat([fill(g) for k, g in df.groupby(g)], ignore_index=True)\nprint(df)\n</code></pre>\n<p>Is it possible to create the following function with Pandas?</p>\n<p><strong>Sample dataframe:</strong></p>\n<pre><code>1029 400\n1035 400\n1031 340\n1039 340\n1020 503\n1025 503\n</code></pre>\n<p><strong>Expected output:</strong></p>\n<pre><code>1029 400\n1030 400\n1031 400\n1032 400\n1033 400\n1034 400\n1035 400\n1031 340\n1032 340\n1033 340\n1034 340\n1035 340\n1036 340\n1037 340\n1038 340\n1039 340\n1020 503\n1021 503\n1022 503\n1023 503\n1024 503\n1025 503\n</code></pre>\n",
        "formatted_input": {
            "qid": 63867276,
            "link": "https://stackoverflow.com/questions/63867276/interpolates-one-series-and-outputs-constant-for-the-second-constant-series",
            "question": {
                "title": "Interpolates one series, and outputs constant for the second (constant) series",
                "ques_desc": "I'm trying to create a function that fills in missing numbers in multiple series, with different numerical scales, and at the same time generates a constant column for each of the series. Is it possible to create the following function with Pandas? Sample dataframe: Expected output: "
            },
            "io": [
                "1029 400\n1035 400\n1031 340\n1039 340\n1020 503\n1025 503\n",
                "1029 400\n1030 400\n1031 400\n1032 400\n1033 400\n1034 400\n1035 400\n1031 340\n1032 340\n1033 340\n1034 340\n1035 340\n1036 340\n1037 340\n1038 340\n1039 340\n1020 503\n1021 503\n1022 503\n1023 503\n1024 503\n1025 503\n"
            ],
            "answer": {
                "ans_desc": " Details: Identify all the monotonically increasing sections in the where there are missing values. This can be done with the help of + and to create a grouper : Then the dataframe on this grouper and for each grouped frame the gaps using a custom define function which makes the use of method of the dataframe to fill the missing values. Finally using , concatenate all these grouped frames after filling the missing values. ",
                "code": [
                    "def fill(d):\n    idx = range(d['col1'].min(), d['col1'].max() + 1)\n    return d.set_index('col1').reindex(idx, method='ffill').reset_index()\n\n\ng = df['col1'].lt(df['col1'].shift()).cumsum()\ndf = pd.concat([fill(g) for k, g in df.groupby(g)], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "drop"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 14257209,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-3-WDbBuQ_zk/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rerBwL3WOFojUJhTrG53Uqv9k831Q/photo.jpg?sz=128",
            "display_name": "R.S",
            "link": "https://stackoverflow.com/users/14257209/r-s"
        },
        "is_answered": true,
        "view_count": 56,
        "accepted_answer_id": 63855320,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1599870005,
        "creation_date": 1599863979,
        "last_edit_date": 1599868401,
        "question_id": 63855152,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63855152/drop-a-pandas-dataframe-row-that-comes-after-a-row-that-contains-a-particular-va",
        "title": "Drop a pandas DataFrame row that comes after a row that contains a particular value",
        "body": "<p>I am trying to drop all rows that come after a row which has <code>yes</code> inside the <code>'Ammend'</code> column</p>\n<p>df:</p>\n<pre><code>  Ammend\n 0  no\n 1  yes\n 2  no\n 3  no\n 4  yes\n 5  no\n</code></pre>\n<p>Required output df:</p>\n<pre><code>  Ammend\n 0  no\n 1  yes\n 3  no\n 4  yes\n</code></pre>\n<p>Look at the following code:</p>\n<pre><code>df = df.drop(df[df['Amended' == 'yes']], inplace=True)\n</code></pre>\n<p>Returns a <code>KeyError: False</code> error message</p>\n<p>I have tried many different variations of this using different methods like <code>.index.tolist()</code> and <code>.loc</code>\nbut I can't seem to figure it out anyway.</p>\n<p>I have also tried truncate:</p>\n<pre><code>filings_df.truncate(after=filings_df.loc[filings_df['Filings'] == '10-K/A'].index[0], before = filings_df.loc[filings_df['Filings'] == '10-K/A'].index[1])\n</code></pre>\n<p>This returns:</p>\n<blockquote>\n<p>IndexError: index 1 is out of bounds for axis 0 with size 1</p>\n</blockquote>\n",
        "answer_body": "<p>Try this</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nnp.random.seed(525)\ndf = pd.DataFrame({'Other': np.random.rand(10), 'Ammend': np.random.choice(['yes', 'no'], 10)})\ndf\n</code></pre>\n<pre><code>      Other Ammend\n0  0.750282     no\n1  0.379455     no\n2  0.766467    yes\n3  0.351025     no\n4  0.965993     no\n5  0.709159     no\n6  0.838831    yes\n7  0.218321     no\n8  0.573360    yes\n9  0.738974     no\n</code></pre>\n<p>Output:</p>\n<pre><code>df.drop(index=df[df['Ammend'].shift() == 'yes'].index)\n\n      Other Ammend\n0  0.750282     no\n1  0.379455     no\n2  0.766467    yes\n4  0.965993     no\n5  0.709159     no\n6  0.838831    yes\n8  0.573360    yes\n</code></pre>\n",
        "question_body": "<p>I am trying to drop all rows that come after a row which has <code>yes</code> inside the <code>'Ammend'</code> column</p>\n<p>df:</p>\n<pre><code>  Ammend\n 0  no\n 1  yes\n 2  no\n 3  no\n 4  yes\n 5  no\n</code></pre>\n<p>Required output df:</p>\n<pre><code>  Ammend\n 0  no\n 1  yes\n 3  no\n 4  yes\n</code></pre>\n<p>Look at the following code:</p>\n<pre><code>df = df.drop(df[df['Amended' == 'yes']], inplace=True)\n</code></pre>\n<p>Returns a <code>KeyError: False</code> error message</p>\n<p>I have tried many different variations of this using different methods like <code>.index.tolist()</code> and <code>.loc</code>\nbut I can't seem to figure it out anyway.</p>\n<p>I have also tried truncate:</p>\n<pre><code>filings_df.truncate(after=filings_df.loc[filings_df['Filings'] == '10-K/A'].index[0], before = filings_df.loc[filings_df['Filings'] == '10-K/A'].index[1])\n</code></pre>\n<p>This returns:</p>\n<blockquote>\n<p>IndexError: index 1 is out of bounds for axis 0 with size 1</p>\n</blockquote>\n",
        "formatted_input": {
            "qid": 63855152,
            "link": "https://stackoverflow.com/questions/63855152/drop-a-pandas-dataframe-row-that-comes-after-a-row-that-contains-a-particular-va",
            "question": {
                "title": "Drop a pandas DataFrame row that comes after a row that contains a particular value",
                "ques_desc": "I am trying to drop all rows that come after a row which has inside the column df: Required output df: Look at the following code: Returns a error message I have tried many different variations of this using different methods like and but I can't seem to figure it out anyway. I have also tried truncate: This returns: IndexError: index 1 is out of bounds for axis 0 with size 1 "
            },
            "io": [
                "  Ammend\n 0  no\n 1  yes\n 2  no\n 3  no\n 4  yes\n 5  no\n",
                "  Ammend\n 0  no\n 1  yes\n 3  no\n 4  yes\n"
            ],
            "answer": {
                "ans_desc": "Try this Output: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\nnp.random.seed(525)\ndf = pd.DataFrame({'Other': np.random.rand(10), 'Ammend': np.random.choice(['yes', 'no'], 10)})\ndf\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "split"
        ],
        "owner": {
            "reputation": 250,
            "user_id": 13906221,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/caf10cef4b7f7319565a87081431de10?s=128&d=identicon&r=PG&f=1",
            "display_name": "Aly",
            "link": "https://stackoverflow.com/users/13906221/aly"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 63849270,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1599835810,
        "creation_date": 1599834937,
        "question_id": 63849171,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63849171/how-to-split-dataframe-made-from-objects",
        "title": "How to split dataframe made from objects?",
        "body": "<p>I want to split one column pandas dataframe that look like this:</p>\n<pre><code>    0\n0   38 A\n1   35 B\n2   14 B\n</code></pre>\n<p>into two columns:</p>\n<p>So it can look like this:</p>\n<pre><code>    Number  Letter\n0   38        A\n1   35        B\n2   14        B\n</code></pre>\n<p>But its showing type as:</p>\n<pre><code>0    object\ndtype: object\n</code></pre>\n",
        "answer_body": "<p>You can do this a couple of ways and possibly more:</p>\n<p>Using: <code>df = pd.read_clipboard(sep='\\s\\s+')</code> capture dataframe above.</p>\n<h3>Option 1 (Use string accessor and split):</h3>\n<pre><code>df['0'].str.split(' ', expand=True).set_axis(['Number', 'Letter'], axis=1)\n</code></pre>\n<h3>Option 2 (use string accessor and extract with named groups regular expressions):</h3>\n<pre><code>df['0'].str.extract('(?P&lt;Number&gt;\\d{2})\\s(?P&lt;Letter&gt;[A-Za-z]{1})')\n</code></pre>\n<h3>Output:</h3>\n<pre><code>  Number Letter\n0     38      A\n1     35      B\n2     14      B\n</code></pre>\n",
        "question_body": "<p>I want to split one column pandas dataframe that look like this:</p>\n<pre><code>    0\n0   38 A\n1   35 B\n2   14 B\n</code></pre>\n<p>into two columns:</p>\n<p>So it can look like this:</p>\n<pre><code>    Number  Letter\n0   38        A\n1   35        B\n2   14        B\n</code></pre>\n<p>But its showing type as:</p>\n<pre><code>0    object\ndtype: object\n</code></pre>\n",
        "formatted_input": {
            "qid": 63849171,
            "link": "https://stackoverflow.com/questions/63849171/how-to-split-dataframe-made-from-objects",
            "question": {
                "title": "How to split dataframe made from objects?",
                "ques_desc": "I want to split one column pandas dataframe that look like this: into two columns: So it can look like this: But its showing type as: "
            },
            "io": [
                "    0\n0   38 A\n1   35 B\n2   14 B\n",
                "    Number  Letter\n0   38        A\n1   35        B\n2   14        B\n"
            ],
            "answer": {
                "ans_desc": "You can do this a couple of ways and possibly more: Using: capture dataframe above. Option 1 (Use string accessor and split): Option 2 (use string accessor and extract with named groups regular expressions): Output: ",
                "code": [
                    "df['0'].str.split(' ', expand=True).set_axis(['Number', 'Letter'], axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "split"
        ],
        "owner": {
            "reputation": 27,
            "user_id": 10381071,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/zi5yv.jpg?s=128&g=1",
            "display_name": "lweislo",
            "link": "https://stackoverflow.com/users/10381071/lweislo"
        },
        "is_answered": true,
        "view_count": 167,
        "accepted_answer_id": 63800531,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1599755072,
        "creation_date": 1599588479,
        "last_edit_date": 1599591010,
        "question_id": 63799400,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63799400/how-to-split-dataframe-at-headers-that-are-in-a-row",
        "title": "How to split dataframe at headers that are in a row",
        "body": "<p>I've got a page I'm scraping and most of the tables are in the format Heading --info. I can iterate through most of the tables and create separate dataframes for all the various information using pandas.read_html.</p>\n<p>However, there are some where they've combined information into one table with subheadings that I want to be separate dataframes with the text of that row as the heading (appending a list).</p>\n<p>Is there an easy way to split this dataframe - It will always be heading followed by associated rows, new heading followed by new associated rows.</p>\n<p>eg.</p>\n<pre><code>   Col1   Col2\n0  thing   \n1  1       2\n2  2       3\n3  thing2  \n4  1       2\n5  2       3\n6  3       4\n</code></pre>\n<p>Should be</p>\n<pre><code>thing\n1   1   1\n2   2   2\n\nthing2\n4   1   2\n5   2   3\n6   3   4\n</code></pre>\n<p>It'd be nice if people would just create web pages that made sense with the data but that's not the case here.</p>\n<p>I've tried iterrows but cannot seem to come up with a good way to create what I want.</p>\n<p>Help would be very much appreciated!</p>\n<pre><code>&lt;div class=&quot;ranking&quot;&gt;\n    &lt;h6&gt;&lt;a href=&quot;javascript:;&quot;&gt;Sprint&lt;/a&gt;&lt;/h6&gt;\n    &lt;table&gt;\n    &lt;tbody&gt;\n    \n    \n    &lt;/tbody&gt;\n    &lt;tbody&gt;\n    &lt;tr&gt;\n     &lt;td class=&quot;title&quot; colspan=&quot;8&quot;&gt;Canneto - km 137&lt;/td&gt;\n    &lt;/tr&gt;\n    \n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;1&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;21&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10010085859&quot; href=&quot;javascript:;&quot;&gt;\n    &lt;abbr title=&quot;Young rider&quot;&gt;*&lt;/abbr&gt;\n    BAGIOLI Nicola\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ANS.png&quot; title=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;5&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;2&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;54&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10008688453&quot; href=&quot;javascript:;&quot;&gt;\n    ORSINI Umberto\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;BARDIANI CSF FAIZANE'&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/BCF.png&quot; title=&quot;BARDIANI CSF FAIZANE'&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;3&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;3&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;247&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10005658114&quot; href=&quot;javascript:;&quot;&gt;\n    ZARDINI Edoardo\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;VINI ZABU' KTM&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/THR.png&quot; title=&quot;VINI ZABU' KTM&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;2&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;4&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;63&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10003349312&quot; href=&quot;javascript:;&quot;&gt;\n    BODNAR Maciej\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;BORA - HANSGROHE&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/BOH.png&quot; title=&quot;BORA - HANSGROHE&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;POL&quot; src=&quot;/Content/images/flags/POL.png&quot; title=&quot;POL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;1&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    \n    &lt;/tbody&gt;\n    &lt;tbody&gt;\n    &lt;tr&gt;\n     &lt;td class=&quot;title&quot; colspan=&quot;8&quot;&gt;Follonica - km 190&lt;/td&gt;\n    &lt;/tr&gt;\n    \n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;1&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;62&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007738055&quot; href=&quot;javascript:;&quot;&gt;\n    ACKERMANN Pascal\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;BORA - HANSGROHE&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/BOH.png&quot; title=&quot;BORA - HANSGROHE&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;GER&quot; src=&quot;/Content/images/flags/GER.png&quot; title=&quot;GER&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;12&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;2&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;231&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10008656828&quot; href=&quot;javascript:;&quot;&gt;\n    GAVIRIA RENDON Fernando\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;UAE TEAM EMIRATES&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/UAD.png&quot; title=&quot;UAE TEAM EMIRATES&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;COL&quot; src=&quot;/Content/images/flags/COL.png&quot; title=&quot;COL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;10&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;3&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;137&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007506366&quot; href=&quot;javascript:;&quot;&gt;\n    ZABEL Rick\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ISRAEL START - UP NATION&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ISN.png&quot; title=&quot;ISRAEL START - UP NATION&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;GER&quot; src=&quot;/Content/images/flags/GER.png&quot; title=&quot;GER&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;8&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;4&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;91&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10008661777&quot; href=&quot;javascript:;&quot;&gt;\n    BALLERINI Davide\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;DECEUNINCK  -  QUICK - STEP &quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/DQT.png&quot; title=&quot;DECEUNINCK  -  QUICK - STEP &quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;7&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;5&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;12&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007096239&quot; href=&quot;javascript:;&quot;&gt;\n    MERLIER Tim\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ALPECIN - FENIX&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/AFC.png&quot; title=&quot;ALPECIN - FENIX&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;BEL&quot; src=&quot;/Content/images/flags/BEL.png&quot; title=&quot;BEL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;6&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;more&quot; colspan=&quot;8&quot;&gt;&lt;a href=&quot;javascript:;&quot;&gt;More...&lt;/a&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;6&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;133&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10028417041&quot; href=&quot;javascript:;&quot;&gt;\n    CIMOLAI Davide\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ISRAEL START - UP NATION&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ISN.png&quot; title=&quot;ISRAEL START - UP NATION&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;5&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;7&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;213&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007216275&quot; href=&quot;javascript:;&quot;&gt;\n    MANZIN Lorrenzo\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;TOTAL DIRECT ENERGIE&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/TDE.png&quot; title=&quot;TOTAL DIRECT ENERGIE&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;FRA&quot; src=&quot;/Content/images/flags/FRA.png&quot; title=&quot;FRA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;4&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;8&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;23&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007744624&quot; href=&quot;javascript:;&quot;&gt;\n    PACIONI Luca\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ANS.png&quot; title=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;3&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;9&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;147&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10010946028&quot; href=&quot;javascript:;&quot;&gt;\n    &lt;abbr title=&quot;Young rider&quot;&gt;*&lt;/abbr&gt;\n    VERMEERSCH Florian\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;LOTTO SOUDAL&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/LTS.png&quot; title=&quot;LOTTO SOUDAL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;BEL&quot; src=&quot;/Content/images/flags/BEL.png&quot; title=&quot;BEL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;2&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;10&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;195&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10006631548&quot; href=&quot;javascript:;&quot;&gt;\n    TEUNISSEN Mike\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;JUMBO - VISMA&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/TJV.png&quot; title=&quot;JUMBO - VISMA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;NED&quot; src=&quot;/Content/images/flags/NED.png&quot; title=&quot;NED&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;1&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    \n    &lt;/tbody&gt;\n    &lt;/table&gt;\n&lt;/div&gt;\n</code></pre>\n",
        "answer_body": "<p>You can use <code>np.split()</code></p>\n<pre><code>import numpy as np\n\n\nres = [x.reset_index(drop=True) for x in np.split(df, np.where(df.applymap(lambda x: x == ''))[0]) if not x.empty]\nfor x in res:\n    x = x.rename(columns=x.iloc[0]).drop(x.index[0])\n    print(x)\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>  thing   \n1     1  2\n2     2  3\n  thing2   \n1      1  2\n2      2  3\n3      3  4\n</code></pre>\n",
        "question_body": "<p>I've got a page I'm scraping and most of the tables are in the format Heading --info. I can iterate through most of the tables and create separate dataframes for all the various information using pandas.read_html.</p>\n<p>However, there are some where they've combined information into one table with subheadings that I want to be separate dataframes with the text of that row as the heading (appending a list).</p>\n<p>Is there an easy way to split this dataframe - It will always be heading followed by associated rows, new heading followed by new associated rows.</p>\n<p>eg.</p>\n<pre><code>   Col1   Col2\n0  thing   \n1  1       2\n2  2       3\n3  thing2  \n4  1       2\n5  2       3\n6  3       4\n</code></pre>\n<p>Should be</p>\n<pre><code>thing\n1   1   1\n2   2   2\n\nthing2\n4   1   2\n5   2   3\n6   3   4\n</code></pre>\n<p>It'd be nice if people would just create web pages that made sense with the data but that's not the case here.</p>\n<p>I've tried iterrows but cannot seem to come up with a good way to create what I want.</p>\n<p>Help would be very much appreciated!</p>\n<pre><code>&lt;div class=&quot;ranking&quot;&gt;\n    &lt;h6&gt;&lt;a href=&quot;javascript:;&quot;&gt;Sprint&lt;/a&gt;&lt;/h6&gt;\n    &lt;table&gt;\n    &lt;tbody&gt;\n    \n    \n    &lt;/tbody&gt;\n    &lt;tbody&gt;\n    &lt;tr&gt;\n     &lt;td class=&quot;title&quot; colspan=&quot;8&quot;&gt;Canneto - km 137&lt;/td&gt;\n    &lt;/tr&gt;\n    \n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;1&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;21&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10010085859&quot; href=&quot;javascript:;&quot;&gt;\n    &lt;abbr title=&quot;Young rider&quot;&gt;*&lt;/abbr&gt;\n    BAGIOLI Nicola\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ANS.png&quot; title=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;5&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;2&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;54&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10008688453&quot; href=&quot;javascript:;&quot;&gt;\n    ORSINI Umberto\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;BARDIANI CSF FAIZANE'&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/BCF.png&quot; title=&quot;BARDIANI CSF FAIZANE'&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;3&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;3&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;247&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10005658114&quot; href=&quot;javascript:;&quot;&gt;\n    ZARDINI Edoardo\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;VINI ZABU' KTM&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/THR.png&quot; title=&quot;VINI ZABU' KTM&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;2&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;4&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;63&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10003349312&quot; href=&quot;javascript:;&quot;&gt;\n    BODNAR Maciej\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;BORA - HANSGROHE&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/BOH.png&quot; title=&quot;BORA - HANSGROHE&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;POL&quot; src=&quot;/Content/images/flags/POL.png&quot; title=&quot;POL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;1&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    \n    &lt;/tbody&gt;\n    &lt;tbody&gt;\n    &lt;tr&gt;\n     &lt;td class=&quot;title&quot; colspan=&quot;8&quot;&gt;Follonica - km 190&lt;/td&gt;\n    &lt;/tr&gt;\n    \n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;1&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;62&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007738055&quot; href=&quot;javascript:;&quot;&gt;\n    ACKERMANN Pascal\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;BORA - HANSGROHE&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/BOH.png&quot; title=&quot;BORA - HANSGROHE&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;GER&quot; src=&quot;/Content/images/flags/GER.png&quot; title=&quot;GER&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;12&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;2&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;231&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10008656828&quot; href=&quot;javascript:;&quot;&gt;\n    GAVIRIA RENDON Fernando\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;UAE TEAM EMIRATES&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/UAD.png&quot; title=&quot;UAE TEAM EMIRATES&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;COL&quot; src=&quot;/Content/images/flags/COL.png&quot; title=&quot;COL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;10&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;3&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;137&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007506366&quot; href=&quot;javascript:;&quot;&gt;\n    ZABEL Rick\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ISRAEL START - UP NATION&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ISN.png&quot; title=&quot;ISRAEL START - UP NATION&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;GER&quot; src=&quot;/Content/images/flags/GER.png&quot; title=&quot;GER&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;8&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;4&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;91&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10008661777&quot; href=&quot;javascript:;&quot;&gt;\n    BALLERINI Davide\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;DECEUNINCK  -  QUICK - STEP &quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/DQT.png&quot; title=&quot;DECEUNINCK  -  QUICK - STEP &quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;7&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;5&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;12&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007096239&quot; href=&quot;javascript:;&quot;&gt;\n    MERLIER Tim\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ALPECIN - FENIX&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/AFC.png&quot; title=&quot;ALPECIN - FENIX&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;BEL&quot; src=&quot;/Content/images/flags/BEL.png&quot; title=&quot;BEL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;6&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr&gt;\n    &lt;td class=&quot;more&quot; colspan=&quot;8&quot;&gt;&lt;a href=&quot;javascript:;&quot;&gt;More...&lt;/a&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;6&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;133&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10028417041&quot; href=&quot;javascript:;&quot;&gt;\n    CIMOLAI Davide\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ISRAEL START - UP NATION&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ISN.png&quot; title=&quot;ISRAEL START - UP NATION&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;5&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;7&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;213&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007216275&quot; href=&quot;javascript:;&quot;&gt;\n    MANZIN Lorrenzo\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;TOTAL DIRECT ENERGIE&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/TDE.png&quot; title=&quot;TOTAL DIRECT ENERGIE&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;FRA&quot; src=&quot;/Content/images/flags/FRA.png&quot; title=&quot;FRA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;4&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;8&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;23&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10007744624&quot; href=&quot;javascript:;&quot;&gt;\n    PACIONI Luca\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/ANS.png&quot; title=&quot;ANDRONI GIOCATTOLI - SIDERMEC&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;ITA&quot; src=&quot;/Content/images/flags/ITA.png&quot; title=&quot;ITA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;3&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;9&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;147&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10010946028&quot; href=&quot;javascript:;&quot;&gt;\n    &lt;abbr title=&quot;Young rider&quot;&gt;*&lt;/abbr&gt;\n    VERMEERSCH Florian\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;LOTTO SOUDAL&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/LTS.png&quot; title=&quot;LOTTO SOUDAL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;BEL&quot; src=&quot;/Content/images/flags/BEL.png&quot; title=&quot;BEL&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;2&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    &lt;tr style=&quot;display: none;&quot;&gt;\n    &lt;td class=&quot;rank&quot;&gt;&lt;span title=&quot;Rank&quot;&gt;10&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;any-progression&quot;&gt;&lt;/td&gt;\n    &lt;td class=&quot;bib&quot;&gt;&lt;span title=&quot;Bib&quot;&gt;195&lt;/span&gt;&lt;/td&gt;\n    &lt;td class=&quot;name&quot;&gt;\n    &lt;a class=&quot;10006631548&quot; href=&quot;javascript:;&quot;&gt;\n    TEUNISSEN Mike\n    &lt;/a&gt;\n    \n    &lt;/td&gt;\n    &lt;td class=&quot;team&quot;&gt;&lt;img alt=&quot;JUMBO - VISMA&quot; src=&quot;/Content/images/event/2020/tirreno-adriatico/jerseys/TJV.png&quot; title=&quot;JUMBO - VISMA&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;noc&quot;&gt;&lt;img alt=&quot;NED&quot; src=&quot;/Content/images/flags/NED.png&quot; title=&quot;NED&quot;/&gt;&lt;/td&gt;\n    \n    &lt;td class=&quot;bonif&quot;&gt;\n    &lt;/td&gt;\n    &lt;td class=&quot;points&quot; title=&quot;Points&quot;&gt;1&lt;/td&gt;\n    \n    &lt;/tr&gt;\n    \n    &lt;/tbody&gt;\n    &lt;/table&gt;\n&lt;/div&gt;\n</code></pre>\n",
        "formatted_input": {
            "qid": 63799400,
            "link": "https://stackoverflow.com/questions/63799400/how-to-split-dataframe-at-headers-that-are-in-a-row",
            "question": {
                "title": "How to split dataframe at headers that are in a row",
                "ques_desc": "I've got a page I'm scraping and most of the tables are in the format Heading --info. I can iterate through most of the tables and create separate dataframes for all the various information using pandas.read_html. However, there are some where they've combined information into one table with subheadings that I want to be separate dataframes with the text of that row as the heading (appending a list). Is there an easy way to split this dataframe - It will always be heading followed by associated rows, new heading followed by new associated rows. eg. Should be It'd be nice if people would just create web pages that made sense with the data but that's not the case here. I've tried iterrows but cannot seem to come up with a good way to create what I want. Help would be very much appreciated! "
            },
            "io": [
                "   Col1   Col2\n0  thing   \n1  1       2\n2  2       3\n3  thing2  \n4  1       2\n5  2       3\n6  3       4\n",
                "thing\n1   1   1\n2   2   2\n\nthing2\n4   1   2\n5   2   3\n6   3   4\n"
            ],
            "answer": {
                "ans_desc": "You can use Output: ",
                "code": [
                    "import numpy as np\n\n\nres = [x.reset_index(drop=True) for x in np.split(df, np.where(df.applymap(lambda x: x == ''))[0]) if not x.empty]\nfor x in res:\n    x = x.rename(columns=x.iloc[0]).drop(x.index[0])\n    print(x)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 14235045,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0456c50e9a857b94549c59ab46969311?s=128&d=identicon&r=PG&f=1",
            "display_name": "Jun_1812",
            "link": "https://stackoverflow.com/users/14235045/jun-1812"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 63779684,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1599489359,
        "creation_date": 1599487783,
        "last_edit_date": 1599488237,
        "question_id": 63779231,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63779231/pandas-fill-in-missing-data-from-columns-in-other-rows",
        "title": "Pandas fill in missing data from columns in other rows",
        "body": "<p>I have a df like below:</p>\n<pre><code>df = pd.DataFrame({'id': ['a','b','c','d'],\n                   'ac' : ['123','223', np.nan, np.nan],\n                   'prev' : [np.nan, np.nan, 'a','b']})\n</code></pre>\n<p>Output:</p>\n<pre><code>    id  ac   prev\n0   a   123  NaN  \n1   b   223  NaN  \n2   c   NaN  a  \n3   d   NaN  b\n</code></pre>\n<p>For ac are null, get prev's value, and then look up at the id column.  Fill in the null with value at ac column.</p>\n<p>Expected output:</p>\n<pre><code>    id  ac   prev\n0   a   123  NaN\n1   b   223  NaN\n2   c   123  a\n3   d   223  b\n</code></pre>\n<p>How do I achieve this?  Thanks.</p>\n",
        "answer_body": "<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isna.html\" rel=\"nofollow noreferrer\"><code>Series.isna</code></a> to create a boolean mask the use boolean indexing with this mask to <code>map</code> the values of <code>prev</code> column to <code>ac</code> column based on <code>id</code>:</p>\n<pre><code>m = df['ac'].isna()\ndf.loc[m, 'ac'] = df.loc[m, 'prev'].map(df.set_index('id')['ac'])\n</code></pre>\n<p>Result:</p>\n<pre><code>  id   ac prev\n0  a  123  NaN\n1  b  223  NaN\n2  c  123    a\n3  d  223    b\n</code></pre>\n",
        "question_body": "<p>I have a df like below:</p>\n<pre><code>df = pd.DataFrame({'id': ['a','b','c','d'],\n                   'ac' : ['123','223', np.nan, np.nan],\n                   'prev' : [np.nan, np.nan, 'a','b']})\n</code></pre>\n<p>Output:</p>\n<pre><code>    id  ac   prev\n0   a   123  NaN  \n1   b   223  NaN  \n2   c   NaN  a  \n3   d   NaN  b\n</code></pre>\n<p>For ac are null, get prev's value, and then look up at the id column.  Fill in the null with value at ac column.</p>\n<p>Expected output:</p>\n<pre><code>    id  ac   prev\n0   a   123  NaN\n1   b   223  NaN\n2   c   123  a\n3   d   223  b\n</code></pre>\n<p>How do I achieve this?  Thanks.</p>\n",
        "formatted_input": {
            "qid": 63779231,
            "link": "https://stackoverflow.com/questions/63779231/pandas-fill-in-missing-data-from-columns-in-other-rows",
            "question": {
                "title": "Pandas fill in missing data from columns in other rows",
                "ques_desc": "I have a df like below: Output: For ac are null, get prev's value, and then look up at the id column. Fill in the null with value at ac column. Expected output: How do I achieve this? Thanks. "
            },
            "io": [
                "    id  ac   prev\n0   a   123  NaN  \n1   b   223  NaN  \n2   c   NaN  a  \n3   d   NaN  b\n",
                "    id  ac   prev\n0   a   123  NaN\n1   b   223  NaN\n2   c   123  a\n3   d   223  b\n"
            ],
            "answer": {
                "ans_desc": "You can use to create a boolean mask the use boolean indexing with this mask to the values of column to column based on : Result: ",
                "code": [
                    "m = df['ac'].isna()\ndf.loc[m, 'ac'] = df.loc[m, 'prev'].map(df.set_index('id')['ac'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 635,
            "user_id": 2681631,
            "user_type": "registered",
            "accept_rate": 54,
            "profile_image": "https://i.stack.imgur.com/dzBgE.jpg?s=128&g=1",
            "display_name": "dynobo",
            "link": "https://stackoverflow.com/users/2681631/dynobo"
        },
        "is_answered": true,
        "view_count": 968,
        "closed_date": 1517056260,
        "accepted_answer_id": 48475758,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1599359755,
        "creation_date": 1517055263,
        "last_edit_date": 1517056039,
        "question_id": 48475687,
        "link": "https://stackoverflow.com/questions/48475687/python-combine-two-pandas-dataframes-extend-index-if-needed",
        "closed_reason": "Duplicate",
        "title": "Python: Combine two Pandas Dataframes, extend index if needed",
        "body": "<p><strong>How can I combine two Dataframes into one with keping all rows and all index values of both Dataframes?</strong></p>\n\n<p>Let's say, I have two dataframes, with <em>partly different index values</em>:</p>\n\n<pre><code>df1 = pd.DataFrame(np.random.randn(5, 1), columns=['a'], index=[0, 2, 3, 4, 5])\ndf2 = pd.DataFrame(np.random.randn(5, 1), columns=['b'], index=[1, 2, 3, 4, 6])\n\n         a\n0 -1.089084\n2 -0.552297\n3 -0.242239\n4  0.247463\n5 -0.139740\n\n          b\n1 -0.407245\n2  1.704591\n3 -0.803438\n4 -1.511515\n6  0.303360\n</code></pre>\n\n<p>I want to create a new dataframe, which contains both columns with a combined index. I tried:</p>\n\n<pre><code>df_combine = pd.DataFrame()\n\ndf_combine['a'] = df1['a']\ndf_combine['b'] = df2['b']\n</code></pre>\n\n<p>which results in:</p>\n\n<pre><code>          a         b\n0 -1.089084       NaN\n2 -0.552297  1.704591\n3 -0.242239 -0.803438\n4  0.247463 -1.511515\n5 -0.139740       NaN\n</code></pre>\n\n<p>where I would like to have, all rows &amp; index values preserved, with NaN, if no value is available for this index value:</p>\n\n<pre><code>          a         b\n0 -1.089084       NaN\n1       NaN -0.407245\n2 -0.552297  1.704591\n3 -0.242239 -0.803438\n4  0.247463 -1.511515\n5 -0.139740       NaN\n6       NaN  0.303360\n</code></pre>\n",
        "answer_body": "<p>Try pandas.concat function: <a href=\"https://pandas.pydata.org/pandas-docs/stable/merging.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/merging.html</a></p>\n<pre><code>dd = pd.concat([df1, df2], axis=1)\nprint(dd)\n</code></pre>\n<p>Output:</p>\n<pre><code>          a         b\n0 -0.603074       NaN\n1       NaN -0.021821\n2  0.501050  0.342474\n3 -2.612637 -0.256383\n4  0.095779 -1.423016\n5 -0.644108       NaN\n6       NaN -1.756023\n</code></pre>\n",
        "question_body": "<p><strong>How can I combine two Dataframes into one with keping all rows and all index values of both Dataframes?</strong></p>\n\n<p>Let's say, I have two dataframes, with <em>partly different index values</em>:</p>\n\n<pre><code>df1 = pd.DataFrame(np.random.randn(5, 1), columns=['a'], index=[0, 2, 3, 4, 5])\ndf2 = pd.DataFrame(np.random.randn(5, 1), columns=['b'], index=[1, 2, 3, 4, 6])\n\n         a\n0 -1.089084\n2 -0.552297\n3 -0.242239\n4  0.247463\n5 -0.139740\n\n          b\n1 -0.407245\n2  1.704591\n3 -0.803438\n4 -1.511515\n6  0.303360\n</code></pre>\n\n<p>I want to create a new dataframe, which contains both columns with a combined index. I tried:</p>\n\n<pre><code>df_combine = pd.DataFrame()\n\ndf_combine['a'] = df1['a']\ndf_combine['b'] = df2['b']\n</code></pre>\n\n<p>which results in:</p>\n\n<pre><code>          a         b\n0 -1.089084       NaN\n2 -0.552297  1.704591\n3 -0.242239 -0.803438\n4  0.247463 -1.511515\n5 -0.139740       NaN\n</code></pre>\n\n<p>where I would like to have, all rows &amp; index values preserved, with NaN, if no value is available for this index value:</p>\n\n<pre><code>          a         b\n0 -1.089084       NaN\n1       NaN -0.407245\n2 -0.552297  1.704591\n3 -0.242239 -0.803438\n4  0.247463 -1.511515\n5 -0.139740       NaN\n6       NaN  0.303360\n</code></pre>\n",
        "formatted_input": {
            "qid": 48475687,
            "link": "https://stackoverflow.com/questions/48475687/python-combine-two-pandas-dataframes-extend-index-if-needed",
            "question": {
                "title": "Python: Combine two Pandas Dataframes, extend index if needed",
                "ques_desc": "How can I combine two Dataframes into one with keping all rows and all index values of both Dataframes? Let's say, I have two dataframes, with partly different index values: I want to create a new dataframe, which contains both columns with a combined index. I tried: which results in: where I would like to have, all rows & index values preserved, with NaN, if no value is available for this index value: "
            },
            "io": [
                "          a         b\n0 -1.089084       NaN\n2 -0.552297  1.704591\n3 -0.242239 -0.803438\n4  0.247463 -1.511515\n5 -0.139740       NaN\n",
                "          a         b\n0 -1.089084       NaN\n1       NaN -0.407245\n2 -0.552297  1.704591\n3 -0.242239 -0.803438\n4  0.247463 -1.511515\n5 -0.139740       NaN\n6       NaN  0.303360\n"
            ],
            "answer": {
                "ans_desc": "Try pandas.concat function: https://pandas.pydata.org/pandas-docs/stable/merging.html Output: ",
                "code": [
                    "dd = pd.concat([df1, df2], axis=1)\nprint(dd)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pivot-table"
        ],
        "owner": {
            "reputation": 550,
            "user_id": 5031397,
            "user_type": "registered",
            "accept_rate": 33,
            "profile_image": "https://lh3.googleusercontent.com/-7o0CJBsl-8k/AAAAAAAAAAI/AAAAAAAARp0/VQwhXZ8i_Hg/photo.jpg?sz=128",
            "display_name": "Mehdi Saffar",
            "link": "https://stackoverflow.com/users/5031397/mehdi-saffar"
        },
        "is_answered": true,
        "view_count": 1476,
        "accepted_answer_id": 63757684,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1599336941,
        "creation_date": 1599332502,
        "question_id": 63757556,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63757556/heatmap-of-counts-of-every-value-in-every-column",
        "title": "Heatmap of counts of every value in every column",
        "body": "<p>I have a dataframe that is like this:</p>\n<pre><code>| A | B | C  | D |  \n|---|---|----|---|  \n| 1 | 3 | 10 | 4 |  \n| 2 | 3 | 1  | 5 |  \n| 1 | 7 | 9  | 3 |  \n</code></pre>\n<p>Where A B C D are categories, and the values are in the range [1, 10] (some values might not appear in a single column)</p>\n<p>I would like to have a dataframe that for every category shows the count of those values. Something like this:</p>\n<pre><code>|    | A | B  | C | D |\n|----|---|----|---|---|  \n| 1  | 2 | 0  | 1 | 0 |\n| 2  | 1 | 0  | 0 | 0 |\n| 3  | 0 | 2  | 0 | 1 |\n| 4  | 0 | 0  | 0 | 1 |\n| 5  | 0 | 0  | 0 | 1 |\n| 6  | 0 | 0  | 0 | 0 |\n| 7  | 0 | 1  | 0 | 0 |\n| 8  | 0 | 0  | 0 | 0 |\n| 9  | 0 | 0  | 1 | 0 |\n| 10 | 0 | 0  | 1 | 0 | \n</code></pre>\n<p>I tried using <code>groupby</code> and <code>pivot_table</code> but I can't seem to understand what parameters to give.</p>\n",
        "answer_body": "<ul>\n<li>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\" rel=\"nofollow noreferrer\"><code>pandas.Series.value_counts</code></a> applies for each column</li>\n<li><a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\" rel=\"nofollow noreferrer\"><code>seaborn.heatmap</code></a> will plot a <code>DataFrame</code>\n<ul>\n<li><em>If a Pandas DataFrame is provided, the index/column information will be used to label the columns and rows.</em></li>\n</ul>\n</li>\n</ul>\n<h2>Option 1</h2>\n<pre class=\"lang-py prettyprint-override\"><code>import seaborn as sns\nimport pandas as pd\n\n# dataframe setup\ndata = {'A': [1, 2, 1], 'B': [3, 3, 7], 'C': [10, 1, 9], 'D': [4, 5, 3]}\ndf = pd.DataFrame(data)\n\n# create a dataframe of the counts for each column\ncounts = df.apply(pd.value_counts)\n\n# display(count)\n      A    B    C    D\n1   2.0  NaN  1.0  NaN\n2   1.0  NaN  NaN  NaN\n3   NaN  2.0  NaN  1.0\n4   NaN  NaN  NaN  1.0\n5   NaN  NaN  NaN  1.0\n7   NaN  1.0  NaN  NaN\n9   NaN  NaN  1.0  NaN\n10  NaN  NaN  1.0  NaN\n\n# plot\nsns.heatmap(counts)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/VG03V.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/VG03V.png\" alt=\"enter image description here\" /></a></p>\n<h2>Option 2</h2>\n<ul>\n<li>There are a number of style options available with heatmap, and changing the color with <code>cmap</code> can improve the visualization.\n<ul>\n<li><a href=\"https://seaborn.pydata.org/tutorial/color_palettes.html\" rel=\"nofollow noreferrer\">seaborn: palettes</a></li>\n</ul>\n</li>\n<li>I think Option 1, without <code>.fillna(0)</code> looks less busy.</li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code># counts\ncounts = df.apply(pd.value_counts).fillna(0)\n\n# plot\nsns.heatmap(counts, cmap=&quot;GnBu&quot;, annot=True)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/vH0QU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vH0QU.png\" alt=\"enter image description here\" /></a></p>\n<h3>default color</h3>\n<pre class=\"lang-py prettyprint-override\"><code>sns.heatmap(counts, annot=True)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/BOba8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/BOba8.png\" alt=\"enter image description here\" /></a></p>\n",
        "question_body": "<p>I have a dataframe that is like this:</p>\n<pre><code>| A | B | C  | D |  \n|---|---|----|---|  \n| 1 | 3 | 10 | 4 |  \n| 2 | 3 | 1  | 5 |  \n| 1 | 7 | 9  | 3 |  \n</code></pre>\n<p>Where A B C D are categories, and the values are in the range [1, 10] (some values might not appear in a single column)</p>\n<p>I would like to have a dataframe that for every category shows the count of those values. Something like this:</p>\n<pre><code>|    | A | B  | C | D |\n|----|---|----|---|---|  \n| 1  | 2 | 0  | 1 | 0 |\n| 2  | 1 | 0  | 0 | 0 |\n| 3  | 0 | 2  | 0 | 1 |\n| 4  | 0 | 0  | 0 | 1 |\n| 5  | 0 | 0  | 0 | 1 |\n| 6  | 0 | 0  | 0 | 0 |\n| 7  | 0 | 1  | 0 | 0 |\n| 8  | 0 | 0  | 0 | 0 |\n| 9  | 0 | 0  | 1 | 0 |\n| 10 | 0 | 0  | 1 | 0 | \n</code></pre>\n<p>I tried using <code>groupby</code> and <code>pivot_table</code> but I can't seem to understand what parameters to give.</p>\n",
        "formatted_input": {
            "qid": 63757556,
            "link": "https://stackoverflow.com/questions/63757556/heatmap-of-counts-of-every-value-in-every-column",
            "question": {
                "title": "Heatmap of counts of every value in every column",
                "ques_desc": "I have a dataframe that is like this: Where A B C D are categories, and the values are in the range [1, 10] (some values might not appear in a single column) I would like to have a dataframe that for every category shows the count of those values. Something like this: I tried using and but I can't seem to understand what parameters to give. "
            },
            "io": [
                "| A | B | C  | D |  \n|---|---|----|---|  \n| 1 | 3 | 10 | 4 |  \n| 2 | 3 | 1  | 5 |  \n| 1 | 7 | 9  | 3 |  \n",
                "|    | A | B  | C | D |\n|----|---|----|---|---|  \n| 1  | 2 | 0  | 1 | 0 |\n| 2  | 1 | 0  | 0 | 0 |\n| 3  | 0 | 2  | 0 | 1 |\n| 4  | 0 | 0  | 0 | 1 |\n| 5  | 0 | 0  | 0 | 1 |\n| 6  | 0 | 0  | 0 | 0 |\n| 7  | 0 | 1  | 0 | 0 |\n| 8  | 0 | 0  | 0 | 0 |\n| 9  | 0 | 0  | 1 | 0 |\n| 10 | 0 | 0  | 1 | 0 | \n"
            ],
            "answer": {
                "ans_desc": " Use applies for each column will plot a If a Pandas DataFrame is provided, the index/column information will be used to label the columns and rows. Option 1 Option 2 There are a number of style options available with heatmap, and changing the color with can improve the visualization. seaborn: palettes I think Option 1, without looks less busy. default color ",
                "code": [
                    "# counts\ncounts = df.apply(pd.value_counts).fillna(0)\n\n# plot\nsns.heatmap(counts, cmap=\"GnBu\", annot=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 141,
            "user_id": 13365279,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d448f2356b808a1fcb2bb3ec720d97df?s=128&d=identicon&r=PG&f=1",
            "display_name": "stackoverflow",
            "link": "https://stackoverflow.com/users/13365279/stackoverflow"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 63713144,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1599080061,
        "creation_date": 1599077387,
        "last_edit_date": 1599079103,
        "question_id": 63712894,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63712894/how-to-re-order-this-dataframe-in-python-wthout-hard-coding-column-values",
        "title": "How to re order this DataFrame in Python wthout hard coding column values?",
        "body": "<p>I have a df ('COL3 SUM' is the full name with a space):</p>\n<pre><code>COL1 COL2 COL3 SUM  COL4  COL5\n   1     2       3     4     5\n</code></pre>\n<p>How can I re order this df so that 'COL3 SUM' always comes at the end of the dataframe like so without re ordering any of the rest of the df?</p>\n<pre><code>   COL1 COL2  COL4  COL5 COL3 SUM \n       1    2    4     5        3\n</code></pre>\n",
        "answer_body": "<p>If you want to move all columns with some keyword e.g. <code>&quot;SUM&quot;</code> to the end:</p>\n<ul>\n<li>create list of unmoved columns</li>\n<li>append excluded columns to end</li>\n<li>reorder dataframe by calling on new column list</li>\n</ul>\n<p>code:</p>\n<pre><code>new_cols = [col for col in df.columns if &quot;SUM&quot; not in col]\nmoved_cols = list(set(df.columns) - set(new_cols)) \nnew_cols.extend(moved_cols)\ndf = df[new_cols]\n</code></pre>\n<p>output:</p>\n<pre><code>   COL1  COL2  COL4  COL5  COL3 SUM\n0     1     2     4     5         3\n</code></pre>\n",
        "question_body": "<p>I have a df ('COL3 SUM' is the full name with a space):</p>\n<pre><code>COL1 COL2 COL3 SUM  COL4  COL5\n   1     2       3     4     5\n</code></pre>\n<p>How can I re order this df so that 'COL3 SUM' always comes at the end of the dataframe like so without re ordering any of the rest of the df?</p>\n<pre><code>   COL1 COL2  COL4  COL5 COL3 SUM \n       1    2    4     5        3\n</code></pre>\n",
        "formatted_input": {
            "qid": 63712894,
            "link": "https://stackoverflow.com/questions/63712894/how-to-re-order-this-dataframe-in-python-wthout-hard-coding-column-values",
            "question": {
                "title": "How to re order this DataFrame in Python wthout hard coding column values?",
                "ques_desc": "I have a df ('COL3 SUM' is the full name with a space): How can I re order this df so that 'COL3 SUM' always comes at the end of the dataframe like so without re ordering any of the rest of the df? "
            },
            "io": [
                "COL1 COL2 COL3 SUM  COL4  COL5\n   1     2       3     4     5\n",
                "   COL1 COL2  COL4  COL5 COL3 SUM \n       1    2    4     5        3\n"
            ],
            "answer": {
                "ans_desc": "If you want to move all columns with some keyword e.g. to the end: create list of unmoved columns append excluded columns to end reorder dataframe by calling on new column list code: output: ",
                "code": [
                    "new_cols = [col for col in df.columns if \"SUM\" not in col]\nmoved_cols = list(set(df.columns) - set(new_cols)) \nnew_cols.extend(moved_cols)\ndf = df[new_cols]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pivot-table",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 505,
            "user_id": 6389099,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/621d0cd73aadad362cd12870cbdee413?s=128&d=identicon&r=PG&f=1",
            "display_name": "RSM",
            "link": "https://stackoverflow.com/users/6389099/rsm"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 63003104,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1598971537,
        "creation_date": 1595266639,
        "last_edit_date": 1595267052,
        "question_id": 63000962,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63000962/python-selecting-rows-matching-feb-till-current-month-in-pandas-dataframe",
        "title": "Python: Selecting rows matching Feb till current month in pandas dataframe",
        "body": "<p>I have below dataframe:</p>\n<pre><code>Name1   Name2   Month   Mode    Value1 Value2\nA       N       Sep     Plan    9       9\nB       N       Nov     Plan    6       6\nB       N       Jan     Plan    6       6\nC       N       Feb     Actual  4       4\nC       N       Jul     Actual  3       3\nD       N       May     Plan    2       2\nE       N       Apr     Actual  6       6\nF       N       Feb     Actual  7       7\nE       N       May     Actual  4       4\nF       N       Jun     Plan    3       3\n&lt;+ 100 more rows&gt;\n</code></pre>\n<p>Considering the current month is June, the expected output is as follows:</p>\n<pre><code>Name1   Name2   Mode    Value1  Value2\nC       N       Actual    4       4                                             \nD       N       Plan      2       2                                             \nE       N       Actual    10      10                                                \nF       N       Actual    7       7                                             \nF       N       Plan      3       3                                             \n</code></pre>\n<p>Here i have done filtering of rows  by months from Feb:Current Month (in this case June) and then group by to find all the names once per mode. (Example: F will be only once for actual and once for plan)</p>\n<p>I previously tried taking a transpose of columns and then using the below to summarize the data till current month:</p>\n<pre><code>df = pd.DataFrame({'Name1':df['Name1'], 'previous_mt':df.loc[:,prev_month], 'current_mt':df.loc[:,this_month]})\n</code></pre>\n<p>where :</p>\n<pre><code>    prev = curr_month.replace(day=1) - timedelta(days=1)\n    prev_month = prev.strftime(&quot;%B&quot;)[:3]\n\n    curr_month = dt.datetime.now()\n    this_month = curr_month.strftime(&quot;%B&quot;)[:3]\n</code></pre>\n<p>But this is getting very complicated since the actual data has lots and lots of modes and also many years of data. Is there any easier solution for the same where this complication can be avoided and the similar solution can be achieved?</p>\n<p>In the end i am expecting to have the below dataframe:</p>\n<pre><code>Name1   Name2   Actual_Value1   Actual_Value2   Plan_Value1 Plan_Value1\nC       N           4              4                            \nD       N                                           2             2         \nE       N           10             10                               \nF       N           7              7                3             3 \n    \n</code></pre>\n<p>I guess i can have this format using pivot_table in pandas:</p>\n<pre><code>df=pd.pivot_table(df_input,index=['Name1', 'Name2'], \n                      columns=['Mode'],\n                      values=['Value1', 'Value2'], \n                      aggfunc=np.sum, fill_value=0).reset_index().rename_axis(1)\n</code></pre>\n",
        "answer_body": "<ul>\n<li>In Pandas, use <strong>pivot_table</strong> to transpose table data (rows become columns).</li>\n<li>Be sure to use <strong>reset_index()</strong> to convert the pivot object to a dataframe</li>\n</ul>\n<p>Based on your sample dataset, this code gave the results you're looking for:</p>\n<pre><code>lstAllMonths=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\ncurMth = datetime.today().month  # 7=July\ncurMth = 6  # Jun for testing\nlstMth = lstAllMonths[1:curMth]\n\ndf = df[df['Month'].isin(lstMth)][['Name1','Name2','Mode','Value1','Value2']]\ngb = df.groupby(['Name1','Name2','Mode'])\ndfagg = gb.agg({'Value1':sum, 'Value2':sum})\n\ndfpvt = pd.pivot_table(dfagg,index=['Name1', 'Name2'], \n                      columns=['Mode'],\n                      values=['Value1', 'Value2'], \n                      aggfunc=np.sum, fill_value=0).reset_index().rename_axis(1)\n                      \ndfpvt.columns=['Name1','Name2','Actual_Value1','Plan_Value1','Actual_Value2','Plan_Value2']\ndfpvt.replace(0,'', inplace=True)\ndfpvt = dfpvt[['Name1','Name2','Actual_Value1','Actual_Value2','Plan_Value1','Plan_Value2']]    \nprint(dfpvt)\n</code></pre>\n",
        "question_body": "<p>I have below dataframe:</p>\n<pre><code>Name1   Name2   Month   Mode    Value1 Value2\nA       N       Sep     Plan    9       9\nB       N       Nov     Plan    6       6\nB       N       Jan     Plan    6       6\nC       N       Feb     Actual  4       4\nC       N       Jul     Actual  3       3\nD       N       May     Plan    2       2\nE       N       Apr     Actual  6       6\nF       N       Feb     Actual  7       7\nE       N       May     Actual  4       4\nF       N       Jun     Plan    3       3\n&lt;+ 100 more rows&gt;\n</code></pre>\n<p>Considering the current month is June, the expected output is as follows:</p>\n<pre><code>Name1   Name2   Mode    Value1  Value2\nC       N       Actual    4       4                                             \nD       N       Plan      2       2                                             \nE       N       Actual    10      10                                                \nF       N       Actual    7       7                                             \nF       N       Plan      3       3                                             \n</code></pre>\n<p>Here i have done filtering of rows  by months from Feb:Current Month (in this case June) and then group by to find all the names once per mode. (Example: F will be only once for actual and once for plan)</p>\n<p>I previously tried taking a transpose of columns and then using the below to summarize the data till current month:</p>\n<pre><code>df = pd.DataFrame({'Name1':df['Name1'], 'previous_mt':df.loc[:,prev_month], 'current_mt':df.loc[:,this_month]})\n</code></pre>\n<p>where :</p>\n<pre><code>    prev = curr_month.replace(day=1) - timedelta(days=1)\n    prev_month = prev.strftime(&quot;%B&quot;)[:3]\n\n    curr_month = dt.datetime.now()\n    this_month = curr_month.strftime(&quot;%B&quot;)[:3]\n</code></pre>\n<p>But this is getting very complicated since the actual data has lots and lots of modes and also many years of data. Is there any easier solution for the same where this complication can be avoided and the similar solution can be achieved?</p>\n<p>In the end i am expecting to have the below dataframe:</p>\n<pre><code>Name1   Name2   Actual_Value1   Actual_Value2   Plan_Value1 Plan_Value1\nC       N           4              4                            \nD       N                                           2             2         \nE       N           10             10                               \nF       N           7              7                3             3 \n    \n</code></pre>\n<p>I guess i can have this format using pivot_table in pandas:</p>\n<pre><code>df=pd.pivot_table(df_input,index=['Name1', 'Name2'], \n                      columns=['Mode'],\n                      values=['Value1', 'Value2'], \n                      aggfunc=np.sum, fill_value=0).reset_index().rename_axis(1)\n</code></pre>\n",
        "formatted_input": {
            "qid": 63000962,
            "link": "https://stackoverflow.com/questions/63000962/python-selecting-rows-matching-feb-till-current-month-in-pandas-dataframe",
            "question": {
                "title": "Python: Selecting rows matching Feb till current month in pandas dataframe",
                "ques_desc": "I have below dataframe: Considering the current month is June, the expected output is as follows: Here i have done filtering of rows by months from Feb:Current Month (in this case June) and then group by to find all the names once per mode. (Example: F will be only once for actual and once for plan) I previously tried taking a transpose of columns and then using the below to summarize the data till current month: where : But this is getting very complicated since the actual data has lots and lots of modes and also many years of data. Is there any easier solution for the same where this complication can be avoided and the similar solution can be achieved? In the end i am expecting to have the below dataframe: I guess i can have this format using pivot_table in pandas: "
            },
            "io": [
                "Name1   Name2   Mode    Value1  Value2\nC       N       Actual    4       4                                             \nD       N       Plan      2       2                                             \nE       N       Actual    10      10                                                \nF       N       Actual    7       7                                             \nF       N       Plan      3       3                                             \n",
                "Name1   Name2   Actual_Value1   Actual_Value2   Plan_Value1 Plan_Value1\nC       N           4              4                            \nD       N                                           2             2         \nE       N           10             10                               \nF       N           7              7                3             3 \n    \n"
            ],
            "answer": {
                "ans_desc": " In Pandas, use pivot_table to transpose table data (rows become columns). Be sure to use reset_index() to convert the pivot object to a dataframe Based on your sample dataset, this code gave the results you're looking for: ",
                "code": [
                    "lstAllMonths=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\ncurMth = datetime.today().month  # 7=July\ncurMth = 6  # Jun for testing\nlstMth = lstAllMonths[1:curMth]\n\ndf = df[df['Month'].isin(lstMth)][['Name1','Name2','Mode','Value1','Value2']]\ngb = df.groupby(['Name1','Name2','Mode'])\ndfagg = gb.agg({'Value1':sum, 'Value2':sum})\n\ndfpvt = pd.pivot_table(dfagg,index=['Name1', 'Name2'], \n                      columns=['Mode'],\n                      values=['Value1', 'Value2'], \n                      aggfunc=np.sum, fill_value=0).reset_index().rename_axis(1)\n                      \ndfpvt.columns=['Name1','Name2','Actual_Value1','Plan_Value1','Actual_Value2','Plan_Value2']\ndfpvt.replace(0,'', inplace=True)\ndfpvt = dfpvt[['Name1','Name2','Actual_Value1','Actual_Value2','Plan_Value1','Plan_Value2']]    \nprint(dfpvt)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 227,
            "user_id": 13382264,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-iJMzrlmzYrc/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJN6kPzXCOJTeOkdlGE7erD1YstwUw/photo.jpg?sz=128",
            "display_name": "Andrew Horowitz",
            "link": "https://stackoverflow.com/users/13382264/andrew-horowitz"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 63674658,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1598899441,
        "creation_date": 1598890988,
        "last_edit_date": 1598899441,
        "question_id": 63674555,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63674555/pandas-drop-diffrent-rows-with-differing-column-values",
        "title": "pandas drop diffrent rows with differing column values",
        "body": "<p>I have DataFrame <code>df = pd.DataFrame({'col1': [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;, &quot;e&quot;], 'col2': [1,3,3,2,6,6], 'col3': [1,2,3,4,5,6]})</code> that looks like</p>\n<p>Input:</p>\n<pre><code>     col1 col2 col3\n    0   a   1   1\n    1   b   3   2\n    2   c   3   3\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n</code></pre>\n<p>I would like to remove rows from &quot;col1&quot; that share a common value in &quot;col2&quot; except values that are the same i.e. letter &quot;e&quot;. I would like it to be where only one value in &quot;col1&quot; can = a unique one in &quot;col2&quot; The expected output would look something like...</p>\n<p>Output:</p>\n<pre><code>     col1 col2 col3\n    0   a   1   1\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n</code></pre>\n<p>What would be the process of doing this?</p>\n",
        "answer_body": "<p>Based on what you described, I understood as follows:</p>\n<ol>\n<li>If two rows have same values in <code>col2</code>, they both are dropped.</li>\n<li>If two rows have same value in <code>col2</code> but have same values in <code>col1</code>, you want to keep them.</li>\n<li>All other rows which do not fall in above two categories, you want to keep.</li>\n</ol>\n<p>This can be achieved as:</p>\n<pre><code>df[np.logical_or(~df.duplicated('col2', keep = False),df.duplicated('col1', keep = False)) ]\n</code></pre>\n",
        "question_body": "<p>I have DataFrame <code>df = pd.DataFrame({'col1': [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;, &quot;e&quot;], 'col2': [1,3,3,2,6,6], 'col3': [1,2,3,4,5,6]})</code> that looks like</p>\n<p>Input:</p>\n<pre><code>     col1 col2 col3\n    0   a   1   1\n    1   b   3   2\n    2   c   3   3\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n</code></pre>\n<p>I would like to remove rows from &quot;col1&quot; that share a common value in &quot;col2&quot; except values that are the same i.e. letter &quot;e&quot;. I would like it to be where only one value in &quot;col1&quot; can = a unique one in &quot;col2&quot; The expected output would look something like...</p>\n<p>Output:</p>\n<pre><code>     col1 col2 col3\n    0   a   1   1\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n</code></pre>\n<p>What would be the process of doing this?</p>\n",
        "formatted_input": {
            "qid": 63674555,
            "link": "https://stackoverflow.com/questions/63674555/pandas-drop-diffrent-rows-with-differing-column-values",
            "question": {
                "title": "pandas drop diffrent rows with differing column values",
                "ques_desc": "I have DataFrame that looks like Input: I would like to remove rows from \"col1\" that share a common value in \"col2\" except values that are the same i.e. letter \"e\". I would like it to be where only one value in \"col1\" can = a unique one in \"col2\" The expected output would look something like... Output: What would be the process of doing this? "
            },
            "io": [
                "     col1 col2 col3\n    0   a   1   1\n    1   b   3   2\n    2   c   3   3\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n",
                "     col1 col2 col3\n    0   a   1   1\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n"
            ],
            "answer": {
                "ans_desc": "Based on what you described, I understood as follows: If two rows have same values in , they both are dropped. If two rows have same value in but have same values in , you want to keep them. All other rows which do not fall in above two categories, you want to keep. This can be achieved as: ",
                "code": [
                    "df[np.logical_or(~df.duplicated('col2', keep = False),df.duplicated('col1', keep = False)) ]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "wildcard",
            "matching"
        ],
        "owner": {
            "reputation": 141,
            "user_id": 13365279,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d448f2356b808a1fcb2bb3ec720d97df?s=128&d=identicon&r=PG&f=1",
            "display_name": "stackoverflow",
            "link": "https://stackoverflow.com/users/13365279/stackoverflow"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 63622164,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1598553633,
        "creation_date": 1598551758,
        "last_edit_date": 1598552818,
        "question_id": 63621901,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63621901/how-to-average-dataframe-row-with-another-row-only-if-the-first-row-is-a-substri",
        "title": "How to average DataFrame row with another row only if the first row is a substring of other next row",
        "body": "<p>I have a dataframe called 'data':</p>\n<pre><code>USER    VALUE\nXOXO      21\nABC-1      2\nABC-1B     4\nABC-2      4\nABC-2B     6\nPEPE      12\n</code></pre>\n<p>I want to combine 'ABC-1' with 'ABC-1B' into a single row using the first USER name and then averaging the two values to arrive here:</p>\n<pre><code>USER    VALUE\nXOXO      21\nABC-1      3\nABC-2      5\nPEPE      12\n</code></pre>\n<p>The dataframe may not be in order and there are other values in there as well that are unrelated that don't need averaging. I only want to average the two rows where 'XXX-X' is in 'XXX-XB'</p>\n<pre><code>data = pd.DataFrame({'USER':['XOXO','ABC-1','ABC-1B','ABC-2','ABC-2B', 'PEPE'], 'VALUE':[21,2,4,4,6,12]})\n</code></pre>\n",
        "answer_body": "<p>Let's try,</p>\n<pre><code>df.USER = df.USER.str.replace('(-\\d)B', r&quot;\\1&quot;)\ndf = df.groupby(&quot;USER&quot;, as_index=False, sort=False).VALUE.mean()\n\nprint(df)\n</code></pre>\n<hr />\n<pre><code>    USER  VALUE\n0   XOXO     21\n1  ABC-1      3\n2  ABC-2      5\n3   PEPE     12\n</code></pre>\n",
        "question_body": "<p>I have a dataframe called 'data':</p>\n<pre><code>USER    VALUE\nXOXO      21\nABC-1      2\nABC-1B     4\nABC-2      4\nABC-2B     6\nPEPE      12\n</code></pre>\n<p>I want to combine 'ABC-1' with 'ABC-1B' into a single row using the first USER name and then averaging the two values to arrive here:</p>\n<pre><code>USER    VALUE\nXOXO      21\nABC-1      3\nABC-2      5\nPEPE      12\n</code></pre>\n<p>The dataframe may not be in order and there are other values in there as well that are unrelated that don't need averaging. I only want to average the two rows where 'XXX-X' is in 'XXX-XB'</p>\n<pre><code>data = pd.DataFrame({'USER':['XOXO','ABC-1','ABC-1B','ABC-2','ABC-2B', 'PEPE'], 'VALUE':[21,2,4,4,6,12]})\n</code></pre>\n",
        "formatted_input": {
            "qid": 63621901,
            "link": "https://stackoverflow.com/questions/63621901/how-to-average-dataframe-row-with-another-row-only-if-the-first-row-is-a-substri",
            "question": {
                "title": "How to average DataFrame row with another row only if the first row is a substring of other next row",
                "ques_desc": "I have a dataframe called 'data': I want to combine 'ABC-1' with 'ABC-1B' into a single row using the first USER name and then averaging the two values to arrive here: The dataframe may not be in order and there are other values in there as well that are unrelated that don't need averaging. I only want to average the two rows where 'XXX-X' is in 'XXX-XB' "
            },
            "io": [
                "USER    VALUE\nXOXO      21\nABC-1      2\nABC-1B     4\nABC-2      4\nABC-2B     6\nPEPE      12\n",
                "USER    VALUE\nXOXO      21\nABC-1      3\nABC-2      5\nPEPE      12\n"
            ],
            "answer": {
                "ans_desc": "Let's try, ",
                "code": [
                    "df.USER = df.USER.str.replace('(-\\d)B', r\"\\1\")\ndf = df.groupby(\"USER\", as_index=False, sort=False).VALUE.mean()\n\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 41,
            "user_id": 13920180,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/37aef7914e560c4db6fad535699deb34?s=128&d=identicon&r=PG&f=1",
            "display_name": "Fariha Baloch",
            "link": "https://stackoverflow.com/users/13920180/fariha-baloch"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 63620674,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1598546916,
        "creation_date": 1598546617,
        "question_id": 63620593,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63620593/replace-nan-with-values-in-a-row-from-previous-matching-values-in-column",
        "title": "Replace NaN with values in a row from previous matching values in column",
        "body": "<p>I have following data frame (df).<br /></p>\n<pre><code>df\n     city    zip_code\n0    city1  90287\n1    city2  90288\n2    city3  80023\n3    city4  90210\n4    city1  NaN\n5    city4  NaN\n6    city7  NaN\n7    NaN    90210\n8    NaN    80023\n</code></pre>\n<p>And I want to get to this state: <br /></p>\n<pre><code>    city     zip_code\n0   city1   90287\n1   city2   90288\n2   city3   80023\n3   city4   90210\n4   city1   90287\n5   city4   90210\n6   city7   NaN\n7   city4   90210\n8   city3   80023\n</code></pre>\n<p>I want to go through both columns and replace NaN with appropriate zip_code or city.<br /></p>\n<p>Here is what I have done but as you can see it didn't fully work.</p>\n<pre><code>bool_series = pd.notnull(df['city'])\ndf_1=df[bool_series].dropna()\ndict_df_1=df_1.to_dict(orient='records')\n\nd={}\nfor i in range(len(dict_df_1)):\n    d[dict_df_1[i]['city']]=dict_df_1[i]['zip_code']\nd1={}\nfor i in range(len(dict_df_1)):\n    d1[dict_df_1[i]['zip_code']]=dict_df_1[i]['city']\nd.update(d1)\n\ndf['zip_mapped']=df['city'].map(d)\ndf['city_mapped']=df['zip_code'].map(d)\n\ndf\n\n     city   zip_code    zip_mapped  city_mapped\n0    city1     90287    90287         city1\n1   city2      90288    90288         city2\n2   city3      80023    80023         city3\n3   city4      90210    90210         city4\n4   city1       NaN     90287         NaN\n5   city4       NaN     90210         NaN\n6   city7       NaN      NaN          NaN\n7   NaN        90210    NaN           city4\n8   NaN        80023    NaN           city3\n\u200b\n</code></pre>\n<p>If columns 'zip_mapped' and 'city_mapped' were properly populated I would have just replaced them with original cols. Can anyone help me here?</p>\n",
        "answer_body": "<p>Let's try <code>fillna</code> twice on different groupby:</p>\n<pre><code>df.zip_code = df.zip_code.fillna(df.zip_code.groupby(df.city).transform('first'))\n\ndf.city = df.city.fillna(df.city.groupby(df.zip_code).transform('first'))\n</code></pre>\n<p>Output:</p>\n<pre><code>    city  zip_code\n0  city1   90287.0\n1  city2   90288.0\n2  city3   80023.0\n3  city4   90210.0\n4  city1   90287.0\n5  city4   90210.0\n6  city7       NaN\n7  city4   90210.0\n8  city3   80023.0\n</code></pre>\n",
        "question_body": "<p>I have following data frame (df).<br /></p>\n<pre><code>df\n     city    zip_code\n0    city1  90287\n1    city2  90288\n2    city3  80023\n3    city4  90210\n4    city1  NaN\n5    city4  NaN\n6    city7  NaN\n7    NaN    90210\n8    NaN    80023\n</code></pre>\n<p>And I want to get to this state: <br /></p>\n<pre><code>    city     zip_code\n0   city1   90287\n1   city2   90288\n2   city3   80023\n3   city4   90210\n4   city1   90287\n5   city4   90210\n6   city7   NaN\n7   city4   90210\n8   city3   80023\n</code></pre>\n<p>I want to go through both columns and replace NaN with appropriate zip_code or city.<br /></p>\n<p>Here is what I have done but as you can see it didn't fully work.</p>\n<pre><code>bool_series = pd.notnull(df['city'])\ndf_1=df[bool_series].dropna()\ndict_df_1=df_1.to_dict(orient='records')\n\nd={}\nfor i in range(len(dict_df_1)):\n    d[dict_df_1[i]['city']]=dict_df_1[i]['zip_code']\nd1={}\nfor i in range(len(dict_df_1)):\n    d1[dict_df_1[i]['zip_code']]=dict_df_1[i]['city']\nd.update(d1)\n\ndf['zip_mapped']=df['city'].map(d)\ndf['city_mapped']=df['zip_code'].map(d)\n\ndf\n\n     city   zip_code    zip_mapped  city_mapped\n0    city1     90287    90287         city1\n1   city2      90288    90288         city2\n2   city3      80023    80023         city3\n3   city4      90210    90210         city4\n4   city1       NaN     90287         NaN\n5   city4       NaN     90210         NaN\n6   city7       NaN      NaN          NaN\n7   NaN        90210    NaN           city4\n8   NaN        80023    NaN           city3\n\u200b\n</code></pre>\n<p>If columns 'zip_mapped' and 'city_mapped' were properly populated I would have just replaced them with original cols. Can anyone help me here?</p>\n",
        "formatted_input": {
            "qid": 63620593,
            "link": "https://stackoverflow.com/questions/63620593/replace-nan-with-values-in-a-row-from-previous-matching-values-in-column",
            "question": {
                "title": "Replace NaN with values in a row from previous matching values in column",
                "ques_desc": "I have following data frame (df). And I want to get to this state: I want to go through both columns and replace NaN with appropriate zip_code or city. Here is what I have done but as you can see it didn't fully work. If columns 'zip_mapped' and 'city_mapped' were properly populated I would have just replaced them with original cols. Can anyone help me here? "
            },
            "io": [
                "df\n     city    zip_code\n0    city1  90287\n1    city2  90288\n2    city3  80023\n3    city4  90210\n4    city1  NaN\n5    city4  NaN\n6    city7  NaN\n7    NaN    90210\n8    NaN    80023\n",
                "    city     zip_code\n0   city1   90287\n1   city2   90288\n2   city3   80023\n3   city4   90210\n4   city1   90287\n5   city4   90210\n6   city7   NaN\n7   city4   90210\n8   city3   80023\n"
            ],
            "answer": {
                "ans_desc": "Let's try twice on different groupby: Output: ",
                "code": [
                    "df.zip_code = df.zip_code.fillna(df.zip_code.groupby(df.city).transform('first'))\n\ndf.city = df.city.fillna(df.city.groupby(df.zip_code).transform('first'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1949,
            "user_id": 12559770,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/80816af816ac0730164c41548116b0db?s=128&d=identicon&r=PG&f=1",
            "display_name": "chippycentra",
            "link": "https://stackoverflow.com/users/12559770/chippycentra"
        },
        "is_answered": true,
        "view_count": 126,
        "accepted_answer_id": 63617000,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1598535249,
        "creation_date": 1598533643,
        "last_edit_date": 1598534055,
        "question_id": 63616688,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63616688/round-in-pandas-column-scientific-numbers",
        "title": "Round in pandas column scientific numbers",
        "body": "<p>Hello I have a df  such as :</p>\n<pre><code>COL1        COL2\n0.005554    0.35200000000000004\n5.622e-11   0.267\n0.006999999999999999    0.307\n2.129e-14   0.469\n2.604e-14   0.39\n1.395e-60   0.27899999999999997\n8.589999999999998e-74   0.29600000000000004\n1.025e-42   0.4270000000000001\n</code></pre>\n<p>I knwo how to roun the digit in the COL2 by using</p>\n<pre><code>df['COL2'] = df['COL2'].round(3)\n</code></pre>\n<p>but if I do the same for COL1 it only displays 0</p>\n<p>how can I get instead:</p>\n<pre><code>COL1        COL2\n0.005   0.352\n5.622e-11   0.267\n0.007   0.307\n2.129e-14   0.469\n2.604e-14   0.39\n1.395e-60   0.279\n8.560e-74   0.296\n1.025e-42   0.427\n</code></pre>\n<p>in fact the big issue is here:</p>\n<p>it displays : <code>8.589999999999998e-74</code>\nand I would like to keep only 3 number after the coma</p>\n<pre><code>8.590e-74\n</code></pre>\n",
        "answer_body": "<p>You don't want to round the values in the first column - rounding means that the lower digits are gone. I think what you actually want to do is to change how Pandas displays the data <em>on screen</em> and not the actual values in the dataframe. The default is already scientific notation.</p>\n<p>You can specify the display format for all columns like so (this will be showing the first three digits after the dot):</p>\n<pre><code>pd.set_option('display.float_format', lambda x: '%.3f' % x)\n</code></pre>\n<p>Since you want to keep the format of the first one but not of the second you might need to set the format for each column individually. You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html\" rel=\"nofollow noreferrer\">styling</a> for column 2:</p>\n<pre><code>df.style.format({'COL2': '{:.3f}')\n</code></pre>\n<p>Edit: Since your answer is for saving to CSV, you'll need to pass this as an argument to <code>to_csv</code>: <code>float_format='{:.3E}'</code>. This will save all columns in scientific notation. If you only want this for column 1, you could try the solutions to <a href=\"https://stackoverflow.com/questions/20003290/output-different-precision-by-column-with-pandas-dataframe-to-csv\">this question</a>.</p>\n",
        "question_body": "<p>Hello I have a df  such as :</p>\n<pre><code>COL1        COL2\n0.005554    0.35200000000000004\n5.622e-11   0.267\n0.006999999999999999    0.307\n2.129e-14   0.469\n2.604e-14   0.39\n1.395e-60   0.27899999999999997\n8.589999999999998e-74   0.29600000000000004\n1.025e-42   0.4270000000000001\n</code></pre>\n<p>I knwo how to roun the digit in the COL2 by using</p>\n<pre><code>df['COL2'] = df['COL2'].round(3)\n</code></pre>\n<p>but if I do the same for COL1 it only displays 0</p>\n<p>how can I get instead:</p>\n<pre><code>COL1        COL2\n0.005   0.352\n5.622e-11   0.267\n0.007   0.307\n2.129e-14   0.469\n2.604e-14   0.39\n1.395e-60   0.279\n8.560e-74   0.296\n1.025e-42   0.427\n</code></pre>\n<p>in fact the big issue is here:</p>\n<p>it displays : <code>8.589999999999998e-74</code>\nand I would like to keep only 3 number after the coma</p>\n<pre><code>8.590e-74\n</code></pre>\n",
        "formatted_input": {
            "qid": 63616688,
            "link": "https://stackoverflow.com/questions/63616688/round-in-pandas-column-scientific-numbers",
            "question": {
                "title": "Round in pandas column scientific numbers",
                "ques_desc": "Hello I have a df such as : I knwo how to roun the digit in the COL2 by using but if I do the same for COL1 it only displays 0 how can I get instead: in fact the big issue is here: it displays : and I would like to keep only 3 number after the coma "
            },
            "io": [
                "COL1        COL2\n0.005554    0.35200000000000004\n5.622e-11   0.267\n0.006999999999999999    0.307\n2.129e-14   0.469\n2.604e-14   0.39\n1.395e-60   0.27899999999999997\n8.589999999999998e-74   0.29600000000000004\n1.025e-42   0.4270000000000001\n",
                "COL1        COL2\n0.005   0.352\n5.622e-11   0.267\n0.007   0.307\n2.129e-14   0.469\n2.604e-14   0.39\n1.395e-60   0.279\n8.560e-74   0.296\n1.025e-42   0.427\n"
            ],
            "answer": {
                "ans_desc": "You don't want to round the values in the first column - rounding means that the lower digits are gone. I think what you actually want to do is to change how Pandas displays the data on screen and not the actual values in the dataframe. The default is already scientific notation. You can specify the display format for all columns like so (this will be showing the first three digits after the dot): Since you want to keep the format of the first one but not of the second you might need to set the format for each column individually. You can use styling for column 2: Edit: Since your answer is for saving to CSV, you'll need to pass this as an argument to : . This will save all columns in scientific notation. If you only want this for column 1, you could try the solutions to this question. ",
                "code": [
                    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 400,
            "user_id": 7859096,
            "user_type": "registered",
            "accept_rate": 69,
            "profile_image": "https://lh3.googleusercontent.com/-p2JSfLd_OtQ/AAAAAAAAAAI/AAAAAAAAABI/UJhDkT-Ahus/photo.jpg?sz=128",
            "display_name": "Jonathan Pacheco",
            "link": "https://stackoverflow.com/users/7859096/jonathan-pacheco"
        },
        "is_answered": true,
        "view_count": 60,
        "accepted_answer_id": 63610410,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1598512772,
        "creation_date": 1598507191,
        "last_edit_date": 1598509201,
        "question_id": 63609893,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63609893/merge-two-dataframes-with-uncertainty-on-keys",
        "title": "Merge two dataframes with uncertainty on keys",
        "body": "<p>I am trying to use 'merge' to combine two data frames with shape:</p>\n<pre><code>df1 = pd.DataFrame({'ID1':[10,32,53,65,3],'A':[1,4,2,5,4],'B':[3,5,2,9,3]})\ndf2 = pd.DataFrame({'ID2':[68,35,93,5,23],'A':[9,5,3,7,4],'B':[6,5,1,9,3]})\n</code></pre>\n<p>Those looks as follows:</p>\n<pre><code>In [29]: df1\nOut[29]: \n   ID1  A  B\n0   10  1  3\n1   32  4  5\n2   53  2  2\n3   65  5  9\n4    3  4  3\n\nIn [32]: df2\nOut[32]: \n   ID2  A  B\n0   68  9  6\n1   35  5  5\n2   93  3  1\n3    5  7  9\n4   23  4  3\n</code></pre>\n<p>What I am looking for is to merge columns 'A' and 'B' with a defined uncertainty. For example + - 0.5. I don't have clear how to handle this. What I was trying to do is to manually add an uncertainty:</p>\n<pre><code>df1['Al'] = (np.around(df1['A']+ 0.5, decimals=1))\ndf1['Bl'] = (np.around(df1['B']+ 0.5, decimals=1))\ndf1['Ar'] = (np.around(df1['A']- 0.5, decimals=1))\ndf1['Br'] = (np.around(df1['B']- 0.5, decimals=1))\ndf2['Al'] = (np.around(df2['A']+ 0.5, decimals=1))\ndf2['Bl'] = (np.around(df2['B']+ 0.5, decimals=1))\ndf2['Ar'] = (np.around(df2['A']- 0.5, decimals=1))\ndf2['Br'] = (np.around(df2['B']- 0.5, decimals=1))\n</code></pre>\n<p>After this, I do the merge:</p>\n<pre><code>df3 = df1.merge(df2, on=['A','B','Al','Bl','Ar','Br'], right_index=True, how='outer', indicator='all')\n</code></pre>\n<p>But here I got stuck because I can not figure it out how to use a conditional merge. The idea is to merge all those rows were columns 'A' and 'B' be the same with a definite certainty</p>\n<p>The expected output would be:</p>\n<pre><code>df3:\n   ID1  A  B  ID2  A  B\n1  32   4  5  35   5  5\n2  53   2  2  93   3  1\n4  3    4  3  23   4  3 \n</code></pre>\n",
        "answer_body": "<h3>Simple but expensive</h3>\n<p>Just do a Cartesian product then select down rows you want.  This will work well for smaller data sets but will be very expensive on large data sets</p>\n<pre><code>dfs = (df1.assign(foo=1).merge(df2.assign(foo=1), on=&quot;foo&quot;, suffixes=(&quot;&quot;,&quot;_df2&quot;))\n .assign(adiff=lambda x: x[&quot;A&quot;]-x[&quot;A_df2&quot;])\n .assign(bdiff=lambda x: x[&quot;B&quot;]-x[&quot;B_df2&quot;])\n .query(&quot;adiff&gt;=-1 and adiff&lt;=1 and bdiff&gt;=-1 and bdiff&lt;=1&quot;)\n .drop(columns=[&quot;adiff&quot;,&quot;bdiff&quot;,&quot;foo&quot;])\n)\n\nprint(dfs.to_string(index=False))\n</code></pre>\n<h3>output</h3>\n<pre><code> ID1  A  B  ID2  A_df2  B_df2\n  32  4  5   35      5      5\n  53  2  2   93      3      1\n   3  4  3   23      4      3\n</code></pre>\n<p>Nearest value match is provided by <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge_asof.html\" rel=\"nofollow noreferrer\">merge_asof</a></p>\n<ol>\n<li>can only match on one column and it must be ordered</li>\n<li>if's a left join so choose which data frame you want to drive it</li>\n<li>it's fairly common to calculate a value which is used as join.  This is represented in second approach below. In GIS / GPS use cases it's typically distance - hence reason I named column <em>d</em></li>\n<li>first example is doing two merges, first on <em>A</em> then on <em>B</em>.  You have all the data together and can then do further logic</li>\n</ol>\n<pre><code>df1 = pd.DataFrame({'ID1':[10,32,53,65,3],'A':[1,4,2,5,4],'B':[3,5,2,9,3]})\ndf2 = pd.DataFrame({'ID2':[68,35,93,5,23],'A':[9,5,3,7,4],'B':[6,5,1,9,3]})\n\n\n# find nearest row in df2 when matching on columns A or B\ndfmab = pd.merge_asof(\n    (pd.merge_asof(df1.sort_values(&quot;A&quot;), df2.sort_values(&quot;A&quot;), \n                   on=&quot;A&quot;, direction=&quot;nearest&quot;, suffixes=(&quot;&quot;,&quot;_Adf2&quot;))\n    ).sort_values(&quot;B&quot;) \n    ,df2.sort_values(&quot;B&quot;),\n    on=&quot;B&quot;, direction=&quot;nearest&quot;, suffixes=(&quot;&quot;,&quot;_Bdf2&quot;)\n)\n\n# calculate a value that represents value of row\ndfmd = pd.merge_asof(\n    df1.assign(d=lambda x: x[&quot;A&quot;]*x[&quot;B&quot;]).sort_values(&quot;d&quot;),\n    df2.assign(d=lambda x: x[&quot;A&quot;]*x[&quot;B&quot;]).sort_values(&quot;d&quot;),\n    on=&quot;d&quot;, direction=&quot;nearest&quot;, suffixes=(&quot;&quot;,&quot;_Ddf2&quot;)\n)\n\nprint(dfmab.to_string(index=False))\nprint(dfmd.to_string(index=False))\n</code></pre>\n<p><strong>output</strong></p>\n<pre><code> ID1  A  B  ID2  B_Adf2  ID2_Bdf2  A_Bdf2\n  53  2  2   93       1        93       3\n  10  1  3   93       1        23       4\n   3  4  3   23       3        23       4\n  32  4  5   23       3        35       5\n  65  5  9   35       5         5       7\n ID1  A  B   d  ID2  A_Ddf2  B_Ddf2\n  10  1  3   3   93       3       1\n  53  2  2   4   93       3       1\n   3  4  3  12   23       4       3\n  32  4  5  20   35       5       5\n  65  5  9  45   68       9       6\n</code></pre>\n",
        "question_body": "<p>I am trying to use 'merge' to combine two data frames with shape:</p>\n<pre><code>df1 = pd.DataFrame({'ID1':[10,32,53,65,3],'A':[1,4,2,5,4],'B':[3,5,2,9,3]})\ndf2 = pd.DataFrame({'ID2':[68,35,93,5,23],'A':[9,5,3,7,4],'B':[6,5,1,9,3]})\n</code></pre>\n<p>Those looks as follows:</p>\n<pre><code>In [29]: df1\nOut[29]: \n   ID1  A  B\n0   10  1  3\n1   32  4  5\n2   53  2  2\n3   65  5  9\n4    3  4  3\n\nIn [32]: df2\nOut[32]: \n   ID2  A  B\n0   68  9  6\n1   35  5  5\n2   93  3  1\n3    5  7  9\n4   23  4  3\n</code></pre>\n<p>What I am looking for is to merge columns 'A' and 'B' with a defined uncertainty. For example + - 0.5. I don't have clear how to handle this. What I was trying to do is to manually add an uncertainty:</p>\n<pre><code>df1['Al'] = (np.around(df1['A']+ 0.5, decimals=1))\ndf1['Bl'] = (np.around(df1['B']+ 0.5, decimals=1))\ndf1['Ar'] = (np.around(df1['A']- 0.5, decimals=1))\ndf1['Br'] = (np.around(df1['B']- 0.5, decimals=1))\ndf2['Al'] = (np.around(df2['A']+ 0.5, decimals=1))\ndf2['Bl'] = (np.around(df2['B']+ 0.5, decimals=1))\ndf2['Ar'] = (np.around(df2['A']- 0.5, decimals=1))\ndf2['Br'] = (np.around(df2['B']- 0.5, decimals=1))\n</code></pre>\n<p>After this, I do the merge:</p>\n<pre><code>df3 = df1.merge(df2, on=['A','B','Al','Bl','Ar','Br'], right_index=True, how='outer', indicator='all')\n</code></pre>\n<p>But here I got stuck because I can not figure it out how to use a conditional merge. The idea is to merge all those rows were columns 'A' and 'B' be the same with a definite certainty</p>\n<p>The expected output would be:</p>\n<pre><code>df3:\n   ID1  A  B  ID2  A  B\n1  32   4  5  35   5  5\n2  53   2  2  93   3  1\n4  3    4  3  23   4  3 \n</code></pre>\n",
        "formatted_input": {
            "qid": 63609893,
            "link": "https://stackoverflow.com/questions/63609893/merge-two-dataframes-with-uncertainty-on-keys",
            "question": {
                "title": "Merge two dataframes with uncertainty on keys",
                "ques_desc": "I am trying to use 'merge' to combine two data frames with shape: Those looks as follows: What I am looking for is to merge columns 'A' and 'B' with a defined uncertainty. For example + - 0.5. I don't have clear how to handle this. What I was trying to do is to manually add an uncertainty: After this, I do the merge: But here I got stuck because I can not figure it out how to use a conditional merge. The idea is to merge all those rows were columns 'A' and 'B' be the same with a definite certainty The expected output would be: "
            },
            "io": [
                "In [29]: df1\nOut[29]: \n   ID1  A  B\n0   10  1  3\n1   32  4  5\n2   53  2  2\n3   65  5  9\n4    3  4  3\n\nIn [32]: df2\nOut[32]: \n   ID2  A  B\n0   68  9  6\n1   35  5  5\n2   93  3  1\n3    5  7  9\n4   23  4  3\n",
                "df3:\n   ID1  A  B  ID2  A  B\n1  32   4  5  35   5  5\n2  53   2  2  93   3  1\n4  3    4  3  23   4  3 \n"
            ],
            "answer": {
                "ans_desc": "Simple but expensive Just do a Cartesian product then select down rows you want. This will work well for smaller data sets but will be very expensive on large data sets output Nearest value match is provided by merge_asof can only match on one column and it must be ordered if's a left join so choose which data frame you want to drive it it's fairly common to calculate a value which is used as join. This is represented in second approach below. In GIS / GPS use cases it's typically distance - hence reason I named column d first example is doing two merges, first on A then on B. You have all the data together and can then do further logic output ",
                "code": [
                    "dfs = (df1.assign(foo=1).merge(df2.assign(foo=1), on=\"foo\", suffixes=(\"\",\"_df2\"))\n .assign(adiff=lambda x: x[\"A\"]-x[\"A_df2\"])\n .assign(bdiff=lambda x: x[\"B\"]-x[\"B_df2\"])\n .query(\"adiff>=-1 and adiff<=1 and bdiff>=-1 and bdiff<=1\")\n .drop(columns=[\"adiff\",\"bdiff\",\"foo\"])\n)\n\nprint(dfs.to_string(index=False))\n",
                    "df1 = pd.DataFrame({'ID1':[10,32,53,65,3],'A':[1,4,2,5,4],'B':[3,5,2,9,3]})\ndf2 = pd.DataFrame({'ID2':[68,35,93,5,23],'A':[9,5,3,7,4],'B':[6,5,1,9,3]})\n\n\n# find nearest row in df2 when matching on columns A or B\ndfmab = pd.merge_asof(\n    (pd.merge_asof(df1.sort_values(\"A\"), df2.sort_values(\"A\"), \n                   on=\"A\", direction=\"nearest\", suffixes=(\"\",\"_Adf2\"))\n    ).sort_values(\"B\") \n    ,df2.sort_values(\"B\"),\n    on=\"B\", direction=\"nearest\", suffixes=(\"\",\"_Bdf2\")\n)\n\n# calculate a value that represents value of row\ndfmd = pd.merge_asof(\n    df1.assign(d=lambda x: x[\"A\"]*x[\"B\"]).sort_values(\"d\"),\n    df2.assign(d=lambda x: x[\"A\"]*x[\"B\"]).sort_values(\"d\"),\n    on=\"d\", direction=\"nearest\", suffixes=(\"\",\"_Ddf2\")\n)\n\nprint(dfmab.to_string(index=False))\nprint(dfmd.to_string(index=False))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "resampling"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 14126173,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/204b0d70367748ae20df99ff2cf01005?s=128&d=identicon&r=PG&f=1",
            "display_name": "Linh Tran",
            "link": "https://stackoverflow.com/users/14126173/linh-tran"
        },
        "is_answered": true,
        "view_count": 95,
        "accepted_answer_id": 63599348,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1598454732,
        "creation_date": 1598448141,
        "last_edit_date": 1598451701,
        "question_id": 63598638,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63598638/applying-a-function-to-chunks-of-the-dataframe",
        "title": "Applying a function to chunks of the Dataframe",
        "body": "<p>I have a <code>Dataframe (df)</code> (for instance - simplified version)</p>\n<pre><code>              A    B \n 0           2.0   3.0\n 1           3.0   4.0\n</code></pre>\n<p>and generated 20 bootstrap resamples that are all now in the same df but differ in the <em>Resample Nr.</em></p>\n<pre><code>                                A    B \n   \n0     1             0           2.0   3.0\n1     1             1           3.0   4.0\n2     2             1           3.0   4.0\n3     2             1           3.0   4.0\n..    ..\n..    .. \n39    20            0           2.0    3.0\n40    20            0           2.0    3.0\n</code></pre>\n<p>Now I want to apply a certain function on each <em>Reample Nr</em>. Say:</p>\n<pre><code>C = sum(df['A'] * df['B']) / sum(df['B'] ** 2)\n</code></pre>\n<p>The outlook would look like this:</p>\n<pre><code>                                 A    B           C\n0     1             0           2.0   3.0   Calculated Value X1\n1     1             1           3.0   4.0   Calculated Value X1\n2     2             1           3.0   4.0   Calculated Value X2\n3     2             1           3.0   4.0   Calculated Value X2\n..    ..\n..    .. \n39    20            0           2.0    3.0  Calculated Value  X20\n40    20            0           2.0    3.0  Calculated Value  X20\n</code></pre>\n<p>So there are 20 different new values.</p>\n<p>I know there is a df.iloc command where I can specify my row selection <code>df.iloc[row, column]</code> but I would like to find a command where I don't have to repeat the code for the 20 samples.\nMy goal is to find a command that identifies the <em>Resample Nr.</em> automatically and then calculates the function for each <em>Resample Nr.</em></p>\n<p>How can I do this?</p>\n<p>Thank you!</p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\" rel=\"nofollow noreferrer\"><code>DataFrame.assign</code></a> to create two new columns <code>x</code> and <code>y</code> that corresponds to <code>df['A'] * df['B']</code> and <code>df['B']**2</code>, then use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>DataFrame.groupby</code></a> on <code>Resample Nr.</code> (or <code>level=1</code>) and <code>transform</code> using <code>sum</code>:</p>\n<pre><code>s = df.assign(x=df['A'].mul(df['B']), y=df['B']**2)\\\n      .groupby(level=1)[['x', 'y']].transform('sum')\ndf['C'] = s['x'].div(s['y'])\n</code></pre>\n<p>Result:</p>\n<pre><code>           A    B         C\n0  1  0  2.0  3.0  0.720000\n1  1  1  3.0  4.0  0.720000\n2  2  1  3.0  4.0  0.750000\n3  2  1  3.0  4.0  0.750000\n39 20 0  2.0  3.0  0.666667\n40 20 0  2.0  3.0  0.666667\n</code></pre>\n",
        "question_body": "<p>I have a <code>Dataframe (df)</code> (for instance - simplified version)</p>\n<pre><code>              A    B \n 0           2.0   3.0\n 1           3.0   4.0\n</code></pre>\n<p>and generated 20 bootstrap resamples that are all now in the same df but differ in the <em>Resample Nr.</em></p>\n<pre><code>                                A    B \n   \n0     1             0           2.0   3.0\n1     1             1           3.0   4.0\n2     2             1           3.0   4.0\n3     2             1           3.0   4.0\n..    ..\n..    .. \n39    20            0           2.0    3.0\n40    20            0           2.0    3.0\n</code></pre>\n<p>Now I want to apply a certain function on each <em>Reample Nr</em>. Say:</p>\n<pre><code>C = sum(df['A'] * df['B']) / sum(df['B'] ** 2)\n</code></pre>\n<p>The outlook would look like this:</p>\n<pre><code>                                 A    B           C\n0     1             0           2.0   3.0   Calculated Value X1\n1     1             1           3.0   4.0   Calculated Value X1\n2     2             1           3.0   4.0   Calculated Value X2\n3     2             1           3.0   4.0   Calculated Value X2\n..    ..\n..    .. \n39    20            0           2.0    3.0  Calculated Value  X20\n40    20            0           2.0    3.0  Calculated Value  X20\n</code></pre>\n<p>So there are 20 different new values.</p>\n<p>I know there is a df.iloc command where I can specify my row selection <code>df.iloc[row, column]</code> but I would like to find a command where I don't have to repeat the code for the 20 samples.\nMy goal is to find a command that identifies the <em>Resample Nr.</em> automatically and then calculates the function for each <em>Resample Nr.</em></p>\n<p>How can I do this?</p>\n<p>Thank you!</p>\n",
        "formatted_input": {
            "qid": 63598638,
            "link": "https://stackoverflow.com/questions/63598638/applying-a-function-to-chunks-of-the-dataframe",
            "question": {
                "title": "Applying a function to chunks of the Dataframe",
                "ques_desc": "I have a (for instance - simplified version) and generated 20 bootstrap resamples that are all now in the same df but differ in the Resample Nr. Now I want to apply a certain function on each Reample Nr. Say: The outlook would look like this: So there are 20 different new values. I know there is a df.iloc command where I can specify my row selection but I would like to find a command where I don't have to repeat the code for the 20 samples. My goal is to find a command that identifies the Resample Nr. automatically and then calculates the function for each Resample Nr. How can I do this? Thank you! "
            },
            "io": [
                "              A    B \n 0           2.0   3.0\n 1           3.0   4.0\n",
                "                                A    B \n   \n0     1             0           2.0   3.0\n1     1             1           3.0   4.0\n2     2             1           3.0   4.0\n3     2             1           3.0   4.0\n..    ..\n..    .. \n39    20            0           2.0    3.0\n40    20            0           2.0    3.0\n"
            ],
            "answer": {
                "ans_desc": "Use to create two new columns and that corresponds to and , then use on (or ) and using : Result: ",
                "code": [
                    "s = df.assign(x=df['A'].mul(df['B']), y=df['B']**2)\\\n      .groupby(level=1)[['x', 'y']].transform('sum')\ndf['C'] = s['x'].div(s['y'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 12047072,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/CfLuU.gif?s=128&g=1",
            "display_name": "Warrior404",
            "link": "https://stackoverflow.com/users/12047072/warrior404"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 63595596,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1598438588,
        "creation_date": 1598437391,
        "last_edit_date": 1598438588,
        "question_id": 63595559,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63595559/change-the-value-of-column-based-on-quantity-of-equals-rows",
        "title": "Change the value of column based on quantity of equals rows",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>df = pd.DataFrame({'id': ['B668441DE83B', 'B668441DE83B', 'B668441DE83B', '89C26DEE41E2', '89C26DEE41E2'],\n                   'desc': ['Car', 'Car', 'Bus', 'Bus', 'Bus'],\n                   'quantity': [2, 2, 1, 3, 3]})\nprint(df, '\\n')\n</code></pre>\n<pre><code>             id desc  quantity\n0  B668441DE83B  Car         2\n1  B668441DE83B  Car         2\n2  B668441DE83B  Bus         1\n3  89C26DEE41E2  Bus         3\n4  89C26DEE41E2  Bus         3 \n</code></pre>\n<p>I need to change the value of <code>quantity</code> column to 1 if <code>quantity</code> value of row equals the actual quantity of rows, where columns <code>id</code> and <code>desc</code> are equals (row0 and row1 in this example).</p>\n<p>Desired output:</p>\n<pre><code>             id desc  quantity\n0  B668441DE83B  Car         1\n1  B668441DE83B  Car         1\n2  B668441DE83B  Bus         1\n3  89C26DEE41E2  Bus         3\n4  89C26DEE41E2  Bus         3 \n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> for count values per groups, compare by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html\" rel=\"nofollow noreferrer\"><code>Series.eq</code></a> for <code>==</code> by original and last set <code>1</code> by mask:</p>\n<pre><code>mask = df.groupby(['id','desc'])['id'].transform('size').eq(df['quantity'])\n\ndf.loc[mask, 'quantity'] = 1\n</code></pre>\n<p>Or:</p>\n<pre><code>df['quantity'] = df['quantity'].mask(mask, 1)\n</code></pre>\n<hr />\n<pre><code>print (df)\n             id desc  quantity\n0  B668441DE83B  Car         1\n1  B668441DE83B  Car         1\n2  B668441DE83B  Bus         1\n3  89C26DEE41E2  Bus         3\n4  89C26DEE41E2  Bus         3\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>df = pd.DataFrame({'id': ['B668441DE83B', 'B668441DE83B', 'B668441DE83B', '89C26DEE41E2', '89C26DEE41E2'],\n                   'desc': ['Car', 'Car', 'Bus', 'Bus', 'Bus'],\n                   'quantity': [2, 2, 1, 3, 3]})\nprint(df, '\\n')\n</code></pre>\n<pre><code>             id desc  quantity\n0  B668441DE83B  Car         2\n1  B668441DE83B  Car         2\n2  B668441DE83B  Bus         1\n3  89C26DEE41E2  Bus         3\n4  89C26DEE41E2  Bus         3 \n</code></pre>\n<p>I need to change the value of <code>quantity</code> column to 1 if <code>quantity</code> value of row equals the actual quantity of rows, where columns <code>id</code> and <code>desc</code> are equals (row0 and row1 in this example).</p>\n<p>Desired output:</p>\n<pre><code>             id desc  quantity\n0  B668441DE83B  Car         1\n1  B668441DE83B  Car         1\n2  B668441DE83B  Bus         1\n3  89C26DEE41E2  Bus         3\n4  89C26DEE41E2  Bus         3 \n</code></pre>\n",
        "formatted_input": {
            "qid": 63595559,
            "link": "https://stackoverflow.com/questions/63595559/change-the-value-of-column-based-on-quantity-of-equals-rows",
            "question": {
                "title": "Change the value of column based on quantity of equals rows",
                "ques_desc": "I have a dataframe like this: I need to change the value of column to 1 if value of row equals the actual quantity of rows, where columns and are equals (row0 and row1 in this example). Desired output: "
            },
            "io": [
                "             id desc  quantity\n0  B668441DE83B  Car         2\n1  B668441DE83B  Car         2\n2  B668441DE83B  Bus         1\n3  89C26DEE41E2  Bus         3\n4  89C26DEE41E2  Bus         3 \n",
                "             id desc  quantity\n0  B668441DE83B  Car         1\n1  B668441DE83B  Car         1\n2  B668441DE83B  Bus         1\n3  89C26DEE41E2  Bus         3\n4  89C26DEE41E2  Bus         3 \n"
            ],
            "answer": {
                "ans_desc": "Use for count values per groups, compare by for by original and last set by mask: Or: ",
                "code": [
                    "mask = df.groupby(['id','desc'])['id'].transform('size').eq(df['quantity'])\n\ndf.loc[mask, 'quantity'] = 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 12047072,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/CfLuU.gif?s=128&g=1",
            "display_name": "Warrior404",
            "link": "https://stackoverflow.com/users/12047072/warrior404"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 63583336,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1598373849,
        "creation_date": 1598372840,
        "question_id": 63583217,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63583217/multiply-dataframe-rows-depends-on-value-in-this-row",
        "title": "Multiply dataframe rows depends on value in this row",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>df = pd.DataFrame({'col1': [69, 77, 88],\n                   'col2': ['bar34', 'barf30', 'barfoo29'],\n                   'col3': [4, 2, 5]})\nprint(df, '\\n')\n</code></pre>\n<pre><code>   col1      col2  col3\n0    69     bar34     4\n1    77    barf30     2\n2    88  barfoo29     5 \n</code></pre>\n<p>I need multiply rows depends on value in 'col3'. Desired output:</p>\n<pre><code>    col1      col2  col3\n0     69     bar34     4\n1     69     bar34     4\n2     69     bar34     4\n3     69     bar34     4\n4     77    barf30     2\n5     77    barf30     2\n6     88  barfoo29     5 \n7     88  barfoo29     5 \n8     88  barfoo29     5 \n9     88  barfoo29     5 \n10    88  barfoo29     5 \n</code></pre>\n",
        "answer_body": "<p>Here is a solution, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.repeat.html#pandas-index-repeat\" rel=\"nofollow noreferrer\"><code>Index.repeat</code></a> then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html#pandas-dataframe-reindex\" rel=\"nofollow noreferrer\"><code>DataFrame.reindex</code></a></p>\n<pre><code>df = df.set_index(df.col3)\n\nprint(\n    df.reindex(df.index.repeat(df.col3))\n        .reset_index(drop=True)\n)\n</code></pre>\n<hr />\n<pre><code># suggested by @anky,\n\ndf.loc[df.index.repeat(df.col3)]\n</code></pre>\n<hr />\n<pre><code>    col1      col2  col3\n0     69     bar34     4\n1     69     bar34     4\n2     69     bar34     4\n3     69     bar34     4\n4     77    barf30     2\n5     77    barf30     2\n6     88  barfoo29     5\n7     88  barfoo29     5\n8     88  barfoo29     5\n9     88  barfoo29     5\n10    88  barfoo29     5\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>df = pd.DataFrame({'col1': [69, 77, 88],\n                   'col2': ['bar34', 'barf30', 'barfoo29'],\n                   'col3': [4, 2, 5]})\nprint(df, '\\n')\n</code></pre>\n<pre><code>   col1      col2  col3\n0    69     bar34     4\n1    77    barf30     2\n2    88  barfoo29     5 \n</code></pre>\n<p>I need multiply rows depends on value in 'col3'. Desired output:</p>\n<pre><code>    col1      col2  col3\n0     69     bar34     4\n1     69     bar34     4\n2     69     bar34     4\n3     69     bar34     4\n4     77    barf30     2\n5     77    barf30     2\n6     88  barfoo29     5 \n7     88  barfoo29     5 \n8     88  barfoo29     5 \n9     88  barfoo29     5 \n10    88  barfoo29     5 \n</code></pre>\n",
        "formatted_input": {
            "qid": 63583217,
            "link": "https://stackoverflow.com/questions/63583217/multiply-dataframe-rows-depends-on-value-in-this-row",
            "question": {
                "title": "Multiply dataframe rows depends on value in this row",
                "ques_desc": "I have a dataframe like this: I need multiply rows depends on value in 'col3'. Desired output: "
            },
            "io": [
                "   col1      col2  col3\n0    69     bar34     4\n1    77    barf30     2\n2    88  barfoo29     5 \n",
                "    col1      col2  col3\n0     69     bar34     4\n1     69     bar34     4\n2     69     bar34     4\n3     69     bar34     4\n4     77    barf30     2\n5     77    barf30     2\n6     88  barfoo29     5 \n7     88  barfoo29     5 \n8     88  barfoo29     5 \n9     88  barfoo29     5 \n10    88  barfoo29     5 \n"
            ],
            "answer": {
                "ans_desc": "Here is a solution, then ",
                "code": [
                    "df = df.set_index(df.col3)\n\nprint(\n    df.reindex(df.index.repeat(df.col3))\n        .reset_index(drop=True)\n)\n",
                    "# suggested by @anky,\n\ndf.loc[df.index.repeat(df.col3)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "python-2.7",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 14128851,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8141b5fb1439fe48ff448dab4bfa35a7?s=128&d=identicon&r=PG&f=1",
            "display_name": "Priyankar Bose",
            "link": "https://stackoverflow.com/users/14128851/priyankar-bose"
        },
        "is_answered": true,
        "view_count": 139,
        "accepted_answer_id": 63479322,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1597805520,
        "creation_date": 1597803234,
        "last_edit_date": 1597803504,
        "question_id": 63479071,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63479071/how-to-convert-a-pandas-dataframe-column-from-string-to-an-array-of-floats",
        "title": "How to convert a pandas dataframe column from string to an array of floats?",
        "body": "<p>I have a dataframe where a column is an array of floats. When I am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows:</p>\n<pre><code>'[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0]', \n'[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0]',....\n</code></pre>\n<p>I want to convert this long character string into an array of floats like this:</p>\n<pre><code>[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0],\n[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0],...\n</code></pre>\n<p>Is there a way to do that?</p>\n",
        "answer_body": "<p>If possible, it would be best to read the csv correctly (as a list of floats) rather than casting them from the strings.  You can however use <code>eval</code> or <code>ast.literal_eval</code> to cast this to a list of floats:</p>\n<pre><code>from ast import literal_eval    \ndf[&quot;a&quot;] = df[&quot;a&quot;].apply(lambda x: literal_eval(x))\n</code></pre>\n",
        "question_body": "<p>I have a dataframe where a column is an array of floats. When I am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows:</p>\n<pre><code>'[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0]', \n'[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0]',....\n</code></pre>\n<p>I want to convert this long character string into an array of floats like this:</p>\n<pre><code>[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0],\n[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0],...\n</code></pre>\n<p>Is there a way to do that?</p>\n",
        "formatted_input": {
            "qid": 63479071,
            "link": "https://stackoverflow.com/questions/63479071/how-to-convert-a-pandas-dataframe-column-from-string-to-an-array-of-floats",
            "question": {
                "title": "How to convert a pandas dataframe column from string to an array of floats?",
                "ques_desc": "I have a dataframe where a column is an array of floats. When I am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows: I want to convert this long character string into an array of floats like this: Is there a way to do that? "
            },
            "io": [
                "'[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0]', \n'[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0]',....\n",
                "[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0],\n[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0],...\n"
            ],
            "answer": {
                "ans_desc": "If possible, it would be best to read the csv correctly (as a list of floats) rather than casting them from the strings. You can however use or to cast this to a list of floats: ",
                "code": [
                    "from ast import literal_eval    \ndf[\"a\"] = df[\"a\"].apply(lambda x: literal_eval(x))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "split"
        ],
        "owner": {
            "reputation": 81,
            "user_id": 11055992,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a24a314d24cc83fb44bac788980b0995?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nightingale",
            "link": "https://stackoverflow.com/users/11055992/nightingale"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 63451838,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1597670941,
        "creation_date": 1597670763,
        "question_id": 63451784,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63451784/how-to-combine-rows-into-seperate-dataframe-python-pandas",
        "title": "How to combine rows into seperate dataframe python pandas",
        "body": "<p>i have the following dataset:</p>\n<pre><code>A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x\n255 253.960131  242.5382584         1   1   x\n251.9665958 235.1105659 185.9121703 1   1   x\n137.9974994 225.3985177 254.4420772 1   1   x\n85.74722877 116.7060415 158.4608395 1   1   x\n123.6969939 140.0524405 132.6798037 1   1   x\n133.3251695 80.08976196 38.81201612 1   1   y\n118.0718812 243.5927927 255         1   1   y\n189.5557302 139.9046713 91.90519519 1   1   y\n172.3117291 188.000268  129.8155501 1   1   y\n48.07634611 21.9183119  25.99669279 1   1   y\n23.40525987 8.395857933 25.62371342 1   1   y\n228.753009  164.0697727 172.6624107 1   1   z\n203.3405006 173.9368303 189.8103708 1   1   z\n184.9801932 117.1591341 87.94739034 1   1   z\n29.55251224 46.03945452 70.7433477  1   1   z\n143.6159623 120.6170926 155.0736604 1   1   z\n142.5421179 128.8916843 169.6013111 1   1   z\n\n</code></pre>\n<p>i want to combine x y z into another dataframe like this:</p>\n<pre><code>A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x  -&gt;first x value\n133.3251695 80.08976196 38.81201612 1   1   y  -&gt;first y value\n228.753009  164.0697727 172.6624107 1   1   z  -&gt;first z value\n</code></pre>\n<p>and i want these dataframes for each x y z value like first, second third and so on.</p>\n<p>how can i select and combine them?</p>\n<p>desired output:</p>\n<pre><code>A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x\n133.3251695 80.08976196 38.81201612 1   1   y\n228.753009  164.0697727 172.6624107 1   1   z\n\nA           B           C           D   E   F\n255 253.960131  242.5382584         1   1   x\n118.0718812 243.5927927 255         1   1   y\n203.3405006 173.9368303 189.8103708 1   1   z\n\nA           B           C           D   E   F\n251.9665958 235.1105659 185.9121703 1   1   x\n189.5557302 139.9046713 91.90519519 1   1   y\n184.9801932 117.1591341 87.94739034 1   1   z\n\nA           B           C           D   E   F\n137.9974994 225.3985177 254.4420772 1   1   x\n172.3117291 188.000268  129.8155501 1   1   y\n29.55251224 46.03945452 70.7433477  1   1   z\n\nA           B           C           D   E   F\n85.74722877 116.7060415 158.4608395 1   1   x\n48.07634611 21.9183119  25.99669279 1   1   y\n143.6159623 120.6170926 155.0736604 1   1   z\n\nA           B           C           D   E   F\n123.6969939 140.0524405 132.6798037 1   1   x\n23.40525987 8.395857933 25.62371342 1   1   y\n142.5421179 128.8916843 169.6013111 1   1   z\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html\" rel=\"nofollow noreferrer\"><code>GroupBy.cumcount</code></a> for counter and then loop by another groupby object:</p>\n<pre><code>g = df.groupby('F').cumcount()\n\nfor i, g in df.groupby(g):\n    print (g)\n</code></pre>\n",
        "question_body": "<p>i have the following dataset:</p>\n<pre><code>A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x\n255 253.960131  242.5382584         1   1   x\n251.9665958 235.1105659 185.9121703 1   1   x\n137.9974994 225.3985177 254.4420772 1   1   x\n85.74722877 116.7060415 158.4608395 1   1   x\n123.6969939 140.0524405 132.6798037 1   1   x\n133.3251695 80.08976196 38.81201612 1   1   y\n118.0718812 243.5927927 255         1   1   y\n189.5557302 139.9046713 91.90519519 1   1   y\n172.3117291 188.000268  129.8155501 1   1   y\n48.07634611 21.9183119  25.99669279 1   1   y\n23.40525987 8.395857933 25.62371342 1   1   y\n228.753009  164.0697727 172.6624107 1   1   z\n203.3405006 173.9368303 189.8103708 1   1   z\n184.9801932 117.1591341 87.94739034 1   1   z\n29.55251224 46.03945452 70.7433477  1   1   z\n143.6159623 120.6170926 155.0736604 1   1   z\n142.5421179 128.8916843 169.6013111 1   1   z\n\n</code></pre>\n<p>i want to combine x y z into another dataframe like this:</p>\n<pre><code>A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x  -&gt;first x value\n133.3251695 80.08976196 38.81201612 1   1   y  -&gt;first y value\n228.753009  164.0697727 172.6624107 1   1   z  -&gt;first z value\n</code></pre>\n<p>and i want these dataframes for each x y z value like first, second third and so on.</p>\n<p>how can i select and combine them?</p>\n<p>desired output:</p>\n<pre><code>A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x\n133.3251695 80.08976196 38.81201612 1   1   y\n228.753009  164.0697727 172.6624107 1   1   z\n\nA           B           C           D   E   F\n255 253.960131  242.5382584         1   1   x\n118.0718812 243.5927927 255         1   1   y\n203.3405006 173.9368303 189.8103708 1   1   z\n\nA           B           C           D   E   F\n251.9665958 235.1105659 185.9121703 1   1   x\n189.5557302 139.9046713 91.90519519 1   1   y\n184.9801932 117.1591341 87.94739034 1   1   z\n\nA           B           C           D   E   F\n137.9974994 225.3985177 254.4420772 1   1   x\n172.3117291 188.000268  129.8155501 1   1   y\n29.55251224 46.03945452 70.7433477  1   1   z\n\nA           B           C           D   E   F\n85.74722877 116.7060415 158.4608395 1   1   x\n48.07634611 21.9183119  25.99669279 1   1   y\n143.6159623 120.6170926 155.0736604 1   1   z\n\nA           B           C           D   E   F\n123.6969939 140.0524405 132.6798037 1   1   x\n23.40525987 8.395857933 25.62371342 1   1   y\n142.5421179 128.8916843 169.6013111 1   1   z\n</code></pre>\n",
        "formatted_input": {
            "qid": 63451784,
            "link": "https://stackoverflow.com/questions/63451784/how-to-combine-rows-into-seperate-dataframe-python-pandas",
            "question": {
                "title": "How to combine rows into seperate dataframe python pandas",
                "ques_desc": "i have the following dataset: i want to combine x y z into another dataframe like this: and i want these dataframes for each x y z value like first, second third and so on. how can i select and combine them? desired output: "
            },
            "io": [
                "A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x\n255 253.960131  242.5382584         1   1   x\n251.9665958 235.1105659 185.9121703 1   1   x\n137.9974994 225.3985177 254.4420772 1   1   x\n85.74722877 116.7060415 158.4608395 1   1   x\n123.6969939 140.0524405 132.6798037 1   1   x\n133.3251695 80.08976196 38.81201612 1   1   y\n118.0718812 243.5927927 255         1   1   y\n189.5557302 139.9046713 91.90519519 1   1   y\n172.3117291 188.000268  129.8155501 1   1   y\n48.07634611 21.9183119  25.99669279 1   1   y\n23.40525987 8.395857933 25.62371342 1   1   y\n228.753009  164.0697727 172.6624107 1   1   z\n203.3405006 173.9368303 189.8103708 1   1   z\n184.9801932 117.1591341 87.94739034 1   1   z\n29.55251224 46.03945452 70.7433477  1   1   z\n143.6159623 120.6170926 155.0736604 1   1   z\n142.5421179 128.8916843 169.6013111 1   1   z\n\n",
                "A           B           C           D   E   F\n154.6175111 148.0112337 155.7859835 1   1   x\n133.3251695 80.08976196 38.81201612 1   1   y\n228.753009  164.0697727 172.6624107 1   1   z\n\nA           B           C           D   E   F\n255 253.960131  242.5382584         1   1   x\n118.0718812 243.5927927 255         1   1   y\n203.3405006 173.9368303 189.8103708 1   1   z\n\nA           B           C           D   E   F\n251.9665958 235.1105659 185.9121703 1   1   x\n189.5557302 139.9046713 91.90519519 1   1   y\n184.9801932 117.1591341 87.94739034 1   1   z\n\nA           B           C           D   E   F\n137.9974994 225.3985177 254.4420772 1   1   x\n172.3117291 188.000268  129.8155501 1   1   y\n29.55251224 46.03945452 70.7433477  1   1   z\n\nA           B           C           D   E   F\n85.74722877 116.7060415 158.4608395 1   1   x\n48.07634611 21.9183119  25.99669279 1   1   y\n143.6159623 120.6170926 155.0736604 1   1   z\n\nA           B           C           D   E   F\n123.6969939 140.0524405 132.6798037 1   1   x\n23.40525987 8.395857933 25.62371342 1   1   y\n142.5421179 128.8916843 169.6013111 1   1   z\n"
            ],
            "answer": {
                "ans_desc": "Use for counter and then loop by another groupby object: ",
                "code": [
                    "g = df.groupby('F').cumcount()\n\nfor i, g in df.groupby(g):\n    print (g)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 14115604,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/10214131673245520/picture?type=large",
            "display_name": "Anmol Sureka",
            "link": "https://stackoverflow.com/users/14115604/anmol-sureka"
        },
        "is_answered": true,
        "view_count": 116,
        "accepted_answer_id": 63440819,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1597625599,
        "creation_date": 1597603532,
        "last_edit_date": 1597610015,
        "question_id": 63440751,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63440751/sort-pandas-dataframe-column-index-by-date",
        "title": "Sort Pandas dataframe column index by date",
        "body": "<p>I want to sort dataframe by column index. The issue is my columns are 'dates' dd/mm/yyyy directly imported from my excel. For ex:</p>\n<pre><code>    10/08/20  12/08/20 11/08/20\n0   2.0        6.0       15.0\n1   6.0        11.0      8.0\n2   4.0        7.0       3.0\n3   7.0        12.0      2.0\n4   12.0       5.0       7.0\n</code></pre>\n<p>The output I want is:</p>\n<pre><code>    10/08/20  11/08/20 12/08/20\n0   2.0        15.0      6.0\n1   6.0        8.0       11.0\n2   4.0        3.0       7.0\n3   7.0        2.0       12.0\n4   12.0       7.0       5.0\n</code></pre>\n<p>I am using</p>\n<pre><code>df.sort_index(axis=1)\n</code></pre>\n<p>It is giving me following error:</p>\n<blockquote>\n<p>TypeError: '&lt;' not supported between instances of 'datetime.datetime'\nand 'str'</p>\n</blockquote>\n<p>I want to do it in panda dataframe. Any help will be appreciated. Thanks</p>\n",
        "answer_body": "<p>First remove the <strong>'.' at the end</strong> of date from the data shource sheet.\nthe for this data</p>\n<pre><code>    10-08-2020  12-08-2020  11-08-2020\n0   2           6           15\n1   6           11          8\n2   4           7           3\n3   7           12          2\n4   12          5           7\n</code></pre>\n<p>try this</p>\n<pre><code>import datetime as dt\ndf.columns=pd.Series(df.columns).apply(lambda d: dt.datetime(d, dt.datetime.strptime(d, '%d/%m/%Y')))\ndf.sort_index(axis = 1)\n</code></pre>\n",
        "question_body": "<p>I want to sort dataframe by column index. The issue is my columns are 'dates' dd/mm/yyyy directly imported from my excel. For ex:</p>\n<pre><code>    10/08/20  12/08/20 11/08/20\n0   2.0        6.0       15.0\n1   6.0        11.0      8.0\n2   4.0        7.0       3.0\n3   7.0        12.0      2.0\n4   12.0       5.0       7.0\n</code></pre>\n<p>The output I want is:</p>\n<pre><code>    10/08/20  11/08/20 12/08/20\n0   2.0        15.0      6.0\n1   6.0        8.0       11.0\n2   4.0        3.0       7.0\n3   7.0        2.0       12.0\n4   12.0       7.0       5.0\n</code></pre>\n<p>I am using</p>\n<pre><code>df.sort_index(axis=1)\n</code></pre>\n<p>It is giving me following error:</p>\n<blockquote>\n<p>TypeError: '&lt;' not supported between instances of 'datetime.datetime'\nand 'str'</p>\n</blockquote>\n<p>I want to do it in panda dataframe. Any help will be appreciated. Thanks</p>\n",
        "formatted_input": {
            "qid": 63440751,
            "link": "https://stackoverflow.com/questions/63440751/sort-pandas-dataframe-column-index-by-date",
            "question": {
                "title": "Sort Pandas dataframe column index by date",
                "ques_desc": "I want to sort dataframe by column index. The issue is my columns are 'dates' dd/mm/yyyy directly imported from my excel. For ex: The output I want is: I am using It is giving me following error: TypeError: '<' not supported between instances of 'datetime.datetime' and 'str' I want to do it in panda dataframe. Any help will be appreciated. Thanks "
            },
            "io": [
                "    10/08/20  12/08/20 11/08/20\n0   2.0        6.0       15.0\n1   6.0        11.0      8.0\n2   4.0        7.0       3.0\n3   7.0        12.0      2.0\n4   12.0       5.0       7.0\n",
                "    10/08/20  11/08/20 12/08/20\n0   2.0        15.0      6.0\n1   6.0        8.0       11.0\n2   4.0        3.0       7.0\n3   7.0        2.0       12.0\n4   12.0       7.0       5.0\n"
            ],
            "answer": {
                "ans_desc": "First remove the '.' at the end of date from the data shource sheet. the for this data try this ",
                "code": [
                    "import datetime as dt\ndf.columns=pd.Series(df.columns).apply(lambda d: dt.datetime(d, dt.datetime.strptime(d, '%d/%m/%Y')))\ndf.sort_index(axis = 1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "minmax"
        ],
        "owner": {
            "reputation": 1878,
            "user_id": 11922765,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ed69b624cdb86e52caf0010e274df7b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mainland",
            "link": "https://stackoverflow.com/users/11922765/mainland"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 63387479,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1597287285,
        "creation_date": 1597287152,
        "question_id": 63387459,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63387459/python-dataframe-rowwise-min-and-max-with-nan-values",
        "title": "Python Dataframe rowwise min and max with NaN values",
        "body": "<p>My dataframe has nans. But I am trying to find row wise min and max. How do I find it.</p>\n<pre><code>df = pd.DataFrame({&quot;A&quot;:[12,NaN,13],&quot;B&quot;:[45,12,65],&quot;C&quot;:[63,78,NaN]})\n\ndf= \n    A    B     C\n0   12   45    63\n1   NaN  12   78\n2   13   65    NaN\n</code></pre>\n<p>I am tring to find min and max in each row and difference between them in column <code>A</code> <code>B</code> and <code>C</code>.\nMy code:</p>\n<pre><code>poacols = ['A','B','C']\ndf['dif'] = np.nanmax(df[poacols])-np.nanmin(df[poacols])\n</code></pre>\n<p>Present output:</p>\n<pre><code>df['dif'] = \n0    66\n1    66\n2    66\n</code></pre>\n<p>Expected output:</p>\n<pre><code>df['dif'] = \n0    51\n1    66\n2    52\n</code></pre>\n",
        "answer_body": "<p>We should add the axis=1, check the min and max for each row</p>\n<pre><code>np.nanmax(df[poacols],1)-np.nanmin(df[poacols],1)\nOut[81]: array([51., 66., 52.])\n</code></pre>\n",
        "question_body": "<p>My dataframe has nans. But I am trying to find row wise min and max. How do I find it.</p>\n<pre><code>df = pd.DataFrame({&quot;A&quot;:[12,NaN,13],&quot;B&quot;:[45,12,65],&quot;C&quot;:[63,78,NaN]})\n\ndf= \n    A    B     C\n0   12   45    63\n1   NaN  12   78\n2   13   65    NaN\n</code></pre>\n<p>I am tring to find min and max in each row and difference between them in column <code>A</code> <code>B</code> and <code>C</code>.\nMy code:</p>\n<pre><code>poacols = ['A','B','C']\ndf['dif'] = np.nanmax(df[poacols])-np.nanmin(df[poacols])\n</code></pre>\n<p>Present output:</p>\n<pre><code>df['dif'] = \n0    66\n1    66\n2    66\n</code></pre>\n<p>Expected output:</p>\n<pre><code>df['dif'] = \n0    51\n1    66\n2    52\n</code></pre>\n",
        "formatted_input": {
            "qid": 63387459,
            "link": "https://stackoverflow.com/questions/63387459/python-dataframe-rowwise-min-and-max-with-nan-values",
            "question": {
                "title": "Python Dataframe rowwise min and max with NaN values",
                "ques_desc": "My dataframe has nans. But I am trying to find row wise min and max. How do I find it. I am tring to find min and max in each row and difference between them in column and . My code: Present output: Expected output: "
            },
            "io": [
                "df['dif'] = \n0    66\n1    66\n2    66\n",
                "df['dif'] = \n0    51\n1    66\n2    52\n"
            ],
            "answer": {
                "ans_desc": "We should add the axis=1, check the min and max for each row ",
                "code": [
                    "np.nanmax(df[poacols],1)-np.nanmin(df[poacols],1)\nOut[81]: array([51., 66., 52.])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv",
            "split"
        ],
        "owner": {
            "reputation": 81,
            "user_id": 11055992,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a24a314d24cc83fb44bac788980b0995?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nightingale",
            "link": "https://stackoverflow.com/users/11055992/nightingale"
        },
        "is_answered": true,
        "view_count": 44,
        "closed_date": 1597129305,
        "accepted_answer_id": 63353179,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1597129683,
        "creation_date": 1597129198,
        "question_id": 63353050,
        "link": "https://stackoverflow.com/questions/63353050/pandas-dataframe-split-into-seperate-csv-by-column-value",
        "closed_reason": "Duplicate",
        "title": "Pandas Dataframe split into seperate csv by column value",
        "body": "<p>Hello i have dataset containing 4 columns</p>\n<pre><code>x       y     z      s\n1      42.8  157.5   1\n1      43.8  13.5    1\n1      44.8  152     2\n         .\n         .\n         .\n4      7528  157.5   2\n4      45.8  13.5    3\n8      72.8  152     3\n\n</code></pre>\n<p>i want to split my dataframe into separate csv files by their &quot;s&quot; column but I couldn't figure out a proper way of doing it.</p>\n<p>&quot;s&quot; column has arbitrary numbers of labels. We don't know how many 1's or 2's dataset has. it is until 30 but not every number is contained in this dataset.</p>\n<p>My desired output is:</p>\n<pre><code>df1\nx       y     z      s\n1      42.8  157.5   1\n           .\n1      43.8  13.5    1\n\ndf2\n1      44.8  152     2\n           .\n4      7528  157.5   2\n\ndf3\n4      45.8  13.5    3\n           .\n8      72.8  152     3\n\n</code></pre>\n<p>after I get this split I can easily write it to separate csv files.\nThe problem  I am having is that I don't know how many different &quot;s&quot; values I have and how much from each of them.</p>\n<p>Thank you</p>\n",
        "answer_body": "<p>Just <code>groupby</code> before sending to csv to do this dynamically:</p>\n<pre><code>for i, x in df.groupby('s'): x.to_csv(f'df{i}.csv', index=False)\n</code></pre>\n",
        "question_body": "<p>Hello i have dataset containing 4 columns</p>\n<pre><code>x       y     z      s\n1      42.8  157.5   1\n1      43.8  13.5    1\n1      44.8  152     2\n         .\n         .\n         .\n4      7528  157.5   2\n4      45.8  13.5    3\n8      72.8  152     3\n\n</code></pre>\n<p>i want to split my dataframe into separate csv files by their &quot;s&quot; column but I couldn't figure out a proper way of doing it.</p>\n<p>&quot;s&quot; column has arbitrary numbers of labels. We don't know how many 1's or 2's dataset has. it is until 30 but not every number is contained in this dataset.</p>\n<p>My desired output is:</p>\n<pre><code>df1\nx       y     z      s\n1      42.8  157.5   1\n           .\n1      43.8  13.5    1\n\ndf2\n1      44.8  152     2\n           .\n4      7528  157.5   2\n\ndf3\n4      45.8  13.5    3\n           .\n8      72.8  152     3\n\n</code></pre>\n<p>after I get this split I can easily write it to separate csv files.\nThe problem  I am having is that I don't know how many different &quot;s&quot; values I have and how much from each of them.</p>\n<p>Thank you</p>\n",
        "formatted_input": {
            "qid": 63353050,
            "link": "https://stackoverflow.com/questions/63353050/pandas-dataframe-split-into-seperate-csv-by-column-value",
            "question": {
                "title": "Pandas Dataframe split into seperate csv by column value",
                "ques_desc": "Hello i have dataset containing 4 columns i want to split my dataframe into separate csv files by their \"s\" column but I couldn't figure out a proper way of doing it. \"s\" column has arbitrary numbers of labels. We don't know how many 1's or 2's dataset has. it is until 30 but not every number is contained in this dataset. My desired output is: after I get this split I can easily write it to separate csv files. The problem I am having is that I don't know how many different \"s\" values I have and how much from each of them. Thank you "
            },
            "io": [
                "x       y     z      s\n1      42.8  157.5   1\n1      43.8  13.5    1\n1      44.8  152     2\n         .\n         .\n         .\n4      7528  157.5   2\n4      45.8  13.5    3\n8      72.8  152     3\n\n",
                "df1\nx       y     z      s\n1      42.8  157.5   1\n           .\n1      43.8  13.5    1\n\ndf2\n1      44.8  152     2\n           .\n4      7528  157.5   2\n\ndf3\n4      45.8  13.5    3\n           .\n8      72.8  152     3\n\n"
            ],
            "answer": {
                "ans_desc": "Just before sending to csv to do this dynamically: ",
                "code": [
                    "for i, x in df.groupby('s'): x.to_csv(f'df{i}.csv', index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 13373759,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-Ktzznk_4Pn4/AAAAAAAAAAI/AAAAAAAAAC0/AAKWJJM8NsgJGy4UtldA0c7bnMSulrYNBg/photo.jpg?sz=128",
            "display_name": "Sash9",
            "link": "https://stackoverflow.com/users/13373759/sash9"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 63286349,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1596726783,
        "creation_date": 1596725556,
        "last_edit_date": 1596725880,
        "question_id": 63286249,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63286249/pandas-drop-lines-from-dataframe-if-column-value-is-in-list-csv",
        "title": "Pandas - Drop lines from dataframe if column value is in list (.csv)",
        "body": "<p>I have a pandas dataframe imported from SQL, and I would like to drop lines for which a column value is in a list, which I get from a csv file. It seems pretty straighforward, I looked it up and I tried several things using <code>.isin()</code> but this is not working as I expect.</p>\n<p>For example the dataframe imported from SQL looks like this, let's call it df :</p>\n<pre><code>    SKU        Brand\n0  AD31KL-A1   BrandA\n1  BC31KL-B3   BrandB\n2  DE31KL-D4   BrandC\n3  FG31KL-F5   BrandD\n</code></pre>\n<p>I import this list this way :</p>\n<pre><code>df2 = pd.read_csv(&quot;list.csv&quot;)\nlist = df2.apply(lambda x: x.tolist(), axis=1)\n</code></pre>\n<p>Let's assume I print the list, this is what I see :</p>\n<pre><code>[BC31KL-B3]\n[DE31KL-D4]\nLength: 2, dtype: object\n</code></pre>\n<p>Then I use the following :</p>\n<pre><code>df = df[~df.SKU.isin(list)]\n</code></pre>\n<p>I would expect to get this (initial df with lines 1 and 2 dropped because they are in the list)</p>\n<pre><code>    SKU        Brand\n0  AD31KL-A1   BrandA\n3  FG31KL-F5   BrandD\n</code></pre>\n<p>However this is not what happens. I get the exact same df as initially with no lines dropped, and also no error message of any kind. What am I doing wrong ?<br />\nI thought the data in the list and in the df column might not be the same type and I tried fiddling with <code>astype()</code>, but without much success. Perhaps i'm using it wrong.</p>\n<p>Would appreciate any help. Thanks !</p>\n",
        "answer_body": "<p>It looks like this line is your problem:</p>\n<pre><code>list = df2.apply(lambda x: x.tolist(), axis=1)\n</code></pre>\n<p>The result of a df apply is another df.  Let's say the .csv file has a column called SKU.  You can make a list out of that column only:</p>\n<pre><code>list = df2['SKU'].tolist()\n</code></pre>\n<p>Here's some sample code showing that a  column of values can be converted to a list just by calling <code>tolist()</code> on the column/series:</p>\n<pre><code># Well, I don't have list.csv, so let me just create a dataframe\ndf = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ], columns = ['SKU'] )\nprint(df)\nlist =  df['SKU'].tolist() \nprint( list ) \n</code></pre>\n<p>Here's the df representing list.csv:</p>\n<pre><code>         SKU\n0  AD31KL-A1\n1  BC31KL-B3\n2  DE31KL-D4\n3  FG31KL-F5\n</code></pre>\n<p>And here's the list:</p>\n<pre><code>['AD31KL-A1', 'BC31KL-B3', 'DE31KL-D4', 'FG31KL-F5']\n</code></pre>\n<p>Lastly, if you don't have a column name, you can just get the first column by it's integer value thusly:</p>\n<pre><code>df = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ] )\nprint(df)\nlist =  df.iloc[:, 0].tolist()  # first column of dataframe\nprint( list ) \n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe imported from SQL, and I would like to drop lines for which a column value is in a list, which I get from a csv file. It seems pretty straighforward, I looked it up and I tried several things using <code>.isin()</code> but this is not working as I expect.</p>\n<p>For example the dataframe imported from SQL looks like this, let's call it df :</p>\n<pre><code>    SKU        Brand\n0  AD31KL-A1   BrandA\n1  BC31KL-B3   BrandB\n2  DE31KL-D4   BrandC\n3  FG31KL-F5   BrandD\n</code></pre>\n<p>I import this list this way :</p>\n<pre><code>df2 = pd.read_csv(&quot;list.csv&quot;)\nlist = df2.apply(lambda x: x.tolist(), axis=1)\n</code></pre>\n<p>Let's assume I print the list, this is what I see :</p>\n<pre><code>[BC31KL-B3]\n[DE31KL-D4]\nLength: 2, dtype: object\n</code></pre>\n<p>Then I use the following :</p>\n<pre><code>df = df[~df.SKU.isin(list)]\n</code></pre>\n<p>I would expect to get this (initial df with lines 1 and 2 dropped because they are in the list)</p>\n<pre><code>    SKU        Brand\n0  AD31KL-A1   BrandA\n3  FG31KL-F5   BrandD\n</code></pre>\n<p>However this is not what happens. I get the exact same df as initially with no lines dropped, and also no error message of any kind. What am I doing wrong ?<br />\nI thought the data in the list and in the df column might not be the same type and I tried fiddling with <code>astype()</code>, but without much success. Perhaps i'm using it wrong.</p>\n<p>Would appreciate any help. Thanks !</p>\n",
        "formatted_input": {
            "qid": 63286249,
            "link": "https://stackoverflow.com/questions/63286249/pandas-drop-lines-from-dataframe-if-column-value-is-in-list-csv",
            "question": {
                "title": "Pandas - Drop lines from dataframe if column value is in list (.csv)",
                "ques_desc": "I have a pandas dataframe imported from SQL, and I would like to drop lines for which a column value is in a list, which I get from a csv file. It seems pretty straighforward, I looked it up and I tried several things using but this is not working as I expect. For example the dataframe imported from SQL looks like this, let's call it df : I import this list this way : Let's assume I print the list, this is what I see : Then I use the following : I would expect to get this (initial df with lines 1 and 2 dropped because they are in the list) However this is not what happens. I get the exact same df as initially with no lines dropped, and also no error message of any kind. What am I doing wrong ? I thought the data in the list and in the df column might not be the same type and I tried fiddling with , but without much success. Perhaps i'm using it wrong. Would appreciate any help. Thanks ! "
            },
            "io": [
                "    SKU        Brand\n0  AD31KL-A1   BrandA\n1  BC31KL-B3   BrandB\n2  DE31KL-D4   BrandC\n3  FG31KL-F5   BrandD\n",
                "    SKU        Brand\n0  AD31KL-A1   BrandA\n3  FG31KL-F5   BrandD\n"
            ],
            "answer": {
                "ans_desc": "It looks like this line is your problem: The result of a df apply is another df. Let's say the .csv file has a column called SKU. You can make a list out of that column only: Here's some sample code showing that a column of values can be converted to a list just by calling on the column/series: Here's the df representing list.csv: And here's the list: Lastly, if you don't have a column name, you can just get the first column by it's integer value thusly: ",
                "code": [
                    "list = df2.apply(lambda x: x.tolist(), axis=1)\n",
                    "# Well, I don't have list.csv, so let me just create a dataframe\ndf = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ], columns = ['SKU'] )\nprint(df)\nlist =  df['SKU'].tolist() \nprint( list ) \n",
                    "df = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ] )\nprint(df)\nlist =  df.iloc[:, 0].tolist()  # first column of dataframe\nprint( list ) \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 14053777,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/685270362a7302484ea42a9516cebffa?s=128&d=identicon&r=PG&f=1",
            "display_name": "pavster",
            "link": "https://stackoverflow.com/users/14053777/pavster"
        },
        "is_answered": true,
        "view_count": 122,
        "accepted_answer_id": 63264823,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1596632772,
        "creation_date": 1596629018,
        "last_edit_date": 1596632604,
        "question_id": 63264777,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63264777/python-append-2-columns-of-a-dataframe-together",
        "title": "Python: Append 2 columns of a dataframe together",
        "body": "<p>I am loading a csv file into a data frame using pandas.</p>\n<p>My dataframe looks something like this:</p>\n<pre><code>col1       col2       col3         \n1           4           1 \n2           5           2\n3           6           3\n</code></pre>\n<p>I wish to append 2 of the columns into a new column:</p>\n<pre><code>  col1       col2        col3       col4   \n    1           4           1         1\n    2           5           2         2\n    3           6           3         3\n                                      4\n                                      5 \n                                      6\n</code></pre>\n<p>col4 needs to be created by appending the contents of col1 and col2 together.</p>\n<p>How can I do this in pandas/python?</p>\n<p><strong>EDIT</strong></p>\n<pre><code>df = df.reset_index(drop=True)\n\ns = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ndf = df.join(s, how='outer')\n\ndf = df.reset_index(drop=True)\n\ns = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\ndf = df.join(s, how='outer')\n</code></pre>\n",
        "answer_body": "<p>First use <code>Series.append</code> or <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> with <code>rename</code> for new <code>Series</code> and then add to original by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html\" rel=\"nofollow noreferrer\"><code>DataFrame.join</code></a> or <code>concat</code>:</p>\n<pre><code>s = df['col1'].append(df['col2'], ignore_index=True).rename('col4')\n#alternative\n#s = pd.concat([df['col1'], df['col2']], ignore_index=True).rename('col4')\n\ndf1 = df.join(s, how='outer')\n#alternative\n#df1 = pd.concat([df, s], axis=1)\n</code></pre>\n<hr />\n<pre><code>print (df1)\n\n   col1  col2  col3  col4\n0   1.0   4.0   1.0     1\n1   2.0   5.0   2.0     2\n2   3.0   6.0   3.0     3\n3   NaN   NaN   NaN     4\n4   NaN   NaN   NaN     5\n5   NaN   NaN   NaN     6\n</code></pre>\n<p>Last for avoid converting to floats is possible use:</p>\n<pre><code>df1 = df1.astype('Int64')\nprint (df1)\n   col1  col2  col3  col4\n0     1     4     1     1\n1     2     5     2     2\n2     3     6     3     3\n3  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     4\n4  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     5\n5  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     6\n</code></pre>\n<p>Or convert missing values to empty strings (what should be problem if need processing df later by some numeric method):</p>\n<pre><code>df1 = df1.fillna('')\nprint (df1)\n\n  col1 col2 col3  col4\n0    1    4    1     1\n1    2    5    2     2\n2    3    6    3     3\n3                    4\n4                    5\n5                    6\n</code></pre>\n<p>EDIT:</p>\n<pre><code>df = df.reset_index(drop=True)\n\ns1 = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ns2 = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\n\ndf1 = pd.concat([df, s1, s2], axis=1)\n</code></pre>\n",
        "question_body": "<p>I am loading a csv file into a data frame using pandas.</p>\n<p>My dataframe looks something like this:</p>\n<pre><code>col1       col2       col3         \n1           4           1 \n2           5           2\n3           6           3\n</code></pre>\n<p>I wish to append 2 of the columns into a new column:</p>\n<pre><code>  col1       col2        col3       col4   \n    1           4           1         1\n    2           5           2         2\n    3           6           3         3\n                                      4\n                                      5 \n                                      6\n</code></pre>\n<p>col4 needs to be created by appending the contents of col1 and col2 together.</p>\n<p>How can I do this in pandas/python?</p>\n<p><strong>EDIT</strong></p>\n<pre><code>df = df.reset_index(drop=True)\n\ns = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ndf = df.join(s, how='outer')\n\ndf = df.reset_index(drop=True)\n\ns = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\ndf = df.join(s, how='outer')\n</code></pre>\n",
        "formatted_input": {
            "qid": 63264777,
            "link": "https://stackoverflow.com/questions/63264777/python-append-2-columns-of-a-dataframe-together",
            "question": {
                "title": "Python: Append 2 columns of a dataframe together",
                "ques_desc": "I am loading a csv file into a data frame using pandas. My dataframe looks something like this: I wish to append 2 of the columns into a new column: col4 needs to be created by appending the contents of col1 and col2 together. How can I do this in pandas/python? EDIT "
            },
            "io": [
                "col1       col2       col3         \n1           4           1 \n2           5           2\n3           6           3\n",
                "  col1       col2        col3       col4   \n    1           4           1         1\n    2           5           2         2\n    3           6           3         3\n                                      4\n                                      5 \n                                      6\n"
            ],
            "answer": {
                "ans_desc": "First use or with for new and then add to original by or : Last for avoid converting to floats is possible use: Or convert missing values to empty strings (what should be problem if need processing df later by some numeric method): EDIT: ",
                "code": [
                    "s = df['col1'].append(df['col2'], ignore_index=True).rename('col4')\n#alternative\n#s = pd.concat([df['col1'], df['col2']], ignore_index=True).rename('col4')\n\ndf1 = df.join(s, how='outer')\n#alternative\n#df1 = pd.concat([df, s], axis=1)\n",
                    "df = df.reset_index(drop=True)\n\ns1 = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ns2 = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\n\ndf1 = pd.concat([df, s1, s2], axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 752,
            "user_id": 6057371,
            "user_type": "registered",
            "accept_rate": 31,
            "profile_image": "https://www.gravatar.com/avatar/b744e3dbff9c7fdb801570de75f6eff7?s=128&d=identicon&r=PG&f=1",
            "display_name": "okuoub",
            "link": "https://stackoverflow.com/users/6057371/okuoub"
        },
        "is_answered": true,
        "view_count": 82,
        "accepted_answer_id": 63259761,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1596609766,
        "creation_date": 1596609580,
        "question_id": 63259726,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63259726/pandas-dataframe-delete-groups-with-more-than-n-rows-in-groupby",
        "title": "pandas dataframe delete groups with more than n rows in groupby",
        "body": "<p>I have a dataframe:</p>\n<pre><code>df = [type1 , type2 , type3 , val1, val2, val3\n       a       b        q       1    2     3\n       a       c        w       3    5     2\n       b       c        t       2    9     0\n       a       b        p       4    6     7\n       a       c        m       2    1     8\n       a       b        h       8    6     3\n       a       b        e       4    2     7]\n</code></pre>\n<p>I want to apply groupby based on columns type1, type2 and delete from the dataframe the groups with more than 2 rows. So the new dataframe will be:</p>\n<pre><code>df = [type1 , type2 , type3 , val1, val2, val3\n       a       c        w       3    5     2\n       b       c        t       2    9     0\n       a       c        m       2    1     8\n  ]\n</code></pre>\n<p>What is the best way to do so?</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> for get counts of groups for <code>Series</code> with same size like original, so possible filter by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.le.html\" rel=\"nofollow noreferrer\"><code>Series.le</code></a> for <code>&lt;=</code> in <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a>:</p>\n<pre><code>df = df[df.groupby(['type1','type2'])['type1'].transform('size').le(2)]\nprint (df)\n  type1 type2 type3  val1  val2  val3\n1     a     c     w     3     5     2\n2     b     c     t     2     9     0\n4     a     c     m     2     1     8\n</code></pre>\n<p>If performace is not important or small DataFrame is possible use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html\" rel=\"nofollow noreferrer\"><code>DataFrameGroupBy.filter</code></a>:</p>\n<pre><code>df =df.groupby(['type1','type2']).filter(lambda x: len(x) &lt;= 2) \n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n<pre><code>df = [type1 , type2 , type3 , val1, val2, val3\n       a       b        q       1    2     3\n       a       c        w       3    5     2\n       b       c        t       2    9     0\n       a       b        p       4    6     7\n       a       c        m       2    1     8\n       a       b        h       8    6     3\n       a       b        e       4    2     7]\n</code></pre>\n<p>I want to apply groupby based on columns type1, type2 and delete from the dataframe the groups with more than 2 rows. So the new dataframe will be:</p>\n<pre><code>df = [type1 , type2 , type3 , val1, val2, val3\n       a       c        w       3    5     2\n       b       c        t       2    9     0\n       a       c        m       2    1     8\n  ]\n</code></pre>\n<p>What is the best way to do so?</p>\n",
        "formatted_input": {
            "qid": 63259726,
            "link": "https://stackoverflow.com/questions/63259726/pandas-dataframe-delete-groups-with-more-than-n-rows-in-groupby",
            "question": {
                "title": "pandas dataframe delete groups with more than n rows in groupby",
                "ques_desc": "I have a dataframe: I want to apply groupby based on columns type1, type2 and delete from the dataframe the groups with more than 2 rows. So the new dataframe will be: What is the best way to do so? "
            },
            "io": [
                "df = [type1 , type2 , type3 , val1, val2, val3\n       a       b        q       1    2     3\n       a       c        w       3    5     2\n       b       c        t       2    9     0\n       a       b        p       4    6     7\n       a       c        m       2    1     8\n       a       b        h       8    6     3\n       a       b        e       4    2     7]\n",
                "df = [type1 , type2 , type3 , val1, val2, val3\n       a       c        w       3    5     2\n       b       c        t       2    9     0\n       a       c        m       2    1     8\n  ]\n"
            ],
            "answer": {
                "ans_desc": "Use for get counts of groups for with same size like original, so possible filter by for in : If performace is not important or small DataFrame is possible use : ",
                "code": [
                    "df =df.groupby(['type1','type2']).filter(lambda x: len(x) <= 2) \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 685,
            "user_id": 13132728,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cfdf40fa14cd976682f6b9d4ce12ec27?s=128&d=identicon&r=PG&f=1",
            "display_name": "bismo",
            "link": "https://stackoverflow.com/users/13132728/bismo"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 63241206,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1596522479,
        "creation_date": 1596521338,
        "question_id": 63241112,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63241112/how-to-to-fuzzy-merge-items-from-a-list-that-repeat-many-times-python-pandas",
        "title": "How to to fuzzy merge items from a list that repeat many times python pandas",
        "body": "<p>I have a one column df called ```logos''' consisting of the following list:\n(note I have searched for similar questions on stackoverflow to no avail</p>\n<pre><code>logos\n\n['DEN.png',\n 'MIN.png',\n 'LA.png',\n 'NE.png',\n 'ARI.png',\n 'TEN.png']\n</code></pre>\n<p>I would like to merge with the following df that consists of each item, minus the .png filename</p>\n<pre><code>0   ARI\n1   ARI\n2   ARI\n3   DEN\n4   DEN\n5   DEN\n</code></pre>\n<p>I would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df</p>\n<pre><code>0   ARI ARI.png\n1   ARI ARI.png\n2   ARI ARI.png\n3   DEN DEN.png\n4   DEN DEN.png\n5   DEN DEN.png\n</code></pre>\n<p>I am wondering how I should go about this considering the <code>ARI</code> and <code>ARI.png</code> aren't identical, and item in the df I would like to merge with is listed multiple times. Is there such thing as a fuzzy join in python like in R? Thanks in advance for any help.</p>\n",
        "answer_body": "<p>AFIK there is no option for 'fuzzy' merge. You can make a new column in logos with</p>\n<pre><code>logos['no_ext'] = logos.column_name.str.split('.').str.get(0) \n</code></pre>\n<p>and then merge with df</p>\n<pre><code>df = df.merge(logos, how='left', left_on='column_name', right_on='no_ext')\n</code></pre>\n<p><em>Edit</em></p>\n<p>Pay atention to the <code>how</code> parameter in merge. If ommited it will default to inner. Then if you encounter a row in df that has no corresponding filename in logos it will be excluded from the merged result.</p>\n",
        "question_body": "<p>I have a one column df called ```logos''' consisting of the following list:\n(note I have searched for similar questions on stackoverflow to no avail</p>\n<pre><code>logos\n\n['DEN.png',\n 'MIN.png',\n 'LA.png',\n 'NE.png',\n 'ARI.png',\n 'TEN.png']\n</code></pre>\n<p>I would like to merge with the following df that consists of each item, minus the .png filename</p>\n<pre><code>0   ARI\n1   ARI\n2   ARI\n3   DEN\n4   DEN\n5   DEN\n</code></pre>\n<p>I would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df</p>\n<pre><code>0   ARI ARI.png\n1   ARI ARI.png\n2   ARI ARI.png\n3   DEN DEN.png\n4   DEN DEN.png\n5   DEN DEN.png\n</code></pre>\n<p>I am wondering how I should go about this considering the <code>ARI</code> and <code>ARI.png</code> aren't identical, and item in the df I would like to merge with is listed multiple times. Is there such thing as a fuzzy join in python like in R? Thanks in advance for any help.</p>\n",
        "formatted_input": {
            "qid": 63241112,
            "link": "https://stackoverflow.com/questions/63241112/how-to-to-fuzzy-merge-items-from-a-list-that-repeat-many-times-python-pandas",
            "question": {
                "title": "How to to fuzzy merge items from a list that repeat many times python pandas",
                "ques_desc": "I have a one column df called ```logos''' consisting of the following list: (note I have searched for similar questions on stackoverflow to no avail I would like to merge with the following df that consists of each item, minus the .png filename I would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df I am wondering how I should go about this considering the and aren't identical, and item in the df I would like to merge with is listed multiple times. Is there such thing as a fuzzy join in python like in R? Thanks in advance for any help. "
            },
            "io": [
                "0   ARI\n1   ARI\n2   ARI\n3   DEN\n4   DEN\n5   DEN\n",
                "0   ARI ARI.png\n1   ARI ARI.png\n2   ARI ARI.png\n3   DEN DEN.png\n4   DEN DEN.png\n5   DEN DEN.png\n"
            ],
            "answer": {
                "ans_desc": "AFIK there is no option for 'fuzzy' merge. You can make a new column in logos with and then merge with df Edit Pay atention to the parameter in merge. If ommited it will default to inner. Then if you encounter a row in df that has no corresponding filename in logos it will be excluded from the merged result. ",
                "code": [
                    "df = df.merge(logos, how='left', left_on='column_name', right_on='no_ext')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "duplicates"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 14024444,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1001051640353203/picture?type=large",
            "display_name": "azam",
            "link": "https://stackoverflow.com/users/14024444/azam"
        },
        "is_answered": true,
        "view_count": 611,
        "accepted_answer_id": 63182226,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1596209225,
        "creation_date": 1596147154,
        "last_edit_date": 1596209225,
        "question_id": 63182136,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63182136/drop-consecutive-duplicates-in-pandas-dataframe-if-repeated-more-than-n-times",
        "title": "Drop consecutive duplicates in Pandas dataframe if repeated more than n times",
        "body": "<p>Building off the question/solution <a href=\"https://stackoverflow.com/questions/19463985/pandas-drop-consecutive-duplicates\">here</a>, I'm trying to set a parameter that will only remove consecutive duplicates if the same value occurs 5 (or more) times consecutively...</p>\n<p>I'm able to apply the solution in the linked post which uses <code>.shift()</code> to check if the previous (or a specified value in the past or future by adjusting the shift periods parameter) equals the current value, but how could I adjust this to check several consecutive values simultaneously?</p>\n<p>Suppose a dataframe that looks like this:</p>\n<pre><code>x    y\n\n1    2\n2    2\n3    3\n4    3\n5    3\n6    3\n7    3\n8    4\n9    4\n10   4\n11   4\n12   2\n</code></pre>\n<p>I'm trying to achieve this:</p>\n<pre><code>x    y\n\n1    2\n2    2\n3    3\n8    4\n9    4\n10   4\n11   4\n12   2\n</code></pre>\n<p>Where we lose rows 4,5,6,7 because we found five consecutive 3's in the y column. But keep rows 1,2 because it we only find two consecutive 2's in the y column. Similarly, keep rows 8,9,10,11 because we only find four consecutive 4's in the y column.</p>\n",
        "answer_body": "<p>Let's try <code>cumsum</code> on the differences to find the consecutive blocks. Then <code>groupby().transform('size')</code> to get the size of the blocks:</p>\n<pre><code>thresh = 5\ns = df['y'].diff().ne(0).cumsum()\n\nsmall_size = s.groupby(s).transform('size') &lt; thresh\nfirst_rows = ~s.duplicated()\n\ndf[small_size | first_rows]\n</code></pre>\n<p>Output:</p>\n<pre><code>     x  y\n0    1  2\n1    2  2\n2    3  3\n7    8  4\n8    9  4\n9   10  4\n10  11  4\n11  12  2\n</code></pre>\n",
        "question_body": "<p>Building off the question/solution <a href=\"https://stackoverflow.com/questions/19463985/pandas-drop-consecutive-duplicates\">here</a>, I'm trying to set a parameter that will only remove consecutive duplicates if the same value occurs 5 (or more) times consecutively...</p>\n<p>I'm able to apply the solution in the linked post which uses <code>.shift()</code> to check if the previous (or a specified value in the past or future by adjusting the shift periods parameter) equals the current value, but how could I adjust this to check several consecutive values simultaneously?</p>\n<p>Suppose a dataframe that looks like this:</p>\n<pre><code>x    y\n\n1    2\n2    2\n3    3\n4    3\n5    3\n6    3\n7    3\n8    4\n9    4\n10   4\n11   4\n12   2\n</code></pre>\n<p>I'm trying to achieve this:</p>\n<pre><code>x    y\n\n1    2\n2    2\n3    3\n8    4\n9    4\n10   4\n11   4\n12   2\n</code></pre>\n<p>Where we lose rows 4,5,6,7 because we found five consecutive 3's in the y column. But keep rows 1,2 because it we only find two consecutive 2's in the y column. Similarly, keep rows 8,9,10,11 because we only find four consecutive 4's in the y column.</p>\n",
        "formatted_input": {
            "qid": 63182136,
            "link": "https://stackoverflow.com/questions/63182136/drop-consecutive-duplicates-in-pandas-dataframe-if-repeated-more-than-n-times",
            "question": {
                "title": "Drop consecutive duplicates in Pandas dataframe if repeated more than n times",
                "ques_desc": "Building off the question/solution here, I'm trying to set a parameter that will only remove consecutive duplicates if the same value occurs 5 (or more) times consecutively... I'm able to apply the solution in the linked post which uses to check if the previous (or a specified value in the past or future by adjusting the shift periods parameter) equals the current value, but how could I adjust this to check several consecutive values simultaneously? Suppose a dataframe that looks like this: I'm trying to achieve this: Where we lose rows 4,5,6,7 because we found five consecutive 3's in the y column. But keep rows 1,2 because it we only find two consecutive 2's in the y column. Similarly, keep rows 8,9,10,11 because we only find four consecutive 4's in the y column. "
            },
            "io": [
                "x    y\n\n1    2\n2    2\n3    3\n4    3\n5    3\n6    3\n7    3\n8    4\n9    4\n10   4\n11   4\n12   2\n",
                "x    y\n\n1    2\n2    2\n3    3\n8    4\n9    4\n10   4\n11   4\n12   2\n"
            ],
            "answer": {
                "ans_desc": "Let's try on the differences to find the consecutive blocks. Then to get the size of the blocks: Output: ",
                "code": [
                    "thresh = 5\ns = df['y'].diff().ne(0).cumsum()\n\nsmall_size = s.groupby(s).transform('size') < thresh\nfirst_rows = ~s.duplicated()\n\ndf[small_size | first_rows]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 3549,
            "user_id": 392233,
            "user_type": "registered",
            "accept_rate": 78,
            "profile_image": "https://www.gravatar.com/avatar/d1e07f7aad2dd526e9e90eeb71262809?s=128&d=identicon&r=PG&f=1",
            "display_name": "misguided",
            "link": "https://stackoverflow.com/users/392233/misguided"
        },
        "is_answered": true,
        "view_count": 1486,
        "closed_date": 1596158074,
        "accepted_answer_id": 63184419,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1596162486,
        "creation_date": 1596157778,
        "question_id": 63184392,
        "link": "https://stackoverflow.com/questions/63184392/pandas-merge-and-keep-only-non-matching-records",
        "closed_reason": "Duplicate",
        "title": "Pandas merge and keep only non-matching records",
        "body": "<p>How can  I merge/join these two dataframes ONLY on &quot;id&quot;. Produce 3 new dataframes:</p>\n<ul>\n<li>1)R1 = Merged records </li>\n<li>2)R2 = (DF1 - Merged records)</li>\n<li>3)R3 = (DF2 - Merged records)</li>\n</ul>\n<p>Using <strong>pandas</strong> in Python.</p>\n<p>First dataframe (DF1)</p>\n<pre><code>|        id | name  |\n|-----------|-------|\n| 1         | Mark  |\n| 2         | Dart  |\n| 3         | Julia |\n| 4         | Oolia |\n| 5         | Talia |\n</code></pre>\n<p>Second dataframe (DF2)</p>\n<pre><code>|        id | salary |\n|-----------|--------|\n| 1         | 20     |\n| 2         | 30     |\n| 3         | 40     |\n| 4         | 50     |\n| 6         | 33     |\n| 7         | 23     |\n| 8         | 24     |\n| 9         | 28     |\n</code></pre>\n<p>My solution for</p>\n<pre><code>R1 =pd.merge(DF1, DF2, on='id', how='inner')\n</code></pre>\n<p>I am unsure that is the easiest way to get R2 and R3</p>\n<p>R2 should look like</p>\n<pre><code>|        id | name  |\n|-----------|-------|\n| 5         | Talia |\n</code></pre>\n<p>R3 should look like:</p>\n<pre><code>|        id | salary |\n|-----------|--------|\n| 6         | 33     |\n| 7         | 23     |\n| 8         | 24     |\n| 9         | 28     |\n</code></pre>\n",
        "answer_body": "<p>You can turn on <code>indicator</code> in <code>merge</code> and look for the corresponding values:</p>\n<pre><code>total_merge = df1.merge(df2, on='id', how='outer', indicator=True)\n\nR1 = total_merge[total_merge['_merge']=='both']\nR2 = total_merge[total_merge['_merge']=='left_only']\nR3 = total_merge[total_merge['_merge']=='right_only']\n</code></pre>\n<hr />\n<p><strong>Update</strong>: Ben's suggestion would be something like this:</p>\n<pre><code>dfs = {k:v for k,v in total_merge.groupby('_merge')}\n</code></pre>\n<p>and then you can do, for examples:</p>\n<pre><code>dfs['both']\n</code></pre>\n",
        "question_body": "<p>How can  I merge/join these two dataframes ONLY on &quot;id&quot;. Produce 3 new dataframes:</p>\n<ul>\n<li>1)R1 = Merged records </li>\n<li>2)R2 = (DF1 - Merged records)</li>\n<li>3)R3 = (DF2 - Merged records)</li>\n</ul>\n<p>Using <strong>pandas</strong> in Python.</p>\n<p>First dataframe (DF1)</p>\n<pre><code>|        id | name  |\n|-----------|-------|\n| 1         | Mark  |\n| 2         | Dart  |\n| 3         | Julia |\n| 4         | Oolia |\n| 5         | Talia |\n</code></pre>\n<p>Second dataframe (DF2)</p>\n<pre><code>|        id | salary |\n|-----------|--------|\n| 1         | 20     |\n| 2         | 30     |\n| 3         | 40     |\n| 4         | 50     |\n| 6         | 33     |\n| 7         | 23     |\n| 8         | 24     |\n| 9         | 28     |\n</code></pre>\n<p>My solution for</p>\n<pre><code>R1 =pd.merge(DF1, DF2, on='id', how='inner')\n</code></pre>\n<p>I am unsure that is the easiest way to get R2 and R3</p>\n<p>R2 should look like</p>\n<pre><code>|        id | name  |\n|-----------|-------|\n| 5         | Talia |\n</code></pre>\n<p>R3 should look like:</p>\n<pre><code>|        id | salary |\n|-----------|--------|\n| 6         | 33     |\n| 7         | 23     |\n| 8         | 24     |\n| 9         | 28     |\n</code></pre>\n",
        "formatted_input": {
            "qid": 63184392,
            "link": "https://stackoverflow.com/questions/63184392/pandas-merge-and-keep-only-non-matching-records",
            "question": {
                "title": "Pandas merge and keep only non-matching records",
                "ques_desc": "How can I merge/join these two dataframes ONLY on \"id\". Produce 3 new dataframes: 1)R1 = Merged records 2)R2 = (DF1 - Merged records) 3)R3 = (DF2 - Merged records) Using pandas in Python. First dataframe (DF1) Second dataframe (DF2) My solution for I am unsure that is the easiest way to get R2 and R3 R2 should look like R3 should look like: "
            },
            "io": [
                "|        id | salary |\n|-----------|--------|\n| 1         | 20     |\n| 2         | 30     |\n| 3         | 40     |\n| 4         | 50     |\n| 6         | 33     |\n| 7         | 23     |\n| 8         | 24     |\n| 9         | 28     |\n",
                "|        id | salary |\n|-----------|--------|\n| 6         | 33     |\n| 7         | 23     |\n| 8         | 24     |\n| 9         | 28     |\n"
            ],
            "answer": {
                "ans_desc": "You can turn on in and look for the corresponding values: Update: Ben's suggestion would be something like this: and then you can do, for examples: ",
                "code": [
                    "total_merge = df1.merge(df2, on='id', how='outer', indicator=True)\n\nR1 = total_merge[total_merge['_merge']=='both']\nR2 = total_merge[total_merge['_merge']=='left_only']\nR3 = total_merge[total_merge['_merge']=='right_only']\n",
                    "dfs = {k:v for k,v in total_merge.groupby('_merge')}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan",
            "series"
        ],
        "owner": {
            "reputation": 35,
            "user_id": 14004878,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ee39ee50c14b3b9bca13776790538006?s=128&d=identicon&r=PG&f=1",
            "display_name": "Tessd",
            "link": "https://stackoverflow.com/users/14004878/tessd"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 63174553,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1596128816,
        "creation_date": 1596113999,
        "last_edit_date": 1596117018,
        "question_id": 63173736,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63173736/justify-data-from-right-to-left",
        "title": "justify data from right to left",
        "body": "<p>I have a dataset of rows containing varying lengths of integer values in a series. I want to separate the series so each integer has its own column but align these values along the right-most column. I want the dataframe to resenble upper triangle of a matrix.</p>\n<p>Currently I have a dataset like:</p>\n<pre><code>    variable    value\n0   0   [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n1   1   [1, 2, 3, 4, 5, 6, 7, 8, 9]\n2   2   [1, 2, 3, 4, 5, 6, 7, 8]\n3   3   [1, 2, 3, 4, 5, 6, 7]\n4   4   [1, 2, 3, 4, 5, 6]\n5   5   [1, 2, 3, 4, 5]\n6   6   [1, 2, 3, 4]\n7   7   [1, 2, 3]\n8   8   [1, 2]\n9   9   [1]\n</code></pre>\n<p>I apply this function</p>\n<pre><code>df = pd.DataFrame([pd.Series(x) for x in df2.value])\ndf.columns = ['{}'.format(x+1) for x in df.columns]\n</code></pre>\n<p>and I get this:</p>\n<pre><code>    1   2   3   4   5   6   7   8   9   10\n0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0\n1   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 NaN\n2   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 NaN NaN\n3   1.0 2.0 3.0 4.0 5.0 6.0 7.0 NaN NaN NaN\n4   1.0 2.0 3.0 4.0 5.0 6.0 NaN NaN NaN NaN\n5   1.0 2.0 3.0 4.0 5.0 NaN NaN NaN NaN NaN\n6   1.0 2.0 3.0 4.0 NaN NaN NaN NaN NaN NaN\n7   1.0 2.0 3.0 NaN NaN NaN NaN NaN NaN NaN\n8   1.0 2.0 NaN NaN NaN NaN NaN NaN NaN NaN\n9   1.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN\n</code></pre>\n<p>But what i want is this:</p>\n<pre><code>    1   2   3   4   5   6   7   8   9   10\n0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0\n1   NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 \n2   NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 \n3   NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 \n4   NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 \n5   NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 \n6   NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0\n7   NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 \n8   NaN NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 \n9   NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 \n</code></pre>\n",
        "answer_body": "<p>One possible approach is to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.len.html\" rel=\"nofollow noreferrer\"><code>Series.str.len</code></a> to calculate the <code>max</code> length of the list in the column <code>value</code> i.e <code>lmax</code> then using list comprehension <code>pad</code> each of the list based on <code>lmax</code>:</p>\n<pre><code>lmax = df['value'].str.len().max()\ndf1 = pd.DataFrame([[np.nan] * (lmax - len(s)) + s\n                    for s in df['value']], columns=range(1, lmax + 1))\n</code></pre>\n<p>Result:</p>\n<pre><code>print(df1)\n     1    2    3    4    5    6    7    8    9  10\n0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0   0\n1  NaN  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0   9\n2  NaN  NaN  1.0  2.0  3.0  4.0  5.0  6.0  7.0   8\n3  NaN  NaN  NaN  1.0  2.0  3.0  4.0  5.0  6.0   7\n4  NaN  NaN  NaN  NaN  1.0  2.0  3.0  4.0  5.0   6\n5  NaN  NaN  NaN  NaN  NaN  1.0  2.0  3.0  4.0   5\n6  NaN  NaN  NaN  NaN  NaN  NaN  1.0  2.0  3.0   4\n7  NaN  NaN  NaN  NaN  NaN  NaN  NaN  1.0  2.0   3\n8  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  1.0   2\n9  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   1\n</code></pre>\n",
        "question_body": "<p>I have a dataset of rows containing varying lengths of integer values in a series. I want to separate the series so each integer has its own column but align these values along the right-most column. I want the dataframe to resenble upper triangle of a matrix.</p>\n<p>Currently I have a dataset like:</p>\n<pre><code>    variable    value\n0   0   [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n1   1   [1, 2, 3, 4, 5, 6, 7, 8, 9]\n2   2   [1, 2, 3, 4, 5, 6, 7, 8]\n3   3   [1, 2, 3, 4, 5, 6, 7]\n4   4   [1, 2, 3, 4, 5, 6]\n5   5   [1, 2, 3, 4, 5]\n6   6   [1, 2, 3, 4]\n7   7   [1, 2, 3]\n8   8   [1, 2]\n9   9   [1]\n</code></pre>\n<p>I apply this function</p>\n<pre><code>df = pd.DataFrame([pd.Series(x) for x in df2.value])\ndf.columns = ['{}'.format(x+1) for x in df.columns]\n</code></pre>\n<p>and I get this:</p>\n<pre><code>    1   2   3   4   5   6   7   8   9   10\n0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0\n1   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 NaN\n2   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 NaN NaN\n3   1.0 2.0 3.0 4.0 5.0 6.0 7.0 NaN NaN NaN\n4   1.0 2.0 3.0 4.0 5.0 6.0 NaN NaN NaN NaN\n5   1.0 2.0 3.0 4.0 5.0 NaN NaN NaN NaN NaN\n6   1.0 2.0 3.0 4.0 NaN NaN NaN NaN NaN NaN\n7   1.0 2.0 3.0 NaN NaN NaN NaN NaN NaN NaN\n8   1.0 2.0 NaN NaN NaN NaN NaN NaN NaN NaN\n9   1.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN\n</code></pre>\n<p>But what i want is this:</p>\n<pre><code>    1   2   3   4   5   6   7   8   9   10\n0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0\n1   NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 \n2   NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 \n3   NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 \n4   NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 \n5   NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 \n6   NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0\n7   NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 \n8   NaN NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 \n9   NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 \n</code></pre>\n",
        "formatted_input": {
            "qid": 63173736,
            "link": "https://stackoverflow.com/questions/63173736/justify-data-from-right-to-left",
            "question": {
                "title": "justify data from right to left",
                "ques_desc": "I have a dataset of rows containing varying lengths of integer values in a series. I want to separate the series so each integer has its own column but align these values along the right-most column. I want the dataframe to resenble upper triangle of a matrix. Currently I have a dataset like: I apply this function and I get this: But what i want is this: "
            },
            "io": [
                "    1   2   3   4   5   6   7   8   9   10\n0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0\n1   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 NaN\n2   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 NaN NaN\n3   1.0 2.0 3.0 4.0 5.0 6.0 7.0 NaN NaN NaN\n4   1.0 2.0 3.0 4.0 5.0 6.0 NaN NaN NaN NaN\n5   1.0 2.0 3.0 4.0 5.0 NaN NaN NaN NaN NaN\n6   1.0 2.0 3.0 4.0 NaN NaN NaN NaN NaN NaN\n7   1.0 2.0 3.0 NaN NaN NaN NaN NaN NaN NaN\n8   1.0 2.0 NaN NaN NaN NaN NaN NaN NaN NaN\n9   1.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
                "    1   2   3   4   5   6   7   8   9   10\n0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0\n1   NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 \n2   NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 \n3   NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 \n4   NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 \n5   NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 \n6   NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0\n7   NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 \n8   NaN NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 \n9   NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 \n"
            ],
            "answer": {
                "ans_desc": "One possible approach is to use to calculate the length of the list in the column i.e then using list comprehension each of the list based on : Result: ",
                "code": [
                    "lmax = df['value'].str.len().max()\ndf1 = pd.DataFrame([[np.nan] * (lmax - len(s)) + s\n                    for s in df['value']], columns=range(1, lmax + 1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "jupyter-notebook"
        ],
        "owner": {
            "reputation": 2093,
            "user_id": 1543579,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://i.stack.imgur.com/7RQCa.jpg?s=128&g=1",
            "display_name": "shirakia",
            "link": "https://stackoverflow.com/users/1543579/shirakia"
        },
        "is_answered": true,
        "view_count": 174,
        "accepted_answer_id": 63130158,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1595925489,
        "creation_date": 1591708607,
        "last_edit_date": 1591709756,
        "question_id": 62283545,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62283545/way-to-show-multiple-spaces-in-pandas-dataframe-on-jupyter-notebook",
        "title": "Way to show multiple spaces in Pandas Dataframe on Jupyter Notebook",
        "body": "<p>When displaying Pandas Dataframe object on notebook, multiple spaces are shown as single space. And I cannot copy multiple spaces. </p>\n\n<p>I would like to show all spaces as they are and copy a string of them.</p>\n\n<p>Any way to do so?</p>\n\n<pre><code>display(pd.DataFrame([[\"ab c\", \"ab  c\"], [\"ab   c\", \"ab    c\"]]))\n</code></pre>\n\n<p>Actual</p>\n\n<pre><code>    0     1   \n0   ab c  ab c\n1   ab c  ab c\n</code></pre>\n\n<p>My expectation</p>\n\n<pre><code>    0       1     \n0   ab c    ab  c\n1   ab   c  ab    c\n</code></pre>\n",
        "answer_body": "<p>This issue has been answered in a different context:</p>\n<p><a href=\"https://stackoverflow.com/questions/58562191/keep-the-extra-whitespaces-in-display-of-pandas-dataframe-in-jupyter-notebook\">keep the extra whitespaces in display of pandas dataframe in jupyter notebook</a></p>\n<pre><code>def print_df(df):\n    return df.style.set_properties(**{'white-space': 'pre'})\n\nprint_df(df)\n</code></pre>\n",
        "question_body": "<p>When displaying Pandas Dataframe object on notebook, multiple spaces are shown as single space. And I cannot copy multiple spaces. </p>\n\n<p>I would like to show all spaces as they are and copy a string of them.</p>\n\n<p>Any way to do so?</p>\n\n<pre><code>display(pd.DataFrame([[\"ab c\", \"ab  c\"], [\"ab   c\", \"ab    c\"]]))\n</code></pre>\n\n<p>Actual</p>\n\n<pre><code>    0     1   \n0   ab c  ab c\n1   ab c  ab c\n</code></pre>\n\n<p>My expectation</p>\n\n<pre><code>    0       1     \n0   ab c    ab  c\n1   ab   c  ab    c\n</code></pre>\n",
        "formatted_input": {
            "qid": 62283545,
            "link": "https://stackoverflow.com/questions/62283545/way-to-show-multiple-spaces-in-pandas-dataframe-on-jupyter-notebook",
            "question": {
                "title": "Way to show multiple spaces in Pandas Dataframe on Jupyter Notebook",
                "ques_desc": "When displaying Pandas Dataframe object on notebook, multiple spaces are shown as single space. And I cannot copy multiple spaces. I would like to show all spaces as they are and copy a string of them. Any way to do so? Actual My expectation "
            },
            "io": [
                "    0     1   \n0   ab c  ab c\n1   ab c  ab c\n",
                "    0       1     \n0   ab c    ab  c\n1   ab   c  ab    c\n"
            ],
            "answer": {
                "ans_desc": "This issue has been answered in a different context: keep the extra whitespaces in display of pandas dataframe in jupyter notebook ",
                "code": [
                    "def print_df(df):\n    return df.style.set_properties(**{'white-space': 'pre'})\n\nprint_df(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 13994610,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-euBG_urLU6I/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclYYMSSOpI0XbXa_wnZgM6fia-01w/photo.jpg?sz=128",
            "display_name": "MatG92",
            "link": "https://stackoverflow.com/users/13994610/matg92"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 63091755,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1595702052,
        "creation_date": 1595699214,
        "last_edit_date": 1595702052,
        "question_id": 63091593,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63091593/easy-way-to-find-find-itinerant-max-value-in-grouped-pandas-list",
        "title": "Easy way to find find itinerant max value in grouped pandas list",
        "body": "<p>I have a dataset where I have multiple value entries per year and some properties per entry. I want to find the maximum value per year and return that as a new data frame (to keep the other properties in the data frame), but only if the value in a year is greater than what it was in the years before (something like the &quot;All-time record value per year&quot;). So far I can find the max value per year, e.g.</p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame(data=[[2015, 40, 'Property A'],\n                        [2012, 35, 'Property B'],\n                        [2014, 37, 'Property C'], \n                        [2013, 43, 'Property D'], \n                        [2013, 40, 'Property E'], \n                        [2015, 41, 'Property F']], \n                  columns=['Year', 'Value', 'Property'])\ndf_sorted_max = df.loc[df.groupby('Year')['Value'].idxmax()].reset_index(drop=True)\nprint(df_sorted_max)\n</code></pre>\n<p>where the output then is</p>\n<pre><code>   Year  Value    Property\n0  2012     35  Property B\n1  2013     43  Property D\n2  2014     37  Property C\n3  2015     41  Property F\n</code></pre>\n<p>This is almost what I want, expect for 2014 where I would like the value of 2013 with its according properties to go (since the value was greater in 2013 than it was in 2014). So the desired outcome would be</p>\n<pre><code>   Year  Value    Property\n0  2012     35  Property B\n1  2013     43  Property D\n2  2014     43  Property D\n3  2015     43  Property D\n</code></pre>\n<p>Is there a good way to achieve this with pandas?</p>\n",
        "answer_body": "<pre><code>df_sorted_max['cummax_value'] = df_sorted_max.Value.cummax()\n\nprint(\n    df_sorted_max.merge(\n        df_sorted_max.groupby('cummax_value')[['Property']].first(),\n        on=&quot;cummax_value&quot;\n    )\n)\n</code></pre>\n<p>Results in</p>\n<pre><code>   Year  Value  Property_x  cummax_value  Property_y\n0  2012     35  Property B            35  Property B\n1  2013     43  Property D            43  Property D\n2  2014     37  Property C            43  Property D\n3  2015     60  Property A            60  Property A\n</code></pre>\n<p>After which you can just drop the original columns.</p>\n<p>The secret sauce here is the <code>cummax</code> function.</p>\n",
        "question_body": "<p>I have a dataset where I have multiple value entries per year and some properties per entry. I want to find the maximum value per year and return that as a new data frame (to keep the other properties in the data frame), but only if the value in a year is greater than what it was in the years before (something like the &quot;All-time record value per year&quot;). So far I can find the max value per year, e.g.</p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame(data=[[2015, 40, 'Property A'],\n                        [2012, 35, 'Property B'],\n                        [2014, 37, 'Property C'], \n                        [2013, 43, 'Property D'], \n                        [2013, 40, 'Property E'], \n                        [2015, 41, 'Property F']], \n                  columns=['Year', 'Value', 'Property'])\ndf_sorted_max = df.loc[df.groupby('Year')['Value'].idxmax()].reset_index(drop=True)\nprint(df_sorted_max)\n</code></pre>\n<p>where the output then is</p>\n<pre><code>   Year  Value    Property\n0  2012     35  Property B\n1  2013     43  Property D\n2  2014     37  Property C\n3  2015     41  Property F\n</code></pre>\n<p>This is almost what I want, expect for 2014 where I would like the value of 2013 with its according properties to go (since the value was greater in 2013 than it was in 2014). So the desired outcome would be</p>\n<pre><code>   Year  Value    Property\n0  2012     35  Property B\n1  2013     43  Property D\n2  2014     43  Property D\n3  2015     43  Property D\n</code></pre>\n<p>Is there a good way to achieve this with pandas?</p>\n",
        "formatted_input": {
            "qid": 63091593,
            "link": "https://stackoverflow.com/questions/63091593/easy-way-to-find-find-itinerant-max-value-in-grouped-pandas-list",
            "question": {
                "title": "Easy way to find find itinerant max value in grouped pandas list",
                "ques_desc": "I have a dataset where I have multiple value entries per year and some properties per entry. I want to find the maximum value per year and return that as a new data frame (to keep the other properties in the data frame), but only if the value in a year is greater than what it was in the years before (something like the \"All-time record value per year\"). So far I can find the max value per year, e.g. where the output then is This is almost what I want, expect for 2014 where I would like the value of 2013 with its according properties to go (since the value was greater in 2013 than it was in 2014). So the desired outcome would be Is there a good way to achieve this with pandas? "
            },
            "io": [
                "   Year  Value    Property\n0  2012     35  Property B\n1  2013     43  Property D\n2  2014     37  Property C\n3  2015     41  Property F\n",
                "   Year  Value    Property\n0  2012     35  Property B\n1  2013     43  Property D\n2  2014     43  Property D\n3  2015     43  Property D\n"
            ],
            "answer": {
                "ans_desc": " Results in After which you can just drop the original columns. The secret sauce here is the function. ",
                "code": [
                    "df_sorted_max['cummax_value'] = df_sorted_max.Value.cummax()\n\nprint(\n    df_sorted_max.merge(\n        df_sorted_max.groupby('cummax_value')[['Property']].first(),\n        on=\"cummax_value\"\n    )\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "timestamp",
            "time-series"
        ],
        "owner": {
            "reputation": 205,
            "user_id": 11863394,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8328cf9758733b5fee02a11cdb32ad02?s=128&d=identicon&r=PG&f=1",
            "display_name": "Gee",
            "link": "https://stackoverflow.com/users/11863394/gee"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 63091187,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1595697611,
        "creation_date": 1595694942,
        "question_id": 63090827,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63090827/dropping-dataframe-rows-in-time-series-dataframe-using-pandas",
        "title": "Dropping dataframe rows in time series dataframe using pandas",
        "body": "<p>I have the below sequence of data as a pandas dataframe</p>\n<pre><code>id,start,end,duration\n303,2012-06-25 17:59:43,2012-06-25 18:01:29,105\n404,2012-06-25 18:01:29,2012-06-25 18:01:55,25\n303,2012-06-25 18:01:56,2012-06-25 18:02:06,10\n303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n404,2012-06-25 18:02:45,2012-06-25 18:02:51,6\n303,2012-06-25 18:02:54,2012-06-25 18:03:17,23\n404,2012-06-25 18:03:24,2012-06-25 18:03:41,17\n303,2012-06-25 18:03:43,2012-06-25 18:05:51,128\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n404,2012-06-25 18:24:24,2012-06-25 18:25:25,61\n101,2012-06-25 18:25:25,2012-06-25 18:25:462,21\n404,2012-06-25 18:25:49,2012-06-25 18:26:00,11\n101,2012-06-25 18:26:01,2012-06-25 18:26:04,3\n404,2012-06-25 18:26:05,2012-06-25 18:28:49,164\n202,2012-06-25 18:28:52,2012-06-25 18:28:57,5\n404,2012-06-25 18:29:00,2012-06-25 18:29:24,24\n</code></pre>\n<p>It should always be the case that id 404 gets repeated after another different id.</p>\n<p>For example if the above is motion sensors in a house e.g. <strong>404</strong>:hallway, <strong>202</strong>:bedroom, <strong>303</strong>:kitchen, <strong>201</strong>:studyroom, where the hallway is in the middle, then moving from bedroom to kitchen to studyroom and back to bedroom should trigger <strong>202</strong>, <strong>404</strong>, <strong>303</strong>, <strong>404</strong>, <strong>201</strong>, <strong>404</strong>, <strong>202</strong> in that order because one always passes through the hallway (404) to any room. My output has cases that violate this sequence and I want to drop such rows.</p>\n<p>For example from the snippet dataframe above the below rows violate this:</p>\n<pre><code>303,2012-06-25 18:01:56,2012-06-25 18:02:06,10\n303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n\n303,2012-06-25 18:03:43,2012-06-25 18:05:51,128\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n</code></pre>\n<p>and therefore the rows below should be droped (but of course I have a much larger dataset).</p>\n<pre><code>303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n</code></pre>\n<p>I have tried shift and drop but the result still has some inconsistencies.</p>\n<pre><code>df['id_ns'] = df['id'].shift(-1)\ndf['id_ps'] = df['id'].shift(1)\n\nif (df['id'] != 404):\n    df.drop(df[(df.id_ns != 404) &amp; (df.id_ps != 404)].index, axis=0, inplace=True)\n</code></pre>\n<p>How best can I approach this?</p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ne.html\" rel=\"nofollow noreferrer\"><code>Series.ne</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html\" rel=\"nofollow noreferrer\"><code>Series.shift</code></a> along with optional parameter <code>fill_value</code> to create a boolean <code>mask</code>, use this mask to filter/drop the rows:</p>\n<pre><code>mask = df['id'].ne(404) &amp; df['id'].shift(fill_value=404).ne(404)\ndf = df[~mask]\n</code></pre>\n<p>Result:</p>\n<pre><code>print(df)\n     id                start                  end  duration\n0   303  2012-06-25 17:59:43  2012-06-25 18:01:29       105\n1   404  2012-06-25 18:01:29  2012-06-25 18:01:55        25\n2   303  2012-06-25 18:01:56  2012-06-25 18:02:06        10\n4   404  2012-06-25 18:02:45  2012-06-25 18:02:51         6\n5   303  2012-06-25 18:02:54  2012-06-25 18:03:17        23\n6   404  2012-06-25 18:03:24  2012-06-25 18:03:41        17\n7   303  2012-06-25 18:03:43  2012-06-25 18:05:51       128\n9   404  2012-06-25 18:24:24  2012-06-25 18:25:25        61\n10  101  2012-06-25 18:25:25  2012-06-25 18:25:46        21\n11  404  2012-06-25 18:25:49  2012-06-25 18:26:00        11\n12  101  2012-06-25 18:26:01  2012-06-25 18:26:04         3\n13  404  2012-06-25 18:26:05  2012-06-25 18:28:49       164\n14  202  2012-06-25 18:28:52  2012-06-25 18:28:57         5\n15  404  2012-06-25 18:29:00  2012-06-25 18:29:24        24\n</code></pre>\n",
        "question_body": "<p>I have the below sequence of data as a pandas dataframe</p>\n<pre><code>id,start,end,duration\n303,2012-06-25 17:59:43,2012-06-25 18:01:29,105\n404,2012-06-25 18:01:29,2012-06-25 18:01:55,25\n303,2012-06-25 18:01:56,2012-06-25 18:02:06,10\n303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n404,2012-06-25 18:02:45,2012-06-25 18:02:51,6\n303,2012-06-25 18:02:54,2012-06-25 18:03:17,23\n404,2012-06-25 18:03:24,2012-06-25 18:03:41,17\n303,2012-06-25 18:03:43,2012-06-25 18:05:51,128\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n404,2012-06-25 18:24:24,2012-06-25 18:25:25,61\n101,2012-06-25 18:25:25,2012-06-25 18:25:462,21\n404,2012-06-25 18:25:49,2012-06-25 18:26:00,11\n101,2012-06-25 18:26:01,2012-06-25 18:26:04,3\n404,2012-06-25 18:26:05,2012-06-25 18:28:49,164\n202,2012-06-25 18:28:52,2012-06-25 18:28:57,5\n404,2012-06-25 18:29:00,2012-06-25 18:29:24,24\n</code></pre>\n<p>It should always be the case that id 404 gets repeated after another different id.</p>\n<p>For example if the above is motion sensors in a house e.g. <strong>404</strong>:hallway, <strong>202</strong>:bedroom, <strong>303</strong>:kitchen, <strong>201</strong>:studyroom, where the hallway is in the middle, then moving from bedroom to kitchen to studyroom and back to bedroom should trigger <strong>202</strong>, <strong>404</strong>, <strong>303</strong>, <strong>404</strong>, <strong>201</strong>, <strong>404</strong>, <strong>202</strong> in that order because one always passes through the hallway (404) to any room. My output has cases that violate this sequence and I want to drop such rows.</p>\n<p>For example from the snippet dataframe above the below rows violate this:</p>\n<pre><code>303,2012-06-25 18:01:56,2012-06-25 18:02:06,10\n303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n\n303,2012-06-25 18:03:43,2012-06-25 18:05:51,128\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n</code></pre>\n<p>and therefore the rows below should be droped (but of course I have a much larger dataset).</p>\n<pre><code>303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n</code></pre>\n<p>I have tried shift and drop but the result still has some inconsistencies.</p>\n<pre><code>df['id_ns'] = df['id'].shift(-1)\ndf['id_ps'] = df['id'].shift(1)\n\nif (df['id'] != 404):\n    df.drop(df[(df.id_ns != 404) &amp; (df.id_ps != 404)].index, axis=0, inplace=True)\n</code></pre>\n<p>How best can I approach this?</p>\n",
        "formatted_input": {
            "qid": 63090827,
            "link": "https://stackoverflow.com/questions/63090827/dropping-dataframe-rows-in-time-series-dataframe-using-pandas",
            "question": {
                "title": "Dropping dataframe rows in time series dataframe using pandas",
                "ques_desc": "I have the below sequence of data as a pandas dataframe It should always be the case that id 404 gets repeated after another different id. For example if the above is motion sensors in a house e.g. 404:hallway, 202:bedroom, 303:kitchen, 201:studyroom, where the hallway is in the middle, then moving from bedroom to kitchen to studyroom and back to bedroom should trigger 202, 404, 303, 404, 201, 404, 202 in that order because one always passes through the hallway (404) to any room. My output has cases that violate this sequence and I want to drop such rows. For example from the snippet dataframe above the below rows violate this: and therefore the rows below should be droped (but of course I have a much larger dataset). I have tried shift and drop but the result still has some inconsistencies. How best can I approach this? "
            },
            "io": [
                "303,2012-06-25 18:01:56,2012-06-25 18:02:06,10\n303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n\n303,2012-06-25 18:03:43,2012-06-25 18:05:51,128\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n",
                "303,2012-06-25 18:02:23,2012-06-25 18:02:44,21\n101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104\n"
            ],
            "answer": {
                "ans_desc": "Use + along with optional parameter to create a boolean , use this mask to filter/drop the rows: Result: ",
                "code": [
                    "mask = df['id'].ne(404) & df['id'].shift(fill_value=404).ne(404)\ndf = df[~mask]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "normalize"
        ],
        "owner": {
            "reputation": 505,
            "user_id": 6389099,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/621d0cd73aadad362cd12870cbdee413?s=128&d=identicon&r=PG&f=1",
            "display_name": "RSM",
            "link": "https://stackoverflow.com/users/6389099/rsm"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 63059400,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1595530836,
        "creation_date": 1595522963,
        "question_id": 63059265,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63059265/filter-dataframe-rows-which-contribute-to-x-of-values-in-one-column",
        "title": "Filter dataframe rows which contribute to X% of values in one column",
        "body": "<p>I have a dataframe:</p>\n<pre><code>df\nCol1   Col2   Col3\nA      B      5\nC      D      4\nE      F      1\n</code></pre>\n<p>I want to see only those rows which contribute to 90% of Col3. In this case the expected output will be :</p>\n<pre><code>Col1   Col2   Col3\nA      B      5\nC      D      4\n</code></pre>\n<p>I tried the below but is doesnt work as expected:</p>\n<pre><code>df['col3'].value_counts(normalize=True) * 100\n</code></pre>\n<p>Is there any solution for the same?</p>\n",
        "answer_body": "<p>Are you looking for this?</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df[df.Col3 &gt; 0] # optionally remove 0 valued rows\ndf = df.sort_values(by='Col3', ascending=False).reset_index(drop=True)\ntotals = df.Col3.cumsum()\ncutoff = totals[totals &gt;= df.Col3.sum() * .7].idxmin()\nprint(df[:cutoff + 1])\n</code></pre>\n<p>Output</p>\n<pre><code>  Col1 Col2  Col3\n0    A    B     5\n1    C    D     4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n<pre><code>df\nCol1   Col2   Col3\nA      B      5\nC      D      4\nE      F      1\n</code></pre>\n<p>I want to see only those rows which contribute to 90% of Col3. In this case the expected output will be :</p>\n<pre><code>Col1   Col2   Col3\nA      B      5\nC      D      4\n</code></pre>\n<p>I tried the below but is doesnt work as expected:</p>\n<pre><code>df['col3'].value_counts(normalize=True) * 100\n</code></pre>\n<p>Is there any solution for the same?</p>\n",
        "formatted_input": {
            "qid": 63059265,
            "link": "https://stackoverflow.com/questions/63059265/filter-dataframe-rows-which-contribute-to-x-of-values-in-one-column",
            "question": {
                "title": "Filter dataframe rows which contribute to X% of values in one column",
                "ques_desc": "I have a dataframe: I want to see only those rows which contribute to 90% of Col3. In this case the expected output will be : I tried the below but is doesnt work as expected: Is there any solution for the same? "
            },
            "io": [
                "df\nCol1   Col2   Col3\nA      B      5\nC      D      4\nE      F      1\n",
                "Col1   Col2   Col3\nA      B      5\nC      D      4\n"
            ],
            "answer": {
                "ans_desc": "Are you looking for this? Output ",
                "code": [
                    "df = df[df.Col3 > 0] # optionally remove 0 valued rows\ndf = df.sort_values(by='Col3', ascending=False).reset_index(drop=True)\ntotals = df.Col3.cumsum()\ncutoff = totals[totals >= df.Col3.sum() * .7].idxmin()\nprint(df[:cutoff + 1])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 47,
            "user_id": 13745609,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1aa5f61574f4ac1c8418c4459338628d?s=128&d=identicon&r=PG&f=1",
            "display_name": "ephemeralhappiness",
            "link": "https://stackoverflow.com/users/13745609/ephemeralhappiness"
        },
        "is_answered": true,
        "view_count": 139,
        "accepted_answer_id": 62990117,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1595315528,
        "creation_date": 1595224498,
        "last_edit_date": 1595224677,
        "question_id": 62989401,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62989401/how-to-find-first-occurrence-of-a-significant-difference-in-values-of-a-pandas-d",
        "title": "How to find first occurrence of a significant difference in values of a pandas dataframe?",
        "body": "<p>In a <strong>Pandas DataFrame</strong>, how would I find the first occurrence of a large difference between two values at two adjacent indices?</p>\n<p>As an example, if I have a <strong>DataFrame</strong> column <em>A</em> with data <code>[1, 1.1, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]</code>, I would want index holding 1.5, which would be 5. In my code below, it would give me the index holding 7.2, because <code>15 - 7.2 &gt; 7 - 1.5</code>.</p>\n<pre><code>idx = df['A'].diff().idxmax() - 1\n</code></pre>\n<p>How should I fix this problem, so I get the index of the first 'large difference' occurrence?</p>\n",
        "answer_body": "<p>The main issue is of course how you define a &quot;large difference&quot;. Your solution is pretty good to get the largest difference, improved only by using <code>.diff(-1)</code> and using absolute values as shown by Jezrael:</p>\n<pre class=\"lang-py prettyprint-override\"><code>differences = df['A'].diff(-1).abs()\n</code></pre>\n<p>Using absolute values matters if your values are not sorted, in which case you can get negative differences.</p>\n<p>Then, you should probably do some clustering on these values and get the smallest index of the cluster with largest values. Jezrael already showed a heuristic by using the largest quartile, however by only slightly modifying your example this doesn\u2019t work:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame({'A': [1, 1.05, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]})\ndifferences = df['A'].diff(-1).abs()\nidx = differences.index[differences &gt;= differences.quantile(.75)][0]\nprint(idx, differences[idx])\n</code></pre>\n<p>This returns <code>1 0.1499999999999999</code></p>\n<p>Here's 3 other heuristics that might work better for you:</p>\n<ul>\n<li><p>If you have a value above which you consider a difference to be \u201clarge\u201d (e.g. <code>1.5</code>):</p>\n<pre class=\"lang-py prettyprint-override\"><code>idx = differences.index[differences &gt;= 1.5][0]\n</code></pre>\n</li>\n<li><p>If you know how many large values there are, you can select those and get the smallest index (e.g. <code>2</code>):</p>\n<pre class=\"lang-py prettyprint-override\"><code>idx = differences.nlargest(2).index.min()\n</code></pre>\n</li>\n<li><p>If you know all small values are grouped together (as are all the 0.1 in your example), you can filter what's larger than the mean (or the mean + 1 standard deviation if your \u201clarge\u201d values are very close to the smaller ones).</p>\n<pre class=\"lang-py prettyprint-override\"><code>idx = differences.index[differences &gt;= differences.mean()][0]\n</code></pre>\n<p>This is because contrarily to the median, your few large differences will pull the mean up significantly.</p>\n</li>\n</ul>\n<p>If you really want to go for proper clustering, you can use the KMeans algorithm from scikit learn:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2).fit(differences.values[:-1].reshape(-1, 1))\nclusters = pd.Series(kmeans.labels_, index=differences.index[:-1])\nidx = clusters.index[clusters.eq(np.squeeze(kmeans.cluster_centers_).argmax())][0]\n</code></pre>\n<p>This classifies the data into 2 classes, and then gets the classification into a pandas Series. We then filter this series\u2019 index by selecting only the cluster that has the highest values, and finally get the first element of this filtered index.</p>\n",
        "question_body": "<p>In a <strong>Pandas DataFrame</strong>, how would I find the first occurrence of a large difference between two values at two adjacent indices?</p>\n<p>As an example, if I have a <strong>DataFrame</strong> column <em>A</em> with data <code>[1, 1.1, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]</code>, I would want index holding 1.5, which would be 5. In my code below, it would give me the index holding 7.2, because <code>15 - 7.2 &gt; 7 - 1.5</code>.</p>\n<pre><code>idx = df['A'].diff().idxmax() - 1\n</code></pre>\n<p>How should I fix this problem, so I get the index of the first 'large difference' occurrence?</p>\n",
        "formatted_input": {
            "qid": 62989401,
            "link": "https://stackoverflow.com/questions/62989401/how-to-find-first-occurrence-of-a-significant-difference-in-values-of-a-pandas-d",
            "question": {
                "title": "How to find first occurrence of a significant difference in values of a pandas dataframe?",
                "ques_desc": "In a Pandas DataFrame, how would I find the first occurrence of a large difference between two values at two adjacent indices? As an example, if I have a DataFrame column A with data , I would want index holding 1.5, which would be 5. In my code below, it would give me the index holding 7.2, because . How should I fix this problem, so I get the index of the first 'large difference' occurrence? "
            },
            "io": [
                "[1, 1.1, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]",
                "15 - 7.2 > 7 - 1.5"
            ],
            "answer": {
                "ans_desc": "The main issue is of course how you define a \"large difference\". Your solution is pretty good to get the largest difference, improved only by using and using absolute values as shown by Jezrael: Using absolute values matters if your values are not sorted, in which case you can get negative differences. Then, you should probably do some clustering on these values and get the smallest index of the cluster with largest values. Jezrael already showed a heuristic by using the largest quartile, however by only slightly modifying your example this doesn\u2019t work: This returns Here's 3 other heuristics that might work better for you: If you have a value above which you consider a difference to be \u201clarge\u201d (e.g. ): If you know how many large values there are, you can select those and get the smallest index (e.g. ): If you know all small values are grouped together (as are all the 0.1 in your example), you can filter what's larger than the mean (or the mean + 1 standard deviation if your \u201clarge\u201d values are very close to the smaller ones). This is because contrarily to the median, your few large differences will pull the mean up significantly. If you really want to go for proper clustering, you can use the KMeans algorithm from scikit learn: This classifies the data into 2 classes, and then gets the classification into a pandas Series. We then filter this series\u2019 index by selecting only the cluster that has the highest values, and finally get the first element of this filtered index. ",
                "code": [
                    "df = pd.DataFrame({'A': [1, 1.05, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]})\ndifferences = df['A'].diff(-1).abs()\nidx = differences.index[differences >= differences.quantile(.75)][0]\nprint(idx, differences[idx])\n",
                    "idx = differences.index[differences >= 1.5][0]\n",
                    "idx = differences.index[differences >= differences.mean()][0]\n",
                    "from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2).fit(differences.values[:-1].reshape(-1, 1))\nclusters = pd.Series(kmeans.labels_, index=differences.index[:-1])\nidx = clusters.index[clusters.eq(np.squeeze(kmeans.cluster_centers_).argmax())][0]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 405,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 62996690,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1595252472,
        "creation_date": 1595251019,
        "last_edit_date": 1595251373,
        "question_id": 62996260,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62996260/how-can-i-sort-dataframe-based-on-a-complicated-string-column",
        "title": "How can I sort dataframe based on a complicated string column?",
        "body": "<p>I'm needing to sort a dataframe based on a string column, which is composed of a variety of letters, numbers, dashes, and string lengths. I'm not even sure sorting is the right method of what I want to do. Example below:</p>\n<p>df</p>\n<pre><code>Col1    Col2\n  A    80NX-265-DF23\n  B    D-87-B-003\n  C    80NX-265-DF23\n  D    0333-DD-02\n  E    D-87-B-003\n  F    80NX-265-DF23\n</code></pre>\n<p>Desired DF:</p>\n<pre><code>Col1     Col2\n A      80NX-265-DF23\n C      80NX-265-DF23\n F      80NX-265-DF23\n D      0333-DD-02\n B      D-87-B-003\n E      D-87-B-003\n</code></pre>\n<p>The order/sorting of either column does not matter, I just want the dataframe reordered and 'grouped' on column2. Grouped might not be the right expression here either cause I don't want to perform any sort of aggregated calculation. Any ideas? Thanks!</p>\n",
        "answer_body": "<p>One approach would be iterration and concat:</p>\n<pre><code>pd.concat([df[df['Col2']==x] for x in df['Col2'].unique()])\n</code></pre>\n",
        "question_body": "<p>I'm needing to sort a dataframe based on a string column, which is composed of a variety of letters, numbers, dashes, and string lengths. I'm not even sure sorting is the right method of what I want to do. Example below:</p>\n<p>df</p>\n<pre><code>Col1    Col2\n  A    80NX-265-DF23\n  B    D-87-B-003\n  C    80NX-265-DF23\n  D    0333-DD-02\n  E    D-87-B-003\n  F    80NX-265-DF23\n</code></pre>\n<p>Desired DF:</p>\n<pre><code>Col1     Col2\n A      80NX-265-DF23\n C      80NX-265-DF23\n F      80NX-265-DF23\n D      0333-DD-02\n B      D-87-B-003\n E      D-87-B-003\n</code></pre>\n<p>The order/sorting of either column does not matter, I just want the dataframe reordered and 'grouped' on column2. Grouped might not be the right expression here either cause I don't want to perform any sort of aggregated calculation. Any ideas? Thanks!</p>\n",
        "formatted_input": {
            "qid": 62996260,
            "link": "https://stackoverflow.com/questions/62996260/how-can-i-sort-dataframe-based-on-a-complicated-string-column",
            "question": {
                "title": "How can I sort dataframe based on a complicated string column?",
                "ques_desc": "I'm needing to sort a dataframe based on a string column, which is composed of a variety of letters, numbers, dashes, and string lengths. I'm not even sure sorting is the right method of what I want to do. Example below: df Desired DF: The order/sorting of either column does not matter, I just want the dataframe reordered and 'grouped' on column2. Grouped might not be the right expression here either cause I don't want to perform any sort of aggregated calculation. Any ideas? Thanks! "
            },
            "io": [
                "Col1    Col2\n  A    80NX-265-DF23\n  B    D-87-B-003\n  C    80NX-265-DF23\n  D    0333-DD-02\n  E    D-87-B-003\n  F    80NX-265-DF23\n",
                "Col1     Col2\n A      80NX-265-DF23\n C      80NX-265-DF23\n F      80NX-265-DF23\n D      0333-DD-02\n B      D-87-B-003\n E      D-87-B-003\n"
            ],
            "answer": {
                "ans_desc": "One approach would be iterration and concat: ",
                "code": [
                    "pd.concat([df[df['Col2']==x] for x in df['Col2'].unique()])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-analysis"
        ],
        "owner": {
            "reputation": 1318,
            "user_id": 3375198,
            "user_type": "registered",
            "accept_rate": 73,
            "profile_image": "https://www.gravatar.com/avatar/9441140cf6f5f6a110d58ea021522b54?s=128&d=identicon&r=PG&f=1",
            "display_name": "adijo",
            "link": "https://stackoverflow.com/users/3375198/adijo"
        },
        "is_answered": true,
        "view_count": 170778,
        "accepted_answer_id": 28236391,
        "answer_count": 2,
        "score": 80,
        "last_activity_date": 1595154027,
        "creation_date": 1422622112,
        "last_edit_date": 1481476885,
        "question_id": 28236305,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/28236305/how-do-i-sum-values-in-a-column-that-match-a-given-condition-using-pandas",
        "title": "How do I sum values in a column that match a given condition using pandas?",
        "body": "<p>Suppose I have a column like so:</p>\n\n<pre><code>a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n</code></pre>\n\n<p>I want to sum up the values for <code>b</code> where <code>a = 1</code>, for example. This would give me <code>5 + 7 + 3 = 15</code>.</p>\n\n<p>How do I do this in pandas?</p>\n",
        "answer_body": "<p>The essential idea here is to select the data you want to sum, and then sum them. This selection of data can be done in several different ways, a few of which are shown below.</p>\n\n<h2>Boolean indexing</h2>\n\n<p>Arguably the most common way to select the values is to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"noreferrer\">Boolean indexing</a>. </p>\n\n<p>With this method, you find out where column 'a' is equal to <code>1</code> and then sum the corresponding rows of column 'b'. You can use <code>loc</code> to handle the indexing of rows and columns:</p>\n\n<pre><code>&gt;&gt;&gt; df.loc[df['a'] == 1, 'b'].sum()\n15\n</code></pre>\n\n<p>The Boolean indexing can be extended to other columns. For example if <code>df</code> also contained a column 'c' and we wanted to sum the rows in 'b' where 'a' was 1 and 'c' was 2, we'd write:</p>\n\n<pre><code>df.loc[(df['a'] == 1) &amp; (df['c'] == 2), 'b'].sum()\n</code></pre>\n\n<h2>Query</h2>\n\n<p>Another way to select the data is to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html\" rel=\"noreferrer\"><code>query</code></a> to filter the rows you're interested in, select column 'b' and then sum:</p>\n\n<pre><code>&gt;&gt;&gt; df.query(\"a == 1\")['b'].sum()\n15\n</code></pre>\n\n<p>Again, the method can be extended to make more complicated selections of the data:</p>\n\n<pre><code>df.query(\"a == 1 and c == 2\")['b'].sum()\n</code></pre>\n\n<p>Note this is a little more concise than the Boolean indexing approach.</p>\n\n<h2>Groupby</h2>\n\n<p>The alternative approach is to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html\" rel=\"noreferrer\"><code>groupby</code></a> to split the DataFrame into parts according to the value in column 'a'. You can then sum each part and pull out the value that the 1s added up to:</p>\n\n<pre><code>&gt;&gt;&gt; df.groupby('a')['b'].sum()[1]\n15\n</code></pre>\n\n<p>This approach is likely to be slower than using Boolean indexing, but it is useful if you want check the sums for other values in column <code>a</code>:</p>\n\n<pre><code>&gt;&gt;&gt; df.groupby('a')['b'].sum()\na\n1    15\n2     8\n</code></pre>\n",
        "question_body": "<p>Suppose I have a column like so:</p>\n\n<pre><code>a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n</code></pre>\n\n<p>I want to sum up the values for <code>b</code> where <code>a = 1</code>, for example. This would give me <code>5 + 7 + 3 = 15</code>.</p>\n\n<p>How do I do this in pandas?</p>\n",
        "formatted_input": {
            "qid": 28236305,
            "link": "https://stackoverflow.com/questions/28236305/how-do-i-sum-values-in-a-column-that-match-a-given-condition-using-pandas",
            "question": {
                "title": "How do I sum values in a column that match a given condition using pandas?",
                "ques_desc": "Suppose I have a column like so: I want to sum up the values for where , for example. This would give me . How do I do this in pandas? "
            },
            "io": [
                "a   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n",
                "5 + 7 + 3 = 15"
            ],
            "answer": {
                "ans_desc": "The essential idea here is to select the data you want to sum, and then sum them. This selection of data can be done in several different ways, a few of which are shown below. Boolean indexing Arguably the most common way to select the values is to use Boolean indexing. With this method, you find out where column 'a' is equal to and then sum the corresponding rows of column 'b'. You can use to handle the indexing of rows and columns: The Boolean indexing can be extended to other columns. For example if also contained a column 'c' and we wanted to sum the rows in 'b' where 'a' was 1 and 'c' was 2, we'd write: Query Another way to select the data is to use to filter the rows you're interested in, select column 'b' and then sum: Again, the method can be extended to make more complicated selections of the data: Note this is a little more concise than the Boolean indexing approach. Groupby The alternative approach is to use to split the DataFrame into parts according to the value in column 'a'. You can then sum each part and pull out the value that the 1s added up to: This approach is likely to be slower than using Boolean indexing, but it is useful if you want check the sums for other values in column : ",
                "code": [
                    ">>> df.loc[df['a'] == 1, 'b'].sum()\n15\n",
                    "df.loc[(df['a'] == 1) & (df['c'] == 2), 'b'].sum()\n",
                    ">>> df.query(\"a == 1\")['b'].sum()\n15\n",
                    "df.query(\"a == 1 and c == 2\")['b'].sum()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 27,
            "user_id": 13954853,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Ggb0MPoJlvpP_sQg6b7BUP-GtJcOnPSS_4-8Uds=k-s128",
            "display_name": "Ghislaine Boogerd",
            "link": "https://stackoverflow.com/users/13954853/ghislaine-boogerd"
        },
        "is_answered": true,
        "view_count": 81,
        "accepted_answer_id": 62973016,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1595138026,
        "creation_date": 1595100531,
        "last_edit_date": 1595103097,
        "question_id": 62972913,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62972913/how-to-shuffle-a-dataframe-while-maintaining-the-order-of-a-specific-column",
        "title": "How to shuffle a dataframe while maintaining the order of a specific column",
        "body": "<p>I have a pandas dataframe which I want to shuffle, but keep the order of 1 column.</p>\n<p>So imagine I have the following df:</p>\n<pre><code>| i | val | val2| ID |    \n| 0 | 2   | 2 |a  |  \n| 1 | 3   | 3 |b  |  \n| 2 | 4   | 4 |a  |  \n| 3 | 6   | 5 |b  |  \n| 4 | 5   | 6 |b  |  \n</code></pre>\n<p>I want to shuffle the rows but keep the order of the ID column of the first df. My wanted result would be something like this:</p>\n<pre><code>| i | val | val2| ID |  \n| 2 | 4   | 4 |a  |    \n| 4 | 5   | 6 |b  |  \n| 0 | 2   | 2 |a  |  \n| 3 | 6   | 5 |b  |  \n| 1 | 3   | 3 |b  |  \n</code></pre>\n<p>How do I do this?</p>\n",
        "answer_body": "<p>Here's a solution:</p>\n<pre><code>df = pd.DataFrame({&quot;val&quot;: [1, 2, 3, 4, 5, 6, 7], &quot;ID&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;]})\ndf[&quot;val&quot;] = df.groupby(&quot;ID&quot;).transform(lambda x: x.sample(frac=1))\nprint(df)\n</code></pre>\n<p>The output is:</p>\n<pre><code>   val ID\n0    5  a\n1    7  b\n2    1  a\n3    2  b\n4    3  a\n5    6  a\n6    4  b\n</code></pre>\n<hr />\n<p>If you have a dataframe with multiple columns, and you'd like to shuffle while maintaining the order of one of the columns, the solution is very similar:</p>\n<pre><code>df = pd.DataFrame({&quot;val&quot;: [1, 2, 3, 4, 5, 6, 7], \n                   &quot;val2&quot;: range(10, 17), \n                   &quot;ID&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;], \n                  })\n\ndf[[&quot;val&quot;, &quot;val2&quot;]] = df.groupby(&quot;ID&quot;).transform(lambda x: x.sample(frac=1))\nprint(df)\n\n==&gt;\n\n   val  val2 ID\n0    3    12  a\n1    7    16  b\n2    5    14  a\n3    2    11  b\n4    6    15  a\n5    1    10  a\n6    4    13  b\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe which I want to shuffle, but keep the order of 1 column.</p>\n<p>So imagine I have the following df:</p>\n<pre><code>| i | val | val2| ID |    \n| 0 | 2   | 2 |a  |  \n| 1 | 3   | 3 |b  |  \n| 2 | 4   | 4 |a  |  \n| 3 | 6   | 5 |b  |  \n| 4 | 5   | 6 |b  |  \n</code></pre>\n<p>I want to shuffle the rows but keep the order of the ID column of the first df. My wanted result would be something like this:</p>\n<pre><code>| i | val | val2| ID |  \n| 2 | 4   | 4 |a  |    \n| 4 | 5   | 6 |b  |  \n| 0 | 2   | 2 |a  |  \n| 3 | 6   | 5 |b  |  \n| 1 | 3   | 3 |b  |  \n</code></pre>\n<p>How do I do this?</p>\n",
        "formatted_input": {
            "qid": 62972913,
            "link": "https://stackoverflow.com/questions/62972913/how-to-shuffle-a-dataframe-while-maintaining-the-order-of-a-specific-column",
            "question": {
                "title": "How to shuffle a dataframe while maintaining the order of a specific column",
                "ques_desc": "I have a pandas dataframe which I want to shuffle, but keep the order of 1 column. So imagine I have the following df: I want to shuffle the rows but keep the order of the ID column of the first df. My wanted result would be something like this: How do I do this? "
            },
            "io": [
                "| i | val | val2| ID |    \n| 0 | 2   | 2 |a  |  \n| 1 | 3   | 3 |b  |  \n| 2 | 4   | 4 |a  |  \n| 3 | 6   | 5 |b  |  \n| 4 | 5   | 6 |b  |  \n",
                "| i | val | val2| ID |  \n| 2 | 4   | 4 |a  |    \n| 4 | 5   | 6 |b  |  \n| 0 | 2   | 2 |a  |  \n| 3 | 6   | 5 |b  |  \n| 1 | 3   | 3 |b  |  \n"
            ],
            "answer": {
                "ans_desc": "Here's a solution: The output is: If you have a dataframe with multiple columns, and you'd like to shuffle while maintaining the order of one of the columns, the solution is very similar: ",
                "code": [
                    "df = pd.DataFrame({\"val\": [1, 2, 3, 4, 5, 6, 7], \"ID\": [\"a\", \"b\", \"a\", \"b\", \"a\", \"a\", \"b\"]})\ndf[\"val\"] = df.groupby(\"ID\").transform(lambda x: x.sample(frac=1))\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 79,
            "user_id": 12397247,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mCqMvoIrUxbaaB6A9EtPwamVb0Q_fO1BWljgOOLKw=k-s128",
            "display_name": "craze",
            "link": "https://stackoverflow.com/users/12397247/craze"
        },
        "is_answered": true,
        "view_count": 108,
        "accepted_answer_id": 62967533,
        "answer_count": 3,
        "score": 4,
        "last_activity_date": 1595093214,
        "creation_date": 1595067792,
        "last_edit_date": 1595093214,
        "question_id": 62967408,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62967408/how-can-i-remove-a-certain-type-of-values-in-a-group-in-pandas",
        "title": "How can I remove a certain type of values in a group in pandas?",
        "body": "<p>I have the following dataframe which is a small part of a bigger one:</p>\n<pre><code>   acc_num   trans_cdi\n0     1         c\n1     1         d\n3     3         d\n4     3         c\n5     3         d\n6     3         d\n</code></pre>\n<p>I'd like to delete all rows where the last items are &quot;d&quot;. So my desired dataframe would look like this:</p>\n<pre><code>   acc_num   trans_cdi\n0     1         c\n3     3         d\n4     3         c\n</code></pre>\n<p>So the point is, that a group shouldn't have &quot;d&quot; as the last item.</p>\n<p>There is a code that deletes the last row in the groups where the last item is &quot;d&quot;. But in this case, I have to run the code twice to delete all last &quot;d&quot;-s in group 3 for example.</p>\n<pre><code>clean_3 = clean_2[clean_2.groupby('account_num')['trans_cdi'].transform(lambda x: (x.iloc[-1] != &quot;d&quot;) | (x.index != x.index[-1]))]\n</code></pre>\n<p>Is there a better solution to this problem?</p>\n",
        "answer_body": "<ol>\n<li>First, compare to the next row with <code>shift</code> if the values are both equal to 'd'. <code>~</code> filters out the specified rows.</li>\n<li>Second, Make sure the last row value is not <code>d</code>. If it is, then delete the row.</li>\n</ol>\n<p>code:</p>\n<pre><code>df = df[~((df['trans_cdi'] == 'd') &amp; (df.shift(1)['trans_cdi'] == 'd'))]\nif df['trans_cdi'].iloc[-1] == 'd': df = df.iloc[0:-1]\ndf\n</code></pre>\n<p>input (I tested it on more input data to ensure there were no bugs):</p>\n<pre><code>    acc_num trans_cdi\n0   1       c\n1   1       d\n3   3       d\n4   3       c\n5   3       d\n6   3       d\n7   1       d\n8   1       d\n9   3       c\n10  3       c\n11  3       d\n12  3       d\n</code></pre>\n<p>output:</p>\n<pre><code>       acc_num  trans_cdi\n0       1       c\n1       1       d\n4       3       c\n5       3       d\n9       3       c\n10      3       c\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe which is a small part of a bigger one:</p>\n<pre><code>   acc_num   trans_cdi\n0     1         c\n1     1         d\n3     3         d\n4     3         c\n5     3         d\n6     3         d\n</code></pre>\n<p>I'd like to delete all rows where the last items are &quot;d&quot;. So my desired dataframe would look like this:</p>\n<pre><code>   acc_num   trans_cdi\n0     1         c\n3     3         d\n4     3         c\n</code></pre>\n<p>So the point is, that a group shouldn't have &quot;d&quot; as the last item.</p>\n<p>There is a code that deletes the last row in the groups where the last item is &quot;d&quot;. But in this case, I have to run the code twice to delete all last &quot;d&quot;-s in group 3 for example.</p>\n<pre><code>clean_3 = clean_2[clean_2.groupby('account_num')['trans_cdi'].transform(lambda x: (x.iloc[-1] != &quot;d&quot;) | (x.index != x.index[-1]))]\n</code></pre>\n<p>Is there a better solution to this problem?</p>\n",
        "formatted_input": {
            "qid": 62967408,
            "link": "https://stackoverflow.com/questions/62967408/how-can-i-remove-a-certain-type-of-values-in-a-group-in-pandas",
            "question": {
                "title": "How can I remove a certain type of values in a group in pandas?",
                "ques_desc": "I have the following dataframe which is a small part of a bigger one: I'd like to delete all rows where the last items are \"d\". So my desired dataframe would look like this: So the point is, that a group shouldn't have \"d\" as the last item. There is a code that deletes the last row in the groups where the last item is \"d\". But in this case, I have to run the code twice to delete all last \"d\"-s in group 3 for example. Is there a better solution to this problem? "
            },
            "io": [
                "   acc_num   trans_cdi\n0     1         c\n1     1         d\n3     3         d\n4     3         c\n5     3         d\n6     3         d\n",
                "   acc_num   trans_cdi\n0     1         c\n3     3         d\n4     3         c\n"
            ],
            "answer": {
                "ans_desc": " First, compare to the next row with if the values are both equal to 'd'. filters out the specified rows. Second, Make sure the last row value is not . If it is, then delete the row. code: input (I tested it on more input data to ensure there were no bugs): output: ",
                "code": [
                    "df = df[~((df['trans_cdi'] == 'd') & (df.shift(1)['trans_cdi'] == 'd'))]\nif df['trans_cdi'].iloc[-1] == 'd': df = df.iloc[0:-1]\ndf\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 13950569,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f8bfb6ced94ce544d712e38046307340?s=128&d=identicon&r=PG&f=1",
            "display_name": "Vlarabor",
            "link": "https://stackoverflow.com/users/13950569/vlarabor"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 62966204,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1595062168,
        "creation_date": 1595024054,
        "last_edit_date": 1595057522,
        "question_id": 62962437,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62962437/slicing-each-dataframe-row-into-3-windows-with-different-slicing-ranges",
        "title": "Slicing each dataframe row into 3 windows with different slicing ranges",
        "body": "<p>I want to slice each row of my dataframe into 3 windows with slice indices that are stored in another dataframe and change for each row of the dataframe. Afterwards i want to return a single dataframe containing the windows in form of a MultiIndex. The rows in each windows that are shorter than the longest row in the window should be filled with NaN values.\nSince my actual dataframe has around 100.000 rows and 600 columns, i am concerned about an efficient solution.</p>\n<p>Consider the following example:</p>\n<p>This is my dataframe which i want to slice into 3 windows</p>\n<pre><code>&gt;&gt;&gt; df\n  0  1  2  3  4  5  6  7\n0 0  1  2  3  4  5  6  7\n1 8  9  10 11 12 13 14 15\n2 16 17 18 19 20 21 22 23\n</code></pre>\n<p>And the second dataframe containing my slicing indices having the same count of rows as <code>df</code>:</p>\n<pre><code>&gt;&gt;&gt; df_slice\n  0 1\n0 3 5\n1 2 6\n2 4 7\n</code></pre>\n<p>I've tried slicing the windows, like so:</p>\n<pre><code>first_window = df.iloc[:, :df_slice.iloc[:, 0]]\nfirst_window.columns = pd.MultiIndex.from_tuples([(&quot;A&quot;, c) for c in first_window.columns])\n\nsecond_window = df.iloc[:, df_slice.iloc[:, 0] : df_slice.iloc[:, 1]]\nsecond_window.columns = pd.MultiIndex.from_tuples([(&quot;B&quot;, c) for c in second_window.columns])\n\nthird_window = df.iloc[:, df_slice.iloc[:, 1]:]\nthird_window.columns = pd.MultiIndex.from_tuples([(&quot;C&quot;, c) for c in third_window.columns])\nresult = pd.concat([first_window,\n                    second_window,\n                    third_window], axis=1)\n</code></pre>\n<p>Which gives me the following error:</p>\n<pre><code>TypeError: cannot do slice indexing on &lt;class 'pandas.core.indexes.range.RangeIndex'&gt; with these indexers [0    3\n1    2\n2    4\nName: 0, dtype: int64] of &lt;class 'pandas.core.series.Series'&gt;\n</code></pre>\n<p>My expected output is something like this:</p>\n<pre><code>&gt;&gt;&gt; result\n    A                   B                   C           \n    0   1     2     3   4   5     6     7   8     9    10\n0   0   1     2   NaN   3   4   NaN   NaN   5     6    7\n1   8   9   NaN   NaN  10  11    12    13  14    15  NaN\n2  16  17    18    19  20  21    22   NaN  23   NaN  NaN\n</code></pre>\n<p>Is there an efficient solution for my problem without iterating over each row of my dataframe?</p>\n",
        "answer_body": "<p>Here's a solution which, using <code>melt</code> and then <code>pivot_table</code>, plus some logic to:</p>\n<ul>\n<li>Identify the three groups 'A', 'B', and 'C'.</li>\n<li>Shift the columns to the left, so that NaN would only appear at the right side of each window.</li>\n<li>Rename columns to get the expected output.</li>\n</ul>\n<pre><code>    t = df.reset_index().melt(id_vars=&quot;index&quot;)\n    t = pd.merge(t, df_slice, left_on=&quot;index&quot;, right_index=True)\n    t.variable = pd.to_numeric(t.variable)\n    \n    t.loc[t.variable &lt; t.c_0,&quot;group&quot;] = &quot;A&quot;\n    t.loc[(t.variable &gt;= t.c_0) &amp; (t.variable &lt; t.c_1), &quot;group&quot;] = &quot;B&quot;\n    t.loc[t.variable &gt;= t.c_1, &quot;group&quot;] = &quot;C&quot;\n\n    # shift relevant values to the left\n    shift_val = t.groupby([&quot;group&quot;, &quot;index&quot;]).variable.transform(&quot;min&quot;) - t.groupby([&quot;group&quot;]).variable.transform(&quot;min&quot;)\n    t.variable = t.variable - shift_val\n    \n    # extract a, b, and c groups, and create a multi-level index for their\n    # columns\n    df_a = pd.pivot_table(t[t.group == &quot;A&quot;], index= &quot;index&quot;, columns=&quot;variable&quot;, values=&quot;value&quot;)\n    df_a.columns = pd.MultiIndex.from_product([[&quot;a&quot;], df_a.columns])\n    \n    df_b = pd.pivot_table(t[t.group == &quot;B&quot;], index= &quot;index&quot;, columns=&quot;variable&quot;, values=&quot;value&quot;)\n    df_b.columns = pd.MultiIndex.from_product([[&quot;b&quot;], df_b.columns])\n    \n    df_c = pd.pivot_table(t[t.group == &quot;C&quot;], index= &quot;index&quot;, columns=&quot;variable&quot;, values=&quot;value&quot;)\n    df_c.columns = pd.MultiIndex.from_product([[&quot;c&quot;], df_c.columns])\n    \n    res = pd.concat([df_a, df_b, df_c], axis=1)\n    \n    res.columns = pd.MultiIndex.from_tuples([(c[0], i) for i, c in enumerate(res.columns)])\n    \n    print(res)\n</code></pre>\n<p>The output is:</p>\n<pre><code>          a                       b                       c           \n         0     1     2     3     4     5     6     7     8     9    10\nindex                                                                 \n0       0.0   1.0   2.0   NaN   3.0   4.0   NaN   NaN   5.0   6.0  7.0\n1       8.0   9.0   NaN   NaN  10.0  11.0  12.0  13.0  14.0  15.0  NaN\n2      16.0  17.0  18.0  19.0  20.0  21.0  22.0   NaN  23.0   NaN  NaN\n</code></pre>\n",
        "question_body": "<p>I want to slice each row of my dataframe into 3 windows with slice indices that are stored in another dataframe and change for each row of the dataframe. Afterwards i want to return a single dataframe containing the windows in form of a MultiIndex. The rows in each windows that are shorter than the longest row in the window should be filled with NaN values.\nSince my actual dataframe has around 100.000 rows and 600 columns, i am concerned about an efficient solution.</p>\n<p>Consider the following example:</p>\n<p>This is my dataframe which i want to slice into 3 windows</p>\n<pre><code>&gt;&gt;&gt; df\n  0  1  2  3  4  5  6  7\n0 0  1  2  3  4  5  6  7\n1 8  9  10 11 12 13 14 15\n2 16 17 18 19 20 21 22 23\n</code></pre>\n<p>And the second dataframe containing my slicing indices having the same count of rows as <code>df</code>:</p>\n<pre><code>&gt;&gt;&gt; df_slice\n  0 1\n0 3 5\n1 2 6\n2 4 7\n</code></pre>\n<p>I've tried slicing the windows, like so:</p>\n<pre><code>first_window = df.iloc[:, :df_slice.iloc[:, 0]]\nfirst_window.columns = pd.MultiIndex.from_tuples([(&quot;A&quot;, c) for c in first_window.columns])\n\nsecond_window = df.iloc[:, df_slice.iloc[:, 0] : df_slice.iloc[:, 1]]\nsecond_window.columns = pd.MultiIndex.from_tuples([(&quot;B&quot;, c) for c in second_window.columns])\n\nthird_window = df.iloc[:, df_slice.iloc[:, 1]:]\nthird_window.columns = pd.MultiIndex.from_tuples([(&quot;C&quot;, c) for c in third_window.columns])\nresult = pd.concat([first_window,\n                    second_window,\n                    third_window], axis=1)\n</code></pre>\n<p>Which gives me the following error:</p>\n<pre><code>TypeError: cannot do slice indexing on &lt;class 'pandas.core.indexes.range.RangeIndex'&gt; with these indexers [0    3\n1    2\n2    4\nName: 0, dtype: int64] of &lt;class 'pandas.core.series.Series'&gt;\n</code></pre>\n<p>My expected output is something like this:</p>\n<pre><code>&gt;&gt;&gt; result\n    A                   B                   C           \n    0   1     2     3   4   5     6     7   8     9    10\n0   0   1     2   NaN   3   4   NaN   NaN   5     6    7\n1   8   9   NaN   NaN  10  11    12    13  14    15  NaN\n2  16  17    18    19  20  21    22   NaN  23   NaN  NaN\n</code></pre>\n<p>Is there an efficient solution for my problem without iterating over each row of my dataframe?</p>\n",
        "formatted_input": {
            "qid": 62962437,
            "link": "https://stackoverflow.com/questions/62962437/slicing-each-dataframe-row-into-3-windows-with-different-slicing-ranges",
            "question": {
                "title": "Slicing each dataframe row into 3 windows with different slicing ranges",
                "ques_desc": "I want to slice each row of my dataframe into 3 windows with slice indices that are stored in another dataframe and change for each row of the dataframe. Afterwards i want to return a single dataframe containing the windows in form of a MultiIndex. The rows in each windows that are shorter than the longest row in the window should be filled with NaN values. Since my actual dataframe has around 100.000 rows and 600 columns, i am concerned about an efficient solution. Consider the following example: This is my dataframe which i want to slice into 3 windows And the second dataframe containing my slicing indices having the same count of rows as : I've tried slicing the windows, like so: Which gives me the following error: My expected output is something like this: Is there an efficient solution for my problem without iterating over each row of my dataframe? "
            },
            "io": [
                ">>> df\n  0  1  2  3  4  5  6  7\n0 0  1  2  3  4  5  6  7\n1 8  9  10 11 12 13 14 15\n2 16 17 18 19 20 21 22 23\n",
                ">>> df_slice\n  0 1\n0 3 5\n1 2 6\n2 4 7\n"
            ],
            "answer": {
                "ans_desc": "Here's a solution which, using and then , plus some logic to: Identify the three groups 'A', 'B', and 'C'. Shift the columns to the left, so that NaN would only appear at the right side of each window. Rename columns to get the expected output. The output is: ",
                "code": [
                    "    t = df.reset_index().melt(id_vars=\"index\")\n    t = pd.merge(t, df_slice, left_on=\"index\", right_index=True)\n    t.variable = pd.to_numeric(t.variable)\n    \n    t.loc[t.variable < t.c_0,\"group\"] = \"A\"\n    t.loc[(t.variable >= t.c_0) & (t.variable < t.c_1), \"group\"] = \"B\"\n    t.loc[t.variable >= t.c_1, \"group\"] = \"C\"\n\n    # shift relevant values to the left\n    shift_val = t.groupby([\"group\", \"index\"]).variable.transform(\"min\") - t.groupby([\"group\"]).variable.transform(\"min\")\n    t.variable = t.variable - shift_val\n    \n    # extract a, b, and c groups, and create a multi-level index for their\n    # columns\n    df_a = pd.pivot_table(t[t.group == \"A\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_a.columns = pd.MultiIndex.from_product([[\"a\"], df_a.columns])\n    \n    df_b = pd.pivot_table(t[t.group == \"B\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_b.columns = pd.MultiIndex.from_product([[\"b\"], df_b.columns])\n    \n    df_c = pd.pivot_table(t[t.group == \"C\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_c.columns = pd.MultiIndex.from_product([[\"c\"], df_c.columns])\n    \n    res = pd.concat([df_a, df_b, df_c], axis=1)\n    \n    res.columns = pd.MultiIndex.from_tuples([(c[0], i) for i, c in enumerate(res.columns)])\n    \n    print(res)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 91,
            "user_id": 11521387,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5aae48f091e3cde2bd2fe33a8ec0de8c?s=128&d=identicon&r=PG&f=1",
            "display_name": "svn",
            "link": "https://stackoverflow.com/users/11521387/svn"
        },
        "is_answered": true,
        "view_count": 146,
        "accepted_answer_id": 62941960,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1594928480,
        "creation_date": 1594926313,
        "last_edit_date": 1594928480,
        "question_id": 62941861,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62941861/python-pandas-ffill-with-groupby",
        "title": "python - pandas ffill with groupby",
        "body": "<p>I am trying to forward fill the missing rows to complete the missing time-series rows in the dataset.</p>\n<p>The size of the dataset is huge. More than 100 million rows.</p>\n<p>The original source dataset is as shown below.</p>\n<pre><code>         col1 col2 col3  col4  col5  col6\n0  2020-01-01   b1   c1     1     9    17\n1  2020-01-05   b1   c1     2    10    18\n2  2020-01-02   b2   c2     3    11    19\n3  2020-01-04   b2   c2     4    12    20\n4  2020-01-10   b3   c3     5    13    21\n5  2020-01-15   b3   c3     6    14    22\n6  2020-01-16   b4   c4     7    15    23\n7  2020-01-30   b4   c4     8    16    24\n\n</code></pre>\n<p>desired output is as below</p>\n<pre><code>         col1 col2 col3  col4  col5  col6\n0  2020-01-01   b1   c1   1.0   9.0  17.0\n1  2020-01-02   b1   c1   1.0   9.0  17.0\n2  2020-01-03   b1   c1   1.0   9.0  17.0\n3  2020-01-04   b1   c1   1.0   9.0  17.0\n4  2020-01-05   b1   c1   2.0  10.0  18.0\n5  2020-01-02   b2   c2   3.0  11.0  19.0\n6  2020-01-03   b2   c2   3.0  11.0  19.0\n7  2020-01-04   b2   c2   4.0  12.0  20.0\n8  2020-01-10   b3   c3   5.0  13.0  21.0\n9  2020-01-11   b3   c3   5.0  13.0  21.0\n10 2020-01-12   b3   c3   5.0  13.0  21.0\n11 2020-01-13   b3   c3   5.0  13.0  21.0\n12 2020-01-14   b3   c3   5.0  13.0  21.0\n13 2020-01-15   b3   c3   6.0  14.0  22.0\n14 2020-01-16   b4   c4   7.0  15.0  23.0\n15 2020-01-17   b4   c4   7.0  15.0  23.0\n16 2020-01-18   b4   c4   7.0  15.0  23.0\n17 2020-01-19   b4   c4   7.0  15.0  23.0\n18 2020-01-20   b4   c4   7.0  15.0  23.0\n19 2020-01-21   b4   c4   7.0  15.0  23.0\n20 2020-01-22   b4   c4   7.0  15.0  23.0\n21 2020-01-23   b4   c4   7.0  15.0  23.0\n22 2020-01-24   b4   c4   7.0  15.0  23.0\n23 2020-01-25   b4   c4   7.0  15.0  23.0\n24 2020-01-26   b4   c4   7.0  15.0  23.0\n25 2020-01-27   b4   c4   7.0  15.0  23.0\n26 2020-01-28   b4   c4   7.0  15.0  23.0\n27 2020-01-29   b4   c4   7.0  15.0  23.0\n28 2020-01-30   b4   c4   8.0  16.0  24.0\n</code></pre>\n<p>I need to group on <code>col2</code> and <code>col3</code> to fill the missing time-series rows in <code>col1</code> for each of the combinations.</p>\n<p>Currently, I have the below code which is working but its extremely slow due to the for-loop.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndef fill_missing_timeseries(subset_df, date_col):\n    if subset_df[date_col].dtype != 'datetime64[ns]':\n        subset_df[date_col] = pd.to_datetime(subset_df[date_col], infer_datetime_format=True)\n    min_date = subset_df[date_col].min()\n    max_date = subset_df[date_col].max()\n\n    # generate a continous date column between the min and max date values\n    date_range = pd.date_range(start=min_date, end=max_date, freq='D',)\n    new_df = pd.DataFrame()\n    new_df[date_col] = date_range\n    \n    # join newly generated df with input df to get all the columns\n    new_df = pd.merge(new_df, subset_df, how='left')\n    \n    # forward fill missing NaN values\n    new_df = new_df.ffill()\n    return new_df\n\norig_df = pd.DataFrame({'col1': ['2020-01-01','2020-01-05', '2020-01-02','2020-01-04','2020-01-10','2020-01-15','2020-01-16','2020-01-30'],\n                        'col2': ['b1','b1','b2','b2','b3','b3','b4','b4'],\n                        'col3': ['c1','c1','c2','c2','c3','c3','c4','c4'],\n                        'col4': [1,2,3,4,5,6,7,8],\n                        'col5': [9,10,11,12,13,14,15,16],\n                        'col6': [17,18,19,20,21,22,23,24],\n                       })\ndata = []\ngrouped_by_df = orig_df.groupby(['col2', 'col3']).size().reset_index().rename(columns={0:'count'})\nfor index, row in grouped_by_df.iterrows():\n    subset_df = orig_df[(orig_df.col2 == row[0]) &amp; (orig_df.col3 == row[1])]\n    subset_filled_df = fill_missing_timeseries(subset_df, date_col='col1')\n    data.append(subset_filled_df)\ndesired_df = pd.concat(data, ignore_index=True)\n</code></pre>\n<p>Is there any way I can avoid the for-loop and send the whole dataset for creating missing rows and ffill()?</p>\n<p>Thanks and Appreciate the help.</p>\n<p>Update:\nThe above code is working but it's too slow. It takes more than 30 minutes for just 300k rows. Hence, I'm looking for help to make it faster and avoid the for-loop.</p>\n",
        "answer_body": "<p>Looks like a <code>resample</code> on <code>groupby</code> would work:</p>\n<pre><code>(df.set_index('col1').groupby(['col2', 'col3'])\n   .resample('D').ffill()\n   .reset_index(['col2','col3'], drop=True)\n   .reset_index()\n)\n</code></pre>\n<p>Output:</p>\n<pre><code>         col1 col2 col3  col4  col5  col6\n0  2020-01-01   b1   c1     1     9    17\n1  2020-01-02   b1   c1     1     9    17\n2  2020-01-03   b1   c1     1     9    17\n3  2020-01-04   b1   c1     1     9    17\n4  2020-01-05   b1   c1     2    10    18\n5  2020-01-02   b2   c2     3    11    19\n6  2020-01-03   b2   c2     3    11    19\n7  2020-01-04   b2   c2     4    12    20\n8  2020-01-10   b3   c3     5    13    21\n9  2020-01-11   b3   c3     5    13    21\n10 2020-01-12   b3   c3     5    13    21\n11 2020-01-13   b3   c3     5    13    21\n12 2020-01-14   b3   c3     5    13    21\n13 2020-01-15   b3   c3     6    14    22\n14 2020-01-16   b4   c4     7    15    23\n15 2020-01-17   b4   c4     7    15    23\n16 2020-01-18   b4   c4     7    15    23\n17 2020-01-19   b4   c4     7    15    23\n18 2020-01-20   b4   c4     7    15    23\n19 2020-01-21   b4   c4     7    15    23\n20 2020-01-22   b4   c4     7    15    23\n21 2020-01-23   b4   c4     7    15    23\n22 2020-01-24   b4   c4     7    15    23\n23 2020-01-25   b4   c4     7    15    23\n24 2020-01-26   b4   c4     7    15    23\n25 2020-01-27   b4   c4     7    15    23\n26 2020-01-28   b4   c4     7    15    23\n27 2020-01-29   b4   c4     7    15    23\n28 2020-01-30   b4   c4     8    16    24\n</code></pre>\n",
        "question_body": "<p>I am trying to forward fill the missing rows to complete the missing time-series rows in the dataset.</p>\n<p>The size of the dataset is huge. More than 100 million rows.</p>\n<p>The original source dataset is as shown below.</p>\n<pre><code>         col1 col2 col3  col4  col5  col6\n0  2020-01-01   b1   c1     1     9    17\n1  2020-01-05   b1   c1     2    10    18\n2  2020-01-02   b2   c2     3    11    19\n3  2020-01-04   b2   c2     4    12    20\n4  2020-01-10   b3   c3     5    13    21\n5  2020-01-15   b3   c3     6    14    22\n6  2020-01-16   b4   c4     7    15    23\n7  2020-01-30   b4   c4     8    16    24\n\n</code></pre>\n<p>desired output is as below</p>\n<pre><code>         col1 col2 col3  col4  col5  col6\n0  2020-01-01   b1   c1   1.0   9.0  17.0\n1  2020-01-02   b1   c1   1.0   9.0  17.0\n2  2020-01-03   b1   c1   1.0   9.0  17.0\n3  2020-01-04   b1   c1   1.0   9.0  17.0\n4  2020-01-05   b1   c1   2.0  10.0  18.0\n5  2020-01-02   b2   c2   3.0  11.0  19.0\n6  2020-01-03   b2   c2   3.0  11.0  19.0\n7  2020-01-04   b2   c2   4.0  12.0  20.0\n8  2020-01-10   b3   c3   5.0  13.0  21.0\n9  2020-01-11   b3   c3   5.0  13.0  21.0\n10 2020-01-12   b3   c3   5.0  13.0  21.0\n11 2020-01-13   b3   c3   5.0  13.0  21.0\n12 2020-01-14   b3   c3   5.0  13.0  21.0\n13 2020-01-15   b3   c3   6.0  14.0  22.0\n14 2020-01-16   b4   c4   7.0  15.0  23.0\n15 2020-01-17   b4   c4   7.0  15.0  23.0\n16 2020-01-18   b4   c4   7.0  15.0  23.0\n17 2020-01-19   b4   c4   7.0  15.0  23.0\n18 2020-01-20   b4   c4   7.0  15.0  23.0\n19 2020-01-21   b4   c4   7.0  15.0  23.0\n20 2020-01-22   b4   c4   7.0  15.0  23.0\n21 2020-01-23   b4   c4   7.0  15.0  23.0\n22 2020-01-24   b4   c4   7.0  15.0  23.0\n23 2020-01-25   b4   c4   7.0  15.0  23.0\n24 2020-01-26   b4   c4   7.0  15.0  23.0\n25 2020-01-27   b4   c4   7.0  15.0  23.0\n26 2020-01-28   b4   c4   7.0  15.0  23.0\n27 2020-01-29   b4   c4   7.0  15.0  23.0\n28 2020-01-30   b4   c4   8.0  16.0  24.0\n</code></pre>\n<p>I need to group on <code>col2</code> and <code>col3</code> to fill the missing time-series rows in <code>col1</code> for each of the combinations.</p>\n<p>Currently, I have the below code which is working but its extremely slow due to the for-loop.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndef fill_missing_timeseries(subset_df, date_col):\n    if subset_df[date_col].dtype != 'datetime64[ns]':\n        subset_df[date_col] = pd.to_datetime(subset_df[date_col], infer_datetime_format=True)\n    min_date = subset_df[date_col].min()\n    max_date = subset_df[date_col].max()\n\n    # generate a continous date column between the min and max date values\n    date_range = pd.date_range(start=min_date, end=max_date, freq='D',)\n    new_df = pd.DataFrame()\n    new_df[date_col] = date_range\n    \n    # join newly generated df with input df to get all the columns\n    new_df = pd.merge(new_df, subset_df, how='left')\n    \n    # forward fill missing NaN values\n    new_df = new_df.ffill()\n    return new_df\n\norig_df = pd.DataFrame({'col1': ['2020-01-01','2020-01-05', '2020-01-02','2020-01-04','2020-01-10','2020-01-15','2020-01-16','2020-01-30'],\n                        'col2': ['b1','b1','b2','b2','b3','b3','b4','b4'],\n                        'col3': ['c1','c1','c2','c2','c3','c3','c4','c4'],\n                        'col4': [1,2,3,4,5,6,7,8],\n                        'col5': [9,10,11,12,13,14,15,16],\n                        'col6': [17,18,19,20,21,22,23,24],\n                       })\ndata = []\ngrouped_by_df = orig_df.groupby(['col2', 'col3']).size().reset_index().rename(columns={0:'count'})\nfor index, row in grouped_by_df.iterrows():\n    subset_df = orig_df[(orig_df.col2 == row[0]) &amp; (orig_df.col3 == row[1])]\n    subset_filled_df = fill_missing_timeseries(subset_df, date_col='col1')\n    data.append(subset_filled_df)\ndesired_df = pd.concat(data, ignore_index=True)\n</code></pre>\n<p>Is there any way I can avoid the for-loop and send the whole dataset for creating missing rows and ffill()?</p>\n<p>Thanks and Appreciate the help.</p>\n<p>Update:\nThe above code is working but it's too slow. It takes more than 30 minutes for just 300k rows. Hence, I'm looking for help to make it faster and avoid the for-loop.</p>\n",
        "formatted_input": {
            "qid": 62941861,
            "link": "https://stackoverflow.com/questions/62941861/python-pandas-ffill-with-groupby",
            "question": {
                "title": "python - pandas ffill with groupby",
                "ques_desc": "I am trying to forward fill the missing rows to complete the missing time-series rows in the dataset. The size of the dataset is huge. More than 100 million rows. The original source dataset is as shown below. desired output is as below I need to group on and to fill the missing time-series rows in for each of the combinations. Currently, I have the below code which is working but its extremely slow due to the for-loop. Is there any way I can avoid the for-loop and send the whole dataset for creating missing rows and ffill()? Thanks and Appreciate the help. Update: The above code is working but it's too slow. It takes more than 30 minutes for just 300k rows. Hence, I'm looking for help to make it faster and avoid the for-loop. "
            },
            "io": [
                "         col1 col2 col3  col4  col5  col6\n0  2020-01-01   b1   c1     1     9    17\n1  2020-01-05   b1   c1     2    10    18\n2  2020-01-02   b2   c2     3    11    19\n3  2020-01-04   b2   c2     4    12    20\n4  2020-01-10   b3   c3     5    13    21\n5  2020-01-15   b3   c3     6    14    22\n6  2020-01-16   b4   c4     7    15    23\n7  2020-01-30   b4   c4     8    16    24\n\n",
                "         col1 col2 col3  col4  col5  col6\n0  2020-01-01   b1   c1   1.0   9.0  17.0\n1  2020-01-02   b1   c1   1.0   9.0  17.0\n2  2020-01-03   b1   c1   1.0   9.0  17.0\n3  2020-01-04   b1   c1   1.0   9.0  17.0\n4  2020-01-05   b1   c1   2.0  10.0  18.0\n5  2020-01-02   b2   c2   3.0  11.0  19.0\n6  2020-01-03   b2   c2   3.0  11.0  19.0\n7  2020-01-04   b2   c2   4.0  12.0  20.0\n8  2020-01-10   b3   c3   5.0  13.0  21.0\n9  2020-01-11   b3   c3   5.0  13.0  21.0\n10 2020-01-12   b3   c3   5.0  13.0  21.0\n11 2020-01-13   b3   c3   5.0  13.0  21.0\n12 2020-01-14   b3   c3   5.0  13.0  21.0\n13 2020-01-15   b3   c3   6.0  14.0  22.0\n14 2020-01-16   b4   c4   7.0  15.0  23.0\n15 2020-01-17   b4   c4   7.0  15.0  23.0\n16 2020-01-18   b4   c4   7.0  15.0  23.0\n17 2020-01-19   b4   c4   7.0  15.0  23.0\n18 2020-01-20   b4   c4   7.0  15.0  23.0\n19 2020-01-21   b4   c4   7.0  15.0  23.0\n20 2020-01-22   b4   c4   7.0  15.0  23.0\n21 2020-01-23   b4   c4   7.0  15.0  23.0\n22 2020-01-24   b4   c4   7.0  15.0  23.0\n23 2020-01-25   b4   c4   7.0  15.0  23.0\n24 2020-01-26   b4   c4   7.0  15.0  23.0\n25 2020-01-27   b4   c4   7.0  15.0  23.0\n26 2020-01-28   b4   c4   7.0  15.0  23.0\n27 2020-01-29   b4   c4   7.0  15.0  23.0\n28 2020-01-30   b4   c4   8.0  16.0  24.0\n"
            ],
            "answer": {
                "ans_desc": "Looks like a on would work: Output: ",
                "code": [
                    "(df.set_index('col1').groupby(['col2', 'col3'])\n   .resample('D').ffill()\n   .reset_index(['col2','col3'], drop=True)\n   .reset_index()\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1854,
            "user_id": 3991782,
            "user_type": "registered",
            "accept_rate": 92,
            "profile_image": "https://i.stack.imgur.com/cobzl.jpg?s=128&g=1",
            "display_name": "gr1zzly be4r",
            "link": "https://stackoverflow.com/users/3991782/gr1zzly-be4r"
        },
        "is_answered": true,
        "view_count": 83,
        "accepted_answer_id": 62935528,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1594909657,
        "creation_date": 1594904356,
        "last_edit_date": 1594909657,
        "question_id": 62935300,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62935300/pandas-conditional-rolling-sum-of-two-columns",
        "title": "Pandas Conditional Rolling Sum of Two Columns",
        "body": "<p>I have four columns in a data frame like so:</p>\n<pre><code>       A   B     C        D\n75472  d1  x    -36.0   0.0\n75555  d2  x    -38.0   0.0\n75638  d3  x    -18.0   0.0\n75721  d4  x    -18.0   1836.0\n75804  d5  x    1151.0  0.0\n75887  d6  x    734.0   0.0\n75970  d7  x    -723.0  0.0\n</code></pre>\n<p>And I want to conditionally sum <code>D</code> by:</p>\n<ul>\n<li>D has a value, then D</li>\n<li>Else, take value from previous row for D plus C</li>\n</ul>\n<p>So for above, D would be <code>[-36, -74, -92, 1836, 2987, 3721, 2998]</code>.</p>\n<p>I've been able to do this successfully with a for loop</p>\n<pre><code>for i, row in me.iterrows():\n    try:\n        if row['D'] &gt; 0:\n            step1 = me.loc[(me['B'] == row['B']) &amp; (me['A'] == row['A']), 'output'].iloc[0]\n            me_copy.iloc[i, me_copy.columns.get_loc('output')] = step1\n        else:\n            step1 = me.loc[(me['B'] == row['B']) &amp; (me['A'] == (row['A'] - pd.DateOffset(days=1))), 'step1'].iloc[0]\n            receipts_adjustments_sales = me.loc[(me['B'] == row['B']) &amp; (me['A'] == row['A']), 'C'].iloc[0]\n            me_copy.iloc[i, me_copy.columns.get_loc('output')] = step1 + receipts_adjustments_sales\n    except:\n        me_copy.iloc[i, me_copy.columns.get_loc('output')] = 0\n</code></pre>\n<p>But the for loop is obviously really expensive, anti-pattern and basically doesn't run over my whole data frame. I'm trying to copy an excel function here that has basically been written over a panel of data, and for the life of me I cannot figure out how to do this with:</p>\n<ul>\n<li><code>pd.Series.shift()</code></li>\n<li><code>pd.Series.rolling()</code></li>\n<li>Simply calculating different column values</li>\n</ul>\n<p>I was attempting to do it with <code>shift()</code> for a while, but I realized that I kept having to make a separate column for each row, and that's why I went with a for loop.</p>\n<p><strong>Generalized to Groups</strong></p>\n<pre><code>df.loc[:, 'A_group'] = df.groupby(['A'])[df['D'] != 0].cumsum()\ndf.loc[:, 'E'] = df['D'].mask(df['D'] == 0).combine_first(df['C'])\ndf.loc[:, 'F'] = me.groupby(['A', 'A_group'])['E'].cumsum()\n</code></pre>\n<p>Thanks to Scott Boston for the help!</p>\n",
        "answer_body": "<p>Here is a way to do it:</p>\n<pre><code>grp = (df['D'] != 0).cumsum()\ndf['D_new'] = df['D'].mask(df['D'] == 0).combine_first(df['C']).groupby(grp).cumsum()\ndf\n</code></pre>\n<p>Output:</p>\n<pre><code>        A  B       C       D   D_new\n75472  d1  x   -36.0     0.0   -36.0\n75555  d2  x   -38.0     0.0   -74.0\n75638  d3  x   -18.0     0.0   -92.0\n75721  d4  x   -18.0  1836.0  1836.0\n75804  d5  x  1151.0     0.0  2987.0\n75887  d6  x   734.0     0.0  3721.0\n75970  d7  x  -723.0     0.0  2998.0\n</code></pre>\n<h3>Details:</h3>\n<p>Create grps to help cumsum. Each group is defined the the appears of a value in 'D' hence you stop cumsum before and pick that value of D and continue cumsum until the next value of 'D'</p>\n<pre><code>grp = (df['D'] != 0).cumsum()\n</code></pre>\n<p>Output:</p>\n<pre><code>        A  B       C       D   D_new  grp\n75472  d1  x   -36.0     0.0   -36.0    0\n75555  d2  x   -38.0     0.0   -74.0    0\n75638  d3  x   -18.0     0.0   -92.0    0\n75721  d4  x   -18.0  1836.0  1836.0    1\n75804  d5  x  1151.0     0.0  2987.0    1\n75887  d6  x   734.0     0.0  3721.0    1\n75970  d7  x  -723.0     0.0  2998.0    1\n</code></pre>\n<p>Now, Let's create new column combining 'C' and 'D' when D has a nonzero number</p>\n<pre><code>df['newCD'] = df['D'].mask(df['D'] == 0).combine_first(df['C'])\n</code></pre>\n<p>Output:</p>\n<pre><code>        A  B       C       D   D_new  grp   newCD\n75472  d1  x   -36.0     0.0   -36.0    0   -36.0\n75555  d2  x   -38.0     0.0   -74.0    0   -38.0\n75638  d3  x   -18.0     0.0   -92.0    0   -18.0\n75721  d4  x   -18.0  1836.0  1836.0    1  1836.0\n75804  d5  x  1151.0     0.0  2987.0    1  1151.0\n75887  d6  x   734.0     0.0  3721.0    1   734.0\n75970  d7  x  -723.0     0.0  2998.0    1  -723.0\n</code></pre>\n<p>And, lastly, groupby 'grp' and <code>cumsum</code> newCD:</p>\n<pre><code>df['D_new_Details'] = df.groupby('grp')['newCD'].cumsum()\n</code></pre>\n<p>Output:</p>\n<pre><code>        A  B       C       D   D_new  grp   newCD  D_new_Details\n75472  d1  x   -36.0     0.0   -36.0    0   -36.0          -36.0\n75555  d2  x   -38.0     0.0   -74.0    0   -38.0          -74.0\n75638  d3  x   -18.0     0.0   -92.0    0   -18.0          -92.0\n75721  d4  x   -18.0  1836.0  1836.0    1  1836.0         1836.0\n75804  d5  x  1151.0     0.0  2987.0    1  1151.0         2987.0\n75887  d6  x   734.0     0.0  3721.0    1   734.0         3721.0\n75970  d7  x  -723.0     0.0  2998.0    1  -723.0         2998.0\n</code></pre>\n",
        "question_body": "<p>I have four columns in a data frame like so:</p>\n<pre><code>       A   B     C        D\n75472  d1  x    -36.0   0.0\n75555  d2  x    -38.0   0.0\n75638  d3  x    -18.0   0.0\n75721  d4  x    -18.0   1836.0\n75804  d5  x    1151.0  0.0\n75887  d6  x    734.0   0.0\n75970  d7  x    -723.0  0.0\n</code></pre>\n<p>And I want to conditionally sum <code>D</code> by:</p>\n<ul>\n<li>D has a value, then D</li>\n<li>Else, take value from previous row for D plus C</li>\n</ul>\n<p>So for above, D would be <code>[-36, -74, -92, 1836, 2987, 3721, 2998]</code>.</p>\n<p>I've been able to do this successfully with a for loop</p>\n<pre><code>for i, row in me.iterrows():\n    try:\n        if row['D'] &gt; 0:\n            step1 = me.loc[(me['B'] == row['B']) &amp; (me['A'] == row['A']), 'output'].iloc[0]\n            me_copy.iloc[i, me_copy.columns.get_loc('output')] = step1\n        else:\n            step1 = me.loc[(me['B'] == row['B']) &amp; (me['A'] == (row['A'] - pd.DateOffset(days=1))), 'step1'].iloc[0]\n            receipts_adjustments_sales = me.loc[(me['B'] == row['B']) &amp; (me['A'] == row['A']), 'C'].iloc[0]\n            me_copy.iloc[i, me_copy.columns.get_loc('output')] = step1 + receipts_adjustments_sales\n    except:\n        me_copy.iloc[i, me_copy.columns.get_loc('output')] = 0\n</code></pre>\n<p>But the for loop is obviously really expensive, anti-pattern and basically doesn't run over my whole data frame. I'm trying to copy an excel function here that has basically been written over a panel of data, and for the life of me I cannot figure out how to do this with:</p>\n<ul>\n<li><code>pd.Series.shift()</code></li>\n<li><code>pd.Series.rolling()</code></li>\n<li>Simply calculating different column values</li>\n</ul>\n<p>I was attempting to do it with <code>shift()</code> for a while, but I realized that I kept having to make a separate column for each row, and that's why I went with a for loop.</p>\n<p><strong>Generalized to Groups</strong></p>\n<pre><code>df.loc[:, 'A_group'] = df.groupby(['A'])[df['D'] != 0].cumsum()\ndf.loc[:, 'E'] = df['D'].mask(df['D'] == 0).combine_first(df['C'])\ndf.loc[:, 'F'] = me.groupby(['A', 'A_group'])['E'].cumsum()\n</code></pre>\n<p>Thanks to Scott Boston for the help!</p>\n",
        "formatted_input": {
            "qid": 62935300,
            "link": "https://stackoverflow.com/questions/62935300/pandas-conditional-rolling-sum-of-two-columns",
            "question": {
                "title": "Pandas Conditional Rolling Sum of Two Columns",
                "ques_desc": "I have four columns in a data frame like so: And I want to conditionally sum by: D has a value, then D Else, take value from previous row for D plus C So for above, D would be . I've been able to do this successfully with a for loop But the for loop is obviously really expensive, anti-pattern and basically doesn't run over my whole data frame. I'm trying to copy an excel function here that has basically been written over a panel of data, and for the life of me I cannot figure out how to do this with: Simply calculating different column values I was attempting to do it with for a while, but I realized that I kept having to make a separate column for each row, and that's why I went with a for loop. Generalized to Groups Thanks to Scott Boston for the help! "
            },
            "io": [
                "       A   B     C        D\n75472  d1  x    -36.0   0.0\n75555  d2  x    -38.0   0.0\n75638  d3  x    -18.0   0.0\n75721  d4  x    -18.0   1836.0\n75804  d5  x    1151.0  0.0\n75887  d6  x    734.0   0.0\n75970  d7  x    -723.0  0.0\n",
                "[-36, -74, -92, 1836, 2987, 3721, 2998]"
            ],
            "answer": {
                "ans_desc": "Here is a way to do it: Output: Details: Create grps to help cumsum. Each group is defined the the appears of a value in 'D' hence you stop cumsum before and pick that value of D and continue cumsum until the next value of 'D' Output: Now, Let's create new column combining 'C' and 'D' when D has a nonzero number Output: And, lastly, groupby 'grp' and newCD: Output: ",
                "code": [
                    "grp = (df['D'] != 0).cumsum()\ndf['D_new'] = df['D'].mask(df['D'] == 0).combine_first(df['C']).groupby(grp).cumsum()\ndf\n",
                    "grp = (df['D'] != 0).cumsum()\n",
                    "df['newCD'] = df['D'].mask(df['D'] == 0).combine_first(df['C'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 5,
            "user_id": 9945004,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-FXsDlcMyZ7s/AAAAAAAAAAI/AAAAAAAAAAA/AB6qoq2ER1TfDRbPnuuduK1x18qlQN0e4g/mo/photo.jpg?sz=128",
            "display_name": "Manan Raval",
            "link": "https://stackoverflow.com/users/9945004/manan-raval"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 62935801,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1594906396,
        "creation_date": 1594904884,
        "last_edit_date": 1594905114,
        "question_id": 62935487,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62935487/trying-to-append-the-sum-of-an-existing-dataframe-into-a-new-excel-sheet",
        "title": "Trying to append the sum of an existing dataframe into a new excel sheet",
        "body": "<p>So I have been trying to append the <code>df.sum()</code> of an existing dataframe into a new dataframe for a new <code>excel.Consider</code> the below example:</p>\n<pre><code>A   B     C     \n10  10   10\n10  10   10\n10  10   10\n</code></pre>\n<p>I want this to be appended to a new excel as:</p>\n<pre><code>A   B   C\n30  30  30\n</code></pre>\n<p>Following is the code where I am merging the files in a particular location</p>\n<pre><code>all_data = pd.DataFrame()\n\nfor f in glob.glob(&quot;D:\\\\data_integration\\\\*.csv&quot;):\n    print(f)\n    df = pd.read_csv(f ,encoding='iso-8859-1')\n    \n    all_data = all_data.append(df,ignore_index=True)\n    print(all_data)\n    \n    ##appended_df = pd.concat(all_data)\n    y = all_data.to_csv(&quot;D:\\\\merged2.csv&quot;,index=False)\n</code></pre>\n<p>I need help with the summing part.</p>\n",
        "answer_body": "<p>Try</p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame({\n    'A':[10,10,10],\n    'B':[10,10,10],\n    'C':[10,10,10]\n})\n\ndf.sum().to_frame().T\n</code></pre>\n<p>Result:</p>\n<pre><code>  A  B  C\n0 30 30 30\n</code></pre>\n<p>when you do:</p>\n<pre><code>df.sum()\nA    30\nB    30\nC    30\ndtype: int64\n</code></pre>\n<p>then you want to make series to df</p>\n<pre><code>df.sum().to_frame()\n\n0 \nA 30\nB 30\nC 30\n \n</code></pre>\n",
        "question_body": "<p>So I have been trying to append the <code>df.sum()</code> of an existing dataframe into a new dataframe for a new <code>excel.Consider</code> the below example:</p>\n<pre><code>A   B     C     \n10  10   10\n10  10   10\n10  10   10\n</code></pre>\n<p>I want this to be appended to a new excel as:</p>\n<pre><code>A   B   C\n30  30  30\n</code></pre>\n<p>Following is the code where I am merging the files in a particular location</p>\n<pre><code>all_data = pd.DataFrame()\n\nfor f in glob.glob(&quot;D:\\\\data_integration\\\\*.csv&quot;):\n    print(f)\n    df = pd.read_csv(f ,encoding='iso-8859-1')\n    \n    all_data = all_data.append(df,ignore_index=True)\n    print(all_data)\n    \n    ##appended_df = pd.concat(all_data)\n    y = all_data.to_csv(&quot;D:\\\\merged2.csv&quot;,index=False)\n</code></pre>\n<p>I need help with the summing part.</p>\n",
        "formatted_input": {
            "qid": 62935487,
            "link": "https://stackoverflow.com/questions/62935487/trying-to-append-the-sum-of-an-existing-dataframe-into-a-new-excel-sheet",
            "question": {
                "title": "Trying to append the sum of an existing dataframe into a new excel sheet",
                "ques_desc": "So I have been trying to append the of an existing dataframe into a new dataframe for a new the below example: I want this to be appended to a new excel as: Following is the code where I am merging the files in a particular location I need help with the summing part. "
            },
            "io": [
                "A   B     C     \n10  10   10\n10  10   10\n10  10   10\n",
                "A   B   C\n30  30  30\n"
            ],
            "answer": {
                "ans_desc": "Try Result: when you do: then you want to make series to df ",
                "code": [
                    "df.sum()\nA    30\nB    30\nC    30\ndtype: int64\n",
                    "df.sum().to_frame()\n\n0 \nA 30\nB 30\nC 30\n \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 6787880,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/725c834ea1c07c324c4ee8cc07fa2626?s=128&d=identicon&r=PG&f=1",
            "display_name": "olly",
            "link": "https://stackoverflow.com/users/6787880/olly"
        },
        "is_answered": true,
        "view_count": 519,
        "accepted_answer_id": 59649153,
        "answer_count": 3,
        "score": 4,
        "last_activity_date": 1594899363,
        "creation_date": 1578497531,
        "last_edit_date": 1594899266,
        "question_id": 59649084,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59649084/how-to-split-a-dataframe-on-each-different-value-in-a-column",
        "title": "How to split a DataFrame on each different value in a column?",
        "body": "<p>Below is an example DataFrame.</p>\n\n<pre><code>      0      1     2     3          4\n0   0.0  13.00  4.50  30.0   0.0,13.0\n1   0.0  13.00  4.75  30.0   0.0,13.0\n2   0.0  13.00  5.00  30.0   0.0,13.0\n3   0.0  13.00  5.25  30.0   0.0,13.0\n4   0.0  13.00  5.50  30.0   0.0,13.0\n5   0.0  13.00  5.75   0.0   0.0,13.0\n6   0.0  13.00  6.00  30.0   0.0,13.0\n7   1.0  13.25  0.00  30.0  0.0,13.25\n8   1.0  13.25  0.25   0.0  0.0,13.25\n9   1.0  13.25  0.50  30.0  0.0,13.25\n10  1.0  13.25  0.75  30.0  0.0,13.25\n11  2.0  13.25  1.00  30.0  0.0,13.25\n12  2.0  13.25  1.25  30.0  0.0,13.25\n13  2.0  13.25  1.50  30.0  0.0,13.25\n14  2.0  13.25  1.75  30.0  0.0,13.25\n15  2.0  13.25  2.00  30.0  0.0,13.25\n16  2.0  13.25  2.25  30.0  0.0,13.25\n</code></pre>\n\n<p>I want to split this into new dataframes when the row in column 0 changes. </p>\n\n<pre><code>      0      1     2     3          4\n0   0.0  13.00  4.50  30.0   0.0,13.0\n1   0.0  13.00  4.75  30.0   0.0,13.0\n2   0.0  13.00  5.00  30.0   0.0,13.0\n3   0.0  13.00  5.25  30.0   0.0,13.0\n4   0.0  13.00  5.50  30.0   0.0,13.0\n5   0.0  13.00  5.75   0.0   0.0,13.0\n6   0.0  13.00  6.00  30.0   0.0,13.0\n\n7   1.0  13.25  0.00  30.0  0.0,13.25\n8   1.0  13.25  0.25   0.0  0.0,13.25\n9   1.0  13.25  0.50  30.0  0.0,13.25\n10  1.0  13.25  0.75  30.0  0.0,13.25\n\n11  2.0  13.25  1.00  30.0  0.0,13.25\n12  2.0  13.25  1.25  30.0  0.0,13.25\n13  2.0  13.25  1.50  30.0  0.0,13.25\n14  2.0  13.25  1.75  30.0  0.0,13.25\n15  2.0  13.25  2.00  30.0  0.0,13.25\n16  2.0  13.25  2.25  30.0  0.0,13.25\n</code></pre>\n\n<p>I've tried adapting the following solutions without any luck so far. <a href=\"https://stackoverflow.com/questions/5274243/split-array-at-value-in-numpy\">Split array at value in numpy</a>\n<a href=\"https://stackoverflow.com/questions/17315737/split-a-large-pandas-dataframe\">Split a large pandas dataframe</a></p>\n",
        "answer_body": "<p>Looks like you want to <code>groupby</code> the first colum. You could create a dictionary from the groupby object, and have the groupby keys be the dictionary keys:</p>\n<pre><code>out = dict(tuple(df.groupby(0)))\n</code></pre>\n<p>Or we could also build a list from the groupby object. This becomes more useful when we only want <em>positional indexing</em> rather than based on the grouping key:</p>\n<pre><code>out = [sub_df for _, sub_df in df.groupby(0)]\n</code></pre>\n<p>We could then index the dict based on the <em>grouping key</em>, or the list based on the group's position:</p>\n<pre><code>print(out[0])\n\n    0     1     2     3         4\n0  0.0  13.0  4.50  30.0  0.0,13.0\n1  0.0  13.0  4.75  30.0  0.0,13.0\n2  0.0  13.0  5.00  30.0  0.0,13.0\n3  0.0  13.0  5.25  30.0  0.0,13.0\n4  0.0  13.0  5.50  30.0  0.0,13.0\n5  0.0  13.0  5.75   0.0  0.0,13.0\n6  0.0  13.0  6.00  30.0  0.0,13.0\n</code></pre>\n",
        "question_body": "<p>Below is an example DataFrame.</p>\n\n<pre><code>      0      1     2     3          4\n0   0.0  13.00  4.50  30.0   0.0,13.0\n1   0.0  13.00  4.75  30.0   0.0,13.0\n2   0.0  13.00  5.00  30.0   0.0,13.0\n3   0.0  13.00  5.25  30.0   0.0,13.0\n4   0.0  13.00  5.50  30.0   0.0,13.0\n5   0.0  13.00  5.75   0.0   0.0,13.0\n6   0.0  13.00  6.00  30.0   0.0,13.0\n7   1.0  13.25  0.00  30.0  0.0,13.25\n8   1.0  13.25  0.25   0.0  0.0,13.25\n9   1.0  13.25  0.50  30.0  0.0,13.25\n10  1.0  13.25  0.75  30.0  0.0,13.25\n11  2.0  13.25  1.00  30.0  0.0,13.25\n12  2.0  13.25  1.25  30.0  0.0,13.25\n13  2.0  13.25  1.50  30.0  0.0,13.25\n14  2.0  13.25  1.75  30.0  0.0,13.25\n15  2.0  13.25  2.00  30.0  0.0,13.25\n16  2.0  13.25  2.25  30.0  0.0,13.25\n</code></pre>\n\n<p>I want to split this into new dataframes when the row in column 0 changes. </p>\n\n<pre><code>      0      1     2     3          4\n0   0.0  13.00  4.50  30.0   0.0,13.0\n1   0.0  13.00  4.75  30.0   0.0,13.0\n2   0.0  13.00  5.00  30.0   0.0,13.0\n3   0.0  13.00  5.25  30.0   0.0,13.0\n4   0.0  13.00  5.50  30.0   0.0,13.0\n5   0.0  13.00  5.75   0.0   0.0,13.0\n6   0.0  13.00  6.00  30.0   0.0,13.0\n\n7   1.0  13.25  0.00  30.0  0.0,13.25\n8   1.0  13.25  0.25   0.0  0.0,13.25\n9   1.0  13.25  0.50  30.0  0.0,13.25\n10  1.0  13.25  0.75  30.0  0.0,13.25\n\n11  2.0  13.25  1.00  30.0  0.0,13.25\n12  2.0  13.25  1.25  30.0  0.0,13.25\n13  2.0  13.25  1.50  30.0  0.0,13.25\n14  2.0  13.25  1.75  30.0  0.0,13.25\n15  2.0  13.25  2.00  30.0  0.0,13.25\n16  2.0  13.25  2.25  30.0  0.0,13.25\n</code></pre>\n\n<p>I've tried adapting the following solutions without any luck so far. <a href=\"https://stackoverflow.com/questions/5274243/split-array-at-value-in-numpy\">Split array at value in numpy</a>\n<a href=\"https://stackoverflow.com/questions/17315737/split-a-large-pandas-dataframe\">Split a large pandas dataframe</a></p>\n",
        "formatted_input": {
            "qid": 59649084,
            "link": "https://stackoverflow.com/questions/59649084/how-to-split-a-dataframe-on-each-different-value-in-a-column",
            "question": {
                "title": "How to split a DataFrame on each different value in a column?",
                "ques_desc": "Below is an example DataFrame. I want to split this into new dataframes when the row in column 0 changes. I've tried adapting the following solutions without any luck so far. Split array at value in numpy Split a large pandas dataframe "
            },
            "io": [
                "      0      1     2     3          4\n0   0.0  13.00  4.50  30.0   0.0,13.0\n1   0.0  13.00  4.75  30.0   0.0,13.0\n2   0.0  13.00  5.00  30.0   0.0,13.0\n3   0.0  13.00  5.25  30.0   0.0,13.0\n4   0.0  13.00  5.50  30.0   0.0,13.0\n5   0.0  13.00  5.75   0.0   0.0,13.0\n6   0.0  13.00  6.00  30.0   0.0,13.0\n7   1.0  13.25  0.00  30.0  0.0,13.25\n8   1.0  13.25  0.25   0.0  0.0,13.25\n9   1.0  13.25  0.50  30.0  0.0,13.25\n10  1.0  13.25  0.75  30.0  0.0,13.25\n11  2.0  13.25  1.00  30.0  0.0,13.25\n12  2.0  13.25  1.25  30.0  0.0,13.25\n13  2.0  13.25  1.50  30.0  0.0,13.25\n14  2.0  13.25  1.75  30.0  0.0,13.25\n15  2.0  13.25  2.00  30.0  0.0,13.25\n16  2.0  13.25  2.25  30.0  0.0,13.25\n",
                "      0      1     2     3          4\n0   0.0  13.00  4.50  30.0   0.0,13.0\n1   0.0  13.00  4.75  30.0   0.0,13.0\n2   0.0  13.00  5.00  30.0   0.0,13.0\n3   0.0  13.00  5.25  30.0   0.0,13.0\n4   0.0  13.00  5.50  30.0   0.0,13.0\n5   0.0  13.00  5.75   0.0   0.0,13.0\n6   0.0  13.00  6.00  30.0   0.0,13.0\n\n7   1.0  13.25  0.00  30.0  0.0,13.25\n8   1.0  13.25  0.25   0.0  0.0,13.25\n9   1.0  13.25  0.50  30.0  0.0,13.25\n10  1.0  13.25  0.75  30.0  0.0,13.25\n\n11  2.0  13.25  1.00  30.0  0.0,13.25\n12  2.0  13.25  1.25  30.0  0.0,13.25\n13  2.0  13.25  1.50  30.0  0.0,13.25\n14  2.0  13.25  1.75  30.0  0.0,13.25\n15  2.0  13.25  2.00  30.0  0.0,13.25\n16  2.0  13.25  2.25  30.0  0.0,13.25\n"
            ],
            "answer": {
                "ans_desc": "Looks like you want to the first colum. You could create a dictionary from the groupby object, and have the groupby keys be the dictionary keys: Or we could also build a list from the groupby object. This becomes more useful when we only want positional indexing rather than based on the grouping key: We could then index the dict based on the grouping key, or the list based on the group's position: ",
                "code": [
                    "out = [sub_df for _, sub_df in df.groupby(0)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dask"
        ],
        "owner": {
            "reputation": 812,
            "user_id": 6150310,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cb499bc6f5987c406bdb42908a73b666?s=128&d=identicon&r=PG&f=1",
            "display_name": "mlenthusiast",
            "link": "https://stackoverflow.com/users/6150310/mlenthusiast"
        },
        "is_answered": true,
        "view_count": 163,
        "accepted_answer_id": 62906716,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1594874548,
        "creation_date": 1594748580,
        "last_edit_date": 1594771649,
        "question_id": 62900970,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62900970/dask-equivalent-to-pandas-dataframe-update",
        "title": "Dask equivalent to pandas.DataFrame.update",
        "body": "<p>I have a few functions that are using <code>pandas.DataFrame.update</code> method, and I'm trying to move into using <code>Dask</code> instead for the datasets, but the Dask Pandas API doesn't have the <code>update</code> method implemented. Is there an alternative way to get the same result in <code>Dask</code>?</p>\n<p>Here are the methods I have using <code>update</code>:</p>\n<ol>\n<li>Forward fills data with last known value</li>\n</ol>\n<p><code>df.update(df.filter(like='/').mask(lambda x: x == 0).ffill(1))</code></p>\n<p>input</p>\n<pre><code>id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....\n1                      10     20     0      40     0      50\n2                      10     30     30     0      0      50\n.\n.\n</code></pre>\n<p>output</p>\n<pre><code>id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....\n1                      10     20     20     40     40      50\n2                      10     30     30     30     30      50\n.\n.\n</code></pre>\n<ol start=\"2\">\n<li>Replaces values in a dataframe with values from another dataframe based on an id/index column</li>\n</ol>\n<pre><code>def replace_names(df1, df2, idxCol = 'id', srcCol = 'name', dstCol = 'name'):\n    df1 = df1.set_index(idxCol)\n    df1[dstCol].update(df2.set_index(idxCol)[srcCol])\n    return df1.reset_index()\ndf_new = replace_names(df1, df2)\n</code></pre>\n<p>input</p>\n<p>df1</p>\n<pre><code>id    name  ...\n123   city a\n456   city b\n789   city c\n789   city c\n456   city b\n123   city a\n.\n.\n.\n</code></pre>\n<p>df2</p>\n<pre><code>id    name  ...\n123   City A\n456   City B\n789   City C\n.\n.\n.\n</code></pre>\n<p>output</p>\n<pre><code>id    name  ...\n123   City A\n456   City B\n789   City C\n789   City C\n456   City B\n123   City A\n.\n.\n.\n</code></pre>\n",
        "answer_body": "<h1>Question 2</h1>\n<p>There is a way to partially solve this. I'm assuming that <code>df2</code> is much smaller than <code>df1</code> and it actually fit in memory so we can read as pandas dataframe. If this is the case the following function work if <code>df1</code> is a <code>pandas</code> or a <code>dask</code> dataframe but <code>df2</code>should be a <code>pandas</code> one.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport dask.dataframe as dd\n\ndef replace_names(df1, # can be pandas or dask dataframe\n                  df2, # this should be pandas.\n                  idxCol='id',\n                  srcCol='name',\n                  dstCol='name'):\n    diz = df2[[idxCol, srcCol]].set_index(idxCol).to_dict()[srcCol]\n    out = df1.copy()\n    out[dstCol] = out[idxCol].map(diz)\n    return out\n</code></pre>\n<h1>Question 1</h1>\n<p>Regarding the first problem the following code works in <code>pandas</code> and <code>dask</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame({'a': {0: 1, 1: 2},\n 'b': {0: 3, 1: 4},\n '1/1/20': {0: 10, 1: 10},\n '1/2/20': {0: 20, 1: 30},\n '1/3/20': {0: 0, 1: 30},\n '1/4/20': {0: 40, 1: 0},\n '1/5/20': {0: 0, 1: 0},\n '1/6/20': {0: 50, 1: 50}})\n\n# if you want to try with dask\n# df = dd.from_pandas(df, npartitions=2)\n\ncols = [col for col in df.columns if &quot;/&quot; in col]\ndf[cols] = df[cols].mask(lambda x: x==0).ffill(1) #.astype(int)\n</code></pre>\n<p>Remove the comment in the last line if you want the output to be integer.</p>\n<p><strong>UPDATE Question 2</strong>\nIn case you want a <code>dask</code> only solution you could try the following.</p>\n<h2>Data</h2>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\nimport dask.dataframe as dd\n\ndf1 = pd.DataFrame({'id': {0: 123, 1: 456, 2: 789, 3: 789, 4: 456, 5: 123},\n 'name': {0: 'city a',\n  1: 'city b',\n  2: 'city c',\n  3: 'city c',\n  4: 'city b',\n  5: 'city a'}})\n\ndf2 = pd.DataFrame({'id': {0: 123, 1: 456, 2: 789},\n 'name': {0: 'City A', 1: 'City B', 2: 'City C'}})\n\ndf1 = dd.from_pandas(df1, npartitions=2)\ndf2 = dd.from_pandas(df2, npartitions=2)\n</code></pre>\n<h2>Case 1</h2>\n<p>In this case if one <code>id</code> is present in <code>df1</code> but not in <code>df2</code> you keep the name in <code>df1</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    if srcCol == dstCol:\n        df2 = df2.rename(columns={srcCol:f&quot;{srcCol}_new&quot;})\n        srcCol = f&quot;{srcCol}_new&quot;\n    \n    def map_replace(x, srcCol, dstCol):\n        x[dstCol] = np.where(x[srcCol].notnull(),\n                             x[srcCol],\n                             x[dstCol])\n        return x\n    \n    df = dd.merge(df1, df2, on=idxCol, how=&quot;left&quot;)\n    df = df.map_partitions(lambda x: map_replace(x, srcCol, dstCol))\n    df = df.drop(srcCol, axis=1)\n    return df\n\ndf = replace_names_dask(df1, df2)\n</code></pre>\n<h2>Case 2</h2>\n<p>In this case if one <code>id</code> is present in <code>df1</code> but not in <code>df2</code> then <code>name</code> in the output <code>df</code> will be <code>NaN</code> (as in a standard left join)</p>\n<pre class=\"lang-py prettyprint-override\"><code>def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    df1 = df1.drop(dstCol, axis=1)\n    df2 = df2.rename(columns={srcCol: dstCol})\n    df = dd.merge(df1, df2, on=idxCol, how=&quot;left&quot;)\n    return df\n\ndf = replace_names_dask(df1, df2)\n</code></pre>\n",
        "question_body": "<p>I have a few functions that are using <code>pandas.DataFrame.update</code> method, and I'm trying to move into using <code>Dask</code> instead for the datasets, but the Dask Pandas API doesn't have the <code>update</code> method implemented. Is there an alternative way to get the same result in <code>Dask</code>?</p>\n<p>Here are the methods I have using <code>update</code>:</p>\n<ol>\n<li>Forward fills data with last known value</li>\n</ol>\n<p><code>df.update(df.filter(like='/').mask(lambda x: x == 0).ffill(1))</code></p>\n<p>input</p>\n<pre><code>id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....\n1                      10     20     0      40     0      50\n2                      10     30     30     0      0      50\n.\n.\n</code></pre>\n<p>output</p>\n<pre><code>id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....\n1                      10     20     20     40     40      50\n2                      10     30     30     30     30      50\n.\n.\n</code></pre>\n<ol start=\"2\">\n<li>Replaces values in a dataframe with values from another dataframe based on an id/index column</li>\n</ol>\n<pre><code>def replace_names(df1, df2, idxCol = 'id', srcCol = 'name', dstCol = 'name'):\n    df1 = df1.set_index(idxCol)\n    df1[dstCol].update(df2.set_index(idxCol)[srcCol])\n    return df1.reset_index()\ndf_new = replace_names(df1, df2)\n</code></pre>\n<p>input</p>\n<p>df1</p>\n<pre><code>id    name  ...\n123   city a\n456   city b\n789   city c\n789   city c\n456   city b\n123   city a\n.\n.\n.\n</code></pre>\n<p>df2</p>\n<pre><code>id    name  ...\n123   City A\n456   City B\n789   City C\n.\n.\n.\n</code></pre>\n<p>output</p>\n<pre><code>id    name  ...\n123   City A\n456   City B\n789   City C\n789   City C\n456   City B\n123   City A\n.\n.\n.\n</code></pre>\n",
        "formatted_input": {
            "qid": 62900970,
            "link": "https://stackoverflow.com/questions/62900970/dask-equivalent-to-pandas-dataframe-update",
            "question": {
                "title": "Dask equivalent to pandas.DataFrame.update",
                "ques_desc": "I have a few functions that are using method, and I'm trying to move into using instead for the datasets, but the Dask Pandas API doesn't have the method implemented. Is there an alternative way to get the same result in ? Here are the methods I have using : Forward fills data with last known value input output Replaces values in a dataframe with values from another dataframe based on an id/index column input df1 df2 output "
            },
            "io": [
                "id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....\n1                      10     20     0      40     0      50\n2                      10     30     30     0      0      50\n.\n.\n",
                "id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....\n1                      10     20     20     40     40      50\n2                      10     30     30     30     30      50\n.\n.\n"
            ],
            "answer": {
                "ans_desc": "Question 2 There is a way to partially solve this. I'm assuming that is much smaller than and it actually fit in memory so we can read as pandas dataframe. If this is the case the following function work if is a or a dataframe but should be a one. Question 1 Regarding the first problem the following code works in and Remove the comment in the last line if you want the output to be integer. UPDATE Question 2 In case you want a only solution you could try the following. Data Case 1 In this case if one is present in but not in you keep the name in . Case 2 In this case if one is present in but not in then in the output will be (as in a standard left join) ",
                "code": [
                    "import pandas as pd\nimport dask.dataframe as dd\n\ndef replace_names(df1, # can be pandas or dask dataframe\n                  df2, # this should be pandas.\n                  idxCol='id',\n                  srcCol='name',\n                  dstCol='name'):\n    diz = df2[[idxCol, srcCol]].set_index(idxCol).to_dict()[srcCol]\n    out = df1.copy()\n    out[dstCol] = out[idxCol].map(diz)\n    return out\n",
                    "def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    if srcCol == dstCol:\n        df2 = df2.rename(columns={srcCol:f\"{srcCol}_new\"})\n        srcCol = f\"{srcCol}_new\"\n    \n    def map_replace(x, srcCol, dstCol):\n        x[dstCol] = np.where(x[srcCol].notnull(),\n                             x[srcCol],\n                             x[dstCol])\n        return x\n    \n    df = dd.merge(df1, df2, on=idxCol, how=\"left\")\n    df = df.map_partitions(lambda x: map_replace(x, srcCol, dstCol))\n    df = df.drop(srcCol, axis=1)\n    return df\n\ndf = replace_names_dask(df1, df2)\n",
                    "def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    df1 = df1.drop(dstCol, axis=1)\n    df2 = df2.rename(columns={srcCol: dstCol})\n    df = dd.merge(df1, df2, on=idxCol, how=\"left\")\n    return df\n\ndf = replace_names_dask(df1, df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science"
        ],
        "owner": {
            "reputation": 27,
            "user_id": 13403199,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dfba44f1b3cb98ac8d65053ab52994d6?s=128&d=identicon&r=PG&f=1",
            "display_name": "TensorFrozen",
            "link": "https://stackoverflow.com/users/13403199/tensorfrozen"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 62923679,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1594846669,
        "creation_date": 1594846145,
        "question_id": 62923547,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62923547/extract-data-from-certain-columns-and-generate-new-rows",
        "title": "Extract data from certain columns and generate new rows",
        "body": "<p>I have a pandas dataframe</p>\n<pre><code>COL1 COL2 COL3        COL4 \n A    B    C    [{COL5: D1, COL6: E1, COL7: F1},\n                 {COL5: D2, COL6: E2, COL7: F2},\n                 {COL5: D3, COL6: E3, COL7: F3},\n                  ...\n\n                 {COL5: D10, COL6: E10, COL7: F10}]\n</code></pre>\n<p>The output I want will be:</p>\n<pre><code>COL1 COL2 COL3  COL5  COL6  COL7\n A    B    C     D1    E1    F1 \n A    B    C     D2    E2    F2\n A    B    C     D3    E3    F3\n...\n A    B    C     D10   E10   F10\n                  \n</code></pre>\n<p>Is there any convenient way I can use?</p>\n",
        "answer_body": "<p><em><strong>Setup</strong></em></p>\n<pre><code>data = [{&quot;COL5&quot;: f&quot;D{i}&quot;, &quot;COL6&quot;: f&quot;E{i}&quot;, &quot;COL7&quot;: f&quot;F{i}&quot;} for i in range(1, 4)]\ndf = pd.DataFrame({&quot;COL1&quot;: [&quot;A&quot;], &quot;COL2&quot;: [&quot;B&quot;], &quot;COL3&quot;: [&quot;C&quot;], &quot;COL4&quot;: [data]})\n\n#   COL1 COL2 COL3                                               COL4\n# 0    A    B    C  [{'COL5': 'D1', 'COL6': 'E1', 'COL7': 'F1'}, {...\n</code></pre>\n<hr />\n<p><code>explode</code> + <code>join</code></p>\n<pre><code>u = df.explode('COL4').reset_index(drop=True)\nu.iloc[:, :-1].join(pd.DataFrame(u['COL4'].tolist()))\n</code></pre>\n<hr />\n<pre><code>  COL1 COL2 COL3 COL5 COL6 COL7\n0    A    B    C   D1   E1   F1\n1    A    B    C   D2   E2   F2\n2    A    B    C   D3   E3   F3\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe</p>\n<pre><code>COL1 COL2 COL3        COL4 \n A    B    C    [{COL5: D1, COL6: E1, COL7: F1},\n                 {COL5: D2, COL6: E2, COL7: F2},\n                 {COL5: D3, COL6: E3, COL7: F3},\n                  ...\n\n                 {COL5: D10, COL6: E10, COL7: F10}]\n</code></pre>\n<p>The output I want will be:</p>\n<pre><code>COL1 COL2 COL3  COL5  COL6  COL7\n A    B    C     D1    E1    F1 \n A    B    C     D2    E2    F2\n A    B    C     D3    E3    F3\n...\n A    B    C     D10   E10   F10\n                  \n</code></pre>\n<p>Is there any convenient way I can use?</p>\n",
        "formatted_input": {
            "qid": 62923547,
            "link": "https://stackoverflow.com/questions/62923547/extract-data-from-certain-columns-and-generate-new-rows",
            "question": {
                "title": "Extract data from certain columns and generate new rows",
                "ques_desc": "I have a pandas dataframe The output I want will be: Is there any convenient way I can use? "
            },
            "io": [
                "COL1 COL2 COL3        COL4 \n A    B    C    [{COL5: D1, COL6: E1, COL7: F1},\n                 {COL5: D2, COL6: E2, COL7: F2},\n                 {COL5: D3, COL6: E3, COL7: F3},\n                  ...\n\n                 {COL5: D10, COL6: E10, COL7: F10}]\n",
                "COL1 COL2 COL3  COL5  COL6  COL7\n A    B    C     D1    E1    F1 \n A    B    C     D2    E2    F2\n A    B    C     D3    E3    F3\n...\n A    B    C     D10   E10   F10\n                  \n"
            ],
            "answer": {
                "ans_desc": "Setup + ",
                "code": [
                    "u = df.explode('COL4').reset_index(drop=True)\nu.iloc[:, :-1].join(pd.DataFrame(u['COL4'].tolist()))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 403,
            "user_id": 1674908,
            "user_type": "registered",
            "accept_rate": 17,
            "profile_image": "https://www.gravatar.com/avatar/4d73f64ba1212e0b5890968f07739826?s=128&d=identicon&r=PG",
            "display_name": "Hamlett",
            "link": "https://stackoverflow.com/users/1674908/hamlett"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 62165316,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1594821247,
        "creation_date": 1591155332,
        "last_edit_date": 1594821247,
        "question_id": 62164809,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62164809/pandas-multiline-data-schema-to-single-line",
        "title": "Pandas: Multiline data schema to single line",
        "body": "<p>I have a big (huge) dataset that have the next schema:</p>\n\n<pre><code>dt | id | val_t | val\n1  |  1 |     1 | 123\n1  |  1 |     2 | 145\n1  |  1 |     3 | 234\n1  |  2 |     1 | 234\n1  |  2 |     2 | 433\n1  |  2 |     3 | 453\n..................\nN  |  X |     1 | 652\nN  |  X |     2 | 543\nN  |  X |     3 | 324\n</code></pre>\n\n<p>and for many reasons, one of them the to reduce the size, I want to transform it to the next schema:</p>\n\n<pre><code>dt | id | val_1 | val_2 | val_3\n1  |  1 |   123 |  145  |  234\n1  |  2 |   234 |  433  |  453\n..................\nN  |  X |   652 |  543  |  324\n</code></pre>\n\n<p>I tried grouping by ['dt', 'id'] and then iterating over each group to build the new rows but it is too slow. I'm not figuring out a way without iterating over every original row. Any idea?</p>\n",
        "answer_body": "<p>Use, the combination of <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pop.html\" rel=\"nofollow noreferrer\"><code>df.pop</code></a>, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html\" rel=\"nofollow noreferrer\"><code>df.unstack</code></a>, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.droplevel.html\" rel=\"nofollow noreferrer\"><code>MultiIndex.droplevel</code></a>, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename_axis.html\" rel=\"nofollow noreferrer\"><code>df.rename_axis</code></a>:</p>\n\n<pre><code>df['temp'] = 'val_'\ndf['val_t'] = df.pop('temp') + df['val_t'].astype(str)\ndf = df.set_index(['dt', 'id', 'val_t']).unstack()\ndf.columns = df.columns.droplevel()\ndf = df.rename_axis(columns=None).reset_index()\n</code></pre>\n\n<p>Result:</p>\n\n<pre><code># print(df)\n  dt id  val_1  val_2  val_3\n0  1  1    123    145    234\n1  1  2    234    433    453\n2  N  X    652    543    324\n</code></pre>\n",
        "question_body": "<p>I have a big (huge) dataset that have the next schema:</p>\n\n<pre><code>dt | id | val_t | val\n1  |  1 |     1 | 123\n1  |  1 |     2 | 145\n1  |  1 |     3 | 234\n1  |  2 |     1 | 234\n1  |  2 |     2 | 433\n1  |  2 |     3 | 453\n..................\nN  |  X |     1 | 652\nN  |  X |     2 | 543\nN  |  X |     3 | 324\n</code></pre>\n\n<p>and for many reasons, one of them the to reduce the size, I want to transform it to the next schema:</p>\n\n<pre><code>dt | id | val_1 | val_2 | val_3\n1  |  1 |   123 |  145  |  234\n1  |  2 |   234 |  433  |  453\n..................\nN  |  X |   652 |  543  |  324\n</code></pre>\n\n<p>I tried grouping by ['dt', 'id'] and then iterating over each group to build the new rows but it is too slow. I'm not figuring out a way without iterating over every original row. Any idea?</p>\n",
        "formatted_input": {
            "qid": 62164809,
            "link": "https://stackoverflow.com/questions/62164809/pandas-multiline-data-schema-to-single-line",
            "question": {
                "title": "Pandas: Multiline data schema to single line",
                "ques_desc": "I have a big (huge) dataset that have the next schema: and for many reasons, one of them the to reduce the size, I want to transform it to the next schema: I tried grouping by ['dt', 'id'] and then iterating over each group to build the new rows but it is too slow. I'm not figuring out a way without iterating over every original row. Any idea? "
            },
            "io": [
                "dt | id | val_t | val\n1  |  1 |     1 | 123\n1  |  1 |     2 | 145\n1  |  1 |     3 | 234\n1  |  2 |     1 | 234\n1  |  2 |     2 | 433\n1  |  2 |     3 | 453\n..................\nN  |  X |     1 | 652\nN  |  X |     2 | 543\nN  |  X |     3 | 324\n",
                "dt | id | val_1 | val_2 | val_3\n1  |  1 |   123 |  145  |  234\n1  |  2 |   234 |  433  |  453\n..................\nN  |  X |   652 |  543  |  324\n"
            ],
            "answer": {
                "ans_desc": "Use, the combination of , , , : Result: ",
                "code": [
                    "df['temp'] = 'val_'\ndf['val_t'] = df.pop('temp') + df['val_t'].astype(str)\ndf = df.set_index(['dt', 'id', 'val_t']).unstack()\ndf.columns = df.columns.droplevel()\ndf = df.rename_axis(columns=None).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 29,
            "user_id": 13935104,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/72c1c42b934af9021069078e0741190b?s=128&d=identicon&r=PG&f=1",
            "display_name": "mathque33",
            "link": "https://stackoverflow.com/users/13935104/mathque33"
        },
        "is_answered": true,
        "view_count": 134,
        "accepted_answer_id": 62913941,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1594814568,
        "creation_date": 1594811019,
        "last_edit_date": 1594814063,
        "question_id": 62913446,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62913446/replace-last-non-nan-value-in-row",
        "title": "Replace last non NaN value in row",
        "body": "<p>I'd like to replace all the last non NaNs in rows in data frame with NaN value. I have 300 rows and 1068 columns in my data frame. and each row have different number of valid values in them padded with NaNs.\nHere is an example of a row:</p>\n<p>a row in dataframe = <code>[1 2 3 NaN NaN NaN]</code>\noutput = <code>[1 2 NaN NaN NaN NaN]</code></p>\n<p>How to replace last non NaN value in rows in CSV file?</p>\n",
        "answer_body": "<p>Here's a numpy based one:</p>\n<pre><code>import numpy as np\ndf = pd.DataFrame([[1, 2, 3, np.nan, np.nan, np.nan], [1, 2, 3, np.nan, np.nan, 2]])\n</code></pre>\n<p>You can slice the array of values, and get it into reverse order, and look for the first valid value. Then get the indices, and use <code>np.put_along_axis</code> to set them to <code>NaN</code>s:</p>\n<pre><code>a = df.to_numpy()\nm = a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\nnp.put_along_axis(a, m[:,None], np.nan, axis=1)\ndf[:] = a\n</code></pre>\n<hr />\n<pre><code>print(df)\n\n     0    1    2   3   4   5\n0  1.0  2.0  NaN NaN NaN NaN\n1  1.0  2.0  3.0 NaN NaN NaN\n</code></pre>\n<hr />\n<p>Further details -</p>\n<p>The first step is to find where the NaNs are. And since we want the last valid value, we should start from the end. So slice to get the array with the columns reversed, and use <code>np.isnan</code>:</p>\n<pre><code>np.isnan(a[:,::-1])\narray([[ True,  True,  True, False, False, False],\n       [False,  True,  True, False, False, False]])\n</code></pre>\n<p>Now we can find the first <code>False</code>, i.e the <em>last</em> valid value using <code>np.argmax</code>:</p>\n<pre><code>np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([3, 0], dtype=int64)\n</code></pre>\n<p>Now by subtracting the col length to the above we get the actual indices:</p>\n<pre><code>a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([2, 5], dtype=int64)\n</code></pre>\n<p>Now we can just set those indices to <code>NaN</code> in the correspondin indices:</p>\n<pre><code>np.put_along_axis(a, m[:,None], np.nan, axis=1)\n</code></pre>\n",
        "question_body": "<p>I'd like to replace all the last non NaNs in rows in data frame with NaN value. I have 300 rows and 1068 columns in my data frame. and each row have different number of valid values in them padded with NaNs.\nHere is an example of a row:</p>\n<p>a row in dataframe = <code>[1 2 3 NaN NaN NaN]</code>\noutput = <code>[1 2 NaN NaN NaN NaN]</code></p>\n<p>How to replace last non NaN value in rows in CSV file?</p>\n",
        "formatted_input": {
            "qid": 62913446,
            "link": "https://stackoverflow.com/questions/62913446/replace-last-non-nan-value-in-row",
            "question": {
                "title": "Replace last non NaN value in row",
                "ques_desc": "I'd like to replace all the last non NaNs in rows in data frame with NaN value. I have 300 rows and 1068 columns in my data frame. and each row have different number of valid values in them padded with NaNs. Here is an example of a row: a row in dataframe = output = How to replace last non NaN value in rows in CSV file? "
            },
            "io": [
                "[1 2 3 NaN NaN NaN]",
                "[1 2 NaN NaN NaN NaN]"
            ],
            "answer": {
                "ans_desc": "Here's a numpy based one: You can slice the array of values, and get it into reverse order, and look for the first valid value. Then get the indices, and use to set them to s: Further details - The first step is to find where the NaNs are. And since we want the last valid value, we should start from the end. So slice to get the array with the columns reversed, and use : Now we can find the first , i.e the last valid value using : Now by subtracting the col length to the above we get the actual indices: Now we can just set those indices to in the correspondin indices: ",
                "code": [
                    "import numpy as np\ndf = pd.DataFrame([[1, 2, 3, np.nan, np.nan, np.nan], [1, 2, 3, np.nan, np.nan, 2]])\n",
                    "a = df.to_numpy()\nm = a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\nnp.put_along_axis(a, m[:,None], np.nan, axis=1)\ndf[:] = a\n",
                    "np.isnan(a[:,::-1])\narray([[ True,  True,  True, False, False, False],\n       [False,  True,  True, False, False, False]])\n",
                    "np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([3, 0], dtype=int64)\n",
                    "a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([2, 5], dtype=int64)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 11739312,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/893d0f74db999a73e727e21e7affd55f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Madness",
            "link": "https://stackoverflow.com/users/11739312/madness"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 62912915,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1594812751,
        "creation_date": 1594808836,
        "last_edit_date": 1594809182,
        "question_id": 62912823,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62912823/merge-two-columns-of-a-dataframe-into-an-already-existing-column-of-dictionaries",
        "title": "Merge two columns of a dataframe into an already existing column of dictionaries as a key value pair",
        "body": "<p>If we have 3 columns of a dataframe as :</p>\n<pre><code>column1 : ['A','A','B','C']\ncolumn2 : [12,13,14,15]\ncolumn3 : [{&quot;key1&quot;:&quot;val1&quot;},{&quot;key2&quot;:&quot;val2&quot;},{&quot;key3&quot;:&quot;val3&quot;},{&quot;key4&quot;:&quot;val4&quot;}]\n</code></pre>\n<p>I want the column3 to be something like :</p>\n<pre><code>column3 : [{&quot;key1&quot;:&quot;val1&quot;, &quot;A&quot;:12},{&quot;key2&quot;:&quot;val2&quot;, &quot;A&quot;:13},{&quot;key3&quot;:&quot;val3&quot;, &quot;B&quot;:14},{&quot;key4&quot;:&quot;val4&quot;, &quot;C&quot;:15}]\n</code></pre>\n<p>I have tried a  few things from using lambda functions with apply to iterating over rows but all were unsuccessful.</p>\n",
        "answer_body": "<p>You could use a list comprehension and unpacking as:</p>\n<pre><code>df['col3'] = [{**d, k:v} for k,v,d in df.values.tolist()]\n</code></pre>\n<hr />\n<pre><code>print(df)\n\n  col1  col2                       col3\n0    A    12  {'key1': 'val1', 'A': 12}\n1    A    13  {'key2': 'val2', 'A': 13}\n2    B    14  {'key3': 'val3', 'B': 14}\n3    C    15  {'key4': 'val4', 'C': 15}\n</code></pre>\n<hr />\n<p>Input data -</p>\n<pre><code>df = pd.DataFrame({'col1':['A','A','B','C'], \n                   'col2':[12,13,14,15],\n                   'col3':[{&quot;key1&quot;:&quot;val1&quot;},{&quot;key2&quot;:&quot;val2&quot;},\n                           {&quot;key3&quot;:&quot;val3&quot;},{&quot;key4&quot;:&quot;val4&quot;}]})\n</code></pre>\n",
        "question_body": "<p>If we have 3 columns of a dataframe as :</p>\n<pre><code>column1 : ['A','A','B','C']\ncolumn2 : [12,13,14,15]\ncolumn3 : [{&quot;key1&quot;:&quot;val1&quot;},{&quot;key2&quot;:&quot;val2&quot;},{&quot;key3&quot;:&quot;val3&quot;},{&quot;key4&quot;:&quot;val4&quot;}]\n</code></pre>\n<p>I want the column3 to be something like :</p>\n<pre><code>column3 : [{&quot;key1&quot;:&quot;val1&quot;, &quot;A&quot;:12},{&quot;key2&quot;:&quot;val2&quot;, &quot;A&quot;:13},{&quot;key3&quot;:&quot;val3&quot;, &quot;B&quot;:14},{&quot;key4&quot;:&quot;val4&quot;, &quot;C&quot;:15}]\n</code></pre>\n<p>I have tried a  few things from using lambda functions with apply to iterating over rows but all were unsuccessful.</p>\n",
        "formatted_input": {
            "qid": 62912823,
            "link": "https://stackoverflow.com/questions/62912823/merge-two-columns-of-a-dataframe-into-an-already-existing-column-of-dictionaries",
            "question": {
                "title": "Merge two columns of a dataframe into an already existing column of dictionaries as a key value pair",
                "ques_desc": "If we have 3 columns of a dataframe as : I want the column3 to be something like : I have tried a few things from using lambda functions with apply to iterating over rows but all were unsuccessful. "
            },
            "io": [
                "column1 : ['A','A','B','C']\ncolumn2 : [12,13,14,15]\ncolumn3 : [{\"key1\":\"val1\"},{\"key2\":\"val2\"},{\"key3\":\"val3\"},{\"key4\":\"val4\"}]\n",
                "column3 : [{\"key1\":\"val1\", \"A\":12},{\"key2\":\"val2\", \"A\":13},{\"key3\":\"val3\", \"B\":14},{\"key4\":\"val4\", \"C\":15}]\n"
            ],
            "answer": {
                "ans_desc": "You could use a list comprehension and unpacking as: Input data - ",
                "code": [
                    "df['col3'] = [{**d, k:v} for k,v,d in df.values.tolist()]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "intersection"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 7389563,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ed8ccc64a29225a2d0ae45f82ce2580f?s=128&d=identicon&r=PG&f=1",
            "display_name": "msv123",
            "link": "https://stackoverflow.com/users/7389563/msv123"
        },
        "is_answered": true,
        "view_count": 2903,
        "accepted_answer_id": 41529444,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1594809259,
        "creation_date": 1483847998,
        "last_edit_date": 1483849569,
        "question_id": 41529340,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/41529340/find-out-intersection-of-2-pandas-dataframe-according-to-2-columns",
        "title": "Find out intersection of 2 pandas DataFrame according to 2 columns",
        "body": "<p>I would to find out intersection of 2 pandas DataFrame according to 2 columns 'x' and 'y' and combine them into 1 DataFrame. The data are:</p>\n\n<pre><code>df[1]:\n    x   y       id    fa\n0   4   5  9283222   3.1\n1   4   5  9283222   3.1\n2  10  12  9224221   3.2\n3   4   5  9284332   1.2\n4   6   1    51249  11.2\n\ndf[2]:\n    x   y        id   fa\n0   4   5  19283222  1.1\n1   9   3  39224221  5.2\n2  10  12  29284332  6.2\n3   6   1     51242  5.2\n4   6   2     51241  9.2\n5   1   1     51241  9.2\n</code></pre>\n\n<p>The expected output is something like (can ignore index):</p>\n\n<pre><code>    x   y       id    fa\n0   4   5  9283222   3.1\n1   4   5  9283222   3.1\n2  10  12  9224221   3.2\n3   4   5  9284332   1.2\n4   6   1    51249  11.2\n0   4   5  19283222  1.1\n2  10  12  29284332  6.2\n3   6   1     51242  5.2\n</code></pre>\n\n<p>Thank you very much!</p>\n",
        "answer_body": "<p>You can find out the intersection by joining the <code>x,y</code> columns from <code>df1</code> and <code>df2</code>, with which you can filter <code>df1</code> and <code>df2</code> by inner join, and then concatenating the two results with <code>pd.concat</code> should give what you need:</p>\n\n<pre><code>intersection = df1[['x', 'y']].merge(df2[['x', 'y']]).drop_duplicates()\npd.concat([df1.merge(intersection), df2.merge(intersection)])\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/kCoBG.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/kCoBG.jpg\" alt=\"enter image description here\"></a></p>\n",
        "question_body": "<p>I would to find out intersection of 2 pandas DataFrame according to 2 columns 'x' and 'y' and combine them into 1 DataFrame. The data are:</p>\n\n<pre><code>df[1]:\n    x   y       id    fa\n0   4   5  9283222   3.1\n1   4   5  9283222   3.1\n2  10  12  9224221   3.2\n3   4   5  9284332   1.2\n4   6   1    51249  11.2\n\ndf[2]:\n    x   y        id   fa\n0   4   5  19283222  1.1\n1   9   3  39224221  5.2\n2  10  12  29284332  6.2\n3   6   1     51242  5.2\n4   6   2     51241  9.2\n5   1   1     51241  9.2\n</code></pre>\n\n<p>The expected output is something like (can ignore index):</p>\n\n<pre><code>    x   y       id    fa\n0   4   5  9283222   3.1\n1   4   5  9283222   3.1\n2  10  12  9224221   3.2\n3   4   5  9284332   1.2\n4   6   1    51249  11.2\n0   4   5  19283222  1.1\n2  10  12  29284332  6.2\n3   6   1     51242  5.2\n</code></pre>\n\n<p>Thank you very much!</p>\n",
        "formatted_input": {
            "qid": 41529340,
            "link": "https://stackoverflow.com/questions/41529340/find-out-intersection-of-2-pandas-dataframe-according-to-2-columns",
            "question": {
                "title": "Find out intersection of 2 pandas DataFrame according to 2 columns",
                "ques_desc": "I would to find out intersection of 2 pandas DataFrame according to 2 columns 'x' and 'y' and combine them into 1 DataFrame. The data are: The expected output is something like (can ignore index): Thank you very much! "
            },
            "io": [
                "df[1]:\n    x   y       id    fa\n0   4   5  9283222   3.1\n1   4   5  9283222   3.1\n2  10  12  9224221   3.2\n3   4   5  9284332   1.2\n4   6   1    51249  11.2\n\ndf[2]:\n    x   y        id   fa\n0   4   5  19283222  1.1\n1   9   3  39224221  5.2\n2  10  12  29284332  6.2\n3   6   1     51242  5.2\n4   6   2     51241  9.2\n5   1   1     51241  9.2\n",
                "    x   y       id    fa\n0   4   5  9283222   3.1\n1   4   5  9283222   3.1\n2  10  12  9224221   3.2\n3   4   5  9284332   1.2\n4   6   1    51249  11.2\n0   4   5  19283222  1.1\n2  10  12  29284332  6.2\n3   6   1     51242  5.2\n"
            ],
            "answer": {
                "ans_desc": "You can find out the intersection by joining the columns from and , with which you can filter and by inner join, and then concatenating the two results with should give what you need: ",
                "code": [
                    "intersection = df1[['x', 'y']].merge(df2[['x', 'y']]).drop_duplicates()\npd.concat([df1.merge(intersection), df2.merge(intersection)])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 512,
            "user_id": 5570089,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/635d0cb6280c263e663808c0dffb3ff4?s=128&d=identicon&r=PG&f=1",
            "display_name": "Gerry",
            "link": "https://stackoverflow.com/users/5570089/gerry"
        },
        "is_answered": true,
        "view_count": 309,
        "accepted_answer_id": 62882947,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1594713432,
        "creation_date": 1594666378,
        "question_id": 62882426,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62882426/drop-last-n-rows-within-pandas-dataframe-groupby",
        "title": "Drop last n rows within pandas dataframe groupby",
        "body": "<p>I have a dataframe <code>df</code> where I want to drop last <code>n</code> rows within a group of columns. For example, say <code>df</code> is defined as below the group is of columns <code>a</code> and <code>b</code>:</p>\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'a':['abd']*4 + ['pqr']*5 + ['xyz']*7, 'b':['john']*7 + ['doe']*9, 'c':range(16), 'd':range(1000,1016)})\n&gt;&gt;&gt; df\n      a     b   c     d\n0   abd  john   0  1000\n1   abd  john   1  1001\n2   abd  john   2  1002\n3   abd  john   3  1003\n4   pqr  john   4  1004\n5   pqr  john   5  1005\n6   pqr  john   6  1006\n7   pqr   doe   7  1007\n8   pqr   doe   8  1008\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n13  xyz   doe  13  1013\n14  xyz   doe  14  1014\n15  xyz   doe  15  1015\n&gt;&gt;&gt; \n</code></pre>\n<p>Desired output for <code>n=2</code> is as follows:<br></p>\n<pre><code>&gt;&gt;&gt; df\n      a     b   c     d\n0   abd  john   0  1000\n1   abd  john   1  1001\n4   pqr  john   4  1004\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n13  xyz   doe  13  1013\n&gt;&gt;&gt;\n</code></pre>\n<p>Desired output for <code>n=3</code> is as follows:<br></p>\n<pre><code>&gt;&gt;&gt; df\n      a     b   c     d\n0   abd  john   0  1000\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n&gt;&gt;&gt; \n</code></pre>\n",
        "answer_body": "<p>You can use <code>groupby</code> and <code>drop</code> as below:</p>\n<pre><code>n = 2\ndf.drop(df.groupby(['a','b']).tail(n).index, axis=0)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe <code>df</code> where I want to drop last <code>n</code> rows within a group of columns. For example, say <code>df</code> is defined as below the group is of columns <code>a</code> and <code>b</code>:</p>\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'a':['abd']*4 + ['pqr']*5 + ['xyz']*7, 'b':['john']*7 + ['doe']*9, 'c':range(16), 'd':range(1000,1016)})\n&gt;&gt;&gt; df\n      a     b   c     d\n0   abd  john   0  1000\n1   abd  john   1  1001\n2   abd  john   2  1002\n3   abd  john   3  1003\n4   pqr  john   4  1004\n5   pqr  john   5  1005\n6   pqr  john   6  1006\n7   pqr   doe   7  1007\n8   pqr   doe   8  1008\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n13  xyz   doe  13  1013\n14  xyz   doe  14  1014\n15  xyz   doe  15  1015\n&gt;&gt;&gt; \n</code></pre>\n<p>Desired output for <code>n=2</code> is as follows:<br></p>\n<pre><code>&gt;&gt;&gt; df\n      a     b   c     d\n0   abd  john   0  1000\n1   abd  john   1  1001\n4   pqr  john   4  1004\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n13  xyz   doe  13  1013\n&gt;&gt;&gt;\n</code></pre>\n<p>Desired output for <code>n=3</code> is as follows:<br></p>\n<pre><code>&gt;&gt;&gt; df\n      a     b   c     d\n0   abd  john   0  1000\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n&gt;&gt;&gt; \n</code></pre>\n",
        "formatted_input": {
            "qid": 62882426,
            "link": "https://stackoverflow.com/questions/62882426/drop-last-n-rows-within-pandas-dataframe-groupby",
            "question": {
                "title": "Drop last n rows within pandas dataframe groupby",
                "ques_desc": "I have a dataframe where I want to drop last rows within a group of columns. For example, say is defined as below the group is of columns and : Desired output for is as follows: Desired output for is as follows: "
            },
            "io": [
                ">>> df\n      a     b   c     d\n0   abd  john   0  1000\n1   abd  john   1  1001\n4   pqr  john   4  1004\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n13  xyz   doe  13  1013\n>>>\n",
                ">>> df\n      a     b   c     d\n0   abd  john   0  1000\n9   xyz   doe   9  1009\n10  xyz   doe  10  1010\n11  xyz   doe  11  1011\n12  xyz   doe  12  1012\n>>> \n"
            ],
            "answer": {
                "ans_desc": "You can use and as below: ",
                "code": [
                    "n = 2\ndf.drop(df.groupby(['a','b']).tail(n).index, axis=0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 587,
            "user_id": 2627859,
            "user_type": "registered",
            "accept_rate": 63,
            "profile_image": "https://www.gravatar.com/avatar/12ecc5a3bcd14a5db683ae316bb43afd?s=128&d=identicon&r=PG&f=1",
            "display_name": "rshah",
            "link": "https://stackoverflow.com/users/2627859/rshah"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 62875333,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1594708442,
        "creation_date": 1594640967,
        "last_edit_date": 1594645432,
        "question_id": 62875221,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62875221/keep-a-single-element-in-dataframe-of-lists",
        "title": "Keep a single element in dataframe of lists",
        "body": "<p>Given the following dataframe:</p>\n<pre><code>  Movement Distance     Speed   Delay    Loss\n0   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n1   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n2   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n3   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n4   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n</code></pre>\n<p>How can I remove all but the first element in each column and then unlist so the dataframe becomes like this:</p>\n<pre><code>  Movement Distance     Speed   Delay    Loss\n0   1      1            25      0        0\n1   1      1            25      0        0\n2   1      1            25      0        0\n3   1      1            25      0        0\n4   1      1            25      0        0\n</code></pre>\n",
        "answer_body": "<p>You can <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.slice.html\" rel=\"nofollow noreferrer\"><code>str.slice</code></a> or indexing equivalently as:</p>\n<pre><code>df.apply(lambda x: pd.to_numeric(x.str[0], downcast='integer', errors='ignore'))\n</code></pre>\n<p>Or if the data is already clean, we have <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html\" rel=\"nofollow noreferrer\"><code>convert_dtypes</code></a> new in pandas 1.0 (thanks cs95):</p>\n<pre><code>df.apply(lambda x: x.str[0]).convert_dtypes()\n\n   Movement  Distance  Speed  Delay  Loss\n0         1         1     25      0     0\n1         1         1     25      0     0\n2         1         1     25      0     0\n3         1         1     25      0     0\n4         1         1     25      0     0\n</code></pre>\n",
        "question_body": "<p>Given the following dataframe:</p>\n<pre><code>  Movement Distance     Speed   Delay    Loss\n0   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n1   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n2   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n3   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n4   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n</code></pre>\n<p>How can I remove all but the first element in each column and then unlist so the dataframe becomes like this:</p>\n<pre><code>  Movement Distance     Speed   Delay    Loss\n0   1      1            25      0        0\n1   1      1            25      0        0\n2   1      1            25      0        0\n3   1      1            25      0        0\n4   1      1            25      0        0\n</code></pre>\n",
        "formatted_input": {
            "qid": 62875221,
            "link": "https://stackoverflow.com/questions/62875221/keep-a-single-element-in-dataframe-of-lists",
            "question": {
                "title": "Keep a single element in dataframe of lists",
                "ques_desc": "Given the following dataframe: How can I remove all but the first element in each column and then unlist so the dataframe becomes like this: "
            },
            "io": [
                "  Movement Distance     Speed   Delay    Loss\n0   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n1   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n2   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n3   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n4   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n",
                "  Movement Distance     Speed   Delay    Loss\n0   1      1            25      0        0\n1   1      1            25      0        0\n2   1      1            25      0        0\n3   1      1            25      0        0\n4   1      1            25      0        0\n"
            ],
            "answer": {
                "ans_desc": "You can with or indexing equivalently as: Or if the data is already clean, we have new in pandas 1.0 (thanks cs95): ",
                "code": [
                    "df.apply(lambda x: pd.to_numeric(x.str[0], downcast='integer', errors='ignore'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 229,
            "user_id": 4606088,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/db2a9d96173162f85e9e56f363678c28?s=128&d=identicon&r=PG&f=1",
            "display_name": "Thustra",
            "link": "https://stackoverflow.com/users/4606088/thustra"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 62862411,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1594564733,
        "creation_date": 1594563646,
        "last_edit_date": 1594564385,
        "question_id": 62862213,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62862213/pandas-creating-values-in-a-column-based-on-the-previous-value-in-that-column",
        "title": "Pandas: creating values in a column based on the previous value in that column",
        "body": "<p>Quick example:</p>\n<p>Before:</p>\n<pre><code>In  Out    \n1    5    \n10   0    \n2    3\n</code></pre>\n<p>After</p>\n<pre><code>In  Out  Value    \n1    5   -4    \n10   0    6    \n2    3    5\n</code></pre>\n<p>So the formula here is <code>Value(rowx) =  Value (rowx - 1) + In(rowx) - Out(rowx)</code>.</p>\n<p>I started with adding a Value column where each cell is 0. I then have looked a shift() but that uses the value in the previous row from the start of the command/function. So it will always use 0 as the value for Value. Is there a way of doing this without using something like iterrows() or a for loop ?</p>\n",
        "answer_body": "<p>It seems in your calculations you could first calculate <code>In - Out</code> and later use <code>cumsum()</code></p>\n<pre><code>import pandas as pd\n\ndata = {\n    'In': [1,10,2],\n    'Out': [5,0,3]\n}\n\ndf = pd.DataFrame(data)\n\ndf['Value'] = df['In'] - df['Out']\ndf['Value'] = df['Value'].cumsum()\n\nprint(df)\n</code></pre>\n<p>or even shorter</p>\n<pre><code>df['Value'] = (df['In'] - df['Out']).cumsum()\n</code></pre>\n",
        "question_body": "<p>Quick example:</p>\n<p>Before:</p>\n<pre><code>In  Out    \n1    5    \n10   0    \n2    3\n</code></pre>\n<p>After</p>\n<pre><code>In  Out  Value    \n1    5   -4    \n10   0    6    \n2    3    5\n</code></pre>\n<p>So the formula here is <code>Value(rowx) =  Value (rowx - 1) + In(rowx) - Out(rowx)</code>.</p>\n<p>I started with adding a Value column where each cell is 0. I then have looked a shift() but that uses the value in the previous row from the start of the command/function. So it will always use 0 as the value for Value. Is there a way of doing this without using something like iterrows() or a for loop ?</p>\n",
        "formatted_input": {
            "qid": 62862213,
            "link": "https://stackoverflow.com/questions/62862213/pandas-creating-values-in-a-column-based-on-the-previous-value-in-that-column",
            "question": {
                "title": "Pandas: creating values in a column based on the previous value in that column",
                "ques_desc": "Quick example: Before: After So the formula here is . I started with adding a Value column where each cell is 0. I then have looked a shift() but that uses the value in the previous row from the start of the command/function. So it will always use 0 as the value for Value. Is there a way of doing this without using something like iterrows() or a for loop ? "
            },
            "io": [
                "In  Out    \n1    5    \n10   0    \n2    3\n",
                "In  Out  Value    \n1    5   -4    \n10   0    6    \n2    3    5\n"
            ],
            "answer": {
                "ans_desc": "It seems in your calculations you could first calculate and later use or even shorter ",
                "code": [
                    "df['Value'] = (df['In'] - df['Out']).cumsum()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 43,
            "user_id": 13095246,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gg3ayzWQbYLenHu0adupvmaHleFzRNzhi1YBnIOles=k-s128",
            "display_name": "wiziruv",
            "link": "https://stackoverflow.com/users/13095246/wiziruv"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 62849171,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1594474105,
        "creation_date": 1594467278,
        "last_edit_date": 1594471125,
        "question_id": 62848641,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62848641/pandas-merging-rows-dataframe-transformation",
        "title": "Pandas merging rows / Dataframe Transformation",
        "body": "<p>I have this example DataFrame:</p>\n<pre><code>e   col1    col2    col3\n1   238.4   238.7   238.2\n2   238.45  238.75  238.2\n3   238.2   238.25  237.95\n4   238.1   238.15  238.05\n5   238.1   238.1   238\n6   229.1   229.05  229.05\n7   229.35  229.35  229.1\n8   229.1   229.15  229\n9   229.05  229.05  229\n</code></pre>\n<p>How would I be able to convert it to this:</p>\n<pre><code>                1                      2            3   \n    col1    col2    col3    col1    col2    col3    col1    col2    col3\n1   238.4   238.7   238.2   238.45  238.75  238.2   238.2   238.25  237.95\n2   238.1   238.15  238.05  238.1   238.1   238     229.1   229.05  229.05\n3   229.35  229.35  229.1   229.1   229.15  229    229.05   229.05  229\n</code></pre>\n<p>I am thinking maybe I should pivot by counting with lens or assigning a index that could be multiple of 3, but I really am not sure what would be the most efficient way.</p>\n",
        "answer_body": "<p>Create a grouping series <code>g</code>, this we will be needed to group the dataframe so that every third element (taking a step size of 3) belongs to the same group, use <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.unique.html\" rel=\"nofollow noreferrer\"><code>np.unique</code></a> to get the unique grouping keys, next use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>DataFrame.groupby</code></a> to group the dataframe on <code>g</code> and use <code>set_index</code> to  set the index of every grouped frame to <code>k</code>, finally use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>pd.concat</code></a> to concat all the grouped dataframes along <code>axis=1</code> and pass the optional parameter <code>keys=k</code> to create <code>MultiLevel</code> columns\n:</p>\n<pre><code>g, k = df.pop('e').sub(1) % 3 + 1, np.unique(g)\ndf1 = pd.concat([g.set_index(k) for _, g in df.groupby(g)], keys=k, axis=1)\n</code></pre>\n<p>Details:</p>\n<pre><code>print(g.tolist())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\nprint(k)\narray([1, 2, 3])\n</code></pre>\n<p>Result:</p>\n<pre><code>print(df1)\n\n        1                       2                      3                \n     col1    col2    col3    col1    col2   col3    col1    col2    col3\n1  238.40  238.70  238.20  238.45  238.75  238.2  238.20  238.25  237.95\n2  238.10  238.15  238.05  238.10  238.10  238.0  229.10  229.05  229.05\n3  229.35  229.35  229.10  229.10  229.15  229.0  229.05  229.05  229.00\n</code></pre>\n",
        "question_body": "<p>I have this example DataFrame:</p>\n<pre><code>e   col1    col2    col3\n1   238.4   238.7   238.2\n2   238.45  238.75  238.2\n3   238.2   238.25  237.95\n4   238.1   238.15  238.05\n5   238.1   238.1   238\n6   229.1   229.05  229.05\n7   229.35  229.35  229.1\n8   229.1   229.15  229\n9   229.05  229.05  229\n</code></pre>\n<p>How would I be able to convert it to this:</p>\n<pre><code>                1                      2            3   \n    col1    col2    col3    col1    col2    col3    col1    col2    col3\n1   238.4   238.7   238.2   238.45  238.75  238.2   238.2   238.25  237.95\n2   238.1   238.15  238.05  238.1   238.1   238     229.1   229.05  229.05\n3   229.35  229.35  229.1   229.1   229.15  229    229.05   229.05  229\n</code></pre>\n<p>I am thinking maybe I should pivot by counting with lens or assigning a index that could be multiple of 3, but I really am not sure what would be the most efficient way.</p>\n",
        "formatted_input": {
            "qid": 62848641,
            "link": "https://stackoverflow.com/questions/62848641/pandas-merging-rows-dataframe-transformation",
            "question": {
                "title": "Pandas merging rows / Dataframe Transformation",
                "ques_desc": "I have this example DataFrame: How would I be able to convert it to this: I am thinking maybe I should pivot by counting with lens or assigning a index that could be multiple of 3, but I really am not sure what would be the most efficient way. "
            },
            "io": [
                "e   col1    col2    col3\n1   238.4   238.7   238.2\n2   238.45  238.75  238.2\n3   238.2   238.25  237.95\n4   238.1   238.15  238.05\n5   238.1   238.1   238\n6   229.1   229.05  229.05\n7   229.35  229.35  229.1\n8   229.1   229.15  229\n9   229.05  229.05  229\n",
                "                1                      2            3   \n    col1    col2    col3    col1    col2    col3    col1    col2    col3\n1   238.4   238.7   238.2   238.45  238.75  238.2   238.2   238.25  237.95\n2   238.1   238.15  238.05  238.1   238.1   238     229.1   229.05  229.05\n3   229.35  229.35  229.1   229.1   229.15  229    229.05   229.05  229\n"
            ],
            "answer": {
                "ans_desc": "Create a grouping series , this we will be needed to group the dataframe so that every third element (taking a step size of 3) belongs to the same group, use to get the unique grouping keys, next use to group the dataframe on and use to set the index of every grouped frame to , finally use to concat all the grouped dataframes along and pass the optional parameter to create columns : Details: Result: ",
                "code": [
                    "g, k = df.pop('e').sub(1) % 3 + 1, np.unique(g)\ndf1 = pd.concat([g.set_index(k) for _, g in df.groupby(g)], keys=k, axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "reshape",
            "data-wrangling"
        ],
        "owner": {
            "reputation": 3340,
            "user_id": 4451315,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/09c02dbbd18039996ae87cfbfe02dd75?s=128&d=identicon&r=PG&f=1",
            "display_name": "ignoring_gravity",
            "link": "https://stackoverflow.com/users/4451315/ignoring-gravity"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 62819231,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1594313887,
        "creation_date": 1594308682,
        "question_id": 62818462,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62818462/turn-column-levels-inside-out",
        "title": "Turn column levels inside-out",
        "body": "<p>I have a pandas DataFrame which looks like this (code to create it is at the bottom of the question):</p>\n<pre class=\"lang-py prettyprint-override\"><code>  col_1 col_2 foo_1       foo_2      \n              col_3 col_4 col_3 col_4\n0     1     4     2     8     5     7\n1     3     1     6     3     8     9\n</code></pre>\n<p>I would like to turn the <code>foo_1</code> and <code>foo_2</code> columns &quot;inside out&quot;, i.e. my expected output is:</p>\n<pre><code>   col_1  col_2                     col_3                     col_4\n0      1      4  {'foo_1': 2, 'foo_2': 5}  {'foo_1': 8, 'foo_2': 7}\n1      3      1  {'foo_1': 6, 'foo_2': 8}  {'foo_1': 3, 'foo_2': 9}\n</code></pre>\n<p>Is there an efficient (i.e. that does not involve writing a python loop that goes through each row one-by-one) way to do this in pandas?</p>\n<hr />\n<p>Code to generate the starting DataFrame:</p>\n<pre><code>import pandas as pd\n\ncols = pd.MultiIndex.from_tuples(\n    [\n        (&quot;col_1&quot;, &quot;&quot;),\n        (&quot;col_2&quot;, &quot;&quot;),\n        (&quot;foo_1&quot;, &quot;col_3&quot;),\n        (&quot;foo_1&quot;, &quot;col_4&quot;),\n        (&quot;foo_2&quot;, &quot;col_3&quot;),\n        (&quot;foo_2&quot;, &quot;col_4&quot;),\n    ]\n)\ndf = pd.DataFrame([[1, 4, 2, 8, 5, 7], [3, 1, 6, 3, 8, 9]], columns=cols)\n</code></pre>\n<p>Code to generate expected output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pd.DataFrame(\n    [\n        {\n            &quot;col_1&quot;: 1,\n            &quot;col_2&quot;: 4,\n            &quot;col_3&quot;: {&quot;foo_1&quot;: 2, &quot;foo_2&quot;: 5},\n            &quot;col_4&quot;: {&quot;foo_1&quot;: 8, &quot;foo_2&quot;: 7},\n        },\n        {\n            &quot;col_1&quot;: 3,\n            &quot;col_2&quot;: 1,\n            &quot;col_3&quot;: {&quot;foo_1&quot;: 6, &quot;foo_2&quot;: 8},\n            &quot;col_4&quot;: {&quot;foo_1&quot;: 3, &quot;foo_2&quot;: 9},\n        },\n    ]\n)\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html\" rel=\"nofollow noreferrer\"><code>DataFrame.filter</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.droplevel.html\" rel=\"nofollow noreferrer\"><code>DataFrame.droplevel</code></a> and aggregate the columns along <code>axis=1</code> using <code>dict</code>, finally use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\" rel=\"nofollow noreferrer\"><code>DataFrame.drop</code></a> to drop the <code>MultiLevel</code> columns:</p>\n<pre><code>df['col_3'] = df.filter(like='col_3').droplevel(1, 1).agg(dict, axis=1)\ndf['col_4'] = df.filter(like='col_4').droplevel(1, 1).agg(dict, axis=1)\n\ndf = df.drop(['foo_1', 'foo_2'], 1).droplevel(1, 1)\n</code></pre>\n<p>Result:</p>\n<pre><code># print(df)\n\n  col_1 col_2                     col_3                     col_4\n0     1     4  {'foo_1': 2, 'foo_2': 5}  {'foo_1': 8, 'foo_2': 7}\n1     3     1  {'foo_1': 6, 'foo_2': 8}  {'foo_1': 3, 'foo_2': 9}\n</code></pre>\n",
        "question_body": "<p>I have a pandas DataFrame which looks like this (code to create it is at the bottom of the question):</p>\n<pre class=\"lang-py prettyprint-override\"><code>  col_1 col_2 foo_1       foo_2      \n              col_3 col_4 col_3 col_4\n0     1     4     2     8     5     7\n1     3     1     6     3     8     9\n</code></pre>\n<p>I would like to turn the <code>foo_1</code> and <code>foo_2</code> columns &quot;inside out&quot;, i.e. my expected output is:</p>\n<pre><code>   col_1  col_2                     col_3                     col_4\n0      1      4  {'foo_1': 2, 'foo_2': 5}  {'foo_1': 8, 'foo_2': 7}\n1      3      1  {'foo_1': 6, 'foo_2': 8}  {'foo_1': 3, 'foo_2': 9}\n</code></pre>\n<p>Is there an efficient (i.e. that does not involve writing a python loop that goes through each row one-by-one) way to do this in pandas?</p>\n<hr />\n<p>Code to generate the starting DataFrame:</p>\n<pre><code>import pandas as pd\n\ncols = pd.MultiIndex.from_tuples(\n    [\n        (&quot;col_1&quot;, &quot;&quot;),\n        (&quot;col_2&quot;, &quot;&quot;),\n        (&quot;foo_1&quot;, &quot;col_3&quot;),\n        (&quot;foo_1&quot;, &quot;col_4&quot;),\n        (&quot;foo_2&quot;, &quot;col_3&quot;),\n        (&quot;foo_2&quot;, &quot;col_4&quot;),\n    ]\n)\ndf = pd.DataFrame([[1, 4, 2, 8, 5, 7], [3, 1, 6, 3, 8, 9]], columns=cols)\n</code></pre>\n<p>Code to generate expected output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pd.DataFrame(\n    [\n        {\n            &quot;col_1&quot;: 1,\n            &quot;col_2&quot;: 4,\n            &quot;col_3&quot;: {&quot;foo_1&quot;: 2, &quot;foo_2&quot;: 5},\n            &quot;col_4&quot;: {&quot;foo_1&quot;: 8, &quot;foo_2&quot;: 7},\n        },\n        {\n            &quot;col_1&quot;: 3,\n            &quot;col_2&quot;: 1,\n            &quot;col_3&quot;: {&quot;foo_1&quot;: 6, &quot;foo_2&quot;: 8},\n            &quot;col_4&quot;: {&quot;foo_1&quot;: 3, &quot;foo_2&quot;: 9},\n        },\n    ]\n)\n</code></pre>\n",
        "formatted_input": {
            "qid": 62818462,
            "link": "https://stackoverflow.com/questions/62818462/turn-column-levels-inside-out",
            "question": {
                "title": "Turn column levels inside-out",
                "ques_desc": "I have a pandas DataFrame which looks like this (code to create it is at the bottom of the question): I would like to turn the and columns \"inside out\", i.e. my expected output is: Is there an efficient (i.e. that does not involve writing a python loop that goes through each row one-by-one) way to do this in pandas? Code to generate the starting DataFrame: Code to generate expected output: "
            },
            "io": [
                "  col_1 col_2 foo_1       foo_2      \n              col_3 col_4 col_3 col_4\n0     1     4     2     8     5     7\n1     3     1     6     3     8     9\n",
                "   col_1  col_2                     col_3                     col_4\n0      1      4  {'foo_1': 2, 'foo_2': 5}  {'foo_1': 8, 'foo_2': 7}\n1      3      1  {'foo_1': 6, 'foo_2': 8}  {'foo_1': 3, 'foo_2': 9}\n"
            ],
            "answer": {
                "ans_desc": "Use + and aggregate the columns along using , finally use to drop the columns: Result: ",
                "code": [
                    "df['col_3'] = df.filter(like='col_3').droplevel(1, 1).agg(dict, axis=1)\ndf['col_4'] = df.filter(like='col_4').droplevel(1, 1).agg(dict, axis=1)\n\ndf = df.drop(['foo_1', 'foo_2'], 1).droplevel(1, 1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 483,
            "user_id": 4172411,
            "user_type": "registered",
            "accept_rate": 61,
            "profile_image": "https://www.gravatar.com/avatar/54d733aa4fd996f14a1c6fddbec6b0ff?s=128&d=identicon&r=PG&f=1",
            "display_name": "jscriptor",
            "link": "https://stackoverflow.com/users/4172411/jscriptor"
        },
        "is_answered": true,
        "view_count": 1084,
        "accepted_answer_id": 56045036,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1594305051,
        "creation_date": 1557330280,
        "last_edit_date": 1594305051,
        "question_id": 56044461,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56044461/position-or-move-pandas-column-to-a-specific-column-index",
        "title": "position or move pandas column to a specific column index",
        "body": "<p>I have a DF <code>mydataframe</code> and it has multiple columns (over 75 columns) with default numeric index:</p>\n<pre><code>Col1 Col2 Col3 ... Coln\n</code></pre>\n<p>I need to arrange/change position to as follows:</p>\n<pre><code>Col1 Col3 Col2 ... Coln \n</code></pre>\n<p>I can get the index of <code>Col2</code> using:</p>\n<pre><code>mydataframe.columns.get_loc(&quot;Col2&quot;)\n</code></pre>\n<p>but I don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list.</p>\n",
        "answer_body": "<p>How to proceed:</p>\n\n<ol>\n<li>store the names of columns in a list;</li>\n<li>swap the names in that list;</li>\n<li>apply the new order on the dataframe.</li>\n</ol>\n\n<p>code:</p>\n\n<pre><code>l = list(df)\n\ni1, i2 = l.index('Col2'), l.index('Col3')\nl[i2], l[i1] = l[i1], l[i2]\n\ndf = df[l]\n</code></pre>\n",
        "question_body": "<p>I have a DF <code>mydataframe</code> and it has multiple columns (over 75 columns) with default numeric index:</p>\n<pre><code>Col1 Col2 Col3 ... Coln\n</code></pre>\n<p>I need to arrange/change position to as follows:</p>\n<pre><code>Col1 Col3 Col2 ... Coln \n</code></pre>\n<p>I can get the index of <code>Col2</code> using:</p>\n<pre><code>mydataframe.columns.get_loc(&quot;Col2&quot;)\n</code></pre>\n<p>but I don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list.</p>\n",
        "formatted_input": {
            "qid": 56044461,
            "link": "https://stackoverflow.com/questions/56044461/position-or-move-pandas-column-to-a-specific-column-index",
            "question": {
                "title": "position or move pandas column to a specific column index",
                "ques_desc": "I have a DF and it has multiple columns (over 75 columns) with default numeric index: I need to arrange/change position to as follows: I can get the index of using: but I don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list. "
            },
            "io": [
                "Col1 Col2 Col3 ... Coln\n",
                "Col1 Col3 Col2 ... Coln \n"
            ],
            "answer": {
                "ans_desc": "How to proceed: store the names of columns in a list; swap the names in that list; apply the new order on the dataframe. code: ",
                "code": [
                    "l = list(df)\n\ni1, i2 = l.index('Col2'), l.index('Col3')\nl[i2], l[i1] = l[i1], l[i2]\n\ndf = df[l]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 372,
            "user_id": 11600323,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2542a03dfeb8d7f36dbf786e0157ae9b?s=128&d=identicon&r=PG&f=1",
            "display_name": "vvk24",
            "link": "https://stackoverflow.com/users/11600323/vvk24"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 62812004,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1594287915,
        "creation_date": 1594287171,
        "question_id": 62811834,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62811834/rename-columns-in-a-pandas-dataframe-with-values-form-dictionary",
        "title": "Rename Columns in a Pandas Dataframe with values form dictionary",
        "body": "<p>I have a pandas data frame read from an excel file. <strong>Note</strong>: the column names remain the same but the position of the column might vary in the excel file.</p>\n<p><strong>df</strong></p>\n<pre><code>    colA    colB    colC   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n</code></pre>\n<p>I have a list of dictionaries that should be used to change the column names, which is as below</p>\n<p><strong>field_map</strong></p>\n<pre><code>    [{&quot;file_field&quot; : &quot;colA&quot; , &quot;table_field&quot; : &quot;tab1&quot;},\n     {&quot;file_field&quot; : &quot;colB&quot; , &quot;table_field&quot; : &quot;tab2&quot;},\n     {&quot;file_field&quot; : &quot;colC&quot; , &quot;table_field&quot; : &quot;tab3&quot;},\n      ... ... ...]\n</code></pre>\n<p>I could convert the column keys for each row in the DataFrame separately in this way and using the <code>new_dt</code> for further operations.</p>\n<pre><code>file_dt = df.to_dict(&quot;records&quot;)\n\nfor each_entry in file_dt:\n    new_dt = {}\n    for field in field_map:      \n        new_dt[field['table_field'] = each_entry[field['file_field']]\n    ... ... ...\n</code></pre>\n<p>This method is taking too long when my file is large.</p>\n<p>I want to change the column headers of the data Frame before processing the entries further, this will reduce a lot of processing time for me. Kindly help me with this. I'm expecting the data frame to be something like this</p>\n<p><strong>Expected df</strong></p>\n<pre><code>    tab1    tab2    tab3   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n</code></pre>\n<p>Thanks in Advance</p>\n",
        "answer_body": "<p>Just use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\" rel=\"nofollow noreferrer\"><code>rename</code></a> function in your existing dataframe <code>df</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.rename(columns={&quot;colA&quot;:&quot;tab1&quot;, &quot;colB&quot;:&quot;tab2&quot;, &quot;colB&quot;:&quot;tab3&quot;})\n</code></pre>\n<p>You would need to modify the <code>field_map</code> dictionary a bit:</p>\n<pre class=\"lang-py prettyprint-override\"><code>col_rename_dict = {el[&quot;file_field&quot;]:el[&quot;table_field&quot;] for el in field_map}\ndf = df.rename(columns=col_rename_dict)\n</code></pre>\n",
        "question_body": "<p>I have a pandas data frame read from an excel file. <strong>Note</strong>: the column names remain the same but the position of the column might vary in the excel file.</p>\n<p><strong>df</strong></p>\n<pre><code>    colA    colB    colC   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n</code></pre>\n<p>I have a list of dictionaries that should be used to change the column names, which is as below</p>\n<p><strong>field_map</strong></p>\n<pre><code>    [{&quot;file_field&quot; : &quot;colA&quot; , &quot;table_field&quot; : &quot;tab1&quot;},\n     {&quot;file_field&quot; : &quot;colB&quot; , &quot;table_field&quot; : &quot;tab2&quot;},\n     {&quot;file_field&quot; : &quot;colC&quot; , &quot;table_field&quot; : &quot;tab3&quot;},\n      ... ... ...]\n</code></pre>\n<p>I could convert the column keys for each row in the DataFrame separately in this way and using the <code>new_dt</code> for further operations.</p>\n<pre><code>file_dt = df.to_dict(&quot;records&quot;)\n\nfor each_entry in file_dt:\n    new_dt = {}\n    for field in field_map:      \n        new_dt[field['table_field'] = each_entry[field['file_field']]\n    ... ... ...\n</code></pre>\n<p>This method is taking too long when my file is large.</p>\n<p>I want to change the column headers of the data Frame before processing the entries further, this will reduce a lot of processing time for me. Kindly help me with this. I'm expecting the data frame to be something like this</p>\n<p><strong>Expected df</strong></p>\n<pre><code>    tab1    tab2    tab3   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n</code></pre>\n<p>Thanks in Advance</p>\n",
        "formatted_input": {
            "qid": 62811834,
            "link": "https://stackoverflow.com/questions/62811834/rename-columns-in-a-pandas-dataframe-with-values-form-dictionary",
            "question": {
                "title": "Rename Columns in a Pandas Dataframe with values form dictionary",
                "ques_desc": "I have a pandas data frame read from an excel file. Note: the column names remain the same but the position of the column might vary in the excel file. df I have a list of dictionaries that should be used to change the column names, which is as below field_map I could convert the column keys for each row in the DataFrame separately in this way and using the for further operations. This method is taking too long when my file is large. I want to change the column headers of the data Frame before processing the entries further, this will reduce a lot of processing time for me. Kindly help me with this. I'm expecting the data frame to be something like this Expected df Thanks in Advance "
            },
            "io": [
                "    colA    colB    colC   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n",
                "    tab1    tab2    tab3   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n"
            ],
            "answer": {
                "ans_desc": "Just use the function in your existing dataframe : You would need to modify the dictionary a bit: ",
                "code": [
                    "df = df.rename(columns={\"colA\":\"tab1\", \"colB\":\"tab2\", \"colB\":\"tab3\"})\n",
                    "col_rename_dict = {el[\"file_field\"]:el[\"table_field\"] for el in field_map}\ndf = df.rename(columns=col_rename_dict)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 274,
        "accepted_answer_id": 62721812,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1593808006,
        "creation_date": 1593804131,
        "question_id": 62721565,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62721565/split-the-data-frame-based-on-consecutive-row-values-differences",
        "title": "Split the data frame based on consecutive row values differences",
        "body": "<p>I have a data frame like this,</p>\n<pre><code>df\ncol1    col2    col3\n 1        2      3\n 2        5      6\n 7        8      9\n10       11     12\n11       12     13\n13       14     15\n14       15     16\n</code></pre>\n<p>Now I want to create multiple data frames from above when the col1 difference of two consecutive rows are more than 1.\nSo the result data frames will look like,</p>\n<pre><code>df1\ncol1    col2    col3\n 1        2      3\n 2        5      6\ndf2\ncol1    col2    col3\n 7        8      9\ndf3\ncol1    col2    col3\n10       11     12\n11       12     13\ndf4\ncol1    col2    col3\n13       14     15\n14       15     16\n</code></pre>\n<p>I can do this using for loop and storing the indices but this will increase execution time, looking for some pandas shortcuts or pythonic way to do this most efficiently.</p>\n",
        "answer_body": "<p>You could define a custom grouper by taking the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\" rel=\"nofollow noreferrer\"><code>diff</code></a>, checking when it is greater than <code>1</code>, and take the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cumsum.html\" rel=\"nofollow noreferrer\"><code>cumsum</code></a> of the boolean series. Then group by the result and build a dictionary from the groupby object:</p>\n<pre><code>d = dict(tuple(df.groupby(df.col1.diff().gt(1).cumsum())))\n\nprint(d[0])\n   col1  col2  col3\n0     1     2     3\n1     2     5     6\n\nprint(d[1])\n   col1  col2  col3\n2     7     8     9\n</code></pre>\n<hr />\n<p>A more detailed break-down:</p>\n<pre><code>df.assign(difference=(diff:=df.col1.diff()), \n          condition=(gt1:=diff.gt(1)), \n          grouper=gt1.cumsum())\n\n   col1  col2  col3  difference  condition  grouper\n0     1     2     3         NaN      False        0\n1     2     5     6         1.0      False        0\n2     7     8     9         5.0       True        1\n3    10    11    12         3.0       True        2\n4    11    12    13         1.0      False        2\n5    13    14    15         2.0       True        3\n6    14    15    16         1.0      False        3\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this,</p>\n<pre><code>df\ncol1    col2    col3\n 1        2      3\n 2        5      6\n 7        8      9\n10       11     12\n11       12     13\n13       14     15\n14       15     16\n</code></pre>\n<p>Now I want to create multiple data frames from above when the col1 difference of two consecutive rows are more than 1.\nSo the result data frames will look like,</p>\n<pre><code>df1\ncol1    col2    col3\n 1        2      3\n 2        5      6\ndf2\ncol1    col2    col3\n 7        8      9\ndf3\ncol1    col2    col3\n10       11     12\n11       12     13\ndf4\ncol1    col2    col3\n13       14     15\n14       15     16\n</code></pre>\n<p>I can do this using for loop and storing the indices but this will increase execution time, looking for some pandas shortcuts or pythonic way to do this most efficiently.</p>\n",
        "formatted_input": {
            "qid": 62721565,
            "link": "https://stackoverflow.com/questions/62721565/split-the-data-frame-based-on-consecutive-row-values-differences",
            "question": {
                "title": "Split the data frame based on consecutive row values differences",
                "ques_desc": "I have a data frame like this, Now I want to create multiple data frames from above when the col1 difference of two consecutive rows are more than 1. So the result data frames will look like, I can do this using for loop and storing the indices but this will increase execution time, looking for some pandas shortcuts or pythonic way to do this most efficiently. "
            },
            "io": [
                "df\ncol1    col2    col3\n 1        2      3\n 2        5      6\n 7        8      9\n10       11     12\n11       12     13\n13       14     15\n14       15     16\n",
                "df1\ncol1    col2    col3\n 1        2      3\n 2        5      6\ndf2\ncol1    col2    col3\n 7        8      9\ndf3\ncol1    col2    col3\n10       11     12\n11       12     13\ndf4\ncol1    col2    col3\n13       14     15\n14       15     16\n"
            ],
            "answer": {
                "ans_desc": "You could define a custom grouper by taking the , checking when it is greater than , and take the of the boolean series. Then group by the result and build a dictionary from the groupby object: A more detailed break-down: ",
                "code": [
                    "df.assign(difference=(diff:=df.col1.diff()), \n          condition=(gt1:=diff.gt(1)), \n          grouper=gt1.cumsum())\n\n   col1  col2  col3  difference  condition  grouper\n0     1     2     3         NaN      False        0\n1     2     5     6         1.0      False        0\n2     7     8     9         5.0       True        1\n3    10    11    12         3.0       True        2\n4    11    12    13         1.0      False        2\n5    13    14    15         2.0       True        3\n6    14    15    16         1.0      False        3\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "multi-index"
        ],
        "owner": {
            "reputation": 551,
            "user_id": 8954691,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/42c149ce2afb68600e3f4d87b8ee20b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Siddhant Tandon",
            "link": "https://stackoverflow.com/users/8954691/siddhant-tandon"
        },
        "is_answered": true,
        "view_count": 187,
        "accepted_answer_id": 62698277,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1593707243,
        "creation_date": 1593697493,
        "last_edit_date": 1593699211,
        "question_id": 62697810,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62697810/replace-specific-values-in-multiindex-dataframe",
        "title": "Replace specific values in multiindex dataframe",
        "body": "<p>I have a multindex dataframe with 3 index levels and 2 numerical columns.</p>\n<pre><code>A   1   2017-04-01  14.0    87.346878\n        2017-06-01  4.0     87.347504\n    2   2014-08-01  1.0     123.110001\n        2015-01-01  4.0     209.612503\nB   3   2014-07-01  1.0     68.540001\n        2014-12-01  1.0     64.370003\n    4   2015-01-01  3.0     75.000000\n</code></pre>\n<p>I want to replace the values in first row of 3rd index level wherever a new second level index begins.\nFor ex: every first row</p>\n<pre><code>(A,1,2017-04-01)-&gt;0.0   0.0 \n(A,2,2014-08-01)-&gt;0.0   0.0  \n(B,3,2014-07-01)-&gt;0.0   0.0  \n(B,4,2015-01-01)-&gt;0.0   0.0\n</code></pre>\n<p>The dataframe is too big and doing it datframe by dataframe like <code>df.xs('A,1')...df.xs(A,2)</code> gets time consuming. Is there some way where i can get a mask and replace with new values in these positions ?</p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.reset_index</code></a> on <code>level=2</code>, then use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>DataFrame.groupby</code></a> on <code>level=[0, 1]</code> and aggregate <code>level_2</code> using <code>first</code>, then using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.from_arrays.html\" rel=\"nofollow noreferrer\"><code>pd.MultiIndex.from_arrays</code></a> create a multilevel index, finally use this <em>multilevel index</em> to change the values in dataframe:</p>\n<pre><code>idx = df.reset_index(level=2).groupby(level=[0, 1])['level_2'].first()\nidx = pd.MultiIndex.from_arrays(idx.reset_index().to_numpy().T)\ndf.loc[idx, :] = 0\n</code></pre>\n<p>Result:</p>\n<pre><code># print(df)\n               col1        col2\nA 1 2017-04-01  0.0    0.000000\n    2017-06-01  4.0   87.347504\n  2 2014-08-01  0.0    0.000000\n    2015-01-01  4.0  209.612503\nB 3 2014-07-01  0.0    0.000000\n    2014-12-01  1.0   64.370003\n  4 2015-01-01  0.0    0.000000\n</code></pre>\n",
        "question_body": "<p>I have a multindex dataframe with 3 index levels and 2 numerical columns.</p>\n<pre><code>A   1   2017-04-01  14.0    87.346878\n        2017-06-01  4.0     87.347504\n    2   2014-08-01  1.0     123.110001\n        2015-01-01  4.0     209.612503\nB   3   2014-07-01  1.0     68.540001\n        2014-12-01  1.0     64.370003\n    4   2015-01-01  3.0     75.000000\n</code></pre>\n<p>I want to replace the values in first row of 3rd index level wherever a new second level index begins.\nFor ex: every first row</p>\n<pre><code>(A,1,2017-04-01)-&gt;0.0   0.0 \n(A,2,2014-08-01)-&gt;0.0   0.0  \n(B,3,2014-07-01)-&gt;0.0   0.0  \n(B,4,2015-01-01)-&gt;0.0   0.0\n</code></pre>\n<p>The dataframe is too big and doing it datframe by dataframe like <code>df.xs('A,1')...df.xs(A,2)</code> gets time consuming. Is there some way where i can get a mask and replace with new values in these positions ?</p>\n",
        "formatted_input": {
            "qid": 62697810,
            "link": "https://stackoverflow.com/questions/62697810/replace-specific-values-in-multiindex-dataframe",
            "question": {
                "title": "Replace specific values in multiindex dataframe",
                "ques_desc": "I have a multindex dataframe with 3 index levels and 2 numerical columns. I want to replace the values in first row of 3rd index level wherever a new second level index begins. For ex: every first row The dataframe is too big and doing it datframe by dataframe like gets time consuming. Is there some way where i can get a mask and replace with new values in these positions ? "
            },
            "io": [
                "A   1   2017-04-01  14.0    87.346878\n        2017-06-01  4.0     87.347504\n    2   2014-08-01  1.0     123.110001\n        2015-01-01  4.0     209.612503\nB   3   2014-07-01  1.0     68.540001\n        2014-12-01  1.0     64.370003\n    4   2015-01-01  3.0     75.000000\n",
                "(A,1,2017-04-01)->0.0   0.0 \n(A,2,2014-08-01)->0.0   0.0  \n(B,3,2014-07-01)->0.0   0.0  \n(B,4,2015-01-01)->0.0   0.0\n"
            ],
            "answer": {
                "ans_desc": "Use on , then use on and aggregate using , then using create a multilevel index, finally use this multilevel index to change the values in dataframe: Result: ",
                "code": [
                    "idx = df.reset_index(level=2).groupby(level=[0, 1])['level_2'].first()\nidx = pd.MultiIndex.from_arrays(idx.reset_index().to_numpy().T)\ndf.loc[idx, :] = 0\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 146,
            "user_id": 7267784,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/52fd24a79165f4d13c2cfc2b3c75576b?s=128&d=identicon&r=PG&f=1",
            "display_name": "Eve Edomenko",
            "link": "https://stackoverflow.com/users/7267784/eve-edomenko"
        },
        "is_answered": true,
        "view_count": 73,
        "accepted_answer_id": 62696315,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1593693881,
        "creation_date": 1593692223,
        "last_edit_date": 1593693384,
        "question_id": 62696143,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62696143/append-list-to-list-in-specific-cell-in-pandas",
        "title": "Append list to list in specific cell in pandas",
        "body": "<p>I have a pandas dataframe, which looks like this:</p>\n<pre><code>   key   arr \na  't1' [1,2]\nb  't2' [3,4]\n</code></pre>\n<p>And I want to append a new list c = [5,6] to the row, where</p>\n<pre><code>row['key'] == 't1'\n</code></pre>\n<p>So the result would be:</p>\n<pre><code>   key   arr \na  't1' [1,2,5,6]\nb  't2' [3,4]\n</code></pre>\n<p>Right now my attempt looks like this:</p>\n<pre><code>df.loc[df['key'] == label]['arr'] = df.loc[df['key'] == 't1']['arr'] + c\n</code></pre>\n<p>Any help is appreciated!</p>\n",
        "answer_body": "<p>First idea is use list comprehension with <code>+</code>:</p>\n<pre><code>import ast\n\nc = [5,6]\n#if necessary\n#df['arr'] = df['arr'].apply(ast.literal_eval)\nm = df['key'] == 't1'\ndf.loc[m, 'arr'] = [x+c for x in df.loc[m, 'arr']]\n\nprint (df)\n  key           arr\na  t1  [1, 2, 5, 6]\nb  t2        [3, 4]\n</code></pre>\n<p>Or you can create new <code>Series</code> with <code>np.repeat</code> and filtered index values by mask and add by <code>+=</code>:</p>\n<pre><code>m = df['key'] == 't1'\n\ndf.loc[m, 'arr'] += pd.Series(np.repeat([c], m.sum(), axis=0).tolist(), index=df.index[m])\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe, which looks like this:</p>\n<pre><code>   key   arr \na  't1' [1,2]\nb  't2' [3,4]\n</code></pre>\n<p>And I want to append a new list c = [5,6] to the row, where</p>\n<pre><code>row['key'] == 't1'\n</code></pre>\n<p>So the result would be:</p>\n<pre><code>   key   arr \na  't1' [1,2,5,6]\nb  't2' [3,4]\n</code></pre>\n<p>Right now my attempt looks like this:</p>\n<pre><code>df.loc[df['key'] == label]['arr'] = df.loc[df['key'] == 't1']['arr'] + c\n</code></pre>\n<p>Any help is appreciated!</p>\n",
        "formatted_input": {
            "qid": 62696143,
            "link": "https://stackoverflow.com/questions/62696143/append-list-to-list-in-specific-cell-in-pandas",
            "question": {
                "title": "Append list to list in specific cell in pandas",
                "ques_desc": "I have a pandas dataframe, which looks like this: And I want to append a new list c = [5,6] to the row, where So the result would be: Right now my attempt looks like this: Any help is appreciated! "
            },
            "io": [
                "   key   arr \na  't1' [1,2]\nb  't2' [3,4]\n",
                "   key   arr \na  't1' [1,2,5,6]\nb  't2' [3,4]\n"
            ],
            "answer": {
                "ans_desc": "First idea is use list comprehension with : Or you can create new with and filtered index values by mask and add by : ",
                "code": [
                    "m = df['key'] == 't1'\n\ndf.loc[m, 'arr'] += pd.Series(np.repeat([c], m.sum(), axis=0).tolist(), index=df.index[m])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 13814998,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/bee57047c5a0dcd0ce018f5246031b0d?s=128&d=identicon&r=PG&f=1",
            "display_name": "onlettinggo",
            "link": "https://stackoverflow.com/users/13814998/onlettinggo"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 62664390,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1593543710,
        "creation_date": 1593542924,
        "question_id": 62664248,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62664248/how-to-select-a-row-by-value-set-it-as-the-header-row-drop-all-the-rows-up-to-th",
        "title": "How to select a row by value-set it as the header row-drop all the rows up to that value in python?",
        "body": "<p>I have a dataframe such as this:</p>\n<pre><code>0  1   2   3   4\n1  a   b   c   d\n2  a   b   c   d\n3  a   b   c   d\n4 gab fob upo tem\n</code></pre>\n<p>What I would like to do is make row 4 the header and drop the rows above it.\nThe key is that the source data is being read from an excel and the format of the report could change, so it may all of the sudden, start looking like this:</p>\n<pre><code>0  1   2   3   4\n1  a   b   c   d\n2 gab fob upo tem\n3  a   b   c   d\n4  a   b   c   d\n</code></pre>\n<p>I know I could do this:</p>\n<pre><code>df.read_excel('filename', skiprows=4, header=1)\n</code></pre>\n<p>However, I need to keep all of this independent of the order of the dataframe because that could change.</p>\n<p>So, I need to select a row with a specific value, make that row the header and then delete/drop all the rows that lead up to that row that I selected if they are left in the dataframe (probably dependent on how the code is written).</p>\n<p>Everything that I've looked up seems to assume the data columns/rows won't change from load to load.</p>\n<p>I hope I worded this well enough to make sense.....</p>\n",
        "answer_body": "<p>If you already know what the headers are, you can search for them:</p>\n<pre><code>df = pd.read_excel('file.xlsx', header=None)\nheaders = [2, 'gab','fob','upo','tem']\n\nstarts = (df==headers).all(1).idxmax()\n\ndf.columns=df.loc[starts]\ndf = df.iloc[starts+1:]\n</code></pre>\n<p>Output:</p>\n<pre><code>1  2 gab fob upo tem\n2  3   a   b   c   d\n3  4   a   b   c   d\n</code></pre>\n",
        "question_body": "<p>I have a dataframe such as this:</p>\n<pre><code>0  1   2   3   4\n1  a   b   c   d\n2  a   b   c   d\n3  a   b   c   d\n4 gab fob upo tem\n</code></pre>\n<p>What I would like to do is make row 4 the header and drop the rows above it.\nThe key is that the source data is being read from an excel and the format of the report could change, so it may all of the sudden, start looking like this:</p>\n<pre><code>0  1   2   3   4\n1  a   b   c   d\n2 gab fob upo tem\n3  a   b   c   d\n4  a   b   c   d\n</code></pre>\n<p>I know I could do this:</p>\n<pre><code>df.read_excel('filename', skiprows=4, header=1)\n</code></pre>\n<p>However, I need to keep all of this independent of the order of the dataframe because that could change.</p>\n<p>So, I need to select a row with a specific value, make that row the header and then delete/drop all the rows that lead up to that row that I selected if they are left in the dataframe (probably dependent on how the code is written).</p>\n<p>Everything that I've looked up seems to assume the data columns/rows won't change from load to load.</p>\n<p>I hope I worded this well enough to make sense.....</p>\n",
        "formatted_input": {
            "qid": 62664248,
            "link": "https://stackoverflow.com/questions/62664248/how-to-select-a-row-by-value-set-it-as-the-header-row-drop-all-the-rows-up-to-th",
            "question": {
                "title": "How to select a row by value-set it as the header row-drop all the rows up to that value in python?",
                "ques_desc": "I have a dataframe such as this: What I would like to do is make row 4 the header and drop the rows above it. The key is that the source data is being read from an excel and the format of the report could change, so it may all of the sudden, start looking like this: I know I could do this: However, I need to keep all of this independent of the order of the dataframe because that could change. So, I need to select a row with a specific value, make that row the header and then delete/drop all the rows that lead up to that row that I selected if they are left in the dataframe (probably dependent on how the code is written). Everything that I've looked up seems to assume the data columns/rows won't change from load to load. I hope I worded this well enough to make sense..... "
            },
            "io": [
                "0  1   2   3   4\n1  a   b   c   d\n2  a   b   c   d\n3  a   b   c   d\n4 gab fob upo tem\n",
                "0  1   2   3   4\n1  a   b   c   d\n2 gab fob upo tem\n3  a   b   c   d\n4  a   b   c   d\n"
            ],
            "answer": {
                "ans_desc": "If you already know what the headers are, you can search for them: Output: ",
                "code": [
                    "df = pd.read_excel('file.xlsx', header=None)\nheaders = [2, 'gab','fob','upo','tem']\n\nstarts = (df==headers).all(1).idxmax()\n\ndf.columns=df.loc[starts]\ndf = df.iloc[starts+1:]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 521,
            "user_id": 12559323,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/18cfdc247d48492a432db262a751a406?s=128&d=identicon&r=PG&f=1",
            "display_name": "idt_tt",
            "link": "https://stackoverflow.com/users/12559323/idt-tt"
        },
        "is_answered": true,
        "view_count": 173,
        "accepted_answer_id": 62611356,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1593307255,
        "creation_date": 1593268292,
        "last_edit_date": 1593307255,
        "question_id": 62611339,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62611339/pandas-multiply-column-value-by-sum-of-group",
        "title": "Pandas: multiply column value by sum of group",
        "body": "<p>I have a dataframe which looks like this:</p>\n<pre><code>   a     b\n0  A  0.15\n1  A  0.25\n2  A  0.10\n3  B  0.20\n4  B  0.10\n5  B  0.25\n6  B  0.60\n7  C  0.50\n8  C  0.70\n</code></pre>\n<p>I want to add a column 'c' which multiplies the value of 'b' by the sum of the group it belongs to in column 'a'. So, first row should be 0.15 * 0.5 (sum of group A) = 0.075. This would be the excel formula for column 'c' =B1*SUMIF($A$1:$A$9,A1,$B$1:$B$9)</p>\n<p>Resulting dataframe should look like this:</p>\n<pre><code>    a   b       c\n0   A   0.15    0.075\n1   A   0.25    0.125\n2   A   0.10    0.05\n3   B   0.20    0.23\n4   B   0.10    0.115\n5   B   0.25    0.2875\n6   B   0.60    0.69\n7   C   0.50    0.6\n8   C   0.70    0.84\n</code></pre>\n",
        "answer_body": "<p>Try groupby + transform and then multiply:</p>\n<pre><code>df['b'] * df.groupby('a')['b'].transform('sum')\n#df['c'] = df['b'] * df.groupby('a')['b'].transform('sum')\n</code></pre>\n<hr />\n<pre><code>0    0.0750\n1    0.1250\n2    0.0500\n3    0.2300\n4    0.1150\n5    0.2875\n6    0.6900\n7    0.6000\n8    0.8400\nName: b, dtype: float64\n</code></pre>\n",
        "question_body": "<p>I have a dataframe which looks like this:</p>\n<pre><code>   a     b\n0  A  0.15\n1  A  0.25\n2  A  0.10\n3  B  0.20\n4  B  0.10\n5  B  0.25\n6  B  0.60\n7  C  0.50\n8  C  0.70\n</code></pre>\n<p>I want to add a column 'c' which multiplies the value of 'b' by the sum of the group it belongs to in column 'a'. So, first row should be 0.15 * 0.5 (sum of group A) = 0.075. This would be the excel formula for column 'c' =B1*SUMIF($A$1:$A$9,A1,$B$1:$B$9)</p>\n<p>Resulting dataframe should look like this:</p>\n<pre><code>    a   b       c\n0   A   0.15    0.075\n1   A   0.25    0.125\n2   A   0.10    0.05\n3   B   0.20    0.23\n4   B   0.10    0.115\n5   B   0.25    0.2875\n6   B   0.60    0.69\n7   C   0.50    0.6\n8   C   0.70    0.84\n</code></pre>\n",
        "formatted_input": {
            "qid": 62611339,
            "link": "https://stackoverflow.com/questions/62611339/pandas-multiply-column-value-by-sum-of-group",
            "question": {
                "title": "Pandas: multiply column value by sum of group",
                "ques_desc": "I have a dataframe which looks like this: I want to add a column 'c' which multiplies the value of 'b' by the sum of the group it belongs to in column 'a'. So, first row should be 0.15 * 0.5 (sum of group A) = 0.075. This would be the excel formula for column 'c' =B1*SUMIF($A$1:$A$9,A1,$B$1:$B$9) Resulting dataframe should look like this: "
            },
            "io": [
                "   a     b\n0  A  0.15\n1  A  0.25\n2  A  0.10\n3  B  0.20\n4  B  0.10\n5  B  0.25\n6  B  0.60\n7  C  0.50\n8  C  0.70\n",
                "    a   b       c\n0   A   0.15    0.075\n1   A   0.25    0.125\n2   A   0.10    0.05\n3   B   0.20    0.23\n4   B   0.10    0.115\n5   B   0.25    0.2875\n6   B   0.60    0.69\n7   C   0.50    0.6\n8   C   0.70    0.84\n"
            ],
            "answer": {
                "ans_desc": "Try groupby + transform and then multiply: ",
                "code": [
                    "df['b'] * df.groupby('a')['b'].transform('sum')\n#df['c'] = df['b'] * df.groupby('a')['b'].transform('sum')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-analysis",
            "data-mapping"
        ],
        "owner": {
            "reputation": 341,
            "user_id": 13765644,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-_fgMIj-YT-E/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclHLDPSUPnvi2O7Q1LbUh50TzhC7A/photo.jpg?sz=128",
            "display_name": "XaviorL",
            "link": "https://stackoverflow.com/users/13765644/xaviorl"
        },
        "is_answered": true,
        "view_count": 155,
        "accepted_answer_id": 62527850,
        "answer_count": 2,
        "score": 5,
        "last_activity_date": 1592966848,
        "creation_date": 1592887418,
        "last_edit_date": 1592966848,
        "question_id": 62527486,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62527486/a-better-way-to-map-data-in-multiple-datasets-with-multiple-data-mapping-rules",
        "title": "A better way to map data in multiple datasets, with multiple data mapping rules",
        "body": "<p>I have three datasets (<code>final_NN</code>, <code>ppt_code</code>, <code>herd_id</code>), and I wish to add a new column called <code>MapValue</code> in <code>final_NN</code> dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes.</p>\n<pre><code>import pandas as pd\n\nfinal_NN = pd.DataFrame({\n    &quot;number&quot;: [123, 456, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;],\n    &quot;ID&quot;: [&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, 799, 813],\n    &quot;code&quot;: [&quot;&quot;, &quot;&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;, &quot;&quot;, &quot;&quot;]\n})\n\nppt_code = pd.DataFrame({\n    &quot;code&quot;: [&quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;],\n    &quot;number&quot;: [11, 11, 22, 22, 33]\n})\n\nherd_id = pd.DataFrame({\n    &quot;ID&quot;: [799, 813],\n    &quot;number&quot;: [678, 789]\n})\n\nnew_column = pd.Series([])\nfor i in range(len(final_NN)):\n    if final_NN[&quot;number&quot;][i] != &quot;&quot; and final_NN[&quot;number&quot;][i] != &quot;Unknown&quot;:\n        new_column[i] = final_NN['number'][i]\n\n    elif final_NN[&quot;code&quot;][i] != &quot;&quot;:\n        for p in range(len(ppt_code)):\n            if ppt_code[&quot;code&quot;][p] == final_NN[&quot;code&quot;][i]:\n                new_column[i] = ppt_code[&quot;number&quot;][p]\n\n    elif final_NN[&quot;ID&quot;][i] != &quot;&quot;:\n        for h in range(len(herd_id)):\n            if herd_id[&quot;ID&quot;][h] == final_NN[&quot;ID&quot;][i]:\n                new_column[i] = herd_id[&quot;number&quot;][h]\n\n    else:\n        new_column[i] = &quot;&quot;\n\nfinal_NN.insert(3, &quot;MapValue&quot;, new_column)\nprint(final_NN)\n</code></pre>\n<p>final_NN:</p>\n<pre><code>    number   ID code\n0      123          \n1      456          \n2  Unknown        AA\n3  Unknown        AA\n4  Unknown        BB\n5  Unknown        BB\n6  Unknown        BB\n7  Unknown        CC\n8  Unknown  799     \n9  Unknown  813 \n</code></pre>\n<p>ppt_code:</p>\n<pre><code>  code  number\n0   AA      11\n1   AA      11\n2   BB      22\n3   BB      22\n4   CC      33\n</code></pre>\n<p>herd_id:</p>\n<pre><code>    ID  number\n0  799     678\n1  813     789\n</code></pre>\n<p>Expected output:</p>\n<pre><code>    number   ID code   MapValue\n0      123                  123\n1      456                  456\n2  Unknown        AA         11\n3  Unknown        AA         11\n4  Unknown        BB         22\n5  Unknown        BB         22\n6  Unknown        BB         22\n7  Unknown        CC         33\n8  Unknown  799             678\n9  Unknown  813             789\n</code></pre>\n<p>The rules is:</p>\n<ol>\n<li>if <code>number</code> in final_NN is not <code>Unknown</code>, <code>MapValue</code> = <code>number</code> in <code>final_NN</code>;</li>\n<li>if <code>number</code> in final_NN is <code>Unknown</code> but <code>code</code> in <code>final_NN</code> is not Null, then search the ppt_code dataframe, and use the <code>code</code> and its corresponding &quot;number&quot; to map and fill in the &quot;MapValue&quot; in <code>final_NN</code>;</li>\n<li>if both <code>number</code> and <code>code</code> in <code>final_NN</code> are <code>Unknown</code> and null respectively, but <code>ID</code> in <code>final_NN</code> is not Null, then search <code>herd_id</code> dataframe, and use the <code>ID</code> and its corresponding <code>number</code> to fill in the <code>MapValue</code> in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result?</li>\n</ol>\n",
        "answer_body": "<p>First create a mapping series from the <code>ppt_code</code> and <code>herd_id</code> dataframes, then use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html\" rel=\"nofollow noreferrer\"><code>Series.replace</code></a> to create a new column <code>MapNumber</code> by replacing the <code>Unknown</code> values in <code>number</code> column with <code>np.NaN</code>, then use two consecutive <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html\" rel=\"nofollow noreferrer\"><code>Series.fillna</code></a> along with <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\" rel=\"nofollow noreferrer\"><code>Series.map</code></a> to fill the missing values in <code>MapNumber</code> column according to the rules:</p>\n<pre><code>ppt_map = ppt_code.drop_duplicates(subset=['code']).set_index('code')['number']\nhrd_map = herd_id.drop_duplicates(subset=['ID']).set_index('ID')['number']\n\nfinal_NN['MapNumber'] = final_NN['number'].replace({'Unknown': np.nan})\nfinal_NN['MapNumber'] = (\n    final_NN['MapNumber']\n    .fillna(final_NN['code'].map(ppt_map))\n    .fillna(final_NN['ID'].map(hrd_map))\n)\n</code></pre>\n<p>Result:</p>\n<pre><code># print(final_NN)\n\n    number   ID code  MapNumber\n0      123                123.0\n1      456                456.0\n2  Unknown        AA       11.0\n3  Unknown        AA       11.0\n4  Unknown        BB       22.0\n5  Unknown        BB       22.0\n6  Unknown        BB       22.0\n7  Unknown        CC       33.0\n8  Unknown  799           678.0\n9  Unknown  813           789.0\n</code></pre>\n",
        "question_body": "<p>I have three datasets (<code>final_NN</code>, <code>ppt_code</code>, <code>herd_id</code>), and I wish to add a new column called <code>MapValue</code> in <code>final_NN</code> dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes.</p>\n<pre><code>import pandas as pd\n\nfinal_NN = pd.DataFrame({\n    &quot;number&quot;: [123, 456, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;],\n    &quot;ID&quot;: [&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, 799, 813],\n    &quot;code&quot;: [&quot;&quot;, &quot;&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;, &quot;&quot;, &quot;&quot;]\n})\n\nppt_code = pd.DataFrame({\n    &quot;code&quot;: [&quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;],\n    &quot;number&quot;: [11, 11, 22, 22, 33]\n})\n\nherd_id = pd.DataFrame({\n    &quot;ID&quot;: [799, 813],\n    &quot;number&quot;: [678, 789]\n})\n\nnew_column = pd.Series([])\nfor i in range(len(final_NN)):\n    if final_NN[&quot;number&quot;][i] != &quot;&quot; and final_NN[&quot;number&quot;][i] != &quot;Unknown&quot;:\n        new_column[i] = final_NN['number'][i]\n\n    elif final_NN[&quot;code&quot;][i] != &quot;&quot;:\n        for p in range(len(ppt_code)):\n            if ppt_code[&quot;code&quot;][p] == final_NN[&quot;code&quot;][i]:\n                new_column[i] = ppt_code[&quot;number&quot;][p]\n\n    elif final_NN[&quot;ID&quot;][i] != &quot;&quot;:\n        for h in range(len(herd_id)):\n            if herd_id[&quot;ID&quot;][h] == final_NN[&quot;ID&quot;][i]:\n                new_column[i] = herd_id[&quot;number&quot;][h]\n\n    else:\n        new_column[i] = &quot;&quot;\n\nfinal_NN.insert(3, &quot;MapValue&quot;, new_column)\nprint(final_NN)\n</code></pre>\n<p>final_NN:</p>\n<pre><code>    number   ID code\n0      123          \n1      456          \n2  Unknown        AA\n3  Unknown        AA\n4  Unknown        BB\n5  Unknown        BB\n6  Unknown        BB\n7  Unknown        CC\n8  Unknown  799     \n9  Unknown  813 \n</code></pre>\n<p>ppt_code:</p>\n<pre><code>  code  number\n0   AA      11\n1   AA      11\n2   BB      22\n3   BB      22\n4   CC      33\n</code></pre>\n<p>herd_id:</p>\n<pre><code>    ID  number\n0  799     678\n1  813     789\n</code></pre>\n<p>Expected output:</p>\n<pre><code>    number   ID code   MapValue\n0      123                  123\n1      456                  456\n2  Unknown        AA         11\n3  Unknown        AA         11\n4  Unknown        BB         22\n5  Unknown        BB         22\n6  Unknown        BB         22\n7  Unknown        CC         33\n8  Unknown  799             678\n9  Unknown  813             789\n</code></pre>\n<p>The rules is:</p>\n<ol>\n<li>if <code>number</code> in final_NN is not <code>Unknown</code>, <code>MapValue</code> = <code>number</code> in <code>final_NN</code>;</li>\n<li>if <code>number</code> in final_NN is <code>Unknown</code> but <code>code</code> in <code>final_NN</code> is not Null, then search the ppt_code dataframe, and use the <code>code</code> and its corresponding &quot;number&quot; to map and fill in the &quot;MapValue&quot; in <code>final_NN</code>;</li>\n<li>if both <code>number</code> and <code>code</code> in <code>final_NN</code> are <code>Unknown</code> and null respectively, but <code>ID</code> in <code>final_NN</code> is not Null, then search <code>herd_id</code> dataframe, and use the <code>ID</code> and its corresponding <code>number</code> to fill in the <code>MapValue</code> in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result?</li>\n</ol>\n",
        "formatted_input": {
            "qid": 62527486,
            "link": "https://stackoverflow.com/questions/62527486/a-better-way-to-map-data-in-multiple-datasets-with-multiple-data-mapping-rules",
            "question": {
                "title": "A better way to map data in multiple datasets, with multiple data mapping rules",
                "ques_desc": "I have three datasets (, , ), and I wish to add a new column called in dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes. final_NN: ppt_code: herd_id: Expected output: The rules is: if in final_NN is not , = in ; if in final_NN is but in is not Null, then search the ppt_code dataframe, and use the and its corresponding \"number\" to map and fill in the \"MapValue\" in ; if both and in are and null respectively, but in is not Null, then search dataframe, and use the and its corresponding to fill in the in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result? "
            },
            "io": [
                "  code  number\n0   AA      11\n1   AA      11\n2   BB      22\n3   BB      22\n4   CC      33\n",
                "    ID  number\n0  799     678\n1  813     789\n"
            ],
            "answer": {
                "ans_desc": "First create a mapping series from the and dataframes, then use to create a new column by replacing the values in column with , then use two consecutive along with to fill the missing values in column according to the rules: Result: ",
                "code": [
                    "ppt_map = ppt_code.drop_duplicates(subset=['code']).set_index('code')['number']\nhrd_map = herd_id.drop_duplicates(subset=['ID']).set_index('ID')['number']\n\nfinal_NN['MapNumber'] = final_NN['number'].replace({'Unknown': np.nan})\nfinal_NN['MapNumber'] = (\n    final_NN['MapNumber']\n    .fillna(final_NN['code'].map(ppt_map))\n    .fillna(final_NN['ID'].map(hrd_map))\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization"
        ],
        "owner": {
            "reputation": 35,
            "user_id": 10291823,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a389c14b21d4e18689aac2deadce9f12?s=128&d=identicon&r=PG&f=1",
            "display_name": "Andi",
            "link": "https://stackoverflow.com/users/10291823/andi"
        },
        "is_answered": true,
        "view_count": 61,
        "accepted_answer_id": 62537252,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1592923184,
        "creation_date": 1592919456,
        "last_edit_date": 1592923184,
        "question_id": 62536086,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62536086/vectorizing-for-loop",
        "title": "Vectorizing for-loop",
        "body": "<p>I have a very large dataframe (~10^8 rows) where I need to change some values. The algorithm I use is complex so I tried to break down the issue into a simple example below. I mostly programmed in C++, so I keep thinking in for-loops. I know I should vectorize but I am new to python and very new to pandas and cannot come up with a better solution. Any solutions which increase performance are welcome.</p>\n<pre><code>#!/usr/bin/python3\n\n\nimport numpy as np\nimport pandas as pd\n\ndata = {'eventID': [1, 1, 1, 2, 2, 3, 4, 5, 6, 6, 6, 6, 7, 8],\n        'types':    [0, -1, -1, -1, 1, 0, 0, 0, -1, -1, -1, 1, -1, -1]\n        }\n\n\nmydf = pd.DataFrame(data, columns=['eventID', 'types'])\nprint(mydf)\n\nMyIntegerCodes = np.array([0, 1])\neventIDs = np.unique(mydf.eventID.values)  # can be up to 10^8 values\n\nfor val in eventIDs:\n\n    currentTypes = mydf[mydf.eventID == val].types.values\n\n    if (0 in currentTypes) &amp; ~(1 in currentTypes):\n        mydf.loc[mydf.eventID == val, 'types'] = 0\n\n    if ~(0 in currentTypes) &amp; (1 in currentTypes):\n        mydf.loc[mydf.eventID == val, 'types'] = 1\n\n\nprint(mydf)\n</code></pre>\n<p>Any ideas?</p>\n<p>EDIT: I was ask to explain what I do with my for-loops.\nFor every eventID I want to know if all corresponding types contain a 1 or a 0 or both. If they contain a 1, all values which are equal to -1 should be changed to 1. If the values are 0, all values equal to -1 should be changed to 0. My problem is to do this efficiently for each eventID independently. There can be one or multiple entries per eventID.</p>\n<p>Input of example:</p>\n<pre><code>    eventID  types\n0         1      0\n1         1     -1\n2         1     -1\n3         2     -1\n4         2      1\n5         3      0\n6         4      0\n7         5      0\n8         6     -1\n9         6     -1\n10        6     -1\n11        6      1\n12        7     -1\n13        8     -1\n</code></pre>\n<p>Output of example:</p>\n<pre><code>    eventID  types\n0         1      0\n1         1      0\n2         1      0\n3         2      1\n4         2      1\n5         3      0\n6         4      0\n7         5      0\n8         6      1\n9         6      1\n10        6      1\n11        6      1\n12        7     -1\n13        8     -1\n</code></pre>\n",
        "answer_body": "<p>First we create boolean masks <code>m1</code> and <code>m2</code> using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html\" rel=\"nofollow noreferrer\"><code>Series.eq</code></a> then use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>DataFrame.groupby</code></a> on this mask and transform using <code>any</code>, then using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html\" rel=\"nofollow noreferrer\"><code>np.select</code></a> chose the elements from <code>1, 0</code> depending upon the conditions <code>m1 or m2</code>:</p>\n<pre><code>m1 = mydf['types'].eq(1).groupby(mydf['eventID']).transform('any')\nm2 = mydf['types'].eq(0).groupby(mydf['eventID']).transform('any')\nmydf['types'] = np.select([m1 , m2], [1, 0], mydf['types'])\n</code></pre>\n<p>Result:</p>\n<pre><code># print(mydf)\n\n    eventID  types\n0         1      0\n1         1      0\n2         1      0\n3         2      1\n4         2      1\n5         3      0\n6         4      0\n7         5      0\n8         6      1\n9         6      1\n10        6      1\n11        6      1\n12        7     -1\n13        8     -1\n</code></pre>\n",
        "question_body": "<p>I have a very large dataframe (~10^8 rows) where I need to change some values. The algorithm I use is complex so I tried to break down the issue into a simple example below. I mostly programmed in C++, so I keep thinking in for-loops. I know I should vectorize but I am new to python and very new to pandas and cannot come up with a better solution. Any solutions which increase performance are welcome.</p>\n<pre><code>#!/usr/bin/python3\n\n\nimport numpy as np\nimport pandas as pd\n\ndata = {'eventID': [1, 1, 1, 2, 2, 3, 4, 5, 6, 6, 6, 6, 7, 8],\n        'types':    [0, -1, -1, -1, 1, 0, 0, 0, -1, -1, -1, 1, -1, -1]\n        }\n\n\nmydf = pd.DataFrame(data, columns=['eventID', 'types'])\nprint(mydf)\n\nMyIntegerCodes = np.array([0, 1])\neventIDs = np.unique(mydf.eventID.values)  # can be up to 10^8 values\n\nfor val in eventIDs:\n\n    currentTypes = mydf[mydf.eventID == val].types.values\n\n    if (0 in currentTypes) &amp; ~(1 in currentTypes):\n        mydf.loc[mydf.eventID == val, 'types'] = 0\n\n    if ~(0 in currentTypes) &amp; (1 in currentTypes):\n        mydf.loc[mydf.eventID == val, 'types'] = 1\n\n\nprint(mydf)\n</code></pre>\n<p>Any ideas?</p>\n<p>EDIT: I was ask to explain what I do with my for-loops.\nFor every eventID I want to know if all corresponding types contain a 1 or a 0 or both. If they contain a 1, all values which are equal to -1 should be changed to 1. If the values are 0, all values equal to -1 should be changed to 0. My problem is to do this efficiently for each eventID independently. There can be one or multiple entries per eventID.</p>\n<p>Input of example:</p>\n<pre><code>    eventID  types\n0         1      0\n1         1     -1\n2         1     -1\n3         2     -1\n4         2      1\n5         3      0\n6         4      0\n7         5      0\n8         6     -1\n9         6     -1\n10        6     -1\n11        6      1\n12        7     -1\n13        8     -1\n</code></pre>\n<p>Output of example:</p>\n<pre><code>    eventID  types\n0         1      0\n1         1      0\n2         1      0\n3         2      1\n4         2      1\n5         3      0\n6         4      0\n7         5      0\n8         6      1\n9         6      1\n10        6      1\n11        6      1\n12        7     -1\n13        8     -1\n</code></pre>\n",
        "formatted_input": {
            "qid": 62536086,
            "link": "https://stackoverflow.com/questions/62536086/vectorizing-for-loop",
            "question": {
                "title": "Vectorizing for-loop",
                "ques_desc": "I have a very large dataframe (~10^8 rows) where I need to change some values. The algorithm I use is complex so I tried to break down the issue into a simple example below. I mostly programmed in C++, so I keep thinking in for-loops. I know I should vectorize but I am new to python and very new to pandas and cannot come up with a better solution. Any solutions which increase performance are welcome. Any ideas? EDIT: I was ask to explain what I do with my for-loops. For every eventID I want to know if all corresponding types contain a 1 or a 0 or both. If they contain a 1, all values which are equal to -1 should be changed to 1. If the values are 0, all values equal to -1 should be changed to 0. My problem is to do this efficiently for each eventID independently. There can be one or multiple entries per eventID. Input of example: Output of example: "
            },
            "io": [
                "    eventID  types\n0         1      0\n1         1     -1\n2         1     -1\n3         2     -1\n4         2      1\n5         3      0\n6         4      0\n7         5      0\n8         6     -1\n9         6     -1\n10        6     -1\n11        6      1\n12        7     -1\n13        8     -1\n",
                "    eventID  types\n0         1      0\n1         1      0\n2         1      0\n3         2      1\n4         2      1\n5         3      0\n6         4      0\n7         5      0\n8         6      1\n9         6      1\n10        6      1\n11        6      1\n12        7     -1\n13        8     -1\n"
            ],
            "answer": {
                "ans_desc": "First we create boolean masks and using then use on this mask and transform using , then using chose the elements from depending upon the conditions : Result: ",
                "code": [
                    "m1 = mydf['types'].eq(1).groupby(mydf['eventID']).transform('any')\nm2 = mydf['types'].eq(0).groupby(mydf['eventID']).transform('any')\nmydf['types'] = np.select([m1 , m2], [1, 0], mydf['types'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "arrays",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 174,
            "user_id": 13492584,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/12d06b58052db1991f468abb4bc323f3?s=128&d=identicon&r=PG&f=1",
            "display_name": "hellomynameisA",
            "link": "https://stackoverflow.com/users/13492584/hellomynameisa"
        },
        "is_answered": true,
        "view_count": 379,
        "accepted_answer_id": 62455948,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1592504473,
        "creation_date": 1592501271,
        "question_id": 62455905,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62455905/from-two-arrays-to-one-dataframe-python",
        "title": "from two arrays to one dataframe python",
        "body": "<p>I am trying to put my values into two arrays and then to make them a dataframe. I am using python, numpy and pandas to do so. </p>\n\n<p>my arrays are: </p>\n\n<pre><code>k = [7.0, 8.0, 6.55, 7.0000001, 10.12]\np = [6.94, 9.0, 4.44444, 13.0, 9.0876]\n</code></pre>\n\n<p>and I would like to put them into a pandas dataframe. When I print my dataframe, I would like to see this:</p>\n\n<pre><code>    a     b    c     d     e\nk  7.0   8.0  6.6   7.0  10.1\np  6.9   9.0  4.4  13.0   9.1\n</code></pre>\n\n<p>How can I do that?</p>\n\n<p>I read some related questions, but I can't get it right. One of the errors says that indexes must not be tuples, but, as you can see, I don't have tuples</p>\n",
        "answer_body": "<p>You can always have as input to <code>pd.DataFrame</code> be a list of lists, which will generate the output you desire:</p>\n\n<pre><code>k = [7.0, 8.0, 6.55, 7.0000001, 10.12]\np = [6.94, 9.0, 4.44444, 13.0, 9.0876]\n\npd.DataFrame([k,p],columns=['a','b','c','d','f'],index=['k','p'])\n\n    a   b   c   d   e\nk   7.00    8.0 6.55000 7.0 10.1200\np   6.94    9.0 4.44444 13.0    9.0876\n</code></pre>\n\n<p>And if you want rounded:</p>\n\n<pre><code>pd.DataFrame([k,p],columns=['a','b','c','d','f'],index=['k','p']).round()\n\n    a   b   c   d   e\nk   7.0 8.0 7.0 7.0 10.0\np   7.0 9.0 4.0 13.0    9.0\n</code></pre>\n\n<p>for dynamic columns:</p>\n\n<pre><code>from string import ascii_lowercase\npd.DataFrame([k,p],columns=list(ascii_lowercase[:len(k)]),index=['k','p']).round()\n</code></pre>\n",
        "question_body": "<p>I am trying to put my values into two arrays and then to make them a dataframe. I am using python, numpy and pandas to do so. </p>\n\n<p>my arrays are: </p>\n\n<pre><code>k = [7.0, 8.0, 6.55, 7.0000001, 10.12]\np = [6.94, 9.0, 4.44444, 13.0, 9.0876]\n</code></pre>\n\n<p>and I would like to put them into a pandas dataframe. When I print my dataframe, I would like to see this:</p>\n\n<pre><code>    a     b    c     d     e\nk  7.0   8.0  6.6   7.0  10.1\np  6.9   9.0  4.4  13.0   9.1\n</code></pre>\n\n<p>How can I do that?</p>\n\n<p>I read some related questions, but I can't get it right. One of the errors says that indexes must not be tuples, but, as you can see, I don't have tuples</p>\n",
        "formatted_input": {
            "qid": 62455905,
            "link": "https://stackoverflow.com/questions/62455905/from-two-arrays-to-one-dataframe-python",
            "question": {
                "title": "from two arrays to one dataframe python",
                "ques_desc": "I am trying to put my values into two arrays and then to make them a dataframe. I am using python, numpy and pandas to do so. my arrays are: and I would like to put them into a pandas dataframe. When I print my dataframe, I would like to see this: How can I do that? I read some related questions, but I can't get it right. One of the errors says that indexes must not be tuples, but, as you can see, I don't have tuples "
            },
            "io": [
                "k = [7.0, 8.0, 6.55, 7.0000001, 10.12]\np = [6.94, 9.0, 4.44444, 13.0, 9.0876]\n",
                "    a     b    c     d     e\nk  7.0   8.0  6.6   7.0  10.1\np  6.9   9.0  4.4  13.0   9.1\n"
            ],
            "answer": {
                "ans_desc": "You can always have as input to be a list of lists, which will generate the output you desire: And if you want rounded: for dynamic columns: ",
                "code": [
                    "from string import ascii_lowercase\npd.DataFrame([k,p],columns=list(ascii_lowercase[:len(k)]),index=['k','p']).round()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 62436928,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1592434871,
        "creation_date": 1592421235,
        "question_id": 62436677,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62436677/group-rows-in-a-pandas-data-frame-when-the-difference-of-consecutive-rows-are-le",
        "title": "group rows in a pandas data frame when the difference of consecutive rows are less than a value",
        "body": "<p>I have a data frame like this, </p>\n\n<pre><code>col1    col2    col3\n 1        2       3\n 2        3       4\n 4        2       3\n 7        2       8\n 8        3       4\n 9        3       3\n 15       1       12\n</code></pre>\n\n<p>Now I want to group those rows where there difference between two consecutive col1 rows is less than 3. and sum other column values, create another column(col4) with the last value of the group,\nSo the final data frame will look like, </p>\n\n<pre><code>col1    col2    col3    col4\n  1       7       10     4\n  7       8       15     9\n</code></pre>\n\n<p>using for loop to do this is tedious, looking for some pandas shortcuts to do it most efficiently.</p>\n",
        "answer_body": "<p>You can do a named aggregation on groupby:</p>\n\n<pre><code>(df.groupby(df.col1.diff().ge(3).cumsum(), as_index=False)\n   .agg(col1=('col1','first'),\n        col2=('col2','sum'),\n        col3=('col3','sum'),\n        col4=('col1','last'))\n)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   col1  col2  col3  col4\n0     1     7    10     4\n1     7     8    15     9\n2    15     1    12    15\n</code></pre>\n\n<p><strong>update</strong> without named aggregation you can do some thing like this:</p>\n\n<pre><code>groups = df.groupby(df.col1.diff().ge(3).cumsum())\nnew_df = groups.agg({'col1':'first', 'col2':'sum','col3':'sum'})\nnew_df['col4'] = groups['col1'].last()\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this, </p>\n\n<pre><code>col1    col2    col3\n 1        2       3\n 2        3       4\n 4        2       3\n 7        2       8\n 8        3       4\n 9        3       3\n 15       1       12\n</code></pre>\n\n<p>Now I want to group those rows where there difference between two consecutive col1 rows is less than 3. and sum other column values, create another column(col4) with the last value of the group,\nSo the final data frame will look like, </p>\n\n<pre><code>col1    col2    col3    col4\n  1       7       10     4\n  7       8       15     9\n</code></pre>\n\n<p>using for loop to do this is tedious, looking for some pandas shortcuts to do it most efficiently.</p>\n",
        "formatted_input": {
            "qid": 62436677,
            "link": "https://stackoverflow.com/questions/62436677/group-rows-in-a-pandas-data-frame-when-the-difference-of-consecutive-rows-are-le",
            "question": {
                "title": "group rows in a pandas data frame when the difference of consecutive rows are less than a value",
                "ques_desc": "I have a data frame like this, Now I want to group those rows where there difference between two consecutive col1 rows is less than 3. and sum other column values, create another column(col4) with the last value of the group, So the final data frame will look like, using for loop to do this is tedious, looking for some pandas shortcuts to do it most efficiently. "
            },
            "io": [
                "col1    col2    col3\n 1        2       3\n 2        3       4\n 4        2       3\n 7        2       8\n 8        3       4\n 9        3       3\n 15       1       12\n",
                "col1    col2    col3    col4\n  1       7       10     4\n  7       8       15     9\n"
            ],
            "answer": {
                "ans_desc": "You can do a named aggregation on groupby: Output: update without named aggregation you can do some thing like this: ",
                "code": [
                    "(df.groupby(df.col1.diff().ge(3).cumsum(), as_index=False)\n   .agg(col1=('col1','first'),\n        col2=('col2','sum'),\n        col3=('col3','sum'),\n        col4=('col1','last'))\n)\n",
                    "groups = df.groupby(df.col1.diff().ge(3).cumsum())\nnew_df = groups.agg({'col1':'first', 'col2':'sum','col3':'sum'})\nnew_df['col4'] = groups['col1'].last()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 145,
            "user_id": 4998865,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/90bf132c70ea3719897ffeb8edc0dd81?s=128&d=identicon&r=PG",
            "display_name": "David",
            "link": "https://stackoverflow.com/users/4998865/david"
        },
        "is_answered": true,
        "view_count": 111,
        "accepted_answer_id": 62377046,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1592166400,
        "creation_date": 1592160500,
        "last_edit_date": 1592161342,
        "question_id": 62376848,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62376848/finding-the-frequency-that-each-column-is-a-row-minimum",
        "title": "Finding the frequency that each column is a row minimum",
        "body": "<p>I have a dataframe that looks like:</p>\n\n<pre><code>       A     B     C     D\n 0   1.2     0   1.1   3.2\n 1   2.3   2.2   2.2   2.5\n 2   1.1   1.5     0   1.7\n 3     0   1.1   1.4   1.2\n 4   3.3   3.0   1.7   1.7\n 5   1.1   1.0   2.2   2.5\n 6   5.0   5.0   5.0   5.0\n</code></pre>\n\n<p>I would like to find the frequency that each column contains the row's minimum. So in some format:</p>\n\n<pre><code>B: 2               # rows 0, 5\nA: 1               # row 3\nC: 1               # row 2\n(B, C): 1          # row 1\n(C, D): 1          # row 4\n(A, B, C, D): 1    # row 6\n</code></pre>\n\n<p>I am currently doing <code>df.min(axis=1)</code> and then looping through each row using <code>df.iloc</code>... but there has to be a better way.</p>\n\n<p>In case it matters, I have a couple hundred columns, a couple thousand rows, and it represents a sample, so I have to perform the operation roughly a million times. I must be missing an obvious pandas or numpy method that will do this both pythonically and reasonably efficiently.</p>\n",
        "answer_body": "<p>IIUC, </p>\n\n<ol>\n<li>Transpose the dataset (making it easier with axis)</li>\n<li>Identify the location of min values in each column. </li>\n<li>Identify the column names for each min</li>\n<li>Value counts</li>\n</ol>\n\n<pre><code>df = df.T\n\nresult = (df.eq(df.min())\n            .apply(lambda x:tuple(x.index[x]))\n            .value_counts())\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like:</p>\n\n<pre><code>       A     B     C     D\n 0   1.2     0   1.1   3.2\n 1   2.3   2.2   2.2   2.5\n 2   1.1   1.5     0   1.7\n 3     0   1.1   1.4   1.2\n 4   3.3   3.0   1.7   1.7\n 5   1.1   1.0   2.2   2.5\n 6   5.0   5.0   5.0   5.0\n</code></pre>\n\n<p>I would like to find the frequency that each column contains the row's minimum. So in some format:</p>\n\n<pre><code>B: 2               # rows 0, 5\nA: 1               # row 3\nC: 1               # row 2\n(B, C): 1          # row 1\n(C, D): 1          # row 4\n(A, B, C, D): 1    # row 6\n</code></pre>\n\n<p>I am currently doing <code>df.min(axis=1)</code> and then looping through each row using <code>df.iloc</code>... but there has to be a better way.</p>\n\n<p>In case it matters, I have a couple hundred columns, a couple thousand rows, and it represents a sample, so I have to perform the operation roughly a million times. I must be missing an obvious pandas or numpy method that will do this both pythonically and reasonably efficiently.</p>\n",
        "formatted_input": {
            "qid": 62376848,
            "link": "https://stackoverflow.com/questions/62376848/finding-the-frequency-that-each-column-is-a-row-minimum",
            "question": {
                "title": "Finding the frequency that each column is a row minimum",
                "ques_desc": "I have a dataframe that looks like: I would like to find the frequency that each column contains the row's minimum. So in some format: I am currently doing and then looping through each row using ... but there has to be a better way. In case it matters, I have a couple hundred columns, a couple thousand rows, and it represents a sample, so I have to perform the operation roughly a million times. I must be missing an obvious pandas or numpy method that will do this both pythonically and reasonably efficiently. "
            },
            "io": [
                "       A     B     C     D\n 0   1.2     0   1.1   3.2\n 1   2.3   2.2   2.2   2.5\n 2   1.1   1.5     0   1.7\n 3     0   1.1   1.4   1.2\n 4   3.3   3.0   1.7   1.7\n 5   1.1   1.0   2.2   2.5\n 6   5.0   5.0   5.0   5.0\n",
                "B: 2               # rows 0, 5\nA: 1               # row 3\nC: 1               # row 2\n(B, C): 1          # row 1\n(C, D): 1          # row 4\n(A, B, C, D): 1    # row 6\n"
            ],
            "answer": {
                "ans_desc": "IIUC, Transpose the dataset (making it easier with axis) Identify the location of min values in each column. Identify the column names for each min Value counts ",
                "code": [
                    "df = df.T\n\nresult = (df.eq(df.min())\n            .apply(lambda x:tuple(x.index[x]))\n            .value_counts())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 473,
            "user_id": 5873325,
            "user_type": "registered",
            "accept_rate": 80,
            "profile_image": "https://graph.facebook.com/10204349886146574/picture?type=large",
            "display_name": "Mejdi Dallel",
            "link": "https://stackoverflow.com/users/5873325/mejdi-dallel"
        },
        "is_answered": true,
        "view_count": 121,
        "accepted_answer_id": 62361716,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1592069879,
        "creation_date": 1592059975,
        "last_edit_date": 1592069244,
        "question_id": 62361446,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62361446/python-dataframe-get-index-start-and-end-of-successive-values",
        "title": "Python dataframe get index start and end of successive values",
        "body": "<p>Let's say I have this dataframe : </p>\n\n<pre><code>   0\n0  1\n1  1\n2  1\n3  2\n4  2\n5  3\n6  3\n7  1\n8  1\n</code></pre>\n\n<p>I want to store the start and end indexes of each value (even repeated ones) in the dataframe as well as the value corresponding.</p>\n\n<p>So that I would get a result like this for example : </p>\n\n<pre><code>Value    |   Start   |   End\n----------------------------\n1        |     0     |    2\n2        |     3     |    4\n3        |     5     |    6\n1        |     7     |    8\n</code></pre>\n\n<p>I tried this (for the value 2 for example here) :</p>\n\n<pre><code>cs[['key']] = pd.DataFrame(cs.pop(0).values.tolist())\ng = cs.groupby('key')\nidx_start, idx_end = g.get_group(2).index[[0,-1]]\n</code></pre>\n\n<p>But this returns only first and last result each time.</p>\n",
        "answer_body": "<p>Given</p>\n\n<pre><code>&gt;&gt;&gt; df\n   0\n0  1\n1  1\n2  1\n3  2\n4  2\n5  3\n6  3\n7  1\n8  1\n</code></pre>\n\n<p>Solution:</p>\n\n<pre><code>starts_bool = df.diff().ne(0)[0]\nstarts = df.index[starts_bool]\nends = df.index[starts_bool.shift(-1, fill_value=True)]\n\nresult = (df.loc[starts]\n            .reset_index(drop=True)\n            .assign(Start=starts, End=ends)\n            .rename({0: 'Value'}, axis='columns')\n          )\n</code></pre>\n\n<p>Result:</p>\n\n<pre><code>&gt;&gt;&gt; result\n   value  Start  End\n0      1      0    2\n1      2      3    4\n2      3      5    6\n3      1      7    8\n</code></pre>\n",
        "question_body": "<p>Let's say I have this dataframe : </p>\n\n<pre><code>   0\n0  1\n1  1\n2  1\n3  2\n4  2\n5  3\n6  3\n7  1\n8  1\n</code></pre>\n\n<p>I want to store the start and end indexes of each value (even repeated ones) in the dataframe as well as the value corresponding.</p>\n\n<p>So that I would get a result like this for example : </p>\n\n<pre><code>Value    |   Start   |   End\n----------------------------\n1        |     0     |    2\n2        |     3     |    4\n3        |     5     |    6\n1        |     7     |    8\n</code></pre>\n\n<p>I tried this (for the value 2 for example here) :</p>\n\n<pre><code>cs[['key']] = pd.DataFrame(cs.pop(0).values.tolist())\ng = cs.groupby('key')\nidx_start, idx_end = g.get_group(2).index[[0,-1]]\n</code></pre>\n\n<p>But this returns only first and last result each time.</p>\n",
        "formatted_input": {
            "qid": 62361446,
            "link": "https://stackoverflow.com/questions/62361446/python-dataframe-get-index-start-and-end-of-successive-values",
            "question": {
                "title": "Python dataframe get index start and end of successive values",
                "ques_desc": "Let's say I have this dataframe : I want to store the start and end indexes of each value (even repeated ones) in the dataframe as well as the value corresponding. So that I would get a result like this for example : I tried this (for the value 2 for example here) : But this returns only first and last result each time. "
            },
            "io": [
                "   0\n0  1\n1  1\n2  1\n3  2\n4  2\n5  3\n6  3\n7  1\n8  1\n",
                "Value    |   Start   |   End\n----------------------------\n1        |     0     |    2\n2        |     3     |    4\n3        |     5     |    6\n1        |     7     |    8\n"
            ],
            "answer": {
                "ans_desc": "Given Solution: Result: ",
                "code": [
                    "starts_bool = df.diff().ne(0)[0]\nstarts = df.index[starts_bool]\nends = df.index[starts_bool.shift(-1, fill_value=True)]\n\nresult = (df.loc[starts]\n            .reset_index(drop=True)\n            .assign(Start=starts, End=ends)\n            .rename({0: 'Value'}, axis='columns')\n          )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 9,
            "user_id": 10178186,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/550b046b860363165383f154498760b4?s=128&d=identicon&r=PG&f=1",
            "display_name": "garaks",
            "link": "https://stackoverflow.com/users/10178186/garaks"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 62351369,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1591990965,
        "creation_date": 1591989359,
        "last_edit_date": 1591990534,
        "question_id": 62351000,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62351000/extract-a-value-from-a-pandas-dataframe-dict-into-another-dataframe",
        "title": "extract a value from a pandas dataframe dict into another dataframe",
        "body": "<p>df.head(10)</p>\n\n<pre><code>    XYZVal\n0   {\"X\":\"56.68\",\"Y\":\"51.56\",\"Z\":\"100\"}\n1   {\"X\":\"58.05\",\"Y\":\"52.37\",\"Z\":\"62.6\"}\n2   {\"X\":\"59.32\",\"Y\":\"54.48\",\"Z\":\"69.59\"}\n3   {\"X\":\"58.51\",\"Y\":\"36.36\",\"Z\":\"82.76\"}\n4   {\"X\":\"65.21\",\"Y\":\"60.26\",\"Z\":\"71.06\"}\n5   {\"X\":\"57.64\",\"Y\":\"52.07\",\"Z\":\"67.89\"}\n6   {\"X\":\"58.24\",\"Y\":\"50\",\"Z\":\"75\"}\n7   {\"X\":\"57.69\",\"Y\":\"52.13\",\"Z\":\"68.64\"}\n8   {\"X\":\"57.83\",\"Y\":\"53.05\",\"Z\":\"65.92\"}\n9   {\"X\":\"60.87\",\"Y\":\"51.73\",\"Z\":\"71.35\"}\n\n</code></pre>\n\n<p>How to convert the above dataframe into a new dataframe by selecting X:</p>\n\n<pre><code>{ 56.68 ,58.05 ,59.32 ,58.51 ,65.21 ,57.64 ,58.24 ,57.69 ,57.83 ,60.87 }\n</code></pre>\n\n<p>df.info()\nshows</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 1 columns):\nXYZVal    100 non-null object\ndtypes: object(1)\n</code></pre>\n",
        "answer_body": "<p>If <code>XYZVal</code> does already contain dicts, just use:</p>\n\n<pre><code>df.XYZVal.apply(lambda x: x[\"X\"])\n</code></pre>\n\n<p>If the column entries are strings, just use:</p>\n\n<pre><code>import json\ndf['XYZVal'].apply(lambda x: json.loads(x)[\"X\"])\n</code></pre>\n\n<p>result (in both ways)</p>\n\n<pre><code>0    56.68\n1    58.05\n2    59.32\n3    58.51\n4    65.21\n5    57.64\n6    58.24\n7    57.69\n8    57.83\n9    60.87\nName: XYZVal, dtype: object\n</code></pre>\n",
        "question_body": "<p>df.head(10)</p>\n\n<pre><code>    XYZVal\n0   {\"X\":\"56.68\",\"Y\":\"51.56\",\"Z\":\"100\"}\n1   {\"X\":\"58.05\",\"Y\":\"52.37\",\"Z\":\"62.6\"}\n2   {\"X\":\"59.32\",\"Y\":\"54.48\",\"Z\":\"69.59\"}\n3   {\"X\":\"58.51\",\"Y\":\"36.36\",\"Z\":\"82.76\"}\n4   {\"X\":\"65.21\",\"Y\":\"60.26\",\"Z\":\"71.06\"}\n5   {\"X\":\"57.64\",\"Y\":\"52.07\",\"Z\":\"67.89\"}\n6   {\"X\":\"58.24\",\"Y\":\"50\",\"Z\":\"75\"}\n7   {\"X\":\"57.69\",\"Y\":\"52.13\",\"Z\":\"68.64\"}\n8   {\"X\":\"57.83\",\"Y\":\"53.05\",\"Z\":\"65.92\"}\n9   {\"X\":\"60.87\",\"Y\":\"51.73\",\"Z\":\"71.35\"}\n\n</code></pre>\n\n<p>How to convert the above dataframe into a new dataframe by selecting X:</p>\n\n<pre><code>{ 56.68 ,58.05 ,59.32 ,58.51 ,65.21 ,57.64 ,58.24 ,57.69 ,57.83 ,60.87 }\n</code></pre>\n\n<p>df.info()\nshows</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 1 columns):\nXYZVal    100 non-null object\ndtypes: object(1)\n</code></pre>\n",
        "formatted_input": {
            "qid": 62351000,
            "link": "https://stackoverflow.com/questions/62351000/extract-a-value-from-a-pandas-dataframe-dict-into-another-dataframe",
            "question": {
                "title": "extract a value from a pandas dataframe dict into another dataframe",
                "ques_desc": "df.head(10) How to convert the above dataframe into a new dataframe by selecting X: df.info() shows "
            },
            "io": [
                "    XYZVal\n0   {\"X\":\"56.68\",\"Y\":\"51.56\",\"Z\":\"100\"}\n1   {\"X\":\"58.05\",\"Y\":\"52.37\",\"Z\":\"62.6\"}\n2   {\"X\":\"59.32\",\"Y\":\"54.48\",\"Z\":\"69.59\"}\n3   {\"X\":\"58.51\",\"Y\":\"36.36\",\"Z\":\"82.76\"}\n4   {\"X\":\"65.21\",\"Y\":\"60.26\",\"Z\":\"71.06\"}\n5   {\"X\":\"57.64\",\"Y\":\"52.07\",\"Z\":\"67.89\"}\n6   {\"X\":\"58.24\",\"Y\":\"50\",\"Z\":\"75\"}\n7   {\"X\":\"57.69\",\"Y\":\"52.13\",\"Z\":\"68.64\"}\n8   {\"X\":\"57.83\",\"Y\":\"53.05\",\"Z\":\"65.92\"}\n9   {\"X\":\"60.87\",\"Y\":\"51.73\",\"Z\":\"71.35\"}\n\n",
                "{ 56.68 ,58.05 ,59.32 ,58.51 ,65.21 ,57.64 ,58.24 ,57.69 ,57.83 ,60.87 }\n"
            ],
            "answer": {
                "ans_desc": "If does already contain dicts, just use: If the column entries are strings, just use: result (in both ways) ",
                "code": [
                    "import json\ndf['XYZVal'].apply(lambda x: json.loads(x)[\"X\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 187,
            "user_id": 7846175,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ca18a63a28a0a94c54ee3a0c951aca94?s=128&d=identicon&r=PG",
            "display_name": "Beveline",
            "link": "https://stackoverflow.com/users/7846175/beveline"
        },
        "is_answered": true,
        "view_count": 139,
        "accepted_answer_id": 62328439,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1591952406,
        "creation_date": 1591890610,
        "last_edit_date": 1591952406,
        "question_id": 62328295,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62328295/create-a-nested-dictionary-from-dataframe-where-first-column-is-the-key-for-par",
        "title": "create a nested dictionary from dataframe, where first column is the key for parent dictionary",
        "body": "<p>I am trying to create a nested dictionary from a pandas dataframe. The first column-values are supposed to be the key for the upper dictionary, which will contaion the other columns as dictionary, where the column header is the key. I would like to avoid loops.</p>\n\n<p>the dataframe:</p>\n\n<pre><code>df = pd.DataFrame({'A': [11, 11, 11, 11, 11, 11, 12, 12],\n                   'B': [1, 2, 3, 1, 2, 3, 4, 5],\n                   'C': [1.0, 0.7, 0.3, 1.0, 0.7, 0.3, 1.0, 1.0]})\n</code></pre>\n\n<p>what I would like to have:</p>\n\n<pre><code>dict_expt = {'11': {'B': [1, 2, 3],\n                  'C': [1.0, 0.7, 0.3]},\n           '12': {'B': [4, 5],\n                  'C': [1.0]}}\n</code></pre>\n\n<p>what I have tried:</p>\n\n<pre><code>df.groupby(['A']).agg({'B':lambda x: list(x.unique()),\n                      'C':lambda x: list(x.unique())}).to_dict()\n</code></pre>\n\n<p>which unfortunately returns:</p>\n\n<pre><code>{'B': \n     {11: [1, 2, 3], \n      12: [4, 5]}, \n'C': {11: [1.0, 0.7, 0.3], \n      12: [1.0]}}\n</code></pre>\n\n<p>Any help is highly appreciated. Thanks</p>\n",
        "answer_body": "<p>You were close, just add <code>\"index\"</code> to <code>to_dict()</code>:</p>\n\n<pre><code>df.groupby(['A']).agg({'B':lambda x: list(x.unique()),\n                      'C':lambda x: list(x.unique())}).to_dict(\"index\")\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>{11: {'B': [1, 2, 3], 'C': [1.0, 0.7, 0.3]}, 12: {'B': [4, 5], 'C': [1.0]}}\n</code></pre>\n",
        "question_body": "<p>I am trying to create a nested dictionary from a pandas dataframe. The first column-values are supposed to be the key for the upper dictionary, which will contaion the other columns as dictionary, where the column header is the key. I would like to avoid loops.</p>\n\n<p>the dataframe:</p>\n\n<pre><code>df = pd.DataFrame({'A': [11, 11, 11, 11, 11, 11, 12, 12],\n                   'B': [1, 2, 3, 1, 2, 3, 4, 5],\n                   'C': [1.0, 0.7, 0.3, 1.0, 0.7, 0.3, 1.0, 1.0]})\n</code></pre>\n\n<p>what I would like to have:</p>\n\n<pre><code>dict_expt = {'11': {'B': [1, 2, 3],\n                  'C': [1.0, 0.7, 0.3]},\n           '12': {'B': [4, 5],\n                  'C': [1.0]}}\n</code></pre>\n\n<p>what I have tried:</p>\n\n<pre><code>df.groupby(['A']).agg({'B':lambda x: list(x.unique()),\n                      'C':lambda x: list(x.unique())}).to_dict()\n</code></pre>\n\n<p>which unfortunately returns:</p>\n\n<pre><code>{'B': \n     {11: [1, 2, 3], \n      12: [4, 5]}, \n'C': {11: [1.0, 0.7, 0.3], \n      12: [1.0]}}\n</code></pre>\n\n<p>Any help is highly appreciated. Thanks</p>\n",
        "formatted_input": {
            "qid": 62328295,
            "link": "https://stackoverflow.com/questions/62328295/create-a-nested-dictionary-from-dataframe-where-first-column-is-the-key-for-par",
            "question": {
                "title": "create a nested dictionary from dataframe, where first column is the key for parent dictionary",
                "ques_desc": "I am trying to create a nested dictionary from a pandas dataframe. The first column-values are supposed to be the key for the upper dictionary, which will contaion the other columns as dictionary, where the column header is the key. I would like to avoid loops. the dataframe: what I would like to have: what I have tried: which unfortunately returns: Any help is highly appreciated. Thanks "
            },
            "io": [
                "dict_expt = {'11': {'B': [1, 2, 3],\n                  'C': [1.0, 0.7, 0.3]},\n           '12': {'B': [4, 5],\n                  'C': [1.0]}}\n",
                "{'B': \n     {11: [1, 2, 3], \n      12: [4, 5]}, \n'C': {11: [1.0, 0.7, 0.3], \n      12: [1.0]}}\n"
            ],
            "answer": {
                "ans_desc": "You were close, just add to : Output: ",
                "code": [
                    "df.groupby(['A']).agg({'B':lambda x: list(x.unique()),\n                      'C':lambda x: list(x.unique())}).to_dict(\"index\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby",
            "export-to-csv"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 10547025,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/bbde8c93be1632a5ef1be89040d75787?s=128&d=identicon&r=PG&f=1",
            "display_name": "Joe Barnes",
            "link": "https://stackoverflow.com/users/10547025/joe-barnes"
        },
        "is_answered": true,
        "view_count": 161,
        "accepted_answer_id": 62302491,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1591790392,
        "creation_date": 1591787400,
        "last_edit_date": 1591790392,
        "question_id": 62302113,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62302113/remove-quotation-marks-and-brackets-from-pandas-dataframe-csv-file-after-perfor",
        "title": "Remove quotation marks and brackets from Pandas DataFrame .csv file after performing a GroupBy with MultiIndex",
        "body": "<p>I'm new to pandas so apologies if my explanations of things are wrong.</p>\n\n<p>I have a data frame created as follows:</p>\n\n<pre><code>        arrays = [array1, array2]\n        index = pd.MultiIndex.from_arrays(arrays, names = (\"name1\", \"name2\"))\n        df = pd.DataFrame({\"name3\": array3, \"name4\": array4}, index=index)\n</code></pre>\n\n<p>Then I perform a weighted mean, using the indices, using code from the second top answer <a href=\"https://stackoverflow.com/questions/26205922/calculate-weighted-average-using-a-pandas-dataframe\">here</a>.</p>\n\n<pre><code>        df2 = df.groupby(df.index).apply(lambda x: np.average(x.name3, weights=x.name4))\n        print(df2)\n</code></pre>\n\n<p>The output on the console looks like this:</p>\n\n<pre><code>        (1, 2) 3\n        (4, 5) 6\n        (7, 8) 9\n</code></pre>\n\n<p>where (x,y) are the indices that I have grouped by and the number at the end is the weighted mean.</p>\n\n<p>When I export to a .csv file, I get a file that looks like this:</p>\n\n<pre><code>        ,0\n        \"(1, 2)\",3\n        \"(4, 5)\",6\n        \"(7, 8)\",9\n</code></pre>\n\n<p>This is not what I want. I would like to get a .csv file that looks like this:</p>\n\n<pre><code>        name1,name2,avg\n        1,2,3\n        4,5,6\n        7,8,9\n</code></pre>\n\n<p>I've tried using reset.index() but this does not work. I want to remove the brackets, quotation marks and the rogue ,0 at the start of the .csv file. How can I do this? Many thanks in advance.</p>\n",
        "answer_body": "<p>Use, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>df.groupby</code></a> level instead of indices:</p>\n\n<pre><code>df2 = df.groupby(level=df.index.names).apply(lambda x: np.average(x.name3, weights=x.name4))\n\n# save the df2 to csv file\ndf2.rename('avg').reset_index().to_csv('data.csv', index=False)\n</code></pre>\n",
        "question_body": "<p>I'm new to pandas so apologies if my explanations of things are wrong.</p>\n\n<p>I have a data frame created as follows:</p>\n\n<pre><code>        arrays = [array1, array2]\n        index = pd.MultiIndex.from_arrays(arrays, names = (\"name1\", \"name2\"))\n        df = pd.DataFrame({\"name3\": array3, \"name4\": array4}, index=index)\n</code></pre>\n\n<p>Then I perform a weighted mean, using the indices, using code from the second top answer <a href=\"https://stackoverflow.com/questions/26205922/calculate-weighted-average-using-a-pandas-dataframe\">here</a>.</p>\n\n<pre><code>        df2 = df.groupby(df.index).apply(lambda x: np.average(x.name3, weights=x.name4))\n        print(df2)\n</code></pre>\n\n<p>The output on the console looks like this:</p>\n\n<pre><code>        (1, 2) 3\n        (4, 5) 6\n        (7, 8) 9\n</code></pre>\n\n<p>where (x,y) are the indices that I have grouped by and the number at the end is the weighted mean.</p>\n\n<p>When I export to a .csv file, I get a file that looks like this:</p>\n\n<pre><code>        ,0\n        \"(1, 2)\",3\n        \"(4, 5)\",6\n        \"(7, 8)\",9\n</code></pre>\n\n<p>This is not what I want. I would like to get a .csv file that looks like this:</p>\n\n<pre><code>        name1,name2,avg\n        1,2,3\n        4,5,6\n        7,8,9\n</code></pre>\n\n<p>I've tried using reset.index() but this does not work. I want to remove the brackets, quotation marks and the rogue ,0 at the start of the .csv file. How can I do this? Many thanks in advance.</p>\n",
        "formatted_input": {
            "qid": 62302113,
            "link": "https://stackoverflow.com/questions/62302113/remove-quotation-marks-and-brackets-from-pandas-dataframe-csv-file-after-perfor",
            "question": {
                "title": "Remove quotation marks and brackets from Pandas DataFrame .csv file after performing a GroupBy with MultiIndex",
                "ques_desc": "I'm new to pandas so apologies if my explanations of things are wrong. I have a data frame created as follows: Then I perform a weighted mean, using the indices, using code from the second top answer here. The output on the console looks like this: where (x,y) are the indices that I have grouped by and the number at the end is the weighted mean. When I export to a .csv file, I get a file that looks like this: This is not what I want. I would like to get a .csv file that looks like this: I've tried using reset.index() but this does not work. I want to remove the brackets, quotation marks and the rogue ,0 at the start of the .csv file. How can I do this? Many thanks in advance. "
            },
            "io": [
                "        (1, 2) 3\n        (4, 5) 6\n        (7, 8) 9\n",
                "        ,0\n        \"(1, 2)\",3\n        \"(4, 5)\",6\n        \"(7, 8)\",9\n"
            ],
            "answer": {
                "ans_desc": "Use, level instead of indices: ",
                "code": [
                    "df2 = df.groupby(level=df.index.names).apply(lambda x: np.average(x.name3, weights=x.name4))\n\n# save the df2 to csv file\ndf2.rename('avg').reset_index().to_csv('data.csv', index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 151,
            "user_id": 13475017,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ba04a6526653d9f500e24949e727f1c9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Anku",
            "link": "https://stackoverflow.com/users/13475017/anku"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 62215791,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1591370616,
        "creation_date": 1591360342,
        "last_edit_date": 1591366990,
        "question_id": 62215453,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62215453/python-pandas-iterate-over-columns-and-also-update-columns-based-on-apply-condit",
        "title": "Python Pandas Iterate over columns and also update columns based on apply conditions",
        "body": "<p>I am trying to update dataframe columns based on consecutive columns values.<br>\nIf columns say col1 and col2 has >0 and &lt;0 values, then same columns should get updated as col2=col1 + col2 and col1=0 and also counter +1 (gives how many fixes has been done throughout the column).</p>\n\n<p>Dataframe look like: </p>\n\n<pre><code>id  col0    col1    col2    col3    col4    col5 col6   col7    col8    col9    col10\n1   0   5   -5  5   -5  0 0 1   4   3   -3 \n2   0   0   0   0   0   0 0 4   -4  0   0 \n3   0   0   1   2   3   0 0 0   5   6   0 \n</code></pre>\n\n<p>Required Dataframe after applying logic:</p>\n\n<pre><code>id  col0    col1    col2    col3    col4    col6    col7    col8    col9    col10   fix\n1   0   0   0   0   0   0   0 1 4   0   0 0 3\n2   0   0   0   0   0   0   0 0 0   0   0 0 1\n3   0   0   1   2   3   0   0 0 5   6   0 9 0\n</code></pre>\n\n<p>I tried:</p>\n\n<pre><code>def fix_count(row):\n    row['fix_cnt'] = 0\n\n    for i in range(0, 10):\n        if ((row['col' + str(i)] &gt; 0) &amp; \n            (row['col' + str(i + 1)] &lt; 0)):\n\n            row['col' + str(i + 1)] = row['col' + str(i)] + row['col' + str(i + 1)]\n            row['col' + str(i)] = 0\n\n            row['fix_cnt'] += 1\n\n            return (row['col' + str(i)],\n                    row['col' + str(i + 1)],\n                    row['fix_cnt'])\n\ndf.apply(fix_count, axis=1)\n</code></pre>\n\n<p>It failed <code>IndexError: index 11 is out of bounds for axis 0 with size 11.</code></p>\n\n<p>I also looked into <code>df.iteritems</code> but I couldn't figure out the way.</p>\n\n<p>DDL to generate DataFrame:</p>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'id': [1, 2, 3],\n                   'col0': [0, 0, 0],\n                   'col1': [5, 0, 0],\n                   'col2': [-5, 0, 1],\n                   'col3': [5, 0, 2],\n                   'col4': [-5, 0, 3],\n                   'col5' : [0, 0, 0],\n                   'col6': [0, 0, 0],\n                   'col7': [1, 4, 0],\n                   'col8': [4, -4, 5],\n                   'col9': [3, 0, 6],\n                   'col10': [-3, 0, 0]})\n</code></pre>\n\n<p>Thanks! </p>\n",
        "answer_body": "<p>Here is an approach without loops:</p>\n\n<pre><code>c = df.gt(0) &amp; df.shift(-1,axis=1).lt(0)\n\nout = (df.mask(c.shift(axis=1).fillna(False),df+df.shift(axis=1))\n      .mask(c,0).assign(Fix=c.sum(1)))\n\nprint(out)\n</code></pre>\n\n<hr>\n\n<pre><code>   id  col0  col1  col2  col3  col4  col6  col7  col8  col9  col10  Fix\n0   1     0     0     0     0     0     0     1     4     0      0    3\n1   2     0     0     0     0     0     0     0     0     0      0    1\n2   3     0     0     1     2     3     0     0     5     6      0    0\n</code></pre>\n\n<p>Details:</p>\n\n<ul>\n<li><code>c</code> checks if current column is > 0 and next column is &lt;0.</li>\n<li>Add current column to next column in the next column to where <code>c</code> is\nTrue.</li>\n<li>Set the current column to 0 if c is True.</li>\n<li>Get sum of c for changes\ndone.</li>\n</ul>\n",
        "question_body": "<p>I am trying to update dataframe columns based on consecutive columns values.<br>\nIf columns say col1 and col2 has >0 and &lt;0 values, then same columns should get updated as col2=col1 + col2 and col1=0 and also counter +1 (gives how many fixes has been done throughout the column).</p>\n\n<p>Dataframe look like: </p>\n\n<pre><code>id  col0    col1    col2    col3    col4    col5 col6   col7    col8    col9    col10\n1   0   5   -5  5   -5  0 0 1   4   3   -3 \n2   0   0   0   0   0   0 0 4   -4  0   0 \n3   0   0   1   2   3   0 0 0   5   6   0 \n</code></pre>\n\n<p>Required Dataframe after applying logic:</p>\n\n<pre><code>id  col0    col1    col2    col3    col4    col6    col7    col8    col9    col10   fix\n1   0   0   0   0   0   0   0 1 4   0   0 0 3\n2   0   0   0   0   0   0   0 0 0   0   0 0 1\n3   0   0   1   2   3   0   0 0 5   6   0 9 0\n</code></pre>\n\n<p>I tried:</p>\n\n<pre><code>def fix_count(row):\n    row['fix_cnt'] = 0\n\n    for i in range(0, 10):\n        if ((row['col' + str(i)] &gt; 0) &amp; \n            (row['col' + str(i + 1)] &lt; 0)):\n\n            row['col' + str(i + 1)] = row['col' + str(i)] + row['col' + str(i + 1)]\n            row['col' + str(i)] = 0\n\n            row['fix_cnt'] += 1\n\n            return (row['col' + str(i)],\n                    row['col' + str(i + 1)],\n                    row['fix_cnt'])\n\ndf.apply(fix_count, axis=1)\n</code></pre>\n\n<p>It failed <code>IndexError: index 11 is out of bounds for axis 0 with size 11.</code></p>\n\n<p>I also looked into <code>df.iteritems</code> but I couldn't figure out the way.</p>\n\n<p>DDL to generate DataFrame:</p>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'id': [1, 2, 3],\n                   'col0': [0, 0, 0],\n                   'col1': [5, 0, 0],\n                   'col2': [-5, 0, 1],\n                   'col3': [5, 0, 2],\n                   'col4': [-5, 0, 3],\n                   'col5' : [0, 0, 0],\n                   'col6': [0, 0, 0],\n                   'col7': [1, 4, 0],\n                   'col8': [4, -4, 5],\n                   'col9': [3, 0, 6],\n                   'col10': [-3, 0, 0]})\n</code></pre>\n\n<p>Thanks! </p>\n",
        "formatted_input": {
            "qid": 62215453,
            "link": "https://stackoverflow.com/questions/62215453/python-pandas-iterate-over-columns-and-also-update-columns-based-on-apply-condit",
            "question": {
                "title": "Python Pandas Iterate over columns and also update columns based on apply conditions",
                "ques_desc": "I am trying to update dataframe columns based on consecutive columns values. If columns say col1 and col2 has >0 and <0 values, then same columns should get updated as col2=col1 + col2 and col1=0 and also counter +1 (gives how many fixes has been done throughout the column). Dataframe look like: Required Dataframe after applying logic: I tried: It failed I also looked into but I couldn't figure out the way. DDL to generate DataFrame: Thanks! "
            },
            "io": [
                "id  col0    col1    col2    col3    col4    col5 col6   col7    col8    col9    col10\n1   0   5   -5  5   -5  0 0 1   4   3   -3 \n2   0   0   0   0   0   0 0 4   -4  0   0 \n3   0   0   1   2   3   0 0 0   5   6   0 \n",
                "id  col0    col1    col2    col3    col4    col6    col7    col8    col9    col10   fix\n1   0   0   0   0   0   0   0 1 4   0   0 0 3\n2   0   0   0   0   0   0   0 0 0   0   0 0 1\n3   0   0   1   2   3   0   0 0 5   6   0 9 0\n"
            ],
            "answer": {
                "ans_desc": "Here is an approach without loops: Details: checks if current column is > 0 and next column is <0. Add current column to next column in the next column to where is True. Set the current column to 0 if c is True. Get sum of c for changes done. ",
                "code": [
                    "c = df.gt(0) & df.shift(-1,axis=1).lt(0)\n\nout = (df.mask(c.shift(axis=1).fillna(False),df+df.shift(axis=1))\n      .mask(c,0).assign(Fix=c.sum(1)))\n\nprint(out)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 333,
            "user_id": 13330700,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/26c4be5ebc5d23f53462bc078dd604b4?s=128&d=identicon&r=PG&f=1",
            "display_name": "mike_gundy123",
            "link": "https://stackoverflow.com/users/13330700/mike-gundy123"
        },
        "is_answered": true,
        "view_count": 54,
        "accepted_answer_id": 62200327,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1591290958,
        "creation_date": 1591290294,
        "question_id": 62200280,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62200280/trying-to-create-a-dataframe-from-a-list-of-tuples-and-a-tuple",
        "title": "Trying to create a dataframe from a list of tuples, and a tuple",
        "body": "<p>As the title states I'm tyring to create a df using a list of tuples and a tuple.</p>\n\n<p>So what I currently have is</p>\n\n<pre><code>a = [(1,2,3), (4,5,6)]\nb = ('A', 'B')\nd = dict(zip(b,a))\ndf = pd.DataFrame(d)\nprint(df)\n</code></pre>\n\n<p>and my result is:</p>\n\n<pre><code>   A  B\n0  1  4\n1  2  5\n2  3  6\n</code></pre>\n\n<p>Where as I want something like:</p>\n\n<pre><code>   0  1\n0  A  1,2,3\n1  B  4,5,6\n</code></pre>\n\n<p>Any and help is appreciated!</p>\n",
        "answer_body": "<p>You can try something like:</p>\n\n<pre><code>d1 = {k: ','.join([*map(str,v)]) for k,v in d.items()}\npd.DataFrame.from_dict(d1,orient='index').reset_index()\n</code></pre>\n",
        "question_body": "<p>As the title states I'm tyring to create a df using a list of tuples and a tuple.</p>\n\n<p>So what I currently have is</p>\n\n<pre><code>a = [(1,2,3), (4,5,6)]\nb = ('A', 'B')\nd = dict(zip(b,a))\ndf = pd.DataFrame(d)\nprint(df)\n</code></pre>\n\n<p>and my result is:</p>\n\n<pre><code>   A  B\n0  1  4\n1  2  5\n2  3  6\n</code></pre>\n\n<p>Where as I want something like:</p>\n\n<pre><code>   0  1\n0  A  1,2,3\n1  B  4,5,6\n</code></pre>\n\n<p>Any and help is appreciated!</p>\n",
        "formatted_input": {
            "qid": 62200280,
            "link": "https://stackoverflow.com/questions/62200280/trying-to-create-a-dataframe-from-a-list-of-tuples-and-a-tuple",
            "question": {
                "title": "Trying to create a dataframe from a list of tuples, and a tuple",
                "ques_desc": "As the title states I'm tyring to create a df using a list of tuples and a tuple. So what I currently have is and my result is: Where as I want something like: Any and help is appreciated! "
            },
            "io": [
                "   A  B\n0  1  4\n1  2  5\n2  3  6\n",
                "   0  1\n0  A  1,2,3\n1  B  4,5,6\n"
            ],
            "answer": {
                "ans_desc": "You can try something like: ",
                "code": [
                    "d1 = {k: ','.join([*map(str,v)]) for k,v in d.items()}\npd.DataFrame.from_dict(d1,orient='index').reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 199,
            "user_id": 7225462,
            "user_type": "registered",
            "accept_rate": 71,
            "profile_image": "https://i.stack.imgur.com/zXjhD.jpg?s=128&g=1",
            "display_name": "SLLegendre",
            "link": "https://stackoverflow.com/users/7225462/sllegendre"
        },
        "is_answered": true,
        "view_count": 100,
        "accepted_answer_id": 62184071,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1591224884,
        "creation_date": 1590443875,
        "question_id": 62011392,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62011392/how-to-merge-or-join-a-stacked-dataframe-in-pandas",
        "title": "How to merge or join a stacked dataframe in pandas?",
        "body": "<p>I cannot find this question answered elsewhere; I would like to do a SQL-like join in pandas but with the slight twist that one dataframe is stacked. \nI have created a dataframe A with a stacked column index from a csv file in pandas that looks as follows:</p>\n\n<pre><code>|           |      | 2013-01-04 | 2013-01-07 |\n|----------:|-----:|-----------:|-----------:|\n| Adj Close |  OWW | NaN        | NaN        |\n|   Close   | OXLC | 4.155157   | 4.147217   |\n|           |  OXM | 40.318089  | 42.988800  |\n|           |  OXY | 50.416079  | 62.934800  |\n</code></pre>\n\n<p>The original csv had repeated what is in the 1st column for every entry like so: </p>\n\n<pre><code>|           |      | 2013-01-04 | 2013-01-07 |\n|----------:|-----:|-----------:|-----------:|\n| Adj Close |  OWW | NaN        | NaN        |\n|   Close   | OXLC | 4.155157   | 4.147217   |\n|   Close   |  OXM | 40.318089  | 42.988800  |\n|   Close   |  OXY | 50.416079  | 62.934800  |\n</code></pre>\n\n<p>The original csv was the transposed version of this. Pandas chose to stack that when converting to dataframe. (I used this code: pd.read_csv(file, header = [0,1], index_col=0).T)  </p>\n\n<p>In another csv/dataframe B I have for all of those so-called ticker symbols another ID that I would rather like to use: CIK. </p>\n\n<pre><code>| CIK     | Ticker | Name                                           |\n|---------|--------|------------------------------------------------|\n| 1090872 | A      | Agilent Technologies Inc                       |\n| 4281    | AA     | Alcoa Inc                                      |\n| 1332552 | AAACU  | Asia Automotive Acquisition Corp               |\n| 1287145 | AABB   | Asia Broadband Inc                             |\n| 1024015 | AABC   | Access Anytime Bancorp Inc                     |\n| 1099290 | AAC    | Sinocoking Coal &amp; Coke Chemical Industries Inc |\n| 1264707 | AACC   | Asset Acceptance Capital Corp                  |\n| 849116  | AACE   | Ace Cash Express Inc                           |\n| 1409430 | AAGC   | All American Gold Corp                         |\n| 948846  | AAI    | Airtran Holdings Inc                           |\n</code></pre>\n\n<p>Desired output: I would like to have the CIK instead of the ticker in a new dataframe otherwise identical to A. </p>\n\n<p>Now in SQL I could easily join on A.name_of_2nd_column = b.Ticker since the table would just have the entry in the 1st column repeated in every line (like the original csv) and the column would have a name but in pandas I cannot. I tried this code: </p>\n\n<pre><code>result = pd.merge(data, tix, how='left', left_on=[1] right_on=['Ticker'])\n</code></pre>\n\n<p>How do I tell pandas to use the 2nd column as the key and/or interpret the first column just as repeated values?</p>\n",
        "answer_body": "<p>I was eventually able to do it the following way:</p>\n\n<pre><code>df = A\ntix = B \n</code></pre>\n\n<pre><code>ticker_2_CIK = dict(zip(tix.Ticker,tix.CIK))  # create a dict\n\ntmp = df.reset_index().assign(CIK=lambda x: x['ticker'].map(ticker_2_CIK)) # use dict to find the correct value for colum \n\n# data was unclean, some ticker symbols were created after the period my data is from \n# and data was incomplete with some tickers missing\nsolution = tmp.dropna(subset=['CIK']).astype({'CIK':int})\n</code></pre>\n",
        "question_body": "<p>I cannot find this question answered elsewhere; I would like to do a SQL-like join in pandas but with the slight twist that one dataframe is stacked. \nI have created a dataframe A with a stacked column index from a csv file in pandas that looks as follows:</p>\n\n<pre><code>|           |      | 2013-01-04 | 2013-01-07 |\n|----------:|-----:|-----------:|-----------:|\n| Adj Close |  OWW | NaN        | NaN        |\n|   Close   | OXLC | 4.155157   | 4.147217   |\n|           |  OXM | 40.318089  | 42.988800  |\n|           |  OXY | 50.416079  | 62.934800  |\n</code></pre>\n\n<p>The original csv had repeated what is in the 1st column for every entry like so: </p>\n\n<pre><code>|           |      | 2013-01-04 | 2013-01-07 |\n|----------:|-----:|-----------:|-----------:|\n| Adj Close |  OWW | NaN        | NaN        |\n|   Close   | OXLC | 4.155157   | 4.147217   |\n|   Close   |  OXM | 40.318089  | 42.988800  |\n|   Close   |  OXY | 50.416079  | 62.934800  |\n</code></pre>\n\n<p>The original csv was the transposed version of this. Pandas chose to stack that when converting to dataframe. (I used this code: pd.read_csv(file, header = [0,1], index_col=0).T)  </p>\n\n<p>In another csv/dataframe B I have for all of those so-called ticker symbols another ID that I would rather like to use: CIK. </p>\n\n<pre><code>| CIK     | Ticker | Name                                           |\n|---------|--------|------------------------------------------------|\n| 1090872 | A      | Agilent Technologies Inc                       |\n| 4281    | AA     | Alcoa Inc                                      |\n| 1332552 | AAACU  | Asia Automotive Acquisition Corp               |\n| 1287145 | AABB   | Asia Broadband Inc                             |\n| 1024015 | AABC   | Access Anytime Bancorp Inc                     |\n| 1099290 | AAC    | Sinocoking Coal &amp; Coke Chemical Industries Inc |\n| 1264707 | AACC   | Asset Acceptance Capital Corp                  |\n| 849116  | AACE   | Ace Cash Express Inc                           |\n| 1409430 | AAGC   | All American Gold Corp                         |\n| 948846  | AAI    | Airtran Holdings Inc                           |\n</code></pre>\n\n<p>Desired output: I would like to have the CIK instead of the ticker in a new dataframe otherwise identical to A. </p>\n\n<p>Now in SQL I could easily join on A.name_of_2nd_column = b.Ticker since the table would just have the entry in the 1st column repeated in every line (like the original csv) and the column would have a name but in pandas I cannot. I tried this code: </p>\n\n<pre><code>result = pd.merge(data, tix, how='left', left_on=[1] right_on=['Ticker'])\n</code></pre>\n\n<p>How do I tell pandas to use the 2nd column as the key and/or interpret the first column just as repeated values?</p>\n",
        "formatted_input": {
            "qid": 62011392,
            "link": "https://stackoverflow.com/questions/62011392/how-to-merge-or-join-a-stacked-dataframe-in-pandas",
            "question": {
                "title": "How to merge or join a stacked dataframe in pandas?",
                "ques_desc": "I cannot find this question answered elsewhere; I would like to do a SQL-like join in pandas but with the slight twist that one dataframe is stacked. I have created a dataframe A with a stacked column index from a csv file in pandas that looks as follows: The original csv had repeated what is in the 1st column for every entry like so: The original csv was the transposed version of this. Pandas chose to stack that when converting to dataframe. (I used this code: pd.read_csv(file, header = [0,1], index_col=0).T) In another csv/dataframe B I have for all of those so-called ticker symbols another ID that I would rather like to use: CIK. Desired output: I would like to have the CIK instead of the ticker in a new dataframe otherwise identical to A. Now in SQL I could easily join on A.name_of_2nd_column = b.Ticker since the table would just have the entry in the 1st column repeated in every line (like the original csv) and the column would have a name but in pandas I cannot. I tried this code: How do I tell pandas to use the 2nd column as the key and/or interpret the first column just as repeated values? "
            },
            "io": [
                "|           |      | 2013-01-04 | 2013-01-07 |\n|----------:|-----:|-----------:|-----------:|\n| Adj Close |  OWW | NaN        | NaN        |\n|   Close   | OXLC | 4.155157   | 4.147217   |\n|           |  OXM | 40.318089  | 42.988800  |\n|           |  OXY | 50.416079  | 62.934800  |\n",
                "|           |      | 2013-01-04 | 2013-01-07 |\n|----------:|-----:|-----------:|-----------:|\n| Adj Close |  OWW | NaN        | NaN        |\n|   Close   | OXLC | 4.155157   | 4.147217   |\n|   Close   |  OXM | 40.318089  | 42.988800  |\n|   Close   |  OXY | 50.416079  | 62.934800  |\n"
            ],
            "answer": {
                "ans_desc": "I was eventually able to do it the following way: ",
                "code": [
                    "ticker_2_CIK = dict(zip(tix.Ticker,tix.CIK))  # create a dict\n\ntmp = df.reset_index().assign(CIK=lambda x: x['ticker'].map(ticker_2_CIK)) # use dict to find the correct value for colum \n\n# data was unclean, some ticker symbols were created after the period my data is from \n# and data was incomplete with some tickers missing\nsolution = tmp.dropna(subset=['CIK']).astype({'CIK':int})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 27,
            "user_id": 4550291,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/acb64a55a7b853e6c9760892e2da22f5?s=128&d=identicon&r=PG&f=1",
            "display_name": "Clives-online",
            "link": "https://stackoverflow.com/users/4550291/clives-online"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 62140476,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1591043193,
        "creation_date": 1591038853,
        "last_edit_date": 1591041575,
        "question_id": 62139372,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62139372/read-csv-and-iterate-through-10-row-blocks",
        "title": "read csv and Iterate through 10 row blocks",
        "body": "<p>I am trying to read a CSV file and Iterate through 10-row blocks.\nThe data is quite unusual, with two columns and 10-row blocks.</p>\n\n<p>57485 rows x 2 columns in the format below:</p>\n\n<pre><code>Grid-ref=   1,148\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n\nGrid-ref=   1,311\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n\nGrid-ref=   1,312\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n</code></pre>\n\n<p>Every 10 rows consist of a grid reference and two records X/Y ref.\nThe grid reference and X value is in column 1, the Y value is in column 2, and then 9 rows with 12 columns, in column one.</p>\n\n<pre><code>The blocks (x) 0 - 9, represent months (Jan - December\nThe blocks (y) 0 - 9, represent years (1991-2000)\n\nSo for 0, is 1991 \n3020 is January 1991, 2820 is February 1991\n\nGrid-ref = 1 (X),148 (Y)\n</code></pre>\n\n<p>The code below reads 10 rows, but keeps repeating the first row in all following 10-row blocks??</p>\n\n<p>I don't understand why it keeps repeating the first row??\nAny suggestions to resolve this would be appreciated..</p>\n\n<pre><code>## Python 3.6\n\n## Read in the datasets (they are in CSV format) \ndata = pd.read_csv('cru-ts-2-10-1991-2000-cutdown.csv', skiprows=5, na_values =  [-999] )\n\n## View data    &gt;&gt; 57485 rows x 2 columns\n#print(data)\n#print(len(data))   ## len = 57485\n\n## header = pd.MultiIndex.from_product([['Grid-ref', 'Xref', 'Yref'], ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec' ] ])\n\n# df = pd.DataFrame(np.random.randn(10, 11), \n# index=['1991','1992','1993','1994','1995', '1996', '1997', '1998', '1999', '2000', '2001'], \n#  columns=header)\n\n# print(data.head(10)) ## prints chunks of 10 rows\n\ndef chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\nfor group in chunker(data, 10):\n    print(group)\n</code></pre>\n\n<p>The first two block:</p>\n\n<pre><code>                                      Grid-ref=   1   148\n0   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n1   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n2   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n3   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n4   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n5   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n6   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n7   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n8   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n9   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n                                        Grid-ref=   1    148\n10                                      Grid-ref=   1  311.0\n11    490  290  280  230  200  250  440  530  460 ...    NaN\n12    490  290  280  230  200  250  440  530  460 ...    NaN\n13    490  290  280  230  200  250  440  530  460 ...    NaN\n14    490  290  280  230  200  250  440  530  460 ...    NaN\n15    490  290  280  230  200  250  440  530  460 ...    NaN\n16    490  290  280  230  200  250  440  530  460 ...    NaN\n17    490  290  280  230  200  250  440  530  460 ...    NaN\n18    490  290  280  230  200  250  440  530  460 ...    NaN\n19    490  290  280  230  200  250  440  530  460 ...    NaN\n</code></pre>\n",
        "answer_body": "<p>Pandas is good for uniform columnar data. If your input isn't uniform, you can preprocess it and then load the dataframe. This one is easy, all you need to do is scan for grid headers and remove them. Since the data itself is numeric, separated by whitespace, a simple split will parse it. This example creates a list but if the dataset is large, it may be reasonable to write to an intermediate file instead.</p>\n\n<pre><code>import csv\nimport re\nimport pandas as  pd\n\ngrid_re = re.compile(r\"Grid-ref=\\s*(\\d+),(\\d+)\")\n\nwith open('test.csv') as fobj:\n    table = []\n    try:\n        while True:\n            # find next block\n            for line in fobj:\n                match = grid_re.search(line)\n                if match:\n                    grid_xy = list(match.groups())\n                    break\n            else:\n                raise StopIteration()\n            for _ in range(10):\n                line = next(fobj)\n                # add row plus grid columns\n                table.append(line.strip().split() + grid_xy)\n    except StopIteration:\n        pass\n\ndf = pd.DataFrame(table)\nprint(df)\n</code></pre>\n",
        "question_body": "<p>I am trying to read a CSV file and Iterate through 10-row blocks.\nThe data is quite unusual, with two columns and 10-row blocks.</p>\n\n<p>57485 rows x 2 columns in the format below:</p>\n\n<pre><code>Grid-ref=   1,148\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n\nGrid-ref=   1,311\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n\nGrid-ref=   1,312\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n</code></pre>\n\n<p>Every 10 rows consist of a grid reference and two records X/Y ref.\nThe grid reference and X value is in column 1, the Y value is in column 2, and then 9 rows with 12 columns, in column one.</p>\n\n<pre><code>The blocks (x) 0 - 9, represent months (Jan - December\nThe blocks (y) 0 - 9, represent years (1991-2000)\n\nSo for 0, is 1991 \n3020 is January 1991, 2820 is February 1991\n\nGrid-ref = 1 (X),148 (Y)\n</code></pre>\n\n<p>The code below reads 10 rows, but keeps repeating the first row in all following 10-row blocks??</p>\n\n<p>I don't understand why it keeps repeating the first row??\nAny suggestions to resolve this would be appreciated..</p>\n\n<pre><code>## Python 3.6\n\n## Read in the datasets (they are in CSV format) \ndata = pd.read_csv('cru-ts-2-10-1991-2000-cutdown.csv', skiprows=5, na_values =  [-999] )\n\n## View data    &gt;&gt; 57485 rows x 2 columns\n#print(data)\n#print(len(data))   ## len = 57485\n\n## header = pd.MultiIndex.from_product([['Grid-ref', 'Xref', 'Yref'], ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec' ] ])\n\n# df = pd.DataFrame(np.random.randn(10, 11), \n# index=['1991','1992','1993','1994','1995', '1996', '1997', '1998', '1999', '2000', '2001'], \n#  columns=header)\n\n# print(data.head(10)) ## prints chunks of 10 rows\n\ndef chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\nfor group in chunker(data, 10):\n    print(group)\n</code></pre>\n\n<p>The first two block:</p>\n\n<pre><code>                                      Grid-ref=   1   148\n0   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n1   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n2   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n3   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n4   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n5   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n6   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n7   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n8   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n9   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n                                        Grid-ref=   1    148\n10                                      Grid-ref=   1  311.0\n11    490  290  280  230  200  250  440  530  460 ...    NaN\n12    490  290  280  230  200  250  440  530  460 ...    NaN\n13    490  290  280  230  200  250  440  530  460 ...    NaN\n14    490  290  280  230  200  250  440  530  460 ...    NaN\n15    490  290  280  230  200  250  440  530  460 ...    NaN\n16    490  290  280  230  200  250  440  530  460 ...    NaN\n17    490  290  280  230  200  250  440  530  460 ...    NaN\n18    490  290  280  230  200  250  440  530  460 ...    NaN\n19    490  290  280  230  200  250  440  530  460 ...    NaN\n</code></pre>\n",
        "formatted_input": {
            "qid": 62139372,
            "link": "https://stackoverflow.com/questions/62139372/read-csv-and-iterate-through-10-row-blocks",
            "question": {
                "title": "read csv and Iterate through 10 row blocks",
                "ques_desc": "I am trying to read a CSV file and Iterate through 10-row blocks. The data is quite unusual, with two columns and 10-row blocks. 57485 rows x 2 columns in the format below: Every 10 rows consist of a grid reference and two records X/Y ref. The grid reference and X value is in column 1, the Y value is in column 2, and then 9 rows with 12 columns, in column one. The code below reads 10 rows, but keeps repeating the first row in all following 10-row blocks?? I don't understand why it keeps repeating the first row?? Any suggestions to resolve this would be appreciated.. The first two block: "
            },
            "io": [
                "Grid-ref=   1,148\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n\nGrid-ref=   1,311\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n\nGrid-ref=   1,312\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n",
                "                                      Grid-ref=   1   148\n0   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n1   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n2   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n3   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n4   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n5   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n6   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n7   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n8   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n9   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n                                        Grid-ref=   1    148\n10                                      Grid-ref=   1  311.0\n11    490  290  280  230  200  250  440  530  460 ...    NaN\n12    490  290  280  230  200  250  440  530  460 ...    NaN\n13    490  290  280  230  200  250  440  530  460 ...    NaN\n14    490  290  280  230  200  250  440  530  460 ...    NaN\n15    490  290  280  230  200  250  440  530  460 ...    NaN\n16    490  290  280  230  200  250  440  530  460 ...    NaN\n17    490  290  280  230  200  250  440  530  460 ...    NaN\n18    490  290  280  230  200  250  440  530  460 ...    NaN\n19    490  290  280  230  200  250  440  530  460 ...    NaN\n"
            ],
            "answer": {
                "ans_desc": "Pandas is good for uniform columnar data. If your input isn't uniform, you can preprocess it and then load the dataframe. This one is easy, all you need to do is scan for grid headers and remove them. Since the data itself is numeric, separated by whitespace, a simple split will parse it. This example creates a list but if the dataset is large, it may be reasonable to write to an intermediate file instead. ",
                "code": [
                    "import csv\nimport re\nimport pandas as  pd\n\ngrid_re = re.compile(r\"Grid-ref=\\s*(\\d+),(\\d+)\")\n\nwith open('test.csv') as fobj:\n    table = []\n    try:\n        while True:\n            # find next block\n            for line in fobj:\n                match = grid_re.search(line)\n                if match:\n                    grid_xy = list(match.groups())\n                    break\n            else:\n                raise StopIteration()\n            for _ in range(10):\n                line = next(fobj)\n                # add row plus grid columns\n                table.append(line.strip().split() + grid_xy)\n    except StopIteration:\n        pass\n\ndf = pd.DataFrame(table)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "duplicates"
        ],
        "owner": {
            "reputation": 767,
            "user_id": 3218151,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/41eba6916046161d4894eb64422413ab?s=128&d=identicon&r=PG&f=1",
            "display_name": "Elsalex",
            "link": "https://stackoverflow.com/users/3218151/elsalex"
        },
        "is_answered": true,
        "view_count": 123104,
        "accepted_answer_id": 32094352,
        "answer_count": 4,
        "score": 72,
        "last_activity_date": 1590985330,
        "creation_date": 1439982604,
        "last_edit_date": 1590985330,
        "question_id": 32093829,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/32093829/remove-duplicates-from-dataframe-based-on-two-columns-a-b-keeping-row-with-max",
        "title": "Remove duplicates from dataframe, based on two columns A,B, keeping row with max value in another column C",
        "body": "<p>I have a pandas dataframe which contains duplicates values according to two columns (A and B):</p>\n\n<pre><code>A B C\n1 2 1\n1 2 4\n2 7 1\n3 4 0\n3 4 8\n</code></pre>\n\n<p>I want to remove duplicates keeping the row with max value in column C. This would lead to: </p>\n\n<pre><code>A B C\n1 2 4\n2 7 1\n3 4 8\n</code></pre>\n\n<p>I cannot figure out how to do that. Should I use <code>drop_duplicates()</code>, something else?</p>\n",
        "answer_body": "<p>You can do it using group by:</p>\n\n<pre><code>c_maxes = df.groupby(['A', 'B']).C.transform(max)\ndf = df.loc[df.C == c_maxes]\n</code></pre>\n\n<p><code>c_maxes</code> is a <code>Series</code> of the maximum values of <code>C</code> in each group but which is of the same length and with the same index as <code>df</code>. If you haven't used <code>.transform</code> then printing <code>c_maxes</code> might be a good idea to see how it works. </p>\n\n<p>Another approach using <code>drop_duplicates</code> would be </p>\n\n<pre><code>df.sort('C').drop_duplicates(subset=['A', 'B'], take_last=True)\n</code></pre>\n\n<p>Not sure which is more efficient but I guess the first approach as it doesn't involve sorting. </p>\n\n<p><strong>EDIT:</strong>\nFrom <code>pandas 0.18</code> up the second solution would be </p>\n\n<pre><code>df.sort_values('C').drop_duplicates(subset=['A', 'B'], keep='last')\n</code></pre>\n\n<p>or, alternatively,</p>\n\n<pre><code>df.sort_values('C', ascending=False).drop_duplicates(subset=['A', 'B'])\n</code></pre>\n\n<p>In any case, the <code>groupby</code> solution seems to be significantly more performing: </p>\n\n<pre><code>%timeit -n 10 df.loc[df.groupby(['A', 'B']).C.max == df.C]\n10 loops, best of 3: 25.7 ms per loop\n\n%timeit -n 10 df.sort_values('C').drop_duplicates(subset=['A', 'B'], keep='last')\n10 loops, best of 3: 101 ms per loop\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe which contains duplicates values according to two columns (A and B):</p>\n\n<pre><code>A B C\n1 2 1\n1 2 4\n2 7 1\n3 4 0\n3 4 8\n</code></pre>\n\n<p>I want to remove duplicates keeping the row with max value in column C. This would lead to: </p>\n\n<pre><code>A B C\n1 2 4\n2 7 1\n3 4 8\n</code></pre>\n\n<p>I cannot figure out how to do that. Should I use <code>drop_duplicates()</code>, something else?</p>\n",
        "formatted_input": {
            "qid": 32093829,
            "link": "https://stackoverflow.com/questions/32093829/remove-duplicates-from-dataframe-based-on-two-columns-a-b-keeping-row-with-max",
            "question": {
                "title": "Remove duplicates from dataframe, based on two columns A,B, keeping row with max value in another column C",
                "ques_desc": "I have a pandas dataframe which contains duplicates values according to two columns (A and B): I want to remove duplicates keeping the row with max value in column C. This would lead to: I cannot figure out how to do that. Should I use , something else? "
            },
            "io": [
                "A B C\n1 2 1\n1 2 4\n2 7 1\n3 4 0\n3 4 8\n",
                "A B C\n1 2 4\n2 7 1\n3 4 8\n"
            ],
            "answer": {
                "ans_desc": "You can do it using group by: is a of the maximum values of in each group but which is of the same length and with the same index as . If you haven't used then printing might be a good idea to see how it works. Another approach using would be Not sure which is more efficient but I guess the first approach as it doesn't involve sorting. EDIT: From up the second solution would be or, alternatively, In any case, the solution seems to be significantly more performing: ",
                "code": [
                    "c_maxes = df.groupby(['A', 'B']).C.transform(max)\ndf = df.loc[df.C == c_maxes]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 39,
            "user_id": 6918891,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-RWZwur0HLTQ/AAAAAAAAAAI/AAAAAAAABMY/7EImnMau9PQ/photo.jpg?sz=128",
            "display_name": "Ashwin Singh",
            "link": "https://stackoverflow.com/users/6918891/ashwin-singh"
        },
        "is_answered": true,
        "view_count": 198,
        "accepted_answer_id": 62112943,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1590933715,
        "creation_date": 1590907131,
        "last_edit_date": 1590933715,
        "question_id": 62112525,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62112525/read-a-text-file-having-row-in-parenthesis-and-values-separated-by-comma-using-p",
        "title": "Read a Text file having row in parenthesis and values separated by comma using pandas",
        "body": "<p>I want to read a text file which contains data in parenthesis as row and values in it as column.The format of txt file is below :  </p>\n\n<pre><code>(a, b, c, d) (a1, b1, (c1,c12,c13), d1) (a2, b2, (c2,c22,c23), d2) (a3, b3, (c3,c32,c33), d3) (a4, b4, (c4,c42,c43), d4)\n</code></pre>\n\n<p>I want the data in this format :</p>\n\n<pre><code>a  b  c  d\na1 b1 c1 d1\na2 b2 c2 d2\na3 b3 c3 d3\na4 b4 c4 d4\n</code></pre>\n\n<p>When i am reading text file as csv file it reads all the data in one row only. It shows 1 row and all the columns.\nPlease help me with this problem.</p>\n",
        "answer_body": "<p>With builtin <code>pandas</code> functions (perhaps faster with large dataframe), you can use:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Use the standard `read_csv` function of pandas.\n# Note the lineterminator option.\ndf = pd.read_csv('data.dat', sep=\",\", lineterminator=\")\")\n# rename the 1st column (remove 1st char)\ndf.columns.values[0] = df.columns.values[0][1:]\n# remove the opening parenthesis for the 1st columns:\ndf.iloc[:, 0] = df.iloc[:, 0].str.replace('^\\ ?\\(', '')\n# remove the last line:\ndf = df[:-1]  \nprint(df)\n</code></pre>\n",
        "question_body": "<p>I want to read a text file which contains data in parenthesis as row and values in it as column.The format of txt file is below :  </p>\n\n<pre><code>(a, b, c, d) (a1, b1, (c1,c12,c13), d1) (a2, b2, (c2,c22,c23), d2) (a3, b3, (c3,c32,c33), d3) (a4, b4, (c4,c42,c43), d4)\n</code></pre>\n\n<p>I want the data in this format :</p>\n\n<pre><code>a  b  c  d\na1 b1 c1 d1\na2 b2 c2 d2\na3 b3 c3 d3\na4 b4 c4 d4\n</code></pre>\n\n<p>When i am reading text file as csv file it reads all the data in one row only. It shows 1 row and all the columns.\nPlease help me with this problem.</p>\n",
        "formatted_input": {
            "qid": 62112525,
            "link": "https://stackoverflow.com/questions/62112525/read-a-text-file-having-row-in-parenthesis-and-values-separated-by-comma-using-p",
            "question": {
                "title": "Read a Text file having row in parenthesis and values separated by comma using pandas",
                "ques_desc": "I want to read a text file which contains data in parenthesis as row and values in it as column.The format of txt file is below : I want the data in this format : When i am reading text file as csv file it reads all the data in one row only. It shows 1 row and all the columns. Please help me with this problem. "
            },
            "io": [
                "(a, b, c, d) (a1, b1, (c1,c12,c13), d1) (a2, b2, (c2,c22,c23), d2) (a3, b3, (c3,c32,c33), d3) (a4, b4, (c4,c42,c43), d4)\n",
                "a  b  c  d\na1 b1 c1 d1\na2 b2 c2 d2\na3 b3 c3 d3\na4 b4 c4 d4\n"
            ],
            "answer": {
                "ans_desc": "With builtin functions (perhaps faster with large dataframe), you can use: ",
                "code": [
                    "# Use the standard `read_csv` function of pandas.\n# Note the lineterminator option.\ndf = pd.read_csv('data.dat', sep=\",\", lineterminator=\")\")\n# rename the 1st column (remove 1st char)\ndf.columns.values[0] = df.columns.values[0][1:]\n# remove the opening parenthesis for the 1st columns:\ndf.iloc[:, 0] = df.iloc[:, 0].str.replace('^\\ ?\\(', '')\n# remove the last line:\ndf = df[:-1]  \nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "multi-index"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 13630392,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3a53423c127058f11fe6643ae1b5fa9?s=128&d=identicon&r=PG&f=1",
            "display_name": "sad303",
            "link": "https://stackoverflow.com/users/13630392/sad303"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 62054655,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1590623547,
        "creation_date": 1590617891,
        "question_id": 62053788,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62053788/how-to-sum-single-row-to-multiple-rows-in-pandas-dataframe-using-multiindex",
        "title": "How to sum single row to multiple rows in pandas dataframe using multiindex?",
        "body": "<p>My dataframe with Quarter and Week as MultiIndex:</p>\n\n<pre><code>Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    15  15  15\n          Q2-W16    16  16  16\n          Q2-W17    17  17  17\n          Q2-W18    18  18  18\n</code></pre>\n\n<p>I am trying to add the last row in Q1 (Q1-W04) to all the rows in Q2 (Q2-W15 through Q2-W18).  This is what I would like the dataframe to look like:</p>\n\n<pre><code>Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    19  19  19\n          Q2-W16    20  20  20\n          Q2-W17    21  21  21\n          Q2-W18    22  22  22\n</code></pre>\n\n<p>When I try to only specify the level 0 index and sumthe specific row, all Q2 values go to NaN.</p>\n\n<pre><code>df.loc['Q2'] += df.loc['Q1','Q1-W04'] \n\nQuarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    NaN NaN NaN\n          Q2-W16    NaN NaN NaN\n          Q2-W17    NaN NaN NaN\n          Q2-W18    NaN NaN NaN\n</code></pre>\n\n<p>I have figured out that if I specify both the level 0 and level 1 index, there is no problem. </p>\n\n<pre><code>df.loc['Q2','Q2-W15'] += df.loc['Q1','Q1-W04']\n\nQuarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    19  19  19\n          Q2-W16    16  16  16\n          Q2-W17    17  17  17\n          Q2-W18    18  18  18\n</code></pre>\n\n<p>Is there a way to sum the specific row to all the rows within the Q2 Level 0 index without having to call out each row individually by its level 1 index?</p>\n\n<p>Any insight/guidance would be greatly appreciated.</p>\n\n<p>Thank you. </p>\n",
        "answer_body": "<p>try this</p>\n\n<pre><code>df.loc['Q2'] = (df.loc['Q2'] + df.loc['Q1', 'Q1-W04']).values.tolist()\n</code></pre>\n\n<p>df.loc returns a DataFrame, to set the value it looks for the list or array. Hence the above.</p>\n",
        "question_body": "<p>My dataframe with Quarter and Week as MultiIndex:</p>\n\n<pre><code>Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    15  15  15\n          Q2-W16    16  16  16\n          Q2-W17    17  17  17\n          Q2-W18    18  18  18\n</code></pre>\n\n<p>I am trying to add the last row in Q1 (Q1-W04) to all the rows in Q2 (Q2-W15 through Q2-W18).  This is what I would like the dataframe to look like:</p>\n\n<pre><code>Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    19  19  19\n          Q2-W16    20  20  20\n          Q2-W17    21  21  21\n          Q2-W18    22  22  22\n</code></pre>\n\n<p>When I try to only specify the level 0 index and sumthe specific row, all Q2 values go to NaN.</p>\n\n<pre><code>df.loc['Q2'] += df.loc['Q1','Q1-W04'] \n\nQuarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    NaN NaN NaN\n          Q2-W16    NaN NaN NaN\n          Q2-W17    NaN NaN NaN\n          Q2-W18    NaN NaN NaN\n</code></pre>\n\n<p>I have figured out that if I specify both the level 0 and level 1 index, there is no problem. </p>\n\n<pre><code>df.loc['Q2','Q2-W15'] += df.loc['Q1','Q1-W04']\n\nQuarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    19  19  19\n          Q2-W16    16  16  16\n          Q2-W17    17  17  17\n          Q2-W18    18  18  18\n</code></pre>\n\n<p>Is there a way to sum the specific row to all the rows within the Q2 Level 0 index without having to call out each row individually by its level 1 index?</p>\n\n<p>Any insight/guidance would be greatly appreciated.</p>\n\n<p>Thank you. </p>\n",
        "formatted_input": {
            "qid": 62053788,
            "link": "https://stackoverflow.com/questions/62053788/how-to-sum-single-row-to-multiple-rows-in-pandas-dataframe-using-multiindex",
            "question": {
                "title": "How to sum single row to multiple rows in pandas dataframe using multiindex?",
                "ques_desc": "My dataframe with Quarter and Week as MultiIndex: I am trying to add the last row in Q1 (Q1-W04) to all the rows in Q2 (Q2-W15 through Q2-W18). This is what I would like the dataframe to look like: When I try to only specify the level 0 index and sumthe specific row, all Q2 values go to NaN. I have figured out that if I specify both the level 0 and level 1 index, there is no problem. Is there a way to sum the specific row to all the rows within the Q2 Level 0 index without having to call out each row individually by its level 1 index? Any insight/guidance would be greatly appreciated. Thank you. "
            },
            "io": [
                "Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    15  15  15\n          Q2-W16    16  16  16\n          Q2-W17    17  17  17\n          Q2-W18    18  18  18\n",
                "Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    19  19  19\n          Q2-W16    20  20  20\n          Q2-W17    21  21  21\n          Q2-W18    22  22  22\n"
            ],
            "answer": {
                "ans_desc": "try this df.loc returns a DataFrame, to set the value it looks for the list or array. Hence the above. ",
                "code": [
                    "df.loc['Q2'] = (df.loc['Q2'] + df.loc['Q1', 'Q1-W04']).values.tolist()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 89,
            "user_id": 12162070,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/2798169146884650/picture?type=large",
            "display_name": "Augusto Baldo",
            "link": "https://stackoverflow.com/users/12162070/augusto-baldo"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 62011596,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1590444897,
        "creation_date": 1590444502,
        "last_edit_date": 1590444672,
        "question_id": 62011520,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62011520/how-to-pick-some-values-of-a-column-and-make-another-one-with-them",
        "title": "How to pick some values of a column and make another one with them?",
        "body": "<p>This is a table similar to the one I'm working with</p>\n\n<pre><code>      A     B   \n0   12.2   43\n1   10.1   32\n2    3.4   34\n3   12.0   55\n4   40.6   31\n</code></pre>\n\n<p>And what I'm trying to do is take some values of the column A that follow a certain pattern and create another column with such values. For example, the column C would have only the values from A that are bigger than 12, and column D the ones smaller or equal:</p>\n\n<pre><code>      A     B      C     D \n0   12.2   43    12.2   NaN \n1   10.1   32     NaN   10.1\n2    3.4   34     NaN   3.4 \n3   12.0   55     NaN   12.0\n4   40.6   31    40.6   NaN\n</code></pre>\n\n<p>I've tried making a list for each group of values, but I can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. I think there's an esasier way to do that, but I can't seem to find it. How can I do that?</p>\n",
        "answer_body": "<p>And another version:</p>\n\n<pre><code>df['C'] = df.loc[df.A &gt; 12, 'A']\ndf['D'] = df.loc[df.A &lt;= 12, 'A']\n\nprint(df)\n</code></pre>\n\n<p>Prints:</p>\n\n<pre><code>      A   B     C     D\n0  12.2  43  12.2   NaN\n1  10.1  32   NaN  10.1\n2   3.4  34   NaN   3.4\n3  12.0  55   NaN  12.0\n4  40.6  31  40.6   NaN\n</code></pre>\n",
        "question_body": "<p>This is a table similar to the one I'm working with</p>\n\n<pre><code>      A     B   \n0   12.2   43\n1   10.1   32\n2    3.4   34\n3   12.0   55\n4   40.6   31\n</code></pre>\n\n<p>And what I'm trying to do is take some values of the column A that follow a certain pattern and create another column with such values. For example, the column C would have only the values from A that are bigger than 12, and column D the ones smaller or equal:</p>\n\n<pre><code>      A     B      C     D \n0   12.2   43    12.2   NaN \n1   10.1   32     NaN   10.1\n2    3.4   34     NaN   3.4 \n3   12.0   55     NaN   12.0\n4   40.6   31    40.6   NaN\n</code></pre>\n\n<p>I've tried making a list for each group of values, but I can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. I think there's an esasier way to do that, but I can't seem to find it. How can I do that?</p>\n",
        "formatted_input": {
            "qid": 62011520,
            "link": "https://stackoverflow.com/questions/62011520/how-to-pick-some-values-of-a-column-and-make-another-one-with-them",
            "question": {
                "title": "How to pick some values of a column and make another one with them?",
                "ques_desc": "This is a table similar to the one I'm working with And what I'm trying to do is take some values of the column A that follow a certain pattern and create another column with such values. For example, the column C would have only the values from A that are bigger than 12, and column D the ones smaller or equal: I've tried making a list for each group of values, but I can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. I think there's an esasier way to do that, but I can't seem to find it. How can I do that? "
            },
            "io": [
                "      A     B   \n0   12.2   43\n1   10.1   32\n2    3.4   34\n3   12.0   55\n4   40.6   31\n",
                "      A     B      C     D \n0   12.2   43    12.2   NaN \n1   10.1   32     NaN   10.1\n2    3.4   34     NaN   3.4 \n3   12.0   55     NaN   12.0\n4   40.6   31    40.6   NaN\n"
            ],
            "answer": {
                "ans_desc": "And another version: Prints: ",
                "code": [
                    "df['C'] = df.loc[df.A > 12, 'A']\ndf['D'] = df.loc[df.A <= 12, 'A']\n\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 13599745,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f5d193f0764e279fa87735e1c82fcd7c?s=128&d=identicon&r=PG&f=1",
            "display_name": "user13599745",
            "link": "https://stackoverflow.com/users/13599745/user13599745"
        },
        "is_answered": true,
        "view_count": 64,
        "accepted_answer_id": 61979656,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1590279307,
        "creation_date": 1590274298,
        "question_id": 61979568,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61979568/how-can-i-find-and-store-how-many-columns-it-takes-to-reach-a-value-greater-than",
        "title": "How can I find and store how many columns it takes to reach a value greater than the first value in each row?",
        "body": "<pre><code>import pandas as pd\ndf = {'a': [3,4,5], 'b': [1,2,3], 'c': [4,3,3], 'd': [1,5,4], 'e': [9,4,6]}\ndf1 = pd.DataFrame(df, columns = ['a', 'b', 'c', 'd', 'e'])\ndg = {'b': [2,3,4]}\ndf2 = pd.DataFrame(dg, columns = ['b'])\n</code></pre>\n\n<p>Original dataframe is df1. For each row, I want to find the first time a value is bigger than the value in the first column and store it in a new dataframe.  </p>\n\n<pre><code>df1\n    a   b   c   d   e\n0   3   1   4   1   9\n1   4   2   3   5   4\n2   5   3   3   4   6\n</code></pre>\n\n<p>df2 is the resulting dataframe. For example, for df1 row 1; the first value is 3 and the first value bigger than 3 is 4 (column c). Hence in df2 row 1, we store 2 (there are two columns from column a to c). For df1 row 2, the first value is 4 and the first value bigger than 4 is 5 (column d). Hence in df2 row 2, we store 3 (there are three columns from column a to d). For df1 row 3, the first value is 5 and the first value bigger than 5 is 6 (column e). Hence in df2 row 3, we store 4 (there are four columns from column a to e).</p>\n\n<pre><code>df2\n\n    b\n0   2\n1   3\n2   4\n</code></pre>\n\n<p>I would appreciate the help.</p>\n",
        "answer_body": "<p>In your case we can do <code>sub</code> , if the value <code>gt</code> than 0 , we get the id with <code>idxmax</code></p>\n\n<pre><code>s=df1.columns.get_indexer(df1.drop('a',1).sub(df1.a,0).ge(0).idxmax(1))\narray([1, 1, 3])\ndf['New']=s\n</code></pre>\n",
        "question_body": "<pre><code>import pandas as pd\ndf = {'a': [3,4,5], 'b': [1,2,3], 'c': [4,3,3], 'd': [1,5,4], 'e': [9,4,6]}\ndf1 = pd.DataFrame(df, columns = ['a', 'b', 'c', 'd', 'e'])\ndg = {'b': [2,3,4]}\ndf2 = pd.DataFrame(dg, columns = ['b'])\n</code></pre>\n\n<p>Original dataframe is df1. For each row, I want to find the first time a value is bigger than the value in the first column and store it in a new dataframe.  </p>\n\n<pre><code>df1\n    a   b   c   d   e\n0   3   1   4   1   9\n1   4   2   3   5   4\n2   5   3   3   4   6\n</code></pre>\n\n<p>df2 is the resulting dataframe. For example, for df1 row 1; the first value is 3 and the first value bigger than 3 is 4 (column c). Hence in df2 row 1, we store 2 (there are two columns from column a to c). For df1 row 2, the first value is 4 and the first value bigger than 4 is 5 (column d). Hence in df2 row 2, we store 3 (there are three columns from column a to d). For df1 row 3, the first value is 5 and the first value bigger than 5 is 6 (column e). Hence in df2 row 3, we store 4 (there are four columns from column a to e).</p>\n\n<pre><code>df2\n\n    b\n0   2\n1   3\n2   4\n</code></pre>\n\n<p>I would appreciate the help.</p>\n",
        "formatted_input": {
            "qid": 61979568,
            "link": "https://stackoverflow.com/questions/61979568/how-can-i-find-and-store-how-many-columns-it-takes-to-reach-a-value-greater-than",
            "question": {
                "title": "How can I find and store how many columns it takes to reach a value greater than the first value in each row?",
                "ques_desc": " Original dataframe is df1. For each row, I want to find the first time a value is bigger than the value in the first column and store it in a new dataframe. df2 is the resulting dataframe. For example, for df1 row 1; the first value is 3 and the first value bigger than 3 is 4 (column c). Hence in df2 row 1, we store 2 (there are two columns from column a to c). For df1 row 2, the first value is 4 and the first value bigger than 4 is 5 (column d). Hence in df2 row 2, we store 3 (there are three columns from column a to d). For df1 row 3, the first value is 5 and the first value bigger than 5 is 6 (column e). Hence in df2 row 3, we store 4 (there are four columns from column a to e). I would appreciate the help. "
            },
            "io": [
                "df1\n    a   b   c   d   e\n0   3   1   4   1   9\n1   4   2   3   5   4\n2   5   3   3   4   6\n",
                "df2\n\n    b\n0   2\n1   3\n2   4\n"
            ],
            "answer": {
                "ans_desc": "In your case we can do , if the value than 0 , we get the id with ",
                "code": [
                    "s=df1.columns.get_indexer(df1.drop('a',1).sub(df1.a,0).ge(0).idxmax(1))\narray([1, 1, 3])\ndf['New']=s\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "calculated-columns",
            "difference"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 13599745,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f5d193f0764e279fa87735e1c82fcd7c?s=128&d=identicon&r=PG&f=1",
            "display_name": "user13599745",
            "link": "https://stackoverflow.com/users/13599745/user13599745"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 61969676,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1590232300,
        "creation_date": 1590223780,
        "question_id": 61969568,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61969568/how-can-i-create-a-new-dataframe-by-subtracting-the-first-column-from-every-othe",
        "title": "How can I create a new dataframe by subtracting the first column from every other column?",
        "body": "<pre><code>import pandas as pd\ndf = {'a': [1,1,1], 'b': [3,3,3,], 'c': [5,5,5,], 'd': [7,7,7], 'e': [9,9,9]}\ndf1 = pd.DataFrame(df, columns = ['a', 'b', 'c', 'd', 'e'])\ndg = {'b': [2,2,2], 'c': [4,4,4], 'd': [6,6,6], 'e': [8,8,8]}\ndf2 = pd.DataFrame(dg, columns = ['b', 'c', 'd', 'e'])\n</code></pre>\n\n<pre><code>df1\n</code></pre>\n\n<pre><code>    a   b   c   d   e\n0   1   3   5   7   9\n1   1   3   5   7   9\n2   1   3   5   7   9\n</code></pre>\n\n<p>df1 is my original dataframe. I want to create another dataframe by substracting column a from every other column (taking the difference between column a and all other columns).</p>\n\n<pre><code>df2\n</code></pre>\n\n<pre><code>    b   c   d   e\n0   2   4   6   8\n1   2   4   6   8\n2   2   4   6   8\n</code></pre>\n\n<p>df2 is the outcome. </p>\n\n<p>I would appreciate your help.</p>\n",
        "answer_body": "<pre><code>df = df1.loc[:,'b':].apply(lambda x: x-df1['a'])\nprint(df)\n</code></pre>\n\n<p>Prints:</p>\n\n<pre><code>   b  c  d  e\n0  2  4  6  8\n1  2  4  6  8\n2  2  4  6  8\n</code></pre>\n",
        "question_body": "<pre><code>import pandas as pd\ndf = {'a': [1,1,1], 'b': [3,3,3,], 'c': [5,5,5,], 'd': [7,7,7], 'e': [9,9,9]}\ndf1 = pd.DataFrame(df, columns = ['a', 'b', 'c', 'd', 'e'])\ndg = {'b': [2,2,2], 'c': [4,4,4], 'd': [6,6,6], 'e': [8,8,8]}\ndf2 = pd.DataFrame(dg, columns = ['b', 'c', 'd', 'e'])\n</code></pre>\n\n<pre><code>df1\n</code></pre>\n\n<pre><code>    a   b   c   d   e\n0   1   3   5   7   9\n1   1   3   5   7   9\n2   1   3   5   7   9\n</code></pre>\n\n<p>df1 is my original dataframe. I want to create another dataframe by substracting column a from every other column (taking the difference between column a and all other columns).</p>\n\n<pre><code>df2\n</code></pre>\n\n<pre><code>    b   c   d   e\n0   2   4   6   8\n1   2   4   6   8\n2   2   4   6   8\n</code></pre>\n\n<p>df2 is the outcome. </p>\n\n<p>I would appreciate your help.</p>\n",
        "formatted_input": {
            "qid": 61969568,
            "link": "https://stackoverflow.com/questions/61969568/how-can-i-create-a-new-dataframe-by-subtracting-the-first-column-from-every-othe",
            "question": {
                "title": "How can I create a new dataframe by subtracting the first column from every other column?",
                "ques_desc": " df1 is my original dataframe. I want to create another dataframe by substracting column a from every other column (taking the difference between column a and all other columns). df2 is the outcome. I would appreciate your help. "
            },
            "io": [
                "    a   b   c   d   e\n0   1   3   5   7   9\n1   1   3   5   7   9\n2   1   3   5   7   9\n",
                "    b   c   d   e\n0   2   4   6   8\n1   2   4   6   8\n2   2   4   6   8\n"
            ],
            "answer": {
                "ans_desc": " Prints: ",
                "code": [
                    "df = df1.loc[:,'b':].apply(lambda x: x-df1['a'])\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 3340,
            "user_id": 4451315,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/09c02dbbd18039996ae87cfbfe02dd75?s=128&d=identicon&r=PG&f=1",
            "display_name": "ignoring_gravity",
            "link": "https://stackoverflow.com/users/4451315/ignoring-gravity"
        },
        "is_answered": true,
        "view_count": 98,
        "accepted_answer_id": 61970550,
        "answer_count": 6,
        "score": 0,
        "last_activity_date": 1590229225,
        "creation_date": 1590225232,
        "question_id": 61969815,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61969815/reverse-the-order-of-elements-by-group",
        "title": "Reverse the order of elements by group",
        "body": "<p>Say I have a DataFrame like this:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'a': [1,1,1,1,2,2,2,2], 'b': [1,2,3,4,5,6,7,8]})\n</code></pre>\n\n<p>which looks like this</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   a  b\n0  1  1\n1  1  2\n2  1  3\n3  1  4\n4  2  5\n5  2  6\n6  2  7\n7  2  8\n</code></pre>\n\n<p>I would like to reverse its elements <em>within each group</em>, where column <code>a</code> determines the group. So, the desired output would be</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   a  b\n0  1  4\n1  1  3\n2  1  2\n3  1  1\n4  2  8\n5  2  7\n6  2  6\n7  2  5\n</code></pre>\n\n<p>How can I do this?</p>\n",
        "answer_body": "<p>This solution should achieve what the OP wants, which is to reverse(not to sort) the order of b for each a.</p>\n\n<pre><code>(\n    df.groupby('a', sort=False)\n    .apply(lambda x: x.iloc[::-1])\n    .reset_index(drop=True)\n)\n</code></pre>\n",
        "question_body": "<p>Say I have a DataFrame like this:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'a': [1,1,1,1,2,2,2,2], 'b': [1,2,3,4,5,6,7,8]})\n</code></pre>\n\n<p>which looks like this</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   a  b\n0  1  1\n1  1  2\n2  1  3\n3  1  4\n4  2  5\n5  2  6\n6  2  7\n7  2  8\n</code></pre>\n\n<p>I would like to reverse its elements <em>within each group</em>, where column <code>a</code> determines the group. So, the desired output would be</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   a  b\n0  1  4\n1  1  3\n2  1  2\n3  1  1\n4  2  8\n5  2  7\n6  2  6\n7  2  5\n</code></pre>\n\n<p>How can I do this?</p>\n",
        "formatted_input": {
            "qid": 61969815,
            "link": "https://stackoverflow.com/questions/61969815/reverse-the-order-of-elements-by-group",
            "question": {
                "title": "Reverse the order of elements by group",
                "ques_desc": "Say I have a DataFrame like this: which looks like this I would like to reverse its elements within each group, where column determines the group. So, the desired output would be How can I do this? "
            },
            "io": [
                "   a  b\n0  1  1\n1  1  2\n2  1  3\n3  1  4\n4  2  5\n5  2  6\n6  2  7\n7  2  8\n",
                "   a  b\n0  1  4\n1  1  3\n2  1  2\n3  1  1\n4  2  8\n5  2  7\n6  2  6\n7  2  5\n"
            ],
            "answer": {
                "ans_desc": "This solution should achieve what the OP wants, which is to reverse(not to sort) the order of b for each a. ",
                "code": [
                    "(\n    df.groupby('a', sort=False)\n    .apply(lambda x: x.iloc[::-1])\n    .reset_index(drop=True)\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 13579519,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GhNrOZVzDM-bDW768wXWfZ_9uOAtlvqtNh0Sx6-LQ=k-s128",
            "display_name": "Kim",
            "link": "https://stackoverflow.com/users/13579519/kim"
        },
        "is_answered": true,
        "view_count": 56,
        "accepted_answer_id": 61928139,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1590105467,
        "creation_date": 1590038790,
        "last_edit_date": 1590105467,
        "question_id": 61927861,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61927861/add-column-in-dataframe-from-make-list",
        "title": "Add column in dataframe from make list",
        "body": "<blockquote>\n  <p>a*.csv</p>\n</blockquote>\n\n<pre><code> D        A     B     E       F\n\npark    KOREA   1   SUM1    hello1\n\nmichel  France  3   SUM3    hello3\n\npark2   USA     4   SUM4    hello4\n</code></pre>\n\n<blockquote>\n  <p>b*.csv</p>\n</blockquote>\n\n<pre><code>A       B   C\n\nKOREA   1   2020\n\nKOREA   2   177\n\nFrance  3   2020\n\nUSA     4   43\n\nSPAIN   7   67\n</code></pre>\n\n<blockquote>\n  <p>example</p>\n</blockquote>\n\n<pre><code> D        A     B    C(add)  E        F\n\npark    KOREA   1   2020    SUM1    hello1\n\nmichel  France  3   2020    SUM3    hello3\n\npark2   USA     4   3       SUM4    hello4\n\npark3   SPAIN   7   67      SUM5    hello5\n\npark4   USA     8   177     SUM6    hello6\n\npark5   KOREA   11  584     SUM7    hello7\n\npark6   DEN     5   43      SUM8    hello8\n</code></pre>\n\n<p>I edited the content and then restored it\nbecause the answer didn't solve it and I solved it myself<p></p>\n\n<p>I'll end the question after commenting it solution </p>\n\n<p>but I chose one answer to close this question<p></p>\n",
        "answer_body": "<p>I think you can use the merge() function.</p>\n\n<pre><code>try_a = glob.glob('a*.csv')\ntry_b = glob.glob('b*.csv')\nlst_a = []\nlst_b = []\nfor (i,j) in zip(try_a,try_b):\n lst_a.append(i)\n lst_b.append(j)\ndf_a = pd.concat(lst_a)\ndf_b = pd.concat(lst_b)\ndf_a.set_index('d', inplace= True)\ndf = pd.DataFrame.merge(df_a,df_b, how = 'inner', left_index = True).reset_index()\n</code></pre>\n\n<p>I hope it works!!</p>\n",
        "question_body": "<blockquote>\n  <p>a*.csv</p>\n</blockquote>\n\n<pre><code> D        A     B     E       F\n\npark    KOREA   1   SUM1    hello1\n\nmichel  France  3   SUM3    hello3\n\npark2   USA     4   SUM4    hello4\n</code></pre>\n\n<blockquote>\n  <p>b*.csv</p>\n</blockquote>\n\n<pre><code>A       B   C\n\nKOREA   1   2020\n\nKOREA   2   177\n\nFrance  3   2020\n\nUSA     4   43\n\nSPAIN   7   67\n</code></pre>\n\n<blockquote>\n  <p>example</p>\n</blockquote>\n\n<pre><code> D        A     B    C(add)  E        F\n\npark    KOREA   1   2020    SUM1    hello1\n\nmichel  France  3   2020    SUM3    hello3\n\npark2   USA     4   3       SUM4    hello4\n\npark3   SPAIN   7   67      SUM5    hello5\n\npark4   USA     8   177     SUM6    hello6\n\npark5   KOREA   11  584     SUM7    hello7\n\npark6   DEN     5   43      SUM8    hello8\n</code></pre>\n\n<p>I edited the content and then restored it\nbecause the answer didn't solve it and I solved it myself<p></p>\n\n<p>I'll end the question after commenting it solution </p>\n\n<p>but I chose one answer to close this question<p></p>\n",
        "formatted_input": {
            "qid": 61927861,
            "link": "https://stackoverflow.com/questions/61927861/add-column-in-dataframe-from-make-list",
            "question": {
                "title": "Add column in dataframe from make list",
                "ques_desc": " a*.csv b*.csv example I edited the content and then restored it because the answer didn't solve it and I solved it myself I'll end the question after commenting it solution but I chose one answer to close this question "
            },
            "io": [
                " D        A     B     E       F\n\npark    KOREA   1   SUM1    hello1\n\nmichel  France  3   SUM3    hello3\n\npark2   USA     4   SUM4    hello4\n",
                "A       B   C\n\nKOREA   1   2020\n\nKOREA   2   177\n\nFrance  3   2020\n\nUSA     4   43\n\nSPAIN   7   67\n"
            ],
            "answer": {
                "ans_desc": "I think you can use the merge() function. I hope it works!! ",
                "code": [
                    "try_a = glob.glob('a*.csv')\ntry_b = glob.glob('b*.csv')\nlst_a = []\nlst_b = []\nfor (i,j) in zip(try_a,try_b):\n lst_a.append(i)\n lst_b.append(j)\ndf_a = pd.concat(lst_a)\ndf_b = pd.concat(lst_b)\ndf_a.set_index('d', inplace= True)\ndf = pd.DataFrame.merge(df_a,df_b, how = 'inner', left_index = True).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "matrix"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 3408820,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/54d70cdaaf3aefc3774472f282aa0f22?s=128&d=identicon&r=PG&f=1",
            "display_name": "Arxk",
            "link": "https://stackoverflow.com/users/3408820/arxk"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 61926284,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1590028807,
        "creation_date": 1590027744,
        "last_edit_date": 1590028313,
        "question_id": 61926146,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61926146/python-convert-matrices-to-permutations-table",
        "title": "Python: Convert matrices to permutations table",
        "body": "<p>Given a set of ids, I need to get the values from a matrix (time A &amp; B) for each id combination, and create a dataframe appending the values for all the permutations.</p>\n\n<p>I have been able to do it by creating the permutations dataframe and then iterating through it while looking &amp; filling the values. However I need to do this for ~3000 ids, not 3, and I don't know how to do it efficiently.</p>\n\n<p>Can I generate a Time A/B dataframe as my example without having to iterate through 9000000* rows? I know I shouldn't be iterating though a dataframe however I haven't found an alternative yet.</p>\n\n<p>Ids (3):</p>\n\n<pre><code>[15, 24, 38]\n</code></pre>\n\n<p>Time A matrix (3x3):</p>\n\n<pre><code>id          15        24      38\n15          0         1.8     1.7\n24          1.2       0       1.9\n38          1.5       1.3     0\n</code></pre>\n\n<p>Time B matrix (3x3):</p>\n\n<pre><code>id          15        24      38\n15          0         88.7    87.3\n24          42.2      0       32.7\n38          65.6      13.5    0\n</code></pre>\n\n<p>Time A/B dataframe (6):</p>\n\n<pre><code>id_start    id_end    A       B\n15          24        1.8     88.7\n15          38        1.9     32.7\n24          15        1.2     42.2\n24          38        1.9     65.6\n38          15        1.5     65.6\n38          24        1.3     13.5\n</code></pre>\n",
        "answer_body": "<p>Here you go:</p>\n\n<pre><code># drop set_index('id') if `id` is already index\n(pd.concat([timeA.set_index('id').stack().to_frame(name='A'),\n           timeB.set_index('id').stack().to_frame(name='B')], axis=1)\n   .rename_axis(index=['id_start','id_end'])\n   .query('id_start != id_end')\n   .reset_index()\n)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   id_start  id_end    A     B\n1        15      24  1.8  88.7\n2        15      38  1.7  87.3\n3        24      15  1.2  42.2\n5        24      38  1.9  32.7\n6        38      15  1.5  65.6\n7        38      24  1.3  13.5\n</code></pre>\n",
        "question_body": "<p>Given a set of ids, I need to get the values from a matrix (time A &amp; B) for each id combination, and create a dataframe appending the values for all the permutations.</p>\n\n<p>I have been able to do it by creating the permutations dataframe and then iterating through it while looking &amp; filling the values. However I need to do this for ~3000 ids, not 3, and I don't know how to do it efficiently.</p>\n\n<p>Can I generate a Time A/B dataframe as my example without having to iterate through 9000000* rows? I know I shouldn't be iterating though a dataframe however I haven't found an alternative yet.</p>\n\n<p>Ids (3):</p>\n\n<pre><code>[15, 24, 38]\n</code></pre>\n\n<p>Time A matrix (3x3):</p>\n\n<pre><code>id          15        24      38\n15          0         1.8     1.7\n24          1.2       0       1.9\n38          1.5       1.3     0\n</code></pre>\n\n<p>Time B matrix (3x3):</p>\n\n<pre><code>id          15        24      38\n15          0         88.7    87.3\n24          42.2      0       32.7\n38          65.6      13.5    0\n</code></pre>\n\n<p>Time A/B dataframe (6):</p>\n\n<pre><code>id_start    id_end    A       B\n15          24        1.8     88.7\n15          38        1.9     32.7\n24          15        1.2     42.2\n24          38        1.9     65.6\n38          15        1.5     65.6\n38          24        1.3     13.5\n</code></pre>\n",
        "formatted_input": {
            "qid": 61926146,
            "link": "https://stackoverflow.com/questions/61926146/python-convert-matrices-to-permutations-table",
            "question": {
                "title": "Python: Convert matrices to permutations table",
                "ques_desc": "Given a set of ids, I need to get the values from a matrix (time A & B) for each id combination, and create a dataframe appending the values for all the permutations. I have been able to do it by creating the permutations dataframe and then iterating through it while looking & filling the values. However I need to do this for ~3000 ids, not 3, and I don't know how to do it efficiently. Can I generate a Time A/B dataframe as my example without having to iterate through 9000000* rows? I know I shouldn't be iterating though a dataframe however I haven't found an alternative yet. Ids (3): Time A matrix (3x3): Time B matrix (3x3): Time A/B dataframe (6): "
            },
            "io": [
                "id          15        24      38\n15          0         1.8     1.7\n24          1.2       0       1.9\n38          1.5       1.3     0\n",
                "id          15        24      38\n15          0         88.7    87.3\n24          42.2      0       32.7\n38          65.6      13.5    0\n"
            ],
            "answer": {
                "ans_desc": "Here you go: Output: ",
                "code": [
                    "# drop set_index('id') if `id` is already index\n(pd.concat([timeA.set_index('id').stack().to_frame(name='A'),\n           timeB.set_index('id').stack().to_frame(name='B')], axis=1)\n   .rename_axis(index=['id_start','id_end'])\n   .query('id_start != id_end')\n   .reset_index()\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 1501,
            "user_id": 7747336,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/6dd274b841adccba7bf69dd44de180da?s=128&d=identicon&r=PG&f=1",
            "display_name": "CHRD",
            "link": "https://stackoverflow.com/users/7747336/chrd"
        },
        "is_answered": true,
        "view_count": 354,
        "accepted_answer_id": 61869338,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1589864672,
        "creation_date": 1589802444,
        "last_edit_date": 1589816380,
        "question_id": 61868871,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61868871/pandas-groupby-expanding-df-based-on-unique-values",
        "title": "pandas groupby expanding df based on unique values",
        "body": "<p>I have <code>df</code> below:</p>\n\n<pre><code>df = pd.DataFrame({\n    'ID': ['a', 'a', 'a', 'b', 'c', 'c'],\n    'V1': [False, False, True, True, False, True],\n    'V2': ['A', 'B', 'C', 'B', 'B', 'C']\n})\n</code></pre>\n\n<p>I want to achieve the following. For each unique <code>ID</code>, the bottom row is <code>True</code> (this is <code>V1</code>). I want to count how many times each unique value of <code>V2</code> occurs where <code>V1==True</code>. This part would be achieved by something like:</p>\n\n<pre><code>df.groupby('V2').V1.sum()\n</code></pre>\n\n<p>However, I also want to add, for each unique value of <code>V2</code>, a column indicating how many times that value occurred after the point where <code>V1==True</code> for the <code>V2</code> value indicated by the row. I understand this might sound confusing; here's how the output woud look like in this example:</p>\n\n<pre><code>df\n    V2  V1  A   B   C\n0   A   0   0   0   0\n1   B   1   0   0   0\n2   C   2   1   2   0\n</code></pre>\n\n<p>It is important that the solution is general enough to be applicable on a similar case with more unique values than just <code>A</code>, <code>B</code> and <code>C</code>.</p>\n\n<p><strong>UPDATE</strong>\n<br>\nAs a bonus, I am also interested in how, instead of the count, one can instead return the sum of some value column, under the same conditions, divided by the corresponding <code>\"count\"</code> in the rows. Example: suppose we now depart from <code>df</code> below instead:</p>\n\n<pre><code>df = pd.DataFrame({\n    'ID': ['a', 'a', 'a', 'b', 'c', 'c'],\n    'V1': [False, False, True, True, False, True],\n    'V2': ['A', 'B', 'C', 'B', 'B', 'C'],\n    'V3': [1, 2, 3, 4, 5, 6],\n})\n</code></pre>\n\n<p>The output would need to sum <code>V3</code> for the cases indicated by the counts in the solution by @jezrael, and divide that number by <code>V1</code>. The output would instead look like:</p>\n\n<pre><code>df\n    V2  V1  A   B   C\n0   A   0   0   0   0\n1   B   1   0   0   0\n2   C   2   1   3.5 0\n</code></pre>\n",
        "answer_body": "<p>First aggregate <code>sum</code>:</p>\n\n<pre><code>df1 = df.groupby('V2').V1.sum().astype(int).reset_index()\nprint (df1)\n  V2  V1\n0  A   0\n1  B   1\n2  C   2\n</code></pre>\n\n<p>Then grouping by <code>ID</code> and create heper column by last value by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> and <code>last</code>, then remove last rows of <code>ID</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.duplicated.html\" rel=\"nofollow noreferrer\"><code>Series.duplicated</code></a> and use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html\" rel=\"nofollow noreferrer\"><code>crosstab</code></a> for counts, add all possible unique values of <code>V2</code> and last append to <code>df1</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html\" rel=\"nofollow noreferrer\"><code>DataFrame.join</code></a>:</p>\n\n<pre><code>val = df['V2'].unique()\ndf['new'] = df.groupby('ID').V2.transform('last')\ndf = df[df.duplicated('ID', keep='last')]\n\ndf = pd.crosstab(df['new'], df['V2']).reindex(columns=val, index=val, fill_value=0)\n\ndf = df1.join(df, on='V2')\nprint (df)\n  V2  V1  A  B  C\n0  A   0  0  0  0\n1  B   1  0  0  0\n2  C   2  1  2  0\n</code></pre>\n\n<p><strong>UPDATE</strong></p>\n\n<p>The updated part of the question should be possible to achieve by changing the <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html\" rel=\"nofollow noreferrer\"><code>crosstab</code></a> part with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html\" rel=\"nofollow noreferrer\"><code>pivot table</code></a>:</p>\n\n<pre><code>df = df.pivot_table(\n    index='n',\n    columns='V2',\n    aggfunc=({\n        'V3': 'mean'\n    })\n).V3.reindex(columns=v, index=v, fill_value=0)\n</code></pre>\n",
        "question_body": "<p>I have <code>df</code> below:</p>\n\n<pre><code>df = pd.DataFrame({\n    'ID': ['a', 'a', 'a', 'b', 'c', 'c'],\n    'V1': [False, False, True, True, False, True],\n    'V2': ['A', 'B', 'C', 'B', 'B', 'C']\n})\n</code></pre>\n\n<p>I want to achieve the following. For each unique <code>ID</code>, the bottom row is <code>True</code> (this is <code>V1</code>). I want to count how many times each unique value of <code>V2</code> occurs where <code>V1==True</code>. This part would be achieved by something like:</p>\n\n<pre><code>df.groupby('V2').V1.sum()\n</code></pre>\n\n<p>However, I also want to add, for each unique value of <code>V2</code>, a column indicating how many times that value occurred after the point where <code>V1==True</code> for the <code>V2</code> value indicated by the row. I understand this might sound confusing; here's how the output woud look like in this example:</p>\n\n<pre><code>df\n    V2  V1  A   B   C\n0   A   0   0   0   0\n1   B   1   0   0   0\n2   C   2   1   2   0\n</code></pre>\n\n<p>It is important that the solution is general enough to be applicable on a similar case with more unique values than just <code>A</code>, <code>B</code> and <code>C</code>.</p>\n\n<p><strong>UPDATE</strong>\n<br>\nAs a bonus, I am also interested in how, instead of the count, one can instead return the sum of some value column, under the same conditions, divided by the corresponding <code>\"count\"</code> in the rows. Example: suppose we now depart from <code>df</code> below instead:</p>\n\n<pre><code>df = pd.DataFrame({\n    'ID': ['a', 'a', 'a', 'b', 'c', 'c'],\n    'V1': [False, False, True, True, False, True],\n    'V2': ['A', 'B', 'C', 'B', 'B', 'C'],\n    'V3': [1, 2, 3, 4, 5, 6],\n})\n</code></pre>\n\n<p>The output would need to sum <code>V3</code> for the cases indicated by the counts in the solution by @jezrael, and divide that number by <code>V1</code>. The output would instead look like:</p>\n\n<pre><code>df\n    V2  V1  A   B   C\n0   A   0   0   0   0\n1   B   1   0   0   0\n2   C   2   1   3.5 0\n</code></pre>\n",
        "formatted_input": {
            "qid": 61868871,
            "link": "https://stackoverflow.com/questions/61868871/pandas-groupby-expanding-df-based-on-unique-values",
            "question": {
                "title": "pandas groupby expanding df based on unique values",
                "ques_desc": "I have below: I want to achieve the following. For each unique , the bottom row is (this is ). I want to count how many times each unique value of occurs where . This part would be achieved by something like: However, I also want to add, for each unique value of , a column indicating how many times that value occurred after the point where for the value indicated by the row. I understand this might sound confusing; here's how the output woud look like in this example: It is important that the solution is general enough to be applicable on a similar case with more unique values than just , and . UPDATE As a bonus, I am also interested in how, instead of the count, one can instead return the sum of some value column, under the same conditions, divided by the corresponding in the rows. Example: suppose we now depart from below instead: The output would need to sum for the cases indicated by the counts in the solution by @jezrael, and divide that number by . The output would instead look like: "
            },
            "io": [
                "df\n    V2  V1  A   B   C\n0   A   0   0   0   0\n1   B   1   0   0   0\n2   C   2   1   2   0\n",
                "df\n    V2  V1  A   B   C\n0   A   0   0   0   0\n1   B   1   0   0   0\n2   C   2   1   3.5 0\n"
            ],
            "answer": {
                "ans_desc": "First aggregate : Then grouping by and create heper column by last value by and , then remove last rows of by and use for counts, add all possible unique values of and last append to by : UPDATE The updated part of the question should be possible to achieve by changing the part with : ",
                "code": [
                    "df = df.pivot_table(\n    index='n',\n    columns='V2',\n    aggfunc=({\n        'V3': 'mean'\n    })\n).V3.reindex(columns=v, index=v, fill_value=0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1479,
            "user_id": 6560773,
            "user_type": "registered",
            "accept_rate": 79,
            "profile_image": "https://i.stack.imgur.com/rUJo3.png?s=128&g=1",
            "display_name": "MattSom",
            "link": "https://stackoverflow.com/users/6560773/mattsom"
        },
        "is_answered": true,
        "view_count": 612,
        "accepted_answer_id": 61808370,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1589494584,
        "creation_date": 1589467973,
        "last_edit_date": 1589494153,
        "question_id": 61800463,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61800463/read-large-json-file-with-index-format-into-pandas-dataframe",
        "title": "Read large .json file with index format into Pandas dataframe",
        "body": "<p>I was following <a href=\"https://datascience.stackexchange.com/a/60269/97099\">this</a> answer but after some discussion with it's writer, it seems it only gives a solution to <code>orient='records'</code> data format.</p>\n\n<p>This is the difference:</p>\n\n<pre><code># orient='records'\n[\n    {\"Product\":\"Desktop Computer\",\"Price\":700},\n    {\"Product\":\"Tablet\",\"Price\":250},\n    {\"Product\":\"iPhone\",\"Price\":800},\n    {\"Product\":\"Laptop\",\"Price\":1200}\n]\n\n# orient='index'\n{\n    \"0\":{\"Product\":\"Desktop Computer\",\"Price\":700},\n    \"1\":{\"Product\":\"Tablet\",\"Price\":250},\n    \"2\":{\"Product\":\"iPhone\",\"Price\":800},\n    \"3\":{\"Product\":\"Laptop\",\"Price\":1200}\n}\n</code></pre>\n\n<p>I have the index format because my data is from an SQL database read into a dataframe and the index field is needed to specify every records.</p>\n\n<p><strong>My json file is 2.5 GB</strong>, had been exported from the dataframe with <code>orient='index'</code> format.</p>\n\n<pre><code>df.to_json('test.json', orient='index')\n</code></pre>\n\n<p>This means that the whole file is actually one huge string and not a list like collection of records:</p>\n\n<pre><code>{\"0\":{\"Product\":\"Desktop Computer\",\"Price\":700},\"1\":{\"Product\":\"Tablet\",\"Price\":250},\"2\":{\"Product\":\"iPhone\",\"Price\":800},\"3\":{\"Product\":\"Laptop\",\"Price\":1200}}\n</code></pre>\n\n<p>This means I can't use any line or chunck based iterative solution like this:</p>\n\n<pre><code>df = pd.read_json('test.json', orient='index', lines=True, chunksize=5)\n</code></pre>\n\n<p>According to the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\" rel=\"nofollow noreferrer\">documentation</a>, <code>lines=True</code> can only be used if the records are in a list like format, this is why <code>pandas.DataFrame.to_json</code> does not even accept this argument unless the orient is not <code>orient='records'</code>. The restriction for <code>chunksize=</code> comes from this as well, it says:</p>\n\n<pre><code>\"This can only be passed if lines=True. If this is None, the file will be read into memory all at once.\"\n</code></pre>\n\n<p>And exactly this is the reason of the question, trying to read such a huge .json file gives back:</p>\n\n<pre><code>df = pd.read_json('test.json', orient='index')\n\nFile \"C:\\Users\\Username\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-\npackages\\pandas\\io\\json\\_json.py\", line 1100,\nin _parse_no_numpy                                                                                          \nloads(json, precise_float=self.precise_float),\nMemoryError \n</code></pre>\n\n<p>I was thinking about adding the index values as a first column as well, this case it wouldn't be lost with the records format; or maybe even store an index list separately. Only I fear it would decrease the search performance later on.</p>\n\n<p>Is there any solution to handle the situation strictly using the .json file and no other database or big-data based technology?</p>\n\n<p><strong>Update #1</strong></p>\n\n<p>For request here is the actual structure of my data. The SQL table:</p>\n\n<pre><code>          Serial           Date                   PatientID     Type Gender  YearWeek\n0         425571118001461E 2011-06-30 20:59:30    186092        3    1.0     2011-w26\n1         425571118001461E 2011-06-30 20:55:30    186092        3    1.0     2011-w26\n2         425571118001461E 2013-08-28 09:29:30    186092        3    1.0     2013-w35\n3         425571118001461E 2013-08-28 07:44:30    186092        3    1.0     2013-w35\n4         425571118001461E 2013-08-27 20:44:30    186092        3    1.0     2013-w35\n...                    ...                 ...       ...      ...    ...         ...\n32290281  4183116300254921 2020-04-09 08:07:50    217553        8    2.0     2020-w15\n32290282  4183116300254921 2020-04-08 10:29:50    217553        8    2.0     2020-w15\n32290283  4141119420031548 2020-04-20 10:18:02    217555       12    2.0     2020-w17\n32290284  4141119420043226 2020-04-20 12:33:11    217560       12    NaN     2020-w17\n32290285  4141119420000825 2020-04-20 17:31:44    217568       12    1.0     2020-w17\n</code></pre>\n\n<p>The pandas pivot table is almost the same as in the example, but with a 50,000 rows and 4,000 columns:</p>\n\n<pre><code>df = df.pivot_table(index='PatientID', values='Serial', columns='YearWeek', aggfunc=len, fill_value=0)\n\nYearWeek  1969-w01  1969-w02  1969-w03  1969-w04  1969-w05  ...  2138-w17  2138-w18  2138-w19  2138-w20  2138-w21\nPatientID\n0                0         0         0         0         0  ...         0         0         0         0         0\n455              1         0         3         0         0  ...         0         0         0         0         0\n40036            0         0         0         0         0  ...         0         0         0         0         0\n40070            0         0         0         0         0  ...         0         0         0         0         0\n40082            0         0         0         0         0  ...         0         0         0         0         0\n...            ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...\n217559           0         0         0         0         0  ...         0         0         0         0         0\n217560           0         0         0         0         0  ...         0         0         0         0         0\n217561           0         0         0         0         0  ...         0         0         0         0         0\n217563           0         0         0         0         0  ...         0         0         0         0         0\n217568           0         1         0         2         0  ...         0         0         0         0         0\n</code></pre>\n\n<p>And this is how it is saved with an index formatted json:</p>\n\n<pre><code>{\n    \"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\n    \"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\n    \"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\n    ...\n    \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}\n}\n</code></pre>\n\n<p>Only I could not give the <code>line=True</code> arg, so it is actually cramped into one huge string making it a one-liner json:</p>\n\n<pre><code>{\"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, ... \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}}\n</code></pre>\n",
        "answer_body": "<p>A few solutions, listed from easiest to more involved:</p>\n\n<h3>1. SQL</h3>\n\n<p>If you can perform the query on the DB, maybe the best solution would be to try writing the data in a nicer format? Alternatively, you could try reading directly from the database - Pandas can do that too  :)  Here is <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"nofollow noreferrer\"><strong>the documentation for pd.read_sql()</strong></a>.</p>\n\n<h3>2. Is <code>orient=...</code> necessary?</h3>\n\n<p>To read the JSON file as you gave the example, and create a DataFrame of the form comparable to your pivot-table example (JSON keys as dataframe index), you can try this simple approach:</p>\n\n<pre><code># read and transpose!\ndf = pd.read_json(\"test.json\").T\n</code></pre>\n\n<p>However, this probably doesn't solve the memory issue.</p>\n\n<h3>3. Splitting into several files</h3>\n\n<p>Perhaps the fastest way would be to simply cut the large file into smaller files,  which can each be read into a Pandas Dataframe (limiting the working memory required at any given time), then <code>pd.merge</code> or <code>pd.concat</code> the resulting dataframes. </p>\n\n<p>There is a nice tool in Linux called <a href=\"https://askubuntu.com/questions/28847/text-editor-to-edit-large-4-3-gb-plain-text-file\"><code>split</code>, which could do it</a>. I notice you are using windows (newer windows version offer a Linux terminal if you enable it!). Otherwise perhaps there is a similar tool, but I don't know one I'm afraid.</p>\n\n<p>If you only need to do this once then get on with your life, you might be able to open your file with some text editor like Emacs or VS Code, then copy-paste portions into new files... lame, but might work   \u00af\\_(\u30c4)_/\u00af</p>\n\n<h3>4. Streaming readers</h3>\n\n<p>One package called <a href=\"https://github.com/ICRAR/ijson\" rel=\"nofollow noreferrer\"><strong><code>ijson</code></strong></a> will iteratively load a JSON file, which allows you to define breaks or do processing to each nested division - you could then for example create the <code>records</code> format for Pandas <em>on-the-fly</em>. This solution also promised low memory consumption, being an iterator (a.k.a generator) - you will need to learn how it works though. Have a look <a href=\"https://stackoverflow.com/a/17326199/3126298\">here for a nice explanation</a>.</p>\n\n<p>Another package called <a href=\"https://github.com/kashifrazzaqui/json-streamer\" rel=\"nofollow noreferrer\">json-streamer</a> can also read partial JSON contents, although it is perhaps going a bit far, given you have a static file.</p>\n",
        "question_body": "<p>I was following <a href=\"https://datascience.stackexchange.com/a/60269/97099\">this</a> answer but after some discussion with it's writer, it seems it only gives a solution to <code>orient='records'</code> data format.</p>\n\n<p>This is the difference:</p>\n\n<pre><code># orient='records'\n[\n    {\"Product\":\"Desktop Computer\",\"Price\":700},\n    {\"Product\":\"Tablet\",\"Price\":250},\n    {\"Product\":\"iPhone\",\"Price\":800},\n    {\"Product\":\"Laptop\",\"Price\":1200}\n]\n\n# orient='index'\n{\n    \"0\":{\"Product\":\"Desktop Computer\",\"Price\":700},\n    \"1\":{\"Product\":\"Tablet\",\"Price\":250},\n    \"2\":{\"Product\":\"iPhone\",\"Price\":800},\n    \"3\":{\"Product\":\"Laptop\",\"Price\":1200}\n}\n</code></pre>\n\n<p>I have the index format because my data is from an SQL database read into a dataframe and the index field is needed to specify every records.</p>\n\n<p><strong>My json file is 2.5 GB</strong>, had been exported from the dataframe with <code>orient='index'</code> format.</p>\n\n<pre><code>df.to_json('test.json', orient='index')\n</code></pre>\n\n<p>This means that the whole file is actually one huge string and not a list like collection of records:</p>\n\n<pre><code>{\"0\":{\"Product\":\"Desktop Computer\",\"Price\":700},\"1\":{\"Product\":\"Tablet\",\"Price\":250},\"2\":{\"Product\":\"iPhone\",\"Price\":800},\"3\":{\"Product\":\"Laptop\",\"Price\":1200}}\n</code></pre>\n\n<p>This means I can't use any line or chunck based iterative solution like this:</p>\n\n<pre><code>df = pd.read_json('test.json', orient='index', lines=True, chunksize=5)\n</code></pre>\n\n<p>According to the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\" rel=\"nofollow noreferrer\">documentation</a>, <code>lines=True</code> can only be used if the records are in a list like format, this is why <code>pandas.DataFrame.to_json</code> does not even accept this argument unless the orient is not <code>orient='records'</code>. The restriction for <code>chunksize=</code> comes from this as well, it says:</p>\n\n<pre><code>\"This can only be passed if lines=True. If this is None, the file will be read into memory all at once.\"\n</code></pre>\n\n<p>And exactly this is the reason of the question, trying to read such a huge .json file gives back:</p>\n\n<pre><code>df = pd.read_json('test.json', orient='index')\n\nFile \"C:\\Users\\Username\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-\npackages\\pandas\\io\\json\\_json.py\", line 1100,\nin _parse_no_numpy                                                                                          \nloads(json, precise_float=self.precise_float),\nMemoryError \n</code></pre>\n\n<p>I was thinking about adding the index values as a first column as well, this case it wouldn't be lost with the records format; or maybe even store an index list separately. Only I fear it would decrease the search performance later on.</p>\n\n<p>Is there any solution to handle the situation strictly using the .json file and no other database or big-data based technology?</p>\n\n<p><strong>Update #1</strong></p>\n\n<p>For request here is the actual structure of my data. The SQL table:</p>\n\n<pre><code>          Serial           Date                   PatientID     Type Gender  YearWeek\n0         425571118001461E 2011-06-30 20:59:30    186092        3    1.0     2011-w26\n1         425571118001461E 2011-06-30 20:55:30    186092        3    1.0     2011-w26\n2         425571118001461E 2013-08-28 09:29:30    186092        3    1.0     2013-w35\n3         425571118001461E 2013-08-28 07:44:30    186092        3    1.0     2013-w35\n4         425571118001461E 2013-08-27 20:44:30    186092        3    1.0     2013-w35\n...                    ...                 ...       ...      ...    ...         ...\n32290281  4183116300254921 2020-04-09 08:07:50    217553        8    2.0     2020-w15\n32290282  4183116300254921 2020-04-08 10:29:50    217553        8    2.0     2020-w15\n32290283  4141119420031548 2020-04-20 10:18:02    217555       12    2.0     2020-w17\n32290284  4141119420043226 2020-04-20 12:33:11    217560       12    NaN     2020-w17\n32290285  4141119420000825 2020-04-20 17:31:44    217568       12    1.0     2020-w17\n</code></pre>\n\n<p>The pandas pivot table is almost the same as in the example, but with a 50,000 rows and 4,000 columns:</p>\n\n<pre><code>df = df.pivot_table(index='PatientID', values='Serial', columns='YearWeek', aggfunc=len, fill_value=0)\n\nYearWeek  1969-w01  1969-w02  1969-w03  1969-w04  1969-w05  ...  2138-w17  2138-w18  2138-w19  2138-w20  2138-w21\nPatientID\n0                0         0         0         0         0  ...         0         0         0         0         0\n455              1         0         3         0         0  ...         0         0         0         0         0\n40036            0         0         0         0         0  ...         0         0         0         0         0\n40070            0         0         0         0         0  ...         0         0         0         0         0\n40082            0         0         0         0         0  ...         0         0         0         0         0\n...            ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...\n217559           0         0         0         0         0  ...         0         0         0         0         0\n217560           0         0         0         0         0  ...         0         0         0         0         0\n217561           0         0         0         0         0  ...         0         0         0         0         0\n217563           0         0         0         0         0  ...         0         0         0         0         0\n217568           0         1         0         2         0  ...         0         0         0         0         0\n</code></pre>\n\n<p>And this is how it is saved with an index formatted json:</p>\n\n<pre><code>{\n    \"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\n    \"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\n    \"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\n    ...\n    \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}\n}\n</code></pre>\n\n<p>Only I could not give the <code>line=True</code> arg, so it is actually cramped into one huge string making it a one-liner json:</p>\n\n<pre><code>{\"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, ... \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}}\n</code></pre>\n",
        "formatted_input": {
            "qid": 61800463,
            "link": "https://stackoverflow.com/questions/61800463/read-large-json-file-with-index-format-into-pandas-dataframe",
            "question": {
                "title": "Read large .json file with index format into Pandas dataframe",
                "ques_desc": "I was following this answer but after some discussion with it's writer, it seems it only gives a solution to data format. This is the difference: I have the index format because my data is from an SQL database read into a dataframe and the index field is needed to specify every records. My json file is 2.5 GB, had been exported from the dataframe with format. This means that the whole file is actually one huge string and not a list like collection of records: This means I can't use any line or chunck based iterative solution like this: According to the documentation, can only be used if the records are in a list like format, this is why does not even accept this argument unless the orient is not . The restriction for comes from this as well, it says: And exactly this is the reason of the question, trying to read such a huge .json file gives back: I was thinking about adding the index values as a first column as well, this case it wouldn't be lost with the records format; or maybe even store an index list separately. Only I fear it would decrease the search performance later on. Is there any solution to handle the situation strictly using the .json file and no other database or big-data based technology? Update #1 For request here is the actual structure of my data. The SQL table: The pandas pivot table is almost the same as in the example, but with a 50,000 rows and 4,000 columns: And this is how it is saved with an index formatted json: Only I could not give the arg, so it is actually cramped into one huge string making it a one-liner json: "
            },
            "io": [
                "{\n    \"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\n    \"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\n    \"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\n    ...\n    \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}\n}\n",
                "{\"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, ... \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}}\n"
            ],
            "answer": {
                "ans_desc": "A few solutions, listed from easiest to more involved: 1. SQL If you can perform the query on the DB, maybe the best solution would be to try writing the data in a nicer format? Alternatively, you could try reading directly from the database - Pandas can do that too :) Here is the documentation for pd.read_sql(). 2. Is necessary? To read the JSON file as you gave the example, and create a DataFrame of the form comparable to your pivot-table example (JSON keys as dataframe index), you can try this simple approach: However, this probably doesn't solve the memory issue. 3. Splitting into several files Perhaps the fastest way would be to simply cut the large file into smaller files, which can each be read into a Pandas Dataframe (limiting the working memory required at any given time), then or the resulting dataframes. There is a nice tool in Linux called , which could do it. I notice you are using windows (newer windows version offer a Linux terminal if you enable it!). Otherwise perhaps there is a similar tool, but I don't know one I'm afraid. If you only need to do this once then get on with your life, you might be able to open your file with some text editor like Emacs or VS Code, then copy-paste portions into new files... lame, but might work \u00af\\_(\u30c4)_/\u00af 4. Streaming readers One package called will iteratively load a JSON file, which allows you to define breaks or do processing to each nested division - you could then for example create the format for Pandas on-the-fly. This solution also promised low memory consumption, being an iterator (a.k.a generator) - you will need to learn how it works though. Have a look here for a nice explanation. Another package called json-streamer can also read partial JSON contents, although it is perhaps going a bit far, given you have a static file. ",
                "code": [
                    "# read and transpose!\ndf = pd.read_json(\"test.json\").T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "series"
        ],
        "owner": {
            "reputation": 87,
            "user_id": 12827931,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0ace77a05c36239fdef3b3b164145a15?s=128&d=identicon&r=PG&f=1",
            "display_name": "thesecond",
            "link": "https://stackoverflow.com/users/12827931/thesecond"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 61679744,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1588967447,
        "creation_date": 1588939929,
        "last_edit_date": 1588940001,
        "question_id": 61678886,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61678886/filter-series-dataframe-by-another-dataframe",
        "title": "Filter Series/DataFrame by another DataFrame",
        "body": "<p>Let's suppose I have a Series (or DataFrame) <code>s1</code>, for example list of all Universities and Colleges in the USA:</p>\n\n<pre><code>                     University\n0            Searcy Harding University\n1          Angwin Pacific Union College\n2    Fairbanks University of Alaska Fairbanks\n3        Ann Arbor University of Michigan\n</code></pre>\n\n<p>And another Series (od DataFrame) <code>s2</code>, for example list of all cities in the USA:</p>\n\n<pre><code>      City\n0    Searcy\n1    Angwin \n2   New York \n3   Ann Arbor \n</code></pre>\n\n<p>And my desired output (bascially an intersection of <code>s1</code> and <code>s2</code>):</p>\n\n<pre><code>     Uni City\n0     Searcy\n1     Angwin \n2    Fairbanks \n3    Ann Arbor \n</code></pre>\n\n<p>The thing is: I'd like to create a Series that consists of <strong>cities</strong> but only these, that have a university/college. My very first thought was to remove \"University\" or \"College\" parts from the <code>s1</code>, but it turns out that it is not enough, as in case of <code>Angwin Pacific Union College</code>. Then I thought of leaving only the first word, but that excludes <code>Ann Arbor</code>. \nFinally, I got a series of all the cities <code>s2</code> and now I'm trying to use it as a filter (something similiar to <code>.contains()</code> or <code>.isin()</code>), so if a string <code>s1</code> (Uni name) contains <strong>any</strong> of the elements of <code>s2</code> (city name), then return only the city name. </p>\n\n<p>My question is: how to do it in a neat way?</p>\n",
        "answer_body": "<p>I would try to build a list comprehension of cities that are contained in at least one university name:</p>\n\n<pre><code>pd.Series([i for i in s2 if s1.str.contains(i).any()], name='Uni City')\n</code></pre>\n\n<p>With your example data it gives:</p>\n\n<pre><code>0       Searcy\n1       Angwin\n2    Ann Arbor\nName: Uni City, dtype: object\n</code></pre>\n",
        "question_body": "<p>Let's suppose I have a Series (or DataFrame) <code>s1</code>, for example list of all Universities and Colleges in the USA:</p>\n\n<pre><code>                     University\n0            Searcy Harding University\n1          Angwin Pacific Union College\n2    Fairbanks University of Alaska Fairbanks\n3        Ann Arbor University of Michigan\n</code></pre>\n\n<p>And another Series (od DataFrame) <code>s2</code>, for example list of all cities in the USA:</p>\n\n<pre><code>      City\n0    Searcy\n1    Angwin \n2   New York \n3   Ann Arbor \n</code></pre>\n\n<p>And my desired output (bascially an intersection of <code>s1</code> and <code>s2</code>):</p>\n\n<pre><code>     Uni City\n0     Searcy\n1     Angwin \n2    Fairbanks \n3    Ann Arbor \n</code></pre>\n\n<p>The thing is: I'd like to create a Series that consists of <strong>cities</strong> but only these, that have a university/college. My very first thought was to remove \"University\" or \"College\" parts from the <code>s1</code>, but it turns out that it is not enough, as in case of <code>Angwin Pacific Union College</code>. Then I thought of leaving only the first word, but that excludes <code>Ann Arbor</code>. \nFinally, I got a series of all the cities <code>s2</code> and now I'm trying to use it as a filter (something similiar to <code>.contains()</code> or <code>.isin()</code>), so if a string <code>s1</code> (Uni name) contains <strong>any</strong> of the elements of <code>s2</code> (city name), then return only the city name. </p>\n\n<p>My question is: how to do it in a neat way?</p>\n",
        "formatted_input": {
            "qid": 61678886,
            "link": "https://stackoverflow.com/questions/61678886/filter-series-dataframe-by-another-dataframe",
            "question": {
                "title": "Filter Series/DataFrame by another DataFrame",
                "ques_desc": "Let's suppose I have a Series (or DataFrame) , for example list of all Universities and Colleges in the USA: And another Series (od DataFrame) , for example list of all cities in the USA: And my desired output (bascially an intersection of and ): The thing is: I'd like to create a Series that consists of cities but only these, that have a university/college. My very first thought was to remove \"University\" or \"College\" parts from the , but it turns out that it is not enough, as in case of . Then I thought of leaving only the first word, but that excludes . Finally, I got a series of all the cities and now I'm trying to use it as a filter (something similiar to or ), so if a string (Uni name) contains any of the elements of (city name), then return only the city name. My question is: how to do it in a neat way? "
            },
            "io": [
                "      City\n0    Searcy\n1    Angwin \n2   New York \n3   Ann Arbor \n",
                "     Uni City\n0     Searcy\n1     Angwin \n2    Fairbanks \n3    Ann Arbor \n"
            ],
            "answer": {
                "ans_desc": "I would try to build a list comprehension of cities that are contained in at least one university name: With your example data it gives: ",
                "code": [
                    "pd.Series([i for i in s2 if s1.str.contains(i).any()], name='Uni City')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 13073029,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-RGObWuv58wI/AAAAAAAAAAI/AAAAAAAAAAA/AKF05nAg_gfhVDEAlcFufD-DfSNwruPNFg/photo.jpg?sz=128",
            "display_name": "CH_0711",
            "link": "https://stackoverflow.com/users/13073029/ch-0711"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 60720729,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1588756388,
        "creation_date": 1584441241,
        "question_id": 60720542,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60720542/get-column-index-nr-from-value-in-other-column",
        "title": "Get column index nr from value in other column",
        "body": "<p>I'm relativly new to python and pandas, so I might not have the full understanding of all possibilities and would appreciate a hint how to solve the following problem:</p>\n\n<p>I have a <code>df</code> like this one:</p>\n\n<pre><code>    Jan  Feb  Mar  Apr  i    j\na   100  200  250  100  1  0.3\nb   120  130   90  100  3  0.7\nc    10   30   10   20  2 0.25\n</code></pre>\n\n<p>I want to construct a column which takes the column with the index according to <code>df['i']</code> and then multiplies the value in the selected column with the value in  <code>df['j']</code>. \nI want to bulid a table like this (<code>df['k']</code> beeing the column constructed):</p>\n\n<pre><code>    Jan  Feb  Mar  Apr  i    j    k\na   100  200  250  100  1  0.3   60\nb   120  130   90  100  3  0.7   70\nc    10   30   10   20  2 0.25  2.5\n</code></pre>\n\n<p>(row <code>a</code> <code>df['k']=200*0.3</code> (<code>df['Feb']*df['j']</code>), in row <code>b</code> <code>df['k']=100*0.7</code> (<code>df['Apr']*df['j']</code>) and in row <code>c</code> <code>df['k']=10*0.25</code>(<code>df['Mar']*df['j']</code>))</p>\n\n<p>The value in <code>df['i']</code> will always be an integer value, so i would love to use the position of a column according to the value in <code>df['i']</code>.</p>\n",
        "answer_body": "<p>IIUC, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\" rel=\"nofollow noreferrer\"><code>DataFrame.rename</code></a> and then we can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lookup.html\" rel=\"nofollow noreferrer\"><code>DataFrame.lookup</code></a> to <code>map</code>. Finally we use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mul.html\" rel=\"nofollow noreferrer\"><code>Series.mul</code></a> </p>\n\n<pre><code>df['k'] = df['j'].mul(df.rename(columns = dict(zip(df.columns,\n                                                   range(len(df.columns)))))\n                        .lookup(df.index, df['i']))\nprint(df)\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>   Jan  Feb  Mar  Apr  i     j     k\na  100  200  250  100  1  0.30  60.0\nb  120  130   90  100  3  0.70  70.0\nc   10   30   10   20  2  0.25   2.5\n</code></pre>\n\n<p><strong>Alternative:</strong></p>\n\n<pre><code>df['j'].mul(df.iloc[:,df['i']].lookup(df.index, \n                                      df['i'].map(dict(zip(range(len(df.columns)),\n                                                           df.columns)))))\n</code></pre>\n",
        "question_body": "<p>I'm relativly new to python and pandas, so I might not have the full understanding of all possibilities and would appreciate a hint how to solve the following problem:</p>\n\n<p>I have a <code>df</code> like this one:</p>\n\n<pre><code>    Jan  Feb  Mar  Apr  i    j\na   100  200  250  100  1  0.3\nb   120  130   90  100  3  0.7\nc    10   30   10   20  2 0.25\n</code></pre>\n\n<p>I want to construct a column which takes the column with the index according to <code>df['i']</code> and then multiplies the value in the selected column with the value in  <code>df['j']</code>. \nI want to bulid a table like this (<code>df['k']</code> beeing the column constructed):</p>\n\n<pre><code>    Jan  Feb  Mar  Apr  i    j    k\na   100  200  250  100  1  0.3   60\nb   120  130   90  100  3  0.7   70\nc    10   30   10   20  2 0.25  2.5\n</code></pre>\n\n<p>(row <code>a</code> <code>df['k']=200*0.3</code> (<code>df['Feb']*df['j']</code>), in row <code>b</code> <code>df['k']=100*0.7</code> (<code>df['Apr']*df['j']</code>) and in row <code>c</code> <code>df['k']=10*0.25</code>(<code>df['Mar']*df['j']</code>))</p>\n\n<p>The value in <code>df['i']</code> will always be an integer value, so i would love to use the position of a column according to the value in <code>df['i']</code>.</p>\n",
        "formatted_input": {
            "qid": 60720542,
            "link": "https://stackoverflow.com/questions/60720542/get-column-index-nr-from-value-in-other-column",
            "question": {
                "title": "Get column index nr from value in other column",
                "ques_desc": "I'm relativly new to python and pandas, so I might not have the full understanding of all possibilities and would appreciate a hint how to solve the following problem: I have a like this one: I want to construct a column which takes the column with the index according to and then multiplies the value in the selected column with the value in . I want to bulid a table like this ( beeing the column constructed): (row (), in row () and in row ()) The value in will always be an integer value, so i would love to use the position of a column according to the value in . "
            },
            "io": [
                "    Jan  Feb  Mar  Apr  i    j\na   100  200  250  100  1  0.3\nb   120  130   90  100  3  0.7\nc    10   30   10   20  2 0.25\n",
                "    Jan  Feb  Mar  Apr  i    j    k\na   100  200  250  100  1  0.3   60\nb   120  130   90  100  3  0.7   70\nc    10   30   10   20  2 0.25  2.5\n"
            ],
            "answer": {
                "ans_desc": "IIUC, and then we can use to . Finally we use Output Alternative: ",
                "code": [
                    "df['k'] = df['j'].mul(df.rename(columns = dict(zip(df.columns,\n                                                   range(len(df.columns)))))\n                        .lookup(df.index, df['i']))\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "arrays",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 663,
            "user_id": 1912104,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/342abcc6cd5ab765d7bc040fb72c0cd5?s=128&d=identicon&r=PG",
            "display_name": "NonSleeper",
            "link": "https://stackoverflow.com/users/1912104/nonsleeper"
        },
        "is_answered": true,
        "view_count": 88,
        "accepted_answer_id": 61615891,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1588691875,
        "creation_date": 1588689691,
        "last_edit_date": 1588690736,
        "question_id": 61615757,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61615757/how-to-find-and-replace-values-of-even-positioned-elements-in-sequence",
        "title": "How to find and replace values of even-positioned elements in sequence",
        "body": "<p>I have a list as follows:</p>\n\n<pre><code>list_1 = [12, 15, 18, 21, 6, 9, 7, 21, 38, 62, 65, 68, 81, 21, 25, 96, 101, 8, 11]\n</code></pre>\n\n<p>There are sequences of elements whose value's distances are equal. In this list, that distance is <code>3</code>, for example, <code>12, 15, 18, 21</code>.</p>\n\n<p>How can I replace the value of the even-positioned elements for each of these sequences / sublists with a new value, say -1. The output will be:</p>\n\n<pre><code>list_2 = [12, -1, 18, -1, 6, -1, 7, 21, 38, 62, -1, 68, 81, 21, 25, 96, 101, 8, -1]\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.diff.html\" rel=\"nofollow noreferrer\"><code>diff()</code></a> </p>\n\n<pre><code>s = pd.Series(list_1)\ns.loc[(s.diff() == 3) &amp; (s.index % 2 == 1)] = -1\n</code></pre>\n\n<p>which yields</p>\n\n<pre><code>[12, -1, 18, -1, 6, -1, 7, 21, 38, 62, 65, -1, 81, 21, 25, 96, 101, 8, 11]\n</code></pre>\n\n<hr>\n\n<p>Now, if you want the even positions related to <em>each</em> of those sequences independently, use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html\" rel=\"nofollow noreferrer\"><code>cumcount</code></a></p>\n\n<pre><code>s.loc[s.groupby(s.diff().ne(3).cumsum()).cumcount() % 2 == 1] = -1\n</code></pre>\n\n<p>which yields</p>\n\n<pre><code>[12, -1, 18, -1, 6, -1, 7, 21, 38, 62, -1, 68, 81, 21, 25, 96, 101, 8, -1]\n</code></pre>\n\n<hr>\n\n<p>The difference will be the last <code>-1</code> in your example. The first solution assumes the <code>11</code> shouldn't be replaced because it's not in an even position in your <code>list_1</code>. The second solutions assumes it should be replace, because it is in an even position in the <em>sublist</em> <code>[8, 11]</code>.</p>\n",
        "question_body": "<p>I have a list as follows:</p>\n\n<pre><code>list_1 = [12, 15, 18, 21, 6, 9, 7, 21, 38, 62, 65, 68, 81, 21, 25, 96, 101, 8, 11]\n</code></pre>\n\n<p>There are sequences of elements whose value's distances are equal. In this list, that distance is <code>3</code>, for example, <code>12, 15, 18, 21</code>.</p>\n\n<p>How can I replace the value of the even-positioned elements for each of these sequences / sublists with a new value, say -1. The output will be:</p>\n\n<pre><code>list_2 = [12, -1, 18, -1, 6, -1, 7, 21, 38, 62, -1, 68, 81, 21, 25, 96, 101, 8, -1]\n</code></pre>\n",
        "formatted_input": {
            "qid": 61615757,
            "link": "https://stackoverflow.com/questions/61615757/how-to-find-and-replace-values-of-even-positioned-elements-in-sequence",
            "question": {
                "title": "How to find and replace values of even-positioned elements in sequence",
                "ques_desc": "I have a list as follows: There are sequences of elements whose value's distances are equal. In this list, that distance is , for example, . How can I replace the value of the even-positioned elements for each of these sequences / sublists with a new value, say -1. The output will be: "
            },
            "io": [
                "list_1 = [12, 15, 18, 21, 6, 9, 7, 21, 38, 62, 65, 68, 81, 21, 25, 96, 101, 8, 11]\n",
                "list_2 = [12, -1, 18, -1, 6, -1, 7, 21, 38, 62, -1, 68, 81, 21, 25, 96, 101, 8, -1]\n"
            ],
            "answer": {
                "ans_desc": "Use which yields Now, if you want the even positions related to each of those sequences independently, use and which yields The difference will be the last in your example. The first solution assumes the shouldn't be replaced because it's not in an even position in your . The second solutions assumes it should be replace, because it is in an even position in the sublist . ",
                "code": [
                    "s = pd.Series(list_1)\ns.loc[(s.diff() == 3) & (s.index % 2 == 1)] = -1\n",
                    "s.loc[s.groupby(s.diff().ne(3).cumsum()).cumcount() % 2 == 1] = -1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 13435770,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/38000b28e51229c626eb39d2f240e63f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Rose16",
            "link": "https://stackoverflow.com/users/13435770/rose16"
        },
        "is_answered": true,
        "view_count": 86,
        "accepted_answer_id": 61587919,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1588581605,
        "creation_date": 1588533454,
        "last_edit_date": 1588557738,
        "question_id": 61580297,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61580297/python-dataframe-data-analysis-of-large-amount-of-data-from-a-text-file",
        "title": "Python DataFrame Data Analysis of Large Amount of Data from a Text File",
        "body": "<p>I have the following code:</p>\n\n<pre><code>datadicts = [ ]\nwith open(\"input.txt\") as f:\n    for line in f:\n        datadicts.append({'col1': line[':'], 'col2': line[':'], 'col3': line[':'], 'col4': line[':']})\n\ndf = pd.DataFrame(datadicts)\ndf = df.drop([0])\nprint(df)\n</code></pre>\n\n<p>I am using a text file (that is not formatted) to pull chunks of data from. When the text file is opened, it looks something like this, except on a way bigger scale: </p>\n\n<pre><code>00 2381    1.3 3.4 1.8 265879 Name \n34 7879    7.6 4.2 2.1 254789 Name \n45 65824   2.3 3.4 1.8 265879 Name \n58 3450    1.3 3.4 1.8 183713 Name \n69 37495   1.3 3.4 1.8 137632 Name \n73 458913  1.3 3.4 1.8 138024 Name \n</code></pre>\n\n<p>Here are the things I'm having trouble doing with this data:</p>\n\n<ol>\n<li>I only need the second, third, sixth, and seventh columns of data. The issue with this one, I believe I've solved with my code above by reading the individual lines and creating a dataframe with the columns necessary. I am open to suggestions if anyone has a better way of doing this. </li>\n<li>I need to skip the first row of data. This one, the open feature doesn't have a skiprows attribute, so when I drop the first row, I also lose my index starting at 0. Is there any way around this? </li>\n<li>I need the resulting dataframe to look like a nice clean dataframe. As of right now, it looks something like this: </li>\n</ol>\n\n<pre><code>Col1   Col2   Col3 Col4\n2381    3.4 265879 Name \n7879    4.2 254789 Name \n65824   3.4 265879 Name \n3450    3.4 183713 Name \n37495   3.4 137632 Name \n458913  3.4 138024 Name \n</code></pre>\n\n<p>Everything is right-aligned under the column and it looks strange. Any ideas how to solve this? </p>\n\n<ol start=\"4\">\n<li>I also need to be able to perform Statistic Analysis on the columns of data, and to be able to find the Name with the highest data and the lowest data, but for some reason, I always get errors because I think that, even though I've got all the data set up as a dataframe, the values inside the dataframe are reading as objects instead of integers, strings, floats, etc.</li>\n</ol>\n\n<p>So, if my data is not analyzable using Python functions, does anyone know how I can fix this to make the data be able to run correctly? </p>\n\n<p>Any help would be greatly appreciated. I hope I've laid out all of my needs clearly. I am new to Python, and I'm not sure if I'm using all the proper terminology. </p>\n",
        "answer_body": "<p>You can use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\" rel=\"nofollow noreferrer\"><code>pandas.read_csv()</code></a> function to accomplish this <strong>very easily</strong>.</p>\n\n<ul>\n<li><code>txt2pd.txt</code> is a text file containing a copy/paste from your source above</li>\n<li><code>sep</code> is using a regex pattern to delimit by one or more consecutive spaces</li>\n<li><code>names</code> uses a <code>list</code> to create your column names</li>\n<li><code>skiprows</code> skips the first row, per your requirements</li>\n</ul>\n\n<h3>Example:</h3>\n\n<pre><code>keep = ['col1', 'col3', 'col5', 'col6']\ndf = pd.read_csv('txt2pd.txt', \n                 sep='\\s+', \n                 names=['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6'], \n                 skiprows=1)\ndf = df[keep]\n</code></pre>\n\n<h3>Output:</h3>\n\n<pre><code>     col1  col3    col5  col6\n0    7879   4.2  254789  Name\n1   65824   3.4  265879  Name\n2    3450   3.4  183713  Name\n3   37495   3.4  137632  Name\n4  458913   3.4  138024  Name\n</code></pre>\n\n<h3>Sample Analysis:</h3>\n\n<p>Using <code>df.describe()</code> you can output a simple, high-level analysis.  (Anything further should be the subject of a new question.)</p>\n\n<pre><code>                col1      col3           col5\ncount       5.000000  5.000000       5.000000\nmean   114712.200000  3.560000  196007.400000\nstd    194048.545838  0.357771   61762.106621\nmin      3450.000000  3.400000  137632.000000\n25%      7879.000000  3.400000  138024.000000\n50%     37495.000000  3.400000  183713.000000\n75%     65824.000000  3.400000  254789.000000\nmax    458913.000000  4.200000  265879.000000\n</code></pre>\n",
        "question_body": "<p>I have the following code:</p>\n\n<pre><code>datadicts = [ ]\nwith open(\"input.txt\") as f:\n    for line in f:\n        datadicts.append({'col1': line[':'], 'col2': line[':'], 'col3': line[':'], 'col4': line[':']})\n\ndf = pd.DataFrame(datadicts)\ndf = df.drop([0])\nprint(df)\n</code></pre>\n\n<p>I am using a text file (that is not formatted) to pull chunks of data from. When the text file is opened, it looks something like this, except on a way bigger scale: </p>\n\n<pre><code>00 2381    1.3 3.4 1.8 265879 Name \n34 7879    7.6 4.2 2.1 254789 Name \n45 65824   2.3 3.4 1.8 265879 Name \n58 3450    1.3 3.4 1.8 183713 Name \n69 37495   1.3 3.4 1.8 137632 Name \n73 458913  1.3 3.4 1.8 138024 Name \n</code></pre>\n\n<p>Here are the things I'm having trouble doing with this data:</p>\n\n<ol>\n<li>I only need the second, third, sixth, and seventh columns of data. The issue with this one, I believe I've solved with my code above by reading the individual lines and creating a dataframe with the columns necessary. I am open to suggestions if anyone has a better way of doing this. </li>\n<li>I need to skip the first row of data. This one, the open feature doesn't have a skiprows attribute, so when I drop the first row, I also lose my index starting at 0. Is there any way around this? </li>\n<li>I need the resulting dataframe to look like a nice clean dataframe. As of right now, it looks something like this: </li>\n</ol>\n\n<pre><code>Col1   Col2   Col3 Col4\n2381    3.4 265879 Name \n7879    4.2 254789 Name \n65824   3.4 265879 Name \n3450    3.4 183713 Name \n37495   3.4 137632 Name \n458913  3.4 138024 Name \n</code></pre>\n\n<p>Everything is right-aligned under the column and it looks strange. Any ideas how to solve this? </p>\n\n<ol start=\"4\">\n<li>I also need to be able to perform Statistic Analysis on the columns of data, and to be able to find the Name with the highest data and the lowest data, but for some reason, I always get errors because I think that, even though I've got all the data set up as a dataframe, the values inside the dataframe are reading as objects instead of integers, strings, floats, etc.</li>\n</ol>\n\n<p>So, if my data is not analyzable using Python functions, does anyone know how I can fix this to make the data be able to run correctly? </p>\n\n<p>Any help would be greatly appreciated. I hope I've laid out all of my needs clearly. I am new to Python, and I'm not sure if I'm using all the proper terminology. </p>\n",
        "formatted_input": {
            "qid": 61580297,
            "link": "https://stackoverflow.com/questions/61580297/python-dataframe-data-analysis-of-large-amount-of-data-from-a-text-file",
            "question": {
                "title": "Python DataFrame Data Analysis of Large Amount of Data from a Text File",
                "ques_desc": "I have the following code: I am using a text file (that is not formatted) to pull chunks of data from. When the text file is opened, it looks something like this, except on a way bigger scale: Here are the things I'm having trouble doing with this data: I only need the second, third, sixth, and seventh columns of data. The issue with this one, I believe I've solved with my code above by reading the individual lines and creating a dataframe with the columns necessary. I am open to suggestions if anyone has a better way of doing this. I need to skip the first row of data. This one, the open feature doesn't have a skiprows attribute, so when I drop the first row, I also lose my index starting at 0. Is there any way around this? I need the resulting dataframe to look like a nice clean dataframe. As of right now, it looks something like this: Everything is right-aligned under the column and it looks strange. Any ideas how to solve this? I also need to be able to perform Statistic Analysis on the columns of data, and to be able to find the Name with the highest data and the lowest data, but for some reason, I always get errors because I think that, even though I've got all the data set up as a dataframe, the values inside the dataframe are reading as objects instead of integers, strings, floats, etc. So, if my data is not analyzable using Python functions, does anyone know how I can fix this to make the data be able to run correctly? Any help would be greatly appreciated. I hope I've laid out all of my needs clearly. I am new to Python, and I'm not sure if I'm using all the proper terminology. "
            },
            "io": [
                "00 2381    1.3 3.4 1.8 265879 Name \n34 7879    7.6 4.2 2.1 254789 Name \n45 65824   2.3 3.4 1.8 265879 Name \n58 3450    1.3 3.4 1.8 183713 Name \n69 37495   1.3 3.4 1.8 137632 Name \n73 458913  1.3 3.4 1.8 138024 Name \n",
                "Col1   Col2   Col3 Col4\n2381    3.4 265879 Name \n7879    4.2 254789 Name \n65824   3.4 265879 Name \n3450    3.4 183713 Name \n37495   3.4 137632 Name \n458913  3.4 138024 Name \n"
            ],
            "answer": {
                "ans_desc": "You can use the function to accomplish this very easily. is a text file containing a copy/paste from your source above is using a regex pattern to delimit by one or more consecutive spaces uses a to create your column names skips the first row, per your requirements Example: Output: Sample Analysis: Using you can output a simple, high-level analysis. (Anything further should be the subject of a new question.) ",
                "code": [
                    "keep = ['col1', 'col3', 'col5', 'col6']\ndf = pd.read_csv('txt2pd.txt', \n                 sep='\\s+', \n                 names=['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6'], \n                 skiprows=1)\ndf = df[keep]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 29,
            "user_id": 12365593,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/96693d1a7489ff7a99718ea092dc80ad?s=128&d=identicon&r=PG&f=1",
            "display_name": "Legolas",
            "link": "https://stackoverflow.com/users/12365593/legolas"
        },
        "is_answered": true,
        "view_count": 95,
        "accepted_answer_id": 61556864,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1588411152,
        "creation_date": 1588409992,
        "question_id": 61556827,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61556827/parsing-a-list-of-lists-to-a-data-frame-in-pandas",
        "title": "Parsing a list of lists to a data frame in pandas",
        "body": "<p>I have list of lists. Below is how my list looks like, I want to parse it into a data frame with continuation of values with columns = A,B,C</p>\n\n<pre><code>[     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n\n      A  B  C\n  0   4  5  6\n  1   4  5  6\n  2   4  5  6\n  3   4  5  6\n]\n\n</code></pre>\n\n<p>The expected data frame is as below</p>\n\n<pre><code>     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n  4   4  5  6\n  5   4  5  6\n  6   4  5  6\n  7   4  5  6\n\n</code></pre>\n\n<p>Really appreciate the help.</p>\n",
        "answer_body": "<p>Try this:</p>\n\n<pre><code>import pandas as pd\nlist1=[]\ncount=0\nwhile count&lt;len(myList):\n    list2= myList[count]\n    list1.append(list2)\n    #print(list2)\n    count+=1\ndf = pd.concat(list1)\nprint(df)\n</code></pre>\n\n<p><code>myList</code> : List which contains sub-lists</p>\n\n<p><code>list2</code> : First it takes sub-lists<code>myList[count]</code> then append it to <code>list1</code></p>\n\n<p><code>list1</code> : Final list which contains all sub-lists as one separate list </p>\n\n<p><code>df</code> : Pandas DataFrame</p>\n",
        "question_body": "<p>I have list of lists. Below is how my list looks like, I want to parse it into a data frame with continuation of values with columns = A,B,C</p>\n\n<pre><code>[     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n\n      A  B  C\n  0   4  5  6\n  1   4  5  6\n  2   4  5  6\n  3   4  5  6\n]\n\n</code></pre>\n\n<p>The expected data frame is as below</p>\n\n<pre><code>     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n  4   4  5  6\n  5   4  5  6\n  6   4  5  6\n  7   4  5  6\n\n</code></pre>\n\n<p>Really appreciate the help.</p>\n",
        "formatted_input": {
            "qid": 61556827,
            "link": "https://stackoverflow.com/questions/61556827/parsing-a-list-of-lists-to-a-data-frame-in-pandas",
            "question": {
                "title": "Parsing a list of lists to a data frame in pandas",
                "ques_desc": "I have list of lists. Below is how my list looks like, I want to parse it into a data frame with continuation of values with columns = A,B,C The expected data frame is as below Really appreciate the help. "
            },
            "io": [
                "[     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n\n      A  B  C\n  0   4  5  6\n  1   4  5  6\n  2   4  5  6\n  3   4  5  6\n]\n\n",
                "     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n  4   4  5  6\n  5   4  5  6\n  6   4  5  6\n  7   4  5  6\n\n"
            ],
            "answer": {
                "ans_desc": "Try this: : List which contains sub-lists : First it takes sub-lists then append it to : Final list which contains all sub-lists as one separate list : Pandas DataFrame ",
                "code": [
                    "import pandas as pd\nlist1=[]\ncount=0\nwhile count<len(myList):\n    list2= myList[count]\n    list1.append(list2)\n    #print(list2)\n    count+=1\ndf = pd.concat(list1)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "filter"
        ],
        "owner": {
            "reputation": 312,
            "user_id": 8499604,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/84f015e339670c2cd02c6cb2fba9b6d1?s=128&d=identicon&r=PG&f=1",
            "display_name": "Arman Mojaver",
            "link": "https://stackoverflow.com/users/8499604/arman-mojaver"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 61545098,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1588346739,
        "creation_date": 1588345364,
        "last_edit_date": 1588345785,
        "question_id": 61545023,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61545023/selecting-rows-on-pandas-dataframe-based-on-conditions",
        "title": "selecting rows on pandas dataframe based on conditions",
        "body": "<p>I have the following code:</p>\n\n<pre><code>import pandas as pd\nimport random\n\n\na = [random.randint(0, 1) for i in range(30)]\nb = [random.randint(0, 1) for i in range(30)]\n\nprint(a)\nprint(b)\n\ndf = pd.DataFrame([a, b])\ndf = df.T\n\ncolumns = ['column1', 'column2']\ndf.columns = columns\nprint(df)\n</code></pre>\n\n<p>that creates a dataframe stored in variable 'df'. It consists of 2 columns (column1 and column2) filled with random 0s and 1s.</p>\n\n<p>This is the output I got when I ran the program (If you try to run it you won't get exactly the same result because of the randomint generation).</p>\n\n<pre><code>    column1  column2\n0         0        1\n1         1        0\n2         0        1\n3         1        1\n4         0        1\n5         1        1\n6         0        1\n7         1        1\n8         1        0\n9         0        1\n10        0        0\n11        1        1\n12        1        1\n13        0        1\n14        0        0\n15        0        1\n16        1        1\n17        1        1\n18        0        1\n19        1        0\n20        0        0\n21        1        0\n22        0        1\n23        1        0\n24        1        1\n25        0        0\n26        1        1\n27        1        0\n28        0        1\n29        1        0\n</code></pre>\n\n<p>I would like to create a filter on column2, showing only the clusters of data when there are three or more 1s in a row. The output would be something like this:</p>\n\n<pre><code>    column1  column2\n2         0        1\n3         1        1\n4         0        1\n5         1        1\n6         0        1\n7         1        1\n\n11        1        1\n12        1        1\n13        0        1\n\n15        0        1\n16        1        1\n17        1        1\n18        0        1\n</code></pre>\n\n<p>I have left a space between the clusters for visual clarity, but the real output would not have the empty spaces in the dataframe.</p>\n\n<p>I would like to do it in the following way. </p>\n\n<pre><code>filter1 = (some boolean condition) &amp;/| (maybe some other stuff)\nfinal_df = df[filter1]\n</code></pre>\n\n<p>Thank you</p>\n",
        "answer_body": "<p>We can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a>.</p>\n\n<pre><code>n = 3\nblocks = df['column2'].ne(df['column2'].shift()).cumsum()\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\nm2 = df['column2'].eq(1)\ndf_filtered = df.loc[m1 &amp; m2]\n# Alternative without df['column2'].eq(1)\n#df_filtered = df.loc[m1.mul(df['column2'])]\nprint(df_filtered)\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>    column1  column2\n2         0        1\n3         1        1\n4         0        1\n5         1        1\n6         0        1\n7         1        1\n\n11        1        1\n12        1        1\n13        0        1\n\n15        0        1\n16        1        1\n17        1        1\n18        0        1\n</code></pre>\n\n<p>If column2 really contains only 1's and 0's in your original DataFrame then we can use <code>transform('sum')</code> instead <code>transform('size')</code></p>\n\n<hr>\n\n<p>blocks has a new value every time the value in <code>column2</code> changes</p>\n\n<pre><code>print(blocks)\n0      1\n1      2\n2      3\n3      3\n4      3\n5      3\n6      3\n7      3\n8      4\n9      5\n10     6\n11     7\n12     7\n13     7\n14     8\n15     9\n16     9\n17     9\n18     9\n19    10\n20    10\n21    10\n22    11\n23    12\n24    13\n25    14\n26    15\n27    16\n28    17\n29    18\nName: column2, dtype: int64\n</code></pre>\n\n<p><strong>Alternative</strong></p>\n\n<p>I often use this code in my projects and I came to the conclusion that it can be generally a little bit faster to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\" rel=\"nofollow noreferrer\"><code>Series.map</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\" rel=\"nofollow noreferrer\"><code>Series.value_counts</code></a>. The performance difference between the two methods will never be great and you can choose the one you want. But I usually use this last one that I have explained and I think it was worth mentioning it</p>\n\n<pre><code>%%timeit\nm1 = blocks.map(blocks.value_counts().ge(n))\n1.41 ms \u00b1 122 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n\n%%timeit\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\n2.12 ms \u00b1 226 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n",
        "question_body": "<p>I have the following code:</p>\n\n<pre><code>import pandas as pd\nimport random\n\n\na = [random.randint(0, 1) for i in range(30)]\nb = [random.randint(0, 1) for i in range(30)]\n\nprint(a)\nprint(b)\n\ndf = pd.DataFrame([a, b])\ndf = df.T\n\ncolumns = ['column1', 'column2']\ndf.columns = columns\nprint(df)\n</code></pre>\n\n<p>that creates a dataframe stored in variable 'df'. It consists of 2 columns (column1 and column2) filled with random 0s and 1s.</p>\n\n<p>This is the output I got when I ran the program (If you try to run it you won't get exactly the same result because of the randomint generation).</p>\n\n<pre><code>    column1  column2\n0         0        1\n1         1        0\n2         0        1\n3         1        1\n4         0        1\n5         1        1\n6         0        1\n7         1        1\n8         1        0\n9         0        1\n10        0        0\n11        1        1\n12        1        1\n13        0        1\n14        0        0\n15        0        1\n16        1        1\n17        1        1\n18        0        1\n19        1        0\n20        0        0\n21        1        0\n22        0        1\n23        1        0\n24        1        1\n25        0        0\n26        1        1\n27        1        0\n28        0        1\n29        1        0\n</code></pre>\n\n<p>I would like to create a filter on column2, showing only the clusters of data when there are three or more 1s in a row. The output would be something like this:</p>\n\n<pre><code>    column1  column2\n2         0        1\n3         1        1\n4         0        1\n5         1        1\n6         0        1\n7         1        1\n\n11        1        1\n12        1        1\n13        0        1\n\n15        0        1\n16        1        1\n17        1        1\n18        0        1\n</code></pre>\n\n<p>I have left a space between the clusters for visual clarity, but the real output would not have the empty spaces in the dataframe.</p>\n\n<p>I would like to do it in the following way. </p>\n\n<pre><code>filter1 = (some boolean condition) &amp;/| (maybe some other stuff)\nfinal_df = df[filter1]\n</code></pre>\n\n<p>Thank you</p>\n",
        "formatted_input": {
            "qid": 61545023,
            "link": "https://stackoverflow.com/questions/61545023/selecting-rows-on-pandas-dataframe-based-on-conditions",
            "question": {
                "title": "selecting rows on pandas dataframe based on conditions",
                "ques_desc": "I have the following code: that creates a dataframe stored in variable 'df'. It consists of 2 columns (column1 and column2) filled with random 0s and 1s. This is the output I got when I ran the program (If you try to run it you won't get exactly the same result because of the randomint generation). I would like to create a filter on column2, showing only the clusters of data when there are three or more 1s in a row. The output would be something like this: I have left a space between the clusters for visual clarity, but the real output would not have the empty spaces in the dataframe. I would like to do it in the following way. Thank you "
            },
            "io": [
                "    column1  column2\n0         0        1\n1         1        0\n2         0        1\n3         1        1\n4         0        1\n5         1        1\n6         0        1\n7         1        1\n8         1        0\n9         0        1\n10        0        0\n11        1        1\n12        1        1\n13        0        1\n14        0        0\n15        0        1\n16        1        1\n17        1        1\n18        0        1\n19        1        0\n20        0        0\n21        1        0\n22        0        1\n23        1        0\n24        1        1\n25        0        0\n26        1        1\n27        1        0\n28        0        1\n29        1        0\n",
                "    column1  column2\n2         0        1\n3         1        1\n4         0        1\n5         1        1\n6         0        1\n7         1        1\n\n11        1        1\n12        1        1\n13        0        1\n\n15        0        1\n16        1        1\n17        1        1\n18        0        1\n"
            ],
            "answer": {
                "ans_desc": "We can use . Output If column2 really contains only 1's and 0's in your original DataFrame then we can use instead blocks has a new value every time the value in changes Alternative I often use this code in my projects and I came to the conclusion that it can be generally a little bit faster to use + . The performance difference between the two methods will never be great and you can choose the one you want. But I usually use this last one that I have explained and I think it was worth mentioning it ",
                "code": [
                    "n = 3\nblocks = df['column2'].ne(df['column2'].shift()).cumsum()\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\nm2 = df['column2'].eq(1)\ndf_filtered = df.loc[m1 & m2]\n# Alternative without df['column2'].eq(1)\n#df_filtered = df.loc[m1.mul(df['column2'])]\nprint(df_filtered)\n",
                    "%%timeit\nm1 = blocks.map(blocks.value_counts().ge(n))\n1.41 ms \u00b1 122 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n\n%%timeit\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\n2.12 ms \u00b1 226 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "interpolation"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 13443223,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-O389lShh5o8/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMqSmGXMtnxMWjtLAxurlPZH3LOyw/photo.jpg?sz=128",
            "display_name": "Mooncake",
            "link": "https://stackoverflow.com/users/13443223/mooncake"
        },
        "is_answered": true,
        "view_count": 931,
        "accepted_answer_id": 61531477,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1588277484,
        "creation_date": 1588273146,
        "last_edit_date": 1592644375,
        "question_id": 61530817,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61530817/pandas-interpolate-nans-from-zero-to-next-valid-value",
        "title": "Pandas interpolate NaNs from zero to next valid value",
        "body": "<p>I am looking for a way to linear interpolate missing values (NaN) from zero to the next valid value.</p>\n<p>E.g.:</p>\n<pre><code>     A    B   C   D  E\n0  NaN  2.0 NaN NaN  0\n1  3.0  4.0 NaN NaN  1\n2  NaN  NaN NaN NaN  5\n3  NaN  3.0 NaN NaN  4\n</code></pre>\n<p>Given this table, i want the output to look like this:</p>\n<pre><code>     A    B   C   D  E\n0  NaN  2.0   0   0  0\n1  3.0  4.0   0 0.5  1\n2  NaN  NaN NaN NaN  5\n3  NaN  3.0   0   2  4\n</code></pre>\n<p>I've tried using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\" rel=\"nofollow noreferrer\">fillna</a> to fill only the next NaN to a valid value to 0 and to then linear interpolate the whole dataframe.\nThe problem I'm facing here is that specifying a value and a limit with fillna won't affect consecutive NaNs, but limit the total amount of columns to be filled.</p>\n<p>If possible please only suggest solutions without iterating over each row manually since I'm working with large dataframes.</p>\n<p>Thanks in advance.</p>\n",
        "answer_body": "<p>Here's a method that will work to replace 0 for the first <code>NaN</code> after a valid number and then will interpolate row-wise. I added extra rows in the end to illustrate the behavior for multiple fillings on the same row, fillings of only one value, or rows that end in NaN streaks.</p>\n\n<h3>Sample Data</h3>\n\n<pre><code>     A    B   C   D  E\n0  NaN  2.0 NaN NaN  0\n1  3.0  4.0 NaN NaN  1\n2  NaN  NaN NaN NaN  5\n3  NaN  3.0 NaN NaN  4\n4  3   NaN  7  NaN   5\n5  NaN  4   7  NaN   6\n6  NaN  4   7  NaN  NaN\n7  5   NaN  5  NaN  NaN\n</code></pre>\n\n<h3>Code</h3>\n\n<pre><code>m = (df.notnull().cummax(axis=1) &amp; df.isnull()).astype(int).diff(axis=1).fillna(0)\nupdate = m.where(m.eq(1) &amp; m.loc[:, ::-1].cummin(axis=1).eq(-1)).replace(1, 0)\n\ndf.update(update)  # Add in 0s\n\ndf = df.interpolate(axis=1, limit_area='inside')\n</code></pre>\n\n<hr>\n\n<pre><code>     A    B    C    D    E\n0  NaN  2.0  0.0  0.0  0.0\n1  3.0  4.0  0.0  0.5  1.0\n2  NaN  NaN  NaN  NaN  5.0\n3  NaN  3.0  0.0  2.0  4.0\n4  3.0  0.0  7.0  0.0  5.0\n5  NaN  4.0  7.0  0.0  6.0\n6  NaN  4.0  7.0  NaN  NaN\n7  5.0  0.0  5.0  NaN  NaN\n</code></pre>\n\n<hr>\n\n<p>How it works: </p>\n\n<pre><code>(df.notnull().cummax(1) &amp; df.isnull())  # True for streaks of null after non-null\n#       A      B      C      D      E\n#0  False  False   True   True  False\n#1  False  False   True   True  False\n#2  False  False  False  False  False\n#3  False  False   True   True  False\n#4  False   True  False   True  False\n#5  False  False  False   True  False\n#6  False  False  False   True   True\n#7  False   True  False   True   True\n\n# Taking the diff then allows you to find only the first NaN after any non-null.\n# I.e. flagged by `1`\n(df.notnull().cummax(1) &amp; df.isnull()).astype(int).diff(axis=1).fillna(0)\n#     A    B    C    D    E\n#0  0.0  0.0  1.0  0.0 -1.0\n#1  0.0  0.0  1.0  0.0 -1.0\n#2  0.0  0.0  0.0  0.0  0.0\n#3  0.0  0.0  1.0  0.0 -1.0\n#4  0.0  1.0 -1.0  1.0 -1.0\n#5  0.0  0.0  0.0  1.0 -1.0\n#6  0.0  0.0  0.0  1.0  0.0\n#7  0.0  1.0 -1.0  1.0  0.0\n\n# The update DataFrame is a like-indexed DF with 0s where they get filled.\n# The reversed cummin ensures fills only if there's a non-null value after the 0.\nm.where(m.eq(1) &amp; m.loc[:, ::-1].cummin(1).eq(-1)).replace(1, 0)\n#    A    B    C    D   E\n#0 NaN  NaN  0.0  NaN NaN\n#1 NaN  NaN  0.0  NaN NaN\n#2 NaN  NaN  NaN  NaN NaN\n#3 NaN  NaN  0.0  NaN NaN\n#4 NaN  0.0  NaN  0.0 NaN\n#5 NaN  NaN  NaN  0.0 NaN\n#6 NaN  NaN  NaN  NaN NaN\n#7 NaN  0.0  NaN  NaN NaN\n</code></pre>\n",
        "question_body": "<p>I am looking for a way to linear interpolate missing values (NaN) from zero to the next valid value.</p>\n<p>E.g.:</p>\n<pre><code>     A    B   C   D  E\n0  NaN  2.0 NaN NaN  0\n1  3.0  4.0 NaN NaN  1\n2  NaN  NaN NaN NaN  5\n3  NaN  3.0 NaN NaN  4\n</code></pre>\n<p>Given this table, i want the output to look like this:</p>\n<pre><code>     A    B   C   D  E\n0  NaN  2.0   0   0  0\n1  3.0  4.0   0 0.5  1\n2  NaN  NaN NaN NaN  5\n3  NaN  3.0   0   2  4\n</code></pre>\n<p>I've tried using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\" rel=\"nofollow noreferrer\">fillna</a> to fill only the next NaN to a valid value to 0 and to then linear interpolate the whole dataframe.\nThe problem I'm facing here is that specifying a value and a limit with fillna won't affect consecutive NaNs, but limit the total amount of columns to be filled.</p>\n<p>If possible please only suggest solutions without iterating over each row manually since I'm working with large dataframes.</p>\n<p>Thanks in advance.</p>\n",
        "formatted_input": {
            "qid": 61530817,
            "link": "https://stackoverflow.com/questions/61530817/pandas-interpolate-nans-from-zero-to-next-valid-value",
            "question": {
                "title": "Pandas interpolate NaNs from zero to next valid value",
                "ques_desc": "I am looking for a way to linear interpolate missing values (NaN) from zero to the next valid value. E.g.: Given this table, i want the output to look like this: I've tried using fillna to fill only the next NaN to a valid value to 0 and to then linear interpolate the whole dataframe. The problem I'm facing here is that specifying a value and a limit with fillna won't affect consecutive NaNs, but limit the total amount of columns to be filled. If possible please only suggest solutions without iterating over each row manually since I'm working with large dataframes. Thanks in advance. "
            },
            "io": [
                "     A    B   C   D  E\n0  NaN  2.0 NaN NaN  0\n1  3.0  4.0 NaN NaN  1\n2  NaN  NaN NaN NaN  5\n3  NaN  3.0 NaN NaN  4\n",
                "     A    B   C   D  E\n0  NaN  2.0   0   0  0\n1  3.0  4.0   0 0.5  1\n2  NaN  NaN NaN NaN  5\n3  NaN  3.0   0   2  4\n"
            ],
            "answer": {
                "ans_desc": "Here's a method that will work to replace 0 for the first after a valid number and then will interpolate row-wise. I added extra rows in the end to illustrate the behavior for multiple fillings on the same row, fillings of only one value, or rows that end in NaN streaks. Sample Data Code How it works: ",
                "code": [
                    "m = (df.notnull().cummax(axis=1) & df.isnull()).astype(int).diff(axis=1).fillna(0)\nupdate = m.where(m.eq(1) & m.loc[:, ::-1].cummin(axis=1).eq(-1)).replace(1, 0)\n\ndf.update(update)  # Add in 0s\n\ndf = df.interpolate(axis=1, limit_area='inside')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 637,
            "user_id": 12292032,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7819db688fd7a9e06647ddbae261cdeb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ewdlam",
            "link": "https://stackoverflow.com/users/12292032/ewdlam"
        },
        "is_answered": true,
        "view_count": 332,
        "accepted_answer_id": 61351894,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1587499012,
        "creation_date": 1587497873,
        "question_id": 61351795,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61351795/how-to-delete-the-first-and-last-rows-with-nan-of-a-dataframe-and-replace-the-re",
        "title": "How to delete the first and last rows with NaN of a dataframe and replace the remaining NaN with the average of the values below and above?",
        "body": "<p>Let's take this dataframe as a simple example:</p>\n\n<pre><code>df = pd.DataFrame(dict(Col1=[np.nan,1,1,2,3,8,7], Col2=[1,1,np.nan,np.nan,3,np.nan,4], Col3=[1,1,np.nan,5,1,1,np.nan]))\n\n   Col1  Col2  Col3\n0   NaN   1.0   1.0\n1   1.0   1.0   1.0\n2   1.0   NaN   NaN\n3   2.0   NaN   5.0\n4   3.0   3.0   1.0\n5   8.0   NaN   1.0\n6   7.0   4.0   NaN\n</code></pre>\n\n<p>I would like first to remove first and last rows until there is no longer NaN in the first and last row.  </p>\n\n<p><strong>Intermediate expected output :</strong></p>\n\n<pre><code>   Col1  Col2  Col3\n1   1.0   1.0   1.0\n2   1.0   NaN   NaN\n3   2.0   NaN   5.0\n4   3.0   3.0   1.0\n</code></pre>\n\n<p>Then, I would like to replace the remaining NaN by the mean of the nearest value below which is not a NaN, and the one above.  </p>\n\n<p><strong>Final expected output :</strong></p>\n\n<pre><code>   Col1  Col2  Col3\n0   1.0   1.0   1.0\n1   1.0   2.0   3.0\n2   2.0   2.0   5.0\n3   3.0   3.0   1.0\n</code></pre>\n\n<p>I know I can have the positions of NaN in my dataframe through</p>\n\n<pre><code>df.isna()\n</code></pre>\n\n<p>But I can't solve my problem. How please could I do ?</p>\n",
        "answer_body": "<p>My approach:</p>\n\n<pre><code># identify the rows with some NaN\ns = df.notnull().all(1)\n\n# remove those with NaN at beginning and at the end:\nnew_df = df.loc[s.idxmax():s[::-1].idxmax()]\n\n# average:\nnew_df = (new_df.ffill()+ new_df.bfill())/2\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   Col1  Col2  Col3\n1   1.0   1.0   1.0\n2   1.0   2.0   3.0\n3   2.0   2.0   5.0\n4   3.0   3.0   1.0\n</code></pre>\n",
        "question_body": "<p>Let's take this dataframe as a simple example:</p>\n\n<pre><code>df = pd.DataFrame(dict(Col1=[np.nan,1,1,2,3,8,7], Col2=[1,1,np.nan,np.nan,3,np.nan,4], Col3=[1,1,np.nan,5,1,1,np.nan]))\n\n   Col1  Col2  Col3\n0   NaN   1.0   1.0\n1   1.0   1.0   1.0\n2   1.0   NaN   NaN\n3   2.0   NaN   5.0\n4   3.0   3.0   1.0\n5   8.0   NaN   1.0\n6   7.0   4.0   NaN\n</code></pre>\n\n<p>I would like first to remove first and last rows until there is no longer NaN in the first and last row.  </p>\n\n<p><strong>Intermediate expected output :</strong></p>\n\n<pre><code>   Col1  Col2  Col3\n1   1.0   1.0   1.0\n2   1.0   NaN   NaN\n3   2.0   NaN   5.0\n4   3.0   3.0   1.0\n</code></pre>\n\n<p>Then, I would like to replace the remaining NaN by the mean of the nearest value below which is not a NaN, and the one above.  </p>\n\n<p><strong>Final expected output :</strong></p>\n\n<pre><code>   Col1  Col2  Col3\n0   1.0   1.0   1.0\n1   1.0   2.0   3.0\n2   2.0   2.0   5.0\n3   3.0   3.0   1.0\n</code></pre>\n\n<p>I know I can have the positions of NaN in my dataframe through</p>\n\n<pre><code>df.isna()\n</code></pre>\n\n<p>But I can't solve my problem. How please could I do ?</p>\n",
        "formatted_input": {
            "qid": 61351795,
            "link": "https://stackoverflow.com/questions/61351795/how-to-delete-the-first-and-last-rows-with-nan-of-a-dataframe-and-replace-the-re",
            "question": {
                "title": "How to delete the first and last rows with NaN of a dataframe and replace the remaining NaN with the average of the values below and above?",
                "ques_desc": "Let's take this dataframe as a simple example: I would like first to remove first and last rows until there is no longer NaN in the first and last row. Intermediate expected output : Then, I would like to replace the remaining NaN by the mean of the nearest value below which is not a NaN, and the one above. Final expected output : I know I can have the positions of NaN in my dataframe through But I can't solve my problem. How please could I do ? "
            },
            "io": [
                "   Col1  Col2  Col3\n1   1.0   1.0   1.0\n2   1.0   NaN   NaN\n3   2.0   NaN   5.0\n4   3.0   3.0   1.0\n",
                "   Col1  Col2  Col3\n0   1.0   1.0   1.0\n1   1.0   2.0   3.0\n2   2.0   2.0   5.0\n3   3.0   3.0   1.0\n"
            ],
            "answer": {
                "ans_desc": "My approach: Output: ",
                "code": [
                    "# identify the rows with some NaN\ns = df.notnull().all(1)\n\n# remove those with NaN at beginning and at the end:\nnew_df = df.loc[s.idxmax():s[::-1].idxmax()]\n\n# average:\nnew_df = (new_df.ffill()+ new_df.bfill())/2\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "arrays",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 663,
            "user_id": 1912104,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/342abcc6cd5ab765d7bc040fb72c0cd5?s=128&d=identicon&r=PG",
            "display_name": "NonSleeper",
            "link": "https://stackoverflow.com/users/1912104/nonsleeper"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 61348362,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1587486889,
        "creation_date": 1587486085,
        "question_id": 61348221,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61348221/how-to-replace-values-among-blocks-of-consecutive-values",
        "title": "How to replace values among blocks of consecutive values",
        "body": "<p>I have a list like this:</p>\n\n<pre><code>list_tmp = [np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN]\n</code></pre>\n\n<p>So in this list there are blocks of consecutive values, separated by <code>NaN</code>. </p>\n\n<p>How can I replace the values before the maximum of each block, for example with -1. The result looks like:</p>\n\n<pre><code>list_tmp = [np.NaN, np.NaN, -1, -1, 3, np.NaN, -1, 2, np.NaN, np.NaN, -1, -1, -1, 4, np.NaN]\n</code></pre>\n",
        "answer_body": "<p>Since the maximum value is just the last non-NaN value, you can obtain the indices of the values to set to <code>-1</code> by checking if a given value is not a <code>NaN</code> and <em>neither</em> is the following:</p>\n\n<pre><code>a = np.array([np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN])\n\na[~np.isnan(a) &amp; ~np.isnan(np.r_[a[1:],np.nan])] = -1\n</code></pre>\n\n<hr>\n\n<pre><code>print(a)\narray([nan, nan, -1., -1.,  3., nan, -1.,  2., nan, nan, -1., -1., -1., 4., nan])\n</code></pre>\n",
        "question_body": "<p>I have a list like this:</p>\n\n<pre><code>list_tmp = [np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN]\n</code></pre>\n\n<p>So in this list there are blocks of consecutive values, separated by <code>NaN</code>. </p>\n\n<p>How can I replace the values before the maximum of each block, for example with -1. The result looks like:</p>\n\n<pre><code>list_tmp = [np.NaN, np.NaN, -1, -1, 3, np.NaN, -1, 2, np.NaN, np.NaN, -1, -1, -1, 4, np.NaN]\n</code></pre>\n",
        "formatted_input": {
            "qid": 61348221,
            "link": "https://stackoverflow.com/questions/61348221/how-to-replace-values-among-blocks-of-consecutive-values",
            "question": {
                "title": "How to replace values among blocks of consecutive values",
                "ques_desc": "I have a list like this: So in this list there are blocks of consecutive values, separated by . How can I replace the values before the maximum of each block, for example with -1. The result looks like: "
            },
            "io": [
                "list_tmp = [np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN]\n",
                "list_tmp = [np.NaN, np.NaN, -1, -1, 3, np.NaN, -1, 2, np.NaN, np.NaN, -1, -1, -1, 4, np.NaN]\n"
            ],
            "answer": {
                "ans_desc": "Since the maximum value is just the last non-NaN value, you can obtain the indices of the values to set to by checking if a given value is not a and neither is the following: ",
                "code": [
                    "a = np.array([np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN])\n\na[~np.isnan(a) & ~np.isnan(np.r_[a[1:],np.nan])] = -1\n",
                    "print(a)\narray([nan, nan, -1., -1.,  3., nan, -1.,  2., nan, nan, -1., -1., -1., 4., nan])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 67,
            "user_id": 10755205,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/tPfka.jpg?s=128&g=1",
            "display_name": "Sarah De Cock",
            "link": "https://stackoverflow.com/users/10755205/sarah-de-cock"
        },
        "is_answered": true,
        "view_count": 200,
        "accepted_answer_id": 61293954,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1587308072,
        "creation_date": 1587231154,
        "last_edit_date": 1587308072,
        "question_id": 61293428,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61293428/match-on-multiple-columns-using-array",
        "title": "Match on multiple columns using array",
        "body": "<p>I'm working on a project where my original dataframe is: </p>\n\n<pre><code>      A     B    C   label\n0     1     2    2    Nan\n1     2     4    5    7\n2     3     6    5    Nan\n3     4     8    7    Nan\n4     5    10    3    8\n5     6    12    4    8\n</code></pre>\n\n<p>But, I have an array with new labels for certain points (for that I only used columns A and B) in the original dataframe. Something like this:</p>\n\n<pre><code>X_labeled = [[2, 4], [3,6]]\ny_labeled = [5,9]\n</code></pre>\n\n<p>My goal is to add the new labels to the original dataframe. I know that the combination of A and B unique is. What is the fastest way to assign the new label to the correct row?</p>\n\n<p>This is my try:</p>\n\n<pre><code>y_labeled = np.array(y).astype('float64')\n\n    current_position = 0\n    for point in X_labeled:\n        row = df.loc[(df['A'] == point[0]) &amp; (df['B'] == point[1])]\n        df.at[row.index, 'label'] = y_labeled[current_position]\n        current_position += 1\n</code></pre>\n\n<p>Wanted output (rows with index 1 and 2 are changed):</p>\n\n<pre><code>       A     B    C   label\n0     1     2    2    Nan\n1     2     4    5    5\n2     3     6    5    9\n3     4     8    7    Nan\n4     5    10    3    8\n5     6    12    4    8\n</code></pre>\n\n<p>For small datasets may this be okay with I'm currently using it for datasets with more than 25000 labels. Is there a way that is faster?</p>\n\n<p>Also, in some cases I used all columns expect the column 'label'. That dataframe exists out of 64 columns so my method can not be used here. Has someone an idea to improve this?</p>\n\n<p>Thanks in advance </p>\n",
        "answer_body": "<p>Here's a numpy based approach aimed at performance. To vectorize this we want a way to check membership of the rows in <code>X_labeled</code> in columns <code>A</code> and <code>B</code>. So what we can do, is view these two columns as 1D arrays (based on <a href=\"https://stackoverflow.com/a/16216866/9698684\">this answer</a>) and <em>then</em> we can use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.in1d.html\" rel=\"nofollow noreferrer\"><code>np.in1d</code></a> to index the dataframe and assign the values in <code>y_labeled</code>:</p>\n\n<pre><code>import numpy as np\n\nX_labeled = [[2, 4], [3,6]]\ny_labeled = [5,9]\n\na = df.values[:,:2].astype(int) #indexing on A and B\n\ndef view_as_1d(a):\n    a = np.ascontiguousarray(a)\n    return a.view(np.dtype((np.void, a.dtype.itemsize * a.shape[-1])))\n\nix = np.in1d(view_as_1d(a), view_as_1d(X_labeled))\ndf.loc[ix, 'label'] = y_labeled\n</code></pre>\n\n<hr>\n\n<pre><code>print(df)\n\n   A   B  C label\n0  1   2  2   Nan\n1  2   4  5     5\n2  3   6  5     9\n3  4   8  7   Nan\n4  5  10  3     8\n5  6  12  4     8\n</code></pre>\n",
        "question_body": "<p>I'm working on a project where my original dataframe is: </p>\n\n<pre><code>      A     B    C   label\n0     1     2    2    Nan\n1     2     4    5    7\n2     3     6    5    Nan\n3     4     8    7    Nan\n4     5    10    3    8\n5     6    12    4    8\n</code></pre>\n\n<p>But, I have an array with new labels for certain points (for that I only used columns A and B) in the original dataframe. Something like this:</p>\n\n<pre><code>X_labeled = [[2, 4], [3,6]]\ny_labeled = [5,9]\n</code></pre>\n\n<p>My goal is to add the new labels to the original dataframe. I know that the combination of A and B unique is. What is the fastest way to assign the new label to the correct row?</p>\n\n<p>This is my try:</p>\n\n<pre><code>y_labeled = np.array(y).astype('float64')\n\n    current_position = 0\n    for point in X_labeled:\n        row = df.loc[(df['A'] == point[0]) &amp; (df['B'] == point[1])]\n        df.at[row.index, 'label'] = y_labeled[current_position]\n        current_position += 1\n</code></pre>\n\n<p>Wanted output (rows with index 1 and 2 are changed):</p>\n\n<pre><code>       A     B    C   label\n0     1     2    2    Nan\n1     2     4    5    5\n2     3     6    5    9\n3     4     8    7    Nan\n4     5    10    3    8\n5     6    12    4    8\n</code></pre>\n\n<p>For small datasets may this be okay with I'm currently using it for datasets with more than 25000 labels. Is there a way that is faster?</p>\n\n<p>Also, in some cases I used all columns expect the column 'label'. That dataframe exists out of 64 columns so my method can not be used here. Has someone an idea to improve this?</p>\n\n<p>Thanks in advance </p>\n",
        "formatted_input": {
            "qid": 61293428,
            "link": "https://stackoverflow.com/questions/61293428/match-on-multiple-columns-using-array",
            "question": {
                "title": "Match on multiple columns using array",
                "ques_desc": "I'm working on a project where my original dataframe is: But, I have an array with new labels for certain points (for that I only used columns A and B) in the original dataframe. Something like this: My goal is to add the new labels to the original dataframe. I know that the combination of A and B unique is. What is the fastest way to assign the new label to the correct row? This is my try: Wanted output (rows with index 1 and 2 are changed): For small datasets may this be okay with I'm currently using it for datasets with more than 25000 labels. Is there a way that is faster? Also, in some cases I used all columns expect the column 'label'. That dataframe exists out of 64 columns so my method can not be used here. Has someone an idea to improve this? Thanks in advance "
            },
            "io": [
                "      A     B    C   label\n0     1     2    2    Nan\n1     2     4    5    7\n2     3     6    5    Nan\n3     4     8    7    Nan\n4     5    10    3    8\n5     6    12    4    8\n",
                "       A     B    C   label\n0     1     2    2    Nan\n1     2     4    5    5\n2     3     6    5    9\n3     4     8    7    Nan\n4     5    10    3    8\n5     6    12    4    8\n"
            ],
            "answer": {
                "ans_desc": "Here's a numpy based approach aimed at performance. To vectorize this we want a way to check membership of the rows in in columns and . So what we can do, is view these two columns as 1D arrays (based on this answer) and then we can use to index the dataframe and assign the values in : ",
                "code": [
                    "import numpy as np\n\nX_labeled = [[2, 4], [3,6]]\ny_labeled = [5,9]\n\na = df.values[:,:2].astype(int) #indexing on A and B\n\ndef view_as_1d(a):\n    a = np.ascontiguousarray(a)\n    return a.view(np.dtype((np.void, a.dtype.itemsize * a.shape[-1])))\n\nix = np.in1d(view_as_1d(a), view_as_1d(X_labeled))\ndf.loc[ix, 'label'] = y_labeled\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 663,
            "user_id": 1912104,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/342abcc6cd5ab765d7bc040fb72c0cd5?s=128&d=identicon&r=PG",
            "display_name": "NonSleeper",
            "link": "https://stackoverflow.com/users/1912104/nonsleeper"
        },
        "is_answered": true,
        "view_count": 352,
        "accepted_answer_id": 61292882,
        "answer_count": 6,
        "score": 9,
        "last_activity_date": 1587290300,
        "creation_date": 1587228323,
        "last_edit_date": 1587232479,
        "question_id": 61292759,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61292759/how-to-fill-elements-between-intervals-of-a-list",
        "title": "How to fill elements between intervals of a list",
        "body": "<p>I have a list like this:</p>\n\n<pre><code>list_1 = [np.NaN, np.NaN, 1, np.NaN, np.NaN, np.NaN, 0, np.NaN, 1, np.NaN, 0, 1, np.NaN, 0, np.NaN,  1, np.NaN]\n</code></pre>\n\n<p>So there are intervals that begin with <code>1</code> and end with <code>0</code>.\nHow can I replace the values in those intervals, say with 1? The outcome will look like this:</p>\n\n<pre><code>list_2 = [np.NaN, np.NaN, 1, 1, 1, 1, 0, np.NaN, 1, 1, 0, 1, 1, 0, np.NaN, 1, np.NaN]\n</code></pre>\n\n<p>I use <code>NaN</code> in this example, but a generalized solution that can apply to any value will also be great </p>\n",
        "answer_body": "<p><strong>Pandas solution:</strong></p>\n\n<pre><code>s = pd.Series(list_1)\ns1 = s.eq(1)\ns0 = s.eq(0)\nm = (s1 | s0).where(s1.cumsum().ge(1),False).cumsum().mod(2).eq(1)\ns.loc[m &amp; s.isna()] = 1\nprint(s.tolist())\n#[nan, nan, 1.0, 1.0, 1.0, 1.0, 0.0, nan, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, nan, 1.0, 1.0]\n</code></pre>\n\n<hr>\n\n<p>but if there is only <code>1</code>, <code>0</code> or <code>NaN</code> you can do:</p>\n\n<pre><code>s = pd.Series(list_1)\ns.fillna(s.ffill().where(lambda x: x.eq(1))).tolist()\n</code></pre>\n\n<p><strong>output</strong></p>\n\n<pre><code>[nan,\n nan,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 0.0,\n nan,\n 1.0,\n 1.0,\n 0.0,\n 1.0,\n 1.0,\n 0.0,\n nan,\n 1.0,\n 1.0]\n</code></pre>\n",
        "question_body": "<p>I have a list like this:</p>\n\n<pre><code>list_1 = [np.NaN, np.NaN, 1, np.NaN, np.NaN, np.NaN, 0, np.NaN, 1, np.NaN, 0, 1, np.NaN, 0, np.NaN,  1, np.NaN]\n</code></pre>\n\n<p>So there are intervals that begin with <code>1</code> and end with <code>0</code>.\nHow can I replace the values in those intervals, say with 1? The outcome will look like this:</p>\n\n<pre><code>list_2 = [np.NaN, np.NaN, 1, 1, 1, 1, 0, np.NaN, 1, 1, 0, 1, 1, 0, np.NaN, 1, np.NaN]\n</code></pre>\n\n<p>I use <code>NaN</code> in this example, but a generalized solution that can apply to any value will also be great </p>\n",
        "formatted_input": {
            "qid": 61292759,
            "link": "https://stackoverflow.com/questions/61292759/how-to-fill-elements-between-intervals-of-a-list",
            "question": {
                "title": "How to fill elements between intervals of a list",
                "ques_desc": "I have a list like this: So there are intervals that begin with and end with . How can I replace the values in those intervals, say with 1? The outcome will look like this: I use in this example, but a generalized solution that can apply to any value will also be great "
            },
            "io": [
                "list_1 = [np.NaN, np.NaN, 1, np.NaN, np.NaN, np.NaN, 0, np.NaN, 1, np.NaN, 0, 1, np.NaN, 0, np.NaN,  1, np.NaN]\n",
                "list_2 = [np.NaN, np.NaN, 1, 1, 1, 1, 0, np.NaN, 1, 1, 0, 1, 1, 0, np.NaN, 1, np.NaN]\n"
            ],
            "answer": {
                "ans_desc": "Pandas solution: but if there is only , or you can do: output ",
                "code": [
                    "s = pd.Series(list_1)\ns.fillna(s.ffill().where(lambda x: x.eq(1))).tolist()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 11125966,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/56900ae23f9aa4c7aa9ca5f26ceaef81?s=128&d=identicon&r=PG&f=1",
            "display_name": "J.James",
            "link": "https://stackoverflow.com/users/11125966/j-james"
        },
        "is_answered": true,
        "view_count": 64,
        "accepted_answer_id": 61233396,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1586969038,
        "creation_date": 1586966118,
        "last_edit_date": 1586967521,
        "question_id": 61233112,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61233112/how-to-replace-pandas-dataframe-values-based-on-lookup-values-in-another-datafra",
        "title": "How to replace pandas dataframe values based on lookup values in another dataframe?",
        "body": "<p>I have a large pandas dataframe with numerical values structured like this:</p>\n\n<pre><code>&gt;&gt;&gt; df1\n   A  B  C\n0  2  1  2\n1  1  2  3\n2  2  3  1\n</code></pre>\n\n<p>I need to replace all of the the above cell values with a 'description' that maps to the field name and cell value as referenced in another dataframe structured like this:</p>\n\n<pre><code>&gt;&gt;&gt; df2\n  field_name  code description\n0          A     1          NO\n1          A     2         YES\n2          A     3       MAYBE\n3          B     1           x\n4          B     2           y\n5          B     3           z\n6          C     1        GOOD\n7          C     2         BAD\n8          C     3        FINE\n</code></pre>\n\n<p>The desired output would be like:</p>\n\n<pre><code>&gt;&gt;&gt; df3\n     A  B     C\n0  YES  x   BAD\n1   NO  y  FINE\n2  YES  z  GOOD\n</code></pre>\n\n<p>I could figure out a way to do this on a small scale using something like .map or .replace - however the actual datasets contain thousands of records with hundreds of different combinations to replace. Any help would be really appreciated.</p>\n\n<p>Thanks.</p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\" rel=\"nofollow noreferrer\"><code>DataFrame.replace</code></a> with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html\" rel=\"nofollow noreferrer\"><code>DataFrame.pivot</code></a>:</p>\n\n<pre><code>df1 = df1.replace(df2.pivot(columns='field_name', index='code', values='description')\n                     .to_dict())\n</code></pre>\n\n<p>maybe you need select columns previously:</p>\n\n<pre><code>df1[cols] = df1[cols].replace(df2.pivot(columns='field_name',\n                                        index='code', values='description')\n                                 .to_dict())\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>print(df1)\n     A  B     C\n0  YES  x   BAD\n1   NO  y  FINE\n2  YES  z  GOOD\n</code></pre>\n",
        "question_body": "<p>I have a large pandas dataframe with numerical values structured like this:</p>\n\n<pre><code>&gt;&gt;&gt; df1\n   A  B  C\n0  2  1  2\n1  1  2  3\n2  2  3  1\n</code></pre>\n\n<p>I need to replace all of the the above cell values with a 'description' that maps to the field name and cell value as referenced in another dataframe structured like this:</p>\n\n<pre><code>&gt;&gt;&gt; df2\n  field_name  code description\n0          A     1          NO\n1          A     2         YES\n2          A     3       MAYBE\n3          B     1           x\n4          B     2           y\n5          B     3           z\n6          C     1        GOOD\n7          C     2         BAD\n8          C     3        FINE\n</code></pre>\n\n<p>The desired output would be like:</p>\n\n<pre><code>&gt;&gt;&gt; df3\n     A  B     C\n0  YES  x   BAD\n1   NO  y  FINE\n2  YES  z  GOOD\n</code></pre>\n\n<p>I could figure out a way to do this on a small scale using something like .map or .replace - however the actual datasets contain thousands of records with hundreds of different combinations to replace. Any help would be really appreciated.</p>\n\n<p>Thanks.</p>\n",
        "formatted_input": {
            "qid": 61233112,
            "link": "https://stackoverflow.com/questions/61233112/how-to-replace-pandas-dataframe-values-based-on-lookup-values-in-another-datafra",
            "question": {
                "title": "How to replace pandas dataframe values based on lookup values in another dataframe?",
                "ques_desc": "I have a large pandas dataframe with numerical values structured like this: I need to replace all of the the above cell values with a 'description' that maps to the field name and cell value as referenced in another dataframe structured like this: The desired output would be like: I could figure out a way to do this on a small scale using something like .map or .replace - however the actual datasets contain thousands of records with hundreds of different combinations to replace. Any help would be really appreciated. Thanks. "
            },
            "io": [
                ">>> df1\n   A  B  C\n0  2  1  2\n1  1  2  3\n2  2  3  1\n",
                ">>> df3\n     A  B     C\n0  YES  x   BAD\n1   NO  y  FINE\n2  YES  z  GOOD\n"
            ],
            "answer": {
                "ans_desc": "Use with : maybe you need select columns previously: Output ",
                "code": [
                    "df1 = df1.replace(df2.pivot(columns='field_name', index='code', values='description')\n                     .to_dict())\n",
                    "df1[cols] = df1[cols].replace(df2.pivot(columns='field_name',\n                                        index='code', values='description')\n                                 .to_dict())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 43,
            "user_id": 12383086,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5eb50974c597addff6eff6e783c43a3d?s=128&d=identicon&r=PG&f=1",
            "display_name": "MHY",
            "link": "https://stackoverflow.com/users/12383086/mhy"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 61195066,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1586806150,
        "creation_date": 1586805166,
        "last_edit_date": 1586806150,
        "question_id": 61195022,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61195022/pivot-table-in-pandas-with-two-columnindex-and-value",
        "title": "Pivot Table in Pandas with two column(Index and Value)",
        "body": "<p>I have a CSV file with <code>obj</code> and <code>VS</code> column.\nI need to sum <code>VS</code> values for each <code>obj</code> and have output like below</p>\n\n<p><strong>Input:</strong></p>\n\n<pre><code>+-----+------+\n| obj | \u00a0VS \u00a0|\n+-----+------+\n| B \u00a0 | 2048 |\n| A \u00a0 | 1024 |\n| B \u00a0 | \u00a0 10 |\n| A \u00a0 | 1024 |\n| B \u00a0 | 1025 |\n| A \u00a0 | 1026 |\n| B \u00a0 | 1027 |\n+-----+------+\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>+---+------+\n| A | 3074 |\n+---+------+\n| B | 4110 |\n+---+------+\n</code></pre>\n\n<p>I have tried below code,As I have just two column to apply I added <code>value</code> column with unique value to have pivot(pivot table need Index,Column and Value).Then <code>Value</code> column is just to help. However out put is sum thing weird!!!</p>\n\n<pre><code>import pandas as pd \nimport numpy as np \n\nfilename='1test.csv'\ndf = pd.read_csv(filename, dtype='str')\ndf[\"value\"]=1\npd.pivot_table(df, values=\"VS\", index=\"obj\", columns=\"value\", aggfunc=np.sum)\n</code></pre>\n\n<p><strong>output of my code</strong>:</p>\n\n<pre><code>+-------+----------------+\n| value | \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 \u00a0 \u00a0|\n+-------+----------------+\n| obj \u00a0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\n| A \u00a0 \u00a0 | \u00a0 102410241026 |\n| B \u00a0 \u00a0 | 20481010251027 |\n+-------+----------------+\n</code></pre>\n",
        "answer_body": "<p>Just consider that as you read from CSV, values are string, you need to convert them to int by <code>df['VS']=pd.to_numeric(df['VS'])</code>\n<code>print(df.dtypes)</code> show the type of column in df</p>\n\n<pre><code>import pandas as pd \nimport numpy as np \n\nfilename='1test.csv'\ndf = pd.read_csv(filename, dtype='str')\ndf[\"value\"]=1\nprint(df.dtypes)\ndf['VS']=pd.to_numeric(df['VS'])\nprint(df.dtypes)\npd.pivot_table(df, values=\"VS\", index=\"obj\", columns=\"value\", aggfunc=np.sum)\n</code></pre>\n",
        "question_body": "<p>I have a CSV file with <code>obj</code> and <code>VS</code> column.\nI need to sum <code>VS</code> values for each <code>obj</code> and have output like below</p>\n\n<p><strong>Input:</strong></p>\n\n<pre><code>+-----+------+\n| obj | \u00a0VS \u00a0|\n+-----+------+\n| B \u00a0 | 2048 |\n| A \u00a0 | 1024 |\n| B \u00a0 | \u00a0 10 |\n| A \u00a0 | 1024 |\n| B \u00a0 | 1025 |\n| A \u00a0 | 1026 |\n| B \u00a0 | 1027 |\n+-----+------+\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>+---+------+\n| A | 3074 |\n+---+------+\n| B | 4110 |\n+---+------+\n</code></pre>\n\n<p>I have tried below code,As I have just two column to apply I added <code>value</code> column with unique value to have pivot(pivot table need Index,Column and Value).Then <code>Value</code> column is just to help. However out put is sum thing weird!!!</p>\n\n<pre><code>import pandas as pd \nimport numpy as np \n\nfilename='1test.csv'\ndf = pd.read_csv(filename, dtype='str')\ndf[\"value\"]=1\npd.pivot_table(df, values=\"VS\", index=\"obj\", columns=\"value\", aggfunc=np.sum)\n</code></pre>\n\n<p><strong>output of my code</strong>:</p>\n\n<pre><code>+-------+----------------+\n| value | \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 \u00a0 \u00a0|\n+-------+----------------+\n| obj \u00a0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\n| A \u00a0 \u00a0 | \u00a0 102410241026 |\n| B \u00a0 \u00a0 | 20481010251027 |\n+-------+----------------+\n</code></pre>\n",
        "formatted_input": {
            "qid": 61195022,
            "link": "https://stackoverflow.com/questions/61195022/pivot-table-in-pandas-with-two-columnindex-and-value",
            "question": {
                "title": "Pivot Table in Pandas with two column(Index and Value)",
                "ques_desc": "I have a CSV file with and column. I need to sum values for each and have output like below Input: Output: I have tried below code,As I have just two column to apply I added column with unique value to have pivot(pivot table need Index,Column and Value).Then column is just to help. However out put is sum thing weird!!! output of my code: "
            },
            "io": [
                "+-----+------+\n| obj | \u00a0VS \u00a0|\n+-----+------+\n| B \u00a0 | 2048 |\n| A \u00a0 | 1024 |\n| B \u00a0 | \u00a0 10 |\n| A \u00a0 | 1024 |\n| B \u00a0 | 1025 |\n| A \u00a0 | 1026 |\n| B \u00a0 | 1027 |\n+-----+------+\n",
                "+---+------+\n| A | 3074 |\n+---+------+\n| B | 4110 |\n+---+------+\n"
            ],
            "answer": {
                "ans_desc": "Just consider that as you read from CSV, values are string, you need to convert them to int by show the type of column in df ",
                "code": [
                    "import pandas as pd \nimport numpy as np \n\nfilename='1test.csv'\ndf = pd.read_csv(filename, dtype='str')\ndf[\"value\"]=1\nprint(df.dtypes)\ndf['VS']=pd.to_numeric(df['VS'])\nprint(df.dtypes)\npd.pivot_table(df, values=\"VS\", index=\"obj\", columns=\"value\", aggfunc=np.sum)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 153,
            "user_id": 12690527,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-hgIclyEvphc/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcL7b17tAEylQ_wgaYdTmfwWDv4VQ/photo.jpg?sz=128",
            "display_name": "Tom&#225;s Carrera de Souza",
            "link": "https://stackoverflow.com/users/12690527/tom%c3%a1s-carrera-de-souza"
        },
        "is_answered": true,
        "view_count": 6018,
        "accepted_answer_id": 61170540,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1586690107,
        "creation_date": 1586687395,
        "question_id": 61170036,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61170036/error-none-of-index-dtype-object-are-in-the-index",
        "title": "Error: None of [Index([&#39;...&#39;], dtype=&#39;object&#39;)] are in the [index]",
        "body": "<p>I am trying to delete a grouped set of rows in pandas according to the following condition:</p>\n\n<p>If a group (grouped by col1) has more than 2 values 'c' in col2, then remove the whole group.</p>\n\n<p>What I have looks like this</p>\n\n<pre><code>  col1  col2                       \n0  A     10:10 \n1  A     20:05\n2  A     c\n3  A     00:10\n4  B     04:15\n2  B     c\n3  B     c\n4  B     13:40\n</code></pre>\n\n<p>And I am trying to get here:</p>\n\n<pre><code>  col1  col2                       \n0  A     10:10 \n1  A     20:05\n2  A     c\n3  A     00:10\n</code></pre>\n\n<p>Typically I do this for other very similar dataframes (and it works):</p>\n\n<pre><code>df = df.groupby('col1').filter(lambda x: x[\"col2\"].value_counts()[['c']].sum() &lt; 2)\n</code></pre>\n\n<p>But for this one is not working and I receive this error:</p>\n\n<pre><code>KeyError: \"None of [Index(['c'], dtype='object')] are in the [index]\"\n</code></pre>\n\n<p>Does someone have an idea on how I could do this?</p>\n\n<p>Thanks!</p>\n",
        "answer_body": "<p>I suggest use for improve performance <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a>:  </p>\n\n<pre><code>df = df[df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2)]\nprint (df)\n  col1   col2\n0    A  10:10\n1    A  20:05\n2    A      c\n3    A  00:10\n</code></pre>\n\n<p><strong>Details</strong>:</p>\n\n<p>First compare values  by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html\" rel=\"nofollow noreferrer\"><code>Series.eq</code></a> for <code>==</code>:</p>\n\n<pre><code>print (df['col2'].eq('c'))\n0    False\n1    False\n2     True\n3    False\n4    False\n2     True\n3     True\n4    False\nName: col2, dtype: bool\n</code></pre>\n\n<p>Then count <code>True</code> value per groups by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> with <code>sum</code>, <code>True</code>s are processing like <code>1</code>:</p>\n\n<pre><code>print (df['col2'].eq('c').groupby(df['col1']).transform('sum'))\n0    1.0\n1    1.0\n2    1.0\n3    1.0\n4    2.0\n2    2.0\n3    2.0\n4    2.0\nName: col2, dtype: float64\n</code></pre>\n\n<p>And last filter by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.lt.html\" rel=\"nofollow noreferrer\"><code>Series.lt</code></a> for less:</p>\n\n<pre><code>print (df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2))\n0     True\n1     True\n2     True\n3     True\n4    False\n2    False\n3    False\n4    False\nName: col2, dtype: bool\n</code></pre>\n",
        "question_body": "<p>I am trying to delete a grouped set of rows in pandas according to the following condition:</p>\n\n<p>If a group (grouped by col1) has more than 2 values 'c' in col2, then remove the whole group.</p>\n\n<p>What I have looks like this</p>\n\n<pre><code>  col1  col2                       \n0  A     10:10 \n1  A     20:05\n2  A     c\n3  A     00:10\n4  B     04:15\n2  B     c\n3  B     c\n4  B     13:40\n</code></pre>\n\n<p>And I am trying to get here:</p>\n\n<pre><code>  col1  col2                       \n0  A     10:10 \n1  A     20:05\n2  A     c\n3  A     00:10\n</code></pre>\n\n<p>Typically I do this for other very similar dataframes (and it works):</p>\n\n<pre><code>df = df.groupby('col1').filter(lambda x: x[\"col2\"].value_counts()[['c']].sum() &lt; 2)\n</code></pre>\n\n<p>But for this one is not working and I receive this error:</p>\n\n<pre><code>KeyError: \"None of [Index(['c'], dtype='object')] are in the [index]\"\n</code></pre>\n\n<p>Does someone have an idea on how I could do this?</p>\n\n<p>Thanks!</p>\n",
        "formatted_input": {
            "qid": 61170036,
            "link": "https://stackoverflow.com/questions/61170036/error-none-of-index-dtype-object-are-in-the-index",
            "question": {
                "title": "Error: None of [Index([&#39;...&#39;], dtype=&#39;object&#39;)] are in the [index]",
                "ques_desc": "I am trying to delete a grouped set of rows in pandas according to the following condition: If a group (grouped by col1) has more than 2 values 'c' in col2, then remove the whole group. What I have looks like this And I am trying to get here: Typically I do this for other very similar dataframes (and it works): But for this one is not working and I receive this error: Does someone have an idea on how I could do this? Thanks! "
            },
            "io": [
                "  col1  col2                       \n0  A     10:10 \n1  A     20:05\n2  A     c\n3  A     00:10\n4  B     04:15\n2  B     c\n3  B     c\n4  B     13:40\n",
                "  col1  col2                       \n0  A     10:10 \n1  A     20:05\n2  A     c\n3  A     00:10\n"
            ],
            "answer": {
                "ans_desc": "I suggest use for improve performance : Details: First compare values by for : Then count value per groups by with , s are processing like : And last filter by for less: ",
                "code": [
                    "print (df['col2'].eq('c'))\n0    False\n1    False\n2     True\n3    False\n4    False\n2     True\n3     True\n4    False\nName: col2, dtype: bool\n",
                    "print (df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2))\n0     True\n1     True\n2     True\n3     True\n4    False\n2    False\n3    False\n4    False\nName: col2, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3657,
            "user_id": 3358927,
            "user_type": "registered",
            "accept_rate": 79,
            "profile_image": "https://www.gravatar.com/avatar/a3e5d657f3fc4576c522193f79b338dc?s=128&d=identicon&r=PG&f=1",
            "display_name": "ddd",
            "link": "https://stackoverflow.com/users/3358927/ddd"
        },
        "is_answered": true,
        "view_count": 158,
        "accepted_answer_id": 61143873,
        "answer_count": 3,
        "score": -1,
        "last_activity_date": 1586543206,
        "creation_date": 1586533333,
        "last_edit_date": 1586541701,
        "question_id": 61143729,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61143729/pandas-how-to-do-value-counts-within-groups",
        "title": "Pandas: how to do value counts within groups",
        "body": "<p>I have the following dataframe. I want to group by <code>a</code> and <code>b</code> first. Within each group, I need to do a value count based on <code>c</code> and only pick the one with most counts. If there are more than one c values for one group with the most counts, just pick any one.</p>\n\n<pre><code>a    b    c\n1    1    x\n1    1    y\n1    1    y\n1    2    y\n1    2    y\n1    2    z\n2    1    z\n2    1    z\n2    1    a\n2    1    a\n</code></pre>\n\n<p>The expected result would be</p>\n\n<pre><code>a    b    c\n1    1    y\n1    2    y\n2    1    z\n</code></pre>\n\n<p>What is the right way to do it? It would be even better if I can print out each group with c's value counts sorted as an intermediate step.</p>\n",
        "answer_body": "<p>group the original dataframe by <code>['a', 'b']</code> and get the <code>.max()</code> should work</p>\n\n<pre><code>df.groupby(['a', 'b'])['c'].max()\n</code></pre>\n\n<p>you can also aggregate <code>'count'</code> and <code>'max'</code> values</p>\n\n<pre><code>df.groupby(['a', 'b'])['c'].agg({'max': max, 'count': 'count'}).reset_index()\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe. I want to group by <code>a</code> and <code>b</code> first. Within each group, I need to do a value count based on <code>c</code> and only pick the one with most counts. If there are more than one c values for one group with the most counts, just pick any one.</p>\n\n<pre><code>a    b    c\n1    1    x\n1    1    y\n1    1    y\n1    2    y\n1    2    y\n1    2    z\n2    1    z\n2    1    z\n2    1    a\n2    1    a\n</code></pre>\n\n<p>The expected result would be</p>\n\n<pre><code>a    b    c\n1    1    y\n1    2    y\n2    1    z\n</code></pre>\n\n<p>What is the right way to do it? It would be even better if I can print out each group with c's value counts sorted as an intermediate step.</p>\n",
        "formatted_input": {
            "qid": 61143729,
            "link": "https://stackoverflow.com/questions/61143729/pandas-how-to-do-value-counts-within-groups",
            "question": {
                "title": "Pandas: how to do value counts within groups",
                "ques_desc": "I have the following dataframe. I want to group by and first. Within each group, I need to do a value count based on and only pick the one with most counts. If there are more than one c values for one group with the most counts, just pick any one. The expected result would be What is the right way to do it? It would be even better if I can print out each group with c's value counts sorted as an intermediate step. "
            },
            "io": [
                "a    b    c\n1    1    x\n1    1    y\n1    1    y\n1    2    y\n1    2    y\n1    2    z\n2    1    z\n2    1    z\n2    1    a\n2    1    a\n",
                "a    b    c\n1    1    y\n1    2    y\n2    1    z\n"
            ],
            "answer": {
                "ans_desc": "group the original dataframe by and get the should work you can also aggregate and values ",
                "code": [
                    "df.groupby(['a', 'b'])['c'].agg({'max': max, 'count': 'count'}).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 5746264,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2378b64c8ffbb2307e230263849b89d3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Buch",
            "link": "https://stackoverflow.com/users/5746264/buch"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 61139400,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1586519250,
        "creation_date": 1586516214,
        "question_id": 61138851,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61138851/count-how-many-cells-are-between-the-last-value-in-the-dataframe-and-the-end-of",
        "title": "Count how many cells are between the last value in the dataframe and the end of the row",
        "body": "<p>I'm using the pandas library in Python.</p>\n\n<p>I have a data frame:</p>\n\n<pre><code>    0   1   2   3   4\n\n0   0   0   0   1   0\n\n1   0   0   0   0   1\n\n2   0   0   1   0   0\n\n3   1   0   0   0   0\n\n4   0   0   1   0   0\n\n5   0   1   0   0   0\n\n6   1   0   0   1   1\n</code></pre>\n\n<p>Is it possible to create a new column that is a count of the number of cells that are empty between the end of the row and the last value above zero? Example data frame below:</p>\n\n<pre><code>    0   1   2   3   4   Value\n\n0   0   0   0   1   0   1\n\n1   0   0   0   0   1   0\n\n2   0   0   1   0   0   2\n\n3   1   0   0   0   0   4\n\n4   0   0   1   0   0   2\n\n5   0   1   0   0   0   3\n\n6   1   0   0   1   1   0\n</code></pre>\n",
        "answer_body": "<p>using <code>argmax</code></p>\n\n<pre><code>df['value'] = df.apply(lambda x: (x.iloc[::-1] == 1).argmax(),1)\n\n##OR\n</code></pre>\n\n<p>using <code>np.where</code></p>\n\n<pre><code>df['Value'] = np.where(df.iloc[:,::-1] == 1,True,False).argmax(1)\n</code></pre>\n\n<hr>\n\n<pre><code>   0  1  2  3  4  Value\n0  0  0  0  1  0      1\n1  0  0  0  0  1      0\n2  0  0  1  0  0      2\n3  1  0  0  0  0      4\n4  0  0  1  0  0      2\n5  0  1  0  0  0      3\n6  1  0  0  1  1      0\n</code></pre>\n",
        "question_body": "<p>I'm using the pandas library in Python.</p>\n\n<p>I have a data frame:</p>\n\n<pre><code>    0   1   2   3   4\n\n0   0   0   0   1   0\n\n1   0   0   0   0   1\n\n2   0   0   1   0   0\n\n3   1   0   0   0   0\n\n4   0   0   1   0   0\n\n5   0   1   0   0   0\n\n6   1   0   0   1   1\n</code></pre>\n\n<p>Is it possible to create a new column that is a count of the number of cells that are empty between the end of the row and the last value above zero? Example data frame below:</p>\n\n<pre><code>    0   1   2   3   4   Value\n\n0   0   0   0   1   0   1\n\n1   0   0   0   0   1   0\n\n2   0   0   1   0   0   2\n\n3   1   0   0   0   0   4\n\n4   0   0   1   0   0   2\n\n5   0   1   0   0   0   3\n\n6   1   0   0   1   1   0\n</code></pre>\n",
        "formatted_input": {
            "qid": 61138851,
            "link": "https://stackoverflow.com/questions/61138851/count-how-many-cells-are-between-the-last-value-in-the-dataframe-and-the-end-of",
            "question": {
                "title": "Count how many cells are between the last value in the dataframe and the end of the row",
                "ques_desc": "I'm using the pandas library in Python. I have a data frame: Is it possible to create a new column that is a count of the number of cells that are empty between the end of the row and the last value above zero? Example data frame below: "
            },
            "io": [
                "    0   1   2   3   4\n\n0   0   0   0   1   0\n\n1   0   0   0   0   1\n\n2   0   0   1   0   0\n\n3   1   0   0   0   0\n\n4   0   0   1   0   0\n\n5   0   1   0   0   0\n\n6   1   0   0   1   1\n",
                "    0   1   2   3   4   Value\n\n0   0   0   0   1   0   1\n\n1   0   0   0   0   1   0\n\n2   0   0   1   0   0   2\n\n3   1   0   0   0   0   4\n\n4   0   0   1   0   0   2\n\n5   0   1   0   0   0   3\n\n6   1   0   0   1   1   0\n"
            ],
            "answer": {
                "ans_desc": "using using ",
                "code": [
                    "df['value'] = df.apply(lambda x: (x.iloc[::-1] == 1).argmax(),1)\n\n##OR\n",
                    "df['Value'] = np.where(df.iloc[:,::-1] == 1,True,False).argmax(1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 768,
            "user_id": 12977233,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uT6rh.jpg?s=128&g=1",
            "display_name": "Shabari nath k",
            "link": "https://stackoverflow.com/users/12977233/shabari-nath-k"
        },
        "is_answered": true,
        "view_count": 222,
        "accepted_answer_id": 61118025,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1586428896,
        "creation_date": 1586424493,
        "question_id": 61117957,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61117957/how-to-split-a-nested-dictionary-inside-a-column-of-a-dataframe-into-new-rows",
        "title": "how to split a nested dictionary inside a column of a dataframe into new rows?",
        "body": "<p>I have a dataframe :</p>\n\n<pre><code>Col1 Col2 Col3\n01   ABC  {'link':'http://smthing1}\n02   DEF  {'link':'http://smthing2}\n</code></pre>\n\n<p>I need to split col3 into new rows:\nexpected output dataframe :</p>\n\n<pre><code>Col1 Col2 Col3\n01   ABC  'http://smthing1'\n02   DEF  'http://smthing2'\n</code></pre>\n\n<p>This doesnt seem to work :</p>\n\n<pre><code>df= df.apply(pd.Series)\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.get.html\" rel=\"nofollow noreferrer\"><code>Series.str.get</code></a>, but first convert to dictionaries if necessary:</p>\n\n<pre><code>#converting to dicts\n#import ast\n#df['Col3'] = df['Col3'].apply(ast.literal_eval)\ndf['Col3'] = df['Col3'].str.get('link')\n</code></pre>\n",
        "question_body": "<p>I have a dataframe :</p>\n\n<pre><code>Col1 Col2 Col3\n01   ABC  {'link':'http://smthing1}\n02   DEF  {'link':'http://smthing2}\n</code></pre>\n\n<p>I need to split col3 into new rows:\nexpected output dataframe :</p>\n\n<pre><code>Col1 Col2 Col3\n01   ABC  'http://smthing1'\n02   DEF  'http://smthing2'\n</code></pre>\n\n<p>This doesnt seem to work :</p>\n\n<pre><code>df= df.apply(pd.Series)\n</code></pre>\n",
        "formatted_input": {
            "qid": 61117957,
            "link": "https://stackoverflow.com/questions/61117957/how-to-split-a-nested-dictionary-inside-a-column-of-a-dataframe-into-new-rows",
            "question": {
                "title": "how to split a nested dictionary inside a column of a dataframe into new rows?",
                "ques_desc": "I have a dataframe : I need to split col3 into new rows: expected output dataframe : This doesnt seem to work : "
            },
            "io": [
                "Col1 Col2 Col3\n01   ABC  {'link':'http://smthing1}\n02   DEF  {'link':'http://smthing2}\n",
                "Col1 Col2 Col3\n01   ABC  'http://smthing1'\n02   DEF  'http://smthing2'\n"
            ],
            "answer": {
                "ans_desc": "Use , but first convert to dictionaries if necessary: ",
                "code": [
                    "#converting to dicts\n#import ast\n#df['Col3'] = df['Col3'].apply(ast.literal_eval)\ndf['Col3'] = df['Col3'].str.get('link')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 43,
            "user_id": 13025106,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-BY_33EGIcNY/AAAAAAAAAAI/AAAAAAAAAAA/AKF05nCJZpVDLF-OBffb-7OqgFhTus6F1A/photo.jpg?sz=128",
            "display_name": "Dexter py",
            "link": "https://stackoverflow.com/users/13025106/dexter-py"
        },
        "is_answered": true,
        "view_count": 58,
        "accepted_answer_id": 61077645,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1586276438,
        "creation_date": 1586254229,
        "last_edit_date": 1586256168,
        "question_id": 61077572,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61077572/find-count-of-unique-value-of-each-column-and-save-in-csv",
        "title": "Find count of unique value of each column and save in CSV",
        "body": "<p>I have data like this:</p>\n\n<pre><code>+---+---+---+\n| A | B | C |\n+---+---+---+\n| 1 | 2 | 7 |\n| 2 | 2 | 7 |\n| 3 | 2 | 1 |\n| 3 | 2 | 1 |\n| 3 | 2 | 1 |\n+---+---+---+\n</code></pre>\n\n<p>Need to count unique value of each column and report it like below:</p>\n\n<pre><code>+---+---+---+\n| A | 3 | 3 |\n| A | 2 | 1 |\n| A | 1 | 1 |\n| B | 2 | 5 |\n| C | 1 | 3 |\n| C | 7 | 2 |\n+---+---+---+\n</code></pre>\n\n<p>I have no issue when number of column are limit and manually name them, when input file is big it become hard,need to have simple way to have output</p>\n\n<p>here is the code I have</p>\n\n<pre><code>import pandas as pd \ndf=pd.read_csv('1.csv')\n\nA=df['A']\nB=df['B']\nC=df['C']\n\ndf1=A.value_counts()\ndf2=B.value_counts()\ndf3=C.value_counts()\n\nall = {'A': df1,'B': df2,'C': df3}\nresult = pd.concat(all)\nresult.to_csv('out.csv')\n\n</code></pre>\n",
        "answer_body": "<p>You can loop over column and insert them in dictionary.\nyou can initiate dictionary by <code>all={}</code>. To be scalable you can read column by <code>colm=df.columns</code>. This would give you all column in your df.</p>\n\n<p>Try this code:</p>\n\n<pre><code>import pandas as pd \ndf=pd.read_csv('1.csv')\nall={}\ncolm=df.columns\nfor i in colm:\n    all.update({i:df[i].value_counts()})\n\nresult = pd.concat(all)\nresult.to_csv('out.csv')\n</code></pre>\n",
        "question_body": "<p>I have data like this:</p>\n\n<pre><code>+---+---+---+\n| A | B | C |\n+---+---+---+\n| 1 | 2 | 7 |\n| 2 | 2 | 7 |\n| 3 | 2 | 1 |\n| 3 | 2 | 1 |\n| 3 | 2 | 1 |\n+---+---+---+\n</code></pre>\n\n<p>Need to count unique value of each column and report it like below:</p>\n\n<pre><code>+---+---+---+\n| A | 3 | 3 |\n| A | 2 | 1 |\n| A | 1 | 1 |\n| B | 2 | 5 |\n| C | 1 | 3 |\n| C | 7 | 2 |\n+---+---+---+\n</code></pre>\n\n<p>I have no issue when number of column are limit and manually name them, when input file is big it become hard,need to have simple way to have output</p>\n\n<p>here is the code I have</p>\n\n<pre><code>import pandas as pd \ndf=pd.read_csv('1.csv')\n\nA=df['A']\nB=df['B']\nC=df['C']\n\ndf1=A.value_counts()\ndf2=B.value_counts()\ndf3=C.value_counts()\n\nall = {'A': df1,'B': df2,'C': df3}\nresult = pd.concat(all)\nresult.to_csv('out.csv')\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 61077572,
            "link": "https://stackoverflow.com/questions/61077572/find-count-of-unique-value-of-each-column-and-save-in-csv",
            "question": {
                "title": "Find count of unique value of each column and save in CSV",
                "ques_desc": "I have data like this: Need to count unique value of each column and report it like below: I have no issue when number of column are limit and manually name them, when input file is big it become hard,need to have simple way to have output here is the code I have "
            },
            "io": [
                "+---+---+---+\n| A | B | C |\n+---+---+---+\n| 1 | 2 | 7 |\n| 2 | 2 | 7 |\n| 3 | 2 | 1 |\n| 3 | 2 | 1 |\n| 3 | 2 | 1 |\n+---+---+---+\n",
                "+---+---+---+\n| A | 3 | 3 |\n| A | 2 | 1 |\n| A | 1 | 1 |\n| B | 2 | 5 |\n| C | 1 | 3 |\n| C | 7 | 2 |\n+---+---+---+\n"
            ],
            "answer": {
                "ans_desc": "You can loop over column and insert them in dictionary. you can initiate dictionary by . To be scalable you can read column by . This would give you all column in your df. Try this code: ",
                "code": [
                    "import pandas as pd \ndf=pd.read_csv('1.csv')\nall={}\ncolm=df.columns\nfor i in colm:\n    all.update({i:df[i].value_counts()})\n\nresult = pd.concat(all)\nresult.to_csv('out.csv')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 663,
            "user_id": 11341120,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/449afe6ae74cba7b8244f85aec2d953f?s=128&d=identicon&r=PG&f=1",
            "display_name": "ML85",
            "link": "https://stackoverflow.com/users/11341120/ml85"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 61075127,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1586245934,
        "creation_date": 1586245270,
        "last_edit_date": 1586245598,
        "question_id": 61075047,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61075047/pandas-data-change-based-on-condition",
        "title": "pandas data change based on condition",
        "body": "<p>I have data which has special characters, I want to change the conditional cell values. </p>\n\n<p>Data is below first few lines\ndf_orig:</p>\n\n<pre><code>idx A   B   C   D\n0   0.5 2   5   #\n1   3   5   8   %\n2   6   8   10  $\n3   9   10  15  $\n4   11  15  18  #\n</code></pre>\n\n<p>I want to change cell values  <strong>where $ in D, A = 0 and B = C</strong></p>\n\n<p>THE OUTPUT SHOULD BE \nchange:</p>\n\n<pre><code>idx   A   B   C   D\n0     0.5 2   5   #\n1     3   5   8   %\n2     0   10  10  $\n3     0   15  15  $\n4     11  15  18  #\n</code></pre>\n\n<p>I tried at my end with </p>\n\n<pre><code>change = df_orig.loc[(df.orig['D'] == '$'), df_orig['A'] == '0'&amp; df_orig['B'] = df_orig['c']\n</code></pre>\n\n<p>but it didn't work. </p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html\" rel=\"nofollow noreferrer\"><code>DataFrame.copy</code></a> if need new <code>DataFrame</code> and then set new values separately:</p>\n\n<pre><code>df = df_orig.copy()\nm = df['D'].eq('$')\n#alternative\n#m = df['D'] == '$'\n\ndf.loc[m, 'A'] = 0\ndf.loc[m, 'B'] = df.C\nprint (df)\n        A   B   C  D\nidx                 \n0     0.5   2   5  #\n1     3.0   5   8  %\n2     0.0  10  10  $\n3     0.0  15  15  $\n4    11.0  15  18  #\n</code></pre>\n\n<p>Also is possible together:</p>\n\n<pre><code>m = df['D'].eq('$')\ndf.loc[m, ['A','B']] = df.assign(E=0).loc[m, ['E','C']].values\n</code></pre>\n",
        "question_body": "<p>I have data which has special characters, I want to change the conditional cell values. </p>\n\n<p>Data is below first few lines\ndf_orig:</p>\n\n<pre><code>idx A   B   C   D\n0   0.5 2   5   #\n1   3   5   8   %\n2   6   8   10  $\n3   9   10  15  $\n4   11  15  18  #\n</code></pre>\n\n<p>I want to change cell values  <strong>where $ in D, A = 0 and B = C</strong></p>\n\n<p>THE OUTPUT SHOULD BE \nchange:</p>\n\n<pre><code>idx   A   B   C   D\n0     0.5 2   5   #\n1     3   5   8   %\n2     0   10  10  $\n3     0   15  15  $\n4     11  15  18  #\n</code></pre>\n\n<p>I tried at my end with </p>\n\n<pre><code>change = df_orig.loc[(df.orig['D'] == '$'), df_orig['A'] == '0'&amp; df_orig['B'] = df_orig['c']\n</code></pre>\n\n<p>but it didn't work. </p>\n",
        "formatted_input": {
            "qid": 61075047,
            "link": "https://stackoverflow.com/questions/61075047/pandas-data-change-based-on-condition",
            "question": {
                "title": "pandas data change based on condition",
                "ques_desc": "I have data which has special characters, I want to change the conditional cell values. Data is below first few lines df_orig: I want to change cell values where $ in D, A = 0 and B = C THE OUTPUT SHOULD BE change: I tried at my end with but it didn't work. "
            },
            "io": [
                "idx A   B   C   D\n0   0.5 2   5   #\n1   3   5   8   %\n2   6   8   10  $\n3   9   10  15  $\n4   11  15  18  #\n",
                "idx   A   B   C   D\n0     0.5 2   5   #\n1     3   5   8   %\n2     0   10  10  $\n3     0   15  15  $\n4     11  15  18  #\n"
            ],
            "answer": {
                "ans_desc": "Use if need new and then set new values separately: Also is possible together: ",
                "code": [
                    "m = df['D'].eq('$')\ndf.loc[m, ['A','B']] = df.assign(E=0).loc[m, ['E','C']].values\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 404,
            "user_id": 11122122,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/xjwRt.png?s=128&g=1",
            "display_name": "Yanqi Huang",
            "link": "https://stackoverflow.com/users/11122122/yanqi-huang"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 61024897,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1585984481,
        "creation_date": 1585981569,
        "question_id": 61024520,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61024520/pandas-assignment-and-copy",
        "title": "Pandas assignment and copy",
        "body": "<p>If we run the following code, </p>\n\n<pre><code>def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n</code></pre>\n\n<p>We get</p>\n\n<pre><code>           0\n0   1.298967\n1  -0.887922\n2   1.913559\n3  -0.082032\n4  -0.466594\n..       ...\n95 -0.845137\n96  0.628542\n97 -0.588897\n98  0.464374\n99  0.267946\n</code></pre>\n\n<p>Whereas, if we run the following, </p>\n\n<pre><code>def f(df):\n    df = df\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n</code></pre>\n\n<p>We get</p>\n\n<pre><code>           0  a\n0  -0.510875  1\n1   0.401580  1\n2  -0.037484  1\n3  -0.935115  1\n4  -1.108471  1\n..       ... ..\n95  0.362075  1\n96 -1.017991  1\n97  1.881081  1\n98  0.376828  1\n99  0.771661  1\n</code></pre>\n\n<p>I know there's a concept of pass by object reference in python. Why don't the <code>df</code> in the second code gets copied? \nThanks</p>\n",
        "answer_body": "<p>For the first function:</p>\n\n<pre><code>def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) #doesnot return the changed columns\n</code></pre>\n\n<p>You are changing the input to a <code>.copy()</code> version of the input , as <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\" rel=\"nofollow noreferrer\"><code>assign</code></a> returns a copy of the actual dataframe , from the docs:</p>\n\n<blockquote>\n  <p>Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten.</p>\n</blockquote>\n\n<p>To return the changes you make in the copy , you should return the copy:</p>\n\n<pre><code>def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n    return df\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df))\n</code></pre>\n\n<p>On the contrary , for your second function , you are assigning the column a on the input parameter in place, hence when you print the dataframe , you can see the changes in the original df.</p>\n\n<pre><code>def f(df):\n    df = df\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n</code></pre>\n\n<p>To achieve a similar behaviour to the first function try assigning <code>df = df.copy()</code></p>\n\n<pre><code>def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) # doesnot return the a column\n</code></pre>\n\n<p>Same as we did for function 1 , you should return the copy :</p>\n\n<pre><code>def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n    return df\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df)) #returns the column a\n</code></pre>\n\n<p>Hope this answers your question.</p>\n",
        "question_body": "<p>If we run the following code, </p>\n\n<pre><code>def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n</code></pre>\n\n<p>We get</p>\n\n<pre><code>           0\n0   1.298967\n1  -0.887922\n2   1.913559\n3  -0.082032\n4  -0.466594\n..       ...\n95 -0.845137\n96  0.628542\n97 -0.588897\n98  0.464374\n99  0.267946\n</code></pre>\n\n<p>Whereas, if we run the following, </p>\n\n<pre><code>def f(df):\n    df = df\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n</code></pre>\n\n<p>We get</p>\n\n<pre><code>           0  a\n0  -0.510875  1\n1   0.401580  1\n2  -0.037484  1\n3  -0.935115  1\n4  -1.108471  1\n..       ... ..\n95  0.362075  1\n96 -1.017991  1\n97  1.881081  1\n98  0.376828  1\n99  0.771661  1\n</code></pre>\n\n<p>I know there's a concept of pass by object reference in python. Why don't the <code>df</code> in the second code gets copied? \nThanks</p>\n",
        "formatted_input": {
            "qid": 61024520,
            "link": "https://stackoverflow.com/questions/61024520/pandas-assignment-and-copy",
            "question": {
                "title": "Pandas assignment and copy",
                "ques_desc": "If we run the following code, We get Whereas, if we run the following, We get I know there's a concept of pass by object reference in python. Why don't the in the second code gets copied? Thanks "
            },
            "io": [
                "           0\n0   1.298967\n1  -0.887922\n2   1.913559\n3  -0.082032\n4  -0.466594\n..       ...\n95 -0.845137\n96  0.628542\n97 -0.588897\n98  0.464374\n99  0.267946\n",
                "           0  a\n0  -0.510875  1\n1   0.401580  1\n2  -0.037484  1\n3  -0.935115  1\n4  -1.108471  1\n..       ... ..\n95  0.362075  1\n96 -1.017991  1\n97  1.881081  1\n98  0.376828  1\n99  0.771661  1\n"
            ],
            "answer": {
                "ans_desc": "For the first function: You are changing the input to a version of the input , as returns a copy of the actual dataframe , from the docs: Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. To return the changes you make in the copy , you should return the copy: On the contrary , for your second function , you are assigning the column a on the input parameter in place, hence when you print the dataframe , you can see the changes in the original df. To achieve a similar behaviour to the first function try assigning Same as we did for function 1 , you should return the copy : Hope this answers your question. ",
                "code": [
                    "def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) #doesnot return the changed columns\n",
                    "def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n    return df\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df))\n",
                    "def f(df):\n    df = df\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n",
                    "def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) # doesnot return the a column\n",
                    "def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n    return df\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df)) #returns the column a\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 188,
            "user_id": 13200216,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/26cb2ead253435f5996422e25207dfb8?s=128&d=identicon&r=PG&f=1",
            "display_name": "omdurg",
            "link": "https://stackoverflow.com/users/13200216/omdurg"
        },
        "is_answered": true,
        "view_count": 93,
        "accepted_answer_id": 61016946,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1585935109,
        "creation_date": 1585932660,
        "last_edit_date": 1585933741,
        "question_id": 61016701,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61016701/issue-in-applying-str-contains-across-multiple-columns-in-python",
        "title": "Issue in applying str.contains across multiple columns in Python",
        "body": "<p>Dataframe:</p>\n\n<pre><code>col1          col2             col3\n132jh.2ad3    34.2             65\n298.487       9879.87          1kjh8kjn0\n98.47         79.8             90\n8763.3        7hkj7kjb.k23l    67\n69.3          3765.9           3510\n</code></pre>\n\n<p>Desired output:</p>\n\n<pre><code>col1          col2             col3\n98.47         79.8             90\n69.3          3765.9           3510\n</code></pre>\n\n<p>What I have tried: (this doesn't delete all rows with alpha-numeric vales)</p>\n\n<pre><code>df=df[~df['col1'].astype(str).str.contains(r'[A-Ba-b]')] #for col1\ndf=df[~df['col2'].astype(str).str.contains(r'[A-Ba-b]')] #for col2\ndf=df[~df['col3'].astype(str).str.contains(r'[A-Ba-b]')] #for col3\n</code></pre>\n\n<p>I want to delete all alphanumeric rows, and have only the rows containing numbers alone. Col1 and Col2 has decimal points, but Col3 has only whole numbers.<br>\nI have tried few other similar threads, but it didn't work.</p>\n\n<p>Thanks for the help!!</p>\n",
        "answer_body": "<p>Run:</p>\n\n<pre><code>df[~df.apply(lambda row: row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n</code></pre>\n\n<p>(<em>import re</em> required).</p>\n\n<p>Your regex contained <em>[A-B]</em>, but it should match <strong>all</strong> letters\n(from <em>A</em> to <em>Z</em>).</p>\n\n<h1>Edit</h1>\n\n<p>If you have also <strong>other</strong> columns, but you want to limit your criterion\nto just your 3 <strong>indicated</strong> columns, assuming that they are consecutive columns, run:</p>\n\n<pre><code>df[~df.loc[:, 'col1':'col3'].apply(lambda row:\n    row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n</code></pre>\n\n<p>This way you apply the same function as above to just these 3 columns.</p>\n",
        "question_body": "<p>Dataframe:</p>\n\n<pre><code>col1          col2             col3\n132jh.2ad3    34.2             65\n298.487       9879.87          1kjh8kjn0\n98.47         79.8             90\n8763.3        7hkj7kjb.k23l    67\n69.3          3765.9           3510\n</code></pre>\n\n<p>Desired output:</p>\n\n<pre><code>col1          col2             col3\n98.47         79.8             90\n69.3          3765.9           3510\n</code></pre>\n\n<p>What I have tried: (this doesn't delete all rows with alpha-numeric vales)</p>\n\n<pre><code>df=df[~df['col1'].astype(str).str.contains(r'[A-Ba-b]')] #for col1\ndf=df[~df['col2'].astype(str).str.contains(r'[A-Ba-b]')] #for col2\ndf=df[~df['col3'].astype(str).str.contains(r'[A-Ba-b]')] #for col3\n</code></pre>\n\n<p>I want to delete all alphanumeric rows, and have only the rows containing numbers alone. Col1 and Col2 has decimal points, but Col3 has only whole numbers.<br>\nI have tried few other similar threads, but it didn't work.</p>\n\n<p>Thanks for the help!!</p>\n",
        "formatted_input": {
            "qid": 61016701,
            "link": "https://stackoverflow.com/questions/61016701/issue-in-applying-str-contains-across-multiple-columns-in-python",
            "question": {
                "title": "Issue in applying str.contains across multiple columns in Python",
                "ques_desc": "Dataframe: Desired output: What I have tried: (this doesn't delete all rows with alpha-numeric vales) I want to delete all alphanumeric rows, and have only the rows containing numbers alone. Col1 and Col2 has decimal points, but Col3 has only whole numbers. I have tried few other similar threads, but it didn't work. Thanks for the help!! "
            },
            "io": [
                "col1          col2             col3\n132jh.2ad3    34.2             65\n298.487       9879.87          1kjh8kjn0\n98.47         79.8             90\n8763.3        7hkj7kjb.k23l    67\n69.3          3765.9           3510\n",
                "col1          col2             col3\n98.47         79.8             90\n69.3          3765.9           3510\n"
            ],
            "answer": {
                "ans_desc": "Run: (import re required). Your regex contained [A-B], but it should match all letters (from A to Z). Edit If you have also other columns, but you want to limit your criterion to just your 3 indicated columns, assuming that they are consecutive columns, run: This way you apply the same function as above to just these 3 columns. ",
                "code": [
                    "df[~df.apply(lambda row: row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n",
                    "df[~df.loc[:, 'col1':'col3'].apply(lambda row:\n    row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nested"
        ],
        "owner": {
            "reputation": 768,
            "user_id": 12977233,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uT6rh.jpg?s=128&g=1",
            "display_name": "Shabari nath k",
            "link": "https://stackoverflow.com/users/12977233/shabari-nath-k"
        },
        "is_answered": true,
        "view_count": 338,
        "accepted_answer_id": 60863263,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1585930199,
        "creation_date": 1585208775,
        "question_id": 60862810,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60862810/how-to-break-pop-a-nested-dictionary-inside-a-list-inside-a-pandas-dataframe",
        "title": "How to break/pop a nested Dictionary inside a list, inside a pandas dataframe?",
        "body": "<p>I have a dataframe which has a dictionary inside a nested list and i want to split the column 'C' :</p>\n\n<pre><code>A B     C     \n1 a    [ {\"id\":2,\"Col\":{\"x\":3,\"y\":4}}]\n2 b    [ {\"id\":5,\"Col\":{\"x\":6,\"y\":7}}]\n</code></pre>\n\n<p>expected output :</p>\n\n<pre><code>A B C_id Col_x Col_y\n1 a  2    3     4 \n2 b  5    6     7\n</code></pre>\n",
        "answer_body": "<p>From the comments, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\" rel=\"nofollow noreferrer\"><code>json_normalize</code></a> might help you.</p>\n\n<p>After extracting <code>id</code> and <code>col</code> columns with:</p>\n\n<pre><code>df[[\"Col\", \"id\"]] = df[\"C\"].apply(lambda x: pd.Series(x[0]))\n</code></pre>\n\n<p>You can explode the dictionary in <code>Col</code> with <code>json_normalize</code> and use concat to merge with existing dataframe:</p>\n\n<pre><code>df = pd.concat([df, json_normalize(df.Col)], axis=1)\n</code></pre>\n\n<p>Also, use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\" rel=\"nofollow noreferrer\">drop</a> to remove old columns.</p>\n\n<p><strong>Full code</strong>:</p>\n\n<pre><code># Import modules\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n# from flatten_json import flatten\n\n# Create dataframe\ndf = pd.DataFrame([[1, \"a\", [ {\"id\":2,\"Col\":{\"x\":3,\"y\":4}}]],\n                   [2, \"b\", [ {\"id\":5,\"Col\":{\"x\":6,\"y\":7}}]]],\n                   columns=[\"A\", \"B\", \"C\"])\n\n# Add col and id column + remove old \"C\" column\ndf = pd.concat([df, df[\"C\"].apply(lambda x: pd.Series(x[0]))], axis=1) \\\n        .drop(\"C\", axis=1)\nprint(df)\n#    A  B               Col  id\n# 0  1  a  {'x': 3, 'y': 4}   2\n# 1  2  b  {'x': 6, 'y': 7}   5\n\n# Show json_normalize behavior\nprint(json_normalize(df.Col))\n#    x  y\n# 0  3  4\n# 1  6  7\n\n# Explode dict in \"col\" column + remove \"Col\" colun\ndf = pd.concat([df, json_normalize(df.Col)], axis=1) \\\n        .drop([\"Col\"], axis=1)\nprint(df)\n#    A  B  id  x  y\n# 0  1  a   2  3  4\n# 1  2  b   5  6  7\n</code></pre>\n",
        "question_body": "<p>I have a dataframe which has a dictionary inside a nested list and i want to split the column 'C' :</p>\n\n<pre><code>A B     C     \n1 a    [ {\"id\":2,\"Col\":{\"x\":3,\"y\":4}}]\n2 b    [ {\"id\":5,\"Col\":{\"x\":6,\"y\":7}}]\n</code></pre>\n\n<p>expected output :</p>\n\n<pre><code>A B C_id Col_x Col_y\n1 a  2    3     4 \n2 b  5    6     7\n</code></pre>\n",
        "formatted_input": {
            "qid": 60862810,
            "link": "https://stackoverflow.com/questions/60862810/how-to-break-pop-a-nested-dictionary-inside-a-list-inside-a-pandas-dataframe",
            "question": {
                "title": "How to break/pop a nested Dictionary inside a list, inside a pandas dataframe?",
                "ques_desc": "I have a dataframe which has a dictionary inside a nested list and i want to split the column 'C' : expected output : "
            },
            "io": [
                "A B     C     \n1 a    [ {\"id\":2,\"Col\":{\"x\":3,\"y\":4}}]\n2 b    [ {\"id\":5,\"Col\":{\"x\":6,\"y\":7}}]\n",
                "A B C_id Col_x Col_y\n1 a  2    3     4 \n2 b  5    6     7\n"
            ],
            "answer": {
                "ans_desc": "From the comments, might help you. After extracting and columns with: You can explode the dictionary in with and use concat to merge with existing dataframe: Also, use drop to remove old columns. Full code: ",
                "code": [
                    "df[[\"Col\", \"id\"]] = df[\"C\"].apply(lambda x: pd.Series(x[0]))\n",
                    "df = pd.concat([df, json_normalize(df.Col)], axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "time-series",
            "nan"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 13210639,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cec394468fc46a4e9d7d058f191676ad?s=128&d=identicon&r=PG&f=1",
            "display_name": "SSSO",
            "link": "https://stackoverflow.com/users/13210639/ssso"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 61012138,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1585919895,
        "creation_date": 1585916907,
        "last_edit_date": 1585919895,
        "question_id": 61011933,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61011933/how-to-freeze-first-numbers-in-sequences-between-nans-in-python-pandas-dataframe",
        "title": "How to freeze first numbers in sequences between NaNs in Python pandas dataframe",
        "body": "<p>Is there a Pythonic way to, in a timeseries dataframe, by column, go down and pick the first number in a sequence, and then push it forward until the next NaN, and then take the next non-NaN number and push that one down until the next NaN, and so on (retaining the indices and NaNs).</p>\n\n<p>For example, I would like to convert this dataframe:</p>\n\n<pre><code>DF = pd.DataFrame(data={'A':[np.nan,1,3,5,7,np.nan,2,4,6,np.nan], 'B':[8,6,4,np.nan,np.nan,9,7,3,np.nan,3], 'C':[np.nan,np.nan,4,2,6,np.nan,1,5,2,8]})\n</code></pre>\n\n<pre><code>     A    B    C\n0  NaN  8.0  NaN\n1  1.0  6.0  NaN\n2  3.0  4.0  4.0\n3  5.0  NaN  2.0\n4  7.0  NaN  6.0\n5  NaN  9.0  NaN\n6  2.0  7.0  1.0\n7  4.0  3.0  5.0\n8  6.0  NaN  2.0\n9  NaN  3.0  8.0\n</code></pre>\n\n<p>To this dataframe:</p>\n\n<pre><code>Result = pd.DataFrame(data={'A':[np.nan,1,1,1,1,np.nan,2,2,2,np.nan], 'B':[8,8,8,np.nan,np.nan,9,9,9,np.nan,3], 'C':[np.nan,np.nan,4,4,4,np.nan,1,1,1,1]})\n</code></pre>\n\n<pre><code>     A    B    C\n0  NaN  8.0  NaN\n1  1.0  8.0  NaN\n2  1.0  8.0  4.0\n3  1.0  NaN  4.0\n4  1.0  NaN  4.0\n5  NaN  9.0  NaN\n6  2.0  9.0  1.0\n7  2.0  9.0  1.0\n8  2.0  NaN  1.0\n9  NaN  3.0  1.0\n</code></pre>\n\n<p>I know I can use a loop to iterate down the columns to do this, but would appreciate some help on how to do it in a more efficient Pythonic way on a very large dataframe. Thank you.</p>\n",
        "answer_body": "<p>IIUC:</p>\n\n<pre><code># where DF is not NaN\nmask = DF.notna()\nResult = (DF.shift(-1)           # fill the original NaN's with their next value\n            .mask(mask)          # replace all the original non-NaN with NaN\n            .ffill()             # forward fill \n            .fillna(DF.iloc[0])  # starting of the the columns with a non-NaN\n            .where(mask)         # replace the original NaN's back\n         )\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>     A    B    C\n0  NaN  8.0  NaN\n1  1.0  8.0  NaN\n2  1.0  8.0  4.0\n3  1.0  NaN  4.0\n4  1.0  NaN  4.0\n5  NaN  9.0  NaN\n6  2.0  9.0  1.0\n7  2.0  9.0  1.0\n8  2.0  NaN  1.0\n9  NaN  3.0  1.0\n</code></pre>\n",
        "question_body": "<p>Is there a Pythonic way to, in a timeseries dataframe, by column, go down and pick the first number in a sequence, and then push it forward until the next NaN, and then take the next non-NaN number and push that one down until the next NaN, and so on (retaining the indices and NaNs).</p>\n\n<p>For example, I would like to convert this dataframe:</p>\n\n<pre><code>DF = pd.DataFrame(data={'A':[np.nan,1,3,5,7,np.nan,2,4,6,np.nan], 'B':[8,6,4,np.nan,np.nan,9,7,3,np.nan,3], 'C':[np.nan,np.nan,4,2,6,np.nan,1,5,2,8]})\n</code></pre>\n\n<pre><code>     A    B    C\n0  NaN  8.0  NaN\n1  1.0  6.0  NaN\n2  3.0  4.0  4.0\n3  5.0  NaN  2.0\n4  7.0  NaN  6.0\n5  NaN  9.0  NaN\n6  2.0  7.0  1.0\n7  4.0  3.0  5.0\n8  6.0  NaN  2.0\n9  NaN  3.0  8.0\n</code></pre>\n\n<p>To this dataframe:</p>\n\n<pre><code>Result = pd.DataFrame(data={'A':[np.nan,1,1,1,1,np.nan,2,2,2,np.nan], 'B':[8,8,8,np.nan,np.nan,9,9,9,np.nan,3], 'C':[np.nan,np.nan,4,4,4,np.nan,1,1,1,1]})\n</code></pre>\n\n<pre><code>     A    B    C\n0  NaN  8.0  NaN\n1  1.0  8.0  NaN\n2  1.0  8.0  4.0\n3  1.0  NaN  4.0\n4  1.0  NaN  4.0\n5  NaN  9.0  NaN\n6  2.0  9.0  1.0\n7  2.0  9.0  1.0\n8  2.0  NaN  1.0\n9  NaN  3.0  1.0\n</code></pre>\n\n<p>I know I can use a loop to iterate down the columns to do this, but would appreciate some help on how to do it in a more efficient Pythonic way on a very large dataframe. Thank you.</p>\n",
        "formatted_input": {
            "qid": 61011933,
            "link": "https://stackoverflow.com/questions/61011933/how-to-freeze-first-numbers-in-sequences-between-nans-in-python-pandas-dataframe",
            "question": {
                "title": "How to freeze first numbers in sequences between NaNs in Python pandas dataframe",
                "ques_desc": "Is there a Pythonic way to, in a timeseries dataframe, by column, go down and pick the first number in a sequence, and then push it forward until the next NaN, and then take the next non-NaN number and push that one down until the next NaN, and so on (retaining the indices and NaNs). For example, I would like to convert this dataframe: To this dataframe: I know I can use a loop to iterate down the columns to do this, but would appreciate some help on how to do it in a more efficient Pythonic way on a very large dataframe. Thank you. "
            },
            "io": [
                "     A    B    C\n0  NaN  8.0  NaN\n1  1.0  6.0  NaN\n2  3.0  4.0  4.0\n3  5.0  NaN  2.0\n4  7.0  NaN  6.0\n5  NaN  9.0  NaN\n6  2.0  7.0  1.0\n7  4.0  3.0  5.0\n8  6.0  NaN  2.0\n9  NaN  3.0  8.0\n",
                "     A    B    C\n0  NaN  8.0  NaN\n1  1.0  8.0  NaN\n2  1.0  8.0  4.0\n3  1.0  NaN  4.0\n4  1.0  NaN  4.0\n5  NaN  9.0  NaN\n6  2.0  9.0  1.0\n7  2.0  9.0  1.0\n8  2.0  NaN  1.0\n9  NaN  3.0  1.0\n"
            ],
            "answer": {
                "ans_desc": "IIUC: Output: ",
                "code": [
                    "# where DF is not NaN\nmask = DF.notna()\nResult = (DF.shift(-1)           # fill the original NaN's with their next value\n            .mask(mask)          # replace all the original non-NaN with NaN\n            .ffill()             # forward fill \n            .fillna(DF.iloc[0])  # starting of the the columns with a non-NaN\n            .where(mask)         # replace the original NaN's back\n         )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 61,
            "user_id": 12973938,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-lb6VH5VU2BI/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdSq96aRcZ9OJy4CTpvEYcX2_ai5w/mo/photo.jpg?sz=128",
            "display_name": "chero",
            "link": "https://stackoverflow.com/users/12973938/chero"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 60994062,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1585843472,
        "creation_date": 1585837205,
        "last_edit_date": 1585843472,
        "question_id": 60993781,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60993781/how-do-i-check-that-the-unique-values-of-a-column-exists-in-another-column-in-da",
        "title": "How do i check that the unique values of a column exists in another column in dataframe?",
        "body": "<p>I have a dataframe like this : </p>\n\n<pre><code>\nA= [ ID COL1 COL2  \n     23  AA   BB    \n     23  AA   AA   \n     23  AA   DD   \n     23  BB   BB \n     23  BB   AA\n     23  BB   DD\n     23  CC   BB\n     23  CC   AA\n     24  AA   BB  ]\n\n\n</code></pre>\n\n<p>What i want to is to check that the unique value of col1 exist in Col2 <strong>for the same ID</strong> ,The ID is not always the same number. the check must be done only among rows with the same id\ni want a result like :</p>\n\n<pre><code>\nA= [ ID COL1 COL2  check \n     23  AA   BB    OK \n     23  AA   AA    OK \n     23  AA   DD    OK\n     23  BB   BB    OK \n     23  BB   AA    OK \n     23  BB   DD    OK \n     23  CC   BB    KO\n     23  CC   AA    KO \n     24  AA   BB    KO \n]\n</code></pre>\n\n<p>i tried</p>\n\n<pre><code> A['check'] = np.where(A.Col1.eq(A['Col2']).groupby(A['ID']).transform('any'), 'Anomalie', 'Valeur OK')\n</code></pre>\n\n<p>I'm not sur it s the right command ,can anyone help please  ? </p>\n",
        "answer_body": "<p>You just want to check whether a cell value exists in a container: <code>isin</code> is the way to go. But as you want to process id by ID, you also need a groupby:</p>\n\n<pre><code>df['check'] = df.groupby(['ID', 'COL1'], group_keys=False\n                         ).apply(lambda x: x['COL1'].isin(x['COL2']))\n</code></pre>\n\n<p>It gives as expected:</p>\n\n<pre><code>   ID COL1 COL2  check\n0  23   AA   BB   True\n1  23   AA   AA   True\n2  23   AA   DD   True\n3  23   BB   BB   True\n4  23   BB   AA   True\n5  23   BB   DD   True\n6  23   CC   BB  False\n7  23   CC   AA  False\n8  24   AA   BB  False\n</code></pre>\n\n<p>If you want OK/KO strings instead of boolean values, just add:</p>\n\n<pre><code>df['check'] = np.where(df['check'], 'OK', 'KO')\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this : </p>\n\n<pre><code>\nA= [ ID COL1 COL2  \n     23  AA   BB    \n     23  AA   AA   \n     23  AA   DD   \n     23  BB   BB \n     23  BB   AA\n     23  BB   DD\n     23  CC   BB\n     23  CC   AA\n     24  AA   BB  ]\n\n\n</code></pre>\n\n<p>What i want to is to check that the unique value of col1 exist in Col2 <strong>for the same ID</strong> ,The ID is not always the same number. the check must be done only among rows with the same id\ni want a result like :</p>\n\n<pre><code>\nA= [ ID COL1 COL2  check \n     23  AA   BB    OK \n     23  AA   AA    OK \n     23  AA   DD    OK\n     23  BB   BB    OK \n     23  BB   AA    OK \n     23  BB   DD    OK \n     23  CC   BB    KO\n     23  CC   AA    KO \n     24  AA   BB    KO \n]\n</code></pre>\n\n<p>i tried</p>\n\n<pre><code> A['check'] = np.where(A.Col1.eq(A['Col2']).groupby(A['ID']).transform('any'), 'Anomalie', 'Valeur OK')\n</code></pre>\n\n<p>I'm not sur it s the right command ,can anyone help please  ? </p>\n",
        "formatted_input": {
            "qid": 60993781,
            "link": "https://stackoverflow.com/questions/60993781/how-do-i-check-that-the-unique-values-of-a-column-exists-in-another-column-in-da",
            "question": {
                "title": "How do i check that the unique values of a column exists in another column in dataframe?",
                "ques_desc": "I have a dataframe like this : What i want to is to check that the unique value of col1 exist in Col2 for the same ID ,The ID is not always the same number. the check must be done only among rows with the same id i want a result like : i tried I'm not sur it s the right command ,can anyone help please ? "
            },
            "io": [
                "\nA= [ ID COL1 COL2  \n     23  AA   BB    \n     23  AA   AA   \n     23  AA   DD   \n     23  BB   BB \n     23  BB   AA\n     23  BB   DD\n     23  CC   BB\n     23  CC   AA\n     24  AA   BB  ]\n\n\n",
                "\nA= [ ID COL1 COL2  check \n     23  AA   BB    OK \n     23  AA   AA    OK \n     23  AA   DD    OK\n     23  BB   BB    OK \n     23  BB   AA    OK \n     23  BB   DD    OK \n     23  CC   BB    KO\n     23  CC   AA    KO \n     24  AA   BB    KO \n]\n"
            ],
            "answer": {
                "ans_desc": "You just want to check whether a cell value exists in a container: is the way to go. But as you want to process id by ID, you also need a groupby: It gives as expected: If you want OK/KO strings instead of boolean values, just add: ",
                "code": [
                    "df['check'] = df.groupby(['ID', 'COL1'], group_keys=False\n                         ).apply(lambda x: x['COL1'].isin(x['COL2']))\n",
                    "df['check'] = np.where(df['check'], 'OK', 'KO')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 232,
            "user_id": 12214867,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/be5d0301ab5cfffc38d71613a0d77703?s=128&d=identicon&r=PG&f=1",
            "display_name": "JJJohn",
            "link": "https://stackoverflow.com/users/12214867/jjjohn"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 60994258,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1585839014,
        "creation_date": 1585838175,
        "question_id": 60994122,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60994122/choosing-rows-by-values-in-dataframe",
        "title": "choosing rows by values in DataFrame",
        "body": "<p>A <a href=\"https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values\">post</a> gives a way to choose rows by column value</p>\n\n<p>Here is a DataFrame</p>\n\n<pre><code>            0           1\n0  877.443401  808.520962\n1  826.300620  848.761594\n2  824.403359  861.395174\n3  866.732033  804.494156\n4  853.461260  874.307851\n5  822.906499  830.102249\n6  852.605652  863.602725\n7  893.421600  825.032893\n8  863.768363  862.298227\n9  899.976622  864.111539\n</code></pre>\n\n<p>with this code <code>df[df.columns[[1]]]&gt;850</code>, I got</p>\n\n<pre><code>    1\n0   False\n1   False\n2   True\n3   False\n4   True\n5   False\n6   True\n7   False\n8   True\n9   True\n</code></pre>\n\n<p>when I run this <code>df.loc[(df[df.columns[[1]]]&gt;850)]</code>, I got error</p>\n\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8a159ef0cec2&gt; in &lt;module&gt;()\n----&gt; 1 df.loc[(df[df.columns[[1]]]&gt;850)]\n</code></pre>\n\n<p>this code <code>df[df[df.columns[[1]]]&gt;850]</code> gives </p>\n\n<pre><code>    0   1\n0   NaN NaN\n1   NaN NaN\n2   NaN 861.395174\n3   NaN NaN\n4   NaN 874.307851\n5   NaN NaN\n6   NaN 863.602725\n7   NaN NaN\n8   NaN 862.298227\n9   NaN 864.111539\n</code></pre>\n\n<p>This is close, what I am trying to get is a new DataFrame consists of rows at [2,4,6,8,9].</p>\n\n<p>How to do that? Thanks to anyone who gives some inspiration. </p>\n",
        "answer_body": "<p><code>df['a']</code> returns a <code>pd.Series</code> while <code>df[['a']]</code> returns a <code>pd.DataFrame</code> with only column being 'a'. For your problem:</p>\n\n<p>Using <code>loc</code></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>new_df = df.loc[df[1] &gt; 850].copy()\n</code></pre>\n\n<p>Using <code>query</code></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>new_df = df.query('a &gt; 850')\n</code></pre>\n\n<p>It's customary using <code>str</code> column names instead of <code>int</code>. For example, the <code>query</code> method would not work with <code>int</code> column names and there are a plethora of weird behaviours you can face with <code>int</code> column names.</p>\n",
        "question_body": "<p>A <a href=\"https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values\">post</a> gives a way to choose rows by column value</p>\n\n<p>Here is a DataFrame</p>\n\n<pre><code>            0           1\n0  877.443401  808.520962\n1  826.300620  848.761594\n2  824.403359  861.395174\n3  866.732033  804.494156\n4  853.461260  874.307851\n5  822.906499  830.102249\n6  852.605652  863.602725\n7  893.421600  825.032893\n8  863.768363  862.298227\n9  899.976622  864.111539\n</code></pre>\n\n<p>with this code <code>df[df.columns[[1]]]&gt;850</code>, I got</p>\n\n<pre><code>    1\n0   False\n1   False\n2   True\n3   False\n4   True\n5   False\n6   True\n7   False\n8   True\n9   True\n</code></pre>\n\n<p>when I run this <code>df.loc[(df[df.columns[[1]]]&gt;850)]</code>, I got error</p>\n\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8a159ef0cec2&gt; in &lt;module&gt;()\n----&gt; 1 df.loc[(df[df.columns[[1]]]&gt;850)]\n</code></pre>\n\n<p>this code <code>df[df[df.columns[[1]]]&gt;850]</code> gives </p>\n\n<pre><code>    0   1\n0   NaN NaN\n1   NaN NaN\n2   NaN 861.395174\n3   NaN NaN\n4   NaN 874.307851\n5   NaN NaN\n6   NaN 863.602725\n7   NaN NaN\n8   NaN 862.298227\n9   NaN 864.111539\n</code></pre>\n\n<p>This is close, what I am trying to get is a new DataFrame consists of rows at [2,4,6,8,9].</p>\n\n<p>How to do that? Thanks to anyone who gives some inspiration. </p>\n",
        "formatted_input": {
            "qid": 60994122,
            "link": "https://stackoverflow.com/questions/60994122/choosing-rows-by-values-in-dataframe",
            "question": {
                "title": "choosing rows by values in DataFrame",
                "ques_desc": "A post gives a way to choose rows by column value Here is a DataFrame with this code , I got when I run this , I got error this code gives This is close, what I am trying to get is a new DataFrame consists of rows at [2,4,6,8,9]. How to do that? Thanks to anyone who gives some inspiration. "
            },
            "io": [
                "            0           1\n0  877.443401  808.520962\n1  826.300620  848.761594\n2  824.403359  861.395174\n3  866.732033  804.494156\n4  853.461260  874.307851\n5  822.906499  830.102249\n6  852.605652  863.602725\n7  893.421600  825.032893\n8  863.768363  862.298227\n9  899.976622  864.111539\n",
                "    0   1\n0   NaN NaN\n1   NaN NaN\n2   NaN 861.395174\n3   NaN NaN\n4   NaN 874.307851\n5   NaN NaN\n6   NaN 863.602725\n7   NaN NaN\n8   NaN 862.298227\n9   NaN 864.111539\n"
            ],
            "answer": {
                "ans_desc": " returns a while returns a with only column being 'a'. For your problem: Using Using It's customary using column names instead of . For example, the method would not work with column names and there are a plethora of weird behaviours you can face with column names. ",
                "code": [
                    "new_df = df.loc[df[1] > 850].copy()\n",
                    "new_df = df.query('a > 850')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 849,
            "user_id": 9192284,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-VqBgemoLyi4/AAAAAAAAAAI/AAAAAAAAAuY/TRI4vmiupzY/photo.jpg?sz=128",
            "display_name": "MDR",
            "link": "https://stackoverflow.com/users/9192284/mdr"
        },
        "is_answered": true,
        "view_count": 2123,
        "accepted_answer_id": 60990079,
        "answer_count": 5,
        "score": 11,
        "last_activity_date": 1585826369,
        "creation_date": 1585824920,
        "question_id": 60989914,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60989914/add-id-found-in-list-to-new-column-in-pandas-dataframe",
        "title": "Add ID found in list to new column in pandas dataframe",
        "body": "<p>Say I have the following dataframe (a column of integers and a column with a list of integers)...</p>\n\n<pre><code>      ID                   Found_IDs\n0  12345        [15443, 15533, 3433]\n1  15533  [2234, 16608, 12002, 7654]\n2   6789      [43322, 876544, 36789]\n</code></pre>\n\n<p>And also a separate list of IDs...</p>\n\n<pre><code>bad_ids = [15533, 876544, 36789, 11111]\n</code></pre>\n\n<p>Given that, and ignoring the <code>df['ID']</code> column and any index, I want to see if any of the IDs in the <code>bad_ids</code> list are mentioned in the <code>df['Found_IDs']</code> column.  The code I have so far is:</p>\n\n<pre><code>df['bad_id'] = [c in l for c, l in zip(bad_ids, df['Found_IDs'])]\n</code></pre>\n\n<p>This works but only if the <code>bad_ids</code> list is longer than the dataframe and for the real dataset the <code>bad_ids</code> list is going to be a lot shorter than the dataframe.  If I set the <code>bad_ids</code> list to only two elements...</p>\n\n<pre><code>bad_ids = [15533, 876544]\n</code></pre>\n\n<p>I get a very popular error (I have read many questions with the same error)...</p>\n\n<pre><code>ValueError: Length of values does not match length of index\n</code></pre>\n\n<p>I have tried converting the list to a series (no change in the error).  I have also tried adding the new column and setting all values to <code>False</code> before doing the comprehension line (again no change in the error).</p>\n\n<p>Two questions:</p>\n\n<ol>\n<li>How do I get my code (below) to work for a list that is shorter than\na dataframe? </li>\n<li>How would I get the code to write the actual  ID found\nback to the <code>df['bad_id']</code> column (more useful than True/False)?</li>\n</ol>\n\n<p>Expected output for <code>bad_ids = [15533, 876544]</code>:</p>\n\n<pre><code>      ID                   Found_IDs  bad_id\n0  12345        [15443, 15533, 3433]    True\n1  15533  [2234, 16608, 12002, 7654]   False\n2   6789      [43322, 876544, 36789]    True\n</code></pre>\n\n<p>Ideal output for <code>bad_ids = [15533, 876544]</code> (ID(s) are written to a new column or columns):</p>\n\n<pre><code>      ID                   Found_IDs  bad_id\n0  12345        [15443, 15533, 3433]    15533\n1  15533  [2234, 16608, 12002, 7654]   False\n2   6789      [43322, 876544, 36789]    876544\n</code></pre>\n\n<p>Code:</p>\n\n<pre><code>import pandas as pd\n\nresult_list = [[12345,[15443,15533,3433]],\n        [15533,[2234,16608,12002,7654]],\n        [6789,[43322,876544,36789]]]\n\ndf = pd.DataFrame(result_list,columns=['ID','Found_IDs'])\n\n# works if list has four elements\n# bad_ids = [15533, 876544, 36789, 11111]\n\n# fails if list has two elements (less elements than the dataframe)\n# ValueError: Length of values does not match length of index\nbad_ids = [15533, 876544]\n\n# coverting to Series doesn't change things\n# bad_ids = pd.Series(bad_ids)\n# print(type(bad_ids))\n\n# setting up a new column of false values doesn't change things\n# df['bad_id'] = False\n\nprint(df)\n\ndf['bad_id'] = [c in l for c, l in zip(bad_ids, df['Found_IDs'])]\n\nprint(bad_ids)\n\nprint(df)\n</code></pre>\n",
        "answer_body": "<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.intersect1d.html\" rel=\"noreferrer\"><code>np.intersect1d</code></a> to get the intersect of the two lists:</p>\n\n<pre><code>df['bad_id'] = df['Found_IDs'].apply(lambda x: np.intersect1d(x, bad_ids))\n\n      ID                   Found_IDs    bad_id\n0  12345        [15443, 15533, 3433]   [15533]\n1  15533  [2234, 16608, 12002, 7654]        []\n2   6789      [43322, 876544, 36789]  [876544]\n</code></pre>\n\n<p>Or with just vanilla python using intersect of <code>sets</code>:</p>\n\n<pre><code>bad_ids_set = set(bad_ids)\ndf['Found_IDs'].apply(lambda x: list(set(x) &amp; bad_ids_set))\n</code></pre>\n",
        "question_body": "<p>Say I have the following dataframe (a column of integers and a column with a list of integers)...</p>\n\n<pre><code>      ID                   Found_IDs\n0  12345        [15443, 15533, 3433]\n1  15533  [2234, 16608, 12002, 7654]\n2   6789      [43322, 876544, 36789]\n</code></pre>\n\n<p>And also a separate list of IDs...</p>\n\n<pre><code>bad_ids = [15533, 876544, 36789, 11111]\n</code></pre>\n\n<p>Given that, and ignoring the <code>df['ID']</code> column and any index, I want to see if any of the IDs in the <code>bad_ids</code> list are mentioned in the <code>df['Found_IDs']</code> column.  The code I have so far is:</p>\n\n<pre><code>df['bad_id'] = [c in l for c, l in zip(bad_ids, df['Found_IDs'])]\n</code></pre>\n\n<p>This works but only if the <code>bad_ids</code> list is longer than the dataframe and for the real dataset the <code>bad_ids</code> list is going to be a lot shorter than the dataframe.  If I set the <code>bad_ids</code> list to only two elements...</p>\n\n<pre><code>bad_ids = [15533, 876544]\n</code></pre>\n\n<p>I get a very popular error (I have read many questions with the same error)...</p>\n\n<pre><code>ValueError: Length of values does not match length of index\n</code></pre>\n\n<p>I have tried converting the list to a series (no change in the error).  I have also tried adding the new column and setting all values to <code>False</code> before doing the comprehension line (again no change in the error).</p>\n\n<p>Two questions:</p>\n\n<ol>\n<li>How do I get my code (below) to work for a list that is shorter than\na dataframe? </li>\n<li>How would I get the code to write the actual  ID found\nback to the <code>df['bad_id']</code> column (more useful than True/False)?</li>\n</ol>\n\n<p>Expected output for <code>bad_ids = [15533, 876544]</code>:</p>\n\n<pre><code>      ID                   Found_IDs  bad_id\n0  12345        [15443, 15533, 3433]    True\n1  15533  [2234, 16608, 12002, 7654]   False\n2   6789      [43322, 876544, 36789]    True\n</code></pre>\n\n<p>Ideal output for <code>bad_ids = [15533, 876544]</code> (ID(s) are written to a new column or columns):</p>\n\n<pre><code>      ID                   Found_IDs  bad_id\n0  12345        [15443, 15533, 3433]    15533\n1  15533  [2234, 16608, 12002, 7654]   False\n2   6789      [43322, 876544, 36789]    876544\n</code></pre>\n\n<p>Code:</p>\n\n<pre><code>import pandas as pd\n\nresult_list = [[12345,[15443,15533,3433]],\n        [15533,[2234,16608,12002,7654]],\n        [6789,[43322,876544,36789]]]\n\ndf = pd.DataFrame(result_list,columns=['ID','Found_IDs'])\n\n# works if list has four elements\n# bad_ids = [15533, 876544, 36789, 11111]\n\n# fails if list has two elements (less elements than the dataframe)\n# ValueError: Length of values does not match length of index\nbad_ids = [15533, 876544]\n\n# coverting to Series doesn't change things\n# bad_ids = pd.Series(bad_ids)\n# print(type(bad_ids))\n\n# setting up a new column of false values doesn't change things\n# df['bad_id'] = False\n\nprint(df)\n\ndf['bad_id'] = [c in l for c, l in zip(bad_ids, df['Found_IDs'])]\n\nprint(bad_ids)\n\nprint(df)\n</code></pre>\n",
        "formatted_input": {
            "qid": 60989914,
            "link": "https://stackoverflow.com/questions/60989914/add-id-found-in-list-to-new-column-in-pandas-dataframe",
            "question": {
                "title": "Add ID found in list to new column in pandas dataframe",
                "ques_desc": "Say I have the following dataframe (a column of integers and a column with a list of integers)... And also a separate list of IDs... Given that, and ignoring the column and any index, I want to see if any of the IDs in the list are mentioned in the column. The code I have so far is: This works but only if the list is longer than the dataframe and for the real dataset the list is going to be a lot shorter than the dataframe. If I set the list to only two elements... I get a very popular error (I have read many questions with the same error)... I have tried converting the list to a series (no change in the error). I have also tried adding the new column and setting all values to before doing the comprehension line (again no change in the error). Two questions: How do I get my code (below) to work for a list that is shorter than a dataframe? How would I get the code to write the actual ID found back to the column (more useful than True/False)? Expected output for : Ideal output for (ID(s) are written to a new column or columns): Code: "
            },
            "io": [
                "      ID                   Found_IDs\n0  12345        [15443, 15533, 3433]\n1  15533  [2234, 16608, 12002, 7654]\n2   6789      [43322, 876544, 36789]\n",
                "bad_ids = [15533, 876544, 36789, 11111]\n"
            ],
            "answer": {
                "ans_desc": "Using to get the intersect of the two lists: Or with just vanilla python using intersect of : ",
                "code": [
                    "bad_ids_set = set(bad_ids)\ndf['Found_IDs'].apply(lambda x: list(set(x) & bad_ids_set))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 49,
            "user_id": 4458456,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e9b7045e28d487d3d45f1af3e59efbba?s=128&d=identicon&r=PG&f=1",
            "display_name": "Tim Schulz",
            "link": "https://stackoverflow.com/users/4458456/tim-schulz"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 60969433,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1585739422,
        "creation_date": 1585737298,
        "question_id": 60968943,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60968943/pandas-reshaping-dataframe",
        "title": "Pandas: Reshaping dataframe",
        "body": "<p>I have a panda's related question. My dataframe looks something like this:</p>\n\n<pre><code>  id val1 val2\n0  1     0    1\n1  1     1    0\n2  1     0    0\n3  2     1    1\n4  2     1    1\n5  2     1    0\n6  3     0    0\n7  3     0    1\n8  3     1    1\n9  4     1    0\n10 4     0    1\n11 4     0    0\n</code></pre>\n\n<p>I want to transform it into something like:</p>\n\n<pre><code>             a         b        c\n   id     a0   a1   b0   b1   c0   c1\n    1     0    1    1    0    0    0\n    2     1    1    1    1    1    0\n    3     0    0    1    1    1    1\n    4     1    0    0    1    0    0\n</code></pre>\n\n<p>I thought of something like adding a sub_id column that is enumerated cyclically by a, b and c and then do an unstack of the frame. Is there an easier/smarter solution? </p>\n\n<p>Thanks a lot!</p>\n\n<p>Tim</p>\n",
        "answer_body": "<p>One of possible solutions:</p>\n\n<p>Start from reformatting values for each <em>id</em> into a single row:</p>\n\n<pre><code>res = df.set_index('id').groupby('id').apply(\n    lambda grp: pd.Series(grp.values.flatten()))\n</code></pre>\n\n<p>For now the result is:</p>\n\n<pre><code>    0  1  2  3  4  5\nid                  \n1   0  1  1  0  0  0\n2   1  1  1  1  1  0\n3   0  0  0  1  1  1\n4   1  0  0  1  0  0\n</code></pre>\n\n<p>Then set proper column names:</p>\n\n<pre><code>res.columns = pd.MultiIndex.from_tuples(\n    [(x, x + y) for x in list('abc') for y in list('01')])\n</code></pre>\n\n<p>The finale result is:</p>\n\n<pre><code>    a     b     c   \n   a0 a1 b0 b1 c0 c1\nid                  \n1   0  1  1  0  0  0\n2   1  1  1  1  1  0\n3   0  0  0  1  1  1\n4   1  0  0  1  0  0\n</code></pre>\n",
        "question_body": "<p>I have a panda's related question. My dataframe looks something like this:</p>\n\n<pre><code>  id val1 val2\n0  1     0    1\n1  1     1    0\n2  1     0    0\n3  2     1    1\n4  2     1    1\n5  2     1    0\n6  3     0    0\n7  3     0    1\n8  3     1    1\n9  4     1    0\n10 4     0    1\n11 4     0    0\n</code></pre>\n\n<p>I want to transform it into something like:</p>\n\n<pre><code>             a         b        c\n   id     a0   a1   b0   b1   c0   c1\n    1     0    1    1    0    0    0\n    2     1    1    1    1    1    0\n    3     0    0    1    1    1    1\n    4     1    0    0    1    0    0\n</code></pre>\n\n<p>I thought of something like adding a sub_id column that is enumerated cyclically by a, b and c and then do an unstack of the frame. Is there an easier/smarter solution? </p>\n\n<p>Thanks a lot!</p>\n\n<p>Tim</p>\n",
        "formatted_input": {
            "qid": 60968943,
            "link": "https://stackoverflow.com/questions/60968943/pandas-reshaping-dataframe",
            "question": {
                "title": "Pandas: Reshaping dataframe",
                "ques_desc": "I have a panda's related question. My dataframe looks something like this: I want to transform it into something like: I thought of something like adding a sub_id column that is enumerated cyclically by a, b and c and then do an unstack of the frame. Is there an easier/smarter solution? Thanks a lot! Tim "
            },
            "io": [
                "  id val1 val2\n0  1     0    1\n1  1     1    0\n2  1     0    0\n3  2     1    1\n4  2     1    1\n5  2     1    0\n6  3     0    0\n7  3     0    1\n8  3     1    1\n9  4     1    0\n10 4     0    1\n11 4     0    0\n",
                "             a         b        c\n   id     a0   a1   b0   b1   c0   c1\n    1     0    1    1    0    0    0\n    2     1    1    1    1    1    0\n    3     0    0    1    1    1    1\n    4     1    0    0    1    0    0\n"
            ],
            "answer": {
                "ans_desc": "One of possible solutions: Start from reformatting values for each id into a single row: For now the result is: Then set proper column names: The finale result is: ",
                "code": [
                    "res = df.set_index('id').groupby('id').apply(\n    lambda grp: pd.Series(grp.values.flatten()))\n",
                    "res.columns = pd.MultiIndex.from_tuples(\n    [(x, x + y) for x in list('abc') for y in list('01')])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "time-series"
        ],
        "owner": {
            "reputation": 187,
            "user_id": 4214996,
            "user_type": "registered",
            "accept_rate": 67,
            "profile_image": "https://www.gravatar.com/avatar/5668b29c7cf878ab5c343ce588362ca3?s=128&d=identicon&r=PG&f=1",
            "display_name": "gizq",
            "link": "https://stackoverflow.com/users/4214996/gizq"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 60959669,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1585688253,
        "creation_date": 1585687212,
        "last_edit_date": 1585687673,
        "question_id": 60959528,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60959528/pandas-select-only-the-first-3-yyyymm-per-group",
        "title": "Pandas select only the first 3 YYYYMM per group",
        "body": "<p>CGood afternoon,</p>\n\n<p>I have a datrame like the one below</p>\n\n<pre><code>+---+---+--------+\n|   |USR| MMMMYY |\n+---+---+--------+\n| 1 | A | 200002 |\n+---+---+--------+\n| 2 | A | 200003 |\n+---+---+--------+\n| 3 | A | 200004 |\n+---+---+--------+\n| 4 | A | 200005 |\n+---+---+--------+\n| 5 | B | 200001 |\n+---+---+--------+\n| 6 | B | 200003 |\n+---+---+--------+\n| 7 | B | 200008 |\n+---+---+--------+\n| 8 | B | 200009 |\n+---+---+--------+\n</code></pre>\n\n<p>I need to get only the first three *CONSECUTIVE MMMMYY per USR.</p>\n\n<pre><code>+---+---+--------+\n|   |USR| MMMMYY |\n+---+---+--------+\n| 1 | A | 200002 |\n+---+---+--------+\n| 2 | A | 200003 |\n+---+---+--------+\n| 3 | A | 200004 |\n+---+---+--------+\n| 5 | B | 200001 |\n+---+---+--------+\n| 6 | B | 200003 |\n+---+---+--------+\n</code></pre>\n\n<p>Im able to get the first 3 records using head(3)</p>\n\n<pre><code>df.sort_values(['USR', 'MMMMYY' ], ascending=[True, True]).groupby('USR', as_index=False).head(3)\n</code></pre>\n\n<p>but of course it dont bring back what I need, neither using </p>\n\n<pre><code>df['mm_dif']=df.groupby(['USR'])['MMMMYY'].diff()\n\ndf['mm_dif2']=df.groupby(['USR'])['MMMMYY'].diff(-1)\n\ndf['check']=np.where((df.mm_dif==1) | (df.mm_dif2==-1),True,False)\n</code></pre>\n\n<p>it gets the consecutive when ['check'] is true but in some cases I may need to get 200001 and 200003 only and they are not consecutive between them. Any guidance will be appreciated</p>\n\n<p>Thanks</p>\n",
        "answer_body": "<p>Your <code>MMMMYY</code> is datetime, then turn it to <code>datetime</code> type first:</p>\n\n<pre><code>df['MMMMYY'] = pd.to_datetime(df.MMMMYY, format='%Y%m')\n\ns = df.groupby('USR')['MMMMYY'].transform('min') + pd.offsets.MonthOffset(3)\n\ndf[df.MMMMYY&lt;s]\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>  USR     MMMMYY\n1   A 2000-02-01\n2   A 2000-03-01\n3   A 2000-04-01\n5   B 2000-01-01\n6   B 2000-03-01\n</code></pre>\n",
        "question_body": "<p>CGood afternoon,</p>\n\n<p>I have a datrame like the one below</p>\n\n<pre><code>+---+---+--------+\n|   |USR| MMMMYY |\n+---+---+--------+\n| 1 | A | 200002 |\n+---+---+--------+\n| 2 | A | 200003 |\n+---+---+--------+\n| 3 | A | 200004 |\n+---+---+--------+\n| 4 | A | 200005 |\n+---+---+--------+\n| 5 | B | 200001 |\n+---+---+--------+\n| 6 | B | 200003 |\n+---+---+--------+\n| 7 | B | 200008 |\n+---+---+--------+\n| 8 | B | 200009 |\n+---+---+--------+\n</code></pre>\n\n<p>I need to get only the first three *CONSECUTIVE MMMMYY per USR.</p>\n\n<pre><code>+---+---+--------+\n|   |USR| MMMMYY |\n+---+---+--------+\n| 1 | A | 200002 |\n+---+---+--------+\n| 2 | A | 200003 |\n+---+---+--------+\n| 3 | A | 200004 |\n+---+---+--------+\n| 5 | B | 200001 |\n+---+---+--------+\n| 6 | B | 200003 |\n+---+---+--------+\n</code></pre>\n\n<p>Im able to get the first 3 records using head(3)</p>\n\n<pre><code>df.sort_values(['USR', 'MMMMYY' ], ascending=[True, True]).groupby('USR', as_index=False).head(3)\n</code></pre>\n\n<p>but of course it dont bring back what I need, neither using </p>\n\n<pre><code>df['mm_dif']=df.groupby(['USR'])['MMMMYY'].diff()\n\ndf['mm_dif2']=df.groupby(['USR'])['MMMMYY'].diff(-1)\n\ndf['check']=np.where((df.mm_dif==1) | (df.mm_dif2==-1),True,False)\n</code></pre>\n\n<p>it gets the consecutive when ['check'] is true but in some cases I may need to get 200001 and 200003 only and they are not consecutive between them. Any guidance will be appreciated</p>\n\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 60959528,
            "link": "https://stackoverflow.com/questions/60959528/pandas-select-only-the-first-3-yyyymm-per-group",
            "question": {
                "title": "Pandas select only the first 3 YYYYMM per group",
                "ques_desc": "CGood afternoon, I have a datrame like the one below I need to get only the first three *CONSECUTIVE MMMMYY per USR. Im able to get the first 3 records using head(3) but of course it dont bring back what I need, neither using it gets the consecutive when ['check'] is true but in some cases I may need to get 200001 and 200003 only and they are not consecutive between them. Any guidance will be appreciated Thanks "
            },
            "io": [
                "+---+---+--------+\n|   |USR| MMMMYY |\n+---+---+--------+\n| 1 | A | 200002 |\n+---+---+--------+\n| 2 | A | 200003 |\n+---+---+--------+\n| 3 | A | 200004 |\n+---+---+--------+\n| 4 | A | 200005 |\n+---+---+--------+\n| 5 | B | 200001 |\n+---+---+--------+\n| 6 | B | 200003 |\n+---+---+--------+\n| 7 | B | 200008 |\n+---+---+--------+\n| 8 | B | 200009 |\n+---+---+--------+\n",
                "+---+---+--------+\n|   |USR| MMMMYY |\n+---+---+--------+\n| 1 | A | 200002 |\n+---+---+--------+\n| 2 | A | 200003 |\n+---+---+--------+\n| 3 | A | 200004 |\n+---+---+--------+\n| 5 | B | 200001 |\n+---+---+--------+\n| 6 | B | 200003 |\n+---+---+--------+\n"
            ],
            "answer": {
                "ans_desc": "Your is datetime, then turn it to type first: Output: ",
                "code": [
                    "df['MMMMYY'] = pd.to_datetime(df.MMMMYY, format='%Y%m')\n\ns = df.groupby('USR')['MMMMYY'].transform('min') + pd.offsets.MonthOffset(3)\n\ndf[df.MMMMYY<s]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15424,
            "user_id": 452587,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/2f3d521daf2fca0116987e9c8354ec55?s=128&d=identicon&r=PG",
            "display_name": "thdoan",
            "link": "https://stackoverflow.com/users/452587/thdoan"
        },
        "is_answered": true,
        "view_count": 283,
        "accepted_answer_id": 60843750,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1585425209,
        "creation_date": 1585115090,
        "last_edit_date": 1585425209,
        "question_id": 60843541,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60843541/fastest-way-to-calculate-in-pandas",
        "title": "Fastest way to calculate in Pandas?",
        "body": "<p>Given these two dataframes:</p>\n\n<pre><code>df1 =\n     Name  Start  End\n  0  A     10     20\n  1  B     20     30\n  2  C     30     40\n\ndf2 =\n     0   1\n  0  5   10\n  1  15  20\n  2  25  30\n</code></pre>\n\n<p><code>df2</code> has no column names, but you can assume column 0 is an offset of <code>df1.Start</code> and column 1 is an offset of <code>df1.End</code>. I would like to transpose <code>df2</code> onto <code>df1</code> to get the Start and End differences. The final <code>df1</code> dataframe should look like this:</p>\n\n<pre><code>  Name  Start  End  Start_Diff_0  End_Diff_0  Start_Diff_1  End_Diff_1  Start_Diff_2  End_Diff_2\n0    A     10   20             5          10            -5           0           -15         -10\n1    B     20   30            15          20             5          10            -5           0\n2    C     30   40            25          30            15          20             5          10\n</code></pre>\n\n<p>I have a solution that works, but I'm not satisfied with it because it takes too long to run when processing a dataframe that has millions of rows. Below is a sample test case to simulate processing 30,000 rows. As you can imagine, running the original solution (method_1) on a 1GB dataframe is going to be a problem. Is there a faster way to do this using Pandas, Numpy, or maybe another package?</p>\n\n<p><strong>UPDATE:</strong> I've added the provided solutions to the benchmarks.</p>\n\n<pre><code># Import required modules\nimport numpy as np\nimport pandas as pd\nimport timeit\n\n# Original\ndef method_1():\n    df1 = pd.DataFrame([['A', 10, 20], ['B', 20, 30], ['C', 30, 40]] * 10000, columns=['Name', 'Start', 'End'])\n    df2 = pd.DataFrame([[5, 10], [15, 20], [25, 30]], columns=None)\n    # Store data for new columns in a dictionary\n    new_columns = {}\n    for index1, row1 in df1.iterrows():\n        for index2, row2 in df2.iterrows():\n            key_start = 'Start_Diff_' + str(index2)\n            key_end = 'End_Diff_' + str(index2)\n            if (key_start in new_columns):\n                new_columns[key_start].append(row1[1]-row2[0])\n            else:\n                new_columns[key_start] = [row1[1]-row2[0]]\n            if (key_end in new_columns):\n                new_columns[key_end].append(row1[2]-row2[1])\n            else:\n                new_columns[key_end] = [row1[2]-row2[1]]\n    # Add dictionary data as new columns\n    for key, value in new_columns.items():\n        df1[key] = value\n\n# jezrael - https://stackoverflow.com/a/60843750/452587\ndef method_2():\n    df1 = pd.DataFrame([['A', 10, 20], ['B', 20, 30], ['C', 30, 40]] * 10000, columns=['Name', 'Start', 'End'])\n    df2 = pd.DataFrame([[5, 10], [15, 20], [25, 30]], columns=None)\n    # Convert selected columns to 2d numpy array\n    a = df1[['Start', 'End']].to_numpy()\n    b = df2[[0, 1]].to_numpy()\n    # Output is 3d array; convert it to 2d array\n    c = (a - b[:, None]).swapaxes(0, 1).reshape(a.shape[0], -1)\n    # Generate columns names and with DataFrame.join; add to original\n    cols = [item for x in range(b.shape[0]) for item in (f'Start_Diff_{x}', f'End_Diff_{x}')]\n    df1 = df1.join(pd.DataFrame(c, columns=cols, index=df1.index))\n\n# sammywemmy - https://stackoverflow.com/a/60844078/452587\ndef method_3():\n    df1 = pd.DataFrame([['A', 10, 20], ['B', 20, 30], ['C', 30, 40]] * 10000, columns=['Name', 'Start', 'End'])\n    df2 = pd.DataFrame([[5, 10], [15, 20], [25, 30]], columns=None)\n    # Create numpy arrays of df1 and df2\n    df1_start = df1.loc[:, 'Start'].to_numpy()\n    df1_end = df1.loc[:, 'End'].to_numpy()\n    df2_start = df2[0].to_numpy()\n    df2_end = df2[1].to_numpy()\n    # Use np tile to create shapes that allow elementwise subtraction\n    tiled_start = np.tile(df1_start, (len(df2), 1)).T\n    tiled_end = np.tile(df1_end, (len(df2), 1)).T\n    # Subtract df2 from df1\n    start = np.subtract(tiled_start, df2_start)\n    end = np.subtract(tiled_end, df2_end)\n    # Create columns for start and end\n    start_columns = [f'Start_Diff_{num}' for num in range(len(df2))]\n    end_columns = [f'End_Diff_{num}' for num in range(len(df2))]\n    # Create dataframes of start and end\n    start_df = pd.DataFrame(start, columns=start_columns)\n    end_df = pd.DataFrame(end, columns=end_columns)\n    # Lump start and end into one dataframe\n    lump = pd.concat([start_df, end_df], axis=1)\n    # Sort the columns by the digits at the end\n    filtered = lump.columns[lump.columns.str.contains('\\d')]\n    cols = sorted(filtered, key=lambda x: x[-1])\n    lump = lump.reindex(cols, axis='columns')\n    # Hook lump back to df1\n    df1 = pd.concat([df1,lump],axis=1)\n\nprint('Method 1:', timeit.timeit(method_1, number=3))\nprint('Method 2:', timeit.timeit(method_2, number=3))\nprint('Method 3:', timeit.timeit(method_3, number=3))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>Method 1: 50.506279182\nMethod 2: 0.08886280600000163\nMethod 3: 0.10297686199999845\n</code></pre>\n",
        "answer_body": "<p>I suggest use here numpy - convert selected columns to <code>2d numpy</code> array in first step::</p>\n\n<pre><code>a = df1[['Start','End']].to_numpy()\nb = df2[[0,1]].to_numpy()\n</code></pre>\n\n<p>Output is 3d array, convert it to <code>2d array</code>:</p>\n\n<pre><code>c = (a - b[:, None]).swapaxes(0,1).reshape(a.shape[0],-1)\nprint (c)\n[[  5  10  -5   0 -15 -10]\n [ 15  20   5  10  -5   0]\n [ 25  30  15  20   5  10]]\n</code></pre>\n\n<p>Last generate columns names and with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html\" rel=\"nofollow noreferrer\"><code>DataFrame.join</code></a> add to original:</p>\n\n<pre><code>cols = [item for x in range(b.shape[0]) for item in (f'Start_Diff_{x}', f'End_Diff_{x}')]\ndf = df1.join(pd.DataFrame(c, columns=cols, index=df1.index))\nprint (df)\n  Name  Start  End  Start_Diff_0  End_Diff_0  Start_Diff_1  End_Diff_1  \\\n0    A     10   20             5          10            -5           0   \n1    B     20   30            15          20             5          10   \n2    C     30   40            25          30            15          20   \n\n   Start_Diff_2  End_Diff_2  \n0           -15         -10  \n1            -5           0  \n2             5          10  \n</code></pre>\n",
        "question_body": "<p>Given these two dataframes:</p>\n\n<pre><code>df1 =\n     Name  Start  End\n  0  A     10     20\n  1  B     20     30\n  2  C     30     40\n\ndf2 =\n     0   1\n  0  5   10\n  1  15  20\n  2  25  30\n</code></pre>\n\n<p><code>df2</code> has no column names, but you can assume column 0 is an offset of <code>df1.Start</code> and column 1 is an offset of <code>df1.End</code>. I would like to transpose <code>df2</code> onto <code>df1</code> to get the Start and End differences. The final <code>df1</code> dataframe should look like this:</p>\n\n<pre><code>  Name  Start  End  Start_Diff_0  End_Diff_0  Start_Diff_1  End_Diff_1  Start_Diff_2  End_Diff_2\n0    A     10   20             5          10            -5           0           -15         -10\n1    B     20   30            15          20             5          10            -5           0\n2    C     30   40            25          30            15          20             5          10\n</code></pre>\n\n<p>I have a solution that works, but I'm not satisfied with it because it takes too long to run when processing a dataframe that has millions of rows. Below is a sample test case to simulate processing 30,000 rows. As you can imagine, running the original solution (method_1) on a 1GB dataframe is going to be a problem. Is there a faster way to do this using Pandas, Numpy, or maybe another package?</p>\n\n<p><strong>UPDATE:</strong> I've added the provided solutions to the benchmarks.</p>\n\n<pre><code># Import required modules\nimport numpy as np\nimport pandas as pd\nimport timeit\n\n# Original\ndef method_1():\n    df1 = pd.DataFrame([['A', 10, 20], ['B', 20, 30], ['C', 30, 40]] * 10000, columns=['Name', 'Start', 'End'])\n    df2 = pd.DataFrame([[5, 10], [15, 20], [25, 30]], columns=None)\n    # Store data for new columns in a dictionary\n    new_columns = {}\n    for index1, row1 in df1.iterrows():\n        for index2, row2 in df2.iterrows():\n            key_start = 'Start_Diff_' + str(index2)\n            key_end = 'End_Diff_' + str(index2)\n            if (key_start in new_columns):\n                new_columns[key_start].append(row1[1]-row2[0])\n            else:\n                new_columns[key_start] = [row1[1]-row2[0]]\n            if (key_end in new_columns):\n                new_columns[key_end].append(row1[2]-row2[1])\n            else:\n                new_columns[key_end] = [row1[2]-row2[1]]\n    # Add dictionary data as new columns\n    for key, value in new_columns.items():\n        df1[key] = value\n\n# jezrael - https://stackoverflow.com/a/60843750/452587\ndef method_2():\n    df1 = pd.DataFrame([['A', 10, 20], ['B', 20, 30], ['C', 30, 40]] * 10000, columns=['Name', 'Start', 'End'])\n    df2 = pd.DataFrame([[5, 10], [15, 20], [25, 30]], columns=None)\n    # Convert selected columns to 2d numpy array\n    a = df1[['Start', 'End']].to_numpy()\n    b = df2[[0, 1]].to_numpy()\n    # Output is 3d array; convert it to 2d array\n    c = (a - b[:, None]).swapaxes(0, 1).reshape(a.shape[0], -1)\n    # Generate columns names and with DataFrame.join; add to original\n    cols = [item for x in range(b.shape[0]) for item in (f'Start_Diff_{x}', f'End_Diff_{x}')]\n    df1 = df1.join(pd.DataFrame(c, columns=cols, index=df1.index))\n\n# sammywemmy - https://stackoverflow.com/a/60844078/452587\ndef method_3():\n    df1 = pd.DataFrame([['A', 10, 20], ['B', 20, 30], ['C', 30, 40]] * 10000, columns=['Name', 'Start', 'End'])\n    df2 = pd.DataFrame([[5, 10], [15, 20], [25, 30]], columns=None)\n    # Create numpy arrays of df1 and df2\n    df1_start = df1.loc[:, 'Start'].to_numpy()\n    df1_end = df1.loc[:, 'End'].to_numpy()\n    df2_start = df2[0].to_numpy()\n    df2_end = df2[1].to_numpy()\n    # Use np tile to create shapes that allow elementwise subtraction\n    tiled_start = np.tile(df1_start, (len(df2), 1)).T\n    tiled_end = np.tile(df1_end, (len(df2), 1)).T\n    # Subtract df2 from df1\n    start = np.subtract(tiled_start, df2_start)\n    end = np.subtract(tiled_end, df2_end)\n    # Create columns for start and end\n    start_columns = [f'Start_Diff_{num}' for num in range(len(df2))]\n    end_columns = [f'End_Diff_{num}' for num in range(len(df2))]\n    # Create dataframes of start and end\n    start_df = pd.DataFrame(start, columns=start_columns)\n    end_df = pd.DataFrame(end, columns=end_columns)\n    # Lump start and end into one dataframe\n    lump = pd.concat([start_df, end_df], axis=1)\n    # Sort the columns by the digits at the end\n    filtered = lump.columns[lump.columns.str.contains('\\d')]\n    cols = sorted(filtered, key=lambda x: x[-1])\n    lump = lump.reindex(cols, axis='columns')\n    # Hook lump back to df1\n    df1 = pd.concat([df1,lump],axis=1)\n\nprint('Method 1:', timeit.timeit(method_1, number=3))\nprint('Method 2:', timeit.timeit(method_2, number=3))\nprint('Method 3:', timeit.timeit(method_3, number=3))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>Method 1: 50.506279182\nMethod 2: 0.08886280600000163\nMethod 3: 0.10297686199999845\n</code></pre>\n",
        "formatted_input": {
            "qid": 60843541,
            "link": "https://stackoverflow.com/questions/60843541/fastest-way-to-calculate-in-pandas",
            "question": {
                "title": "Fastest way to calculate in Pandas?",
                "ques_desc": "Given these two dataframes: has no column names, but you can assume column 0 is an offset of and column 1 is an offset of . I would like to transpose onto to get the Start and End differences. The final dataframe should look like this: I have a solution that works, but I'm not satisfied with it because it takes too long to run when processing a dataframe that has millions of rows. Below is a sample test case to simulate processing 30,000 rows. As you can imagine, running the original solution (method_1) on a 1GB dataframe is going to be a problem. Is there a faster way to do this using Pandas, Numpy, or maybe another package? UPDATE: I've added the provided solutions to the benchmarks. Output: "
            },
            "io": [
                "df1 =\n     Name  Start  End\n  0  A     10     20\n  1  B     20     30\n  2  C     30     40\n\ndf2 =\n     0   1\n  0  5   10\n  1  15  20\n  2  25  30\n",
                "  Name  Start  End  Start_Diff_0  End_Diff_0  Start_Diff_1  End_Diff_1  Start_Diff_2  End_Diff_2\n0    A     10   20             5          10            -5           0           -15         -10\n1    B     20   30            15          20             5          10            -5           0\n2    C     30   40            25          30            15          20             5          10\n"
            ],
            "answer": {
                "ans_desc": "I suggest use here numpy - convert selected columns to array in first step:: Output is 3d array, convert it to : Last generate columns names and with add to original: ",
                "code": [
                    "a = df1[['Start','End']].to_numpy()\nb = df2[[0,1]].to_numpy()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "sorting"
        ],
        "owner": {
            "reputation": 151,
            "user_id": 7176741,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9a11bff4ff2c37a9f3115cc021659455?s=128&d=identicon&r=PG&f=1",
            "display_name": "Samael Olascoaga",
            "link": "https://stackoverflow.com/users/7176741/samael-olascoaga"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 60897543,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1585410919,
        "creation_date": 1585369036,
        "last_edit_date": 1585410919,
        "question_id": 60897173,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60897173/sort-pandas-dataframe-by-index",
        "title": "Sort pandas dataframe by index",
        "body": "<p>I have the following pandas dataframe:</p>\n\n<pre><code>    Cmpd1   Cmpd2   Cmpd3   Cmpd4   Cmpd5   Cmpd6\nCmpd1   1                   \nCmpd2   0.4   1             \nCmpd3   0.6   0.3   1           \nCmpd4   0.46  0.69  0.32    1       \nCmpd5   0.57  0.44  0.41    0.51    1   \nCmpd6   0.41  0.79  0.33    0.56    0.43    1\n</code></pre>\n\n<p>I would like to order it from highest to lowest based on the index no matter if it is repeated, what I say would look like this:</p>\n\n<p>The maximum value corresponds to Cmpd6 = 0.79, followed by Cmpd4 = 0.69 ... at some point Cmpd6 = 0.56 which I would like to leave the list like this:</p>\n\n<pre><code>Cmpd6 = 0.79\nCmpd4 = 0.69\nCmpdx = x\nCmpd6 = 0.56\n</code></pre>\n\n<p>This with each value of the array, no matter how many times the indexes are repeated, I was trying with <code>.sort_index (axis = 1)</code> but it does not produce any of this, I also tried <code>.ravel ()</code> but it does not give me the indexes. How can i fix this?</p>\n\n<p>Thanks!</p>\n\n<p>My solution:</p>\n\n<pre><code>df = df.where(np.tril(np.ones(df.shape), -1).astype(np.bool))\ndf = df.stack().reset_index().drop(\"level_1\", axis=1).sort_values(by=0, ascending=False)\n</code></pre>\n",
        "answer_body": "<p>I assume that each empty element in the source <em>df</em> contain actualy\nan empty string (not <em>NaN</em>, as these would be printed just as <em>NaN</em>).</p>\n\n<p>I also noticed that you want to keey only values <em>&lt; 1</em>.</p>\n\n<p>To get your result, run:</p>\n\n<pre><code>s = df.stack()\ns[s.apply(lambda x: type(x) is not str and x &lt; 1)]\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\\\n    .astype(float)\n</code></pre>\n\n<p>For your data the result is:</p>\n\n<pre><code>Cmpd6    0.79\nCmpd4    0.69\nCmpd3    0.60\nCmpd5    0.57\nCmpd6    0.56\nCmpd5    0.51\nCmpd4    0.46\nCmpd5    0.44\nCmpd6    0.43\nCmpd6    0.41\nCmpd5    0.41\nCmpd2    0.40\nCmpd6    0.33\nCmpd4    0.32\nCmpd3    0.30\ndtype: float64\n</code></pre>\n\n<p>Other possible solution:</p>\n\n<pre><code>s = df.replace(r'^\\s*$', np.nan, regex=True).stack()\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\ns[s &lt; 1]\n</code></pre>\n",
        "question_body": "<p>I have the following pandas dataframe:</p>\n\n<pre><code>    Cmpd1   Cmpd2   Cmpd3   Cmpd4   Cmpd5   Cmpd6\nCmpd1   1                   \nCmpd2   0.4   1             \nCmpd3   0.6   0.3   1           \nCmpd4   0.46  0.69  0.32    1       \nCmpd5   0.57  0.44  0.41    0.51    1   \nCmpd6   0.41  0.79  0.33    0.56    0.43    1\n</code></pre>\n\n<p>I would like to order it from highest to lowest based on the index no matter if it is repeated, what I say would look like this:</p>\n\n<p>The maximum value corresponds to Cmpd6 = 0.79, followed by Cmpd4 = 0.69 ... at some point Cmpd6 = 0.56 which I would like to leave the list like this:</p>\n\n<pre><code>Cmpd6 = 0.79\nCmpd4 = 0.69\nCmpdx = x\nCmpd6 = 0.56\n</code></pre>\n\n<p>This with each value of the array, no matter how many times the indexes are repeated, I was trying with <code>.sort_index (axis = 1)</code> but it does not produce any of this, I also tried <code>.ravel ()</code> but it does not give me the indexes. How can i fix this?</p>\n\n<p>Thanks!</p>\n\n<p>My solution:</p>\n\n<pre><code>df = df.where(np.tril(np.ones(df.shape), -1).astype(np.bool))\ndf = df.stack().reset_index().drop(\"level_1\", axis=1).sort_values(by=0, ascending=False)\n</code></pre>\n",
        "formatted_input": {
            "qid": 60897173,
            "link": "https://stackoverflow.com/questions/60897173/sort-pandas-dataframe-by-index",
            "question": {
                "title": "Sort pandas dataframe by index",
                "ques_desc": "I have the following pandas dataframe: I would like to order it from highest to lowest based on the index no matter if it is repeated, what I say would look like this: The maximum value corresponds to Cmpd6 = 0.79, followed by Cmpd4 = 0.69 ... at some point Cmpd6 = 0.56 which I would like to leave the list like this: This with each value of the array, no matter how many times the indexes are repeated, I was trying with but it does not produce any of this, I also tried but it does not give me the indexes. How can i fix this? Thanks! My solution: "
            },
            "io": [
                "    Cmpd1   Cmpd2   Cmpd3   Cmpd4   Cmpd5   Cmpd6\nCmpd1   1                   \nCmpd2   0.4   1             \nCmpd3   0.6   0.3   1           \nCmpd4   0.46  0.69  0.32    1       \nCmpd5   0.57  0.44  0.41    0.51    1   \nCmpd6   0.41  0.79  0.33    0.56    0.43    1\n",
                "Cmpd6 = 0.79\nCmpd4 = 0.69\nCmpdx = x\nCmpd6 = 0.56\n"
            ],
            "answer": {
                "ans_desc": "I assume that each empty element in the source df contain actualy an empty string (not NaN, as these would be printed just as NaN). I also noticed that you want to keey only values < 1. To get your result, run: For your data the result is: Other possible solution: ",
                "code": [
                    "s = df.stack()\ns[s.apply(lambda x: type(x) is not str and x < 1)]\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\\\n    .astype(float)\n",
                    "s = df.replace(r'^\\s*$', np.nan, regex=True).stack()\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\ns[s < 1]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "group-by"
        ],
        "owner": {
            "reputation": 5739,
            "user_id": 8741356,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/W1hSm.jpg?s=128&g=1",
            "display_name": "Tomasz Bartkowiak",
            "link": "https://stackoverflow.com/users/8741356/tomasz-bartkowiak"
        },
        "is_answered": true,
        "view_count": 117,
        "accepted_answer_id": 60877327,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1585336658,
        "creation_date": 1585255166,
        "last_edit_date": 1585317433,
        "question_id": 60875750,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60875750/pandas-groupby-and-change-reassign-one-element",
        "title": "Pandas groupby and change/reassign one element",
        "body": "<p>I want to <code>groupby</code> given dataframe, and then, for each group, for given column <code>p</code> overwrite the value of its last element (of each group) to <code>1 - sum(p[:-1])</code> (with <code>sum</code> being the sum of all the elements apart from the last one).</p>\n\n<p>Note that after performing the operation, the sum of all values in <code>p</code> for each group is equal to <code>1</code>.</p>\n\n<p>For example, for this input dataframe (grouping by <code>c1</code> and <code>c2</code>):</p>\n\n<pre><code>  c1 c2    p\n0  x  a  0.4\n1  y  a  0.2\n2  x  a  0.3\n3  y  b  0.6\n</code></pre>\n\n<p>the expected output would be:</p>\n\n<pre><code>  c1 c2    p\n0  x  a  0.4\n1  y  a  1.0\n2  x  a  0.6\n3  y  b  1.0\n</code></pre>\n\n<p>I managed to perform the operation using <code>for</code> loop:</p>\n\n<pre><code>for _, g in df.groupby(['c1', 'c2']):\n    df.loc[g.tail(1).index, 'p'] = 1 - g['p'][:-1].sum()\n</code></pre>\n\n<p>but I am looking for more elegant way of doing this, <strong>without explicitly looping through each group</strong>.</p>\n\n<p>I tried this:</p>\n\n<pre><code>&gt;&gt;&gt; df.loc[df.groupby(['c1', 'c2']).tail(1).index, 'p']\n\n1    0.2\n2    0.3\n3    0.6\n\n&gt;&gt;&gt; 1 - df.groupby(['c1', 'c2']).apply(lambda x: x.iloc[:-1].sum())['p']\n\nc1  c2\nx   a     0.6\ny   a     1.0\n    b     1.0\n</code></pre>\n\n<p>But I don't really know how to assemble those outputs given that their indices differ.</p>\n",
        "answer_body": "<p>Here's a possible one-line solution:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df.groupby(['c1', 'c2']).apply(\n        lambda x: x.assign(p=x['p'][:-1].tolist()+[1 - x.iloc[:-1].sum()['p']])\n).reset_index(level=[0,1], drop=True)\n</code></pre>\n\n<p>To make the above code more readable, here is a nearly equivalent version of my one-line solution:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def func(row):\n    result = 1 - row.iloc[:-1].sum()['p']\n    row['p'].iloc[-1] = result\n    return row\n\ndf.groupby(['c1', 'c2']).apply(func)\n</code></pre>\n\n<p>With those solutions in mind, I am not entirely sure why you don't want to use the <code>.groupby</code> call in an explicit python for-loop. My hunch is that an explicit python for-loop would be more than adequate, but I don't know your specific use case/data. I would highly recommend doing some speed comparisons using <code>%timeit</code> with your specific data, as I think that will help shed light on which approach you ultimately end up using.</p>\n",
        "question_body": "<p>I want to <code>groupby</code> given dataframe, and then, for each group, for given column <code>p</code> overwrite the value of its last element (of each group) to <code>1 - sum(p[:-1])</code> (with <code>sum</code> being the sum of all the elements apart from the last one).</p>\n\n<p>Note that after performing the operation, the sum of all values in <code>p</code> for each group is equal to <code>1</code>.</p>\n\n<p>For example, for this input dataframe (grouping by <code>c1</code> and <code>c2</code>):</p>\n\n<pre><code>  c1 c2    p\n0  x  a  0.4\n1  y  a  0.2\n2  x  a  0.3\n3  y  b  0.6\n</code></pre>\n\n<p>the expected output would be:</p>\n\n<pre><code>  c1 c2    p\n0  x  a  0.4\n1  y  a  1.0\n2  x  a  0.6\n3  y  b  1.0\n</code></pre>\n\n<p>I managed to perform the operation using <code>for</code> loop:</p>\n\n<pre><code>for _, g in df.groupby(['c1', 'c2']):\n    df.loc[g.tail(1).index, 'p'] = 1 - g['p'][:-1].sum()\n</code></pre>\n\n<p>but I am looking for more elegant way of doing this, <strong>without explicitly looping through each group</strong>.</p>\n\n<p>I tried this:</p>\n\n<pre><code>&gt;&gt;&gt; df.loc[df.groupby(['c1', 'c2']).tail(1).index, 'p']\n\n1    0.2\n2    0.3\n3    0.6\n\n&gt;&gt;&gt; 1 - df.groupby(['c1', 'c2']).apply(lambda x: x.iloc[:-1].sum())['p']\n\nc1  c2\nx   a     0.6\ny   a     1.0\n    b     1.0\n</code></pre>\n\n<p>But I don't really know how to assemble those outputs given that their indices differ.</p>\n",
        "formatted_input": {
            "qid": 60875750,
            "link": "https://stackoverflow.com/questions/60875750/pandas-groupby-and-change-reassign-one-element",
            "question": {
                "title": "Pandas groupby and change/reassign one element",
                "ques_desc": "I want to given dataframe, and then, for each group, for given column overwrite the value of its last element (of each group) to (with being the sum of all the elements apart from the last one). Note that after performing the operation, the sum of all values in for each group is equal to . For example, for this input dataframe (grouping by and ): the expected output would be: I managed to perform the operation using loop: but I am looking for more elegant way of doing this, without explicitly looping through each group. I tried this: But I don't really know how to assemble those outputs given that their indices differ. "
            },
            "io": [
                "  c1 c2    p\n0  x  a  0.4\n1  y  a  0.2\n2  x  a  0.3\n3  y  b  0.6\n",
                "  c1 c2    p\n0  x  a  0.4\n1  y  a  1.0\n2  x  a  0.6\n3  y  b  1.0\n"
            ],
            "answer": {
                "ans_desc": "Here's a possible one-line solution: To make the above code more readable, here is a nearly equivalent version of my one-line solution: With those solutions in mind, I am not entirely sure why you don't want to use the call in an explicit python for-loop. My hunch is that an explicit python for-loop would be more than adequate, but I don't know your specific use case/data. I would highly recommend doing some speed comparisons using with your specific data, as I think that will help shed light on which approach you ultimately end up using. ",
                "code": [
                    "df.groupby(['c1', 'c2']).apply(\n        lambda x: x.assign(p=x['p'][:-1].tolist()+[1 - x.iloc[:-1].sum()['p']])\n).reset_index(level=[0,1], drop=True)\n",
                    "def func(row):\n    result = 1 - row.iloc[:-1].sum()['p']\n    row['p'].iloc[-1] = result\n    return row\n\ndf.groupby(['c1', 'c2']).apply(func)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "multiple-columns"
        ],
        "owner": {
            "reputation": 543,
            "user_id": 12681165,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mCG7nUFtQ7l1e8n4UEatDeRxxYDV0T_zCyvXEEr=k-s128",
            "display_name": "ybin",
            "link": "https://stackoverflow.com/users/12681165/ybin"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 60825897,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1585031553,
        "creation_date": 1585030559,
        "last_edit_date": 1585031002,
        "question_id": 60825821,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60825821/merge-columns-with-have-n",
        "title": "Merge columns with have \\n",
        "body": "<p>ex)</p>\n\n<pre><code>   C1  C2  C3  C4  C5  C6\n0  A   B   nan C   A   nan\n1  B   C   D   nan B   nan\n2  D   E   F   nan C   nan\n3  nan nan A   nan nan B\n</code></pre>\n\n<p>I'm merging columns, but I want to give '\\n\\n' in the merging process.</p>\n\n<p>so output what I want</p>\n\n<pre><code>   C\n0  A  \n\n   B  \n\n   C\n\n   A\n1  B  \n\n   C  \n\n   D\n\n   B\n2  D  \n\n   E  \n\n   F\n\n   C\n3. A\n\n   B\n</code></pre>\n\n<p>I want 'nan' to drop.</p>\n\n<p>I tried</p>\n\n<pre><code>df['merge'] = df['C1'].map(str) + '\\n\\n' + tt['C2'].map(str) + '\\n\\n' + tt['C3'].map(str) + '\\n\\n' + df['C4'].map(str)\n</code></pre>\n\n<p>However, this includes all nan values.</p>\n\n<p>thank you for reading.</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>DataFrame.stack</code></a> for Series, misisng values are removed, so you can aggregate <code>join</code>:</p>\n\n<pre><code>df['merge'] = df.stack().groupby(level=0).agg('\\n\\n'.join)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').stack().groupby(level=0).agg('\\n\\n'.join)\n</code></pre>\n\n<p>Or remove missing values by join per rows by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dropna.html\" rel=\"nofollow noreferrer\"><code>Series.dropna</code></a>:</p>\n\n<pre><code>df['merge'] = df.apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n    C1   C2   C3   C4   C5   C6             merge\n0    A    B  NaN    C    A  NaN  A\\n\\nB\\n\\nC\\n\\nA\n1    B    C    D  NaN    B  NaN  B\\n\\nC\\n\\nD\\n\\nB\n2    D    E    F  NaN    C  NaN  D\\n\\nE\\n\\nF\\n\\nC\n3  NaN  NaN    A  NaN  NaN    B            A\\n\\nB\n</code></pre>\n",
        "question_body": "<p>ex)</p>\n\n<pre><code>   C1  C2  C3  C4  C5  C6\n0  A   B   nan C   A   nan\n1  B   C   D   nan B   nan\n2  D   E   F   nan C   nan\n3  nan nan A   nan nan B\n</code></pre>\n\n<p>I'm merging columns, but I want to give '\\n\\n' in the merging process.</p>\n\n<p>so output what I want</p>\n\n<pre><code>   C\n0  A  \n\n   B  \n\n   C\n\n   A\n1  B  \n\n   C  \n\n   D\n\n   B\n2  D  \n\n   E  \n\n   F\n\n   C\n3. A\n\n   B\n</code></pre>\n\n<p>I want 'nan' to drop.</p>\n\n<p>I tried</p>\n\n<pre><code>df['merge'] = df['C1'].map(str) + '\\n\\n' + tt['C2'].map(str) + '\\n\\n' + tt['C3'].map(str) + '\\n\\n' + df['C4'].map(str)\n</code></pre>\n\n<p>However, this includes all nan values.</p>\n\n<p>thank you for reading.</p>\n",
        "formatted_input": {
            "qid": 60825821,
            "link": "https://stackoverflow.com/questions/60825821/merge-columns-with-have-n",
            "question": {
                "title": "Merge columns with have \\n",
                "ques_desc": "ex) I'm merging columns, but I want to give '\\n\\n' in the merging process. so output what I want I want 'nan' to drop. I tried However, this includes all nan values. thank you for reading. "
            },
            "io": [
                "   C1  C2  C3  C4  C5  C6\n0  A   B   nan C   A   nan\n1  B   C   D   nan B   nan\n2  D   E   F   nan C   nan\n3  nan nan A   nan nan B\n",
                "   C\n0  A  \n\n   B  \n\n   C\n\n   A\n1  B  \n\n   C  \n\n   D\n\n   B\n2  D  \n\n   E  \n\n   F\n\n   C\n3. A\n\n   B\n"
            ],
            "answer": {
                "ans_desc": "Use for Series, misisng values are removed, so you can aggregate : Or remove missing values by join per rows by : ",
                "code": [
                    "df['merge'] = df.stack().groupby(level=0).agg('\\n\\n'.join)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').stack().groupby(level=0).agg('\\n\\n'.join)\n",
                    "df['merge'] = df.apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 7766063,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b4a66679375bc8020737b381440659bf?s=128&d=identicon&r=PG&f=1",
            "display_name": "table_101",
            "link": "https://stackoverflow.com/users/7766063/table-101"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 60764393,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1584649525,
        "creation_date": 1584643061,
        "last_edit_date": 1584646730,
        "question_id": 60763203,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60763203/how-to-get-absolute-difference-amongst-all-the-values-of-a-column-with-each-othe",
        "title": "How to get absolute difference amongst all the values of a column with each other?",
        "body": "<p>I am trying to find difference among-st all values in key column keeping these 4 digit code as my index value.\nI tried using pivot for this operation but failed.\nIt would be really helpful if I can get the approach for this presentation. </p>\n\n<p><strong>df1</strong></p>\n\n<pre><code>Name | Key | 1001 | 1002 | 1003 |\nAbb    AB      5      8      10     \nBaa    BA     10     11      33  \nCbb    CB     12     40      90  \n</code></pre>\n\n<p><strong>Expected df</strong></p>\n\n<pre><code>Code  | Key | AB | BA | CB |\n1001    AB     0   5    7\n1001    BA     5   0    2\n1001    CB     7   2    0\n1002\n.\n.\n.\n1003\n</code></pre>\n",
        "answer_body": "<p>Not the best solution though.</p>\n\n<p><strong>df:</strong></p>\n\n<pre><code>    Name    Key 1001 1002 1003\n0   Abb     AB  5    8    10\n1   Baa     BA  10   11   33\n2   Cbb     CB  12   40   90\n</code></pre>\n\n<hr>\n\n<pre><code>df1 = df.set_index(['Name','Key']).T\n</code></pre>\n\n<p><strong>df1:</strong></p>\n\n<pre><code>Name    Abb Baa Cbb\nKey     AB  BA  CB\n1001    5   10  12\n1002    8   11  40\n1003    10  33  90\n</code></pre>\n\n<hr>\n\n<pre><code>df2 = df1.loc[np.repeat(df1.index.values,len(df1.columns))] \ndf2.columns = df2.columns.droplevel(0)\n</code></pre>\n\n<p><strong>df2:</strong></p>\n\n<pre><code>Key     AB  BA  CB\n1001    5   10  12\n1001    5   10  12\n1001    5   10  12\n1002    8   11  40\n1002    8   11  40\n1002    8   11  40\n1003    10  33  90\n1003    10  33  90\n1003    10  33  90\n</code></pre>\n\n<hr>\n\n<pre><code>x = df1.unstack().swaplevel(0,2).sort_index()\n</code></pre>\n\n<p><strong>x:</strong></p>\n\n<pre><code>      Key  Name\n1001  AB   Abb      5\n      BA   Baa     10\n      CB   Cbb     12\n1002  AB   Abb      8\n      BA   Baa     11\n      CB   Cbb     40\n1003  AB   Abb     10\n      BA   Baa     33\n      CB   Cbb     90\ndtype: int64\n</code></pre>\n\n<hr>\n\n<pre><code>df3 = df2.sub(x.values, axis=0).abs()\n</code></pre>\n\n<p><strong>df3:</strong></p>\n\n<pre><code>Key     AB  BA  CB\n1001    0   5   7\n1001    5   0   2\n1001    7   2   0\n1002    0   3   32\n1002    3   0   29\n1002    32  29  0\n1003    0   23  80\n1003    23  0   57\n1003    80  57  0\n</code></pre>\n",
        "question_body": "<p>I am trying to find difference among-st all values in key column keeping these 4 digit code as my index value.\nI tried using pivot for this operation but failed.\nIt would be really helpful if I can get the approach for this presentation. </p>\n\n<p><strong>df1</strong></p>\n\n<pre><code>Name | Key | 1001 | 1002 | 1003 |\nAbb    AB      5      8      10     \nBaa    BA     10     11      33  \nCbb    CB     12     40      90  \n</code></pre>\n\n<p><strong>Expected df</strong></p>\n\n<pre><code>Code  | Key | AB | BA | CB |\n1001    AB     0   5    7\n1001    BA     5   0    2\n1001    CB     7   2    0\n1002\n.\n.\n.\n1003\n</code></pre>\n",
        "formatted_input": {
            "qid": 60763203,
            "link": "https://stackoverflow.com/questions/60763203/how-to-get-absolute-difference-amongst-all-the-values-of-a-column-with-each-othe",
            "question": {
                "title": "How to get absolute difference amongst all the values of a column with each other?",
                "ques_desc": "I am trying to find difference among-st all values in key column keeping these 4 digit code as my index value. I tried using pivot for this operation but failed. It would be really helpful if I can get the approach for this presentation. df1 Expected df "
            },
            "io": [
                "Name | Key | 1001 | 1002 | 1003 |\nAbb    AB      5      8      10     \nBaa    BA     10     11      33  \nCbb    CB     12     40      90  \n",
                "Code  | Key | AB | BA | CB |\n1001    AB     0   5    7\n1001    BA     5   0    2\n1001    CB     7   2    0\n1002\n.\n.\n.\n1003\n"
            ],
            "answer": {
                "ans_desc": "Not the best solution though. df: df1: df2: x: df3: ",
                "code": [
                    "df2 = df1.loc[np.repeat(df1.index.values,len(df1.columns))] \ndf2.columns = df2.columns.droplevel(0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2005,
            "user_id": 3879858,
            "user_type": "registered",
            "accept_rate": 74,
            "profile_image": "https://www.gravatar.com/avatar/50664a4ca094f147d526e739406e6726?s=128&d=identicon&r=PG&f=1",
            "display_name": "s900n",
            "link": "https://stackoverflow.com/users/3879858/s900n"
        },
        "is_answered": true,
        "view_count": 2648,
        "accepted_answer_id": 46488708,
        "answer_count": 3,
        "score": 6,
        "last_activity_date": 1584649027,
        "creation_date": 1506686987,
        "last_edit_date": 1584649027,
        "question_id": 46488530,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46488530/python-pandas-dataframe-make-whole-row-nan-according-to-condition",
        "title": "Python pandas.DataFrame: Make whole row NaN according to condition",
        "body": "<p>I want to make the whole row NaN according to a condition, based on a column. For example, if <code>B &gt; 5</code>, I want to make the whole row NaN.</p>\n\n<p>Unprocessed data frame looks like this:</p>\n\n<pre><code>   A  B\n0  1  4\n1  3  5\n2  4  6\n3  8  7\n</code></pre>\n\n<p>Make whole row NaN, if <code>B &gt; 5</code>:</p>\n\n<pre><code>     A    B\n0  1.0  4.0\n1  3.0  5.0\n2  NaN  NaN\n3  NaN  NaN\n</code></pre>\n\n<p>Thank you. </p>\n",
        "answer_body": "<p>You can also use <code>df.loc[df.B &gt; 5, :] = np.nan</code> </p>\n\n<hr>\n\n<p>Example </p>\n\n<pre><code>In [14]: df\nOut[14]: \n   A  B\n0  1  4\n1  3  5\n2  4  6\n3  8  7\n\nIn [15]: df.loc[df.B &gt; 5, :] = np.nan \n\nIn [16]: df\nOut[16]: \n     A    B\n0  1.0  4.0\n1  3.0  5.0\n2  NaN  NaN\n3  NaN  NaN\n</code></pre>\n\n<p>in human language <code>df.loc[df.B &gt; 5, :] = np.nan</code> can be translated to: </p>\n\n<blockquote>\n  <p>assign <code>np.nan</code> to any column (<code>:</code>) of the dataframe ( <code>df</code> ) where the\n  condition <code>df.B &gt; 5</code> is valid.</p>\n</blockquote>\n",
        "question_body": "<p>I want to make the whole row NaN according to a condition, based on a column. For example, if <code>B &gt; 5</code>, I want to make the whole row NaN.</p>\n\n<p>Unprocessed data frame looks like this:</p>\n\n<pre><code>   A  B\n0  1  4\n1  3  5\n2  4  6\n3  8  7\n</code></pre>\n\n<p>Make whole row NaN, if <code>B &gt; 5</code>:</p>\n\n<pre><code>     A    B\n0  1.0  4.0\n1  3.0  5.0\n2  NaN  NaN\n3  NaN  NaN\n</code></pre>\n\n<p>Thank you. </p>\n",
        "formatted_input": {
            "qid": 46488530,
            "link": "https://stackoverflow.com/questions/46488530/python-pandas-dataframe-make-whole-row-nan-according-to-condition",
            "question": {
                "title": "Python pandas.DataFrame: Make whole row NaN according to condition",
                "ques_desc": "I want to make the whole row NaN according to a condition, based on a column. For example, if , I want to make the whole row NaN. Unprocessed data frame looks like this: Make whole row NaN, if : Thank you. "
            },
            "io": [
                "   A  B\n0  1  4\n1  3  5\n2  4  6\n3  8  7\n",
                "     A    B\n0  1.0  4.0\n1  3.0  5.0\n2  NaN  NaN\n3  NaN  NaN\n"
            ],
            "answer": {
                "ans_desc": "You can also use Example in human language can be translated to: assign to any column () of the dataframe ( ) where the condition is valid. ",
                "code": [
                    "df.loc[df.B > 5, :] = np.nan",
                    "df.loc[df.B > 5, :] = np.nan"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 27,
            "user_id": 4526139,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://graph.facebook.com/606478372818450/picture?type=large",
            "display_name": "Hunter Durnford",
            "link": "https://stackoverflow.com/users/4526139/hunter-durnford"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 60728103,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1584487829,
        "creation_date": 1584468718,
        "last_edit_date": 1584487829,
        "question_id": 60728016,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60728016/need-help-getting-the-frequency-of-each-number-in-a-pandas-dataframe",
        "title": "Need help getting the frequency of each number in a pandas dataframe",
        "body": "<p>I am trying to find a simple way of converting a pandas dataframe into another dataframe with frequency of each feature. I'll provide an example of what I'm trying to do below</p>\n\n<p>Current dataframe example (feature labels are just index values here):</p>\n\n<pre><code>   0   1   2   3   4   ...   n\n0  2   3   1   4   2         ~\n1  4   3   4   3   2         ~\n2  2   3   2   3   2         ~\n3  1   3   0   3   2         ~\n...\nm  ~   ~   ~   ~   ~         ~\n</code></pre>\n\n<p>Dataframe I would like to convert this to:</p>\n\n<pre><code>   0   1   2   3   4   ...   n\n0  0   1   2   1   1         ~\n1  0   0   1   2   2         ~\n2  0   0   3   2   0         ~\n3  1   1   1   2   0         ~\n...\nm  ~   ~   ~   ~   ~         ~\n</code></pre>\n\n<p>As you can see, the column label corresponds to the possible numbers within the dataframe and each frequency of that number per row is put into that specific feature for the row in question. Is there a simple way to do this with python? I have a large dataframe that I am trying to transform into a dataframe of frequencies for feature selection.</p>\n\n<p>If any more information is needed I will update my post. </p>\n",
        "answer_body": "<p>Use <code>pd.value_counts</code> with <code>apply</code>:</p>\n\n<pre><code>df.apply(pd.value_counts, axis=1).fillna(0)\n\n     0    1    2    3    4\n0  0.0  1.0  2.0  1.0  1.0\n1  0.0  0.0  1.0  2.0  2.0\n2  0.0  0.0  3.0  2.0  0.0\n3  1.0  1.0  1.0  2.0  0.0\n</code></pre>\n\n<p><strong>Alternative</strong> <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html#pandas.DataFrame.melt\" rel=\"nofollow noreferrer\"><code>DataFrame.melt</code></a> with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html\" rel=\"nofollow noreferrer\"><code>pd.crosstab</code></a></p>\n\n<pre><code>df2 = df.T.melt()\npd.crosstab(df2['variable'], df2['value'])\n</code></pre>\n",
        "question_body": "<p>I am trying to find a simple way of converting a pandas dataframe into another dataframe with frequency of each feature. I'll provide an example of what I'm trying to do below</p>\n\n<p>Current dataframe example (feature labels are just index values here):</p>\n\n<pre><code>   0   1   2   3   4   ...   n\n0  2   3   1   4   2         ~\n1  4   3   4   3   2         ~\n2  2   3   2   3   2         ~\n3  1   3   0   3   2         ~\n...\nm  ~   ~   ~   ~   ~         ~\n</code></pre>\n\n<p>Dataframe I would like to convert this to:</p>\n\n<pre><code>   0   1   2   3   4   ...   n\n0  0   1   2   1   1         ~\n1  0   0   1   2   2         ~\n2  0   0   3   2   0         ~\n3  1   1   1   2   0         ~\n...\nm  ~   ~   ~   ~   ~         ~\n</code></pre>\n\n<p>As you can see, the column label corresponds to the possible numbers within the dataframe and each frequency of that number per row is put into that specific feature for the row in question. Is there a simple way to do this with python? I have a large dataframe that I am trying to transform into a dataframe of frequencies for feature selection.</p>\n\n<p>If any more information is needed I will update my post. </p>\n",
        "formatted_input": {
            "qid": 60728016,
            "link": "https://stackoverflow.com/questions/60728016/need-help-getting-the-frequency-of-each-number-in-a-pandas-dataframe",
            "question": {
                "title": "Need help getting the frequency of each number in a pandas dataframe",
                "ques_desc": "I am trying to find a simple way of converting a pandas dataframe into another dataframe with frequency of each feature. I'll provide an example of what I'm trying to do below Current dataframe example (feature labels are just index values here): Dataframe I would like to convert this to: As you can see, the column label corresponds to the possible numbers within the dataframe and each frequency of that number per row is put into that specific feature for the row in question. Is there a simple way to do this with python? I have a large dataframe that I am trying to transform into a dataframe of frequencies for feature selection. If any more information is needed I will update my post. "
            },
            "io": [
                "   0   1   2   3   4   ...   n\n0  2   3   1   4   2         ~\n1  4   3   4   3   2         ~\n2  2   3   2   3   2         ~\n3  1   3   0   3   2         ~\n...\nm  ~   ~   ~   ~   ~         ~\n",
                "   0   1   2   3   4   ...   n\n0  0   1   2   1   1         ~\n1  0   0   1   2   2         ~\n2  0   0   3   2   0         ~\n3  1   1   1   2   0         ~\n...\nm  ~   ~   ~   ~   ~         ~\n"
            ],
            "answer": {
                "ans_desc": "Use with : Alternative with ",
                "code": [
                    "df2 = df.T.melt()\npd.crosstab(df2['variable'], df2['value'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 73,
            "user_id": 12054441,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b8acfbb9d703668959dd3d1eef500678?s=128&d=identicon&r=PG&f=1",
            "display_name": "Pranav101py",
            "link": "https://stackoverflow.com/users/12054441/pranav101py"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 60709770,
        "answer_count": 3,
        "score": -1,
        "last_activity_date": 1584376346,
        "creation_date": 1584375408,
        "question_id": 60709527,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60709527/removing-duplicate-values-from-a-cell-of-dataframein-python",
        "title": "Removing Duplicate values from a Cell of DataFramein python",
        "body": "<p>DataFrame</p>\n\n<pre><code>ID                                                                         Source\n 1                     [192.168.1.121, 10.1.161.10, 192.168.1.121, 192.168.1.121]\n 2          [192.168.1.121, 10.1.161.10, 10.1.161.10, 10.1.161.10, 192.168.1.121]\n 3                                  [192.168.1.121, 192.168.1.121, 192.168.1.121]\n 4                         [10.1.161.10, 192.168.1.121, 10.1.161.10, 10.1.161.10]\n</code></pre>\n\n<p>Output I want</p>\n\n<pre><code>ID                                Source\n 1            192.168.1.121, 10.1.161.10\n 2            192.168.1.121, 10.1.161.10\n 3                         192.168.1.121\n 4            10.1.161.10, 192.168.1.121\n</code></pre>\n\n<p>Any Help will be Appreciated</p>\n",
        "answer_body": "<p>Try using <code>pd.unique</code>: </p>\n\n<pre><code>df['source'] = df['source'].apply(\nlambda x: ', '.join(pd.unique(x)),\n)\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>ID                                Source\n 1            192.168.1.121, 10.1.161.10\n 2            192.168.1.121, 10.1.161.10\n 3                         192.168.1.121\n 4            10.1.161.10, 192.168.1.121\n</code></pre>\n",
        "question_body": "<p>DataFrame</p>\n\n<pre><code>ID                                                                         Source\n 1                     [192.168.1.121, 10.1.161.10, 192.168.1.121, 192.168.1.121]\n 2          [192.168.1.121, 10.1.161.10, 10.1.161.10, 10.1.161.10, 192.168.1.121]\n 3                                  [192.168.1.121, 192.168.1.121, 192.168.1.121]\n 4                         [10.1.161.10, 192.168.1.121, 10.1.161.10, 10.1.161.10]\n</code></pre>\n\n<p>Output I want</p>\n\n<pre><code>ID                                Source\n 1            192.168.1.121, 10.1.161.10\n 2            192.168.1.121, 10.1.161.10\n 3                         192.168.1.121\n 4            10.1.161.10, 192.168.1.121\n</code></pre>\n\n<p>Any Help will be Appreciated</p>\n",
        "formatted_input": {
            "qid": 60709527,
            "link": "https://stackoverflow.com/questions/60709527/removing-duplicate-values-from-a-cell-of-dataframein-python",
            "question": {
                "title": "Removing Duplicate values from a Cell of DataFramein python",
                "ques_desc": "DataFrame Output I want Any Help will be Appreciated "
            },
            "io": [
                "ID                                                                         Source\n 1                     [192.168.1.121, 10.1.161.10, 192.168.1.121, 192.168.1.121]\n 2          [192.168.1.121, 10.1.161.10, 10.1.161.10, 10.1.161.10, 192.168.1.121]\n 3                                  [192.168.1.121, 192.168.1.121, 192.168.1.121]\n 4                         [10.1.161.10, 192.168.1.121, 10.1.161.10, 10.1.161.10]\n",
                "ID                                Source\n 1            192.168.1.121, 10.1.161.10\n 2            192.168.1.121, 10.1.161.10\n 3                         192.168.1.121\n 4            10.1.161.10, 192.168.1.121\n"
            ],
            "answer": {
                "ans_desc": "Try using : Output: ",
                "code": [
                    "df['source'] = df['source'].apply(\nlambda x: ', '.join(pd.unique(x)),\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 8396360,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b13501753541c7f34ba5f26845561430?s=128&d=identicon&r=PG&f=1",
            "display_name": "Arkansin",
            "link": "https://stackoverflow.com/users/8396360/arkansin"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 60645058,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1583968820,
        "creation_date": 1583963190,
        "last_edit_date": 1583967061,
        "question_id": 60644834,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60644834/pandas-search-if-full-rows-of-a-large-df-contain-template-rows-from-a-another-sm",
        "title": "Pandas search if full rows of a large df contain template rows from a another smaller df?",
        "body": "<p>I have a large df (df1) with binary outputs in each column like so:</p>\n\n<pre><code>df1:\n\n  a b c d\n1 1 0 1 0\n2 0 0 0 0\n3 0 1 0 1\n4 1 1 0 0\n5 1 0 0 0\n6 1 0 1 1\n...\n</code></pre>\n\n<p>I also have another smaller df (df2) with some \"template\" rows and I want to check if df1s rows contain. Templates looks like this:</p>\n\n<pre><code>df2:\n\n  a b c d\n1 1 0 1 0\n2 1 1 1 1\n3 0 0 0 1\n4 1 1 0 0\n</code></pre>\n\n<p>What I'm trying to do is to search the large df efficiently for these small number of templates, so in this example, rows 1, 3, 4, 6 would match, but 2 and 5 would not match. I want any row in the large df which has any extra 1s to pass the test (i.e. a template row is there but it also has some extra 1s in that row).</p>\n\n<p>I know that I could just have a nested loop and iterate over all the rows of the large and small dfs and match rows as np.arrays, but this seems like an extremely inefficient way to do this. I'm wondering if there are any non-iterative pd-based solutions to this problem?</p>\n\n<p>Thank you so much!</p>\n\n<p>Minor functionality edit: Along with searching and matching, I'm also trying to retain a list of which template row from df2 each row in df1 matched with so I can do statistics on how many templates show up in the large df and which ones they are. This is one of the reasons why this answer(<a href=\"https://stackoverflow.com/questions/29464234/compare-python-pandas-dataframes-for-matching-rows\">Compare Python Pandas DataFrames for matching rows</a>) doesn't work.</p>\n",
        "answer_body": "<p>This logic will tell you where there are matches based on your requirements. Here I just created a new column in the original DF, but you'd probably want to create a third DF and keep adding columns to it for each test.  Then you can just sum up the columns/rows to get your totals:</p>\n\n<p>df1</p>\n\n<pre><code>   a  b  c  d\n1  1  0  1  0\n2  0  0  0  0\n3  0  1  0  1\n4  1  1  0  0\n</code></pre>\n\n<p>df2</p>\n\n<pre><code>   a  b  c  d\n1  1  0  1  0\n2  1  1  1  1\n3  0  0  0  1\n4  1  1  0  0\n</code></pre>\n\n<p>Logic</p>\n\n<pre><code>t_match =[]\nfor index, row in df2.iterrows():\n    t_match.append(((df1-row) &gt;= 0).all(axis=1).sum())\n</code></pre>\n\n<p>Output</p>\n\n<pre><code>t_match\n[1, 0, 1, 1]\n</code></pre>\n",
        "question_body": "<p>I have a large df (df1) with binary outputs in each column like so:</p>\n\n<pre><code>df1:\n\n  a b c d\n1 1 0 1 0\n2 0 0 0 0\n3 0 1 0 1\n4 1 1 0 0\n5 1 0 0 0\n6 1 0 1 1\n...\n</code></pre>\n\n<p>I also have another smaller df (df2) with some \"template\" rows and I want to check if df1s rows contain. Templates looks like this:</p>\n\n<pre><code>df2:\n\n  a b c d\n1 1 0 1 0\n2 1 1 1 1\n3 0 0 0 1\n4 1 1 0 0\n</code></pre>\n\n<p>What I'm trying to do is to search the large df efficiently for these small number of templates, so in this example, rows 1, 3, 4, 6 would match, but 2 and 5 would not match. I want any row in the large df which has any extra 1s to pass the test (i.e. a template row is there but it also has some extra 1s in that row).</p>\n\n<p>I know that I could just have a nested loop and iterate over all the rows of the large and small dfs and match rows as np.arrays, but this seems like an extremely inefficient way to do this. I'm wondering if there are any non-iterative pd-based solutions to this problem?</p>\n\n<p>Thank you so much!</p>\n\n<p>Minor functionality edit: Along with searching and matching, I'm also trying to retain a list of which template row from df2 each row in df1 matched with so I can do statistics on how many templates show up in the large df and which ones they are. This is one of the reasons why this answer(<a href=\"https://stackoverflow.com/questions/29464234/compare-python-pandas-dataframes-for-matching-rows\">Compare Python Pandas DataFrames for matching rows</a>) doesn't work.</p>\n",
        "formatted_input": {
            "qid": 60644834,
            "link": "https://stackoverflow.com/questions/60644834/pandas-search-if-full-rows-of-a-large-df-contain-template-rows-from-a-another-sm",
            "question": {
                "title": "Pandas search if full rows of a large df contain template rows from a another smaller df?",
                "ques_desc": "I have a large df (df1) with binary outputs in each column like so: I also have another smaller df (df2) with some \"template\" rows and I want to check if df1s rows contain. Templates looks like this: What I'm trying to do is to search the large df efficiently for these small number of templates, so in this example, rows 1, 3, 4, 6 would match, but 2 and 5 would not match. I want any row in the large df which has any extra 1s to pass the test (i.e. a template row is there but it also has some extra 1s in that row). I know that I could just have a nested loop and iterate over all the rows of the large and small dfs and match rows as np.arrays, but this seems like an extremely inefficient way to do this. I'm wondering if there are any non-iterative pd-based solutions to this problem? Thank you so much! Minor functionality edit: Along with searching and matching, I'm also trying to retain a list of which template row from df2 each row in df1 matched with so I can do statistics on how many templates show up in the large df and which ones they are. This is one of the reasons why this answer(Compare Python Pandas DataFrames for matching rows) doesn't work. "
            },
            "io": [
                "df1:\n\n  a b c d\n1 1 0 1 0\n2 0 0 0 0\n3 0 1 0 1\n4 1 1 0 0\n5 1 0 0 0\n6 1 0 1 1\n...\n",
                "df2:\n\n  a b c d\n1 1 0 1 0\n2 1 1 1 1\n3 0 0 0 1\n4 1 1 0 0\n"
            ],
            "answer": {
                "ans_desc": "This logic will tell you where there are matches based on your requirements. Here I just created a new column in the original DF, but you'd probably want to create a third DF and keep adding columns to it for each test. Then you can just sum up the columns/rows to get your totals: df1 df2 Logic Output ",
                "code": [
                    "t_match =[]\nfor index, row in df2.iterrows():\n    t_match.append(((df1-row) >= 0).all(axis=1).sum())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 75,
            "user_id": 3105140,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cfb0129da87368b5ac208e60d96a52b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "SMO",
            "link": "https://stackoverflow.com/users/3105140/smo"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 60585557,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1583654264,
        "creation_date": 1583653012,
        "last_edit_date": 1583653549,
        "question_id": 60585417,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60585417/show-records-contain-multiple-key-words-using-or-or-if-elif-filter-rows",
        "title": "Show records contain multiple key words using OR or if elif (filter rows)",
        "body": "<p>I am trying to show the rows that contain set of key words.</p>\n\n<p>The table look like this </p>\n\n<pre><code>Col0     col1      col2     col3\n1         LD        AN       CC\n2         AB      LD SS      BB\n1         AA      LD AD      CC\n3         LD        AC       NN\n2         FF        UH       BB\n</code></pre>\n\n<p>What I want is to filter this table where the row contain the tow strings (LD and AB) OR (LD and AD) OR (AC) </p>\n\n<p>So I get this result</p>\n\n<pre><code>Col0     col1      col2     col3\n2         AB      LD SS      BB\n1         AA      LD AD      CC\n3         LD        AC       NN\n</code></pre>\n\n<p>I tried</p>\n\n<pre><code>count = df.groupby(['Col0','Col3'])['Col3'].transform('size')\ns = df['Col1'] + ' ---- ' + df['Col2'] \n\n#The condition function \ndf = df[count.isin([1,2]) (s.str.contains('LD') &amp; s.str.contains('AB')) | (s.str.contains('LD') &amp; s.str.contains('AD')) | (s.str.contains('LD') &amp; s.str.contains('AC'))]\n</code></pre>\n\n<p>This obviously didn't work , so I tried using the if function:</p>\n\n<pre><code>if s.str.contains('L/D') &amp; s.str.contains('AB'):\n   df = df[count.eq(2)]\nelif s.str.contains('L/D') &amp; s.str.contains('AD'):\n    df = df[count.eq(2)]\nelif s.str.contains('L/D') &amp; s.str.contains('AC'):\n    df = df[count.eq(2)]\n</code></pre>\n\n<p>and using this</p>\n\n<pre><code>if 'L/D' &amp; 'AB' in s:\n    df = df[count.eq(2)]\nelif 'L/D' &amp; 'AD' in s:\n    df = df[count.eq(2)]\nelif 'L/D' &amp; 'AC' in s:\n    df = df[count.eq(2)]\n</code></pre>\n\n<p>They didn't work</p>\n\n<p>So can someone help with what I made wrong </p>\n",
        "answer_body": "<p>This line:</p>\n\n<pre><code>df = df[count.isin([1,2]) (s.str.contains('LD') &amp; s.str.contains('AB'))\n        | (s.str.contains('LD') &amp; s.str.contains('AD'))\n        | (s.str.contains('LD') &amp; s.str.contains('AC'))]\n</code></pre>\n\n<p>contains a syntax error because of <code>count</code>. Remove it and you get the expected result:</p>\n\n<pre><code>df = df[(s.str.contains('LD') &amp; s.str.contains('AB'))\n        | (s.str.contains('LD') &amp; s.str.contains('AD'))\n        | (s.str.contains('LD') &amp; s.str.contains('AC'))]\n</code></pre>\n\n<p>giving:</p>\n\n<pre><code>   Col0 Col1   Col2 Col3\n1     2   AB  LD SS   BB\n2     1   AA  LD AD   CC\n3     3   LD     AC   NN\n</code></pre>\n",
        "question_body": "<p>I am trying to show the rows that contain set of key words.</p>\n\n<p>The table look like this </p>\n\n<pre><code>Col0     col1      col2     col3\n1         LD        AN       CC\n2         AB      LD SS      BB\n1         AA      LD AD      CC\n3         LD        AC       NN\n2         FF        UH       BB\n</code></pre>\n\n<p>What I want is to filter this table where the row contain the tow strings (LD and AB) OR (LD and AD) OR (AC) </p>\n\n<p>So I get this result</p>\n\n<pre><code>Col0     col1      col2     col3\n2         AB      LD SS      BB\n1         AA      LD AD      CC\n3         LD        AC       NN\n</code></pre>\n\n<p>I tried</p>\n\n<pre><code>count = df.groupby(['Col0','Col3'])['Col3'].transform('size')\ns = df['Col1'] + ' ---- ' + df['Col2'] \n\n#The condition function \ndf = df[count.isin([1,2]) (s.str.contains('LD') &amp; s.str.contains('AB')) | (s.str.contains('LD') &amp; s.str.contains('AD')) | (s.str.contains('LD') &amp; s.str.contains('AC'))]\n</code></pre>\n\n<p>This obviously didn't work , so I tried using the if function:</p>\n\n<pre><code>if s.str.contains('L/D') &amp; s.str.contains('AB'):\n   df = df[count.eq(2)]\nelif s.str.contains('L/D') &amp; s.str.contains('AD'):\n    df = df[count.eq(2)]\nelif s.str.contains('L/D') &amp; s.str.contains('AC'):\n    df = df[count.eq(2)]\n</code></pre>\n\n<p>and using this</p>\n\n<pre><code>if 'L/D' &amp; 'AB' in s:\n    df = df[count.eq(2)]\nelif 'L/D' &amp; 'AD' in s:\n    df = df[count.eq(2)]\nelif 'L/D' &amp; 'AC' in s:\n    df = df[count.eq(2)]\n</code></pre>\n\n<p>They didn't work</p>\n\n<p>So can someone help with what I made wrong </p>\n",
        "formatted_input": {
            "qid": 60585417,
            "link": "https://stackoverflow.com/questions/60585417/show-records-contain-multiple-key-words-using-or-or-if-elif-filter-rows",
            "question": {
                "title": "Show records contain multiple key words using OR or if elif (filter rows)",
                "ques_desc": "I am trying to show the rows that contain set of key words. The table look like this What I want is to filter this table where the row contain the tow strings (LD and AB) OR (LD and AD) OR (AC) So I get this result I tried This obviously didn't work , so I tried using the if function: and using this They didn't work So can someone help with what I made wrong "
            },
            "io": [
                "Col0     col1      col2     col3\n1         LD        AN       CC\n2         AB      LD SS      BB\n1         AA      LD AD      CC\n3         LD        AC       NN\n2         FF        UH       BB\n",
                "Col0     col1      col2     col3\n2         AB      LD SS      BB\n1         AA      LD AD      CC\n3         LD        AC       NN\n"
            ],
            "answer": {
                "ans_desc": "This line: contains a syntax error because of . Remove it and you get the expected result: giving: ",
                "code": [
                    "df = df[count.isin([1,2]) (s.str.contains('LD') & s.str.contains('AB'))\n        | (s.str.contains('LD') & s.str.contains('AD'))\n        | (s.str.contains('LD') & s.str.contains('AC'))]\n",
                    "df = df[(s.str.contains('LD') & s.str.contains('AB'))\n        | (s.str.contains('LD') & s.str.contains('AD'))\n        | (s.str.contains('LD') & s.str.contains('AC'))]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "linear-regression"
        ],
        "owner": {
            "reputation": 107,
            "user_id": 9245359,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b435c5ddb42e4a553c580a03038bfd53?s=128&d=identicon&r=PG&f=1",
            "display_name": "NewNY1990",
            "link": "https://stackoverflow.com/users/9245359/newny1990"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 60579040,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1583621027,
        "creation_date": 1583591330,
        "last_edit_date": 1583605965,
        "question_id": 60578553,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60578553/using-values-from-dataframe-for-calculation",
        "title": "Using values from dataframe for calculation",
        "body": "<p>I am selecting dedicated data from a dataframe and would like to make a linear interpolation based on my defined formula:</p>\n\n<pre><code>y = y0 + (y1 - y0) * [(x-x0)/(x1-x0)]\n</code></pre>\n\n<p>I would like to interpolate e.g. between rank 2.0 and 3.0, where the needed rank is 2.5. The calculation looks like the following: </p>\n\n<p>y = -9.080002 + (-9.039993 - (-9.080002))*[(2.5-2)/(3-2)] =  -9.059997500000</p>\n\n<p>where the values are defined in the code as the following:</p>\n\n<p>The code looks like the following:</p>\n\n<pre><code>import pandas as pd\n\n#define path where the Excel File is\npath = 'Desktop'\n#define file name\nfilename = 'Test_Calculation.xlsx'\n#add to full path\nfinal_path = path + '/' + filename\nprint(final_path)\n#define excel worksheet name\nws_name = 'Test_Sheet'\ndf = pd.read_excel(final_path, sheet_name=ws_name)\n\n#sort column DataPoint and inclkude column rank\ndf.sort_values('DataPoint', inplace = True)\ndf['Rank'] = df['DataPoint'].rank() \n\n#define the VaR function\ndef Calc(data):\n\n    Rank = 2.5\n    Rank_Up = 3.0\n    Rank_Down = 2.0\n\n    Calculation = 0.0\n\n    check_int = isinstance(Rank, int)\n    if not check_int:\n        for ind in data.index:\n            if (data['Rank'][ind] in (Rank_Down, Rank_Up)):\n                Calculation = data['DataPoint'][ind] + (data['DataPoint'][ind+1]- data['DataPoint'][ind]) *((Rank-Rank_Down)/(Rank_Up-Rank_Down))\n                print(data['DataPoint'][ind], data['Rank'][ind])\n                print(data['DataPoint'][ind+1], data['Rank'][ind+1])\n                break\n\n    return Calculation\n\n\nCalc_int = Calc(df)\n\nprint(Calc_int)\n</code></pre>\n\n<p>The result looks like the following:</p>\n\n<pre><code>-9.080002000000007 2.0\n-9.360001000000011 1.0\n-9.22000150000001\n</code></pre>\n\n<p>The Excel file looks like the following:</p>\n\n<pre><code>Number  DataPoint\n1   -5.910004\n2   -9.080002\n3   -9.360001\n4   -0.779999\n5   0.389999\n6   0.089996\n7   0.009995\n8   -0.380005\n9   1.139999\n10  2.389999\n11  2.279999\n12  0.089996\n13  -1.220001\n14  -0.960007\n15  -5.419998\n16  -6.410004\n17  -5.029999\n18  -7.529999\n19  -7.479996\n20  -4.580002\n21  -5.589996\n22  -6.339996\n23  -5.570007\n24  -3.520004\n25  -5.490005\n26  -1.860001\n27  -1.589996\n28  -2.470001\n29  -4.350006\n30  -2.630005\n31  -3.800003\n32  -4.949997\n33  -2.770004\n34  -6.300003\n35  -3.220001\n36  -4.949997\n37  -7.320007\n38  -2.110001\n39  -2.020004\n40  -1.460007\n41  -3.300003\n42  -9.039993\n43  -6.529999\n44  -1.149994\n45  2.660004\n46  3.940002\n47  6.009995\n48  6.309998\n49  5.75\n50  5.070007\n51  6.160004\n52  5.509995\n53  6.75\n54  5.119995\n55  6.320007\n56  6.599992\n57  6.302474\n58  7.403336\n59  10.65633\n60  9.634812\n61  9.763731\n62  8.7323\n63  7.760373\n64  8.434767\n65  10.398461\n66  9.029833\n67  11.72744\n68  21.793869\n69  21.089708\n70  19.592145\n71  17.142479\n72  17.221823\n73  17.985487\n74  15.575486\n75  12.501017\n76  11.816696\n77  13.641543\n78  13.373762\n79  13.730799\n80  13.135748\n81  11.320812\n82  11.628258\n83  14.097763\n84  14.712653\n85  15.595325\n86  17.231735\n87  16.349063\n88  15.615165\n89  16.864786\n90  15.793676\n91  15.466391\n92  14.891164\n93  15.615165\n94  17.390421\n95  17.529268\n96  14.315938\n97  13.978741\n98  16.021778\n99  16.99372\n100 18.521035\n101 18.253254\n102 18.778889\n103 18.253254\n104 20.296306\n105 20.583912\n106 18.729313\n107 20.802102\n108 21.912876\n109 22.329416\n110 22.051723\n111 21.635182\n112 21.833533\n113 22.617037\n114 20.861605\n115 19.145853\n116 19.711152\n117 21.258306\n118 19.582233\n119 18.649969\n120 20.365723\n121 21.241765\n122 20.946467\n123 18.603786\n124 21.665015\n125 21.192546\n126 18.05257\n127 17.117471\n128 17.570264\n129 12.766789\n130 11.684033\n131 13.554246\n132 8.150336\n133 10.571764\n134 11.556082\n135 9.754779\n136 9.833526\n137 10.305994\n138 9.794145\n139 9.311839\n140 9.52839\n141 9.498846\n142 7.933785\n143 6.683697\n144 7.983004\n145 10.118971\n146 7.884566\n147 8.071589\n148 6.703387\n149 10.443798\n150 12.048241\n151 10.325685\n152 7.303822\n153 7.303822\n154 4.498525\n155 4.616639\n156 1.860546\n157 5.502519\n158 4.341031\n159 5.335186\n160 8.248774\n161 6.014368\n162 8.652332\n163 6.919939\n164 8.504691\n165 7.963313\n166 9.400424\n167 8.347196\n168 10.256775\n169 12.245101\n170 10.355213\n171 11.083599\n172 11.31984\n173 9.646503\n174 8.435796\n175 7.884566\n176 6.034059\n177 5.71907\n178 6.073425\n179 3.307494\n180 4.262284\n181 4.350876\n182 3.422291\n183 4.898252\n184 1.174128\n185 -0.14545\n186 1.975638\n187 -1.865776\n188 0.206436\n189 2.835809\n190 2.356848\n191 1.88767\n192 -1.337954\n193 3.412507\n194 3.598227\n195 1.08616\n196 2.288433\n197 3.275663\n198 2.914007\n199 5.523827\n200 4.849376\n201 1.555337\n202 2.552352\n203 5.914805\n204 8.407334\n205 6.823852\n206 6.442642\n207 6.892266\n208 8.681022\n209 8.172753\n210 8.583285\n211 7.801313\n212 4.937344\n213 4.12605\n214 -1.494351\n215 -2.00262\n216 -0.389816\n217 1.877901\n218 7.439657\n219 6.892266\n220 9.31638\n221 8.211845\n222 9.590068\n223 10.352487\n224 10.811894\n225 9.003586\n226 7.89905\n227 8.514855\n228 8.299812\n229 8.114092\n230 8.495302\n231 7.293029\n232 7.019341\n233 7.791529\n234 8.241167\n235 10.460008\n236 10.460008\n237 9.120876\n238 8.27049\n239 9.619391\n240 9.805111\n241 8.417118\n242 7.820866\n243 7.097551\n244 7.903316\n245 9.563419\n246 9.602245\n247 9.553716\n248 8.68969\n249 8.825604\n250 7.14608\n</code></pre>\n",
        "answer_body": "<p>The IndexError is because of <code>[1:]</code>.</p>\n\n<pre><code>def Calc(data):\n\n    Rank = 2.5\n    Rank_Up = 3.0\n    Rank_Down = 2.0\n\n    Calculation = 0.0\n\n    check_int = isinstance(Rank, int)\n    if not check_int:\n        for ind in data.Rank:\n            if (ind in (Rank_Down, Rank_Up)):\n                index0 = data.Rank.index[ind-1]\n                index1 = data.Rank.index[ind]\n                Calculation = data['DataPoint'][index0] + (data['DataPoint'][index1]- data['DataPoint'][index0]) *((Rank-Rank_Down)/(Rank_Up-Rank_Down))\n                print(data['DataPoint'][index0], ind)\n                print(data['DataPoint'][index1], ind+1)\n                break\n\n    return Calculation\n</code></pre>\n",
        "question_body": "<p>I am selecting dedicated data from a dataframe and would like to make a linear interpolation based on my defined formula:</p>\n\n<pre><code>y = y0 + (y1 - y0) * [(x-x0)/(x1-x0)]\n</code></pre>\n\n<p>I would like to interpolate e.g. between rank 2.0 and 3.0, where the needed rank is 2.5. The calculation looks like the following: </p>\n\n<p>y = -9.080002 + (-9.039993 - (-9.080002))*[(2.5-2)/(3-2)] =  -9.059997500000</p>\n\n<p>where the values are defined in the code as the following:</p>\n\n<p>The code looks like the following:</p>\n\n<pre><code>import pandas as pd\n\n#define path where the Excel File is\npath = 'Desktop'\n#define file name\nfilename = 'Test_Calculation.xlsx'\n#add to full path\nfinal_path = path + '/' + filename\nprint(final_path)\n#define excel worksheet name\nws_name = 'Test_Sheet'\ndf = pd.read_excel(final_path, sheet_name=ws_name)\n\n#sort column DataPoint and inclkude column rank\ndf.sort_values('DataPoint', inplace = True)\ndf['Rank'] = df['DataPoint'].rank() \n\n#define the VaR function\ndef Calc(data):\n\n    Rank = 2.5\n    Rank_Up = 3.0\n    Rank_Down = 2.0\n\n    Calculation = 0.0\n\n    check_int = isinstance(Rank, int)\n    if not check_int:\n        for ind in data.index:\n            if (data['Rank'][ind] in (Rank_Down, Rank_Up)):\n                Calculation = data['DataPoint'][ind] + (data['DataPoint'][ind+1]- data['DataPoint'][ind]) *((Rank-Rank_Down)/(Rank_Up-Rank_Down))\n                print(data['DataPoint'][ind], data['Rank'][ind])\n                print(data['DataPoint'][ind+1], data['Rank'][ind+1])\n                break\n\n    return Calculation\n\n\nCalc_int = Calc(df)\n\nprint(Calc_int)\n</code></pre>\n\n<p>The result looks like the following:</p>\n\n<pre><code>-9.080002000000007 2.0\n-9.360001000000011 1.0\n-9.22000150000001\n</code></pre>\n\n<p>The Excel file looks like the following:</p>\n\n<pre><code>Number  DataPoint\n1   -5.910004\n2   -9.080002\n3   -9.360001\n4   -0.779999\n5   0.389999\n6   0.089996\n7   0.009995\n8   -0.380005\n9   1.139999\n10  2.389999\n11  2.279999\n12  0.089996\n13  -1.220001\n14  -0.960007\n15  -5.419998\n16  -6.410004\n17  -5.029999\n18  -7.529999\n19  -7.479996\n20  -4.580002\n21  -5.589996\n22  -6.339996\n23  -5.570007\n24  -3.520004\n25  -5.490005\n26  -1.860001\n27  -1.589996\n28  -2.470001\n29  -4.350006\n30  -2.630005\n31  -3.800003\n32  -4.949997\n33  -2.770004\n34  -6.300003\n35  -3.220001\n36  -4.949997\n37  -7.320007\n38  -2.110001\n39  -2.020004\n40  -1.460007\n41  -3.300003\n42  -9.039993\n43  -6.529999\n44  -1.149994\n45  2.660004\n46  3.940002\n47  6.009995\n48  6.309998\n49  5.75\n50  5.070007\n51  6.160004\n52  5.509995\n53  6.75\n54  5.119995\n55  6.320007\n56  6.599992\n57  6.302474\n58  7.403336\n59  10.65633\n60  9.634812\n61  9.763731\n62  8.7323\n63  7.760373\n64  8.434767\n65  10.398461\n66  9.029833\n67  11.72744\n68  21.793869\n69  21.089708\n70  19.592145\n71  17.142479\n72  17.221823\n73  17.985487\n74  15.575486\n75  12.501017\n76  11.816696\n77  13.641543\n78  13.373762\n79  13.730799\n80  13.135748\n81  11.320812\n82  11.628258\n83  14.097763\n84  14.712653\n85  15.595325\n86  17.231735\n87  16.349063\n88  15.615165\n89  16.864786\n90  15.793676\n91  15.466391\n92  14.891164\n93  15.615165\n94  17.390421\n95  17.529268\n96  14.315938\n97  13.978741\n98  16.021778\n99  16.99372\n100 18.521035\n101 18.253254\n102 18.778889\n103 18.253254\n104 20.296306\n105 20.583912\n106 18.729313\n107 20.802102\n108 21.912876\n109 22.329416\n110 22.051723\n111 21.635182\n112 21.833533\n113 22.617037\n114 20.861605\n115 19.145853\n116 19.711152\n117 21.258306\n118 19.582233\n119 18.649969\n120 20.365723\n121 21.241765\n122 20.946467\n123 18.603786\n124 21.665015\n125 21.192546\n126 18.05257\n127 17.117471\n128 17.570264\n129 12.766789\n130 11.684033\n131 13.554246\n132 8.150336\n133 10.571764\n134 11.556082\n135 9.754779\n136 9.833526\n137 10.305994\n138 9.794145\n139 9.311839\n140 9.52839\n141 9.498846\n142 7.933785\n143 6.683697\n144 7.983004\n145 10.118971\n146 7.884566\n147 8.071589\n148 6.703387\n149 10.443798\n150 12.048241\n151 10.325685\n152 7.303822\n153 7.303822\n154 4.498525\n155 4.616639\n156 1.860546\n157 5.502519\n158 4.341031\n159 5.335186\n160 8.248774\n161 6.014368\n162 8.652332\n163 6.919939\n164 8.504691\n165 7.963313\n166 9.400424\n167 8.347196\n168 10.256775\n169 12.245101\n170 10.355213\n171 11.083599\n172 11.31984\n173 9.646503\n174 8.435796\n175 7.884566\n176 6.034059\n177 5.71907\n178 6.073425\n179 3.307494\n180 4.262284\n181 4.350876\n182 3.422291\n183 4.898252\n184 1.174128\n185 -0.14545\n186 1.975638\n187 -1.865776\n188 0.206436\n189 2.835809\n190 2.356848\n191 1.88767\n192 -1.337954\n193 3.412507\n194 3.598227\n195 1.08616\n196 2.288433\n197 3.275663\n198 2.914007\n199 5.523827\n200 4.849376\n201 1.555337\n202 2.552352\n203 5.914805\n204 8.407334\n205 6.823852\n206 6.442642\n207 6.892266\n208 8.681022\n209 8.172753\n210 8.583285\n211 7.801313\n212 4.937344\n213 4.12605\n214 -1.494351\n215 -2.00262\n216 -0.389816\n217 1.877901\n218 7.439657\n219 6.892266\n220 9.31638\n221 8.211845\n222 9.590068\n223 10.352487\n224 10.811894\n225 9.003586\n226 7.89905\n227 8.514855\n228 8.299812\n229 8.114092\n230 8.495302\n231 7.293029\n232 7.019341\n233 7.791529\n234 8.241167\n235 10.460008\n236 10.460008\n237 9.120876\n238 8.27049\n239 9.619391\n240 9.805111\n241 8.417118\n242 7.820866\n243 7.097551\n244 7.903316\n245 9.563419\n246 9.602245\n247 9.553716\n248 8.68969\n249 8.825604\n250 7.14608\n</code></pre>\n",
        "formatted_input": {
            "qid": 60578553,
            "link": "https://stackoverflow.com/questions/60578553/using-values-from-dataframe-for-calculation",
            "question": {
                "title": "Using values from dataframe for calculation",
                "ques_desc": "I am selecting dedicated data from a dataframe and would like to make a linear interpolation based on my defined formula: I would like to interpolate e.g. between rank 2.0 and 3.0, where the needed rank is 2.5. The calculation looks like the following: y = -9.080002 + (-9.039993 - (-9.080002))*[(2.5-2)/(3-2)] = -9.059997500000 where the values are defined in the code as the following: The code looks like the following: The result looks like the following: The Excel file looks like the following: "
            },
            "io": [
                "y = y0 + (y1 - y0) * [(x-x0)/(x1-x0)]\n",
                "-9.080002000000007 2.0\n-9.360001000000011 1.0\n-9.22000150000001\n"
            ],
            "answer": {
                "ans_desc": "The IndexError is because of . ",
                "code": [
                    "def Calc(data):\n\n    Rank = 2.5\n    Rank_Up = 3.0\n    Rank_Down = 2.0\n\n    Calculation = 0.0\n\n    check_int = isinstance(Rank, int)\n    if not check_int:\n        for ind in data.Rank:\n            if (ind in (Rank_Down, Rank_Up)):\n                index0 = data.Rank.index[ind-1]\n                index1 = data.Rank.index[ind]\n                Calculation = data['DataPoint'][index0] + (data['DataPoint'][index1]- data['DataPoint'][index0]) *((Rank-Rank_Down)/(Rank_Up-Rank_Down))\n                print(data['DataPoint'][index0], ind)\n                print(data['DataPoint'][index1], ind+1)\n                break\n\n    return Calculation\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2070,
            "user_id": 2015933,
            "user_type": "registered",
            "accept_rate": 89,
            "profile_image": "https://www.gravatar.com/avatar/6693c2273791de1e7d6b42f901ea825e?s=128&d=identicon&r=PG",
            "display_name": "pistal",
            "link": "https://stackoverflow.com/users/2015933/pistal"
        },
        "is_answered": true,
        "view_count": 36591,
        "accepted_answer_id": 24413233,
        "answer_count": 2,
        "score": 5,
        "last_activity_date": 1583433053,
        "creation_date": 1403709274,
        "last_edit_date": 1403710642,
        "question_id": 24412510,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/24412510/transpose-pandas-dataframe",
        "title": "Transpose pandas dataframe",
        "body": "<p>How do I convert a list of lists to a panda dataframe?</p>\n\n<p>it is not in the form of coloumns but instead in the form of rows.</p>\n\n<pre><code>#!/usr/bin/env python\n\nfrom random import randrange\nimport pandas\n\ndata = [[[randrange(0,100) for j in range(0, 12)] for y in range(0, 12)] for x in range(0, 5)]\nprint data\ndf = pandas.DataFrame(data[0], columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])\nprint df\n</code></pre>\n\n<p>for example:</p>\n\n<pre><code>data[0][0] == [64, 73, 76, 64, 61, 32, 36, 94, 81, 49, 94, 48]\n</code></pre>\n\n<p>I want it to be shown as rows and not coloumns. </p>\n\n<p>currently it shows somethign like this</p>\n\n<pre><code>     B   P   F   I  FP  BP   2   M   3   1   I   L\n0   64  73  76  64  61  32  36  94  81  49  94  48\n1   57  58  69  46  34  66  15  24  20  49  25  98\n2   99  61  73  69  21  33  78  31  16  11  77  71\n3   41   1  55  34  97  64  98   9  42  77  95  41\n4   36  50  54  27  74   0   8  59  27  54   6  90\n5   74  72  75  30  62  42  90  26  13  49  74   9\n6   41  92  11  38  24  48  34  74  50  10  42   9\n7   77   9  77  63  23   5  50  66  49   5  66  98\n8   90  66  97  16  39  55  38   4  33  52  64   5\n9   18  14  62  87  54  38  29  10  66  18  15  86\n10  60  89  57  28  18  68  11  29  94  34  37  59\n11  78  67  93  18  14  28  64  11  77  79  94  66\n</code></pre>\n\n<p>I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists?</p>\n\n<p>This is how I want the output to look like with other coloumns also filled in.</p>\n\n<pre><code>     B   P   F   I  FP  BP   2   M   3   1   I   L\n0    64 \n1    73  \n1    76  \n2    64  \n3    61  \n4    32  \n5    36  \n6    94  \n7    81  \n8    49  \n9    94  \n10   48\n</code></pre>\n\n<p>However. <code>df.transpose()</code> won't help.</p>\n",
        "answer_body": "<pre><code>import numpy\n\ndf = pandas.DataFrame(numpy.asarray(data[x]).T.tolist(),\n                      columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])\n</code></pre>\n",
        "question_body": "<p>How do I convert a list of lists to a panda dataframe?</p>\n\n<p>it is not in the form of coloumns but instead in the form of rows.</p>\n\n<pre><code>#!/usr/bin/env python\n\nfrom random import randrange\nimport pandas\n\ndata = [[[randrange(0,100) for j in range(0, 12)] for y in range(0, 12)] for x in range(0, 5)]\nprint data\ndf = pandas.DataFrame(data[0], columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])\nprint df\n</code></pre>\n\n<p>for example:</p>\n\n<pre><code>data[0][0] == [64, 73, 76, 64, 61, 32, 36, 94, 81, 49, 94, 48]\n</code></pre>\n\n<p>I want it to be shown as rows and not coloumns. </p>\n\n<p>currently it shows somethign like this</p>\n\n<pre><code>     B   P   F   I  FP  BP   2   M   3   1   I   L\n0   64  73  76  64  61  32  36  94  81  49  94  48\n1   57  58  69  46  34  66  15  24  20  49  25  98\n2   99  61  73  69  21  33  78  31  16  11  77  71\n3   41   1  55  34  97  64  98   9  42  77  95  41\n4   36  50  54  27  74   0   8  59  27  54   6  90\n5   74  72  75  30  62  42  90  26  13  49  74   9\n6   41  92  11  38  24  48  34  74  50  10  42   9\n7   77   9  77  63  23   5  50  66  49   5  66  98\n8   90  66  97  16  39  55  38   4  33  52  64   5\n9   18  14  62  87  54  38  29  10  66  18  15  86\n10  60  89  57  28  18  68  11  29  94  34  37  59\n11  78  67  93  18  14  28  64  11  77  79  94  66\n</code></pre>\n\n<p>I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists?</p>\n\n<p>This is how I want the output to look like with other coloumns also filled in.</p>\n\n<pre><code>     B   P   F   I  FP  BP   2   M   3   1   I   L\n0    64 \n1    73  \n1    76  \n2    64  \n3    61  \n4    32  \n5    36  \n6    94  \n7    81  \n8    49  \n9    94  \n10   48\n</code></pre>\n\n<p>However. <code>df.transpose()</code> won't help.</p>\n",
        "formatted_input": {
            "qid": 24412510,
            "link": "https://stackoverflow.com/questions/24412510/transpose-pandas-dataframe",
            "question": {
                "title": "Transpose pandas dataframe",
                "ques_desc": "How do I convert a list of lists to a panda dataframe? it is not in the form of coloumns but instead in the form of rows. for example: I want it to be shown as rows and not coloumns. currently it shows somethign like this I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists? This is how I want the output to look like with other coloumns also filled in. However. won't help. "
            },
            "io": [
                "     B   P   F   I  FP  BP   2   M   3   1   I   L\n0   64  73  76  64  61  32  36  94  81  49  94  48\n1   57  58  69  46  34  66  15  24  20  49  25  98\n2   99  61  73  69  21  33  78  31  16  11  77  71\n3   41   1  55  34  97  64  98   9  42  77  95  41\n4   36  50  54  27  74   0   8  59  27  54   6  90\n5   74  72  75  30  62  42  90  26  13  49  74   9\n6   41  92  11  38  24  48  34  74  50  10  42   9\n7   77   9  77  63  23   5  50  66  49   5  66  98\n8   90  66  97  16  39  55  38   4  33  52  64   5\n9   18  14  62  87  54  38  29  10  66  18  15  86\n10  60  89  57  28  18  68  11  29  94  34  37  59\n11  78  67  93  18  14  28  64  11  77  79  94  66\n",
                "     B   P   F   I  FP  BP   2   M   3   1   I   L\n0    64 \n1    73  \n1    76  \n2    64  \n3    61  \n4    32  \n5    36  \n6    94  \n7    81  \n8    49  \n9    94  \n10   48\n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "import numpy\n\ndf = pandas.DataFrame(numpy.asarray(data[x]).T.tolist(),\n                      columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 119,
            "user_id": 1412129,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://i.stack.imgur.com/FqNKz.png?s=128&g=1",
            "display_name": "Brom",
            "link": "https://stackoverflow.com/users/1412129/brom"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 60486257,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1583323300,
        "creation_date": 1583142630,
        "last_edit_date": 1583323300,
        "question_id": 60486231,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60486231/pandas-dataframe-compare-columns-with-one-value-and-get-this-row-and-previous-ro",
        "title": "Pandas dataframe compare columns with one value and get this row and previous row into another dataframe",
        "body": "<p>I have a dataframe like this:</p>\n\n<pre><code>&gt;&gt;&gt; df\n   A    B\n0  1   56\n1  2   75\n2  3  102\n3  4   15\n4  5   19\n5  6  116\n</code></pre>\n\n<p>I want to create another dataframe with <code>B&gt;100</code> and also store its previous row. It should look like this:</p>\n\n<pre><code>&gt;&gt;&gt; df1\n   A    B\n1  2   75\n2  3  102\n4  5   19\n5  6  116\n</code></pre>\n\n<p>What is the best way. Thanks in advance.</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a> with 2 conditions chained by <code>|</code> for bitwise <code>OR</code>, second with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html\" rel=\"nofollow noreferrer\"><code>Series.shift</code></a>, also for cpmpare is used <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.gt.html\" rel=\"nofollow noreferrer\"><code>Series.gt</code></a>:</p>\n\n<pre><code>df1 = df[df.B.gt(100) | df.B.shift(-1).gt(100)]\n</code></pre>\n\n<p>Alternative:</p>\n\n<pre><code>df1 = df[(df.B&gt;100) | (df.B.shift(-1)&gt;100)]\n</code></pre>\n\n<hr>\n\n<pre><code>print (df1)\n   A    B\n1  2   75\n2  3  102\n4  5   19\n5  6  116\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n\n<pre><code>&gt;&gt;&gt; df\n   A    B\n0  1   56\n1  2   75\n2  3  102\n3  4   15\n4  5   19\n5  6  116\n</code></pre>\n\n<p>I want to create another dataframe with <code>B&gt;100</code> and also store its previous row. It should look like this:</p>\n\n<pre><code>&gt;&gt;&gt; df1\n   A    B\n1  2   75\n2  3  102\n4  5   19\n5  6  116\n</code></pre>\n\n<p>What is the best way. Thanks in advance.</p>\n",
        "formatted_input": {
            "qid": 60486231,
            "link": "https://stackoverflow.com/questions/60486231/pandas-dataframe-compare-columns-with-one-value-and-get-this-row-and-previous-ro",
            "question": {
                "title": "Pandas dataframe compare columns with one value and get this row and previous row into another dataframe",
                "ques_desc": "I have a dataframe like this: I want to create another dataframe with and also store its previous row. It should look like this: What is the best way. Thanks in advance. "
            },
            "io": [
                ">>> df\n   A    B\n0  1   56\n1  2   75\n2  3  102\n3  4   15\n4  5   19\n5  6  116\n",
                ">>> df1\n   A    B\n1  2   75\n2  3  102\n4  5   19\n5  6  116\n"
            ],
            "answer": {
                "ans_desc": "Use with 2 conditions chained by for bitwise , second with , also for cpmpare is used : Alternative: ",
                "code": [
                    "df1 = df[df.B.gt(100) | df.B.shift(-1).gt(100)]\n",
                    "df1 = df[(df.B>100) | (df.B.shift(-1)>100)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1989,
            "user_id": 7668467,
            "user_type": "registered",
            "accept_rate": 71,
            "profile_image": "https://www.gravatar.com/avatar/0e880601228777466cc33bca31a7581e?s=128&d=identicon&r=PG&f=1",
            "display_name": "OverflowingTheGlass",
            "link": "https://stackoverflow.com/users/7668467/overflowingtheglass"
        },
        "is_answered": true,
        "view_count": 2509,
        "accepted_answer_id": 45336646,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1583254155,
        "creation_date": 1501099508,
        "last_edit_date": 1501102193,
        "question_id": 45336415,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/45336415/pandas-drop-columns-with-only-one-unique-value-for-a-group",
        "title": "Pandas Drop Columns with Only One Unique Value for a Group",
        "body": "<p>I have a df that consists of duplicate <code>id</code>:</p>\n\n<pre><code>id    text      text2     text3\n1     hello     hello     hello\n1     hello     hello     hello\n2     hello     hello     goodbye\n2     goodbye   hello     goodbye\n2     hello     hello     goodbye\n</code></pre>\n\n<p>I want to remove columns where all values for an <code>id</code> are the same. This could mean that all values in the column are the same (<code>text2</code>) or all the values are the same for each <code>id</code> (<code>text3</code>).</p>\n\n<p>Desired result:</p>\n\n<pre><code>id    text     \n1     hello     \n1     hello       \n2     hello        \n2     goodbye        \n2     hello\n</code></pre>\n\n<p>I used this to identify the counts of unique values in each column:</p>\n\n<pre><code>df.apply(lambda x: len(x.unique()))\n</code></pre>\n\n<p>If I drop all columns where this count is equal to 1, this would take care of the <code>text2</code> scenario. However, how should I take care of the <code>text3</code> scenario? The df has already been grouped by id to find duplicate, but do I need to use <code>groupby</code> again?</p>\n\n<p>As a \"bonus\", I wouldn't mind knowing how to identify where even one <code>id</code> has text that is all the same (i.e. <code>text</code>). I'm essentially trying to find which columns cause there to be duplicates.</p>\n\n<p>Thank you for any and all insight you all might have!</p>\n",
        "answer_body": "<p>Here's one way.</p>\n\n<p>Get unique values for each column</p>\n\n<pre><code>In [1227]: u = df.nunique()\n</code></pre>\n\n<p>Get if any column has single value in each <code>id</code> group</p>\n\n<pre><code>In [1228]: gu = gu = (df.groupby('id').agg('nunique') == 1).all()\n</code></pre>\n\n<p>Get the index aka column names for satisfying conditions, </p>\n\n<p><code>u[u == 1].index.union(gu[gu].index).drop('id')</code> Then, use <code>drop</code></p>\n\n<pre><code>In [1229]: df.drop(u[u == 1].index.union(gu[gu].index).drop('id'), axis=1)\nOut[1229]:\n   id     text\n0   1    hello\n1   1    hello\n2   2    hello\n3   2  goodbye\n4   2    hello\n</code></pre>\n\n<p>Details</p>\n\n<pre><code>In [1304]: u\nOut[1304]:\nid       2\ntext     2\ntext2    1\ntext3    2\ndtype: int64\n\nIn [1305]: gu\nOut[1305]:\nid        True\ntext     False\ntext2     True\ntext3     True\ndtype: bool\n\nIn [1306]: u[u == 1].index.union(gu[gu].index).drop('id')\nOut[1306]: Index([u'text2', u'text3'], dtype='object')\n</code></pre>\n",
        "question_body": "<p>I have a df that consists of duplicate <code>id</code>:</p>\n\n<pre><code>id    text      text2     text3\n1     hello     hello     hello\n1     hello     hello     hello\n2     hello     hello     goodbye\n2     goodbye   hello     goodbye\n2     hello     hello     goodbye\n</code></pre>\n\n<p>I want to remove columns where all values for an <code>id</code> are the same. This could mean that all values in the column are the same (<code>text2</code>) or all the values are the same for each <code>id</code> (<code>text3</code>).</p>\n\n<p>Desired result:</p>\n\n<pre><code>id    text     \n1     hello     \n1     hello       \n2     hello        \n2     goodbye        \n2     hello\n</code></pre>\n\n<p>I used this to identify the counts of unique values in each column:</p>\n\n<pre><code>df.apply(lambda x: len(x.unique()))\n</code></pre>\n\n<p>If I drop all columns where this count is equal to 1, this would take care of the <code>text2</code> scenario. However, how should I take care of the <code>text3</code> scenario? The df has already been grouped by id to find duplicate, but do I need to use <code>groupby</code> again?</p>\n\n<p>As a \"bonus\", I wouldn't mind knowing how to identify where even one <code>id</code> has text that is all the same (i.e. <code>text</code>). I'm essentially trying to find which columns cause there to be duplicates.</p>\n\n<p>Thank you for any and all insight you all might have!</p>\n",
        "formatted_input": {
            "qid": 45336415,
            "link": "https://stackoverflow.com/questions/45336415/pandas-drop-columns-with-only-one-unique-value-for-a-group",
            "question": {
                "title": "Pandas Drop Columns with Only One Unique Value for a Group",
                "ques_desc": "I have a df that consists of duplicate : I want to remove columns where all values for an are the same. This could mean that all values in the column are the same () or all the values are the same for each (). Desired result: I used this to identify the counts of unique values in each column: If I drop all columns where this count is equal to 1, this would take care of the scenario. However, how should I take care of the scenario? The df has already been grouped by id to find duplicate, but do I need to use again? As a \"bonus\", I wouldn't mind knowing how to identify where even one has text that is all the same (i.e. ). I'm essentially trying to find which columns cause there to be duplicates. Thank you for any and all insight you all might have! "
            },
            "io": [
                "id    text      text2     text3\n1     hello     hello     hello\n1     hello     hello     hello\n2     hello     hello     goodbye\n2     goodbye   hello     goodbye\n2     hello     hello     goodbye\n",
                "id    text     \n1     hello     \n1     hello       \n2     hello        \n2     goodbye        \n2     hello\n"
            ],
            "answer": {
                "ans_desc": "Here's one way. Get unique values for each column Get if any column has single value in each group Get the index aka column names for satisfying conditions, Then, use Details ",
                "code": [
                    "In [1227]: u = df.nunique()\n",
                    "In [1228]: gu = gu = (df.groupby('id').agg('nunique') == 1).all()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3971,
            "user_id": 7585973,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/1cf9f40e4f8e5e17076d55814a9c2ad9?s=128&d=identicon&r=PG",
            "display_name": "Nabih Bawazir",
            "link": "https://stackoverflow.com/users/7585973/nabih-bawazir"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 60502889,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1583224218,
        "creation_date": 1583223533,
        "question_id": 60502832,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60502832/how-to-drop-pandas-consecutive-column-by-column-name-simultaneously",
        "title": "How to drop pandas consecutive column by column name simultaneously?",
        "body": "<p>Here's my data</p>\n\n<pre><code>Id   Column1  Column2  Column3  Column4 ....  Column112  Column113 ... Column143\n1         67       89       86       43              56         72            67\n</code></pre>\n\n<p>The Output I expected, </p>\n\n<pre><code>Id   Column1  Column113 ... Column143\n1         67         72            67\n</code></pre>\n\n<p>What I did</p>\n\n<pre><code>df.drop(['Column2', 'Column3', ... ,'Column112'], axis = 1)\n</code></pre>\n\n<p>But this is not efficient, how to do this effectively?</p>\n",
        "answer_body": "<p>Use:</p>\n\n<pre><code>df1 = df.drop(df.loc[:, 'Column2':'Column112'].columns, axis=1)\n</code></pre>\n",
        "question_body": "<p>Here's my data</p>\n\n<pre><code>Id   Column1  Column2  Column3  Column4 ....  Column112  Column113 ... Column143\n1         67       89       86       43              56         72            67\n</code></pre>\n\n<p>The Output I expected, </p>\n\n<pre><code>Id   Column1  Column113 ... Column143\n1         67         72            67\n</code></pre>\n\n<p>What I did</p>\n\n<pre><code>df.drop(['Column2', 'Column3', ... ,'Column112'], axis = 1)\n</code></pre>\n\n<p>But this is not efficient, how to do this effectively?</p>\n",
        "formatted_input": {
            "qid": 60502832,
            "link": "https://stackoverflow.com/questions/60502832/how-to-drop-pandas-consecutive-column-by-column-name-simultaneously",
            "question": {
                "title": "How to drop pandas consecutive column by column name simultaneously?",
                "ques_desc": "Here's my data The Output I expected, What I did But this is not efficient, how to do this effectively? "
            },
            "io": [
                "Id   Column1  Column2  Column3  Column4 ....  Column112  Column113 ... Column143\n1         67       89       86       43              56         72            67\n",
                "Id   Column1  Column113 ... Column143\n1         67         72            67\n"
            ],
            "answer": {
                "ans_desc": "Use: ",
                "code": [
                    "df1 = df.drop(df.loc[:, 'Column2':'Column112'].columns, axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matplotlib",
            "data-visualization"
        ],
        "owner": {
            "reputation": 826,
            "user_id": 5250136,
            "user_type": "registered",
            "accept_rate": 40,
            "profile_image": "https://www.gravatar.com/avatar/039eacdbb4a6fe7174e273913c3d5001?s=128&d=identicon&r=PG&f=1",
            "display_name": "Chris Maverick",
            "link": "https://stackoverflow.com/users/5250136/chris-maverick"
        },
        "is_answered": true,
        "view_count": 78,
        "accepted_answer_id": 60492903,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1583166268,
        "creation_date": 1583166090,
        "question_id": 60492857,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60492857/pandas-groupby-or-cut-multi-dataframes-to-bins",
        "title": "Pandas - Groupby or Cut multi dataframes to bins",
        "body": "<p>I'm having a dataframe with starting axis points and their end points like this</p>\n\n<pre><code>x       y       x_end   y_end   distance\n14.14   30.450  31.71   41.265  20.631750\n-27.02  55.650  -33.60  63.000  9.865034\n-19.25  70.665  -28.98  80.115  13.563753\n16.45   59.115  9.94    41.895  18.409468\n</code></pre>\n\n<p>I'm drawing a heatmap. I need each \"zone\" of that map has a line which shows the average distance and angle of lines which have x/y from that zone and their x_end/y_end. It looks like this\n<a href=\"https://i.stack.imgur.com/0BpKT.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/0BpKT.png\" alt=\"enter image description here\"></a></p>\n\n<p>My bins is</p>\n\n<pre><code>xbins = np.linspace(-35, 35, 11)\n\nybins = np.linspace(0, 105, 22)\n</code></pre>\n\n<p>I've already plotted a heatmap</p>\n\n<p><a href=\"https://i.stack.imgur.com/3ryyZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/3ryyZ.png\" alt=\"enter image description here\"></a></p>\n\n<p>I'm looking for something like this</p>\n\n<pre><code>Bins_X           Bins_Y     Average_X_End   Average_Y_End   Average_Distance\n(-35.0, -28.0]  (0, 5.0]    31.71           41.265          20.631750\n(-28.0, -21.0]  (0, 5.0]    -33.60          63.000          9.865034\n(-21.0, -14.0]  (0, 5.0]    -28.98          80.115          13.563753\n(-14.0, -7.0]   (0, 5.0]    9.94            41.895          18.409468\n(-35.0, -28.0]  (5.0, 10.0] 41.265          31.71           13.563753\n(-28.0, -21.0]  (5.0, 10.0] 63.000          -33.60          18.409468\n(-21.0, -14.0]  (5.0, 10.0] 80.115          -28.98          20.631750\n(-14.0, -7.0]   (5.0, 10.0] 41.895          9.94            9.865034\n</code></pre>\n",
        "answer_body": "<p>Something like this?</p>\n\n<pre><code>(df.drop(['x','y'], axis=1)\n  .groupby([pd.cut(df.x, xbins),\n            pd.cut(df.y, ybins)],\n          )\n   .mean()\n   .dropna(how='all')\n   .add_prefix('Average_')\n)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>                             Average_x_end  Average_y_end  Average_distance\nx              y                                                           \n(-28.0, -21.0] (55.0, 60.0]         -33.60         63.000          9.865034\n(-21.0, -14.0] (70.0, 75.0]         -28.98         80.115         13.563753\n(14.0, 21.0]   (30.0, 35.0]          31.71         41.265         20.631750\n               (55.0, 60.0]           9.94         41.895         18.409468\n</code></pre>\n",
        "question_body": "<p>I'm having a dataframe with starting axis points and their end points like this</p>\n\n<pre><code>x       y       x_end   y_end   distance\n14.14   30.450  31.71   41.265  20.631750\n-27.02  55.650  -33.60  63.000  9.865034\n-19.25  70.665  -28.98  80.115  13.563753\n16.45   59.115  9.94    41.895  18.409468\n</code></pre>\n\n<p>I'm drawing a heatmap. I need each \"zone\" of that map has a line which shows the average distance and angle of lines which have x/y from that zone and their x_end/y_end. It looks like this\n<a href=\"https://i.stack.imgur.com/0BpKT.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/0BpKT.png\" alt=\"enter image description here\"></a></p>\n\n<p>My bins is</p>\n\n<pre><code>xbins = np.linspace(-35, 35, 11)\n\nybins = np.linspace(0, 105, 22)\n</code></pre>\n\n<p>I've already plotted a heatmap</p>\n\n<p><a href=\"https://i.stack.imgur.com/3ryyZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/3ryyZ.png\" alt=\"enter image description here\"></a></p>\n\n<p>I'm looking for something like this</p>\n\n<pre><code>Bins_X           Bins_Y     Average_X_End   Average_Y_End   Average_Distance\n(-35.0, -28.0]  (0, 5.0]    31.71           41.265          20.631750\n(-28.0, -21.0]  (0, 5.0]    -33.60          63.000          9.865034\n(-21.0, -14.0]  (0, 5.0]    -28.98          80.115          13.563753\n(-14.0, -7.0]   (0, 5.0]    9.94            41.895          18.409468\n(-35.0, -28.0]  (5.0, 10.0] 41.265          31.71           13.563753\n(-28.0, -21.0]  (5.0, 10.0] 63.000          -33.60          18.409468\n(-21.0, -14.0]  (5.0, 10.0] 80.115          -28.98          20.631750\n(-14.0, -7.0]   (5.0, 10.0] 41.895          9.94            9.865034\n</code></pre>\n",
        "formatted_input": {
            "qid": 60492857,
            "link": "https://stackoverflow.com/questions/60492857/pandas-groupby-or-cut-multi-dataframes-to-bins",
            "question": {
                "title": "Pandas - Groupby or Cut multi dataframes to bins",
                "ques_desc": "I'm having a dataframe with starting axis points and their end points like this I'm drawing a heatmap. I need each \"zone\" of that map has a line which shows the average distance and angle of lines which have x/y from that zone and their x_end/y_end. It looks like this My bins is I've already plotted a heatmap I'm looking for something like this "
            },
            "io": [
                "x       y       x_end   y_end   distance\n14.14   30.450  31.71   41.265  20.631750\n-27.02  55.650  -33.60  63.000  9.865034\n-19.25  70.665  -28.98  80.115  13.563753\n16.45   59.115  9.94    41.895  18.409468\n",
                "xbins = np.linspace(-35, 35, 11)\n\nybins = np.linspace(0, 105, 22)\n"
            ],
            "answer": {
                "ans_desc": "Something like this? Output: ",
                "code": [
                    "(df.drop(['x','y'], axis=1)\n  .groupby([pd.cut(df.x, xbins),\n            pd.cut(df.y, ybins)],\n          )\n   .mean()\n   .dropna(how='all')\n   .add_prefix('Average_')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2702,
            "user_id": 6653602,
            "user_type": "registered",
            "accept_rate": 80,
            "profile_image": "https://www.gravatar.com/avatar/308958dfcf4e6e51275ac0a387a24363?s=128&d=identicon&r=PG&f=1",
            "display_name": "Alex T",
            "link": "https://stackoverflow.com/users/6653602/alex-t"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 60455673,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1582907871,
        "creation_date": 1582906512,
        "last_edit_date": 1582906981,
        "question_id": 60455539,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60455539/rolling-over-values-from-one-column-to-other-based-on-another-dataframe",
        "title": "Rolling over values from one column to other based on another dataframe",
        "body": "<p>I have two dataframes:\n<code>DF1</code></p>\n\n<pre><code>ID  DatePaid   Remaining\nA1  2018-01-01   8500\nA2  2018-02-15   2000\nA2  2018-02-28   1900\nA3  2018-04-12   3000\nA3  2018-05-12   2700\nA3  2018-05-17   110\nA3  2018-06-17   0\nA4  2018-06-18   10\nA5  2018-07-13   500 \n</code></pre>\n\n<p>Now I have another dataframe <code>DF2</code> which only have unique IDs from first dataframe, and dates that represent months:</p>\n\n<pre><code>ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31\nA1\nA2\nA3\nA4\nA5\n</code></pre>\n\n<p>So based on first dataframe I need to fill the values based on the <code>Remaining</code> value that is in the first dataframe that is within the corresponding month ( so for example I take the last value for the <code>A3</code> from <code>2018-05</code> and put it in the <code>2018-05-31</code> column in <code>DF2</code>. IF there are no other values for that ID just fill all the remaining columns in <code>DF</code> with the value in the most right filled column(roll over to the right).</p>\n\n<p>So the end result is exactly like this</p>\n\n<pre><code>ID  2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31\nA1  8500        8500        8500      8500        8500        8500         8500\nA2   NA         1900        1900      1900        1900        1900         1900\nA3   NA          NA          NA       3000        110           0             0\nA4   NA          NA          NA        NA         NA           10            10\nA5   NA          NA          NA        NA         NA           NA           500\n</code></pre>\n",
        "answer_body": "<p>This gives you the data in <code>df2</code> form:</p>\n\n<pre><code>month_ends = pd.to_datetime(df1.DatePaid).dt.to_period('M')\n# also\n# month_ends = pd.to_datetime(df1.DatePaid).add(pd.offsets.MonthEnd(0))\n\n(df1.groupby(['ID', month_ends])\n    ['Remaining'].last()\n    .unstack(-1)\n    .ffill(1)\n    .reset_index()\n    .rename_axis(columns=None)\n)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   ID  2018-01  2018-02  2018-04  2018-05  2018-06  2018-07\n0  A1   8500.0   8500.0   8500.0   8500.0   8500.0   8500.0\n1  A2      NaN   1900.0   1900.0   1900.0   1900.0   1900.0\n2  A3      NaN      NaN   3000.0    110.0      0.0      0.0\n3  A4      NaN      NaN      NaN      NaN     10.0     10.0\n4  A5      NaN      NaN      NaN      NaN      NaN    500.0\n</code></pre>\n",
        "question_body": "<p>I have two dataframes:\n<code>DF1</code></p>\n\n<pre><code>ID  DatePaid   Remaining\nA1  2018-01-01   8500\nA2  2018-02-15   2000\nA2  2018-02-28   1900\nA3  2018-04-12   3000\nA3  2018-05-12   2700\nA3  2018-05-17   110\nA3  2018-06-17   0\nA4  2018-06-18   10\nA5  2018-07-13   500 \n</code></pre>\n\n<p>Now I have another dataframe <code>DF2</code> which only have unique IDs from first dataframe, and dates that represent months:</p>\n\n<pre><code>ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31\nA1\nA2\nA3\nA4\nA5\n</code></pre>\n\n<p>So based on first dataframe I need to fill the values based on the <code>Remaining</code> value that is in the first dataframe that is within the corresponding month ( so for example I take the last value for the <code>A3</code> from <code>2018-05</code> and put it in the <code>2018-05-31</code> column in <code>DF2</code>. IF there are no other values for that ID just fill all the remaining columns in <code>DF</code> with the value in the most right filled column(roll over to the right).</p>\n\n<p>So the end result is exactly like this</p>\n\n<pre><code>ID  2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31\nA1  8500        8500        8500      8500        8500        8500         8500\nA2   NA         1900        1900      1900        1900        1900         1900\nA3   NA          NA          NA       3000        110           0             0\nA4   NA          NA          NA        NA         NA           10            10\nA5   NA          NA          NA        NA         NA           NA           500\n</code></pre>\n",
        "formatted_input": {
            "qid": 60455539,
            "link": "https://stackoverflow.com/questions/60455539/rolling-over-values-from-one-column-to-other-based-on-another-dataframe",
            "question": {
                "title": "Rolling over values from one column to other based on another dataframe",
                "ques_desc": "I have two dataframes: Now I have another dataframe which only have unique IDs from first dataframe, and dates that represent months: So based on first dataframe I need to fill the values based on the value that is in the first dataframe that is within the corresponding month ( so for example I take the last value for the from and put it in the column in . IF there are no other values for that ID just fill all the remaining columns in with the value in the most right filled column(roll over to the right). So the end result is exactly like this "
            },
            "io": [
                "ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31\nA1\nA2\nA3\nA4\nA5\n",
                "ID  2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31\nA1  8500        8500        8500      8500        8500        8500         8500\nA2   NA         1900        1900      1900        1900        1900         1900\nA3   NA          NA          NA       3000        110           0             0\nA4   NA          NA          NA        NA         NA           10            10\nA5   NA          NA          NA        NA         NA           NA           500\n"
            ],
            "answer": {
                "ans_desc": "This gives you the data in form: Output: ",
                "code": [
                    "month_ends = pd.to_datetime(df1.DatePaid).dt.to_period('M')\n# also\n# month_ends = pd.to_datetime(df1.DatePaid).add(pd.offsets.MonthEnd(0))\n\n(df1.groupby(['ID', month_ends])\n    ['Remaining'].last()\n    .unstack(-1)\n    .ffill(1)\n    .reset_index()\n    .rename_axis(columns=None)\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 9818397,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1871984062833755/picture?type=large",
            "display_name": "HazelR",
            "link": "https://stackoverflow.com/users/9818397/hazelr"
        },
        "is_answered": true,
        "view_count": 783,
        "accepted_answer_id": 60440967,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1582833909,
        "creation_date": 1582833512,
        "question_id": 60440882,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60440882/remove-unnamed-colums-pandas-dataframe",
        "title": "remove unnamed colums pandas dataframe",
        "body": "<p>i'm a student and have a problem that i cant figure it out how to solve it.i have csv data like this :</p>\n\n<pre><code>\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"report\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"bla1\",\"bla2\",\"\",\"\",\"\",\"bla3\",\"\",\"\",\"\"\n\"\",\"bla4\",\"bla5\",\"\",\"\",\"\",\"\",\"bla6\",\"\",\"\"\n\"\",\"bla6\",\"bla7\",\"bla8\",\"\",\"1\",\"2\",\"3\",\"4\",\"5\"\n\"\",\"bla9\",\"bla10\",\"bla11\",\"\",\"6\",\"7\",\"8\",\"9\",\"10\"\n\"\",\"bla12\",\"bla13\",\"bla14\",\"\",\"11\",\"12\",\"13\",\"14\",\"15\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n</code></pre>\n\n<p>code for reading csv like this :</p>\n\n<pre><code>SMT = pd.read_csv(file.csv, usecols=(5,6,7,8), skiprows=(1,2,3), nrows=(3))\nSMT.fillna(0, inplace=True)\n</code></pre>\n\n<p>SMT print out :</p>\n\n<pre><code>  Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8\n0          1          2          3          4\n1          6          7          8          9\n2         11         12         13         14\n</code></pre>\n\n<p>expected output :</p>\n\n<pre><code> 1          2          3          4\n 6          7          8          9\n11         12         13         14\n</code></pre>\n\n<p>i already trying <code>skiprows=(0,1,2,3)</code> but it will be like this :</p>\n\n<pre><code>           1          2          3          4\n0          6          7          8          9\n1         11         12         13         14\n2          0          0          0          0\n</code></pre>\n\n<p>i already trying to put <code>index=Flase</code> <code>SMT = pd.read_csv(file.csv,index=False, usecols=(5,6,7,8), skiprows=(1,2,3), nrows=(3))</code> or <code>index_col=0</code>/<code>None</code>/<code>False</code>is not working, and the last time I tried it like this :</p>\n\n<pre><code>df1 = SMT.loc[:, ~SMT.columns.str.contains('^Unnamed')]\n</code></pre>\n\n<p>and i got </p>\n\n<p><code>Empty DataFrame\ncolumns: []\nIndex: [0, 1, 2]</code></p>\n\n<p>i just want to get rid the Unnamed: 5 ~ Unnamed: 8, how the correct way to get rid of this Unnamed thing ?</p>\n",
        "answer_body": "<p>The \"unnamed\" just says, that pandas does not know how to name the columns. So these are just names. You could set the names like this in the <code>read_csv</code></p>\n\n<pre><code>pd.read_csv(\"test.csv\", usecols=(5,6,7,8), skiprows=3, nrows=3, header=0, names=[\"c1\", \"c2\", \"c3\", \"c4\"])\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   c1  c2  c3  c4\n0   1   2   3   4\n1   6   7   8   9\n2  11  12  13  14\n</code></pre>\n\n<p>You have to set <code>header=0</code> so that pandas knows that this is usually the header. Or you set <code>skiprows=4</code></p>\n",
        "question_body": "<p>i'm a student and have a problem that i cant figure it out how to solve it.i have csv data like this :</p>\n\n<pre><code>\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"report\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"bla1\",\"bla2\",\"\",\"\",\"\",\"bla3\",\"\",\"\",\"\"\n\"\",\"bla4\",\"bla5\",\"\",\"\",\"\",\"\",\"bla6\",\"\",\"\"\n\"\",\"bla6\",\"bla7\",\"bla8\",\"\",\"1\",\"2\",\"3\",\"4\",\"5\"\n\"\",\"bla9\",\"bla10\",\"bla11\",\"\",\"6\",\"7\",\"8\",\"9\",\"10\"\n\"\",\"bla12\",\"bla13\",\"bla14\",\"\",\"11\",\"12\",\"13\",\"14\",\"15\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n</code></pre>\n\n<p>code for reading csv like this :</p>\n\n<pre><code>SMT = pd.read_csv(file.csv, usecols=(5,6,7,8), skiprows=(1,2,3), nrows=(3))\nSMT.fillna(0, inplace=True)\n</code></pre>\n\n<p>SMT print out :</p>\n\n<pre><code>  Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8\n0          1          2          3          4\n1          6          7          8          9\n2         11         12         13         14\n</code></pre>\n\n<p>expected output :</p>\n\n<pre><code> 1          2          3          4\n 6          7          8          9\n11         12         13         14\n</code></pre>\n\n<p>i already trying <code>skiprows=(0,1,2,3)</code> but it will be like this :</p>\n\n<pre><code>           1          2          3          4\n0          6          7          8          9\n1         11         12         13         14\n2          0          0          0          0\n</code></pre>\n\n<p>i already trying to put <code>index=Flase</code> <code>SMT = pd.read_csv(file.csv,index=False, usecols=(5,6,7,8), skiprows=(1,2,3), nrows=(3))</code> or <code>index_col=0</code>/<code>None</code>/<code>False</code>is not working, and the last time I tried it like this :</p>\n\n<pre><code>df1 = SMT.loc[:, ~SMT.columns.str.contains('^Unnamed')]\n</code></pre>\n\n<p>and i got </p>\n\n<p><code>Empty DataFrame\ncolumns: []\nIndex: [0, 1, 2]</code></p>\n\n<p>i just want to get rid the Unnamed: 5 ~ Unnamed: 8, how the correct way to get rid of this Unnamed thing ?</p>\n",
        "formatted_input": {
            "qid": 60440882,
            "link": "https://stackoverflow.com/questions/60440882/remove-unnamed-colums-pandas-dataframe",
            "question": {
                "title": "remove unnamed colums pandas dataframe",
                "ques_desc": "i'm a student and have a problem that i cant figure it out how to solve it.i have csv data like this : code for reading csv like this : SMT print out : expected output : i already trying but it will be like this : i already trying to put or //is not working, and the last time I tried it like this : and i got i just want to get rid the Unnamed: 5 ~ Unnamed: 8, how the correct way to get rid of this Unnamed thing ? "
            },
            "io": [
                " 1          2          3          4\n 6          7          8          9\n11         12         13         14\n",
                "           1          2          3          4\n0          6          7          8          9\n1         11         12         13         14\n2          0          0          0          0\n"
            ],
            "answer": {
                "ans_desc": "The \"unnamed\" just says, that pandas does not know how to name the columns. So these are just names. You could set the names like this in the Output: You have to set so that pandas knows that this is usually the header. Or you set ",
                "code": [
                    "pd.read_csv(\"test.csv\", usecols=(5,6,7,8), skiprows=3, nrows=3, header=0, names=[\"c1\", \"c2\", \"c3\", \"c4\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "series"
        ],
        "owner": {
            "reputation": 1731,
            "user_id": 4106261,
            "user_type": "registered",
            "accept_rate": 97,
            "profile_image": "https://lh5.googleusercontent.com/-iJUgSCa5y0A/AAAAAAAAAAI/AAAAAAAADY0/Kjs5T9umLVE/photo.jpg?sz=128",
            "display_name": "Alex Poca",
            "link": "https://stackoverflow.com/users/4106261/alex-poca"
        },
        "is_answered": true,
        "view_count": 318,
        "accepted_answer_id": 60395021,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1582635846,
        "creation_date": 1582634668,
        "question_id": 60394977,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60394977/pandas-how-to-remove-non-alphanumeric-columns-in-series",
        "title": "Pandas: How to remove non-alphanumeric columns in Series",
        "body": "<p>A Pandas' Series can contain invalid values:</p>\n\n<pre><code>a     b     c     d      e      f     g \n1    \"\"   \"a3\"  np.nan  \"\\n\"   \"6\"   \" \"\n</code></pre>\n\n<pre><code>df = pd.DataFrame([{\"a\":1, \"b\":\"\", \"c\":\"a3\", \"d\":np.nan, \"e\":\"\\n\", \"f\":\"6\", \"g\":\" \"}])\nrow = df.iloc[0]\n</code></pre>\n\n<p>I want to produce a clean Series keeping only the columns that contain a <strong>numeric value</strong> or a <strong>non-empty non-space-only alphanumeric string</strong>:</p>\n\n<ul>\n<li><code>b</code> should be dropped because it is an empty string;</li>\n<li><code>d</code> because <code>np.nan</code>;</li>\n<li><code>e</code> and <code>g</code> because space-only strings.</li>\n</ul>\n\n<p>The expected result:</p>\n\n<pre><code>a      c     f\n1    \"a3\"   \"6\"\n</code></pre>\n\n<p><strong>How can I filter the columns that contain numeric or valid alphanumeric?</strong></p>\n\n<ul>\n<li><code>row.str.isalnum()</code> returns <code>NaN</code> for <code>a</code>, instead of the True I would expect.</li>\n<li><code>row.astype(str).str.isalnum()</code> changes <code>d</code>'s <code>np.nan</code> to string <code>\"nan\"</code> and later considers it a valid string.</li>\n<li><code>row.dropna()</code> of course drops only <code>d</code> (<code>np.nan</code>).</li>\n</ul>\n\n<p>I don't see so many other possibilities listed at <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/series.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/series.html</a></p>\n\n<p>As a workaround I can loop on the items() checking type and content, and create a new Series from the values I want to keep, but this approach is inefficient (and ugly):</p>\n\n<pre><code>for index, value in row.items():\n    print (index, value, type(value))\n\n\n# a 1 &lt;class 'numpy.int64'&gt;\n# b  &lt;class 'str'&gt;\n# c a3 &lt;class 'str'&gt;\n# d nan &lt;class 'numpy.float64'&gt;\n# e \n#  &lt;class 'str'&gt;\n# f 6 &lt;class 'str'&gt;\n# g   &lt;class 'str'&gt;\n</code></pre>\n\n<p><strong>Is there any boolean filter that can help me to single out the good columns?</strong> </p>\n",
        "answer_body": "<p>Convert values to strings and chain another mask by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.notna.html\" rel=\"nofollow noreferrer\"><code>Series.notna</code></a> with bitwise <code>AND</code> - <code>&amp;</code>:</p>\n\n<pre><code>row = row[row.astype(str).str.isalnum() &amp; row.notna()]\nprint (row)\na     1\nc    a3\nf     6\nName: 0, dtype: object\n</code></pre>\n",
        "question_body": "<p>A Pandas' Series can contain invalid values:</p>\n\n<pre><code>a     b     c     d      e      f     g \n1    \"\"   \"a3\"  np.nan  \"\\n\"   \"6\"   \" \"\n</code></pre>\n\n<pre><code>df = pd.DataFrame([{\"a\":1, \"b\":\"\", \"c\":\"a3\", \"d\":np.nan, \"e\":\"\\n\", \"f\":\"6\", \"g\":\" \"}])\nrow = df.iloc[0]\n</code></pre>\n\n<p>I want to produce a clean Series keeping only the columns that contain a <strong>numeric value</strong> or a <strong>non-empty non-space-only alphanumeric string</strong>:</p>\n\n<ul>\n<li><code>b</code> should be dropped because it is an empty string;</li>\n<li><code>d</code> because <code>np.nan</code>;</li>\n<li><code>e</code> and <code>g</code> because space-only strings.</li>\n</ul>\n\n<p>The expected result:</p>\n\n<pre><code>a      c     f\n1    \"a3\"   \"6\"\n</code></pre>\n\n<p><strong>How can I filter the columns that contain numeric or valid alphanumeric?</strong></p>\n\n<ul>\n<li><code>row.str.isalnum()</code> returns <code>NaN</code> for <code>a</code>, instead of the True I would expect.</li>\n<li><code>row.astype(str).str.isalnum()</code> changes <code>d</code>'s <code>np.nan</code> to string <code>\"nan\"</code> and later considers it a valid string.</li>\n<li><code>row.dropna()</code> of course drops only <code>d</code> (<code>np.nan</code>).</li>\n</ul>\n\n<p>I don't see so many other possibilities listed at <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/series.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/series.html</a></p>\n\n<p>As a workaround I can loop on the items() checking type and content, and create a new Series from the values I want to keep, but this approach is inefficient (and ugly):</p>\n\n<pre><code>for index, value in row.items():\n    print (index, value, type(value))\n\n\n# a 1 &lt;class 'numpy.int64'&gt;\n# b  &lt;class 'str'&gt;\n# c a3 &lt;class 'str'&gt;\n# d nan &lt;class 'numpy.float64'&gt;\n# e \n#  &lt;class 'str'&gt;\n# f 6 &lt;class 'str'&gt;\n# g   &lt;class 'str'&gt;\n</code></pre>\n\n<p><strong>Is there any boolean filter that can help me to single out the good columns?</strong> </p>\n",
        "formatted_input": {
            "qid": 60394977,
            "link": "https://stackoverflow.com/questions/60394977/pandas-how-to-remove-non-alphanumeric-columns-in-series",
            "question": {
                "title": "Pandas: How to remove non-alphanumeric columns in Series",
                "ques_desc": "A Pandas' Series can contain invalid values: I want to produce a clean Series keeping only the columns that contain a numeric value or a non-empty non-space-only alphanumeric string: should be dropped because it is an empty string; because ; and because space-only strings. The expected result: How can I filter the columns that contain numeric or valid alphanumeric? returns for , instead of the True I would expect. changes 's to string and later considers it a valid string. of course drops only (). I don't see so many other possibilities listed at https://pandas.pydata.org/pandas-docs/stable/reference/series.html As a workaround I can loop on the items() checking type and content, and create a new Series from the values I want to keep, but this approach is inefficient (and ugly): Is there any boolean filter that can help me to single out the good columns? "
            },
            "io": [
                "a     b     c     d      e      f     g \n1    \"\"   \"a3\"  np.nan  \"\\n\"   \"6\"   \" \"\n",
                "a      c     f\n1    \"a3\"   \"6\"\n"
            ],
            "answer": {
                "ans_desc": "Convert values to strings and chain another mask by with bitwise - : ",
                "code": [
                    "row = row[row.astype(str).str.isalnum() & row.notna()]\nprint (row)\na     1\nc    a3\nf     6\nName: 0, dtype: object\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 83,
            "user_id": 4229867,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-NcCLOVybdpo/AAAAAAAAAAI/AAAAAAAAAsA/VdGNbOeMfUs/photo.jpg?sz=128",
            "display_name": "Inderjeet Singh",
            "link": "https://stackoverflow.com/users/4229867/inderjeet-singh"
        },
        "is_answered": true,
        "view_count": 76,
        "accepted_answer_id": 60382731,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1582573662,
        "creation_date": 1582566039,
        "last_edit_date": 1582573662,
        "question_id": 60381266,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60381266/check-for-each-value-in-column-has-only-one-corresponding-value-in-another-colum",
        "title": "Check for each value in column has only one corresponding value in another column pandas",
        "body": "<p>I have my data in pandas data frame as follows:</p>\n\n<pre><code>df = pd.DataFrame({'ID': [1234, 1234, 1234, 5678, 5678, 5678, 9999, 9999, 1234, 1234, 1234, 1234],\n 'Name': ['AA', 'AA', 'AA', 'BB', 'BB', 'DD', 'EE', 'EE', 'CC', 'CC', 'BB', 'BB'])\n</code></pre>\n\n<pre><code>      ID Name\n0   1234   AA\n1   1234   AA\n2   1234   AA\n3   5678   BB\n4   5678   BB\n5   5678   DD\n6   9999   EE\n7   9999   EE\n8   1234   CC\n9   1234   CC\n10  1234   BB\n11  1234   BB\n</code></pre>\n\n<p>I would like to add another field in this dataframe(with True/False) such that. for each id value, there should be only one corresponding values.</p>\n\n<p>So, my expected output looks like this.. \nfor the id - 1234 there are two corresponding values (AA and BB), the one with lesser count should be flagged.</p>\n\n<pre><code>      ID Name\n5   5678   DD\n8   1234   CC\n9   1234   CC\n10  1234   BB\n11  1234   BB\n</code></pre>\n",
        "answer_body": "<p>Here's my solution:</p>\n\n<pre><code># toy data\ndf = pd.DataFrame({'ID': [1234, 1234, 1234, 5678, 5678, 5678, 9999, 9999, 1234, 1234, 1234, 1234],\n 'Name': ['AA', 'AA', 'AA', 'BB', 'BB', 'DD', 'EE', 'EE', 'CC', 'CC', 'BB', 'BB']}\n)\n\n# filter those ID's that appear with multiple names\nnon_unique = df.groupby('ID').Name.transform('nunique').ne(1)\ndf = df[non_unique]\n\n# count the occurrences of each combination ['ID','Name']\ncounts = df[non_unique].groupby(['ID','Name']).Name.transform('count')\n\n# filter those with minimal occurrences within each ID\ndf[counts == counts.groupby(df['ID']).transform('min')]\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>      ID Name\n5   5678   DD\n8   1234   CC\n9   1234   CC\n10  1234   BB\n11  1234   BB\n</code></pre>\n",
        "question_body": "<p>I have my data in pandas data frame as follows:</p>\n\n<pre><code>df = pd.DataFrame({'ID': [1234, 1234, 1234, 5678, 5678, 5678, 9999, 9999, 1234, 1234, 1234, 1234],\n 'Name': ['AA', 'AA', 'AA', 'BB', 'BB', 'DD', 'EE', 'EE', 'CC', 'CC', 'BB', 'BB'])\n</code></pre>\n\n<pre><code>      ID Name\n0   1234   AA\n1   1234   AA\n2   1234   AA\n3   5678   BB\n4   5678   BB\n5   5678   DD\n6   9999   EE\n7   9999   EE\n8   1234   CC\n9   1234   CC\n10  1234   BB\n11  1234   BB\n</code></pre>\n\n<p>I would like to add another field in this dataframe(with True/False) such that. for each id value, there should be only one corresponding values.</p>\n\n<p>So, my expected output looks like this.. \nfor the id - 1234 there are two corresponding values (AA and BB), the one with lesser count should be flagged.</p>\n\n<pre><code>      ID Name\n5   5678   DD\n8   1234   CC\n9   1234   CC\n10  1234   BB\n11  1234   BB\n</code></pre>\n",
        "formatted_input": {
            "qid": 60381266,
            "link": "https://stackoverflow.com/questions/60381266/check-for-each-value-in-column-has-only-one-corresponding-value-in-another-colum",
            "question": {
                "title": "Check for each value in column has only one corresponding value in another column pandas",
                "ques_desc": "I have my data in pandas data frame as follows: I would like to add another field in this dataframe(with True/False) such that. for each id value, there should be only one corresponding values. So, my expected output looks like this.. for the id - 1234 there are two corresponding values (AA and BB), the one with lesser count should be flagged. "
            },
            "io": [
                "      ID Name\n0   1234   AA\n1   1234   AA\n2   1234   AA\n3   5678   BB\n4   5678   BB\n5   5678   DD\n6   9999   EE\n7   9999   EE\n8   1234   CC\n9   1234   CC\n10  1234   BB\n11  1234   BB\n",
                "      ID Name\n5   5678   DD\n8   1234   CC\n9   1234   CC\n10  1234   BB\n11  1234   BB\n"
            ],
            "answer": {
                "ans_desc": "Here's my solution: Output: ",
                "code": [
                    "# toy data\ndf = pd.DataFrame({'ID': [1234, 1234, 1234, 5678, 5678, 5678, 9999, 9999, 1234, 1234, 1234, 1234],\n 'Name': ['AA', 'AA', 'AA', 'BB', 'BB', 'DD', 'EE', 'EE', 'CC', 'CC', 'BB', 'BB']}\n)\n\n# filter those ID's that appear with multiple names\nnon_unique = df.groupby('ID').Name.transform('nunique').ne(1)\ndf = df[non_unique]\n\n# count the occurrences of each combination ['ID','Name']\ncounts = df[non_unique].groupby(['ID','Name']).Name.transform('count')\n\n# filter those with minimal occurrences within each ID\ndf[counts == counts.groupby(df['ID']).transform('min')]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 151,
            "user_id": 11815537,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f40f14075c65ebb4c7a9a82c346d4745?s=128&d=identicon&r=PG&f=1",
            "display_name": "Alessandrini",
            "link": "https://stackoverflow.com/users/11815537/alessandrini"
        },
        "is_answered": true,
        "view_count": 69,
        "closed_date": 1582287685,
        "accepted_answer_id": 60336897,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1582282890,
        "creation_date": 1582280436,
        "last_edit_date": 1582281304,
        "question_id": 60336550,
        "link": "https://stackoverflow.com/questions/60336550/use-duplicated-values-to-increment-column",
        "closed_reason": "Needs more focus",
        "title": "Use duplicated values to increment column",
        "body": "<p>I have a Pandas dataframe and I want to increment a column based on the amount of duplicated values. So when a duplicate is found, all other occurrences is incremented. So given this input dataframe</p>\n\n<pre><code>    SM\n 0  AB\n 1  AC\n 2  AD\n 3  AB\n 4  AB\n 5  AC\n 6  AE\n 7  AD\n</code></pre>\n\n<p>return</p>\n\n<pre><code>     SM DM\n  0  AB AB\n  1  AC AC\n  2  AD AD\n  3  AB AB_1\n  4  AB AB_2\n  5  AC AC_1\n  6  AE AE\n  7  AD AD_1\n</code></pre>\n\n<p>I tried this line of code but I don't know how to increment</p>\n\n<pre><code> np.where(a.SM.duplicated(keep='first'), a.SM+'_1', a.SM)\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html\" rel=\"nofollow noreferrer\"><code>groupby.cumcount</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.where.html\" rel=\"nofollow noreferrer\"><code>Series.where</code></a>:</p>\n\n<pre><code>s = df.groupby('SM').cumcount()\n\ndf['DM'] = df['SM'].where(s.eq(0), df['SM'] + '_' + s.astype(str))\n</code></pre>\n\n<p>[out]</p>\n\n<pre><code>   SM    DM\n0  AB    AB\n1  AC    AC\n2  AD    AD\n3  AB  AB_1\n4  AB  AB_2\n5  AC  AC_1\n6  AE    AE\n7  AD  AD_1\n</code></pre>\n",
        "question_body": "<p>I have a Pandas dataframe and I want to increment a column based on the amount of duplicated values. So when a duplicate is found, all other occurrences is incremented. So given this input dataframe</p>\n\n<pre><code>    SM\n 0  AB\n 1  AC\n 2  AD\n 3  AB\n 4  AB\n 5  AC\n 6  AE\n 7  AD\n</code></pre>\n\n<p>return</p>\n\n<pre><code>     SM DM\n  0  AB AB\n  1  AC AC\n  2  AD AD\n  3  AB AB_1\n  4  AB AB_2\n  5  AC AC_1\n  6  AE AE\n  7  AD AD_1\n</code></pre>\n\n<p>I tried this line of code but I don't know how to increment</p>\n\n<pre><code> np.where(a.SM.duplicated(keep='first'), a.SM+'_1', a.SM)\n</code></pre>\n",
        "formatted_input": {
            "qid": 60336550,
            "link": "https://stackoverflow.com/questions/60336550/use-duplicated-values-to-increment-column",
            "question": {
                "title": "Use duplicated values to increment column",
                "ques_desc": "I have a Pandas dataframe and I want to increment a column based on the amount of duplicated values. So when a duplicate is found, all other occurrences is incremented. So given this input dataframe return I tried this line of code but I don't know how to increment "
            },
            "io": [
                "    SM\n 0  AB\n 1  AC\n 2  AD\n 3  AB\n 4  AB\n 5  AC\n 6  AE\n 7  AD\n",
                "     SM DM\n  0  AB AB\n  1  AC AC\n  2  AD AD\n  3  AB AB_1\n  4  AB AB_2\n  5  AC AC_1\n  6  AE AE\n  7  AD AD_1\n"
            ],
            "answer": {
                "ans_desc": "Use and : [out] ",
                "code": [
                    "s = df.groupby('SM').cumcount()\n\ndf['DM'] = df['SM'].where(s.eq(0), df['SM'] + '_' + s.astype(str))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "replace",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 534,
            "user_id": 9008162,
            "user_type": "registered",
            "accept_rate": 89,
            "profile_image": "https://www.gravatar.com/avatar/173472de06313d25656f7f91bc143a86?s=128&d=identicon&r=PG&f=1",
            "display_name": "saga",
            "link": "https://stackoverflow.com/users/9008162/saga"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 60307235,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1582145404,
        "creation_date": 1582132468,
        "last_edit_date": 1582145404,
        "question_id": 60305830,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60305830/how-to-replace-the-value-of-a-dataframe-column-with-the-value-of-another-column",
        "title": "How to replace the value of a dataframe column with the value of another column using groupby.first()?",
        "body": "<p>I have a df like this:</p>\n\n<pre><code>               Value1     Value2\n2008-01-01       -1          4\n2008-01-01       -1          5\n2008-01-03       -1          6\n2008-02-25        0          7\n2008-02-26       -1          8\n2008-02-27        0          9\n2008-03-02        5         10 \n2008-03-16       -1         11\n2008-03-17       -1         12 \n2009-04-04       -1         13\n2009-04-07        0         14\n</code></pre>\n\n<p>I want to check the <strong>first</strong> <code>value1</code> of every Year-Month. If it's &lt; 0, I want value2 to replace with value1. How can I do that? </p>\n\n<p>In this example, the result should be: </p>\n\n<pre><code>               Value1     Value2\n2008-01-01       -1         -1\n2008-01-01       -1          5\n2008-01-03       -1          6\n2008-02-25        0          7\n2008-02-26       -1          8\n2008-02-27        0          9\n2008-03-02        5         10 \n2008-03-16       -1         11\n2008-03-17       -1         12 \n2009-04-04       -1         -1\n2009-04-07        0         14\n</code></pre>\n\n<p>Because only <code>2008-01, 2009-04</code> <strong>first</strong> <code>value1</code> are negative, <code>2008-2, 2008-03</code> <strong>first</strong> <code>value1</code> are positive, just leave it.</p>\n\n<p>I used: </p>\n\n<pre><code>g = df.groupby([df.index.year,df.index.month])\n\nif g['value1'].first() &lt; 0:\n   g['value1'].first() = g['value2'].first()\n</code></pre>\n\n<p>it doesn't seem to work. Thanks.</p>\n",
        "answer_body": "<p>My approach with <code>groupby().head()</code> to extract the index and <code>loc</code> to update:</p>\n\n<pre><code>s = df.groupby(df.index.to_period('M'), as_index=False).head(1)\ndf.loc[s[s['Value1'].lt(0)].index, 'Value1'] = df['Value2']\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>            Value1  Value2\n2008-01-01       5       5\n2008-01-03      -1       6\n2008-02-25       0       7\n2008-02-26      -1       8\n2008-02-27       0       9\n2008-03-02       5      10\n2008-03-16      -1      11\n2008-03-17      -1      12\n2009-04-04      13      13\n2009-04-07       0      14\n</code></pre>\n",
        "question_body": "<p>I have a df like this:</p>\n\n<pre><code>               Value1     Value2\n2008-01-01       -1          4\n2008-01-01       -1          5\n2008-01-03       -1          6\n2008-02-25        0          7\n2008-02-26       -1          8\n2008-02-27        0          9\n2008-03-02        5         10 \n2008-03-16       -1         11\n2008-03-17       -1         12 \n2009-04-04       -1         13\n2009-04-07        0         14\n</code></pre>\n\n<p>I want to check the <strong>first</strong> <code>value1</code> of every Year-Month. If it's &lt; 0, I want value2 to replace with value1. How can I do that? </p>\n\n<p>In this example, the result should be: </p>\n\n<pre><code>               Value1     Value2\n2008-01-01       -1         -1\n2008-01-01       -1          5\n2008-01-03       -1          6\n2008-02-25        0          7\n2008-02-26       -1          8\n2008-02-27        0          9\n2008-03-02        5         10 \n2008-03-16       -1         11\n2008-03-17       -1         12 \n2009-04-04       -1         -1\n2009-04-07        0         14\n</code></pre>\n\n<p>Because only <code>2008-01, 2009-04</code> <strong>first</strong> <code>value1</code> are negative, <code>2008-2, 2008-03</code> <strong>first</strong> <code>value1</code> are positive, just leave it.</p>\n\n<p>I used: </p>\n\n<pre><code>g = df.groupby([df.index.year,df.index.month])\n\nif g['value1'].first() &lt; 0:\n   g['value1'].first() = g['value2'].first()\n</code></pre>\n\n<p>it doesn't seem to work. Thanks.</p>\n",
        "formatted_input": {
            "qid": 60305830,
            "link": "https://stackoverflow.com/questions/60305830/how-to-replace-the-value-of-a-dataframe-column-with-the-value-of-another-column",
            "question": {
                "title": "How to replace the value of a dataframe column with the value of another column using groupby.first()?",
                "ques_desc": "I have a df like this: I want to check the first of every Year-Month. If it's < 0, I want value2 to replace with value1. How can I do that? In this example, the result should be: Because only first are negative, first are positive, just leave it. I used: it doesn't seem to work. Thanks. "
            },
            "io": [
                "               Value1     Value2\n2008-01-01       -1          4\n2008-01-01       -1          5\n2008-01-03       -1          6\n2008-02-25        0          7\n2008-02-26       -1          8\n2008-02-27        0          9\n2008-03-02        5         10 \n2008-03-16       -1         11\n2008-03-17       -1         12 \n2009-04-04       -1         13\n2009-04-07        0         14\n",
                "               Value1     Value2\n2008-01-01       -1         -1\n2008-01-01       -1          5\n2008-01-03       -1          6\n2008-02-25        0          7\n2008-02-26       -1          8\n2008-02-27        0          9\n2008-03-02        5         10 \n2008-03-16       -1         11\n2008-03-17       -1         12 \n2009-04-04       -1         -1\n2009-04-07        0         14\n"
            ],
            "answer": {
                "ans_desc": "My approach with to extract the index and to update: Output: ",
                "code": [
                    "s = df.groupby(df.index.to_period('M'), as_index=False).head(1)\ndf.loc[s[s['Value1'].lt(0)].index, 'Value1'] = df['Value2']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 166,
            "user_id": 7301203,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-f0J_JCyvTDI/AAAAAAAAAAI/AAAAAAAAAh8/f61V2wxWTTc/photo.jpg?sz=128",
            "display_name": "Ran Elgiser",
            "link": "https://stackoverflow.com/users/7301203/ran-elgiser"
        },
        "is_answered": true,
        "view_count": 58,
        "accepted_answer_id": 60279721,
        "answer_count": 3,
        "score": -1,
        "last_activity_date": 1582026594,
        "creation_date": 1582023662,
        "question_id": 60279369,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60279369/python-dataframe-merge-columns-according-to-other-column-values",
        "title": "python dataframe merge columns according to other column values",
        "body": "<p>What I want to do is merge columns according to values in another column\nIt is better illustrated with a simple example:\nI have a dataframe with 5 columns:</p>\n\n<pre><code>| player_num    | team_1.x  | team_1.y  | team_2.x  | team_2.y  |\n|------------   |---------- |---------- |---------- |---------- |\n| 1             | x_1       | y_1       | x_2       | y_2       |\n| 4             | x_3       | y_3       | x_4       | y_4       |\n| 8             | x_5       | y_5       | x_6       | y_6       |\n</code></pre>\n\n<p>I want to get the following table:</p>\n\n<pre><code>| x     | y     |\n|-----  |-----  |\n| x_1   | y_1   |\n| x_3   | y_3   |\n| x_6   | y_6   |\n</code></pre>\n\n<p>where the columns are filled with values from team_1.x and team_1.y for rows of players with number less than 5 and values from team_2.x and team_2.y for rows of players with number bigger than 5</p>\n",
        "answer_body": "<p>You can use Numpy's np.where for this:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\n...\ndf['x'] = np.where(df['player_num'] &lt; 5, df['team_1.x'], df['team_2.x'])\ndf['y'] = np.where(df['player_num'] &lt; 5, df['team_1.y'], df['team_2.y'])\n</code></pre>\n\n<p>EDIT:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Extract column names and remove prefix to get a list of x,y,z, etc.\ncols = [col.split('.')[1] for col in list(df) if 'team_' in col]\n\n# Loop over and create new column for each prefix (x, y, z, etc)\nfor col in cols:\n    col1 = 'team_1.' + col\n    col2 = 'team_2.' + col\n    df[col] = np.where(df['player_num']&lt;5, df[col1], df[col2])\n</code></pre>\n",
        "question_body": "<p>What I want to do is merge columns according to values in another column\nIt is better illustrated with a simple example:\nI have a dataframe with 5 columns:</p>\n\n<pre><code>| player_num    | team_1.x  | team_1.y  | team_2.x  | team_2.y  |\n|------------   |---------- |---------- |---------- |---------- |\n| 1             | x_1       | y_1       | x_2       | y_2       |\n| 4             | x_3       | y_3       | x_4       | y_4       |\n| 8             | x_5       | y_5       | x_6       | y_6       |\n</code></pre>\n\n<p>I want to get the following table:</p>\n\n<pre><code>| x     | y     |\n|-----  |-----  |\n| x_1   | y_1   |\n| x_3   | y_3   |\n| x_6   | y_6   |\n</code></pre>\n\n<p>where the columns are filled with values from team_1.x and team_1.y for rows of players with number less than 5 and values from team_2.x and team_2.y for rows of players with number bigger than 5</p>\n",
        "formatted_input": {
            "qid": 60279369,
            "link": "https://stackoverflow.com/questions/60279369/python-dataframe-merge-columns-according-to-other-column-values",
            "question": {
                "title": "python dataframe merge columns according to other column values",
                "ques_desc": "What I want to do is merge columns according to values in another column It is better illustrated with a simple example: I have a dataframe with 5 columns: I want to get the following table: where the columns are filled with values from team_1.x and team_1.y for rows of players with number less than 5 and values from team_2.x and team_2.y for rows of players with number bigger than 5 "
            },
            "io": [
                "| player_num    | team_1.x  | team_1.y  | team_2.x  | team_2.y  |\n|------------   |---------- |---------- |---------- |---------- |\n| 1             | x_1       | y_1       | x_2       | y_2       |\n| 4             | x_3       | y_3       | x_4       | y_4       |\n| 8             | x_5       | y_5       | x_6       | y_6       |\n",
                "| x     | y     |\n|-----  |-----  |\n| x_1   | y_1   |\n| x_3   | y_3   |\n| x_6   | y_6   |\n"
            ],
            "answer": {
                "ans_desc": "You can use Numpy's np.where for this: EDIT: ",
                "code": [
                    "import numpy as np\n...\ndf['x'] = np.where(df['player_num'] < 5, df['team_1.x'], df['team_2.x'])\ndf['y'] = np.where(df['player_num'] < 5, df['team_1.y'], df['team_2.y'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "dataframe",
            "pandas"
        ],
        "owner": {
            "reputation": 437,
            "user_id": 2005075,
            "user_type": "registered",
            "accept_rate": 80,
            "profile_image": "https://www.gravatar.com/avatar/88d7936aeb665135e6fe06ec12a27cc8?s=128&d=identicon&r=PG",
            "display_name": "naz",
            "link": "https://stackoverflow.com/users/2005075/naz"
        },
        "is_answered": true,
        "view_count": 93299,
        "accepted_answer_id": 15112264,
        "answer_count": 2,
        "score": 36,
        "last_activity_date": 1581787929,
        "creation_date": 1361968399,
        "last_edit_date": 1578420263,
        "question_id": 15112234,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/15112234/how-can-i-convert-columns-of-a-pandas-dataframe-into-a-list-of-lists",
        "title": "How can I convert columns of a pandas DataFrame into a list of lists?",
        "body": "<p>I have a pandas DataFrame with multiple columns.</p>\n\n<pre><code>2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\n</code></pre>\n\n<p>What I want to do is to convert this <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\" rel=\"noreferrer\"><code>pandas.DataFrame</code></a> into a list like following</p>\n\n<pre><code>X = [\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1],\n     [1, 0, 0, 0, 1, 1],\n     [0, 1, 1, 0, 0, 0],\n     [0, 0, 0, 1, 0, 0],\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1]\n    ]\n</code></pre>\n\n<p>2u    2s    4r     4n     4m   7h   7v   are column headings. It will change in different situations, so don't bother about it.</p>\n",
        "answer_body": "<p>It looks like a transposed matrix:</p>\n\n<pre><code>df.values.T.tolist()\n</code></pre>\n\n<hr>\n\n<pre><code>[list(l) for l in zip(*df.values)]\n</code></pre>\n\n<p></p>\n\n<pre><code>[[0, 0, 1, 1, 1, 0],\n [1, 1, 0, 0, 0, 1],\n [1, 0, 0, 0, 1, 1],\n [0, 1, 1, 0, 0, 0],\n [0, 0, 0, 1, 0, 0],\n [0, 0, 1, 1, 1, 0],\n [1, 1, 0, 0, 0, 1]]\n</code></pre>\n",
        "question_body": "<p>I have a pandas DataFrame with multiple columns.</p>\n\n<pre><code>2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\n</code></pre>\n\n<p>What I want to do is to convert this <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\" rel=\"noreferrer\"><code>pandas.DataFrame</code></a> into a list like following</p>\n\n<pre><code>X = [\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1],\n     [1, 0, 0, 0, 1, 1],\n     [0, 1, 1, 0, 0, 0],\n     [0, 0, 0, 1, 0, 0],\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1]\n    ]\n</code></pre>\n\n<p>2u    2s    4r     4n     4m   7h   7v   are column headings. It will change in different situations, so don't bother about it.</p>\n",
        "formatted_input": {
            "qid": 15112234,
            "link": "https://stackoverflow.com/questions/15112234/how-can-i-convert-columns-of-a-pandas-dataframe-into-a-list-of-lists",
            "question": {
                "title": "How can I convert columns of a pandas DataFrame into a list of lists?",
                "ques_desc": "I have a pandas DataFrame with multiple columns. What I want to do is to convert this into a list like following 2u 2s 4r 4n 4m 7h 7v are column headings. It will change in different situations, so don't bother about it. "
            },
            "io": [
                "2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\n",
                "X = [\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1],\n     [1, 0, 0, 0, 1, 1],\n     [0, 1, 1, 0, 0, 0],\n     [0, 0, 0, 1, 0, 0],\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1]\n    ]\n"
            ],
            "answer": {
                "ans_desc": "It looks like a transposed matrix: ",
                "code": [
                    "[list(l) for l in zip(*df.values)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "replace",
            "dataframe"
        ],
        "owner": {
            "reputation": 1996,
            "user_id": 5318634,
            "user_type": "registered",
            "accept_rate": 91,
            "profile_image": "https://lh4.googleusercontent.com/-WLoDLz8e0ag/AAAAAAAAAAI/AAAAAAAABFE/hE7C6EgHNhI/photo.jpg?sz=128",
            "display_name": "Pablo",
            "link": "https://stackoverflow.com/users/5318634/pablo"
        },
        "is_answered": true,
        "view_count": 57304,
        "accepted_answer_id": 39903944,
        "answer_count": 3,
        "score": 24,
        "last_activity_date": 1581602443,
        "creation_date": 1475778093,
        "last_edit_date": 1581602443,
        "question_id": 39903090,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/39903090/efficiently-replace-values-from-a-column-to-another-column-pandas-dataframe",
        "title": "Efficiently replace values from a column to another column Pandas DataFrame",
        "body": "<p>I have a Pandas DataFrame like this: </p>\n\n<pre><code>   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3     0  0.4  0.4\n4     0    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n</code></pre>\n\n<p>I want to replace the <code>col1</code> values with the values in the second column (<code>col2</code>) only if <code>col1</code> values are equal to 0, and after (for the zero values remaining),  do it again but with the third column (<code>col3</code>). The Desired Result is the next one:</p>\n\n<pre><code>   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3   0.4  0.4  0.4\n4   0.3    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n</code></pre>\n\n<p>I did it using the <code>pd.replace</code> function, but it seems too slow.. I think must be a faster way to accomplish that. </p>\n\n<pre><code>df.col1.replace(0,df.col2,inplace=True)\ndf.col1.replace(0,df.col3,inplace=True)\n</code></pre>\n\n<p>is there a faster way to do that?, using some other function instead of the <code>pd.replace</code> function?</p>\n",
        "answer_body": "<p>Using <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\" rel=\"noreferrer\"><code>np.where</code></a> is faster.  Using a similar pattern as you used with <code>replace</code>:</p>\n\n<pre><code>df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\ndf['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n</code></pre>\n\n<p>However, using a nested <code>np.where</code> is slightly faster:</p>\n\n<pre><code>df['col1'] = np.where(df['col1'] == 0, \n                      np.where(df['col2'] == 0, df['col3'], df['col2']),\n                      df['col1'])\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<p>Using the following setup to produce a larger sample DataFrame and timing functions:</p>\n\n<pre><code>df = pd.concat([df]*10**4, ignore_index=True)\n\ndef root_nested(df):\n    df['col1'] = np.where(df['col1'] == 0, np.where(df['col2'] == 0, df['col3'], df['col2']), df['col1'])\n    return df\n\ndef root_split(df):\n    df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\n    df['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n    return df\n\ndef pir2(df):\n    df['col1'] = df.where(df.ne(0), np.nan).bfill(axis=1).col1.fillna(0)\n    return df\n\ndef pir2_2(df):\n    slc = (df.values != 0).argmax(axis=1)\n    return df.values[np.arange(slc.shape[0]), slc]\n\ndef andrew(df):\n    df.col1[df.col1 == 0] = df.col2\n    df.col1[df.col1 == 0] = df.col3\n    return df\n\ndef pablo(df):\n    df['col1'] = df['col1'].replace(0,df['col2'])\n    df['col1'] = df['col1'].replace(0,df['col3'])\n    return df\n</code></pre>\n\n<p>I get the following timings:</p>\n\n<pre><code>%timeit root_nested(df.copy())\n100 loops, best of 3: 2.25 ms per loop\n\n%timeit root_split(df.copy())\n100 loops, best of 3: 2.62 ms per loop\n\n%timeit pir2(df.copy())\n100 loops, best of 3: 6.25 ms per loop\n\n%timeit pir2_2(df.copy())\n1 loop, best of 3: 2.4 ms per loop\n\n%timeit andrew(df.copy())\n100 loops, best of 3: 8.55 ms per loop\n</code></pre>\n\n<p>I tried timing your method, but it's been running for multiple minutes without completing.  As a comparison, timing your method on just the 6 row example DataFrame (not the much larger one tested above) took 12.8 ms.</p>\n",
        "question_body": "<p>I have a Pandas DataFrame like this: </p>\n\n<pre><code>   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3     0  0.4  0.4\n4     0    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n</code></pre>\n\n<p>I want to replace the <code>col1</code> values with the values in the second column (<code>col2</code>) only if <code>col1</code> values are equal to 0, and after (for the zero values remaining),  do it again but with the third column (<code>col3</code>). The Desired Result is the next one:</p>\n\n<pre><code>   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3   0.4  0.4  0.4\n4   0.3    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n</code></pre>\n\n<p>I did it using the <code>pd.replace</code> function, but it seems too slow.. I think must be a faster way to accomplish that. </p>\n\n<pre><code>df.col1.replace(0,df.col2,inplace=True)\ndf.col1.replace(0,df.col3,inplace=True)\n</code></pre>\n\n<p>is there a faster way to do that?, using some other function instead of the <code>pd.replace</code> function?</p>\n",
        "formatted_input": {
            "qid": 39903090,
            "link": "https://stackoverflow.com/questions/39903090/efficiently-replace-values-from-a-column-to-another-column-pandas-dataframe",
            "question": {
                "title": "Efficiently replace values from a column to another column Pandas DataFrame",
                "ques_desc": "I have a Pandas DataFrame like this: I want to replace the values with the values in the second column () only if values are equal to 0, and after (for the zero values remaining), do it again but with the third column (). The Desired Result is the next one: I did it using the function, but it seems too slow.. I think must be a faster way to accomplish that. is there a faster way to do that?, using some other function instead of the function? "
            },
            "io": [
                "   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3     0  0.4  0.4\n4     0    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n",
                "   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3   0.4  0.4  0.4\n4   0.3    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n"
            ],
            "answer": {
                "ans_desc": "Using is faster. Using a similar pattern as you used with : However, using a nested is slightly faster: Timings Using the following setup to produce a larger sample DataFrame and timing functions: I get the following timings: I tried timing your method, but it's been running for multiple minutes without completing. As a comparison, timing your method on just the 6 row example DataFrame (not the much larger one tested above) took 12.8 ms. ",
                "code": [
                    "df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\ndf['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n",
                    "df['col1'] = np.where(df['col1'] == 0, \n                      np.where(df['col2'] == 0, df['col3'], df['col2']),\n                      df['col1'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe",
            "split"
        ],
        "owner": {
            "reputation": 5293,
            "user_id": 8410477,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dVgCN.jpg?s=128&g=1",
            "display_name": "ah bon",
            "link": "https://stackoverflow.com/users/8410477/ah-bon"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 60203666,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1581583688,
        "creation_date": 1581581825,
        "last_edit_date": 1581582775,
        "question_id": 60203245,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60203245/split-multiple-columns-by-numeric-or-alphabetic-symbols",
        "title": "Split multiple columns by numeric or alphabetic symbols",
        "body": "<p>I'm working on splitting multiple columns by numeric or alphabetic symbols for columns <code>v1</code> to <code>v3</code>, then take the first part as the values of this column. For example, <code>\u7ea2\u5c97\u82b1\u56ed12\u680b110\u623f</code> will be split by <code>12</code> then take <code>\u7ea2\u5c97\u82b1\u56ed</code>, <code>\u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1C4\u680bC5\u680bC4\u5ea71403\u623f</code> will be splited by <code>C4</code> and take <code>\u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1</code>.</p>\n\n<pre><code>    id       v1                      v2                     v3\n0    1      \u6ce5\u5c97\u8def             \u7ea2\u5c97\u82b1\u56ed12\u680b110\u623f                    NaN\n1    2     \u6c99\u4e95\u8857\u9053                     \u4e07\u4e30\u8def                     \u4e1c\u4fa7\n2    3      \u4e2d\u5fc3\u533a                    N15\u533a          \u5e78\u798f\u00b7\u6d77\u5cb810\u680bA\u5ea711A\n3    4      \u9f99\u5c97\u9547                     \u5357\u8054\u6751       \u957f\u6d77\u96c5\u56ed2\u680bD301D302\u623f\u4ea7\n4    5    \u86c7\u53e3\u5de5\u4e1a\u533a                     \u5174\u534e\u8def  \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c06\u680b504\u623f\u4ea7\n5    6      \u5b9d\u5b89\u8def         \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b103                    NaN\n6    7      \u5b9d\u5b89\u8def         \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b203                    NaN\n7    8      \u9f99\u5c97\u9547                     \u4e2d\u5fc3\u57ce            \u5c1a\u666f\u534e\u56ed12\u680b307\u623f\n8    9     \u6c99\u6cb3\u897f\u8def            \u897f\u535a\u6d77\u540d\u82d11\u680b30C\u623f\u4ea7                    NaN\n9   10  \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def              \u5929\u9e45\u5821\u4e09\u671fP\u680b4D\u623f                    NaN\n10  11      \u5e03\u5409\u9547  \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1C4\u680bC5\u680bC4\u5ea71403\u623f                    NaN\n</code></pre>\n\n<p>The code I have tried:</p>\n\n<pre><code>cols = ['v1', 'v2', 'v3']\ndf[cols] = df[cols].apply(lambda x: ''.join(re.compile(r'(\\d+|\\w+)')[0], x.str))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>TypeError: (\"'_sre.SRE_Pattern' object is not subscriptable\", 'occurred at index v1')\n</code></pre>\n\n<p>My desired output will like this:</p>\n\n<pre><code>    id       v1         v2             v3\n0    1      \u6ce5\u5c97\u8def       \u7ea2\u5c97\u82b1\u56ed            NaN\n1    2     \u6c99\u4e95\u8857\u9053        \u4e07\u4e30\u8def             \u4e1c\u4fa7\n2    3      \u4e2d\u5fc3\u533a        NaN          \u5e78\u798f\u00b7\u6d77\u5cb8\n3    4      \u9f99\u5c97\u9547        \u5357\u8054\u6751           \u957f\u6d77\u96c5\u56ed\n4    5    \u86c7\u53e3\u5de5\u4e1a\u533a        \u5174\u534e\u8def  \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c\n5    6      \u5b9d\u5b89\u8def  \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c            NaN\n6    7      \u5b9d\u5b89\u8def  \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c            NaN\n7    8      \u9f99\u5c97\u9547        \u4e2d\u5fc3\u57ce           \u5c1a\u666f\u534e\u56ed\n8    9     \u6c99\u6cb3\u897f\u8def      \u897f\u535a\u6d77\u540d\u82d1            NaN\n9   10  \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def      \u5929\u9e45\u5821\u4e09\u671f            NaN\n10  11      \u5e03\u5409\u9547   \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1            NaN\n</code></pre>\n\n<p>Thanks for your help.</p>\n",
        "answer_body": "<p>You may remove all text after the first ASCII alphanumeric character in the columns you need to modify:</p>\n\n<pre><code>cols = ['v1', 'v2', 'v3']\ndf[cols] = df[cols].apply(lambda x: x.str.replace(r'[A-Za-z0-9].*', ''))\n</code></pre>\n\n<p>If your columns can contain multiline texts, use</p>\n\n<pre><code>r'(?s)[A-Za-z0-9].*'\n</code></pre>\n\n<p>where <code>(?s)</code> inline modifier will let <code>.</code> match line break chars, too.</p>\n",
        "question_body": "<p>I'm working on splitting multiple columns by numeric or alphabetic symbols for columns <code>v1</code> to <code>v3</code>, then take the first part as the values of this column. For example, <code>\u7ea2\u5c97\u82b1\u56ed12\u680b110\u623f</code> will be split by <code>12</code> then take <code>\u7ea2\u5c97\u82b1\u56ed</code>, <code>\u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1C4\u680bC5\u680bC4\u5ea71403\u623f</code> will be splited by <code>C4</code> and take <code>\u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1</code>.</p>\n\n<pre><code>    id       v1                      v2                     v3\n0    1      \u6ce5\u5c97\u8def             \u7ea2\u5c97\u82b1\u56ed12\u680b110\u623f                    NaN\n1    2     \u6c99\u4e95\u8857\u9053                     \u4e07\u4e30\u8def                     \u4e1c\u4fa7\n2    3      \u4e2d\u5fc3\u533a                    N15\u533a          \u5e78\u798f\u00b7\u6d77\u5cb810\u680bA\u5ea711A\n3    4      \u9f99\u5c97\u9547                     \u5357\u8054\u6751       \u957f\u6d77\u96c5\u56ed2\u680bD301D302\u623f\u4ea7\n4    5    \u86c7\u53e3\u5de5\u4e1a\u533a                     \u5174\u534e\u8def  \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c06\u680b504\u623f\u4ea7\n5    6      \u5b9d\u5b89\u8def         \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b103                    NaN\n6    7      \u5b9d\u5b89\u8def         \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b203                    NaN\n7    8      \u9f99\u5c97\u9547                     \u4e2d\u5fc3\u57ce            \u5c1a\u666f\u534e\u56ed12\u680b307\u623f\n8    9     \u6c99\u6cb3\u897f\u8def            \u897f\u535a\u6d77\u540d\u82d11\u680b30C\u623f\u4ea7                    NaN\n9   10  \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def              \u5929\u9e45\u5821\u4e09\u671fP\u680b4D\u623f                    NaN\n10  11      \u5e03\u5409\u9547  \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1C4\u680bC5\u680bC4\u5ea71403\u623f                    NaN\n</code></pre>\n\n<p>The code I have tried:</p>\n\n<pre><code>cols = ['v1', 'v2', 'v3']\ndf[cols] = df[cols].apply(lambda x: ''.join(re.compile(r'(\\d+|\\w+)')[0], x.str))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>TypeError: (\"'_sre.SRE_Pattern' object is not subscriptable\", 'occurred at index v1')\n</code></pre>\n\n<p>My desired output will like this:</p>\n\n<pre><code>    id       v1         v2             v3\n0    1      \u6ce5\u5c97\u8def       \u7ea2\u5c97\u82b1\u56ed            NaN\n1    2     \u6c99\u4e95\u8857\u9053        \u4e07\u4e30\u8def             \u4e1c\u4fa7\n2    3      \u4e2d\u5fc3\u533a        NaN          \u5e78\u798f\u00b7\u6d77\u5cb8\n3    4      \u9f99\u5c97\u9547        \u5357\u8054\u6751           \u957f\u6d77\u96c5\u56ed\n4    5    \u86c7\u53e3\u5de5\u4e1a\u533a        \u5174\u534e\u8def  \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c\n5    6      \u5b9d\u5b89\u8def  \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c            NaN\n6    7      \u5b9d\u5b89\u8def  \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c            NaN\n7    8      \u9f99\u5c97\u9547        \u4e2d\u5fc3\u57ce           \u5c1a\u666f\u534e\u56ed\n8    9     \u6c99\u6cb3\u897f\u8def      \u897f\u535a\u6d77\u540d\u82d1            NaN\n9   10  \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def      \u5929\u9e45\u5821\u4e09\u671f            NaN\n10  11      \u5e03\u5409\u9547   \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1            NaN\n</code></pre>\n\n<p>Thanks for your help.</p>\n",
        "formatted_input": {
            "qid": 60203245,
            "link": "https://stackoverflow.com/questions/60203245/split-multiple-columns-by-numeric-or-alphabetic-symbols",
            "question": {
                "title": "Split multiple columns by numeric or alphabetic symbols",
                "ques_desc": "I'm working on splitting multiple columns by numeric or alphabetic symbols for columns to , then take the first part as the values of this column. For example, will be split by then take , will be splited by and take . The code I have tried: Output: My desired output will like this: Thanks for your help. "
            },
            "io": [
                "    id       v1                      v2                     v3\n0    1      \u6ce5\u5c97\u8def             \u7ea2\u5c97\u82b1\u56ed12\u680b110\u623f                    NaN\n1    2     \u6c99\u4e95\u8857\u9053                     \u4e07\u4e30\u8def                     \u4e1c\u4fa7\n2    3      \u4e2d\u5fc3\u533a                    N15\u533a          \u5e78\u798f\u00b7\u6d77\u5cb810\u680bA\u5ea711A\n3    4      \u9f99\u5c97\u9547                     \u5357\u8054\u6751       \u957f\u6d77\u96c5\u56ed2\u680bD301D302\u623f\u4ea7\n4    5    \u86c7\u53e3\u5de5\u4e1a\u533a                     \u5174\u534e\u8def  \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c06\u680b504\u623f\u4ea7\n5    6      \u5b9d\u5b89\u8def         \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b103                    NaN\n6    7      \u5b9d\u5b89\u8def         \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b203                    NaN\n7    8      \u9f99\u5c97\u9547                     \u4e2d\u5fc3\u57ce            \u5c1a\u666f\u534e\u56ed12\u680b307\u623f\n8    9     \u6c99\u6cb3\u897f\u8def            \u897f\u535a\u6d77\u540d\u82d11\u680b30C\u623f\u4ea7                    NaN\n9   10  \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def              \u5929\u9e45\u5821\u4e09\u671fP\u680b4D\u623f                    NaN\n10  11      \u5e03\u5409\u9547  \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1C4\u680bC5\u680bC4\u5ea71403\u623f                    NaN\n",
                "    id       v1         v2             v3\n0    1      \u6ce5\u5c97\u8def       \u7ea2\u5c97\u82b1\u56ed            NaN\n1    2     \u6c99\u4e95\u8857\u9053        \u4e07\u4e30\u8def             \u4e1c\u4fa7\n2    3      \u4e2d\u5fc3\u533a        NaN          \u5e78\u798f\u00b7\u6d77\u5cb8\n3    4      \u9f99\u5c97\u9547        \u5357\u8054\u6751           \u957f\u6d77\u96c5\u56ed\n4    5    \u86c7\u53e3\u5de5\u4e1a\u533a        \u5174\u534e\u8def  \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c\n5    6      \u5b9d\u5b89\u8def  \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c            NaN\n6    7      \u5b9d\u5b89\u8def  \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c            NaN\n7    8      \u9f99\u5c97\u9547        \u4e2d\u5fc3\u57ce           \u5c1a\u666f\u534e\u56ed\n8    9     \u6c99\u6cb3\u897f\u8def      \u897f\u535a\u6d77\u540d\u82d1            NaN\n9   10  \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def      \u5929\u9e45\u5821\u4e09\u671f            NaN\n10  11      \u5e03\u5409\u9547   \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1            NaN\n"
            ],
            "answer": {
                "ans_desc": "You may remove all text after the first ASCII alphanumeric character in the columns you need to modify: If your columns can contain multiline texts, use where inline modifier will let match line break chars, too. ",
                "code": [
                    "cols = ['v1', 'v2', 'v3']\ndf[cols] = df[cols].apply(lambda x: x.str.replace(r'[A-Za-z0-9].*', ''))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 32643,
            "user_id": 9081267,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://i.stack.imgur.com/UDzOl.jpg?s=128&g=1",
            "display_name": "Erfan",
            "link": "https://stackoverflow.com/users/9081267/erfan"
        },
        "is_answered": true,
        "view_count": 1087,
        "accepted_answer_id": 56293440,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1581566055,
        "creation_date": 1558703467,
        "last_edit_date": 1558703977,
        "question_id": 56293392,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56293392/renaming-columns-on-slice-of-dataframe-not-performing-as-expected",
        "title": "Renaming columns on slice of dataframe not performing as expected",
        "body": "<p>I was trying to clean up column names in a dataframe but only a part of the columns.</p>\n\n<p>It doesn't work when trying to replace column names on a slice of the dataframe somehow, why is that?</p>\n\n<p>Lets say we have the following dataframe:<br>\n<em><strong>Note</strong>, on the bottom is copy-able code to reproduce the data:</em></p>\n\n<pre><code>   Value ColAfjkj ColBhuqwa ColCouiqw\n0      1        a         e         i\n1      2        b         f         j\n2      3        c         g         k\n3      4        d         h         l\n</code></pre>\n\n<p>I want to clean up the column names (expected output):</p>\n\n<pre><code>   Value ColA ColB ColC\n0      1    a    e    i\n1      2    b    f    j\n2      3    c    g    k\n3      4    d    h    l\n</code></pre>\n\n<hr>\n\n<p><strong>Approach 1</strong>:</p>\n\n<p>I can get the clean column names like this:</p>\n\n<pre><code>df.iloc[:, 1:].columns.str[:4]\n\nIndex(['ColA', 'ColB', 'ColC'], dtype='object')\n</code></pre>\n\n<p>Or</p>\n\n<p><strong>Approach 2</strong>:</p>\n\n<pre><code>s = df.iloc[:, 1:].columns\n[col[:4] for col in s]\n\n['ColA', 'ColB', 'ColC']\n</code></pre>\n\n<hr>\n\n<p><strong>But</strong> when I try to overwrite the column names, nothing happens:</p>\n\n<pre><code>df.iloc[:, 1:].columns = df.iloc[:, 1:].columns.str[:4]\n\n   Value ColAfjkj ColBhuqwa ColCouiqw\n0      1        a         e         i\n1      2        b         f         j\n2      3        c         g         k\n3      4        d         h         l\n</code></pre>\n\n<p>Same for the second approach:</p>\n\n<pre><code>s = df.iloc[:, 1:].columns\ncols = [col[:4] for col in s]\n\ndf.iloc[:, 1:].columns = cols\n\n   Value ColAfjkj ColBhuqwa ColCouiqw\n0      1        a         e         i\n1      2        b         f         j\n2      3        c         g         k\n3      4        d         h         l\n</code></pre>\n\n<hr>\n\n<p><strong>This does work</strong>, but you have to manually concat the name of the first column, which is not ideal:</p>\n\n<pre><code>df.columns = ['Value'] + df.iloc[:, 1:].columns.str[:4].tolist()\n\n   Value ColA ColB ColC\n0      1    a    e    i\n1      2    b    f    j\n2      3    c    g    k\n3      4    d    h    l\n</code></pre>\n\n<p>Is there an easier way to achieve this? Am I missing something?</p>\n\n<hr>\n\n<p>Dataframe for reproduction:</p>\n\n<pre><code>df = pd.DataFrame({'Value':[1,2,3,4],\n                   'ColAfjkj':['a', 'b', 'c', 'd'],\n                   'ColBhuqwa':['e', 'f', 'g', 'h'],\n                   'ColCouiqw':['i', 'j', 'k', 'l']})\n</code></pre>\n",
        "answer_body": "<p>This is because pandas' index is immutable. If you check the documentation for <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html\" rel=\"nofollow noreferrer\"><code>class pandas.Index</code></a>, you'll see that it is defined as:</p>\n\n<blockquote>\n  <p>Immutable ndarray implementing an ordered, sliceable set</p>\n</blockquote>\n\n<p>So in order to modify it you'll have to create a new list of column names, for instance with:</p>\n\n<pre><code>df.columns = [df.columns[0]] + list(df.iloc[:, 1:].columns.str[:4])\n</code></pre>\n\n<hr>\n\n<p>Another option is to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\" rel=\"nofollow noreferrer\"><code>rename</code></a> with a dictionary containing the columns to replace:</p>\n\n<pre><code>df.rename(columns=dict(zip(df.columns[1:], df.columns[1:].str[:4])))\n</code></pre>\n",
        "question_body": "<p>I was trying to clean up column names in a dataframe but only a part of the columns.</p>\n\n<p>It doesn't work when trying to replace column names on a slice of the dataframe somehow, why is that?</p>\n\n<p>Lets say we have the following dataframe:<br>\n<em><strong>Note</strong>, on the bottom is copy-able code to reproduce the data:</em></p>\n\n<pre><code>   Value ColAfjkj ColBhuqwa ColCouiqw\n0      1        a         e         i\n1      2        b         f         j\n2      3        c         g         k\n3      4        d         h         l\n</code></pre>\n\n<p>I want to clean up the column names (expected output):</p>\n\n<pre><code>   Value ColA ColB ColC\n0      1    a    e    i\n1      2    b    f    j\n2      3    c    g    k\n3      4    d    h    l\n</code></pre>\n\n<hr>\n\n<p><strong>Approach 1</strong>:</p>\n\n<p>I can get the clean column names like this:</p>\n\n<pre><code>df.iloc[:, 1:].columns.str[:4]\n\nIndex(['ColA', 'ColB', 'ColC'], dtype='object')\n</code></pre>\n\n<p>Or</p>\n\n<p><strong>Approach 2</strong>:</p>\n\n<pre><code>s = df.iloc[:, 1:].columns\n[col[:4] for col in s]\n\n['ColA', 'ColB', 'ColC']\n</code></pre>\n\n<hr>\n\n<p><strong>But</strong> when I try to overwrite the column names, nothing happens:</p>\n\n<pre><code>df.iloc[:, 1:].columns = df.iloc[:, 1:].columns.str[:4]\n\n   Value ColAfjkj ColBhuqwa ColCouiqw\n0      1        a         e         i\n1      2        b         f         j\n2      3        c         g         k\n3      4        d         h         l\n</code></pre>\n\n<p>Same for the second approach:</p>\n\n<pre><code>s = df.iloc[:, 1:].columns\ncols = [col[:4] for col in s]\n\ndf.iloc[:, 1:].columns = cols\n\n   Value ColAfjkj ColBhuqwa ColCouiqw\n0      1        a         e         i\n1      2        b         f         j\n2      3        c         g         k\n3      4        d         h         l\n</code></pre>\n\n<hr>\n\n<p><strong>This does work</strong>, but you have to manually concat the name of the first column, which is not ideal:</p>\n\n<pre><code>df.columns = ['Value'] + df.iloc[:, 1:].columns.str[:4].tolist()\n\n   Value ColA ColB ColC\n0      1    a    e    i\n1      2    b    f    j\n2      3    c    g    k\n3      4    d    h    l\n</code></pre>\n\n<p>Is there an easier way to achieve this? Am I missing something?</p>\n\n<hr>\n\n<p>Dataframe for reproduction:</p>\n\n<pre><code>df = pd.DataFrame({'Value':[1,2,3,4],\n                   'ColAfjkj':['a', 'b', 'c', 'd'],\n                   'ColBhuqwa':['e', 'f', 'g', 'h'],\n                   'ColCouiqw':['i', 'j', 'k', 'l']})\n</code></pre>\n",
        "formatted_input": {
            "qid": 56293392,
            "link": "https://stackoverflow.com/questions/56293392/renaming-columns-on-slice-of-dataframe-not-performing-as-expected",
            "question": {
                "title": "Renaming columns on slice of dataframe not performing as expected",
                "ques_desc": "I was trying to clean up column names in a dataframe but only a part of the columns. It doesn't work when trying to replace column names on a slice of the dataframe somehow, why is that? Lets say we have the following dataframe: Note, on the bottom is copy-able code to reproduce the data: I want to clean up the column names (expected output): Approach 1: I can get the clean column names like this: Or Approach 2: But when I try to overwrite the column names, nothing happens: Same for the second approach: This does work, but you have to manually concat the name of the first column, which is not ideal: Is there an easier way to achieve this? Am I missing something? Dataframe for reproduction: "
            },
            "io": [
                "   Value ColAfjkj ColBhuqwa ColCouiqw\n0      1        a         e         i\n1      2        b         f         j\n2      3        c         g         k\n3      4        d         h         l\n",
                "   Value ColA ColB ColC\n0      1    a    e    i\n1      2    b    f    j\n2      3    c    g    k\n3      4    d    h    l\n"
            ],
            "answer": {
                "ans_desc": "This is because pandas' index is immutable. If you check the documentation for , you'll see that it is defined as: Immutable ndarray implementing an ordered, sliceable set So in order to modify it you'll have to create a new list of column names, for instance with: Another option is to use with a dictionary containing the columns to replace: ",
                "code": [
                    "df.columns = [df.columns[0]] + list(df.iloc[:, 1:].columns.str[:4])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 73,
            "user_id": 12575476,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9a81206f06d36a1bc76b2ff7ff7f3d18?s=128&d=identicon&r=PG&f=1",
            "display_name": "user0",
            "link": "https://stackoverflow.com/users/12575476/user0"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 60158147,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1581367532,
        "creation_date": 1581363503,
        "last_edit_date": 1581365105,
        "question_id": 60157250,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60157250/pandas-dataframe-efficient-way-to-add-a-column-seconds-since-last-event",
        "title": "Pandas.DataFrame: efficient way to add a column &quot;seconds since last event&quot;",
        "body": "<p>I have a Pandas.DataFrame with a standard index representing seconds, and I want to add a column \"seconds elapsed since last event\" where the events are given in a list. Specifically, say</p>\n\n<pre><code>event = [2, 5]\n</code></pre>\n\n<p>and</p>\n\n<pre><code>df = pd.DataFrame(np.zeros((7, 1)))\n|    |   0 |\n|---:|----:|\n|  0 |   0 |\n|  1 |   0 |\n|  2 |   0 |\n|  3 |   0 |\n|  4 |   0 |\n|  5 |   0 |\n|  6 |   0 |\n</code></pre>\n\n<p>Then I want to obtain</p>\n\n<pre><code>|    |   0 |    x |\n|---:|----:|-----:|\n|  0 |   0 | &lt;NA&gt; |\n|  1 |   0 | &lt;NA&gt; |\n|  2 |   0 |    0 |\n|  3 |   0 |    1 |\n|  4 |   0 |    2 |\n|  5 |   0 |    0 |\n|  6 |   0 |    1 |\n</code></pre>\n\n<p>I tried</p>\n\n<pre><code>df[\"x\"] = pd.Series(range(5)).shift(2)\n\n|    |   0 |   x |\n|---:|----:|----:|\n|  0 |   0 | nan |\n|  1 |   0 | nan |\n|  2 |   0 |   0 |\n|  3 |   0 |   1 |\n|  4 |   0 |   2 |\n|  5 |   0 | nan |\n|  6 |   0 | nan |\n</code></pre>\n\n<p>so apparently to make it work I need to write <code>df[\"x\"] = pd.Series(range(5+2)).shift(2)</code>.</p>\n\n<p>More importantly, when I then do <code>df[\"x\"] = pd.Series(range(2+5)).shift(5)</code> I obtain</p>\n\n<pre><code>|    |   0 |   x |\n|---:|----:|----:|\n|  0 |   0 | nan |\n|  1 |   0 | nan |\n|  2 |   0 | nan |\n|  3 |   0 | nan |\n|  4 |   0 | nan |\n|  5 |   0 |   0 |\n|  6 |   0 |   1 |\n</code></pre>\n\n<p>That is: the previous has been overwritten. Is there a way to assign new values without overwriting existing values by nan ?\nThen, I can do something like</p>\n\n<pre><code>for i in event:\n    df[\"x\"] = pd.Series(range(len(df))).shift(i)\n</code></pre>\n\n<p>Or is there a more efficient way ?</p>\n\n<p>For the record, here is my naive code. It works, but looks inefficient and of poor design:</p>\n\n<pre><code>c = 1000000\ndf[\"x\"] = c\nif event:\n    idx = 0\n    for i in df.itertuples():\n        print(i)\n        if idx &lt; len(event) and i.Index == event[idx]:\n            c = 0\n            idx += 1\n        df.loc[i.Index, \"x\"] = c\n        c += 1\nreturn df\n</code></pre>\n",
        "answer_body": "<p>IIUC, you can do double groupby:</p>\n\n<pre><code>s = df.index.isin(event).cumsum()\n# or equivalently\n# s = df.loc[event, 0].reindex(df.index).isna().cumsum()\n\ndf['x'] = np.where(s&gt;0,df.groupby(s).cumcount(), np.nan)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>     0    x\n0  0.0  NaN\n1  0.0  NaN\n2  0.0  0.0\n3  0.0  1.0\n4  0.0  2.0\n5  0.0  0.0\n6  0.0  1.0\n</code></pre>\n",
        "question_body": "<p>I have a Pandas.DataFrame with a standard index representing seconds, and I want to add a column \"seconds elapsed since last event\" where the events are given in a list. Specifically, say</p>\n\n<pre><code>event = [2, 5]\n</code></pre>\n\n<p>and</p>\n\n<pre><code>df = pd.DataFrame(np.zeros((7, 1)))\n|    |   0 |\n|---:|----:|\n|  0 |   0 |\n|  1 |   0 |\n|  2 |   0 |\n|  3 |   0 |\n|  4 |   0 |\n|  5 |   0 |\n|  6 |   0 |\n</code></pre>\n\n<p>Then I want to obtain</p>\n\n<pre><code>|    |   0 |    x |\n|---:|----:|-----:|\n|  0 |   0 | &lt;NA&gt; |\n|  1 |   0 | &lt;NA&gt; |\n|  2 |   0 |    0 |\n|  3 |   0 |    1 |\n|  4 |   0 |    2 |\n|  5 |   0 |    0 |\n|  6 |   0 |    1 |\n</code></pre>\n\n<p>I tried</p>\n\n<pre><code>df[\"x\"] = pd.Series(range(5)).shift(2)\n\n|    |   0 |   x |\n|---:|----:|----:|\n|  0 |   0 | nan |\n|  1 |   0 | nan |\n|  2 |   0 |   0 |\n|  3 |   0 |   1 |\n|  4 |   0 |   2 |\n|  5 |   0 | nan |\n|  6 |   0 | nan |\n</code></pre>\n\n<p>so apparently to make it work I need to write <code>df[\"x\"] = pd.Series(range(5+2)).shift(2)</code>.</p>\n\n<p>More importantly, when I then do <code>df[\"x\"] = pd.Series(range(2+5)).shift(5)</code> I obtain</p>\n\n<pre><code>|    |   0 |   x |\n|---:|----:|----:|\n|  0 |   0 | nan |\n|  1 |   0 | nan |\n|  2 |   0 | nan |\n|  3 |   0 | nan |\n|  4 |   0 | nan |\n|  5 |   0 |   0 |\n|  6 |   0 |   1 |\n</code></pre>\n\n<p>That is: the previous has been overwritten. Is there a way to assign new values without overwriting existing values by nan ?\nThen, I can do something like</p>\n\n<pre><code>for i in event:\n    df[\"x\"] = pd.Series(range(len(df))).shift(i)\n</code></pre>\n\n<p>Or is there a more efficient way ?</p>\n\n<p>For the record, here is my naive code. It works, but looks inefficient and of poor design:</p>\n\n<pre><code>c = 1000000\ndf[\"x\"] = c\nif event:\n    idx = 0\n    for i in df.itertuples():\n        print(i)\n        if idx &lt; len(event) and i.Index == event[idx]:\n            c = 0\n            idx += 1\n        df.loc[i.Index, \"x\"] = c\n        c += 1\nreturn df\n</code></pre>\n",
        "formatted_input": {
            "qid": 60157250,
            "link": "https://stackoverflow.com/questions/60157250/pandas-dataframe-efficient-way-to-add-a-column-seconds-since-last-event",
            "question": {
                "title": "Pandas.DataFrame: efficient way to add a column &quot;seconds since last event&quot;",
                "ques_desc": "I have a Pandas.DataFrame with a standard index representing seconds, and I want to add a column \"seconds elapsed since last event\" where the events are given in a list. Specifically, say and Then I want to obtain I tried so apparently to make it work I need to write . More importantly, when I then do I obtain That is: the previous has been overwritten. Is there a way to assign new values without overwriting existing values by nan ? Then, I can do something like Or is there a more efficient way ? For the record, here is my naive code. It works, but looks inefficient and of poor design: "
            },
            "io": [
                "|    |   0 |    x |\n|---:|----:|-----:|\n|  0 |   0 | <NA> |\n|  1 |   0 | <NA> |\n|  2 |   0 |    0 |\n|  3 |   0 |    1 |\n|  4 |   0 |    2 |\n|  5 |   0 |    0 |\n|  6 |   0 |    1 |\n",
                "|    |   0 |   x |\n|---:|----:|----:|\n|  0 |   0 | nan |\n|  1 |   0 | nan |\n|  2 |   0 | nan |\n|  3 |   0 | nan |\n|  4 |   0 | nan |\n|  5 |   0 |   0 |\n|  6 |   0 |   1 |\n"
            ],
            "answer": {
                "ans_desc": "IIUC, you can do double groupby: Output: ",
                "code": [
                    "s = df.index.isin(event).cumsum()\n# or equivalently\n# s = df.loc[event, 0].reindex(df.index).isna().cumsum()\n\ndf['x'] = np.where(s>0,df.groupby(s).cumcount(), np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "cluster-computing"
        ],
        "owner": {
            "reputation": 175,
            "user_id": 8325835,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://i.stack.imgur.com/7dRYm.jpg?s=128&g=1",
            "display_name": "Joseph0210",
            "link": "https://stackoverflow.com/users/8325835/joseph0210"
        },
        "is_answered": true,
        "view_count": 75,
        "accepted_answer_id": 60154510,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1581364049,
        "creation_date": 1581349506,
        "last_edit_date": 1581364049,
        "question_id": 60153802,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60153802/group-together-matched-pairs-across-multiple-columns-python",
        "title": "Group together matched pairs across multiple columns Python",
        "body": "<p>Thank you for reading.</p>\n\n<p>I have a dataframe which looks like this:</p>\n\n<pre><code>Col_A  Col_B   Col_C  Col_D  Col_E  \n 1     2       null   null   null  \n 1     null    3      null   null  \n null  2       3      null   null  \n null  2       null   4      null  \n 1     null    null   null   5 \n</code></pre>\n\n<p>Each row consists of a match between two IDs (e.g. ID1 from Col_A matches to ID2 from Col_B on the first row). </p>\n\n<p>In the example above, all 5 IDs are connected (1 is connected to 2, 2 to 3, 2 to 4, 1 to 5). I therefore want to create a new column which clusters all of these rows together so that I can easily access each group of matched pairs:</p>\n\n<pre><code>Col_A  Col_B   Col_C  Col_D  Col_E  Group ID\n 1     2       null   null   null      1\n 1     null    3      null   null      1\n null  2       3      null   null      1\n null  2       null   4      null      1\n 1     null    null   null   5         1\n</code></pre>\n\n<p>I have not yet been able to find a similar question, but apologies if this is a duplicate. Many thanks in advance for any advice.</p>\n",
        "answer_body": "<p>As @YOBEN_S and @QuangHoang suggests, you can use networkx library and <a href=\"https://en.wikipedia.org/wiki/Component_(graph_theory)\" rel=\"nofollow noreferrer\">Graph Theory connnected components</a> like this.</p>\n\n<p>Given df, </p>\n\n<pre><code>df = pd.DataFrame({'Col_A': {0: 1.0, 1: 1.0, 2: np.nan, 3: np.nan, 4: 1.0, 5: np.nan},\n 'Col_B': {0: 2.0, 1: np.nan, 2: 2.0, 3: 2.0, 4: np.nan, 5: np.nan},\n 'Col_C': {0: np.nan, 1: 3.0, 2: 3.0, 3: np.nan, 4: np.nan, 5: np.nan},\n 'Col_D': {0: np.nan, 1: np.nan, 2: np.nan, 3: 4.0, 4: np.nan, 5: np.nan},\n 'Col_E': {0: np.nan, 1: np.nan, 2: np.nan, 3: np.nan, 4: 5.0, 5: np.nan},\n 'Col_F': {0: np.nan, 1: np.nan, 2: np.nan, 3: np.nan, 4: np.nan, 5: 6.0},\n 'Col_G': {0: np.nan, 1: np.nan, 2: np.nan, 3: np.nan, 4: np.nan, 5: 7.0}})\n\n|    |   Col_A |   Col_B |   Col_C |   Col_D |   Col_E |   Col_F |   Col_G |\n|---:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|\n|  0 |       1 |       2 |     nan |     nan |     nan |     nan |     nan |\n|  1 |       1 |     nan |       3 |     nan |     nan |     nan |     nan |\n|  2 |     nan |       2 |       3 |     nan |     nan |     nan |     nan |\n|  3 |     nan |       2 |     nan |       4 |     nan |     nan |     nan |\n|  4 |       1 |     nan |     nan |     nan |       5 |     nan |     nan |\n|  5 |     nan |     nan |     nan |     nan |     nan |       6 |       7 |\n</code></pre>\n\n<p>Use</p>\n\n<pre><code>import networkx as nx\nd_edge = df.apply(lambda x: x.dropna().to_numpy(), axis=1)\nG = nx.from_edgelist(d_edge.to_numpy().tolist())\ncc_list = list(nx.connected_components(G))\ndf['groupid'] = d_edge.apply(lambda  x: [n for n, i in enumerate(cc_list) if x[0] in i][0] + 1)\ndf\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>|    |   Col_A |   Col_B |   Col_C |   Col_D |   Col_E |   Col_F |   Col_G |   groupid |\n|---:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|----------:|\n|  0 |       1 |       2 |     nan |     nan |     nan |     nan |     nan |         1 |\n|  1 |       1 |     nan |       3 |     nan |     nan |     nan |     nan |         1 |\n|  2 |     nan |       2 |       3 |     nan |     nan |     nan |     nan |         1 |\n|  3 |     nan |       2 |     nan |       4 |     nan |     nan |     nan |         1 |\n|  4 |       1 |     nan |     nan |     nan |       5 |     nan |     nan |         1 |\n|  5 |     nan |     nan |     nan |     nan |     nan |       6 |       7 |         2 |\n</code></pre>\n",
        "question_body": "<p>Thank you for reading.</p>\n\n<p>I have a dataframe which looks like this:</p>\n\n<pre><code>Col_A  Col_B   Col_C  Col_D  Col_E  \n 1     2       null   null   null  \n 1     null    3      null   null  \n null  2       3      null   null  \n null  2       null   4      null  \n 1     null    null   null   5 \n</code></pre>\n\n<p>Each row consists of a match between two IDs (e.g. ID1 from Col_A matches to ID2 from Col_B on the first row). </p>\n\n<p>In the example above, all 5 IDs are connected (1 is connected to 2, 2 to 3, 2 to 4, 1 to 5). I therefore want to create a new column which clusters all of these rows together so that I can easily access each group of matched pairs:</p>\n\n<pre><code>Col_A  Col_B   Col_C  Col_D  Col_E  Group ID\n 1     2       null   null   null      1\n 1     null    3      null   null      1\n null  2       3      null   null      1\n null  2       null   4      null      1\n 1     null    null   null   5         1\n</code></pre>\n\n<p>I have not yet been able to find a similar question, but apologies if this is a duplicate. Many thanks in advance for any advice.</p>\n",
        "formatted_input": {
            "qid": 60153802,
            "link": "https://stackoverflow.com/questions/60153802/group-together-matched-pairs-across-multiple-columns-python",
            "question": {
                "title": "Group together matched pairs across multiple columns Python",
                "ques_desc": "Thank you for reading. I have a dataframe which looks like this: Each row consists of a match between two IDs (e.g. ID1 from Col_A matches to ID2 from Col_B on the first row). In the example above, all 5 IDs are connected (1 is connected to 2, 2 to 3, 2 to 4, 1 to 5). I therefore want to create a new column which clusters all of these rows together so that I can easily access each group of matched pairs: I have not yet been able to find a similar question, but apologies if this is a duplicate. Many thanks in advance for any advice. "
            },
            "io": [
                "Col_A  Col_B   Col_C  Col_D  Col_E  \n 1     2       null   null   null  \n 1     null    3      null   null  \n null  2       3      null   null  \n null  2       null   4      null  \n 1     null    null   null   5 \n",
                "Col_A  Col_B   Col_C  Col_D  Col_E  Group ID\n 1     2       null   null   null      1\n 1     null    3      null   null      1\n null  2       3      null   null      1\n null  2       null   4      null      1\n 1     null    null   null   5         1\n"
            ],
            "answer": {
                "ans_desc": "As @YOBEN_S and @QuangHoang suggests, you can use networkx library and Graph Theory connnected components like this. Given df, Use Output: ",
                "code": [
                    "import networkx as nx\nd_edge = df.apply(lambda x: x.dropna().to_numpy(), axis=1)\nG = nx.from_edgelist(d_edge.to_numpy().tolist())\ncc_list = list(nx.connected_components(G))\ndf['groupid'] = d_edge.apply(lambda  x: [n for n, i in enumerate(cc_list) if x[0] in i][0] + 1)\ndf\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 60095414,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1580994392,
        "creation_date": 1580993256,
        "question_id": 60095370,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60095370/fill-the-values-with-each-column-combination-with-some-default-values-in-pandas",
        "title": "Fill the values with each column combination with some default values in pandas data frame",
        "body": "<p>I have a dataframe like this,</p>\n\n<pre><code>df\ncol1    col2    col3\n1907    CD       49\n1907    FR       33\n1907    SA       34\n1908    PR        1\n1908    SA       37\n1909    PR       16\n1909    SA       38\n</code></pre>\n\n<p>Now CD is not present with col1 1908 and 1909 values, FR not present with 1908 and 1909 values and PR not present wth 1907. </p>\n\n<p>Now I want to create rows with col2 values which are not with all col1 values with col3 values as 0. </p>\n\n<p>So final dataframe will look like, </p>\n\n<pre><code>df\ncol1    col2    col3\n1907    CD       49\n1907    FR       33\n1907    SA       34\n1907    PR        0\n1908    CD        0\n1908    FR        0\n1908    PR        1\n1908    SA       37\n1908    CD        0\n1908    FR        0\n1909    PR       16\n1909    SA       38\n</code></pre>\n\n<p>I could do this using a for loop with every possible col2 values and comparing with every col1 group. But I am looking for shortcuts to do it most efficiently.</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html\" rel=\"nofollow noreferrer\"><code>DataFrame.unstack</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>DataFrame.stack</code></a> for all combinations filled by <code>0</code>:</p>\n\n<pre><code>df = df.set_index(['col1','col2']).unstack(fill_value=0).stack().reset_index()\nprint (df)\n    col1 col2  col3\n0   1907   CD    49\n1   1907   FR    33\n2   1907   PR     0\n3   1907   SA    34\n4   1908   CD     0\n5   1908   FR     0\n6   1908   PR     1\n7   1908   SA    37\n8   1909   CD     0\n9   1909   FR     0\n10  1909   PR    16\n11  1909   SA    38\n</code></pre>\n\n<p>Another idea is use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html\" rel=\"nofollow noreferrer\"><code>DataFrame.reindex</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.from_product.html\" rel=\"nofollow noreferrer\"><code>MultiIndex.from_product</code></a>:</p>\n\n<pre><code>mux = pd.MultiIndex.from_product([df['col1'].unique(), \n                                  df['col2'].unique()], names=['col1','col2'])\ndf = df.set_index(['col1','col2']).reindex(mux, fill_value=0).reset_index()\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this,</p>\n\n<pre><code>df\ncol1    col2    col3\n1907    CD       49\n1907    FR       33\n1907    SA       34\n1908    PR        1\n1908    SA       37\n1909    PR       16\n1909    SA       38\n</code></pre>\n\n<p>Now CD is not present with col1 1908 and 1909 values, FR not present with 1908 and 1909 values and PR not present wth 1907. </p>\n\n<p>Now I want to create rows with col2 values which are not with all col1 values with col3 values as 0. </p>\n\n<p>So final dataframe will look like, </p>\n\n<pre><code>df\ncol1    col2    col3\n1907    CD       49\n1907    FR       33\n1907    SA       34\n1907    PR        0\n1908    CD        0\n1908    FR        0\n1908    PR        1\n1908    SA       37\n1908    CD        0\n1908    FR        0\n1909    PR       16\n1909    SA       38\n</code></pre>\n\n<p>I could do this using a for loop with every possible col2 values and comparing with every col1 group. But I am looking for shortcuts to do it most efficiently.</p>\n",
        "formatted_input": {
            "qid": 60095370,
            "link": "https://stackoverflow.com/questions/60095370/fill-the-values-with-each-column-combination-with-some-default-values-in-pandas",
            "question": {
                "title": "Fill the values with each column combination with some default values in pandas data frame",
                "ques_desc": "I have a dataframe like this, Now CD is not present with col1 1908 and 1909 values, FR not present with 1908 and 1909 values and PR not present wth 1907. Now I want to create rows with col2 values which are not with all col1 values with col3 values as 0. So final dataframe will look like, I could do this using a for loop with every possible col2 values and comparing with every col1 group. But I am looking for shortcuts to do it most efficiently. "
            },
            "io": [
                "df\ncol1    col2    col3\n1907    CD       49\n1907    FR       33\n1907    SA       34\n1908    PR        1\n1908    SA       37\n1909    PR       16\n1909    SA       38\n",
                "df\ncol1    col2    col3\n1907    CD       49\n1907    FR       33\n1907    SA       34\n1907    PR        0\n1908    CD        0\n1908    FR        0\n1908    PR        1\n1908    SA       37\n1908    CD        0\n1908    FR        0\n1909    PR       16\n1909    SA       38\n"
            ],
            "answer": {
                "ans_desc": "Use with for all combinations filled by : Another idea is use with : ",
                "code": [
                    "mux = pd.MultiIndex.from_product([df['col1'].unique(), \n                                  df['col2'].unique()], names=['col1','col2'])\ndf = df.set_index(['col1','col2']).reindex(mux, fill_value=0).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 6802,
            "user_id": 11232091,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vt9vH.png?s=128&g=1",
            "display_name": "moys",
            "link": "https://stackoverflow.com/users/11232091/moys"
        },
        "is_answered": true,
        "view_count": 82,
        "accepted_answer_id": 60088357,
        "answer_count": 3,
        "score": 4,
        "last_activity_date": 1580969946,
        "creation_date": 1580964141,
        "last_edit_date": 1580969946,
        "question_id": 60087855,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60087855/name-of-the-dataframe-from-which-the-minimum-value-is-taken",
        "title": "Name of the dataframe from which the minimum value is taken",
        "body": "<p>I have 3 data-frames as below.</p>\n\n<pre><code>df1 = pd.DataFrame( {\"val\" : [1, 11, 111, 1111, 11111, 11111],\n                    \"val2\" : [2, 22, 23, 24, 25, 26],\"val3\" : [33333, 33333, 3333, 333, 33, 3]} )\nval     val2    val3\n1          2    33333\n11        22    33333\n111       23    3333\n1111      24    333\n11111     25    33\n11111     26    3\n</code></pre>\n\n<pre><code>df2 = pd.DataFrame( {\"val\" : [2, 22, 23, 24, 25, 26],\"val2\" : [1, 11, 111, 1111, 11111, 11111],\"val3\" : [3, 3, 3, 3, 3, 3] } )\nval     val2    val3\n2         1     3\n22       11     3\n23      111     3\n24     1111     3\n25    11111     3\n26    11111     3\n</code></pre>\n\n<p>&amp; </p>\n\n<pre><code>df3 = pd.DataFrame( {\"val\" : [33333, 33333, 3333, 333, 33, 3],\"val2\" : [3, 3, 3, 3333, 3, 3],\"val3\" : [2, 22, 23, 24, 25, 26] } )\nval     val2    val3\n33333      3    2\n33333      3    22\n3333       3    23\n333     3333    24\n33         3    25\n3          3    26\n</code></pre>\n\n<p>With the code <code>pd.concat([df1,df2,df3]).min(level=0)</code> , I get a dateframe which has the min value of each cell of these 3 dataframes</p>\n\n<pre><code>val     val2    val3\n1         1     2\n11        3     3\n23        3     3\n24       24     3\n25        3     3\n3         3     3\n</code></pre>\n\n<p>Now, my question is there a way to get a dataframe which shows from which dataframe these individual values have come from? The expected out put is as below</p>\n\n<pre><code>val     val2    val3\ndf1     df2     df3\ndf1     df3     df2\ndf2     df3     df2\ndf2     df1     df2\ndf2     df3     df2\ndf3     df3     df1,df2\n</code></pre>\n\n<p>Is this even possible in Pandas?</p>\n",
        "answer_body": "<p>Using <code>numpy.ma.apple_along_axis</code>:</p>\n\n<pre><code>def min_finder(arr1d,sep=\",\"):\n    return \",\".join([\"df%s\" % i for i in np.argwhere(arr1d == arr1d.min()).ravel()+1])\n\nnew_df = pd.DataFrame(np.ma.apply_along_axis(min_finder, 2, np.stack(dfs, 0)).T)\nprint(new_df)\n\n     0    1        2\n0  df1  df2      df3\n1  df1  df3      df2\n2  df2  df3      df2\n3  df2  df3      df3\n4  df2  df3      df2\n5  df3  df3  df1,df2\n</code></pre>\n\n<p>If you don't want tie, use <code>numpy.argmin</code>:</p>\n\n<pre><code>dfs = [df1,df2,df3]\nnp.stack(dfs, 0).argmin(0)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>array([[0, 1, 2],\n       [0, 2, 1],\n       [1, 2, 1],\n       [1, 0, 1],\n       [1, 2, 1],\n       [2, 2, 0]])\n</code></pre>\n\n<p>Or make it bit prettier:</p>\n\n<pre><code>np.char.add(\"df\", (np.stack(dfs, 0).argmin(0)+1).astype(str))\n\narray([['df1', 'df2', 'df3'],\n       ['df1', 'df3', 'df2'],\n       ['df2', 'df3', 'df2'],\n       ['df2', 'df1', 'df2'],\n       ['df2', 'df3', 'df2'],\n       ['df3', 'df3', 'df1']], dtype='&lt;U23')\n</code></pre>\n",
        "question_body": "<p>I have 3 data-frames as below.</p>\n\n<pre><code>df1 = pd.DataFrame( {\"val\" : [1, 11, 111, 1111, 11111, 11111],\n                    \"val2\" : [2, 22, 23, 24, 25, 26],\"val3\" : [33333, 33333, 3333, 333, 33, 3]} )\nval     val2    val3\n1          2    33333\n11        22    33333\n111       23    3333\n1111      24    333\n11111     25    33\n11111     26    3\n</code></pre>\n\n<pre><code>df2 = pd.DataFrame( {\"val\" : [2, 22, 23, 24, 25, 26],\"val2\" : [1, 11, 111, 1111, 11111, 11111],\"val3\" : [3, 3, 3, 3, 3, 3] } )\nval     val2    val3\n2         1     3\n22       11     3\n23      111     3\n24     1111     3\n25    11111     3\n26    11111     3\n</code></pre>\n\n<p>&amp; </p>\n\n<pre><code>df3 = pd.DataFrame( {\"val\" : [33333, 33333, 3333, 333, 33, 3],\"val2\" : [3, 3, 3, 3333, 3, 3],\"val3\" : [2, 22, 23, 24, 25, 26] } )\nval     val2    val3\n33333      3    2\n33333      3    22\n3333       3    23\n333     3333    24\n33         3    25\n3          3    26\n</code></pre>\n\n<p>With the code <code>pd.concat([df1,df2,df3]).min(level=0)</code> , I get a dateframe which has the min value of each cell of these 3 dataframes</p>\n\n<pre><code>val     val2    val3\n1         1     2\n11        3     3\n23        3     3\n24       24     3\n25        3     3\n3         3     3\n</code></pre>\n\n<p>Now, my question is there a way to get a dataframe which shows from which dataframe these individual values have come from? The expected out put is as below</p>\n\n<pre><code>val     val2    val3\ndf1     df2     df3\ndf1     df3     df2\ndf2     df3     df2\ndf2     df1     df2\ndf2     df3     df2\ndf3     df3     df1,df2\n</code></pre>\n\n<p>Is this even possible in Pandas?</p>\n",
        "formatted_input": {
            "qid": 60087855,
            "link": "https://stackoverflow.com/questions/60087855/name-of-the-dataframe-from-which-the-minimum-value-is-taken",
            "question": {
                "title": "Name of the dataframe from which the minimum value is taken",
                "ques_desc": "I have 3 data-frames as below. & With the code , I get a dateframe which has the min value of each cell of these 3 dataframes Now, my question is there a way to get a dataframe which shows from which dataframe these individual values have come from? The expected out put is as below Is this even possible in Pandas? "
            },
            "io": [
                "val     val2    val3\n1         1     2\n11        3     3\n23        3     3\n24       24     3\n25        3     3\n3         3     3\n",
                "val     val2    val3\ndf1     df2     df3\ndf1     df3     df2\ndf2     df3     df2\ndf2     df1     df2\ndf2     df3     df2\ndf3     df3     df1,df2\n"
            ],
            "answer": {
                "ans_desc": "Using : If you don't want tie, use : Output: Or make it bit prettier: ",
                "code": [
                    "np.char.add(\"df\", (np.stack(dfs, 0).argmin(0)+1).astype(str))\n\narray([['df1', 'df2', 'df3'],\n       ['df1', 'df3', 'df2'],\n       ['df2', 'df3', 'df2'],\n       ['df2', 'df1', 'df2'],\n       ['df2', 'df3', 'df2'],\n       ['df3', 'df3', 'df1']], dtype='<U23')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 173,
            "user_id": 10410251,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-nDsyd2H8Q6Q/AAAAAAAAAAI/AAAAAAAAAAA/AAN31DUbRL_lfKXp1Be09JxrxqpJ2rlOMg/mo/photo.jpg?sz=128",
            "display_name": "Spring",
            "link": "https://stackoverflow.com/users/10410251/spring"
        },
        "is_answered": true,
        "view_count": 137,
        "accepted_answer_id": 60065651,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1580853492,
        "creation_date": 1580850145,
        "last_edit_date": 1580853492,
        "question_id": 60065499,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60065499/multiply-two-columns-from-two-different-pandas-dataframes",
        "title": "multiply two columns from two different pandas dataframes",
        "body": "<p>I have a pandas.DataFrame.</p>\n\n<pre><code>      df1\n           Year    Class     Price    EL\n           2024     PC1       $243    Base\n           2025     PC1       $215    Base\n           2024     PC1       $217    EL_1\n           2025     PC1       $255    EL_1\n           2024     PC2       $217    Base\n           2025     PC2       $232    Base\n           2024     PC2       $265    EL_1\n           2025     PC2       $215    EL_1\n</code></pre>\n\n<p>I have another pandas.DataFrame</p>\n\n<pre><code>           df2  \n              Year    Price_factor\n              2024      1\n              2025      0.98\n</code></pre>\n\n<p>I want to apply df2['Price_factor'] to df1['Price'] column. I tried my code but it didn't work.</p>\n\n<pre><code>          df3=df1.groupby(['Class','EL'])['Price']*df2['Price_factor]\n</code></pre>\n\n<p>Thank  you for your help in advance.</p>\n",
        "answer_body": "<p>Use map,</p>\n\n<pre><code>df1['Price_factor'] = df1['Year'].map(df2.set_index('Year')['Price_factor'])\ndf1['Price_adjusted']= df1['Price'].str.strip('$').astype(int) * df1['Price_factor']\ndf1\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>|    |   Year | Class   | Price   | EL   |   Price_factor |   Price_adjusted |\n|----|--------|---------|---------|------|----------------|------------------|\n|  0 |   2024 | PC1     | $243    | Base |           1    |           243    |\n|  1 |   2025 | PC1     | $215    | Base |           0.98 |           210.7  |\n|  2 |   2024 | PC1     | $217    | EL_1 |           1    |           217    |\n|  3 |   2025 | PC1     | $255    | EL_1 |           0.98 |           249.9  |\n|  4 |   2024 | PC2     | $217    | Base |           1    |           217    |\n|  5 |   2025 | PC2     | $232    | Base |           0.98 |           227.36 |\n|  6 |   2024 | PC2     | $265    | EL_1 |           1    |           265    |\n|  7 |   2025 | PC2     | $215    | EL_1 |           0.98 |           210.7  |\n</code></pre>\n",
        "question_body": "<p>I have a pandas.DataFrame.</p>\n\n<pre><code>      df1\n           Year    Class     Price    EL\n           2024     PC1       $243    Base\n           2025     PC1       $215    Base\n           2024     PC1       $217    EL_1\n           2025     PC1       $255    EL_1\n           2024     PC2       $217    Base\n           2025     PC2       $232    Base\n           2024     PC2       $265    EL_1\n           2025     PC2       $215    EL_1\n</code></pre>\n\n<p>I have another pandas.DataFrame</p>\n\n<pre><code>           df2  \n              Year    Price_factor\n              2024      1\n              2025      0.98\n</code></pre>\n\n<p>I want to apply df2['Price_factor'] to df1['Price'] column. I tried my code but it didn't work.</p>\n\n<pre><code>          df3=df1.groupby(['Class','EL'])['Price']*df2['Price_factor]\n</code></pre>\n\n<p>Thank  you for your help in advance.</p>\n",
        "formatted_input": {
            "qid": 60065499,
            "link": "https://stackoverflow.com/questions/60065499/multiply-two-columns-from-two-different-pandas-dataframes",
            "question": {
                "title": "multiply two columns from two different pandas dataframes",
                "ques_desc": "I have a pandas.DataFrame. I have another pandas.DataFrame I want to apply df2['Price_factor'] to df1['Price'] column. I tried my code but it didn't work. Thank you for your help in advance. "
            },
            "io": [
                "      df1\n           Year    Class     Price    EL\n           2024     PC1       $243    Base\n           2025     PC1       $215    Base\n           2024     PC1       $217    EL_1\n           2025     PC1       $255    EL_1\n           2024     PC2       $217    Base\n           2025     PC2       $232    Base\n           2024     PC2       $265    EL_1\n           2025     PC2       $215    EL_1\n",
                "           df2  \n              Year    Price_factor\n              2024      1\n              2025      0.98\n"
            ],
            "answer": {
                "ans_desc": "Use map, Output: ",
                "code": [
                    "df1['Price_factor'] = df1['Year'].map(df2.set_index('Year')['Price_factor'])\ndf1['Price_adjusted']= df1['Price'].str.strip('$').astype(int) * df1['Price_factor']\ndf1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 12833785,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e8705a86dd7e4e50e37b9dbfb6c5ec17?s=128&d=identicon&r=PG&f=1",
            "display_name": "lost_in_pandas",
            "link": "https://stackoverflow.com/users/12833785/lost-in-pandas"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 60042517,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1580744771,
        "creation_date": 1580743733,
        "last_edit_date": 1580744293,
        "question_id": 60042289,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60042289/multi-column-to-single-column-in-pandas",
        "title": "Multi-column to single column in Pandas",
        "body": "<p>I have the following data frame :</p>\n\n<pre><code>    parent          0        1      2   3\n0   14026529    14062504     0      0   0\n1   14103793    14036094     0      0   0\n2   14025454    14036094     0      0   0\n3   14030252    14030253  14062647  0   0\n4   14034704    14086964     0      0   0\n</code></pre>\n\n<p>And I need this :</p>\n\n<pre><code>    parent_id   child_id\n 0   14026529   14062504\n 1   14025454   14036094\n 2   14030252   14030253  \n 3   14030252   14062647\n 4   14103793   14036094\n 5   14034704   14086964\n</code></pre>\n\n<p>This is just a basic example, the real deal can have over 60 children.</p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.where.html\" rel=\"nofollow noreferrer\"><code>DataFrame.where</code></a>, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>stack</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>reset_index</code></a>.<br>\nCasting as <code>Int64</code> first will prevent child_Id's being cast to floats during the stacking process.</p>\n\n<pre><code>(df.astype('Int64').where(df.ne(0))\n .set_index('parent')\n .stack()\n .reset_index(level=0, name='child'))\n</code></pre>\n\n<p>[out]</p>\n\n<pre><code>     parent     child\n0  14026529  14062504\n0  14103793  14036094\n0  14025454  14036094\n0  14030252  14030253\n1  14030252  14062647\n0  14034704  14086964\n</code></pre>\n",
        "question_body": "<p>I have the following data frame :</p>\n\n<pre><code>    parent          0        1      2   3\n0   14026529    14062504     0      0   0\n1   14103793    14036094     0      0   0\n2   14025454    14036094     0      0   0\n3   14030252    14030253  14062647  0   0\n4   14034704    14086964     0      0   0\n</code></pre>\n\n<p>And I need this :</p>\n\n<pre><code>    parent_id   child_id\n 0   14026529   14062504\n 1   14025454   14036094\n 2   14030252   14030253  \n 3   14030252   14062647\n 4   14103793   14036094\n 5   14034704   14086964\n</code></pre>\n\n<p>This is just a basic example, the real deal can have over 60 children.</p>\n",
        "formatted_input": {
            "qid": 60042289,
            "link": "https://stackoverflow.com/questions/60042289/multi-column-to-single-column-in-pandas",
            "question": {
                "title": "Multi-column to single column in Pandas",
                "ques_desc": "I have the following data frame : And I need this : This is just a basic example, the real deal can have over 60 children. "
            },
            "io": [
                "    parent          0        1      2   3\n0   14026529    14062504     0      0   0\n1   14103793    14036094     0      0   0\n2   14025454    14036094     0      0   0\n3   14030252    14030253  14062647  0   0\n4   14034704    14086964     0      0   0\n",
                "    parent_id   child_id\n 0   14026529   14062504\n 1   14025454   14036094\n 2   14030252   14030253  \n 3   14030252   14062647\n 4   14103793   14036094\n 5   14034704   14086964\n"
            ],
            "answer": {
                "ans_desc": "Use , and . Casting as first will prevent child_Id's being cast to floats during the stacking process. [out] ",
                "code": [
                    "(df.astype('Int64').where(df.ne(0))\n .set_index('parent')\n .stack()\n .reset_index(level=0, name='child'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 265,
            "user_id": 12364600,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/BDPUy.jpg?s=128&g=1",
            "display_name": "Temp_coder",
            "link": "https://stackoverflow.com/users/12364600/temp-coder"
        },
        "is_answered": true,
        "view_count": 60,
        "accepted_answer_id": 59917039,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1580576096,
        "creation_date": 1580018298,
        "last_edit_date": 1580034861,
        "question_id": 59915944,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59915944/apply-qcut-for-complete-dataframe",
        "title": "Apply qcut for complete dataframe",
        "body": "<p>I want to replace the column values with bin numbers based on quantiles but for each and every column present in the dataframe.</p>\n\n<p>I know how to do this with qcut and labels as its parameter for a single column, but do not know whether it can be applied for complete dataframe or not.\nsay the dataframe looks like below..</p>\n\n<pre><code>    ID  CC  DD  EE\n0   Q1  0   23  18\n1   Q2  2   32  19\n2   Q3  3   45  20\n3   Q4  4   54  21\n4   Q5  5   67  22\n5   Q6  6   76  23\n</code></pre>\n\n<p>The ID column should remain unchanged but the other columns should be replaced by bin numbers, like below..</p>\n\n<pre><code>    ID  CC  DD  EE\n0   Q1  1   1   1\n1   Q2  2   2   1\n2   Q3  3   2   2\n3   Q4  4   3   3\n4   Q5  5   4   4\n5   Q6  5   5   5\n</code></pre>\n\n<p>the bin numbers I have provided here for CC, DD, EE are not exact and for understanding purpose only.</p>\n\n<p>And in the real dataset, there are more than 100 columns and 1000 rows, and I do not want to replace the 'ID' column, but all the other columns.</p>\n\n<p>How to do this?</p>\n",
        "answer_body": "<p>you have to use pandas.cut() </p>\n\n<pre><code>import pandas as pd\n\ndf['CC'] = pd.cut(df['CC'], [0, 5, 10,20])\n</code></pre>\n\n<p>Similarly you can do for other columns as well.</p>\n",
        "question_body": "<p>I want to replace the column values with bin numbers based on quantiles but for each and every column present in the dataframe.</p>\n\n<p>I know how to do this with qcut and labels as its parameter for a single column, but do not know whether it can be applied for complete dataframe or not.\nsay the dataframe looks like below..</p>\n\n<pre><code>    ID  CC  DD  EE\n0   Q1  0   23  18\n1   Q2  2   32  19\n2   Q3  3   45  20\n3   Q4  4   54  21\n4   Q5  5   67  22\n5   Q6  6   76  23\n</code></pre>\n\n<p>The ID column should remain unchanged but the other columns should be replaced by bin numbers, like below..</p>\n\n<pre><code>    ID  CC  DD  EE\n0   Q1  1   1   1\n1   Q2  2   2   1\n2   Q3  3   2   2\n3   Q4  4   3   3\n4   Q5  5   4   4\n5   Q6  5   5   5\n</code></pre>\n\n<p>the bin numbers I have provided here for CC, DD, EE are not exact and for understanding purpose only.</p>\n\n<p>And in the real dataset, there are more than 100 columns and 1000 rows, and I do not want to replace the 'ID' column, but all the other columns.</p>\n\n<p>How to do this?</p>\n",
        "formatted_input": {
            "qid": 59915944,
            "link": "https://stackoverflow.com/questions/59915944/apply-qcut-for-complete-dataframe",
            "question": {
                "title": "Apply qcut for complete dataframe",
                "ques_desc": "I want to replace the column values with bin numbers based on quantiles but for each and every column present in the dataframe. I know how to do this with qcut and labels as its parameter for a single column, but do not know whether it can be applied for complete dataframe or not. say the dataframe looks like below.. The ID column should remain unchanged but the other columns should be replaced by bin numbers, like below.. the bin numbers I have provided here for CC, DD, EE are not exact and for understanding purpose only. And in the real dataset, there are more than 100 columns and 1000 rows, and I do not want to replace the 'ID' column, but all the other columns. How to do this? "
            },
            "io": [
                "    ID  CC  DD  EE\n0   Q1  0   23  18\n1   Q2  2   32  19\n2   Q3  3   45  20\n3   Q4  4   54  21\n4   Q5  5   67  22\n5   Q6  6   76  23\n",
                "    ID  CC  DD  EE\n0   Q1  1   1   1\n1   Q2  2   2   1\n2   Q3  3   2   2\n3   Q4  4   3   3\n4   Q5  5   4   4\n5   Q6  5   5   5\n"
            ],
            "answer": {
                "ans_desc": "you have to use pandas.cut() Similarly you can do for other columns as well. ",
                "code": [
                    "import pandas as pd\n\ndf['CC'] = pd.cut(df['CC'], [0, 5, 10,20])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 4935,
            "user_id": 3590067,
            "user_type": "registered",
            "accept_rate": 68,
            "profile_image": "https://graph.facebook.com/100003877435114/picture?type=large",
            "display_name": "emax",
            "link": "https://stackoverflow.com/users/3590067/emax"
        },
        "is_answered": true,
        "view_count": 20231,
        "accepted_answer_id": 36438987,
        "answer_count": 2,
        "score": 11,
        "last_activity_date": 1580160299,
        "creation_date": 1459893685,
        "question_id": 36437849,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/36437849/python-how-to-add-a-column-to-a-pandas-dataframe-between-two-columns",
        "title": "Python: how to add a column to a pandas dataframe between two columns?",
        "body": "<p>I would like to add a column to a dataframe between two columns in number labeled columns dataframe. In the following dataframe the first column corresponds to the index while the first row to the name of the columns.</p>\n\n<pre><code>df\n   0 0 1 2 3 4 5\n   1 6 7 4 5 2 1\n   2 0 3 1 3 3 4\n   3 9 8 4 3 6 2 \n</code></pre>\n\n<p>I have <code>tmp=[2,3,5]</code> that I want to put between the columns <code>4</code> and <code>5</code>, so</p>\n\n<pre><code>df\n   0 0 1 2 3 4 5 6 \n   1 6 7 4 5 2 2 1\n   2 0 3 1 3 3 3 4\n   3 9 8 4 3 6 5 2 \n</code></pre>\n",
        "answer_body": "<p>You can use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.insert.html\" rel=\"noreferrer\"><code>insert</code></a>:</p>\n\n<pre><code>df.insert(4, 'new_col_name', tmp)\n</code></pre>\n\n<p>Note: The <code>insert</code> method mutates the original DataFrame and does not return a copy. </p>\n\n<p>If you use <code>df = df.insert(4, 'new_col_name', tmp)</code>, <code>df</code> will be <code>None</code>.</p>\n",
        "question_body": "<p>I would like to add a column to a dataframe between two columns in number labeled columns dataframe. In the following dataframe the first column corresponds to the index while the first row to the name of the columns.</p>\n\n<pre><code>df\n   0 0 1 2 3 4 5\n   1 6 7 4 5 2 1\n   2 0 3 1 3 3 4\n   3 9 8 4 3 6 2 \n</code></pre>\n\n<p>I have <code>tmp=[2,3,5]</code> that I want to put between the columns <code>4</code> and <code>5</code>, so</p>\n\n<pre><code>df\n   0 0 1 2 3 4 5 6 \n   1 6 7 4 5 2 2 1\n   2 0 3 1 3 3 3 4\n   3 9 8 4 3 6 5 2 \n</code></pre>\n",
        "formatted_input": {
            "qid": 36437849,
            "link": "https://stackoverflow.com/questions/36437849/python-how-to-add-a-column-to-a-pandas-dataframe-between-two-columns",
            "question": {
                "title": "Python: how to add a column to a pandas dataframe between two columns?",
                "ques_desc": "I would like to add a column to a dataframe between two columns in number labeled columns dataframe. In the following dataframe the first column corresponds to the index while the first row to the name of the columns. I have that I want to put between the columns and , so "
            },
            "io": [
                "df\n   0 0 1 2 3 4 5\n   1 6 7 4 5 2 1\n   2 0 3 1 3 3 4\n   3 9 8 4 3 6 2 \n",
                "df\n   0 0 1 2 3 4 5 6 \n   1 6 7 4 5 2 2 1\n   2 0 3 1 3 3 3 4\n   3 9 8 4 3 6 5 2 \n"
            ],
            "answer": {
                "ans_desc": "You can use : Note: The method mutates the original DataFrame and does not return a copy. If you use , will be . ",
                "code": [
                    "df = df.insert(4, 'new_col_name', tmp)"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "multidimensional-array"
        ],
        "owner": {
            "reputation": 73,
            "user_id": 12575476,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9a81206f06d36a1bc76b2ff7ff7f3d18?s=128&d=identicon&r=PG&f=1",
            "display_name": "user0",
            "link": "https://stackoverflow.com/users/12575476/user0"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 59937578,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1580152936,
        "creation_date": 1580145580,
        "question_id": 59935954,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59935954/multidimensional-numpy-ndarray-from-multi-indexed-pandas-dataframe",
        "title": "Multidimensional numpy.ndarray from multi-indexed pandas.DataFrame",
        "body": "<p>I want to produce a 3-dimensional numpy.ndarray from a multi-indexed pandas.DataFrame. More precisely, say I have:</p>\n\n<pre><code>df = pd.DataFrame([[1, '1' , 1, 10], [1, '2', 2, 20], [2, '1', 5, 30]], columns=['x', 'y', 'z', 't'])\ndf = df.set_index(['x','y'])\ndf\n</code></pre>\n\n<p>which gives me</p>\n\n<pre><code>        z  t\n  x  y\n  1  1  1 10\n     2  2 20\n  2  1  5 30\n</code></pre>\n\n<p>and I want to write a function which returns, with the above argument, the numpy.ndarray</p>\n\n<pre><code>  [[[1, 10],\n    [2, 20]],\n   [[5, 30],\n    [NaN, NaN]]]\n</code></pre>\n\n<p>Pandas multi-index looks like a substitute for multidimensional arrays, but it does not provide (or at least does not document) ways to go back and forth...</p>\n\n<p>Thanks.</p>\n",
        "answer_body": "<p>Use: </p>\n\n<pre><code>df.to_xarray().to_array().values.transpose(1,2,0)\n\n&gt;&gt;[[[ 1. 10.]\n  [ 2. 20.]]\n\n [[ 5. 30.]\n  [nan nan]]]\n</code></pre>\n",
        "question_body": "<p>I want to produce a 3-dimensional numpy.ndarray from a multi-indexed pandas.DataFrame. More precisely, say I have:</p>\n\n<pre><code>df = pd.DataFrame([[1, '1' , 1, 10], [1, '2', 2, 20], [2, '1', 5, 30]], columns=['x', 'y', 'z', 't'])\ndf = df.set_index(['x','y'])\ndf\n</code></pre>\n\n<p>which gives me</p>\n\n<pre><code>        z  t\n  x  y\n  1  1  1 10\n     2  2 20\n  2  1  5 30\n</code></pre>\n\n<p>and I want to write a function which returns, with the above argument, the numpy.ndarray</p>\n\n<pre><code>  [[[1, 10],\n    [2, 20]],\n   [[5, 30],\n    [NaN, NaN]]]\n</code></pre>\n\n<p>Pandas multi-index looks like a substitute for multidimensional arrays, but it does not provide (or at least does not document) ways to go back and forth...</p>\n\n<p>Thanks.</p>\n",
        "formatted_input": {
            "qid": 59935954,
            "link": "https://stackoverflow.com/questions/59935954/multidimensional-numpy-ndarray-from-multi-indexed-pandas-dataframe",
            "question": {
                "title": "Multidimensional numpy.ndarray from multi-indexed pandas.DataFrame",
                "ques_desc": "I want to produce a 3-dimensional numpy.ndarray from a multi-indexed pandas.DataFrame. More precisely, say I have: which gives me and I want to write a function which returns, with the above argument, the numpy.ndarray Pandas multi-index looks like a substitute for multidimensional arrays, but it does not provide (or at least does not document) ways to go back and forth... Thanks. "
            },
            "io": [
                "        z  t\n  x  y\n  1  1  1 10\n     2  2 20\n  2  1  5 30\n",
                "  [[[1, 10],\n    [2, 20]],\n   [[5, 30],\n    [NaN, NaN]]]\n"
            ],
            "answer": {
                "ans_desc": "Use: ",
                "code": [
                    "df.to_xarray().to_array().values.transpose(1,2,0)\n\n>>[[[ 1. 10.]\n  [ 2. 20.]]\n\n [[ 5. 30.]\n  [nan nan]]]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 308,
            "user_id": 10171862,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/DUzws.jpg?s=128&g=1",
            "display_name": "Jose Macedo",
            "link": "https://stackoverflow.com/users/10171862/jose-macedo"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 59931186,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1580128067,
        "creation_date": 1579885449,
        "last_edit_date": 1579889930,
        "question_id": 59900864,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59900864/change-value-of-only-1-cell-based-on-criteria-dataframe",
        "title": "Change value of only 1 cell based on criteria DataFrame",
        "body": "<p>Based on a condition, I want to change the value of the first row on a certain column, so far this is what I have</p>\n\n<pre><code>despesas['recibos'] =''\nfor a in recibos['recibos']:\n    if len(despesas.loc[(despesas['despesas']==a) &amp; (despesas['recibos']==''), 'recibos'])&gt;0:\n        despesas.loc[(despesas['despesas']==a) &amp; (despesas['recibos']==''), \n'recibos'].iloc[0] =a\n</code></pre>\n\n<p>So I want to change only the first value of the column <em>recibos</em> by the value on <em>a</em> where <em>(despesas['despesas']==a) &amp; (despesas['recibos']=='')</em></p>\n\n<p><strong>Edit 1</strong></p>\n\n<p>Example:</p>\n\n<pre><code>despesas['despesas'] = [11.95,  2.5,  1.2 ,  0.6 ,  2.66,  2.66,  3.  , 47.5 , 16.95,17.56]\nrecibos['recibos'] = [11.95,  1.2 ,  1.2 ,  0.2 ,  2.66,  2.66,  3.  , 47.5 , 16.95, 17.56]\n</code></pre>\n\n<p>And the result should be:</p>\n\n<pre><code>[[11.95, 11.95],  [2.5, null] ,  [1.2, 1.2] ,  [0.6, null] ,  [2.66, 2.66],  [2.66, 2.66],  [3., 3] , [47.5, 45.5 ], [16.95, 16.95], [17.56, 17.56]]\n</code></pre>\n",
        "answer_body": "<p>I found the solution that I was looking for</p>\n\n<pre><code>from itertools import count, filterfalse\n\ndespesas['recibos'] =''\nfor index, a in despesas.iterrows():\n    if len(recibos.loc[recibos['recibos']==a['despesas']])&gt;0:\n        despesas.iloc[index,1]=True\n        recibos.drop(recibos.loc[recibos['recibos']==a['despesas']][:1].index, inplace=True)\n</code></pre>\n",
        "question_body": "<p>Based on a condition, I want to change the value of the first row on a certain column, so far this is what I have</p>\n\n<pre><code>despesas['recibos'] =''\nfor a in recibos['recibos']:\n    if len(despesas.loc[(despesas['despesas']==a) &amp; (despesas['recibos']==''), 'recibos'])&gt;0:\n        despesas.loc[(despesas['despesas']==a) &amp; (despesas['recibos']==''), \n'recibos'].iloc[0] =a\n</code></pre>\n\n<p>So I want to change only the first value of the column <em>recibos</em> by the value on <em>a</em> where <em>(despesas['despesas']==a) &amp; (despesas['recibos']=='')</em></p>\n\n<p><strong>Edit 1</strong></p>\n\n<p>Example:</p>\n\n<pre><code>despesas['despesas'] = [11.95,  2.5,  1.2 ,  0.6 ,  2.66,  2.66,  3.  , 47.5 , 16.95,17.56]\nrecibos['recibos'] = [11.95,  1.2 ,  1.2 ,  0.2 ,  2.66,  2.66,  3.  , 47.5 , 16.95, 17.56]\n</code></pre>\n\n<p>And the result should be:</p>\n\n<pre><code>[[11.95, 11.95],  [2.5, null] ,  [1.2, 1.2] ,  [0.6, null] ,  [2.66, 2.66],  [2.66, 2.66],  [3., 3] , [47.5, 45.5 ], [16.95, 16.95], [17.56, 17.56]]\n</code></pre>\n",
        "formatted_input": {
            "qid": 59900864,
            "link": "https://stackoverflow.com/questions/59900864/change-value-of-only-1-cell-based-on-criteria-dataframe",
            "question": {
                "title": "Change value of only 1 cell based on criteria DataFrame",
                "ques_desc": "Based on a condition, I want to change the value of the first row on a certain column, so far this is what I have So I want to change only the first value of the column recibos by the value on a where (despesas['despesas']==a) & (despesas['recibos']=='') Edit 1 Example: And the result should be: "
            },
            "io": [
                "despesas['despesas'] = [11.95,  2.5,  1.2 ,  0.6 ,  2.66,  2.66,  3.  , 47.5 , 16.95,17.56]\nrecibos['recibos'] = [11.95,  1.2 ,  1.2 ,  0.2 ,  2.66,  2.66,  3.  , 47.5 , 16.95, 17.56]\n",
                "[[11.95, 11.95],  [2.5, null] ,  [1.2, 1.2] ,  [0.6, null] ,  [2.66, 2.66],  [2.66, 2.66],  [3., 3] , [47.5, 45.5 ], [16.95, 16.95], [17.56, 17.56]]\n"
            ],
            "answer": {
                "ans_desc": "I found the solution that I was looking for ",
                "code": [
                    "from itertools import count, filterfalse\n\ndespesas['recibos'] =''\nfor index, a in despesas.iterrows():\n    if len(recibos.loc[recibos['recibos']==a['despesas']])>0:\n        despesas.iloc[index,1]=True\n        recibos.drop(recibos.loc[recibos['recibos']==a['despesas']][:1].index, inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "sorting",
            "series"
        ],
        "owner": {
            "reputation": 43544,
            "user_id": 8708364,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/nMdFT.jpg?s=128&g=1",
            "display_name": "U11-Forward",
            "link": "https://stackoverflow.com/users/8708364/u11-forward"
        },
        "is_answered": true,
        "view_count": 150,
        "accepted_answer_id": 59925198,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1580099935,
        "creation_date": 1580099785,
        "question_id": 59925197,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59925197/sort-a-pandas-dataframe-by-a-column-in-another-dataframe-pandas",
        "title": "Sort a pandas DataFrame by a column in another dataframe - pandas",
        "body": "<p>Let's say I have a Pandas DataFrame with two columns, like:</p>\n\n<pre><code>df = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [100, 200, 300, 400]})\nprint(df)\n</code></pre>\n\n<hr>\n\n<pre><code>   a    b\n0  1  100\n1  2  200\n2  3  300\n3  4  400\n</code></pre>\n\n<p>And let's say I also have a Pandas Series, like:</p>\n\n<pre><code>s = pd.Series([1, 3, 2, 4])\nprint(s)\n</code></pre>\n\n<hr>\n\n<pre><code>0    1\n1    3\n2    2\n3    4\ndtype: int64\n</code></pre>\n\n<p>How can I sort the <code>a</code> column to become the same order as the <code>s</code> series, with the corresponding row values sorted together?</p>\n\n<p>My desired output would be:</p>\n\n<pre><code>   a    b\n0  1  100\n1  3  300\n2  2  200\n3  4  400\n</code></pre>\n\n<p>Is there any way to achieve this?</p>\n\n<p><strong>Please check self-answer below.</strong></p>\n",
        "answer_body": "<p>I have ran into these issues quite often, so I just thought to share my solutions in Pandas.</p>\n\n<h1>Solutions:</h1>\n\n<p><strong>Solution 1:</strong></p>\n\n<p>Using <code>set_index</code> to convert the <code>a</code> column to the index, then use <code>reindex</code> to change the order, then use <code>rename_axis</code> to change the index name back to <code>a</code>, then use <code>reset_index</code> to convert the <code>a</code> column from an index back to a column:</p>\n\n<pre><code>print(df.set_index('a').reindex(s).rename_axis('a').reset_index('a'))\n</code></pre>\n\n<p><strong>Solution 2:</strong></p>\n\n<p>Using <code>set_index</code> to convert the <code>a</code> column to the index, then use <code>loc</code> to change the order, then use <code>reset_index</code> to convert the <code>a</code> column from an index back to a column:</p>\n\n<pre><code>print(df.set_index('a').loc[s].reset_index())\n</code></pre>\n\n<p><strong>Solution 3:</strong></p>\n\n<p>Using <code>iloc</code> to index the rows in a different order, then use <code>map</code> to get that order that would fit the <code>df</code> to make it get sorted with the <code>s</code> series:</p>\n\n<pre><code>print(df.iloc[list(map(df['a'].tolist().index, s))])\n</code></pre>\n\n<p><strong>Solution 4:</strong></p>\n\n<p>Using <code>pd.DataFrame</code> to create a new DataFrame object, then use <code>sorted</code> with a <code>key</code> argument to sort the DataFrame by the <code>s</code> series:</p>\n\n<pre><code>print(pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns))\n</code></pre>\n\n<h1>Timings:</h1>\n\n<p>Timing with the below code:</p>\n\n<pre><code>import pandas as pd\nfrom timeit import timeit\ndf = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [100, 200, 300, 400]})\ns = pd.Series([1, 3, 2, 4])\ndef u10_1():\n    return df.set_index('a').reindex(s).rename_axis('a').reset_index('a')\ndef u10_2():\n    return df.set_index('a').loc[s].reset_index()\ndef u10_3():\n    return df.iloc[list(map(df['a'].tolist().index, s))]\ndef u10_4():\n    return pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns)\nprint('u10_1:', timeit(u10_1, number=1000))\nprint('u10_2:', timeit(u10_2, number=1000))\nprint('u10_3:', timeit(u10_3, number=1000))\nprint('u10_4:', timeit(u10_4, number=1000))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>u10_1: 3.012849470495621\nu10_2: 3.072132612502147\nu10_3: 0.7498072134665241\nu10_4: 0.8109911930595484\n</code></pre>\n\n<p>@Allen has a pretty good answer too.</p>\n",
        "question_body": "<p>Let's say I have a Pandas DataFrame with two columns, like:</p>\n\n<pre><code>df = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [100, 200, 300, 400]})\nprint(df)\n</code></pre>\n\n<hr>\n\n<pre><code>   a    b\n0  1  100\n1  2  200\n2  3  300\n3  4  400\n</code></pre>\n\n<p>And let's say I also have a Pandas Series, like:</p>\n\n<pre><code>s = pd.Series([1, 3, 2, 4])\nprint(s)\n</code></pre>\n\n<hr>\n\n<pre><code>0    1\n1    3\n2    2\n3    4\ndtype: int64\n</code></pre>\n\n<p>How can I sort the <code>a</code> column to become the same order as the <code>s</code> series, with the corresponding row values sorted together?</p>\n\n<p>My desired output would be:</p>\n\n<pre><code>   a    b\n0  1  100\n1  3  300\n2  2  200\n3  4  400\n</code></pre>\n\n<p>Is there any way to achieve this?</p>\n\n<p><strong>Please check self-answer below.</strong></p>\n",
        "formatted_input": {
            "qid": 59925197,
            "link": "https://stackoverflow.com/questions/59925197/sort-a-pandas-dataframe-by-a-column-in-another-dataframe-pandas",
            "question": {
                "title": "Sort a pandas DataFrame by a column in another dataframe - pandas",
                "ques_desc": "Let's say I have a Pandas DataFrame with two columns, like: And let's say I also have a Pandas Series, like: How can I sort the column to become the same order as the series, with the corresponding row values sorted together? My desired output would be: Is there any way to achieve this? Please check self-answer below. "
            },
            "io": [
                "   a    b\n0  1  100\n1  2  200\n2  3  300\n3  4  400\n",
                "   a    b\n0  1  100\n1  3  300\n2  2  200\n3  4  400\n"
            ],
            "answer": {
                "ans_desc": "I have ran into these issues quite often, so I just thought to share my solutions in Pandas. Solutions: Solution 1: Using to convert the column to the index, then use to change the order, then use to change the index name back to , then use to convert the column from an index back to a column: Solution 2: Using to convert the column to the index, then use to change the order, then use to convert the column from an index back to a column: Solution 3: Using to index the rows in a different order, then use to get that order that would fit the to make it get sorted with the series: Solution 4: Using to create a new DataFrame object, then use with a argument to sort the DataFrame by the series: Timings: Timing with the below code: Output: @Allen has a pretty good answer too. ",
                "code": [
                    "print(pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns))\n",
                    "import pandas as pd\nfrom timeit import timeit\ndf = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [100, 200, 300, 400]})\ns = pd.Series([1, 3, 2, 4])\ndef u10_1():\n    return df.set_index('a').reindex(s).rename_axis('a').reset_index('a')\ndef u10_2():\n    return df.set_index('a').loc[s].reset_index()\ndef u10_3():\n    return df.iloc[list(map(df['a'].tolist().index, s))]\ndef u10_4():\n    return pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns)\nprint('u10_1:', timeit(u10_1, number=1000))\nprint('u10_2:', timeit(u10_2, number=1000))\nprint('u10_3:', timeit(u10_3, number=1000))\nprint('u10_4:', timeit(u10_4, number=1000))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 171,
            "user_id": 11508196,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/713030ab02859b9daffd6b0c8d3652e4?s=128&d=identicon&r=PG&f=1",
            "display_name": "user0204",
            "link": "https://stackoverflow.com/users/11508196/user0204"
        },
        "is_answered": true,
        "view_count": 227,
        "accepted_answer_id": 59830037,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1579550034,
        "creation_date": 1579548201,
        "question_id": 59829680,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59829680/split-string-column-based-on-delimiter-and-convert-it-to-dict-in-pandas-without",
        "title": "Split string column based on delimiter and convert it to dict in Pandas without loop",
        "body": "<p>I have below dataframe</p>\n\n<pre><code>clm1, clm2, clm3\n10, a, clm4=1|clm5=5\n11, b, clm4=2\n</code></pre>\n\n<p>My desired result is</p>\n\n<pre><code>clm1, clm2, clm4, clm5\n10, a, 1, 5\n11, b, 2, Nan\n</code></pre>\n\n<p>I have tried below method</p>\n\n<pre><code>rows = list(df.index)    \n\ndictlist = []\n\n    for index in rows: #loop through each row to convert clm3 to dict\n        i = df.at[index, \"clm3\"]        \n\n        mydict = dict(map(lambda x: x.split('='), [x for x in i.split('|') if '=' in x]))\n        dictlist.append(mydict)\n\n\nl=json_normalize(dictlist) #convert dict column to flat dataframe\n\nresultdf = example.join(l).drop('clm3',axis=1)\n</code></pre>\n\n<p>This is giving me desired result but I am looking for a more efficient way to convert clm3 to dict which does not involve looping through each row.</p>\n",
        "answer_body": "<p>two steps : </p>\n\n<p>idea is to create a double split and then group by the index and unstack the values as columns</p>\n\n<pre><code>s = (\n    df[\"clm3\"]\n    .str.split(\"|\", expand=True)\n    .stack()\n    .str.split(\"=\", expand=True)\n    .reset_index(level=1, drop=True)\n)\n\nfinal = pd.concat([df, s.groupby([s.index, s[0]])[1].sum().unstack()], axis=1).drop(\n    \"clm3\", axis=1\n)\n</code></pre>\n\n<hr>\n\n<pre><code>print(final)\n   clm1 clm2  clm4 clm5\n0    10    a     1    5\n1    11    b     2  NaN\n</code></pre>\n",
        "question_body": "<p>I have below dataframe</p>\n\n<pre><code>clm1, clm2, clm3\n10, a, clm4=1|clm5=5\n11, b, clm4=2\n</code></pre>\n\n<p>My desired result is</p>\n\n<pre><code>clm1, clm2, clm4, clm5\n10, a, 1, 5\n11, b, 2, Nan\n</code></pre>\n\n<p>I have tried below method</p>\n\n<pre><code>rows = list(df.index)    \n\ndictlist = []\n\n    for index in rows: #loop through each row to convert clm3 to dict\n        i = df.at[index, \"clm3\"]        \n\n        mydict = dict(map(lambda x: x.split('='), [x for x in i.split('|') if '=' in x]))\n        dictlist.append(mydict)\n\n\nl=json_normalize(dictlist) #convert dict column to flat dataframe\n\nresultdf = example.join(l).drop('clm3',axis=1)\n</code></pre>\n\n<p>This is giving me desired result but I am looking for a more efficient way to convert clm3 to dict which does not involve looping through each row.</p>\n",
        "formatted_input": {
            "qid": 59829680,
            "link": "https://stackoverflow.com/questions/59829680/split-string-column-based-on-delimiter-and-convert-it-to-dict-in-pandas-without",
            "question": {
                "title": "Split string column based on delimiter and convert it to dict in Pandas without loop",
                "ques_desc": "I have below dataframe My desired result is I have tried below method This is giving me desired result but I am looking for a more efficient way to convert clm3 to dict which does not involve looping through each row. "
            },
            "io": [
                "clm1, clm2, clm3\n10, a, clm4=1|clm5=5\n11, b, clm4=2\n",
                "clm1, clm2, clm4, clm5\n10, a, 1, 5\n11, b, 2, Nan\n"
            ],
            "answer": {
                "ans_desc": "two steps : idea is to create a double split and then group by the index and unstack the values as columns ",
                "code": [
                    "s = (\n    df[\"clm3\"]\n    .str.split(\"|\", expand=True)\n    .stack()\n    .str.split(\"=\", expand=True)\n    .reset_index(level=1, drop=True)\n)\n\nfinal = pd.concat([df, s.groupby([s.index, s[0]])[1].sum().unstack()], axis=1).drop(\n    \"clm3\", axis=1\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 351,
            "user_id": 8147329,
            "user_type": "registered",
            "accept_rate": 92,
            "profile_image": "https://www.gravatar.com/avatar/42a176e937b7c23e988cde052941e26c?s=128&d=identicon&r=PG&f=1",
            "display_name": "Bong Kyo Seo",
            "link": "https://stackoverflow.com/users/8147329/bong-kyo-seo"
        },
        "is_answered": true,
        "view_count": 121,
        "accepted_answer_id": 59827760,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1579539055,
        "creation_date": 1579508311,
        "question_id": 59819257,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59819257/using-np-split-array-and-then-saving-each-split-into-dataframes",
        "title": "Using np.split_array and then saving each split into dataframes",
        "body": "<p><a href=\"https://stackoverflow.com/questions/59817359/appending-data-to-a-dataframe-but-changing-rows-after-certain-of-columns\">Appending data to a dataframe but changing rows after certain # of columns</a></p>\n\n<p>The above is my previous post, where I attempted to convert 1800 row x 1 column dataframe into 300 row x 6 column dataframe through:</p>\n\n<pre><code>i = 0\nk = 2\nj = 2\n\nresult = []\ndf = pd.DataFrame()\nprint(data.shape)\nwhile j &lt; data.shape[1]:\n    tstat, data_stat = ttest_ind_from_stats(data.loc[i][k], data.loc[i + 1][k], data.loc[i + 2][k], data.loc[i][j],\n                                            data.loc[i + 1][j], data.loc[i + 2][j])\n    result.append([data_stat])\n    #print(i, k, i, j)\n    #print(i + 1, k, i + 1, j)\n    #print(i + 2, k, i + 2, j)\n    j+=1\n    if j == data.shape[1]:\n        j = 2\n        i = i + 3\n    if i == data.shape[0]:\n        k = k + 1\n        i = 0\n        if k &gt; data.shape[1]-1:\n            break\n\ndata_result = pd.DataFrame(result)\n\na = np.array(data_result)\nb = a.reshape(int(data.shape[0]*2),6)\ndata_result_new = pd.DataFrame(b)\ndata_result_new.columns = ['col1','col2','col3','col4','col5','col6']\n</code></pre>\n\n<p>I would then would like to further split the dataframe into six chunks. I was thinking about using np split like:</p>\n\n<p><code>c = np.array_split(b,6)</code></p>\n\n<p>This line would be added right after <code>b = a.reshape(int(data.shape[0]*2),6)</code> (I know the <code>data_result_new</code> lines won't work if split is applied).</p>\n\n<p>For example:</p>\n\n<p>The starting data table would look like:</p>\n\n<pre><code>col1    col2   col3    col4    col5    col6\n1       0.658  0.1067  0.777   0.459   0.3307\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n.\n.\n.\n.\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n.\n.\n.\n</code></pre>\n\n<p>and so on (please note that the numbers are just random for this post, and for testing, you can use any floating numbers, these are essentially p-values). The rows are in groups of 50 rows and hence why I would like to separate the 300x6 df into 6 df of 50x6. Because of the data size, I wasn't able to insert all of it and had to express the table as above, but for the actual testing, you can probably generate random values with 300x6 shape df (not counting the headers). </p>\n\n<p>what I want is:</p>\n\n<pre><code>[df1]\ncol1    col2   col3    col4    col5    col6\n1       0.658  0.1067  0.777   0.459   0.3307\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n\n[df2]\ncol1    col2   col3    col4    col5    col6\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n</code></pre>\n\n<p>and so on. I am not sure how I would iterate over each split from <code>np.array_split</code> then save as separate dataframes. Any help or suggestions would be appreciated.</p>\n",
        "answer_body": "<p>It might depend on how you are wanting to access the data afterwards, but you could make an extra column in the dataframe to assign group labels, and then group the data by this column and create a list of dataframes from that.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndata = np.random.rand(300,6)\ndf = pd.DataFrame(data)\n\ndf[\"label\"] = df.apply(lambda x: x.name//50, axis=1)\ngb = df.groupby(\"label\")\ndf_list = [gb.get_group(x).set_index(\"label\") for x in gb.groups]\n</code></pre>\n\n<pre><code>df.head(3)\n</code></pre>\n\n<pre><code>df.tail(3)\n</code></pre>\n\n<pre><code>for x in df_list: # each dataframe should have 50 rows and 6 columns\n    assert x.shape == (50, 6)\n</code></pre>\n\n<pre><code># print first dataframe head (rows should be same as head printed above)\ndf_list[0].head(3) # and access the values/numpy array by df_list[0].values\n</code></pre>\n\n<pre><code># print last section (rows should be same as tail printed above)\ndf_list[5].tail(3) # and access the values/numpy array by df_list[5].values\n</code></pre>\n",
        "question_body": "<p><a href=\"https://stackoverflow.com/questions/59817359/appending-data-to-a-dataframe-but-changing-rows-after-certain-of-columns\">Appending data to a dataframe but changing rows after certain # of columns</a></p>\n\n<p>The above is my previous post, where I attempted to convert 1800 row x 1 column dataframe into 300 row x 6 column dataframe through:</p>\n\n<pre><code>i = 0\nk = 2\nj = 2\n\nresult = []\ndf = pd.DataFrame()\nprint(data.shape)\nwhile j &lt; data.shape[1]:\n    tstat, data_stat = ttest_ind_from_stats(data.loc[i][k], data.loc[i + 1][k], data.loc[i + 2][k], data.loc[i][j],\n                                            data.loc[i + 1][j], data.loc[i + 2][j])\n    result.append([data_stat])\n    #print(i, k, i, j)\n    #print(i + 1, k, i + 1, j)\n    #print(i + 2, k, i + 2, j)\n    j+=1\n    if j == data.shape[1]:\n        j = 2\n        i = i + 3\n    if i == data.shape[0]:\n        k = k + 1\n        i = 0\n        if k &gt; data.shape[1]-1:\n            break\n\ndata_result = pd.DataFrame(result)\n\na = np.array(data_result)\nb = a.reshape(int(data.shape[0]*2),6)\ndata_result_new = pd.DataFrame(b)\ndata_result_new.columns = ['col1','col2','col3','col4','col5','col6']\n</code></pre>\n\n<p>I would then would like to further split the dataframe into six chunks. I was thinking about using np split like:</p>\n\n<p><code>c = np.array_split(b,6)</code></p>\n\n<p>This line would be added right after <code>b = a.reshape(int(data.shape[0]*2),6)</code> (I know the <code>data_result_new</code> lines won't work if split is applied).</p>\n\n<p>For example:</p>\n\n<p>The starting data table would look like:</p>\n\n<pre><code>col1    col2   col3    col4    col5    col6\n1       0.658  0.1067  0.777   0.459   0.3307\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n.\n.\n.\n.\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n.\n.\n.\n</code></pre>\n\n<p>and so on (please note that the numbers are just random for this post, and for testing, you can use any floating numbers, these are essentially p-values). The rows are in groups of 50 rows and hence why I would like to separate the 300x6 df into 6 df of 50x6. Because of the data size, I wasn't able to insert all of it and had to express the table as above, but for the actual testing, you can probably generate random values with 300x6 shape df (not counting the headers). </p>\n\n<p>what I want is:</p>\n\n<pre><code>[df1]\ncol1    col2   col3    col4    col5    col6\n1       0.658  0.1067  0.777   0.459   0.3307\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n\n[df2]\ncol1    col2   col3    col4    col5    col6\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n</code></pre>\n\n<p>and so on. I am not sure how I would iterate over each split from <code>np.array_split</code> then save as separate dataframes. Any help or suggestions would be appreciated.</p>\n",
        "formatted_input": {
            "qid": 59819257,
            "link": "https://stackoverflow.com/questions/59819257/using-np-split-array-and-then-saving-each-split-into-dataframes",
            "question": {
                "title": "Using np.split_array and then saving each split into dataframes",
                "ques_desc": "Appending data to a dataframe but changing rows after certain # of columns The above is my previous post, where I attempted to convert 1800 row x 1 column dataframe into 300 row x 6 column dataframe through: I would then would like to further split the dataframe into six chunks. I was thinking about using np split like: This line would be added right after (I know the lines won't work if split is applied). For example: The starting data table would look like: and so on (please note that the numbers are just random for this post, and for testing, you can use any floating numbers, these are essentially p-values). The rows are in groups of 50 rows and hence why I would like to separate the 300x6 df into 6 df of 50x6. Because of the data size, I wasn't able to insert all of it and had to express the table as above, but for the actual testing, you can probably generate random values with 300x6 shape df (not counting the headers). what I want is: and so on. I am not sure how I would iterate over each split from then save as separate dataframes. Any help or suggestions would be appreciated. "
            },
            "io": [
                "col1    col2   col3    col4    col5    col6\n1       0.658  0.1067  0.777   0.459   0.3307\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n.\n.\n.\n.\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n.\n.\n.\n",
                "[df1]\ncol1    col2   col3    col4    col5    col6\n1       0.658  0.1067  0.777   0.459   0.3307\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n1       0.622  0.4178  0.3158  0.7674  0.7426\n\n[df2]\ncol1    col2   col3    col4    col5    col6\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n0.123   1      0.1222  0.111   0.123   0.1234\n"
            ],
            "answer": {
                "ans_desc": "It might depend on how you are wanting to access the data afterwards, but you could make an extra column in the dataframe to assign group labels, and then group the data by this column and create a list of dataframes from that. ",
                "code": [
                    "import numpy as np\nimport pandas as pd\n\ndata = np.random.rand(300,6)\ndf = pd.DataFrame(data)\n\ndf[\"label\"] = df.apply(lambda x: x.name//50, axis=1)\ngb = df.groupby(\"label\")\ndf_list = [gb.get_group(x).set_index(\"label\") for x in gb.groups]\n",
                    "for x in df_list: # each dataframe should have 50 rows and 6 columns\n    assert x.shape == (50, 6)\n",
                    "# print first dataframe head (rows should be same as head printed above)\ndf_list[0].head(3) # and access the values/numpy array by df_list[0].values\n",
                    "# print last section (rows should be same as tail printed above)\ndf_list[5].tail(3) # and access the values/numpy array by df_list[5].values\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 818,
            "user_id": 8084805,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbabd4de54e1d4fec4babb756da48817?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nils",
            "link": "https://stackoverflow.com/users/8084805/nils"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 59786336,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1579269574,
        "creation_date": 1579259104,
        "question_id": 59786133,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59786133/setting-subset-of-a-pandas-dataframe-by-a-dataframe-if-a-value-matches",
        "title": "Setting subset of a pandas DataFrame by a DataFrame if a value matches",
        "body": "<p>I think the easiest way to explain what I am trying to do is by showing an example:</p>\n\n<p>Given a DataFrame</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>        V_set   V_reset     I_set   I_reset           HRS          LRS      ID\n0     0.599417 -0.658417  0.000021 -0.000606  84562.252849  1097.226787  1383.0\n1     0.595250 -0.684708  0.000023 -0.000617  43234.544776  1144.445368  1384.0\n2     0.621229 -0.710812  0.000026 -0.000625  51719.718749  1216.609759  1385.0\n3     0.625292 -0.720104  0.000029 -0.000625  40827.993527  1209.966052  1386.0\n4     0.634563 -0.735937  0.000029 -0.000641  46881.785573  1219.497465  1387.0\n       ...       ...       ...       ...           ...          ...     ...\n1066  0.167521  0.000000  0.000581  0.000000    720.116614   708.098519  2811.0\n1067  0.167360  0.000000  0.000581  0.000000    718.165882   708.284487  2812.0\n1068  0.172812  0.000000  0.000278  0.000000    715.302620   708.167571  2813.0\n1069  0.167729  0.000000  0.000581  0.000000    716.096291   708.333064  2814.0\n1070  0.173037  0.000000  0.000278  0.000000    715.474310   707.980273  2815.0\n</code></pre>\n\n<p>and a subset of a second DataFrame <code>df.loc[(df['HRS'].isnull()) &amp; (df['wfm']=='shr'), ['HRS','LRS','V_set','V_reset','I_set','I_reset', 'ID']]</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>        V_set   V_reset     I_set   I_reset           HRS          LRS      ID\n1383       NaN       NaN       NaN       NaN           NaN          NaN  1383.0     \n1384       NaN       NaN       NaN       NaN           NaN          NaN  1384.0 \n1385       NaN       NaN       NaN       NaN           NaN          NaN  1385.0\n1386       NaN       NaN       NaN       NaN           NaN          NaN  1386.0\n1387       NaN       NaN       NaN       NaN           NaN          NaN  1387.0\n       ...       ...       ...       ...           ...          ...     ...\n2811       NaN       NaN       NaN       NaN           NaN          NaN  2811.0\n2812       NaN       NaN       NaN       NaN           NaN          NaN  2812.0\n2813       NaN       NaN       NaN       NaN           NaN          NaN  2813.0\n2814       NaN       NaN       NaN       NaN           NaN          NaN  2814.0\n2815       NaN       NaN       NaN       NaN           NaN          NaN  2815.0\n</code></pre>\n\n<p>I want to replace the NaN's from the second DataFrame by the first, <strong>BUT</strong> at the place where the ID matches, as I am not sure that the selected data will always be in the same order or if all IDs will be included.</p>\n\n<p>I know I could do it with a for and if loop, but I am wondering if there is a faster way. \nIf an ID form the second DataFrame is not included in the first DataFrame the values should just stay as NaN's.</p>\n\n<p>Any help is highly appreciated.</p>\n",
        "answer_body": "<p>IIUC,</p>\n\n<p>you have common column names and want to replace NaN values with values from your first df.</p>\n\n<p>here's a solution using <code>map</code> and <code>fillna</code> this will work if your ID's have a 1 to 1 relationship. </p>\n\n<pre><code>df.set_index('ID',inplace=True)\nfor column in df.columns:\n    df2[column] = df2[column].fillna(df2['ID'].map(df[column]))\n</code></pre>\n\n<hr>\n\n<pre><code>print(df2)\n\n\n         V_set    V_reset     I_set    I_reset           HRS          LRS  \\\n1383  0.599417  -0.658417  0.000021  -0.000606  84562.252849  1097.226787   \n1384  0.595250  -0.684708  0.000023  -0.000617  43234.544776  1144.445368   \n1385  0.621229  -0.710812  0.000026  -0.000625  51719.718749  1216.609759   \n1386  0.625292  -0.720104  0.000029  -0.000625  40827.993527  1209.966052   \n1387  0.634563  -0.735937  0.000029  -0.000641  46881.785573  1219.497465   \n...        ...        ...       ...        ...           ...          ...   \n2811  0.167521   0.000000  0.000581   0.000000    720.116614   708.098519   \n2812  0.167360   0.000000  0.000581   0.000000    718.165882   708.284487   \n2813  0.172812   0.000000  0.000278   0.000000    715.302620   708.167571   \n2814  0.167729   0.000000  0.000581   0.000000    716.096291   708.333064   \n2815  0.173037   0.000000  0.000278   0.000000    715.474310   707.980273   \n\n          ID  \n1383  1383.0  \n1384  1384.0  \n1385  1385.0  \n1386  1386.0  \n1387  1387.0  \n...      NaN  \n2811  2811.0  \n2812  2812.0  \n2813  2813.0  \n2814  2814.0  \n2815  2815.0  \n</code></pre>\n\n<p>if you want to fill the entire dataframe and your keys are unique - you can set both ID's as the index and use <code>.fillna</code></p>\n\n<pre><code>df2.set_index('ID').fillna(df.set_index('ID'))\nprint(df2)\n           V_set    V_reset     I_set    I_reset           HRS          LRS\nID                                                                         \n1383.0  0.599417  -0.658417  0.000021  -0.000606  84562.252849  1097.226787\n1384.0  0.595250  -0.684708  0.000023  -0.000617  43234.544776  1144.445368\n1385.0  0.621229  -0.710812  0.000026  -0.000625  51719.718749  1216.609759\n1386.0  0.625292  -0.720104  0.000029  -0.000625  40827.993527  1209.966052\n1387.0  0.634563  -0.735937  0.000029  -0.000641  46881.785573  1219.497465\nNaN          ...        ...       ...        ...           ...          ...\n2811.0  0.167521   0.000000  0.000581   0.000000    720.116614   708.098519\n2812.0  0.167360   0.000000  0.000581   0.000000    718.165882   708.284487\n2813.0  0.172812   0.000000  0.000278   0.000000    715.302620   708.167571\n2814.0  0.167729   0.000000  0.000581   0.000000    716.096291   708.333064\n2815.0  0.173037   0.000000  0.000278   0.000000    715.474310   707.980273\n</code></pre>\n",
        "question_body": "<p>I think the easiest way to explain what I am trying to do is by showing an example:</p>\n\n<p>Given a DataFrame</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>        V_set   V_reset     I_set   I_reset           HRS          LRS      ID\n0     0.599417 -0.658417  0.000021 -0.000606  84562.252849  1097.226787  1383.0\n1     0.595250 -0.684708  0.000023 -0.000617  43234.544776  1144.445368  1384.0\n2     0.621229 -0.710812  0.000026 -0.000625  51719.718749  1216.609759  1385.0\n3     0.625292 -0.720104  0.000029 -0.000625  40827.993527  1209.966052  1386.0\n4     0.634563 -0.735937  0.000029 -0.000641  46881.785573  1219.497465  1387.0\n       ...       ...       ...       ...           ...          ...     ...\n1066  0.167521  0.000000  0.000581  0.000000    720.116614   708.098519  2811.0\n1067  0.167360  0.000000  0.000581  0.000000    718.165882   708.284487  2812.0\n1068  0.172812  0.000000  0.000278  0.000000    715.302620   708.167571  2813.0\n1069  0.167729  0.000000  0.000581  0.000000    716.096291   708.333064  2814.0\n1070  0.173037  0.000000  0.000278  0.000000    715.474310   707.980273  2815.0\n</code></pre>\n\n<p>and a subset of a second DataFrame <code>df.loc[(df['HRS'].isnull()) &amp; (df['wfm']=='shr'), ['HRS','LRS','V_set','V_reset','I_set','I_reset', 'ID']]</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>        V_set   V_reset     I_set   I_reset           HRS          LRS      ID\n1383       NaN       NaN       NaN       NaN           NaN          NaN  1383.0     \n1384       NaN       NaN       NaN       NaN           NaN          NaN  1384.0 \n1385       NaN       NaN       NaN       NaN           NaN          NaN  1385.0\n1386       NaN       NaN       NaN       NaN           NaN          NaN  1386.0\n1387       NaN       NaN       NaN       NaN           NaN          NaN  1387.0\n       ...       ...       ...       ...           ...          ...     ...\n2811       NaN       NaN       NaN       NaN           NaN          NaN  2811.0\n2812       NaN       NaN       NaN       NaN           NaN          NaN  2812.0\n2813       NaN       NaN       NaN       NaN           NaN          NaN  2813.0\n2814       NaN       NaN       NaN       NaN           NaN          NaN  2814.0\n2815       NaN       NaN       NaN       NaN           NaN          NaN  2815.0\n</code></pre>\n\n<p>I want to replace the NaN's from the second DataFrame by the first, <strong>BUT</strong> at the place where the ID matches, as I am not sure that the selected data will always be in the same order or if all IDs will be included.</p>\n\n<p>I know I could do it with a for and if loop, but I am wondering if there is a faster way. \nIf an ID form the second DataFrame is not included in the first DataFrame the values should just stay as NaN's.</p>\n\n<p>Any help is highly appreciated.</p>\n",
        "formatted_input": {
            "qid": 59786133,
            "link": "https://stackoverflow.com/questions/59786133/setting-subset-of-a-pandas-dataframe-by-a-dataframe-if-a-value-matches",
            "question": {
                "title": "Setting subset of a pandas DataFrame by a DataFrame if a value matches",
                "ques_desc": "I think the easiest way to explain what I am trying to do is by showing an example: Given a DataFrame and a subset of a second DataFrame : I want to replace the NaN's from the second DataFrame by the first, BUT at the place where the ID matches, as I am not sure that the selected data will always be in the same order or if all IDs will be included. I know I could do it with a for and if loop, but I am wondering if there is a faster way. If an ID form the second DataFrame is not included in the first DataFrame the values should just stay as NaN's. Any help is highly appreciated. "
            },
            "io": [
                "        V_set   V_reset     I_set   I_reset           HRS          LRS      ID\n0     0.599417 -0.658417  0.000021 -0.000606  84562.252849  1097.226787  1383.0\n1     0.595250 -0.684708  0.000023 -0.000617  43234.544776  1144.445368  1384.0\n2     0.621229 -0.710812  0.000026 -0.000625  51719.718749  1216.609759  1385.0\n3     0.625292 -0.720104  0.000029 -0.000625  40827.993527  1209.966052  1386.0\n4     0.634563 -0.735937  0.000029 -0.000641  46881.785573  1219.497465  1387.0\n       ...       ...       ...       ...           ...          ...     ...\n1066  0.167521  0.000000  0.000581  0.000000    720.116614   708.098519  2811.0\n1067  0.167360  0.000000  0.000581  0.000000    718.165882   708.284487  2812.0\n1068  0.172812  0.000000  0.000278  0.000000    715.302620   708.167571  2813.0\n1069  0.167729  0.000000  0.000581  0.000000    716.096291   708.333064  2814.0\n1070  0.173037  0.000000  0.000278  0.000000    715.474310   707.980273  2815.0\n",
                "        V_set   V_reset     I_set   I_reset           HRS          LRS      ID\n1383       NaN       NaN       NaN       NaN           NaN          NaN  1383.0     \n1384       NaN       NaN       NaN       NaN           NaN          NaN  1384.0 \n1385       NaN       NaN       NaN       NaN           NaN          NaN  1385.0\n1386       NaN       NaN       NaN       NaN           NaN          NaN  1386.0\n1387       NaN       NaN       NaN       NaN           NaN          NaN  1387.0\n       ...       ...       ...       ...           ...          ...     ...\n2811       NaN       NaN       NaN       NaN           NaN          NaN  2811.0\n2812       NaN       NaN       NaN       NaN           NaN          NaN  2812.0\n2813       NaN       NaN       NaN       NaN           NaN          NaN  2813.0\n2814       NaN       NaN       NaN       NaN           NaN          NaN  2814.0\n2815       NaN       NaN       NaN       NaN           NaN          NaN  2815.0\n"
            ],
            "answer": {
                "ans_desc": "IIUC, you have common column names and want to replace NaN values with values from your first df. here's a solution using and this will work if your ID's have a 1 to 1 relationship. if you want to fill the entire dataframe and your keys are unique - you can set both ID's as the index and use ",
                "code": [
                    "df.set_index('ID',inplace=True)\nfor column in df.columns:\n    df2[column] = df2[column].fillna(df2['ID'].map(df[column]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "function",
            "dataframe"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 12510771,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6403482458e2665a2d27d79d2e69e545?s=128&d=identicon&r=PG&f=1",
            "display_name": "Aibloy",
            "link": "https://stackoverflow.com/users/12510771/aibloy"
        },
        "is_answered": true,
        "view_count": 215,
        "accepted_answer_id": 59767136,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1579171217,
        "creation_date": 1579168689,
        "question_id": 59767001,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59767001/save-in-dataframe-unique-values-for-every-column",
        "title": "Save in DataFrame unique values for every column",
        "body": "<p>If I have a data (df) like this: </p>\n\n<pre><code>X1 X2 X3 \nA  A  C\nB  A  C\nC  B  C\n</code></pre>\n\n<p>With the next fuction: </p>\n\n<pre><code>for col in df:\n    print(pd.unique(df[col]))\n</code></pre>\n\n<p>It returns something like:</p>\n\n<pre><code>[A,B,C]\n[A,B]\n[C]\n</code></pre>\n\n<p>\u00bfHow can I save the return of the fuction in a DataFrame?, I would like to see it like this: </p>\n\n<pre><code>X1 X2 X3 \nA  A  C\nB  B  \nC    \n</code></pre>\n\n<p>Thanks you !</p>\n",
        "answer_body": "<p>Use lambda function with <code>Series</code> constructor and then repalce missing values: </p>\n\n<pre><code>df1 = df.apply(lambda x: pd.Series(pd.unique(x))).fillna('')\n</code></pre>\n\n<p>Or use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.drop_duplicates.html\" rel=\"nofollow noreferrer\"><code>Series.drop_duplicates</code></a>:</p>\n\n<pre><code>df1 = df.apply(lambda x: x.drop_duplicates().reset_index(drop=True)).fillna('')\n</code></pre>\n\n<hr>\n\n<pre><code>print (df1)\n  X1 X2 X3\n0  A  A  C\n1  B  B   \n2  C      \n</code></pre>\n",
        "question_body": "<p>If I have a data (df) like this: </p>\n\n<pre><code>X1 X2 X3 \nA  A  C\nB  A  C\nC  B  C\n</code></pre>\n\n<p>With the next fuction: </p>\n\n<pre><code>for col in df:\n    print(pd.unique(df[col]))\n</code></pre>\n\n<p>It returns something like:</p>\n\n<pre><code>[A,B,C]\n[A,B]\n[C]\n</code></pre>\n\n<p>\u00bfHow can I save the return of the fuction in a DataFrame?, I would like to see it like this: </p>\n\n<pre><code>X1 X2 X3 \nA  A  C\nB  B  \nC    \n</code></pre>\n\n<p>Thanks you !</p>\n",
        "formatted_input": {
            "qid": 59767001,
            "link": "https://stackoverflow.com/questions/59767001/save-in-dataframe-unique-values-for-every-column",
            "question": {
                "title": "Save in DataFrame unique values for every column",
                "ques_desc": "If I have a data (df) like this: With the next fuction: It returns something like: \u00bfHow can I save the return of the fuction in a DataFrame?, I would like to see it like this: Thanks you ! "
            },
            "io": [
                "X1 X2 X3 \nA  A  C\nB  A  C\nC  B  C\n",
                "X1 X2 X3 \nA  A  C\nB  B  \nC    \n"
            ],
            "answer": {
                "ans_desc": "Use lambda function with constructor and then repalce missing values: Or use : ",
                "code": [
                    "df1 = df.apply(lambda x: pd.Series(pd.unique(x))).fillna('')\n",
                    "df1 = df.apply(lambda x: x.drop_duplicates().reset_index(drop=True)).fillna('')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "outliers"
        ],
        "owner": {
            "reputation": 462,
            "user_id": 6399544,
            "user_type": "registered",
            "accept_rate": 57,
            "profile_image": "https://i.stack.imgur.com/3PGab.jpg?s=128&g=1",
            "display_name": "Cesar",
            "link": "https://stackoverflow.com/users/6399544/cesar"
        },
        "is_answered": true,
        "view_count": 61,
        "accepted_answer_id": 59760994,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1579131107,
        "creation_date": 1579129933,
        "question_id": 59760868,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59760868/replace-outliers-with-median-exept-nan",
        "title": "Replace outliers with median exept NaN",
        "body": "<p>I would like to replace outliers with median in a dataframe but only outliers and not NaN.</p>\n\n<p>First : </p>\n\n<pre><code>      January  February \n0      -5.0     -7.0 \n1      -6.0     -6.0 \n2      -5.0     -5.0  \n3      -3.0     -6.0 \n4      -6.0     -8.0   \n5     -11.0     -9.0    \n6      -6.0      5.0    \n7      -8.0    -11.0  \n8     -11.0    -12.0  \n9      -8.0     -9.0     \n10     -8.0     -6.0   \n11     -8.0     -5.0    \n12     -8.0     -4.0   \n13    -10.0      1.0    \n14    -10.0      3.0   \n15     -9.0     -9.0    \n16     -6.0     -6.0   \n17     -6.0     -6.0   \n18     -4.0     -4.0  \n19     -8.0      2.0    \n20     -9.0      3.0      \n21    -14.0      1.0 \n22    -15.0     -3.0  \n23    -17.0     -4.0   \n24    -19.0     -6.0     \n25    -60.0     -8.0   \n26     -8.0     -8.0   \n27     -9.0    -11.0    \n28     -5.0      NaN    \n29     -6.0      NaN    \n30     -7.0      NaN \n</code></pre>\n\n<p>I would like to replace the -60 which is an outlier with the median using : </p>\n\n<pre><code>df = df[df.apply(lambda x: np.abs(x - x.mean()) / x.std() &lt; 4).all(axis=1)]\n</code></pre>\n\n<p>It works fine but it also delete all rows containing a NaN how can I avoid that ?</p>\n\n<p>Output :</p>\n\n<pre><code>  January  February \n0      -5.0     -7.0 \n1      -6.0     -6.0 \n2      -5.0     -5.0  \n3      -3.0     -6.0 \n4      -6.0     -8.0   \n5     -11.0     -9.0    \n6      -6.0      5.0    \n7      -8.0    -11.0  \n8     -11.0    -12.0  \n9      -8.0     -9.0     \n10     -8.0     -6.0   \n11     -8.0     -5.0    \n12     -8.0     -4.0   \n13    -10.0      1.0    \n14    -10.0      3.0   \n15     -9.0     -9.0    \n16     -6.0     -6.0   \n17     -6.0     -6.0   \n18     -4.0     -4.0  \n19     -8.0      2.0    \n20     -9.0      3.0      \n21    -14.0      1.0 \n22    -15.0     -3.0  \n23    -17.0     -4.0   \n24    -19.0     -6.0     \n25    -10.0     -8.0   \n26     -8.0     -8.0   \n27     -9.0    -11.0\n</code></pre>\n\n<p>As you can see, 3 rows have been deleted which is not very convenient. Any ideas ? Thanks !</p>\n",
        "answer_body": "<p>You could use <code>.isna()</code> in your logic:</p>\n\n<pre><code>df = df[df.apply(lambda x: (np.abs(x - x.mean()) / x.std() &lt; 4) | x.isna()).all(axis=1)]\nprint(df)\n</code></pre>\n\n<p>Prints (notice index 25 (<code>-60.0</code>) is missing:</p>\n\n<pre><code>      January  February\n0        -5.0      -7.0\n1        -6.0      -6.0\n2        -5.0      -5.0\n3        -3.0      -6.0\n4        -6.0      -8.0\n5       -11.0      -9.0\n6        -6.0       5.0\n7        -8.0     -11.0\n8       -11.0     -12.0\n9        -8.0      -9.0\n10       -8.0      -6.0\n11       -8.0      -5.0\n12       -8.0      -4.0\n13      -10.0       1.0\n14      -10.0       3.0\n15       -9.0      -9.0\n16       -6.0      -6.0\n17       -6.0      -6.0\n18       -4.0      -4.0\n19       -8.0       2.0\n20       -9.0       3.0\n21      -14.0       1.0\n22      -15.0      -3.0\n23      -17.0      -4.0\n24      -19.0      -6.0\n26       -8.0      -8.0\n27       -9.0     -11.0\n28       -5.0       NaN\n29       -6.0       NaN\n30       -7.0       NaN\n</code></pre>\n",
        "question_body": "<p>I would like to replace outliers with median in a dataframe but only outliers and not NaN.</p>\n\n<p>First : </p>\n\n<pre><code>      January  February \n0      -5.0     -7.0 \n1      -6.0     -6.0 \n2      -5.0     -5.0  \n3      -3.0     -6.0 \n4      -6.0     -8.0   \n5     -11.0     -9.0    \n6      -6.0      5.0    \n7      -8.0    -11.0  \n8     -11.0    -12.0  \n9      -8.0     -9.0     \n10     -8.0     -6.0   \n11     -8.0     -5.0    \n12     -8.0     -4.0   \n13    -10.0      1.0    \n14    -10.0      3.0   \n15     -9.0     -9.0    \n16     -6.0     -6.0   \n17     -6.0     -6.0   \n18     -4.0     -4.0  \n19     -8.0      2.0    \n20     -9.0      3.0      \n21    -14.0      1.0 \n22    -15.0     -3.0  \n23    -17.0     -4.0   \n24    -19.0     -6.0     \n25    -60.0     -8.0   \n26     -8.0     -8.0   \n27     -9.0    -11.0    \n28     -5.0      NaN    \n29     -6.0      NaN    \n30     -7.0      NaN \n</code></pre>\n\n<p>I would like to replace the -60 which is an outlier with the median using : </p>\n\n<pre><code>df = df[df.apply(lambda x: np.abs(x - x.mean()) / x.std() &lt; 4).all(axis=1)]\n</code></pre>\n\n<p>It works fine but it also delete all rows containing a NaN how can I avoid that ?</p>\n\n<p>Output :</p>\n\n<pre><code>  January  February \n0      -5.0     -7.0 \n1      -6.0     -6.0 \n2      -5.0     -5.0  \n3      -3.0     -6.0 \n4      -6.0     -8.0   \n5     -11.0     -9.0    \n6      -6.0      5.0    \n7      -8.0    -11.0  \n8     -11.0    -12.0  \n9      -8.0     -9.0     \n10     -8.0     -6.0   \n11     -8.0     -5.0    \n12     -8.0     -4.0   \n13    -10.0      1.0    \n14    -10.0      3.0   \n15     -9.0     -9.0    \n16     -6.0     -6.0   \n17     -6.0     -6.0   \n18     -4.0     -4.0  \n19     -8.0      2.0    \n20     -9.0      3.0      \n21    -14.0      1.0 \n22    -15.0     -3.0  \n23    -17.0     -4.0   \n24    -19.0     -6.0     \n25    -10.0     -8.0   \n26     -8.0     -8.0   \n27     -9.0    -11.0\n</code></pre>\n\n<p>As you can see, 3 rows have been deleted which is not very convenient. Any ideas ? Thanks !</p>\n",
        "formatted_input": {
            "qid": 59760868,
            "link": "https://stackoverflow.com/questions/59760868/replace-outliers-with-median-exept-nan",
            "question": {
                "title": "Replace outliers with median exept NaN",
                "ques_desc": "I would like to replace outliers with median in a dataframe but only outliers and not NaN. First : I would like to replace the -60 which is an outlier with the median using : It works fine but it also delete all rows containing a NaN how can I avoid that ? Output : As you can see, 3 rows have been deleted which is not very convenient. Any ideas ? Thanks ! "
            },
            "io": [
                "      January  February \n0      -5.0     -7.0 \n1      -6.0     -6.0 \n2      -5.0     -5.0  \n3      -3.0     -6.0 \n4      -6.0     -8.0   \n5     -11.0     -9.0    \n6      -6.0      5.0    \n7      -8.0    -11.0  \n8     -11.0    -12.0  \n9      -8.0     -9.0     \n10     -8.0     -6.0   \n11     -8.0     -5.0    \n12     -8.0     -4.0   \n13    -10.0      1.0    \n14    -10.0      3.0   \n15     -9.0     -9.0    \n16     -6.0     -6.0   \n17     -6.0     -6.0   \n18     -4.0     -4.0  \n19     -8.0      2.0    \n20     -9.0      3.0      \n21    -14.0      1.0 \n22    -15.0     -3.0  \n23    -17.0     -4.0   \n24    -19.0     -6.0     \n25    -60.0     -8.0   \n26     -8.0     -8.0   \n27     -9.0    -11.0    \n28     -5.0      NaN    \n29     -6.0      NaN    \n30     -7.0      NaN \n",
                "  January  February \n0      -5.0     -7.0 \n1      -6.0     -6.0 \n2      -5.0     -5.0  \n3      -3.0     -6.0 \n4      -6.0     -8.0   \n5     -11.0     -9.0    \n6      -6.0      5.0    \n7      -8.0    -11.0  \n8     -11.0    -12.0  \n9      -8.0     -9.0     \n10     -8.0     -6.0   \n11     -8.0     -5.0    \n12     -8.0     -4.0   \n13    -10.0      1.0    \n14    -10.0      3.0   \n15     -9.0     -9.0    \n16     -6.0     -6.0   \n17     -6.0     -6.0   \n18     -4.0     -4.0  \n19     -8.0      2.0    \n20     -9.0      3.0      \n21    -14.0      1.0 \n22    -15.0     -3.0  \n23    -17.0     -4.0   \n24    -19.0     -6.0     \n25    -10.0     -8.0   \n26     -8.0     -8.0   \n27     -9.0    -11.0\n"
            ],
            "answer": {
                "ans_desc": "You could use in your logic: Prints (notice index 25 () is missing: ",
                "code": [
                    "df = df[df.apply(lambda x: (np.abs(x - x.mean()) / x.std() < 4) | x.isna()).all(axis=1)]\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 89,
            "user_id": 11461479,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-z0JiJujP0q0/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rc1__jdVQC07c1w_0aufqFeSbJqyQ/mo/photo.jpg?sz=128",
            "display_name": "Rafid Reaz",
            "link": "https://stackoverflow.com/users/11461479/rafid-reaz"
        },
        "is_answered": true,
        "view_count": 211,
        "accepted_answer_id": 59754657,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1579104146,
        "creation_date": 1579101913,
        "last_edit_date": 1579102553,
        "question_id": 59754548,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59754548/is-there-a-way-to-replace-each-cell-value-in-a-dataframe-with-the-column-name-r",
        "title": "Is there a way to replace each cell value in a dataframe with the column name, row value in the first column and the value itself?",
        "body": "<p>I have a matrix in excel which I am reading in as a pandas dataframe in python</p>\n\n<pre><code> col1   col2   col3    \n C_0    a      f \n C_1    b      g\n C_2    c      h\n C_3    d      i\n C_4    e      j\n</code></pre>\n\n<p>I want to be able to concatenate the column name, cell values from the first column and the current value of the cell for all cells in columns greater than col1.</p>\n\n<p>I essentially want the following output:   </p>\n\n<pre><code> col1   col2            col3    \n C_0    col2_C_0_a      col3_C_0_f \n C_1    col2_C_1_b      col3_C_1_g\n C_2    col2_C_2_c      col3_C_2_h\n C_3    col2_C_3_d      col3_C_3_i\n C_4    col2_C_4_e      col3_C_4_j\n</code></pre>\n\n<p>I could not figure out a way to do this in python.</p>\n",
        "answer_body": "<p>Using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.radd.html\" rel=\"nofollow noreferrer\"><strong><code>radd</code></strong></a>:</p>\n\n<pre><code>m = df.astype(str).set_index('col1')\nm.radd(m.index+'_',axis=0).radd(m.columns + '_').reset_index()\n</code></pre>\n\n<hr>\n\n<pre><code>  col1        col2        col3\n0  C_0  col2_C_0_a  col3_C_0_f\n1  C_1  col2_C_1_b  col3_C_1_g\n2  C_2  col2_C_2_c  col3_C_2_h\n3  C_3  col2_C_3_d  col3_C_3_i\n4  C_4  col2_C_4_e  col3_C_4_j\n</code></pre>\n",
        "question_body": "<p>I have a matrix in excel which I am reading in as a pandas dataframe in python</p>\n\n<pre><code> col1   col2   col3    \n C_0    a      f \n C_1    b      g\n C_2    c      h\n C_3    d      i\n C_4    e      j\n</code></pre>\n\n<p>I want to be able to concatenate the column name, cell values from the first column and the current value of the cell for all cells in columns greater than col1.</p>\n\n<p>I essentially want the following output:   </p>\n\n<pre><code> col1   col2            col3    \n C_0    col2_C_0_a      col3_C_0_f \n C_1    col2_C_1_b      col3_C_1_g\n C_2    col2_C_2_c      col3_C_2_h\n C_3    col2_C_3_d      col3_C_3_i\n C_4    col2_C_4_e      col3_C_4_j\n</code></pre>\n\n<p>I could not figure out a way to do this in python.</p>\n",
        "formatted_input": {
            "qid": 59754548,
            "link": "https://stackoverflow.com/questions/59754548/is-there-a-way-to-replace-each-cell-value-in-a-dataframe-with-the-column-name-r",
            "question": {
                "title": "Is there a way to replace each cell value in a dataframe with the column name, row value in the first column and the value itself?",
                "ques_desc": "I have a matrix in excel which I am reading in as a pandas dataframe in python I want to be able to concatenate the column name, cell values from the first column and the current value of the cell for all cells in columns greater than col1. I essentially want the following output: I could not figure out a way to do this in python. "
            },
            "io": [
                " col1   col2   col3    \n C_0    a      f \n C_1    b      g\n C_2    c      h\n C_3    d      i\n C_4    e      j\n",
                " col1   col2            col3    \n C_0    col2_C_0_a      col3_C_0_f \n C_1    col2_C_1_b      col3_C_1_g\n C_2    col2_C_2_c      col3_C_2_h\n C_3    col2_C_3_d      col3_C_3_i\n C_4    col2_C_4_e      col3_C_4_j\n"
            ],
            "answer": {
                "ans_desc": "Using : ",
                "code": [
                    "m = df.astype(str).set_index('col1')\nm.radd(m.index+'_',axis=0).radd(m.columns + '_').reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "bigdata"
        ],
        "owner": {
            "reputation": 2542,
            "user_id": 625609,
            "user_type": "registered",
            "accept_rate": 94,
            "profile_image": "https://www.gravatar.com/avatar/b97ccc4485e43492e36567d9fd8700c6?s=128&d=identicon&r=PG",
            "display_name": "jackson5",
            "link": "https://stackoverflow.com/users/625609/jackson5"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 59673412,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1578612315,
        "creation_date": 1578609109,
        "last_edit_date": 1578610567,
        "question_id": 59673066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59673066/most-efficient-way-to-multiply-every-column-of-a-large-pandas-dataframe-with-eve",
        "title": "Most efficient way to multiply every column of a large pandas dataframe with every other column of the same dataframe",
        "body": "<p>Suppose I have a dataset that looks something like:</p>\n\n<pre><code>INDEX   A   B   C\n    1   1   1   0.75\n    2   1   1   1\n    3   1   0   0.35\n    4   0   0   1\n    5   1   1   0\n</code></pre>\n\n<p>I want to get a dataframe that looks like the following, with the original columns, and all possible interactions between columns:</p>\n\n<pre><code>INDEX   A   B   C       A_B     A_C     B_C\n    1   1   1   0.75    1       0.75    0.75\n    2   1   1   1       1       1       1\n    3   1   0   0.35    0       0.35    0\n    4   0   0   1       0       0       0\n    5   1   1   0       1       0       0\n</code></pre>\n\n<p>My actual datasets are pretty large (~100 columns). What is the fastest way to achieve this?</p>\n\n<p>I could, of course, do a nested loop, or similar, to achieve this but I was hoping there is a more efficient way.</p>\n",
        "answer_body": "<p>You could use <a href=\"https://docs.python.org/3/library/itertools.html#itertools.combinations\" rel=\"nofollow noreferrer\">itertools.combinations</a> for this:</p>\n\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from itertools import combinations\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"A\": [1,1,1,0,1],\n...     \"B\": [1,1,0,0,1],\n...     \"C\": [.75,1,.35,1,0]\n... })\n&gt;&gt;&gt; df.head()\n   A  B     C\n0  1  1  0.75\n1  1  1  1.00\n2  1  0  0.35\n3  0  0  1.00\n4  1  1  0.00\n&gt;&gt;&gt; for col1, col2 in combinations(df.columns, 2):\n...     df[f\"{col1}_{col2}\"] = df[col1] * df[col2]\n...\n&gt;&gt;&gt; df.head()\n   A  B     C  A_B   A_C   B_C\n0  1  1  0.75    1  0.75  0.75\n1  1  1  1.00    1  1.00  1.00\n2  1  0  0.35    0  0.35  0.00\n3  0  0  1.00    0  0.00  0.00\n4  1  1  0.00    1  0.00  0.00\n</code></pre>\n\n<p>If you need to vectorize an arbitrary function on the pairs of columns you could use:</p>\n\n<pre><code>import numpy as np\n\ndef fx(x, y):\n    return np.multiply(x, y)\n\nfor col1, col2 in combinations(df.columns, 2):\n    df[f\"{col1}_{col2}\"] = np.vectorize(fx)(df[col1], df[col2])\n</code></pre>\n",
        "question_body": "<p>Suppose I have a dataset that looks something like:</p>\n\n<pre><code>INDEX   A   B   C\n    1   1   1   0.75\n    2   1   1   1\n    3   1   0   0.35\n    4   0   0   1\n    5   1   1   0\n</code></pre>\n\n<p>I want to get a dataframe that looks like the following, with the original columns, and all possible interactions between columns:</p>\n\n<pre><code>INDEX   A   B   C       A_B     A_C     B_C\n    1   1   1   0.75    1       0.75    0.75\n    2   1   1   1       1       1       1\n    3   1   0   0.35    0       0.35    0\n    4   0   0   1       0       0       0\n    5   1   1   0       1       0       0\n</code></pre>\n\n<p>My actual datasets are pretty large (~100 columns). What is the fastest way to achieve this?</p>\n\n<p>I could, of course, do a nested loop, or similar, to achieve this but I was hoping there is a more efficient way.</p>\n",
        "formatted_input": {
            "qid": 59673066,
            "link": "https://stackoverflow.com/questions/59673066/most-efficient-way-to-multiply-every-column-of-a-large-pandas-dataframe-with-eve",
            "question": {
                "title": "Most efficient way to multiply every column of a large pandas dataframe with every other column of the same dataframe",
                "ques_desc": "Suppose I have a dataset that looks something like: I want to get a dataframe that looks like the following, with the original columns, and all possible interactions between columns: My actual datasets are pretty large (~100 columns). What is the fastest way to achieve this? I could, of course, do a nested loop, or similar, to achieve this but I was hoping there is a more efficient way. "
            },
            "io": [
                "INDEX   A   B   C\n    1   1   1   0.75\n    2   1   1   1\n    3   1   0   0.35\n    4   0   0   1\n    5   1   1   0\n",
                "INDEX   A   B   C       A_B     A_C     B_C\n    1   1   1   0.75    1       0.75    0.75\n    2   1   1   1       1       1       1\n    3   1   0   0.35    0       0.35    0\n    4   0   0   1       0       0       0\n    5   1   1   0       1       0       0\n"
            ],
            "answer": {
                "ans_desc": "You could use itertools.combinations for this: If you need to vectorize an arbitrary function on the pairs of columns you could use: ",
                "code": [
                    "import numpy as np\n\ndef fx(x, y):\n    return np.multiply(x, y)\n\nfor col1, col2 in combinations(df.columns, 2):\n    df[f\"{col1}_{col2}\"] = np.vectorize(fx)(df[col1], df[col2])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "conditional-statements"
        ],
        "owner": {
            "reputation": 923,
            "user_id": 6293038,
            "user_type": "registered",
            "accept_rate": 77,
            "profile_image": "https://www.gravatar.com/avatar/7167908694353eb297e1ed2dc4941d21?s=128&d=identicon&r=PG&f=1",
            "display_name": "dayum",
            "link": "https://stackoverflow.com/users/6293038/dayum"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 50121845,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1578503294,
        "creation_date": 1525199181,
        "last_edit_date": 1578503294,
        "question_id": 50121789,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50121789/pandas-conditionally-creating-a-new-dataframe-using-another",
        "title": "Pandas conditionally creating a new dataframe using another",
        "body": "<p>I have a list;</p>\n\n<pre><code>orig= [2, 3, 4, -5, -6, -7]\n</code></pre>\n\n<p>I want to create another where entries corresponding to positive values above are sum of positives, and those corresponding to negative values above are sum negatives. So the desired output is:</p>\n\n<pre><code>final = [9, 9, 9, 18, 18, 18]\n</code></pre>\n\n<p>I am doing this:</p>\n\n<pre><code>raw = pd.DataFrame(orig, columns =['raw'])\nraw\n   raw\n0    2\n1    3\n2    4\n3   -5\n4   -6\n5   -7\n\nsum_pos = raw[raw&gt; 0].sum()\nsum_neg = -1*raw[raw &lt; 0].sum()\n\nfinal = pd.DataFrame(index = raw.index, columns = ['final'])\n\nfinal\n  final\n0   NaN\n1   NaN\n2   NaN\n3   NaN\n4   NaN\n5   NaN\n\nfinal.loc[raw &gt;0, 'final'] = sum_pos\nKeyError: \"[('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w')\\n ('r', 'a', 'w') ('r', 'a', 'w')] not in index\"\n</code></pre>\n\n<p>So basically i was trying to create an empty dataframe like raw, and then conditionally fill it. However, the above method is failing.</p>\n\n<p>Even when i try to create a new column instead of a new df, it fails:</p>\n\n<pre><code>raw.loc[raw&gt;0, 'final']= sum_pos\nKeyError: \"[('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w')\\n ('r', 'a', 'w') ('r', 'a', 'w')] not in index\"\n</code></pre>\n\n<p>The best solution I've found so far is this:</p>\n\n<pre><code>pd.DataFrame(np.where(raw&gt;0, sum_pos, sum_neg), index= raw.index, columns=['final'])\n   final\n0    9.0\n1    9.0\n2    9.0\n3   18.0\n4   18.0\n5   18.0\n</code></pre>\n\n<p>However, I dont understand what is wrong with the other approaches. Is there something I am missing here?</p>\n",
        "answer_body": "<p>You can try grouping on <code>np.sign</code>, then <code>sum</code> and <code>abs</code>:</p>\n\n<pre><code>s = pd.Series(orig)\ns.groupby(np.sign(s)).transform('sum').abs().tolist()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[9, 9, 9, 18, 18, 18]\n</code></pre>\n\n<p>You're not aligning indexes.  'sum_pos' is a series with a single element that has an index of 'raw'.  And, you are trying to assign that series to a part of dataframe that doesn't have 'raw' as an index.</p>\n\n<p>Pandas does almost everything using index alignment.  To properly do this you need to extract the values from the sum_pos series:</p>\n\n<pre><code>final.loc[raw['raw'] &gt; 0, 'final'] = sum_pos.values\n\nprint(final)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   final\n0    9.0\n1    9.0\n2    9.0\n3    NaN\n4    NaN\n5    NaN\n</code></pre>\n",
        "question_body": "<p>I have a list;</p>\n\n<pre><code>orig= [2, 3, 4, -5, -6, -7]\n</code></pre>\n\n<p>I want to create another where entries corresponding to positive values above are sum of positives, and those corresponding to negative values above are sum negatives. So the desired output is:</p>\n\n<pre><code>final = [9, 9, 9, 18, 18, 18]\n</code></pre>\n\n<p>I am doing this:</p>\n\n<pre><code>raw = pd.DataFrame(orig, columns =['raw'])\nraw\n   raw\n0    2\n1    3\n2    4\n3   -5\n4   -6\n5   -7\n\nsum_pos = raw[raw&gt; 0].sum()\nsum_neg = -1*raw[raw &lt; 0].sum()\n\nfinal = pd.DataFrame(index = raw.index, columns = ['final'])\n\nfinal\n  final\n0   NaN\n1   NaN\n2   NaN\n3   NaN\n4   NaN\n5   NaN\n\nfinal.loc[raw &gt;0, 'final'] = sum_pos\nKeyError: \"[('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w')\\n ('r', 'a', 'w') ('r', 'a', 'w')] not in index\"\n</code></pre>\n\n<p>So basically i was trying to create an empty dataframe like raw, and then conditionally fill it. However, the above method is failing.</p>\n\n<p>Even when i try to create a new column instead of a new df, it fails:</p>\n\n<pre><code>raw.loc[raw&gt;0, 'final']= sum_pos\nKeyError: \"[('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w') ('r', 'a', 'w')\\n ('r', 'a', 'w') ('r', 'a', 'w')] not in index\"\n</code></pre>\n\n<p>The best solution I've found so far is this:</p>\n\n<pre><code>pd.DataFrame(np.where(raw&gt;0, sum_pos, sum_neg), index= raw.index, columns=['final'])\n   final\n0    9.0\n1    9.0\n2    9.0\n3   18.0\n4   18.0\n5   18.0\n</code></pre>\n\n<p>However, I dont understand what is wrong with the other approaches. Is there something I am missing here?</p>\n",
        "formatted_input": {
            "qid": 50121789,
            "link": "https://stackoverflow.com/questions/50121789/pandas-conditionally-creating-a-new-dataframe-using-another",
            "question": {
                "title": "Pandas conditionally creating a new dataframe using another",
                "ques_desc": "I have a list; I want to create another where entries corresponding to positive values above are sum of positives, and those corresponding to negative values above are sum negatives. So the desired output is: I am doing this: So basically i was trying to create an empty dataframe like raw, and then conditionally fill it. However, the above method is failing. Even when i try to create a new column instead of a new df, it fails: The best solution I've found so far is this: However, I dont understand what is wrong with the other approaches. Is there something I am missing here? "
            },
            "io": [
                "orig= [2, 3, 4, -5, -6, -7]\n",
                "final = [9, 9, 9, 18, 18, 18]\n"
            ],
            "answer": {
                "ans_desc": "You can try grouping on , then and : Output: You're not aligning indexes. 'sum_pos' is a series with a single element that has an index of 'raw'. And, you are trying to assign that series to a part of dataframe that doesn't have 'raw' as an index. Pandas does almost everything using index alignment. To properly do this you need to extract the values from the sum_pos series: Output: ",
                "code": [
                    "final.loc[raw['raw'] > 0, 'final'] = sum_pos.values\n\nprint(final)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 75,
            "user_id": 12298040,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4a593994b29e3eabb708481e133ad5ec?s=128&d=identicon&r=PG&f=1",
            "display_name": "xzxz",
            "link": "https://stackoverflow.com/users/12298040/xzxz"
        },
        "is_answered": true,
        "view_count": 443,
        "accepted_answer_id": 59645672,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1578485698,
        "creation_date": 1578484264,
        "question_id": 59645274,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59645274/pandas-split-dataframe-according-to-sorted-sequence-in-columns",
        "title": "Pandas - split dataframe according to sorted sequence in columns",
        "body": "<p>I have a pandas dataframe with this type of structure:</p>\n\n<pre><code>df\nVal1 Val2 Col1 Col2\n1    1    0    3\n1    2    2    4\n2    1    2    3\n3    2    2    5\n1    2    3    4\n2    1    3    1\n3    4    2    1\n</code></pre>\n\n<p>Basically, I sort the data frame according to the values of val1 and val2 beforehand, so I know I'll have two ascending sequences afterwards. \nWhat I want is to split this df in two new dfs, according to the two sequences, which in my example would be this:</p>\n\n<pre><code>df1\nVal1 Val2 Col1 Col2\n1    1    0    3\n1    2    2    4\n2    1    2    3\n3    2    2    5\n\ndf2\nVal1 Val2 Col1 Col2\n1    2    3    4\n2    1    3    1\n3    4    2    1\n</code></pre>\n\n<p>I have checked <a href=\"https://stackoverflow.com/questions/41624241/pandas-split-dataframe-into-two-dataframes\">this question</a> and <a href=\"https://stackoverflow.com/questions/53177962/how-to-split-dataframe-length-wise?noredirect=1&amp;lq=1\">this</a>, but I don't know the number of values/rows beforehand... I've also checked <a href=\"https://stackoverflow.com/questions/41467561/split-columns-according-to-sequence-numbers\">another question</a>, so I thought about using split with a regular expression. But I only know the sequences will be ascending, there's no guarantee that the values will be continuous, so it doesn't work as expected.</p>\n\n<p>Is this possible to achieve? I appreciate in advance any help!  </p>\n",
        "answer_body": "<p>Using <code>Series.shift</code> and <code>Series.cumsum</code>:</p>\n\n<pre><code>m = df['Val1'].shift() &gt; df['Val1']\ndfs = [df for _, df in df.groupby(m.cumsum())]\n</code></pre>\n\n<p>Now we have each df in a list, we can access them:</p>\n\n<pre><code>print(dfs[0])\nprint(dfs[1])\n\n   Val1  Val2  Col1  Col2\n0     1     1     0     3\n1     1     2     2     4\n2     2     1     2     3\n3     3     2     2     5 \n\n   Val1  Val2  Col1  Col2\n4     1     2     3     4\n5     2     1     3     1\n6     3     4     2     1\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe with this type of structure:</p>\n\n<pre><code>df\nVal1 Val2 Col1 Col2\n1    1    0    3\n1    2    2    4\n2    1    2    3\n3    2    2    5\n1    2    3    4\n2    1    3    1\n3    4    2    1\n</code></pre>\n\n<p>Basically, I sort the data frame according to the values of val1 and val2 beforehand, so I know I'll have two ascending sequences afterwards. \nWhat I want is to split this df in two new dfs, according to the two sequences, which in my example would be this:</p>\n\n<pre><code>df1\nVal1 Val2 Col1 Col2\n1    1    0    3\n1    2    2    4\n2    1    2    3\n3    2    2    5\n\ndf2\nVal1 Val2 Col1 Col2\n1    2    3    4\n2    1    3    1\n3    4    2    1\n</code></pre>\n\n<p>I have checked <a href=\"https://stackoverflow.com/questions/41624241/pandas-split-dataframe-into-two-dataframes\">this question</a> and <a href=\"https://stackoverflow.com/questions/53177962/how-to-split-dataframe-length-wise?noredirect=1&amp;lq=1\">this</a>, but I don't know the number of values/rows beforehand... I've also checked <a href=\"https://stackoverflow.com/questions/41467561/split-columns-according-to-sequence-numbers\">another question</a>, so I thought about using split with a regular expression. But I only know the sequences will be ascending, there's no guarantee that the values will be continuous, so it doesn't work as expected.</p>\n\n<p>Is this possible to achieve? I appreciate in advance any help!  </p>\n",
        "formatted_input": {
            "qid": 59645274,
            "link": "https://stackoverflow.com/questions/59645274/pandas-split-dataframe-according-to-sorted-sequence-in-columns",
            "question": {
                "title": "Pandas - split dataframe according to sorted sequence in columns",
                "ques_desc": "I have a pandas dataframe with this type of structure: Basically, I sort the data frame according to the values of val1 and val2 beforehand, so I know I'll have two ascending sequences afterwards. What I want is to split this df in two new dfs, according to the two sequences, which in my example would be this: I have checked this question and this, but I don't know the number of values/rows beforehand... I've also checked another question, so I thought about using split with a regular expression. But I only know the sequences will be ascending, there's no guarantee that the values will be continuous, so it doesn't work as expected. Is this possible to achieve? I appreciate in advance any help! "
            },
            "io": [
                "df\nVal1 Val2 Col1 Col2\n1    1    0    3\n1    2    2    4\n2    1    2    3\n3    2    2    5\n1    2    3    4\n2    1    3    1\n3    4    2    1\n",
                "df1\nVal1 Val2 Col1 Col2\n1    1    0    3\n1    2    2    4\n2    1    2    3\n3    2    2    5\n\ndf2\nVal1 Val2 Col1 Col2\n1    2    3    4\n2    1    3    1\n3    4    2    1\n"
            ],
            "answer": {
                "ans_desc": "Using and : Now we have each df in a list, we can access them: ",
                "code": [
                    "m = df['Val1'].shift() > df['Val1']\ndfs = [df for _, df in df.groupby(m.cumsum())]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "python-2.7",
            "dataframe"
        ],
        "owner": {
            "reputation": 35,
            "user_id": 12663164,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mDZgxX-6DMjMe-8lttcfttEjt5dAbqZ6f-9bUKr4A=k-s128",
            "display_name": "Alessio Rovere",
            "link": "https://stackoverflow.com/users/12663164/alessio-rovere"
        },
        "is_answered": true,
        "view_count": 588,
        "accepted_answer_id": 59617233,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1578338140,
        "creation_date": 1578334780,
        "question_id": 59617019,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59617019/replace-comma-separated-values-in-a-dataframe-with-values-from-another-dataframe",
        "title": "Replace comma-separated values in a dataframe with values from another dataframe",
        "body": "<p>this is my first question on StackOverflow, so please pardon if I am not clear enough. I usually find my answers here but this time I had no luck. Maybe I am being dense, but here we go.</p>\n\n<p>I have two pandas dataframes formatted as follows</p>\n\n<p><strong>df1</strong></p>\n\n<pre><code>+------------+-------------+\n| References | Description |\n+------------+-------------+\n| 1,2        | Descr 1     |\n| 3          | Descr 2     |\n| 2,3,5      | Descr 3     |\n+------------+-------------+\n</code></pre>\n\n<p><strong>df2</strong></p>\n\n<pre><code>+--------+--------------+\n| Ref_ID |   ShortRef   |\n+--------+--------------+\n|      1 | Smith (2006) |\n|      2 | Mike (2009)  |\n|      3 | John (2014)  |\n|      4 | Cole (2007)  |\n|      5 | Jill (2019)  |\n|      6 | Tom (2007)   |\n+--------+--------------+\n</code></pre>\n\n<p>Basically, <em>Ref_ID</em> in <strong>df2</strong> contains IDs that form the string contained in the field <em>References</em> in <strong>df1</strong></p>\n\n<p>What I would like to do is to replace values in the <em>References</em> field in <strong>df1</strong> so it looks like this:</p>\n\n<pre><code>+-------------------------------------+-------------+\n|             References              | Description |\n+-------------------------------------+-------------+\n| Smith (2006); Mike (2009)           | Descr 1     |\n| John (2014)                         | Descr 2     |\n| Mike (2009);John (2014);Jill (2019) | Descr 3     |\n+-------------------------------------+-------------+\n</code></pre>\n\n<p>So far, I had to deal with columns and IDs with a 1-1 relationship, and this works perfectly\n<a href=\"https://stackoverflow.com/questions/53818434/pandas-replacing-values-by-looking-up-in-an-another-dataframe\">Pandas - Replacing Values by Looking Up in an Another Dataframe</a></p>\n\n<p>But I cannot get my mind around this slightly different problem. The only solution I could think of is to re-iterate a for and if cycles that compare every string of <strong>df1</strong> to <strong>df2</strong> and make the substitution. </p>\n\n<p>This would be, I am afraid, very slow as I have ca. 2000 unique <em>Ref_ID</em>s and I have to repeat this operation in several columns similar to the <em>References</em> one.</p>\n\n<p>Anyone is willing to point me in the right direction?</p>\n\n<p>Many thanks in advance.</p>\n",
        "answer_body": "<p>Let's try this:</p>\n\n<pre><code>df1 = pd.DataFrame({'Reference':['1,2','3','1,3,5'], 'Description':['Descr 1', 'Descr 2', 'Descr 3']})\ndf2 = pd.DataFrame({'Ref_ID':[1,2,3,4,5,6], 'ShortRef':['Smith (2006)',\n                                                       'Mike (2009)',\n                                                       'John (2014)',\n                                                       'Cole (2007)',\n                                                       'Jill (2019)',\n                                                       'Tom (2007)']})\n\ndf1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(list))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>  Reference Description                                Reference2\n0       1,2     Descr 1               [Smith (2006), Mike (2009)]\n1         3     Descr 2                             [John (2014)]\n2     1,3,5     Descr 3  [Smith (2006), John (2014), Jill (2019)]\n</code></pre>\n\n<p>@Datanovice thanks for the update.</p>\n\n<pre><code>df1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(';'.join))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>  Reference Description                            Reference2\n0       1,2     Descr 1              Smith (2006);Mike (2009)\n1         3     Descr 2                           John (2014)\n2     1,3,5     Descr 3  Smith (2006);John (2014);Jill (2019)\n</code></pre>\n",
        "question_body": "<p>this is my first question on StackOverflow, so please pardon if I am not clear enough. I usually find my answers here but this time I had no luck. Maybe I am being dense, but here we go.</p>\n\n<p>I have two pandas dataframes formatted as follows</p>\n\n<p><strong>df1</strong></p>\n\n<pre><code>+------------+-------------+\n| References | Description |\n+------------+-------------+\n| 1,2        | Descr 1     |\n| 3          | Descr 2     |\n| 2,3,5      | Descr 3     |\n+------------+-------------+\n</code></pre>\n\n<p><strong>df2</strong></p>\n\n<pre><code>+--------+--------------+\n| Ref_ID |   ShortRef   |\n+--------+--------------+\n|      1 | Smith (2006) |\n|      2 | Mike (2009)  |\n|      3 | John (2014)  |\n|      4 | Cole (2007)  |\n|      5 | Jill (2019)  |\n|      6 | Tom (2007)   |\n+--------+--------------+\n</code></pre>\n\n<p>Basically, <em>Ref_ID</em> in <strong>df2</strong> contains IDs that form the string contained in the field <em>References</em> in <strong>df1</strong></p>\n\n<p>What I would like to do is to replace values in the <em>References</em> field in <strong>df1</strong> so it looks like this:</p>\n\n<pre><code>+-------------------------------------+-------------+\n|             References              | Description |\n+-------------------------------------+-------------+\n| Smith (2006); Mike (2009)           | Descr 1     |\n| John (2014)                         | Descr 2     |\n| Mike (2009);John (2014);Jill (2019) | Descr 3     |\n+-------------------------------------+-------------+\n</code></pre>\n\n<p>So far, I had to deal with columns and IDs with a 1-1 relationship, and this works perfectly\n<a href=\"https://stackoverflow.com/questions/53818434/pandas-replacing-values-by-looking-up-in-an-another-dataframe\">Pandas - Replacing Values by Looking Up in an Another Dataframe</a></p>\n\n<p>But I cannot get my mind around this slightly different problem. The only solution I could think of is to re-iterate a for and if cycles that compare every string of <strong>df1</strong> to <strong>df2</strong> and make the substitution. </p>\n\n<p>This would be, I am afraid, very slow as I have ca. 2000 unique <em>Ref_ID</em>s and I have to repeat this operation in several columns similar to the <em>References</em> one.</p>\n\n<p>Anyone is willing to point me in the right direction?</p>\n\n<p>Many thanks in advance.</p>\n",
        "formatted_input": {
            "qid": 59617019,
            "link": "https://stackoverflow.com/questions/59617019/replace-comma-separated-values-in-a-dataframe-with-values-from-another-dataframe",
            "question": {
                "title": "Replace comma-separated values in a dataframe with values from another dataframe",
                "ques_desc": "this is my first question on StackOverflow, so please pardon if I am not clear enough. I usually find my answers here but this time I had no luck. Maybe I am being dense, but here we go. I have two pandas dataframes formatted as follows df1 df2 Basically, Ref_ID in df2 contains IDs that form the string contained in the field References in df1 What I would like to do is to replace values in the References field in df1 so it looks like this: So far, I had to deal with columns and IDs with a 1-1 relationship, and this works perfectly Pandas - Replacing Values by Looking Up in an Another Dataframe But I cannot get my mind around this slightly different problem. The only solution I could think of is to re-iterate a for and if cycles that compare every string of df1 to df2 and make the substitution. This would be, I am afraid, very slow as I have ca. 2000 unique Ref_IDs and I have to repeat this operation in several columns similar to the References one. Anyone is willing to point me in the right direction? Many thanks in advance. "
            },
            "io": [
                "+------------+-------------+\n| References | Description |\n+------------+-------------+\n| 1,2        | Descr 1     |\n| 3          | Descr 2     |\n| 2,3,5      | Descr 3     |\n+------------+-------------+\n",
                "+-------------------------------------+-------------+\n|             References              | Description |\n+-------------------------------------+-------------+\n| Smith (2006); Mike (2009)           | Descr 1     |\n| John (2014)                         | Descr 2     |\n| Mike (2009);John (2014);Jill (2019) | Descr 3     |\n+-------------------------------------+-------------+\n"
            ],
            "answer": {
                "ans_desc": "Let's try this: Output: @Datanovice thanks for the update. Output: ",
                "code": [
                    "df1 = pd.DataFrame({'Reference':['1,2','3','1,3,5'], 'Description':['Descr 1', 'Descr 2', 'Descr 3']})\ndf2 = pd.DataFrame({'Ref_ID':[1,2,3,4,5,6], 'ShortRef':['Smith (2006)',\n                                                       'Mike (2009)',\n                                                       'John (2014)',\n                                                       'Cole (2007)',\n                                                       'Jill (2019)',\n                                                       'Tom (2007)']})\n\ndf1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(list))\n",
                    "df1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(';'.join))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "apply"
        ],
        "owner": {
            "reputation": 27,
            "user_id": 11009876,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/4w9uQ.jpg?s=128&g=1",
            "display_name": "Jim Waldron",
            "link": "https://stackoverflow.com/users/11009876/jim-waldron"
        },
        "is_answered": true,
        "view_count": 81,
        "accepted_answer_id": 59596239,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1578208125,
        "creation_date": 1578168362,
        "last_edit_date": 1578208125,
        "question_id": 59594392,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59594392/replace-negatives-with-zeros-in-a-dataframe-column-of-lists",
        "title": "Replace negatives with zeros in a dataframe column of lists",
        "body": "<p>I have a dataframe containing two columns.</p>\n\n<p>The first column is the date index.</p>\n\n<p>Each row of the second column is a list of 60 numbers that include negative values.</p>\n\n<pre><code>                    Spc\n1976-10-31 15:00:00 [0.0124, 0.0096, 0.0325, 0.1562, 0.4494, 0.738...-1., -1., -1., -1.]\n1976-11-01 03:00:00 [0.0254, 0.0299, 0.0273, 0.1229, 0.596, 0.9833...-1., -1., -1., -1.]\n1976-11-01 15:00:00 [0.0226, 0.0236, 0.0269, 0.085, 0.4163, 0.8011...-1., -1., -1., -1.]\n1976-11-02 03:00:00 [0.0132, 0.0154, 0.0172, 0.1336, 0.4743, 0.694...-1., -1., -1., -1.]\n1976-11-02 15:00:00 [0.0124, 0.0169, 0.028, 0.5028, 1.4503, 1.6055...-1., -1., -1., -1.]\n     :     :     :     :     :     :     :     :     :     :\n2017-05-20 04:00:00 [5.374061e-13, 1.2720002e-06, 0.00052255474, 0...2.8157034e-03, 1.4578120e-03]\n2017-05-20 04:30:00 [1.2021946e-12, 3.3477074e-06, 0.0014435094, 0...5.88221522e-03, 3.44922021e-03]\n2017-05-20 05:00:00 [1.2236685e-13, 5.018357e-07, 0.00023753957, 0...2.28277827e-03, 1.07194704e-03]\n2017-05-20 05:30:00 [3.5527579e-13, 1.1004944e-06, 0.0005480177, 0...2.0632602e-03, 1.6171171e-03]\n2017-05-20 06:00:00 [4.968573e-13, 1.4969078e-06, 0.00065009575, 0...1.21051911e-03, 1.18123344e-03]\n</code></pre>\n\n<p><strong>I want to replace all negative values in this column with zeros.</strong></p>\n\n<p>Here is the complete data for the first two rows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>1976-10-31 15:00:00 [ 0.0013,  0.0016,  0.007,   0.03,    0.0803,  0.2318,  0.5842,  0.8401,  0.6,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,\n -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]\n1976-11-01 03:00:00 [ 0.0022,  0.004,   0.0104,  0.0512,  0.1112,  0.2227,  0.5263,  0.7085,  0.4,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,\n -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]\n</code></pre>\n\n<p>Currently, my solution is to convert the column of lists into a separate df of 60 columns. I can then convert the negatives into zeros in this df.</p>\n\n<pre><code># Convert the spectral ordinates from DF column of lists into columns\nSpc = df_PRIM_SECO.Spc.apply(pd.Series)\n\n# Set all negative values in DF to zero\nSpc[Spc &lt; 0] = 0\n</code></pre>\n\n<p>Although this does the job, the .apply() operation is slow (taking 1.3 minutes for a df with 400,000 rows).</p>\n\n<p>Could someone please offer a more efficient (faster) alternative?</p>\n",
        "answer_body": "<p>Here are a few solutions, with some basic timings.</p>\n\n<hr>\n\n<p>Setup code, and test data:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\n\nnum_rows = 100000\n\ndf_1 = pd.DataFrame(data={'col_1': np.random.randint(0, 100, num_rows),\n                          'col_2': np.random.uniform(-10, 10, (num_rows, 60)).tolist()})\n</code></pre>\n\n<hr>\n\n<h3>1. <code>.apply(pd.Series)</code> : ~18,400 ms</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>def func_1(df_in):\n    df_in = df_in.copy()\n    temp = df_in['col_2'].apply(pd.Series)\n    temp[temp &lt; 0] = 0\n    df_in['col_2'] = temp.to_numpy().tolist()\n    return df_in\n</code></pre>\n\n<h3>2. List comprehension : ~460 ms</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>def func_2(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = df_in['col_2'].map(lambda l: [0 if elem &lt; 0 else elem for elem in l])\n    return df_in\n</code></pre>\n\n<h3>3. Solution by @Valdi_Bo : ~832 ms</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>def func_3(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array([np.array(elem) for elem in df_in['col_2']]).clip(min=0).tolist()\n    return df_in\n</code></pre>\n\n<h3>4. Solution similar in principle to the one by @Valdi_Bo : ~926 ms</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>def func_4(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.stack(df_in['col_2'].to_numpy()).clip(min=0).tolist()\n    return df_in\n</code></pre>\n\n<h3>5. A cousin of 3 and 4 : ~691 ms</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>def func_5(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array(df_in['col_2'].tolist()).clip(min=0).tolist()\n    return df_in\n</code></pre>\n\n<hr>\n\n<p>Note: The relative speed differences are maintained on an input with 10 times the amount of rows (1,000,000 instead of 100,000).</p>\n",
        "question_body": "<p>I have a dataframe containing two columns.</p>\n\n<p>The first column is the date index.</p>\n\n<p>Each row of the second column is a list of 60 numbers that include negative values.</p>\n\n<pre><code>                    Spc\n1976-10-31 15:00:00 [0.0124, 0.0096, 0.0325, 0.1562, 0.4494, 0.738...-1., -1., -1., -1.]\n1976-11-01 03:00:00 [0.0254, 0.0299, 0.0273, 0.1229, 0.596, 0.9833...-1., -1., -1., -1.]\n1976-11-01 15:00:00 [0.0226, 0.0236, 0.0269, 0.085, 0.4163, 0.8011...-1., -1., -1., -1.]\n1976-11-02 03:00:00 [0.0132, 0.0154, 0.0172, 0.1336, 0.4743, 0.694...-1., -1., -1., -1.]\n1976-11-02 15:00:00 [0.0124, 0.0169, 0.028, 0.5028, 1.4503, 1.6055...-1., -1., -1., -1.]\n     :     :     :     :     :     :     :     :     :     :\n2017-05-20 04:00:00 [5.374061e-13, 1.2720002e-06, 0.00052255474, 0...2.8157034e-03, 1.4578120e-03]\n2017-05-20 04:30:00 [1.2021946e-12, 3.3477074e-06, 0.0014435094, 0...5.88221522e-03, 3.44922021e-03]\n2017-05-20 05:00:00 [1.2236685e-13, 5.018357e-07, 0.00023753957, 0...2.28277827e-03, 1.07194704e-03]\n2017-05-20 05:30:00 [3.5527579e-13, 1.1004944e-06, 0.0005480177, 0...2.0632602e-03, 1.6171171e-03]\n2017-05-20 06:00:00 [4.968573e-13, 1.4969078e-06, 0.00065009575, 0...1.21051911e-03, 1.18123344e-03]\n</code></pre>\n\n<p><strong>I want to replace all negative values in this column with zeros.</strong></p>\n\n<p>Here is the complete data for the first two rows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>1976-10-31 15:00:00 [ 0.0013,  0.0016,  0.007,   0.03,    0.0803,  0.2318,  0.5842,  0.8401,  0.6,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,\n -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]\n1976-11-01 03:00:00 [ 0.0022,  0.004,   0.0104,  0.0512,  0.1112,  0.2227,  0.5263,  0.7085,  0.4,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,\n -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]\n</code></pre>\n\n<p>Currently, my solution is to convert the column of lists into a separate df of 60 columns. I can then convert the negatives into zeros in this df.</p>\n\n<pre><code># Convert the spectral ordinates from DF column of lists into columns\nSpc = df_PRIM_SECO.Spc.apply(pd.Series)\n\n# Set all negative values in DF to zero\nSpc[Spc &lt; 0] = 0\n</code></pre>\n\n<p>Although this does the job, the .apply() operation is slow (taking 1.3 minutes for a df with 400,000 rows).</p>\n\n<p>Could someone please offer a more efficient (faster) alternative?</p>\n",
        "formatted_input": {
            "qid": 59594392,
            "link": "https://stackoverflow.com/questions/59594392/replace-negatives-with-zeros-in-a-dataframe-column-of-lists",
            "question": {
                "title": "Replace negatives with zeros in a dataframe column of lists",
                "ques_desc": "I have a dataframe containing two columns. The first column is the date index. Each row of the second column is a list of 60 numbers that include negative values. I want to replace all negative values in this column with zeros. Here is the complete data for the first two rows: Currently, my solution is to convert the column of lists into a separate df of 60 columns. I can then convert the negatives into zeros in this df. Although this does the job, the .apply() operation is slow (taking 1.3 minutes for a df with 400,000 rows). Could someone please offer a more efficient (faster) alternative? "
            },
            "io": [
                "                    Spc\n1976-10-31 15:00:00 [0.0124, 0.0096, 0.0325, 0.1562, 0.4494, 0.738...-1., -1., -1., -1.]\n1976-11-01 03:00:00 [0.0254, 0.0299, 0.0273, 0.1229, 0.596, 0.9833...-1., -1., -1., -1.]\n1976-11-01 15:00:00 [0.0226, 0.0236, 0.0269, 0.085, 0.4163, 0.8011...-1., -1., -1., -1.]\n1976-11-02 03:00:00 [0.0132, 0.0154, 0.0172, 0.1336, 0.4743, 0.694...-1., -1., -1., -1.]\n1976-11-02 15:00:00 [0.0124, 0.0169, 0.028, 0.5028, 1.4503, 1.6055...-1., -1., -1., -1.]\n     :     :     :     :     :     :     :     :     :     :\n2017-05-20 04:00:00 [5.374061e-13, 1.2720002e-06, 0.00052255474, 0...2.8157034e-03, 1.4578120e-03]\n2017-05-20 04:30:00 [1.2021946e-12, 3.3477074e-06, 0.0014435094, 0...5.88221522e-03, 3.44922021e-03]\n2017-05-20 05:00:00 [1.2236685e-13, 5.018357e-07, 0.00023753957, 0...2.28277827e-03, 1.07194704e-03]\n2017-05-20 05:30:00 [3.5527579e-13, 1.1004944e-06, 0.0005480177, 0...2.0632602e-03, 1.6171171e-03]\n2017-05-20 06:00:00 [4.968573e-13, 1.4969078e-06, 0.00065009575, 0...1.21051911e-03, 1.18123344e-03]\n",
                "1976-10-31 15:00:00 [ 0.0013,  0.0016,  0.007,   0.03,    0.0803,  0.2318,  0.5842,  0.8401,  0.6,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,\n -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]\n1976-11-01 03:00:00 [ 0.0022,  0.004,   0.0104,  0.0512,  0.1112,  0.2227,  0.5263,  0.7085,  0.4,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,\n  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,\n -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]\n"
            ],
            "answer": {
                "ans_desc": "Here are a few solutions, with some basic timings. Setup code, and test data: 1. : ~18,400 ms 2. List comprehension : ~460 ms 3. Solution by @Valdi_Bo : ~832 ms 4. Solution similar in principle to the one by @Valdi_Bo : ~926 ms 5. A cousin of 3 and 4 : ~691 ms Note: The relative speed differences are maintained on an input with 10 times the amount of rows (1,000,000 instead of 100,000). ",
                "code": [
                    "def func_1(df_in):\n    df_in = df_in.copy()\n    temp = df_in['col_2'].apply(pd.Series)\n    temp[temp < 0] = 0\n    df_in['col_2'] = temp.to_numpy().tolist()\n    return df_in\n",
                    "def func_2(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = df_in['col_2'].map(lambda l: [0 if elem < 0 else elem for elem in l])\n    return df_in\n",
                    "def func_3(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array([np.array(elem) for elem in df_in['col_2']]).clip(min=0).tolist()\n    return df_in\n",
                    "def func_4(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.stack(df_in['col_2'].to_numpy()).clip(min=0).tolist()\n    return df_in\n",
                    "def func_5(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array(df_in['col_2'].tolist()).clip(min=0).tolist()\n    return df_in\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 218,
        "accepted_answer_id": 59589875,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1578134394,
        "creation_date": 1578133819,
        "question_id": 59589861,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59589861/pandas-group-by-the-column-values-with-all-values-less-than-certain-numbers-and",
        "title": "pandas group by the column values with all values less than certain numbers and assign the group numbers as new columns",
        "body": "<p>I have a data frame like this,</p>\n\n<pre><code>df\ncol1    col2\n A        2\n B        3\n C        1\n D        4\n E        6\n F        1\n G        2\n H        8\n I        1\n J       10\n</code></pre>\n\n<p>Now I want to create another column col3 with grouping all the col2 values which are under below 5 and keep col3 values as 1 to number of groups, so the final data frame would look like,</p>\n\n<pre><code>col1    col2     col3\n A        2        1\n B        3        1\n C        1        1\n D        4        1\n E        6        2\n F        1        2\n G        2        2\n H        8        3\n I        1        3\n J       10        4\n</code></pre>\n\n<p>I could do this comparing the the prev values with the current values and store into a list and make it the col3. </p>\n\n<p>But the execution time will be huge in this case, so looking for some shortcuts/pythonic way to do it most efficiently.</p>\n",
        "answer_body": "<p>Compare by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.gt.html\" rel=\"nofollow noreferrer\"><code>Series.gt</code></a> for <code>&gt;</code> and then use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html\" rel=\"nofollow noreferrer\"><code>Series.cumsum</code></a>. New column always starts by <code>0</code>, because first values of column is less like <code>5</code>, else it should be <code>1</code>:</p>\n\n<pre><code>df['col3'] = df['col2'].gt(5).cumsum()\nprint (df)\n  col1  col2  col3\n0    A     2     0\n1    B     3     0\n2    C     1     0\n3    D     4     0\n4    E     6     1\n5    F     1     1\n6    G     2     1\n7    H     8     2\n8    I     1     2\n9    J    10     3\n</code></pre>\n\n<p>So  for general solution starting by <code>1</code> use this trick - compare first values if less like <code>5</code>, convert to integers for <code>True-&gt;1</code> and <code>False-&gt;0</code> and add to column:</p>\n\n<pre><code>N = 5\ndf['col3'] = df['col2'].gt(N).cumsum() + int(df.loc[0, 'col2'] &lt; 5)\n</code></pre>\n\n<hr>\n\n<pre><code>df = df.assign(col21 = df['col2'].add(pd.Series({0:5}), fill_value=0).astype(int))\n\nN = 5\ndf['col3'] = df['col2'].gt(N).cumsum() + int(df.loc[0, 'col2'] &lt; N)\n#test for first value &gt; 5\ndf['col31'] = df['col21'].gt(N).cumsum() + int(df.loc[0, 'col21'] &lt; N)\nprint (df)\n  col1  col2  col21  col3  col31\n0    A     2      7     1      1\n1    B     3      3     1      1\n2    C     1      1     1      1\n3    D     4      4     1      1\n4    E     6      6     2      2\n5    F     1      1     2      2\n6    G     2      2     2      2\n7    H     8      8     3      3\n8    I     1      1     3      3\n9    J    10     10     4      4\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this,</p>\n\n<pre><code>df\ncol1    col2\n A        2\n B        3\n C        1\n D        4\n E        6\n F        1\n G        2\n H        8\n I        1\n J       10\n</code></pre>\n\n<p>Now I want to create another column col3 with grouping all the col2 values which are under below 5 and keep col3 values as 1 to number of groups, so the final data frame would look like,</p>\n\n<pre><code>col1    col2     col3\n A        2        1\n B        3        1\n C        1        1\n D        4        1\n E        6        2\n F        1        2\n G        2        2\n H        8        3\n I        1        3\n J       10        4\n</code></pre>\n\n<p>I could do this comparing the the prev values with the current values and store into a list and make it the col3. </p>\n\n<p>But the execution time will be huge in this case, so looking for some shortcuts/pythonic way to do it most efficiently.</p>\n",
        "formatted_input": {
            "qid": 59589861,
            "link": "https://stackoverflow.com/questions/59589861/pandas-group-by-the-column-values-with-all-values-less-than-certain-numbers-and",
            "question": {
                "title": "pandas group by the column values with all values less than certain numbers and assign the group numbers as new columns",
                "ques_desc": "I have a data frame like this, Now I want to create another column col3 with grouping all the col2 values which are under below 5 and keep col3 values as 1 to number of groups, so the final data frame would look like, I could do this comparing the the prev values with the current values and store into a list and make it the col3. But the execution time will be huge in this case, so looking for some shortcuts/pythonic way to do it most efficiently. "
            },
            "io": [
                "df\ncol1    col2\n A        2\n B        3\n C        1\n D        4\n E        6\n F        1\n G        2\n H        8\n I        1\n J       10\n",
                "col1    col2     col3\n A        2        1\n B        3        1\n C        1        1\n D        4        1\n E        6        2\n F        1        2\n G        2        2\n H        8        3\n I        1        3\n J       10        4\n"
            ],
            "answer": {
                "ans_desc": "Compare by for and then use . New column always starts by , because first values of column is less like , else it should be : So for general solution starting by use this trick - compare first values if less like , convert to integers for and and add to column: ",
                "code": [
                    "N = 5\ndf['col3'] = df['col2'].gt(N).cumsum() + int(df.loc[0, 'col2'] < 5)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "list-comprehension"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 12633730,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/520ddfe88898ba28cd9aae4610a40f67?s=128&d=identicon&r=PG&f=1",
            "display_name": "freshestcereal",
            "link": "https://stackoverflow.com/users/12633730/freshestcereal"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 59548116,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1577841467,
        "creation_date": 1577822127,
        "last_edit_date": 1577841467,
        "question_id": 59548017,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59548017/create-a-list-in-list-comprehension-and-then-create-another-list-from-that-list",
        "title": "Create a list in list comprehension and then create another list from that list inside the same list comprehension",
        "body": "<p>That title is a mouthful, so it may be easier to show what I am trying to achieve via code.</p>\n\n<pre><code>import pandas as pd\n\ndata = []\nfor i in range(0,6):\n    row = []\n    for j in range(0,6):\n        if i == j:\n            row.append(1)\n        else:\n            row.append(0)\n    data.append(row)\n\ncolnames = [(1,2),(1,3),(1,4),(2,1),(3,1),(4,1)]\ndf = pd.DataFrame(data, columns = colnames)\n</code></pre>\n\n<p>The above is not where I am having trouble with, but I wanted to provide as much context as possible.</p>\n\n<p>I am trying to iterate through the dataframe and retrieve the first element of the column name where the dataframe's element equals 1. I can do this using the following:</p>\n\n<pre><code>[col[0] for col in df if (df[col] == 1).any()]\n</code></pre>\n\n<p>This generates the first list I need to create:</p>\n\n<pre><code>[1, 1, 1, 2, 3, 4]\n</code></pre>\n\n<p>I can assign that output to a variable and perform another list comprehension to get my final output:</p>\n\n<pre><code>res = [col[0] for col in df if (df[col] == 1).any()]\n\n[(res[i], res[i+1]) for i in range(len(res)-1)]\n</code></pre>\n\n<p>This outputs my final list of:</p>\n\n<pre><code>[(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)]\n</code></pre>\n\n<p>Is it possible to perform both of these inside the same list comprehension while only receiving that final list as the output?</p>\n\n<p>This is somewhat inelegant, but this is what it would look like in non-list comprehension:</p>\n\n<pre><code>x = []\ny = []\nfor i in range(0,1):\n    for col in df:\n        if df[col].any() == 1:\n            x.append(col[0])\n    for j in range(len(x)-1):\n        y.append((x[j],x[j+1]))\n</code></pre>\n\n<p>Thank you for taking the time to look this over!</p>\n",
        "answer_body": "<p>you can use a <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dot.html\" rel=\"nofollow noreferrer\"><code>dot</code></a> product with a conditional statement , then zip and extract a tuple:</p>\n\n<pre><code>[(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(df.columns.str[0])[1:])]\n#same with .iloc -&gt; [(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(\n#                                          df.columns.str[0]).iloc[1:])]\n</code></pre>\n\n<hr>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>[(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)]\n</code></pre>\n\n<p><strong>Full Working Code</strong>:</p>\n\n<pre><code>data = []\nfor i in range(0,6):\n    row = []\n    for j in range(0,6):\n        if i == j:\n            row.append(1)\n        else:\n            row.append(0)\n    data.append(row)\n\ncolnames = [(1,2),(1,3),(1,4),(2,1),(3,1),(4,1)]\ndf = pd.DataFrame(data, columns = colnames)\n[(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(df.columns.str[0])[1:])]\n#[(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)]\n</code></pre>\n",
        "question_body": "<p>That title is a mouthful, so it may be easier to show what I am trying to achieve via code.</p>\n\n<pre><code>import pandas as pd\n\ndata = []\nfor i in range(0,6):\n    row = []\n    for j in range(0,6):\n        if i == j:\n            row.append(1)\n        else:\n            row.append(0)\n    data.append(row)\n\ncolnames = [(1,2),(1,3),(1,4),(2,1),(3,1),(4,1)]\ndf = pd.DataFrame(data, columns = colnames)\n</code></pre>\n\n<p>The above is not where I am having trouble with, but I wanted to provide as much context as possible.</p>\n\n<p>I am trying to iterate through the dataframe and retrieve the first element of the column name where the dataframe's element equals 1. I can do this using the following:</p>\n\n<pre><code>[col[0] for col in df if (df[col] == 1).any()]\n</code></pre>\n\n<p>This generates the first list I need to create:</p>\n\n<pre><code>[1, 1, 1, 2, 3, 4]\n</code></pre>\n\n<p>I can assign that output to a variable and perform another list comprehension to get my final output:</p>\n\n<pre><code>res = [col[0] for col in df if (df[col] == 1).any()]\n\n[(res[i], res[i+1]) for i in range(len(res)-1)]\n</code></pre>\n\n<p>This outputs my final list of:</p>\n\n<pre><code>[(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)]\n</code></pre>\n\n<p>Is it possible to perform both of these inside the same list comprehension while only receiving that final list as the output?</p>\n\n<p>This is somewhat inelegant, but this is what it would look like in non-list comprehension:</p>\n\n<pre><code>x = []\ny = []\nfor i in range(0,1):\n    for col in df:\n        if df[col].any() == 1:\n            x.append(col[0])\n    for j in range(len(x)-1):\n        y.append((x[j],x[j+1]))\n</code></pre>\n\n<p>Thank you for taking the time to look this over!</p>\n",
        "formatted_input": {
            "qid": 59548017,
            "link": "https://stackoverflow.com/questions/59548017/create-a-list-in-list-comprehension-and-then-create-another-list-from-that-list",
            "question": {
                "title": "Create a list in list comprehension and then create another list from that list inside the same list comprehension",
                "ques_desc": "That title is a mouthful, so it may be easier to show what I am trying to achieve via code. The above is not where I am having trouble with, but I wanted to provide as much context as possible. I am trying to iterate through the dataframe and retrieve the first element of the column name where the dataframe's element equals 1. I can do this using the following: This generates the first list I need to create: I can assign that output to a variable and perform another list comprehension to get my final output: This outputs my final list of: Is it possible to perform both of these inside the same list comprehension while only receiving that final list as the output? This is somewhat inelegant, but this is what it would look like in non-list comprehension: Thank you for taking the time to look this over! "
            },
            "io": [
                "[1, 1, 1, 2, 3, 4]\n",
                "[(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)]\n"
            ],
            "answer": {
                "ans_desc": "you can use a product with a conditional statement , then zip and extract a tuple: Output: Full Working Code: ",
                "code": [
                    "[(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(df.columns.str[0])[1:])]\n#same with .iloc -> [(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(\n#                                          df.columns.str[0]).iloc[1:])]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 455,
            "user_id": 11455772,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-3igl9y_OX-E/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rce1slUOfzNzFk1wXaxEZ8Js0sa-g/mo/photo.jpg?sz=128",
            "display_name": "gal leshem",
            "link": "https://stackoverflow.com/users/11455772/gal-leshem"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 59533885,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1577732323,
        "creation_date": 1577721245,
        "question_id": 59533480,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59533480/add-column-to-groups-on-dataframe-pandas",
        "title": "add column to groups on dataframe pandas",
        "body": "<p>I have a dataframe:</p>\n\n<pre><code>  id  |   x   |   y \n   1  |  0.3  |  0.4\n   1  |  0.2  |  0.5\n   2  |  0.1  |  0.6\n   2  |  0.9  |  0.1\n   3  |  0.8  |  0.2\n   3  |  0.7  |  0.3\n</code></pre>\n\n<p>How can I add a new column to dataframe relative to the id column?</p>\n\n<p>for example: </p>\n\n<pre><code>  id  |   x   |   y   |  color\n   1  |  0.3  |  0.4  | 'green'\n   1  |  0.2  |  0.5  | 'green'\n   2  |  0.1  |  0.6  | 'black'\n   2  |  0.9  |  0.1  | 'black'\n   3  |  0.8  |  0.2  |  'red'\n   3  |  0.7  |  0.3  |  'red'\n</code></pre>\n",
        "answer_body": "<p>So your function doesn't return color names but instead the RGB values, if this it what you want in the color column build the dictionary first from the unique id values and apply the dictionary the way @anky_91 mentioned in the comments.</p>\n\n<pre><code>d={x:random_color() for x in df.id.unique()}\ndf['color']=df['id'].map(d)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n\n<pre><code>  id  |   x   |   y \n   1  |  0.3  |  0.4\n   1  |  0.2  |  0.5\n   2  |  0.1  |  0.6\n   2  |  0.9  |  0.1\n   3  |  0.8  |  0.2\n   3  |  0.7  |  0.3\n</code></pre>\n\n<p>How can I add a new column to dataframe relative to the id column?</p>\n\n<p>for example: </p>\n\n<pre><code>  id  |   x   |   y   |  color\n   1  |  0.3  |  0.4  | 'green'\n   1  |  0.2  |  0.5  | 'green'\n   2  |  0.1  |  0.6  | 'black'\n   2  |  0.9  |  0.1  | 'black'\n   3  |  0.8  |  0.2  |  'red'\n   3  |  0.7  |  0.3  |  'red'\n</code></pre>\n",
        "formatted_input": {
            "qid": 59533480,
            "link": "https://stackoverflow.com/questions/59533480/add-column-to-groups-on-dataframe-pandas",
            "question": {
                "title": "add column to groups on dataframe pandas",
                "ques_desc": "I have a dataframe: How can I add a new column to dataframe relative to the id column? for example: "
            },
            "io": [
                "  id  |   x   |   y \n   1  |  0.3  |  0.4\n   1  |  0.2  |  0.5\n   2  |  0.1  |  0.6\n   2  |  0.9  |  0.1\n   3  |  0.8  |  0.2\n   3  |  0.7  |  0.3\n",
                "  id  |   x   |   y   |  color\n   1  |  0.3  |  0.4  | 'green'\n   1  |  0.2  |  0.5  | 'green'\n   2  |  0.1  |  0.6  | 'black'\n   2  |  0.9  |  0.1  | 'black'\n   3  |  0.8  |  0.2  |  'red'\n   3  |  0.7  |  0.3  |  'red'\n"
            ],
            "answer": {
                "ans_desc": "So your function doesn't return color names but instead the RGB values, if this it what you want in the color column build the dictionary first from the unique id values and apply the dictionary the way @anky_91 mentioned in the comments. ",
                "code": [
                    "d={x:random_color() for x in df.id.unique()}\ndf['color']=df['id'].map(d)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10361,
            "user_id": 6031995,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/KAcWh.png?s=128&g=1",
            "display_name": "Kenan",
            "link": "https://stackoverflow.com/users/6031995/kenan"
        },
        "is_answered": true,
        "view_count": 134,
        "accepted_answer_id": 59506490,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1577489794,
        "creation_date": 1577476409,
        "last_edit_date": 1577477528,
        "question_id": 59505475,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59505475/pandas-swap-rows-between-columns",
        "title": "Pandas: Swap rows between columns",
        "body": "<p>Some rows were input in the wrong columns so now I need to swap them. </p>\n\n<p><code>df = pd.DataFrame({'c': {0: '22:58:00', 1: '23:03:00', 2: '0', 3: '10'}, 'a': {0: '0', 1: '10', 2: '22:58:00', 3: '23:03:00'}, 'd': {0: '23:27:00', 1: '23:39:00', 2: '10', 3: '17'}, 'b': {0: '10', 1: '17', 2: '23:27:00', 3: '23:39:00'}})</code></p>\n\n<pre><code>          a         b         c         d\n0         0        10  22:58:00  23:27:00\n1        10        17  23:03:00  23:39:00\n2  22:58:00  23:27:00         0        10\n3  23:03:00  23:39:00        10        17\n</code></pre>\n\n<p>My current approach</p>\n\n<pre><code>cpy = df[['a', 'b']]\ndf.loc[2:, 'a'] = df['c']\ndf.loc[2:, 'b'] = df['d']\ndf.loc[2:, 'c'] = cpy['a']\ndf.loc[2:, 'd'] = cpy['b']\n</code></pre>\n\n<p>Expected output</p>\n\n<pre><code>    a   b         c         d\n0   0  10  22:58:00  23:27:00\n1  10  17  23:03:00  23:39:00\n2   0  10  22:58:00  23:27:00\n3  10  17  23:03:00  23:39:00\n</code></pre>\n\n<p>It works but this is only possible because it was 4 columns.\nIs there a better way to do this?</p>\n\n<p>Note the dtypes can cause issues with sorting\n<code>df.loc[0]['c']</code> is <code>datetime.time(22, 58)</code></p>\n\n<p>Maybe there is something like</p>\n\n<p><code>df.swap_row_col(index=[2:], columns_from=['a', 'b'], columns_to=['c', 'd'])</code></p>\n",
        "answer_body": "<p>For swapping and separating <code>datetime.time</code> and <code>string</code> in your sample, you may use <code>applymap</code>, <code>np.argsort</code> and numpy indexing (Note: your numbers in sample are in string format, so I check type <code>str</code>)</p>\n\n<pre><code>arr = np.argsort(df.applymap(type).ne(str), 1).to_numpy()\n\nOut[985]:\narray([[0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [2, 3, 0, 1],\n       [2, 3, 0, 1]], dtype=int32)\n\ndf_out = pd.DataFrame(df.to_numpy()[df.index[:,None], arr], columns=df.columns)\n\nOut[989]:\n    a   b         c         d\n0   0  10  22:58:00  23:27:00\n1  10  17  23:03:00  23:39:00\n2   0  10  22:58:00  23:27:00\n3  10  17  23:03:00  23:39:00\n</code></pre>\n\n<p>If you get <code>AttributeError: 'DataFrame' object has no attribute 'to_numpy'</code>\nreplace <code>to_numpy</code> with <code>values</code></p>\n",
        "question_body": "<p>Some rows were input in the wrong columns so now I need to swap them. </p>\n\n<p><code>df = pd.DataFrame({'c': {0: '22:58:00', 1: '23:03:00', 2: '0', 3: '10'}, 'a': {0: '0', 1: '10', 2: '22:58:00', 3: '23:03:00'}, 'd': {0: '23:27:00', 1: '23:39:00', 2: '10', 3: '17'}, 'b': {0: '10', 1: '17', 2: '23:27:00', 3: '23:39:00'}})</code></p>\n\n<pre><code>          a         b         c         d\n0         0        10  22:58:00  23:27:00\n1        10        17  23:03:00  23:39:00\n2  22:58:00  23:27:00         0        10\n3  23:03:00  23:39:00        10        17\n</code></pre>\n\n<p>My current approach</p>\n\n<pre><code>cpy = df[['a', 'b']]\ndf.loc[2:, 'a'] = df['c']\ndf.loc[2:, 'b'] = df['d']\ndf.loc[2:, 'c'] = cpy['a']\ndf.loc[2:, 'd'] = cpy['b']\n</code></pre>\n\n<p>Expected output</p>\n\n<pre><code>    a   b         c         d\n0   0  10  22:58:00  23:27:00\n1  10  17  23:03:00  23:39:00\n2   0  10  22:58:00  23:27:00\n3  10  17  23:03:00  23:39:00\n</code></pre>\n\n<p>It works but this is only possible because it was 4 columns.\nIs there a better way to do this?</p>\n\n<p>Note the dtypes can cause issues with sorting\n<code>df.loc[0]['c']</code> is <code>datetime.time(22, 58)</code></p>\n\n<p>Maybe there is something like</p>\n\n<p><code>df.swap_row_col(index=[2:], columns_from=['a', 'b'], columns_to=['c', 'd'])</code></p>\n",
        "formatted_input": {
            "qid": 59505475,
            "link": "https://stackoverflow.com/questions/59505475/pandas-swap-rows-between-columns",
            "question": {
                "title": "Pandas: Swap rows between columns",
                "ques_desc": "Some rows were input in the wrong columns so now I need to swap them. My current approach Expected output It works but this is only possible because it was 4 columns. Is there a better way to do this? Note the dtypes can cause issues with sorting is Maybe there is something like "
            },
            "io": [
                "          a         b         c         d\n0         0        10  22:58:00  23:27:00\n1        10        17  23:03:00  23:39:00\n2  22:58:00  23:27:00         0        10\n3  23:03:00  23:39:00        10        17\n",
                "    a   b         c         d\n0   0  10  22:58:00  23:27:00\n1  10  17  23:03:00  23:39:00\n2   0  10  22:58:00  23:27:00\n3  10  17  23:03:00  23:39:00\n"
            ],
            "answer": {
                "ans_desc": "For swapping and separating and in your sample, you may use , and numpy indexing (Note: your numbers in sample are in string format, so I check type ) If you get replace with ",
                "code": [
                    "AttributeError: 'DataFrame' object has no attribute 'to_numpy'"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 569,
            "user_id": 11591931,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-_bwex08hZkM/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rf3YCdJxr49lBvfUaNMzAjQFu_30g/mo/photo.jpg?sz=128",
            "display_name": "Alex Dana",
            "link": "https://stackoverflow.com/users/11591931/alex-dana"
        },
        "is_answered": true,
        "view_count": 471,
        "accepted_answer_id": 59502976,
        "answer_count": 4,
        "score": 14,
        "last_activity_date": 1577462639,
        "creation_date": 1577460962,
        "question_id": 59502905,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59502905/calculate-percentage-of-similar-values-in-pandas-dataframe",
        "title": "Calculate percentage of similar values in pandas dataframe",
        "body": "<p>I have one dataframe <code>df</code>, with two columns : Script (with text) and Speaker</p>\n\n<pre><code>Script  Speaker\naze     Speaker 1 \nart     Speaker 2\nghb     Speaker 3\njka     Speaker 1\ntyc     Speaker 1\navv     Speaker 2 \nbhj     Speaker 1\n</code></pre>\n\n<p>And I have the following list : <code>L = ['a','b','c']</code></p>\n\n<p>With the following code,</p>\n\n<pre><code>df = (df.set_index('Speaker')['Script'].str.findall('|'.join(L))\n        .str.join('|')\n        .str.get_dummies()\n        .sum(level=0))\nprint (df)\n</code></pre>\n\n<p>I obtain this dataframe <code>df2</code> :</p>\n\n<pre><code>Speaker     a    b    c\nSpeaker 1   2    1    1\nSpeaker 2   2    0    0\nSpeaker 3   0    1    0\n</code></pre>\n\n<p>Which line can I add in my code to obtain, for each line of my dataframe <code>df2</code>, a percentage value of all lines spoken by speaker, in order to have the following dataframe <code>df3</code> :</p>\n\n<pre><code>Speaker     a    b    c\nSpeaker 1   50%  25%   25%\nSpeaker 2  100%    0   0\nSpeaker 3   0   100%   0\n</code></pre>\n",
        "answer_body": "<p>You could divide by the <code>sum</code> along the first axis and then cast to string and add <code>%</code>:</p>\n\n<pre><code>out = (df.set_index('Speaker')['Script'].str.findall('|'.join(L))\n         .str.join('|')\n         .str.get_dummies()\n         .sum(level=0))\n</code></pre>\n\n<hr>\n\n<pre><code>(out/out.sum(0)[:,None]).mul(100).astype(int).astype(str).add('%')\n\n            a     b    c\nSpeaker                  \nSpeaker1   50%   25%  25%\nSpeaker2  100%    0%   0%\nSpeaker3    0%  100%   0%\n</code></pre>\n",
        "question_body": "<p>I have one dataframe <code>df</code>, with two columns : Script (with text) and Speaker</p>\n\n<pre><code>Script  Speaker\naze     Speaker 1 \nart     Speaker 2\nghb     Speaker 3\njka     Speaker 1\ntyc     Speaker 1\navv     Speaker 2 \nbhj     Speaker 1\n</code></pre>\n\n<p>And I have the following list : <code>L = ['a','b','c']</code></p>\n\n<p>With the following code,</p>\n\n<pre><code>df = (df.set_index('Speaker')['Script'].str.findall('|'.join(L))\n        .str.join('|')\n        .str.get_dummies()\n        .sum(level=0))\nprint (df)\n</code></pre>\n\n<p>I obtain this dataframe <code>df2</code> :</p>\n\n<pre><code>Speaker     a    b    c\nSpeaker 1   2    1    1\nSpeaker 2   2    0    0\nSpeaker 3   0    1    0\n</code></pre>\n\n<p>Which line can I add in my code to obtain, for each line of my dataframe <code>df2</code>, a percentage value of all lines spoken by speaker, in order to have the following dataframe <code>df3</code> :</p>\n\n<pre><code>Speaker     a    b    c\nSpeaker 1   50%  25%   25%\nSpeaker 2  100%    0   0\nSpeaker 3   0   100%   0\n</code></pre>\n",
        "formatted_input": {
            "qid": 59502905,
            "link": "https://stackoverflow.com/questions/59502905/calculate-percentage-of-similar-values-in-pandas-dataframe",
            "question": {
                "title": "Calculate percentage of similar values in pandas dataframe",
                "ques_desc": "I have one dataframe , with two columns : Script (with text) and Speaker And I have the following list : With the following code, I obtain this dataframe : Which line can I add in my code to obtain, for each line of my dataframe , a percentage value of all lines spoken by speaker, in order to have the following dataframe : "
            },
            "io": [
                "Speaker     a    b    c\nSpeaker 1   2    1    1\nSpeaker 2   2    0    0\nSpeaker 3   0    1    0\n",
                "Speaker     a    b    c\nSpeaker 1   50%  25%   25%\nSpeaker 2  100%    0   0\nSpeaker 3   0   100%   0\n"
            ],
            "answer": {
                "ans_desc": "You could divide by the along the first axis and then cast to string and add : ",
                "code": [
                    "out = (df.set_index('Speaker')['Script'].str.findall('|'.join(L))\n         .str.join('|')\n         .str.get_dummies()\n         .sum(level=0))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 704,
            "user_id": 5953619,
            "user_type": "registered",
            "accept_rate": 45,
            "profile_image": "https://www.gravatar.com/avatar/8c277b6a367dfa614b3d42efdb3edca1?s=128&d=identicon&r=PG&f=1",
            "display_name": "Michael",
            "link": "https://stackoverflow.com/users/5953619/michael"
        },
        "is_answered": true,
        "view_count": 151,
        "closed_date": 1577323559,
        "accepted_answer_id": 59482518,
        "answer_count": 5,
        "score": 4,
        "last_activity_date": 1577316606,
        "creation_date": 1577315676,
        "question_id": 59482469,
        "link": "https://stackoverflow.com/questions/59482469/python-convert-two-columns-of-dataframe-into-one-interposed-list",
        "closed_reason": "Duplicate",
        "title": "Python: Convert two columns of dataframe into one interposed list",
        "body": "<p>How can I convert two columns in a dataframe into an interposed list?</p>\n\n<p>ex: I want to do something like</p>\n\n<p><code>df</code></p>\n\n<pre><code>             Open  Close\nDate\n2016-12-23  1      2\n2016-12-27  3      4\n2016-12-28  5      6\n2016-12-29  0      -1\n</code></pre>\n\n<p><code>someFunction(df)</code></p>\n\n<p><code>&gt;&gt;&gt; [1, 2, 3, 4, 5, 6, 0, -1]</code></p>\n\n<p>Closest I've found is <code>list(zip(df.Open, df.Close)</code> but that returns a bunch of tuples in the list like this: <code>[(1, 2), (3, 4), (5, 6), (0, -1)]</code> </p>\n",
        "answer_body": "<p>Try this:</p>\n\n<pre><code>df.values.flatten()                                                                                                                                                                  \n# array([ 1,  2,  3,  4,  5,  6,  0, -1])\n</code></pre>\n",
        "question_body": "<p>How can I convert two columns in a dataframe into an interposed list?</p>\n\n<p>ex: I want to do something like</p>\n\n<p><code>df</code></p>\n\n<pre><code>             Open  Close\nDate\n2016-12-23  1      2\n2016-12-27  3      4\n2016-12-28  5      6\n2016-12-29  0      -1\n</code></pre>\n\n<p><code>someFunction(df)</code></p>\n\n<p><code>&gt;&gt;&gt; [1, 2, 3, 4, 5, 6, 0, -1]</code></p>\n\n<p>Closest I've found is <code>list(zip(df.Open, df.Close)</code> but that returns a bunch of tuples in the list like this: <code>[(1, 2), (3, 4), (5, 6), (0, -1)]</code> </p>\n",
        "formatted_input": {
            "qid": 59482469,
            "link": "https://stackoverflow.com/questions/59482469/python-convert-two-columns-of-dataframe-into-one-interposed-list",
            "question": {
                "title": "Python: Convert two columns of dataframe into one interposed list",
                "ques_desc": "How can I convert two columns in a dataframe into an interposed list? ex: I want to do something like Closest I've found is but that returns a bunch of tuples in the list like this: "
            },
            "io": [
                ">>> [1, 2, 3, 4, 5, 6, 0, -1]",
                "[(1, 2), (3, 4), (5, 6), (0, -1)]"
            ],
            "answer": {
                "ans_desc": "Try this: ",
                "code": [
                    "df.values.flatten()                                                                                                                                                                  \n# array([ 1,  2,  3,  4,  5,  6,  0, -1])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 106,
            "user_id": 3956691,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/bf3df77ae3aae0ede971c77be6ce4c61?s=128&d=identicon&r=PG&f=1",
            "display_name": "Jesus Rincon",
            "link": "https://stackoverflow.com/users/3956691/jesus-rincon"
        },
        "is_answered": true,
        "view_count": 89,
        "accepted_answer_id": 47872490,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1576966419,
        "creation_date": 1513613458,
        "last_edit_date": 1513613920,
        "question_id": 47872271,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/47872271/iterative-comparison-with-pandas",
        "title": "Iterative comparison with pandas",
        "body": "<p>I don't know to approach this issue. I have a data frame that looks like this</p>\n\n<pre><code>cuenta_bancaria nombre_empresa  perfil_cobranza  usuario_id  usuario_web \n5545              a              123              500199         5012\n5551              a              123              500199         3321\n5551              a               55              500199         5541\n5551              b               55              500199         5246\n</code></pre>\n\n<p>What I need to do is to iterate between each row per usuario_id and check if there's a difference between each row, and create a new data set with the row changed and the usuario_web in charge of this change, to generate a data frame that looks like this:</p>\n\n<pre><code>usuario_id     cambio           usuario_web\n 500199       cuenta_bancaria    3321\n 500199       perfil_cobranza    5541\n 500199       nombre_empresa     5246\n</code></pre>\n\n<p>Is there any way to do this? I'm working with pandas on python and this dataset could be a little big, let's say around 10000 rows, sorted by usuario_id.</p>\n\n<p>Thanks for any advice.</p>\n",
        "answer_body": "<p>Compare adjacent rows with <code>ne</code> + <code>shift</code>, obtain a mask, and use this to </p>\n\n<ul>\n<li>index into <code>df</code> to get the required rows</li>\n<li>index into <code>df.columns</code> to get the required columns which change</li>\n</ul>\n\n\n\n<pre><code>c = df.columns.intersection(\n        ['nombre_empresa', 'perfil_cobranza', 'cuenta_bancaria']\n)\n\ni = df[c].ne(df[c].shift())\nj = i.sum(1).eq(1)\n</code></pre>\n\n\n\n<pre><code>df = df.loc[j, ['usuario_id', 'usuario_web']]\ndf.insert(1, 'cambio', c[i[j].values.argmax(1)])\n\ndf\n\n   usuario_id           cambio  usuario_web\n1      500199  cuenta_bancaria         3321\n2      500199  perfil_cobranza         5541\n3      500199   nombre_empresa         5246\n</code></pre>\n",
        "question_body": "<p>I don't know to approach this issue. I have a data frame that looks like this</p>\n\n<pre><code>cuenta_bancaria nombre_empresa  perfil_cobranza  usuario_id  usuario_web \n5545              a              123              500199         5012\n5551              a              123              500199         3321\n5551              a               55              500199         5541\n5551              b               55              500199         5246\n</code></pre>\n\n<p>What I need to do is to iterate between each row per usuario_id and check if there's a difference between each row, and create a new data set with the row changed and the usuario_web in charge of this change, to generate a data frame that looks like this:</p>\n\n<pre><code>usuario_id     cambio           usuario_web\n 500199       cuenta_bancaria    3321\n 500199       perfil_cobranza    5541\n 500199       nombre_empresa     5246\n</code></pre>\n\n<p>Is there any way to do this? I'm working with pandas on python and this dataset could be a little big, let's say around 10000 rows, sorted by usuario_id.</p>\n\n<p>Thanks for any advice.</p>\n",
        "formatted_input": {
            "qid": 47872271,
            "link": "https://stackoverflow.com/questions/47872271/iterative-comparison-with-pandas",
            "question": {
                "title": "Iterative comparison with pandas",
                "ques_desc": "I don't know to approach this issue. I have a data frame that looks like this What I need to do is to iterate between each row per usuario_id and check if there's a difference between each row, and create a new data set with the row changed and the usuario_web in charge of this change, to generate a data frame that looks like this: Is there any way to do this? I'm working with pandas on python and this dataset could be a little big, let's say around 10000 rows, sorted by usuario_id. Thanks for any advice. "
            },
            "io": [
                "cuenta_bancaria nombre_empresa  perfil_cobranza  usuario_id  usuario_web \n5545              a              123              500199         5012\n5551              a              123              500199         3321\n5551              a               55              500199         5541\n5551              b               55              500199         5246\n",
                "usuario_id     cambio           usuario_web\n 500199       cuenta_bancaria    3321\n 500199       perfil_cobranza    5541\n 500199       nombre_empresa     5246\n"
            ],
            "answer": {
                "ans_desc": "Compare adjacent rows with + , obtain a mask, and use this to index into to get the required rows index into to get the required columns which change ",
                "code": [
                    "c = df.columns.intersection(\n        ['nombre_empresa', 'perfil_cobranza', 'cuenta_bancaria']\n)\n\ni = df[c].ne(df[c].shift())\nj = i.sum(1).eq(1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "reindex"
        ],
        "owner": {
            "reputation": 2787,
            "user_id": 776515,
            "user_type": "registered",
            "accept_rate": 85,
            "profile_image": "https://i.stack.imgur.com/IrXxz.png?s=128&g=1",
            "display_name": "Luis",
            "link": "https://stackoverflow.com/users/776515/luis"
        },
        "is_answered": true,
        "view_count": 5718,
        "accepted_answer_id": 56462151,
        "answer_count": 2,
        "score": 12,
        "last_activity_date": 1576714049,
        "creation_date": 1559743327,
        "last_edit_date": 1559743759,
        "question_id": 56462088,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56462088/why-doesnt-pandas-reindex-operate-in-place",
        "title": "Why doesn&#39;t pandas reindex() operate in-place?",
        "body": "<p>From the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html\" rel=\"noreferrer\">reindex docs</a>:</p>\n\n<blockquote>\n  <p>Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.</p>\n</blockquote>\n\n<p>Therefore, I thought that I would get a reordered <code>Dataframe</code> by setting <code>copy=False</code> <strong>in place (!)</strong>. It appears, however, that I do get a copy and need to assign it to the original object again. I don't want to assign it back, if I can avoid it (<a href=\"https://stackoverflow.com/questions/56459810/\">the reason comes from this other question</a>).</p>\n\n<p>This is what I am doing:</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(5, 5))\n\ndf.columns = [ 'a', 'b', 'c', 'd', 'e' ]\n\ndf.head()\n</code></pre>\n\n<p>Outs:</p>\n\n<pre><code>          a         b         c         d         e\n0  0.234296  0.011235  0.664617  0.983243  0.177639\n1  0.378308  0.659315  0.949093  0.872945  0.383024\n2  0.976728  0.419274  0.993282  0.668539  0.970228\n3  0.322936  0.555642  0.862659  0.134570  0.675897\n4  0.167638  0.578831  0.141339  0.232592  0.976057\n</code></pre>\n\n<p>Reindex gives me the correct output, but I'd need to assign it back to the original object, which is what I wanted to avoid by using <code>copy=False</code>:</p>\n\n<pre><code>df.reindex( columns=['e', 'd', 'c', 'b', 'a'], copy=False )\n</code></pre>\n\n<p>The desired output after that line is:</p>\n\n<pre><code>          e         d         c         b         a\n0  0.177639  0.983243  0.664617  0.011235  0.234296\n1  0.383024  0.872945  0.949093  0.659315  0.378308\n2  0.970228  0.668539  0.993282  0.419274  0.976728\n3  0.675897  0.134570  0.862659  0.555642  0.322936\n4  0.976057  0.232592  0.141339  0.578831  0.167638\n</code></pre>\n\n<hr>\n\n<p>Why is <code>copy=False</code> not working in place? </p>\n\n<p>Is it possible to do that at all?</p>\n\n<hr>\n\n<p>Working with python 3.5.3, pandas 0.23.3</p>\n",
        "answer_body": "<p><code>reindex</code> is a structural change, not a cosmetic or transformative one. As such, a copy is always returned because the operation cannot be done in-place (it would require allocating new memory for underlying arrays, etc). This means you <em>have</em> to assign the result back, there's no other choice.</p>\n\n<pre><code>df = df.reindex(['e', 'd', 'c', 'b', 'a'], axis=1)  \n</code></pre>\n\n<p>Also see the discussion on <a href=\"https://github.com/pandas-dev/pandas/issues/21598\" rel=\"noreferrer\">GH21598</a>.</p>\n\n<hr>\n\n<p>The one corner case where <code>copy=False</code> is actually of any use is when the indices used to reindex <code>df</code> are identical to the ones it already has. You can check by comparing the ids:</p>\n\n<pre><code>id(df)\n# 4839372504\n\nid(df.reindex(df.index, copy=False)) # same object returned \n# 4839372504\n\nid(df.reindex(df.index, copy=True))  # new object created - ids are different\n# 4839371608  \n</code></pre>\n",
        "question_body": "<p>From the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html\" rel=\"noreferrer\">reindex docs</a>:</p>\n\n<blockquote>\n  <p>Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.</p>\n</blockquote>\n\n<p>Therefore, I thought that I would get a reordered <code>Dataframe</code> by setting <code>copy=False</code> <strong>in place (!)</strong>. It appears, however, that I do get a copy and need to assign it to the original object again. I don't want to assign it back, if I can avoid it (<a href=\"https://stackoverflow.com/questions/56459810/\">the reason comes from this other question</a>).</p>\n\n<p>This is what I am doing:</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(5, 5))\n\ndf.columns = [ 'a', 'b', 'c', 'd', 'e' ]\n\ndf.head()\n</code></pre>\n\n<p>Outs:</p>\n\n<pre><code>          a         b         c         d         e\n0  0.234296  0.011235  0.664617  0.983243  0.177639\n1  0.378308  0.659315  0.949093  0.872945  0.383024\n2  0.976728  0.419274  0.993282  0.668539  0.970228\n3  0.322936  0.555642  0.862659  0.134570  0.675897\n4  0.167638  0.578831  0.141339  0.232592  0.976057\n</code></pre>\n\n<p>Reindex gives me the correct output, but I'd need to assign it back to the original object, which is what I wanted to avoid by using <code>copy=False</code>:</p>\n\n<pre><code>df.reindex( columns=['e', 'd', 'c', 'b', 'a'], copy=False )\n</code></pre>\n\n<p>The desired output after that line is:</p>\n\n<pre><code>          e         d         c         b         a\n0  0.177639  0.983243  0.664617  0.011235  0.234296\n1  0.383024  0.872945  0.949093  0.659315  0.378308\n2  0.970228  0.668539  0.993282  0.419274  0.976728\n3  0.675897  0.134570  0.862659  0.555642  0.322936\n4  0.976057  0.232592  0.141339  0.578831  0.167638\n</code></pre>\n\n<hr>\n\n<p>Why is <code>copy=False</code> not working in place? </p>\n\n<p>Is it possible to do that at all?</p>\n\n<hr>\n\n<p>Working with python 3.5.3, pandas 0.23.3</p>\n",
        "formatted_input": {
            "qid": 56462088,
            "link": "https://stackoverflow.com/questions/56462088/why-doesnt-pandas-reindex-operate-in-place",
            "question": {
                "title": "Why doesn&#39;t pandas reindex() operate in-place?",
                "ques_desc": "From the reindex docs: Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Therefore, I thought that I would get a reordered by setting in place (!). It appears, however, that I do get a copy and need to assign it to the original object again. I don't want to assign it back, if I can avoid it (the reason comes from this other question). This is what I am doing: Outs: Reindex gives me the correct output, but I'd need to assign it back to the original object, which is what I wanted to avoid by using : The desired output after that line is: Why is not working in place? Is it possible to do that at all? Working with python 3.5.3, pandas 0.23.3 "
            },
            "io": [
                "          a         b         c         d         e\n0  0.234296  0.011235  0.664617  0.983243  0.177639\n1  0.378308  0.659315  0.949093  0.872945  0.383024\n2  0.976728  0.419274  0.993282  0.668539  0.970228\n3  0.322936  0.555642  0.862659  0.134570  0.675897\n4  0.167638  0.578831  0.141339  0.232592  0.976057\n",
                "          e         d         c         b         a\n0  0.177639  0.983243  0.664617  0.011235  0.234296\n1  0.383024  0.872945  0.949093  0.659315  0.378308\n2  0.970228  0.668539  0.993282  0.419274  0.976728\n3  0.675897  0.134570  0.862659  0.555642  0.322936\n4  0.976057  0.232592  0.141339  0.578831  0.167638\n"
            ],
            "answer": {
                "ans_desc": " is a structural change, not a cosmetic or transformative one. As such, a copy is always returned because the operation cannot be done in-place (it would require allocating new memory for underlying arrays, etc). This means you have to assign the result back, there's no other choice. Also see the discussion on GH21598. The one corner case where is actually of any use is when the indices used to reindex are identical to the ones it already has. You can check by comparing the ids: ",
                "code": [
                    "df = df.reindex(['e', 'd', 'c', 'b', 'a'], axis=1)  \n",
                    "id(df)\n# 4839372504\n\nid(df.reindex(df.index, copy=False)) # same object returned \n# 4839372504\n\nid(df.reindex(df.index, copy=True))  # new object created - ids are different\n# 4839371608  \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 215,
            "user_id": 11238061,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7841c4c475c47b535a2ffed39eb95911?s=128&d=identicon&r=PG&f=1",
            "display_name": "Iceberg_Slim",
            "link": "https://stackoverflow.com/users/11238061/iceberg-slim"
        },
        "is_answered": true,
        "view_count": 150,
        "accepted_answer_id": 59372232,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1576590514,
        "creation_date": 1576574334,
        "last_edit_date": 1576576816,
        "question_id": 59371013,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59371013/pandas-how-to-return-the-rows-where-col-value-is-greater-than-x-in-rolling-wi",
        "title": "Pandas: How to return the rows where col value is greater than &#39;x&#39; in rolling window",
        "body": "<p>I have a large df and I am trying find all rows where the value in a specific column is above a given number but within a window of say 3 rows and returning only the rows with the highest value over the given number.</p>\n\n<pre><code>A    B    C    D    E\n1    5    9    10   15\n2    4    7    12   16\n3    3    5    10   18\n4    2    3    15   17\n5    1    1    10   14\n6    5    9    17   13\n7    4    7    10   14\n8    3    5    19   19\n9    2    3    10   18\n10   4    7    5   14\n11   3    5    6   19\n12   2    3    7   18\n</code></pre>\n\n<p>If I wanted to do this with the above example for column D, where the value must be above 11, the output would be.</p>\n\n<pre><code>A    B    C    D    E\n2    4    7    12   16\n6    5    9    17   13\n8    3    5    19   19.\n</code></pre>\n\n<p>What would be the best way to go about this? </p>\n\n<p>I've tried:\n <code>df.rolling(3,win_type=None, on='D')</code> \nbut can't find a way to include the greater than condition.</p>\n\n<p>Any help is appreciatted. Thanks!</p>\n",
        "answer_body": "<p><strong>Edited</strong>:\nTry this:</p>\n\n<pre><code>threshold = 11\nwindow = 3\ndf['r'] = np.floor(df.index / window)\nprint(df.groupby('r').apply(lambda x : (x.loc[x['D'] == x['D'].max() ,:]) if x['D'].max() &gt; threshold else None))\n</code></pre>\n\n<p>You can drop column 'r' after usage.</p>\n\n<p>Output:</p>\n\n<pre><code>      A  B  C   D   E    r\nr                          \n0.0 1  2  4  7  12  16  0.0\n1.0 5  6  5  9  17  13  1.0\n2.0 7  8  3  5  19  19  2.0\n</code></pre>\n",
        "question_body": "<p>I have a large df and I am trying find all rows where the value in a specific column is above a given number but within a window of say 3 rows and returning only the rows with the highest value over the given number.</p>\n\n<pre><code>A    B    C    D    E\n1    5    9    10   15\n2    4    7    12   16\n3    3    5    10   18\n4    2    3    15   17\n5    1    1    10   14\n6    5    9    17   13\n7    4    7    10   14\n8    3    5    19   19\n9    2    3    10   18\n10   4    7    5   14\n11   3    5    6   19\n12   2    3    7   18\n</code></pre>\n\n<p>If I wanted to do this with the above example for column D, where the value must be above 11, the output would be.</p>\n\n<pre><code>A    B    C    D    E\n2    4    7    12   16\n6    5    9    17   13\n8    3    5    19   19.\n</code></pre>\n\n<p>What would be the best way to go about this? </p>\n\n<p>I've tried:\n <code>df.rolling(3,win_type=None, on='D')</code> \nbut can't find a way to include the greater than condition.</p>\n\n<p>Any help is appreciatted. Thanks!</p>\n",
        "formatted_input": {
            "qid": 59371013,
            "link": "https://stackoverflow.com/questions/59371013/pandas-how-to-return-the-rows-where-col-value-is-greater-than-x-in-rolling-wi",
            "question": {
                "title": "Pandas: How to return the rows where col value is greater than &#39;x&#39; in rolling window",
                "ques_desc": "I have a large df and I am trying find all rows where the value in a specific column is above a given number but within a window of say 3 rows and returning only the rows with the highest value over the given number. If I wanted to do this with the above example for column D, where the value must be above 11, the output would be. What would be the best way to go about this? I've tried: but can't find a way to include the greater than condition. Any help is appreciatted. Thanks! "
            },
            "io": [
                "A    B    C    D    E\n1    5    9    10   15\n2    4    7    12   16\n3    3    5    10   18\n4    2    3    15   17\n5    1    1    10   14\n6    5    9    17   13\n7    4    7    10   14\n8    3    5    19   19\n9    2    3    10   18\n10   4    7    5   14\n11   3    5    6   19\n12   2    3    7   18\n",
                "A    B    C    D    E\n2    4    7    12   16\n6    5    9    17   13\n8    3    5    19   19.\n"
            ],
            "answer": {
                "ans_desc": "Edited: Try this: You can drop column 'r' after usage. Output: ",
                "code": [
                    "threshold = 11\nwindow = 3\ndf['r'] = np.floor(df.index / window)\nprint(df.groupby('r').apply(lambda x : (x.loc[x['D'] == x['D'].max() ,:]) if x['D'].max() > threshold else None))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 10834132,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1fb1dbed1b429ec153237654ccbe7733?s=128&d=identicon&r=PG&f=1",
            "display_name": "sam",
            "link": "https://stackoverflow.com/users/10834132/sam"
        },
        "is_answered": true,
        "view_count": 104,
        "accepted_answer_id": 59345439,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1576425203,
        "creation_date": 1576422594,
        "last_edit_date": 1576422639,
        "question_id": 59345178,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59345178/symmetrical-column-values-in-pandas-data-frame",
        "title": "Symmetrical column values in pandas data frame",
        "body": "<p>I have one set of variable as in below data frame:</p>\n\n<pre><code>    v1\n----------\n0   0.036286\n\n1  -0.018490\n\n2   0.011699\n\n3   0.028955\n\n4  -0.000373\n</code></pre>\n\n<p>Another set of variable in below data frame:</p>\n\n<pre><code>      v2\n----------\n41    12.31\n\n42    12.20\n\n43    12.12\n\n44    12.31\n\n45    12.47\n</code></pre>\n\n<p>1st columns are index columns. I want to add each row (v1+v2) to get v3. How do I make the index column values (0 to 4) and (41 to 45) symmetrical ( either 0-4) or (42-45) in both data fame?</p>\n\n<p>I am working on pandas (python) jupyter notebook.</p>\n",
        "answer_body": "<p>You have multiple options here I think:</p>\n\n<ul>\n<li>You can directly concatenate them using <code>pd.concat</code> using the <code>ignore_index=True</code> parameter and then create the new column:</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.concat(df1, df2, axis=1, ignore_index=True)\ndf['v3'] = df.sum(axis=1) # or df[['v1','v2']].sum(axis=1)\n</code></pre>\n\n<ul>\n<li>You can create a new vector/dataframe with the result:</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>v3 = df1['v1'].values + df2['v2'].values\n</code></pre>\n\n<p>which then you can manipulate as you prefer.</p>\n\n<ul>\n<li>All other solutions people already proposed.</li>\n</ul>\n",
        "question_body": "<p>I have one set of variable as in below data frame:</p>\n\n<pre><code>    v1\n----------\n0   0.036286\n\n1  -0.018490\n\n2   0.011699\n\n3   0.028955\n\n4  -0.000373\n</code></pre>\n\n<p>Another set of variable in below data frame:</p>\n\n<pre><code>      v2\n----------\n41    12.31\n\n42    12.20\n\n43    12.12\n\n44    12.31\n\n45    12.47\n</code></pre>\n\n<p>1st columns are index columns. I want to add each row (v1+v2) to get v3. How do I make the index column values (0 to 4) and (41 to 45) symmetrical ( either 0-4) or (42-45) in both data fame?</p>\n\n<p>I am working on pandas (python) jupyter notebook.</p>\n",
        "formatted_input": {
            "qid": 59345178,
            "link": "https://stackoverflow.com/questions/59345178/symmetrical-column-values-in-pandas-data-frame",
            "question": {
                "title": "Symmetrical column values in pandas data frame",
                "ques_desc": "I have one set of variable as in below data frame: Another set of variable in below data frame: 1st columns are index columns. I want to add each row (v1+v2) to get v3. How do I make the index column values (0 to 4) and (41 to 45) symmetrical ( either 0-4) or (42-45) in both data fame? I am working on pandas (python) jupyter notebook. "
            },
            "io": [
                "    v1\n----------\n0   0.036286\n\n1  -0.018490\n\n2   0.011699\n\n3   0.028955\n\n4  -0.000373\n",
                "      v2\n----------\n41    12.31\n\n42    12.20\n\n43    12.12\n\n44    12.31\n\n45    12.47\n"
            ],
            "answer": {
                "ans_desc": "You have multiple options here I think: You can directly concatenate them using using the parameter and then create the new column: You can create a new vector/dataframe with the result: which then you can manipulate as you prefer. All other solutions people already proposed. ",
                "code": [
                    "df = pd.concat(df1, df2, axis=1, ignore_index=True)\ndf['v3'] = df.sum(axis=1) # or df[['v1','v2']].sum(axis=1)\n",
                    "v3 = df1['v1'].values + df2['v2'].values\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "concat"
        ],
        "owner": {
            "reputation": 887,
            "user_id": 5239480,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/Fupht.png?s=128&g=1",
            "display_name": "CtnDev",
            "link": "https://stackoverflow.com/users/5239480/ctndev"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 59284544,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1576063874,
        "creation_date": 1576061187,
        "last_edit_date": 1576062024,
        "question_id": 59284049,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59284049/how-to-concat-2-columns-in-single-column-with-column-value-check",
        "title": "How to Concat 2 columns in single column with column value check",
        "body": "<p>I want to concat two column from data frame where Column1 not equals to ANY:</p>\n\n<p>DataFrame :</p>\n\n<pre><code>   COLUMN1 | COLUMN2\n0     A    |   FOO\n1     B    |   BAR  \n2    ANY   |   FOO\n3    ANY   |   BAR\n4     C    |   FOO\n</code></pre>\n\n<p>as a result I want dataframe as follows</p>\n\n<pre><code>   COLUMN1 | COLUMN2\n0     A    |  FOO_A\n1     B    |  BAR_B\n2    ANY   |  FOO\n3    ANY   |  BAR  \n4     C    |  FOO_C\n</code></pre>\n\n<p>ANY is variable, could represent Null, EmptyString, String, Number. </p>\n\n<p>Thanks.</p>\n",
        "answer_body": "<p>You can do </p>\n\n<pre><code>df['COLUMN2']=df.apply(lambda row:row['COLUMN2']+'_'+row['COLUMN1'] if row['COLUMN1']!='ANY' else row['COLUMN2'],axis=1)\n</code></pre>\n",
        "question_body": "<p>I want to concat two column from data frame where Column1 not equals to ANY:</p>\n\n<p>DataFrame :</p>\n\n<pre><code>   COLUMN1 | COLUMN2\n0     A    |   FOO\n1     B    |   BAR  \n2    ANY   |   FOO\n3    ANY   |   BAR\n4     C    |   FOO\n</code></pre>\n\n<p>as a result I want dataframe as follows</p>\n\n<pre><code>   COLUMN1 | COLUMN2\n0     A    |  FOO_A\n1     B    |  BAR_B\n2    ANY   |  FOO\n3    ANY   |  BAR  \n4     C    |  FOO_C\n</code></pre>\n\n<p>ANY is variable, could represent Null, EmptyString, String, Number. </p>\n\n<p>Thanks.</p>\n",
        "formatted_input": {
            "qid": 59284049,
            "link": "https://stackoverflow.com/questions/59284049/how-to-concat-2-columns-in-single-column-with-column-value-check",
            "question": {
                "title": "How to Concat 2 columns in single column with column value check",
                "ques_desc": "I want to concat two column from data frame where Column1 not equals to ANY: DataFrame : as a result I want dataframe as follows ANY is variable, could represent Null, EmptyString, String, Number. Thanks. "
            },
            "io": [
                "   COLUMN1 | COLUMN2\n0     A    |   FOO\n1     B    |   BAR  \n2    ANY   |   FOO\n3    ANY   |   BAR\n4     C    |   FOO\n",
                "   COLUMN1 | COLUMN2\n0     A    |  FOO_A\n1     B    |  BAR_B\n2    ANY   |  FOO\n3    ANY   |  BAR  \n4     C    |  FOO_C\n"
            ],
            "answer": {
                "ans_desc": "You can do ",
                "code": [
                    "df['COLUMN2']=df.apply(lambda row:row['COLUMN2']+'_'+row['COLUMN1'] if row['COLUMN1']!='ANY' else row['COLUMN2'],axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "jupyter",
            "vaex"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 12346577,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/fe20e1e00210ece26a6924c191bb277c?s=128&d=identicon&r=PG&f=1",
            "display_name": "dalayr",
            "link": "https://stackoverflow.com/users/12346577/dalayr"
        },
        "is_answered": true,
        "view_count": 217,
        "accepted_answer_id": 59233293,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1576055961,
        "creation_date": 1575789422,
        "last_edit_date": 1575795744,
        "question_id": 59233282,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59233282/jupyter-pandas-dropping-items-which-have-average-over-a-threshold",
        "title": "Jupyter Pandas - dropping items which have average over a threshold",
        "body": "<p>I have a data frame with items and their prices, something like this:\n<code>\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Item \u2551 Day \u2551 Price \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 A    \u2551   1 \u2551    10 \u2551\n\u2551 B    \u2551   1 \u2551    20 \u2551\n\u2551 C    \u2551   1 \u2551    30 \u2551\n\u2551 D    \u2551   1 \u2551    40 \u2551\n\u2551 A    \u2551   2 \u2551   100 \u2551\n\u2551 B    \u2551   2 \u2551    20 \u2551\n\u2551 C    \u2551   2 \u2551    30 \u2551\n\u2551 D    \u2551   2 \u2551    40 \u2551\n\u2551 A    \u2551   3 \u2551   500 \u2551\n\u2551 B    \u2551   3 \u2551    25 \u2551\n\u2551 C    \u2551   3 \u2551    35 \u2551\n\u2551 D    \u2551   3 \u2551  1000 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d</code></p>\n\n<p>I want to exclude all rows from this df where the item has an average price over 200. So filtered df should look like this:\n<code>\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Item \u2551 Day \u2551 Price \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 B    \u2551   1 \u2551    20 \u2551\n\u2551 C    \u2551   1 \u2551    30 \u2551\n\u2551 B    \u2551   2 \u2551    20 \u2551\n\u2551 C    \u2551   2 \u2551    30 \u2551\n\u2551 B    \u2551   3 \u2551    25 \u2551\n\u2551 C    \u2551   3 \u2551    35 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d</code></p>\n\n<p>I'm new to python and pandas but as a first step was thinking something like this to get a new df for avg prices: avg_prices_df = df.groupby('ItemID').Price.mean().reset_index and then not sure how to proceed from there. Not sure even that first step is correct.</p>\n\n<p>To further complicate the matter, I am using vaex to read the data in ndf5 form as I have over 400 million rows.</p>\n\n<p>Many thanks in advance for any advice.</p>\n\n<p>EDIT: So I got the following code working, though I am sure it is not optimised..</p>\n\n<p>`</p>\n\n<h1>create dataframe of ItemIDs and their average prices</h1>\n\n<p>df_item_avg_price = df.groupby(df.ItemID, agg=[vaex.agg.count('ItemID'), vaex.agg.mean('Price')])</p>\n\n<h1>filter this new dataframe by average price threshold</h1>\n\n<p>df_item_avg_price = (df_item_avg_price[df_item_avg_price[\"P_r_i_c_e_mean\"] &lt;= 50000000])</p>\n\n<h1>create list of ItemIDs which have average price under the threshold</h1>\n\n<p>items_in_price_range = df_item_avg_price['ItemID'].tolist()</p>\n\n<h1>filter the original dataframe to include rows only with the items in price range</h1>\n\n<p>filtered_df = df[df.ItemID.isin(items_in_price_range)]\n`\nAny better way to do this?</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> for <code>mean</code>s per groups with same size like original, so possible filter out by <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a> all groups with means less like <code>200</code>:</p>\n\n<pre><code>avg_prices_df = df[df.groupby('Item')['Price'].transform('mean') &lt; 200]\n</code></pre>\n\n<p>Another solution with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html\" rel=\"nofollow noreferrer\"><code>DataFrameGroupBy.filter</code></a>:</p>\n\n<pre><code>avg_prices_df = df.groupby('Item').filter(lambda x: x['Price'].mean() &lt; 200)\n</code></pre>\n\n<hr>\n\n<pre><code>print (avg_prices_df)\n   Item  Day  Price\n1     B    1     20\n2     C    1     30\n5     B    2     20\n6     C    2     30\n9     B    3     25\n10    C    3     35\n\nprint (df.groupby('Item')['Price'].transform('mean'))\n0     203.333333\n1      21.666667\n2      31.666667\n3     360.000000\n4     203.333333\n5      21.666667\n6      31.666667\n7     360.000000\n8     203.333333\n9      21.666667\n10     31.666667\n11    360.000000\nName: Price, dtype: float64\n</code></pre>\n\n<p>Solution for vaex:</p>\n\n<pre><code>df_item_avg_price = df.groupby(df.ItemID).agg({'Price' : 'mean'})\ndf_item_avg_price = (df_item_avg_price[df_item_avg_price[\"Price\"] &lt;= 200])\n\ndf = df_item_avg_price.drop(['Price']).join(df, on='ItemID')\nprint (df)\n  ItemID  Day  Price\n0      B    1     20\n1      B    2     20\n2      B    3     25\n3      C    1     30\n4      C    2     30\n5      C    3     35\n</code></pre>\n",
        "question_body": "<p>I have a data frame with items and their prices, something like this:\n<code>\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Item \u2551 Day \u2551 Price \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 A    \u2551   1 \u2551    10 \u2551\n\u2551 B    \u2551   1 \u2551    20 \u2551\n\u2551 C    \u2551   1 \u2551    30 \u2551\n\u2551 D    \u2551   1 \u2551    40 \u2551\n\u2551 A    \u2551   2 \u2551   100 \u2551\n\u2551 B    \u2551   2 \u2551    20 \u2551\n\u2551 C    \u2551   2 \u2551    30 \u2551\n\u2551 D    \u2551   2 \u2551    40 \u2551\n\u2551 A    \u2551   3 \u2551   500 \u2551\n\u2551 B    \u2551   3 \u2551    25 \u2551\n\u2551 C    \u2551   3 \u2551    35 \u2551\n\u2551 D    \u2551   3 \u2551  1000 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d</code></p>\n\n<p>I want to exclude all rows from this df where the item has an average price over 200. So filtered df should look like this:\n<code>\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Item \u2551 Day \u2551 Price \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 B    \u2551   1 \u2551    20 \u2551\n\u2551 C    \u2551   1 \u2551    30 \u2551\n\u2551 B    \u2551   2 \u2551    20 \u2551\n\u2551 C    \u2551   2 \u2551    30 \u2551\n\u2551 B    \u2551   3 \u2551    25 \u2551\n\u2551 C    \u2551   3 \u2551    35 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d</code></p>\n\n<p>I'm new to python and pandas but as a first step was thinking something like this to get a new df for avg prices: avg_prices_df = df.groupby('ItemID').Price.mean().reset_index and then not sure how to proceed from there. Not sure even that first step is correct.</p>\n\n<p>To further complicate the matter, I am using vaex to read the data in ndf5 form as I have over 400 million rows.</p>\n\n<p>Many thanks in advance for any advice.</p>\n\n<p>EDIT: So I got the following code working, though I am sure it is not optimised..</p>\n\n<p>`</p>\n\n<h1>create dataframe of ItemIDs and their average prices</h1>\n\n<p>df_item_avg_price = df.groupby(df.ItemID, agg=[vaex.agg.count('ItemID'), vaex.agg.mean('Price')])</p>\n\n<h1>filter this new dataframe by average price threshold</h1>\n\n<p>df_item_avg_price = (df_item_avg_price[df_item_avg_price[\"P_r_i_c_e_mean\"] &lt;= 50000000])</p>\n\n<h1>create list of ItemIDs which have average price under the threshold</h1>\n\n<p>items_in_price_range = df_item_avg_price['ItemID'].tolist()</p>\n\n<h1>filter the original dataframe to include rows only with the items in price range</h1>\n\n<p>filtered_df = df[df.ItemID.isin(items_in_price_range)]\n`\nAny better way to do this?</p>\n",
        "formatted_input": {
            "qid": 59233282,
            "link": "https://stackoverflow.com/questions/59233282/jupyter-pandas-dropping-items-which-have-average-over-a-threshold",
            "question": {
                "title": "Jupyter Pandas - dropping items which have average over a threshold",
                "ques_desc": "I have a data frame with items and their prices, something like this: I want to exclude all rows from this df where the item has an average price over 200. So filtered df should look like this: I'm new to python and pandas but as a first step was thinking something like this to get a new df for avg prices: avg_prices_df = df.groupby('ItemID').Price.mean().reset_index and then not sure how to proceed from there. Not sure even that first step is correct. To further complicate the matter, I am using vaex to read the data in ndf5 form as I have over 400 million rows. Many thanks in advance for any advice. EDIT: So I got the following code working, though I am sure it is not optimised.. ` create dataframe of ItemIDs and their average prices df_item_avg_price = df.groupby(df.ItemID, agg=[vaex.agg.count('ItemID'), vaex.agg.mean('Price')]) filter this new dataframe by average price threshold df_item_avg_price = (df_item_avg_price[df_item_avg_price[\"P_r_i_c_e_mean\"] <= 50000000]) create list of ItemIDs which have average price under the threshold items_in_price_range = df_item_avg_price['ItemID'].tolist() filter the original dataframe to include rows only with the items in price range filtered_df = df[df.ItemID.isin(items_in_price_range)] ` Any better way to do this? "
            },
            "io": [
                "\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Item \u2551 Day \u2551 Price \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 A    \u2551   1 \u2551    10 \u2551\n\u2551 B    \u2551   1 \u2551    20 \u2551\n\u2551 C    \u2551   1 \u2551    30 \u2551\n\u2551 D    \u2551   1 \u2551    40 \u2551\n\u2551 A    \u2551   2 \u2551   100 \u2551\n\u2551 B    \u2551   2 \u2551    20 \u2551\n\u2551 C    \u2551   2 \u2551    30 \u2551\n\u2551 D    \u2551   2 \u2551    40 \u2551\n\u2551 A    \u2551   3 \u2551   500 \u2551\n\u2551 B    \u2551   3 \u2551    25 \u2551\n\u2551 C    \u2551   3 \u2551    35 \u2551\n\u2551 D    \u2551   3 \u2551  1000 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d",
                "\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Item \u2551 Day \u2551 Price \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 B    \u2551   1 \u2551    20 \u2551\n\u2551 C    \u2551   1 \u2551    30 \u2551\n\u2551 B    \u2551   2 \u2551    20 \u2551\n\u2551 C    \u2551   2 \u2551    30 \u2551\n\u2551 B    \u2551   3 \u2551    25 \u2551\n\u2551 C    \u2551   3 \u2551    35 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d"
            ],
            "answer": {
                "ans_desc": "Use for s per groups with same size like original, so possible filter out by all groups with means less like : Another solution with : Solution for vaex: ",
                "code": [
                    "avg_prices_df = df[df.groupby('Item')['Price'].transform('mean') < 200]\n",
                    "avg_prices_df = df.groupby('Item').filter(lambda x: x['Price'].mean() < 200)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "group-by"
        ],
        "owner": {
            "reputation": 6802,
            "user_id": 11232091,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vt9vH.png?s=128&g=1",
            "display_name": "moys",
            "link": "https://stackoverflow.com/users/11232091/moys"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 59281944,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1576054718,
        "creation_date": 1576053252,
        "question_id": 59281644,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59281644/groupby-sum-from-occurance-of-a-particular-value-till-the-occurance-of-another",
        "title": "Groupby &amp; Sum from occurance of a particular value till the occurance of another particular value or the same value",
        "body": "<p>I have a dataframe as below.\nI want to <code>groupby</code> 'user' &amp; 'eve' and <code>sum</code> 'Ses' till 100/200 &amp; from 100 to 200.\nAlso, return the value of column 'Name' where 100/200 occurs.</p>\n\n<p>If after an hundred, there is no 100 or 200 (like last row in group a &amp; 123 or a &amp; 456), ignore it.</p>\n\n<pre><code>User    eve Ses ID  Name\na   123 1   10  a\na   123 2   11  a\na   123 3   12  a\na   123 4   13  a\na   123 3   100 xyz\na   123 6   10  a\na   456 1   11  a\na   456 2   12  a\na   456 3   13  a\na   456 4   40  a\na   456 1   100 mno\na   456 14  10  a\na   456 7   20  a\na   456 8   30  a\na   456 12  200 pqr\na   456 10  10  a\nb   123 1   20  a\nb   123 2   30  a\nb   123 3   40  a\nb   123 4   50  a\nb   123 1   70  a\nb   123 6   100 abc\nb   888 1   20  a\nb   888 1   200 jkl\nb   888 3   10  a\nb   888 4   20  a\nb   888 5   30  a\nb   888 1   100 rrr\nb   888 7   50  a\nb   888 8   70  a\n</code></pre>\n\n<p>The expected output for the above input df is a df below.</p>\n\n<pre><code>User    eve Ses Name\na   123 13  xyz\na   456 11  mno\na   456 41  pqr\nb   123 17  abc\nb   888 2   jkl\nb   888 13  rrr\n</code></pre>\n",
        "answer_body": "<p>This is my approach:</p>\n\n<pre><code># valid IDs\ndf['valids'] = df['ID'].isin([100,200])\n\n# mask the trailing non-hundred ids\nheads = (df['ID'].where(df['valids'])\n             .groupby([df['User'],df['eve']])\n             .bfill().notnull()\n        )\ndf = df[heads]\n\n# groupby and output:\n(df.groupby(['User','eve', df['valids'].shift(fill_value=0).cumsum()],\n           as_index=False)\n   .agg({'Ses':'sum', 'Name':'last'})\n)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>  User  eve  Ses Name\n0    a  123   13  xyz\n1    a  456   11  mno\n2    a  456   41  pqr\n3    b  123   17  abc\n4    b  888    2  jkl\n5    b  888   13  rrr\n</code></pre>\n",
        "question_body": "<p>I have a dataframe as below.\nI want to <code>groupby</code> 'user' &amp; 'eve' and <code>sum</code> 'Ses' till 100/200 &amp; from 100 to 200.\nAlso, return the value of column 'Name' where 100/200 occurs.</p>\n\n<p>If after an hundred, there is no 100 or 200 (like last row in group a &amp; 123 or a &amp; 456), ignore it.</p>\n\n<pre><code>User    eve Ses ID  Name\na   123 1   10  a\na   123 2   11  a\na   123 3   12  a\na   123 4   13  a\na   123 3   100 xyz\na   123 6   10  a\na   456 1   11  a\na   456 2   12  a\na   456 3   13  a\na   456 4   40  a\na   456 1   100 mno\na   456 14  10  a\na   456 7   20  a\na   456 8   30  a\na   456 12  200 pqr\na   456 10  10  a\nb   123 1   20  a\nb   123 2   30  a\nb   123 3   40  a\nb   123 4   50  a\nb   123 1   70  a\nb   123 6   100 abc\nb   888 1   20  a\nb   888 1   200 jkl\nb   888 3   10  a\nb   888 4   20  a\nb   888 5   30  a\nb   888 1   100 rrr\nb   888 7   50  a\nb   888 8   70  a\n</code></pre>\n\n<p>The expected output for the above input df is a df below.</p>\n\n<pre><code>User    eve Ses Name\na   123 13  xyz\na   456 11  mno\na   456 41  pqr\nb   123 17  abc\nb   888 2   jkl\nb   888 13  rrr\n</code></pre>\n",
        "formatted_input": {
            "qid": 59281644,
            "link": "https://stackoverflow.com/questions/59281644/groupby-sum-from-occurance-of-a-particular-value-till-the-occurance-of-another",
            "question": {
                "title": "Groupby &amp; Sum from occurance of a particular value till the occurance of another particular value or the same value",
                "ques_desc": "I have a dataframe as below. I want to 'user' & 'eve' and 'Ses' till 100/200 & from 100 to 200. Also, return the value of column 'Name' where 100/200 occurs. If after an hundred, there is no 100 or 200 (like last row in group a & 123 or a & 456), ignore it. The expected output for the above input df is a df below. "
            },
            "io": [
                "User    eve Ses ID  Name\na   123 1   10  a\na   123 2   11  a\na   123 3   12  a\na   123 4   13  a\na   123 3   100 xyz\na   123 6   10  a\na   456 1   11  a\na   456 2   12  a\na   456 3   13  a\na   456 4   40  a\na   456 1   100 mno\na   456 14  10  a\na   456 7   20  a\na   456 8   30  a\na   456 12  200 pqr\na   456 10  10  a\nb   123 1   20  a\nb   123 2   30  a\nb   123 3   40  a\nb   123 4   50  a\nb   123 1   70  a\nb   123 6   100 abc\nb   888 1   20  a\nb   888 1   200 jkl\nb   888 3   10  a\nb   888 4   20  a\nb   888 5   30  a\nb   888 1   100 rrr\nb   888 7   50  a\nb   888 8   70  a\n",
                "User    eve Ses Name\na   123 13  xyz\na   456 11  mno\na   456 41  pqr\nb   123 17  abc\nb   888 2   jkl\nb   888 13  rrr\n"
            ],
            "answer": {
                "ans_desc": "This is my approach: Output: ",
                "code": [
                    "# valid IDs\ndf['valids'] = df['ID'].isin([100,200])\n\n# mask the trailing non-hundred ids\nheads = (df['ID'].where(df['valids'])\n             .groupby([df['User'],df['eve']])\n             .bfill().notnull()\n        )\ndf = df[heads]\n\n# groupby and output:\n(df.groupby(['User','eve', df['valids'].shift(fill_value=0).cumsum()],\n           as_index=False)\n   .agg({'Ses':'sum', 'Name':'last'})\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "apply"
        ],
        "owner": {
            "reputation": 97,
            "user_id": 12457673,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3248b1ac1b17bf9d7e70ea52c0044d11?s=128&d=identicon&r=PG&f=1",
            "display_name": "Lucas",
            "link": "https://stackoverflow.com/users/12457673/lucas"
        },
        "is_answered": true,
        "view_count": 166,
        "accepted_answer_id": 59255556,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1576000469,
        "creation_date": 1575914126,
        "last_edit_date": 1575923576,
        "question_id": 59254205,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59254205/how-to-append-a-list-in-pandas",
        "title": "How to append a list in Pandas?",
        "body": "<p>I'm reading a dataframe and trying to insert a list inside another list and then converting it to json file. I'm using python 3 and 0.25.3 version of pandas for it. </p>\n\n<p>============================</p>\n\n<p><strong>Data that I'm reading:</strong></p>\n\n<pre><code>id     label        id_customer     label_customer    part_number   number_client\n\n6     Sao Paulo      CUST-99992         Brazil          7897           982\n\n6     Sao Paulo      CUST-99992         Brazil          888            12\n\n92    Hong Kong      CUST-88888         China           147            288\n</code></pre>\n\n<p>============================</p>\n\n<p><strong>Here is my code:</strong></p>\n\n<pre><code>import pandas as pd \nimport json\n\ndata = pd.read_excel(path)\n\ndata[\"part_number\"] = data[\"part_number\"].apply(lambda x: str(x))\ndata[\"number_client\"]  = data[\"number_client\"].apply(lambda x: str(x))\n\ndata = data.groupby([\"id\", \"label\", \"id_customer\", \"label_customer\"], as_index=False).agg(\"#\".join)\n\ndata[\"part_number\"] = data[\"part_number\"].apply(lambda x: {\"part\": x})\ndata[\"number_client\"] = data[\"number_client\"].apply(lambda x: {\"client\": x})\n\ndata[\"id_customer\"] = data[\"id_customer\"].apply(lambda x: {\"id\": x})\ndata[\"label_customer\"] = data[\"label_customer\"].apply(lambda x: {\"label\": x})\n\ndata[\"Customer\"] = data.apply(lambda x: [{**x[\"id_customer\"], **x[\"label_customer\"]}],axis=1)\ndata[\"number\"] = data.apply(lambda x: [{**x[\"part_number\"], **x[\"number_client\"]}], axis=1)\n\ndata = data[[\"id\", \"label\", \"Customer\",\"number\"]]\n\ndata.to_json(path)\n</code></pre>\n\n<p>=============================</p>\n\n<p><strong>What is expected:</strong></p>\n\n<pre><code>[{\n    \"id\": 6,\n    \"label\": \"Sao Paulo\",\n    \"Customer\": [{\n        \"id\": \"CUS-99992\",\n        \"label\": \"Brazil\",\n        \"number\": [{\n        \"part\": \"7897\",\n        \"client\": \"892\"\n    },\n    {\n       \"part\": \"888\",\n       \"client\": \"12\"\n    }]\n    }]  \n}, \n{\n    \"id\": 92,\n    \"label\": \"Hong Kong\",\n    \"Customer\": [{\n        \"id\": \"CUS-88888\",\n        \"label\": \"China\",\n        \"number\": [{\n        \"part\": \"147\",\n        \"client\": \"288\"\n    }]\n    }] \n}]\n</code></pre>\n\n<p>============================</p>\n\n<p><strong>What I'm getting:</strong></p>\n\n<pre><code>[{\n    \"id\": 6,\n    \"label\": \"Sao Paulo\",\n    \"Customer\": [{\n        \"id\": \"CUS-99992\",\n        \"label\": \"Brazil\"\n    }],\n    \"number\": [{\n        \"part\": \"7897\",\n        \"client\": \"892\"\n    }],\n    \"number\": [{\n        \"part\": \"888\",\n        \"client\": \"12\"\n    }]\n}, {\n    \"id\": 92,\n    \"label\": \"Hong Kong\",\n    \"Customer\": [{\n        \"id\": \"CUS-88888\",\n        \"label\": \"China\"\n    }],\n    \"number\": [{\n        \"part\": \"147\",\n        \"client\": \"288\"\n    }]\n}]\n</code></pre>\n\n<p>======================</p>\n\n<p>I tried to do the same thing using <code>iterrows</code> function (and posted a question here 'Dataframe and conversion to JSON using Pandas'), but some people recommend me to try another way using another function. I know that is a stupid thing add <code>number</code> object inside my <code>data</code>, but I already tried of others way. </p>\n\n<p>Could you help me?</p>\n",
        "answer_body": "<p>Define the following reformatting function:</p>\n\n<pre><code>def reformat(row):\n    d1 = { 'part': str(row.part_number), 'client': str(row.number_client)}\n    d2 = { 'id': row.id_customer, 'label': row.label_customer, 'number': [d1] }\n    return { 'id': row.id, 'label': row.label, 'Customer': [d2] }\n</code></pre>\n\n<p>Then apply it the following way:</p>\n\n<pre><code>df.apply(reformat, axis=1).to_json('result.json', orient='records')\n</code></pre>\n\n<p>The result (reformatted for readability) is:</p>\n\n<pre><code>[ { \"id\":6,\n    \"label\":\"Sao Paulo\",\n    \"Customer\":[\n      { \"id\":\"CUST-99992\",\n        \"label\":\"Brazil\",\n        \"number\":[{\"part\":\"7897\",\"client\":\"982\"}]\n      }\n    ]\n  },\n  { \"id\":92,\n    \"label\":\"Hong Kong\",\n    \"Customer\":[\n      { \"id\":\"CUST-88888\",\n        \"label\":\"China\",\n        \"number\":[{\"part\":\"147\",\"client\":\"288\"}]\n      }\n    ]\n  }\n]\n</code></pre>\n\n<h1>Edit following the comment</h1>\n\n<p>To cope with the variant of <strong>multiple</strong> rows for a <strong>single</strong> <em>label</em> /\n<em>label_customer</em>, take another approach:</p>\n\n<p>Start from defining the following functions:</p>\n\n<ol>\n<li><p>Get the content of <em>number</em> attribute:</p>\n\n<pre><code>def getNum(grp):\n    return eval(grp[['part', 'client']].to_json(orient='records'))\n</code></pre>\n\n<p>Note <em>eval</em> in this function. Otherwise the result would be a string\n(instead of list of dictionaries).</p></li>\n<li><p>Get the content of <em>Customer</em> attribute:</p>\n\n<pre><code>def getCust(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id_customer, 'label': r0.label_customer, 'number': getNum(grp) }\n</code></pre></li>\n<li><p>Get the content of the whole JSON element for the current group:</p>\n\n<pre><code>def getGrp(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id, 'label': r0.label, 'Customer': getCust(grp) }\n</code></pre></li>\n</ol>\n\n<p>Then convert column types to <em>string</em>:</p>\n\n<pre><code>df.part_number = df.part_number.astype('str')\ndf.number_client = df.number_client.astype('str')\n</code></pre>\n\n<p>And to get the final result, run:</p>\n\n<pre><code>df.rename(columns={'part_number': 'part', 'number_client': 'client'})\\\n    .groupby(['id', 'label', 'id_customer', 'label_customer'])\\\n    .apply(getGrp).to_json(orient='values')\n</code></pre>\n\n<p>The above code:</p>\n\n<ul>\n<li>Renames <em>part_number</em> and <em>number_client</em> to <em>part</em> and <em>client</em>,\nrespectively. This change is needed to generate proper element\nnames by <em>getNum</em>.</li>\n<li>Groups the DataFrame (as in your code).</li>\n<li>Applies <em>getGrp</em> function to each group. The result is a <em>Series</em> of\nJSON elements.</li>\n<li>And finally <em>to_json</em> converts this <em>Series</em> to a list of JSON elements.</li>\n</ul>\n",
        "question_body": "<p>I'm reading a dataframe and trying to insert a list inside another list and then converting it to json file. I'm using python 3 and 0.25.3 version of pandas for it. </p>\n\n<p>============================</p>\n\n<p><strong>Data that I'm reading:</strong></p>\n\n<pre><code>id     label        id_customer     label_customer    part_number   number_client\n\n6     Sao Paulo      CUST-99992         Brazil          7897           982\n\n6     Sao Paulo      CUST-99992         Brazil          888            12\n\n92    Hong Kong      CUST-88888         China           147            288\n</code></pre>\n\n<p>============================</p>\n\n<p><strong>Here is my code:</strong></p>\n\n<pre><code>import pandas as pd \nimport json\n\ndata = pd.read_excel(path)\n\ndata[\"part_number\"] = data[\"part_number\"].apply(lambda x: str(x))\ndata[\"number_client\"]  = data[\"number_client\"].apply(lambda x: str(x))\n\ndata = data.groupby([\"id\", \"label\", \"id_customer\", \"label_customer\"], as_index=False).agg(\"#\".join)\n\ndata[\"part_number\"] = data[\"part_number\"].apply(lambda x: {\"part\": x})\ndata[\"number_client\"] = data[\"number_client\"].apply(lambda x: {\"client\": x})\n\ndata[\"id_customer\"] = data[\"id_customer\"].apply(lambda x: {\"id\": x})\ndata[\"label_customer\"] = data[\"label_customer\"].apply(lambda x: {\"label\": x})\n\ndata[\"Customer\"] = data.apply(lambda x: [{**x[\"id_customer\"], **x[\"label_customer\"]}],axis=1)\ndata[\"number\"] = data.apply(lambda x: [{**x[\"part_number\"], **x[\"number_client\"]}], axis=1)\n\ndata = data[[\"id\", \"label\", \"Customer\",\"number\"]]\n\ndata.to_json(path)\n</code></pre>\n\n<p>=============================</p>\n\n<p><strong>What is expected:</strong></p>\n\n<pre><code>[{\n    \"id\": 6,\n    \"label\": \"Sao Paulo\",\n    \"Customer\": [{\n        \"id\": \"CUS-99992\",\n        \"label\": \"Brazil\",\n        \"number\": [{\n        \"part\": \"7897\",\n        \"client\": \"892\"\n    },\n    {\n       \"part\": \"888\",\n       \"client\": \"12\"\n    }]\n    }]  \n}, \n{\n    \"id\": 92,\n    \"label\": \"Hong Kong\",\n    \"Customer\": [{\n        \"id\": \"CUS-88888\",\n        \"label\": \"China\",\n        \"number\": [{\n        \"part\": \"147\",\n        \"client\": \"288\"\n    }]\n    }] \n}]\n</code></pre>\n\n<p>============================</p>\n\n<p><strong>What I'm getting:</strong></p>\n\n<pre><code>[{\n    \"id\": 6,\n    \"label\": \"Sao Paulo\",\n    \"Customer\": [{\n        \"id\": \"CUS-99992\",\n        \"label\": \"Brazil\"\n    }],\n    \"number\": [{\n        \"part\": \"7897\",\n        \"client\": \"892\"\n    }],\n    \"number\": [{\n        \"part\": \"888\",\n        \"client\": \"12\"\n    }]\n}, {\n    \"id\": 92,\n    \"label\": \"Hong Kong\",\n    \"Customer\": [{\n        \"id\": \"CUS-88888\",\n        \"label\": \"China\"\n    }],\n    \"number\": [{\n        \"part\": \"147\",\n        \"client\": \"288\"\n    }]\n}]\n</code></pre>\n\n<p>======================</p>\n\n<p>I tried to do the same thing using <code>iterrows</code> function (and posted a question here 'Dataframe and conversion to JSON using Pandas'), but some people recommend me to try another way using another function. I know that is a stupid thing add <code>number</code> object inside my <code>data</code>, but I already tried of others way. </p>\n\n<p>Could you help me?</p>\n",
        "formatted_input": {
            "qid": 59254205,
            "link": "https://stackoverflow.com/questions/59254205/how-to-append-a-list-in-pandas",
            "question": {
                "title": "How to append a list in Pandas?",
                "ques_desc": "I'm reading a dataframe and trying to insert a list inside another list and then converting it to json file. I'm using python 3 and 0.25.3 version of pandas for it. ============================ Data that I'm reading: ============================ Here is my code: ============================= What is expected: ============================ What I'm getting: ====================== I tried to do the same thing using function (and posted a question here 'Dataframe and conversion to JSON using Pandas'), but some people recommend me to try another way using another function. I know that is a stupid thing add object inside my , but I already tried of others way. Could you help me? "
            },
            "io": [
                "[{\n    \"id\": 6,\n    \"label\": \"Sao Paulo\",\n    \"Customer\": [{\n        \"id\": \"CUS-99992\",\n        \"label\": \"Brazil\",\n        \"number\": [{\n        \"part\": \"7897\",\n        \"client\": \"892\"\n    },\n    {\n       \"part\": \"888\",\n       \"client\": \"12\"\n    }]\n    }]  \n}, \n{\n    \"id\": 92,\n    \"label\": \"Hong Kong\",\n    \"Customer\": [{\n        \"id\": \"CUS-88888\",\n        \"label\": \"China\",\n        \"number\": [{\n        \"part\": \"147\",\n        \"client\": \"288\"\n    }]\n    }] \n}]\n",
                "[{\n    \"id\": 6,\n    \"label\": \"Sao Paulo\",\n    \"Customer\": [{\n        \"id\": \"CUS-99992\",\n        \"label\": \"Brazil\"\n    }],\n    \"number\": [{\n        \"part\": \"7897\",\n        \"client\": \"892\"\n    }],\n    \"number\": [{\n        \"part\": \"888\",\n        \"client\": \"12\"\n    }]\n}, {\n    \"id\": 92,\n    \"label\": \"Hong Kong\",\n    \"Customer\": [{\n        \"id\": \"CUS-88888\",\n        \"label\": \"China\"\n    }],\n    \"number\": [{\n        \"part\": \"147\",\n        \"client\": \"288\"\n    }]\n}]\n"
            ],
            "answer": {
                "ans_desc": "Define the following reformatting function: Then apply it the following way: The result (reformatted for readability) is: Edit following the comment To cope with the variant of multiple rows for a single label / label_customer, take another approach: Start from defining the following functions: Get the content of number attribute: Note eval in this function. Otherwise the result would be a string (instead of list of dictionaries). Get the content of Customer attribute: Get the content of the whole JSON element for the current group: Then convert column types to string: And to get the final result, run: The above code: Renames part_number and number_client to part and client, respectively. This change is needed to generate proper element names by getNum. Groups the DataFrame (as in your code). Applies getGrp function to each group. The result is a Series of JSON elements. And finally to_json converts this Series to a list of JSON elements. ",
                "code": [
                    "def getNum(grp):\n    return eval(grp[['part', 'client']].to_json(orient='records'))\n",
                    "def getCust(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id_customer, 'label': r0.label_customer, 'number': getNum(grp) }\n",
                    "def getGrp(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id, 'label': r0.label, 'Customer': getCust(grp) }\n",
                    "df.part_number = df.part_number.astype('str')\ndf.number_client = df.number_client.astype('str')\n",
                    "df.rename(columns={'part_number': 'part', 'number_client': 'client'})\\\n    .groupby(['id', 'label', 'id_customer', 'label_customer'])\\\n    .apply(getGrp).to_json(orient='values')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 39,
            "user_id": 11646543,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/fd03f78da3e5c5f18cb9ae055295c5fd?s=128&d=identicon&r=PG&f=1",
            "display_name": "user11646543",
            "link": "https://stackoverflow.com/users/11646543/user11646543"
        },
        "is_answered": true,
        "view_count": 545,
        "closed_date": 1575982894,
        "accepted_answer_id": 59266380,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1575979789,
        "creation_date": 1575977736,
        "question_id": 59266300,
        "link": "https://stackoverflow.com/questions/59266300/how-to-drop-the-rows-if-two-columns-cells-are-empty",
        "closed_reason": "Needs more focus",
        "title": "How to drop the rows if two columns cells are empty?",
        "body": "<p>This is my DF</p>\n\n<pre><code>A  B  C\n1  10 10\n2  \n3  12 12\n4      \n5  21 22\n</code></pre>\n\n<p>i want to compare the columns B and C then i have to check both are null after that i want to remove that rows from DF.</p>\n\n<p>Output looks like,this</p>\n\n<pre><code>A  B  C\n1  10 10\n3  12 12     \n5  21 22\n</code></pre>\n\n<p>Then i need to check again both columns of B and C like whether the values or same or not , if same i need to create one column say validation_results and print Y and if not same print N.</p>\n\n<pre><code>A  B  C  Validation_Results\n1  10 10  Y\n3  12 12  Y  \n5  21 22  N\n</code></pre>\n\n<p>I am new to python so anybody here tell me how can i do this with minimum lines of code.</p>\n",
        "answer_body": "<p>Solution if no values are missing values:</p>\n\n<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\" rel=\"nofollow noreferrer\"><code>DataFrame.dropna</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\" rel=\"nofollow noreferrer\"><code>DataFrame.assign</code></a> new column created by <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\" rel=\"nofollow noreferrer\"><code>numpy.where</code></a>:</p>\n\n<pre><code>print (df)\n   A     B     C\n0  1  10.0  10.0\n1  2   NaN   NaN\n2  3  12.0  12.0\n3  4   NaN   NaN\n4  5  21.0  22.0\n\ndf1 = (df.dropna(subset=['B','C'], how='all')\n         .assign(Validation_Results = lambda x: np.where(x.B==x.C, 'Y', 'N')))\nprint (df1)\n   A     B     C Validation_Results\n0  1  10.0  10.0                  Y\n2  3  12.0  12.0                  Y\n4  5  21.0  22.0                  N\n</code></pre>\n\n<p>Solution if no values are empty strings:</p>\n\n<pre><code>print (df)\n   A   B   C\n0  1  10  10\n1  2        \n2  3  12  12\n3  4        \n4  5  21  22\n\ndf1 = (df[df[['B','C']].ne('').all(axis=1)]\n         .assign(Validation_Results = lambda x: np.where(x.B==x.C, 'Y', 'N')))\nprint (df1)\n   A   B   C Validation_Results\n0  1  10  10                  Y\n2  3  12  12                  Y\n4  5  21  22                  N\n</code></pre>\n\n<p><strong>Details</strong>:</p>\n\n<p>First compare both columns by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ne.html\" rel=\"nofollow noreferrer\"><code>DataFrame.ne</code></a> for not equal <code>''</code> - empty string:</p>\n\n<pre><code>print (df[['B','C']].ne(''))\n       B      C\n0   True   True\n1  False  False\n2   True   True\n3  False  False\n4   True   True\n</code></pre>\n\n<p>And then test if both values in row are <code>True</code>s by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.all.html\" rel=\"nofollow noreferrer\"><code>DataFrame.all</code></a>:</p>\n\n<pre><code>print (df[['B','C']].ne('').all(axis=1))\n0     True\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n</code></pre>\n\n<p>And filter them by <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a>:</p>\n\n<pre><code>print (df[df[['B','C']].ne('').all(axis=1)])\n   A   B   C\n0  1  10  10\n2  3  12  12\n4  5  21  22\n</code></pre>\n",
        "question_body": "<p>This is my DF</p>\n\n<pre><code>A  B  C\n1  10 10\n2  \n3  12 12\n4      \n5  21 22\n</code></pre>\n\n<p>i want to compare the columns B and C then i have to check both are null after that i want to remove that rows from DF.</p>\n\n<p>Output looks like,this</p>\n\n<pre><code>A  B  C\n1  10 10\n3  12 12     \n5  21 22\n</code></pre>\n\n<p>Then i need to check again both columns of B and C like whether the values or same or not , if same i need to create one column say validation_results and print Y and if not same print N.</p>\n\n<pre><code>A  B  C  Validation_Results\n1  10 10  Y\n3  12 12  Y  \n5  21 22  N\n</code></pre>\n\n<p>I am new to python so anybody here tell me how can i do this with minimum lines of code.</p>\n",
        "formatted_input": {
            "qid": 59266300,
            "link": "https://stackoverflow.com/questions/59266300/how-to-drop-the-rows-if-two-columns-cells-are-empty",
            "question": {
                "title": "How to drop the rows if two columns cells are empty?",
                "ques_desc": "This is my DF i want to compare the columns B and C then i have to check both are null after that i want to remove that rows from DF. Output looks like,this Then i need to check again both columns of B and C like whether the values or same or not , if same i need to create one column say validation_results and print Y and if not same print N. I am new to python so anybody here tell me how can i do this with minimum lines of code. "
            },
            "io": [
                "A  B  C\n1  10 10\n2  \n3  12 12\n4      \n5  21 22\n",
                "A  B  C\n1  10 10\n3  12 12     \n5  21 22\n"
            ],
            "answer": {
                "ans_desc": "Solution if no values are missing values: Use with new column created by : Solution if no values are empty strings: Details: First compare both columns by for not equal - empty string: And then test if both values in row are s by : And filter them by : ",
                "code": [
                    "print (df[['B','C']].ne(''))\n       B      C\n0   True   True\n1  False  False\n2   True   True\n3  False  False\n4   True   True\n",
                    "print (df[['B','C']].ne('').all(axis=1))\n0     True\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "date",
            "dataframe"
        ],
        "owner": {
            "reputation": 441,
            "user_id": 9843081,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e4a16df0896b1ea32fa57fbda2eed47a?s=128&d=identicon&r=PG&f=1",
            "display_name": "Rik",
            "link": "https://stackoverflow.com/users/9843081/rik"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 59250985,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1575902833,
        "creation_date": 1575901402,
        "last_edit_date": 1575902833,
        "question_id": 59250863,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59250863/python-sort-subsection-of-columns",
        "title": "Python: Sort subsection of columns",
        "body": "<p>Suppose we have the following dataframe:</p>\n\n<pre><code>Label1  2016-03-31  2016-05-31  2016-04-30\n0   A   A1              1            6\n1   B   B1              3            4\n2   C   C2              5            7\n3   D   D1              7            2\n4   E   E4              9            4\n5   F   F1              11           6\n</code></pre>\n\n<p>Which can be computed as follows</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nA = pd.DataFrame([['A','A1',1, 6], ['B','B1' ,3,4], ['C', 'C2', 5,7], ['D','D1',7,2], ['E','E4',9,4], ['F','F1',11,6]], columns=['Label1',pd.to_datetime('2016-03-31') , pd.to_datetime('2016-05-31'),pd.to_datetime('2016-04-30')])\n</code></pre>\n\n<p>I was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. I would want the end result to look as </p>\n\n<pre><code>Label1  2016-03-31  2016-04-30 2016-05-31   \n0   A   A1              6            1\n1   B   B1              4            3\n2   C   C2              7            5\n3   D   D1              2            7\n4   E   E4              4            9\n5   F   F1              6            11\n</code></pre>\n",
        "answer_body": "<p>This should work:</p>\n\n<pre><code>new_cols = list(df.columns[:-3]) + list(df.columns[-3:].sort_values())\n\ndf[new_cols]\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>  Label1 2016-03-31  2016-04-30  2016-05-31\n0      A         A1           6           1\n1      B         B1           4           3\n2      C         C2           7           5\n3      D         D1           2           7\n4      E         E4           4           9\n5      F         F1           6          11\n</code></pre>\n",
        "question_body": "<p>Suppose we have the following dataframe:</p>\n\n<pre><code>Label1  2016-03-31  2016-05-31  2016-04-30\n0   A   A1              1            6\n1   B   B1              3            4\n2   C   C2              5            7\n3   D   D1              7            2\n4   E   E4              9            4\n5   F   F1              11           6\n</code></pre>\n\n<p>Which can be computed as follows</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nA = pd.DataFrame([['A','A1',1, 6], ['B','B1' ,3,4], ['C', 'C2', 5,7], ['D','D1',7,2], ['E','E4',9,4], ['F','F1',11,6]], columns=['Label1',pd.to_datetime('2016-03-31') , pd.to_datetime('2016-05-31'),pd.to_datetime('2016-04-30')])\n</code></pre>\n\n<p>I was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. I would want the end result to look as </p>\n\n<pre><code>Label1  2016-03-31  2016-04-30 2016-05-31   \n0   A   A1              6            1\n1   B   B1              4            3\n2   C   C2              7            5\n3   D   D1              2            7\n4   E   E4              4            9\n5   F   F1              6            11\n</code></pre>\n",
        "formatted_input": {
            "qid": 59250863,
            "link": "https://stackoverflow.com/questions/59250863/python-sort-subsection-of-columns",
            "question": {
                "title": "Python: Sort subsection of columns",
                "ques_desc": "Suppose we have the following dataframe: Which can be computed as follows I was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. I would want the end result to look as "
            },
            "io": [
                "Label1  2016-03-31  2016-05-31  2016-04-30\n0   A   A1              1            6\n1   B   B1              3            4\n2   C   C2              5            7\n3   D   D1              7            2\n4   E   E4              9            4\n5   F   F1              11           6\n",
                "Label1  2016-03-31  2016-04-30 2016-05-31   \n0   A   A1              6            1\n1   B   B1              4            3\n2   C   C2              7            5\n3   D   D1              2            7\n4   E   E4              4            9\n5   F   F1              6            11\n"
            ],
            "answer": {
                "ans_desc": "This should work: Output: ",
                "code": [
                    "new_cols = list(df.columns[:-3]) + list(df.columns[-3:].sort_values())\n\ndf[new_cols]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 6802,
            "user_id": 11232091,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vt9vH.png?s=128&g=1",
            "display_name": "moys",
            "link": "https://stackoverflow.com/users/11232091/moys"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 59189341,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1575527143,
        "creation_date": 1575526583,
        "question_id": 59189297,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59189297/new-dataframe-which-contais-a-certain-value-within-each-group",
        "title": "New dataframe which contais a certain value within each group",
        "body": "<p>I have a dataframe as below</p>\n\n<pre><code>User    eve Ses\na   123 1\na   123 2\na   123 3\na   123 4\na   123 5\na   123 6\na   456 1\na   456 2\na   456 3\na   456 4\na   456 5\na   456 14\na   456 7\na   456 8\na   456 9\na   456 10\na   888 1\na   888 2\na   888 3\na   888 4\na   888 5\na   888 5\na   888 7\na   888 8\nb   123 1\nb   123 2\nb   123 3\nb   123 4\nb   123 5\nb   123 6\nb   456 1\nb   456 2\nb   456 3\nb   456 4\nb   456 5\nb   456 9\nb   456 7\nb   456 8\nb   456 9\nb   456 10\nb   888 1\nb   888 2\nb   888 3\nb   888 4\nb   888 5\nb   888 6\nb   888 7\nb   888 8\n</code></pre>\n\n<p>I want to group by <code>User</code> &amp; <code>eve</code> and get a new dataframe with all the groups that contains 6 or 14</p>\n\n<p>When I use the code below</p>\n\n<p><code>df.groupby(['User','eve']).apply(lambda x: (x['Ses']==6).any()|(x['Ses']==14).any())</code>\nI accurately get the groups which have either 6 or 14 as below</p>\n\n<pre><code>User  eve\na     123     True\n      456     True\n      888    False\nb     123     True\n      456    False\n      888     True\ndtype: bool\n</code></pre>\n\n<p>I am just not able to use this information to get the new dataframe which has the groups that are <code>True</code>.\nThe expected output is the new dataframe as below. Can anyone guide?</p>\n\n<pre><code>User    eve Ses\na   123 1\na   123 2\na   123 3\na   123 4\na   123 5\na   123 6\na   456 1\na   456 2\na   456 3\na   456 4\na   456 5\na   456 14\na   456 7\na   456 8\na   456 9\na   456 10\nb   123 1\nb   123 2\nb   123 3\nb   123 4\nb   123 5\nb   123 6\nb   888 1\nb   888 2\nb   888 3\nb   888 4\nb   888 5\nb   888 6\nb   888 7\nb   888 8\n</code></pre>\n",
        "answer_body": "<p>For improve performance is possible use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.any.html\" rel=\"nofollow noreferrer\"><code>GroupBy.any</code></a> with mask created <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isin.html\" rel=\"nofollow noreferrer\"><code>Series.isin</code></a> and helper column by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\" rel=\"nofollow noreferrer\"><code>DataFrame.assign</code></a>:</p>\n\n<pre><code>df = df[df.assign(m=df['Ses'].isin([6,14])).groupby(['User','eve'])['m'].transform('any')]\nprint (df)\n   User  eve  Ses\n0     a  123    1\n1     a  123    2\n2     a  123    3\n3     a  123    4\n4     a  123    5\n5     a  123    6\n6     a  456    1\n7     a  456    2\n8     a  456    3\n9     a  456    4\n10    a  456    5\n11    a  456   14\n12    a  456    7\n13    a  456    8\n14    a  456    9\n15    a  456   10\n24    b  123    1\n25    b  123    2\n26    b  123    3\n27    b  123    4\n28    b  123    5\n29    b  123    6\n40    b  888    1\n41    b  888    2\n42    b  888    3\n43    b  888    4\n44    b  888    5\n45    b  888    6\n46    b  888    7\n47    b  888    8\n</code></pre>\n\n<p>Your solution should be changed with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html\" rel=\"nofollow noreferrer\"><code>DataFrameGroupBy.filter</code></a>, but if larger DataFrame or many groups solution is really slow:</p>\n\n<pre><code>df = df.groupby(['User','eve']).filter(lambda x: (x['Ses']==6).any()|(x['Ses']==14).any())\n</code></pre>\n",
        "question_body": "<p>I have a dataframe as below</p>\n\n<pre><code>User    eve Ses\na   123 1\na   123 2\na   123 3\na   123 4\na   123 5\na   123 6\na   456 1\na   456 2\na   456 3\na   456 4\na   456 5\na   456 14\na   456 7\na   456 8\na   456 9\na   456 10\na   888 1\na   888 2\na   888 3\na   888 4\na   888 5\na   888 5\na   888 7\na   888 8\nb   123 1\nb   123 2\nb   123 3\nb   123 4\nb   123 5\nb   123 6\nb   456 1\nb   456 2\nb   456 3\nb   456 4\nb   456 5\nb   456 9\nb   456 7\nb   456 8\nb   456 9\nb   456 10\nb   888 1\nb   888 2\nb   888 3\nb   888 4\nb   888 5\nb   888 6\nb   888 7\nb   888 8\n</code></pre>\n\n<p>I want to group by <code>User</code> &amp; <code>eve</code> and get a new dataframe with all the groups that contains 6 or 14</p>\n\n<p>When I use the code below</p>\n\n<p><code>df.groupby(['User','eve']).apply(lambda x: (x['Ses']==6).any()|(x['Ses']==14).any())</code>\nI accurately get the groups which have either 6 or 14 as below</p>\n\n<pre><code>User  eve\na     123     True\n      456     True\n      888    False\nb     123     True\n      456    False\n      888     True\ndtype: bool\n</code></pre>\n\n<p>I am just not able to use this information to get the new dataframe which has the groups that are <code>True</code>.\nThe expected output is the new dataframe as below. Can anyone guide?</p>\n\n<pre><code>User    eve Ses\na   123 1\na   123 2\na   123 3\na   123 4\na   123 5\na   123 6\na   456 1\na   456 2\na   456 3\na   456 4\na   456 5\na   456 14\na   456 7\na   456 8\na   456 9\na   456 10\nb   123 1\nb   123 2\nb   123 3\nb   123 4\nb   123 5\nb   123 6\nb   888 1\nb   888 2\nb   888 3\nb   888 4\nb   888 5\nb   888 6\nb   888 7\nb   888 8\n</code></pre>\n",
        "formatted_input": {
            "qid": 59189297,
            "link": "https://stackoverflow.com/questions/59189297/new-dataframe-which-contais-a-certain-value-within-each-group",
            "question": {
                "title": "New dataframe which contais a certain value within each group",
                "ques_desc": "I have a dataframe as below I want to group by & and get a new dataframe with all the groups that contains 6 or 14 When I use the code below I accurately get the groups which have either 6 or 14 as below I am just not able to use this information to get the new dataframe which has the groups that are . The expected output is the new dataframe as below. Can anyone guide? "
            },
            "io": [
                "User    eve Ses\na   123 1\na   123 2\na   123 3\na   123 4\na   123 5\na   123 6\na   456 1\na   456 2\na   456 3\na   456 4\na   456 5\na   456 14\na   456 7\na   456 8\na   456 9\na   456 10\na   888 1\na   888 2\na   888 3\na   888 4\na   888 5\na   888 5\na   888 7\na   888 8\nb   123 1\nb   123 2\nb   123 3\nb   123 4\nb   123 5\nb   123 6\nb   456 1\nb   456 2\nb   456 3\nb   456 4\nb   456 5\nb   456 9\nb   456 7\nb   456 8\nb   456 9\nb   456 10\nb   888 1\nb   888 2\nb   888 3\nb   888 4\nb   888 5\nb   888 6\nb   888 7\nb   888 8\n",
                "User    eve Ses\na   123 1\na   123 2\na   123 3\na   123 4\na   123 5\na   123 6\na   456 1\na   456 2\na   456 3\na   456 4\na   456 5\na   456 14\na   456 7\na   456 8\na   456 9\na   456 10\nb   123 1\nb   123 2\nb   123 3\nb   123 4\nb   123 5\nb   123 6\nb   888 1\nb   888 2\nb   888 3\nb   888 4\nb   888 5\nb   888 6\nb   888 7\nb   888 8\n"
            ],
            "answer": {
                "ans_desc": "For improve performance is possible use and with mask created and helper column by : Your solution should be changed with , but if larger DataFrame or many groups solution is really slow: ",
                "code": [
                    "df = df.groupby(['User','eve']).filter(lambda x: (x['Ses']==6).any()|(x['Ses']==14).any())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 6757984,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/802dc1e2851f2c598f3412bb0819362f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Bruno Cazzaniga",
            "link": "https://stackoverflow.com/users/6757984/bruno-cazzaniga"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 59171580,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1575448697,
        "creation_date": 1575447114,
        "question_id": 59171537,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59171537/create-a-new-pandas-dataframe-column-based-on-other-column-of-the-dataframe",
        "title": "Create a new pandas dataframe column based on other column of the dataframe",
        "body": "<p>I have a Dataframe that consists in 2 columns:</p>\n\n<ul>\n<li><p>'String' -> numpy array like [47, 0, 49, 12, 46]</p></li>\n<li><p>'Is Isogram' -> 1 or 0</p></li>\n</ul>\n\n<pre><code>    String              Is Isogram\n0   [47, 0, 49, 12, 46] 1\n1   [43, 50, 22, 1, 13] 1\n2   [10, 1, 24, 22, 16] 1\n3   [2, 24, 3, 24, 51]  0\n4   [40, 1, 41, 18, 3]  1\n</code></pre>\n\n<p>I would like to create another column, with the value 'Is Isogram' appended in the 'String' array, something like this:</p>\n\n<pre><code>    String              Is Isogram  IsoString\n0   [47, 0, 49, 12, 46] 1           [47, 0, 49, 12, 46, 1]\n1   [43, 50, 22, 1, 13] 1           [43, 50, 22, 1, 13, 1]\n2   [10, 1, 24, 22, 16] 1           [10, 1, 24, 22, 16, 1]\n3   [2, 24, 3, 24, 51]  0           [2, 24, 3, 24, 51, 0]\n4   [40, 1, 41, 18, 3]  1           [40, 1, 41, 18, 3, 1]\n</code></pre>\n\n<p>I've tried using the apply function with a lambda:</p>\n\n<pre><code>df[''IsoString] = df.apply(lambda x: np.append(x['String'].values, x['Is Isogram'].values, axis=1))\n</code></pre>\n\n<p>But it throws me a KeyError that i don't really understand</p>\n\n<pre><code>KeyError: ('String', 'occurred at index String')\n</code></pre>\n\n<p>How can i takle this problem?</p>\n",
        "answer_body": "<p>There is problem <code>axis=1</code> is called for <code>np.append</code> instead <code>.apply</code> function:</p>\n\n<pre><code>df['IsoString'] = df.apply(lambda x: np.append(x['String'], x['Is Isogram']), axis=1)\n</code></pre>\n\n<p>Better/faster is use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\" rel=\"nofollow noreferrer\"><code>numpy.hstack</code></a> if same length of each lists in <code>String</code>:</p>\n\n<pre><code>arr = np.hstack((np.array(df['String'].tolist()), df['Is Isogram'].values[:, None]))\nprint (arr)\n[[47  0 49 12 46  1]\n [43 50 22  1 13  1]\n [10  1 24 22 16  1]\n [ 2 24  3 24 51  0]\n [40  1 41 18  3  1]]\n\ndf['IsoString'] = arr.tolist()\nprint (df)\n                String  Is Isogram               IsoString\n0  [47, 0, 49, 12, 46]           1  [47, 0, 49, 12, 46, 1]\n1  [43, 50, 22, 1, 13]           1  [43, 50, 22, 1, 13, 1]\n2  [10, 1, 24, 22, 16]           1  [10, 1, 24, 22, 16, 1]\n3   [2, 24, 3, 24, 51]           0   [2, 24, 3, 24, 51, 0]\n4   [40, 1, 41, 18, 3]           1   [40, 1, 41, 18, 3, 1]\n</code></pre>\n",
        "question_body": "<p>I have a Dataframe that consists in 2 columns:</p>\n\n<ul>\n<li><p>'String' -> numpy array like [47, 0, 49, 12, 46]</p></li>\n<li><p>'Is Isogram' -> 1 or 0</p></li>\n</ul>\n\n<pre><code>    String              Is Isogram\n0   [47, 0, 49, 12, 46] 1\n1   [43, 50, 22, 1, 13] 1\n2   [10, 1, 24, 22, 16] 1\n3   [2, 24, 3, 24, 51]  0\n4   [40, 1, 41, 18, 3]  1\n</code></pre>\n\n<p>I would like to create another column, with the value 'Is Isogram' appended in the 'String' array, something like this:</p>\n\n<pre><code>    String              Is Isogram  IsoString\n0   [47, 0, 49, 12, 46] 1           [47, 0, 49, 12, 46, 1]\n1   [43, 50, 22, 1, 13] 1           [43, 50, 22, 1, 13, 1]\n2   [10, 1, 24, 22, 16] 1           [10, 1, 24, 22, 16, 1]\n3   [2, 24, 3, 24, 51]  0           [2, 24, 3, 24, 51, 0]\n4   [40, 1, 41, 18, 3]  1           [40, 1, 41, 18, 3, 1]\n</code></pre>\n\n<p>I've tried using the apply function with a lambda:</p>\n\n<pre><code>df[''IsoString] = df.apply(lambda x: np.append(x['String'].values, x['Is Isogram'].values, axis=1))\n</code></pre>\n\n<p>But it throws me a KeyError that i don't really understand</p>\n\n<pre><code>KeyError: ('String', 'occurred at index String')\n</code></pre>\n\n<p>How can i takle this problem?</p>\n",
        "formatted_input": {
            "qid": 59171537,
            "link": "https://stackoverflow.com/questions/59171537/create-a-new-pandas-dataframe-column-based-on-other-column-of-the-dataframe",
            "question": {
                "title": "Create a new pandas dataframe column based on other column of the dataframe",
                "ques_desc": "I have a Dataframe that consists in 2 columns: 'String' -> numpy array like [47, 0, 49, 12, 46] 'Is Isogram' -> 1 or 0 I would like to create another column, with the value 'Is Isogram' appended in the 'String' array, something like this: I've tried using the apply function with a lambda: But it throws me a KeyError that i don't really understand How can i takle this problem? "
            },
            "io": [
                "    String              Is Isogram\n0   [47, 0, 49, 12, 46] 1\n1   [43, 50, 22, 1, 13] 1\n2   [10, 1, 24, 22, 16] 1\n3   [2, 24, 3, 24, 51]  0\n4   [40, 1, 41, 18, 3]  1\n",
                "    String              Is Isogram  IsoString\n0   [47, 0, 49, 12, 46] 1           [47, 0, 49, 12, 46, 1]\n1   [43, 50, 22, 1, 13] 1           [43, 50, 22, 1, 13, 1]\n2   [10, 1, 24, 22, 16] 1           [10, 1, 24, 22, 16, 1]\n3   [2, 24, 3, 24, 51]  0           [2, 24, 3, 24, 51, 0]\n4   [40, 1, 41, 18, 3]  1           [40, 1, 41, 18, 3, 1]\n"
            ],
            "answer": {
                "ans_desc": "There is problem is called for instead function: Better/faster is use if same length of each lists in : ",
                "code": [
                    "df['IsoString'] = df.apply(lambda x: np.append(x['String'], x['Is Isogram']), axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 12406191,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d3347265a961f051657b4b0892052325?s=128&d=identicon&r=PG&f=1",
            "display_name": "JeffGray",
            "link": "https://stackoverflow.com/users/12406191/jeffgray"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 59149415,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1575340550,
        "creation_date": 1575339280,
        "question_id": 59149281,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59149281/how-to-create-rows-for-unique-values-in-columns-in-pandas",
        "title": "How to create rows for unique values in columns in pandas?",
        "body": "<p>I have a pandas dataframe  with thousands of rows like so:</p>\n\n<pre><code>IntentID     IntentName         Query           Response\n1            Intent Name 1      Query 1         Response1\n2            Intent Name 1      Query 1         Response2\n3            Intent Name 2      Query 2         Response3\n4            Intent Name 2      Query 2         Response4\n5            Intent Name 3      Query 3         Response5\n</code></pre>\n\n<p>I need all unique values in \"IntentName\" to have the same IntentID value like so:</p>\n\n<pre><code>IntentID     IntentName         Query           Response\n1            Intent Name 1      Query 1         Response1\n1            Intent Name 1      Query 1         Response2\n2            Intent Name 2      Query 2         Response3\n2            Intent Name 2      Query 2         Response4\n3            Intent Name 3      Query 3         Response5\n</code></pre>\n\n<p>What is the easiest way to do this?</p>\n",
        "answer_body": "<p>Try this:</p>\n\n<pre><code>df['IntentID'] = df.groupby('IntentName') \\\n                    ['IntentID'].transform('first') \\\n                    .rank(method='dense') \\\n                    .astype('int')\n</code></pre>\n\n<p>How it works:</p>\n\n<ul>\n<li>Group the rows by <code>IntentName</code></li>\n<li>For each group, keep the first <code>IntentID</code></li>\n<li>Rank those <code>IntentID</code>s 1, 1, 2, 2, 3, etc. (<code>method=dense</code>)</li>\n<li>Convert the ranks to int</li>\n</ul>\n",
        "question_body": "<p>I have a pandas dataframe  with thousands of rows like so:</p>\n\n<pre><code>IntentID     IntentName         Query           Response\n1            Intent Name 1      Query 1         Response1\n2            Intent Name 1      Query 1         Response2\n3            Intent Name 2      Query 2         Response3\n4            Intent Name 2      Query 2         Response4\n5            Intent Name 3      Query 3         Response5\n</code></pre>\n\n<p>I need all unique values in \"IntentName\" to have the same IntentID value like so:</p>\n\n<pre><code>IntentID     IntentName         Query           Response\n1            Intent Name 1      Query 1         Response1\n1            Intent Name 1      Query 1         Response2\n2            Intent Name 2      Query 2         Response3\n2            Intent Name 2      Query 2         Response4\n3            Intent Name 3      Query 3         Response5\n</code></pre>\n\n<p>What is the easiest way to do this?</p>\n",
        "formatted_input": {
            "qid": 59149281,
            "link": "https://stackoverflow.com/questions/59149281/how-to-create-rows-for-unique-values-in-columns-in-pandas",
            "question": {
                "title": "How to create rows for unique values in columns in pandas?",
                "ques_desc": "I have a pandas dataframe with thousands of rows like so: I need all unique values in \"IntentName\" to have the same IntentID value like so: What is the easiest way to do this? "
            },
            "io": [
                "IntentID     IntentName         Query           Response\n1            Intent Name 1      Query 1         Response1\n2            Intent Name 1      Query 1         Response2\n3            Intent Name 2      Query 2         Response3\n4            Intent Name 2      Query 2         Response4\n5            Intent Name 3      Query 3         Response5\n",
                "IntentID     IntentName         Query           Response\n1            Intent Name 1      Query 1         Response1\n1            Intent Name 1      Query 1         Response2\n2            Intent Name 2      Query 2         Response3\n2            Intent Name 2      Query 2         Response4\n3            Intent Name 3      Query 3         Response5\n"
            ],
            "answer": {
                "ans_desc": "Try this: How it works: Group the rows by For each group, keep the first Rank those s 1, 1, 2, 2, 3, etc. () Convert the ranks to int ",
                "code": [
                    "df['IntentID'] = df.groupby('IntentName') \\\n                    ['IntentID'].transform('first') \\\n                    .rank(method='dense') \\\n                    .astype('int')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "lambda",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 161,
            "user_id": 3458906,
            "user_type": "registered",
            "accept_rate": 71,
            "profile_image": "https://www.gravatar.com/avatar/dcb759e07faa5f8bfd27f36b75b6e30e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Hans Vader",
            "link": "https://stackoverflow.com/users/3458906/hans-vader"
        },
        "is_answered": true,
        "view_count": 142,
        "accepted_answer_id": 59067220,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1574847969,
        "creation_date": 1574847707,
        "last_edit_date": 1574847815,
        "question_id": 59067194,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59067194/phyton-how-to-get-the-average-of-the-n-largest-values-for-each-column-grouped-b",
        "title": "Phyton: How to get the average of the n largest values for each column grouped by id",
        "body": "<p>I'm trying to get the mean for each column while grouped by id. But I don't get it to work as I want to.</p>\n\n<p><em>The data:</em></p>\n\n<pre><code>ID       Property3   Property2   Property3\n1        10.2        ...         ...\n1        20.1\n1        51.9\n1        15.8\n1        12.5\n...\n1203     104.4\n1203     11.5\n1203     19.4\n1203     23.1\n</code></pre>\n\n<p><em>What I got so far:</em><br>\nI got those two tries. But they are both just for one column and I don't know how to do it for more then just one.:  </p>\n\n<pre><code>data.groupby('id')['property1'].apply(lambda grp: grp.nlargest(100).mean())\n1       37.897989\n2       33.059432\n3       34.926530\n4       33.036137\n\ndata.groupby('id').agg({'property1': {lambda grp: grp.nlargest(100).mean()}})\nid  property1 &lt;lambda&gt;\n1   37.897989\n2   33.059432\n3   34.926530\n4   33.036137\n</code></pre>\n\n<p><em>What I want:</em><br>\nIdealy, I would like to have a dataframe as follows:</p>\n\n<pre><code>ID       Property3   Property2   Property3\n1        37.8        5.6         2.3\n2        33.0        1.5         10.4\n3        34.9        91.5        10.3\n4        33.0        10.3        14.3\n</code></pre>\n\n<p>So that each row contains the mean values for the 100 biggest values for EACH column grouped by id.</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.agg.html\" rel=\"nofollow noreferrer\"><code>GroupBy.agg</code></a> with omit columns for processing all columns in DataFrame without <code>ID</code>:</p>\n\n<pre><code>df = data.groupby('ID').agg(lambda grp: grp.nlargest(100).mean()).reset_index()\n\nprint (df)\n       ID  Property1  Property2  Property3\n0     1.0       22.1       ...       ...\n1  1203.0       39.6       ...       ...\n</code></pre>\n\n<p>Or specify columns after <code>groupby</code>:</p>\n\n<pre><code>df = (data.groupby('ID')['Property1','Property2','Property3']\n          .agg(lambda grp: grp.nlargest(100).mean())\n          .reset_index())\n</code></pre>\n",
        "question_body": "<p>I'm trying to get the mean for each column while grouped by id. But I don't get it to work as I want to.</p>\n\n<p><em>The data:</em></p>\n\n<pre><code>ID       Property3   Property2   Property3\n1        10.2        ...         ...\n1        20.1\n1        51.9\n1        15.8\n1        12.5\n...\n1203     104.4\n1203     11.5\n1203     19.4\n1203     23.1\n</code></pre>\n\n<p><em>What I got so far:</em><br>\nI got those two tries. But they are both just for one column and I don't know how to do it for more then just one.:  </p>\n\n<pre><code>data.groupby('id')['property1'].apply(lambda grp: grp.nlargest(100).mean())\n1       37.897989\n2       33.059432\n3       34.926530\n4       33.036137\n\ndata.groupby('id').agg({'property1': {lambda grp: grp.nlargest(100).mean()}})\nid  property1 &lt;lambda&gt;\n1   37.897989\n2   33.059432\n3   34.926530\n4   33.036137\n</code></pre>\n\n<p><em>What I want:</em><br>\nIdealy, I would like to have a dataframe as follows:</p>\n\n<pre><code>ID       Property3   Property2   Property3\n1        37.8        5.6         2.3\n2        33.0        1.5         10.4\n3        34.9        91.5        10.3\n4        33.0        10.3        14.3\n</code></pre>\n\n<p>So that each row contains the mean values for the 100 biggest values for EACH column grouped by id.</p>\n",
        "formatted_input": {
            "qid": 59067194,
            "link": "https://stackoverflow.com/questions/59067194/phyton-how-to-get-the-average-of-the-n-largest-values-for-each-column-grouped-b",
            "question": {
                "title": "Phyton: How to get the average of the n largest values for each column grouped by id",
                "ques_desc": "I'm trying to get the mean for each column while grouped by id. But I don't get it to work as I want to. The data: What I got so far: I got those two tries. But they are both just for one column and I don't know how to do it for more then just one.: What I want: Idealy, I would like to have a dataframe as follows: So that each row contains the mean values for the 100 biggest values for EACH column grouped by id. "
            },
            "io": [
                "ID       Property3   Property2   Property3\n1        10.2        ...         ...\n1        20.1\n1        51.9\n1        15.8\n1        12.5\n...\n1203     104.4\n1203     11.5\n1203     19.4\n1203     23.1\n",
                "ID       Property3   Property2   Property3\n1        37.8        5.6         2.3\n2        33.0        1.5         10.4\n3        34.9        91.5        10.3\n4        33.0        10.3        14.3\n"
            ],
            "answer": {
                "ans_desc": "Use with omit columns for processing all columns in DataFrame without : Or specify columns after : ",
                "code": [
                    "df = (data.groupby('ID')['Property1','Property2','Property3']\n          .agg(lambda grp: grp.nlargest(100).mean())\n          .reset_index())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1865,
            "user_id": 4666912,
            "user_type": "registered",
            "accept_rate": 93,
            "profile_image": "https://www.gravatar.com/avatar/4095d3b4fd5f4902a717f07892393069?s=128&d=identicon&r=PG&f=1",
            "display_name": "BKS",
            "link": "https://stackoverflow.com/users/4666912/bks"
        },
        "is_answered": true,
        "view_count": 6958,
        "accepted_answer_id": 45064951,
        "answer_count": 3,
        "score": 6,
        "last_activity_date": 1574762542,
        "creation_date": 1499883409,
        "last_edit_date": 1499887508,
        "question_id": 45064916,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/45064916/how-to-find-the-correlation-between-a-group-of-values-in-a-pandas-dataframe-colu",
        "title": "How to find the correlation between a group of values in a pandas dataframe column",
        "body": "<p>I have a dataframe df:</p>\n\n<pre><code>ID    Var1     Var2\n1     1.2        4\n1     2.1        6\n1     3.0        7\n2     1.3        8\n2     2.1        9\n2     3.2        13\n</code></pre>\n\n<p>I want to find the pearson correlation coefficient value between <code>Var1</code> and <code>Var2</code> for every <code>ID</code></p>\n\n<p>So the result should look like this:</p>\n\n<pre><code>ID    Corr_Coef\n1     0.98198\n2     0.97073\n</code></pre>\n\n<p>update:</p>\n\n<p>Must make sure all columns of variables are <code>int</code> or <code>float</code></p>\n",
        "answer_body": "<pre><code>df.groupby('ID').corr()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>             Var1      Var2\nID                         \n1  Var1  1.000000  0.981981\n   Var2  0.981981  1.000000\n2  Var1  1.000000  0.970725\n   Var2  0.970725  1.000000\n</code></pre>\n\n<p>With OP output formating.</p>\n\n<pre><code>df_out = df.groupby('ID').corr()\n(df_out[~df_out['Var1'].eq(1)]\n          .reset_index(1, drop=True)['Var1']\n          .rename('Corr_Coef')\n          .reset_index())\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   ID  Corr_Coef\n0   1   0.981981\n1   2   0.970725\n</code></pre>\n",
        "question_body": "<p>I have a dataframe df:</p>\n\n<pre><code>ID    Var1     Var2\n1     1.2        4\n1     2.1        6\n1     3.0        7\n2     1.3        8\n2     2.1        9\n2     3.2        13\n</code></pre>\n\n<p>I want to find the pearson correlation coefficient value between <code>Var1</code> and <code>Var2</code> for every <code>ID</code></p>\n\n<p>So the result should look like this:</p>\n\n<pre><code>ID    Corr_Coef\n1     0.98198\n2     0.97073\n</code></pre>\n\n<p>update:</p>\n\n<p>Must make sure all columns of variables are <code>int</code> or <code>float</code></p>\n",
        "formatted_input": {
            "qid": 45064916,
            "link": "https://stackoverflow.com/questions/45064916/how-to-find-the-correlation-between-a-group-of-values-in-a-pandas-dataframe-colu",
            "question": {
                "title": "How to find the correlation between a group of values in a pandas dataframe column",
                "ques_desc": "I have a dataframe df: I want to find the pearson correlation coefficient value between and for every So the result should look like this: update: Must make sure all columns of variables are or "
            },
            "io": [
                "ID    Var1     Var2\n1     1.2        4\n1     2.1        6\n1     3.0        7\n2     1.3        8\n2     2.1        9\n2     3.2        13\n",
                "ID    Corr_Coef\n1     0.98198\n2     0.97073\n"
            ],
            "answer": {
                "ans_desc": " Output: With OP output formating. Output: ",
                "code": [
                    "df_out = df.groupby('ID').corr()\n(df_out[~df_out['Var1'].eq(1)]\n          .reset_index(1, drop=True)['Var1']\n          .rename('Corr_Coef')\n          .reset_index())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 311,
            "user_id": 10990958,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-zPVNZ6eNpvQ/AAAAAAAAAAI/AAAAAAAAAAA/ACevoQPKSQgglOQmsCtj-6IpCvlDqqDRcQ/mo/photo.jpg?sz=128",
            "display_name": "Pandas INC",
            "link": "https://stackoverflow.com/users/10990958/pandas-inc"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 59037069,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1574707954,
        "creation_date": 1574701948,
        "last_edit_date": 1574707954,
        "question_id": 59036924,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59036924/in-python-is-there-a-way-to-delete-parts-of-a-column",
        "title": "In python is there a way to delete parts of a column?",
        "body": "<p>I want to trim the values of a pandas data frame. For example, I have the following:</p>\n\n<pre><code> A            B        C\n 33344-10   5555-78  999902\n 3444441    5555679  2334\n 2334       5555     3344\n</code></pre>\n\n<p>And I would like the result to be:</p>\n\n<pre><code>A            B         C\n3334       5555     9999\n3444       5555     2334\n2334       5555     3344\n</code></pre>\n\n<p>If anyone could help it would be very appreciated.</p>\n",
        "answer_body": "<p>To keep first 4 characters in string columns:</p>\n\n<pre><code>for c in ['A', 'B', 'C']:\n    df[c] = df[c].str.slice(stop=4)\n</code></pre>\n",
        "question_body": "<p>I want to trim the values of a pandas data frame. For example, I have the following:</p>\n\n<pre><code> A            B        C\n 33344-10   5555-78  999902\n 3444441    5555679  2334\n 2334       5555     3344\n</code></pre>\n\n<p>And I would like the result to be:</p>\n\n<pre><code>A            B         C\n3334       5555     9999\n3444       5555     2334\n2334       5555     3344\n</code></pre>\n\n<p>If anyone could help it would be very appreciated.</p>\n",
        "formatted_input": {
            "qid": 59036924,
            "link": "https://stackoverflow.com/questions/59036924/in-python-is-there-a-way-to-delete-parts-of-a-column",
            "question": {
                "title": "In python is there a way to delete parts of a column?",
                "ques_desc": "I want to trim the values of a pandas data frame. For example, I have the following: And I would like the result to be: If anyone could help it would be very appreciated. "
            },
            "io": [
                " A            B        C\n 33344-10   5555-78  999902\n 3444441    5555679  2334\n 2334       5555     3344\n",
                "A            B         C\n3334       5555     9999\n3444       5555     2334\n2334       5555     3344\n"
            ],
            "answer": {
                "ans_desc": "To keep first 4 characters in string columns: ",
                "code": [
                    "for c in ['A', 'B', 'C']:\n    df[c] = df[c].str.slice(stop=4)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 327,
            "user_id": 4462289,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/fbca19e0d646b7f901a48c29c8bef5f7?s=128&d=identicon&r=PG&f=1",
            "display_name": "maxmijn",
            "link": "https://stackoverflow.com/users/4462289/maxmijn"
        },
        "is_answered": true,
        "view_count": 7285,
        "accepted_answer_id": 34657336,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1574460428,
        "creation_date": 1452175641,
        "question_id": 34657183,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/34657183/adding-rows-that-have-the-same-column-value-in-a-pandas-dataframe",
        "title": "Adding rows that have the same column value in a pandas dataframe",
        "body": "<p>I have a pandas dataframe with dates and hours as columns. Now I want to add the hours of the same dates. For example to make this:</p>\n\n<pre><code>7-1-2016 | 4\n7-1-2016 | 2\n4-1-2016 | 5\n</code></pre>\n\n<p>Into this:</p>\n\n<pre><code>7-1-2016 | 6\n4-1-2016 | 5\n</code></pre>\n\n<p>Is there a quick way to do this on big files?</p>\n",
        "answer_body": "<p>Here <code>GroupBy</code> can be used to provide the desired output.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False)\n</code></pre>\n\n<p>Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns.</p>\n\n<p>Try:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df.groupby('date')['hours'].sum()\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe with dates and hours as columns. Now I want to add the hours of the same dates. For example to make this:</p>\n\n<pre><code>7-1-2016 | 4\n7-1-2016 | 2\n4-1-2016 | 5\n</code></pre>\n\n<p>Into this:</p>\n\n<pre><code>7-1-2016 | 6\n4-1-2016 | 5\n</code></pre>\n\n<p>Is there a quick way to do this on big files?</p>\n",
        "formatted_input": {
            "qid": 34657183,
            "link": "https://stackoverflow.com/questions/34657183/adding-rows-that-have-the-same-column-value-in-a-pandas-dataframe",
            "question": {
                "title": "Adding rows that have the same column value in a pandas dataframe",
                "ques_desc": "I have a pandas dataframe with dates and hours as columns. Now I want to add the hours of the same dates. For example to make this: Into this: Is there a quick way to do this on big files? "
            },
            "io": [
                "7-1-2016 | 4\n7-1-2016 | 2\n4-1-2016 | 5\n",
                "7-1-2016 | 6\n4-1-2016 | 5\n"
            ],
            "answer": {
                "ans_desc": "Here can be used to provide the desired output. Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns. Try: ",
                "code": [
                    "DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "function",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 6788625,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/28a6f74df79e4a0ac8884b848ccfc564?s=128&d=identicon&r=PG&f=1",
            "display_name": "rnkv2",
            "link": "https://stackoverflow.com/users/6788625/rnkv2"
        },
        "is_answered": true,
        "view_count": 106,
        "accepted_answer_id": 58996395,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1574433773,
        "creation_date": 1574376129,
        "last_edit_date": 1574378583,
        "question_id": 58984851,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58984851/how-to-use-row-index-and-cell-value-in-function-applied-to-dataframe",
        "title": "How to use row index and cell value in function applied to dataframe?",
        "body": "<p>I have a table similar to this, with the blank spaces being empty strings and the numbers being floats:</p>\n\n<pre><code>   1   2   3   4   5   6\nA  \nB                  8   5\nC      5       7\nD  2   3   5\nE  0\n</code></pre>\n\n<p>I want to replace the value of each cell with the output of a function which takes two arguments: the index of the row and the value of the cell.</p>\n\n<p>For example, the values in the first column should be replaced with the output of func(D, 2) and func(E, 0) and the empty cells should stay empty. The function output is a string.</p>\n\n<p>Expected output table:</p>\n\n<p>if func(D, 2) returns X and func(E, 0) returns Y, then column 1 should look like:</p>\n\n<pre><code>   1   2   3   4   5   6\nA  \nB                  8   5\nC      5       7\nD  X   3   5\nE  Y\n</code></pre>\n\n<p>How do I do this?</p>\n",
        "answer_body": "<p>First of all I would fill the dataframe: </p>\n\n<pre><code>df.fillna(0, inplace=True) \n</code></pre>\n\n<p>then let's assume that your function is called <code>func</code> and it's first argument is row index, you can apply it using the <code>map</code> method while iterating over rows:</p>\n\n<pre><code>for idx in df.index:\n   df.loc[idx] = df.loc[idx].map(lambda x: func(idx, x))\n</code></pre>\n",
        "question_body": "<p>I have a table similar to this, with the blank spaces being empty strings and the numbers being floats:</p>\n\n<pre><code>   1   2   3   4   5   6\nA  \nB                  8   5\nC      5       7\nD  2   3   5\nE  0\n</code></pre>\n\n<p>I want to replace the value of each cell with the output of a function which takes two arguments: the index of the row and the value of the cell.</p>\n\n<p>For example, the values in the first column should be replaced with the output of func(D, 2) and func(E, 0) and the empty cells should stay empty. The function output is a string.</p>\n\n<p>Expected output table:</p>\n\n<p>if func(D, 2) returns X and func(E, 0) returns Y, then column 1 should look like:</p>\n\n<pre><code>   1   2   3   4   5   6\nA  \nB                  8   5\nC      5       7\nD  X   3   5\nE  Y\n</code></pre>\n\n<p>How do I do this?</p>\n",
        "formatted_input": {
            "qid": 58984851,
            "link": "https://stackoverflow.com/questions/58984851/how-to-use-row-index-and-cell-value-in-function-applied-to-dataframe",
            "question": {
                "title": "How to use row index and cell value in function applied to dataframe?",
                "ques_desc": "I have a table similar to this, with the blank spaces being empty strings and the numbers being floats: I want to replace the value of each cell with the output of a function which takes two arguments: the index of the row and the value of the cell. For example, the values in the first column should be replaced with the output of func(D, 2) and func(E, 0) and the empty cells should stay empty. The function output is a string. Expected output table: if func(D, 2) returns X and func(E, 0) returns Y, then column 1 should look like: How do I do this? "
            },
            "io": [
                "   1   2   3   4   5   6\nA  \nB                  8   5\nC      5       7\nD  2   3   5\nE  0\n",
                "   1   2   3   4   5   6\nA  \nB                  8   5\nC      5       7\nD  X   3   5\nE  Y\n"
            ],
            "answer": {
                "ans_desc": "First of all I would fill the dataframe: then let's assume that your function is called and it's first argument is row index, you can apply it using the method while iterating over rows: ",
                "code": [
                    "for idx in df.index:\n   df.loc[idx] = df.loc[idx].map(lambda x: func(idx, x))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "append"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 7114620,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b5e2697603e0745492719dec8faa5ce6?s=128&d=identicon&r=PG&f=1",
            "display_name": "Jonas Jo",
            "link": "https://stackoverflow.com/users/7114620/jonas-jo"
        },
        "is_answered": true,
        "view_count": 94,
        "accepted_answer_id": 58942314,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1574292964,
        "creation_date": 1574194858,
        "last_edit_date": 1574194992,
        "question_id": 58942218,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58942218/pandas-documentation-example-for-append-does-not-work-pandas-dataframe-append",
        "title": "pandas documentation example for append does not work (pandas.DataFrame.append)",
        "body": "<p>I copied the example from the pandas documentation for the append method, but it isn't working for me.\n<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html</a></p>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\ndf.append(df2)\nprint(df)\n</code></pre>\n\n<p>outputs:</p>\n\n<pre><code>   A  B\n0  1  2\n1  3  4\n</code></pre>\n\n<p>and not:</p>\n\n<pre><code>   A  B\n0  1  2\n1  3  4\n0  5  6\n1  7  8\n</code></pre>\n\n<p>What are possible reasons for this? Where is my mistake?</p>\n\n<p>Thanks for your help.</p>\n",
        "answer_body": "<p>Pandas <code>append</code> method returns a concatenated dataframe, it does not concatenate the two dataframes inplace. This means <code>df.append(df2)</code> does not modify the original <code>df</code> but instead returns a new dataframe. The pandas example is in IPython and is correct. This should work:</p>\n\n<pre><code>df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n\ndf = df.append(df2)\n\nprint(df)\n</code></pre>\n\n<p>This will print out</p>\n\n<pre><code>   A  B\n0  1  2\n1  3  4\n0  5  6\n1  7  8\n</code></pre>\n",
        "question_body": "<p>I copied the example from the pandas documentation for the append method, but it isn't working for me.\n<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html</a></p>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\ndf.append(df2)\nprint(df)\n</code></pre>\n\n<p>outputs:</p>\n\n<pre><code>   A  B\n0  1  2\n1  3  4\n</code></pre>\n\n<p>and not:</p>\n\n<pre><code>   A  B\n0  1  2\n1  3  4\n0  5  6\n1  7  8\n</code></pre>\n\n<p>What are possible reasons for this? Where is my mistake?</p>\n\n<p>Thanks for your help.</p>\n",
        "formatted_input": {
            "qid": 58942218,
            "link": "https://stackoverflow.com/questions/58942218/pandas-documentation-example-for-append-does-not-work-pandas-dataframe-append",
            "question": {
                "title": "pandas documentation example for append does not work (pandas.DataFrame.append)",
                "ques_desc": "I copied the example from the pandas documentation for the append method, but it isn't working for me. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html outputs: and not: What are possible reasons for this? Where is my mistake? Thanks for your help. "
            },
            "io": [
                "   A  B\n0  1  2\n1  3  4\n",
                "   A  B\n0  1  2\n1  3  4\n0  5  6\n1  7  8\n"
            ],
            "answer": {
                "ans_desc": "Pandas method returns a concatenated dataframe, it does not concatenate the two dataframes inplace. This means does not modify the original but instead returns a new dataframe. The pandas example is in IPython and is correct. This should work: This will print out ",
                "code": [
                    "df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n\ndf = df.append(df2)\n\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "parsing",
            "datetime",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 627,
            "user_id": 3861925,
            "user_type": "registered",
            "accept_rate": 52,
            "profile_image": "https://www.gravatar.com/avatar/ff06907fe687221adfd550fa3fa1a549?s=128&d=identicon&r=PG&f=1",
            "display_name": "user3861925",
            "link": "https://stackoverflow.com/users/3861925/user3861925"
        },
        "is_answered": true,
        "view_count": 32873,
        "accepted_answer_id": 28990322,
        "answer_count": 1,
        "score": 14,
        "last_activity_date": 1574103953,
        "creation_date": 1426086471,
        "last_edit_date": 1426087071,
        "question_id": 28990256,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/28990256/python-pandas-time-series-year-extraction",
        "title": "python pandas time series year extraction",
        "body": "<p>I have a DF containing timestamps:</p>\n\n<pre><code>0     2005-08-31 16:39:40\n1     2005-12-28 16:00:34\n2     2005-10-21 17:52:10\n3     2014-01-28 12:23:15\n4     2014-01-28 12:23:15\n5     2011-02-04 18:32:34\n6     2011-02-04 18:32:34\n7     2011-02-04 18:32:34\n</code></pre>\n\n<p>I would like to extract the year from each timestamp, creating additional column in the DF that would look like:</p>\n\n<pre><code>0     2005-08-31 16:39:40 2005\n1     2005-12-28 16:00:34 2005\n2     2005-10-21 17:52:10 2005\n3     2014-01-28 12:23:15 2014\n4     2014-01-28 12:23:15 2014\n5     2011-02-04 18:32:34 2011\n6     2011-02-04 18:32:34 2011\n7     2011-02-04 18:32:34 2011\n</code></pre>\n\n<p>Obviously I can go over all DF entries stripping off the first 4 characters of the date. Which is very slow. I wonder if there is a fast python-way to do this. \nI saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime(DF,'%Y-%m-%d %H:%M:%S') but when I try to then apply datetime.datetime.year(DF) it doesn't work. I will also need to parse the timestamps to months and combinations of years-months and so on...\nHelp please.\nThanks.</p>\n",
        "answer_body": "<p>No need to apply a function for each row there is a new <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/series.html#accessors\" rel=\"noreferrer\">datetime</a> accessor you can call to access the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/series.html#api-series-dt\" rel=\"noreferrer\">year</a> property:</p>\n\n<pre><code>In [35]:\n\ndf1['year'] = df1['timestamp'].dt.year\ndf1\nOut[35]:\n            timestamp  year\n0 2005-08-31 16:39:40  2005\n1 2005-12-28 16:00:34  2005\n2 2005-10-21 17:52:10  2005\n3 2014-01-28 12:23:15  2014\n4 2014-01-28 12:23:15  2014\n5 2011-02-04 18:32:34  2011\n6 2011-02-04 18:32:34  2011\n7 2011-02-04 18:32:34  2011\n</code></pre>\n\n<p>If your timestamps are str then you can convert to datetime64 using <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html#pandas.to_datetime\" rel=\"noreferrer\"><code>pd.to_dateime</code></a>:</p>\n\n<pre><code>df['timestamp'] = pd.to_datetime(df['timestamp'])\n</code></pre>\n\n<p>You can access the months and other attributes using <code>dt</code> like the above.</p>\n\n<p>For version prior to <code>0.15.0</code> you can perform the following:</p>\n\n<pre><code>df1['year'] = df1['timestamp'].apply(lambda x: x.year)\n</code></pre>\n",
        "question_body": "<p>I have a DF containing timestamps:</p>\n\n<pre><code>0     2005-08-31 16:39:40\n1     2005-12-28 16:00:34\n2     2005-10-21 17:52:10\n3     2014-01-28 12:23:15\n4     2014-01-28 12:23:15\n5     2011-02-04 18:32:34\n6     2011-02-04 18:32:34\n7     2011-02-04 18:32:34\n</code></pre>\n\n<p>I would like to extract the year from each timestamp, creating additional column in the DF that would look like:</p>\n\n<pre><code>0     2005-08-31 16:39:40 2005\n1     2005-12-28 16:00:34 2005\n2     2005-10-21 17:52:10 2005\n3     2014-01-28 12:23:15 2014\n4     2014-01-28 12:23:15 2014\n5     2011-02-04 18:32:34 2011\n6     2011-02-04 18:32:34 2011\n7     2011-02-04 18:32:34 2011\n</code></pre>\n\n<p>Obviously I can go over all DF entries stripping off the first 4 characters of the date. Which is very slow. I wonder if there is a fast python-way to do this. \nI saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime(DF,'%Y-%m-%d %H:%M:%S') but when I try to then apply datetime.datetime.year(DF) it doesn't work. I will also need to parse the timestamps to months and combinations of years-months and so on...\nHelp please.\nThanks.</p>\n",
        "formatted_input": {
            "qid": 28990256,
            "link": "https://stackoverflow.com/questions/28990256/python-pandas-time-series-year-extraction",
            "question": {
                "title": "python pandas time series year extraction",
                "ques_desc": "I have a DF containing timestamps: I would like to extract the year from each timestamp, creating additional column in the DF that would look like: Obviously I can go over all DF entries stripping off the first 4 characters of the date. Which is very slow. I wonder if there is a fast python-way to do this. I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime(DF,'%Y-%m-%d %H:%M:%S') but when I try to then apply datetime.datetime.year(DF) it doesn't work. I will also need to parse the timestamps to months and combinations of years-months and so on... Help please. Thanks. "
            },
            "io": [
                "0     2005-08-31 16:39:40\n1     2005-12-28 16:00:34\n2     2005-10-21 17:52:10\n3     2014-01-28 12:23:15\n4     2014-01-28 12:23:15\n5     2011-02-04 18:32:34\n6     2011-02-04 18:32:34\n7     2011-02-04 18:32:34\n",
                "0     2005-08-31 16:39:40 2005\n1     2005-12-28 16:00:34 2005\n2     2005-10-21 17:52:10 2005\n3     2014-01-28 12:23:15 2014\n4     2014-01-28 12:23:15 2014\n5     2011-02-04 18:32:34 2011\n6     2011-02-04 18:32:34 2011\n7     2011-02-04 18:32:34 2011\n"
            ],
            "answer": {
                "ans_desc": "No need to apply a function for each row there is a new datetime accessor you can call to access the year property: If your timestamps are str then you can convert to datetime64 using : You can access the months and other attributes using like the above. For version prior to you can perform the following: ",
                "code": [
                    "df1['year'] = df1['timestamp'].apply(lambda x: x.year)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2702,
            "user_id": 6653602,
            "user_type": "registered",
            "accept_rate": 80,
            "profile_image": "https://www.gravatar.com/avatar/308958dfcf4e6e51275ac0a387a24363?s=128&d=identicon&r=PG&f=1",
            "display_name": "Alex T",
            "link": "https://stackoverflow.com/users/6653602/alex-t"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 58821351,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1573571186,
        "creation_date": 1573570489,
        "last_edit_date": 1573570661,
        "question_id": 58821180,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58821180/dropping-rows-that-have-string-cointained-in-other-rows-in-pandas",
        "title": "Dropping rows that have string cointained in other rows in pandas",
        "body": "<p>Given dataframe in this form:</p>\n\n<pre><code> ID      A\n130     Yes\n130-1   Yes\n130-2   Yes\n200     No\n201     No\n201-10  No\n201-101 Yes\n201-22  Yes\n300     No\n</code></pre>\n\n<p>I want to drop the rows that have value from <code>ID</code> column present in another string before the hyphen (<code>-</code>) in other rows\nSo based on this I would drop value <code>201</code> since there are <code>201-10</code>, <code>201-101</code> etc.</p>\n\n<p>Expected output:</p>\n\n<pre><code> ID      A\n130-1   Yes\n130-2   Yes\n200     No\n201-10  No\n201-101 Yes\n201-22  Yes\n300     No\n</code></pre>\n",
        "answer_body": "<p>Using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html\" rel=\"nofollow noreferrer\"><code>duplicated</code></a> and some bitwise operations.  This does rely on the values without hyphens being <em>before</em> the values with hyphens.</p>\n\n<hr>\n\n<pre><code>s = df['ID'].str.split('-').str[0]\nm = s.duplicated(keep=False) ^ s.duplicated()\n\ndf[~m]\n</code></pre>\n\n<p></p>\n\n<pre><code>        ID    A\n1    130-1  Yes\n2    130-2  Yes\n3      200   No\n5   201-10   No\n6  201-101  Yes\n7   201-22  Yes\n8      300   No\n</code></pre>\n",
        "question_body": "<p>Given dataframe in this form:</p>\n\n<pre><code> ID      A\n130     Yes\n130-1   Yes\n130-2   Yes\n200     No\n201     No\n201-10  No\n201-101 Yes\n201-22  Yes\n300     No\n</code></pre>\n\n<p>I want to drop the rows that have value from <code>ID</code> column present in another string before the hyphen (<code>-</code>) in other rows\nSo based on this I would drop value <code>201</code> since there are <code>201-10</code>, <code>201-101</code> etc.</p>\n\n<p>Expected output:</p>\n\n<pre><code> ID      A\n130-1   Yes\n130-2   Yes\n200     No\n201-10  No\n201-101 Yes\n201-22  Yes\n300     No\n</code></pre>\n",
        "formatted_input": {
            "qid": 58821180,
            "link": "https://stackoverflow.com/questions/58821180/dropping-rows-that-have-string-cointained-in-other-rows-in-pandas",
            "question": {
                "title": "Dropping rows that have string cointained in other rows in pandas",
                "ques_desc": "Given dataframe in this form: I want to drop the rows that have value from column present in another string before the hyphen () in other rows So based on this I would drop value since there are , etc. Expected output: "
            },
            "io": [
                " ID      A\n130     Yes\n130-1   Yes\n130-2   Yes\n200     No\n201     No\n201-10  No\n201-101 Yes\n201-22  Yes\n300     No\n",
                " ID      A\n130-1   Yes\n130-2   Yes\n200     No\n201-10  No\n201-101 Yes\n201-22  Yes\n300     No\n"
            ],
            "answer": {
                "ans_desc": "Using and some bitwise operations. This does rely on the values without hyphens being before the values with hyphens. ",
                "code": [
                    "s = df['ID'].str.split('-').str[0]\nm = s.duplicated(keep=False) ^ s.duplicated()\n\ndf[~m]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 145,
            "user_id": 11538729,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-pik8jnSxK4c/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfY5z3ni1gEAC9G9XYzsAI6MQv5bw/mo/photo.jpg?sz=128",
            "display_name": "Mono Neu Elogy",
            "link": "https://stackoverflow.com/users/11538729/mono-neu-elogy"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 58820420,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1573568604,
        "creation_date": 1573567844,
        "question_id": 58820383,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58820383/drop-values-in-specific-condition-in-pandas-dataframe",
        "title": "Drop values in specific condition in pandas dataframe",
        "body": "<p>I have a dataframe like below:</p>\n\n<pre><code>A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70  -0.03    0.55\n51.75   0.29    1.45\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n</code></pre>\n\n<p>now If Column A > 7, I want to drop Column B and C like below:</p>\n\n<pre><code>A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70   NaN     NaN\n51.75   NaN     NaN\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n</code></pre>\n\n<p>How can I achieve that?</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"noreferrer\"><code>DataFrame.mask</code></a> with default value <code>NaN</code> for replace by mask:</p>\n\n<pre><code>df[['B','C']] = df[['B','C']].mask(df.A &gt; 7)\n</code></pre>\n\n<p>Or <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"noreferrer\"><code>DataFrame.loc</code></a> with specify <code>np.nan</code>:</p>\n\n<pre><code>df.loc[df.A &gt; 7, ['B','C']] = np.nan\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n        A     B     C\n0    4.43   NaN  1.11\n1    3.70  0.48  0.79\n2    2.78 -0.29  1.26\n3    1.78  2.90  1.13\n4   40.70   NaN   NaN\n5   51.75   NaN   NaN\n6    3.65  1.74  0.37\n7    2.93  1.56  1.64\n8    3.43   NaN   NaN\n9    2.93   NaN   NaN\n10  10.37   NaN   NaN\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like below:</p>\n\n<pre><code>A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70  -0.03    0.55\n51.75   0.29    1.45\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n</code></pre>\n\n<p>now If Column A > 7, I want to drop Column B and C like below:</p>\n\n<pre><code>A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70   NaN     NaN\n51.75   NaN     NaN\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n</code></pre>\n\n<p>How can I achieve that?</p>\n",
        "formatted_input": {
            "qid": 58820383,
            "link": "https://stackoverflow.com/questions/58820383/drop-values-in-specific-condition-in-pandas-dataframe",
            "question": {
                "title": "Drop values in specific condition in pandas dataframe",
                "ques_desc": "I have a dataframe like below: now If Column A > 7, I want to drop Column B and C like below: How can I achieve that? "
            },
            "io": [
                "A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70  -0.03    0.55\n51.75   0.29    1.45\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n",
                "A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70   NaN     NaN\n51.75   NaN     NaN\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n"
            ],
            "answer": {
                "ans_desc": "Use with default value for replace by mask: Or with specify : ",
                "code": [
                    "df[['B','C']] = df[['B','C']].mask(df.A > 7)\n",
                    "df.loc[df.A > 7, ['B','C']] = np.nan\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 10753488,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-Qlw-QRg4ny0/AAAAAAAAAAI/AAAAAAAADN8/jMRcyw2G57g/photo.jpg?sz=128",
            "display_name": "Tobias To",
            "link": "https://stackoverflow.com/users/10753488/tobias-to"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 58811545,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1573528843,
        "creation_date": 1573526771,
        "last_edit_date": 1573526926,
        "question_id": 58811263,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58811263/how-to-update-the-index-of-df-with-a-new-index",
        "title": "How to update the Index of df with a new Index?",
        "body": "<p>I am currently having one df which has an incomplete Index. \nlike this:</p>\n\n<pre><code>Idx  bar  baz  zoo\n001   A    1    x\n003   B    2    y\n005   C    3    z\n007   A    4    q\n008   B    5    w\n009   C    6    t\n</code></pre>\n\n<p>I have the complete <code>Index([001, 002, ...... 010])</code>. \nWould like to how to supplement the complete Index into the incomplete df. </p>\n\n<pre><code>Idx  bar  baz  zoo\n001   A    1    x\n002  nan  nan  nan\n003   B    2    y\n004  nan  nan  nan\n005   C    3    z\n006  nan  nan  nan\n007   A    4    q\n008   B    5    w\n009   C    6    t \n010  nan  nan  nan\n</code></pre>\n\n<p>The <code>nan</code> can be \"\", the purpose is for me to identify which case I am currently missing.\nIt's the first time I ask question on stackover, apology for the poor formatting.</p>\n",
        "answer_body": "<p>you can do this easily by using the pandas df reindex method..</p>\n\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html\" rel=\"nofollow noreferrer\">df.reindex</a></p>\n\n<p>all you have to do is supply a list to be used as the new index i.e.</p>\n\n<pre><code>full_index = ['001','002','003','004','005','006','007','008','009','010']  \n</code></pre>\n\n<p>then pass this into the reindex method like this:</p>\n\n<pre><code>df = df.reindex(full_index)\n</code></pre>\n\n<p>the method will automatically put nan values into the rows with indices that were not in the original index...</p>\n\n<p>e.g.:</p>\n\n<pre><code>df = pd.DataFrame({'bar':['A','B','C','A','B','C'],'baz':[1,2,3,4,5,6],'zoo':['x','y','z','q','w','t']}, index = ['001','003','005','007','008','009']) #your original df\nfull_index = ['001','002','003','004','005','006','007','008','009','010']  \ndf = df.reindex(full_index)\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>     bar  baz  zoo\n001    A  1.0    x\n002  NaN  NaN  NaN\n003    B  2.0    y\n004  NaN  NaN  NaN\n005    C  3.0    z\n006  NaN  NaN  NaN\n007    A  4.0    q\n008    B  5.0    w\n009    C  6.0    t\n010  NaN  NaN  NaN\n</code></pre>\n",
        "question_body": "<p>I am currently having one df which has an incomplete Index. \nlike this:</p>\n\n<pre><code>Idx  bar  baz  zoo\n001   A    1    x\n003   B    2    y\n005   C    3    z\n007   A    4    q\n008   B    5    w\n009   C    6    t\n</code></pre>\n\n<p>I have the complete <code>Index([001, 002, ...... 010])</code>. \nWould like to how to supplement the complete Index into the incomplete df. </p>\n\n<pre><code>Idx  bar  baz  zoo\n001   A    1    x\n002  nan  nan  nan\n003   B    2    y\n004  nan  nan  nan\n005   C    3    z\n006  nan  nan  nan\n007   A    4    q\n008   B    5    w\n009   C    6    t \n010  nan  nan  nan\n</code></pre>\n\n<p>The <code>nan</code> can be \"\", the purpose is for me to identify which case I am currently missing.\nIt's the first time I ask question on stackover, apology for the poor formatting.</p>\n",
        "formatted_input": {
            "qid": 58811263,
            "link": "https://stackoverflow.com/questions/58811263/how-to-update-the-index-of-df-with-a-new-index",
            "question": {
                "title": "How to update the Index of df with a new Index?",
                "ques_desc": "I am currently having one df which has an incomplete Index. like this: I have the complete . Would like to how to supplement the complete Index into the incomplete df. The can be \"\", the purpose is for me to identify which case I am currently missing. It's the first time I ask question on stackover, apology for the poor formatting. "
            },
            "io": [
                "Idx  bar  baz  zoo\n001   A    1    x\n003   B    2    y\n005   C    3    z\n007   A    4    q\n008   B    5    w\n009   C    6    t\n",
                "Idx  bar  baz  zoo\n001   A    1    x\n002  nan  nan  nan\n003   B    2    y\n004  nan  nan  nan\n005   C    3    z\n006  nan  nan  nan\n007   A    4    q\n008   B    5    w\n009   C    6    t \n010  nan  nan  nan\n"
            ],
            "answer": {
                "ans_desc": "you can do this easily by using the pandas df reindex method.. df.reindex all you have to do is supply a list to be used as the new index i.e. then pass this into the reindex method like this: the method will automatically put nan values into the rows with indices that were not in the original index... e.g.: output: ",
                "code": [
                    "df = pd.DataFrame({'bar':['A','B','C','A','B','C'],'baz':[1,2,3,4,5,6],'zoo':['x','y','z','q','w','t']}, index = ['001','003','005','007','008','009']) #your original df\nfull_index = ['001','002','003','004','005','006','007','008','009','010']  \ndf = df.reindex(full_index)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 43,
            "user_id": 12344502,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/442ab6a02c0fbce9bd37c388d0f117ef?s=128&d=identicon&r=PG&f=1",
            "display_name": "lnper00",
            "link": "https://stackoverflow.com/users/12344502/lnper00"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 58771800,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1573237547,
        "creation_date": 1573235941,
        "question_id": 58771645,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58771645/group-identical-consecutive-values-in-pandas-dataframe",
        "title": "Group identical consecutive values in pandas DataFrame",
        "body": "<p>I have the following pandas dataframe :</p>\n\n<pre><code>   a\n0  0\n1  0\n2  1\n3  2\n4  2\n5  2\n6  3\n7  2\n8  2\n9  1\n</code></pre>\n\n<p>I want to store the values in another dataframe such as every group of consecutive indentical values make a labeled group like this : </p>\n\n<pre><code>   A  B\n0  0  2\n1  1  1\n2  2  3\n3  3  1\n4  2  2\n5  1  1\n</code></pre>\n\n<p>The column A represent the value of the group and B represents the number of occurences.</p>\n\n<p>this is what i've done so far:</p>\n\n<pre><code>df = pd.DataFrame({'a':[0,0,1,2,2,2,3,2,2,1]})\ndf2 = pd.DataFrame()\nfor i,g in df.groupby([(df.a != df.a.shift()).cumsum()]):\n    vc = g.a.value_counts()\n    df2 = df2.append({'A':vc.index[0], 'B': vc.iloc[0]}, ignore_index=True).astype(int)\n</code></pre>\n\n<p>It works but it's a bit messy. </p>\n\n<p>Do you think of a shortest/better way of doing this  ?</p>\n",
        "answer_body": "<p>use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.agg.html\" rel=\"nofollow noreferrer\"><code>GrouBy.agg</code></a> <strong>in Pandas >0.25.0</strong>:</p>\n\n<pre><code>new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .agg(A=('a','first'),B=('a','count')) )\n\nprint(new_df)\n</code></pre>\n\n<hr>\n\n<pre><code>   A  B\n0  0  2\n1  1  1\n2  2  3\n3  3  1\n4  2  2\n5  1  1\n</code></pre>\n\n<hr>\n\n<p><strong>pandas &lt;0.25.0</strong></p>\n\n<pre><code>new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .a\n            .agg({'A':'first','B':'count'}) )\n</code></pre>\n",
        "question_body": "<p>I have the following pandas dataframe :</p>\n\n<pre><code>   a\n0  0\n1  0\n2  1\n3  2\n4  2\n5  2\n6  3\n7  2\n8  2\n9  1\n</code></pre>\n\n<p>I want to store the values in another dataframe such as every group of consecutive indentical values make a labeled group like this : </p>\n\n<pre><code>   A  B\n0  0  2\n1  1  1\n2  2  3\n3  3  1\n4  2  2\n5  1  1\n</code></pre>\n\n<p>The column A represent the value of the group and B represents the number of occurences.</p>\n\n<p>this is what i've done so far:</p>\n\n<pre><code>df = pd.DataFrame({'a':[0,0,1,2,2,2,3,2,2,1]})\ndf2 = pd.DataFrame()\nfor i,g in df.groupby([(df.a != df.a.shift()).cumsum()]):\n    vc = g.a.value_counts()\n    df2 = df2.append({'A':vc.index[0], 'B': vc.iloc[0]}, ignore_index=True).astype(int)\n</code></pre>\n\n<p>It works but it's a bit messy. </p>\n\n<p>Do you think of a shortest/better way of doing this  ?</p>\n",
        "formatted_input": {
            "qid": 58771645,
            "link": "https://stackoverflow.com/questions/58771645/group-identical-consecutive-values-in-pandas-dataframe",
            "question": {
                "title": "Group identical consecutive values in pandas DataFrame",
                "ques_desc": "I have the following pandas dataframe : I want to store the values in another dataframe such as every group of consecutive indentical values make a labeled group like this : The column A represent the value of the group and B represents the number of occurences. this is what i've done so far: It works but it's a bit messy. Do you think of a shortest/better way of doing this ? "
            },
            "io": [
                "   a\n0  0\n1  0\n2  1\n3  2\n4  2\n5  2\n6  3\n7  2\n8  2\n9  1\n",
                "   A  B\n0  0  2\n1  1  1\n2  2  3\n3  3  1\n4  2  2\n5  1  1\n"
            ],
            "answer": {
                "ans_desc": "use in Pandas >0.25.0: pandas <0.25.0 ",
                "code": [
                    "new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .agg(A=('a','first'),B=('a','count')) )\n\nprint(new_df)\n",
                    "new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .a\n            .agg({'A':'first','B':'count'}) )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "subset"
        ],
        "owner": {
            "reputation": 5073,
            "user_id": 3396911,
            "user_type": "registered",
            "accept_rate": 96,
            "profile_image": "https://i.stack.imgur.com/JJTmY.jpg?s=128&g=1",
            "display_name": "hernanavella",
            "link": "https://stackoverflow.com/users/3396911/hernanavella"
        },
        "is_answered": true,
        "view_count": 243,
        "accepted_answer_id": 58758157,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1573168832,
        "creation_date": 1573167702,
        "question_id": 58758098,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58758098/how-can-i-remove-columns-of-pandas-dataframe-conditional-on-last-row-values",
        "title": "How can I remove columns of pandas dataframe conditional on last row values?",
        "body": "<p>Given a data-frame like:</p>\n\n<pre><code>             A  B  C\n2019-11-02  120 25 11\n2019-11-03  119 28 15\n2019-11-04  115 23 18\n2019-11-05  119 30 20\n2019-11-06  121 32 25\n2019-11-07  117 24 30\n</code></pre>\n\n<p>I would like to remove the columns in which the value of the last row is less than (&lt;) a constant X,  say X = 25. In this example It would remove the column B only and the output would be:</p>\n\n<pre><code>             A  C\n2019-11-02  120 11\n2019-11-03  119 15\n2019-11-04  115 18\n2019-11-05  119 20\n2019-11-06  121 25\n2019-11-07  117 30\n</code></pre>\n\n<p>Thanks</p>\n",
        "answer_body": "<h3>Method 1:</h3>\n\n<p>Use <code>iloc</code>, <code>lt</code>, and <code>drop</code>:</p>\n\n<p>We select the last row with <code>iloc[-1]</code>, then check which column is <code>less than (lt)</code> <code>25</code> and pass that column to <code>DataFrame.drop</code></p>\n\n<pre><code>df = df.drop(columns = df.columns[df.iloc[-1].lt(25)])\n</code></pre>\n\n<h3>Method 2:</h3>\n\n<p>Using <code>tail</code>, <code>iloc</code>, <code>gt</code> and <code>all</code>:</p>\n\n<pre><code>df = df.loc[:, df.tail(1).gt(25).all()]\n</code></pre>\n\n<pre><code>              A   C\n2019-11-02  120  11\n2019-11-03  119  15\n2019-11-04  115  18\n2019-11-05  119  20\n2019-11-06  121  25\n2019-11-07  117  30\n</code></pre>\n\n<hr>\n\n<p><strong>Step by step method 1</strong>:</p>\n\n<pre><code># select last row\n\ndf.iloc[-1]\n\nA    117\nB     24\nC     30\nName: 2019-11-07, dtype: int64\n</code></pre>\n\n<pre><code># check which columns have value &lt; 25:\n\ndf.iloc[-1].lt(25)\n\nA    False\nB     True\nC    False\nName: 2019-11-07, dtype: bool\n</code></pre>\n\n<pre><code># select those column(s) with boolean indexing:\n\ndf.columns[df.iloc[-1].lt(25)]\n\nIndex(['B'], dtype='object')\n</code></pre>\n\n<pre><code># finally pass it DataFrame.drop\n\ndf.drop(columns = df.columns[df.iloc[-1].lt(25)])\n\n              A   C\n2019-11-02  120  11\n2019-11-03  119  15\n2019-11-04  115  18\n2019-11-05  119  20\n2019-11-06  121  25\n2019-11-07  117  30\n</code></pre>\n",
        "question_body": "<p>Given a data-frame like:</p>\n\n<pre><code>             A  B  C\n2019-11-02  120 25 11\n2019-11-03  119 28 15\n2019-11-04  115 23 18\n2019-11-05  119 30 20\n2019-11-06  121 32 25\n2019-11-07  117 24 30\n</code></pre>\n\n<p>I would like to remove the columns in which the value of the last row is less than (&lt;) a constant X,  say X = 25. In this example It would remove the column B only and the output would be:</p>\n\n<pre><code>             A  C\n2019-11-02  120 11\n2019-11-03  119 15\n2019-11-04  115 18\n2019-11-05  119 20\n2019-11-06  121 25\n2019-11-07  117 30\n</code></pre>\n\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 58758098,
            "link": "https://stackoverflow.com/questions/58758098/how-can-i-remove-columns-of-pandas-dataframe-conditional-on-last-row-values",
            "question": {
                "title": "How can I remove columns of pandas dataframe conditional on last row values?",
                "ques_desc": "Given a data-frame like: I would like to remove the columns in which the value of the last row is less than (<) a constant X, say X = 25. In this example It would remove the column B only and the output would be: Thanks "
            },
            "io": [
                "             A  B  C\n2019-11-02  120 25 11\n2019-11-03  119 28 15\n2019-11-04  115 23 18\n2019-11-05  119 30 20\n2019-11-06  121 32 25\n2019-11-07  117 24 30\n",
                "             A  C\n2019-11-02  120 11\n2019-11-03  119 15\n2019-11-04  115 18\n2019-11-05  119 20\n2019-11-06  121 25\n2019-11-07  117 30\n"
            ],
            "answer": {
                "ans_desc": "Method 1: Use , , and : We select the last row with , then check which column is and pass that column to Method 2: Using , , and : Step by step method 1: ",
                "code": [
                    "df = df.drop(columns = df.columns[df.iloc[-1].lt(25)])\n",
                    "# select last row\n\ndf.iloc[-1]\n\nA    117\nB     24\nC     30\nName: 2019-11-07, dtype: int64\n",
                    "# check which columns have value < 25:\n\ndf.iloc[-1].lt(25)\n\nA    False\nB     True\nC    False\nName: 2019-11-07, dtype: bool\n",
                    "# select those column(s) with boolean indexing:\n\ndf.columns[df.iloc[-1].lt(25)]\n\nIndex(['B'], dtype='object')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 105,
            "user_id": 8682533,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/53b0402cc20e361834230eaa096a6dc4?s=128&d=identicon&r=PG&f=1",
            "display_name": "Saturate",
            "link": "https://stackoverflow.com/users/8682533/saturate"
        },
        "is_answered": true,
        "view_count": 324,
        "accepted_answer_id": 46451515,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1572889139,
        "creation_date": 1506524643,
        "last_edit_date": 1527651673,
        "question_id": 46451284,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46451284/groupby-and-perform-row-wise-calculation-using-a-custom-function",
        "title": "Groupby and perform row-wise calculation using a custom function",
        "body": "<p>Following on from this question: <a href=\"https://stackoverflow.com/questions/46446863/python-group-by-and-add-new-row-which-is-calculation-of-other-rows/46447447#\">python - Group by and add new row which is calculation of other rows</a></p>\n\n<p>I have a pandas dataframe as follows:</p>\n\n<pre><code>col_1   col_2   col_3  col_4\na       X        5      1\na       Y        3      2\na       Z        6      4\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\n</code></pre>\n\n<p>And I want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to X and Z from col_2 and create a new row with these values. So the output would be as below:</p>\n\n<pre><code>col_1   col_2   col_3  col_4 \na       X        5      1\na       Y        3      2\na       Z        6      4\na       NEW      *      *\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\nb       NEW      *      *\n</code></pre>\n\n<p>Where <code>*</code> are the outputs of the function.</p>\n\n<p>Original question (which only requires a simple addition) was answered with:</p>\n\n<pre><code>new = df[df.col_2.isin(['X', 'Z'])]\\\n  .groupby(['col_1'], as_index=False).sum()\\\n  .assign(col_2='NEW')\n\ndf = pd.concat([df, new]).sort_values('col_1')\n</code></pre>\n\n<p>I'm now looking for a way to use a custom function, such as <code>(X/Y)</code> or <code>((X+Y)*2)</code>, rather than <code>X+Y</code>. How can I modify this code to work with my new requirements?</p>\n",
        "answer_body": "<p>I'm not sure if this is what you're looking for, but here goes:</p>\n\n<pre><code>def f(x):\n    y = x.values\n    return y[0] / y[1] # replace with your function\n</code></pre>\n\n<p>And, the change to <code>new</code> is:</p>\n\n<pre><code>new = (\n    df[df.col_2.isin(['X', 'Z'])]\n      .groupby(['col_1'], as_index=False)[['col_3', 'col_4']]\n      .agg(f)\n      .assign(col_2='NEW')\n)\n\n  col_1     col_3  col_4 col_2\n0     a  0.833333   0.25   NEW\n1     b  1.166667   1.60   NEW\n\ndf = pd.concat([df, new]).sort_values('col_1')\n\ndf\n  col_1 col_2     col_3  col_4\n0     a     X  5.000000   1.00\n1     a     Y  3.000000   2.00\n2     a     Z  6.000000   4.00\n0     a   NEW  0.833333   0.25\n3     b     X  7.000000   8.00\n4     b     Y  4.000000   3.00\n5     b     Z  6.000000   5.00\n1     b   NEW  1.166667   1.60\n</code></pre>\n\n<p>I'm taking a leap of faith in <code>f</code> and assuming those columns are sorted before they hit the function. If this isn't the case, an additional <code>sort_values</code> call is needed:</p>\n\n<pre><code>df = df.sort_values(['col_1, 'col_2'])\n</code></pre>\n\n<p>Should do the trick.</p>\n",
        "question_body": "<p>Following on from this question: <a href=\"https://stackoverflow.com/questions/46446863/python-group-by-and-add-new-row-which-is-calculation-of-other-rows/46447447#\">python - Group by and add new row which is calculation of other rows</a></p>\n\n<p>I have a pandas dataframe as follows:</p>\n\n<pre><code>col_1   col_2   col_3  col_4\na       X        5      1\na       Y        3      2\na       Z        6      4\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\n</code></pre>\n\n<p>And I want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to X and Z from col_2 and create a new row with these values. So the output would be as below:</p>\n\n<pre><code>col_1   col_2   col_3  col_4 \na       X        5      1\na       Y        3      2\na       Z        6      4\na       NEW      *      *\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\nb       NEW      *      *\n</code></pre>\n\n<p>Where <code>*</code> are the outputs of the function.</p>\n\n<p>Original question (which only requires a simple addition) was answered with:</p>\n\n<pre><code>new = df[df.col_2.isin(['X', 'Z'])]\\\n  .groupby(['col_1'], as_index=False).sum()\\\n  .assign(col_2='NEW')\n\ndf = pd.concat([df, new]).sort_values('col_1')\n</code></pre>\n\n<p>I'm now looking for a way to use a custom function, such as <code>(X/Y)</code> or <code>((X+Y)*2)</code>, rather than <code>X+Y</code>. How can I modify this code to work with my new requirements?</p>\n",
        "formatted_input": {
            "qid": 46451284,
            "link": "https://stackoverflow.com/questions/46451284/groupby-and-perform-row-wise-calculation-using-a-custom-function",
            "question": {
                "title": "Groupby and perform row-wise calculation using a custom function",
                "ques_desc": "Following on from this question: python - Group by and add new row which is calculation of other rows I have a pandas dataframe as follows: And I want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to X and Z from col_2 and create a new row with these values. So the output would be as below: Where are the outputs of the function. Original question (which only requires a simple addition) was answered with: I'm now looking for a way to use a custom function, such as or , rather than . How can I modify this code to work with my new requirements? "
            },
            "io": [
                "col_1   col_2   col_3  col_4\na       X        5      1\na       Y        3      2\na       Z        6      4\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\n",
                "col_1   col_2   col_3  col_4 \na       X        5      1\na       Y        3      2\na       Z        6      4\na       NEW      *      *\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\nb       NEW      *      *\n"
            ],
            "answer": {
                "ans_desc": "I'm not sure if this is what you're looking for, but here goes: And, the change to is: I'm taking a leap of faith in and assuming those columns are sorted before they hit the function. If this isn't the case, an additional call is needed: Should do the trick. ",
                "code": [
                    "def f(x):\n    y = x.values\n    return y[0] / y[1] # replace with your function\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 5293,
            "user_id": 8410477,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dVgCN.jpg?s=128&g=1",
            "display_name": "ah bon",
            "link": "https://stackoverflow.com/users/8410477/ah-bon"
        },
        "is_answered": true,
        "view_count": 27,
        "closed_date": 1572574314,
        "accepted_answer_id": 58653301,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1572574133,
        "creation_date": 1572574072,
        "question_id": 58653299,
        "link": "https://stackoverflow.com/questions/58653299/convert-standard-date-format-to-string-splitting-by-point-in-python",
        "closed_reason": "Duplicate",
        "title": "Convert standard date format to string splitting by point in Python",
        "body": "<p>One <code>df</code> have one column <code>date</code> which has the following format:</p>\n\n<pre><code>0         2019/5/20 22:49:29\n1         2019/5/20 23:18:23\n2           2019/3/8 9:11:35\n3           2019/3/8 9:19:58\n4         2019/5/20 22:57:12\n5           2019/3/8 9:06:41\n</code></pre>\n\n<p>How can I convert it to format six digits <code>year.month.day</code> format? I have tried with the following code to but only get <code>year-month-day</code>:</p>\n\n<pre><code>df['date'] = pd.to_datetime(df['date']).dt.date\n</code></pre>\n\n<p>But my desired output:</p>\n\n<pre><code>0         19.05.20\n1         19.05.20\n2         19.03.08\n3         19.03.08\n4         19.05.20\n5         19.03.08\n</code></pre>\n\n<p>Thank you.</p>\n",
        "answer_body": "<p>If you want strings:</p>\n\n<pre><code>df['date'] = df['date'].dt.strftime('%y.%m.%d')\n\n# if your date is not datetime type:\n# df['date'] = pd.to_datetime(df['date']).dt.strftime('%y.%m.%d')\n</code></pre>\n",
        "question_body": "<p>One <code>df</code> have one column <code>date</code> which has the following format:</p>\n\n<pre><code>0         2019/5/20 22:49:29\n1         2019/5/20 23:18:23\n2           2019/3/8 9:11:35\n3           2019/3/8 9:19:58\n4         2019/5/20 22:57:12\n5           2019/3/8 9:06:41\n</code></pre>\n\n<p>How can I convert it to format six digits <code>year.month.day</code> format? I have tried with the following code to but only get <code>year-month-day</code>:</p>\n\n<pre><code>df['date'] = pd.to_datetime(df['date']).dt.date\n</code></pre>\n\n<p>But my desired output:</p>\n\n<pre><code>0         19.05.20\n1         19.05.20\n2         19.03.08\n3         19.03.08\n4         19.05.20\n5         19.03.08\n</code></pre>\n\n<p>Thank you.</p>\n",
        "formatted_input": {
            "qid": 58653299,
            "link": "https://stackoverflow.com/questions/58653299/convert-standard-date-format-to-string-splitting-by-point-in-python",
            "question": {
                "title": "Convert standard date format to string splitting by point in Python",
                "ques_desc": "One have one column which has the following format: How can I convert it to format six digits format? I have tried with the following code to but only get : But my desired output: Thank you. "
            },
            "io": [
                "0         2019/5/20 22:49:29\n1         2019/5/20 23:18:23\n2           2019/3/8 9:11:35\n3           2019/3/8 9:19:58\n4         2019/5/20 22:57:12\n5           2019/3/8 9:06:41\n",
                "0         19.05.20\n1         19.05.20\n2         19.03.08\n3         19.03.08\n4         19.05.20\n5         19.03.08\n"
            ],
            "answer": {
                "ans_desc": "If you want strings: ",
                "code": [
                    "df['date'] = df['date'].dt.strftime('%y.%m.%d')\n\n# if your date is not datetime type:\n# df['date'] = pd.to_datetime(df['date']).dt.strftime('%y.%m.%d')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 115,
            "user_id": 5060866,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a981cb0e2a08510fe5fbbf6ea9ef408e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mat",
            "link": "https://stackoverflow.com/users/5060866/mat"
        },
        "is_answered": true,
        "view_count": 337,
        "accepted_answer_id": 58649517,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1572548479,
        "creation_date": 1572545008,
        "question_id": 58649009,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58649009/write-pandas-dataframe-to-csv-in-columns-with-trailing-zeros",
        "title": "Write pandas dataframe to_csv in columns with trailing zeros",
        "body": "<p>I have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns).</p>\n\n<p>The complicating factor is I also want each column to be rounded to different number of decimals (some need much higher accuracy).</p>\n\n<h3>To reproduce:</h3>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame( [[1.00000, 3.00000, 5.00000],\n                    [1.45454, 3.45454, 5.45454]] )\n\ndf_rounded = df.round( {0:1, 1:3, 2:5} )\ndf_rounded.to_csv('out.txt', sep=' ', header=False)\n</code></pre>\n\n<h3>Current result for out.txt:</h3>\n\n<pre><code>0 1.0 3.0 5.0\n1 1.5 3.455 5.45454\n\n</code></pre>\n\n<h3>Desired:</h3>\n\n<pre><code>0 1.0 3.000 5.00000\n1 1.5 3.455 5.45454\n</code></pre>\n",
        "answer_body": "<p>You can get the string representation of the dataframe using <code>df.to_string()</code> (<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_string.html\" rel=\"nofollow noreferrer\">docs</a>). Then simply write this string to a text file.</p>\n\n<p>This method also has <code>col_space</code> parameter to further adjust the spacing between columns. </p>\n\n<p>Example:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>with open('out.txt', 'w') as file:\n    file.writelines(df_rounded.to_string(header=False))\n</code></pre>\n\n<p>Outputs:</p>\n\n<pre><code>0  1.0  3.000  5.00000\n1  1.5  3.455  5.45454\n</code></pre>\n\n<hr>\n\n<p>There is pandas <code>df.to_csv(float_format='%.5f')</code> (<a href=\"https://stackoverflow.com/questions/12877189/float64-with-pandas-to-csv\">more info</a>) but it applies the formatting to values in all columns. Whereas, in your case, you need different formatting for each column.</p>\n",
        "question_body": "<p>I have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns).</p>\n\n<p>The complicating factor is I also want each column to be rounded to different number of decimals (some need much higher accuracy).</p>\n\n<h3>To reproduce:</h3>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame( [[1.00000, 3.00000, 5.00000],\n                    [1.45454, 3.45454, 5.45454]] )\n\ndf_rounded = df.round( {0:1, 1:3, 2:5} )\ndf_rounded.to_csv('out.txt', sep=' ', header=False)\n</code></pre>\n\n<h3>Current result for out.txt:</h3>\n\n<pre><code>0 1.0 3.0 5.0\n1 1.5 3.455 5.45454\n\n</code></pre>\n\n<h3>Desired:</h3>\n\n<pre><code>0 1.0 3.000 5.00000\n1 1.5 3.455 5.45454\n</code></pre>\n",
        "formatted_input": {
            "qid": 58649009,
            "link": "https://stackoverflow.com/questions/58649009/write-pandas-dataframe-to-csv-in-columns-with-trailing-zeros",
            "question": {
                "title": "Write pandas dataframe to_csv in columns with trailing zeros",
                "ques_desc": "I have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns). The complicating factor is I also want each column to be rounded to different number of decimals (some need much higher accuracy). To reproduce: Current result for out.txt: Desired: "
            },
            "io": [
                "0 1.0 3.0 5.0\n1 1.5 3.455 5.45454\n\n",
                "0 1.0 3.000 5.00000\n1 1.5 3.455 5.45454\n"
            ],
            "answer": {
                "ans_desc": "You can get the string representation of the dataframe using (docs). Then simply write this string to a text file. This method also has parameter to further adjust the spacing between columns. Example: Outputs: There is pandas (more info) but it applies the formatting to values in all columns. Whereas, in your case, you need different formatting for each column. ",
                "code": [
                    "with open('out.txt', 'w') as file:\n    file.writelines(df_rounded.to_string(header=False))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "rounding"
        ],
        "owner": {
            "reputation": 519,
            "user_id": 12231431,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-yplJHX1tNto/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfvAF3HIKjtY2X4NHzhZQ3oWecsvA/photo.jpg?sz=128",
            "display_name": "Salih",
            "link": "https://stackoverflow.com/users/12231431/salih"
        },
        "is_answered": true,
        "view_count": 58,
        "accepted_answer_id": 58540190,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1571915889,
        "creation_date": 1571914279,
        "question_id": 58539757,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58539757/python-round-dataframe-columns-with-specific-value-if-exists",
        "title": "Python Round Dataframe Columns with Specific Value If Exists",
        "body": "<p>My input dataframe;</p>\n\n<pre><code>A       B \n0.3     0.6\n0.4     3.05\n1.6     4.35\n0.15    5.47\n4.19    9.99\n</code></pre>\n\n<p>I want to round my dataframe columns according to a specifi value if exists. My code is like below;</p>\n\n<pre><code>rounding=0.25\n\ndf['A']=round(df['A'] - rounding + 0.5)\ndf['B']=round(df['B'] - rounding + 0.5)\n</code></pre>\n\n<p>Output is;</p>\n\n<pre><code>A    B\n1    1\n1    3\n2    5\n0    6\n4    10\n</code></pre>\n\n<p>The issue is if there is no \"rounding\" variable, it should be run automatically as default (0.5).\nI need a code that can run for both together. Something like this or different;</p>\n\n<pre><code>if rounding==rounding:\n    df['A']=round(df['A'] - rounding + 0.5)\n\nelse:\n    df['A']=round(df['A'])\n</code></pre>\n\n<p>I saw many topics about rounding with specific value but i couldn' t see for this.</p>\n\n<p>Could you please help me about this?</p>\n",
        "answer_body": "<p>I believe you think missing values - then replace it to <code>0</code> or another scalar  with <code>if-else</code> statement:</p>\n\n<pre><code>rounding = np.nan\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>rounding = 0 if pd.isna(rounding) else rounding\nprint (rounding)\n0\n</code></pre>\n\n<p>If exist value (not missing value):</p>\n\n<pre><code>rounding = 0.25\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0.25\n</code></pre>\n\n<hr>\n\n<pre><code>df['A']=round(df['A'] - rounding + 0.5)\ndf['B']=round(df['B'] - rounding + 0.5)\nprint (df)\n     A     B\n0  1.0   1.0\n1  1.0   4.0\n2  2.0   5.0\n3  1.0   6.0\n4  5.0  10.0\n</code></pre>\n",
        "question_body": "<p>My input dataframe;</p>\n\n<pre><code>A       B \n0.3     0.6\n0.4     3.05\n1.6     4.35\n0.15    5.47\n4.19    9.99\n</code></pre>\n\n<p>I want to round my dataframe columns according to a specifi value if exists. My code is like below;</p>\n\n<pre><code>rounding=0.25\n\ndf['A']=round(df['A'] - rounding + 0.5)\ndf['B']=round(df['B'] - rounding + 0.5)\n</code></pre>\n\n<p>Output is;</p>\n\n<pre><code>A    B\n1    1\n1    3\n2    5\n0    6\n4    10\n</code></pre>\n\n<p>The issue is if there is no \"rounding\" variable, it should be run automatically as default (0.5).\nI need a code that can run for both together. Something like this or different;</p>\n\n<pre><code>if rounding==rounding:\n    df['A']=round(df['A'] - rounding + 0.5)\n\nelse:\n    df['A']=round(df['A'])\n</code></pre>\n\n<p>I saw many topics about rounding with specific value but i couldn' t see for this.</p>\n\n<p>Could you please help me about this?</p>\n",
        "formatted_input": {
            "qid": 58539757,
            "link": "https://stackoverflow.com/questions/58539757/python-round-dataframe-columns-with-specific-value-if-exists",
            "question": {
                "title": "Python Round Dataframe Columns with Specific Value If Exists",
                "ques_desc": "My input dataframe; I want to round my dataframe columns according to a specifi value if exists. My code is like below; Output is; The issue is if there is no \"rounding\" variable, it should be run automatically as default (0.5). I need a code that can run for both together. Something like this or different; I saw many topics about rounding with specific value but i couldn' t see for this. Could you please help me about this? "
            },
            "io": [
                "A       B \n0.3     0.6\n0.4     3.05\n1.6     4.35\n0.15    5.47\n4.19    9.99\n",
                "A    B\n1    1\n1    3\n2    5\n0    6\n4    10\n"
            ],
            "answer": {
                "ans_desc": "I believe you think missing values - then replace it to or another scalar with statement: Or: If exist value (not missing value): ",
                "code": [
                    "rounding = np.nan\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0\n",
                    "rounding = 0 if pd.isna(rounding) else rounding\nprint (rounding)\n0\n",
                    "rounding = 0.25\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0.25\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 58538888,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1571912045,
        "creation_date": 1571911184,
        "question_id": 58538845,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58538845/replace-repetitive-number-with-nan-values-except-the-first-in-pandas-column",
        "title": "Replace repetitive number with NAN values except the first, in pandas column",
        "body": "<p>I have a data frame like this,</p>\n\n<pre><code>df\ncol1    col2\n  1       A\n  2       A\n  3       B\n  4       C\n  5       C\n  6       C\n  7       B\n  8       B\n  9       A\n</code></pre>\n\n<p>Now we can see that there is continuous occurrence of A, B and C. I want only the rows where the occurrence is starting. And the other values of the same occurrence will be nan.</p>\n\n<p>The final data frame I am looking for will look like,</p>\n\n<pre><code>df\ncol1    col2\n  1       A\n  2       NA\n  3       B\n  4       C\n  5       NA\n  6       NA\n  7       B\n  8       NA\n  9       A\n</code></pre>\n\n<p>I can do it using for loop and comparing, But the execution time will be more. I am looking for pythonic way to do it. Some panda shortcuts may be.</p>\n",
        "answer_body": "<p>Compare by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html\" rel=\"nofollow noreferrer\"><code>Series.shift</code></a>ed values and missing values by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.where.html\" rel=\"nofollow noreferrer\"><code>Series.where</code></a> or <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\" rel=\"nofollow noreferrer\"><code>numpy.where</code></a>:</p>\n\n<pre><code>df['col2'] = df['col2'].where(df['col2'].ne(df['col2'].shift()))\n#alternative\n#df['col2'] = np.where(df['col2'].ne(df['col2'].shift()), df['col2'], np.nan)\n</code></pre>\n\n<p>Or by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>DataFrame.loc</code></a> with inverted condition by <code>~</code>:</p>\n\n<pre><code>df.loc[~df['col2'].ne(df['col2'].shift()), 'col2'] = np.nan\n</code></pre>\n\n<p>Or thanks @Daniel Mesejo - use <code>eq</code> for <code>==</code>:</p>\n\n<pre><code>df.loc[df['col2'].eq(df['col2'].shift()), 'col2'] = np.nan\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n   col1 col2\n0     1    A\n1     2  NaN\n2     3    B\n3     4    C\n4     5  NaN\n5     6  NaN\n6     7    B\n7     8  NaN\n8     9    A\n</code></pre>\n\n<p><strong>Detail</strong>:</p>\n\n<pre><code>print (df['col2'].ne(df['col2'].shift()))\n0     True\n1    False\n2     True\n3     True\n4    False\n5    False\n6     True\n7    False\n8     True\nName: col2, dtype: bool\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this,</p>\n\n<pre><code>df\ncol1    col2\n  1       A\n  2       A\n  3       B\n  4       C\n  5       C\n  6       C\n  7       B\n  8       B\n  9       A\n</code></pre>\n\n<p>Now we can see that there is continuous occurrence of A, B and C. I want only the rows where the occurrence is starting. And the other values of the same occurrence will be nan.</p>\n\n<p>The final data frame I am looking for will look like,</p>\n\n<pre><code>df\ncol1    col2\n  1       A\n  2       NA\n  3       B\n  4       C\n  5       NA\n  6       NA\n  7       B\n  8       NA\n  9       A\n</code></pre>\n\n<p>I can do it using for loop and comparing, But the execution time will be more. I am looking for pythonic way to do it. Some panda shortcuts may be.</p>\n",
        "formatted_input": {
            "qid": 58538845,
            "link": "https://stackoverflow.com/questions/58538845/replace-repetitive-number-with-nan-values-except-the-first-in-pandas-column",
            "question": {
                "title": "Replace repetitive number with NAN values except the first, in pandas column",
                "ques_desc": "I have a data frame like this, Now we can see that there is continuous occurrence of A, B and C. I want only the rows where the occurrence is starting. And the other values of the same occurrence will be nan. The final data frame I am looking for will look like, I can do it using for loop and comparing, But the execution time will be more. I am looking for pythonic way to do it. Some panda shortcuts may be. "
            },
            "io": [
                "df\ncol1    col2\n  1       A\n  2       A\n  3       B\n  4       C\n  5       C\n  6       C\n  7       B\n  8       B\n  9       A\n",
                "df\ncol1    col2\n  1       A\n  2       NA\n  3       B\n  4       C\n  5       NA\n  6       NA\n  7       B\n  8       NA\n  9       A\n"
            ],
            "answer": {
                "ans_desc": "Compare by ed values and missing values by or : Or by with inverted condition by : Or thanks @Daniel Mesejo - use for : Detail: ",
                "code": [
                    "df['col2'] = df['col2'].where(df['col2'].ne(df['col2'].shift()))\n#alternative\n#df['col2'] = np.where(df['col2'].ne(df['col2'].shift()), df['col2'], np.nan)\n",
                    "print (df['col2'].ne(df['col2'].shift()))\n0     True\n1    False\n2     True\n3     True\n4    False\n5    False\n6     True\n7    False\n8     True\nName: col2, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "if-statement",
            "conditional-statements"
        ],
        "owner": {
            "reputation": 519,
            "user_id": 12231431,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-yplJHX1tNto/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfvAF3HIKjtY2X4NHzhZQ3oWecsvA/photo.jpg?sz=128",
            "display_name": "Salih",
            "link": "https://stackoverflow.com/users/12231431/salih"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 58522413,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1571832072,
        "creation_date": 1571831228,
        "question_id": 58522174,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58522174/python-pandas-get-values-according-to-if-else",
        "title": "Python Pandas Get Values According to If/Else",
        "body": "<p>My input dataframe;</p>\n\n<pre><code>Order    Need   WarehouseStock   StoreStock\n1        3      74               5\n0        4      44               44\n0        0      44               44\n6        12     44               44\n0        6      644              44\n6        6      44               44\n</code></pre>\n\n<p>I want to count whether any difference or not among \"Order\" and Need values with below code; </p>\n\n<pre><code>difference = df['Need'] - df['Order']\nmask = difference.between(-1,1)                                                              \nprint (f'Count: {(~mask).sum()}')\n</code></pre>\n\n<p>I want to that something like this;</p>\n\n<p>If (WarehouseStock-StoreStock) >= Need:</p>\n\n<pre><code>difference1 = df['Need'] - df['Order']\nmask1 = difference1.between(-1,1)                                                              \nprint (f'Count: {(~mask1).sum()}')\n</code></pre>\n\n<p>Else</p>\n\n<pre><code>difference2 = df['Need'] - df['Order']\nmask2 = difference2.between(-5,5)                                                              \nprint (f'Count: {(~mask2).sum()}')\n</code></pre>\n\n<p>Desired Outputs are;</p>\n\n<p>Count 3</p>\n\n<pre><code>Order    Need   WarehouseStock   StoreStock\n1        3      74               5\n6        12     44               44\n0        6      644              44\n</code></pre>\n\n<p>Could you please help me about this?</p>\n",
        "answer_body": "<p>Using <code>numpy.where</code> with <code>pandas.Series.between</code>:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ns = df['Need'] - df['Order']\nind = np.where((df['WarehouseStock'] - df['StoreStock']).ge(df['Need']), ~s.between(-1, 1), ~s.between(-5 , 5))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>ind.sum()\n# 3\n\ndf[ind]\n   Order  Need  WarehouseStock  StoreStock\n0      1     3              74           5\n3      6    12              44          44\n4      0     6             644          44\n</code></pre>\n",
        "question_body": "<p>My input dataframe;</p>\n\n<pre><code>Order    Need   WarehouseStock   StoreStock\n1        3      74               5\n0        4      44               44\n0        0      44               44\n6        12     44               44\n0        6      644              44\n6        6      44               44\n</code></pre>\n\n<p>I want to count whether any difference or not among \"Order\" and Need values with below code; </p>\n\n<pre><code>difference = df['Need'] - df['Order']\nmask = difference.between(-1,1)                                                              \nprint (f'Count: {(~mask).sum()}')\n</code></pre>\n\n<p>I want to that something like this;</p>\n\n<p>If (WarehouseStock-StoreStock) >= Need:</p>\n\n<pre><code>difference1 = df['Need'] - df['Order']\nmask1 = difference1.between(-1,1)                                                              \nprint (f'Count: {(~mask1).sum()}')\n</code></pre>\n\n<p>Else</p>\n\n<pre><code>difference2 = df['Need'] - df['Order']\nmask2 = difference2.between(-5,5)                                                              \nprint (f'Count: {(~mask2).sum()}')\n</code></pre>\n\n<p>Desired Outputs are;</p>\n\n<p>Count 3</p>\n\n<pre><code>Order    Need   WarehouseStock   StoreStock\n1        3      74               5\n6        12     44               44\n0        6      644              44\n</code></pre>\n\n<p>Could you please help me about this?</p>\n",
        "formatted_input": {
            "qid": 58522174,
            "link": "https://stackoverflow.com/questions/58522174/python-pandas-get-values-according-to-if-else",
            "question": {
                "title": "Python Pandas Get Values According to If/Else",
                "ques_desc": "My input dataframe; I want to count whether any difference or not among \"Order\" and Need values with below code; I want to that something like this; If (WarehouseStock-StoreStock) >= Need: Else Desired Outputs are; Count 3 Could you please help me about this? "
            },
            "io": [
                "Order    Need   WarehouseStock   StoreStock\n1        3      74               5\n0        4      44               44\n0        0      44               44\n6        12     44               44\n0        6      644              44\n6        6      44               44\n",
                "Order    Need   WarehouseStock   StoreStock\n1        3      74               5\n6        12     44               44\n0        6      644              44\n"
            ],
            "answer": {
                "ans_desc": "Using with : Output: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ns = df['Need'] - df['Order']\nind = np.where((df['WarehouseStock'] - df['StoreStock']).ge(df['Need']), ~s.between(-1, 1), ~s.between(-5 , 5))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 151,
            "user_id": 11815537,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f40f14075c65ebb4c7a9a82c346d4745?s=128&d=identicon&r=PG&f=1",
            "display_name": "Alessandrini",
            "link": "https://stackoverflow.com/users/11815537/alessandrini"
        },
        "is_answered": true,
        "view_count": 432,
        "accepted_answer_id": 58490121,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1571674573,
        "creation_date": 1571673736,
        "question_id": 58490071,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58490071/pandas-drop-duplicates-based-on-row-value",
        "title": "Pandas: Drop duplicates based on row value",
        "body": "<p>I have a dataframe and I want to drop duplicates based on different conditions....</p>\n\n<pre><code>        A      B\n  0     1     1.0\n  1     1     1.0\n  2     2     2.0\n  3     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  8     -     5.1\n  9     -     5.3\n</code></pre>\n\n<p>I want to drop all the duplicates from column A except rows with \"-\". After this, I want to drop duplicates from column A with \"-\" as a value based on their column B value. Given the input dataframe, this should return the following:- </p>\n\n<pre><code>        A      B\n  0     1     1.0\n  2     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  9     -     5.3\n</code></pre>\n\n<p>I have the following code but it's not very efficient for very large amounts of data, how can I improve this....</p>\n\n<pre><code> def generate(df):\n     str_col = df[df[\"A\"] == \"-\"]\n\n     df.drop(df[df[\"A\"] == \"-\"].index, inplace=True)\n\n     df = df.drop_duplicates(subset=\"A\")\n\n     str_col = b.drop_duplicates(subset=\"B\")\n\n     bigdata = df.append(str_col, ignore_index=True)\n\n     return bigdata.sort_values(\"B\")\n</code></pre>\n",
        "answer_body": "<p><code>duplicated</code> and <code>eq</code>:</p>\n\n<pre><code>df[~df.duplicated('A')            # keep those not duplicates in A\n   | (df['A'].eq('-')             # or those '-' in A\n      &amp; ~df['B'].duplicated())]   # which are not duplicates in B\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   A    B\n0  1  1.0\n2  2  2.0\n4  3  3.0\n5  4  4.0\n6  5  5.0\n7  -  5.1\n9  -  5.3\n</code></pre>\n",
        "question_body": "<p>I have a dataframe and I want to drop duplicates based on different conditions....</p>\n\n<pre><code>        A      B\n  0     1     1.0\n  1     1     1.0\n  2     2     2.0\n  3     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  8     -     5.1\n  9     -     5.3\n</code></pre>\n\n<p>I want to drop all the duplicates from column A except rows with \"-\". After this, I want to drop duplicates from column A with \"-\" as a value based on their column B value. Given the input dataframe, this should return the following:- </p>\n\n<pre><code>        A      B\n  0     1     1.0\n  2     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  9     -     5.3\n</code></pre>\n\n<p>I have the following code but it's not very efficient for very large amounts of data, how can I improve this....</p>\n\n<pre><code> def generate(df):\n     str_col = df[df[\"A\"] == \"-\"]\n\n     df.drop(df[df[\"A\"] == \"-\"].index, inplace=True)\n\n     df = df.drop_duplicates(subset=\"A\")\n\n     str_col = b.drop_duplicates(subset=\"B\")\n\n     bigdata = df.append(str_col, ignore_index=True)\n\n     return bigdata.sort_values(\"B\")\n</code></pre>\n",
        "formatted_input": {
            "qid": 58490071,
            "link": "https://stackoverflow.com/questions/58490071/pandas-drop-duplicates-based-on-row-value",
            "question": {
                "title": "Pandas: Drop duplicates based on row value",
                "ques_desc": "I have a dataframe and I want to drop duplicates based on different conditions.... I want to drop all the duplicates from column A except rows with \"-\". After this, I want to drop duplicates from column A with \"-\" as a value based on their column B value. Given the input dataframe, this should return the following:- I have the following code but it's not very efficient for very large amounts of data, how can I improve this.... "
            },
            "io": [
                "        A      B\n  0     1     1.0\n  1     1     1.0\n  2     2     2.0\n  3     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  8     -     5.1\n  9     -     5.3\n",
                "        A      B\n  0     1     1.0\n  2     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  9     -     5.3\n"
            ],
            "answer": {
                "ans_desc": " and : Output: ",
                "code": [
                    "df[~df.duplicated('A')            # keep those not duplicates in A\n   | (df['A'].eq('-')             # or those '-' in A\n      & ~df['B'].duplicated())]   # which are not duplicates in B\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 519,
            "user_id": 12231431,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-yplJHX1tNto/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfvAF3HIKjtY2X4NHzhZQ3oWecsvA/photo.jpg?sz=128",
            "display_name": "Salih",
            "link": "https://stackoverflow.com/users/12231431/salih"
        },
        "is_answered": true,
        "view_count": 69,
        "closed_date": 1571411014,
        "accepted_answer_id": 58448707,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1571599939,
        "creation_date": 1571393405,
        "last_edit_date": 1571599939,
        "question_id": 58448671,
        "link": "https://stackoverflow.com/questions/58448671/python-list-to-pandas-dataframe-with-number-strings",
        "closed_reason": "Needs details or clarity",
        "title": "Python List to Pandas DataFrame with Number &amp; Strings",
        "body": "<p>If I have a following list (this list need the separator for each comma);</p>\n\n<pre><code>[(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)]\n</code></pre>\n\n<p>And also another list;</p>\n\n<pre><code>['A',\n 'B',\n 'C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H']\n</code></pre>\n\n<p>How can i get this desire output with python?</p>\n\n<pre><code>A      B     C   D              E    F   G   H\n5461   1.20  A   BR SK-EL 7 EP  146  E   52  0\n</code></pre>\n\n<p>Could you please help me about this?</p>\n",
        "answer_body": "<p>Solution if data are list of tuples:</p>\n\n<pre><code>data = [(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)]\ncols = ['A', 'B','C', 'D', 'E', 'F', 'G', 'H']\n\n\ndf = pd.DataFrame(data, columns=cols)\nprint (df)\n      A     B  C              D    E  F   G  H\n0  5461  1.20  A  BR SK-EL 7 EP  146  E  52  0\n</code></pre>\n\n<p>EDIT:</p>\n\n<pre><code>df = pd.DataFrame([list(x) for x in data], columns=cols)\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df = pd.DataFrame([[x for x in data]], columns=cols)\n</code></pre>\n",
        "question_body": "<p>If I have a following list (this list need the separator for each comma);</p>\n\n<pre><code>[(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)]\n</code></pre>\n\n<p>And also another list;</p>\n\n<pre><code>['A',\n 'B',\n 'C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H']\n</code></pre>\n\n<p>How can i get this desire output with python?</p>\n\n<pre><code>A      B     C   D              E    F   G   H\n5461   1.20  A   BR SK-EL 7 EP  146  E   52  0\n</code></pre>\n\n<p>Could you please help me about this?</p>\n",
        "formatted_input": {
            "qid": 58448671,
            "link": "https://stackoverflow.com/questions/58448671/python-list-to-pandas-dataframe-with-number-strings",
            "question": {
                "title": "Python List to Pandas DataFrame with Number &amp; Strings",
                "ques_desc": "If I have a following list (this list need the separator for each comma); And also another list; How can i get this desire output with python? Could you please help me about this? "
            },
            "io": [
                "[(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)]\n",
                "A      B     C   D              E    F   G   H\n5461   1.20  A   BR SK-EL 7 EP  146  E   52  0\n"
            ],
            "answer": {
                "ans_desc": "Solution if data are list of tuples: EDIT: Or: ",
                "code": [
                    "df = pd.DataFrame([list(x) for x in data], columns=cols)\n",
                    "df = pd.DataFrame([[x for x in data]], columns=cols)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "deep-copy"
        ],
        "owner": {
            "reputation": 1,
            "user_id": 10627840,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9168acab503edf2a7705fbaaa05bae71?s=128&d=identicon&r=PG&f=1",
            "display_name": "R. Nieuw",
            "link": "https://stackoverflow.com/users/10627840/r-nieuw"
        },
        "is_answered": true,
        "view_count": 127,
        "accepted_answer_id": 58450021,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1571398305,
        "creation_date": 1571310315,
        "last_edit_date": 1571311283,
        "question_id": 58431148,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58431148/how-to-make-a-deepcopy-of-a-dataframe-with-dataframes-within-it-python",
        "title": "How to make a deepcopy of a dataframe with dataframes within it? (python)",
        "body": "<p>I want a copy of a dataframe which contains a dataframe. When I change something in the nested dataframe, it shouldn't change in the original dataframe.</p>\n\n<p>I have a dataframe like this:</p>\n\n<pre><code>   0  1                                                  2\n0  1  2  &lt;__main__.PossibleCombinations object at 0x000...\n1  4  5                                                  6\n</code></pre>\n\n<p>Generated with the next code:</p>\n\n<pre><code>import copy\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.DataFrame(data= [[1,2,3],[4,5,6]])\n\nclass PossibleCombinations:\n    def __init__(self, dfCombinations, numberCombinations):\n        self.dfCombinations = dfCombinations\n        self.numberCombinations = numberCombinations\n\ndf.iloc[0,2] = PossibleCombinations(pd.DataFrame(data= [[1,2,3],[4,5,6]]),6)\nprint(df)\n</code></pre>\n\n<p>When I make a deepcopy of the hole dataframe and the nested dataframe and change something in the \nnested dataframe in the copy, the value also changes in the original.</p>\n\n<pre><code>deepCopy = copy.deepcopy(df)\ndeepCopy.iloc[0,2].dfCombinations = copy.deepcopy(df.iloc[0,2].dfCombinations)\n\ndeepCopy.iloc[0,2].dfCombinations.iloc[0,2] = \"doei\"\n\nprint(deepCopy.iloc[0,2].dfCombinations)\nprint(\" \")\nprint(df.iloc[0,2].dfCombinations)\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2  doei\n1  4  5     6\n</code></pre>\n\n<p>but I want:</p>\n\n<pre><code>   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2     3\n1  4  5     6\n</code></pre>\n\n<p><strong>What is the fix for this problem?</strong></p>\n",
        "answer_body": "<p>This is the fix:</p>\n\n<pre><code>import pickle\ndeepCopy = pickle.loads(pickle.dumps(df))\n\ndeepCopy.iloc[0,2].dfCombinations.iloc[0,2] = \"doei\"\n\nprint(deepCopy.iloc[0,2].dfCombinations)\nprint(\" \")\nprint(df.iloc[0,2].dfCombinations)\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>3\n   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1  2\n0  1  2  3\n1  4  5  6\n</code></pre>\n\n<p>This solves the problem!</p>\n",
        "question_body": "<p>I want a copy of a dataframe which contains a dataframe. When I change something in the nested dataframe, it shouldn't change in the original dataframe.</p>\n\n<p>I have a dataframe like this:</p>\n\n<pre><code>   0  1                                                  2\n0  1  2  &lt;__main__.PossibleCombinations object at 0x000...\n1  4  5                                                  6\n</code></pre>\n\n<p>Generated with the next code:</p>\n\n<pre><code>import copy\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.DataFrame(data= [[1,2,3],[4,5,6]])\n\nclass PossibleCombinations:\n    def __init__(self, dfCombinations, numberCombinations):\n        self.dfCombinations = dfCombinations\n        self.numberCombinations = numberCombinations\n\ndf.iloc[0,2] = PossibleCombinations(pd.DataFrame(data= [[1,2,3],[4,5,6]]),6)\nprint(df)\n</code></pre>\n\n<p>When I make a deepcopy of the hole dataframe and the nested dataframe and change something in the \nnested dataframe in the copy, the value also changes in the original.</p>\n\n<pre><code>deepCopy = copy.deepcopy(df)\ndeepCopy.iloc[0,2].dfCombinations = copy.deepcopy(df.iloc[0,2].dfCombinations)\n\ndeepCopy.iloc[0,2].dfCombinations.iloc[0,2] = \"doei\"\n\nprint(deepCopy.iloc[0,2].dfCombinations)\nprint(\" \")\nprint(df.iloc[0,2].dfCombinations)\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2  doei\n1  4  5     6\n</code></pre>\n\n<p>but I want:</p>\n\n<pre><code>   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2     3\n1  4  5     6\n</code></pre>\n\n<p><strong>What is the fix for this problem?</strong></p>\n",
        "formatted_input": {
            "qid": 58431148,
            "link": "https://stackoverflow.com/questions/58431148/how-to-make-a-deepcopy-of-a-dataframe-with-dataframes-within-it-python",
            "question": {
                "title": "How to make a deepcopy of a dataframe with dataframes within it? (python)",
                "ques_desc": "I want a copy of a dataframe which contains a dataframe. When I change something in the nested dataframe, it shouldn't change in the original dataframe. I have a dataframe like this: Generated with the next code: When I make a deepcopy of the hole dataframe and the nested dataframe and change something in the nested dataframe in the copy, the value also changes in the original. output: but I want: What is the fix for this problem? "
            },
            "io": [
                "   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2  doei\n1  4  5     6\n",
                "   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2     3\n1  4  5     6\n"
            ],
            "answer": {
                "ans_desc": "This is the fix: output: This solves the problem! ",
                "code": [
                    "import pickle\ndeepCopy = pickle.loads(pickle.dumps(df))\n\ndeepCopy.iloc[0,2].dfCombinations.iloc[0,2] = \"doei\"\n\nprint(deepCopy.iloc[0,2].dfCombinations)\nprint(\" \")\nprint(df.iloc[0,2].dfCombinations)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 519,
            "user_id": 12231431,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-yplJHX1tNto/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfvAF3HIKjtY2X4NHzhZQ3oWecsvA/photo.jpg?sz=128",
            "display_name": "Salih",
            "link": "https://stackoverflow.com/users/12231431/salih"
        },
        "is_answered": true,
        "view_count": 12095,
        "accepted_answer_id": 58434124,
        "answer_count": 2,
        "score": 11,
        "last_activity_date": 1571320278,
        "creation_date": 1571319382,
        "question_id": 58434018,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58434018/pandas-adding-row-with-all-values-zero",
        "title": "Pandas Adding Row with All Values Zero",
        "body": "<p>If I have a following dataframe:</p>\n\n<pre><code>          A       B       C       D       E\n\n1         1       2       0       1       0\n2         0       0       0       1      -1\n3         1       1       3      -5       2\n4        -3       4       2       6       0\n5         2       4       1       9      -1\n6         1       2       2       4       1\n</code></pre>\n\n<p>How can i add a row end of the dataframe with all values \"0 (Zero)\"?</p>\n\n<p>Desired Output is;</p>\n\n<pre><code>          A       B       C       D       E\n\n1         1       2       0       1       0\n2         0       0       0       1      -1\n3         1       1       3      -5       2\n4        -3       4       2       6       0\n5         2       4       1       9      -1\n6         1       2       2       4       1\n7         0       0       0       0       0\n</code></pre>\n\n<p>Could you please help me about this?</p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#setting-with-enlargement\" rel=\"noreferrer\">Setting with enlargement</a>:</p>\n\n<pre><code>df.loc[len(df)] = 0\nprint (df)\n   A  B  C  D  E\n1  1  2  0  1  0\n2  0  0  0  1 -1\n3  1  1  3 -5  2\n4 -3  4  2  6  0\n5  2  4  1  9 -1\n6  0  0  0  0  0\n</code></pre>\n\n<p>Or <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\" rel=\"noreferrer\"><code>DataFrame.append</code></a> with <code>Series</code> filled by <code>0</code> and index by columns of <code>DataFrame</code>:</p>\n\n<pre><code>df = df.append(pd.Series(0, index=df.columns), ignore_index=True)\n</code></pre>\n\n<hr>\n",
        "question_body": "<p>If I have a following dataframe:</p>\n\n<pre><code>          A       B       C       D       E\n\n1         1       2       0       1       0\n2         0       0       0       1      -1\n3         1       1       3      -5       2\n4        -3       4       2       6       0\n5         2       4       1       9      -1\n6         1       2       2       4       1\n</code></pre>\n\n<p>How can i add a row end of the dataframe with all values \"0 (Zero)\"?</p>\n\n<p>Desired Output is;</p>\n\n<pre><code>          A       B       C       D       E\n\n1         1       2       0       1       0\n2         0       0       0       1      -1\n3         1       1       3      -5       2\n4        -3       4       2       6       0\n5         2       4       1       9      -1\n6         1       2       2       4       1\n7         0       0       0       0       0\n</code></pre>\n\n<p>Could you please help me about this?</p>\n",
        "formatted_input": {
            "qid": 58434018,
            "link": "https://stackoverflow.com/questions/58434018/pandas-adding-row-with-all-values-zero",
            "question": {
                "title": "Pandas Adding Row with All Values Zero",
                "ques_desc": "If I have a following dataframe: How can i add a row end of the dataframe with all values \"0 (Zero)\"? Desired Output is; Could you please help me about this? "
            },
            "io": [
                "          A       B       C       D       E\n\n1         1       2       0       1       0\n2         0       0       0       1      -1\n3         1       1       3      -5       2\n4        -3       4       2       6       0\n5         2       4       1       9      -1\n6         1       2       2       4       1\n",
                "          A       B       C       D       E\n\n1         1       2       0       1       0\n2         0       0       0       1      -1\n3         1       1       3      -5       2\n4        -3       4       2       6       0\n5         2       4       1       9      -1\n6         1       2       2       4       1\n7         0       0       0       0       0\n"
            ],
            "answer": {
                "ans_desc": "Use Setting with enlargement: Or with filled by and index by columns of : ",
                "code": [
                    "df = df.append(pd.Series(0, index=df.columns), ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2037,
            "user_id": 7836530,
            "user_type": "registered",
            "accept_rate": 96,
            "profile_image": "https://www.gravatar.com/avatar/f11697069f27f74c1f2af72df755ef95?s=128&d=identicon&r=PG&f=1",
            "display_name": "JamesHudson81",
            "link": "https://stackoverflow.com/users/7836530/jameshudson81"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 58402081,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1571170905,
        "creation_date": 1571170245,
        "last_edit_date": 1571170905,
        "question_id": 58402008,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58402008/using-a-dictionary-to-modify-the-dfs-values",
        "title": "using a dictionary to modify the dfs values",
        "body": "<p>I have a df like this:</p>\n\n<pre><code>      xx   yy   zz\n A    6     5    2\n B    4     4    5\n B    5     6    7\n C    6     6    6\n C    7     7    7\n</code></pre>\n\n<p>Then I have a dictionary with some keys (which correspond to the index names of the df) and values (column names):</p>\n\n<pre><code>{'A':['xx'],'B':['yy','zz'],'C':['xx','zz']}\n</code></pre>\n\n<p>I would like to use the dictionary to check that those column names that do not appear in the dict values , are set to zero to generate this output:</p>\n\n<pre><code>      xx   yy   zz\n A    6     0    0\n B    0     4    5\n B    0     6    7\n C    6     0    6\n C    7     0    7\n</code></pre>\n\n<p>How could I use the dictionary to generate the desired output?</p>\n",
        "answer_body": "<p>You may use indexing</p>\n\n<pre><code>mask = (pd.DataFrame(d.values(), index=d.keys())\n          .stack()\n          .reset_index(level=1, drop=True)\n          .str.get_dummies()\n          .groupby(level=0).sum()\n          .astype(bool)\n        )\n</code></pre>\n\n<p></p>\n\n<pre><code>df[mask].fillna(0)\n</code></pre>\n\n<hr>\n\n<pre><code>    xx   yy   zz\nA  6.0  0.0  0.0\nB  0.0  4.0  5.0\nB  0.0  6.0  7.0\nC  6.0  0.0  6.0\nC  7.0  0.0  7.0\n</code></pre>\n",
        "question_body": "<p>I have a df like this:</p>\n\n<pre><code>      xx   yy   zz\n A    6     5    2\n B    4     4    5\n B    5     6    7\n C    6     6    6\n C    7     7    7\n</code></pre>\n\n<p>Then I have a dictionary with some keys (which correspond to the index names of the df) and values (column names):</p>\n\n<pre><code>{'A':['xx'],'B':['yy','zz'],'C':['xx','zz']}\n</code></pre>\n\n<p>I would like to use the dictionary to check that those column names that do not appear in the dict values , are set to zero to generate this output:</p>\n\n<pre><code>      xx   yy   zz\n A    6     0    0\n B    0     4    5\n B    0     6    7\n C    6     0    6\n C    7     0    7\n</code></pre>\n\n<p>How could I use the dictionary to generate the desired output?</p>\n",
        "formatted_input": {
            "qid": 58402008,
            "link": "https://stackoverflow.com/questions/58402008/using-a-dictionary-to-modify-the-dfs-values",
            "question": {
                "title": "using a dictionary to modify the dfs values",
                "ques_desc": "I have a df like this: Then I have a dictionary with some keys (which correspond to the index names of the df) and values (column names): I would like to use the dictionary to check that those column names that do not appear in the dict values , are set to zero to generate this output: How could I use the dictionary to generate the desired output? "
            },
            "io": [
                "      xx   yy   zz\n A    6     5    2\n B    4     4    5\n B    5     6    7\n C    6     6    6\n C    7     7    7\n",
                "      xx   yy   zz\n A    6     0    0\n B    0     4    5\n B    0     6    7\n C    6     0    6\n C    7     0    7\n"
            ],
            "answer": {
                "ans_desc": "You may use indexing ",
                "code": [
                    "mask = (pd.DataFrame(d.values(), index=d.keys())\n          .stack()\n          .reset_index(level=1, drop=True)\n          .str.get_dummies()\n          .groupby(level=0).sum()\n          .astype(bool)\n        )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 703,
            "user_id": 8541953,
            "user_type": "registered",
            "accept_rate": 93,
            "profile_image": "https://i.stack.imgur.com/dnESJ.jpg?s=128&g=1",
            "display_name": "GCGM",
            "link": "https://stackoverflow.com/users/8541953/gcgm"
        },
        "is_answered": true,
        "view_count": 207,
        "accepted_answer_id": 58320462,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1570716592,
        "creation_date": 1570700549,
        "last_edit_date": 1570714589,
        "question_id": 58319979,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58319979/list-to-pandas-dataframe-python",
        "title": "list to pandas dataframe - Python",
        "body": "<p>I have the following list:</p>\n\n<pre><code>list = [-0.14626096918979603,\n 0.017925919395027533,\n 0.41265398151061766]\n</code></pre>\n\n<p>I have created a <code>pandas</code> <code>dataframe</code> using the following code: </p>\n\n<pre><code>df = pd.DataFrame(list, index=['var1','var2','var3'], columns=['Col1'])\ndf\n               Col1\nvar1         -0.146261\nvar2         0.017926\nvar3         0.412654\n</code></pre>\n\n<p>Now I have a new list:</p>\n\n<pre><code>list2 = [-0.14626096918979603,\n 0.017925919395027533,\n 0.41265398151061766,\n -0.8538301985671065,\n 0.08182534201640915,\n 0.40291331836021105]\n</code></pre>\n\n<p>I would like to arrange the <code>dataframe</code> in a way that the output looks like this (MANUAL EDIT)</p>\n\n<pre><code>               Col1            Col2\nvar1         -0.146261   -0.8538301985671065\nvar2         0.017926   0.08182534201640915\nvar3         0.412654   0.40291331836021105\n</code></pre>\n\n<p>and that whenever there is a third or foruth colum... the data gets arranged in the same way. I have tried to convert the list to a <code>dict</code> but since I am new with python I am not getting the desired output but only errors due to invalid shapes. </p>\n\n<p>-- EDIT --</p>\n\n<p>Once I have the dataframe created, I want to plot it using <code>df.plot()</code>. However, the way the data is shown is not what I would like. I am comming from <code>R</code> so I am not sure if this is because of the data structure used in the <code>dataframe</code>. Is is it that I need one measurement in each row?</p>\n\n<p><a href=\"https://i.stack.imgur.com/83MyK.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/83MyK.png\" alt=\"enter image description here\"></a></p>\n\n<p>My idea would be to have the <code>col1</code>, <code>col2</code>, <code>col3</code> in the x-axis (it's a temporal series). In the y-axis the range of values (so that is ok in that plot) and the differnet lines should be showing the evolution of <code>var1</code>, <code>var2</code>, <code>var3</code>, etc. </p>\n",
        "answer_body": "<p>you could run something like</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame(index = ['var1', 'var2', 'var3'])\n\nn_cols = int(np.ceil(len(list2) / len(df)))\nfor ii in range(n_cols):\n    L = list2[ii * len(df) : (ii + 1) * len(df)]\n    df['col_{}'.format(ii)] = L\n</code></pre>\n\n<p>if the length of your list is not multiple of the length of the dataframe (<code>len(list2) % len(df) != 0</code>, you should extend L (in the last loop) with <code>len(df) - (len(list2) % len(df))</code> NaN values</p>\n\n<p>to answer the <strong>second question,</strong> should be sufficient to run</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df.T.plot()\n</code></pre>\n\n<p>for the <strong>third question,</strong> then it's a matter of how was originally designed the dataframe.\nYou could edit the code we wrote at the beginning to invert rows and columns</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame(columns = ['var1', 'var2', 'var3'])\nn_rows = int(np.ceil(len(list2) / len(df.columns)))\nfor ii in range(n_rows):\n    L = list2[ii * len(df.columns) : (ii + 1) * len(df.columns)]\n    df.loc['col_{}'.format(ii)] = L\n</code></pre>\n\n<p>but once you created the dataframe with the first designed way, there's nothing wrong in running</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df = df.T\n</code></pre>\n",
        "question_body": "<p>I have the following list:</p>\n\n<pre><code>list = [-0.14626096918979603,\n 0.017925919395027533,\n 0.41265398151061766]\n</code></pre>\n\n<p>I have created a <code>pandas</code> <code>dataframe</code> using the following code: </p>\n\n<pre><code>df = pd.DataFrame(list, index=['var1','var2','var3'], columns=['Col1'])\ndf\n               Col1\nvar1         -0.146261\nvar2         0.017926\nvar3         0.412654\n</code></pre>\n\n<p>Now I have a new list:</p>\n\n<pre><code>list2 = [-0.14626096918979603,\n 0.017925919395027533,\n 0.41265398151061766,\n -0.8538301985671065,\n 0.08182534201640915,\n 0.40291331836021105]\n</code></pre>\n\n<p>I would like to arrange the <code>dataframe</code> in a way that the output looks like this (MANUAL EDIT)</p>\n\n<pre><code>               Col1            Col2\nvar1         -0.146261   -0.8538301985671065\nvar2         0.017926   0.08182534201640915\nvar3         0.412654   0.40291331836021105\n</code></pre>\n\n<p>and that whenever there is a third or foruth colum... the data gets arranged in the same way. I have tried to convert the list to a <code>dict</code> but since I am new with python I am not getting the desired output but only errors due to invalid shapes. </p>\n\n<p>-- EDIT --</p>\n\n<p>Once I have the dataframe created, I want to plot it using <code>df.plot()</code>. However, the way the data is shown is not what I would like. I am comming from <code>R</code> so I am not sure if this is because of the data structure used in the <code>dataframe</code>. Is is it that I need one measurement in each row?</p>\n\n<p><a href=\"https://i.stack.imgur.com/83MyK.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/83MyK.png\" alt=\"enter image description here\"></a></p>\n\n<p>My idea would be to have the <code>col1</code>, <code>col2</code>, <code>col3</code> in the x-axis (it's a temporal series). In the y-axis the range of values (so that is ok in that plot) and the differnet lines should be showing the evolution of <code>var1</code>, <code>var2</code>, <code>var3</code>, etc. </p>\n",
        "formatted_input": {
            "qid": 58319979,
            "link": "https://stackoverflow.com/questions/58319979/list-to-pandas-dataframe-python",
            "question": {
                "title": "list to pandas dataframe - Python",
                "ques_desc": "I have the following list: I have created a using the following code: Now I have a new list: I would like to arrange the in a way that the output looks like this (MANUAL EDIT) and that whenever there is a third or foruth colum... the data gets arranged in the same way. I have tried to convert the list to a but since I am new with python I am not getting the desired output but only errors due to invalid shapes. -- EDIT -- Once I have the dataframe created, I want to plot it using . However, the way the data is shown is not what I would like. I am comming from so I am not sure if this is because of the data structure used in the . Is is it that I need one measurement in each row? My idea would be to have the , , in the x-axis (it's a temporal series). In the y-axis the range of values (so that is ok in that plot) and the differnet lines should be showing the evolution of , , , etc. "
            },
            "io": [
                "list = [-0.14626096918979603,\n 0.017925919395027533,\n 0.41265398151061766]\n",
                "list2 = [-0.14626096918979603,\n 0.017925919395027533,\n 0.41265398151061766,\n -0.8538301985671065,\n 0.08182534201640915,\n 0.40291331836021105]\n"
            ],
            "answer": {
                "ans_desc": "you could run something like if the length of your list is not multiple of the length of the dataframe (, you should extend L (in the last loop) with NaN values to answer the second question, should be sufficient to run for the third question, then it's a matter of how was originally designed the dataframe. You could edit the code we wrote at the beginning to invert rows and columns but once you created the dataframe with the first designed way, there's nothing wrong in running ",
                "code": [
                    "df = pd.DataFrame(index = ['var1', 'var2', 'var3'])\n\nn_cols = int(np.ceil(len(list2) / len(df)))\nfor ii in range(n_cols):\n    L = list2[ii * len(df) : (ii + 1) * len(df)]\n    df['col_{}'.format(ii)] = L\n",
                    "len(list2) % len(df) != 0",
                    "len(df) - (len(list2) % len(df))",
                    "df = pd.DataFrame(columns = ['var1', 'var2', 'var3'])\nn_rows = int(np.ceil(len(list2) / len(df.columns)))\nfor ii in range(n_rows):\n    L = list2[ii * len(df.columns) : (ii + 1) * len(df.columns)]\n    df.loc['col_{}'.format(ii)] = L\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 292,
            "user_id": 11807541,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-G3r4_SnrBO8/AAAAAAAAAAI/AAAAAAAAAW8/7oz8gJHx35Y/photo.jpg?sz=128",
            "display_name": "Sayantan Ghosh",
            "link": "https://stackoverflow.com/users/11807541/sayantan-ghosh"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 58322060,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1570708491,
        "creation_date": 1570707345,
        "last_edit_date": 1570707425,
        "question_id": 58321988,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58321988/how-to-split-a-string-in-a-column-within-a-pandas-dataframe",
        "title": "How to split a string in a column within a pandas dataframe?",
        "body": "<p>This is an example of the file I have,</p>\n\n<pre><code>Name     Att1     Att2     Att3\nAB_EN    1        2        3\nCD       5        6        7\nFG_EN    7        8        9\n</code></pre>\n\n<p>So, in the column 'Name', where '_EN' is present, I want to remove the '_EN' part. The output should be as follows:</p>\n\n<pre><code>Name     Att1     Att2     Att3\nAB       1        2        3\nCD       5        6        7\nFG       7        8        9\n</code></pre>\n\n<p>This is what I was trying:</p>\n\n<pre><code>name = df['Name']\n\nfor entry in name:\n    if \"_EN\" in entry:\n       entry = entry.split('_')[0]\n</code></pre>\n\n<p>However, this is not working. What is a good way to do this?</p>\n",
        "answer_body": "<p>Use <code>str.split</code></p>\n\n<p><strong>Ex:</strong></p>\n\n<pre><code>df = pd.DataFrame({\"Name\": [\"AB_EN\", \"CD\", \"FG_EN\"]})\ndf['Name'] = df['Name'].str.split(\"_\").str[0]\nprint(df)\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>  Name\n0   AB\n1   CD\n2   FG\n</code></pre>\n",
        "question_body": "<p>This is an example of the file I have,</p>\n\n<pre><code>Name     Att1     Att2     Att3\nAB_EN    1        2        3\nCD       5        6        7\nFG_EN    7        8        9\n</code></pre>\n\n<p>So, in the column 'Name', where '_EN' is present, I want to remove the '_EN' part. The output should be as follows:</p>\n\n<pre><code>Name     Att1     Att2     Att3\nAB       1        2        3\nCD       5        6        7\nFG       7        8        9\n</code></pre>\n\n<p>This is what I was trying:</p>\n\n<pre><code>name = df['Name']\n\nfor entry in name:\n    if \"_EN\" in entry:\n       entry = entry.split('_')[0]\n</code></pre>\n\n<p>However, this is not working. What is a good way to do this?</p>\n",
        "formatted_input": {
            "qid": 58321988,
            "link": "https://stackoverflow.com/questions/58321988/how-to-split-a-string-in-a-column-within-a-pandas-dataframe",
            "question": {
                "title": "How to split a string in a column within a pandas dataframe?",
                "ques_desc": "This is an example of the file I have, So, in the column 'Name', where '_EN' is present, I want to remove the '_EN' part. The output should be as follows: This is what I was trying: However, this is not working. What is a good way to do this? "
            },
            "io": [
                "Name     Att1     Att2     Att3\nAB_EN    1        2        3\nCD       5        6        7\nFG_EN    7        8        9\n",
                "Name     Att1     Att2     Att3\nAB       1        2        3\nCD       5        6        7\nFG       7        8        9\n"
            ],
            "answer": {
                "ans_desc": "Use Ex: Output: ",
                "code": [
                    "df = pd.DataFrame({\"Name\": [\"AB_EN\", \"CD\", \"FG_EN\"]})\ndf['Name'] = df['Name'].str.split(\"_\").str[0]\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "percentile"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 8908940,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8431242bcdd3a85d8053fc2b677e9140?s=128&d=identicon&r=PG&f=1",
            "display_name": "Swapnil",
            "link": "https://stackoverflow.com/users/8908940/swapnil"
        },
        "is_answered": true,
        "view_count": 256,
        "accepted_answer_id": 58275073,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1570472966,
        "creation_date": 1570471644,
        "last_edit_date": 1570471920,
        "question_id": 58274940,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58274940/calculating-percentile-values-for-each-columns-group-by-another-column-values",
        "title": "calculating percentile values for each columns group by another column values - Pandas dataframe",
        "body": "<p>I have a dataframe that looks like below - </p>\n\n<pre><code>   Year  Salary  Amount\n0  2019    1200      53\n1  2020    3443     455\n2  2021    6777     123\n3  2019    5466     313\n4  2020    4656     545\n5  2021    4565     775\n6  2019    4654     567\n7  2020    7867     657\n8  2021    6766     567\n</code></pre>\n\n<p>Python script to get the dataframe below - </p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\nd = pd.DataFrame({\n    'Year': [\n        2019,\n        2020,\n        2021,\n    ] * 3,\n    'Salary': [\n        1200,\n        3443,\n        6777,\n        5466,\n        4656,\n        4565,\n        4654,\n        7867,\n        6766\n    ],\n    'Amount': [\n        53,\n        455,\n        123,\n        313,\n        545,\n        775,\n        567,\n        657,\n        567\n    ]\n})\n</code></pre>\n\n<p>I want to calculate certain percentile values for all the columns grouped by 'Year'.\nDesired output should look like - </p>\n\n<p><img src=\"https://i.stack.imgur.com/9v2vN.png\" alt=\"desired output\"></p>\n\n<p>I am running below python script to perform the calculations to calculate certain percentile values-</p>\n\n<pre><code>df_percentile = pd.DataFrame()\np_list = [0.05, 0.10, 0.25, 0.50, 0.75, 0.95, 0.99]\nc_list = []\np_values = []\nfor cols in d.columns[1:]:\n    for p in p_list:\n        c_list.append(cols + '_' + str(p))\n        p_values.append(np.percentile(d[cols], p))\nprint(len(c_list), len(p_values))\ndf_percentile['Name'] = pd.Series(c_list)\ndf_percentile['Value'] = pd.Series(p_values)\nprint(df_percentile)\n\n</code></pre>\n\n<p>Output - </p>\n\n<pre><code>           Name      Value\n0   Salary_0.05  1208.9720\n1    Salary_0.1  1217.9440\n2   Salary_0.25  1244.8600\n3    Salary_0.5  1289.7200\n4   Salary_0.75  1334.5800\n5   Salary_0.95  1370.4680\n6   Salary_0.99  1377.6456\n7   Amount_0.05    53.2800\n8    Amount_0.1    53.5600\n9   Amount_0.25    54.4000\n10   Amount_0.5    55.8000\n11  Amount_0.75    57.2000\n12  Amount_0.95    58.3200\n13  Amount_0.99    58.5440\n</code></pre>\n\n<p>How can I get the output in the required format without having to do extra data manipulation/formatting or in fewer lines of code?</p>\n",
        "answer_body": "<p>You can try <code>pivot</code> followed by <code>quantile</code>:</p>\n\n<pre><code>(df.pivot(columns='Year')\n   .quantile([0.01,0.05,0.75, 0.95, 0.99])\n   .stack('Year')\n)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>            Salary  Amount\n     Year                 \n0.01 2019  1269.08   58.20\n     2020  3467.26  456.80\n     2021  4609.02  131.88\n0.05 2019  1545.40   79.00\n     2020  3564.30  464.00\n     2021  4785.10  167.40\n0.75 2019  5060.00  440.00\n     2020  6261.50  601.00\n     2021  6771.50  671.00\n0.95 2019  5384.80  541.60\n     2020  7545.90  645.80\n     2021  6775.90  754.20\n0.99 2019  5449.76  561.92\n     2020  7802.78  654.76\n     2021  6776.78  770.84\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like below - </p>\n\n<pre><code>   Year  Salary  Amount\n0  2019    1200      53\n1  2020    3443     455\n2  2021    6777     123\n3  2019    5466     313\n4  2020    4656     545\n5  2021    4565     775\n6  2019    4654     567\n7  2020    7867     657\n8  2021    6766     567\n</code></pre>\n\n<p>Python script to get the dataframe below - </p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\nd = pd.DataFrame({\n    'Year': [\n        2019,\n        2020,\n        2021,\n    ] * 3,\n    'Salary': [\n        1200,\n        3443,\n        6777,\n        5466,\n        4656,\n        4565,\n        4654,\n        7867,\n        6766\n    ],\n    'Amount': [\n        53,\n        455,\n        123,\n        313,\n        545,\n        775,\n        567,\n        657,\n        567\n    ]\n})\n</code></pre>\n\n<p>I want to calculate certain percentile values for all the columns grouped by 'Year'.\nDesired output should look like - </p>\n\n<p><img src=\"https://i.stack.imgur.com/9v2vN.png\" alt=\"desired output\"></p>\n\n<p>I am running below python script to perform the calculations to calculate certain percentile values-</p>\n\n<pre><code>df_percentile = pd.DataFrame()\np_list = [0.05, 0.10, 0.25, 0.50, 0.75, 0.95, 0.99]\nc_list = []\np_values = []\nfor cols in d.columns[1:]:\n    for p in p_list:\n        c_list.append(cols + '_' + str(p))\n        p_values.append(np.percentile(d[cols], p))\nprint(len(c_list), len(p_values))\ndf_percentile['Name'] = pd.Series(c_list)\ndf_percentile['Value'] = pd.Series(p_values)\nprint(df_percentile)\n\n</code></pre>\n\n<p>Output - </p>\n\n<pre><code>           Name      Value\n0   Salary_0.05  1208.9720\n1    Salary_0.1  1217.9440\n2   Salary_0.25  1244.8600\n3    Salary_0.5  1289.7200\n4   Salary_0.75  1334.5800\n5   Salary_0.95  1370.4680\n6   Salary_0.99  1377.6456\n7   Amount_0.05    53.2800\n8    Amount_0.1    53.5600\n9   Amount_0.25    54.4000\n10   Amount_0.5    55.8000\n11  Amount_0.75    57.2000\n12  Amount_0.95    58.3200\n13  Amount_0.99    58.5440\n</code></pre>\n\n<p>How can I get the output in the required format without having to do extra data manipulation/formatting or in fewer lines of code?</p>\n",
        "formatted_input": {
            "qid": 58274940,
            "link": "https://stackoverflow.com/questions/58274940/calculating-percentile-values-for-each-columns-group-by-another-column-values",
            "question": {
                "title": "calculating percentile values for each columns group by another column values - Pandas dataframe",
                "ques_desc": "I have a dataframe that looks like below - Python script to get the dataframe below - I want to calculate certain percentile values for all the columns grouped by 'Year'. Desired output should look like - I am running below python script to perform the calculations to calculate certain percentile values- Output - How can I get the output in the required format without having to do extra data manipulation/formatting or in fewer lines of code? "
            },
            "io": [
                "   Year  Salary  Amount\n0  2019    1200      53\n1  2020    3443     455\n2  2021    6777     123\n3  2019    5466     313\n4  2020    4656     545\n5  2021    4565     775\n6  2019    4654     567\n7  2020    7867     657\n8  2021    6766     567\n",
                "           Name      Value\n0   Salary_0.05  1208.9720\n1    Salary_0.1  1217.9440\n2   Salary_0.25  1244.8600\n3    Salary_0.5  1289.7200\n4   Salary_0.75  1334.5800\n5   Salary_0.95  1370.4680\n6   Salary_0.99  1377.6456\n7   Amount_0.05    53.2800\n8    Amount_0.1    53.5600\n9   Amount_0.25    54.4000\n10   Amount_0.5    55.8000\n11  Amount_0.75    57.2000\n12  Amount_0.95    58.3200\n13  Amount_0.99    58.5440\n"
            ],
            "answer": {
                "ans_desc": "You can try followed by : Output: ",
                "code": [
                    "(df.pivot(columns='Year')\n   .quantile([0.01,0.05,0.75, 0.95, 0.99])\n   .stack('Year')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 10874776,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-ZrAzmHda_rc/AAAAAAAAAAI/AAAAAAAABcs/upTtvmnR9Jk/photo.jpg?sz=128",
            "display_name": "Nishant Kumar",
            "link": "https://stackoverflow.com/users/10874776/nishant-kumar"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 58255740,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1570352413,
        "creation_date": 1570341996,
        "last_edit_date": 1570344131,
        "question_id": 58254668,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58254668/merging-two-rows-of-data-in-a-single-row-with-python-pandas",
        "title": "merging two rows of data in a single row with Python/Pandas",
        "body": "<p>I have a dataframe like this:</p>\n\n<pre><code>   ID   A1    A2    A3    A4                                      \n0  01  100   101   103   104\n1  01  501   502   503   504\n2  01  701   702   703   704\n3  02  1001  1002  1003  1004\n4  03  2001  2002  2003  2004\n5  03  5001  5002  5003  5004\n</code></pre>\n\n<p>I need the rows belonging to the same ID to be merged in a single row, the merged dataframe will be like this</p>\n\n<pre><code>   ID   A1    A2    A3    A4    B1    B2    B3     B4     C1   C2    C3    C4                                                   \n0  01  101   102   103   104   501   502    503    504    701  702   703   704 \n1  02  1001  2001  1003  1004  \n2  03  2001  2002  2003  2004  5001  5002   5003   5004\n</code></pre>\n\n<p>I tried using np.random.permutation, np.roll etc but unable to get the desired result. The count of rows in my original data set is in thousands so loops and creating individual data sets and then merging is not helping</p>\n",
        "answer_body": "<p>This is how you do it:</p>\n\n<pre><code>import pandas as pd\n\n\ndef widen(x):\n    num_rows = len(x)\n    num_cols = len(x.columns)\n\n    new_index = [\n        chr(ord('A') + row_number) + str(col_number + 1)\n        for row_number in range(num_rows)\n        for col_number in range(num_cols)\n    ]\n\n    return pd.Series(x.loc[:, 'A1':].unstack().values, index=new_index)\n\nres = df.groupby('ID').apply(widen).unstack()\n</code></pre>\n\n<p>The output would be:</p>\n\n<pre><code>        A1      A2      A3      A4      B1  ...      B4     C1     C2     C3     C4\nID                                          ...                                    \n1    100.0   501.0   701.0   101.0   502.0  ...   503.0  703.0  104.0  504.0  704.0\n2   1001.0  1002.0  1003.0  1004.0     NaN  ...     NaN    NaN    NaN    NaN    NaN\n3   2001.0  5001.0  2002.0  5002.0  2003.0  ...  5004.0    NaN    NaN    NaN    NaN\n</code></pre>\n\n<p><strong>caveat</strong>: this will only work assuming each ID won't have more than 26 rows.</p>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n\n<pre><code>   ID   A1    A2    A3    A4                                      \n0  01  100   101   103   104\n1  01  501   502   503   504\n2  01  701   702   703   704\n3  02  1001  1002  1003  1004\n4  03  2001  2002  2003  2004\n5  03  5001  5002  5003  5004\n</code></pre>\n\n<p>I need the rows belonging to the same ID to be merged in a single row, the merged dataframe will be like this</p>\n\n<pre><code>   ID   A1    A2    A3    A4    B1    B2    B3     B4     C1   C2    C3    C4                                                   \n0  01  101   102   103   104   501   502    503    504    701  702   703   704 \n1  02  1001  2001  1003  1004  \n2  03  2001  2002  2003  2004  5001  5002   5003   5004\n</code></pre>\n\n<p>I tried using np.random.permutation, np.roll etc but unable to get the desired result. The count of rows in my original data set is in thousands so loops and creating individual data sets and then merging is not helping</p>\n",
        "formatted_input": {
            "qid": 58254668,
            "link": "https://stackoverflow.com/questions/58254668/merging-two-rows-of-data-in-a-single-row-with-python-pandas",
            "question": {
                "title": "merging two rows of data in a single row with Python/Pandas",
                "ques_desc": "I have a dataframe like this: I need the rows belonging to the same ID to be merged in a single row, the merged dataframe will be like this I tried using np.random.permutation, np.roll etc but unable to get the desired result. The count of rows in my original data set is in thousands so loops and creating individual data sets and then merging is not helping "
            },
            "io": [
                "   ID   A1    A2    A3    A4                                      \n0  01  100   101   103   104\n1  01  501   502   503   504\n2  01  701   702   703   704\n3  02  1001  1002  1003  1004\n4  03  2001  2002  2003  2004\n5  03  5001  5002  5003  5004\n",
                "   ID   A1    A2    A3    A4    B1    B2    B3     B4     C1   C2    C3    C4                                                   \n0  01  101   102   103   104   501   502    503    504    701  702   703   704 \n1  02  1001  2001  1003  1004  \n2  03  2001  2002  2003  2004  5001  5002   5003   5004\n"
            ],
            "answer": {
                "ans_desc": "This is how you do it: The output would be: caveat: this will only work assuming each ID won't have more than 26 rows. ",
                "code": [
                    "import pandas as pd\n\n\ndef widen(x):\n    num_rows = len(x)\n    num_cols = len(x.columns)\n\n    new_index = [\n        chr(ord('A') + row_number) + str(col_number + 1)\n        for row_number in range(num_rows)\n        for col_number in range(num_cols)\n    ]\n\n    return pd.Series(x.loc[:, 'A1':].unstack().values, index=new_index)\n\nres = df.groupby('ID').apply(widen).unstack()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 571,
            "user_id": 2132478,
            "user_type": "registered",
            "accept_rate": 12,
            "profile_image": "https://i.stack.imgur.com/ypIdH.gif?s=128&g=1",
            "display_name": "Alfonso_MA",
            "link": "https://stackoverflow.com/users/2132478/alfonso-ma"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 58210649,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1570098344,
        "creation_date": 1570059159,
        "last_edit_date": 1570087679,
        "question_id": 58210576,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58210576/grouping-panda-by-time-interval-aggregate-function",
        "title": "Grouping panda by time interval + aggregate function",
        "body": "<p>Let's say I have a panda like that:</p>\n\n<pre><code>2010-01-01 04:10:00:025     69\n2010-01-01 04:10:01:669     1\n2010-01-01 04:10:03:027     3\n2010-01-01 04:10:04:003     8\n2010-01-01 04:10:05:987     10\n2010-01-01 04:10:06:330     99\n2010-01-01 04:10:08:369     55\n2010-01-01 04:10:09:987     5000\n2010-01-01 04:10:11:148     13\n</code></pre>\n\n<p>And I need convert it in a format as following:</p>\n\n<pre><code>2010-01-01 04:10:00:000     69      69\n2010-01-01 04:10:05:000     5000    10\n2010-01-01 04:10:10:000     13      13\n</code></pre>\n\n<p>The first column corresponds to each 5 seconds interval starting at 2010-01-01 04:10:00:000.</p>\n\n<p>The second column is the max of all the grouped rows. </p>\n\n<p>The third column is the first of all the grouped rows. </p>\n\n<p>How can I get that?</p>\n",
        "answer_body": "<p>Assuming you mean <code>5 seconds</code>, we can use <code>pd.Grouper</code> with <code>agg</code> and <code>min, first</code>:</p>\n\n<pre><code># use this line if your first column is not datetime type yet.\n# df['col1'] = pd.to_datetime(df['col1'], format='%Y-%m-%d %H:%M:%S:%f')\n\ndf.groupby(pd.Grouper(key='col1', freq='5s'))['col2'].agg(['max', 'first']).reset_index()\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>                 col1   max  first\n0 2010-01-01 04:10:00    69     69\n1 2010-01-01 04:10:05  5000     10\n2 2010-01-01 04:10:10    13     13\n</code></pre>\n\n<hr>\n\n<p>Note: since you didn't provide column names, I called them <code>col1, col2</code></p>\n",
        "question_body": "<p>Let's say I have a panda like that:</p>\n\n<pre><code>2010-01-01 04:10:00:025     69\n2010-01-01 04:10:01:669     1\n2010-01-01 04:10:03:027     3\n2010-01-01 04:10:04:003     8\n2010-01-01 04:10:05:987     10\n2010-01-01 04:10:06:330     99\n2010-01-01 04:10:08:369     55\n2010-01-01 04:10:09:987     5000\n2010-01-01 04:10:11:148     13\n</code></pre>\n\n<p>And I need convert it in a format as following:</p>\n\n<pre><code>2010-01-01 04:10:00:000     69      69\n2010-01-01 04:10:05:000     5000    10\n2010-01-01 04:10:10:000     13      13\n</code></pre>\n\n<p>The first column corresponds to each 5 seconds interval starting at 2010-01-01 04:10:00:000.</p>\n\n<p>The second column is the max of all the grouped rows. </p>\n\n<p>The third column is the first of all the grouped rows. </p>\n\n<p>How can I get that?</p>\n",
        "formatted_input": {
            "qid": 58210576,
            "link": "https://stackoverflow.com/questions/58210576/grouping-panda-by-time-interval-aggregate-function",
            "question": {
                "title": "Grouping panda by time interval + aggregate function",
                "ques_desc": "Let's say I have a panda like that: And I need convert it in a format as following: The first column corresponds to each 5 seconds interval starting at 2010-01-01 04:10:00:000. The second column is the max of all the grouped rows. The third column is the first of all the grouped rows. How can I get that? "
            },
            "io": [
                "2010-01-01 04:10:00:025     69\n2010-01-01 04:10:01:669     1\n2010-01-01 04:10:03:027     3\n2010-01-01 04:10:04:003     8\n2010-01-01 04:10:05:987     10\n2010-01-01 04:10:06:330     99\n2010-01-01 04:10:08:369     55\n2010-01-01 04:10:09:987     5000\n2010-01-01 04:10:11:148     13\n",
                "2010-01-01 04:10:00:000     69      69\n2010-01-01 04:10:05:000     5000    10\n2010-01-01 04:10:10:000     13      13\n"
            ],
            "answer": {
                "ans_desc": "Assuming you mean , we can use with and : Output Note: since you didn't provide column names, I called them ",
                "code": [
                    "# use this line if your first column is not datetime type yet.\n# df['col1'] = pd.to_datetime(df['col1'], format='%Y-%m-%d %H:%M:%S:%f')\n\ndf.groupby(pd.Grouper(key='col1', freq='5s'))['col2'].agg(['max', 'first']).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 457,
            "user_id": 5507389,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8e10d016f5ee83c4ac36b3d60381a28c?s=128&d=identicon&r=PG&f=1",
            "display_name": "glpsx",
            "link": "https://stackoverflow.com/users/5507389/glpsx"
        },
        "is_answered": true,
        "view_count": 909,
        "accepted_answer_id": 58198329,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1570010401,
        "creation_date": 1570006355,
        "last_edit_date": 1570009477,
        "question_id": 58198249,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58198249/filter-duplicate-rows-of-a-pandas-dataframe",
        "title": "Filter duplicate rows of a pandas DataFrame",
        "body": "<p>I'm trying to filter the rows of a pandas DataFrame based on some conditions and I'm having difficulties with it. The DataFrame is like so: </p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'cus_id': [1111, 2222, 2222, 3333, 4444, 4444, 4444, 5555, 5555], \n                  'cus_group' : [1, 1, 0, 0, 1, 1, 0, 0, 0]})\n\nprint(df)\n\n   cus_id  cus_group\n0    1111          1\n1    2222          1\n2    2222          0\n3    3333          0\n4    4444          1\n5    4444          1\n6    4444          0\n7    5555          0\n8    5555          0\n</code></pre>\n\n<p>The selection I would like to apply is the following: </p>\n\n<p>For all <strong>cus_id</strong> that appear more than once (i.e. for all duplicates <strong>cus_id</strong>), keep only the ones where <strong>cus_group</strong> is equal to 1. Caution: If a <strong>cus_id</strong> appears more than once but it only belongs to group 0, we keep all instances of this customer.</p>\n\n<p>Visually, the resulting DataFrame I want is like so:</p>\n\n<pre><code>   cus_id  cus_group\n0    1111          1\n1    2222          1\n2    3333          0\n3    4444          1\n4    4444          1\n5    5555          0\n6    5555          0\n</code></pre>\n\n<p>As you can see for <strong>cus_id</strong> = 5555, even though it does appear twice, we keep both records since it only belongs to group 0. I have tried a few things using the duplicated() method but with no success. Any additional help is would be appreciated. </p>\n\n<hr>\n\n<p><strong>EDIT:</strong> The solution provided by <strong>jezrael</strong> works perfectly for the example above. I have noticed that in the real DataFrame I'm using there are cases where customers are linked to <code>NaN</code> group. For example:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'cus_id': [1111, 2222, 2222, 3333, 4444, 4444, 4444, 5555, 5555, 6666, 7777, 7777, ], \n                  'cus_group' : [1, 1, 0, 0, 1, 1, 0, 0, 0, np.nan, np.nan, np.nan]})\n\nprint(df)\n\n    cus_id  cus_group\n0     1111        1.0\n1     2222        1.0\n2     2222        0.0\n3     3333        0.0\n4     4444        1.0\n5     4444        1.0\n6     4444        0.0\n7     5555        0.0\n8     5555        0.0\n9     6666        NaN\n10    7777        NaN\n11    7777        NaN\n</code></pre>\n\n<p>Using the solution of <strong>jezrael</strong> those customers are dropped. Is there a quick fix to keep <strong>ALL</strong> (duplicates included) such cases in the final DataFrame? Visually (after filtering):</p>\n\n<pre><code>    cus_id  cus_group\n0     1111        1.0\n1     2222        1.0\n2     3333        0.0\n3     4444        1.0\n4     4444        1.0\n5     5555        0.0\n6     5555        0.0\n7     6666        NaN\n8     7777        NaN\n9     7777        NaN\n</code></pre>\n",
        "answer_body": "<p>One idea is filter all <code>0</code> groups with compare <code>0</code> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.all.html\" rel=\"nofollow noreferrer\"><code>GroupBy.all</code></a> and chain with <code>|</code> for bitwise <code>OR</code> for <code>1</code> rows:</p>\n\n<pre><code>df = df[df['cus_group'].eq(0).groupby(df['cus_id']).transform('all') | df['cus_group'].eq(1)]\n</code></pre>\n\n<p>Or if possible only <code>1</code> and <code>0</code> values in <code>cus_group</code> column:</p>\n\n<pre><code>df = df[df.groupby('cus_id')['cus_group'].transform('nunique').eq(1) | df['cus_group'].eq(1)]\n</code></pre>\n\n<hr>\n\n<pre><code>print(df)\n   cus_id  cus_group\n0    1111          1\n1    2222          1\n3    3333          0\n4    4444          1\n5    4444          1\n7    5555          0\n8    5555          0\n</code></pre>\n",
        "question_body": "<p>I'm trying to filter the rows of a pandas DataFrame based on some conditions and I'm having difficulties with it. The DataFrame is like so: </p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'cus_id': [1111, 2222, 2222, 3333, 4444, 4444, 4444, 5555, 5555], \n                  'cus_group' : [1, 1, 0, 0, 1, 1, 0, 0, 0]})\n\nprint(df)\n\n   cus_id  cus_group\n0    1111          1\n1    2222          1\n2    2222          0\n3    3333          0\n4    4444          1\n5    4444          1\n6    4444          0\n7    5555          0\n8    5555          0\n</code></pre>\n\n<p>The selection I would like to apply is the following: </p>\n\n<p>For all <strong>cus_id</strong> that appear more than once (i.e. for all duplicates <strong>cus_id</strong>), keep only the ones where <strong>cus_group</strong> is equal to 1. Caution: If a <strong>cus_id</strong> appears more than once but it only belongs to group 0, we keep all instances of this customer.</p>\n\n<p>Visually, the resulting DataFrame I want is like so:</p>\n\n<pre><code>   cus_id  cus_group\n0    1111          1\n1    2222          1\n2    3333          0\n3    4444          1\n4    4444          1\n5    5555          0\n6    5555          0\n</code></pre>\n\n<p>As you can see for <strong>cus_id</strong> = 5555, even though it does appear twice, we keep both records since it only belongs to group 0. I have tried a few things using the duplicated() method but with no success. Any additional help is would be appreciated. </p>\n\n<hr>\n\n<p><strong>EDIT:</strong> The solution provided by <strong>jezrael</strong> works perfectly for the example above. I have noticed that in the real DataFrame I'm using there are cases where customers are linked to <code>NaN</code> group. For example:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'cus_id': [1111, 2222, 2222, 3333, 4444, 4444, 4444, 5555, 5555, 6666, 7777, 7777, ], \n                  'cus_group' : [1, 1, 0, 0, 1, 1, 0, 0, 0, np.nan, np.nan, np.nan]})\n\nprint(df)\n\n    cus_id  cus_group\n0     1111        1.0\n1     2222        1.0\n2     2222        0.0\n3     3333        0.0\n4     4444        1.0\n5     4444        1.0\n6     4444        0.0\n7     5555        0.0\n8     5555        0.0\n9     6666        NaN\n10    7777        NaN\n11    7777        NaN\n</code></pre>\n\n<p>Using the solution of <strong>jezrael</strong> those customers are dropped. Is there a quick fix to keep <strong>ALL</strong> (duplicates included) such cases in the final DataFrame? Visually (after filtering):</p>\n\n<pre><code>    cus_id  cus_group\n0     1111        1.0\n1     2222        1.0\n2     3333        0.0\n3     4444        1.0\n4     4444        1.0\n5     5555        0.0\n6     5555        0.0\n7     6666        NaN\n8     7777        NaN\n9     7777        NaN\n</code></pre>\n",
        "formatted_input": {
            "qid": 58198249,
            "link": "https://stackoverflow.com/questions/58198249/filter-duplicate-rows-of-a-pandas-dataframe",
            "question": {
                "title": "Filter duplicate rows of a pandas DataFrame",
                "ques_desc": "I'm trying to filter the rows of a pandas DataFrame based on some conditions and I'm having difficulties with it. The DataFrame is like so: The selection I would like to apply is the following: For all cus_id that appear more than once (i.e. for all duplicates cus_id), keep only the ones where cus_group is equal to 1. Caution: If a cus_id appears more than once but it only belongs to group 0, we keep all instances of this customer. Visually, the resulting DataFrame I want is like so: As you can see for cus_id = 5555, even though it does appear twice, we keep both records since it only belongs to group 0. I have tried a few things using the duplicated() method but with no success. Any additional help is would be appreciated. EDIT: The solution provided by jezrael works perfectly for the example above. I have noticed that in the real DataFrame I'm using there are cases where customers are linked to group. For example: Using the solution of jezrael those customers are dropped. Is there a quick fix to keep ALL (duplicates included) such cases in the final DataFrame? Visually (after filtering): "
            },
            "io": [
                "   cus_id  cus_group\n0    1111          1\n1    2222          1\n2    3333          0\n3    4444          1\n4    4444          1\n5    5555          0\n6    5555          0\n",
                "    cus_id  cus_group\n0     1111        1.0\n1     2222        1.0\n2     3333        0.0\n3     4444        1.0\n4     4444        1.0\n5     5555        0.0\n6     5555        0.0\n7     6666        NaN\n8     7777        NaN\n9     7777        NaN\n"
            ],
            "answer": {
                "ans_desc": "One idea is filter all groups with compare and with and chain with for bitwise for rows: Or if possible only and values in column: ",
                "code": [
                    "df = df[df['cus_group'].eq(0).groupby(df['cus_id']).transform('all') | df['cus_group'].eq(1)]\n",
                    "df = df[df.groupby('cus_id')['cus_group'].transform('nunique').eq(1) | df['cus_group'].eq(1)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "null"
        ],
        "owner": {
            "reputation": 1592,
            "user_id": 1974919,
            "user_type": "registered",
            "accept_rate": 78,
            "profile_image": "https://www.gravatar.com/avatar/43e23c461cc56c2d5aca32366bfddff4?s=128&d=identicon&r=PG",
            "display_name": "MEhsan",
            "link": "https://stackoverflow.com/users/1974919/mehsan"
        },
        "is_answered": true,
        "view_count": 16968,
        "accepted_answer_id": 38702360,
        "answer_count": 3,
        "score": 11,
        "last_activity_date": 1569867885,
        "creation_date": 1470064236,
        "question_id": 38702332,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/38702332/what-is-the-right-syntax-when-using-notnull-in-pandas",
        "title": "What is the Right Syntax When Using .notnull() in Pandas?",
        "body": "<p>I want to use <code>.notnull()</code> on several columns of a dataframe to eliminate the rows which contain \"NaN\" values.  </p>\n\n<p>Let say I have the following <code>df</code>:</p>\n\n<pre><code>  A   B   C\n0 1   1   1\n1 1   NaN 1\n2 1   NaN NaN\n3 NaN 1   1\n</code></pre>\n\n<p>I tried to use this syntax but it does not work? do you know what I am doing wrong?</p>\n\n<pre><code>df[[df.A.notnull()],[df.B.notnull()],[df.C.notnull()]]\n</code></pre>\n\n<p>I get this Error:</p>\n\n<pre><code>TypeError: 'Series' objects are mutable, thus they cannot be hashed\n</code></pre>\n\n<p>What should I do to get the following output?</p>\n\n<pre><code>  A   B   C\n0 1   1   1\n</code></pre>\n\n<p>Any idea?</p>\n",
        "answer_body": "<p>You can first select subset of columns by <code>df[['A','B','C']]</code>, then apply <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.notnull.html\" rel=\"noreferrer\"><code>notnull</code></a> and specify if <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.all.html\" rel=\"noreferrer\"><code>all</code></a> values in mask are <code>True</code>:</p>\n\n<pre><code>print (df[['A','B','C']].notnull())\n       A      B      C\n0   True   True   True\n1   True  False   True\n2   True  False  False\n3  False   True   True\n\nprint (df[['A','B','C']].notnull().all(1))\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\nprint (df[df[['A','B','C']].notnull().all(1)])\n     A    B    C\n0  1.0  1.0  1.0\n</code></pre>\n\n<p>Another solution is from <a href=\"https://stackoverflow.com/questions/38702332/what-is-the-right-syntax-when-using-notnull-in-pandas#comment64782445_38702332\"><code>Ayhan</code></a> comment with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\" rel=\"noreferrer\"><code>dropna</code></a>:</p>\n\n<pre><code>print (df.dropna(subset=['A', 'B', 'C']))\n     A    B    C\n0  1.0  1.0  1.0\n</code></pre>\n\n<p>what is same as:</p>\n\n<pre><code>print (df.dropna(subset=['A', 'B', 'C'], how='any'))\n</code></pre>\n\n<p>and means drop all rows, where is at least one <code>NaN</code> value.</p>\n",
        "question_body": "<p>I want to use <code>.notnull()</code> on several columns of a dataframe to eliminate the rows which contain \"NaN\" values.  </p>\n\n<p>Let say I have the following <code>df</code>:</p>\n\n<pre><code>  A   B   C\n0 1   1   1\n1 1   NaN 1\n2 1   NaN NaN\n3 NaN 1   1\n</code></pre>\n\n<p>I tried to use this syntax but it does not work? do you know what I am doing wrong?</p>\n\n<pre><code>df[[df.A.notnull()],[df.B.notnull()],[df.C.notnull()]]\n</code></pre>\n\n<p>I get this Error:</p>\n\n<pre><code>TypeError: 'Series' objects are mutable, thus they cannot be hashed\n</code></pre>\n\n<p>What should I do to get the following output?</p>\n\n<pre><code>  A   B   C\n0 1   1   1\n</code></pre>\n\n<p>Any idea?</p>\n",
        "formatted_input": {
            "qid": 38702332,
            "link": "https://stackoverflow.com/questions/38702332/what-is-the-right-syntax-when-using-notnull-in-pandas",
            "question": {
                "title": "What is the Right Syntax When Using .notnull() in Pandas?",
                "ques_desc": "I want to use on several columns of a dataframe to eliminate the rows which contain \"NaN\" values. Let say I have the following : I tried to use this syntax but it does not work? do you know what I am doing wrong? I get this Error: What should I do to get the following output? Any idea? "
            },
            "io": [
                "  A   B   C\n0 1   1   1\n1 1   NaN 1\n2 1   NaN NaN\n3 NaN 1   1\n",
                "  A   B   C\n0 1   1   1\n"
            ],
            "answer": {
                "ans_desc": "You can first select subset of columns by , then apply and specify if values in mask are : Another solution is from comment with : what is same as: and means drop all rows, where is at least one value. ",
                "code": [
                    "print (df.dropna(subset=['A', 'B', 'C'], how='any'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 977,
            "user_id": 8281509,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/21f00c08610978d5e9eed0c52939c0f9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Natasha",
            "link": "https://stackoverflow.com/users/8281509/natasha"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 58128363,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1569562258,
        "creation_date": 1569562169,
        "question_id": 58128354,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58128354/filtering-rows-of-a-dataframe-based-on-values-in-columns",
        "title": "Filtering rows of a dataframe based on values in columns",
        "body": "<p>I want to filter the rows of a dataframe that contains values less than ,say 10.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\nfrom pprint import pprint\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 4)), columns=list('ABCD'))\ndf = df[df &lt;10]\n</code></pre>\n\n<p>gives,</p>\n\n<pre><code>A   B    C    D\n0  5.0 NaN  NaN  NaN\n1  NaN NaN  NaN  NaN\n2  0.0 NaN  6.0  NaN\n3  NaN NaN  NaN  NaN\n4  NaN NaN  NaN  NaN\n5  6.0 NaN  NaN  NaN\n6  NaN NaN  NaN  NaN\n7  NaN NaN  NaN  7.0\n8  NaN NaN  NaN  NaN\n9  NaN NaN  NaN  NaN\n</code></pre>\n\n<p>Expected:</p>\n\n<pre><code>0   5  57  87  95\n2   0  80   6  82\n5   6  33  74  75\n7  71  44  60   7\n</code></pre>\n\n<p>Any suggestions on how to obtain expected result?</p>\n",
        "answer_body": "<p>Use:</p>\n\n<pre><code>np.random.seed(21)\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 4)), columns=list('ABCD'))\n</code></pre>\n\n<p>If want filter by any value of condition, is necessary add <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.any.html\" rel=\"nofollow noreferrer\"><code>DataFrame.any</code></a> for test at least one <code>True</code> of boolean <code>DataFrame</code>:</p>\n\n<pre><code>df1 = df[(df &lt; 10).any(axis=1)]\nprint (df1)\n    A   B   C   D\n0  73  79  56   4\n5   5  18  70  50\n7   5  80  35  91\n9   6  84  90  28\n</code></pre>\n\n<hr>\n\n<pre><code>print (df &lt; 10)\n       A      B      C      D\n0  False  False  False   True\n1  False  False  False  False\n2  False  False  False  False\n3  False  False  False  False\n4  False  False  False  False\n5   True  False  False  False\n6  False  False  False  False\n7   True  False  False  False\n8  False  False  False  False\n9   True  False  False  False\n\nprint ((df &lt; 10).any(axis=1))\n0     True\n1    False\n2    False\n3    False\n4    False\n5     True\n6    False\n7     True\n8    False\n9     True\ndtype: bool\n</code></pre>\n",
        "question_body": "<p>I want to filter the rows of a dataframe that contains values less than ,say 10.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\nfrom pprint import pprint\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 4)), columns=list('ABCD'))\ndf = df[df &lt;10]\n</code></pre>\n\n<p>gives,</p>\n\n<pre><code>A   B    C    D\n0  5.0 NaN  NaN  NaN\n1  NaN NaN  NaN  NaN\n2  0.0 NaN  6.0  NaN\n3  NaN NaN  NaN  NaN\n4  NaN NaN  NaN  NaN\n5  6.0 NaN  NaN  NaN\n6  NaN NaN  NaN  NaN\n7  NaN NaN  NaN  7.0\n8  NaN NaN  NaN  NaN\n9  NaN NaN  NaN  NaN\n</code></pre>\n\n<p>Expected:</p>\n\n<pre><code>0   5  57  87  95\n2   0  80   6  82\n5   6  33  74  75\n7  71  44  60   7\n</code></pre>\n\n<p>Any suggestions on how to obtain expected result?</p>\n",
        "formatted_input": {
            "qid": 58128354,
            "link": "https://stackoverflow.com/questions/58128354/filtering-rows-of-a-dataframe-based-on-values-in-columns",
            "question": {
                "title": "Filtering rows of a dataframe based on values in columns",
                "ques_desc": "I want to filter the rows of a dataframe that contains values less than ,say 10. gives, Expected: Any suggestions on how to obtain expected result? "
            },
            "io": [
                "A   B    C    D\n0  5.0 NaN  NaN  NaN\n1  NaN NaN  NaN  NaN\n2  0.0 NaN  6.0  NaN\n3  NaN NaN  NaN  NaN\n4  NaN NaN  NaN  NaN\n5  6.0 NaN  NaN  NaN\n6  NaN NaN  NaN  NaN\n7  NaN NaN  NaN  7.0\n8  NaN NaN  NaN  NaN\n9  NaN NaN  NaN  NaN\n",
                "0   5  57  87  95\n2   0  80   6  82\n5   6  33  74  75\n7  71  44  60   7\n"
            ],
            "answer": {
                "ans_desc": "Use: If want filter by any value of condition, is necessary add for test at least one of boolean : ",
                "code": [
                    "np.random.seed(21)\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 4)), columns=list('ABCD'))\n",
                    "print (df < 10)\n       A      B      C      D\n0  False  False  False   True\n1  False  False  False  False\n2  False  False  False  False\n3  False  False  False  False\n4  False  False  False  False\n5   True  False  False  False\n6  False  False  False  False\n7   True  False  False  False\n8  False  False  False  False\n9   True  False  False  False\n\nprint ((df < 10).any(axis=1))\n0     True\n1    False\n2    False\n3    False\n4    False\n5     True\n6    False\n7     True\n8    False\n9     True\ndtype: bool\n"
                ]
            }
        }
    }
]