"qid": 64110166
"link": https://stackoverflow.com/questions/64110166/how-to-create-bins-for-a-dataframe-column-if-the-range-is-given
"question": {
	"title": How to create bins for a dataframe column if the range is given
	"desc":  This is an example data frame that I want to play with If I do this, I get the output as: Here's the twist: Let's say the age columns can take values between 18 to 58(the range of the column) and I want the bins(or the output) as: How can I do that? because 'cut' takes the values which are in the column. I got the desired result by doing it manually but if the values of bins were say 100 - how can I do it? 
}
"io": {
	"Frame-1": 
		0    (17.964, 25.2]
		1    (17.964, 25.2]
		2      (25.2, 32.4]
		3      (25.2, 32.4]
		4      (32.4, 39.6]
		5      (32.4, 39.6]
		6      (39.6, 46.8]
		7      (46.8, 54.0]
		8      (46.8, 54.0]
		
	"Frame-2":
		0    (18.0, 26.0]
		1    (18.0, 26.0]
		2    (26.0, 34.0]
		3    (26.0, 34.0]
		4    (34.0, 42.0]
		5    (34.0, 42.0]
		6    (34.0, 42.0]
		7    (50.0, 58.0]
		8    (42.0, 50.0]
		
}
"answer": {
	"desc": %s You can pass an array to : Output: 
	"code-snippets": [
		pd.cut(df['Age'], bins=np.linspace(18, 58, 100), include_lowest=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 41428539
"link": https://stackoverflow.com/questions/41428539/data-frame-to-file-txt-python
"question": {
	"title": data frame to file.txt python
	"desc": I have this dataframe I want to save it as a text file with this format I tried this code but is not working: 
}
"io": {
	"Frame-1": 
		       X    Y  Z    Value 
		0      18   55  1      70   
		1      18   55  2      67 
		2      18   57  2      75     
		3      18   58  1      35  
		4      19   54  2      70   
		
	"Frame-2":
		   X    Y  Z    Value 
		   18   55  1      70   
		   18   55  2      67 
		   18   57  2      75     
		   18   58  1      35  
		   19   54  2      70   
		
}
"answer": {
	"desc": %s This is an almost exact duplicate of the following: Python, Pandas : write content of DataFrame into text File I report again here the answer from the cited SO question with some very small modifications to fit this case. You can use two methods. np.savetxt(), in which case you should have something like the following: assuming is the dataframe. Of course you can change the delimiter you want (tab, comma, space,etc.). The other option, as mentioned in the answer I attached and in the answer here from @MYGz, is to use the to_csv method, i.e.: 
	"code-snippets": [
		np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter="\t", header="X\tY\tZ\tValue")  
		
		----------------------------------------------------------------------
		a.to_csv('xgboost.txt', header=True, index=False, sep='\t', mode='a')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64032701
"link": https://stackoverflow.com/questions/64032701/get-last-purchase-year-from-sales-data-pivot-in-pandas
"question": {
	"title": Get &quot;Last Purchase Year&quot; from Sales Data Pivot in Pandas
	"desc": I have pivoted the Customer ID against their year of purchase, so that I know how many times each customer purchased in different years: My desired result is to append the column names with the latest year of purchase, and thus the number of years since their last purchase: Here is what I tried: However what I got is "TypeError: cannot convert the series to <class 'float'>" Could anyone help me to get the result I need? Thanks a lot! Dennis 
}
"io": {
	"Frame-1": 
		Customer ID      1996  1997 ... 2019  2020
		100000000000001     7     7 ...  NaN   NaN
		100000000000002     8     8 ...  NaN   NaN
		100000000000003     7     4 ...  NaN   NaN
		100000000000004   NaN   NaN ...   21    24              
		100000000000005    17    11 ...   18   NaN
		
	"Frame-2":
		Customer ID      1996  1997 ... 2019  2020   Last   Recency
		100000000000001     7     7 ...  NaN   NaN   1997        23 
		100000000000002     8     8 ...  NaN   NaN   1997        23
		100000000000003     7     4 ...  NaN   NaN   1997        23
		100000000000004   NaN   NaN ...   21    24   2020         0 
		100000000000005    17    11 ...   18   NaN   2019         1
		
}
"answer": {
	"desc": %s You can get last year of purchase using + and along then subtract this last year of purchase from the max year to compute : 
	"code-snippets": [
		c = df.filter(regex=r'\d+').columns
		df['Last'] = df[c].notna().cumsum(1).idxmax(1)
		df['Recency'] = c.max() - df['Last']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63974864
"link": https://stackoverflow.com/questions/63974864/dataframes-average-columns
"question": {
	"title": DataFrames - Average Columns
	"desc": I have the following dataframe in pandas I am looking to create a dataframe which contains averages of columns 1& 2, Columns 3 &4, and so on. I was using this, but it is averaging everything. Is there a way, that I can add the column headers, when averaging each row. If not, another way would be to create two arrays, average them and then create a new dataframe. 
}
"io": {
	"Frame-1": 
		Column 1   Column 2  Column3   Column 4
		   2           2         2         4
		   1           2         2         3
		
	"Frame-2":
		  ColumnAvg(12)      ColumnAvg(34)
		      2                   3
		      1.5                 1.5
		
}
"answer": {
	"desc": %s you can use and group it with `//2' th index, which will group it by exactly 2 columns to start with index 2, as you mentioned in comment 
	"code-snippets": [
		  df.groupby((np.arange(len(df.columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')
		
		----------------------------------------------------------------------
		 dfgrp=  df.iloc[:,2:].groupby((np.arange(len(df.iloc[:,2:].columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')
		 dfnew = pd.concat([df.iloc[:,2:],dfgrp])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63970182
"link": https://stackoverflow.com/questions/63970182/concat-list-of-dataframes-containing-empty-dataframes
"question": {
	"title": Concat list of dataframes containing empty dataframes
	"desc": I have this list of dataframes and some of them are Empty. I do not want to discard them as it would change my number of rows when I concatenate it. I want to convert them into NA values so that these 5 dataframes are converted to 5 rows. List of dataframes: Code: Output: Desired Output: 
}
"io": {
	"Frame-1": 
		0   102,000,000.00  2,000,000.00    1,400,000.00    0.00
		0   60,900,000.00   1,300,000.00    0.00            0.00
		
	"Frame-2":
		0   102,000,000.00  2,000,000.00    1,400,000.00    0.00
		0   NA                   NA           NA             NA
		0   NA                   NA           NA             NA
		0   NA                   NA           NA             NA
		0   60,900,000.00   1,300,000.00    0.00            0.00
		
}
"answer": {
	"desc": %s I would recommend a list comprehension approach where you update the empty data frames with a template data frame: This updates data frames where with the data frame defined in . A working example: 
	"code-snippets": [
		template = pd.DataFrame(data = [[pd.NA, pd.NA, pd.NA, pd.NA]], columns = [0,1,2,3])
		dataframes= [i if not i.empty else template for i in dataframes]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63867276
"link": https://stackoverflow.com/questions/63867276/interpolates-one-series-and-outputs-constant-for-the-second-constant-series
"question": {
	"title": Interpolates one series, and outputs constant for the second (constant) series
	"desc": I'm trying to create a function that fills in missing numbers in multiple series, with different numerical scales, and at the same time generates a constant column for each of the series. Is it possible to create the following function with Pandas? Sample dataframe: Expected output: 
}
"io": {
	"Frame-1": 
		1029 400
		1035 400
		1031 340
		1039 340
		1020 503
		1025 503
		
	"Frame-2":
		1029 400
		1030 400
		1031 400
		1032 400
		1033 400
		1034 400
		1035 400
		1031 340
		1032 340
		1033 340
		1034 340
		1035 340
		1036 340
		1037 340
		1038 340
		1039 340
		1020 503
		1021 503
		1022 503
		1023 503
		1024 503
		1025 503
		
}
"answer": {
	"desc": %s  Details: Identify all the monotonically increasing sections in the where there are missing values. This can be done with the help of + and to create a grouper : Then the dataframe on this grouper and for each grouped frame the gaps using a custom define function which makes the use of method of the dataframe to fill the missing values. Finally using , concatenate all these grouped frames after filling the missing values. 
	"code-snippets": [
		def fill(d):
		    idx = range(d['col1'].min(), d['col1'].max() + 1)
		    return d.set_index('col1').reindex(idx, method='ffill').reset_index()
		
		
		g = df['col1'].lt(df['col1'].shift()).cumsum()
		df = pd.concat([fill(g) for k, g in df.groupby(g)], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63855152
"link": https://stackoverflow.com/questions/63855152/drop-a-pandas-dataframe-row-that-comes-after-a-row-that-contains-a-particular-va
"question": {
	"title": Drop a pandas DataFrame row that comes after a row that contains a particular value
	"desc": I am trying to drop all rows that come after a row which has inside the column df: Required output df: Look at the following code: Returns a error message I have tried many different variations of this using different methods like and but I can't seem to figure it out anyway. I have also tried truncate: This returns: IndexError: index 1 is out of bounds for axis 0 with size 1 
}
"io": {
	"Frame-1": 
		  Ammend
		 0  no
		 1  yes
		 2  no
		 3  no
		 4  yes
		 5  no
		
	"Frame-2":
		  Ammend
		 0  no
		 1  yes
		 3  no
		 4  yes
		
}
"answer": {
	"desc": %s Try this Output: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		np.random.seed(525)
		df = pd.DataFrame({'Other': np.random.rand(10), 'Ammend': np.random.choice(['yes', 'no'], 10)})
		df
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63849171
"link": https://stackoverflow.com/questions/63849171/how-to-split-dataframe-made-from-objects
"question": {
	"title": How to split dataframe made from objects?
	"desc": I want to split one column pandas dataframe that look like this: into two columns: So it can look like this: But its showing type as: 
}
"io": {
	"Frame-1": 
		    0
		0   38 A
		1   35 B
		2   14 B
		
	"Frame-2":
		    Number  Letter
		0   38        A
		1   35        B
		2   14        B
		
}
"answer": {
	"desc": %s You can do this a couple of ways and possibly more: Using: capture dataframe above. Option 1 (Use string accessor and split): Option 2 (use string accessor and extract with named groups regular expressions): Output: 
	"code-snippets": [
		df['0'].str.split(' ', expand=True).set_axis(['Number', 'Letter'], axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63799400
"link": https://stackoverflow.com/questions/63799400/how-to-split-dataframe-at-headers-that-are-in-a-row
"question": {
	"title": How to split dataframe at headers that are in a row
	"desc": I've got a page I'm scraping and most of the tables are in the format Heading --info. I can iterate through most of the tables and create separate dataframes for all the various information using pandas.read_html. However, there are some where they've combined information into one table with subheadings that I want to be separate dataframes with the text of that row as the heading (appending a list). Is there an easy way to split this dataframe - It will always be heading followed by associated rows, new heading followed by new associated rows. eg. Should be It'd be nice if people would just create web pages that made sense with the data but that's not the case here. I've tried iterrows but cannot seem to come up with a good way to create what I want. Help would be very much appreciated! 
}
"io": {
	"Frame-1": 
		   Col1   Col2
		0  thing   
		1  1       2
		2  2       3
		3  thing2  
		4  1       2
		5  2       3
		6  3       4
		
	"Frame-2":
		thing
		1   1   1
		2   2   2
		
		thing2
		4   1   2
		5   2   3
		6   3   4
		
}
"answer": {
	"desc": %s You can use Output: 
	"code-snippets": [
		import numpy as np
		
		
		res = [x.reset_index(drop=True) for x in np.split(df, np.where(df.applymap(lambda x: x == ''))[0]) if not x.empty]
		for x in res:
		    x = x.rename(columns=x.iloc[0]).drop(x.index[0])
		    print(x)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63779231
"link": https://stackoverflow.com/questions/63779231/pandas-fill-in-missing-data-from-columns-in-other-rows
"question": {
	"title": Pandas fill in missing data from columns in other rows
	"desc": I have a df like below: Output: For ac are null, get prev's value, and then look up at the id column. Fill in the null with value at ac column. Expected output: How do I achieve this? Thanks. 
}
"io": {
	"Frame-1": 
		    id  ac   prev
		0   a   123  NaN  
		1   b   223  NaN  
		2   c   NaN  a  
		3   d   NaN  b
		
	"Frame-2":
		    id  ac   prev
		0   a   123  NaN
		1   b   223  NaN
		2   c   123  a
		3   d   223  b
		
}
"answer": {
	"desc": %s You can use to create a boolean mask the use boolean indexing with this mask to the values of column to column based on : Result: 
	"code-snippets": [
		m = df['ac'].isna()
		df.loc[m, 'ac'] = df.loc[m, 'prev'].map(df.set_index('id')['ac'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 48475687
"link": https://stackoverflow.com/questions/48475687/python-combine-two-pandas-dataframes-extend-index-if-needed
"question": {
	"title": Python: Combine two Pandas Dataframes, extend index if needed
	"desc": How can I combine two Dataframes into one with keping all rows and all index values of both Dataframes? Let's say, I have two dataframes, with partly different index values: I want to create a new dataframe, which contains both columns with a combined index. I tried: which results in: where I would like to have, all rows & index values preserved, with NaN, if no value is available for this index value: 
}
"io": {
	"Frame-1": 
		          a         b
		0 -1.089084       NaN
		2 -0.552297  1.704591
		3 -0.242239 -0.803438
		4  0.247463 -1.511515
		5 -0.139740       NaN
		
	"Frame-2":
		          a         b
		0 -1.089084       NaN
		1       NaN -0.407245
		2 -0.552297  1.704591
		3 -0.242239 -0.803438
		4  0.247463 -1.511515
		5 -0.139740       NaN
		6       NaN  0.303360
		
}
"answer": {
	"desc": %s Try pandas.concat function: https://pandas.pydata.org/pandas-docs/stable/merging.html Output: 
	"code-snippets": [
		dd = pd.concat([df1, df2], axis=1)
		print(dd)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63757556
"link": https://stackoverflow.com/questions/63757556/heatmap-of-counts-of-every-value-in-every-column
"question": {
	"title": Heatmap of counts of every value in every column
	"desc": I have a dataframe that is like this: Where A B C D are categories, and the values are in the range [1, 10] (some values might not appear in a single column) I would like to have a dataframe that for every category shows the count of those values. Something like this: I tried using and but I can't seem to understand what parameters to give. 
}
"io": {
	"Frame-1": 
		| A | B | C  | D |  
		|---|---|----|---|  
		| 1 | 3 | 10 | 4 |  
		| 2 | 3 | 1  | 5 |  
		| 1 | 7 | 9  | 3 |  
		
	"Frame-2":
		|    | A | B  | C | D |
		|----|---|----|---|---|  
		| 1  | 2 | 0  | 1 | 0 |
		| 2  | 1 | 0  | 0 | 0 |
		| 3  | 0 | 2  | 0 | 1 |
		| 4  | 0 | 0  | 0 | 1 |
		| 5  | 0 | 0  | 0 | 1 |
		| 6  | 0 | 0  | 0 | 0 |
		| 7  | 0 | 1  | 0 | 0 |
		| 8  | 0 | 0  | 0 | 0 |
		| 9  | 0 | 0  | 1 | 0 |
		| 10 | 0 | 0  | 1 | 0 | 
		
}
"answer": {
	"desc": %s  Use applies for each column will plot a If a Pandas DataFrame is provided, the index/column information will be used to label the columns and rows. Option 1 Option 2 There are a number of style options available with heatmap, and changing the color with can improve the visualization. seaborn: palettes I think Option 1, without looks less busy. default color 
	"code-snippets": [
		# counts
		counts = df.apply(pd.value_counts).fillna(0)
		
		# plot
		sns.heatmap(counts, cmap="GnBu", annot=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63712894
"link": https://stackoverflow.com/questions/63712894/how-to-re-order-this-dataframe-in-python-wthout-hard-coding-column-values
"question": {
	"title": How to re order this DataFrame in Python wthout hard coding column values?
	"desc": I have a df ('COL3 SUM' is the full name with a space): How can I re order this df so that 'COL3 SUM' always comes at the end of the dataframe like so without re ordering any of the rest of the df? 
}
"io": {
	"Frame-1": 
		COL1 COL2 COL3 SUM  COL4  COL5
		   1     2       3     4     5
		
	"Frame-2":
		   COL1 COL2  COL4  COL5 COL3 SUM 
		       1    2    4     5        3
		
}
"answer": {
	"desc": %s If you want to move all columns with some keyword e.g. to the end: create list of unmoved columns append excluded columns to end reorder dataframe by calling on new column list code: output: 
	"code-snippets": [
		new_cols = [col for col in df.columns if "SUM" not in col]
		moved_cols = list(set(df.columns) - set(new_cols)) 
		new_cols.extend(moved_cols)
		df = df[new_cols]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63000962
"link": https://stackoverflow.com/questions/63000962/python-selecting-rows-matching-feb-till-current-month-in-pandas-dataframe
"question": {
	"title": Python: Selecting rows matching Feb till current month in pandas dataframe
	"desc": I have below dataframe: Considering the current month is June, the expected output is as follows: Here i have done filtering of rows by months from Feb:Current Month (in this case June) and then group by to find all the names once per mode. (Example: F will be only once for actual and once for plan) I previously tried taking a transpose of columns and then using the below to summarize the data till current month: where : But this is getting very complicated since the actual data has lots and lots of modes and also many years of data. Is there any easier solution for the same where this complication can be avoided and the similar solution can be achieved? In the end i am expecting to have the below dataframe: I guess i can have this format using pivot_table in pandas: 
}
"io": {
	"Frame-1": 
		Name1   Name2   Mode    Value1  Value2
		C       N       Actual    4       4                                             
		D       N       Plan      2       2                                             
		E       N       Actual    10      10                                                
		F       N       Actual    7       7                                             
		F       N       Plan      3       3                                             
		
	"Frame-2":
		Name1   Name2   Actual_Value1   Actual_Value2   Plan_Value1 Plan_Value1
		C       N           4              4                            
		D       N                                           2             2         
		E       N           10             10                               
		F       N           7              7                3             3 
		    
		
}
"answer": {
	"desc": %s  In Pandas, use pivot_table to transpose table data (rows become columns). Be sure to use reset_index() to convert the pivot object to a dataframe Based on your sample dataset, this code gave the results you're looking for: 
	"code-snippets": [
		lstAllMonths=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
		curMth = datetime.today().month  # 7=July
		curMth = 6  # Jun for testing
		lstMth = lstAllMonths[1:curMth]
		
		df = df[df['Month'].isin(lstMth)][['Name1','Name2','Mode','Value1','Value2']]
		gb = df.groupby(['Name1','Name2','Mode'])
		dfagg = gb.agg({'Value1':sum, 'Value2':sum})
		
		dfpvt = pd.pivot_table(dfagg,index=['Name1', 'Name2'], 
		                      columns=['Mode'],
		                      values=['Value1', 'Value2'], 
		                      aggfunc=np.sum, fill_value=0).reset_index().rename_axis(1)
		                      
		dfpvt.columns=['Name1','Name2','Actual_Value1','Plan_Value1','Actual_Value2','Plan_Value2']
		dfpvt.replace(0,'', inplace=True)
		dfpvt = dfpvt[['Name1','Name2','Actual_Value1','Actual_Value2','Plan_Value1','Plan_Value2']]    
		print(dfpvt)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63674555
"link": https://stackoverflow.com/questions/63674555/pandas-drop-diffrent-rows-with-differing-column-values
"question": {
	"title": pandas drop diffrent rows with differing column values
	"desc": I have DataFrame that looks like Input: I would like to remove rows from "col1" that share a common value in "col2" except values that are the same i.e. letter "e". I would like it to be where only one value in "col1" can = a unique one in "col2" The expected output would look something like... Output: What would be the process of doing this? 
}
"io": {
	"Frame-1": 
		     col1 col2 col3
		    0   a   1   1
		    1   b   3   2
		    2   c   3   3
		    3   d   2   4
		    4   e   6   5
		    5   e   6   6
		
	"Frame-2":
		     col1 col2 col3
		    0   a   1   1
		    3   d   2   4
		    4   e   6   5
		    5   e   6   6
		
}
"answer": {
	"desc": %s Based on what you described, I understood as follows: If two rows have same values in , they both are dropped. If two rows have same value in but have same values in , you want to keep them. All other rows which do not fall in above two categories, you want to keep. This can be achieved as: 
	"code-snippets": [
		df[np.logical_or(~df.duplicated('col2', keep = False),df.duplicated('col1', keep = False)) ]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63621901
"link": https://stackoverflow.com/questions/63621901/how-to-average-dataframe-row-with-another-row-only-if-the-first-row-is-a-substri
"question": {
	"title": How to average DataFrame row with another row only if the first row is a substring of other next row
	"desc": I have a dataframe called 'data': I want to combine 'ABC-1' with 'ABC-1B' into a single row using the first USER name and then averaging the two values to arrive here: The dataframe may not be in order and there are other values in there as well that are unrelated that don't need averaging. I only want to average the two rows where 'XXX-X' is in 'XXX-XB' 
}
"io": {
	"Frame-1": 
		USER    VALUE
		XOXO      21
		ABC-1      2
		ABC-1B     4
		ABC-2      4
		ABC-2B     6
		PEPE      12
		
	"Frame-2":
		USER    VALUE
		XOXO      21
		ABC-1      3
		ABC-2      5
		PEPE      12
		
}
"answer": {
	"desc": %s Let's try, 
	"code-snippets": [
		df.USER = df.USER.str.replace('(-\d)B', r"\1")
		df = df.groupby("USER", as_index=False, sort=False).VALUE.mean()
		
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63620593
"link": https://stackoverflow.com/questions/63620593/replace-nan-with-values-in-a-row-from-previous-matching-values-in-column
"question": {
	"title": Replace NaN with values in a row from previous matching values in column
	"desc": I have following data frame (df). And I want to get to this state: I want to go through both columns and replace NaN with appropriate zip_code or city. Here is what I have done but as you can see it didn't fully work. If columns 'zip_mapped' and 'city_mapped' were properly populated I would have just replaced them with original cols. Can anyone help me here? 
}
"io": {
	"Frame-1": 
		df
		     city    zip_code
		0    city1  90287
		1    city2  90288
		2    city3  80023
		3    city4  90210
		4    city1  NaN
		5    city4  NaN
		6    city7  NaN
		7    NaN    90210
		8    NaN    80023
		
	"Frame-2":
		    city     zip_code
		0   city1   90287
		1   city2   90288
		2   city3   80023
		3   city4   90210
		4   city1   90287
		5   city4   90210
		6   city7   NaN
		7   city4   90210
		8   city3   80023
		
}
"answer": {
	"desc": %s Let's try twice on different groupby: Output: 
	"code-snippets": [
		df.zip_code = df.zip_code.fillna(df.zip_code.groupby(df.city).transform('first'))
		
		df.city = df.city.fillna(df.city.groupby(df.zip_code).transform('first'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63616688
"link": https://stackoverflow.com/questions/63616688/round-in-pandas-column-scientific-numbers
"question": {
	"title": Round in pandas column scientific numbers
	"desc": Hello I have a df such as : I knwo how to roun the digit in the COL2 by using but if I do the same for COL1 it only displays 0 how can I get instead: in fact the big issue is here: it displays : and I would like to keep only 3 number after the coma 
}
"io": {
	"Frame-1": 
		COL1        COL2
		0.005554    0.35200000000000004
		5.622e-11   0.267
		0.006999999999999999    0.307
		2.129e-14   0.469
		2.604e-14   0.39
		1.395e-60   0.27899999999999997
		8.589999999999998e-74   0.29600000000000004
		1.025e-42   0.4270000000000001
		
	"Frame-2":
		COL1        COL2
		0.005   0.352
		5.622e-11   0.267
		0.007   0.307
		2.129e-14   0.469
		2.604e-14   0.39
		1.395e-60   0.279
		8.560e-74   0.296
		1.025e-42   0.427
		
}
"answer": {
	"desc": %s You don't want to round the values in the first column - rounding means that the lower digits are gone. I think what you actually want to do is to change how Pandas displays the data on screen and not the actual values in the dataframe. The default is already scientific notation. You can specify the display format for all columns like so (this will be showing the first three digits after the dot): Since you want to keep the format of the first one but not of the second you might need to set the format for each column individually. You can use styling for column 2: Edit: Since your answer is for saving to CSV, you'll need to pass this as an argument to : . This will save all columns in scientific notation. If you only want this for column 1, you could try the solutions to this question. 
	"code-snippets": [
		pd.set_option('display.float_format', lambda x: '%.3f' % x)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63609893
"link": https://stackoverflow.com/questions/63609893/merge-two-dataframes-with-uncertainty-on-keys
"question": {
	"title": Merge two dataframes with uncertainty on keys
	"desc": I am trying to use 'merge' to combine two data frames with shape: Those looks as follows: What I am looking for is to merge columns 'A' and 'B' with a defined uncertainty. For example + - 0.5. I don't have clear how to handle this. What I was trying to do is to manually add an uncertainty: After this, I do the merge: But here I got stuck because I can not figure it out how to use a conditional merge. The idea is to merge all those rows were columns 'A' and 'B' be the same with a definite certainty The expected output would be: 
}
"io": {
	"Frame-1": 
		In [29]: df1
		Out[29]: 
		   ID1  A  B
		0   10  1  3
		1   32  4  5
		2   53  2  2
		3   65  5  9
		4    3  4  3
		
		In [32]: df2
		Out[32]: 
		   ID2  A  B
		0   68  9  6
		1   35  5  5
		2   93  3  1
		3    5  7  9
		4   23  4  3
		
	"Frame-2":
		df3:
		   ID1  A  B  ID2  A  B
		1  32   4  5  35   5  5
		2  53   2  2  93   3  1
		4  3    4  3  23   4  3 
		
}
"answer": {
	"desc": %s Simple but expensive Just do a Cartesian product then select down rows you want. This will work well for smaller data sets but will be very expensive on large data sets output Nearest value match is provided by merge_asof can only match on one column and it must be ordered if's a left join so choose which data frame you want to drive it it's fairly common to calculate a value which is used as join. This is represented in second approach below. In GIS / GPS use cases it's typically distance - hence reason I named column d first example is doing two merges, first on A then on B. You have all the data together and can then do further logic output 
	"code-snippets": [
		dfs = (df1.assign(foo=1).merge(df2.assign(foo=1), on="foo", suffixes=("","_df2"))
		 .assign(adiff=lambda x: x["A"]-x["A_df2"])
		 .assign(bdiff=lambda x: x["B"]-x["B_df2"])
		 .query("adiff>=-1 and adiff<=1 and bdiff>=-1 and bdiff<=1")
		 .drop(columns=["adiff","bdiff","foo"])
		)
		
		print(dfs.to_string(index=False))
		
		----------------------------------------------------------------------
		df1 = pd.DataFrame({'ID1':[10,32,53,65,3],'A':[1,4,2,5,4],'B':[3,5,2,9,3]})
		df2 = pd.DataFrame({'ID2':[68,35,93,5,23],'A':[9,5,3,7,4],'B':[6,5,1,9,3]})
		
		
		# find nearest row in df2 when matching on columns A or B
		dfmab = pd.merge_asof(
		    (pd.merge_asof(df1.sort_values("A"), df2.sort_values("A"), 
		                   on="A", direction="nearest", suffixes=("","_Adf2"))
		    ).sort_values("B") 
		    ,df2.sort_values("B"),
		    on="B", direction="nearest", suffixes=("","_Bdf2")
		)
		
		# calculate a value that represents value of row
		dfmd = pd.merge_asof(
		    df1.assign(d=lambda x: x["A"]*x["B"]).sort_values("d"),
		    df2.assign(d=lambda x: x["A"]*x["B"]).sort_values("d"),
		    on="d", direction="nearest", suffixes=("","_Ddf2")
		)
		
		print(dfmab.to_string(index=False))
		print(dfmd.to_string(index=False))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63598638
"link": https://stackoverflow.com/questions/63598638/applying-a-function-to-chunks-of-the-dataframe
"question": {
	"title": Applying a function to chunks of the Dataframe
	"desc": I have a (for instance - simplified version) and generated 20 bootstrap resamples that are all now in the same df but differ in the Resample Nr. Now I want to apply a certain function on each Reample Nr. Say: The outlook would look like this: So there are 20 different new values. I know there is a df.iloc command where I can specify my row selection but I would like to find a command where I don't have to repeat the code for the 20 samples. My goal is to find a command that identifies the Resample Nr. automatically and then calculates the function for each Resample Nr. How can I do this? Thank you! 
}
"io": {
	"Frame-1": 
		              A    B 
		 0           2.0   3.0
		 1           3.0   4.0
		
	"Frame-2":
		                                A    B 
		   
		0     1             0           2.0   3.0
		1     1             1           3.0   4.0
		2     2             1           3.0   4.0
		3     2             1           3.0   4.0
		..    ..
		..    .. 
		39    20            0           2.0    3.0
		40    20            0           2.0    3.0
		
}
"answer": {
	"desc": %s Use to create two new columns and that corresponds to and , then use on (or ) and using : Result: 
	"code-snippets": [
		s = df.assign(x=df['A'].mul(df['B']), y=df['B']**2)\
		      .groupby(level=1)[['x', 'y']].transform('sum')
		df['C'] = s['x'].div(s['y'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63595559
"link": https://stackoverflow.com/questions/63595559/change-the-value-of-column-based-on-quantity-of-equals-rows
"question": {
	"title": Change the value of column based on quantity of equals rows
	"desc": I have a dataframe like this: I need to change the value of column to 1 if value of row equals the actual quantity of rows, where columns and are equals (row0 and row1 in this example). Desired output: 
}
"io": {
	"Frame-1": 
		             id desc  quantity
		0  B668441DE83B  Car         2
		1  B668441DE83B  Car         2
		2  B668441DE83B  Bus         1
		3  89C26DEE41E2  Bus         3
		4  89C26DEE41E2  Bus         3 
		
	"Frame-2":
		             id desc  quantity
		0  B668441DE83B  Car         1
		1  B668441DE83B  Car         1
		2  B668441DE83B  Bus         1
		3  89C26DEE41E2  Bus         3
		4  89C26DEE41E2  Bus         3 
		
}
"answer": {
	"desc": %s Use for count values per groups, compare by for by original and last set by mask: Or: 
	"code-snippets": [
		mask = df.groupby(['id','desc'])['id'].transform('size').eq(df['quantity'])
		
		df.loc[mask, 'quantity'] = 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63583217
"link": https://stackoverflow.com/questions/63583217/multiply-dataframe-rows-depends-on-value-in-this-row
"question": {
	"title": Multiply dataframe rows depends on value in this row
	"desc": I have a dataframe like this: I need multiply rows depends on value in 'col3'. Desired output: 
}
"io": {
	"Frame-1": 
		   col1      col2  col3
		0    69     bar34     4
		1    77    barf30     2
		2    88  barfoo29     5 
		
	"Frame-2":
		    col1      col2  col3
		0     69     bar34     4
		1     69     bar34     4
		2     69     bar34     4
		3     69     bar34     4
		4     77    barf30     2
		5     77    barf30     2
		6     88  barfoo29     5 
		7     88  barfoo29     5 
		8     88  barfoo29     5 
		9     88  barfoo29     5 
		10    88  barfoo29     5 
		
}
"answer": {
	"desc": %s Here is a solution, then 
	"code-snippets": [
		df = df.set_index(df.col3)
		
		print(
		    df.reindex(df.index.repeat(df.col3))
		        .reset_index(drop=True)
		)
		
		----------------------------------------------------------------------
		# suggested by @anky,
		
		df.loc[df.index.repeat(df.col3)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63479071
"link": https://stackoverflow.com/questions/63479071/how-to-convert-a-pandas-dataframe-column-from-string-to-an-array-of-floats
"question": {
	"title": How to convert a pandas dataframe column from string to an array of floats?
	"desc": I have a dataframe where a column is an array of floats. When I am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows: I want to convert this long character string into an array of floats like this: Is there a way to do that? 
}
"io": {
	"Frame-1": 
		'[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0]', 
		'[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0]',....
		
	"Frame-2":
		[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0],
		[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0],...
		
}
"answer": {
	"desc": %s If possible, it would be best to read the csv correctly (as a list of floats) rather than casting them from the strings. You can however use or to cast this to a list of floats: 
	"code-snippets": [
		from ast import literal_eval    
		df["a"] = df["a"].apply(lambda x: literal_eval(x))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63451784
"link": https://stackoverflow.com/questions/63451784/how-to-combine-rows-into-seperate-dataframe-python-pandas
"question": {
	"title": How to combine rows into seperate dataframe python pandas
	"desc": i have the following dataset: i want to combine x y z into another dataframe like this: and i want these dataframes for each x y z value like first, second third and so on. how can i select and combine them? desired output: 
}
"io": {
	"Frame-1": 
		A           B           C           D   E   F
		154.6175111 148.0112337 155.7859835 1   1   x
		255 253.960131  242.5382584         1   1   x
		251.9665958 235.1105659 185.9121703 1   1   x
		137.9974994 225.3985177 254.4420772 1   1   x
		85.74722877 116.7060415 158.4608395 1   1   x
		123.6969939 140.0524405 132.6798037 1   1   x
		133.3251695 80.08976196 38.81201612 1   1   y
		118.0718812 243.5927927 255         1   1   y
		189.5557302 139.9046713 91.90519519 1   1   y
		172.3117291 188.000268  129.8155501 1   1   y
		48.07634611 21.9183119  25.99669279 1   1   y
		23.40525987 8.395857933 25.62371342 1   1   y
		228.753009  164.0697727 172.6624107 1   1   z
		203.3405006 173.9368303 189.8103708 1   1   z
		184.9801932 117.1591341 87.94739034 1   1   z
		29.55251224 46.03945452 70.7433477  1   1   z
		143.6159623 120.6170926 155.0736604 1   1   z
		142.5421179 128.8916843 169.6013111 1   1   z
		
		
	"Frame-2":
		A           B           C           D   E   F
		154.6175111 148.0112337 155.7859835 1   1   x
		133.3251695 80.08976196 38.81201612 1   1   y
		228.753009  164.0697727 172.6624107 1   1   z
		
		A           B           C           D   E   F
		255 253.960131  242.5382584         1   1   x
		118.0718812 243.5927927 255         1   1   y
		203.3405006 173.9368303 189.8103708 1   1   z
		
		A           B           C           D   E   F
		251.9665958 235.1105659 185.9121703 1   1   x
		189.5557302 139.9046713 91.90519519 1   1   y
		184.9801932 117.1591341 87.94739034 1   1   z
		
		A           B           C           D   E   F
		137.9974994 225.3985177 254.4420772 1   1   x
		172.3117291 188.000268  129.8155501 1   1   y
		29.55251224 46.03945452 70.7433477  1   1   z
		
		A           B           C           D   E   F
		85.74722877 116.7060415 158.4608395 1   1   x
		48.07634611 21.9183119  25.99669279 1   1   y
		143.6159623 120.6170926 155.0736604 1   1   z
		
		A           B           C           D   E   F
		123.6969939 140.0524405 132.6798037 1   1   x
		23.40525987 8.395857933 25.62371342 1   1   y
		142.5421179 128.8916843 169.6013111 1   1   z
		
}
"answer": {
	"desc": %s Use for counter and then loop by another groupby object: 
	"code-snippets": [
		g = df.groupby('F').cumcount()
		
		for i, g in df.groupby(g):
		    print (g)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63440751
"link": https://stackoverflow.com/questions/63440751/sort-pandas-dataframe-column-index-by-date
"question": {
	"title": Sort Pandas dataframe column index by date
	"desc": I want to sort dataframe by column index. The issue is my columns are 'dates' dd/mm/yyyy directly imported from my excel. For ex: The output I want is: I am using It is giving me following error: TypeError: '<' not supported between instances of 'datetime.datetime' and 'str' I want to do it in panda dataframe. Any help will be appreciated. Thanks 
}
"io": {
	"Frame-1": 
		    10/08/20  12/08/20 11/08/20
		0   2.0        6.0       15.0
		1   6.0        11.0      8.0
		2   4.0        7.0       3.0
		3   7.0        12.0      2.0
		4   12.0       5.0       7.0
		
	"Frame-2":
		    10/08/20  11/08/20 12/08/20
		0   2.0        15.0      6.0
		1   6.0        8.0       11.0
		2   4.0        3.0       7.0
		3   7.0        2.0       12.0
		4   12.0       7.0       5.0
		
}
"answer": {
	"desc": %s First remove the '.' at the end of date from the data shource sheet. the for this data try this 
	"code-snippets": [
		import datetime as dt
		df.columns=pd.Series(df.columns).apply(lambda d: dt.datetime(d, dt.datetime.strptime(d, '%d/%m/%Y')))
		df.sort_index(axis = 1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63387459
"link": https://stackoverflow.com/questions/63387459/python-dataframe-rowwise-min-and-max-with-nan-values
"question": {
	"title": Python Dataframe rowwise min and max with NaN values
	"desc": My dataframe has nans. But I am trying to find row wise min and max. How do I find it. I am tring to find min and max in each row and difference between them in column and . My code: Present output: Expected output: 
}
"io": {
	"Frame-1": 
		df['dif'] = 
		0    66
		1    66
		2    66
		
	"Frame-2":
		df['dif'] = 
		0    51
		1    66
		2    52
		
}
"answer": {
	"desc": %s We should add the axis=1, check the min and max for each row 
	"code-snippets": [
		np.nanmax(df[poacols],1)-np.nanmin(df[poacols],1)
		Out[81]: array([51., 66., 52.])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63353050
"link": https://stackoverflow.com/questions/63353050/pandas-dataframe-split-into-seperate-csv-by-column-value
"question": {
	"title": Pandas Dataframe split into seperate csv by column value
	"desc": Hello i have dataset containing 4 columns i want to split my dataframe into separate csv files by their "s" column but I couldn't figure out a proper way of doing it. "s" column has arbitrary numbers of labels. We don't know how many 1's or 2's dataset has. it is until 30 but not every number is contained in this dataset. My desired output is: after I get this split I can easily write it to separate csv files. The problem I am having is that I don't know how many different "s" values I have and how much from each of them. Thank you 
}
"io": {
	"Frame-1": 
		x       y     z      s
		1      42.8  157.5   1
		1      43.8  13.5    1
		1      44.8  152     2
		         .
		         .
		         .
		4      7528  157.5   2
		4      45.8  13.5    3
		8      72.8  152     3
		
		
	"Frame-2":
		df1
		x       y     z      s
		1      42.8  157.5   1
		           .
		1      43.8  13.5    1
		
		df2
		1      44.8  152     2
		           .
		4      7528  157.5   2
		
		df3
		4      45.8  13.5    3
		           .
		8      72.8  152     3
		
		
}
"answer": {
	"desc": %s Just before sending to csv to do this dynamically: 
	"code-snippets": [
		for i, x in df.groupby('s'): x.to_csv(f'df{i}.csv', index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63286249
"link": https://stackoverflow.com/questions/63286249/pandas-drop-lines-from-dataframe-if-column-value-is-in-list-csv
"question": {
	"title": Pandas - Drop lines from dataframe if column value is in list (.csv)
	"desc": I have a pandas dataframe imported from SQL, and I would like to drop lines for which a column value is in a list, which I get from a csv file. It seems pretty straighforward, I looked it up and I tried several things using but this is not working as I expect. For example the dataframe imported from SQL looks like this, let's call it df : I import this list this way : Let's assume I print the list, this is what I see : Then I use the following : I would expect to get this (initial df with lines 1 and 2 dropped because they are in the list) However this is not what happens. I get the exact same df as initially with no lines dropped, and also no error message of any kind. What am I doing wrong ? I thought the data in the list and in the df column might not be the same type and I tried fiddling with , but without much success. Perhaps i'm using it wrong. Would appreciate any help. Thanks ! 
}
"io": {
	"Frame-1": 
		    SKU        Brand
		0  AD31KL-A1   BrandA
		1  BC31KL-B3   BrandB
		2  DE31KL-D4   BrandC
		3  FG31KL-F5   BrandD
		
	"Frame-2":
		    SKU        Brand
		0  AD31KL-A1   BrandA
		3  FG31KL-F5   BrandD
		
}
"answer": {
	"desc": %s It looks like this line is your problem: The result of a df apply is another df. Let's say the .csv file has a column called SKU. You can make a list out of that column only: Here's some sample code showing that a column of values can be converted to a list just by calling on the column/series: Here's the df representing list.csv: And here's the list: Lastly, if you don't have a column name, you can just get the first column by it's integer value thusly: 
	"code-snippets": [
		list = df2.apply(lambda x: x.tolist(), axis=1)
		
		----------------------------------------------------------------------
		# Well, I don't have list.csv, so let me just create a dataframe
		df = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ], columns = ['SKU'] )
		print(df)
		list =  df['SKU'].tolist() 
		print( list ) 
		
		----------------------------------------------------------------------
		df = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ] )
		print(df)
		list =  df.iloc[:, 0].tolist()  # first column of dataframe
		print( list ) 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63264777
"link": https://stackoverflow.com/questions/63264777/python-append-2-columns-of-a-dataframe-together
"question": {
	"title": Python: Append 2 columns of a dataframe together
	"desc": I am loading a csv file into a data frame using pandas. My dataframe looks something like this: I wish to append 2 of the columns into a new column: col4 needs to be created by appending the contents of col1 and col2 together. How can I do this in pandas/python? EDIT 
}
"io": {
	"Frame-1": 
		col1       col2       col3         
		1           4           1 
		2           5           2
		3           6           3
		
	"Frame-2":
		  col1       col2        col3       col4   
		    1           4           1         1
		    2           5           2         2
		    3           6           3         3
		                                      4
		                                      5 
		                                      6
		
}
"answer": {
	"desc": %s First use or with for new and then add to original by or : Last for avoid converting to floats is possible use: Or convert missing values to empty strings (what should be problem if need processing df later by some numeric method): EDIT: 
	"code-snippets": [
		s = df['col1'].append(df['col2'], ignore_index=True).rename('col4')
		#alternative
		#s = pd.concat([df['col1'], df['col2']], ignore_index=True).rename('col4')
		
		df1 = df.join(s, how='outer')
		#alternative
		#df1 = pd.concat([df, s], axis=1)
		
		----------------------------------------------------------------------
		df = df.reset_index(drop=True)
		
		s1 = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')
		s2 = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')
		
		df1 = pd.concat([df, s1, s2], axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63259726
"link": https://stackoverflow.com/questions/63259726/pandas-dataframe-delete-groups-with-more-than-n-rows-in-groupby
"question": {
	"title": pandas dataframe delete groups with more than n rows in groupby
	"desc": I have a dataframe: I want to apply groupby based on columns type1, type2 and delete from the dataframe the groups with more than 2 rows. So the new dataframe will be: What is the best way to do so? 
}
"io": {
	"Frame-1": 
		df = [type1 , type2 , type3 , val1, val2, val3
		       a       b        q       1    2     3
		       a       c        w       3    5     2
		       b       c        t       2    9     0
		       a       b        p       4    6     7
		       a       c        m       2    1     8
		       a       b        h       8    6     3
		       a       b        e       4    2     7]
		
	"Frame-2":
		df = [type1 , type2 , type3 , val1, val2, val3
		       a       c        w       3    5     2
		       b       c        t       2    9     0
		       a       c        m       2    1     8
		  ]
		
}
"answer": {
	"desc": %s Use for get counts of groups for with same size like original, so possible filter by for in : If performace is not important or small DataFrame is possible use : 
	"code-snippets": [
		df =df.groupby(['type1','type2']).filter(lambda x: len(x) <= 2) 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63241112
"link": https://stackoverflow.com/questions/63241112/how-to-to-fuzzy-merge-items-from-a-list-that-repeat-many-times-python-pandas
"question": {
	"title": How to to fuzzy merge items from a list that repeat many times python pandas
	"desc": I have a one column df called ```logos''' consisting of the following list: (note I have searched for similar questions on stackoverflow to no avail I would like to merge with the following df that consists of each item, minus the .png filename I would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df I am wondering how I should go about this considering the and aren't identical, and item in the df I would like to merge with is listed multiple times. Is there such thing as a fuzzy join in python like in R? Thanks in advance for any help. 
}
"io": {
	"Frame-1": 
		0   ARI
		1   ARI
		2   ARI
		3   DEN
		4   DEN
		5   DEN
		
	"Frame-2":
		0   ARI ARI.png
		1   ARI ARI.png
		2   ARI ARI.png
		3   DEN DEN.png
		4   DEN DEN.png
		5   DEN DEN.png
		
}
"answer": {
	"desc": %s AFIK there is no option for 'fuzzy' merge. You can make a new column in logos with and then merge with df Edit Pay atention to the parameter in merge. If ommited it will default to inner. Then if you encounter a row in df that has no corresponding filename in logos it will be excluded from the merged result. 
	"code-snippets": [
		df = df.merge(logos, how='left', left_on='column_name', right_on='no_ext')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63182136
"link": https://stackoverflow.com/questions/63182136/drop-consecutive-duplicates-in-pandas-dataframe-if-repeated-more-than-n-times
"question": {
	"title": Drop consecutive duplicates in Pandas dataframe if repeated more than n times
	"desc": Building off the question/solution here, I'm trying to set a parameter that will only remove consecutive duplicates if the same value occurs 5 (or more) times consecutively... I'm able to apply the solution in the linked post which uses to check if the previous (or a specified value in the past or future by adjusting the shift periods parameter) equals the current value, but how could I adjust this to check several consecutive values simultaneously? Suppose a dataframe that looks like this: I'm trying to achieve this: Where we lose rows 4,5,6,7 because we found five consecutive 3's in the y column. But keep rows 1,2 because it we only find two consecutive 2's in the y column. Similarly, keep rows 8,9,10,11 because we only find four consecutive 4's in the y column. 
}
"io": {
	"Frame-1": 
		x    y
		
		1    2
		2    2
		3    3
		4    3
		5    3
		6    3
		7    3
		8    4
		9    4
		10   4
		11   4
		12   2
		
	"Frame-2":
		x    y
		
		1    2
		2    2
		3    3
		8    4
		9    4
		10   4
		11   4
		12   2
		
}
"answer": {
	"desc": %s Let's try on the differences to find the consecutive blocks. Then to get the size of the blocks: Output: 
	"code-snippets": [
		thresh = 5
		s = df['y'].diff().ne(0).cumsum()
		
		small_size = s.groupby(s).transform('size') < thresh
		first_rows = ~s.duplicated()
		
		df[small_size | first_rows]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63184392
"link": https://stackoverflow.com/questions/63184392/pandas-merge-and-keep-only-non-matching-records
"question": {
	"title": Pandas merge and keep only non-matching records
	"desc": How can I merge/join these two dataframes ONLY on "id". Produce 3 new dataframes: 1)R1 = Merged records 2)R2 = (DF1 - Merged records) 3)R3 = (DF2 - Merged records) Using pandas in Python. First dataframe (DF1) Second dataframe (DF2) My solution for I am unsure that is the easiest way to get R2 and R3 R2 should look like R3 should look like: 
}
"io": {
	"Frame-1": 
		|        id | salary |
		|-----------|--------|
		| 1         | 20     |
		| 2         | 30     |
		| 3         | 40     |
		| 4         | 50     |
		| 6         | 33     |
		| 7         | 23     |
		| 8         | 24     |
		| 9         | 28     |
		
	"Frame-2":
		|        id | salary |
		|-----------|--------|
		| 6         | 33     |
		| 7         | 23     |
		| 8         | 24     |
		| 9         | 28     |
		
}
"answer": {
	"desc": %s You can turn on in and look for the corresponding values: Update: Ben's suggestion would be something like this: and then you can do, for examples: 
	"code-snippets": [
		total_merge = df1.merge(df2, on='id', how='outer', indicator=True)
		
		R1 = total_merge[total_merge['_merge']=='both']
		R2 = total_merge[total_merge['_merge']=='left_only']
		R3 = total_merge[total_merge['_merge']=='right_only']
		
		----------------------------------------------------------------------
		dfs = {k:v for k,v in total_merge.groupby('_merge')}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63173736
"link": https://stackoverflow.com/questions/63173736/justify-data-from-right-to-left
"question": {
	"title": justify data from right to left
	"desc": I have a dataset of rows containing varying lengths of integer values in a series. I want to separate the series so each integer has its own column but align these values along the right-most column. I want the dataframe to resenble upper triangle of a matrix. Currently I have a dataset like: I apply this function and I get this: But what i want is this: 
}
"io": {
	"Frame-1": 
		    1   2   3   4   5   6   7   8   9   10
		0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0
		1   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 NaN
		2   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 NaN NaN
		3   1.0 2.0 3.0 4.0 5.0 6.0 7.0 NaN NaN NaN
		4   1.0 2.0 3.0 4.0 5.0 6.0 NaN NaN NaN NaN
		5   1.0 2.0 3.0 4.0 5.0 NaN NaN NaN NaN NaN
		6   1.0 2.0 3.0 4.0 NaN NaN NaN NaN NaN NaN
		7   1.0 2.0 3.0 NaN NaN NaN NaN NaN NaN NaN
		8   1.0 2.0 NaN NaN NaN NaN NaN NaN NaN NaN
		9   1.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN
		
	"Frame-2":
		    1   2   3   4   5   6   7   8   9   10
		0   1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0
		1   NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 
		2   NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 
		3   NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 
		4   NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 
		5   NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 
		6   NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0
		7   NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 
		8   NaN NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 
		9   NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 
		
}
"answer": {
	"desc": %s One possible approach is to use to calculate the length of the list in the column i.e then using list comprehension each of the list based on : Result: 
	"code-snippets": [
		lmax = df['value'].str.len().max()
		df1 = pd.DataFrame([[np.nan] * (lmax - len(s)) + s
		                    for s in df['value']], columns=range(1, lmax + 1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62283545
"link": https://stackoverflow.com/questions/62283545/way-to-show-multiple-spaces-in-pandas-dataframe-on-jupyter-notebook
"question": {
	"title": Way to show multiple spaces in Pandas Dataframe on Jupyter Notebook
	"desc": When displaying Pandas Dataframe object on notebook, multiple spaces are shown as single space. And I cannot copy multiple spaces. I would like to show all spaces as they are and copy a string of them. Any way to do so? Actual My expectation 
}
"io": {
	"Frame-1": 
		    0     1   
		0   ab c  ab c
		1   ab c  ab c
		
	"Frame-2":
		    0       1     
		0   ab c    ab  c
		1   ab   c  ab    c
		
}
"answer": {
	"desc": %s This issue has been answered in a different context: keep the extra whitespaces in display of pandas dataframe in jupyter notebook 
	"code-snippets": [
		def print_df(df):
		    return df.style.set_properties(**{'white-space': 'pre'})
		
		print_df(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63091593
"link": https://stackoverflow.com/questions/63091593/easy-way-to-find-find-itinerant-max-value-in-grouped-pandas-list
"question": {
	"title": Easy way to find find itinerant max value in grouped pandas list
	"desc": I have a dataset where I have multiple value entries per year and some properties per entry. I want to find the maximum value per year and return that as a new data frame (to keep the other properties in the data frame), but only if the value in a year is greater than what it was in the years before (something like the "All-time record value per year"). So far I can find the max value per year, e.g. where the output then is This is almost what I want, expect for 2014 where I would like the value of 2013 with its according properties to go (since the value was greater in 2013 than it was in 2014). So the desired outcome would be Is there a good way to achieve this with pandas? 
}
"io": {
	"Frame-1": 
		   Year  Value    Property
		0  2012     35  Property B
		1  2013     43  Property D
		2  2014     37  Property C
		3  2015     41  Property F
		
	"Frame-2":
		   Year  Value    Property
		0  2012     35  Property B
		1  2013     43  Property D
		2  2014     43  Property D
		3  2015     43  Property D
		
}
"answer": {
	"desc": %s  Results in After which you can just drop the original columns. The secret sauce here is the function. 
	"code-snippets": [
		df_sorted_max['cummax_value'] = df_sorted_max.Value.cummax()
		
		print(
		    df_sorted_max.merge(
		        df_sorted_max.groupby('cummax_value')[['Property']].first(),
		        on="cummax_value"
		    )
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63090827
"link": https://stackoverflow.com/questions/63090827/dropping-dataframe-rows-in-time-series-dataframe-using-pandas
"question": {
	"title": Dropping dataframe rows in time series dataframe using pandas
	"desc": I have the below sequence of data as a pandas dataframe It should always be the case that id 404 gets repeated after another different id. For example if the above is motion sensors in a house e.g. 404:hallway, 202:bedroom, 303:kitchen, 201:studyroom, where the hallway is in the middle, then moving from bedroom to kitchen to studyroom and back to bedroom should trigger 202, 404, 303, 404, 201, 404, 202 in that order because one always passes through the hallway (404) to any room. My output has cases that violate this sequence and I want to drop such rows. For example from the snippet dataframe above the below rows violate this: and therefore the rows below should be droped (but of course I have a much larger dataset). I have tried shift and drop but the result still has some inconsistencies. How best can I approach this? 
}
"io": {
	"Frame-1": 
		303,2012-06-25 18:01:56,2012-06-25 18:02:06,10
		303,2012-06-25 18:02:23,2012-06-25 18:02:44,21
		
		303,2012-06-25 18:03:43,2012-06-25 18:05:51,128
		101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104
		
	"Frame-2":
		303,2012-06-25 18:02:23,2012-06-25 18:02:44,21
		101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104
		
}
"answer": {
	"desc": %s Use + along with optional parameter to create a boolean , use this mask to filter/drop the rows: Result: 
	"code-snippets": [
		mask = df['id'].ne(404) & df['id'].shift(fill_value=404).ne(404)
		df = df[~mask]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63059265
"link": https://stackoverflow.com/questions/63059265/filter-dataframe-rows-which-contribute-to-x-of-values-in-one-column
"question": {
	"title": Filter dataframe rows which contribute to X% of values in one column
	"desc": I have a dataframe: I want to see only those rows which contribute to 90% of Col3. In this case the expected output will be : I tried the below but is doesnt work as expected: Is there any solution for the same? 
}
"io": {
	"Frame-1": 
		df
		Col1   Col2   Col3
		A      B      5
		C      D      4
		E      F      1
		
	"Frame-2":
		Col1   Col2   Col3
		A      B      5
		C      D      4
		
}
"answer": {
	"desc": %s Are you looking for this? Output 
	"code-snippets": [
		df = df[df.Col3 > 0] # optionally remove 0 valued rows
		df = df.sort_values(by='Col3', ascending=False).reset_index(drop=True)
		totals = df.Col3.cumsum()
		cutoff = totals[totals >= df.Col3.sum() * .7].idxmin()
		print(df[:cutoff + 1])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62989401
"link": https://stackoverflow.com/questions/62989401/how-to-find-first-occurrence-of-a-significant-difference-in-values-of-a-pandas-d
"question": {
	"title": How to find first occurrence of a significant difference in values of a pandas dataframe?
	"desc": In a Pandas DataFrame, how would I find the first occurrence of a large difference between two values at two adjacent indices? As an example, if I have a DataFrame column A with data , I would want index holding 1.5, which would be 5. In my code below, it would give me the index holding 7.2, because . How should I fix this problem, so I get the index of the first 'large difference' occurrence? 
}
"io": {
	"Frame-1": 
		[1, 1.1, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]
	"Frame-2":
		15 - 7.2 > 7 - 1.5
}
"answer": {
	"desc": %s The main issue is of course how you define a "large difference". Your solution is pretty good to get the largest difference, improved only by using and using absolute values as shown by Jezrael: Using absolute values matters if your values are not sorted, in which case you can get negative differences. Then, you should probably do some clustering on these values and get the smallest index of the cluster with largest values. Jezrael already showed a heuristic by using the largest quartile, however by only slightly modifying your example this doesnt work: This returns Here's 3 other heuristics that might work better for you: If you have a value above which you consider a difference to be large (e.g. ): If you know how many large values there are, you can select those and get the smallest index (e.g. ): If you know all small values are grouped together (as are all the 0.1 in your example), you can filter what's larger than the mean (or the mean + 1 standard deviation if your large values are very close to the smaller ones). This is because contrarily to the median, your few large differences will pull the mean up significantly. If you really want to go for proper clustering, you can use the KMeans algorithm from scikit learn: This classifies the data into 2 classes, and then gets the classification into a pandas Series. We then filter this series index by selecting only the cluster that has the highest values, and finally get the first element of this filtered index. 
	"code-snippets": [
		df = pd.DataFrame({'A': [1, 1.05, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]})
		differences = df['A'].diff(-1).abs()
		idx = differences.index[differences >= differences.quantile(.75)][0]
		print(idx, differences[idx])
		
		----------------------------------------------------------------------
		idx = differences.index[differences >= 1.5][0]
		
		----------------------------------------------------------------------
		idx = differences.index[differences >= differences.mean()][0]
		
		----------------------------------------------------------------------
		from sklearn.cluster import KMeans
		
		kmeans = KMeans(n_clusters=2).fit(differences.values[:-1].reshape(-1, 1))
		clusters = pd.Series(kmeans.labels_, index=differences.index[:-1])
		idx = clusters.index[clusters.eq(np.squeeze(kmeans.cluster_centers_).argmax())][0]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62996260
"link": https://stackoverflow.com/questions/62996260/how-can-i-sort-dataframe-based-on-a-complicated-string-column
"question": {
	"title": How can I sort dataframe based on a complicated string column?
	"desc": I'm needing to sort a dataframe based on a string column, which is composed of a variety of letters, numbers, dashes, and string lengths. I'm not even sure sorting is the right method of what I want to do. Example below: df Desired DF: The order/sorting of either column does not matter, I just want the dataframe reordered and 'grouped' on column2. Grouped might not be the right expression here either cause I don't want to perform any sort of aggregated calculation. Any ideas? Thanks! 
}
"io": {
	"Frame-1": 
		Col1    Col2
		  A    80NX-265-DF23
		  B    D-87-B-003
		  C    80NX-265-DF23
		  D    0333-DD-02
		  E    D-87-B-003
		  F    80NX-265-DF23
		
	"Frame-2":
		Col1     Col2
		 A      80NX-265-DF23
		 C      80NX-265-DF23
		 F      80NX-265-DF23
		 D      0333-DD-02
		 B      D-87-B-003
		 E      D-87-B-003
		
}
"answer": {
	"desc": %s One approach would be iterration and concat: 
	"code-snippets": [
		pd.concat([df[df['Col2']==x] for x in df['Col2'].unique()])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 28236305
"link": https://stackoverflow.com/questions/28236305/how-do-i-sum-values-in-a-column-that-match-a-given-condition-using-pandas
"question": {
	"title": How do I sum values in a column that match a given condition using pandas?
	"desc": Suppose I have a column like so: I want to sum up the values for where , for example. This would give me . How do I do this in pandas? 
}
"io": {
	"Frame-1": 
		a   b  
		1   5   
		1   7
		2   3
		1   3
		2   5
		
	"Frame-2":
		5 + 7 + 3 = 15
}
"answer": {
	"desc": %s The essential idea here is to select the data you want to sum, and then sum them. This selection of data can be done in several different ways, a few of which are shown below. Boolean indexing Arguably the most common way to select the values is to use Boolean indexing. With this method, you find out where column 'a' is equal to and then sum the corresponding rows of column 'b'. You can use to handle the indexing of rows and columns: The Boolean indexing can be extended to other columns. For example if also contained a column 'c' and we wanted to sum the rows in 'b' where 'a' was 1 and 'c' was 2, we'd write: Query Another way to select the data is to use to filter the rows you're interested in, select column 'b' and then sum: Again, the method can be extended to make more complicated selections of the data: Note this is a little more concise than the Boolean indexing approach. Groupby The alternative approach is to use to split the DataFrame into parts according to the value in column 'a'. You can then sum each part and pull out the value that the 1s added up to: This approach is likely to be slower than using Boolean indexing, but it is useful if you want check the sums for other values in column : 
	"code-snippets": [
		>>> df.loc[df['a'] == 1, 'b'].sum()
		15
		
		----------------------------------------------------------------------
		df.loc[(df['a'] == 1) & (df['c'] == 2), 'b'].sum()
		
		----------------------------------------------------------------------
		>>> df.query("a == 1")['b'].sum()
		15
		
		----------------------------------------------------------------------
		df.query("a == 1 and c == 2")['b'].sum()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62972913
"link": https://stackoverflow.com/questions/62972913/how-to-shuffle-a-dataframe-while-maintaining-the-order-of-a-specific-column
"question": {
	"title": How to shuffle a dataframe while maintaining the order of a specific column
	"desc": I have a pandas dataframe which I want to shuffle, but keep the order of 1 column. So imagine I have the following df: I want to shuffle the rows but keep the order of the ID column of the first df. My wanted result would be something like this: How do I do this? 
}
"io": {
	"Frame-1": 
		| i | val | val2| ID |    
		| 0 | 2   | 2 |a  |  
		| 1 | 3   | 3 |b  |  
		| 2 | 4   | 4 |a  |  
		| 3 | 6   | 5 |b  |  
		| 4 | 5   | 6 |b  |  
		
	"Frame-2":
		| i | val | val2| ID |  
		| 2 | 4   | 4 |a  |    
		| 4 | 5   | 6 |b  |  
		| 0 | 2   | 2 |a  |  
		| 3 | 6   | 5 |b  |  
		| 1 | 3   | 3 |b  |  
		
}
"answer": {
	"desc": %s Here's a solution: The output is: If you have a dataframe with multiple columns, and you'd like to shuffle while maintaining the order of one of the columns, the solution is very similar: 
	"code-snippets": [
		df = pd.DataFrame({"val": [1, 2, 3, 4, 5, 6, 7], "ID": ["a", "b", "a", "b", "a", "a", "b"]})
		df["val"] = df.groupby("ID").transform(lambda x: x.sample(frac=1))
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62967408
"link": https://stackoverflow.com/questions/62967408/how-can-i-remove-a-certain-type-of-values-in-a-group-in-pandas
"question": {
	"title": How can I remove a certain type of values in a group in pandas?
	"desc": I have the following dataframe which is a small part of a bigger one: I'd like to delete all rows where the last items are "d". So my desired dataframe would look like this: So the point is, that a group shouldn't have "d" as the last item. There is a code that deletes the last row in the groups where the last item is "d". But in this case, I have to run the code twice to delete all last "d"-s in group 3 for example. Is there a better solution to this problem? 
}
"io": {
	"Frame-1": 
		   acc_num   trans_cdi
		0     1         c
		1     1         d
		3     3         d
		4     3         c
		5     3         d
		6     3         d
		
	"Frame-2":
		   acc_num   trans_cdi
		0     1         c
		3     3         d
		4     3         c
		
}
"answer": {
	"desc": %s  First, compare to the next row with if the values are both equal to 'd'. filters out the specified rows. Second, Make sure the last row value is not . If it is, then delete the row. code: input (I tested it on more input data to ensure there were no bugs): output: 
	"code-snippets": [
		df = df[~((df['trans_cdi'] == 'd') & (df.shift(1)['trans_cdi'] == 'd'))]
		if df['trans_cdi'].iloc[-1] == 'd': df = df.iloc[0:-1]
		df
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62962437
"link": https://stackoverflow.com/questions/62962437/slicing-each-dataframe-row-into-3-windows-with-different-slicing-ranges
"question": {
	"title": Slicing each dataframe row into 3 windows with different slicing ranges
	"desc": I want to slice each row of my dataframe into 3 windows with slice indices that are stored in another dataframe and change for each row of the dataframe. Afterwards i want to return a single dataframe containing the windows in form of a MultiIndex. The rows in each windows that are shorter than the longest row in the window should be filled with NaN values. Since my actual dataframe has around 100.000 rows and 600 columns, i am concerned about an efficient solution. Consider the following example: This is my dataframe which i want to slice into 3 windows And the second dataframe containing my slicing indices having the same count of rows as : I've tried slicing the windows, like so: Which gives me the following error: My expected output is something like this: Is there an efficient solution for my problem without iterating over each row of my dataframe? 
}
"io": {
	"Frame-1": 
		>>> df
		  0  1  2  3  4  5  6  7
		0 0  1  2  3  4  5  6  7
		1 8  9  10 11 12 13 14 15
		2 16 17 18 19 20 21 22 23
		
	"Frame-2":
		>>> df_slice
		  0 1
		0 3 5
		1 2 6
		2 4 7
		
}
"answer": {
	"desc": %s Here's a solution which, using and then , plus some logic to: Identify the three groups 'A', 'B', and 'C'. Shift the columns to the left, so that NaN would only appear at the right side of each window. Rename columns to get the expected output. The output is: 
	"code-snippets": [
		    t = df.reset_index().melt(id_vars="index")
		    t = pd.merge(t, df_slice, left_on="index", right_index=True)
		    t.variable = pd.to_numeric(t.variable)
		    
		    t.loc[t.variable < t.c_0,"group"] = "A"
		    t.loc[(t.variable >= t.c_0) & (t.variable < t.c_1), "group"] = "B"
		    t.loc[t.variable >= t.c_1, "group"] = "C"
		
		    # shift relevant values to the left
		    shift_val = t.groupby(["group", "index"]).variable.transform("min") - t.groupby(["group"]).variable.transform("min")
		    t.variable = t.variable - shift_val
		    
		    # extract a, b, and c groups, and create a multi-level index for their
		    # columns
		    df_a = pd.pivot_table(t[t.group == "A"], index= "index", columns="variable", values="value")
		    df_a.columns = pd.MultiIndex.from_product([["a"], df_a.columns])
		    
		    df_b = pd.pivot_table(t[t.group == "B"], index= "index", columns="variable", values="value")
		    df_b.columns = pd.MultiIndex.from_product([["b"], df_b.columns])
		    
		    df_c = pd.pivot_table(t[t.group == "C"], index= "index", columns="variable", values="value")
		    df_c.columns = pd.MultiIndex.from_product([["c"], df_c.columns])
		    
		    res = pd.concat([df_a, df_b, df_c], axis=1)
		    
		    res.columns = pd.MultiIndex.from_tuples([(c[0], i) for i, c in enumerate(res.columns)])
		    
		    print(res)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62941861
"link": https://stackoverflow.com/questions/62941861/python-pandas-ffill-with-groupby
"question": {
	"title": python - pandas ffill with groupby
	"desc": I am trying to forward fill the missing rows to complete the missing time-series rows in the dataset. The size of the dataset is huge. More than 100 million rows. The original source dataset is as shown below. desired output is as below I need to group on and to fill the missing time-series rows in for each of the combinations. Currently, I have the below code which is working but its extremely slow due to the for-loop. Is there any way I can avoid the for-loop and send the whole dataset for creating missing rows and ffill()? Thanks and Appreciate the help. Update: The above code is working but it's too slow. It takes more than 30 minutes for just 300k rows. Hence, I'm looking for help to make it faster and avoid the for-loop. 
}
"io": {
	"Frame-1": 
		         col1 col2 col3  col4  col5  col6
		0  2020-01-01   b1   c1     1     9    17
		1  2020-01-05   b1   c1     2    10    18
		2  2020-01-02   b2   c2     3    11    19
		3  2020-01-04   b2   c2     4    12    20
		4  2020-01-10   b3   c3     5    13    21
		5  2020-01-15   b3   c3     6    14    22
		6  2020-01-16   b4   c4     7    15    23
		7  2020-01-30   b4   c4     8    16    24
		
		
	"Frame-2":
		         col1 col2 col3  col4  col5  col6
		0  2020-01-01   b1   c1   1.0   9.0  17.0
		1  2020-01-02   b1   c1   1.0   9.0  17.0
		2  2020-01-03   b1   c1   1.0   9.0  17.0
		3  2020-01-04   b1   c1   1.0   9.0  17.0
		4  2020-01-05   b1   c1   2.0  10.0  18.0
		5  2020-01-02   b2   c2   3.0  11.0  19.0
		6  2020-01-03   b2   c2   3.0  11.0  19.0
		7  2020-01-04   b2   c2   4.0  12.0  20.0
		8  2020-01-10   b3   c3   5.0  13.0  21.0
		9  2020-01-11   b3   c3   5.0  13.0  21.0
		10 2020-01-12   b3   c3   5.0  13.0  21.0
		11 2020-01-13   b3   c3   5.0  13.0  21.0
		12 2020-01-14   b3   c3   5.0  13.0  21.0
		13 2020-01-15   b3   c3   6.0  14.0  22.0
		14 2020-01-16   b4   c4   7.0  15.0  23.0
		15 2020-01-17   b4   c4   7.0  15.0  23.0
		16 2020-01-18   b4   c4   7.0  15.0  23.0
		17 2020-01-19   b4   c4   7.0  15.0  23.0
		18 2020-01-20   b4   c4   7.0  15.0  23.0
		19 2020-01-21   b4   c4   7.0  15.0  23.0
		20 2020-01-22   b4   c4   7.0  15.0  23.0
		21 2020-01-23   b4   c4   7.0  15.0  23.0
		22 2020-01-24   b4   c4   7.0  15.0  23.0
		23 2020-01-25   b4   c4   7.0  15.0  23.0
		24 2020-01-26   b4   c4   7.0  15.0  23.0
		25 2020-01-27   b4   c4   7.0  15.0  23.0
		26 2020-01-28   b4   c4   7.0  15.0  23.0
		27 2020-01-29   b4   c4   7.0  15.0  23.0
		28 2020-01-30   b4   c4   8.0  16.0  24.0
		
}
"answer": {
	"desc": %s Looks like a on would work: Output: 
	"code-snippets": [
		(df.set_index('col1').groupby(['col2', 'col3'])
		   .resample('D').ffill()
		   .reset_index(['col2','col3'], drop=True)
		   .reset_index()
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62935300
"link": https://stackoverflow.com/questions/62935300/pandas-conditional-rolling-sum-of-two-columns
"question": {
	"title": Pandas Conditional Rolling Sum of Two Columns
	"desc": I have four columns in a data frame like so: And I want to conditionally sum by: D has a value, then D Else, take value from previous row for D plus C So for above, D would be . I've been able to do this successfully with a for loop But the for loop is obviously really expensive, anti-pattern and basically doesn't run over my whole data frame. I'm trying to copy an excel function here that has basically been written over a panel of data, and for the life of me I cannot figure out how to do this with: Simply calculating different column values I was attempting to do it with for a while, but I realized that I kept having to make a separate column for each row, and that's why I went with a for loop. Generalized to Groups Thanks to Scott Boston for the help! 
}
"io": {
	"Frame-1": 
		       A   B     C        D
		75472  d1  x    -36.0   0.0
		75555  d2  x    -38.0   0.0
		75638  d3  x    -18.0   0.0
		75721  d4  x    -18.0   1836.0
		75804  d5  x    1151.0  0.0
		75887  d6  x    734.0   0.0
		75970  d7  x    -723.0  0.0
		
	"Frame-2":
		[-36, -74, -92, 1836, 2987, 3721, 2998]
}
"answer": {
	"desc": %s Here is a way to do it: Output: Details: Create grps to help cumsum. Each group is defined the the appears of a value in 'D' hence you stop cumsum before and pick that value of D and continue cumsum until the next value of 'D' Output: Now, Let's create new column combining 'C' and 'D' when D has a nonzero number Output: And, lastly, groupby 'grp' and newCD: Output: 
	"code-snippets": [
		grp = (df['D'] != 0).cumsum()
		df['D_new'] = df['D'].mask(df['D'] == 0).combine_first(df['C']).groupby(grp).cumsum()
		df
		
		----------------------------------------------------------------------
		grp = (df['D'] != 0).cumsum()
		
		----------------------------------------------------------------------
		df['newCD'] = df['D'].mask(df['D'] == 0).combine_first(df['C'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62935487
"link": https://stackoverflow.com/questions/62935487/trying-to-append-the-sum-of-an-existing-dataframe-into-a-new-excel-sheet
"question": {
	"title": Trying to append the sum of an existing dataframe into a new excel sheet
	"desc": So I have been trying to append the of an existing dataframe into a new dataframe for a new the below example: I want this to be appended to a new excel as: Following is the code where I am merging the files in a particular location I need help with the summing part. 
}
"io": {
	"Frame-1": 
		A   B     C     
		10  10   10
		10  10   10
		10  10   10
		
	"Frame-2":
		A   B   C
		30  30  30
		
}
"answer": {
	"desc": %s Try Result: when you do: then you want to make series to df 
	"code-snippets": [
		df.sum()
		A    30
		B    30
		C    30
		dtype: int64
		
		----------------------------------------------------------------------
		df.sum().to_frame()
		
		0 
		A 30
		B 30
		C 30
		 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59649084
"link": https://stackoverflow.com/questions/59649084/how-to-split-a-dataframe-on-each-different-value-in-a-column
"question": {
	"title": How to split a DataFrame on each different value in a column?
	"desc": Below is an example DataFrame. I want to split this into new dataframes when the row in column 0 changes. I've tried adapting the following solutions without any luck so far. Split array at value in numpy Split a large pandas dataframe 
}
"io": {
	"Frame-1": 
		      0      1     2     3          4
		0   0.0  13.00  4.50  30.0   0.0,13.0
		1   0.0  13.00  4.75  30.0   0.0,13.0
		2   0.0  13.00  5.00  30.0   0.0,13.0
		3   0.0  13.00  5.25  30.0   0.0,13.0
		4   0.0  13.00  5.50  30.0   0.0,13.0
		5   0.0  13.00  5.75   0.0   0.0,13.0
		6   0.0  13.00  6.00  30.0   0.0,13.0
		7   1.0  13.25  0.00  30.0  0.0,13.25
		8   1.0  13.25  0.25   0.0  0.0,13.25
		9   1.0  13.25  0.50  30.0  0.0,13.25
		10  1.0  13.25  0.75  30.0  0.0,13.25
		11  2.0  13.25  1.00  30.0  0.0,13.25
		12  2.0  13.25  1.25  30.0  0.0,13.25
		13  2.0  13.25  1.50  30.0  0.0,13.25
		14  2.0  13.25  1.75  30.0  0.0,13.25
		15  2.0  13.25  2.00  30.0  0.0,13.25
		16  2.0  13.25  2.25  30.0  0.0,13.25
		
	"Frame-2":
		      0      1     2     3          4
		0   0.0  13.00  4.50  30.0   0.0,13.0
		1   0.0  13.00  4.75  30.0   0.0,13.0
		2   0.0  13.00  5.00  30.0   0.0,13.0
		3   0.0  13.00  5.25  30.0   0.0,13.0
		4   0.0  13.00  5.50  30.0   0.0,13.0
		5   0.0  13.00  5.75   0.0   0.0,13.0
		6   0.0  13.00  6.00  30.0   0.0,13.0
		
		7   1.0  13.25  0.00  30.0  0.0,13.25
		8   1.0  13.25  0.25   0.0  0.0,13.25
		9   1.0  13.25  0.50  30.0  0.0,13.25
		10  1.0  13.25  0.75  30.0  0.0,13.25
		
		11  2.0  13.25  1.00  30.0  0.0,13.25
		12  2.0  13.25  1.25  30.0  0.0,13.25
		13  2.0  13.25  1.50  30.0  0.0,13.25
		14  2.0  13.25  1.75  30.0  0.0,13.25
		15  2.0  13.25  2.00  30.0  0.0,13.25
		16  2.0  13.25  2.25  30.0  0.0,13.25
		
}
"answer": {
	"desc": %s Looks like you want to the first colum. You could create a dictionary from the groupby object, and have the groupby keys be the dictionary keys: Or we could also build a list from the groupby object. This becomes more useful when we only want positional indexing rather than based on the grouping key: We could then index the dict based on the grouping key, or the list based on the group's position: 
	"code-snippets": [
		out = [sub_df for _, sub_df in df.groupby(0)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62900970
"link": https://stackoverflow.com/questions/62900970/dask-equivalent-to-pandas-dataframe-update
"question": {
	"title": Dask equivalent to pandas.DataFrame.update
	"desc": I have a few functions that are using method, and I'm trying to move into using instead for the datasets, but the Dask Pandas API doesn't have the method implemented. Is there an alternative way to get the same result in ? Here are the methods I have using : Forward fills data with last known value input output Replaces values in a dataframe with values from another dataframe based on an id/index column input df1 df2 output 
}
"io": {
	"Frame-1": 
		id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....
		1                      10     20     0      40     0      50
		2                      10     30     30     0      0      50
		.
		.
		
	"Frame-2":
		id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 ....
		1                      10     20     20     40     40      50
		2                      10     30     30     30     30      50
		.
		.
		
}
"answer": {
	"desc": %s Question 2 There is a way to partially solve this. I'm assuming that is much smaller than and it actually fit in memory so we can read as pandas dataframe. If this is the case the following function work if is a or a dataframe but should be a one. Question 1 Regarding the first problem the following code works in and Remove the comment in the last line if you want the output to be integer. UPDATE Question 2 In case you want a only solution you could try the following. Data Case 1 In this case if one is present in but not in you keep the name in . Case 2 In this case if one is present in but not in then in the output will be (as in a standard left join) 
	"code-snippets": [
		import pandas as pd
		import dask.dataframe as dd
		
		def replace_names(df1, # can be pandas or dask dataframe
		                  df2, # this should be pandas.
		                  idxCol='id',
		                  srcCol='name',
		                  dstCol='name'):
		    diz = df2[[idxCol, srcCol]].set_index(idxCol).to_dict()[srcCol]
		    out = df1.copy()
		    out[dstCol] = out[idxCol].map(diz)
		    return out
		
		----------------------------------------------------------------------
		def replace_names_dask(df1, df2,
		                       idxCol='id',
		                       srcCol='name',
		                       dstCol='name'):
		    if srcCol == dstCol:
		        df2 = df2.rename(columns={srcCol:f"{srcCol}_new"})
		        srcCol = f"{srcCol}_new"
		    
		    def map_replace(x, srcCol, dstCol):
		        x[dstCol] = np.where(x[srcCol].notnull(),
		                             x[srcCol],
		                             x[dstCol])
		        return x
		    
		    df = dd.merge(df1, df2, on=idxCol, how="left")
		    df = df.map_partitions(lambda x: map_replace(x, srcCol, dstCol))
		    df = df.drop(srcCol, axis=1)
		    return df
		
		df = replace_names_dask(df1, df2)
		
		----------------------------------------------------------------------
		def replace_names_dask(df1, df2,
		                       idxCol='id',
		                       srcCol='name',
		                       dstCol='name'):
		    df1 = df1.drop(dstCol, axis=1)
		    df2 = df2.rename(columns={srcCol: dstCol})
		    df = dd.merge(df1, df2, on=idxCol, how="left")
		    return df
		
		df = replace_names_dask(df1, df2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62923547
"link": https://stackoverflow.com/questions/62923547/extract-data-from-certain-columns-and-generate-new-rows
"question": {
	"title": Extract data from certain columns and generate new rows
	"desc": I have a pandas dataframe The output I want will be: Is there any convenient way I can use? 
}
"io": {
	"Frame-1": 
		COL1 COL2 COL3        COL4 
		 A    B    C    [{COL5: D1, COL6: E1, COL7: F1},
		                 {COL5: D2, COL6: E2, COL7: F2},
		                 {COL5: D3, COL6: E3, COL7: F3},
		                  ...
		
		                 {COL5: D10, COL6: E10, COL7: F10}]
		
	"Frame-2":
		COL1 COL2 COL3  COL5  COL6  COL7
		 A    B    C     D1    E1    F1 
		 A    B    C     D2    E2    F2
		 A    B    C     D3    E3    F3
		...
		 A    B    C     D10   E10   F10
		                  
		
}
"answer": {
	"desc": %s Setup + 
	"code-snippets": [
		u = df.explode('COL4').reset_index(drop=True)
		u.iloc[:, :-1].join(pd.DataFrame(u['COL4'].tolist()))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62164809
"link": https://stackoverflow.com/questions/62164809/pandas-multiline-data-schema-to-single-line
"question": {
	"title": Pandas: Multiline data schema to single line
	"desc": I have a big (huge) dataset that have the next schema: and for many reasons, one of them the to reduce the size, I want to transform it to the next schema: I tried grouping by ['dt', 'id'] and then iterating over each group to build the new rows but it is too slow. I'm not figuring out a way without iterating over every original row. Any idea? 
}
"io": {
	"Frame-1": 
		dt | id | val_t | val
		1  |  1 |     1 | 123
		1  |  1 |     2 | 145
		1  |  1 |     3 | 234
		1  |  2 |     1 | 234
		1  |  2 |     2 | 433
		1  |  2 |     3 | 453
		..................
		N  |  X |     1 | 652
		N  |  X |     2 | 543
		N  |  X |     3 | 324
		
	"Frame-2":
		dt | id | val_1 | val_2 | val_3
		1  |  1 |   123 |  145  |  234
		1  |  2 |   234 |  433  |  453
		..................
		N  |  X |   652 |  543  |  324
		
}
"answer": {
	"desc": %s Use, the combination of , , , : Result: 
	"code-snippets": [
		df['temp'] = 'val_'
		df['val_t'] = df.pop('temp') + df['val_t'].astype(str)
		df = df.set_index(['dt', 'id', 'val_t']).unstack()
		df.columns = df.columns.droplevel()
		df = df.rename_axis(columns=None).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62913446
"link": https://stackoverflow.com/questions/62913446/replace-last-non-nan-value-in-row
"question": {
	"title": Replace last non NaN value in row
	"desc": I'd like to replace all the last non NaNs in rows in data frame with NaN value. I have 300 rows and 1068 columns in my data frame. and each row have different number of valid values in them padded with NaNs. Here is an example of a row: a row in dataframe = output = How to replace last non NaN value in rows in CSV file? 
}
"io": {
	"Frame-1": 
		[1 2 3 NaN NaN NaN]
	"Frame-2":
		[1 2 NaN NaN NaN NaN]
}
"answer": {
	"desc": %s Here's a numpy based one: You can slice the array of values, and get it into reverse order, and look for the first valid value. Then get the indices, and use to set them to s: Further details - The first step is to find where the NaNs are. And since we want the last valid value, we should start from the end. So slice to get the array with the columns reversed, and use : Now we can find the first , i.e the last valid value using : Now by subtracting the col length to the above we get the actual indices: Now we can just set those indices to in the correspondin indices: 
	"code-snippets": [
		import numpy as np
		df = pd.DataFrame([[1, 2, 3, np.nan, np.nan, np.nan], [1, 2, 3, np.nan, np.nan, 2]])
		
		----------------------------------------------------------------------
		a = df.to_numpy()
		m = a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)
		np.put_along_axis(a, m[:,None], np.nan, axis=1)
		df[:] = a
		
		----------------------------------------------------------------------
		np.isnan(a[:,::-1])
		array([[ True,  True,  True, False, False, False],
		       [False,  True,  True, False, False, False]])
		
		----------------------------------------------------------------------
		np.argmax(~np.isnan(a[:,::-1]), axis=1)
		# array([3, 0], dtype=int64)
		
		----------------------------------------------------------------------
		a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)
		# array([2, 5], dtype=int64)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62912823
"link": https://stackoverflow.com/questions/62912823/merge-two-columns-of-a-dataframe-into-an-already-existing-column-of-dictionaries
"question": {
	"title": Merge two columns of a dataframe into an already existing column of dictionaries as a key value pair
	"desc": If we have 3 columns of a dataframe as : I want the column3 to be something like : I have tried a few things from using lambda functions with apply to iterating over rows but all were unsuccessful. 
}
"io": {
	"Frame-1": 
		column1 : ['A','A','B','C']
		column2 : [12,13,14,15]
		column3 : [{"key1":"val1"},{"key2":"val2"},{"key3":"val3"},{"key4":"val4"}]
		
	"Frame-2":
		column3 : [{"key1":"val1", "A":12},{"key2":"val2", "A":13},{"key3":"val3", "B":14},{"key4":"val4", "C":15}]
		
}
"answer": {
	"desc": %s You could use a list comprehension and unpacking as: Input data - 
	"code-snippets": [
		df['col3'] = [{**d, k:v} for k,v,d in df.values.tolist()]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 41529340
"link": https://stackoverflow.com/questions/41529340/find-out-intersection-of-2-pandas-dataframe-according-to-2-columns
"question": {
	"title": Find out intersection of 2 pandas DataFrame according to 2 columns
	"desc": I would to find out intersection of 2 pandas DataFrame according to 2 columns 'x' and 'y' and combine them into 1 DataFrame. The data are: The expected output is something like (can ignore index): Thank you very much! 
}
"io": {
	"Frame-1": 
		df[1]:
		    x   y       id    fa
		0   4   5  9283222   3.1
		1   4   5  9283222   3.1
		2  10  12  9224221   3.2
		3   4   5  9284332   1.2
		4   6   1    51249  11.2
		
		df[2]:
		    x   y        id   fa
		0   4   5  19283222  1.1
		1   9   3  39224221  5.2
		2  10  12  29284332  6.2
		3   6   1     51242  5.2
		4   6   2     51241  9.2
		5   1   1     51241  9.2
		
	"Frame-2":
		    x   y       id    fa
		0   4   5  9283222   3.1
		1   4   5  9283222   3.1
		2  10  12  9224221   3.2
		3   4   5  9284332   1.2
		4   6   1    51249  11.2
		0   4   5  19283222  1.1
		2  10  12  29284332  6.2
		3   6   1     51242  5.2
		
}
"answer": {
	"desc": %s You can find out the intersection by joining the columns from and , with which you can filter and by inner join, and then concatenating the two results with should give what you need: 
	"code-snippets": [
		intersection = df1[['x', 'y']].merge(df2[['x', 'y']]).drop_duplicates()
		pd.concat([df1.merge(intersection), df2.merge(intersection)])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62882426
"link": https://stackoverflow.com/questions/62882426/drop-last-n-rows-within-pandas-dataframe-groupby
"question": {
	"title": Drop last n rows within pandas dataframe groupby
	"desc": I have a dataframe where I want to drop last rows within a group of columns. For example, say is defined as below the group is of columns and : Desired output for is as follows: Desired output for is as follows: 
}
"io": {
	"Frame-1": 
		>>> df
		      a     b   c     d
		0   abd  john   0  1000
		1   abd  john   1  1001
		4   pqr  john   4  1004
		9   xyz   doe   9  1009
		10  xyz   doe  10  1010
		11  xyz   doe  11  1011
		12  xyz   doe  12  1012
		13  xyz   doe  13  1013
		>>>
		
	"Frame-2":
		>>> df
		      a     b   c     d
		0   abd  john   0  1000
		9   xyz   doe   9  1009
		10  xyz   doe  10  1010
		11  xyz   doe  11  1011
		12  xyz   doe  12  1012
		>>> 
		
}
"answer": {
	"desc": %s You can use and as below: 
	"code-snippets": [
		n = 2
		df.drop(df.groupby(['a','b']).tail(n).index, axis=0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62875221
"link": https://stackoverflow.com/questions/62875221/keep-a-single-element-in-dataframe-of-lists
"question": {
	"title": Keep a single element in dataframe of lists
	"desc": Given the following dataframe: How can I remove all but the first element in each column and then unlist so the dataframe becomes like this: 
}
"io": {
	"Frame-1": 
		  Movement Distance     Speed   Delay    Loss
		0   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]
		1   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]
		2   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]
		3   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]
		4   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]
		
	"Frame-2":
		  Movement Distance     Speed   Delay    Loss
		0   1      1            25      0        0
		1   1      1            25      0        0
		2   1      1            25      0        0
		3   1      1            25      0        0
		4   1      1            25      0        0
		
}
"answer": {
	"desc": %s You can with or indexing equivalently as: Or if the data is already clean, we have new in pandas 1.0 (thanks cs95): 
	"code-snippets": [
		df.apply(lambda x: pd.to_numeric(x.str[0], downcast='integer', errors='ignore'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62862213
"link": https://stackoverflow.com/questions/62862213/pandas-creating-values-in-a-column-based-on-the-previous-value-in-that-column
"question": {
	"title": Pandas: creating values in a column based on the previous value in that column
	"desc": Quick example: Before: After So the formula here is . I started with adding a Value column where each cell is 0. I then have looked a shift() but that uses the value in the previous row from the start of the command/function. So it will always use 0 as the value for Value. Is there a way of doing this without using something like iterrows() or a for loop ? 
}
"io": {
	"Frame-1": 
		In  Out    
		1    5    
		10   0    
		2    3
		
	"Frame-2":
		In  Out  Value    
		1    5   -4    
		10   0    6    
		2    3    5
		
}
"answer": {
	"desc": %s It seems in your calculations you could first calculate and later use or even shorter 
	"code-snippets": [
		df['Value'] = (df['In'] - df['Out']).cumsum()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62848641
"link": https://stackoverflow.com/questions/62848641/pandas-merging-rows-dataframe-transformation
"question": {
	"title": Pandas merging rows / Dataframe Transformation
	"desc": I have this example DataFrame: How would I be able to convert it to this: I am thinking maybe I should pivot by counting with lens or assigning a index that could be multiple of 3, but I really am not sure what would be the most efficient way. 
}
"io": {
	"Frame-1": 
		e   col1    col2    col3
		1   238.4   238.7   238.2
		2   238.45  238.75  238.2
		3   238.2   238.25  237.95
		4   238.1   238.15  238.05
		5   238.1   238.1   238
		6   229.1   229.05  229.05
		7   229.35  229.35  229.1
		8   229.1   229.15  229
		9   229.05  229.05  229
		
	"Frame-2":
		                1                      2            3   
		    col1    col2    col3    col1    col2    col3    col1    col2    col3
		1   238.4   238.7   238.2   238.45  238.75  238.2   238.2   238.25  237.95
		2   238.1   238.15  238.05  238.1   238.1   238     229.1   229.05  229.05
		3   229.35  229.35  229.1   229.1   229.15  229    229.05   229.05  229
		
}
"answer": {
	"desc": %s Create a grouping series , this we will be needed to group the dataframe so that every third element (taking a step size of 3) belongs to the same group, use to get the unique grouping keys, next use to group the dataframe on and use to set the index of every grouped frame to , finally use to concat all the grouped dataframes along and pass the optional parameter to create columns : Details: Result: 
	"code-snippets": [
		g, k = df.pop('e').sub(1) % 3 + 1, np.unique(g)
		df1 = pd.concat([g.set_index(k) for _, g in df.groupby(g)], keys=k, axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62818462
"link": https://stackoverflow.com/questions/62818462/turn-column-levels-inside-out
"question": {
	"title": Turn column levels inside-out
	"desc": I have a pandas DataFrame which looks like this (code to create it is at the bottom of the question): I would like to turn the and columns "inside out", i.e. my expected output is: Is there an efficient (i.e. that does not involve writing a python loop that goes through each row one-by-one) way to do this in pandas? Code to generate the starting DataFrame: Code to generate expected output: 
}
"io": {
	"Frame-1": 
		  col_1 col_2 foo_1       foo_2      
		              col_3 col_4 col_3 col_4
		0     1     4     2     8     5     7
		1     3     1     6     3     8     9
		
	"Frame-2":
		   col_1  col_2                     col_3                     col_4
		0      1      4  {'foo_1': 2, 'foo_2': 5}  {'foo_1': 8, 'foo_2': 7}
		1      3      1  {'foo_1': 6, 'foo_2': 8}  {'foo_1': 3, 'foo_2': 9}
		
}
"answer": {
	"desc": %s Use + and aggregate the columns along using , finally use to drop the columns: Result: 
	"code-snippets": [
		df['col_3'] = df.filter(like='col_3').droplevel(1, 1).agg(dict, axis=1)
		df['col_4'] = df.filter(like='col_4').droplevel(1, 1).agg(dict, axis=1)
		
		df = df.drop(['foo_1', 'foo_2'], 1).droplevel(1, 1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56044461
"link": https://stackoverflow.com/questions/56044461/position-or-move-pandas-column-to-a-specific-column-index
"question": {
	"title": position or move pandas column to a specific column index
	"desc": I have a DF and it has multiple columns (over 75 columns) with default numeric index: I need to arrange/change position to as follows: I can get the index of using: but I don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list. 
}
"io": {
	"Frame-1": 
		Col1 Col2 Col3 ... Coln
		
	"Frame-2":
		Col1 Col3 Col2 ... Coln 
		
}
"answer": {
	"desc": %s How to proceed: store the names of columns in a list; swap the names in that list; apply the new order on the dataframe. code: 
	"code-snippets": [
		l = list(df)
		
		i1, i2 = l.index('Col2'), l.index('Col3')
		l[i2], l[i1] = l[i1], l[i2]
		
		df = df[l]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62811834
"link": https://stackoverflow.com/questions/62811834/rename-columns-in-a-pandas-dataframe-with-values-form-dictionary
"question": {
	"title": Rename Columns in a Pandas Dataframe with values form dictionary
	"desc": I have a pandas data frame read from an excel file. Note: the column names remain the same but the position of the column might vary in the excel file. df I have a list of dictionaries that should be used to change the column names, which is as below field_map I could convert the column keys for each row in the DataFrame separately in this way and using the for further operations. This method is taking too long when my file is large. I want to change the column headers of the data Frame before processing the entries further, this will reduce a lot of processing time for me. Kindly help me with this. I'm expecting the data frame to be something like this Expected df Thanks in Advance 
}
"io": {
	"Frame-1": 
		    colA    colB    colC   ...
		0   val11   val12   val13  ... 
		1   val21   val22   val23  ...
		... ... ...
		
	"Frame-2":
		    tab1    tab2    tab3   ...
		0   val11   val12   val13  ... 
		1   val21   val22   val23  ...
		... ... ...
		
}
"answer": {
	"desc": %s Just use the function in your existing dataframe : You would need to modify the dictionary a bit: 
	"code-snippets": [
		df = df.rename(columns={"colA":"tab1", "colB":"tab2", "colB":"tab3"})
		
		----------------------------------------------------------------------
		col_rename_dict = {el["file_field"]:el["table_field"] for el in field_map}
		df = df.rename(columns=col_rename_dict)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62721565
"link": https://stackoverflow.com/questions/62721565/split-the-data-frame-based-on-consecutive-row-values-differences
"question": {
	"title": Split the data frame based on consecutive row values differences
	"desc": I have a data frame like this, Now I want to create multiple data frames from above when the col1 difference of two consecutive rows are more than 1. So the result data frames will look like, I can do this using for loop and storing the indices but this will increase execution time, looking for some pandas shortcuts or pythonic way to do this most efficiently. 
}
"io": {
	"Frame-1": 
		df
		col1    col2    col3
		 1        2      3
		 2        5      6
		 7        8      9
		10       11     12
		11       12     13
		13       14     15
		14       15     16
		
	"Frame-2":
		df1
		col1    col2    col3
		 1        2      3
		 2        5      6
		df2
		col1    col2    col3
		 7        8      9
		df3
		col1    col2    col3
		10       11     12
		11       12     13
		df4
		col1    col2    col3
		13       14     15
		14       15     16
		
}
"answer": {
	"desc": %s You could define a custom grouper by taking the , checking when it is greater than , and take the of the boolean series. Then group by the result and build a dictionary from the groupby object: A more detailed break-down: 
	"code-snippets": [
		df.assign(difference=(diff:=df.col1.diff()), 
		          condition=(gt1:=diff.gt(1)), 
		          grouper=gt1.cumsum())
		
		   col1  col2  col3  difference  condition  grouper
		0     1     2     3         NaN      False        0
		1     2     5     6         1.0      False        0
		2     7     8     9         5.0       True        1
		3    10    11    12         3.0       True        2
		4    11    12    13         1.0      False        2
		5    13    14    15         2.0       True        3
		6    14    15    16         1.0      False        3
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62697810
"link": https://stackoverflow.com/questions/62697810/replace-specific-values-in-multiindex-dataframe
"question": {
	"title": Replace specific values in multiindex dataframe
	"desc": I have a multindex dataframe with 3 index levels and 2 numerical columns. I want to replace the values in first row of 3rd index level wherever a new second level index begins. For ex: every first row The dataframe is too big and doing it datframe by dataframe like gets time consuming. Is there some way where i can get a mask and replace with new values in these positions ? 
}
"io": {
	"Frame-1": 
		A   1   2017-04-01  14.0    87.346878
		        2017-06-01  4.0     87.347504
		    2   2014-08-01  1.0     123.110001
		        2015-01-01  4.0     209.612503
		B   3   2014-07-01  1.0     68.540001
		        2014-12-01  1.0     64.370003
		    4   2015-01-01  3.0     75.000000
		
	"Frame-2":
		(A,1,2017-04-01)->0.0   0.0 
		(A,2,2014-08-01)->0.0   0.0  
		(B,3,2014-07-01)->0.0   0.0  
		(B,4,2015-01-01)->0.0   0.0
		
}
"answer": {
	"desc": %s Use on , then use on and aggregate using , then using create a multilevel index, finally use this multilevel index to change the values in dataframe: Result: 
	"code-snippets": [
		idx = df.reset_index(level=2).groupby(level=[0, 1])['level_2'].first()
		idx = pd.MultiIndex.from_arrays(idx.reset_index().to_numpy().T)
		df.loc[idx, :] = 0
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62696143
"link": https://stackoverflow.com/questions/62696143/append-list-to-list-in-specific-cell-in-pandas
"question": {
	"title": Append list to list in specific cell in pandas
	"desc": I have a pandas dataframe, which looks like this: And I want to append a new list c = [5,6] to the row, where So the result would be: Right now my attempt looks like this: Any help is appreciated! 
}
"io": {
	"Frame-1": 
		   key   arr 
		a  't1' [1,2]
		b  't2' [3,4]
		
	"Frame-2":
		   key   arr 
		a  't1' [1,2,5,6]
		b  't2' [3,4]
		
}
"answer": {
	"desc": %s First idea is use list comprehension with : Or you can create new with and filtered index values by mask and add by : 
	"code-snippets": [
		m = df['key'] == 't1'
		
		df.loc[m, 'arr'] += pd.Series(np.repeat([c], m.sum(), axis=0).tolist(), index=df.index[m])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62664248
"link": https://stackoverflow.com/questions/62664248/how-to-select-a-row-by-value-set-it-as-the-header-row-drop-all-the-rows-up-to-th
"question": {
	"title": How to select a row by value-set it as the header row-drop all the rows up to that value in python?
	"desc": I have a dataframe such as this: What I would like to do is make row 4 the header and drop the rows above it. The key is that the source data is being read from an excel and the format of the report could change, so it may all of the sudden, start looking like this: I know I could do this: However, I need to keep all of this independent of the order of the dataframe because that could change. So, I need to select a row with a specific value, make that row the header and then delete/drop all the rows that lead up to that row that I selected if they are left in the dataframe (probably dependent on how the code is written). Everything that I've looked up seems to assume the data columns/rows won't change from load to load. I hope I worded this well enough to make sense..... 
}
"io": {
	"Frame-1": 
		0  1   2   3   4
		1  a   b   c   d
		2  a   b   c   d
		3  a   b   c   d
		4 gab fob upo tem
		
	"Frame-2":
		0  1   2   3   4
		1  a   b   c   d
		2 gab fob upo tem
		3  a   b   c   d
		4  a   b   c   d
		
}
"answer": {
	"desc": %s If you already know what the headers are, you can search for them: Output: 
	"code-snippets": [
		df = pd.read_excel('file.xlsx', header=None)
		headers = [2, 'gab','fob','upo','tem']
		
		starts = (df==headers).all(1).idxmax()
		
		df.columns=df.loc[starts]
		df = df.iloc[starts+1:]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62611339
"link": https://stackoverflow.com/questions/62611339/pandas-multiply-column-value-by-sum-of-group
"question": {
	"title": Pandas: multiply column value by sum of group
	"desc": I have a dataframe which looks like this: I want to add a column 'c' which multiplies the value of 'b' by the sum of the group it belongs to in column 'a'. So, first row should be 0.15 * 0.5 (sum of group A) = 0.075. This would be the excel formula for column 'c' =B1*SUMIF($A$1:$A$9,A1,$B$1:$B$9) Resulting dataframe should look like this: 
}
"io": {
	"Frame-1": 
		   a     b
		0  A  0.15
		1  A  0.25
		2  A  0.10
		3  B  0.20
		4  B  0.10
		5  B  0.25
		6  B  0.60
		7  C  0.50
		8  C  0.70
		
	"Frame-2":
		    a   b       c
		0   A   0.15    0.075
		1   A   0.25    0.125
		2   A   0.10    0.05
		3   B   0.20    0.23
		4   B   0.10    0.115
		5   B   0.25    0.2875
		6   B   0.60    0.69
		7   C   0.50    0.6
		8   C   0.70    0.84
		
}
"answer": {
	"desc": %s Try groupby + transform and then multiply: 
	"code-snippets": [
		df['b'] * df.groupby('a')['b'].transform('sum')
		#df['c'] = df['b'] * df.groupby('a')['b'].transform('sum')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62527486
"link": https://stackoverflow.com/questions/62527486/a-better-way-to-map-data-in-multiple-datasets-with-multiple-data-mapping-rules
"question": {
	"title": A better way to map data in multiple datasets, with multiple data mapping rules
	"desc": I have three datasets (, , ), and I wish to add a new column called in dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes. final_NN: ppt_code: herd_id: Expected output: The rules is: if in final_NN is not , = in ; if in final_NN is but in is not Null, then search the ppt_code dataframe, and use the and its corresponding "number" to map and fill in the "MapValue" in ; if both and in are and null respectively, but in is not Null, then search dataframe, and use the and its corresponding to fill in the in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result? 
}
"io": {
	"Frame-1": 
		  code  number
		0   AA      11
		1   AA      11
		2   BB      22
		3   BB      22
		4   CC      33
		
	"Frame-2":
		    ID  number
		0  799     678
		1  813     789
		
}
"answer": {
	"desc": %s First create a mapping series from the and dataframes, then use to create a new column by replacing the values in column with , then use two consecutive along with to fill the missing values in column according to the rules: Result: 
	"code-snippets": [
		ppt_map = ppt_code.drop_duplicates(subset=['code']).set_index('code')['number']
		hrd_map = herd_id.drop_duplicates(subset=['ID']).set_index('ID')['number']
		
		final_NN['MapNumber'] = final_NN['number'].replace({'Unknown': np.nan})
		final_NN['MapNumber'] = (
		    final_NN['MapNumber']
		    .fillna(final_NN['code'].map(ppt_map))
		    .fillna(final_NN['ID'].map(hrd_map))
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62536086
"link": https://stackoverflow.com/questions/62536086/vectorizing-for-loop
"question": {
	"title": Vectorizing for-loop
	"desc": I have a very large dataframe (~10^8 rows) where I need to change some values. The algorithm I use is complex so I tried to break down the issue into a simple example below. I mostly programmed in C++, so I keep thinking in for-loops. I know I should vectorize but I am new to python and very new to pandas and cannot come up with a better solution. Any solutions which increase performance are welcome. Any ideas? EDIT: I was ask to explain what I do with my for-loops. For every eventID I want to know if all corresponding types contain a 1 or a 0 or both. If they contain a 1, all values which are equal to -1 should be changed to 1. If the values are 0, all values equal to -1 should be changed to 0. My problem is to do this efficiently for each eventID independently. There can be one or multiple entries per eventID. Input of example: Output of example: 
}
"io": {
	"Frame-1": 
		    eventID  types
		0         1      0
		1         1     -1
		2         1     -1
		3         2     -1
		4         2      1
		5         3      0
		6         4      0
		7         5      0
		8         6     -1
		9         6     -1
		10        6     -1
		11        6      1
		12        7     -1
		13        8     -1
		
	"Frame-2":
		    eventID  types
		0         1      0
		1         1      0
		2         1      0
		3         2      1
		4         2      1
		5         3      0
		6         4      0
		7         5      0
		8         6      1
		9         6      1
		10        6      1
		11        6      1
		12        7     -1
		13        8     -1
		
}
"answer": {
	"desc": %s First we create boolean masks and using then use on this mask and transform using , then using chose the elements from depending upon the conditions : Result: 
	"code-snippets": [
		m1 = mydf['types'].eq(1).groupby(mydf['eventID']).transform('any')
		m2 = mydf['types'].eq(0).groupby(mydf['eventID']).transform('any')
		mydf['types'] = np.select([m1 , m2], [1, 0], mydf['types'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62455905
"link": https://stackoverflow.com/questions/62455905/from-two-arrays-to-one-dataframe-python
"question": {
	"title": from two arrays to one dataframe python
	"desc": I am trying to put my values into two arrays and then to make them a dataframe. I am using python, numpy and pandas to do so. my arrays are: and I would like to put them into a pandas dataframe. When I print my dataframe, I would like to see this: How can I do that? I read some related questions, but I can't get it right. One of the errors says that indexes must not be tuples, but, as you can see, I don't have tuples 
}
"io": {
	"Frame-1": 
		k = [7.0, 8.0, 6.55, 7.0000001, 10.12]
		p = [6.94, 9.0, 4.44444, 13.0, 9.0876]
		
	"Frame-2":
		    a     b    c     d     e
		k  7.0   8.0  6.6   7.0  10.1
		p  6.9   9.0  4.4  13.0   9.1
		
}
"answer": {
	"desc": %s You can always have as input to be a list of lists, which will generate the output you desire: And if you want rounded: for dynamic columns: 
	"code-snippets": [
		from string import ascii_lowercase
		pd.DataFrame([k,p],columns=list(ascii_lowercase[:len(k)]),index=['k','p']).round()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62436677
"link": https://stackoverflow.com/questions/62436677/group-rows-in-a-pandas-data-frame-when-the-difference-of-consecutive-rows-are-le
"question": {
	"title": group rows in a pandas data frame when the difference of consecutive rows are less than a value
	"desc": I have a data frame like this, Now I want to group those rows where there difference between two consecutive col1 rows is less than 3. and sum other column values, create another column(col4) with the last value of the group, So the final data frame will look like, using for loop to do this is tedious, looking for some pandas shortcuts to do it most efficiently. 
}
"io": {
	"Frame-1": 
		col1    col2    col3
		 1        2       3
		 2        3       4
		 4        2       3
		 7        2       8
		 8        3       4
		 9        3       3
		 15       1       12
		
	"Frame-2":
		col1    col2    col3    col4
		  1       7       10     4
		  7       8       15     9
		
}
"answer": {
	"desc": %s You can do a named aggregation on groupby: Output: update without named aggregation you can do some thing like this: 
	"code-snippets": [
		(df.groupby(df.col1.diff().ge(3).cumsum(), as_index=False)
		   .agg(col1=('col1','first'),
		        col2=('col2','sum'),
		        col3=('col3','sum'),
		        col4=('col1','last'))
		)
		
		----------------------------------------------------------------------
		groups = df.groupby(df.col1.diff().ge(3).cumsum())
		new_df = groups.agg({'col1':'first', 'col2':'sum','col3':'sum'})
		new_df['col4'] = groups['col1'].last()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62376848
"link": https://stackoverflow.com/questions/62376848/finding-the-frequency-that-each-column-is-a-row-minimum
"question": {
	"title": Finding the frequency that each column is a row minimum
	"desc": I have a dataframe that looks like: I would like to find the frequency that each column contains the row's minimum. So in some format: I am currently doing and then looping through each row using ... but there has to be a better way. In case it matters, I have a couple hundred columns, a couple thousand rows, and it represents a sample, so I have to perform the operation roughly a million times. I must be missing an obvious pandas or numpy method that will do this both pythonically and reasonably efficiently. 
}
"io": {
	"Frame-1": 
		       A     B     C     D
		 0   1.2     0   1.1   3.2
		 1   2.3   2.2   2.2   2.5
		 2   1.1   1.5     0   1.7
		 3     0   1.1   1.4   1.2
		 4   3.3   3.0   1.7   1.7
		 5   1.1   1.0   2.2   2.5
		 6   5.0   5.0   5.0   5.0
		
	"Frame-2":
		B: 2               # rows 0, 5
		A: 1               # row 3
		C: 1               # row 2
		(B, C): 1          # row 1
		(C, D): 1          # row 4
		(A, B, C, D): 1    # row 6
		
}
"answer": {
	"desc": %s IIUC, Transpose the dataset (making it easier with axis) Identify the location of min values in each column. Identify the column names for each min Value counts 
	"code-snippets": [
		df = df.T
		
		result = (df.eq(df.min())
		            .apply(lambda x:tuple(x.index[x]))
		            .value_counts())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62361446
"link": https://stackoverflow.com/questions/62361446/python-dataframe-get-index-start-and-end-of-successive-values
"question": {
	"title": Python dataframe get index start and end of successive values
	"desc": Let's say I have this dataframe : I want to store the start and end indexes of each value (even repeated ones) in the dataframe as well as the value corresponding. So that I would get a result like this for example : I tried this (for the value 2 for example here) : But this returns only first and last result each time. 
}
"io": {
	"Frame-1": 
		   0
		0  1
		1  1
		2  1
		3  2
		4  2
		5  3
		6  3
		7  1
		8  1
		
	"Frame-2":
		Value    |   Start   |   End
		----------------------------
		1        |     0     |    2
		2        |     3     |    4
		3        |     5     |    6
		1        |     7     |    8
		
}
"answer": {
	"desc": %s Given Solution: Result: 
	"code-snippets": [
		starts_bool = df.diff().ne(0)[0]
		starts = df.index[starts_bool]
		ends = df.index[starts_bool.shift(-1, fill_value=True)]
		
		result = (df.loc[starts]
		            .reset_index(drop=True)
		            .assign(Start=starts, End=ends)
		            .rename({0: 'Value'}, axis='columns')
		          )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62351000
"link": https://stackoverflow.com/questions/62351000/extract-a-value-from-a-pandas-dataframe-dict-into-another-dataframe
"question": {
	"title": extract a value from a pandas dataframe dict into another dataframe
	"desc": df.head(10) How to convert the above dataframe into a new dataframe by selecting X: df.info() shows 
}
"io": {
	"Frame-1": 
		    XYZVal
		0   {"X":"56.68","Y":"51.56","Z":"100"}
		1   {"X":"58.05","Y":"52.37","Z":"62.6"}
		2   {"X":"59.32","Y":"54.48","Z":"69.59"}
		3   {"X":"58.51","Y":"36.36","Z":"82.76"}
		4   {"X":"65.21","Y":"60.26","Z":"71.06"}
		5   {"X":"57.64","Y":"52.07","Z":"67.89"}
		6   {"X":"58.24","Y":"50","Z":"75"}
		7   {"X":"57.69","Y":"52.13","Z":"68.64"}
		8   {"X":"57.83","Y":"53.05","Z":"65.92"}
		9   {"X":"60.87","Y":"51.73","Z":"71.35"}
		
		
	"Frame-2":
		{ 56.68 ,58.05 ,59.32 ,58.51 ,65.21 ,57.64 ,58.24 ,57.69 ,57.83 ,60.87 }
		
}
"answer": {
	"desc": %s If does already contain dicts, just use: If the column entries are strings, just use: result (in both ways) 
	"code-snippets": [
		import json
		df['XYZVal'].apply(lambda x: json.loads(x)["X"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62328295
"link": https://stackoverflow.com/questions/62328295/create-a-nested-dictionary-from-dataframe-where-first-column-is-the-key-for-par
"question": {
	"title": create a nested dictionary from dataframe, where first column is the key for parent dictionary
	"desc": I am trying to create a nested dictionary from a pandas dataframe. The first column-values are supposed to be the key for the upper dictionary, which will contaion the other columns as dictionary, where the column header is the key. I would like to avoid loops. the dataframe: what I would like to have: what I have tried: which unfortunately returns: Any help is highly appreciated. Thanks 
}
"io": {
	"Frame-1": 
		dict_expt = {'11': {'B': [1, 2, 3],
		                  'C': [1.0, 0.7, 0.3]},
		           '12': {'B': [4, 5],
		                  'C': [1.0]}}
		
	"Frame-2":
		{'B': 
		     {11: [1, 2, 3], 
		      12: [4, 5]}, 
		'C': {11: [1.0, 0.7, 0.3], 
		      12: [1.0]}}
		
}
"answer": {
	"desc": %s You were close, just add to : Output: 
	"code-snippets": [
		df.groupby(['A']).agg({'B':lambda x: list(x.unique()),
		                      'C':lambda x: list(x.unique())}).to_dict("index")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62302113
"link": https://stackoverflow.com/questions/62302113/remove-quotation-marks-and-brackets-from-pandas-dataframe-csv-file-after-perfor
"question": {
	"title": Remove quotation marks and brackets from Pandas DataFrame .csv file after performing a GroupBy with MultiIndex
	"desc": I'm new to pandas so apologies if my explanations of things are wrong. I have a data frame created as follows: Then I perform a weighted mean, using the indices, using code from the second top answer here. The output on the console looks like this: where (x,y) are the indices that I have grouped by and the number at the end is the weighted mean. When I export to a .csv file, I get a file that looks like this: This is not what I want. I would like to get a .csv file that looks like this: I've tried using reset.index() but this does not work. I want to remove the brackets, quotation marks and the rogue ,0 at the start of the .csv file. How can I do this? Many thanks in advance. 
}
"io": {
	"Frame-1": 
		        (1, 2) 3
		        (4, 5) 6
		        (7, 8) 9
		
	"Frame-2":
		        ,0
		        "(1, 2)",3
		        "(4, 5)",6
		        "(7, 8)",9
		
}
"answer": {
	"desc": %s Use, level instead of indices: 
	"code-snippets": [
		df2 = df.groupby(level=df.index.names).apply(lambda x: np.average(x.name3, weights=x.name4))
		
		# save the df2 to csv file
		df2.rename('avg').reset_index().to_csv('data.csv', index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62215453
"link": https://stackoverflow.com/questions/62215453/python-pandas-iterate-over-columns-and-also-update-columns-based-on-apply-condit
"question": {
	"title": Python Pandas Iterate over columns and also update columns based on apply conditions
	"desc": I am trying to update dataframe columns based on consecutive columns values. If columns say col1 and col2 has >0 and <0 values, then same columns should get updated as col2=col1 + col2 and col1=0 and also counter +1 (gives how many fixes has been done throughout the column). Dataframe look like: Required Dataframe after applying logic: I tried: It failed I also looked into but I couldn't figure out the way. DDL to generate DataFrame: Thanks! 
}
"io": {
	"Frame-1": 
		id  col0    col1    col2    col3    col4    col5 col6   col7    col8    col9    col10
		1   0   5   -5  5   -5  0 0 1   4   3   -3 
		2   0   0   0   0   0   0 0 4   -4  0   0 
		3   0   0   1   2   3   0 0 0   5   6   0 
		
	"Frame-2":
		id  col0    col1    col2    col3    col4    col6    col7    col8    col9    col10   fix
		1   0   0   0   0   0   0   0 1 4   0   0 0 3
		2   0   0   0   0   0   0   0 0 0   0   0 0 1
		3   0   0   1   2   3   0   0 0 5   6   0 9 0
		
}
"answer": {
	"desc": %s Here is an approach without loops: Details: checks if current column is > 0 and next column is <0. Add current column to next column in the next column to where is True. Set the current column to 0 if c is True. Get sum of c for changes done. 
	"code-snippets": [
		c = df.gt(0) & df.shift(-1,axis=1).lt(0)
		
		out = (df.mask(c.shift(axis=1).fillna(False),df+df.shift(axis=1))
		      .mask(c,0).assign(Fix=c.sum(1)))
		
		print(out)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62200280
"link": https://stackoverflow.com/questions/62200280/trying-to-create-a-dataframe-from-a-list-of-tuples-and-a-tuple
"question": {
	"title": Trying to create a dataframe from a list of tuples, and a tuple
	"desc": As the title states I'm tyring to create a df using a list of tuples and a tuple. So what I currently have is and my result is: Where as I want something like: Any and help is appreciated! 
}
"io": {
	"Frame-1": 
		   A  B
		0  1  4
		1  2  5
		2  3  6
		
	"Frame-2":
		   0  1
		0  A  1,2,3
		1  B  4,5,6
		
}
"answer": {
	"desc": %s You can try something like: 
	"code-snippets": [
		d1 = {k: ','.join([*map(str,v)]) for k,v in d.items()}
		pd.DataFrame.from_dict(d1,orient='index').reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62011392
"link": https://stackoverflow.com/questions/62011392/how-to-merge-or-join-a-stacked-dataframe-in-pandas
"question": {
	"title": How to merge or join a stacked dataframe in pandas?
	"desc": I cannot find this question answered elsewhere; I would like to do a SQL-like join in pandas but with the slight twist that one dataframe is stacked. I have created a dataframe A with a stacked column index from a csv file in pandas that looks as follows: The original csv had repeated what is in the 1st column for every entry like so: The original csv was the transposed version of this. Pandas chose to stack that when converting to dataframe. (I used this code: pd.read_csv(file, header = [0,1], index_col=0).T) In another csv/dataframe B I have for all of those so-called ticker symbols another ID that I would rather like to use: CIK. Desired output: I would like to have the CIK instead of the ticker in a new dataframe otherwise identical to A. Now in SQL I could easily join on A.name_of_2nd_column = b.Ticker since the table would just have the entry in the 1st column repeated in every line (like the original csv) and the column would have a name but in pandas I cannot. I tried this code: How do I tell pandas to use the 2nd column as the key and/or interpret the first column just as repeated values? 
}
"io": {
	"Frame-1": 
		|           |      | 2013-01-04 | 2013-01-07 |
		|----------:|-----:|-----------:|-----------:|
		| Adj Close |  OWW | NaN        | NaN        |
		|   Close   | OXLC | 4.155157   | 4.147217   |
		|           |  OXM | 40.318089  | 42.988800  |
		|           |  OXY | 50.416079  | 62.934800  |
		
	"Frame-2":
		|           |      | 2013-01-04 | 2013-01-07 |
		|----------:|-----:|-----------:|-----------:|
		| Adj Close |  OWW | NaN        | NaN        |
		|   Close   | OXLC | 4.155157   | 4.147217   |
		|   Close   |  OXM | 40.318089  | 42.988800  |
		|   Close   |  OXY | 50.416079  | 62.934800  |
		
}
"answer": {
	"desc": %s I was eventually able to do it the following way: 
	"code-snippets": [
		ticker_2_CIK = dict(zip(tix.Ticker,tix.CIK))  # create a dict
		
		tmp = df.reset_index().assign(CIK=lambda x: x['ticker'].map(ticker_2_CIK)) # use dict to find the correct value for colum 
		
		# data was unclean, some ticker symbols were created after the period my data is from 
		# and data was incomplete with some tickers missing
		solution = tmp.dropna(subset=['CIK']).astype({'CIK':int})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62139372
"link": https://stackoverflow.com/questions/62139372/read-csv-and-iterate-through-10-row-blocks
"question": {
	"title": read csv and Iterate through 10 row blocks
	"desc": I am trying to read a CSV file and Iterate through 10-row blocks. The data is quite unusual, with two columns and 10-row blocks. 57485 rows x 2 columns in the format below: Every 10 rows consist of a grid reference and two records X/Y ref. The grid reference and X value is in column 1, the Y value is in column 2, and then 9 rows with 12 columns, in column one. The code below reads 10 rows, but keeps repeating the first row in all following 10-row blocks?? I don't understand why it keeps repeating the first row?? Any suggestions to resolve this would be appreciated.. The first two block: 
}
"io": {
	"Frame-1": 
		Grid-ref=   1,148
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630
		
		Grid-ref=   1,311
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		  490  290  280  230  200  250  440  530  460  420  530  450
		
		Grid-ref=   1,312
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		  460  280  260  220  190  240  430  520  450  400  520  410
		
	"Frame-2":
		                                      Grid-ref=   1   148
		0   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		1   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		2   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		3   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		4   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		5   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		6   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		7   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		8   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		9   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN
		                                        Grid-ref=   1    148
		10                                      Grid-ref=   1  311.0
		11    490  290  280  230  200  250  440  530  460 ...    NaN
		12    490  290  280  230  200  250  440  530  460 ...    NaN
		13    490  290  280  230  200  250  440  530  460 ...    NaN
		14    490  290  280  230  200  250  440  530  460 ...    NaN
		15    490  290  280  230  200  250  440  530  460 ...    NaN
		16    490  290  280  230  200  250  440  530  460 ...    NaN
		17    490  290  280  230  200  250  440  530  460 ...    NaN
		18    490  290  280  230  200  250  440  530  460 ...    NaN
		19    490  290  280  230  200  250  440  530  460 ...    NaN
		
}
"answer": {
	"desc": %s Pandas is good for uniform columnar data. If your input isn't uniform, you can preprocess it and then load the dataframe. This one is easy, all you need to do is scan for grid headers and remove them. Since the data itself is numeric, separated by whitespace, a simple split will parse it. This example creates a list but if the dataset is large, it may be reasonable to write to an intermediate file instead. 
	"code-snippets": [
		import csv
		import re
		import pandas as  pd
		
		grid_re = re.compile(r"Grid-ref=\s*(\d+),(\d+)")
		
		with open('test.csv') as fobj:
		    table = []
		    try:
		        while True:
		            # find next block
		            for line in fobj:
		                match = grid_re.search(line)
		                if match:
		                    grid_xy = list(match.groups())
		                    break
		            else:
		                raise StopIteration()
		            for _ in range(10):
		                line = next(fobj)
		                # add row plus grid columns
		                table.append(line.strip().split() + grid_xy)
		    except StopIteration:
		        pass
		
		df = pd.DataFrame(table)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 32093829
"link": https://stackoverflow.com/questions/32093829/remove-duplicates-from-dataframe-based-on-two-columns-a-b-keeping-row-with-max
"question": {
	"title": Remove duplicates from dataframe, based on two columns A,B, keeping row with max value in another column C
	"desc": I have a pandas dataframe which contains duplicates values according to two columns (A and B): I want to remove duplicates keeping the row with max value in column C. This would lead to: I cannot figure out how to do that. Should I use , something else? 
}
"io": {
	"Frame-1": 
		A B C
		1 2 1
		1 2 4
		2 7 1
		3 4 0
		3 4 8
		
	"Frame-2":
		A B C
		1 2 4
		2 7 1
		3 4 8
		
}
"answer": {
	"desc": %s You can do it using group by: is a of the maximum values of in each group but which is of the same length and with the same index as . If you haven't used then printing might be a good idea to see how it works. Another approach using would be Not sure which is more efficient but I guess the first approach as it doesn't involve sorting. EDIT: From up the second solution would be or, alternatively, In any case, the solution seems to be significantly more performing: 
	"code-snippets": [
		c_maxes = df.groupby(['A', 'B']).C.transform(max)
		df = df.loc[df.C == c_maxes]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62112525
"link": https://stackoverflow.com/questions/62112525/read-a-text-file-having-row-in-parenthesis-and-values-separated-by-comma-using-p
"question": {
	"title": Read a Text file having row in parenthesis and values separated by comma using pandas
	"desc": I want to read a text file which contains data in parenthesis as row and values in it as column.The format of txt file is below : I want the data in this format : When i am reading text file as csv file it reads all the data in one row only. It shows 1 row and all the columns. Please help me with this problem. 
}
"io": {
	"Frame-1": 
		(a, b, c, d) (a1, b1, (c1,c12,c13), d1) (a2, b2, (c2,c22,c23), d2) (a3, b3, (c3,c32,c33), d3) (a4, b4, (c4,c42,c43), d4)
		
	"Frame-2":
		a  b  c  d
		a1 b1 c1 d1
		a2 b2 c2 d2
		a3 b3 c3 d3
		a4 b4 c4 d4
		
}
"answer": {
	"desc": %s With builtin functions (perhaps faster with large dataframe), you can use: 
	"code-snippets": [
		# Use the standard `read_csv` function of pandas.
		# Note the lineterminator option.
		df = pd.read_csv('data.dat', sep=",", lineterminator=")")
		# rename the 1st column (remove 1st char)
		df.columns.values[0] = df.columns.values[0][1:]
		# remove the opening parenthesis for the 1st columns:
		df.iloc[:, 0] = df.iloc[:, 0].str.replace('^\ ?\(', '')
		# remove the last line:
		df = df[:-1]  
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62053788
"link": https://stackoverflow.com/questions/62053788/how-to-sum-single-row-to-multiple-rows-in-pandas-dataframe-using-multiindex
"question": {
	"title": How to sum single row to multiple rows in pandas dataframe using multiindex?
	"desc": My dataframe with Quarter and Week as MultiIndex: I am trying to add the last row in Q1 (Q1-W04) to all the rows in Q2 (Q2-W15 through Q2-W18). This is what I would like the dataframe to look like: When I try to only specify the level 0 index and sumthe specific row, all Q2 values go to NaN. I have figured out that if I specify both the level 0 and level 1 index, there is no problem. Is there a way to sum the specific row to all the rows within the Q2 Level 0 index without having to call out each row individually by its level 1 index? Any insight/guidance would be greatly appreciated. Thank you. 
}
"io": {
	"Frame-1": 
		Quarter   Week      X   Y   Z
		Q1        Q1-W01    1   1   1
		          Q1-W02    2   2   2
		          Q1-W03    3   3   3
		          Q1-W04    4   4   4
		Q2        Q2-W15    15  15  15
		          Q2-W16    16  16  16
		          Q2-W17    17  17  17
		          Q2-W18    18  18  18
		
	"Frame-2":
		Quarter   Week      X   Y   Z
		Q1        Q1-W01    1   1   1
		          Q1-W02    2   2   2
		          Q1-W03    3   3   3
		          Q1-W04    4   4   4
		Q2        Q2-W15    19  19  19
		          Q2-W16    20  20  20
		          Q2-W17    21  21  21
		          Q2-W18    22  22  22
		
}
"answer": {
	"desc": %s try this df.loc returns a DataFrame, to set the value it looks for the list or array. Hence the above. 
	"code-snippets": [
		df.loc['Q2'] = (df.loc['Q2'] + df.loc['Q1', 'Q1-W04']).values.tolist()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62011520
"link": https://stackoverflow.com/questions/62011520/how-to-pick-some-values-of-a-column-and-make-another-one-with-them
"question": {
	"title": How to pick some values of a column and make another one with them?
	"desc": This is a table similar to the one I'm working with And what I'm trying to do is take some values of the column A that follow a certain pattern and create another column with such values. For example, the column C would have only the values from A that are bigger than 12, and column D the ones smaller or equal: I've tried making a list for each group of values, but I can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. I think there's an esasier way to do that, but I can't seem to find it. How can I do that? 
}
"io": {
	"Frame-1": 
		      A     B   
		0   12.2   43
		1   10.1   32
		2    3.4   34
		3   12.0   55
		4   40.6   31
		
	"Frame-2":
		      A     B      C     D 
		0   12.2   43    12.2   NaN 
		1   10.1   32     NaN   10.1
		2    3.4   34     NaN   3.4 
		3   12.0   55     NaN   12.0
		4   40.6   31    40.6   NaN
		
}
"answer": {
	"desc": %s And another version: Prints: 
	"code-snippets": [
		df['C'] = df.loc[df.A > 12, 'A']
		df['D'] = df.loc[df.A <= 12, 'A']
		
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61979568
"link": https://stackoverflow.com/questions/61979568/how-can-i-find-and-store-how-many-columns-it-takes-to-reach-a-value-greater-than
"question": {
	"title": How can I find and store how many columns it takes to reach a value greater than the first value in each row?
	"desc":  Original dataframe is df1. For each row, I want to find the first time a value is bigger than the value in the first column and store it in a new dataframe. df2 is the resulting dataframe. For example, for df1 row 1; the first value is 3 and the first value bigger than 3 is 4 (column c). Hence in df2 row 1, we store 2 (there are two columns from column a to c). For df1 row 2, the first value is 4 and the first value bigger than 4 is 5 (column d). Hence in df2 row 2, we store 3 (there are three columns from column a to d). For df1 row 3, the first value is 5 and the first value bigger than 5 is 6 (column e). Hence in df2 row 3, we store 4 (there are four columns from column a to e). I would appreciate the help. 
}
"io": {
	"Frame-1": 
		df1
		    a   b   c   d   e
		0   3   1   4   1   9
		1   4   2   3   5   4
		2   5   3   3   4   6
		
	"Frame-2":
		df2
		
		    b
		0   2
		1   3
		2   4
		
}
"answer": {
	"desc": %s In your case we can do , if the value than 0 , we get the id with 
	"code-snippets": [
		s=df1.columns.get_indexer(df1.drop('a',1).sub(df1.a,0).ge(0).idxmax(1))
		array([1, 1, 3])
		df['New']=s
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61969568
"link": https://stackoverflow.com/questions/61969568/how-can-i-create-a-new-dataframe-by-subtracting-the-first-column-from-every-othe
"question": {
	"title": How can I create a new dataframe by subtracting the first column from every other column?
	"desc":  df1 is my original dataframe. I want to create another dataframe by substracting column a from every other column (taking the difference between column a and all other columns). df2 is the outcome. I would appreciate your help. 
}
"io": {
	"Frame-1": 
		    a   b   c   d   e
		0   1   3   5   7   9
		1   1   3   5   7   9
		2   1   3   5   7   9
		
	"Frame-2":
		    b   c   d   e
		0   2   4   6   8
		1   2   4   6   8
		2   2   4   6   8
		
}
"answer": {
	"desc": %s  Prints: 
	"code-snippets": [
		df = df1.loc[:,'b':].apply(lambda x: x-df1['a'])
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61969815
"link": https://stackoverflow.com/questions/61969815/reverse-the-order-of-elements-by-group
"question": {
	"title": Reverse the order of elements by group
	"desc": Say I have a DataFrame like this: which looks like this I would like to reverse its elements within each group, where column determines the group. So, the desired output would be How can I do this? 
}
"io": {
	"Frame-1": 
		   a  b
		0  1  1
		1  1  2
		2  1  3
		3  1  4
		4  2  5
		5  2  6
		6  2  7
		7  2  8
		
	"Frame-2":
		   a  b
		0  1  4
		1  1  3
		2  1  2
		3  1  1
		4  2  8
		5  2  7
		6  2  6
		7  2  5
		
}
"answer": {
	"desc": %s This solution should achieve what the OP wants, which is to reverse(not to sort) the order of b for each a. 
	"code-snippets": [
		(
		    df.groupby('a', sort=False)
		    .apply(lambda x: x.iloc[::-1])
		    .reset_index(drop=True)
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61927861
"link": https://stackoverflow.com/questions/61927861/add-column-in-dataframe-from-make-list
"question": {
	"title": Add column in dataframe from make list
	"desc":  a*.csv b*.csv example I edited the content and then restored it because the answer didn't solve it and I solved it myself I'll end the question after commenting it solution but I chose one answer to close this question 
}
"io": {
	"Frame-1": 
		 D        A     B     E       F
		
		park    KOREA   1   SUM1    hello1
		
		michel  France  3   SUM3    hello3
		
		park2   USA     4   SUM4    hello4
		
	"Frame-2":
		A       B   C
		
		KOREA   1   2020
		
		KOREA   2   177
		
		France  3   2020
		
		USA     4   43
		
		SPAIN   7   67
		
}
"answer": {
	"desc": %s I think you can use the merge() function. I hope it works!! 
	"code-snippets": [
		try_a = glob.glob('a*.csv')
		try_b = glob.glob('b*.csv')
		lst_a = []
		lst_b = []
		for (i,j) in zip(try_a,try_b):
		 lst_a.append(i)
		 lst_b.append(j)
		df_a = pd.concat(lst_a)
		df_b = pd.concat(lst_b)
		df_a.set_index('d', inplace= True)
		df = pd.DataFrame.merge(df_a,df_b, how = 'inner', left_index = True).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61926146
"link": https://stackoverflow.com/questions/61926146/python-convert-matrices-to-permutations-table
"question": {
	"title": Python: Convert matrices to permutations table
	"desc": Given a set of ids, I need to get the values from a matrix (time A & B) for each id combination, and create a dataframe appending the values for all the permutations. I have been able to do it by creating the permutations dataframe and then iterating through it while looking & filling the values. However I need to do this for ~3000 ids, not 3, and I don't know how to do it efficiently. Can I generate a Time A/B dataframe as my example without having to iterate through 9000000* rows? I know I shouldn't be iterating though a dataframe however I haven't found an alternative yet. Ids (3): Time A matrix (3x3): Time B matrix (3x3): Time A/B dataframe (6): 
}
"io": {
	"Frame-1": 
		id          15        24      38
		15          0         1.8     1.7
		24          1.2       0       1.9
		38          1.5       1.3     0
		
	"Frame-2":
		id          15        24      38
		15          0         88.7    87.3
		24          42.2      0       32.7
		38          65.6      13.5    0
		
}
"answer": {
	"desc": %s Here you go: Output: 
	"code-snippets": [
		# drop set_index('id') if `id` is already index
		(pd.concat([timeA.set_index('id').stack().to_frame(name='A'),
		           timeB.set_index('id').stack().to_frame(name='B')], axis=1)
		   .rename_axis(index=['id_start','id_end'])
		   .query('id_start != id_end')
		   .reset_index()
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61868871
"link": https://stackoverflow.com/questions/61868871/pandas-groupby-expanding-df-based-on-unique-values
"question": {
	"title": pandas groupby expanding df based on unique values
	"desc": I have below: I want to achieve the following. For each unique , the bottom row is (this is ). I want to count how many times each unique value of occurs where . This part would be achieved by something like: However, I also want to add, for each unique value of , a column indicating how many times that value occurred after the point where for the value indicated by the row. I understand this might sound confusing; here's how the output woud look like in this example: It is important that the solution is general enough to be applicable on a similar case with more unique values than just , and . UPDATE As a bonus, I am also interested in how, instead of the count, one can instead return the sum of some value column, under the same conditions, divided by the corresponding in the rows. Example: suppose we now depart from below instead: The output would need to sum for the cases indicated by the counts in the solution by @jezrael, and divide that number by . The output would instead look like: 
}
"io": {
	"Frame-1": 
		df
		    V2  V1  A   B   C
		0   A   0   0   0   0
		1   B   1   0   0   0
		2   C   2   1   2   0
		
	"Frame-2":
		df
		    V2  V1  A   B   C
		0   A   0   0   0   0
		1   B   1   0   0   0
		2   C   2   1   3.5 0
		
}
"answer": {
	"desc": %s First aggregate : Then grouping by and create heper column by last value by and , then remove last rows of by and use for counts, add all possible unique values of and last append to by : UPDATE The updated part of the question should be possible to achieve by changing the part with : 
	"code-snippets": [
		df = df.pivot_table(
		    index='n',
		    columns='V2',
		    aggfunc=({
		        'V3': 'mean'
		    })
		).V3.reindex(columns=v, index=v, fill_value=0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61800463
"link": https://stackoverflow.com/questions/61800463/read-large-json-file-with-index-format-into-pandas-dataframe
"question": {
	"title": Read large .json file with index format into Pandas dataframe
	"desc": I was following this answer but after some discussion with it's writer, it seems it only gives a solution to data format. This is the difference: I have the index format because my data is from an SQL database read into a dataframe and the index field is needed to specify every records. My json file is 2.5 GB, had been exported from the dataframe with format. This means that the whole file is actually one huge string and not a list like collection of records: This means I can't use any line or chunck based iterative solution like this: According to the documentation, can only be used if the records are in a list like format, this is why does not even accept this argument unless the orient is not . The restriction for comes from this as well, it says: And exactly this is the reason of the question, trying to read such a huge .json file gives back: I was thinking about adding the index values as a first column as well, this case it wouldn't be lost with the records format; or maybe even store an index list separately. Only I fear it would decrease the search performance later on. Is there any solution to handle the situation strictly using the .json file and no other database or big-data based technology? Update #1 For request here is the actual structure of my data. The SQL table: The pandas pivot table is almost the same as in the example, but with a 50,000 rows and 4,000 columns: And this is how it is saved with an index formatted json: Only I could not give the arg, so it is actually cramped into one huge string making it a one-liner json: 
}
"io": {
	"Frame-1": 
		{
		    "0":{"1969-w01":0,"1969-w02":0,"1969-w03":0,"1969-w04":0, ...},
		    "455":{"1969-w01":1,"1969-w02":0,"1969-w03":3,"1969-w04":0, ...},
		    "40036":{"1969-w01":0,"1969-w02":0,"1969-w03":0,"1969-w04":0, ...},
		    ...
		    "217568":{"1969-w01":0,"1969-w02":1,"1969-w03":0,"1969-w04":2, ...}
		}
		
	"Frame-2":
		{"0":{"1969-w01":0,"1969-w02":0,"1969-w03":0,"1969-w04":0, ...},"455":{"1969-w01":1,"1969-w02":0,"1969-w03":3,"1969-w04":0, ...},"40036":{"1969-w01":0,"1969-w02":0,"1969-w03":0,"1969-w04":0, ...}, ... "217568":{"1969-w01":0,"1969-w02":1,"1969-w03":0,"1969-w04":2, ...}}
		
}
"answer": {
	"desc": %s A few solutions, listed from easiest to more involved: 1. SQL If you can perform the query on the DB, maybe the best solution would be to try writing the data in a nicer format? Alternatively, you could try reading directly from the database - Pandas can do that too :) Here is the documentation for pd.read_sql(). 2. Is necessary? To read the JSON file as you gave the example, and create a DataFrame of the form comparable to your pivot-table example (JSON keys as dataframe index), you can try this simple approach: However, this probably doesn't solve the memory issue. 3. Splitting into several files Perhaps the fastest way would be to simply cut the large file into smaller files, which can each be read into a Pandas Dataframe (limiting the working memory required at any given time), then or the resulting dataframes. There is a nice tool in Linux called , which could do it. I notice you are using windows (newer windows version offer a Linux terminal if you enable it!). Otherwise perhaps there is a similar tool, but I don't know one I'm afraid. If you only need to do this once then get on with your life, you might be able to open your file with some text editor like Emacs or VS Code, then copy-paste portions into new files... lame, but might work \_()_/ 4. Streaming readers One package called will iteratively load a JSON file, which allows you to define breaks or do processing to each nested division - you could then for example create the format for Pandas on-the-fly. This solution also promised low memory consumption, being an iterator (a.k.a generator) - you will need to learn how it works though. Have a look here for a nice explanation. Another package called json-streamer can also read partial JSON contents, although it is perhaps going a bit far, given you have a static file. 
	"code-snippets": [
		# read and transpose!
		df = pd.read_json("test.json").T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61678886
"link": https://stackoverflow.com/questions/61678886/filter-series-dataframe-by-another-dataframe
"question": {
	"title": Filter Series/DataFrame by another DataFrame
	"desc": Let's suppose I have a Series (or DataFrame) , for example list of all Universities and Colleges in the USA: And another Series (od DataFrame) , for example list of all cities in the USA: And my desired output (bascially an intersection of and ): The thing is: I'd like to create a Series that consists of cities but only these, that have a university/college. My very first thought was to remove "University" or "College" parts from the , but it turns out that it is not enough, as in case of . Then I thought of leaving only the first word, but that excludes . Finally, I got a series of all the cities and now I'm trying to use it as a filter (something similiar to or ), so if a string (Uni name) contains any of the elements of (city name), then return only the city name. My question is: how to do it in a neat way? 
}
"io": {
	"Frame-1": 
		      City
		0    Searcy
		1    Angwin 
		2   New York 
		3   Ann Arbor 
		
	"Frame-2":
		     Uni City
		0     Searcy
		1     Angwin 
		2    Fairbanks 
		3    Ann Arbor 
		
}
"answer": {
	"desc": %s I would try to build a list comprehension of cities that are contained in at least one university name: With your example data it gives: 
	"code-snippets": [
		pd.Series([i for i in s2 if s1.str.contains(i).any()], name='Uni City')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60720542
"link": https://stackoverflow.com/questions/60720542/get-column-index-nr-from-value-in-other-column
"question": {
	"title": Get column index nr from value in other column
	"desc": I'm relativly new to python and pandas, so I might not have the full understanding of all possibilities and would appreciate a hint how to solve the following problem: I have a like this one: I want to construct a column which takes the column with the index according to and then multiplies the value in the selected column with the value in . I want to bulid a table like this ( beeing the column constructed): (row (), in row () and in row ()) The value in will always be an integer value, so i would love to use the position of a column according to the value in . 
}
"io": {
	"Frame-1": 
		    Jan  Feb  Mar  Apr  i    j
		a   100  200  250  100  1  0.3
		b   120  130   90  100  3  0.7
		c    10   30   10   20  2 0.25
		
	"Frame-2":
		    Jan  Feb  Mar  Apr  i    j    k
		a   100  200  250  100  1  0.3   60
		b   120  130   90  100  3  0.7   70
		c    10   30   10   20  2 0.25  2.5
		
}
"answer": {
	"desc": %s IIUC, and then we can use to . Finally we use Output Alternative: 
	"code-snippets": [
		df['k'] = df['j'].mul(df.rename(columns = dict(zip(df.columns,
		                                                   range(len(df.columns)))))
		                        .lookup(df.index, df['i']))
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61615757
"link": https://stackoverflow.com/questions/61615757/how-to-find-and-replace-values-of-even-positioned-elements-in-sequence
"question": {
	"title": How to find and replace values of even-positioned elements in sequence
	"desc": I have a list as follows: There are sequences of elements whose value's distances are equal. In this list, that distance is , for example, . How can I replace the value of the even-positioned elements for each of these sequences / sublists with a new value, say -1. The output will be: 
}
"io": {
	"Frame-1": 
		list_1 = [12, 15, 18, 21, 6, 9, 7, 21, 38, 62, 65, 68, 81, 21, 25, 96, 101, 8, 11]
		
	"Frame-2":
		list_2 = [12, -1, 18, -1, 6, -1, 7, 21, 38, 62, -1, 68, 81, 21, 25, 96, 101, 8, -1]
		
}
"answer": {
	"desc": %s Use which yields Now, if you want the even positions related to each of those sequences independently, use and which yields The difference will be the last in your example. The first solution assumes the shouldn't be replaced because it's not in an even position in your . The second solutions assumes it should be replace, because it is in an even position in the sublist . 
	"code-snippets": [
		s = pd.Series(list_1)
		s.loc[(s.diff() == 3) & (s.index % 2 == 1)] = -1
		
		----------------------------------------------------------------------
		s.loc[s.groupby(s.diff().ne(3).cumsum()).cumcount() % 2 == 1] = -1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61580297
"link": https://stackoverflow.com/questions/61580297/python-dataframe-data-analysis-of-large-amount-of-data-from-a-text-file
"question": {
	"title": Python DataFrame Data Analysis of Large Amount of Data from a Text File
	"desc": I have the following code: I am using a text file (that is not formatted) to pull chunks of data from. When the text file is opened, it looks something like this, except on a way bigger scale: Here are the things I'm having trouble doing with this data: I only need the second, third, sixth, and seventh columns of data. The issue with this one, I believe I've solved with my code above by reading the individual lines and creating a dataframe with the columns necessary. I am open to suggestions if anyone has a better way of doing this. I need to skip the first row of data. This one, the open feature doesn't have a skiprows attribute, so when I drop the first row, I also lose my index starting at 0. Is there any way around this? I need the resulting dataframe to look like a nice clean dataframe. As of right now, it looks something like this: Everything is right-aligned under the column and it looks strange. Any ideas how to solve this? I also need to be able to perform Statistic Analysis on the columns of data, and to be able to find the Name with the highest data and the lowest data, but for some reason, I always get errors because I think that, even though I've got all the data set up as a dataframe, the values inside the dataframe are reading as objects instead of integers, strings, floats, etc. So, if my data is not analyzable using Python functions, does anyone know how I can fix this to make the data be able to run correctly? Any help would be greatly appreciated. I hope I've laid out all of my needs clearly. I am new to Python, and I'm not sure if I'm using all the proper terminology. 
}
"io": {
	"Frame-1": 
		00 2381    1.3 3.4 1.8 265879 Name 
		34 7879    7.6 4.2 2.1 254789 Name 
		45 65824   2.3 3.4 1.8 265879 Name 
		58 3450    1.3 3.4 1.8 183713 Name 
		69 37495   1.3 3.4 1.8 137632 Name 
		73 458913  1.3 3.4 1.8 138024 Name 
		
	"Frame-2":
		Col1   Col2   Col3 Col4
		2381    3.4 265879 Name 
		7879    4.2 254789 Name 
		65824   3.4 265879 Name 
		3450    3.4 183713 Name 
		37495   3.4 137632 Name 
		458913  3.4 138024 Name 
		
}
"answer": {
	"desc": %s You can use the function to accomplish this very easily. is a text file containing a copy/paste from your source above is using a regex pattern to delimit by one or more consecutive spaces uses a to create your column names skips the first row, per your requirements Example: Output: Sample Analysis: Using you can output a simple, high-level analysis. (Anything further should be the subject of a new question.) 
	"code-snippets": [
		keep = ['col1', 'col3', 'col5', 'col6']
		df = pd.read_csv('txt2pd.txt', 
		                 sep='\s+', 
		                 names=['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6'], 
		                 skiprows=1)
		df = df[keep]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61556827
"link": https://stackoverflow.com/questions/61556827/parsing-a-list-of-lists-to-a-data-frame-in-pandas
"question": {
	"title": Parsing a list of lists to a data frame in pandas
	"desc": I have list of lists. Below is how my list looks like, I want to parse it into a data frame with continuation of values with columns = A,B,C The expected data frame is as below Really appreciate the help. 
}
"io": {
	"Frame-1": 
		[     A  B  C
		  0   1  2  3
		  1   1  2  3
		  2   1  2  3
		  3   1  2  3
		
		      A  B  C
		  0   4  5  6
		  1   4  5  6
		  2   4  5  6
		  3   4  5  6
		]
		
		
	"Frame-2":
		     A  B  C
		  0   1  2  3
		  1   1  2  3
		  2   1  2  3
		  3   1  2  3
		  4   4  5  6
		  5   4  5  6
		  6   4  5  6
		  7   4  5  6
		
		
}
"answer": {
	"desc": %s Try this: : List which contains sub-lists : First it takes sub-lists then append it to : Final list which contains all sub-lists as one separate list : Pandas DataFrame 
	"code-snippets": [
		import pandas as pd
		list1=[]
		count=0
		while count<len(myList):
		    list2= myList[count]
		    list1.append(list2)
		    #print(list2)
		    count+=1
		df = pd.concat(list1)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61545023
"link": https://stackoverflow.com/questions/61545023/selecting-rows-on-pandas-dataframe-based-on-conditions
"question": {
	"title": selecting rows on pandas dataframe based on conditions
	"desc": I have the following code: that creates a dataframe stored in variable 'df'. It consists of 2 columns (column1 and column2) filled with random 0s and 1s. This is the output I got when I ran the program (If you try to run it you won't get exactly the same result because of the randomint generation). I would like to create a filter on column2, showing only the clusters of data when there are three or more 1s in a row. The output would be something like this: I have left a space between the clusters for visual clarity, but the real output would not have the empty spaces in the dataframe. I would like to do it in the following way. Thank you 
}
"io": {
	"Frame-1": 
		    column1  column2
		0         0        1
		1         1        0
		2         0        1
		3         1        1
		4         0        1
		5         1        1
		6         0        1
		7         1        1
		8         1        0
		9         0        1
		10        0        0
		11        1        1
		12        1        1
		13        0        1
		14        0        0
		15        0        1
		16        1        1
		17        1        1
		18        0        1
		19        1        0
		20        0        0
		21        1        0
		22        0        1
		23        1        0
		24        1        1
		25        0        0
		26        1        1
		27        1        0
		28        0        1
		29        1        0
		
	"Frame-2":
		    column1  column2
		2         0        1
		3         1        1
		4         0        1
		5         1        1
		6         0        1
		7         1        1
		
		11        1        1
		12        1        1
		13        0        1
		
		15        0        1
		16        1        1
		17        1        1
		18        0        1
		
}
"answer": {
	"desc": %s We can use . Output If column2 really contains only 1's and 0's in your original DataFrame then we can use instead blocks has a new value every time the value in changes Alternative I often use this code in my projects and I came to the conclusion that it can be generally a little bit faster to use + . The performance difference between the two methods will never be great and you can choose the one you want. But I usually use this last one that I have explained and I think it was worth mentioning it 
	"code-snippets": [
		n = 3
		blocks = df['column2'].ne(df['column2'].shift()).cumsum()
		m1 = (df.groupby(blocks)['column2']
		        .transform('size').ge(n))
		m2 = df['column2'].eq(1)
		df_filtered = df.loc[m1 & m2]
		# Alternative without df['column2'].eq(1)
		#df_filtered = df.loc[m1.mul(df['column2'])]
		print(df_filtered)
		
		----------------------------------------------------------------------
		%%timeit
		m1 = blocks.map(blocks.value_counts().ge(n))
		1.41 ms  122 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
		
		
		%%timeit
		m1 = (df.groupby(blocks)['column2']
		        .transform('size').ge(n))
		2.12 ms  226 s per loop (mean  std. dev. of 7 runs, 100 loops each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61530817
"link": https://stackoverflow.com/questions/61530817/pandas-interpolate-nans-from-zero-to-next-valid-value
"question": {
	"title": Pandas interpolate NaNs from zero to next valid value
	"desc": I am looking for a way to linear interpolate missing values (NaN) from zero to the next valid value. E.g.: Given this table, i want the output to look like this: I've tried using fillna to fill only the next NaN to a valid value to 0 and to then linear interpolate the whole dataframe. The problem I'm facing here is that specifying a value and a limit with fillna won't affect consecutive NaNs, but limit the total amount of columns to be filled. If possible please only suggest solutions without iterating over each row manually since I'm working with large dataframes. Thanks in advance. 
}
"io": {
	"Frame-1": 
		     A    B   C   D  E
		0  NaN  2.0 NaN NaN  0
		1  3.0  4.0 NaN NaN  1
		2  NaN  NaN NaN NaN  5
		3  NaN  3.0 NaN NaN  4
		
	"Frame-2":
		     A    B   C   D  E
		0  NaN  2.0   0   0  0
		1  3.0  4.0   0 0.5  1
		2  NaN  NaN NaN NaN  5
		3  NaN  3.0   0   2  4
		
}
"answer": {
	"desc": %s Here's a method that will work to replace 0 for the first after a valid number and then will interpolate row-wise. I added extra rows in the end to illustrate the behavior for multiple fillings on the same row, fillings of only one value, or rows that end in NaN streaks. Sample Data Code How it works: 
	"code-snippets": [
		m = (df.notnull().cummax(axis=1) & df.isnull()).astype(int).diff(axis=1).fillna(0)
		update = m.where(m.eq(1) & m.loc[:, ::-1].cummin(axis=1).eq(-1)).replace(1, 0)
		
		df.update(update)  # Add in 0s
		
		df = df.interpolate(axis=1, limit_area='inside')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61351795
"link": https://stackoverflow.com/questions/61351795/how-to-delete-the-first-and-last-rows-with-nan-of-a-dataframe-and-replace-the-re
"question": {
	"title": How to delete the first and last rows with NaN of a dataframe and replace the remaining NaN with the average of the values below and above?
	"desc": Let's take this dataframe as a simple example: I would like first to remove first and last rows until there is no longer NaN in the first and last row. Intermediate expected output : Then, I would like to replace the remaining NaN by the mean of the nearest value below which is not a NaN, and the one above. Final expected output : I know I can have the positions of NaN in my dataframe through But I can't solve my problem. How please could I do ? 
}
"io": {
	"Frame-1": 
		   Col1  Col2  Col3
		1   1.0   1.0   1.0
		2   1.0   NaN   NaN
		3   2.0   NaN   5.0
		4   3.0   3.0   1.0
		
	"Frame-2":
		   Col1  Col2  Col3
		0   1.0   1.0   1.0
		1   1.0   2.0   3.0
		2   2.0   2.0   5.0
		3   3.0   3.0   1.0
		
}
"answer": {
	"desc": %s My approach: Output: 
	"code-snippets": [
		# identify the rows with some NaN
		s = df.notnull().all(1)
		
		# remove those with NaN at beginning and at the end:
		new_df = df.loc[s.idxmax():s[::-1].idxmax()]
		
		# average:
		new_df = (new_df.ffill()+ new_df.bfill())/2
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61348221
"link": https://stackoverflow.com/questions/61348221/how-to-replace-values-among-blocks-of-consecutive-values
"question": {
	"title": How to replace values among blocks of consecutive values
	"desc": I have a list like this: So in this list there are blocks of consecutive values, separated by . How can I replace the values before the maximum of each block, for example with -1. The result looks like: 
}
"io": {
	"Frame-1": 
		list_tmp = [np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN]
		
	"Frame-2":
		list_tmp = [np.NaN, np.NaN, -1, -1, 3, np.NaN, -1, 2, np.NaN, np.NaN, -1, -1, -1, 4, np.NaN]
		
}
"answer": {
	"desc": %s Since the maximum value is just the last non-NaN value, you can obtain the indices of the values to set to by checking if a given value is not a and neither is the following: 
	"code-snippets": [
		a = np.array([np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN])
		
		a[~np.isnan(a) & ~np.isnan(np.r_[a[1:],np.nan])] = -1
		
		----------------------------------------------------------------------
		print(a)
		array([nan, nan, -1., -1.,  3., nan, -1.,  2., nan, nan, -1., -1., -1., 4., nan])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61293428
"link": https://stackoverflow.com/questions/61293428/match-on-multiple-columns-using-array
"question": {
	"title": Match on multiple columns using array
	"desc": I'm working on a project where my original dataframe is: But, I have an array with new labels for certain points (for that I only used columns A and B) in the original dataframe. Something like this: My goal is to add the new labels to the original dataframe. I know that the combination of A and B unique is. What is the fastest way to assign the new label to the correct row? This is my try: Wanted output (rows with index 1 and 2 are changed): For small datasets may this be okay with I'm currently using it for datasets with more than 25000 labels. Is there a way that is faster? Also, in some cases I used all columns expect the column 'label'. That dataframe exists out of 64 columns so my method can not be used here. Has someone an idea to improve this? Thanks in advance 
}
"io": {
	"Frame-1": 
		      A     B    C   label
		0     1     2    2    Nan
		1     2     4    5    7
		2     3     6    5    Nan
		3     4     8    7    Nan
		4     5    10    3    8
		5     6    12    4    8
		
	"Frame-2":
		       A     B    C   label
		0     1     2    2    Nan
		1     2     4    5    5
		2     3     6    5    9
		3     4     8    7    Nan
		4     5    10    3    8
		5     6    12    4    8
		
}
"answer": {
	"desc": %s Here's a numpy based approach aimed at performance. To vectorize this we want a way to check membership of the rows in in columns and . So what we can do, is view these two columns as 1D arrays (based on this answer) and then we can use to index the dataframe and assign the values in : 
	"code-snippets": [
		import numpy as np
		
		X_labeled = [[2, 4], [3,6]]
		y_labeled = [5,9]
		
		a = df.values[:,:2].astype(int) #indexing on A and B
		
		def view_as_1d(a):
		    a = np.ascontiguousarray(a)
		    return a.view(np.dtype((np.void, a.dtype.itemsize * a.shape[-1])))
		
		ix = np.in1d(view_as_1d(a), view_as_1d(X_labeled))
		df.loc[ix, 'label'] = y_labeled
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61292759
"link": https://stackoverflow.com/questions/61292759/how-to-fill-elements-between-intervals-of-a-list
"question": {
	"title": How to fill elements between intervals of a list
	"desc": I have a list like this: So there are intervals that begin with and end with . How can I replace the values in those intervals, say with 1? The outcome will look like this: I use in this example, but a generalized solution that can apply to any value will also be great 
}
"io": {
	"Frame-1": 
		list_1 = [np.NaN, np.NaN, 1, np.NaN, np.NaN, np.NaN, 0, np.NaN, 1, np.NaN, 0, 1, np.NaN, 0, np.NaN,  1, np.NaN]
		
	"Frame-2":
		list_2 = [np.NaN, np.NaN, 1, 1, 1, 1, 0, np.NaN, 1, 1, 0, 1, 1, 0, np.NaN, 1, np.NaN]
		
}
"answer": {
	"desc": %s Pandas solution: but if there is only , or you can do: output 
	"code-snippets": [
		s = pd.Series(list_1)
		s.fillna(s.ffill().where(lambda x: x.eq(1))).tolist()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61233112
"link": https://stackoverflow.com/questions/61233112/how-to-replace-pandas-dataframe-values-based-on-lookup-values-in-another-datafra
"question": {
	"title": How to replace pandas dataframe values based on lookup values in another dataframe?
	"desc": I have a large pandas dataframe with numerical values structured like this: I need to replace all of the the above cell values with a 'description' that maps to the field name and cell value as referenced in another dataframe structured like this: The desired output would be like: I could figure out a way to do this on a small scale using something like .map or .replace - however the actual datasets contain thousands of records with hundreds of different combinations to replace. Any help would be really appreciated. Thanks. 
}
"io": {
	"Frame-1": 
		>>> df1
		   A  B  C
		0  2  1  2
		1  1  2  3
		2  2  3  1
		
	"Frame-2":
		>>> df3
		     A  B     C
		0  YES  x   BAD
		1   NO  y  FINE
		2  YES  z  GOOD
		
}
"answer": {
	"desc": %s Use with : maybe you need select columns previously: Output 
	"code-snippets": [
		df1 = df1.replace(df2.pivot(columns='field_name', index='code', values='description')
		                     .to_dict())
		
		----------------------------------------------------------------------
		df1[cols] = df1[cols].replace(df2.pivot(columns='field_name',
		                                        index='code', values='description')
		                                 .to_dict())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61195022
"link": https://stackoverflow.com/questions/61195022/pivot-table-in-pandas-with-two-columnindex-and-value
"question": {
	"title": Pivot Table in Pandas with two column(Index and Value)
	"desc": I have a CSV file with and column. I need to sum values for each and have output like below Input: Output: I have tried below code,As I have just two column to apply I added column with unique value to have pivot(pivot table need Index,Column and Value).Then column is just to help. However out put is sum thing weird!!! output of my code: 
}
"io": {
	"Frame-1": 
		+-----+------+
		| obj | VS |
		+-----+------+
		| B  | 2048 |
		| A  | 1024 |
		| B  |  10 |
		| A  | 1024 |
		| B  | 1025 |
		| A  | 1026 |
		| B  | 1027 |
		+-----+------+
		
	"Frame-2":
		+---+------+
		| A | 3074 |
		+---+------+
		| B | 4110 |
		+---+------+
		
}
"answer": {
	"desc": %s Just consider that as you read from CSV, values are string, you need to convert them to int by show the type of column in df 
	"code-snippets": [
		import pandas as pd 
		import numpy as np 
		
		filename='1test.csv'
		df = pd.read_csv(filename, dtype='str')
		df["value"]=1
		print(df.dtypes)
		df['VS']=pd.to_numeric(df['VS'])
		print(df.dtypes)
		pd.pivot_table(df, values="VS", index="obj", columns="value", aggfunc=np.sum)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61170036
"link": https://stackoverflow.com/questions/61170036/error-none-of-index-dtype-object-are-in-the-index
"question": {
	"title": Error: None of [Index([&#39;...&#39;], dtype=&#39;object&#39;)] are in the [index]
	"desc": I am trying to delete a grouped set of rows in pandas according to the following condition: If a group (grouped by col1) has more than 2 values 'c' in col2, then remove the whole group. What I have looks like this And I am trying to get here: Typically I do this for other very similar dataframes (and it works): But for this one is not working and I receive this error: Does someone have an idea on how I could do this? Thanks! 
}
"io": {
	"Frame-1": 
		  col1  col2                       
		0  A     10:10 
		1  A     20:05
		2  A     c
		3  A     00:10
		4  B     04:15
		2  B     c
		3  B     c
		4  B     13:40
		
	"Frame-2":
		  col1  col2                       
		0  A     10:10 
		1  A     20:05
		2  A     c
		3  A     00:10
		
}
"answer": {
	"desc": %s I suggest use for improve performance : Details: First compare values by for : Then count value per groups by with , s are processing like : And last filter by for less: 
	"code-snippets": [
		print (df['col2'].eq('c'))
		0    False
		1    False
		2     True
		3    False
		4    False
		2     True
		3     True
		4    False
		Name: col2, dtype: bool
		
		----------------------------------------------------------------------
		print (df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2))
		0     True
		1     True
		2     True
		3     True
		4    False
		2    False
		3    False
		4    False
		Name: col2, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61143729
"link": https://stackoverflow.com/questions/61143729/pandas-how-to-do-value-counts-within-groups
"question": {
	"title": Pandas: how to do value counts within groups
	"desc": I have the following dataframe. I want to group by and first. Within each group, I need to do a value count based on and only pick the one with most counts. If there are more than one c values for one group with the most counts, just pick any one. The expected result would be What is the right way to do it? It would be even better if I can print out each group with c's value counts sorted as an intermediate step. 
}
"io": {
	"Frame-1": 
		a    b    c
		1    1    x
		1    1    y
		1    1    y
		1    2    y
		1    2    y
		1    2    z
		2    1    z
		2    1    z
		2    1    a
		2    1    a
		
	"Frame-2":
		a    b    c
		1    1    y
		1    2    y
		2    1    z
		
}
"answer": {
	"desc": %s group the original dataframe by and get the should work you can also aggregate and values 
	"code-snippets": [
		df.groupby(['a', 'b'])['c'].agg({'max': max, 'count': 'count'}).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61138851
"link": https://stackoverflow.com/questions/61138851/count-how-many-cells-are-between-the-last-value-in-the-dataframe-and-the-end-of
"question": {
	"title": Count how many cells are between the last value in the dataframe and the end of the row
	"desc": I'm using the pandas library in Python. I have a data frame: Is it possible to create a new column that is a count of the number of cells that are empty between the end of the row and the last value above zero? Example data frame below: 
}
"io": {
	"Frame-1": 
		    0   1   2   3   4
		
		0   0   0   0   1   0
		
		1   0   0   0   0   1
		
		2   0   0   1   0   0
		
		3   1   0   0   0   0
		
		4   0   0   1   0   0
		
		5   0   1   0   0   0
		
		6   1   0   0   1   1
		
	"Frame-2":
		    0   1   2   3   4   Value
		
		0   0   0   0   1   0   1
		
		1   0   0   0   0   1   0
		
		2   0   0   1   0   0   2
		
		3   1   0   0   0   0   4
		
		4   0   0   1   0   0   2
		
		5   0   1   0   0   0   3
		
		6   1   0   0   1   1   0
		
}
"answer": {
	"desc": %s using using 
	"code-snippets": [
		df['value'] = df.apply(lambda x: (x.iloc[::-1] == 1).argmax(),1)
		
		##OR
		
		----------------------------------------------------------------------
		df['Value'] = np.where(df.iloc[:,::-1] == 1,True,False).argmax(1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61117957
"link": https://stackoverflow.com/questions/61117957/how-to-split-a-nested-dictionary-inside-a-column-of-a-dataframe-into-new-rows
"question": {
	"title": how to split a nested dictionary inside a column of a dataframe into new rows?
	"desc": I have a dataframe : I need to split col3 into new rows: expected output dataframe : This doesnt seem to work : 
}
"io": {
	"Frame-1": 
		Col1 Col2 Col3
		01   ABC  {'link':'http://smthing1}
		02   DEF  {'link':'http://smthing2}
		
	"Frame-2":
		Col1 Col2 Col3
		01   ABC  'http://smthing1'
		02   DEF  'http://smthing2'
		
}
"answer": {
	"desc": %s Use , but first convert to dictionaries if necessary: 
	"code-snippets": [
		#converting to dicts
		#import ast
		#df['Col3'] = df['Col3'].apply(ast.literal_eval)
		df['Col3'] = df['Col3'].str.get('link')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61077572
"link": https://stackoverflow.com/questions/61077572/find-count-of-unique-value-of-each-column-and-save-in-csv
"question": {
	"title": Find count of unique value of each column and save in CSV
	"desc": I have data like this: Need to count unique value of each column and report it like below: I have no issue when number of column are limit and manually name them, when input file is big it become hard,need to have simple way to have output here is the code I have 
}
"io": {
	"Frame-1": 
		+---+---+---+
		| A | B | C |
		+---+---+---+
		| 1 | 2 | 7 |
		| 2 | 2 | 7 |
		| 3 | 2 | 1 |
		| 3 | 2 | 1 |
		| 3 | 2 | 1 |
		+---+---+---+
		
	"Frame-2":
		+---+---+---+
		| A | 3 | 3 |
		| A | 2 | 1 |
		| A | 1 | 1 |
		| B | 2 | 5 |
		| C | 1 | 3 |
		| C | 7 | 2 |
		+---+---+---+
		
}
"answer": {
	"desc": %s You can loop over column and insert them in dictionary. you can initiate dictionary by . To be scalable you can read column by . This would give you all column in your df. Try this code: 
	"code-snippets": [
		import pandas as pd 
		df=pd.read_csv('1.csv')
		all={}
		colm=df.columns
		for i in colm:
		    all.update({i:df[i].value_counts()})
		
		result = pd.concat(all)
		result.to_csv('out.csv')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61075047
"link": https://stackoverflow.com/questions/61075047/pandas-data-change-based-on-condition
"question": {
	"title": pandas data change based on condition
	"desc": I have data which has special characters, I want to change the conditional cell values. Data is below first few lines df_orig: I want to change cell values where $ in D, A = 0 and B = C THE OUTPUT SHOULD BE change: I tried at my end with but it didn't work. 
}
"io": {
	"Frame-1": 
		idx A   B   C   D
		0   0.5 2   5   #
		1   3   5   8   %
		2   6   8   10  $
		3   9   10  15  $
		4   11  15  18  #
		
	"Frame-2":
		idx   A   B   C   D
		0     0.5 2   5   #
		1     3   5   8   %
		2     0   10  10  $
		3     0   15  15  $
		4     11  15  18  #
		
}
"answer": {
	"desc": %s Use if need new and then set new values separately: Also is possible together: 
	"code-snippets": [
		m = df['D'].eq('$')
		df.loc[m, ['A','B']] = df.assign(E=0).loc[m, ['E','C']].values
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61024520
"link": https://stackoverflow.com/questions/61024520/pandas-assignment-and-copy
"question": {
	"title": Pandas assignment and copy
	"desc": If we run the following code, We get Whereas, if we run the following, We get I know there's a concept of pass by object reference in python. Why don't the in the second code gets copied? Thanks 
}
"io": {
	"Frame-1": 
		           0
		0   1.298967
		1  -0.887922
		2   1.913559
		3  -0.082032
		4  -0.466594
		..       ...
		95 -0.845137
		96  0.628542
		97 -0.588897
		98  0.464374
		99  0.267946
		
	"Frame-2":
		           0  a
		0  -0.510875  1
		1   0.401580  1
		2  -0.037484  1
		3  -0.935115  1
		4  -1.108471  1
		..       ... ..
		95  0.362075  1
		96 -1.017991  1
		97  1.881081  1
		98  0.376828  1
		99  0.771661  1
		
}
"answer": {
	"desc": %s For the first function: You are changing the input to a version of the input , as returns a copy of the actual dataframe , from the docs: Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. To return the changes you make in the copy , you should return the copy: On the contrary , for your second function , you are assigning the column a on the input parameter in place, hence when you print the dataframe , you can see the changes in the original df. To achieve a similar behaviour to the first function try assigning Same as we did for function 1 , you should return the copy : Hope this answers your question. 
	"code-snippets": [
		def f(df):
		    df = df.assign(b = 1)
		    df["a"] = 1
		
		df = pd.DataFrame(np.random.randn(100, 1))
		f(df)
		print(df) #doesnot return the changed columns
		
		----------------------------------------------------------------------
		def f(df):
		    df = df.assign(b = 1)
		    df["a"] = 1
		    return df
		
		df = pd.DataFrame(np.random.randn(100, 1))
		print(f(df))
		
		----------------------------------------------------------------------
		def f(df):
		    df = df
		    df["a"] = 1
		
		df = pd.DataFrame(np.random.randn(100, 1))
		f(df)
		print(df)
		
		----------------------------------------------------------------------
		def f(df):
		    df = df.copy()
		    df["a"] = 1
		
		df = pd.DataFrame(np.random.randn(100, 1))
		f(df)
		print(df) # doesnot return the a column
		
		----------------------------------------------------------------------
		def f(df):
		    df = df.copy()
		    df["a"] = 1
		    return df
		df = pd.DataFrame(np.random.randn(100, 1))
		print(f(df)) #returns the column a
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61016701
"link": https://stackoverflow.com/questions/61016701/issue-in-applying-str-contains-across-multiple-columns-in-python
"question": {
	"title": Issue in applying str.contains across multiple columns in Python
	"desc": Dataframe: Desired output: What I have tried: (this doesn't delete all rows with alpha-numeric vales) I want to delete all alphanumeric rows, and have only the rows containing numbers alone. Col1 and Col2 has decimal points, but Col3 has only whole numbers. I have tried few other similar threads, but it didn't work. Thanks for the help!! 
}
"io": {
	"Frame-1": 
		col1          col2             col3
		132jh.2ad3    34.2             65
		298.487       9879.87          1kjh8kjn0
		98.47         79.8             90
		8763.3        7hkj7kjb.k23l    67
		69.3          3765.9           3510
		
	"Frame-2":
		col1          col2             col3
		98.47         79.8             90
		69.3          3765.9           3510
		
}
"answer": {
	"desc": %s Run: (import re required). Your regex contained [A-B], but it should match all letters (from A to Z). Edit If you have also other columns, but you want to limit your criterion to just your 3 indicated columns, assuming that they are consecutive columns, run: This way you apply the same function as above to just these 3 columns. 
	"code-snippets": [
		df[~df.apply(lambda row: row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]
		
		----------------------------------------------------------------------
		df[~df.loc[:, 'col1':'col3'].apply(lambda row:
		    row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60862810
"link": https://stackoverflow.com/questions/60862810/how-to-break-pop-a-nested-dictionary-inside-a-list-inside-a-pandas-dataframe
"question": {
	"title": How to break/pop a nested Dictionary inside a list, inside a pandas dataframe?
	"desc": I have a dataframe which has a dictionary inside a nested list and i want to split the column 'C' : expected output : 
}
"io": {
	"Frame-1": 
		A B     C     
		1 a    [ {"id":2,"Col":{"x":3,"y":4}}]
		2 b    [ {"id":5,"Col":{"x":6,"y":7}}]
		
	"Frame-2":
		A B C_id Col_x Col_y
		1 a  2    3     4 
		2 b  5    6     7
		
}
"answer": {
	"desc": %s From the comments, might help you. After extracting and columns with: You can explode the dictionary in with and use concat to merge with existing dataframe: Also, use drop to remove old columns. Full code: 
	"code-snippets": [
		df[["Col", "id"]] = df["C"].apply(lambda x: pd.Series(x[0]))
		
		----------------------------------------------------------------------
		df = pd.concat([df, json_normalize(df.Col)], axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61011933
"link": https://stackoverflow.com/questions/61011933/how-to-freeze-first-numbers-in-sequences-between-nans-in-python-pandas-dataframe
"question": {
	"title": How to freeze first numbers in sequences between NaNs in Python pandas dataframe
	"desc": Is there a Pythonic way to, in a timeseries dataframe, by column, go down and pick the first number in a sequence, and then push it forward until the next NaN, and then take the next non-NaN number and push that one down until the next NaN, and so on (retaining the indices and NaNs). For example, I would like to convert this dataframe: To this dataframe: I know I can use a loop to iterate down the columns to do this, but would appreciate some help on how to do it in a more efficient Pythonic way on a very large dataframe. Thank you. 
}
"io": {
	"Frame-1": 
		     A    B    C
		0  NaN  8.0  NaN
		1  1.0  6.0  NaN
		2  3.0  4.0  4.0
		3  5.0  NaN  2.0
		4  7.0  NaN  6.0
		5  NaN  9.0  NaN
		6  2.0  7.0  1.0
		7  4.0  3.0  5.0
		8  6.0  NaN  2.0
		9  NaN  3.0  8.0
		
	"Frame-2":
		     A    B    C
		0  NaN  8.0  NaN
		1  1.0  8.0  NaN
		2  1.0  8.0  4.0
		3  1.0  NaN  4.0
		4  1.0  NaN  4.0
		5  NaN  9.0  NaN
		6  2.0  9.0  1.0
		7  2.0  9.0  1.0
		8  2.0  NaN  1.0
		9  NaN  3.0  1.0
		
}
"answer": {
	"desc": %s IIUC: Output: 
	"code-snippets": [
		# where DF is not NaN
		mask = DF.notna()
		Result = (DF.shift(-1)           # fill the original NaN's with their next value
		            .mask(mask)          # replace all the original non-NaN with NaN
		            .ffill()             # forward fill 
		            .fillna(DF.iloc[0])  # starting of the the columns with a non-NaN
		            .where(mask)         # replace the original NaN's back
		         )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60993781
"link": https://stackoverflow.com/questions/60993781/how-do-i-check-that-the-unique-values-of-a-column-exists-in-another-column-in-da
"question": {
	"title": How do i check that the unique values of a column exists in another column in dataframe?
	"desc": I have a dataframe like this : What i want to is to check that the unique value of col1 exist in Col2 for the same ID ,The ID is not always the same number. the check must be done only among rows with the same id i want a result like : i tried I'm not sur it s the right command ,can anyone help please ? 
}
"io": {
	"Frame-1": 
		
		A= [ ID COL1 COL2  
		     23  AA   BB    
		     23  AA   AA   
		     23  AA   DD   
		     23  BB   BB 
		     23  BB   AA
		     23  BB   DD
		     23  CC   BB
		     23  CC   AA
		     24  AA   BB  ]
		
		
		
	"Frame-2":
		
		A= [ ID COL1 COL2  check 
		     23  AA   BB    OK 
		     23  AA   AA    OK 
		     23  AA   DD    OK
		     23  BB   BB    OK 
		     23  BB   AA    OK 
		     23  BB   DD    OK 
		     23  CC   BB    KO
		     23  CC   AA    KO 
		     24  AA   BB    KO 
		]
		
}
"answer": {
	"desc": %s You just want to check whether a cell value exists in a container: is the way to go. But as you want to process id by ID, you also need a groupby: It gives as expected: If you want OK/KO strings instead of boolean values, just add: 
	"code-snippets": [
		df['check'] = df.groupby(['ID', 'COL1'], group_keys=False
		                         ).apply(lambda x: x['COL1'].isin(x['COL2']))
		
		----------------------------------------------------------------------
		df['check'] = np.where(df['check'], 'OK', 'KO')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60994122
"link": https://stackoverflow.com/questions/60994122/choosing-rows-by-values-in-dataframe
"question": {
	"title": choosing rows by values in DataFrame
	"desc": A post gives a way to choose rows by column value Here is a DataFrame with this code , I got when I run this , I got error this code gives This is close, what I am trying to get is a new DataFrame consists of rows at [2,4,6,8,9]. How to do that? Thanks to anyone who gives some inspiration. 
}
"io": {
	"Frame-1": 
		            0           1
		0  877.443401  808.520962
		1  826.300620  848.761594
		2  824.403359  861.395174
		3  866.732033  804.494156
		4  853.461260  874.307851
		5  822.906499  830.102249
		6  852.605652  863.602725
		7  893.421600  825.032893
		8  863.768363  862.298227
		9  899.976622  864.111539
		
	"Frame-2":
		    0   1
		0   NaN NaN
		1   NaN NaN
		2   NaN 861.395174
		3   NaN NaN
		4   NaN 874.307851
		5   NaN NaN
		6   NaN 863.602725
		7   NaN NaN
		8   NaN 862.298227
		9   NaN 864.111539
		
}
"answer": {
	"desc": %s  returns a while returns a with only column being 'a'. For your problem: Using Using It's customary using column names instead of . For example, the method would not work with column names and there are a plethora of weird behaviours you can face with column names. 
	"code-snippets": [
		new_df = df.loc[df[1] > 850].copy()
		
		----------------------------------------------------------------------
		new_df = df.query('a > 850')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60989914
"link": https://stackoverflow.com/questions/60989914/add-id-found-in-list-to-new-column-in-pandas-dataframe
"question": {
	"title": Add ID found in list to new column in pandas dataframe
	"desc": Say I have the following dataframe (a column of integers and a column with a list of integers)... And also a separate list of IDs... Given that, and ignoring the column and any index, I want to see if any of the IDs in the list are mentioned in the column. The code I have so far is: This works but only if the list is longer than the dataframe and for the real dataset the list is going to be a lot shorter than the dataframe. If I set the list to only two elements... I get a very popular error (I have read many questions with the same error)... I have tried converting the list to a series (no change in the error). I have also tried adding the new column and setting all values to before doing the comprehension line (again no change in the error). Two questions: How do I get my code (below) to work for a list that is shorter than a dataframe? How would I get the code to write the actual ID found back to the column (more useful than True/False)? Expected output for : Ideal output for (ID(s) are written to a new column or columns): Code: 
}
"io": {
	"Frame-1": 
		      ID                   Found_IDs
		0  12345        [15443, 15533, 3433]
		1  15533  [2234, 16608, 12002, 7654]
		2   6789      [43322, 876544, 36789]
		
	"Frame-2":
		bad_ids = [15533, 876544, 36789, 11111]
		
}
"answer": {
	"desc": %s Using to get the intersect of the two lists: Or with just vanilla python using intersect of : 
	"code-snippets": [
		bad_ids_set = set(bad_ids)
		df['Found_IDs'].apply(lambda x: list(set(x) & bad_ids_set))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60968943
"link": https://stackoverflow.com/questions/60968943/pandas-reshaping-dataframe
"question": {
	"title": Pandas: Reshaping dataframe
	"desc": I have a panda's related question. My dataframe looks something like this: I want to transform it into something like: I thought of something like adding a sub_id column that is enumerated cyclically by a, b and c and then do an unstack of the frame. Is there an easier/smarter solution? Thanks a lot! Tim 
}
"io": {
	"Frame-1": 
		  id val1 val2
		0  1     0    1
		1  1     1    0
		2  1     0    0
		3  2     1    1
		4  2     1    1
		5  2     1    0
		6  3     0    0
		7  3     0    1
		8  3     1    1
		9  4     1    0
		10 4     0    1
		11 4     0    0
		
	"Frame-2":
		             a         b        c
		   id     a0   a1   b0   b1   c0   c1
		    1     0    1    1    0    0    0
		    2     1    1    1    1    1    0
		    3     0    0    1    1    1    1
		    4     1    0    0    1    0    0
		
}
"answer": {
	"desc": %s One of possible solutions: Start from reformatting values for each id into a single row: For now the result is: Then set proper column names: The finale result is: 
	"code-snippets": [
		res = df.set_index('id').groupby('id').apply(
		    lambda grp: pd.Series(grp.values.flatten()))
		
		----------------------------------------------------------------------
		res.columns = pd.MultiIndex.from_tuples(
		    [(x, x + y) for x in list('abc') for y in list('01')])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60959528
"link": https://stackoverflow.com/questions/60959528/pandas-select-only-the-first-3-yyyymm-per-group
"question": {
	"title": Pandas select only the first 3 YYYYMM per group
	"desc": CGood afternoon, I have a datrame like the one below I need to get only the first three *CONSECUTIVE MMMMYY per USR. Im able to get the first 3 records using head(3) but of course it dont bring back what I need, neither using it gets the consecutive when ['check'] is true but in some cases I may need to get 200001 and 200003 only and they are not consecutive between them. Any guidance will be appreciated Thanks 
}
"io": {
	"Frame-1": 
		+---+---+--------+
		|   |USR| MMMMYY |
		+---+---+--------+
		| 1 | A | 200002 |
		+---+---+--------+
		| 2 | A | 200003 |
		+---+---+--------+
		| 3 | A | 200004 |
		+---+---+--------+
		| 4 | A | 200005 |
		+---+---+--------+
		| 5 | B | 200001 |
		+---+---+--------+
		| 6 | B | 200003 |
		+---+---+--------+
		| 7 | B | 200008 |
		+---+---+--------+
		| 8 | B | 200009 |
		+---+---+--------+
		
	"Frame-2":
		+---+---+--------+
		|   |USR| MMMMYY |
		+---+---+--------+
		| 1 | A | 200002 |
		+---+---+--------+
		| 2 | A | 200003 |
		+---+---+--------+
		| 3 | A | 200004 |
		+---+---+--------+
		| 5 | B | 200001 |
		+---+---+--------+
		| 6 | B | 200003 |
		+---+---+--------+
		
}
"answer": {
	"desc": %s Your is datetime, then turn it to type first: Output: 
	"code-snippets": [
		df['MMMMYY'] = pd.to_datetime(df.MMMMYY, format='%Y%m')
		
		s = df.groupby('USR')['MMMMYY'].transform('min') + pd.offsets.MonthOffset(3)
		
		df[df.MMMMYY<s]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60843541
"link": https://stackoverflow.com/questions/60843541/fastest-way-to-calculate-in-pandas
"question": {
	"title": Fastest way to calculate in Pandas?
	"desc": Given these two dataframes: has no column names, but you can assume column 0 is an offset of and column 1 is an offset of . I would like to transpose onto to get the Start and End differences. The final dataframe should look like this: I have a solution that works, but I'm not satisfied with it because it takes too long to run when processing a dataframe that has millions of rows. Below is a sample test case to simulate processing 30,000 rows. As you can imagine, running the original solution (method_1) on a 1GB dataframe is going to be a problem. Is there a faster way to do this using Pandas, Numpy, or maybe another package? UPDATE: I've added the provided solutions to the benchmarks. Output: 
}
"io": {
	"Frame-1": 
		df1 =
		     Name  Start  End
		  0  A     10     20
		  1  B     20     30
		  2  C     30     40
		
		df2 =
		     0   1
		  0  5   10
		  1  15  20
		  2  25  30
		
	"Frame-2":
		  Name  Start  End  Start_Diff_0  End_Diff_0  Start_Diff_1  End_Diff_1  Start_Diff_2  End_Diff_2
		0    A     10   20             5          10            -5           0           -15         -10
		1    B     20   30            15          20             5          10            -5           0
		2    C     30   40            25          30            15          20             5          10
		
}
"answer": {
	"desc": %s I suggest use here numpy - convert selected columns to array in first step:: Output is 3d array, convert it to : Last generate columns names and with add to original: 
	"code-snippets": [
		a = df1[['Start','End']].to_numpy()
		b = df2[[0,1]].to_numpy()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60897173
"link": https://stackoverflow.com/questions/60897173/sort-pandas-dataframe-by-index
"question": {
	"title": Sort pandas dataframe by index
	"desc": I have the following pandas dataframe: I would like to order it from highest to lowest based on the index no matter if it is repeated, what I say would look like this: The maximum value corresponds to Cmpd6 = 0.79, followed by Cmpd4 = 0.69 ... at some point Cmpd6 = 0.56 which I would like to leave the list like this: This with each value of the array, no matter how many times the indexes are repeated, I was trying with but it does not produce any of this, I also tried but it does not give me the indexes. How can i fix this? Thanks! My solution: 
}
"io": {
	"Frame-1": 
		    Cmpd1   Cmpd2   Cmpd3   Cmpd4   Cmpd5   Cmpd6
		Cmpd1   1                   
		Cmpd2   0.4   1             
		Cmpd3   0.6   0.3   1           
		Cmpd4   0.46  0.69  0.32    1       
		Cmpd5   0.57  0.44  0.41    0.51    1   
		Cmpd6   0.41  0.79  0.33    0.56    0.43    1
		
	"Frame-2":
		Cmpd6 = 0.79
		Cmpd4 = 0.69
		Cmpdx = x
		Cmpd6 = 0.56
		
}
"answer": {
	"desc": %s I assume that each empty element in the source df contain actualy an empty string (not NaN, as these would be printed just as NaN). I also noticed that you want to keey only values < 1. To get your result, run: For your data the result is: Other possible solution: 
	"code-snippets": [
		s = df.stack()
		s[s.apply(lambda x: type(x) is not str and x < 1)]\
		    .reset_index(level=1, drop=True).sort_values(ascending=False)\
		    .astype(float)
		
		----------------------------------------------------------------------
		s = df.replace(r'^\s*$', np.nan, regex=True).stack()\
		    .reset_index(level=1, drop=True).sort_values(ascending=False)
		s[s < 1]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60875750
"link": https://stackoverflow.com/questions/60875750/pandas-groupby-and-change-reassign-one-element
"question": {
	"title": Pandas groupby and change/reassign one element
	"desc": I want to given dataframe, and then, for each group, for given column overwrite the value of its last element (of each group) to (with being the sum of all the elements apart from the last one). Note that after performing the operation, the sum of all values in for each group is equal to . For example, for this input dataframe (grouping by and ): the expected output would be: I managed to perform the operation using loop: but I am looking for more elegant way of doing this, without explicitly looping through each group. I tried this: But I don't really know how to assemble those outputs given that their indices differ. 
}
"io": {
	"Frame-1": 
		  c1 c2    p
		0  x  a  0.4
		1  y  a  0.2
		2  x  a  0.3
		3  y  b  0.6
		
	"Frame-2":
		  c1 c2    p
		0  x  a  0.4
		1  y  a  1.0
		2  x  a  0.6
		3  y  b  1.0
		
}
"answer": {
	"desc": %s Here's a possible one-line solution: To make the above code more readable, here is a nearly equivalent version of my one-line solution: With those solutions in mind, I am not entirely sure why you don't want to use the call in an explicit python for-loop. My hunch is that an explicit python for-loop would be more than adequate, but I don't know your specific use case/data. I would highly recommend doing some speed comparisons using with your specific data, as I think that will help shed light on which approach you ultimately end up using. 
	"code-snippets": [
		df.groupby(['c1', 'c2']).apply(
		        lambda x: x.assign(p=x['p'][:-1].tolist()+[1 - x.iloc[:-1].sum()['p']])
		).reset_index(level=[0,1], drop=True)
		
		----------------------------------------------------------------------
		def func(row):
		    result = 1 - row.iloc[:-1].sum()['p']
		    row['p'].iloc[-1] = result
		    return row
		
		df.groupby(['c1', 'c2']).apply(func)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60825821
"link": https://stackoverflow.com/questions/60825821/merge-columns-with-have-n
"question": {
	"title": Merge columns with have \n
	"desc": ex) I'm merging columns, but I want to give '\n\n' in the merging process. so output what I want I want 'nan' to drop. I tried However, this includes all nan values. thank you for reading. 
}
"io": {
	"Frame-1": 
		   C1  C2  C3  C4  C5  C6
		0  A   B   nan C   A   nan
		1  B   C   D   nan B   nan
		2  D   E   F   nan C   nan
		3  nan nan A   nan nan B
		
	"Frame-2":
		   C
		0  A  
		
		   B  
		
		   C
		
		   A
		1  B  
		
		   C  
		
		   D
		
		   B
		2  D  
		
		   E  
		
		   F
		
		   C
		3. A
		
		   B
		
}
"answer": {
	"desc": %s Use for Series, misisng values are removed, so you can aggregate : Or remove missing values by join per rows by : 
	"code-snippets": [
		df['merge'] = df.stack().groupby(level=0).agg('\n\n'.join)
		#for filter only C columns
		df['merge'] = df.filter(like='C').stack().groupby(level=0).agg('\n\n'.join)
		
		----------------------------------------------------------------------
		df['merge'] = df.apply(lambda x: '\n\n'.join(x.dropna()), axis=1)
		#for filter only C columns
		df['merge'] = df.filter(like='C').apply(lambda x: '\n\n'.join(x.dropna()), axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60763203
"link": https://stackoverflow.com/questions/60763203/how-to-get-absolute-difference-amongst-all-the-values-of-a-column-with-each-othe
"question": {
	"title": How to get absolute difference amongst all the values of a column with each other?
	"desc": I am trying to find difference among-st all values in key column keeping these 4 digit code as my index value. I tried using pivot for this operation but failed. It would be really helpful if I can get the approach for this presentation. df1 Expected df 
}
"io": {
	"Frame-1": 
		Name | Key | 1001 | 1002 | 1003 |
		Abb    AB      5      8      10     
		Baa    BA     10     11      33  
		Cbb    CB     12     40      90  
		
	"Frame-2":
		Code  | Key | AB | BA | CB |
		1001    AB     0   5    7
		1001    BA     5   0    2
		1001    CB     7   2    0
		1002
		.
		.
		.
		1003
		
}
"answer": {
	"desc": %s Not the best solution though. df: df1: df2: x: df3: 
	"code-snippets": [
		df2 = df1.loc[np.repeat(df1.index.values,len(df1.columns))] 
		df2.columns = df2.columns.droplevel(0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46488530
"link": https://stackoverflow.com/questions/46488530/python-pandas-dataframe-make-whole-row-nan-according-to-condition
"question": {
	"title": Python pandas.DataFrame: Make whole row NaN according to condition
	"desc": I want to make the whole row NaN according to a condition, based on a column. For example, if , I want to make the whole row NaN. Unprocessed data frame looks like this: Make whole row NaN, if : Thank you. 
}
"io": {
	"Frame-1": 
		   A  B
		0  1  4
		1  3  5
		2  4  6
		3  8  7
		
	"Frame-2":
		     A    B
		0  1.0  4.0
		1  3.0  5.0
		2  NaN  NaN
		3  NaN  NaN
		
}
"answer": {
	"desc": %s You can also use Example in human language can be translated to: assign to any column () of the dataframe ( ) where the condition is valid. 
	"code-snippets": [
		df.loc[df.B > 5, :] = np.nan
		----------------------------------------------------------------------
		df.loc[df.B > 5, :] = np.nan
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60728016
"link": https://stackoverflow.com/questions/60728016/need-help-getting-the-frequency-of-each-number-in-a-pandas-dataframe
"question": {
	"title": Need help getting the frequency of each number in a pandas dataframe
	"desc": I am trying to find a simple way of converting a pandas dataframe into another dataframe with frequency of each feature. I'll provide an example of what I'm trying to do below Current dataframe example (feature labels are just index values here): Dataframe I would like to convert this to: As you can see, the column label corresponds to the possible numbers within the dataframe and each frequency of that number per row is put into that specific feature for the row in question. Is there a simple way to do this with python? I have a large dataframe that I am trying to transform into a dataframe of frequencies for feature selection. If any more information is needed I will update my post. 
}
"io": {
	"Frame-1": 
		   0   1   2   3   4   ...   n
		0  2   3   1   4   2         ~
		1  4   3   4   3   2         ~
		2  2   3   2   3   2         ~
		3  1   3   0   3   2         ~
		...
		m  ~   ~   ~   ~   ~         ~
		
	"Frame-2":
		   0   1   2   3   4   ...   n
		0  0   1   2   1   1         ~
		1  0   0   1   2   2         ~
		2  0   0   3   2   0         ~
		3  1   1   1   2   0         ~
		...
		m  ~   ~   ~   ~   ~         ~
		
}
"answer": {
	"desc": %s Use with : Alternative with 
	"code-snippets": [
		df2 = df.T.melt()
		pd.crosstab(df2['variable'], df2['value'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60709527
"link": https://stackoverflow.com/questions/60709527/removing-duplicate-values-from-a-cell-of-dataframein-python
"question": {
	"title": Removing Duplicate values from a Cell of DataFramein python
	"desc": DataFrame Output I want Any Help will be Appreciated 
}
"io": {
	"Frame-1": 
		ID                                                                         Source
		 1                     [192.168.1.121, 10.1.161.10, 192.168.1.121, 192.168.1.121]
		 2          [192.168.1.121, 10.1.161.10, 10.1.161.10, 10.1.161.10, 192.168.1.121]
		 3                                  [192.168.1.121, 192.168.1.121, 192.168.1.121]
		 4                         [10.1.161.10, 192.168.1.121, 10.1.161.10, 10.1.161.10]
		
	"Frame-2":
		ID                                Source
		 1            192.168.1.121, 10.1.161.10
		 2            192.168.1.121, 10.1.161.10
		 3                         192.168.1.121
		 4            10.1.161.10, 192.168.1.121
		
}
"answer": {
	"desc": %s Try using : Output: 
	"code-snippets": [
		df['source'] = df['source'].apply(
		lambda x: ', '.join(pd.unique(x)),
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60644834
"link": https://stackoverflow.com/questions/60644834/pandas-search-if-full-rows-of-a-large-df-contain-template-rows-from-a-another-sm
"question": {
	"title": Pandas search if full rows of a large df contain template rows from a another smaller df?
	"desc": I have a large df (df1) with binary outputs in each column like so: I also have another smaller df (df2) with some "template" rows and I want to check if df1s rows contain. Templates looks like this: What I'm trying to do is to search the large df efficiently for these small number of templates, so in this example, rows 1, 3, 4, 6 would match, but 2 and 5 would not match. I want any row in the large df which has any extra 1s to pass the test (i.e. a template row is there but it also has some extra 1s in that row). I know that I could just have a nested loop and iterate over all the rows of the large and small dfs and match rows as np.arrays, but this seems like an extremely inefficient way to do this. I'm wondering if there are any non-iterative pd-based solutions to this problem? Thank you so much! Minor functionality edit: Along with searching and matching, I'm also trying to retain a list of which template row from df2 each row in df1 matched with so I can do statistics on how many templates show up in the large df and which ones they are. This is one of the reasons why this answer(Compare Python Pandas DataFrames for matching rows) doesn't work. 
}
"io": {
	"Frame-1": 
		df1:
		
		  a b c d
		1 1 0 1 0
		2 0 0 0 0
		3 0 1 0 1
		4 1 1 0 0
		5 1 0 0 0
		6 1 0 1 1
		...
		
	"Frame-2":
		df2:
		
		  a b c d
		1 1 0 1 0
		2 1 1 1 1
		3 0 0 0 1
		4 1 1 0 0
		
}
"answer": {
	"desc": %s This logic will tell you where there are matches based on your requirements. Here I just created a new column in the original DF, but you'd probably want to create a third DF and keep adding columns to it for each test. Then you can just sum up the columns/rows to get your totals: df1 df2 Logic Output 
	"code-snippets": [
		t_match =[]
		for index, row in df2.iterrows():
		    t_match.append(((df1-row) >= 0).all(axis=1).sum())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60585417
"link": https://stackoverflow.com/questions/60585417/show-records-contain-multiple-key-words-using-or-or-if-elif-filter-rows
"question": {
	"title": Show records contain multiple key words using OR or if elif (filter rows)
	"desc": I am trying to show the rows that contain set of key words. The table look like this What I want is to filter this table where the row contain the tow strings (LD and AB) OR (LD and AD) OR (AC) So I get this result I tried This obviously didn't work , so I tried using the if function: and using this They didn't work So can someone help with what I made wrong 
}
"io": {
	"Frame-1": 
		Col0     col1      col2     col3
		1         LD        AN       CC
		2         AB      LD SS      BB
		1         AA      LD AD      CC
		3         LD        AC       NN
		2         FF        UH       BB
		
	"Frame-2":
		Col0     col1      col2     col3
		2         AB      LD SS      BB
		1         AA      LD AD      CC
		3         LD        AC       NN
		
}
"answer": {
	"desc": %s This line: contains a syntax error because of . Remove it and you get the expected result: giving: 
	"code-snippets": [
		df = df[count.isin([1,2]) (s.str.contains('LD') & s.str.contains('AB'))
		        | (s.str.contains('LD') & s.str.contains('AD'))
		        | (s.str.contains('LD') & s.str.contains('AC'))]
		
		----------------------------------------------------------------------
		df = df[(s.str.contains('LD') & s.str.contains('AB'))
		        | (s.str.contains('LD') & s.str.contains('AD'))
		        | (s.str.contains('LD') & s.str.contains('AC'))]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60578553
"link": https://stackoverflow.com/questions/60578553/using-values-from-dataframe-for-calculation
"question": {
	"title": Using values from dataframe for calculation
	"desc": I am selecting dedicated data from a dataframe and would like to make a linear interpolation based on my defined formula: I would like to interpolate e.g. between rank 2.0 and 3.0, where the needed rank is 2.5. The calculation looks like the following: y = -9.080002 + (-9.039993 - (-9.080002))*[(2.5-2)/(3-2)] = -9.059997500000 where the values are defined in the code as the following: The code looks like the following: The result looks like the following: The Excel file looks like the following: 
}
"io": {
	"Frame-1": 
		y = y0 + (y1 - y0) * [(x-x0)/(x1-x0)]
		
	"Frame-2":
		-9.080002000000007 2.0
		-9.360001000000011 1.0
		-9.22000150000001
		
}
"answer": {
	"desc": %s The IndexError is because of . 
	"code-snippets": [
		def Calc(data):
		
		    Rank = 2.5
		    Rank_Up = 3.0
		    Rank_Down = 2.0
		
		    Calculation = 0.0
		
		    check_int = isinstance(Rank, int)
		    if not check_int:
		        for ind in data.Rank:
		            if (ind in (Rank_Down, Rank_Up)):
		                index0 = data.Rank.index[ind-1]
		                index1 = data.Rank.index[ind]
		                Calculation = data['DataPoint'][index0] + (data['DataPoint'][index1]- data['DataPoint'][index0]) *((Rank-Rank_Down)/(Rank_Up-Rank_Down))
		                print(data['DataPoint'][index0], ind)
		                print(data['DataPoint'][index1], ind+1)
		                break
		
		    return Calculation
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 24412510
"link": https://stackoverflow.com/questions/24412510/transpose-pandas-dataframe
"question": {
	"title": Transpose pandas dataframe
	"desc": How do I convert a list of lists to a panda dataframe? it is not in the form of coloumns but instead in the form of rows. for example: I want it to be shown as rows and not coloumns. currently it shows somethign like this I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists? This is how I want the output to look like with other coloumns also filled in. However. won't help. 
}
"io": {
	"Frame-1": 
		     B   P   F   I  FP  BP   2   M   3   1   I   L
		0   64  73  76  64  61  32  36  94  81  49  94  48
		1   57  58  69  46  34  66  15  24  20  49  25  98
		2   99  61  73  69  21  33  78  31  16  11  77  71
		3   41   1  55  34  97  64  98   9  42  77  95  41
		4   36  50  54  27  74   0   8  59  27  54   6  90
		5   74  72  75  30  62  42  90  26  13  49  74   9
		6   41  92  11  38  24  48  34  74  50  10  42   9
		7   77   9  77  63  23   5  50  66  49   5  66  98
		8   90  66  97  16  39  55  38   4  33  52  64   5
		9   18  14  62  87  54  38  29  10  66  18  15  86
		10  60  89  57  28  18  68  11  29  94  34  37  59
		11  78  67  93  18  14  28  64  11  77  79  94  66
		
	"Frame-2":
		     B   P   F   I  FP  BP   2   M   3   1   I   L
		0    64 
		1    73  
		1    76  
		2    64  
		3    61  
		4    32  
		5    36  
		6    94  
		7    81  
		8    49  
		9    94  
		10   48
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		import numpy
		
		df = pandas.DataFrame(numpy.asarray(data[x]).T.tolist(),
		                      columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60486231
"link": https://stackoverflow.com/questions/60486231/pandas-dataframe-compare-columns-with-one-value-and-get-this-row-and-previous-ro
"question": {
	"title": Pandas dataframe compare columns with one value and get this row and previous row into another dataframe
	"desc": I have a dataframe like this: I want to create another dataframe with and also store its previous row. It should look like this: What is the best way. Thanks in advance. 
}
"io": {
	"Frame-1": 
		>>> df
		   A    B
		0  1   56
		1  2   75
		2  3  102
		3  4   15
		4  5   19
		5  6  116
		
	"Frame-2":
		>>> df1
		   A    B
		1  2   75
		2  3  102
		4  5   19
		5  6  116
		
}
"answer": {
	"desc": %s Use with 2 conditions chained by for bitwise , second with , also for cpmpare is used : Alternative: 
	"code-snippets": [
		df1 = df[df.B.gt(100) | df.B.shift(-1).gt(100)]
		
		----------------------------------------------------------------------
		df1 = df[(df.B>100) | (df.B.shift(-1)>100)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 45336415
"link": https://stackoverflow.com/questions/45336415/pandas-drop-columns-with-only-one-unique-value-for-a-group
"question": {
	"title": Pandas Drop Columns with Only One Unique Value for a Group
	"desc": I have a df that consists of duplicate : I want to remove columns where all values for an are the same. This could mean that all values in the column are the same () or all the values are the same for each (). Desired result: I used this to identify the counts of unique values in each column: If I drop all columns where this count is equal to 1, this would take care of the scenario. However, how should I take care of the scenario? The df has already been grouped by id to find duplicate, but do I need to use again? As a "bonus", I wouldn't mind knowing how to identify where even one has text that is all the same (i.e. ). I'm essentially trying to find which columns cause there to be duplicates. Thank you for any and all insight you all might have! 
}
"io": {
	"Frame-1": 
		id    text      text2     text3
		1     hello     hello     hello
		1     hello     hello     hello
		2     hello     hello     goodbye
		2     goodbye   hello     goodbye
		2     hello     hello     goodbye
		
	"Frame-2":
		id    text     
		1     hello     
		1     hello       
		2     hello        
		2     goodbye        
		2     hello
		
}
"answer": {
	"desc": %s Here's one way. Get unique values for each column Get if any column has single value in each group Get the index aka column names for satisfying conditions, Then, use Details 
	"code-snippets": [
		In [1227]: u = df.nunique()
		
		----------------------------------------------------------------------
		In [1228]: gu = gu = (df.groupby('id').agg('nunique') == 1).all()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60502832
"link": https://stackoverflow.com/questions/60502832/how-to-drop-pandas-consecutive-column-by-column-name-simultaneously
"question": {
	"title": How to drop pandas consecutive column by column name simultaneously?
	"desc": Here's my data The Output I expected, What I did But this is not efficient, how to do this effectively? 
}
"io": {
	"Frame-1": 
		Id   Column1  Column2  Column3  Column4 ....  Column112  Column113 ... Column143
		1         67       89       86       43              56         72            67
		
	"Frame-2":
		Id   Column1  Column113 ... Column143
		1         67         72            67
		
}
"answer": {
	"desc": %s Use: 
	"code-snippets": [
		df1 = df.drop(df.loc[:, 'Column2':'Column112'].columns, axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60492857
"link": https://stackoverflow.com/questions/60492857/pandas-groupby-or-cut-multi-dataframes-to-bins
"question": {
	"title": Pandas - Groupby or Cut multi dataframes to bins
	"desc": I'm having a dataframe with starting axis points and their end points like this I'm drawing a heatmap. I need each "zone" of that map has a line which shows the average distance and angle of lines which have x/y from that zone and their x_end/y_end. It looks like this My bins is I've already plotted a heatmap I'm looking for something like this 
}
"io": {
	"Frame-1": 
		x       y       x_end   y_end   distance
		14.14   30.450  31.71   41.265  20.631750
		-27.02  55.650  -33.60  63.000  9.865034
		-19.25  70.665  -28.98  80.115  13.563753
		16.45   59.115  9.94    41.895  18.409468
		
	"Frame-2":
		xbins = np.linspace(-35, 35, 11)
		
		ybins = np.linspace(0, 105, 22)
		
}
"answer": {
	"desc": %s Something like this? Output: 
	"code-snippets": [
		(df.drop(['x','y'], axis=1)
		  .groupby([pd.cut(df.x, xbins),
		            pd.cut(df.y, ybins)],
		          )
		   .mean()
		   .dropna(how='all')
		   .add_prefix('Average_')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60455539
"link": https://stackoverflow.com/questions/60455539/rolling-over-values-from-one-column-to-other-based-on-another-dataframe
"question": {
	"title": Rolling over values from one column to other based on another dataframe
	"desc": I have two dataframes: Now I have another dataframe which only have unique IDs from first dataframe, and dates that represent months: So based on first dataframe I need to fill the values based on the value that is in the first dataframe that is within the corresponding month ( so for example I take the last value for the from and put it in the column in . IF there are no other values for that ID just fill all the remaining columns in with the value in the most right filled column(roll over to the right). So the end result is exactly like this 
}
"io": {
	"Frame-1": 
		ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31
		A1
		A2
		A3
		A4
		A5
		
	"Frame-2":
		ID  2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31
		A1  8500        8500        8500      8500        8500        8500         8500
		A2   NA         1900        1900      1900        1900        1900         1900
		A3   NA          NA          NA       3000        110           0             0
		A4   NA          NA          NA        NA         NA           10            10
		A5   NA          NA          NA        NA         NA           NA           500
		
}
"answer": {
	"desc": %s This gives you the data in form: Output: 
	"code-snippets": [
		month_ends = pd.to_datetime(df1.DatePaid).dt.to_period('M')
		# also
		# month_ends = pd.to_datetime(df1.DatePaid).add(pd.offsets.MonthEnd(0))
		
		(df1.groupby(['ID', month_ends])
		    ['Remaining'].last()
		    .unstack(-1)
		    .ffill(1)
		    .reset_index()
		    .rename_axis(columns=None)
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60440882
"link": https://stackoverflow.com/questions/60440882/remove-unnamed-colums-pandas-dataframe
"question": {
	"title": remove unnamed colums pandas dataframe
	"desc": i'm a student and have a problem that i cant figure it out how to solve it.i have csv data like this : code for reading csv like this : SMT print out : expected output : i already trying but it will be like this : i already trying to put or //is not working, and the last time I tried it like this : and i got i just want to get rid the Unnamed: 5 ~ Unnamed: 8, how the correct way to get rid of this Unnamed thing ? 
}
"io": {
	"Frame-1": 
		 1          2          3          4
		 6          7          8          9
		11         12         13         14
		
	"Frame-2":
		           1          2          3          4
		0          6          7          8          9
		1         11         12         13         14
		2          0          0          0          0
		
}
"answer": {
	"desc": %s The "unnamed" just says, that pandas does not know how to name the columns. So these are just names. You could set the names like this in the Output: You have to set so that pandas knows that this is usually the header. Or you set 
	"code-snippets": [
		pd.read_csv("test.csv", usecols=(5,6,7,8), skiprows=3, nrows=3, header=0, names=["c1", "c2", "c3", "c4"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60394977
"link": https://stackoverflow.com/questions/60394977/pandas-how-to-remove-non-alphanumeric-columns-in-series
"question": {
	"title": Pandas: How to remove non-alphanumeric columns in Series
	"desc": A Pandas' Series can contain invalid values: I want to produce a clean Series keeping only the columns that contain a numeric value or a non-empty non-space-only alphanumeric string: should be dropped because it is an empty string; because ; and because space-only strings. The expected result: How can I filter the columns that contain numeric or valid alphanumeric? returns for , instead of the True I would expect. changes 's to string and later considers it a valid string. of course drops only (). I don't see so many other possibilities listed at https://pandas.pydata.org/pandas-docs/stable/reference/series.html As a workaround I can loop on the items() checking type and content, and create a new Series from the values I want to keep, but this approach is inefficient (and ugly): Is there any boolean filter that can help me to single out the good columns? 
}
"io": {
	"Frame-1": 
		a     b     c     d      e      f     g 
		1    ""   "a3"  np.nan  "\n"   "6"   " "
		
	"Frame-2":
		a      c     f
		1    "a3"   "6"
		
}
"answer": {
	"desc": %s Convert values to strings and chain another mask by with bitwise - : 
	"code-snippets": [
		row = row[row.astype(str).str.isalnum() & row.notna()]
		print (row)
		a     1
		c    a3
		f     6
		Name: 0, dtype: object
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60381266
"link": https://stackoverflow.com/questions/60381266/check-for-each-value-in-column-has-only-one-corresponding-value-in-another-colum
"question": {
	"title": Check for each value in column has only one corresponding value in another column pandas
	"desc": I have my data in pandas data frame as follows: I would like to add another field in this dataframe(with True/False) such that. for each id value, there should be only one corresponding values. So, my expected output looks like this.. for the id - 1234 there are two corresponding values (AA and BB), the one with lesser count should be flagged. 
}
"io": {
	"Frame-1": 
		      ID Name
		0   1234   AA
		1   1234   AA
		2   1234   AA
		3   5678   BB
		4   5678   BB
		5   5678   DD
		6   9999   EE
		7   9999   EE
		8   1234   CC
		9   1234   CC
		10  1234   BB
		11  1234   BB
		
	"Frame-2":
		      ID Name
		5   5678   DD
		8   1234   CC
		9   1234   CC
		10  1234   BB
		11  1234   BB
		
}
"answer": {
	"desc": %s Here's my solution: Output: 
	"code-snippets": [
		# toy data
		df = pd.DataFrame({'ID': [1234, 1234, 1234, 5678, 5678, 5678, 9999, 9999, 1234, 1234, 1234, 1234],
		 'Name': ['AA', 'AA', 'AA', 'BB', 'BB', 'DD', 'EE', 'EE', 'CC', 'CC', 'BB', 'BB']}
		)
		
		# filter those ID's that appear with multiple names
		non_unique = df.groupby('ID').Name.transform('nunique').ne(1)
		df = df[non_unique]
		
		# count the occurrences of each combination ['ID','Name']
		counts = df[non_unique].groupby(['ID','Name']).Name.transform('count')
		
		# filter those with minimal occurrences within each ID
		df[counts == counts.groupby(df['ID']).transform('min')]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60336550
"link": https://stackoverflow.com/questions/60336550/use-duplicated-values-to-increment-column
"question": {
	"title": Use duplicated values to increment column
	"desc": I have a Pandas dataframe and I want to increment a column based on the amount of duplicated values. So when a duplicate is found, all other occurrences is incremented. So given this input dataframe return I tried this line of code but I don't know how to increment 
}
"io": {
	"Frame-1": 
		    SM
		 0  AB
		 1  AC
		 2  AD
		 3  AB
		 4  AB
		 5  AC
		 6  AE
		 7  AD
		
	"Frame-2":
		     SM DM
		  0  AB AB
		  1  AC AC
		  2  AD AD
		  3  AB AB_1
		  4  AB AB_2
		  5  AC AC_1
		  6  AE AE
		  7  AD AD_1
		
}
"answer": {
	"desc": %s Use and : [out] 
	"code-snippets": [
		s = df.groupby('SM').cumcount()
		
		df['DM'] = df['SM'].where(s.eq(0), df['SM'] + '_' + s.astype(str))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60305830
"link": https://stackoverflow.com/questions/60305830/how-to-replace-the-value-of-a-dataframe-column-with-the-value-of-another-column
"question": {
	"title": How to replace the value of a dataframe column with the value of another column using groupby.first()?
	"desc": I have a df like this: I want to check the first of every Year-Month. If it's < 0, I want value2 to replace with value1. How can I do that? In this example, the result should be: Because only first are negative, first are positive, just leave it. I used: it doesn't seem to work. Thanks. 
}
"io": {
	"Frame-1": 
		               Value1     Value2
		2008-01-01       -1          4
		2008-01-01       -1          5
		2008-01-03       -1          6
		2008-02-25        0          7
		2008-02-26       -1          8
		2008-02-27        0          9
		2008-03-02        5         10 
		2008-03-16       -1         11
		2008-03-17       -1         12 
		2009-04-04       -1         13
		2009-04-07        0         14
		
	"Frame-2":
		               Value1     Value2
		2008-01-01       -1         -1
		2008-01-01       -1          5
		2008-01-03       -1          6
		2008-02-25        0          7
		2008-02-26       -1          8
		2008-02-27        0          9
		2008-03-02        5         10 
		2008-03-16       -1         11
		2008-03-17       -1         12 
		2009-04-04       -1         -1
		2009-04-07        0         14
		
}
"answer": {
	"desc": %s My approach with to extract the index and to update: Output: 
	"code-snippets": [
		s = df.groupby(df.index.to_period('M'), as_index=False).head(1)
		df.loc[s[s['Value1'].lt(0)].index, 'Value1'] = df['Value2']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60279369
"link": https://stackoverflow.com/questions/60279369/python-dataframe-merge-columns-according-to-other-column-values
"question": {
	"title": python dataframe merge columns according to other column values
	"desc": What I want to do is merge columns according to values in another column It is better illustrated with a simple example: I have a dataframe with 5 columns: I want to get the following table: where the columns are filled with values from team_1.x and team_1.y for rows of players with number less than 5 and values from team_2.x and team_2.y for rows of players with number bigger than 5 
}
"io": {
	"Frame-1": 
		| player_num    | team_1.x  | team_1.y  | team_2.x  | team_2.y  |
		|------------   |---------- |---------- |---------- |---------- |
		| 1             | x_1       | y_1       | x_2       | y_2       |
		| 4             | x_3       | y_3       | x_4       | y_4       |
		| 8             | x_5       | y_5       | x_6       | y_6       |
		
	"Frame-2":
		| x     | y     |
		|-----  |-----  |
		| x_1   | y_1   |
		| x_3   | y_3   |
		| x_6   | y_6   |
		
}
"answer": {
	"desc": %s You can use Numpy's np.where for this: EDIT: 
	"code-snippets": [
		import numpy as np
		...
		df['x'] = np.where(df['player_num'] < 5, df['team_1.x'], df['team_2.x'])
		df['y'] = np.where(df['player_num'] < 5, df['team_1.y'], df['team_2.y'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 15112234
"link": https://stackoverflow.com/questions/15112234/how-can-i-convert-columns-of-a-pandas-dataframe-into-a-list-of-lists
"question": {
	"title": How can I convert columns of a pandas DataFrame into a list of lists?
	"desc": I have a pandas DataFrame with multiple columns. What I want to do is to convert this into a list like following 2u 2s 4r 4n 4m 7h 7v are column headings. It will change in different situations, so don't bother about it. 
}
"io": {
	"Frame-1": 
		2u    2s    4r     4n     4m   7h   7v
		0     1     1      0      0     0    1
		0     1     0      1      0     0    1
		1     0     0      1      0     1    0
		1     0     0      0      1     1    0
		1     0     1      0      0     1    0
		0     1     1      0      0     0    1
		
	"Frame-2":
		X = [
		     [0, 0, 1, 1, 1, 0],
		     [1, 1, 0, 0, 0, 1],
		     [1, 0, 0, 0, 1, 1],
		     [0, 1, 1, 0, 0, 0],
		     [0, 0, 0, 1, 0, 0],
		     [0, 0, 1, 1, 1, 0],
		     [1, 1, 0, 0, 0, 1]
		    ]
		
}
"answer": {
	"desc": %s It looks like a transposed matrix: 
	"code-snippets": [
		[list(l) for l in zip(*df.values)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 39903090
"link": https://stackoverflow.com/questions/39903090/efficiently-replace-values-from-a-column-to-another-column-pandas-dataframe
"question": {
	"title": Efficiently replace values from a column to another column Pandas DataFrame
	"desc": I have a Pandas DataFrame like this: I want to replace the values with the values in the second column () only if values are equal to 0, and after (for the zero values remaining), do it again but with the third column (). The Desired Result is the next one: I did it using the function, but it seems too slow.. I think must be a faster way to accomplish that. is there a faster way to do that?, using some other function instead of the function? 
}
"io": {
	"Frame-1": 
		   col1 col2 col3
		1   0.2  0.3  0.3
		2   0.2  0.3  0.3
		3     0  0.4  0.4
		4     0    0  0.3
		5     0    0    0
		6   0.1  0.4  0.4
		
	"Frame-2":
		   col1 col2 col3
		1   0.2  0.3  0.3
		2   0.2  0.3  0.3
		3   0.4  0.4  0.4
		4   0.3    0  0.3
		5     0    0    0
		6   0.1  0.4  0.4
		
}
"answer": {
	"desc": %s Using is faster. Using a similar pattern as you used with : However, using a nested is slightly faster: Timings Using the following setup to produce a larger sample DataFrame and timing functions: I get the following timings: I tried timing your method, but it's been running for multiple minutes without completing. As a comparison, timing your method on just the 6 row example DataFrame (not the much larger one tested above) took 12.8 ms. 
	"code-snippets": [
		df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])
		df['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])
		
		----------------------------------------------------------------------
		df['col1'] = np.where(df['col1'] == 0, 
		                      np.where(df['col2'] == 0, df['col3'], df['col2']),
		                      df['col1'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60203245
"link": https://stackoverflow.com/questions/60203245/split-multiple-columns-by-numeric-or-alphabetic-symbols
"question": {
	"title": Split multiple columns by numeric or alphabetic symbols
	"desc": I'm working on splitting multiple columns by numeric or alphabetic symbols for columns to , then take the first part as the values of this column. For example, will be split by then take , will be splited by and take . The code I have tried: Output: My desired output will like this: Thanks for your help. 
}
"io": {
	"Frame-1": 
		    id       v1                      v2                     v3
		0    1                   12110                    NaN
		1    2                                               
		2    3                          N15          10A11A
		3    4                                  2D301D302
		4    5                           06504
		5    6               10103                    NaN
		6    7               10203                    NaN
		7    8                                       12307
		8    9                 130C                    NaN
		9   10                P4D                    NaN
		10  11        C4C5C41403                    NaN
		
	"Frame-2":
		    id       v1         v2             v3
		0    1                         NaN
		1    2                          
		2    3              NaN          
		3    4                         
		4    5              
		5    6                    NaN
		6    7                    NaN
		7    8                         
		8    9                       NaN
		9   10                    NaN
		10  11                     NaN
		
}
"answer": {
	"desc": %s You may remove all text after the first ASCII alphanumeric character in the columns you need to modify: If your columns can contain multiline texts, use where inline modifier will let match line break chars, too. 
	"code-snippets": [
		cols = ['v1', 'v2', 'v3']
		df[cols] = df[cols].apply(lambda x: x.str.replace(r'[A-Za-z0-9].*', ''))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56293392
"link": https://stackoverflow.com/questions/56293392/renaming-columns-on-slice-of-dataframe-not-performing-as-expected
"question": {
	"title": Renaming columns on slice of dataframe not performing as expected
	"desc": I was trying to clean up column names in a dataframe but only a part of the columns. It doesn't work when trying to replace column names on a slice of the dataframe somehow, why is that? Lets say we have the following dataframe: Note, on the bottom is copy-able code to reproduce the data: I want to clean up the column names (expected output): Approach 1: I can get the clean column names like this: Or Approach 2: But when I try to overwrite the column names, nothing happens: Same for the second approach: This does work, but you have to manually concat the name of the first column, which is not ideal: Is there an easier way to achieve this? Am I missing something? Dataframe for reproduction: 
}
"io": {
	"Frame-1": 
		   Value ColAfjkj ColBhuqwa ColCouiqw
		0      1        a         e         i
		1      2        b         f         j
		2      3        c         g         k
		3      4        d         h         l
		
	"Frame-2":
		   Value ColA ColB ColC
		0      1    a    e    i
		1      2    b    f    j
		2      3    c    g    k
		3      4    d    h    l
		
}
"answer": {
	"desc": %s This is because pandas' index is immutable. If you check the documentation for , you'll see that it is defined as: Immutable ndarray implementing an ordered, sliceable set So in order to modify it you'll have to create a new list of column names, for instance with: Another option is to use with a dictionary containing the columns to replace: 
	"code-snippets": [
		df.columns = [df.columns[0]] + list(df.iloc[:, 1:].columns.str[:4])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60157250
"link": https://stackoverflow.com/questions/60157250/pandas-dataframe-efficient-way-to-add-a-column-seconds-since-last-event
"question": {
	"title": Pandas.DataFrame: efficient way to add a column &quot;seconds since last event&quot;
	"desc": I have a Pandas.DataFrame with a standard index representing seconds, and I want to add a column "seconds elapsed since last event" where the events are given in a list. Specifically, say and Then I want to obtain I tried so apparently to make it work I need to write . More importantly, when I then do I obtain That is: the previous has been overwritten. Is there a way to assign new values without overwriting existing values by nan ? Then, I can do something like Or is there a more efficient way ? For the record, here is my naive code. It works, but looks inefficient and of poor design: 
}
"io": {
	"Frame-1": 
		|    |   0 |    x |
		|---:|----:|-----:|
		|  0 |   0 | <NA> |
		|  1 |   0 | <NA> |
		|  2 |   0 |    0 |
		|  3 |   0 |    1 |
		|  4 |   0 |    2 |
		|  5 |   0 |    0 |
		|  6 |   0 |    1 |
		
	"Frame-2":
		|    |   0 |   x |
		|---:|----:|----:|
		|  0 |   0 | nan |
		|  1 |   0 | nan |
		|  2 |   0 | nan |
		|  3 |   0 | nan |
		|  4 |   0 | nan |
		|  5 |   0 |   0 |
		|  6 |   0 |   1 |
		
}
"answer": {
	"desc": %s IIUC, you can do double groupby: Output: 
	"code-snippets": [
		s = df.index.isin(event).cumsum()
		# or equivalently
		# s = df.loc[event, 0].reindex(df.index).isna().cumsum()
		
		df['x'] = np.where(s>0,df.groupby(s).cumcount(), np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60153802
"link": https://stackoverflow.com/questions/60153802/group-together-matched-pairs-across-multiple-columns-python
"question": {
	"title": Group together matched pairs across multiple columns Python
	"desc": Thank you for reading. I have a dataframe which looks like this: Each row consists of a match between two IDs (e.g. ID1 from Col_A matches to ID2 from Col_B on the first row). In the example above, all 5 IDs are connected (1 is connected to 2, 2 to 3, 2 to 4, 1 to 5). I therefore want to create a new column which clusters all of these rows together so that I can easily access each group of matched pairs: I have not yet been able to find a similar question, but apologies if this is a duplicate. Many thanks in advance for any advice. 
}
"io": {
	"Frame-1": 
		Col_A  Col_B   Col_C  Col_D  Col_E  
		 1     2       null   null   null  
		 1     null    3      null   null  
		 null  2       3      null   null  
		 null  2       null   4      null  
		 1     null    null   null   5 
		
	"Frame-2":
		Col_A  Col_B   Col_C  Col_D  Col_E  Group ID
		 1     2       null   null   null      1
		 1     null    3      null   null      1
		 null  2       3      null   null      1
		 null  2       null   4      null      1
		 1     null    null   null   5         1
		
}
"answer": {
	"desc": %s As @YOBEN_S and @QuangHoang suggests, you can use networkx library and Graph Theory connnected components like this. Given df, Use Output: 
	"code-snippets": [
		import networkx as nx
		d_edge = df.apply(lambda x: x.dropna().to_numpy(), axis=1)
		G = nx.from_edgelist(d_edge.to_numpy().tolist())
		cc_list = list(nx.connected_components(G))
		df['groupid'] = d_edge.apply(lambda  x: [n for n, i in enumerate(cc_list) if x[0] in i][0] + 1)
		df
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60095370
"link": https://stackoverflow.com/questions/60095370/fill-the-values-with-each-column-combination-with-some-default-values-in-pandas
"question": {
	"title": Fill the values with each column combination with some default values in pandas data frame
	"desc": I have a dataframe like this, Now CD is not present with col1 1908 and 1909 values, FR not present with 1908 and 1909 values and PR not present wth 1907. Now I want to create rows with col2 values which are not with all col1 values with col3 values as 0. So final dataframe will look like, I could do this using a for loop with every possible col2 values and comparing with every col1 group. But I am looking for shortcuts to do it most efficiently. 
}
"io": {
	"Frame-1": 
		df
		col1    col2    col3
		1907    CD       49
		1907    FR       33
		1907    SA       34
		1908    PR        1
		1908    SA       37
		1909    PR       16
		1909    SA       38
		
	"Frame-2":
		df
		col1    col2    col3
		1907    CD       49
		1907    FR       33
		1907    SA       34
		1907    PR        0
		1908    CD        0
		1908    FR        0
		1908    PR        1
		1908    SA       37
		1908    CD        0
		1908    FR        0
		1909    PR       16
		1909    SA       38
		
}
"answer": {
	"desc": %s Use with for all combinations filled by : Another idea is use with : 
	"code-snippets": [
		mux = pd.MultiIndex.from_product([df['col1'].unique(), 
		                                  df['col2'].unique()], names=['col1','col2'])
		df = df.set_index(['col1','col2']).reindex(mux, fill_value=0).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60087855
"link": https://stackoverflow.com/questions/60087855/name-of-the-dataframe-from-which-the-minimum-value-is-taken
"question": {
	"title": Name of the dataframe from which the minimum value is taken
	"desc": I have 3 data-frames as below. & With the code , I get a dateframe which has the min value of each cell of these 3 dataframes Now, my question is there a way to get a dataframe which shows from which dataframe these individual values have come from? The expected out put is as below Is this even possible in Pandas? 
}
"io": {
	"Frame-1": 
		val     val2    val3
		1         1     2
		11        3     3
		23        3     3
		24       24     3
		25        3     3
		3         3     3
		
	"Frame-2":
		val     val2    val3
		df1     df2     df3
		df1     df3     df2
		df2     df3     df2
		df2     df1     df2
		df2     df3     df2
		df3     df3     df1,df2
		
}
"answer": {
	"desc": %s Using : If you don't want tie, use : Output: Or make it bit prettier: 
	"code-snippets": [
		np.char.add("df", (np.stack(dfs, 0).argmin(0)+1).astype(str))
		
		array([['df1', 'df2', 'df3'],
		       ['df1', 'df3', 'df2'],
		       ['df2', 'df3', 'df2'],
		       ['df2', 'df1', 'df2'],
		       ['df2', 'df3', 'df2'],
		       ['df3', 'df3', 'df1']], dtype='<U23')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60065499
"link": https://stackoverflow.com/questions/60065499/multiply-two-columns-from-two-different-pandas-dataframes
"question": {
	"title": multiply two columns from two different pandas dataframes
	"desc": I have a pandas.DataFrame. I have another pandas.DataFrame I want to apply df2['Price_factor'] to df1['Price'] column. I tried my code but it didn't work. Thank you for your help in advance. 
}
"io": {
	"Frame-1": 
		      df1
		           Year    Class     Price    EL
		           2024     PC1       $243    Base
		           2025     PC1       $215    Base
		           2024     PC1       $217    EL_1
		           2025     PC1       $255    EL_1
		           2024     PC2       $217    Base
		           2025     PC2       $232    Base
		           2024     PC2       $265    EL_1
		           2025     PC2       $215    EL_1
		
	"Frame-2":
		           df2  
		              Year    Price_factor
		              2024      1
		              2025      0.98
		
}
"answer": {
	"desc": %s Use map, Output: 
	"code-snippets": [
		df1['Price_factor'] = df1['Year'].map(df2.set_index('Year')['Price_factor'])
		df1['Price_adjusted']= df1['Price'].str.strip('$').astype(int) * df1['Price_factor']
		df1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 60042289
"link": https://stackoverflow.com/questions/60042289/multi-column-to-single-column-in-pandas
"question": {
	"title": Multi-column to single column in Pandas
	"desc": I have the following data frame : And I need this : This is just a basic example, the real deal can have over 60 children. 
}
"io": {
	"Frame-1": 
		    parent          0        1      2   3
		0   14026529    14062504     0      0   0
		1   14103793    14036094     0      0   0
		2   14025454    14036094     0      0   0
		3   14030252    14030253  14062647  0   0
		4   14034704    14086964     0      0   0
		
	"Frame-2":
		    parent_id   child_id
		 0   14026529   14062504
		 1   14025454   14036094
		 2   14030252   14030253  
		 3   14030252   14062647
		 4   14103793   14036094
		 5   14034704   14086964
		
}
"answer": {
	"desc": %s Use , and . Casting as first will prevent child_Id's being cast to floats during the stacking process. [out] 
	"code-snippets": [
		(df.astype('Int64').where(df.ne(0))
		 .set_index('parent')
		 .stack()
		 .reset_index(level=0, name='child'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59915944
"link": https://stackoverflow.com/questions/59915944/apply-qcut-for-complete-dataframe
"question": {
	"title": Apply qcut for complete dataframe
	"desc": I want to replace the column values with bin numbers based on quantiles but for each and every column present in the dataframe. I know how to do this with qcut and labels as its parameter for a single column, but do not know whether it can be applied for complete dataframe or not. say the dataframe looks like below.. The ID column should remain unchanged but the other columns should be replaced by bin numbers, like below.. the bin numbers I have provided here for CC, DD, EE are not exact and for understanding purpose only. And in the real dataset, there are more than 100 columns and 1000 rows, and I do not want to replace the 'ID' column, but all the other columns. How to do this? 
}
"io": {
	"Frame-1": 
		    ID  CC  DD  EE
		0   Q1  0   23  18
		1   Q2  2   32  19
		2   Q3  3   45  20
		3   Q4  4   54  21
		4   Q5  5   67  22
		5   Q6  6   76  23
		
	"Frame-2":
		    ID  CC  DD  EE
		0   Q1  1   1   1
		1   Q2  2   2   1
		2   Q3  3   2   2
		3   Q4  4   3   3
		4   Q5  5   4   4
		5   Q6  5   5   5
		
}
"answer": {
	"desc": %s you have to use pandas.cut() Similarly you can do for other columns as well. 
	"code-snippets": [
		import pandas as pd
		
		df['CC'] = pd.cut(df['CC'], [0, 5, 10,20])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 36437849
"link": https://stackoverflow.com/questions/36437849/python-how-to-add-a-column-to-a-pandas-dataframe-between-two-columns
"question": {
	"title": Python: how to add a column to a pandas dataframe between two columns?
	"desc": I would like to add a column to a dataframe between two columns in number labeled columns dataframe. In the following dataframe the first column corresponds to the index while the first row to the name of the columns. I have that I want to put between the columns and , so 
}
"io": {
	"Frame-1": 
		df
		   0 0 1 2 3 4 5
		   1 6 7 4 5 2 1
		   2 0 3 1 3 3 4
		   3 9 8 4 3 6 2 
		
	"Frame-2":
		df
		   0 0 1 2 3 4 5 6 
		   1 6 7 4 5 2 2 1
		   2 0 3 1 3 3 3 4
		   3 9 8 4 3 6 5 2 
		
}
"answer": {
	"desc": %s You can use : Note: The method mutates the original DataFrame and does not return a copy. If you use , will be . 
	"code-snippets": [
		df = df.insert(4, 'new_col_name', tmp)
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59935954
"link": https://stackoverflow.com/questions/59935954/multidimensional-numpy-ndarray-from-multi-indexed-pandas-dataframe
"question": {
	"title": Multidimensional numpy.ndarray from multi-indexed pandas.DataFrame
	"desc": I want to produce a 3-dimensional numpy.ndarray from a multi-indexed pandas.DataFrame. More precisely, say I have: which gives me and I want to write a function which returns, with the above argument, the numpy.ndarray Pandas multi-index looks like a substitute for multidimensional arrays, but it does not provide (or at least does not document) ways to go back and forth... Thanks. 
}
"io": {
	"Frame-1": 
		        z  t
		  x  y
		  1  1  1 10
		     2  2 20
		  2  1  5 30
		
	"Frame-2":
		  [[[1, 10],
		    [2, 20]],
		   [[5, 30],
		    [NaN, NaN]]]
		
}
"answer": {
	"desc": %s Use: 
	"code-snippets": [
		df.to_xarray().to_array().values.transpose(1,2,0)
		
		>>[[[ 1. 10.]
		  [ 2. 20.]]
		
		 [[ 5. 30.]
		  [nan nan]]]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59900864
"link": https://stackoverflow.com/questions/59900864/change-value-of-only-1-cell-based-on-criteria-dataframe
"question": {
	"title": Change value of only 1 cell based on criteria DataFrame
	"desc": Based on a condition, I want to change the value of the first row on a certain column, so far this is what I have So I want to change only the first value of the column recibos by the value on a where (despesas['despesas']==a) & (despesas['recibos']=='') Edit 1 Example: And the result should be: 
}
"io": {
	"Frame-1": 
		despesas['despesas'] = [11.95,  2.5,  1.2 ,  0.6 ,  2.66,  2.66,  3.  , 47.5 , 16.95,17.56]
		recibos['recibos'] = [11.95,  1.2 ,  1.2 ,  0.2 ,  2.66,  2.66,  3.  , 47.5 , 16.95, 17.56]
		
	"Frame-2":
		[[11.95, 11.95],  [2.5, null] ,  [1.2, 1.2] ,  [0.6, null] ,  [2.66, 2.66],  [2.66, 2.66],  [3., 3] , [47.5, 45.5 ], [16.95, 16.95], [17.56, 17.56]]
		
}
"answer": {
	"desc": %s I found the solution that I was looking for 
	"code-snippets": [
		from itertools import count, filterfalse
		
		despesas['recibos'] =''
		for index, a in despesas.iterrows():
		    if len(recibos.loc[recibos['recibos']==a['despesas']])>0:
		        despesas.iloc[index,1]=True
		        recibos.drop(recibos.loc[recibos['recibos']==a['despesas']][:1].index, inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59925197
"link": https://stackoverflow.com/questions/59925197/sort-a-pandas-dataframe-by-a-column-in-another-dataframe-pandas
"question": {
	"title": Sort a pandas DataFrame by a column in another dataframe - pandas
	"desc": Let's say I have a Pandas DataFrame with two columns, like: And let's say I also have a Pandas Series, like: How can I sort the column to become the same order as the series, with the corresponding row values sorted together? My desired output would be: Is there any way to achieve this? Please check self-answer below. 
}
"io": {
	"Frame-1": 
		   a    b
		0  1  100
		1  2  200
		2  3  300
		3  4  400
		
	"Frame-2":
		   a    b
		0  1  100
		1  3  300
		2  2  200
		3  4  400
		
}
"answer": {
	"desc": %s I have ran into these issues quite often, so I just thought to share my solutions in Pandas. Solutions: Solution 1: Using to convert the column to the index, then use to change the order, then use to change the index name back to , then use to convert the column from an index back to a column: Solution 2: Using to convert the column to the index, then use to change the order, then use to convert the column from an index back to a column: Solution 3: Using to index the rows in a different order, then use to get that order that would fit the to make it get sorted with the series: Solution 4: Using to create a new DataFrame object, then use with a argument to sort the DataFrame by the series: Timings: Timing with the below code: Output: @Allen has a pretty good answer too. 
	"code-snippets": [
		print(pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns))
		
		----------------------------------------------------------------------
		import pandas as pd
		from timeit import timeit
		df = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [100, 200, 300, 400]})
		s = pd.Series([1, 3, 2, 4])
		def u10_1():
		    return df.set_index('a').reindex(s).rename_axis('a').reset_index('a')
		def u10_2():
		    return df.set_index('a').loc[s].reset_index()
		def u10_3():
		    return df.iloc[list(map(df['a'].tolist().index, s))]
		def u10_4():
		    return pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns)
		print('u10_1:', timeit(u10_1, number=1000))
		print('u10_2:', timeit(u10_2, number=1000))
		print('u10_3:', timeit(u10_3, number=1000))
		print('u10_4:', timeit(u10_4, number=1000))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59829680
"link": https://stackoverflow.com/questions/59829680/split-string-column-based-on-delimiter-and-convert-it-to-dict-in-pandas-without
"question": {
	"title": Split string column based on delimiter and convert it to dict in Pandas without loop
	"desc": I have below dataframe My desired result is I have tried below method This is giving me desired result but I am looking for a more efficient way to convert clm3 to dict which does not involve looping through each row. 
}
"io": {
	"Frame-1": 
		clm1, clm2, clm3
		10, a, clm4=1|clm5=5
		11, b, clm4=2
		
	"Frame-2":
		clm1, clm2, clm4, clm5
		10, a, 1, 5
		11, b, 2, Nan
		
}
"answer": {
	"desc": %s two steps : idea is to create a double split and then group by the index and unstack the values as columns 
	"code-snippets": [
		s = (
		    df["clm3"]
		    .str.split("|", expand=True)
		    .stack()
		    .str.split("=", expand=True)
		    .reset_index(level=1, drop=True)
		)
		
		final = pd.concat([df, s.groupby([s.index, s[0]])[1].sum().unstack()], axis=1).drop(
		    "clm3", axis=1
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59819257
"link": https://stackoverflow.com/questions/59819257/using-np-split-array-and-then-saving-each-split-into-dataframes
"question": {
	"title": Using np.split_array and then saving each split into dataframes
	"desc": Appending data to a dataframe but changing rows after certain # of columns The above is my previous post, where I attempted to convert 1800 row x 1 column dataframe into 300 row x 6 column dataframe through: I would then would like to further split the dataframe into six chunks. I was thinking about using np split like: This line would be added right after (I know the lines won't work if split is applied). For example: The starting data table would look like: and so on (please note that the numbers are just random for this post, and for testing, you can use any floating numbers, these are essentially p-values). The rows are in groups of 50 rows and hence why I would like to separate the 300x6 df into 6 df of 50x6. Because of the data size, I wasn't able to insert all of it and had to express the table as above, but for the actual testing, you can probably generate random values with 300x6 shape df (not counting the headers). what I want is: and so on. I am not sure how I would iterate over each split from then save as separate dataframes. Any help or suggestions would be appreciated. 
}
"io": {
	"Frame-1": 
		col1    col2   col3    col4    col5    col6
		1       0.658  0.1067  0.777   0.459   0.3307
		1       0.622  0.4178  0.3158  0.7674  0.7426
		1       0.622  0.4178  0.3158  0.7674  0.7426
		1       0.622  0.4178  0.3158  0.7674  0.7426
		1       0.622  0.4178  0.3158  0.7674  0.7426
		.
		.
		.
		.
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		.
		.
		.
		
	"Frame-2":
		[df1]
		col1    col2   col3    col4    col5    col6
		1       0.658  0.1067  0.777   0.459   0.3307
		1       0.622  0.4178  0.3158  0.7674  0.7426
		1       0.622  0.4178  0.3158  0.7674  0.7426
		1       0.622  0.4178  0.3158  0.7674  0.7426
		1       0.622  0.4178  0.3158  0.7674  0.7426
		
		[df2]
		col1    col2   col3    col4    col5    col6
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		0.123   1      0.1222  0.111   0.123   0.1234
		
}
"answer": {
	"desc": %s It might depend on how you are wanting to access the data afterwards, but you could make an extra column in the dataframe to assign group labels, and then group the data by this column and create a list of dataframes from that. 
	"code-snippets": [
		import numpy as np
		import pandas as pd
		
		data = np.random.rand(300,6)
		df = pd.DataFrame(data)
		
		df["label"] = df.apply(lambda x: x.name//50, axis=1)
		gb = df.groupby("label")
		df_list = [gb.get_group(x).set_index("label") for x in gb.groups]
		
		----------------------------------------------------------------------
		for x in df_list: # each dataframe should have 50 rows and 6 columns
		    assert x.shape == (50, 6)
		
		----------------------------------------------------------------------
		# print first dataframe head (rows should be same as head printed above)
		df_list[0].head(3) # and access the values/numpy array by df_list[0].values
		
		----------------------------------------------------------------------
		# print last section (rows should be same as tail printed above)
		df_list[5].tail(3) # and access the values/numpy array by df_list[5].values
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59786133
"link": https://stackoverflow.com/questions/59786133/setting-subset-of-a-pandas-dataframe-by-a-dataframe-if-a-value-matches
"question": {
	"title": Setting subset of a pandas DataFrame by a DataFrame if a value matches
	"desc": I think the easiest way to explain what I am trying to do is by showing an example: Given a DataFrame and a subset of a second DataFrame : I want to replace the NaN's from the second DataFrame by the first, BUT at the place where the ID matches, as I am not sure that the selected data will always be in the same order or if all IDs will be included. I know I could do it with a for and if loop, but I am wondering if there is a faster way. If an ID form the second DataFrame is not included in the first DataFrame the values should just stay as NaN's. Any help is highly appreciated. 
}
"io": {
	"Frame-1": 
		        V_set   V_reset     I_set   I_reset           HRS          LRS      ID
		0     0.599417 -0.658417  0.000021 -0.000606  84562.252849  1097.226787  1383.0
		1     0.595250 -0.684708  0.000023 -0.000617  43234.544776  1144.445368  1384.0
		2     0.621229 -0.710812  0.000026 -0.000625  51719.718749  1216.609759  1385.0
		3     0.625292 -0.720104  0.000029 -0.000625  40827.993527  1209.966052  1386.0
		4     0.634563 -0.735937  0.000029 -0.000641  46881.785573  1219.497465  1387.0
		       ...       ...       ...       ...           ...          ...     ...
		1066  0.167521  0.000000  0.000581  0.000000    720.116614   708.098519  2811.0
		1067  0.167360  0.000000  0.000581  0.000000    718.165882   708.284487  2812.0
		1068  0.172812  0.000000  0.000278  0.000000    715.302620   708.167571  2813.0
		1069  0.167729  0.000000  0.000581  0.000000    716.096291   708.333064  2814.0
		1070  0.173037  0.000000  0.000278  0.000000    715.474310   707.980273  2815.0
		
	"Frame-2":
		        V_set   V_reset     I_set   I_reset           HRS          LRS      ID
		1383       NaN       NaN       NaN       NaN           NaN          NaN  1383.0     
		1384       NaN       NaN       NaN       NaN           NaN          NaN  1384.0 
		1385       NaN       NaN       NaN       NaN           NaN          NaN  1385.0
		1386       NaN       NaN       NaN       NaN           NaN          NaN  1386.0
		1387       NaN       NaN       NaN       NaN           NaN          NaN  1387.0
		       ...       ...       ...       ...           ...          ...     ...
		2811       NaN       NaN       NaN       NaN           NaN          NaN  2811.0
		2812       NaN       NaN       NaN       NaN           NaN          NaN  2812.0
		2813       NaN       NaN       NaN       NaN           NaN          NaN  2813.0
		2814       NaN       NaN       NaN       NaN           NaN          NaN  2814.0
		2815       NaN       NaN       NaN       NaN           NaN          NaN  2815.0
		
}
"answer": {
	"desc": %s IIUC, you have common column names and want to replace NaN values with values from your first df. here's a solution using and this will work if your ID's have a 1 to 1 relationship. if you want to fill the entire dataframe and your keys are unique - you can set both ID's as the index and use 
	"code-snippets": [
		df.set_index('ID',inplace=True)
		for column in df.columns:
		    df2[column] = df2[column].fillna(df2['ID'].map(df[column]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59767001
"link": https://stackoverflow.com/questions/59767001/save-in-dataframe-unique-values-for-every-column
"question": {
	"title": Save in DataFrame unique values for every column
	"desc": If I have a data (df) like this: With the next fuction: It returns something like: How can I save the return of the fuction in a DataFrame?, I would like to see it like this: Thanks you ! 
}
"io": {
	"Frame-1": 
		X1 X2 X3 
		A  A  C
		B  A  C
		C  B  C
		
	"Frame-2":
		X1 X2 X3 
		A  A  C
		B  B  
		C    
		
}
"answer": {
	"desc": %s Use lambda function with constructor and then repalce missing values: Or use : 
	"code-snippets": [
		df1 = df.apply(lambda x: pd.Series(pd.unique(x))).fillna('')
		
		----------------------------------------------------------------------
		df1 = df.apply(lambda x: x.drop_duplicates().reset_index(drop=True)).fillna('')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59760868
"link": https://stackoverflow.com/questions/59760868/replace-outliers-with-median-exept-nan
"question": {
	"title": Replace outliers with median exept NaN
	"desc": I would like to replace outliers with median in a dataframe but only outliers and not NaN. First : I would like to replace the -60 which is an outlier with the median using : It works fine but it also delete all rows containing a NaN how can I avoid that ? Output : As you can see, 3 rows have been deleted which is not very convenient. Any ideas ? Thanks ! 
}
"io": {
	"Frame-1": 
		      January  February 
		0      -5.0     -7.0 
		1      -6.0     -6.0 
		2      -5.0     -5.0  
		3      -3.0     -6.0 
		4      -6.0     -8.0   
		5     -11.0     -9.0    
		6      -6.0      5.0    
		7      -8.0    -11.0  
		8     -11.0    -12.0  
		9      -8.0     -9.0     
		10     -8.0     -6.0   
		11     -8.0     -5.0    
		12     -8.0     -4.0   
		13    -10.0      1.0    
		14    -10.0      3.0   
		15     -9.0     -9.0    
		16     -6.0     -6.0   
		17     -6.0     -6.0   
		18     -4.0     -4.0  
		19     -8.0      2.0    
		20     -9.0      3.0      
		21    -14.0      1.0 
		22    -15.0     -3.0  
		23    -17.0     -4.0   
		24    -19.0     -6.0     
		25    -60.0     -8.0   
		26     -8.0     -8.0   
		27     -9.0    -11.0    
		28     -5.0      NaN    
		29     -6.0      NaN    
		30     -7.0      NaN 
		
	"Frame-2":
		  January  February 
		0      -5.0     -7.0 
		1      -6.0     -6.0 
		2      -5.0     -5.0  
		3      -3.0     -6.0 
		4      -6.0     -8.0   
		5     -11.0     -9.0    
		6      -6.0      5.0    
		7      -8.0    -11.0  
		8     -11.0    -12.0  
		9      -8.0     -9.0     
		10     -8.0     -6.0   
		11     -8.0     -5.0    
		12     -8.0     -4.0   
		13    -10.0      1.0    
		14    -10.0      3.0   
		15     -9.0     -9.0    
		16     -6.0     -6.0   
		17     -6.0     -6.0   
		18     -4.0     -4.0  
		19     -8.0      2.0    
		20     -9.0      3.0      
		21    -14.0      1.0 
		22    -15.0     -3.0  
		23    -17.0     -4.0   
		24    -19.0     -6.0     
		25    -10.0     -8.0   
		26     -8.0     -8.0   
		27     -9.0    -11.0
		
}
"answer": {
	"desc": %s You could use in your logic: Prints (notice index 25 () is missing: 
	"code-snippets": [
		df = df[df.apply(lambda x: (np.abs(x - x.mean()) / x.std() < 4) | x.isna()).all(axis=1)]
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59754548
"link": https://stackoverflow.com/questions/59754548/is-there-a-way-to-replace-each-cell-value-in-a-dataframe-with-the-column-name-r
"question": {
	"title": Is there a way to replace each cell value in a dataframe with the column name, row value in the first column and the value itself?
	"desc": I have a matrix in excel which I am reading in as a pandas dataframe in python I want to be able to concatenate the column name, cell values from the first column and the current value of the cell for all cells in columns greater than col1. I essentially want the following output: I could not figure out a way to do this in python. 
}
"io": {
	"Frame-1": 
		 col1   col2   col3    
		 C_0    a      f 
		 C_1    b      g
		 C_2    c      h
		 C_3    d      i
		 C_4    e      j
		
	"Frame-2":
		 col1   col2            col3    
		 C_0    col2_C_0_a      col3_C_0_f 
		 C_1    col2_C_1_b      col3_C_1_g
		 C_2    col2_C_2_c      col3_C_2_h
		 C_3    col2_C_3_d      col3_C_3_i
		 C_4    col2_C_4_e      col3_C_4_j
		
}
"answer": {
	"desc": %s Using : 
	"code-snippets": [
		m = df.astype(str).set_index('col1')
		m.radd(m.index+'_',axis=0).radd(m.columns + '_').reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59673066
"link": https://stackoverflow.com/questions/59673066/most-efficient-way-to-multiply-every-column-of-a-large-pandas-dataframe-with-eve
"question": {
	"title": Most efficient way to multiply every column of a large pandas dataframe with every other column of the same dataframe
	"desc": Suppose I have a dataset that looks something like: I want to get a dataframe that looks like the following, with the original columns, and all possible interactions between columns: My actual datasets are pretty large (~100 columns). What is the fastest way to achieve this? I could, of course, do a nested loop, or similar, to achieve this but I was hoping there is a more efficient way. 
}
"io": {
	"Frame-1": 
		INDEX   A   B   C
		    1   1   1   0.75
		    2   1   1   1
		    3   1   0   0.35
		    4   0   0   1
		    5   1   1   0
		
	"Frame-2":
		INDEX   A   B   C       A_B     A_C     B_C
		    1   1   1   0.75    1       0.75    0.75
		    2   1   1   1       1       1       1
		    3   1   0   0.35    0       0.35    0
		    4   0   0   1       0       0       0
		    5   1   1   0       1       0       0
		
}
"answer": {
	"desc": %s You could use itertools.combinations for this: If you need to vectorize an arbitrary function on the pairs of columns you could use: 
	"code-snippets": [
		import numpy as np
		
		def fx(x, y):
		    return np.multiply(x, y)
		
		for col1, col2 in combinations(df.columns, 2):
		    df[f"{col1}_{col2}"] = np.vectorize(fx)(df[col1], df[col2])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50121789
"link": https://stackoverflow.com/questions/50121789/pandas-conditionally-creating-a-new-dataframe-using-another
"question": {
	"title": Pandas conditionally creating a new dataframe using another
	"desc": I have a list; I want to create another where entries corresponding to positive values above are sum of positives, and those corresponding to negative values above are sum negatives. So the desired output is: I am doing this: So basically i was trying to create an empty dataframe like raw, and then conditionally fill it. However, the above method is failing. Even when i try to create a new column instead of a new df, it fails: The best solution I've found so far is this: However, I dont understand what is wrong with the other approaches. Is there something I am missing here? 
}
"io": {
	"Frame-1": 
		orig= [2, 3, 4, -5, -6, -7]
		
	"Frame-2":
		final = [9, 9, 9, 18, 18, 18]
		
}
"answer": {
	"desc": %s You can try grouping on , then and : Output: You're not aligning indexes. 'sum_pos' is a series with a single element that has an index of 'raw'. And, you are trying to assign that series to a part of dataframe that doesn't have 'raw' as an index. Pandas does almost everything using index alignment. To properly do this you need to extract the values from the sum_pos series: Output: 
	"code-snippets": [
		final.loc[raw['raw'] > 0, 'final'] = sum_pos.values
		
		print(final)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59645274
"link": https://stackoverflow.com/questions/59645274/pandas-split-dataframe-according-to-sorted-sequence-in-columns
"question": {
	"title": Pandas - split dataframe according to sorted sequence in columns
	"desc": I have a pandas dataframe with this type of structure: Basically, I sort the data frame according to the values of val1 and val2 beforehand, so I know I'll have two ascending sequences afterwards. What I want is to split this df in two new dfs, according to the two sequences, which in my example would be this: I have checked this question and this, but I don't know the number of values/rows beforehand... I've also checked another question, so I thought about using split with a regular expression. But I only know the sequences will be ascending, there's no guarantee that the values will be continuous, so it doesn't work as expected. Is this possible to achieve? I appreciate in advance any help! 
}
"io": {
	"Frame-1": 
		df
		Val1 Val2 Col1 Col2
		1    1    0    3
		1    2    2    4
		2    1    2    3
		3    2    2    5
		1    2    3    4
		2    1    3    1
		3    4    2    1
		
	"Frame-2":
		df1
		Val1 Val2 Col1 Col2
		1    1    0    3
		1    2    2    4
		2    1    2    3
		3    2    2    5
		
		df2
		Val1 Val2 Col1 Col2
		1    2    3    4
		2    1    3    1
		3    4    2    1
		
}
"answer": {
	"desc": %s Using and : Now we have each df in a list, we can access them: 
	"code-snippets": [
		m = df['Val1'].shift() > df['Val1']
		dfs = [df for _, df in df.groupby(m.cumsum())]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59617019
"link": https://stackoverflow.com/questions/59617019/replace-comma-separated-values-in-a-dataframe-with-values-from-another-dataframe
"question": {
	"title": Replace comma-separated values in a dataframe with values from another dataframe
	"desc": this is my first question on StackOverflow, so please pardon if I am not clear enough. I usually find my answers here but this time I had no luck. Maybe I am being dense, but here we go. I have two pandas dataframes formatted as follows df1 df2 Basically, Ref_ID in df2 contains IDs that form the string contained in the field References in df1 What I would like to do is to replace values in the References field in df1 so it looks like this: So far, I had to deal with columns and IDs with a 1-1 relationship, and this works perfectly Pandas - Replacing Values by Looking Up in an Another Dataframe But I cannot get my mind around this slightly different problem. The only solution I could think of is to re-iterate a for and if cycles that compare every string of df1 to df2 and make the substitution. This would be, I am afraid, very slow as I have ca. 2000 unique Ref_IDs and I have to repeat this operation in several columns similar to the References one. Anyone is willing to point me in the right direction? Many thanks in advance. 
}
"io": {
	"Frame-1": 
		+------------+-------------+
		| References | Description |
		+------------+-------------+
		| 1,2        | Descr 1     |
		| 3          | Descr 2     |
		| 2,3,5      | Descr 3     |
		+------------+-------------+
		
	"Frame-2":
		+-------------------------------------+-------------+
		|             References              | Description |
		+-------------------------------------+-------------+
		| Smith (2006); Mike (2009)           | Descr 1     |
		| John (2014)                         | Descr 2     |
		| Mike (2009);John (2014);Jill (2019) | Descr 3     |
		+-------------------------------------+-------------+
		
}
"answer": {
	"desc": %s Let's try this: Output: @Datanovice thanks for the update. Output: 
	"code-snippets": [
		df1 = pd.DataFrame({'Reference':['1,2','3','1,3,5'], 'Description':['Descr 1', 'Descr 2', 'Descr 3']})
		df2 = pd.DataFrame({'Ref_ID':[1,2,3,4,5,6], 'ShortRef':['Smith (2006)',
		                                                       'Mike (2009)',
		                                                       'John (2014)',
		                                                       'Cole (2007)',
		                                                       'Jill (2019)',
		                                                       'Tom (2007)']})
		
		df1['Reference2'] = (df1['Reference'].str.split(',')
		                                     .explode()
		                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))
		                                             .set_index('Ref_ID')['ShortRef'])
		                                     .groupby(level=0).agg(list))
		
		----------------------------------------------------------------------
		df1['Reference2'] = (df1['Reference'].str.split(',')
		                                     .explode()
		                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))
		                                             .set_index('Ref_ID')['ShortRef'])
		                                     .groupby(level=0).agg(';'.join))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59594392
"link": https://stackoverflow.com/questions/59594392/replace-negatives-with-zeros-in-a-dataframe-column-of-lists
"question": {
	"title": Replace negatives with zeros in a dataframe column of lists
	"desc": I have a dataframe containing two columns. The first column is the date index. Each row of the second column is a list of 60 numbers that include negative values. I want to replace all negative values in this column with zeros. Here is the complete data for the first two rows: Currently, my solution is to convert the column of lists into a separate df of 60 columns. I can then convert the negatives into zeros in this df. Although this does the job, the .apply() operation is slow (taking 1.3 minutes for a df with 400,000 rows). Could someone please offer a more efficient (faster) alternative? 
}
"io": {
	"Frame-1": 
		                    Spc
		1976-10-31 15:00:00 [0.0124, 0.0096, 0.0325, 0.1562, 0.4494, 0.738...-1., -1., -1., -1.]
		1976-11-01 03:00:00 [0.0254, 0.0299, 0.0273, 0.1229, 0.596, 0.9833...-1., -1., -1., -1.]
		1976-11-01 15:00:00 [0.0226, 0.0236, 0.0269, 0.085, 0.4163, 0.8011...-1., -1., -1., -1.]
		1976-11-02 03:00:00 [0.0132, 0.0154, 0.0172, 0.1336, 0.4743, 0.694...-1., -1., -1., -1.]
		1976-11-02 15:00:00 [0.0124, 0.0169, 0.028, 0.5028, 1.4503, 1.6055...-1., -1., -1., -1.]
		     :     :     :     :     :     :     :     :     :     :
		2017-05-20 04:00:00 [5.374061e-13, 1.2720002e-06, 0.00052255474, 0...2.8157034e-03, 1.4578120e-03]
		2017-05-20 04:30:00 [1.2021946e-12, 3.3477074e-06, 0.0014435094, 0...5.88221522e-03, 3.44922021e-03]
		2017-05-20 05:00:00 [1.2236685e-13, 5.018357e-07, 0.00023753957, 0...2.28277827e-03, 1.07194704e-03]
		2017-05-20 05:30:00 [3.5527579e-13, 1.1004944e-06, 0.0005480177, 0...2.0632602e-03, 1.6171171e-03]
		2017-05-20 06:00:00 [4.968573e-13, 1.4969078e-06, 0.00065009575, 0...1.21051911e-03, 1.18123344e-03]
		
	"Frame-2":
		1976-10-31 15:00:00 [ 0.0013,  0.0016,  0.007,   0.03,    0.0803,  0.2318,  0.5842,  0.8401,  0.6,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,
		 -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]
		1976-11-01 03:00:00 [ 0.0022,  0.004,   0.0104,  0.0512,  0.1112,  0.2227,  0.5263,  0.7085,  0.4,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,      0.,
		  0.,      0.,      0.,      0.,      0.,      0.,     -1.,     -1.,     -1.,
		 -1.,     -1.,     -1.,     -1.,     -1.,     -1.    ]
		
}
"answer": {
	"desc": %s Here are a few solutions, with some basic timings. Setup code, and test data: 1. : ~18,400 ms 2. List comprehension : ~460 ms 3. Solution by @Valdi_Bo : ~832 ms 4. Solution similar in principle to the one by @Valdi_Bo : ~926 ms 5. A cousin of 3 and 4 : ~691 ms Note: The relative speed differences are maintained on an input with 10 times the amount of rows (1,000,000 instead of 100,000). 
	"code-snippets": [
		def func_1(df_in):
		    df_in = df_in.copy()
		    temp = df_in['col_2'].apply(pd.Series)
		    temp[temp < 0] = 0
		    df_in['col_2'] = temp.to_numpy().tolist()
		    return df_in
		
		----------------------------------------------------------------------
		def func_2(df_in):
		    df_in = df_in.copy()
		    df_in['col_2'] = df_in['col_2'].map(lambda l: [0 if elem < 0 else elem for elem in l])
		    return df_in
		
		----------------------------------------------------------------------
		def func_3(df_in):
		    df_in = df_in.copy()
		    df_in['col_2'] = np.array([np.array(elem) for elem in df_in['col_2']]).clip(min=0).tolist()
		    return df_in
		
		----------------------------------------------------------------------
		def func_4(df_in):
		    df_in = df_in.copy()
		    df_in['col_2'] = np.stack(df_in['col_2'].to_numpy()).clip(min=0).tolist()
		    return df_in
		
		----------------------------------------------------------------------
		def func_5(df_in):
		    df_in = df_in.copy()
		    df_in['col_2'] = np.array(df_in['col_2'].tolist()).clip(min=0).tolist()
		    return df_in
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59589861
"link": https://stackoverflow.com/questions/59589861/pandas-group-by-the-column-values-with-all-values-less-than-certain-numbers-and
"question": {
	"title": pandas group by the column values with all values less than certain numbers and assign the group numbers as new columns
	"desc": I have a data frame like this, Now I want to create another column col3 with grouping all the col2 values which are under below 5 and keep col3 values as 1 to number of groups, so the final data frame would look like, I could do this comparing the the prev values with the current values and store into a list and make it the col3. But the execution time will be huge in this case, so looking for some shortcuts/pythonic way to do it most efficiently. 
}
"io": {
	"Frame-1": 
		df
		col1    col2
		 A        2
		 B        3
		 C        1
		 D        4
		 E        6
		 F        1
		 G        2
		 H        8
		 I        1
		 J       10
		
	"Frame-2":
		col1    col2     col3
		 A        2        1
		 B        3        1
		 C        1        1
		 D        4        1
		 E        6        2
		 F        1        2
		 G        2        2
		 H        8        3
		 I        1        3
		 J       10        4
		
}
"answer": {
	"desc": %s Compare by for and then use . New column always starts by , because first values of column is less like , else it should be : So for general solution starting by use this trick - compare first values if less like , convert to integers for and and add to column: 
	"code-snippets": [
		N = 5
		df['col3'] = df['col2'].gt(N).cumsum() + int(df.loc[0, 'col2'] < 5)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59548017
"link": https://stackoverflow.com/questions/59548017/create-a-list-in-list-comprehension-and-then-create-another-list-from-that-list
"question": {
	"title": Create a list in list comprehension and then create another list from that list inside the same list comprehension
	"desc": That title is a mouthful, so it may be easier to show what I am trying to achieve via code. The above is not where I am having trouble with, but I wanted to provide as much context as possible. I am trying to iterate through the dataframe and retrieve the first element of the column name where the dataframe's element equals 1. I can do this using the following: This generates the first list I need to create: I can assign that output to a variable and perform another list comprehension to get my final output: This outputs my final list of: Is it possible to perform both of these inside the same list comprehension while only receiving that final list as the output? This is somewhat inelegant, but this is what it would look like in non-list comprehension: Thank you for taking the time to look this over! 
}
"io": {
	"Frame-1": 
		[1, 1, 1, 2, 3, 4]
		
	"Frame-2":
		[(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)]
		
}
"answer": {
	"desc": %s you can use a product with a conditional statement , then zip and extract a tuple: Output: Full Working Code: 
	"code-snippets": [
		[(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(df.columns.str[0])[1:])]
		#same with .iloc -> [(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(
		#                                          df.columns.str[0]).iloc[1:])]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59533480
"link": https://stackoverflow.com/questions/59533480/add-column-to-groups-on-dataframe-pandas
"question": {
	"title": add column to groups on dataframe pandas
	"desc": I have a dataframe: How can I add a new column to dataframe relative to the id column? for example: 
}
"io": {
	"Frame-1": 
		  id  |   x   |   y 
		   1  |  0.3  |  0.4
		   1  |  0.2  |  0.5
		   2  |  0.1  |  0.6
		   2  |  0.9  |  0.1
		   3  |  0.8  |  0.2
		   3  |  0.7  |  0.3
		
	"Frame-2":
		  id  |   x   |   y   |  color
		   1  |  0.3  |  0.4  | 'green'
		   1  |  0.2  |  0.5  | 'green'
		   2  |  0.1  |  0.6  | 'black'
		   2  |  0.9  |  0.1  | 'black'
		   3  |  0.8  |  0.2  |  'red'
		   3  |  0.7  |  0.3  |  'red'
		
}
"answer": {
	"desc": %s So your function doesn't return color names but instead the RGB values, if this it what you want in the color column build the dictionary first from the unique id values and apply the dictionary the way @anky_91 mentioned in the comments. 
	"code-snippets": [
		d={x:random_color() for x in df.id.unique()}
		df['color']=df['id'].map(d)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59505475
"link": https://stackoverflow.com/questions/59505475/pandas-swap-rows-between-columns
"question": {
	"title": Pandas: Swap rows between columns
	"desc": Some rows were input in the wrong columns so now I need to swap them. My current approach Expected output It works but this is only possible because it was 4 columns. Is there a better way to do this? Note the dtypes can cause issues with sorting is Maybe there is something like 
}
"io": {
	"Frame-1": 
		          a         b         c         d
		0         0        10  22:58:00  23:27:00
		1        10        17  23:03:00  23:39:00
		2  22:58:00  23:27:00         0        10
		3  23:03:00  23:39:00        10        17
		
	"Frame-2":
		    a   b         c         d
		0   0  10  22:58:00  23:27:00
		1  10  17  23:03:00  23:39:00
		2   0  10  22:58:00  23:27:00
		3  10  17  23:03:00  23:39:00
		
}
"answer": {
	"desc": %s For swapping and separating and in your sample, you may use , and numpy indexing (Note: your numbers in sample are in string format, so I check type ) If you get replace with 
	"code-snippets": [
		AttributeError: 'DataFrame' object has no attribute 'to_numpy'
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59502905
"link": https://stackoverflow.com/questions/59502905/calculate-percentage-of-similar-values-in-pandas-dataframe
"question": {
	"title": Calculate percentage of similar values in pandas dataframe
	"desc": I have one dataframe , with two columns : Script (with text) and Speaker And I have the following list : With the following code, I obtain this dataframe : Which line can I add in my code to obtain, for each line of my dataframe , a percentage value of all lines spoken by speaker, in order to have the following dataframe : 
}
"io": {
	"Frame-1": 
		Speaker     a    b    c
		Speaker 1   2    1    1
		Speaker 2   2    0    0
		Speaker 3   0    1    0
		
	"Frame-2":
		Speaker     a    b    c
		Speaker 1   50%  25%   25%
		Speaker 2  100%    0   0
		Speaker 3   0   100%   0
		
}
"answer": {
	"desc": %s You could divide by the along the first axis and then cast to string and add : 
	"code-snippets": [
		out = (df.set_index('Speaker')['Script'].str.findall('|'.join(L))
		         .str.join('|')
		         .str.get_dummies()
		         .sum(level=0))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59482469
"link": https://stackoverflow.com/questions/59482469/python-convert-two-columns-of-dataframe-into-one-interposed-list
"question": {
	"title": Python: Convert two columns of dataframe into one interposed list
	"desc": How can I convert two columns in a dataframe into an interposed list? ex: I want to do something like Closest I've found is but that returns a bunch of tuples in the list like this: 
}
"io": {
	"Frame-1": 
		>>> [1, 2, 3, 4, 5, 6, 0, -1]
	"Frame-2":
		[(1, 2), (3, 4), (5, 6), (0, -1)]
}
"answer": {
	"desc": %s Try this: 
	"code-snippets": [
		df.values.flatten()                                                                                                                                                                  
		# array([ 1,  2,  3,  4,  5,  6,  0, -1])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 47872271
"link": https://stackoverflow.com/questions/47872271/iterative-comparison-with-pandas
"question": {
	"title": Iterative comparison with pandas
	"desc": I don't know to approach this issue. I have a data frame that looks like this What I need to do is to iterate between each row per usuario_id and check if there's a difference between each row, and create a new data set with the row changed and the usuario_web in charge of this change, to generate a data frame that looks like this: Is there any way to do this? I'm working with pandas on python and this dataset could be a little big, let's say around 10000 rows, sorted by usuario_id. Thanks for any advice. 
}
"io": {
	"Frame-1": 
		cuenta_bancaria nombre_empresa  perfil_cobranza  usuario_id  usuario_web 
		5545              a              123              500199         5012
		5551              a              123              500199         3321
		5551              a               55              500199         5541
		5551              b               55              500199         5246
		
	"Frame-2":
		usuario_id     cambio           usuario_web
		 500199       cuenta_bancaria    3321
		 500199       perfil_cobranza    5541
		 500199       nombre_empresa     5246
		
}
"answer": {
	"desc": %s Compare adjacent rows with + , obtain a mask, and use this to index into to get the required rows index into to get the required columns which change 
	"code-snippets": [
		c = df.columns.intersection(
		        ['nombre_empresa', 'perfil_cobranza', 'cuenta_bancaria']
		)
		
		i = df[c].ne(df[c].shift())
		j = i.sum(1).eq(1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56462088
"link": https://stackoverflow.com/questions/56462088/why-doesnt-pandas-reindex-operate-in-place
"question": {
	"title": Why doesn&#39;t pandas reindex() operate in-place?
	"desc": From the reindex docs: Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Therefore, I thought that I would get a reordered by setting in place (!). It appears, however, that I do get a copy and need to assign it to the original object again. I don't want to assign it back, if I can avoid it (the reason comes from this other question). This is what I am doing: Outs: Reindex gives me the correct output, but I'd need to assign it back to the original object, which is what I wanted to avoid by using : The desired output after that line is: Why is not working in place? Is it possible to do that at all? Working with python 3.5.3, pandas 0.23.3 
}
"io": {
	"Frame-1": 
		          a         b         c         d         e
		0  0.234296  0.011235  0.664617  0.983243  0.177639
		1  0.378308  0.659315  0.949093  0.872945  0.383024
		2  0.976728  0.419274  0.993282  0.668539  0.970228
		3  0.322936  0.555642  0.862659  0.134570  0.675897
		4  0.167638  0.578831  0.141339  0.232592  0.976057
		
	"Frame-2":
		          e         d         c         b         a
		0  0.177639  0.983243  0.664617  0.011235  0.234296
		1  0.383024  0.872945  0.949093  0.659315  0.378308
		2  0.970228  0.668539  0.993282  0.419274  0.976728
		3  0.675897  0.134570  0.862659  0.555642  0.322936
		4  0.976057  0.232592  0.141339  0.578831  0.167638
		
}
"answer": {
	"desc": %s  is a structural change, not a cosmetic or transformative one. As such, a copy is always returned because the operation cannot be done in-place (it would require allocating new memory for underlying arrays, etc). This means you have to assign the result back, there's no other choice. Also see the discussion on GH21598. The one corner case where is actually of any use is when the indices used to reindex are identical to the ones it already has. You can check by comparing the ids: 
	"code-snippets": [
		df = df.reindex(['e', 'd', 'c', 'b', 'a'], axis=1)  
		
		----------------------------------------------------------------------
		id(df)
		# 4839372504
		
		id(df.reindex(df.index, copy=False)) # same object returned 
		# 4839372504
		
		id(df.reindex(df.index, copy=True))  # new object created - ids are different
		# 4839371608  
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59371013
"link": https://stackoverflow.com/questions/59371013/pandas-how-to-return-the-rows-where-col-value-is-greater-than-x-in-rolling-wi
"question": {
	"title": Pandas: How to return the rows where col value is greater than &#39;x&#39; in rolling window
	"desc": I have a large df and I am trying find all rows where the value in a specific column is above a given number but within a window of say 3 rows and returning only the rows with the highest value over the given number. If I wanted to do this with the above example for column D, where the value must be above 11, the output would be. What would be the best way to go about this? I've tried: but can't find a way to include the greater than condition. Any help is appreciatted. Thanks! 
}
"io": {
	"Frame-1": 
		A    B    C    D    E
		1    5    9    10   15
		2    4    7    12   16
		3    3    5    10   18
		4    2    3    15   17
		5    1    1    10   14
		6    5    9    17   13
		7    4    7    10   14
		8    3    5    19   19
		9    2    3    10   18
		10   4    7    5   14
		11   3    5    6   19
		12   2    3    7   18
		
	"Frame-2":
		A    B    C    D    E
		2    4    7    12   16
		6    5    9    17   13
		8    3    5    19   19.
		
}
"answer": {
	"desc": %s Edited: Try this: You can drop column 'r' after usage. Output: 
	"code-snippets": [
		threshold = 11
		window = 3
		df['r'] = np.floor(df.index / window)
		print(df.groupby('r').apply(lambda x : (x.loc[x['D'] == x['D'].max() ,:]) if x['D'].max() > threshold else None))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59345178
"link": https://stackoverflow.com/questions/59345178/symmetrical-column-values-in-pandas-data-frame
"question": {
	"title": Symmetrical column values in pandas data frame
	"desc": I have one set of variable as in below data frame: Another set of variable in below data frame: 1st columns are index columns. I want to add each row (v1+v2) to get v3. How do I make the index column values (0 to 4) and (41 to 45) symmetrical ( either 0-4) or (42-45) in both data fame? I am working on pandas (python) jupyter notebook. 
}
"io": {
	"Frame-1": 
		    v1
		----------
		0   0.036286
		
		1  -0.018490
		
		2   0.011699
		
		3   0.028955
		
		4  -0.000373
		
	"Frame-2":
		      v2
		----------
		41    12.31
		
		42    12.20
		
		43    12.12
		
		44    12.31
		
		45    12.47
		
}
"answer": {
	"desc": %s You have multiple options here I think: You can directly concatenate them using using the parameter and then create the new column: You can create a new vector/dataframe with the result: which then you can manipulate as you prefer. All other solutions people already proposed. 
	"code-snippets": [
		df = pd.concat(df1, df2, axis=1, ignore_index=True)
		df['v3'] = df.sum(axis=1) # or df[['v1','v2']].sum(axis=1)
		
		----------------------------------------------------------------------
		v3 = df1['v1'].values + df2['v2'].values
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59284049
"link": https://stackoverflow.com/questions/59284049/how-to-concat-2-columns-in-single-column-with-column-value-check
"question": {
	"title": How to Concat 2 columns in single column with column value check
	"desc": I want to concat two column from data frame where Column1 not equals to ANY: DataFrame : as a result I want dataframe as follows ANY is variable, could represent Null, EmptyString, String, Number. Thanks. 
}
"io": {
	"Frame-1": 
		   COLUMN1 | COLUMN2
		0     A    |   FOO
		1     B    |   BAR  
		2    ANY   |   FOO
		3    ANY   |   BAR
		4     C    |   FOO
		
	"Frame-2":
		   COLUMN1 | COLUMN2
		0     A    |  FOO_A
		1     B    |  BAR_B
		2    ANY   |  FOO
		3    ANY   |  BAR  
		4     C    |  FOO_C
		
}
"answer": {
	"desc": %s You can do 
	"code-snippets": [
		df['COLUMN2']=df.apply(lambda row:row['COLUMN2']+'_'+row['COLUMN1'] if row['COLUMN1']!='ANY' else row['COLUMN2'],axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59233282
"link": https://stackoverflow.com/questions/59233282/jupyter-pandas-dropping-items-which-have-average-over-a-threshold
"question": {
	"title": Jupyter Pandas - dropping items which have average over a threshold
	"desc": I have a data frame with items and their prices, something like this: I want to exclude all rows from this df where the item has an average price over 200. So filtered df should look like this: I'm new to python and pandas but as a first step was thinking something like this to get a new df for avg prices: avg_prices_df = df.groupby('ItemID').Price.mean().reset_index and then not sure how to proceed from there. Not sure even that first step is correct. To further complicate the matter, I am using vaex to read the data in ndf5 form as I have over 400 million rows. Many thanks in advance for any advice. EDIT: So I got the following code working, though I am sure it is not optimised.. ` create dataframe of ItemIDs and their average prices df_item_avg_price = df.groupby(df.ItemID, agg=[vaex.agg.count('ItemID'), vaex.agg.mean('Price')]) filter this new dataframe by average price threshold df_item_avg_price = (df_item_avg_price[df_item_avg_price["P_r_i_c_e_mean"] <= 50000000]) create list of ItemIDs which have average price under the threshold items_in_price_range = df_item_avg_price['ItemID'].tolist() filter the original dataframe to include rows only with the items in price range filtered_df = df[df.ItemID.isin(items_in_price_range)] ` Any better way to do this? 
}
"io": {
	"Frame-1": 
		
		
		 Item  Day  Price 
		
		 A       1     10 
		 B       1     20 
		 C       1     30 
		 D       1     40 
		 A       2    100 
		 B       2     20 
		 C       2     30 
		 D       2     40 
		 A       3    500 
		 B       3     25 
		 C       3     35 
		 D       3   1000 
		
	"Frame-2":
		
		
		 Item  Day  Price 
		
		 B       1     20 
		 C       1     30 
		 B       2     20 
		 C       2     30 
		 B       3     25 
		 C       3     35 
		
}
"answer": {
	"desc": %s Use for s per groups with same size like original, so possible filter out by all groups with means less like : Another solution with : Solution for vaex: 
	"code-snippets": [
		avg_prices_df = df[df.groupby('Item')['Price'].transform('mean') < 200]
		
		----------------------------------------------------------------------
		avg_prices_df = df.groupby('Item').filter(lambda x: x['Price'].mean() < 200)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59281644
"link": https://stackoverflow.com/questions/59281644/groupby-sum-from-occurance-of-a-particular-value-till-the-occurance-of-another
"question": {
	"title": Groupby &amp; Sum from occurance of a particular value till the occurance of another particular value or the same value
	"desc": I have a dataframe as below. I want to 'user' & 'eve' and 'Ses' till 100/200 & from 100 to 200. Also, return the value of column 'Name' where 100/200 occurs. If after an hundred, there is no 100 or 200 (like last row in group a & 123 or a & 456), ignore it. The expected output for the above input df is a df below. 
}
"io": {
	"Frame-1": 
		User    eve Ses ID  Name
		a   123 1   10  a
		a   123 2   11  a
		a   123 3   12  a
		a   123 4   13  a
		a   123 3   100 xyz
		a   123 6   10  a
		a   456 1   11  a
		a   456 2   12  a
		a   456 3   13  a
		a   456 4   40  a
		a   456 1   100 mno
		a   456 14  10  a
		a   456 7   20  a
		a   456 8   30  a
		a   456 12  200 pqr
		a   456 10  10  a
		b   123 1   20  a
		b   123 2   30  a
		b   123 3   40  a
		b   123 4   50  a
		b   123 1   70  a
		b   123 6   100 abc
		b   888 1   20  a
		b   888 1   200 jkl
		b   888 3   10  a
		b   888 4   20  a
		b   888 5   30  a
		b   888 1   100 rrr
		b   888 7   50  a
		b   888 8   70  a
		
	"Frame-2":
		User    eve Ses Name
		a   123 13  xyz
		a   456 11  mno
		a   456 41  pqr
		b   123 17  abc
		b   888 2   jkl
		b   888 13  rrr
		
}
"answer": {
	"desc": %s This is my approach: Output: 
	"code-snippets": [
		# valid IDs
		df['valids'] = df['ID'].isin([100,200])
		
		# mask the trailing non-hundred ids
		heads = (df['ID'].where(df['valids'])
		             .groupby([df['User'],df['eve']])
		             .bfill().notnull()
		        )
		df = df[heads]
		
		# groupby and output:
		(df.groupby(['User','eve', df['valids'].shift(fill_value=0).cumsum()],
		           as_index=False)
		   .agg({'Ses':'sum', 'Name':'last'})
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59254205
"link": https://stackoverflow.com/questions/59254205/how-to-append-a-list-in-pandas
"question": {
	"title": How to append a list in Pandas?
	"desc": I'm reading a dataframe and trying to insert a list inside another list and then converting it to json file. I'm using python 3 and 0.25.3 version of pandas for it. ============================ Data that I'm reading: ============================ Here is my code: ============================= What is expected: ============================ What I'm getting: ====================== I tried to do the same thing using function (and posted a question here 'Dataframe and conversion to JSON using Pandas'), but some people recommend me to try another way using another function. I know that is a stupid thing add object inside my , but I already tried of others way. Could you help me? 
}
"io": {
	"Frame-1": 
		[{
		    "id": 6,
		    "label": "Sao Paulo",
		    "Customer": [{
		        "id": "CUS-99992",
		        "label": "Brazil",
		        "number": [{
		        "part": "7897",
		        "client": "892"
		    },
		    {
		       "part": "888",
		       "client": "12"
		    }]
		    }]  
		}, 
		{
		    "id": 92,
		    "label": "Hong Kong",
		    "Customer": [{
		        "id": "CUS-88888",
		        "label": "China",
		        "number": [{
		        "part": "147",
		        "client": "288"
		    }]
		    }] 
		}]
		
	"Frame-2":
		[{
		    "id": 6,
		    "label": "Sao Paulo",
		    "Customer": [{
		        "id": "CUS-99992",
		        "label": "Brazil"
		    }],
		    "number": [{
		        "part": "7897",
		        "client": "892"
		    }],
		    "number": [{
		        "part": "888",
		        "client": "12"
		    }]
		}, {
		    "id": 92,
		    "label": "Hong Kong",
		    "Customer": [{
		        "id": "CUS-88888",
		        "label": "China"
		    }],
		    "number": [{
		        "part": "147",
		        "client": "288"
		    }]
		}]
		
}
"answer": {
	"desc": %s Define the following reformatting function: Then apply it the following way: The result (reformatted for readability) is: Edit following the comment To cope with the variant of multiple rows for a single label / label_customer, take another approach: Start from defining the following functions: Get the content of number attribute: Note eval in this function. Otherwise the result would be a string (instead of list of dictionaries). Get the content of Customer attribute: Get the content of the whole JSON element for the current group: Then convert column types to string: And to get the final result, run: The above code: Renames part_number and number_client to part and client, respectively. This change is needed to generate proper element names by getNum. Groups the DataFrame (as in your code). Applies getGrp function to each group. The result is a Series of JSON elements. And finally to_json converts this Series to a list of JSON elements. 
	"code-snippets": [
		def getNum(grp):
		    return eval(grp[['part', 'client']].to_json(orient='records'))
		
		----------------------------------------------------------------------
		def getCust(grp):
		    r0 = grp.iloc[0]
		    return { 'id': r0.id_customer, 'label': r0.label_customer, 'number': getNum(grp) }
		
		----------------------------------------------------------------------
		def getGrp(grp):
		    r0 = grp.iloc[0]
		    return { 'id': r0.id, 'label': r0.label, 'Customer': getCust(grp) }
		
		----------------------------------------------------------------------
		df.part_number = df.part_number.astype('str')
		df.number_client = df.number_client.astype('str')
		
		----------------------------------------------------------------------
		df.rename(columns={'part_number': 'part', 'number_client': 'client'})\
		    .groupby(['id', 'label', 'id_customer', 'label_customer'])\
		    .apply(getGrp).to_json(orient='values')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59266300
"link": https://stackoverflow.com/questions/59266300/how-to-drop-the-rows-if-two-columns-cells-are-empty
"question": {
	"title": How to drop the rows if two columns cells are empty?
	"desc": This is my DF i want to compare the columns B and C then i have to check both are null after that i want to remove that rows from DF. Output looks like,this Then i need to check again both columns of B and C like whether the values or same or not , if same i need to create one column say validation_results and print Y and if not same print N. I am new to python so anybody here tell me how can i do this with minimum lines of code. 
}
"io": {
	"Frame-1": 
		A  B  C
		1  10 10
		2  
		3  12 12
		4      
		5  21 22
		
	"Frame-2":
		A  B  C
		1  10 10
		3  12 12     
		5  21 22
		
}
"answer": {
	"desc": %s Solution if no values are missing values: Use with new column created by : Solution if no values are empty strings: Details: First compare both columns by for not equal - empty string: And then test if both values in row are s by : And filter them by : 
	"code-snippets": [
		print (df[['B','C']].ne(''))
		       B      C
		0   True   True
		1  False  False
		2   True   True
		3  False  False
		4   True   True
		
		----------------------------------------------------------------------
		print (df[['B','C']].ne('').all(axis=1))
		0     True
		1    False
		2     True
		3    False
		4     True
		dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59250863
"link": https://stackoverflow.com/questions/59250863/python-sort-subsection-of-columns
"question": {
	"title": Python: Sort subsection of columns
	"desc": Suppose we have the following dataframe: Which can be computed as follows I was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. I would want the end result to look as 
}
"io": {
	"Frame-1": 
		Label1  2016-03-31  2016-05-31  2016-04-30
		0   A   A1              1            6
		1   B   B1              3            4
		2   C   C2              5            7
		3   D   D1              7            2
		4   E   E4              9            4
		5   F   F1              11           6
		
	"Frame-2":
		Label1  2016-03-31  2016-04-30 2016-05-31   
		0   A   A1              6            1
		1   B   B1              4            3
		2   C   C2              7            5
		3   D   D1              2            7
		4   E   E4              4            9
		5   F   F1              6            11
		
}
"answer": {
	"desc": %s This should work: Output: 
	"code-snippets": [
		new_cols = list(df.columns[:-3]) + list(df.columns[-3:].sort_values())
		
		df[new_cols]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59189297
"link": https://stackoverflow.com/questions/59189297/new-dataframe-which-contais-a-certain-value-within-each-group
"question": {
	"title": New dataframe which contais a certain value within each group
	"desc": I have a dataframe as below I want to group by & and get a new dataframe with all the groups that contains 6 or 14 When I use the code below I accurately get the groups which have either 6 or 14 as below I am just not able to use this information to get the new dataframe which has the groups that are . The expected output is the new dataframe as below. Can anyone guide? 
}
"io": {
	"Frame-1": 
		User    eve Ses
		a   123 1
		a   123 2
		a   123 3
		a   123 4
		a   123 5
		a   123 6
		a   456 1
		a   456 2
		a   456 3
		a   456 4
		a   456 5
		a   456 14
		a   456 7
		a   456 8
		a   456 9
		a   456 10
		a   888 1
		a   888 2
		a   888 3
		a   888 4
		a   888 5
		a   888 5
		a   888 7
		a   888 8
		b   123 1
		b   123 2
		b   123 3
		b   123 4
		b   123 5
		b   123 6
		b   456 1
		b   456 2
		b   456 3
		b   456 4
		b   456 5
		b   456 9
		b   456 7
		b   456 8
		b   456 9
		b   456 10
		b   888 1
		b   888 2
		b   888 3
		b   888 4
		b   888 5
		b   888 6
		b   888 7
		b   888 8
		
	"Frame-2":
		User    eve Ses
		a   123 1
		a   123 2
		a   123 3
		a   123 4
		a   123 5
		a   123 6
		a   456 1
		a   456 2
		a   456 3
		a   456 4
		a   456 5
		a   456 14
		a   456 7
		a   456 8
		a   456 9
		a   456 10
		b   123 1
		b   123 2
		b   123 3
		b   123 4
		b   123 5
		b   123 6
		b   888 1
		b   888 2
		b   888 3
		b   888 4
		b   888 5
		b   888 6
		b   888 7
		b   888 8
		
}
"answer": {
	"desc": %s For improve performance is possible use and with mask created and helper column by : Your solution should be changed with , but if larger DataFrame or many groups solution is really slow: 
	"code-snippets": [
		df = df.groupby(['User','eve']).filter(lambda x: (x['Ses']==6).any()|(x['Ses']==14).any())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59171537
"link": https://stackoverflow.com/questions/59171537/create-a-new-pandas-dataframe-column-based-on-other-column-of-the-dataframe
"question": {
	"title": Create a new pandas dataframe column based on other column of the dataframe
	"desc": I have a Dataframe that consists in 2 columns: 'String' -> numpy array like [47, 0, 49, 12, 46] 'Is Isogram' -> 1 or 0 I would like to create another column, with the value 'Is Isogram' appended in the 'String' array, something like this: I've tried using the apply function with a lambda: But it throws me a KeyError that i don't really understand How can i takle this problem? 
}
"io": {
	"Frame-1": 
		    String              Is Isogram
		0   [47, 0, 49, 12, 46] 1
		1   [43, 50, 22, 1, 13] 1
		2   [10, 1, 24, 22, 16] 1
		3   [2, 24, 3, 24, 51]  0
		4   [40, 1, 41, 18, 3]  1
		
	"Frame-2":
		    String              Is Isogram  IsoString
		0   [47, 0, 49, 12, 46] 1           [47, 0, 49, 12, 46, 1]
		1   [43, 50, 22, 1, 13] 1           [43, 50, 22, 1, 13, 1]
		2   [10, 1, 24, 22, 16] 1           [10, 1, 24, 22, 16, 1]
		3   [2, 24, 3, 24, 51]  0           [2, 24, 3, 24, 51, 0]
		4   [40, 1, 41, 18, 3]  1           [40, 1, 41, 18, 3, 1]
		
}
"answer": {
	"desc": %s There is problem is called for instead function: Better/faster is use if same length of each lists in : 
	"code-snippets": [
		df['IsoString'] = df.apply(lambda x: np.append(x['String'], x['Is Isogram']), axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59149281
"link": https://stackoverflow.com/questions/59149281/how-to-create-rows-for-unique-values-in-columns-in-pandas
"question": {
	"title": How to create rows for unique values in columns in pandas?
	"desc": I have a pandas dataframe with thousands of rows like so: I need all unique values in "IntentName" to have the same IntentID value like so: What is the easiest way to do this? 
}
"io": {
	"Frame-1": 
		IntentID     IntentName         Query           Response
		1            Intent Name 1      Query 1         Response1
		2            Intent Name 1      Query 1         Response2
		3            Intent Name 2      Query 2         Response3
		4            Intent Name 2      Query 2         Response4
		5            Intent Name 3      Query 3         Response5
		
	"Frame-2":
		IntentID     IntentName         Query           Response
		1            Intent Name 1      Query 1         Response1
		1            Intent Name 1      Query 1         Response2
		2            Intent Name 2      Query 2         Response3
		2            Intent Name 2      Query 2         Response4
		3            Intent Name 3      Query 3         Response5
		
}
"answer": {
	"desc": %s Try this: How it works: Group the rows by For each group, keep the first Rank those s 1, 1, 2, 2, 3, etc. () Convert the ranks to int 
	"code-snippets": [
		df['IntentID'] = df.groupby('IntentName') \
		                    ['IntentID'].transform('first') \
		                    .rank(method='dense') \
		                    .astype('int')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59067194
"link": https://stackoverflow.com/questions/59067194/phyton-how-to-get-the-average-of-the-n-largest-values-for-each-column-grouped-b
"question": {
	"title": Phyton: How to get the average of the n largest values for each column grouped by id
	"desc": I'm trying to get the mean for each column while grouped by id. But I don't get it to work as I want to. The data: What I got so far: I got those two tries. But they are both just for one column and I don't know how to do it for more then just one.: What I want: Idealy, I would like to have a dataframe as follows: So that each row contains the mean values for the 100 biggest values for EACH column grouped by id. 
}
"io": {
	"Frame-1": 
		ID       Property3   Property2   Property3
		1        10.2        ...         ...
		1        20.1
		1        51.9
		1        15.8
		1        12.5
		...
		1203     104.4
		1203     11.5
		1203     19.4
		1203     23.1
		
	"Frame-2":
		ID       Property3   Property2   Property3
		1        37.8        5.6         2.3
		2        33.0        1.5         10.4
		3        34.9        91.5        10.3
		4        33.0        10.3        14.3
		
}
"answer": {
	"desc": %s Use with omit columns for processing all columns in DataFrame without : Or specify columns after : 
	"code-snippets": [
		df = (data.groupby('ID')['Property1','Property2','Property3']
		          .agg(lambda grp: grp.nlargest(100).mean())
		          .reset_index())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 45064916
"link": https://stackoverflow.com/questions/45064916/how-to-find-the-correlation-between-a-group-of-values-in-a-pandas-dataframe-colu
"question": {
	"title": How to find the correlation between a group of values in a pandas dataframe column
	"desc": I have a dataframe df: I want to find the pearson correlation coefficient value between and for every So the result should look like this: update: Must make sure all columns of variables are or 
}
"io": {
	"Frame-1": 
		ID    Var1     Var2
		1     1.2        4
		1     2.1        6
		1     3.0        7
		2     1.3        8
		2     2.1        9
		2     3.2        13
		
	"Frame-2":
		ID    Corr_Coef
		1     0.98198
		2     0.97073
		
}
"answer": {
	"desc": %s  Output: With OP output formating. Output: 
	"code-snippets": [
		df_out = df.groupby('ID').corr()
		(df_out[~df_out['Var1'].eq(1)]
		          .reset_index(1, drop=True)['Var1']
		          .rename('Corr_Coef')
		          .reset_index())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59036924
"link": https://stackoverflow.com/questions/59036924/in-python-is-there-a-way-to-delete-parts-of-a-column
"question": {
	"title": In python is there a way to delete parts of a column?
	"desc": I want to trim the values of a pandas data frame. For example, I have the following: And I would like the result to be: If anyone could help it would be very appreciated. 
}
"io": {
	"Frame-1": 
		 A            B        C
		 33344-10   5555-78  999902
		 3444441    5555679  2334
		 2334       5555     3344
		
	"Frame-2":
		A            B         C
		3334       5555     9999
		3444       5555     2334
		2334       5555     3344
		
}
"answer": {
	"desc": %s To keep first 4 characters in string columns: 
	"code-snippets": [
		for c in ['A', 'B', 'C']:
		    df[c] = df[c].str.slice(stop=4)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 34657183
"link": https://stackoverflow.com/questions/34657183/adding-rows-that-have-the-same-column-value-in-a-pandas-dataframe
"question": {
	"title": Adding rows that have the same column value in a pandas dataframe
	"desc": I have a pandas dataframe with dates and hours as columns. Now I want to add the hours of the same dates. For example to make this: Into this: Is there a quick way to do this on big files? 
}
"io": {
	"Frame-1": 
		7-1-2016 | 4
		7-1-2016 | 2
		4-1-2016 | 5
		
	"Frame-2":
		7-1-2016 | 6
		4-1-2016 | 5
		
}
"answer": {
	"desc": %s Here can be used to provide the desired output. Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns. Try: 
	"code-snippets": [
		DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58984851
"link": https://stackoverflow.com/questions/58984851/how-to-use-row-index-and-cell-value-in-function-applied-to-dataframe
"question": {
	"title": How to use row index and cell value in function applied to dataframe?
	"desc": I have a table similar to this, with the blank spaces being empty strings and the numbers being floats: I want to replace the value of each cell with the output of a function which takes two arguments: the index of the row and the value of the cell. For example, the values in the first column should be replaced with the output of func(D, 2) and func(E, 0) and the empty cells should stay empty. The function output is a string. Expected output table: if func(D, 2) returns X and func(E, 0) returns Y, then column 1 should look like: How do I do this? 
}
"io": {
	"Frame-1": 
		   1   2   3   4   5   6
		A  
		B                  8   5
		C      5       7
		D  2   3   5
		E  0
		
	"Frame-2":
		   1   2   3   4   5   6
		A  
		B                  8   5
		C      5       7
		D  X   3   5
		E  Y
		
}
"answer": {
	"desc": %s First of all I would fill the dataframe: then let's assume that your function is called and it's first argument is row index, you can apply it using the method while iterating over rows: 
	"code-snippets": [
		for idx in df.index:
		   df.loc[idx] = df.loc[idx].map(lambda x: func(idx, x))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58942218
"link": https://stackoverflow.com/questions/58942218/pandas-documentation-example-for-append-does-not-work-pandas-dataframe-append
"question": {
	"title": pandas documentation example for append does not work (pandas.DataFrame.append)
	"desc": I copied the example from the pandas documentation for the append method, but it isn't working for me. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html outputs: and not: What are possible reasons for this? Where is my mistake? Thanks for your help. 
}
"io": {
	"Frame-1": 
		   A  B
		0  1  2
		1  3  4
		
	"Frame-2":
		   A  B
		0  1  2
		1  3  4
		0  5  6
		1  7  8
		
}
"answer": {
	"desc": %s Pandas method returns a concatenated dataframe, it does not concatenate the two dataframes inplace. This means does not modify the original but instead returns a new dataframe. The pandas example is in IPython and is correct. This should work: This will print out 
	"code-snippets": [
		df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))
		
		df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))
		
		df = df.append(df2)
		
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 28990256
"link": https://stackoverflow.com/questions/28990256/python-pandas-time-series-year-extraction
"question": {
	"title": python pandas time series year extraction
	"desc": I have a DF containing timestamps: I would like to extract the year from each timestamp, creating additional column in the DF that would look like: Obviously I can go over all DF entries stripping off the first 4 characters of the date. Which is very slow. I wonder if there is a fast python-way to do this. I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime(DF,'%Y-%m-%d %H:%M:%S') but when I try to then apply datetime.datetime.year(DF) it doesn't work. I will also need to parse the timestamps to months and combinations of years-months and so on... Help please. Thanks. 
}
"io": {
	"Frame-1": 
		0     2005-08-31 16:39:40
		1     2005-12-28 16:00:34
		2     2005-10-21 17:52:10
		3     2014-01-28 12:23:15
		4     2014-01-28 12:23:15
		5     2011-02-04 18:32:34
		6     2011-02-04 18:32:34
		7     2011-02-04 18:32:34
		
	"Frame-2":
		0     2005-08-31 16:39:40 2005
		1     2005-12-28 16:00:34 2005
		2     2005-10-21 17:52:10 2005
		3     2014-01-28 12:23:15 2014
		4     2014-01-28 12:23:15 2014
		5     2011-02-04 18:32:34 2011
		6     2011-02-04 18:32:34 2011
		7     2011-02-04 18:32:34 2011
		
}
"answer": {
	"desc": %s No need to apply a function for each row there is a new datetime accessor you can call to access the year property: If your timestamps are str then you can convert to datetime64 using : You can access the months and other attributes using like the above. For version prior to you can perform the following: 
	"code-snippets": [
		df1['year'] = df1['timestamp'].apply(lambda x: x.year)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58821180
"link": https://stackoverflow.com/questions/58821180/dropping-rows-that-have-string-cointained-in-other-rows-in-pandas
"question": {
	"title": Dropping rows that have string cointained in other rows in pandas
	"desc": Given dataframe in this form: I want to drop the rows that have value from column present in another string before the hyphen () in other rows So based on this I would drop value since there are , etc. Expected output: 
}
"io": {
	"Frame-1": 
		 ID      A
		130     Yes
		130-1   Yes
		130-2   Yes
		200     No
		201     No
		201-10  No
		201-101 Yes
		201-22  Yes
		300     No
		
	"Frame-2":
		 ID      A
		130-1   Yes
		130-2   Yes
		200     No
		201-10  No
		201-101 Yes
		201-22  Yes
		300     No
		
}
"answer": {
	"desc": %s Using and some bitwise operations. This does rely on the values without hyphens being before the values with hyphens. 
	"code-snippets": [
		s = df['ID'].str.split('-').str[0]
		m = s.duplicated(keep=False) ^ s.duplicated()
		
		df[~m]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58820383
"link": https://stackoverflow.com/questions/58820383/drop-values-in-specific-condition-in-pandas-dataframe
"question": {
	"title": Drop values in specific condition in pandas dataframe
	"desc": I have a dataframe like below: now If Column A > 7, I want to drop Column B and C like below: How can I achieve that? 
}
"io": {
	"Frame-1": 
		A       B       C
		4.43    NaN     1.11
		3.70    0.48    0.79
		2.78   -0.29    1.26
		1.78    2.90    1.13
		40.70  -0.03    0.55
		51.75   0.29    1.45
		3.65    1.74    0.37
		2.93    1.56    1.64
		3.43    NaN     NaN
		2.93    NaN     NaN
		10.37   NaN     NaN
		
	"Frame-2":
		A       B       C
		4.43    NaN     1.11
		3.70    0.48    0.79
		2.78   -0.29    1.26
		1.78    2.90    1.13
		40.70   NaN     NaN
		51.75   NaN     NaN
		3.65    1.74    0.37
		2.93    1.56    1.64
		3.43    NaN     NaN
		2.93    NaN     NaN
		10.37   NaN     NaN
		
}
"answer": {
	"desc": %s Use with default value for replace by mask: Or with specify : 
	"code-snippets": [
		df[['B','C']] = df[['B','C']].mask(df.A > 7)
		
		----------------------------------------------------------------------
		df.loc[df.A > 7, ['B','C']] = np.nan
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58811263
"link": https://stackoverflow.com/questions/58811263/how-to-update-the-index-of-df-with-a-new-index
"question": {
	"title": How to update the Index of df with a new Index?
	"desc": I am currently having one df which has an incomplete Index. like this: I have the complete . Would like to how to supplement the complete Index into the incomplete df. The can be "", the purpose is for me to identify which case I am currently missing. It's the first time I ask question on stackover, apology for the poor formatting. 
}
"io": {
	"Frame-1": 
		Idx  bar  baz  zoo
		001   A    1    x
		003   B    2    y
		005   C    3    z
		007   A    4    q
		008   B    5    w
		009   C    6    t
		
	"Frame-2":
		Idx  bar  baz  zoo
		001   A    1    x
		002  nan  nan  nan
		003   B    2    y
		004  nan  nan  nan
		005   C    3    z
		006  nan  nan  nan
		007   A    4    q
		008   B    5    w
		009   C    6    t 
		010  nan  nan  nan
		
}
"answer": {
	"desc": %s you can do this easily by using the pandas df reindex method.. df.reindex all you have to do is supply a list to be used as the new index i.e. then pass this into the reindex method like this: the method will automatically put nan values into the rows with indices that were not in the original index... e.g.: output: 
	"code-snippets": [
		df = pd.DataFrame({'bar':['A','B','C','A','B','C'],'baz':[1,2,3,4,5,6],'zoo':['x','y','z','q','w','t']}, index = ['001','003','005','007','008','009']) #your original df
		full_index = ['001','002','003','004','005','006','007','008','009','010']  
		df = df.reindex(full_index)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58771645
"link": https://stackoverflow.com/questions/58771645/group-identical-consecutive-values-in-pandas-dataframe
"question": {
	"title": Group identical consecutive values in pandas DataFrame
	"desc": I have the following pandas dataframe : I want to store the values in another dataframe such as every group of consecutive indentical values make a labeled group like this : The column A represent the value of the group and B represents the number of occurences. this is what i've done so far: It works but it's a bit messy. Do you think of a shortest/better way of doing this ? 
}
"io": {
	"Frame-1": 
		   a
		0  0
		1  0
		2  1
		3  2
		4  2
		5  2
		6  3
		7  2
		8  2
		9  1
		
	"Frame-2":
		   A  B
		0  0  2
		1  1  1
		2  2  3
		3  3  1
		4  2  2
		5  1  1
		
}
"answer": {
	"desc": %s use in Pandas >0.25.0: pandas <0.25.0 
	"code-snippets": [
		new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)
		            .agg(A=('a','first'),B=('a','count')) )
		
		print(new_df)
		
		----------------------------------------------------------------------
		new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)
		            .a
		            .agg({'A':'first','B':'count'}) )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58758098
"link": https://stackoverflow.com/questions/58758098/how-can-i-remove-columns-of-pandas-dataframe-conditional-on-last-row-values
"question": {
	"title": How can I remove columns of pandas dataframe conditional on last row values?
	"desc": Given a data-frame like: I would like to remove the columns in which the value of the last row is less than (<) a constant X, say X = 25. In this example It would remove the column B only and the output would be: Thanks 
}
"io": {
	"Frame-1": 
		             A  B  C
		2019-11-02  120 25 11
		2019-11-03  119 28 15
		2019-11-04  115 23 18
		2019-11-05  119 30 20
		2019-11-06  121 32 25
		2019-11-07  117 24 30
		
	"Frame-2":
		             A  C
		2019-11-02  120 11
		2019-11-03  119 15
		2019-11-04  115 18
		2019-11-05  119 20
		2019-11-06  121 25
		2019-11-07  117 30
		
}
"answer": {
	"desc": %s Method 1: Use , , and : We select the last row with , then check which column is and pass that column to Method 2: Using , , and : Step by step method 1: 
	"code-snippets": [
		df = df.drop(columns = df.columns[df.iloc[-1].lt(25)])
		
		----------------------------------------------------------------------
		# select last row
		
		df.iloc[-1]
		
		A    117
		B     24
		C     30
		Name: 2019-11-07, dtype: int64
		
		----------------------------------------------------------------------
		# check which columns have value < 25:
		
		df.iloc[-1].lt(25)
		
		A    False
		B     True
		C    False
		Name: 2019-11-07, dtype: bool
		
		----------------------------------------------------------------------
		# select those column(s) with boolean indexing:
		
		df.columns[df.iloc[-1].lt(25)]
		
		Index(['B'], dtype='object')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46451284
"link": https://stackoverflow.com/questions/46451284/groupby-and-perform-row-wise-calculation-using-a-custom-function
"question": {
	"title": Groupby and perform row-wise calculation using a custom function
	"desc": Following on from this question: python - Group by and add new row which is calculation of other rows I have a pandas dataframe as follows: And I want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to X and Z from col_2 and create a new row with these values. So the output would be as below: Where are the outputs of the function. Original question (which only requires a simple addition) was answered with: I'm now looking for a way to use a custom function, such as or , rather than . How can I modify this code to work with my new requirements? 
}
"io": {
	"Frame-1": 
		col_1   col_2   col_3  col_4
		a       X        5      1
		a       Y        3      2
		a       Z        6      4
		b       X        7      8
		b       Y        4      3
		b       Z        6      5
		
	"Frame-2":
		col_1   col_2   col_3  col_4 
		a       X        5      1
		a       Y        3      2
		a       Z        6      4
		a       NEW      *      *
		b       X        7      8
		b       Y        4      3
		b       Z        6      5
		b       NEW      *      *
		
}
"answer": {
	"desc": %s I'm not sure if this is what you're looking for, but here goes: And, the change to is: I'm taking a leap of faith in and assuming those columns are sorted before they hit the function. If this isn't the case, an additional call is needed: Should do the trick. 
	"code-snippets": [
		def f(x):
		    y = x.values
		    return y[0] / y[1] # replace with your function
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58653299
"link": https://stackoverflow.com/questions/58653299/convert-standard-date-format-to-string-splitting-by-point-in-python
"question": {
	"title": Convert standard date format to string splitting by point in Python
	"desc": One have one column which has the following format: How can I convert it to format six digits format? I have tried with the following code to but only get : But my desired output: Thank you. 
}
"io": {
	"Frame-1": 
		0         2019/5/20 22:49:29
		1         2019/5/20 23:18:23
		2           2019/3/8 9:11:35
		3           2019/3/8 9:19:58
		4         2019/5/20 22:57:12
		5           2019/3/8 9:06:41
		
	"Frame-2":
		0         19.05.20
		1         19.05.20
		2         19.03.08
		3         19.03.08
		4         19.05.20
		5         19.03.08
		
}
"answer": {
	"desc": %s If you want strings: 
	"code-snippets": [
		df['date'] = df['date'].dt.strftime('%y.%m.%d')
		
		# if your date is not datetime type:
		# df['date'] = pd.to_datetime(df['date']).dt.strftime('%y.%m.%d')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58649009
"link": https://stackoverflow.com/questions/58649009/write-pandas-dataframe-to-csv-in-columns-with-trailing-zeros
"question": {
	"title": Write pandas dataframe to_csv in columns with trailing zeros
	"desc": I have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns). The complicating factor is I also want each column to be rounded to different number of decimals (some need much higher accuracy). To reproduce: Current result for out.txt: Desired: 
}
"io": {
	"Frame-1": 
		0 1.0 3.0 5.0
		1 1.5 3.455 5.45454
		
		
	"Frame-2":
		0 1.0 3.000 5.00000
		1 1.5 3.455 5.45454
		
}
"answer": {
	"desc": %s You can get the string representation of the dataframe using (docs). Then simply write this string to a text file. This method also has parameter to further adjust the spacing between columns. Example: Outputs: There is pandas (more info) but it applies the formatting to values in all columns. Whereas, in your case, you need different formatting for each column. 
	"code-snippets": [
		with open('out.txt', 'w') as file:
		    file.writelines(df_rounded.to_string(header=False))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58539757
"link": https://stackoverflow.com/questions/58539757/python-round-dataframe-columns-with-specific-value-if-exists
"question": {
	"title": Python Round Dataframe Columns with Specific Value If Exists
	"desc": My input dataframe; I want to round my dataframe columns according to a specifi value if exists. My code is like below; Output is; The issue is if there is no "rounding" variable, it should be run automatically as default (0.5). I need a code that can run for both together. Something like this or different; I saw many topics about rounding with specific value but i couldn' t see for this. Could you please help me about this? 
}
"io": {
	"Frame-1": 
		A       B 
		0.3     0.6
		0.4     3.05
		1.6     4.35
		0.15    5.47
		4.19    9.99
		
	"Frame-2":
		A    B
		1    1
		1    3
		2    5
		0    6
		4    10
		
}
"answer": {
	"desc": %s I believe you think missing values - then replace it to or another scalar with statement: Or: If exist value (not missing value): 
	"code-snippets": [
		rounding = np.nan
		
		rounding = 0 if rounding != rounding else rounding
		print (rounding)
		0
		
		----------------------------------------------------------------------
		rounding = 0 if pd.isna(rounding) else rounding
		print (rounding)
		0
		
		----------------------------------------------------------------------
		rounding = 0.25
		
		rounding = 0 if rounding != rounding else rounding
		print (rounding)
		0.25
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58538845
"link": https://stackoverflow.com/questions/58538845/replace-repetitive-number-with-nan-values-except-the-first-in-pandas-column
"question": {
	"title": Replace repetitive number with NAN values except the first, in pandas column
	"desc": I have a data frame like this, Now we can see that there is continuous occurrence of A, B and C. I want only the rows where the occurrence is starting. And the other values of the same occurrence will be nan. The final data frame I am looking for will look like, I can do it using for loop and comparing, But the execution time will be more. I am looking for pythonic way to do it. Some panda shortcuts may be. 
}
"io": {
	"Frame-1": 
		df
		col1    col2
		  1       A
		  2       A
		  3       B
		  4       C
		  5       C
		  6       C
		  7       B
		  8       B
		  9       A
		
	"Frame-2":
		df
		col1    col2
		  1       A
		  2       NA
		  3       B
		  4       C
		  5       NA
		  6       NA
		  7       B
		  8       NA
		  9       A
		
}
"answer": {
	"desc": %s Compare by ed values and missing values by or : Or by with inverted condition by : Or thanks @Daniel Mesejo - use for : Detail: 
	"code-snippets": [
		df['col2'] = df['col2'].where(df['col2'].ne(df['col2'].shift()))
		#alternative
		#df['col2'] = np.where(df['col2'].ne(df['col2'].shift()), df['col2'], np.nan)
		
		----------------------------------------------------------------------
		print (df['col2'].ne(df['col2'].shift()))
		0     True
		1    False
		2     True
		3     True
		4    False
		5    False
		6     True
		7    False
		8     True
		Name: col2, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58522174
"link": https://stackoverflow.com/questions/58522174/python-pandas-get-values-according-to-if-else
"question": {
	"title": Python Pandas Get Values According to If/Else
	"desc": My input dataframe; I want to count whether any difference or not among "Order" and Need values with below code; I want to that something like this; If (WarehouseStock-StoreStock) >= Need: Else Desired Outputs are; Count 3 Could you please help me about this? 
}
"io": {
	"Frame-1": 
		Order    Need   WarehouseStock   StoreStock
		1        3      74               5
		0        4      44               44
		0        0      44               44
		6        12     44               44
		0        6      644              44
		6        6      44               44
		
	"Frame-2":
		Order    Need   WarehouseStock   StoreStock
		1        3      74               5
		6        12     44               44
		0        6      644              44
		
}
"answer": {
	"desc": %s Using with : Output: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		s = df['Need'] - df['Order']
		ind = np.where((df['WarehouseStock'] - df['StoreStock']).ge(df['Need']), ~s.between(-1, 1), ~s.between(-5 , 5))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58490071
"link": https://stackoverflow.com/questions/58490071/pandas-drop-duplicates-based-on-row-value
"question": {
	"title": Pandas: Drop duplicates based on row value
	"desc": I have a dataframe and I want to drop duplicates based on different conditions.... I want to drop all the duplicates from column A except rows with "-". After this, I want to drop duplicates from column A with "-" as a value based on their column B value. Given the input dataframe, this should return the following:- I have the following code but it's not very efficient for very large amounts of data, how can I improve this.... 
}
"io": {
	"Frame-1": 
		        A      B
		  0     1     1.0
		  1     1     1.0
		  2     2     2.0
		  3     2     2.0
		  4     3     3.0
		  5     4     4.0
		  6     5     5.0
		  7     -     5.1
		  8     -     5.1
		  9     -     5.3
		
	"Frame-2":
		        A      B
		  0     1     1.0
		  2     2     2.0
		  4     3     3.0
		  5     4     4.0
		  6     5     5.0
		  7     -     5.1
		  9     -     5.3
		
}
"answer": {
	"desc": %s  and : Output: 
	"code-snippets": [
		df[~df.duplicated('A')            # keep those not duplicates in A
		   | (df['A'].eq('-')             # or those '-' in A
		      & ~df['B'].duplicated())]   # which are not duplicates in B
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58448671
"link": https://stackoverflow.com/questions/58448671/python-list-to-pandas-dataframe-with-number-strings
"question": {
	"title": Python List to Pandas DataFrame with Number &amp; Strings
	"desc": If I have a following list (this list need the separator for each comma); And also another list; How can i get this desire output with python? Could you please help me about this? 
}
"io": {
	"Frame-1": 
		[(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)]
		
	"Frame-2":
		A      B     C   D              E    F   G   H
		5461   1.20  A   BR SK-EL 7 EP  146  E   52  0
		
}
"answer": {
	"desc": %s Solution if data are list of tuples: EDIT: Or: 
	"code-snippets": [
		df = pd.DataFrame([list(x) for x in data], columns=cols)
		
		----------------------------------------------------------------------
		df = pd.DataFrame([[x for x in data]], columns=cols)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58431148
"link": https://stackoverflow.com/questions/58431148/how-to-make-a-deepcopy-of-a-dataframe-with-dataframes-within-it-python
"question": {
	"title": How to make a deepcopy of a dataframe with dataframes within it? (python)
	"desc": I want a copy of a dataframe which contains a dataframe. When I change something in the nested dataframe, it shouldn't change in the original dataframe. I have a dataframe like this: Generated with the next code: When I make a deepcopy of the hole dataframe and the nested dataframe and change something in the nested dataframe in the copy, the value also changes in the original. output: but I want: What is the fix for this problem? 
}
"io": {
	"Frame-1": 
		   0  1     2
		0  1  2  doei
		1  4  5     6
		
		   0  1     2
		0  1  2  doei
		1  4  5     6
		
	"Frame-2":
		   0  1     2
		0  1  2  doei
		1  4  5     6
		
		   0  1     2
		0  1  2     3
		1  4  5     6
		
}
"answer": {
	"desc": %s This is the fix: output: This solves the problem! 
	"code-snippets": [
		import pickle
		deepCopy = pickle.loads(pickle.dumps(df))
		
		deepCopy.iloc[0,2].dfCombinations.iloc[0,2] = "doei"
		
		print(deepCopy.iloc[0,2].dfCombinations)
		print(" ")
		print(df.iloc[0,2].dfCombinations)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58434018
"link": https://stackoverflow.com/questions/58434018/pandas-adding-row-with-all-values-zero
"question": {
	"title": Pandas Adding Row with All Values Zero
	"desc": If I have a following dataframe: How can i add a row end of the dataframe with all values "0 (Zero)"? Desired Output is; Could you please help me about this? 
}
"io": {
	"Frame-1": 
		          A       B       C       D       E
		
		1         1       2       0       1       0
		2         0       0       0       1      -1
		3         1       1       3      -5       2
		4        -3       4       2       6       0
		5         2       4       1       9      -1
		6         1       2       2       4       1
		
	"Frame-2":
		          A       B       C       D       E
		
		1         1       2       0       1       0
		2         0       0       0       1      -1
		3         1       1       3      -5       2
		4        -3       4       2       6       0
		5         2       4       1       9      -1
		6         1       2       2       4       1
		7         0       0       0       0       0
		
}
"answer": {
	"desc": %s Use Setting with enlargement: Or with filled by and index by columns of : 
	"code-snippets": [
		df = df.append(pd.Series(0, index=df.columns), ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58402008
"link": https://stackoverflow.com/questions/58402008/using-a-dictionary-to-modify-the-dfs-values
"question": {
	"title": using a dictionary to modify the dfs values
	"desc": I have a df like this: Then I have a dictionary with some keys (which correspond to the index names of the df) and values (column names): I would like to use the dictionary to check that those column names that do not appear in the dict values , are set to zero to generate this output: How could I use the dictionary to generate the desired output? 
}
"io": {
	"Frame-1": 
		      xx   yy   zz
		 A    6     5    2
		 B    4     4    5
		 B    5     6    7
		 C    6     6    6
		 C    7     7    7
		
	"Frame-2":
		      xx   yy   zz
		 A    6     0    0
		 B    0     4    5
		 B    0     6    7
		 C    6     0    6
		 C    7     0    7
		
}
"answer": {
	"desc": %s You may use indexing 
	"code-snippets": [
		mask = (pd.DataFrame(d.values(), index=d.keys())
		          .stack()
		          .reset_index(level=1, drop=True)
		          .str.get_dummies()
		          .groupby(level=0).sum()
		          .astype(bool)
		        )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58319979
"link": https://stackoverflow.com/questions/58319979/list-to-pandas-dataframe-python
"question": {
	"title": list to pandas dataframe - Python
	"desc": I have the following list: I have created a using the following code: Now I have a new list: I would like to arrange the in a way that the output looks like this (MANUAL EDIT) and that whenever there is a third or foruth colum... the data gets arranged in the same way. I have tried to convert the list to a but since I am new with python I am not getting the desired output but only errors due to invalid shapes. -- EDIT -- Once I have the dataframe created, I want to plot it using . However, the way the data is shown is not what I would like. I am comming from so I am not sure if this is because of the data structure used in the . Is is it that I need one measurement in each row? My idea would be to have the , , in the x-axis (it's a temporal series). In the y-axis the range of values (so that is ok in that plot) and the differnet lines should be showing the evolution of , , , etc. 
}
"io": {
	"Frame-1": 
		list = [-0.14626096918979603,
		 0.017925919395027533,
		 0.41265398151061766]
		
	"Frame-2":
		list2 = [-0.14626096918979603,
		 0.017925919395027533,
		 0.41265398151061766,
		 -0.8538301985671065,
		 0.08182534201640915,
		 0.40291331836021105]
		
}
"answer": {
	"desc": %s you could run something like if the length of your list is not multiple of the length of the dataframe (, you should extend L (in the last loop) with NaN values to answer the second question, should be sufficient to run for the third question, then it's a matter of how was originally designed the dataframe. You could edit the code we wrote at the beginning to invert rows and columns but once you created the dataframe with the first designed way, there's nothing wrong in running 
	"code-snippets": [
		df = pd.DataFrame(index = ['var1', 'var2', 'var3'])
		
		n_cols = int(np.ceil(len(list2) / len(df)))
		for ii in range(n_cols):
		    L = list2[ii * len(df) : (ii + 1) * len(df)]
		    df['col_{}'.format(ii)] = L
		
		----------------------------------------------------------------------
		len(list2) % len(df) != 0
		----------------------------------------------------------------------
		len(df) - (len(list2) % len(df))
		----------------------------------------------------------------------
		df = pd.DataFrame(columns = ['var1', 'var2', 'var3'])
		n_rows = int(np.ceil(len(list2) / len(df.columns)))
		for ii in range(n_rows):
		    L = list2[ii * len(df.columns) : (ii + 1) * len(df.columns)]
		    df.loc['col_{}'.format(ii)] = L
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58321988
"link": https://stackoverflow.com/questions/58321988/how-to-split-a-string-in-a-column-within-a-pandas-dataframe
"question": {
	"title": How to split a string in a column within a pandas dataframe?
	"desc": This is an example of the file I have, So, in the column 'Name', where '_EN' is present, I want to remove the '_EN' part. The output should be as follows: This is what I was trying: However, this is not working. What is a good way to do this? 
}
"io": {
	"Frame-1": 
		Name     Att1     Att2     Att3
		AB_EN    1        2        3
		CD       5        6        7
		FG_EN    7        8        9
		
	"Frame-2":
		Name     Att1     Att2     Att3
		AB       1        2        3
		CD       5        6        7
		FG       7        8        9
		
}
"answer": {
	"desc": %s Use Ex: Output: 
	"code-snippets": [
		df = pd.DataFrame({"Name": ["AB_EN", "CD", "FG_EN"]})
		df['Name'] = df['Name'].str.split("_").str[0]
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58274940
"link": https://stackoverflow.com/questions/58274940/calculating-percentile-values-for-each-columns-group-by-another-column-values
"question": {
	"title": calculating percentile values for each columns group by another column values - Pandas dataframe
	"desc": I have a dataframe that looks like below - Python script to get the dataframe below - I want to calculate certain percentile values for all the columns grouped by 'Year'. Desired output should look like - I am running below python script to perform the calculations to calculate certain percentile values- Output - How can I get the output in the required format without having to do extra data manipulation/formatting or in fewer lines of code? 
}
"io": {
	"Frame-1": 
		   Year  Salary  Amount
		0  2019    1200      53
		1  2020    3443     455
		2  2021    6777     123
		3  2019    5466     313
		4  2020    4656     545
		5  2021    4565     775
		6  2019    4654     567
		7  2020    7867     657
		8  2021    6766     567
		
	"Frame-2":
		           Name      Value
		0   Salary_0.05  1208.9720
		1    Salary_0.1  1217.9440
		2   Salary_0.25  1244.8600
		3    Salary_0.5  1289.7200
		4   Salary_0.75  1334.5800
		5   Salary_0.95  1370.4680
		6   Salary_0.99  1377.6456
		7   Amount_0.05    53.2800
		8    Amount_0.1    53.5600
		9   Amount_0.25    54.4000
		10   Amount_0.5    55.8000
		11  Amount_0.75    57.2000
		12  Amount_0.95    58.3200
		13  Amount_0.99    58.5440
		
}
"answer": {
	"desc": %s You can try followed by : Output: 
	"code-snippets": [
		(df.pivot(columns='Year')
		   .quantile([0.01,0.05,0.75, 0.95, 0.99])
		   .stack('Year')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58254668
"link": https://stackoverflow.com/questions/58254668/merging-two-rows-of-data-in-a-single-row-with-python-pandas
"question": {
	"title": merging two rows of data in a single row with Python/Pandas
	"desc": I have a dataframe like this: I need the rows belonging to the same ID to be merged in a single row, the merged dataframe will be like this I tried using np.random.permutation, np.roll etc but unable to get the desired result. The count of rows in my original data set is in thousands so loops and creating individual data sets and then merging is not helping 
}
"io": {
	"Frame-1": 
		   ID   A1    A2    A3    A4                                      
		0  01  100   101   103   104
		1  01  501   502   503   504
		2  01  701   702   703   704
		3  02  1001  1002  1003  1004
		4  03  2001  2002  2003  2004
		5  03  5001  5002  5003  5004
		
	"Frame-2":
		   ID   A1    A2    A3    A4    B1    B2    B3     B4     C1   C2    C3    C4                                                   
		0  01  101   102   103   104   501   502    503    504    701  702   703   704 
		1  02  1001  2001  1003  1004  
		2  03  2001  2002  2003  2004  5001  5002   5003   5004
		
}
"answer": {
	"desc": %s This is how you do it: The output would be: caveat: this will only work assuming each ID won't have more than 26 rows. 
	"code-snippets": [
		import pandas as pd
		
		
		def widen(x):
		    num_rows = len(x)
		    num_cols = len(x.columns)
		
		    new_index = [
		        chr(ord('A') + row_number) + str(col_number + 1)
		        for row_number in range(num_rows)
		        for col_number in range(num_cols)
		    ]
		
		    return pd.Series(x.loc[:, 'A1':].unstack().values, index=new_index)
		
		res = df.groupby('ID').apply(widen).unstack()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58210576
"link": https://stackoverflow.com/questions/58210576/grouping-panda-by-time-interval-aggregate-function
"question": {
	"title": Grouping panda by time interval + aggregate function
	"desc": Let's say I have a panda like that: And I need convert it in a format as following: The first column corresponds to each 5 seconds interval starting at 2010-01-01 04:10:00:000. The second column is the max of all the grouped rows. The third column is the first of all the grouped rows. How can I get that? 
}
"io": {
	"Frame-1": 
		2010-01-01 04:10:00:025     69
		2010-01-01 04:10:01:669     1
		2010-01-01 04:10:03:027     3
		2010-01-01 04:10:04:003     8
		2010-01-01 04:10:05:987     10
		2010-01-01 04:10:06:330     99
		2010-01-01 04:10:08:369     55
		2010-01-01 04:10:09:987     5000
		2010-01-01 04:10:11:148     13
		
	"Frame-2":
		2010-01-01 04:10:00:000     69      69
		2010-01-01 04:10:05:000     5000    10
		2010-01-01 04:10:10:000     13      13
		
}
"answer": {
	"desc": %s Assuming you mean , we can use with and : Output Note: since you didn't provide column names, I called them 
	"code-snippets": [
		# use this line if your first column is not datetime type yet.
		# df['col1'] = pd.to_datetime(df['col1'], format='%Y-%m-%d %H:%M:%S:%f')
		
		df.groupby(pd.Grouper(key='col1', freq='5s'))['col2'].agg(['max', 'first']).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58198249
"link": https://stackoverflow.com/questions/58198249/filter-duplicate-rows-of-a-pandas-dataframe
"question": {
	"title": Filter duplicate rows of a pandas DataFrame
	"desc": I'm trying to filter the rows of a pandas DataFrame based on some conditions and I'm having difficulties with it. The DataFrame is like so: The selection I would like to apply is the following: For all cus_id that appear more than once (i.e. for all duplicates cus_id), keep only the ones where cus_group is equal to 1. Caution: If a cus_id appears more than once but it only belongs to group 0, we keep all instances of this customer. Visually, the resulting DataFrame I want is like so: As you can see for cus_id = 5555, even though it does appear twice, we keep both records since it only belongs to group 0. I have tried a few things using the duplicated() method but with no success. Any additional help is would be appreciated. EDIT: The solution provided by jezrael works perfectly for the example above. I have noticed that in the real DataFrame I'm using there are cases where customers are linked to group. For example: Using the solution of jezrael those customers are dropped. Is there a quick fix to keep ALL (duplicates included) such cases in the final DataFrame? Visually (after filtering): 
}
"io": {
	"Frame-1": 
		   cus_id  cus_group
		0    1111          1
		1    2222          1
		2    3333          0
		3    4444          1
		4    4444          1
		5    5555          0
		6    5555          0
		
	"Frame-2":
		    cus_id  cus_group
		0     1111        1.0
		1     2222        1.0
		2     3333        0.0
		3     4444        1.0
		4     4444        1.0
		5     5555        0.0
		6     5555        0.0
		7     6666        NaN
		8     7777        NaN
		9     7777        NaN
		
}
"answer": {
	"desc": %s One idea is filter all groups with compare and with and chain with for bitwise for rows: Or if possible only and values in column: 
	"code-snippets": [
		df = df[df['cus_group'].eq(0).groupby(df['cus_id']).transform('all') | df['cus_group'].eq(1)]
		
		----------------------------------------------------------------------
		df = df[df.groupby('cus_id')['cus_group'].transform('nunique').eq(1) | df['cus_group'].eq(1)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 38702332
"link": https://stackoverflow.com/questions/38702332/what-is-the-right-syntax-when-using-notnull-in-pandas
"question": {
	"title": What is the Right Syntax When Using .notnull() in Pandas?
	"desc": I want to use on several columns of a dataframe to eliminate the rows which contain "NaN" values. Let say I have the following : I tried to use this syntax but it does not work? do you know what I am doing wrong? I get this Error: What should I do to get the following output? Any idea? 
}
"io": {
	"Frame-1": 
		  A   B   C
		0 1   1   1
		1 1   NaN 1
		2 1   NaN NaN
		3 NaN 1   1
		
	"Frame-2":
		  A   B   C
		0 1   1   1
		
}
"answer": {
	"desc": %s You can first select subset of columns by , then apply and specify if values in mask are : Another solution is from comment with : what is same as: and means drop all rows, where is at least one value. 
	"code-snippets": [
		print (df.dropna(subset=['A', 'B', 'C'], how='any'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58128354
"link": https://stackoverflow.com/questions/58128354/filtering-rows-of-a-dataframe-based-on-values-in-columns
"question": {
	"title": Filtering rows of a dataframe based on values in columns
	"desc": I want to filter the rows of a dataframe that contains values less than ,say 10. gives, Expected: Any suggestions on how to obtain expected result? 
}
"io": {
	"Frame-1": 
		A   B    C    D
		0  5.0 NaN  NaN  NaN
		1  NaN NaN  NaN  NaN
		2  0.0 NaN  6.0  NaN
		3  NaN NaN  NaN  NaN
		4  NaN NaN  NaN  NaN
		5  6.0 NaN  NaN  NaN
		6  NaN NaN  NaN  NaN
		7  NaN NaN  NaN  7.0
		8  NaN NaN  NaN  NaN
		9  NaN NaN  NaN  NaN
		
	"Frame-2":
		0   5  57  87  95
		2   0  80   6  82
		5   6  33  74  75
		7  71  44  60   7
		
}
"answer": {
	"desc": %s Use: If want filter by any value of condition, is necessary add for test at least one of boolean : 
	"code-snippets": [
		np.random.seed(21)
		df = pd.DataFrame(np.random.randint(0,100,size=(10, 4)), columns=list('ABCD'))
		
		----------------------------------------------------------------------
		print (df < 10)
		       A      B      C      D
		0  False  False  False   True
		1  False  False  False  False
		2  False  False  False  False
		3  False  False  False  False
		4  False  False  False  False
		5   True  False  False  False
		6  False  False  False  False
		7   True  False  False  False
		8  False  False  False  False
		9   True  False  False  False
		
		print ((df < 10).any(axis=1))
		0     True
		1    False
		2    False
		3    False
		4    False
		5     True
		6    False
		7     True
		8    False
		9     True
		dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


