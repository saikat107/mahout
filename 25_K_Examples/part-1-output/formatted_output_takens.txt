"qid": 68314886
"link": https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value
"question": {
	"title": Pandas: Loop through rows to update column value
	"desc": Here is sample dataframe look like: From this dataFrame I want to update value. Condition is when or is not immediate next value of will be replaced by previous value afterthat next point value should be reindexed(cycle .1 to .6). eg. in row index(2) when So, the next value should be also 0.3 instead of 0.4, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively) OUTPUT I want: Code I tried: 
}
"io": {
	"Frame-1": 
		>>> df
		  point    x      y
		0  0.1   NaN    NaN
		1  0.2   NaN    NaN
		2  0.3   5.0    NaN
		3  0.4   NaN    NaN
		4  0.5   NaN    1.0
		5  0.6   NaN    NaN
		6  0.7   1.0    1.0
		7  0.8   NaN    NaN
		8  0.9   NaN    NaN
		9  1.1   NaN    NaN
		10 1.2   NaN    NaN
		11 1.3   NaN    NaN
		12 1.4   NaN    2.0
		13 1.5   NaN    NaN
		14 1.6   NaN    NaN
		15 1.7   NaN    NaN
		16 0.1   NaN    NaN
		17 0.2   NaN    NaN
		18 0.3   NaN    NaN
		19 0.4   NaN    NaN
		20 0.5   NaN    NaN
		21 0.6   2.0    NaN
		22 0.7   NaN    NaN
		23 1.1   NaN    NaN
		
	"Frame-2":
		  point    x      y
		0  0.1   NaN    NaN
		1  0.2   NaN    NaN
		2  0.3   5.0    NaN
		3  0.3   NaN    NaN
		4  0.4   NaN    1.0
		5  0.4   NaN    NaN
		6  0.5   1.0    1.0
		7  0.5   NaN    NaN
		8  0.6   NaN    NaN
		9  1.1   NaN    NaN
		10 1.2   NaN    NaN
		11 1.3   NaN    NaN
		12 1.4   NaN    2.0
		13 1.4   NaN    NaN
		14 1.5   NaN    NaN
		15 1.6   NaN    NaN
		16 0.1   NaN    NaN
		17 0.2   NaN    NaN
		18 0.3   NaN    NaN
		19 0.4   NaN    NaN
		20 0.5   NaN    NaN
		21 0.6   2.0    NaN
		22 0.6   NaN    NaN
		23 1.1   NaN    NaN
		
}
"answer": {
	"desc": %s Can you try that: 
	"code-snippets": [
		mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)
		point = df['point'].astype(int)
		group = point.sub(point.shift(1)).ne(0).cumsum()
		
		df['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68264711
"link": https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting
"question": {
	"title": pd.read_html changed number formatting
	"desc": Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result 
}
"io": {
	"Frame-1": 
		 [        BBBBBB      CCCCCCC  AAAAAAA
		 0       DDDDDD       123456  1234.56
		 1    EEEEEEEEE       123456  1234.56
		 2    EEEEEEEEE       123456  1234.56
		 3    EEEEEEEEE       123456  1234.56
		 4    FFFFFFFFF       123456  1234.56
		 5    GGGGGGGGG       123456  1234.56
		 6    HHHHHHHHH       123456  1234.56
		 7   IIIIIIIIII       123456  1234.56
		 8     JJJJJJJJ       123456  1234.56
		 9     KKKKKKKK  1/2/3/4/5/6  1234.56
		 10    KKKKKKKK  1/2/3/4/5/6  1234.56]
		
	"Frame-2":
		 [        BBBBBB      CCCCCCC  AAAAAAA
		 0       DDDDDD       1,2,3,4,5,6  1234.56
		 1    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 2    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 3    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 4    FFFFFFFFF       1,2,3,4,5,6  1234.56
		 5    GGGGGGGGG       1,2,3,4,5,6  1234.56
		 6    HHHHHHHHH       1,2,3,4,5,6  1234.56
		 7   IIIIIIIIII       1,2,3,4,5,6  1234.56
		 8     JJJJJJJJ       1,2,3,4,5,6  1234.56
		 9     KKKKKKKK       1/2/3/4/5/6  1234.56
		 10    KKKKKKKK       1/2/3/4/5/6  1234.56]
		 
		
}
"answer": {
	"desc": %s You need to add the parameter and set it to by default it's . OUTPUT: 
	"code-snippets": [
		from bs4 import BeautifulSoup
		import pandas as pd
		
		soup = BeautifulSoup(html,'html.parser')
		table = soup.find('div', attrs={'id':'MMMMMMMM'})
		df_list = pd.read_html(str(table), header=1, thousands=None)
		df_list
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68243146
"link": https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas
"question": {
	"title": replace zero with value of an other column using pandas
	"desc": I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: 
}
"io": {
	"Frame-1": 
		    ref   Name   id  Score
		  8400   John    0     12
		  3840  Peter  414      0
		  7400  David  612     64
		  5200  Karen    0      0
		
	"Frame-2":
		   ref    Name   id   Score
		  8400   John  8400     12
		  3840  Peter  414      0
		  7400  David  612     64
		  5200  Karen 5200      0
		
}
"answer": {
	"desc": %s via : OR via numpy's : 
	"code-snippets": [
		#import numpy as np
		df['id']=np.where(df['id'].eq(0),df['ref'],df['id'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68231389
"link": https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas
"question": {
	"title": Compare two columns that contains timestamps in pandas
	"desc": Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 
}
"io": {
	"Frame-1": 
		  Col0       Col1                    Col2                   Col3                   Col4
		   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30
		   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30
		
	"Frame-2":
		  Col0       Col1                    Col2               Col3                   Col4
		   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN
		   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN
		
}
"answer": {
	"desc": %s A straightforward way with boolean mask: 
	"code-snippets": [
		dt = df.select_dtypes('datetime')
		dt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))
		
		df.loc[:, dt.columns.tolist()] = dt
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68231104
"link": https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe
"question": {
	"title": Extract part of a 3 D dataframe
	"desc": I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: 
}
"io": {
	"Frame-1": 
		     d1        d2            d3
		   A B C D...   A B C D...   A B C D..
		0  
		1
		2
		
	"Frame-2":
		    d1    d2    d3
		  A  B   A  B   A  B
		0
		1
		2
		
}
"answer": {
	"desc": %s Use on the level 1 values of columns then select with : : Sample Data Used: 
	"code-snippets": [
		filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]
		
		----------------------------------------------------------------------
		import numpy as np
		import pandas as pd
		
		df = pd.DataFrame(
		    np.arange(1, 25).reshape((-1, 8)),
		    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68229806
"link": https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe
"question": {
	"title": Insert values from variable and DataFrame into another DataFrame
	"desc": On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code 
}
"io": {
	"Frame-1": 
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   NaN    14    24
		2   1   NaN    15    25
		
	"Frame-2":
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   3.0    14    24
		2   1   3.0    15    25
		
}
"answer": {
	"desc": %s Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: 
	"code-snippets": [
		import pandas as pd
		
		id=1
		df1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})
		df2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})
		
		df2.insert(0, "id", id)
		df2.insert(1, "col0", df1[df1['id']==id]['col0'][0])
		
		print(df2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68213612
"link": https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun
"question": {
	"title": How to combine rows in a dataframe in a pairwise fashion while applying some function
	"desc": I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: 
}
"io": {
	"Frame-1": 
		ID    Val1    Val2
		id0     10      20
		id0     11      19
		id1      5       5
		id1      1       1
		id1      2       4
		
	"Frame-2":
		ID      Val1    Val2
		id0_1   10.5    19.5
		id1_1   3       3
		id1_2   1.5     2.5
		
}
"answer": {
	"desc": %s You can use after grouping by : 
	"code-snippets": [
		out = df.groupby('ID').rolling(2).mean() \
		        .dropna(how='all').reset_index(level=1, drop=True)
		
		out.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68211888
"link": https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base
"question": {
	"title": Loop through multiple small Pandas dataframes and create summary dataframes based on a single column
	"desc": I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. 
}
"io": {
	"Frame-1": 
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		player2  2     6     8
		player3  3     6     7
		
		NAME     VAL1  VAL2  VAL3
		player2  5     7     7
		player3  2     6     8
		player5  3     6     7
		
	"Frame-2":
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		
		NAME     VAL1  VAL2  VAL3
		player2  2     6     8
		player2  5     7     7
		
		NAME     VAL1  VAL2  VAL3
		player3  3     6     7
		player3  2     6     8
		
		NAME     VAL1  VAL2  VAL3
		player5  3     6     7
		
}
"answer": {
	"desc": %s Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : 
	"code-snippets": [
		dfs = {group_name: df_
		       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}
		
		----------------------------------------------------------------------
		dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193558
"link": https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values
"question": {
	"title": pandas group many columns to one column where every cell is a list of values
	"desc": I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? 
}
"io": {
	"Frame-1": 
		df = 
		c1 c2 c3 c4 c5
		1.  2. 3. 1. 5
		8.  2. 1. 3. 8
		4.  9. 1  2. 3
		
	"Frame-2":
		df = 
		    l
		[1,2,3,1,5]
		[8,2,1,3,8]
		[4,9,1,2,3]
		
}
"answer": {
	"desc": %s Try: 
	"code-snippets": [
		#best way:
		df['l']=df.values.tolist()
		#OR
		df['l']=df.to_numpy().tolist()
		
		
		#another way:
		df['l']=df.agg(list,1)
		#OR
		df['l']=df.apply(list,1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193521
"link": https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame
"question": {
	"title": Concatenate values and column names in a data frame to create a new data frame
	"desc": I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used 
}
"io": {
	"Frame-1": 
		  Value col1 col2 col3
		0     a   aa   ab   ac
		1     b   ba   bb   bc
		2     c   ca   cb   cc
		3     d   da   db   dc
		4     e   ea   eb   ec
		
	"Frame-2":
		      Value Col 1
		0   a_Col 1    aa
		1   a_Col 2    ab
		2   a_Col 3    ac
		3   b_Col 1    ba
		4   b_Col 2    bb
		5   b_Col 3    bc
		6   c_Col 1    ca
		7   c_Col 2    cb
		8   c_Col 3    cc
		9   d_Col 1    da
		10  d_Col 2    db
		11  d_Col 3    dc
		12  e_Col 1    ea
		13  e_Col 2    eb
		14  e_Col 3    ec
		
}
"answer": {
	"desc": %s Try: Prints: Optionally, you can sort values afterwards: 
	"code-snippets": [
		x = df.melt("Value", value_name="Col 1")
		x.Value += "_" + x.variable
		x = x.drop(columns="variable")
		print(x)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174614
"link": https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json
"question": {
	"title": Why does it add .0 to the value while converting Dataframe columns to JSON
	"desc": I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? 
}
"io": {
	"Frame-1": 
		A   B   C   D
		2   6   5   8.0
		6   11  2   3.6 
		1   5   7   5.2
		
	"Frame-2":
		{"A":2.0, "B":6.0, "C":5.0, "D":8.0}
		{"A":6.0, "B":11.0, "C":2.0, "D":3.6}
		{"A":1.0, "B":5.0, "C":7.0, "D":5.2}
		
}
"answer": {
	"desc": %s The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: 
	"code-snippets": [
		df.iloc[0]
		A    2.0
		B    6.0
		C    5.0
		D    8.0
		Name: 0, dtype: float64
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174113
"link": https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction
"question": {
	"title": Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries
	"desc": I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? 
}
"io": {
	"Frame-1": 
		bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}
		
	"Frame-2":
		ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}
		
}
"answer": {
	"desc": %s A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: 
	"code-snippets": [
		def reclass(group, name):
		    bins = bins_dic[name]
		    ids = ids_dic[name]
		    return pd.cut(group, bins, labels=ids)
		    
		df['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68150020
"link": https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector
"question": {
	"title": Getting first/second/third... value in row of numpy array after nan using vectorization
	"desc": I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. 
}
"io": {
	"Frame-1": 
		f(offset=0)
		
		
		| 0  | 1  |
		| -- | -- |
		| 1  | 25 |
		| 2  | 29 |
		| 3  | 33 |
		| 4  | 31 |
		| 5  | 30 |
		| 6  | 35 |
		| 7  | 31 |
		| 8  | 33 |
		| 9  | 26 |
		| 10 | 27 |
		| 11 | 35 |
		| 12 | 33 |
		| 13 | 28 |
		| 14 | 25 |
		| 15 | 25 |
		| 16 | 26 |
		| 17 | 34 |
		| 18 | 28 |
		| 19 | 34 |
		| 20 | 28 |
		
	"Frame-2":
		f(offset=1)
		
		| 0  | 1   |
		| -- | --- |
		| 1  | nan |
		| 2  | nan |
		| 3  | nan |
		| 4  | 35  |
		| 5  | 34  |
		| 6  | 34  |
		| 7  | 26  |
		| 8  | 25  |
		| 9  | 31  |
		| 10 | 26  |
		| 11 | 25  |
		| 12 | 35  |
		| 13 | 25  |
		| 14 | 25  |
		| 15 | 26  |
		| 16 | 31  |
		| 17 | 29  |
		| 18 | 29  |
		| 19 | 26  |
		| 20 | 30  |
		
}
"answer": {
	"desc": %s Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster 
	"code-snippets": [
		def first_valid(arr, offset=0):
		    m = ~np.isnan(arr)
		    i =  m.argmax(axis=1) + offset
		    iy = np.clip(i, 0, arr.shape[1] - 1)
		
		    vals = arr[np.r_[:arr.shape[0]], iy]
		    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan
		    return vals
		
		----------------------------------------------------------------------
		def first_valid(df, offset=0):
		    return df.stack().groupby(level=0)\
		                     .nth(offset).reindex(df.index)
		
		----------------------------------------------------------------------
		# Sample dataframe for testing purpose
		df_test = pd.concat([df] * 10000, ignore_index=True)
		
		%%timeit # Numpy approach
		_ = first_valid(df_test.to_numpy(), 1)
		# 6.9 ms ± 212 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		
		%%timeit # Pandas approach
		_ = first_valid(df_test, 1)
		# 90 ms ± 867 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
		
		
		%%timeit # OP's approach
		_ = f(df_test, 1)
		# 2.03 s ± 183 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57033657
"link": https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe
"question": {
	"title": How to Extract Month Name and Year from Date column of DataFrame
	"desc": I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. 
}
"io": {
	"Frame-1": 
		45    2018-01-01
		73    2018-02-08
		74    2018-02-08
		75    2018-02-08
		76    2018-02-08
		
	"Frame-2":
		45    Jan-2018
		73    Feb-2018
		74    Feb-2018
		75    Feb-2018
		76    Feb-2018
		
}
"answer": {
	"desc": %s Cast you date from object to actual datetime and use dt to access what you need. 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})
		
		df['Date'] = pd.to_datetime(df['Date'])
		
		# You can format your date as you wish
		df['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')
		
		# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.
		
		print(df['Mon_Year'])
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68107298
"link": https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data
"question": {
	"title": How to Create a Correlation Dataframe from already related data
	"desc": I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? 
}
"io": {
	"Frame-1": 
		    0       1       2
		0   English Spanish 0.50
		1   English Russian 0.15
		
	"Frame-2":
		        English Spanish Russian
		English 1       0.5     0.15
		Spanish 0.5     1       -
		Russian 0.15    -       1
		
}
"answer": {
	"desc": %s Use to create the all language combinations and fill with the existing data: 
	"code-snippets": [
		lg = pd.concat([df[0], df[1]]).unique()  # ['English', 'Spanish', 'Russian']
		cx = pd.crosstab(lg, lg)
		
		cx.update(df.set_index([0, 1]).squeeze().unstack())
		cx.update(df.set_index([0, 1]).squeeze().unstack().T)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68098150
"link": https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas
"question": {
	"title": set some rule to groupby in pandas
	"desc": I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have "dup by" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. 
}
"io": {
	"Frame-1": 
		1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
	"Frame-2":
		1|F08210020403|GO|2014-05-17 00:00:00|yes
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
}
"answer": {
	"desc": %s IIUC: try: 
	"code-snippets": [
		c=df['keep'].str.contains('dup by')
		#created a condition which check if 'keep' column contains 'dup by' or not
		df['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')
		most_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()
		#excluded those rows in groupby where 'keep' contains 'dup by'
		df['most_recent_date']=df['VIP_ID'].map(most_recent_date)
		df['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])
		df['keep'] = np.where(
		    df['keep'] != 'same tier',df['keep'],
		    (np.where(
		         df['most_recent_date'] == df['datetime'],
		         'yes',
		         'dup by ' + df['VIP_ID'].astype(str)))
		)
		df.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'
		df = df.drop(columns = ['both','most_recent_date'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68090463
"link": https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas
"question": {
	"title": Merging more than two columns of the same dataframe in pandas
	"desc": Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? 
}
"io": {
	"Frame-1": 
		VAR 1   VAR 2   VAR 3   GROUP 
		                3   [0-10]
		1               3   [0-10]
		1               3   [0-10]
		1       2           [0-10]
		        2           [0-10]
		3              3    [10-20]
		3       1           [10-20]
		        1           [10-20]
		        2           [10-20]
		               2    [10-20]
		               2    [10-20]
		
	"Frame-2":
		VAR_MERGED  GROUP 
		1           [0-10]
		1           [0-10]
		1           [0-10]
		2           [0-10]
		2           [0-10]
		3           [0-10]
		3           [0-10]
		3           [0-10]
		1           [10-20]
		1           [10-20]
		2           [10-20]
		2           [10-20]
		2           [10-20]
		3           [10-20]
		3           [10-20]
		3           [10-20]
		
}
"answer": {
	"desc": %s Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: 
	"code-snippets": [
		id_vars = df.columns[~df.columns.str.startswith('VAR')]
		
		out = df.melt(id_vars, value_name='VAR_MERGED') \
		        .dropna() \
		        .sort_values(['GROUP', 'VAR_MERGED']) \
		        .reset_index(drop=True) \
		        [['VAR_MERGED'] + id_vars]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 33130586
"link": https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val
"question": {
	"title": python pandas - creating a column which keeps a running count of consecutive values
	"desc": I am trying to create a column (“consec”) which will keep a running count of consecutive values in another (“binary”) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. 
}
"io": {
	"Frame-1": 
		.    binary consec
		1       0      0
		2       1      1
		3       1      2
		4       1      3
		5       1      4
		5       0      0
		6       1      1
		7       1      2
		8       0      0
		
	"Frame-2":
		.  binary   consec
		0     1       NaN
		1     1       1
		2     1       1
		3     0       0
		4     1       1
		5     0       0
		6     1       1
		7     1       1
		8     1       1
		9     0       0
		
}
"answer": {
	"desc": %s You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. 
	"code-snippets": [
		>>> (df["binary"] == 0)
		0     True
		1    False
		2    False
		3    False
		4     True
		5     True
		6    False
		7    False
		8     True
		Name: binary, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62099066
"link": https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da
"question": {
	"title": is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?
	"desc": I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: 
}
"io": {
	"Frame-1": 
		(a) (b) (c) (d)
		a1  b1  c1  d1
		a2  b2  c2  d2
		
	"Frame-2":
		(a) (b) (e) (f)
		a3  b3  e1  f1
		a4  b4  e1  f1
		a5  b5  e2  f2
		a6  b6  e4  f4
		a7  a8  e4  f5
		
}
"answer": {
	"desc": %s Here you go: Check for the output. 
	"code-snippets": [
		import pandas as pd
		from pandas import ExcelWriter
		
		def detect_duplicate(group):
		    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)
		    return group
		
		with ExcelWriter('output.xlsx') as output:
		    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():
		        df = df.drop(['a', 'b'], axis=1)
		        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)
		        df = df.groupby(['concat']).apply(detect_duplicate)
		        df = df.drop_duplicates(keep='last', subset=['concat'])
		        df.to_excel(output, sheet_name=sheet_name, index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67989744
"link": https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column
"question": {
	"title": Pandas replacing values in a column by values in another column
	"desc": Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: 
}
"io": {
	"Frame-1": 
		    ppid  col2 ...
		1   'id1'  '1'
		2   'id2'  '2'
		3   'id3'  '3'
		...
		
	"Frame-2":
		    ppid  val
		1   'id1' '5'
		2   'id2' '6'
		
}
"answer": {
	"desc": %s Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 
	"code-snippets": [
		df1 = df1.set_index('ppid')
		df2 = df2.set_index('ppid')
		df1.update(df2)
		df1.reset_index(inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67986537
"link": https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe
"question": {
	"title": How do I check for conflict between columns in a pandas dataframe?
	"desc": I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. 
}
"io": {
	"Frame-1": 
		    Item  Local A  Local B  Local C
		0  Item1      NaN      6.0        5
		1  Item2      6.0      7.0        5
		2  Item3      NaN      NaN        5
		3  Item4      5.0      5.0        5
		4  Item5      5.0      NaN        5
		
	"Frame-2":
		    Item  Local A  Local B  Local C Conflict
		0  Item1      NaN      6.0        5      yes
		1  Item2      6.0      7.0        5      yes
		2  Item3      NaN      NaN        5      NaN
		3  Item4      5.0      5.0        5      NaN
		4  Item5      5.0      NaN        5      NaN
		
}
"answer": {
	"desc": %s IIUC, try: Output: 
	"code-snippets": [
		df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46124699
"link": https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files
"question": {
	"title": Splitting a dataframe into separate CSV files
	"desc": I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. 
}
"io": {
	"Frame-1": 
		+---------+---------+
		| Column1 | Column2 |
		+---------+---------+
		|       1 |   93644 |
		|       2 |   63246 |
		|       3 |   47790 |
		|       3 |   39644 |
		|       3 |   32585 |
		|       1 |   19593 |
		|       1 |   12707 |
		|       2 |   53480 |
		+---------+---------+
		
	"Frame-2":
		+---+-------+----------------+
		| 1 | 19593 | NewColumnValue |
		| 1 | 93644 | NewColumnValue |
		| 1 | 12707 | NewColumnValue |
		+---+-------+----------------+
		
		+---+-------+-----------------+
		| 2 | 63246 | NewColumnValue |
		| 2 | 53480 | NewColumnValue |
		+---+-------+-----------------+
		
		+---+-------+-----------------+
		| 3 | 47790 | NewColumnValue |
		| 3 | 39644 | NewColumnValue |
		| 3 | 32585 | NewColumnValue |
		+---+-------+-----------------+
		
}
"answer": {
	"desc": %s Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. 
	"code-snippets": [
		for i, g in df.groupby('Column1'):
		    g.to_csv('{}.csv'.format(i), header=False, index_label=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67919385
"link": https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes
"question": {
	"title": Find unique column values out of two different Dataframes
	"desc": How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read 
}
"io": {
	"Frame-1": 
		67      Hij
		14      Xyz 
		87      Pqr
		
	"Frame-2":
		43      Def
		67      Lmn
		14      Xyz
		
}
"answer": {
	"desc": %s TRY: NOTE : Replace in with the first column name. 
	"code-snippets": [
		unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67917573
"link": https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas
"question": {
	"title": replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas
	"desc": I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! 
}
"io": {
	"Frame-1": 
		 L1      D1     L2      D2         L3
		 1.0    ABC     1.1     4.1        NaN
		 NaN    NaN     1.7     NaN        NaN
		 NaN    4.1     NaN     NaN        NaN
		 NaN    1.8     3.2     PQR        NaN
		 NaN    NaN     1.6     NaN        NaN
		
	"Frame-2":
		 L1      D1      L2      D2         L3
		 1.0    ABC     1.1     4.1        -
		 NaN    NaN     1.7     -          -
		 NaN    4.1      -      -          -
		 NaN    1.8     3.2     PQR        -
		 NaN    NaN     1.6     -          -
		
		
}
"answer": {
	"desc": %s Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get 
	"code-snippets": [
		minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)
		out = df.mask(minus_mask, "-")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67910688
"link": https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe
"question": {
	"title": move column above and delete rows in pandas python dataframe
	"desc": I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be 
}
"io": {
	"Frame-1": 
		A        B        C        D        E        F        G        H
		a.1      b.1     
		                  
		                  c.1      d.1 
		                  c.2      d.2           e.1      f.1 
		                                                      
		
		                                                     g.1       h.1
		  
		
		
		
	"Frame-2":
		A        B        C        D        E        F        G        H
		a.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1
		                  c.2      d.2                                                   
		
}
"answer": {
	"desc": %s You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: 
	"code-snippets": [
		out = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		         .dropna(how="all")
		         .fillna(""))
		
		----------------------------------------------------------------------
		index = df.index
		df = df.reset_index(drop=True)
		df = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		        .dropna(how="all")
		        .fillna(""))
		df.index = index[:len(df)]
		
		----------------------------------------------------------------------
		def pull_up(s):
		    # this will be a column number; `s.name` is the column name
		    col_index = df.columns.get_indexer([s.name])
		
		   # for example: if `col_index` is either 7 or 8, pull by 4
		   if col_index in (7, 8):
		       return s.shift(-4)
		   else:
		       # otherwise, pull as much
		       return s.shift(-s.first_valid_index())
		
		# applying
		df.apply(pull_up)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67905723
"link": https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner
"question": {
	"title": Replicating the DataFrame row in a special manner
	"desc": I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: 
}
"io": {
	"Frame-1": 
		col1        mob_no             col3
		 a    9382949201/3245622535    45
		 b    8383459345/4325562678    67
		 c    8976247543/1827472398    89
		 d    7844329432               09
		
	"Frame-2":
		col1    mob_no      col3
		 a    9382949201     45
		 a    3245622535     45
		 b    8383459345     67
		 b    4325562678     67
		 c    8976247543     89
		 c    1827472398     89
		 d    7844329432     09
		
}
"answer": {
	"desc": %s Try with + : 
	"code-snippets": [
		df['mob_no'] = df['mob_no'].str.split('/')
		df = df.explode('mob_no')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67882067
"link": https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe
"question": {
	"title": Dictionary making for a transportation model from a Dataframe
	"desc": I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. 
}
"io": {
	"Frame-1": 
		 d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand
		 M = {p1:500, p2:500, p3:500}               # factory capacity
		 I = [c1,c2,c3,c4,c5]                         # Customers
		 J = [p1,p2,p3]                             # Factories
		 cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,
		 (p1,c4):8,    (p1,c5):10, ......
		  } 
		
	"Frame-2":
		  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		df1 = df.set_index(["Unnamed: 0", "Unnamed: 1"])
		plants = df1.loc[np.NaN]  # remove demand from dataframe
		
		d = dict(df1.loc["demand"].T.squeeze().dropna().iteritems())
		M = dict(plants["capacity"].iteritems())
		I = list(plants.drop(columns="capacity").columns)
		J = list(plants.index)
		cost = dict(plants.drop(columns="capacity").stack().iteritems())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870323
"link": https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe
"question": {
	"title": How to get list of previous n values of a column conditionally in DataFrame?
	"desc": My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? 
}
"io": {
	"Frame-1": 
		Subject     Score
		    1       15
		    2       0
		    3       18
		    2       30
		    3       17
		    1       5
		    4       9
		    2       7
		    1       20
		    1       8
		    2       9
		    1       12
		
	"Frame-2":
		Subject   Score Previous
		1       15      []
		2       0       []
		3       18      []
		2       30      [0]
		3       17      [18]
		1       5       [15]
		4       9       []
		2       7       [30,0]
		1       20      [5,15]
		1       8       [20,5,15]
		2       9       [7,30,0]
		1       12      [8,20,5]
		
}
"answer": {
	"desc": %s Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: 
	"code-snippets": [
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [
		    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)
		]
		
		----------------------------------------------------------------------
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [x.agg(list)[-2::-1]
		                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]
		df = df.sort_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870585
"link": https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column
"question": {
	"title": Python dataframe create index column based on other id column
	"desc": I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: 
}
"io": {
	"Frame-1": 
		ID                  Price
		000afb96ded6677c    1514.5
		000afb96ded6677c    13.0
		000afb96ded6677c    611.0
		000afb96ded6677c    723.0
		000afb96ded6677c    2065.0
		ffea14e87a4e1269    2286.0
		ffea14e87a4e1269    1150.0
		ffea14e87a4e1269    80.0
		fff455057ad492da    650.0
		fff5fc66c1fd66c2    450.0
		
	"Frame-2":
		ID                  Price    ID 2
		000afb96ded6677c    1514.5   1
		000afb96ded6677c    13.0     1
		000afb96ded6677c    611.0    1
		000afb96ded6677c    723.0    1
		000afb96ded6677c    2065.0   1
		ffea14e87a4e1269    2286.0   2
		ffea14e87a4e1269    1150.0   2
		ffea14e87a4e1269    80.0     2
		fff455057ad492da    650.0    3
		fff5fc66c1fd66c2    450.0    4
		
}
"answer": {
	"desc": %s Try + 1 : Or with : : 
	"code-snippets": [
		df['ID_2'] = df.groupby('ID').ngroup() + 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67856992
"link": https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list
"question": {
	"title": Element wise numeric comparison in Pandas dataframe column value with list
	"desc": I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: 
}
"io": {
	"Frame-1": 
		  |          A          |           B           |          C         |
		  |         Val         |          Val          |         Val        |
		  |---------------------|-----------------------|--------------------|
		0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		
	"Frame-2":
		  |  A    |   B   |  C   |
		  |  Max  |  Max  |  Max |
		  |-------|-------|------|
		0 | 28.68 | 18.42 | 1.37 |
		1 | 29.50 | 17.31 | 1.47 |
		2 | 29.87 | 20.45 | 1.39 |
		
}
"answer": {
	"desc": %s Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 µs, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 µs, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms 
	"code-snippets": [
		def bool_check(row):
		    col = row.name[0]
		    min_val = df1[pd.IndexSlice[col]].to_numpy()
		    max_val = df3[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		----------------------------------------------------------------------
		def bool_check(row):
		    col = row.name[0]
		    min_val = min_df[pd.IndexSlice[col]].to_numpy()
		    max_val = max_df[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		res = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67845362
"link": https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column
"question": {
	"title": Sort pandas df subset of rows (within a group) by specific column
	"desc": I have the following dataframe let’s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? 
}
"io": {
	"Frame-1": 
		
		A B C D E
		z k s 7 d
		z k s 6 l
		x t r 2 e
		x t r 1 x
		u c r 8 f
		u c r 9 h
		y t s 5 l
		y t s 2 o
		
	"Frame-2":
		
		A B C D E
		z k s 6 l
		z k s 7 d
		x t r 1 x
		x t r 2 e
		u c r 8 f
		u c r 9 h
		y t s 2 o
		y t s 5 l
		
}
"answer": {
	"desc": %s I think it should be as simple as this: 
	"code-snippets": [
		df = df.sort_values(["A", "B", "C", "D"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61624957
"link": https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json
"question": {
	"title": Creating a table in pandas from json
	"desc": I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! 
}
"io": {
	"Frame-1": 
		0     website
		0  1  NaN
		1  2  NaN 
		
	"Frame-2":
		0     website
		0  1  https://bitcoin.org/
		1  2  https://litecoin.org/
		
}
"answer": {
	"desc": %s You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' 
	"code-snippets": [
		(
		    pd.DataFrame(data['data'])
		    .loc['urls']
		    .apply(lambda x: (x['technical_doc']+[''])[0])
		    .to_frame('website')
		)
		
		----------------------------------------------------------------------
		(
		    pd.DataFrame(data['data'])
		    .loc['logo']
		    .to_frame('logo')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67822403
"link": https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1
"question": {
	"title": Replicate dataframe n number of times and increment another column by 1
	"desc": I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. 
}
"io": {
	"Frame-1": 
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		
	"Frame-2":
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		2      123     abc     2.20
		2      234     bcd     1.19
		2      345     cde     1.22
		3      123     abc     2.20
		3      234     bcd     1.19
		3      345     cde     1.22
		
}
"answer": {
	"desc": %s Something along these lines should work: 
	"code-snippets": [
		df1=df.copy()
		for i in range(1,20):
		    df['S/No']=df['S/No']+1
		    df1=pd.concat([df1,df])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67782727
"link": https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas
"question": {
	"title": Python - Delete lines from dataframe (pandas)
	"desc": I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. 
}
"io": {
	"Frame-1": 
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		10     3927905909    ///    ///
		11     3872948210    ///   0.15
		12  9783293003781    ///   0.15
		13  9783423246842    ///    ///
		14  9783423247146    ///    ///
		15  9783423246934    ///    ///
		16     387294116x    ///    ///
		17  9783935597456   0,16   0.15
		18  9783423204545    ///    ///
		
	"Frame-2":
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		
}
"answer": {
	"desc": %s Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. 
	"code-snippets": [
		data[['1', '2']] = data[['1', '2']].replace({"///": np.nan, ",": "."}, regex=True)
		                                   .astype(float)
		data = data.loc[data[["1", "2"]].ge(1.).all(axis="columns")]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67770056
"link": https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe
"question": {
	"title": How to create columns from a string in a dataframe?
	"desc": WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. 
}
"io": {
	"Frame-1": 
		    long string
		0   ha: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ho...
		1   hi: (tra: 1 la: 2) \n ha: (tra: 1 la: 2) \n ho...
		2   ho: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ha...
		
	"Frame-2":
		    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la
		0   1       2       1       2       1       2
		1   1       2       1       2       1       2
		2   1       2       1       2       1       2
		
}
"answer": {
	"desc": %s  Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get 
	"code-snippets": [
		ndf = (df["long string"]
		         .str.extractall(r"(ha|hi|ho):\s\((?:tra|la):\s(\d+)\s(?:tra|la):\s(\d+)\)")
		         .droplevel("match")
		         .set_index(0, append=True)
		         .set_axis(["tra", "la"], axis=1)
		         .unstack()
		         .swaplevel(axis=1))
		ndf.columns = ndf.columns.map("-".join)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67696814
"link": https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe
"question": {
	"title": Convert dictionary with sub-list of dictionaries into pandas dataframe
	"desc": I have this code with a dictionary "dict": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! 
}
"io": {
	"Frame-1": 
		                                                 0
		2000  {'team': 'Manchester United', 'points': '91'}
		2001  {'team': 'Manchester United', 'points': '80'}
		2002            {'team': 'Arsenal', 'points': '87'}
		
		
	"Frame-2":
		        team                  points
		2000    Manchester United     91
		2001    Manchester United     80
		2002    Arsenal               87
		
		
}
"answer": {
	"desc": %s Tr this. This would depend on how large your data is. 
	"code-snippets": [
		
		diction =  {
		    '2000': [{'team': 'Manchester United', 'points': '91'}],
		    '2001': [{'team': 'Manchester United', 'points': '80'}],
		    '2002': [{'team': 'Arsenal', 'points': '87'}]
		}
		transformed_dict= {x:d for x,y in diction.items() for d in y }
		df = pd.DataFrame(transformed_dict)
		df.T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67672199
"link": https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa
"question": {
	"title": How to concat the row output of iterrows to another pandas DataFrame with the same columns?
	"desc": Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? 
}
"io": {
	"Frame-1": 
		A    B    C
		4    c    12
		5    d    19
		2    b    43
		
	"Frame-2":
		    0   A   B   C
		A   2   NaN     NaN     NaN
		B   b   NaN     NaN     NaN
		C   43  NaN     NaN     NaN
		0   NaN     4.0     c   12.0
		1   NaN     5.0     d   19.0
		
}
"answer": {
	"desc": %s I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: 
	"code-snippets": [
		pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67662415
"link": https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe
"question": {
	"title": Working with list inside a Pandas dataframe
	"desc": I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? 
}
"io": {
	"Frame-1": 
		ID | Column1 |
		0  |  []     | 
		1  |  [1,2]  | 
		2  |  []     |
		
	"Frame-2":
		ID | Column1 | Column2 |
		0  |  []     |   0     |
		1  |  [1,2]  |   2     |
		2  |  []     |   0     |
		
}
"answer": {
	"desc": %s I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: 
	"code-snippets": [
		df['Column2'] = [len(x) for x in df['Column1']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67595888
"link": https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list
"question": {
	"title": how to extract each numbers from pandas string column to list?
	"desc": How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: 
}
"io": {
	"Frame-1": 
		Column_A
		11.2 some text 17 some text 21
		some text 25.2 4.1 some text 53 17 78
		121.1 bla bla bla 14 some text
		12 some text
		
	"Frame-2":
		listA[0] = 11.2 listA[1] = 17 listA[2] = 21
		listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78
		listC[0] = 121.1 listC[1] = 14
		listD[0] = 12
		
}
"answer": {
	"desc": %s You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: 
	"code-snippets": [
		df['Column_A'].apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()
		
		----------------------------------------------------------------------
		listA, listB, listC, listD = df.Column_A.str.findall(r"[-+]?\d*\.\d+|\d+")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67580031
"link": https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas
"question": {
	"title": Groupby, counts in ranges and spread in Pandas
	"desc": I want to group by "" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then "transpose" the dataframe and making the ranges new columns Expected output: 
}
"io": {
	"Frame-1": 
		
		           a    b
		   a        
		(0, 10]     2   BBB
		(10, 20]    3   BBB
		(20, 30]    1   AAA
		
	"Frame-2":
		    (0, 10]   (10, 20]   (20, 30]
		 
		AAA    0          0         1      
		BBB    2          3         0
		
		
}
"answer": {
	"desc": %s You can use : : 
	"code-snippets": [
		df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67561501
"link": https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns
"question": {
	"title": splitting list in dataframe columns to separate columns
	"desc": my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success 
}
"io": {
	"Frame-1": 
		    col1     col2     col3
		0  [1, a]  [1, a1]  [1, a2]
		1  [2, b]  [2, b1]  [2, b2]
		2  [3, c]  [3, c1]  [3, c2]
		
	"Frame-2":
		   col1     col2     col3  col4
		0  a         a1      a2    1
		1  b         b1      b2    2
		2  c         c1      c2    3
		
}
"answer": {
	"desc": %s Here is a way using and : 
	"code-snippets": [
		df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67506798
"link": https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met
"question": {
	"title": Reformat Dataframe / Add rows when condition is met
	"desc": I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only "1's". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! 
}
"io": {
	"Frame-1": 
		   ColA        ColB
		2021-03-09       1
		2021-03-09       3
		2021-03-10       2
		2021-03-10       1
		2021-03-10       2
		2021-03-11       2
		
	"Frame-2":
		   ColA         ColB
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-11       1
		2021-03-11       1
		
}
"answer": {
	"desc": %s Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = (pd.DataFrame(np.repeat(df.ColA, df.ColB))
		        .assign(ColB=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67361824
"link": https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns
"question": {
	"title": Convert dataframe objects to float by iterating over columns
	"desc": I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance 
}
"io": {
	"Frame-1": 
		   c1   c2
		0  -    75.0%
		1 -5.5% 65.8%
		.
		n  -    6.9%
		
	"Frame-2":
		   c1    c2
		0  0.0   75.0
		1 -5.5   65.8
		.
		n  0.0    6.9
		
}
"answer": {
	"desc": %s EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. 
	"code-snippets": [
		# Thanks to @tdy
		df.replace({'\%':'', r'^\s*-\s*$':0}, regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67357814
"link": https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan
"question": {
	"title": Python pandas dataframe apply result of function to multiple columns where NaN
	"desc": I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: 
}
"io": {
	"Frame-1": 
		    x   y   z
		0   a   1.0 2.0
		1   b   1.0 2.0
		2   c   1.0 2.0
		3   d   NaN NaN
		4   e   NaN NaN
		5   f   NaN NaN
		
	"Frame-2":
		   x    y    z
		0  a  1.0   2.0
		1  b  1.0   2.0
		2  c  1.0   2.0
		3  d   a1   a2
		4  e   b2   b1
		5  f   c3   c4
		
}
"answer": {
	"desc": %s you can fillna after calculating for the full dataframe and 
	"code-snippets": [
		out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')
		                       .set_axis(['y','z'],inplace=False,axis=1)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67335759
"link": https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python
"question": {
	"title": Checking if column headers match PYTHON
	"desc": I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? 
}
"io": {
	"Frame-1": 
		      ID  Open High Low  
		       1  64   66   52   
		
	"Frame-2":
		      ID Open High  Volume
		      1   33   45   30043
		
}
"answer": {
	"desc": %s You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: 
	"code-snippets": [
		import pandas as pd
		
		df1 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [64],
		        "High": [66],
		        "Low": [52]
		    }
		)
		
		df2 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [33],
		        "High": [45],
		        "Volume": [30043]
		    }
		)
		
		df1_columns = set(df1.columns)
		df2_columns = set(df2.columns)
		
		common_columns = df1_columns & df2_columns
		
		df1_columns_only = df1_columns - common_columns
		df2_columns_only = df2_columns - common_columns
		
		print("Columns only available in df1", df1_columns_only)
		print("Columns only available in df2", df2_columns_only)
		
		----------------------------------------------------------------------
		Columns only available in df1 {'Low'}
		Columns only available in df2 {'Volume'}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 26837998
"link": https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string
"question": {
	"title": Pandas Replace NaN with blank/empty string
	"desc": I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: 
}
"io": {
	"Frame-1": 
		    1    2       3
		 0  a  NaN    read
		 1  b    l  unread
		 2  c  NaN    read
		
	"Frame-2":
		    1    2       3
		 0  a   ""    read
		 1  b    l  unread
		 2  c   ""    read
		
}
"answer": {
	"desc": %s  This might help. It will replace all NaNs with an empty string. 
	"code-snippets": [
		import numpy as np
		df1 = df.replace(np.nan, '', regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67288220
"link": https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition
"question": {
	"title": How can I add a new line in pandas dataframe based in a condition?
	"desc": I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame. 
}
"io": {
	"Frame-1": 
		[1]   [2] [3]
		  1     30  2
		  1     30  1
		  1     30  3
		  1     90  3
		  1    370  3
		  1    430  3
		  1    705  3
		  1    805  3
		  1    880  2
		  1    905  3
		  1   1005  3
		  1   1170  3
		  1   1230  3
		  1   1970  3
		  1   2030  3
		  1   2970  3
		  1   3030  3
		  1   3970  3
		  1   4030  3
		  1   4423  3
		  1   4539  3
		  1   4575  3
		  1   4630  2
		  1   4635  3
		  1   4671  3
		  1   4787  3
		  1   4957  3
		  1   5057  3
		  1   5270  3
		  1   5330  3
		  1   5970  3
		  1   6030  3
		  1   6970  3
		  1   7030  3
		  1   7970  3
		  1   8030  3
		  1   8158  3
		  1   8257  3
		  1   8332  2
		  1   8357  3
		  1   8457  3
		  1   8970  3
		  1   9030  3
		  1   9970  3
		  1  10030  3
		  1  10970  3
		  1  11030  3
		  1  11470  3
		  1  11530  3
		  1  11853  3
		  1  11953  3
		
	"Frame-2":
		  1     90  3
		  1    190  0
		  1    370  3
		
}
"answer": {
	"desc": %s Try: Prints: EDIT: To change only one value: 
	"code-snippets": [
		m = df["[2]"].diff() > 100
		
		df.loc[m, "[2]"] = pd.Series(
		    [
		        [str(df.iloc[v - 1]["[2]"] + 100), df.iloc[v]["[2]"]]
		        for v in df.index[m]
		    ],
		    index=df.index[m],
		)
		
		df = df.explode("[2]")
		df["[3]"] = np.where(
		    df["[2]"].apply(lambda x: isinstance(x, str)), 0, df["[3]"]
		)
		df["[2]"] = df["[2]"].astype(int)
		print(df)
		
		----------------------------------------------------------------------
		mask = df["[2]"].diff() > 100
		if True in mask:
		    m = [False] * len(df)
		    m[mask.idxmax()] = True
		
		    df.loc[m, "[2]"] = pd.Series(
		        [
		            [str(df.iloc[v - 1]["[2]"] + 100), df.iloc[v]["[2]"]]
		            for v in df.index[m]
		        ],
		        index=df.index[m],
		    )
		
		    df = df.explode("[2]")
		    df["[3]"] = np.where(
		        df["[2]"].apply(lambda x: isinstance(x, str)), 0, df["[3]"]
		    )
		    df["[2]"] = df["[2]"].astype(int)
		    print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67257898
"link": https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column
"question": {
	"title": How to add a value to a new column by referencing the values in a column
	"desc": I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? 
}
"io": {
	"Frame-1": 
		id  reason  x1    x2   x3   x4   x5 
		 1  x1      100   15   10   20   25
		 2  x1      15    16   14   10   10
		 3  x4      10    50   40   30   25
		 4  x3      12    15   60   5    1
		 5  x1      80    15   10   20   25
		 6  x1      15    19   84   10   10
		 7  x4      90    40   90   30   25
		 8  x4      12    85   60   50   10
		
	"Frame-2":
		id  reason  x1    x2   x3   x4   x5   xy
		 1  x1      100   15   10   20   25   100
		 2  x1      15    16   14   10   10   15
		 3  x4      10    50   40   30   25   30
		 4  x3      12    15   60   5    1    60
		 5  x1      80    15   10   20   25   80
		 6  x1      15    19   84   10   10   15
		 7  x4      90    40   90   30   25   30
		 8  x4      12    85   60   50   10   50
		
}
"answer": {
	"desc": %s  Prints: 
	"code-snippets": [
		df["xy"] = df.apply(lambda x: x[x["reason"]], axis=1)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67255732
"link": https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe
"question": {
	"title": Summation of operation in dataframe
	"desc": I want to implement a function that does the operation that you can see in the image: But i not sure how to implement the Summation for the moment i doing something like that: And the problem is in the summation. If someone can help me. For example, i have two dataframes like that: Where for the abs operation we will obtain this: And for the sum: Finally we do the summation: where and 
}
"io": {
	"Frame-1": 
		   A  B
		0  5  14
		1  4  2
		
	"Frame-2":
		   A   B
		0  11  38
		1  8   36
		
}
"answer": {
	"desc": %s Use to sum the dataframe across columns/rows: Prints: 
	"code-snippets": [
		result = (
		    dataframe1.sub(dataframe2).abs().sum().sum()
		    / dataframe1.add(dataframe2).sum().sum()
		) * 100
		print(result)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67243081
"link": https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas
"question": {
	"title": Best way to change column data for all rows over multiple dataframes in pandas?
	"desc": Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example : 
}
"io": {
	"Frame-1": 
		+---+----+
		|   | id |
		+---+----+
		| 1 | a1 |
		+---+----+
		| 2 | c1 |
		+---+----+
		
	"Frame-2":
		+---+----+
		|   | id |
		+---+----+
		| 1 | b1 |
		+---+----+
		
}
"answer": {
	"desc": %s A very simple approach: 
	"code-snippets": [
		import itertools
		import uuid
		
		def rand_uuid():
		    return uuid.uuid4()
		
		rep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}
		
		df3.replace(rep_dict, inplace=True)
		df3.id = df3.id.map(lambda x: rand_uuid())
		
		df1.replace(rep_dict, inplace=True)
		df2.replace(rep_dict, inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67246859
"link": https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id
"question": {
	"title": How to convert rows into columns and filter using the ID
	"desc": I have a CSV file that looks like this: and I would like to use simple python or pandas to: Make each unique customer id in a separate row convert key_id to the columns titles and the values are the quantity The output table should look like this: I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips? 
}
"io": {
	"Frame-1": 
		customer_id |  key_id.  |  quantity |
		1           |    777    |    3      |
		1           |    888    |    2      |
		1           |    999    |    3      |
		2           |    777    |    6      |
		2           |    888    |    1      |
		
	"Frame-2":
		            |  777    |  888  |   999  | 
		1           |   3     |   2   |    3   |
		2           |   6     |   1   |    0   |
		
}
"answer": {
	"desc": %s You can pivot into columns using : To handle duplicates, averages them by default. To override this aggregation method, you can set the param (, , , , , etc.): 
	"code-snippets": [
		df.pivot_table(
		    index='customer_id',
		    columns='key_id',
		    values='quantity',
		    aggfunc='max',
		).fillna(0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67213950
"link": https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf
"question": {
	"title": How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?
	"desc": I have two dataframes: df1 df2 df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be: ABD123 = 1 (because it has 1 different character from ABC123) DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one) GHI789 = 3 (because it has an equivalent at df1) I know how to check for the differences of each code individually and save the "length" of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way? 
}
"io": {
	"Frame-1": 
		     Code     Number
		0   ABC123      1
		1   DEF456      2
		2   GHI789      3
		3   DEA456      4
		
	"Frame-2":
		     Code 
		0   ABD123
		1   DEA458
		2   GHI789
		
}
"answer": {
	"desc": %s  don't know how to compare each row from df2 against all rows from df1. Nested loops will work. If you had a function named it would look like this... Nested loops are usually not ideal when working with Pandas or Numpy but they do work. There may be better solutions. DataFrame.iterrows() 
	"code-snippets": [
		for index2, row2 in df2.iterrows():
		    for index1, row1 in df1.iterrows():
		        difference = compare(row2,row1)
		        #do something with the difference.
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67179293
"link": https://stackoverflow.com/questions/67179293/moving-data-from-rows-to-columns-based-on-another-column
"question": {
	"title": Moving data from rows to columns based on another column
	"desc": I have a huge dataset with contents such as given below: HHID can be present in the file at a maximum of three times. If HHID is found once, then the VAL_CD64/VAL_CD32 should be moved to VAL1_CD64/VAL1_CD32 columns, if found 2nd time, second value should be moved to VAL2_CD64/VAL2_CD32 columns, and if found 3rd time, third value should be moved to VAL3_CD64/VAL3_CD32 columns. If value is not found, then these columns should be left blank. Output should look something like this: I tried using pivot/melt in pandas but unable to get an idea to implement it. Can anyone help in giving me a lead? Thanks 
}
"io": {
	"Frame-1": 
		+------+------------------------------------------------------------------+----------------------------------+--+
		| HHID |                             VAL_CD64                             |             VAL_CD32             |  |
		+------+------------------------------------------------------------------+----------------------------------+--+
		|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 373aeda34c0b4ab91a02ecf55af58e15 |  |
		|  203 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | 6f3606577eadacef1b956307558a1efd |  |
		|  203 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 332321ab150879e930869c15b1d10c83 |  |
		|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | 2c4f97a04f02db5a36a85f48dab39b5b |  |
		|  720 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | 2111293e946703652070968b224875c9 |  |
		|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 5c80a555fcda02d028fc60afa29c4a40 |  |
		|  348 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 6c10cd11b805fa57d2ca36df91654576 |  |
		|  348 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 6040b29107adf1a41c4f5964e0ff6dcb |  |
		|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | 71a91c4768bd314f3c9dc74e9c7937e8 |  |
		+------+------------------------------------------------------------------+----------------------------------+--+
		
	"Frame-2":
		+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+
		| HHID |                            VAL1_CD64                             |                            VAL2_CD64                             |                            VAL3_CD64                             |            VAL1_CD32             |            VAL2_CD32             |            VAL3_CD32             |  |
		+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+
		|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 373aeda34c0b4ab91a02ecf55af58e15 | 6f3606577eadacef1b956307558a1efd | 332321ab150879e930869c15b1d10c83 |  |
		|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 |                                                                  | 2c4f97a04f02db5a36a85f48dab39b5b | 2111293e946703652070968b224875c9 |                                  |  |
		|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 5c80a555fcda02d028fc60afa29c4a40 | 6c10cd11b805fa57d2ca36df91654576 | 6040b29107adf1a41c4f5964e0ff6dcb |  |
		|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 |                                                                  |                                                                  | 71a91c4768bd314f3c9dc74e9c7937e8 |                                  |                                  |  |
		+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+
		
}
"answer": {
	"desc": %s One possible way is to combine and into list then split those list into columns: 
	"code-snippets": [
		df_ = df.groupby('HHID').agg({'VAL_CD32': list, 'VAL_CD64': list})
		
		data = []
		for col in df_.columns:
		    d = pd.DataFrame(df_[col].values.tolist(), index=df_.index)
		    d.columns = [f'{col}_{i}' for i in map(str, range(1, len(d.columns)+1))]
		    data.append(d)
		
		res = pd.concat(data, axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67111775
"link": https://stackoverflow.com/questions/67111775/python-pandas-check-each-element-in-list-values-of-column-to-exist-in-other-da
"question": {
	"title": Python, Pandas: check each element in list values of column to exist in other dataframe
	"desc": I have dataframe column with values in lists, want to add new column with filtered values from list if they are in other dataframe. df: df2: I need to add new column with filtered column in so that it contains lists with only elements which are in column . Result: Speed is crucial, as there is a huge amount of records. What I did for now: created a set of possible values Try to use with comprehensive lists, but it's not quite working and too slow. Appreciate any help. UPD In lists and df2 not always integer values, sometimes it's strings. 
}
"io": {
	"Frame-1": 
		**a**|**b**
		:-----:|:-----:
		1|[10, 1, 'xxx']
		2|[]
		5|[1, 2, 3]
		7|[5]
		9|[25, 27]
		
	"Frame-2":
		**a**|**b**|**c**
		:-----:|:-----:|:-----:
		1|[10, 1, 'xxx']|[1,'xxx']
		2|[]|[]
		5|[1, 2, 3]|[1]
		7|[5]|[5]
		
}
"answer": {
	"desc": %s You can try casting to and then or shorter on the findall pattern courtesy @Shubham: 
	"code-snippets": [
		l = map(str,df2['e'].unique())
		df['c'] = df['b'].astype(str).str.findall('|'.join([fr"\b{i}\b" for i in l]))
		
		----------------------------------------------------------------------
		l = map(str,df2['e'].unique())
		df['c'] = df['b'].astype(str).str.findall(fr"\b({'|'.join(l)})\b")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67105986
"link": https://stackoverflow.com/questions/67105986/how-to-edit-excel-file-using-dataframe-and-save-it-back-as-excel-file
"question": {
	"title": How to edit Excel file using DataFrame and save it back as Excel file?
	"desc": I have this Excel file. I also put the screenshot of my the file below. I want to edit the data on column with this 2 criteria: removing mark between the text. removing values. removing mark. So, for example, from this text: I want to make it look like this: Of course, I can do this manually one by one, but unfortunately because I have about 20 similar files that I have to edit, I can't do it manually, so I think I might need help from Python. My idea to do it on Python is to load the Excel file to a DataFrame, edit the data row by row (maybe using and method), and put the edit result back to original Excel file, or maybe generate a new one consisting an edited data column. But, I kinda have no idea on how to do code it. So far, what I've tried to do is this: read the Excel files to Python. read column in that Excel file. load it to a dataframe. Below is my current code. My question is how can I edit the data per row and put the edit result back again to original or a new Excel file? I have difficulties accessing the data because I can't get the string value. Is there any way in Python to achieve it? 
}
"io": {
	"Frame-1": 
		['0', 'E3', 'F3', 'F#3 / Gb3', 'G3', 'G#3 / Ab3', 'A3', 'A#3 / Bb3', 'B3', 'C4', 'C#4 / Db4', 'D4']
		
	"Frame-2":
		[E3, F3, F#3 / Gb3, G3, G#3 / Ab3, A3, A#3 / Bb3, B3, C4, C#4 / Db4, D4]
		
}
"answer": {
	"desc": %s In general you do not want to iterate over every row in a pandas dataframe, it is very slow. There are a lot of ways (that you can lean by practice over time) to apply functions over a column/row/the whole dataframe in pandas. In this example: Convert the column to type string, and replace the ' character with a blank space 
	"code-snippets": [
		df = pd.read_excel("014_twinkle_twinkle 300 0.0001 dataframe.xlsx")
		df["pitch-class"] = df["pitch-class"].astype(str).str.replace("'0', ", "")
		df["pitch-class"] = df["pitch-class"].astype(str).str.replace("'", "")
		df.to_excel("results.xlsx")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67087432
"link": https://stackoverflow.com/questions/67087432/match-multiple-columns-on-python-to-a-single-value
"question": {
	"title": Match multiple columns on Python to a single value
	"desc": I hope you are doing well. I am trying to perform a match based on multiple columns where my values of Column B of df1 is scattered in three to four columns in df2. The goal here is the the return the values of Column A of df2 if values of Column B matches any values in the columns C,D,E. What I did until now was actually to do multiple left merges (and changing the name of Column B to match the name of columns C,D,E of df2). I am trying to simplify the process but I am unsure how I am supposed to do this? My dataset looks like that: Df1: DF2: My goal is to have in df1: Thank you very much ! 
}
"io": {
	"Frame-1": 
		    ID
		0   77  
		1   4859    
		2   LSP
		
	"Frame-2":
		    ID     X
		0   77     AAAAA_XX
		1   4859   BBBBB_XX 
		2   LSP    CCCC_YY
		
}
"answer": {
	"desc": %s you can get all the values in the columns to one first with then we merge the tables like this: not the most beautiful code in the world, but it works. output: 
	"code-snippets": [
		df3 = pd.concat([df2.id1, df2.id2]).reset_index()
		df1 = df2.merge(df3, how="left", left_on = df1.ID, right_on = df3[0])
		df1 = df1.iloc[:, :2]
		df1 = df1.rename(columns={"key_0": "ID"})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67053308
"link": https://stackoverflow.com/questions/67053308/how-to-replace-zero-by-one-for-particular-row-in-data-frame
"question": {
	"title": How to replace &#39;Zero&#39; by &#39;One&#39; for particular row in data frame
	"desc": I've this dataframe:df1 I would like to Find the minimum value of last two entry of Variance row. I would like to last two entries and finding minimum , like in variance last two entries are 474.0 and 1101.0 and that should be added in Nan place. Output look like I've tried this code: 
}
"io": {
	"Frame-1": 
		Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  NaN  -0.0
		
	"Frame-2":
		Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  474.0 -0.0
		
}
"answer": {
	"desc": %s Use with set values by min (there is selected by position, it means for last previous label use ): Or is possible use for position by label name: Or if need select by use for seelct by labels, for dynamic columns names use indexing : 
	"code-snippets": [
		df1.iloc[-2, -2] = df1.iloc[-2, -4:-2].min()
		
		----------------------------------------------------------------------
		pos = df1.index.get_loc('Variance')
		df1.iloc[pos, -2] = df1.iloc[pos, -4:-2].min()
		
		----------------------------------------------------------------------
		df1.loc['Variance', df1.columns[-2]] = df1.loc['Variance', df1.columns[-4:-2]].min()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62050416
"link": https://stackoverflow.com/questions/62050416/pandas-dataframe-calculate-shared-fraction
"question": {
	"title": Pandas Dataframe: Calculate Shared Fraction
	"desc": Suppose I have a dataframe, , consisting of a class of two objects, , a set of co-ordinates associated with them, and , and a value, , that was measured there. The dataframe looks like this: I would like to know the commands that allow me to go from this picture to the one where each is converted to a series of columns where: represents the sum of all the shared coordinates; and represent the fractions of the V for each possible class, . For example: I can sum and fraction calculate the fraction by using What are the next steps? 
}
"io": {
	"Frame-1": 
		S X Y V
		0 1 1 1
		1 2 2 1
		1 9 9 2
		0 9 9 8
		
	"Frame-2":
		X Y V_s  F0  F1
		1 1 1  1.0 0.0
		2 2 1  0.0 1.0
		9 9 10 0.2 0.8
		
}
"answer": {
	"desc": %s You could try this: Output: Update for unknown/large number of classes in : And you get same output. 
	"code-snippets": [
		(df.groupby(['X','Y','S']).sum()
		   .unstack('S', fill_value=0)['V']
		   .rename(columns=lambda x: f"F{x}")
		   .assign(V_s=lambda x: x.sum(1),
		           F0 =lambda x: x['F0']/x['V_s'],
		           F1 =lambda x: x['F1']/x['V_s'])
		   .reset_index()
		)
		
		----------------------------------------------------------------------
		new_df = (df.groupby(['X','Y','S']).sum()
		   .unstack('S', fill_value=0)['V']
		   .rename(columns=lambda x: f"F{x}")
		)
		
		vs = new_df.sum(1)
		new_df = (new_df.div(vs,axis='rows')
		                .assign(V_s=vs)
		                .reset_index()
		         )
		          
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67027649
"link": https://stackoverflow.com/questions/67027649/how-to-unlist-a-list-with-one-value-inside-a-pandas-columns
"question": {
	"title": How to unlist a list with one value inside a pandas columns?
	"desc": I have a pandas data frame: Is possible to convert the data frame into another data frame that look like this? I tried with this way but only get the . Thanks for your time! 
}
"io": {
	"Frame-1": 
		Id       Col1
		1     ['string']
		2     ['string2']
		
	"Frame-2":
		Id     Col1
		1     string
		2     string2
		
}
"answer": {
	"desc": %s  I tried with this way but only get the [. Then this means they are ings, not s. You can convert them to s by ing and then : to get 
	"code-snippets": [
		import ast
		
		df.Col1 = df.Col1.apply(ast.literal_eval).explode()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66991966
"link": https://stackoverflow.com/questions/66991966/how-to-convert-this-dataframe-into-json
"question": {
	"title": How to convert this DataFrame into Json
	"desc": I have this with 2 columns when I try to convert it into it goes wrong: I don't even know ehere the numbers come from. My desired : 
}
"io": {
	"Frame-1": 
		print(df)
		
		     a                b
		
		     10          {'A': 'foo', ...}
		     20          {'B': 'faa', ...}
		     30          {'C': 'fee', ...}
		     40          {'D': 'fii', ...}
		     50          {'E': 'foo', ...}
		
		
	"Frame-2":
		[{
		   'a': 10,
		   'b': {
		         'A': 'foo', 
		         ...
		        }, 
		    ...
		    'a': 50,
		    'b': {
		         'E': 'foo', 
		         ...
		        }
		}
		]
		
		
}
"answer": {
	"desc": %s You could try the following: This should give you your desired output. If you want to convert this into a JSON file then you can do the following: 
	"code-snippets": [
		data = []
		for i in df:
		    data.append({'a': df[i[0]], 'b': df(i[1])})
		
		----------------------------------------------------------------------
		with open("myjson.json", "w") as f:
		    json.dump(data, f, indent=4)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66977794
"link": https://stackoverflow.com/questions/66977794/splitting-by-indices-i-want-to-split-the-train-test-from-the-data-whose-indic
"question": {
	"title": Splitting by indices: I want to split the train + test from the data whose indices have been given. How shall I get train/test df?
	"desc": for example= df is the data with features. I want to split the train + test from the data whose indices have been given. How shall I get train/test df. where train.txt is where in this dataframe indices are given. How should I get the training data from those indices? Contents in data_train.txt(there are 10000 of data in which train indices are given in this txt file) I want these indices for training data with feature:- like final train should look like this (see the index): 
}
"io": {
	"Frame-1": 
		df=
		0 2 0.3 0.5 0.5
		1 4 0.5 0.7 0.4
		2 2 0.5 0.1 0.4
		3 4 0.4 0.1 0.3
		4 2 0.3 0.1 0.5
		
	"Frame-2":
		0 2 0.3 0.5 0.5
		2 2 0.5 0.1 0.4
		4 2 0.3 0.1 0.5
		
}
"answer": {
	"desc": %s If you have a df as given by: and another train_indices as given by: then all you need to do to get the corresponding rows of depends on how the data is organised: Both of these (in this case) return: 
	"code-snippets": [
		#if you're trying to match the index of the df itself
		train_df = df.iloc[train_indices]
		#if you're trying to match column 0, which might be important 
		#if it's not aligned to the index
		train_df =  df.loc[df[0].isin(train_indices)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66939075
"link": https://stackoverflow.com/questions/66939075/how-to-divide-second-column-by-first-column-in-dataframe
"question": {
	"title": How to divide second column by first column in dataframe?
	"desc":  I've this triangle dataframe(df1) , i wanted to calculate new dataframe(df2) that contains the result:second_column(df2)/first_column(df2) and third_column(df2)/second_column(df2) and so on.. i tried like this(i know its wrong). and i wanted df2 like this: Thank You for your time.. 
}
"io": {
	"Frame-1": 
		     DP 1       DP 2        DP 3       DP 4        DP 5         DP 6        DP 7        DP 8       DP 9        DP 10
		 3,57,848    11,24,788   17,35,330   22,18,270   27,45,596   33,19,994   34,66,336   36,06,286   38,33,515   39,01,463 
		 3,52,118    12,36,139   21,70,033   33,53,322   37,99,067   41,20,063   46,47,867   49,14,039   53,39,085  
		 2,90,507    12,92,306   22,18,525   32,35,179   39,85,995   41,32,918   46,28,910   49,09,315      
		 3,10,608    14,18,858   21,95,047   37,57,447   40,29,929   43,81,982   45,88,268          
		 4,43,160    11,36,350   21,28,333   28,97,821   34,02,672   38,73,311              
		 3,96,132    13,33,217   21,80,715   29,85,752   36,91,712                  
		 4,40,832    12,88,463   24,19,861   34,83,130                      
		 3,59,480    14,21,128   28,64,498                          
		 3,76,686    13,63,294                              
		 3,44,014                                   
		
	"Frame-2":
		  DP 1   DP 2    DP 3    DP 4    DP 5    DP 6    DP 7    DP 8    DP 9   DP 10
		 3.14    1.54    1.28    1.24    1.21    1.04    1.04    1.06    1.02    -   
		 3.51    1.76    1.55    1.13    1.08    1.13    1.06    1.09    -      
		 4.45    1.72    1.46    1.23    1.04    1.12    1.06    -          
		 4.57    1.55    1.71    1.07    1.09    1.05    -              
		 2.56    1.87    1.36    1.17    1.14    -                  
		 3.37    1.64    1.37    1.24    -                      
		 2.92    1.88    1.44    -                          
		 3.95    2.02    -                              
		 3.62    -      
		
		                        
		
}
"answer": {
	"desc": %s First of all you need to remove the commas and convert the dataframe to float type: Then you can divide directly by shifting the columns once: Output Optionally you can round with: 
	"code-snippets": [
		df2 = df1.replace(',', '', regex = True).astype(float)
		
		----------------------------------------------------------------------
		df2 = df2.shift(-1, axis = 1).div(df2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66940820
"link": https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function
"question": {
	"title": Python: How to pass Dataframe Columns as parameters to a function?
	"desc": I have a dataframe with 2 columns of text embeddings namely and . I want to create a third column in named which should contain the cosine_similarity between every row of and . But when I try to implement this using the following code I get a . How to fix it? Dataframe Code to Calculate Cosine Similarity Error Required Dataframe 
}
"io": {
	"Frame-1": 
		           embedding_1              |            embedding_2                                 
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]
		 [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]
		
	"Frame-2":
		       embedding_1              |            embedding_2                 |  distances                        
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882
		 [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665
		 [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312
		
}
"answer": {
	"desc": %s You can use to use on each row: or one liner 
	"code-snippets": [
		def cal_cosine_similarity(row):
		    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])
		
		df['distances'] = df.apply(cal_cosine_similarity, axis=1)
		
		----------------------------------------------------------------------
		df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66934423
"link": https://stackoverflow.com/questions/66934423/how-to-convert-pandas-dataframe-into-the-numpy-array-with-column-names
"question": {
	"title": How to convert pandas dataframe into the numpy array with column names?
	"desc": How can I convert pandas into the following Numpy array with column names? This is my pandas DataFrame : I tried to convert it as follows: But it gives me the output as follows: For some reason, the rows of data are grouped instead of . 
}
"io": {
	"Frame-1": 
		col1  col2
		3     5
		3     1
		4     5    
		1     5
		2     2
		
	"Frame-2":
		[(3, 5), (3, 1), (4, 5), (1, 5), (1, 2)]
}
"answer": {
	"desc": %s Use the pandas function , which converts a dataframe to a numpy record array. the link is the following: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_records.html Some examples given in the website are the following: The index can be excluded from the record array: 
	"code-snippets": [
		>>> df.to_records(index=False)
		rec.array([(1, 0.5 ), (2, 0.75)],
		          dtype=[('A', '<i8'), ('B', '<f8')])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66929674
"link": https://stackoverflow.com/questions/66929674/duplicate-rows-and-rename-dataframe-indexes-using-a-list-of-suffixes
"question": {
	"title": Duplicate rows and rename DataFrame indexes using a list of suffixes
	"desc": I have a pandas DataFrame object as follow: for which I'd like to duplicate each using a list of suffixes: The list of suffixes is: and the list of indexes that must be changed is: . Notice , and are left untouched from the original DataFrame. From this answer: https://stackoverflow.com/a/50490890/6630397 I was able to duplicate the desired rows of my initial DataFrame to the right number according to the length of the list : But now all my DataFrame indices are an array of 10 of each , , and (except for , and where there are only 1 row) where I'd like them to follow the pattern of suffixes from as shown here above. How could I elegantly achieve that with good performances? (note: if it's much better to work from a column containing the indexes it's also fine, because my objects , , , , , and in the index column previously come from a standalone column named ). 
}
"io": {
	"Frame-1": 
		              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13
		object                                                                  
		A            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		B            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		C            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0
		D            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0
		E            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0
		F            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0
		G            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0
		
	"Frame-2":
		              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13
		object                                                                  
		A_XS         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		A_S          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		A_M          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		A_L          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		A_XL         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		A            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		B_XS         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		B_S          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		B_M          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		B_L          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		B_XL         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		B            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0
		C_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0
		C_S          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0
		C_M          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0
		C_L          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0
		C_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0
		C            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0
		D_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0
		D_S          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0
		D_M          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0
		D_L          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0
		D_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0
		D            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0
		E            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0
		F            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0
		G            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0
		
}
"answer": {
	"desc": %s You should first compute a list containing the new indexes for each row, then explode that list: gives as expected: 
	"code-snippets": [
		newvals = [['{}{}'.format(i,j) for j in ('_' + k if k != '' else k
		                                         for k in list_of_suffixes)]
		           if i in list_init else [i] for i in df.index]
		df.reset_index().assign(object=newvals).explode('object').set_index('object')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66846548
"link": https://stackoverflow.com/questions/66846548/assign-unique-id-to-pandas-group-but-add-one-if-repeated
"question": {
	"title": Assign unique ID to Pandas group but add one if repeated
	"desc": I couldn't find a solution and want something faster than what I already have. So, the idea is to assign a unique ID for 'fruit' column, e.g. However, if repeated, add 1 to the last result, so that instead of: I will end up with: So it adds up until the end, even if there may only be 4 fruits changing their positions. Here is my solution but it's really slow and I bet there is something that Pandas can do, inherently: Any ideas? 
}
"io": {
	"Frame-1": 
		df['id'] = [0, 0, 1, 1, 2, 0, 0, 2, 2]
		
	"Frame-2":
		df['id'] = [0, 0, 1, 1, 2, 3, 3, 4, 4]
		
}
"answer": {
	"desc": %s You can use followed by : Prints: Or if you prefer : 
	"code-snippets": [
		df["id"] = df.groupby((df["fruit"] != df["fruit"].shift(1)).cumsum()).ngroup()
		print(df)
		
		----------------------------------------------------------------------
		    fruit  id
		0   apple   0
		1   apple   0
		2  orange   1
		3  orange   1
		4   lemon   2
		5   apple   3
		6   apple   3
		7   lemon   4
		8   lemon   4
		
		----------------------------------------------------------------------
		from itertools import groupby
		
		data, i = [], 0
		for _, g in groupby(df["fruit"]):
		    data.extend([i] * sum(1 for _ in g))
		    i += 1
		
		df["id"] = data
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 33283249
"link": https://stackoverflow.com/questions/33283249/pandas-write-df-to-text-file-indent-df-to-right-by-5-white-spaces
"question": {
	"title": pandas: write df to text file - indent df to right by 5 white spaces
	"desc": I am writing a df to a text file like so: This works fine but how can I indent my df so it sits 5 white spaces to the right. so from this: to: Is this possible? Thanks. 
}
"io": {
	"Frame-1": 
		                    dim_pptx  qp_pptx
		Absolute Radio        0.0739   0.0753
		BBC Asian Network     0.0013   0.0013
		BBC Radio 1           0.1441   0.1455
		BBC Radio 1Xtra       0.0057   0.0058
		BBC Radio 2           0.2336   0.2339
		
	"Frame-2":
		                         dim_pptx  qp_pptx
		     Absolute Radio        0.0739   0.0753
		     BBC Asian Network     0.0013   0.0013
		     BBC Radio 1           0.1441   0.1455
		     BBC Radio 1Xtra       0.0057   0.0058
		     BBC Radio 2           0.2336   0.2339
		
}
"answer": {
	"desc": %s You can just do it with string manipulation after the conversion by replacing with . 
	"code-snippets": [
		" "*5 + df.to_string().replace("\n", "\n     ")
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66801447
"link": https://stackoverflow.com/questions/66801447/merge-pandas-dataframes-by-timestamps
"question": {
	"title": Merge pandas dataframes by timestamps
	"desc": I've got a few pandas dataframes indexed with timestamps and I would like to merge them into one dataframe, matching nearest timestamp. So I would like to have for example: What exact timestamp there is going to be in final DataFrame is not important to me. BTW. Is there an easy way to leter convert "absolute" timestamps into time from start (either in seconds or miliseconds)? So for this example: 
}
"io": {
	"Frame-1": 
		a = 
		                         CPU
		2021-03-25 13:40:44.208  70.571797
		2021-03-25 13:40:44.723  14.126870
		2021-03-25 13:40:45.228  17.182844
		
		b = 
		                          X   Y
		2021-03-25 13:40:44.193   45  1
		2021-03-25 13:40:44.707   46  1
		2021-03-25 13:40:45.216   50  2
		
		a + b =
		                         CPU       X   Y
		2021-03-25 13:40:44.208  70.571797 45  1
		2021-03-25 13:40:44.723  14.126870 46  1
		2021-03-25 13:40:45.228  17.182844 50  2
		
	"Frame-2":
		
		     CPU       X   Y
		0.0  70.571797 45  1
		0.5  14.126870 46  1
		1.0  17.182844 50  2
		
}
"answer": {
	"desc": %s Use with : 
	"code-snippets": [
		pd.merge_asof(df1, df2, left_index=True, right_index=True, direction='nearest')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66782284
"link": https://stackoverflow.com/questions/66782284/python-pandas-give-comma-separated-values-into-columns-with-title
"question": {
	"title": python pandas give comma separated values into columns with &quot;title&quot;
	"desc": I have some comma-separated data in the same column and I wish to separate each value into different columns. I have done separation using below and the output I got is but I need something like below (has to give some titles for each column) How would I do that? 
}
"io": {
	"Frame-1": 
		0          13.4119837, 42.082885, 13.4119837, 42.082885
		1        11.6285463, 42.4193742, 11.6285463, 42.4193742
		2            -3.606772, 39.460299, -3.606772, 39.460299
		3            -0.515639, 38.988847, -0.515639, 38.988847
		4            -2.403309, 37.241792, -2.403309, 37.241792
		
	"Frame-2":
		     0           1           2           3
		0   13.4119837  42.082885   13.4119837  42.082885
		1   11.6285463  42.4193742  11.6285463  42.4193742
		2   -3.606772   39.460299   -3.606772   39.460299
		3   -0.515639   38.988847   -0.515639   38.988847
		4   -2.403309   37.241792   -2.403309   37.241792
		
}
"answer": {
	"desc": %s Use : 
	"code-snippets": [
		data = data['column_name'].str.split(",", n = 3, expand = True)
		data.columns = ['minLat', 'maxLat', 'minLong', 'maxLong']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66638218
"link": https://stackoverflow.com/questions/66638218/group-a-dataframe-on-one-column-and-take-max-from-one-column-and-its-correspondi
"question": {
	"title": Group a dataframe on one column and take max from one column and its corresponding value from the other col
	"desc": I have a large dataframe which has a similar pattern as below: And can be constructed as: Now I want to group this dataframe by the first column i.e., and take from the column and its corresponding value from . And if there are two max values in , then I would like to take alphabetically first value from . So my expected result would look like: I have tried but this selects max from and first from both at the same time. Additionally I know there is a approach, but this would take a lot of time for my dataset. Any suggestions on how could I proceed would be appreciated. Thanks in advance:) 
}
"io": {
	"Frame-1": 
		    X   Y   Z
		0   a   p   2
		1   a   q   5
		2   a   r   6
		3   a   s   3
		4   b   w   10
		5   b   z   20
		6   b   y   9
		7   b   x   20
		
	"Frame-2":
		df = {
		    'X': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],
		    'Y': ['p', 'q', 'r', 's', 'w', 'x', 'y', 'z'],
		    'Z': [2, 5, 6, 3, 10, 20, 9, 5]
		}
		
}
"answer": {
	"desc": %s Let us try + : 
	"code-snippets": [
		df.sort_values(['X', 'Z', 'Y'], ascending=[True, False, True]).drop_duplicates('X')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 20025882
"link": https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas
"question": {
	"title": add a string prefix to each value in a string column using Pandas
	"desc": I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly). I already figured out how to kind-of do this and I am currently using: This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)? In case this is yet unclear, I would like to turn: into: 
}
"io": {
	"Frame-1": 
		    col 
		1     a
		2     0
		
	"Frame-2":
		       col 
		1     stra
		2     str0
		
}
"answer": {
	"desc": %s  Example: 
	"code-snippets": [
		df['col'] = 'str' + df['col'].astype(str)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59225169
"link": https://stackoverflow.com/questions/59225169/how-to-convert-data-frame-column-list-value-to-element
"question": {
	"title": How to convert data frame column list value to element
	"desc": Hi I have a dataframe like this: I want to change it into: How can I do that? I tries with 
}
"io": {
	"Frame-1": 
		        A
		   0    []
		   1    [1234] 
		   2    []
		
	"Frame-2":
		        A
		   0    0
		   1    1234 
		   2    0
		
}
"answer": {
	"desc": %s First idea is select first value, replace missing values to from empty lists and last convert to integers: Or use by mask with compare lenghts of lists: 
	"code-snippets": [
		df['A'] = np.where(df['A'].str.len() == 0, 0, df['A'].str[0]) 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54669058
"link": https://stackoverflow.com/questions/54669058/find-average-of-every-column-in-a-dataframe-grouped-by-column-exluding-one-val
"question": {
	"title": Find average of every column in a dataframe, grouped by column, exluding one value
	"desc": I have a Dataframe like the one presented below: What I want is to and find the average of each label. So far I have this which works just fine and get the results as follows: The only thing I haven't yet found is how to exclude everything that is labeled as . Is there a way to do that? 
}
"io": {
	"Frame-1": 
		    CPU Memory Disk  Label
		0    21     28   29      0
		1    46     53   55      1
		2    48     45   49      2
		3    48     52   50      3
		4    51     54   55      4
		5    45     50   56      5
		6    50     83   44     -1 
		
	"Frame-2":
		Label           CPU     Memory       Disk 
		    -1     46.441176  53.882353  54.176471
		     0     48.500000  58.500000  60.750000
		     1     45.000000  51.000000  60.000000
		     2     54.000000  49.000000  56.000000
		     3     55.000000  71.500000  67.500000
		     4     53.000000  70.000000  71.000000
		     5     21.333333  30.000000  30.666667
		
}
"answer": {
	"desc": %s You could filter the dataframe before grouping: 
	"code-snippets": [
		# Exclude rows with Label=-1
		dataset = dataset.loc[dataset['Label'] != -1]
		
		# Group by on filtered result
		dataset.groupby('Label')['CPU', 'Memory', 'Disk'].mean()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66567933
"link": https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t
"question": {
	"title": Can I shift specific values in one data column to another column while keeping the other values unchanged?
	"desc": Here is an example dataset that I have: I want to take all the values that have "1" in them in the Column "C2" and shift them to replace the adjacent values in column "C1". So the output should look like: Alternatively, I could create a new column with these values replaced. Main point is, that I need all the "1s" in C2 TO replace the NaN values in C1. I can't do find all NaN and replace with 1, because there are some NaN values that should stay in C1. Is there a way to do this? Thanks for the help in advance. 
}
"io": {
	"Frame-1": 
		C1      C2
		 1       1
		NaN      1
		 2       0
		NaN      0
		NaN      1
		 1       1
		 2       2
		 2       2
		NaN      1
		
	"Frame-2":
		C1      C2
		 1       1
		 1       1
		 2       0
		 NaN     0
		 1       1
		 1       1
		 2       2
		 2       2
		 1       1
		
		
}
"answer": {
	"desc": %s You could use the api to apply values from one column to another in rows where some condition is true (e.g. ==1). 
	"code-snippets": [
		value = 1
		source_col = 1
		target_col = 0
		
		condition = df[source_col] == value
		
		df[target_col] = df[target_col].mask(condition,
		                                     df[source_col])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66545100
"link": https://stackoverflow.com/questions/66545100/how-to-transform-rows-of-other-columns-to-columns-on-the-basis-of-unique-values
"question": {
	"title": How to transform rows of other columns to columns on the basis of unique values of a column?
	"desc": Suppose I have a df in the following structure, relation between column1 to column2 - one to many relation between column2 to column1 - one to many Expected Output: Also, while transforming, for every column7 can I create an empty column right beside column6_yyyymm? Final Output, How can I achieve Final Output using a python function and/or pandas library? If there is anything unclear please let me know. UPDATE: For all empty_yyyymm columns I want to implement the following function, How can achieve this too? Note: yyyymm is generic way of referring column7. It is not actually a column. 
}
"io": {
	"Frame-1": 
		column1 | column2 | column3 | column4 | column5 | column6 | column7
		   A    |    B    |    C    |    10   |    78   |   12    |  202001
		   A    |    B    |    D    |    21   |    64   |   87    |  202001
		   A    |    B    |    E    |    21   |    64   |   87    |  202001
		   X    |    K    |    C    |    54   |    23   |   23    |  202001
		   X    |    K    |    D    |    21   |    55   |   87    |  202001
		   X    |    K    |    E    |    21   |    43   |   22    |  202001
		   A    |    B    |    C    |    10   |    78   |   12    |  202002
		   A    |    B    |    D    |    23   |    64   |   87    |  202002
		   A    |    B    |    E    |    21   |    11   |   34    |  202002
		   Z    |    K    |    C    |    10   |    78   |   12    |  202002
		   Z    |    K    |    D    |    21   |    13   |   56    |  202002
		   Z    |    K    |    E    |    12   |    77   |   34    |  202002
		
	"Frame-2":
		column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 | column4_202002 | column5_202002 | column6_202002 |
		   A    |    B    |    C    |      10        |       78       |       12       |      10        |      78        |      12        |
		   A    |    B    |    D    |      21        |       64       |       87       |      23        |      64        |      87        |
		   A    |    B    |    E    |      21        |       64       |       87       |      21        |      11        |      34        |   
		   X    |    K    |    C    |      54        |       23       |       23       |       0        |       0        |       0        |   
		   X    |    K    |    D    |      21        |       55       |       87       |       0        |       0        |       0        |   
		   X    |    K    |    E    |      21        |       43       |       22       |       0        |       0        |       0        |    
		   Z    |    K    |    C    |       0        |        0       |        0       |      10        |      78        |      12        |    
		   Z    |    K    |    D    |       0        |        0       |        0       |      21        |      13        |      56        |   
		   Z    |    K    |    E    |       0        |        0       |        0       |      12        |      77        |      34        |  
		
}
"answer": {
	"desc": %s First create empty column by , then reshape by with and sorting datetimes in second level by : Then set values missing to all columns, flatten by and last convert to columns by : EDIT: First count new column by conditions and then apply solution above without set like: 
	"code-snippets": [
		df = (df.assign(empty = np.nan)
		        .set_index(['column1','column2','column3','column7'])
		        .unstack(fill_value=0)
		        .sort_index(level=1, axis=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66523605
"link": https://stackoverflow.com/questions/66523605/copy-the-last-seen-non-empty-value-of-a-column-based-on-a-condition-in-most-effi
"question": {
	"title": Copy the last seen non empty value of a column based on a condition in most efficient way in Pandas/Python
	"desc": I need to copy and paste the previos non-empty value of a column based on a condition. I need to do it in the most efficient way because the number of rows is a couple of millions. Using for loop will be computationally costly. So it will be highly appreciated if somebody can help me in this regard. Based on the condition, whenever the Col_A will have any value (not null) 10.2.6.1 in this example, the last seen value in Col_B (51,61 respectively) will be paste on that corresponding row where the Col_A value is not null. And the dataset should look like this: I tried with this code below but it's not working: 
}
"io": {
	"Frame-1": 
		|Col_A   |Col_B   |
		|--------|--------|
		|10.2.6.1| NaN    |
		|  NaN   | 51     |
		|  NaN   | NaN    |
		|10.2.6.1| NaN    |
		|  NaN   | 64     |
		|  NaN   | NaN    |
		|  NaN   | NaN    |
		|10.2.6.1| NaN    |
		
	"Frame-2":
		|Col_A   |Col_B   |
		|--------|--------|
		|10.2.6.1| NaN    |
		|  NaN   | 51     |
		|  NaN   | NaN    |
		|10.2.6.1| 51     |
		|  NaN   | 64     |
		|  NaN   | NaN    |
		|  NaN   | NaN    |
		|10.2.6.1| 64     |
		
}
"answer": {
	"desc": %s You can forward-fill the NaN values using with the most recent non-NaN value. If you want to keep the NaNs in then simply create a new column () as follows: Then replace the value in where has a value: Result: The above can be simplified if you do not need to keep all NaN rows. For example, it's possible to do: Result: 
	"code-snippets": [
		df.loc[df['Col_A'].notnull(), 'Col_B'] = df.loc[df['Col_A'].notnull(), 'Col_C']
		df = df.drop(columns=['Col_C'])
		
		----------------------------------------------------------------------
		df['Col_B'] = df['Col_B'].ffill()
		df = df.dropna()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66517526
"link": https://stackoverflow.com/questions/66517526/fill-a-dataframe-column-with-list-of-values-if-condition-is-not-satisfied-based
"question": {
	"title": Fill a Dataframe column with list of values if condition is not satisfied based on some other column
	"desc": I have a dataframe that looks like this - I also have a list of values like this I want to construct a third column which should have "Yes" if the colour is "red" in , else should have 1,3,2 on respective rows. Basically, should have values from the label one after the other if colour is "blue". Expected output - My Approach - I have tried to impute using like this , but the I believe this is due to the difference in the size of and (5 vs 3). Can anyone help me out, please? Thanks EDIT: Expected Output added Made some mistake in My Approach demonstration, corrected that. 
}
"io": {
	"Frame-1": 
		col_1   |   col_2
		-------------------
		"red"   |    21
		-------------------
		"blue"  |    31
		-------------------
		"red"   |    12
		-------------------
		"blue"  |    99
		-------------------
		"blue"  |    102
		
	"Frame-2":
		col_1   |   col_2    | col_3
		---------------------------
		"red"   |    21      |  "Yes"
		-----------------------------
		"blue"  |    31      |  "1"
		------------------------------
		"red"   |    12      | "Yes"
		------------------------------
		"blue"  |    99      |  "3"
		------------------------------
		"blue"  |    102     |  "2"
		
}
"answer": {
	"desc": %s You can try this with boolean masking. First, preemptively assign to the whole column, then create a boolean mask using . In your case create a mask where values are not equal to and use that mask to populate values. 
	"code-snippets": [
		df['col_3'] = 'Yes'
		m = df['col_1'].ne('Red') # ne -> not equal to
		df.loc[m, 'col_3'] = label
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66422275
"link": https://stackoverflow.com/questions/66422275/how-to-manipulate-data-cell-by-cell-in-pandas-df
"question": {
	"title": How to manipulate data cell by cell in pandas df?
	"desc": Let the sample df (df1) be, We can achieve df2 or final data-frame by manipulating the data of df1 in the following manner, Step 1: Remove all positive numbers including zeros After Step 1 the sample data should look like, Step 2: If A row is a negative number and B is blank, then remove the -ve number of A row Step 3: If A row is blank and B is a negative number, then keep the -ve number of B row After Steps 1,2 and 3 are done, Step 4: If both A and B of are negative then, For each A and B row of , check the left-side (LHS) value (for a given month) of the same A and B row of Step 4.1: If either of the LHS values of A or B is a -ve number, then delete the current row value of B and keep the current row value of A After Step 4.1, the sample data should look like this, Step 4.2: If the LHS value of A and B is blank, then keep the current row value of B and delete the current row value of A Sample data after Step 4.2 should look like, Since we see two negative numbers still, we perform Step 4.1 again and then the final data-frame or df2 will look like, How may I achieve the above using pandas? I was able to achieve till Step 1 but have no idea as to how to proceed further. Any help would be greatly appreciated. This is the approach that I took, Small Test data: df1, df2 (expected output), Test data: df1 df2 (expected output) , Note: I have implemented my code on the basis of the Test data provided. The sample data is merely to focus on the columns that are supposed to be manipulated. 
}
"io": {
	"Frame-1": 
		{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [0, 10, 0, 0], 'Mar-21': [0, 0, 70, 70], 'Apr-21': [-10, -10, -8, 60], 'May-21': [-30, -60, -10, 40], 'Jun-21': [-20, 9, -40, -20], 'Jul-21': [30, -10, 0, -20], 'Aug-21': [-30, -20, 0, -20], 'Sep-21': [0, -15, 0, -20], 'Oct-21': [0, -15, 0, -20]}
		
	"Frame-2":
		{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan], 'Apr-21': [nan, -10.0, nan, nan], 'May-21': [-30.0, nan, nan, nan], 'Jun-21': [nan, nan, nan, -20.0], 'Jul-21': [nan, -10.0, nan, -20.0], 'Aug-21': [-30.0, nan, nan, -20.0], 'Sep-21': [nan, -15.0, nan, -20.0], 'Oct-21': [nan, -15.0, nan, -20.0]}
		
}
"answer": {
	"desc": %s What about this ? For each step, I group on , then set as index and work on the transpose matrix with your criteria. Note that I've extrapoled a bit on your criteria to match your attended results (I hope it is correct but you will have to check that). Note also that I have kept each step separate to make it easier to read. But it would be more efficient to make the grouping/indexing/transposing in one shot and work on your algorithm from there. EDIT I'll assume here (based on your previous comment) that your dataframe will always be composed of A/B rows in alternance (and that the order in the dataframe is valid). We will then need to compute an artificial index to indentify each pair of rows. Note that I used a on your columns (mainly column5) as it is good practice with the commands. Due to multiple levels of columns, I'm not sure it will have an impact anyway... Boolean indexing begins to be tricky when managing multiple levels of columns. You will see I will compute each "column" to a numpy.array (using the method). Somehow, pandas won't perform the boolean match on multiple columns, I'm not exactly sure why. So this goes : And if you want to restore your column5 : 
	"code-snippets": [
		df = pd.DataFrame(  ...  )
		
		#Compute the unique index for each pair of rows
		df.reset_index(drop=False, inplace=True)
		ix = df.index
		ix = ix[ix%2==0]
		df.loc[ix, 'index'] = df.shift(-1).loc[ix, 'index']
		
		#step1 :
		cols = [x for x in df.columns.tolist() if not x.startswith('column') and x != "index"]
		df[cols] = df[cols].where(df[cols] < 0, np.nan)
		
		
		cols_index = ["column4", "column1", "column2", "column3", "column5", "column6", "column7"]
		df[cols_index] = df[cols_index].fillna(-1)
		
		#step2 :
		def step2(df):
		    df = df.set_index(cols_index).drop('index', axis=1).T
		    ix = df[
		            (df.A<=0).values
		            & df.B.isnull().values
		         ].index
		    df.loc[ix, "A"] = np.nan
		    return df.T
		df = df.groupby('index').apply(step2)
		print(df)
		df.reset_index(drop=False, inplace=True)
		print(df)
		print('-'*50)
		
		
		#step3 :
		def step3(df):
		    df = df.set_index(cols_index).drop('index', axis=1).T
		    ix = df[
		            df.A.isnull().values 
		            & (df.B>=0).values
		            ].index
		    df.loc[ix, "B"] = df.loc[ix, "A"]
		    return df.T
		df = df.groupby('index').apply(step3)
		df.reset_index(drop=False, inplace=True)
		print(df)
		print('-'*50)
		
		#step4 :
		def step4(df):
		    df = df.set_index(cols_index).drop('index', axis=1).T
		    a_pos = df.columns.get_loc('A')
		    b_pos = df.columns.get_loc('B')
		    
		    #step 4.1
		    ix = df[
		            (df.A<0).values
		            & (df.B<0).values
		            ].index
		    if len(ix):
		        ix = df.index.get_indexer(ix)
		        left_pos = ix-1
		        condition_left = df.iloc[left_pos].notnull().any(axis=1)
		        ix = condition_left[condition_left].index
		        ix = df.index.get_indexer(ix)
		        df.iloc[ix+1, b_pos] = np.nan
		    
		    #step 4.2
		    ix = df[
		            (df.A<0).values
		            & (df.B<0).values
		            ].index
		    if len(ix):
		        ix = df.index.get_indexer(ix)
		        left_pos = ix-1
		        condition_left = df.iloc[left_pos].isnull().all(axis=1)    
		        ix = condition_left[condition_left].index
		        ix = df.index.get_indexer(ix)
		        df.iloc[ix+1, a_pos] = np.nan
		    
		    #step 4.1 (again)
		    ix = df[
		            (df.A<0).values
		            & (df.B<0).values
		            ].index
		    if len(ix):
		        ix = df.index.get_indexer(ix)
		        left_pos = ix-1
		        condition_left = df.iloc[left_pos].notnull().any(axis=1)
		        ix = condition_left[condition_left].index
		        ix = df.index.get_indexer(ix)
		        df.iloc[ix+1, b_pos] = np.nan
		        
		    return df.T
		
		df = df.groupby('index').apply(step4)
		df.reset_index(drop=False, inplace=True)
		print(df)
		print('-'*50)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66515491
"link": https://stackoverflow.com/questions/66515491/how-to-convert-lists-array-entries-in-a-column-to-one-row-with-different-columns
"question": {
	"title": how to convert lists/array entries in a column to one row with different columns for each entry
	"desc": I have a dataframe where one column called has its entries as lists of numbers over 1064 rows. So each row contains 6 to 7 columns with the column where over each row it contains a list of numbers. I want to take this list, and spread it over the columns till while the number of columns is equal to . Here's an example: That's the first entry of the column let's say I want to spread it over the dataframe in a way where it would be formatted like the following: And most importantly I want to iterate this process over the 1064 rows. Thank you for your help! 
}
"io": {
	"Frame-1": 
		print(df1[0])>>>
		[-0.03980884 -0.18056028  0.11624704  0.08659928  0.02749503 -0.23401791
		  0.10772136 -0.32243717 -0.09397306 -0.08458275  0.11873401  0.10531124
		  0.11620065 -0.1100786  -0.27929837 -0.06915713 -0.11539902  0.26890758
		 -0.16375561  0.00525901  0.01196074  0.15442082  0.10281886 -0.15471214
		 -0.22901823  0.11486725 -0.05937155 -0.00580112 -0.25958595 -0.27098128
		 -0.03174639 -0.20656739 -0.13286862 -0.07104845 -0.04765386 -0.08396237
		  0.14032942 -0.15563552 -0.17417437  0.02441286  0.06222694 -0.08691377
		  0.08214904 -0.08121296 -0.079873    0.06362587  0.06934057  0.07980402
		 -0.08373277 -0.08293616 -0.07830499 -0.08762348  0.07899728 -0.04922628
		 -0.02680833 -0.0853695  -0.03179847  0.00792945  0.02782207]
		
	"Frame-2":
		col0        col1         col2        col3       ......col59      
		-0.03980884 -0.18056028  0.11624704  0.08659928 ......0.02782207
		
}
"answer": {
	"desc": %s You can generate another dataframe which contains the values you have in the lists of the column by using the last row of the following code (first three rows are used to generate data): 
	"code-snippets": [
		import numpy as np 
		df = pd.DataFrame(columns = ['features']) 
		df['features'] = list(np.random.random((6,3))) + list(np.random.random((6,4)))
		res = pd.DataFrame(df['features'].values.tolist()).add_prefix('col')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66507107
"link": https://stackoverflow.com/questions/66507107/select-only-those-rows-from-a-dataframe-where-certain-columns-with-suffix-have-v
"question": {
	"title": Select only those rows from a Dataframe where certain columns with suffix have values not equal to zero
	"desc": I want to select only those rows from a dataframe where certain columns with suffix have values not equal to zero. Also the number of columns is more so I need a generalised solution. eg: The dataframe would be : The desired output is : (because of 2 in CA_DIFF and 1 in BC_DIFF) This works with using multiple conditions but what if the number of DIFF columns are more? Like 20? Can someone provide a general solution? Thanks. 
}
"io": {
	"Frame-1": 
		   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF
		0   1     10     10       0      10      10        0      10      10        0
		1   2     12     12       0      12      12        0      12      12        0
		2   3     14     14       0      16      14        2      14      14        0
		3   4     16     16       0      16      16        0      16      16        0
		4   5     18     18       0      18      18        0      18      17        1
		
	"Frame-2":
		   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF
		0   3     14     14       0      16      14        2      14      14        0
		1   5     18     18       0      18      18        0      18      17        1
		
}
"answer": {
	"desc": %s You can do this: 
	"code-snippets": [
		
		...
		# get all columns with X_DIFF
		columns = df.columns[df.columns.str.contains('_DIFF')]
		
		# check if any has value greater than 0
		df[df[columns].transform(lambda x: x > 0).any(axis=1)]
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66496528
"link": https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex
"question": {
	"title": Repeating single DataFrame with changing DateTimeIndex
	"desc": Let's say I have very simple DataFrame like this: Output: I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this: It's still the same DataFrame, repeating again and again, and year is incrementally changed. I could do something like this (example for 3 years): I have mainly two questions: Is there a way how to do this in a single command? What is the best way how to deal with leap-year? 
}
"io": {
	"Frame-1": 
		             A  B   C   D
		2010-01-31   6  0   8  10
		2010-02-28   7  8  10   3
		2010-03-31  10  5   8  10
		2010-04-30   4  4   9   7
		2010-05-31   2  3   0  11
		2010-06-30   8  7  10   8
		2010-07-31  11  9   0   4
		2010-08-31   0  3   8   6
		2010-09-30   4  6   7   9
		2010-10-31   1  0  11   9
		2010-11-30   5  4   8   4
		2010-12-31   1  4   5   1
		
	"Frame-2":
		             A  B   C   D
		2010-01-31   6  0   8  10
		2010-02-28   7  8  10   3
		2010-03-31  10  5   8  10
		2010-04-30   4  4   9   7
		2010-05-31   2  3   0  11
		2010-06-30   8  7  10   8
		2010-07-31  11  9   0   4
		2010-08-31   0  3   8   6
		2010-09-30   4  6   7   9
		2010-10-31   1  0  11   9
		2010-11-30   5  4   8   4
		2010-12-31   1  4   5   1
		2011-01-31   6  0   8  10
		2011-02-28   7  8  10   3
		2011-03-31  10  5   8  10
		2011-04-30   4  4   9   7
		2011-05-31   2  3   0  11
		2011-06-30   8  7  10   8
		2011-07-31  11  9   0   4
		2011-08-31   0  3   8   6
		2011-09-30   4  6   7   9
		2011-10-31   1  0  11   9
		2011-11-30   5  4   8   4
		2011-12-31   1  4   5   1
		2012-01-31   6  0   8  10
		2012-02-28   7  8  10   3
		2012-03-31  10  5   8  10
		2012-04-30   4  4   9   7
		2012-05-31   2  3   0  11
		2012-06-30   8  7  10   8
		2012-07-31  11  9   0   4
		2012-08-31   0  3   8   6
		2012-09-30   4  6   7   9
		2012-10-31   1  0  11   9
		2012-11-30   5  4   8   4
		2012-12-31   1  4   5   1
		
}
"answer": {
	"desc": %s Here's an option with applying to the original index in list comprehension: 
	"code-snippets": [
		data_new = pd.concat([
		    df.set_index(df.index + pd.DateOffset(year=x)) for x in [2010, 2011, 2012]])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66375262
"link": https://stackoverflow.com/questions/66375262/specify-number-of-spaces-between-pandas-dataframe-columns-when-printing
"question": {
	"title": specify number of spaces between pandas DataFrame columns when printing
	"desc": When you print a pandas DataFrame, which calls DataFrame.to_string, it normally inserts a minimum of 2 spaces between the columns. For example, this code outputs which has a minimum of 2 spaces between each column. I am copying DataFarames printed on the console and pasting it into documents, and I have received feedback that it is hard to read: people would like more spaces between the columns. Is there a standard way to do that? I see no option in either DataFrame.to_string or pandas.set_option. I have done a web search, and not found an answer. This question asks how to remove those 2 spaces, while this question asks why sometimes only 1 space is between columns instead of 2 (I also have seen this bug, hope someone answers that question). My hack solution is to define a function that converts a DataFrame's columns to type str, and then prepends each element with a string of the specified number of spaces. This code (added to the code above) outputs which is the desired effect. But I think that pandas surely must have some builtin simple standard way to do this. Did I miss how? Also, the solution needs to handle a DataFrame whose columns are a MultiIndex. To continue the code example, consider this modification: 
}
"io": {
	"Frame-1": 
		       c1  c2  a3235235235
		0       a  11            1
		1      bb  22            2
		2     ccc  33            3
		3    dddd  44            4
		4  eeeeee  55            5
		
	"Frame-2":
		          c1     c2    a3235235235
		0          a     11              1
		1         bb     22              2
		2        ccc     33              3
		3       dddd     44              4
		4     eeeeee     55              5
		
}
"answer": {
	"desc": %s You can accomplish this through ; it takes a bit of code to create the dictionary . Find the max character length in each column or the length of the column header, whichever is greater, add some padding, and then pass a formatting string. Use from as the formatters expect a one parameter function, yet we need to specify a different width for each column. Sample Data Code 
	"code-snippets": [
		from functools import partial
		
		# Formatting string 
		def get_fmt_str(x, fill):
		    return '{message: >{fill}}'.format(message=x, fill=fill)
		
		# Max character length per column
		s = df.astype(str).agg(lambda x: x.str.len()).max() 
		
		pad = 6  # How many spaces between 
		fmts = {}
		for idx, c_len in s.iteritems():
		    # Deal with MultIndex tuples or simple string labels. 
		    if isinstance(idx, tuple):
		        lab_len = max([len(str(x)) for x in idx])
		    else:
		        lab_len = len(str(idx))
		
		    fill = max(lab_len, c_len) + pad - 1
		    fmts[idx] = partial(get_fmt_str, fill=fill)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66422239
"link": https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list
"question": {
	"title": Python transposing multiple dataframes in a list
	"desc": I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you. 
}
"io": {
	"Frame-1": 
		0    email           factor1_final   factor2_final   factor3_final
		1    john@abc.com    85%             90%             50%
		2    peter@abc.com   80%             60%             60%
		3    shelby@abc.com  50%             70%             60%
		4    jess@abc.com    60%             65%             50% 
		5    mark@abc.com    98%             50%             60%
		
	"Frame-2":
		
		email     john@abc.com   peter@abc.com   shelby@abc.com   jess@abc.com   mark@abc.com
		factor1     85%          80%             50%              60%            98%
		factor2     90%          60%             70%              65%            50% 
		factor3     50%          60%             60%              50%            60%
		
}
"answer": {
	"desc": %s For me second solution working, here is small alternative: Solution with create new list of DataFrames: 
	"code-snippets": [
		dfs = [df.set_index('email').T for df in df_list]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66401782
"link": https://stackoverflow.com/questions/66401782/update-table-information-based-on-columns-of-another-table
"question": {
	"title": update table information based on columns of another table
	"desc": I am new in python have two dataframes, df1 contains information about all students with their group and score, and df2 contains updated information about few students when they change their group and score. How could I update the information in df1 based on the values of df2 (group and score)? df1 The result df: 3 my code to update df1 from df2 but I face the following error 
}
"io": {
	"Frame-1": 
		   +----+----------+-----------+----------------+
		    |    |student No|   group   |       score    |
		    |----+----------+-----------+----------------|
		    |  0 |        0 |         0 |       0.839626 |
		    |  1 |        1 |         0 |       0.845435 |
		    |  2 |        2 |         3 |       0.830778 |
		    |  3 |        3 |         2 |       0.831565 |
		    |  4 |        4 |         3 |       0.823569 |
		    |  5 |        5 |         0 |       0.808109 |
		    |  6 |        6 |         4 |       0.831645 |
		    |  7 |        7 |         1 |       0.851048 |
		    |  8 |        8 |         3 |       0.843209 |
		    |  9 |        9 |         4 |       0.84902  |
		    | 10 |       10 |         0 |       0.835143 |
		    | 11 |       11 |         4 |       0.843228 |
		    | 12 |       12 |         2 |       0.826949 |
		    | 13 |       13 |         0 |       0.84196  |
		    | 14 |       14 |         1 |       0.821634 |
		    | 15 |       15 |         3 |       0.840702 |
		    | 16 |       16 |         0 |       0.828994 |
		    | 17 |       17 |         2 |       0.843043 |
		    | 18 |       18 |         4 |       0.809093 |
		    | 19 |       19 |         1 |       0.85426  |
		    +----+----------+-----------+----------------+
		
		df2
		+----+-----------+----------+----------------+
		|    |   group   |student No|       score    |
		|----+-----------+----------+----------------|
		|  0 |         2 |        1 |       0.887435 |
		|  1 |         0 |       19 |       0.81214  |
		|  2 |         3 |       17 |       0.899041 |
		|  3 |         0 |        8 |       0.853333 |
		|  4 |         4 |        9 |       0.88512  |
		+----+-----------+----------+----------------+
		
	"Frame-2":
		   +----+----------+-----------+----------------+
		    |    |student No|   group   |       score    |
		    |----+----------+-----------+----------------|
		    |  0 |        0 |         0 |       0.839626 |
		    |  1 |        1 |         2 |       0.887435 |
		    |  2 |        2 |         3 |       0.830778 |
		    |  3 |        3 |         2 |       0.831565 |
		    |  4 |        4 |         3 |       0.823569 |
		    |  5 |        5 |         0 |       0.808109 |
		    |  6 |        6 |         4 |       0.831645 |
		    |  7 |        7 |         1 |       0.851048 |
		    |  8 |        8 |         0 |       0.853333 |
		    |  9 |        9 |         4 |       0.88512  |
		    | 10 |       10 |         0 |       0.835143 |
		    | 11 |       11 |         4 |       0.843228 |
		    | 12 |       12 |         2 |       0.826949 |
		    | 13 |       13 |         0 |       0.84196  |
		    | 14 |       14 |         1 |       0.821634 |
		    | 15 |       15 |         3 |       0.840702 |
		    | 16 |       16 |         0 |       0.828994 |
		    | 17 |       17 |         3 |       0.899041 |
		    | 18 |       18 |         4 |       0.809093 |
		    | 19 |       19 |         0 |       0.81214  |
		    +----+----------+-----------+----------------+
		
}
"answer": {
	"desc": %s I don't know what's wrong I ran same and got the answer giving a different way to solve it try : will give you the solution you want. to get the max from each group 
	"code-snippets": [
		dfupdated = df1.merge(df2, on='student No', how='left')
		dfupdated['group'] = dfupdated['group_y'].fillna(dfupdated['group_x'])
		dfupdated['score'] = dfupdated['score_y'].fillna(dfupdated['score_x'])
		dfupdated.drop(['group_x', 'group_y','score_x', 'score_y'], axis=1,inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66398540
"link": https://stackoverflow.com/questions/66398540/how-to-set-new-columns-in-a-multi-column-index-from-a-dict-with-partially-specif
"question": {
	"title": How to set new columns in a multi-column index from a dict with partially specified tuple keys?
	"desc": I have a pandas dataframe initialized in the following way: which gives: Now I'd like to add a new column to this dataframe using partial key slicing BUT not in code, I'd like to do this from configuration i.e. a dictionary with partial tuple keys: which gives: notice that setting 'x' doesn't depend on the second component of the key and setting 'y1' and 'y2' do depend on the second component of the key. A possible solution is to fully specify the mapping but this is also not desirable if I have a 100 whose assignment doesn't depend on the second component. I wish to reach the above result but not hard-coding the sliced assignments, instead I'd like to do it from an externalized dictionary: My configuration dictionary would look like this: Is there a pythonic and pandas-tonic way to apply this dictionary with partially specified keys to reach the sliced assignment produced before? 
}
"io": {
	"Frame-1": 
		#            col1  col2
		# key1 key2            
		# a    a1       1     2
		#      a2       3     4
		# b    b1       5     6
		#      b2       7     8
		
	"Frame-2":
		# key1 key2                  
		# a    a1       1     2     x
		#      a2       3     4     x
		# b    b1       5     6    y1
		#      b2       7     8    y2
		
}
"answer": {
	"desc": %s We can leverage the fact that we can pass tuples as a MultiIndex slicer. Also we slightly adjust your . Then we apply a simple for loop: Second option would be to use and filling in the first value in your dict, so we can use : 
	"code-snippets": [
		my_dict = {
		    ('a',): 'x',
		    ('b', 'b1'): 'y1',
		    ('b', 'b2'): 'y2'
		}
		
		for idx, value in my_dict.items():
		    df.loc[idx, 'desc1'] = value
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66383901
"link": https://stackoverflow.com/questions/66383901/changing-value-of-adjacent-column-based-on-value-of-of-another-column
"question": {
	"title": Changing Value of adjacent column based on value of of another column
	"desc": I have following dataframe: I want to change value in column A1 to NaN whenever corresponding value in column A2 is No or NA. Same for B1. Note: NA here is a string objects not NaN. 
}
"io": {
	"Frame-1": 
		    A1  A2  B1  B2
		 0  10  20  20   NA
		 1  20  40  30   No
		 2  50  No  50   10
		 3  40  NA  50   20
		
	"Frame-2":
		     A1  A2  B1   B2
		 0  10   20  NaN  NA
		 1  20   40  NaN  No
		 2  NaN  No  50   10
		 3  NaN  NA  50   20
		
}
"answer": {
	"desc": %s Use if and are strings use in or : Or : If is missing value test it by : Or: 
	"code-snippets": [
		df.loc[df.A2.isna() | df.A2.eq('No'), 'A1'] = np.nan
		
		----------------------------------------------------------------------
		df['A1'] = df['A1'].mask(df.A2.isna() | df.A2.eq('No'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66371637
"link": https://stackoverflow.com/questions/66371637/sum-of-row-in-the-same-columns-in-pandas
"question": {
	"title": sum of row in the same columns in pandas
	"desc": i have a dataframe something like this how do i get the sum of values between the same column in a new column in a dataframe for example: i want a new column with the sum of d1[i] + d1[i+1] .i know .sum() in pandas but i cant do sum between the same column 
}
"io": {
	"Frame-1": 
		   d1  d2    d3    d4
		  780  37.0  21.4  122840.0
		  784  38.1  21.4  122860.0
		  846  38.1  21.4  122880.0
		  843  38.0  21.5  122900.0
		  820  36.3  22.9  133220.0
		  819  36.3  22.9  133240.0
		  819  36.4  22.9  133260.0
		  820  36.3  22.9  133280.0
		  822  36.4  22.9  133300.0
		
	"Frame-2":
		 d1    d2    d3    d4       d5 
		780  37.0  21.4  122840.0  1564
		784  38.1  21.4  122860.0  1630
		846  38.1  21.4  122880.0  1689
		
}
"answer": {
	"desc": %s Your question is not fully clear to me, but I think what you mean to do is: Now you have to decide what you want to happen for the last element of the series. 
	"code-snippets": [
		df['d5'] = df['d1'] + df['d1'].shift(-1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66357650
"link": https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns
"question": {
	"title": transform a pandas dataframe in a pandas with multicolumns
	"desc": I have the following pandas dataframe, where the columna is the dataframe index And i want to convert this datframe in to a multi column data frame, that looks like this I've tried transforming my old pandas dataframe in to a dict this way: But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome. 
}
"io": {
	"Frame-1": 
		+----+-----------+------------+-----------+------------+
		|    |   price_A |   amount_A |   price_B |   amount_b |
		|----+-----------+------------+-----------+------------|
		|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |
		|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |
		|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |
		|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |
		|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |
		|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |
		|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |
		|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |
		|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |
		|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |
		+----+-----------+------------+-----------+------------+
		
		
	"Frame-2":
		
		+----+-----------+------------+-----------+------------+
		|    |           A            |           B            |
		+----+-----------+------------+-----------+------------+
		| id |   price   |   amount   |   price   |   amount   |
		|----+-----------+------------+-----------+------------|
		|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |
		|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |
		|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |
		|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |
		|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |
		|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |
		|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |
		|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |
		|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |
		|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |
		+----+-----------+------------+-----------+------------+
		
		
}
"answer": {
	"desc": %s Try renaming columns manually: Output: 
	"code-snippets": [
		df.columns=pd.MultiIndex.from_tuples([x.split('_')[::-1] for x in df.columns])
		df.index.name='id'
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 26716616
"link": https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary
"question": {
	"title": Convert a Pandas DataFrame to a dictionary
	"desc": I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be and the elements of other columns in same row be . DataFrame: Output should be like this: Dictionary: 
}
"io": {
	"Frame-1": 
		    ID   A   B   C
		0   p    1   3   2
		1   q    4   3   2
		2   r    4   0   9  
		
	"Frame-2":
		{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}
		
}
"answer": {
	"desc": %s The method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this. also accepts an 'orient' argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form will be returned for each column. These steps can be done with the following line: In case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame: Then the options are as follows. dict - the default: column names are keys, values are dictionaries of index:data pairs list - keys are column names, values are lists of column data series - like 'list', but values are Series split - splits columns/data/index as keys with values being column names, data values by row and index labels respectively records - each row becomes a dictionary where key is column name and value is the data in the cell index - like 'records', but a dictionary of dictionaries with keys as index labels (rather than a list) 
	"code-snippets": [
		>>> df.to_dict('split')
		{'columns': ['a', 'b'],
		 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],
		 'index': [0, 1, 2]}
		
		----------------------------------------------------------------------
		>>> df.to_dict('records')
		[{'a': 'red', 'b': 0.5}, 
		 {'a': 'yellow', 'b': 0.25}, 
		 {'a': 'blue', 'b': 0.125}]
		
		----------------------------------------------------------------------
		>>> df.to_dict('index')
		{0: {'a': 'red', 'b': 0.5},
		 1: {'a': 'yellow', 'b': 0.25},
		 2: {'a': 'blue', 'b': 0.125}}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66277212
"link": https://stackoverflow.com/questions/66277212/merge-padas-rows-if-the-difference-between-consecutive-rows-are-less-than-two
"question": {
	"title": Merge padas rows if the difference between consecutive rows are less than two
	"desc": I have a data frame like this, the df is sorted by col1. Now for each col1 values of the previous or/and next row if difference between consecutive col3 values are less than 2 then merge col2 values in a single row. So the data frame would look like, This could be done using for loop by filtering col1 values each time but it will take more time to execute, looking for some pandas shortcuts to do it most efficiently. 
}
"io": {
	"Frame-1": 
		df
		col1     col2         col3
		 A       [p,s]         2
		 A       [q]           3
		 A       [r,t]         4
		 A       [p,x]         7
		 B       [x,y]         8
		 C       [s]           4
		 C       [t,v]         6
		 C       [u,x]         7 
		
	"Frame-2":
		df
		col1    col2
		 A      [p,s,q,r,t]
		 A      [p,x]
		 B      [x,y]
		 C      [s]
		 C      [t,v,u,x]
		
}
"answer": {
	"desc": %s You can create groups by compared if differency is greater of equal 2 with cumulative sums first: And then use column for aggregate with flatten list of lists in lambda function: 
	"code-snippets": [
		df['g'] = df.groupby('col1')['col3'].apply(lambda x: x.diff().ge(2).cumsum())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66266438
"link": https://stackoverflow.com/questions/66266438/python-how-to-combine-the-rows-into-a-single-row-in-pandas-not-group-by
"question": {
	"title": Python - How to combine the rows into a single row in Pandas? (not group by)
	"desc": I am working with a weird dataframe using Pandas: I want to combine the three rows into 1 row and the expected output is: I really don't know how to do this and appreciate your help! Thank you. 
}
"io": {
	"Frame-1": 
		print(df)
		
		Active        Dead       Hold
		Product1      n/a        n/a
		n/a           Product2   n/a
		n/a           n/a        Product3
		
	"Frame-2":
		Active        Dead       Hold
		Product1      Product2   Product3
		
}
"answer": {
	"desc": %s Here is one slightly faster alternative: 
	"code-snippets": [
		new = df.apply(lambda x: x.dropna().values)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66266188
"link": https://stackoverflow.com/questions/66266188/how-to-stack-append-all-columns-into-one-column-in-pandas
"question": {
	"title": How to stack/append all columns into one column in Pandas?
	"desc": I am working with a DF similar to this one: I want the values of all columns to be stacked into the first column Desired Output: 
}
"io": {
	"Frame-1": 
		    A   B   C
		0   1   4   7
		1   2   5   8
		2   3   6   9
		
	"Frame-2":
		    A
		0   1
		1   2
		2   3
		3   4
		4   5
		5   6
		6   7
		7   8
		8   9
		
}
"answer": {
	"desc": %s Very simply with : 
	"code-snippets": [
		import pandas as pd
		df.melt().drop('variable',axis=1).rename({'value':'A'},axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66255574
"link": https://stackoverflow.com/questions/66255574/reverse-columns-of-dataframe-based-on-the-column-name
"question": {
	"title": Reverse columns of dataframe based on the column name
	"desc": I have a dataframe: I would like to reverse the columns that their column names have their 1st and 2nd letters reversed and their 3rd and 4th as-is. i.e. 1st col: 1000 → 2nd col: 0100 3rd col: 0010 → 5th col: 1110 4th col: 0001 → 6th col: 1101 7th col: 1011 → 8th col: 0111 I would like to have a dataframe like this: This is what I have for the reversion: 
}
"io": {
	"Frame-1": 
		     '1000'    '0100'    '0010'    '0001'    '1110'    '1101'    '1011'    '0111'
		0        0         1         2         3         4         5         6         7
		1       00        11        22        33        44        55        66        77
		
	"Frame-2":
		     '0100'    '1000'    '1110'    '1101'    '0010'    '0001'    '1011'    '0111'
		0        1         0         4         5         2         3         7         6
		1       11        00        44        55        22        33        77        66
		
}
"answer": {
	"desc": %s Use your function in list comprehenion with add next 2 values and then change order of columns by original ordering in subset: EDIT: More general solution is convert values to 2d numpy array filled by boolean: And then for inverting use indexing, here means all rows and columns positions form list and invert mask: Last join back to list of strings: 
	"code-snippets": [
		df.columns = df.columns.str.strip("'")
		cols = df.columns
		
		arr = np.array([[bool(int(y)) for y in x] for x in df.columns])
		print (arr)
		[[ True False False False]
		 [False  True False False]
		 [False False  True False]
		 [False False False  True]
		 [ True  True  True False]
		 [ True  True False  True]
		 [ True False  True  True]
		 [False  True  True  True]]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66255188
"link": https://stackoverflow.com/questions/66255188/create-pandas-duplicate-rows-based-on-the-number-of-items-in-a-list-type-column
"question": {
	"title": Create pandas duplicate rows based on the number of items in a list type column
	"desc": I have a data frame like this, Now I want to create new rows based on the number of values in the col2 list where the col1 values will be same so the final data frame would look like, I am looking for some pandas short cuts to do it more efficiently 
}
"io": {
	"Frame-1": 
		  df
		  col1     col2
		   A        [1]
		   B        [1,2]
		   A        [2,3,4]
		   C        [1,2]
		   B        [4]
		
	"Frame-2":
		  df
		  col1    col2
		   A       [1]
		   B       [1]
		   B       [2]
		   A       [2]
		   A       [3]
		   A       [4]
		   C       [1]
		   C       [2]
		   B       [4]
		
}
"answer": {
	"desc": %s Use and then create one element lists: Another idea, I hope faster in large data is use numpy with for flatten values: 
	"code-snippets": [
		df2 = df.explode('col2')
		
		df2['col2'] = df2['col2'].apply(lambda x: [x])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66229269
"link": https://stackoverflow.com/questions/66229269/in-panda-python-how-do-i-blacklist-or-whitelist-numbers-in-a-dataframe
"question": {
	"title": In panda python how do I &quot;blacklist&quot; or &quot;whitelist&quot; numbers in a DataFrame
	"desc": I have two DataFrames, and I want to add a new column to the df with the first allowed numbers in df1, but only as many as are in each group, when it starts again at 1 it needs to look at the first number in Allowed_numbers. and want this: I have tried a few things and get this I was also considering some kind of mapping numbers against eachother, since 1 in order_out will always be 3 in y_goals. the order_out might also have different lengths of number row, not always up to 9. 
}
"io": {
	"Frame-1": 
		      order_y  y_goal
		0         1       3
		1         2       4
		2         3       5
		3         4       6
		4         5       8
		5         6       9
		6         7      12
		7         8      15
		8         9      17
		9         1       3
		10        2       4
		11        3       5
		12        4       6
		13        5       8
		14        6       9
		15        7      12
		16        8      15
		17        9      17
		...
		
	"Frame-2":
		    order_out y_goal
		0         1     3.0
		1         2     4.0
		2         3     5.0
		3         4     6.0
		4         5     8.0
		5         6     9.0
		6         7    12.0
		7         8    15.0
		8         9    17.0
		9         1    24.0
		10        2    28.0
		11        3    29.0
		12        4    30.0
		13        5     NaN
		14        6     NaN
		15        7     NaN
		16        8     NaN
		17        9     NaN
		...
		
		
}
"answer": {
	"desc": %s You can use as an index to df1 (we subtract 1 to convert from 1-indexing to 0-indexing): Now we can reset the index on y_goal and neatly merge the two columns: 
	"code-snippets": [
		df['y_goal'] = y_goal.reset_index(drop = True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66194784
"link": https://stackoverflow.com/questions/66194784/retrieve-rows-with-highest-value-with-condition
"question": {
	"title": Retrieve rows with highest value with condition
	"desc": I have a dataframe that looks like this: I want to write a function that takes the rows with same id and label A and filter it based on the highest width so the after applying the function the dataframe would be: 
}
"io": {
	"Frame-1": 
		| Id | Label | Width |
		|----|-------| ------|
		| 0  |   A   |   5   |
		| 0  |   A   |   3   |
		| 0  |   B   |   4   |
		| 1  |   A   |   7   |
		| 1  |   A   |   9   |
		
	"Frame-2":
		| Id | Label | Width |
		|----|-------| ------|
		| 0  |   A   |   5   |
		| 0  |   B   |   4   |
		| 1  |   A   |   9   |
		
}
"answer": {
	"desc": %s Let us try: Details: Create a boolean mask with specifying the condition where equals : filter the rows using the above mask and group this dataframe on and and aggregate using to get the indices on max values: finally the above dataframe with the dataframe containing labels other that and the index to maintain the order: 
	"code-snippets": [
		m = df['Label'].eq('A')
		df_a = df.loc[df[m].groupby(['Id', 'Label'])['Width'].idxmax()]
		
		df_out = pd.concat([df[~m], df_a]).sort_index()
		
		----------------------------------------------------------------------
		>>> m
		
		0     True
		1     True
		2    False
		3     True
		4     True
		Name: Label, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66194236
"link": https://stackoverflow.com/questions/66194236/dataframe-pandas-how-ploting-same-columns-of-two-dataframe-for-visualising-t
"question": {
	"title": Dataframe - Pandas - How ploting same columns of two dataframe for visualising the differences
	"desc": There are two dataframes and I would like to have a plot that shows the both price columns on X axis and sum on the Y axis to see how are the difference between these two dataframes. I tried the below but does nothing. What is the best way to show the differences between the two patterns of the price in these two dataframes? I thought of something like this, but if there is a better way to show the differences please mention it. 
}
"io": {
	"Frame-1": 
		df1
		+-----+-----+-------+
		|     | id  | price |
		+-----+-----+-------+
		| 1   | 1   | 5    |
		+-----+-----+-------+
		| 2   | 2   | 12    |
		+-----+-----+-------+
		| 3   | 3   | 34    |
		+-----+-----+-------+
		| 4   | 4   | 62    |
		+-----+-----+-------+
		| ... | ... | ...   |
		+-----+-----+-------+
		| 125 | 125 | 90    |
		+-----+-----+-------+
		
	"Frame-2":
		df2
		+-----+-----+-------+
		|     | id  | price |
		+-----+-----+-------+
		| 1   | 1   | 14    |
		+-----+-----+-------+
		| 2   | 2   | 15    |
		+-----+-----+-------+
		| 3   | 3   | 45    |
		+-----+-----+-------+
		| 4   | 4   | 62    |
		+-----+-----+-------+
		| ... | ... | ...   |
		+-----+-----+-------+
		| 125 | 125 | 31    |
		+-----+-----+-------+
		
}
"answer": {
	"desc": %s If you take first column from df1, and second column from df2, there will be no couple of lines, only one. For qualitatively compartion you can use matplotlib in simple way, because it automaticly creates a figure. Here is the result: For every arg on x it shows point on df1[col1] and df2[col2]. But with this plot you can not compare it quantitatively. PS: Here is logic you tried to realize but with seaborn. Result: Optional Quantitative comparison. Result: 
	"code-snippets": [
		import pandas as pd 
		import matplotlib.pyplot as plt
		import random
		import seaborn as sns
		
		df1 = pd.DataFrame({
		    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])
		                  })
		df2 = pd.DataFrame({
		    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])
		                  })
		
		plt.plot(df1['col2'], label='first')
		plt.plot(df2['col2'], label='second')
		plt.legend()
		
		----------------------------------------------------------------------
		df3 = pd.merge(df1,df2, on='col1')
		sns.lineplot(x='col2_x', y='col2_y', data=df3)
		
		----------------------------------------------------------------------
		df3['dif'] = abs(df3['col2_x'] - df3['col2_y'])
		
		sns.lineplot(x='col1', y='dif', data=df3)
		plt.xticks(df3['col1'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66146761
"link": https://stackoverflow.com/questions/66146761/is-there-a-way-to-apply-a-condition-while-using-apply-and-lambda-in-a-dataframe
"question": {
	"title": Is there a way to apply a condition while using apply and lambda in a DataFrame?
	"desc": I have a Pandas dataframe that looks like this: And I'm looking for a way to iter trough the Dyn column, generating another one that sums only the numbers that are bigger than a cutoff, i.e.: 0.150, assigning all the values that pass it a value of one. This is what the expected result should look like: I thought I could use apply, while ittering trough all of the rows: But I'm lost on how to apply the condition (only sum it if it's greater than 0.150) to all the values inside 'Dyn' and how to assign the value of 1 to them. All advice is accepted. Thanks! 
}
"io": {
	"Frame-1": 
		    ID                                               Dyn
		0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1
		1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200
		
	"Frame-2":
		    ID                                               Dyn Sum
		0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1   2
		1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200   7
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		#Create temp column to hold Dyn convereted into list
		df=df.assign(sum=df['Dyn'].str.split(','))
		
		#Explode DataFrame
		df=df.explode('sum')
		#Convert to float
		df['sum']=df['sum'].astype(float)
		#Filter out values greater that 0.015, groupby and sum
		df[df['sum'].gt(0.150)].groupby(['ID','Dyn'])['sum'].sum().reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66138245
"link": https://stackoverflow.com/questions/66138245/pandas-regex-comprehension-isolate-single-result
"question": {
	"title": Pandas regex comprehension - isolate single result
	"desc": I have a dataframe that's been extracted from an SQL server, and I used regex to extract a string of three dimensions. I need all three dimensions in three new columns so I have used a regular expression for a number optionally separated by a period, and created a column from this findall result. But the result shows as a list and I am unable to index the second dimension. Due to the urgency I have been able to temporarily solve this with a lookaround. But how can I directly extract dimension column extract - not all are in this format code extract for finding dims sample output using the findall result I would need a column for 1493.4 and a second column for 204.2 - I can do the first one but how would I create a column for specific indexes in the regex results. I have tried lambda, list comprehension, and everything else I can think of. So far I cannot find a similar question online and I know it should be simple - but its taken me 2 days! Many thanks for all your help EDIT: To confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern. Additionally, my focus is on obtaining only list item n to a new column and not every item in the list 
}
"io": {
	"Frame-1": 
		[1103.5 x 308.8 x 25.4 mm]
		[33.3 x 13 x 9.5mm]
		[136.5 x 15 x 12.7 mm]
		
	"Frame-2":
		[1493.4, 204.2, 25.4, 0013, 900, 4]
		[136.5, 15, 12.7, 001, 900, 2]
		
}
"answer": {
	"desc": %s If I understand correctly you are looking for way of expanding column holding lists, assuming that number of matches is equal in all record, you might do: output: If only n-th element is required you might use following way: 
	"code-snippets": [
		import pandas as pd
		df = pd.DataFrame({'dim':['10x10','12x10','10x12']})
		df['dim_list'] = df.dim.str.findall(r'\d+')
		df[['width','height']] = df.dim_list.apply(pd.Series)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66128637
"link": https://stackoverflow.com/questions/66128637/python-find-closest-neighbors-to-a-value-in-a-dataframe
"question": {
	"title": Python find closest neighbors to a value in a dataframe
	"desc": I have a dataframe or list. I want to find the closest values and their index to a given value. My code: Present output (val_idx): Expected output (val_idx): 
}
"io": {
	"Frame-1": 
		    num
		1   24
		0   20
		
	"Frame-2":
		    num
		2   35
		1   24
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		import numpy as np
		import pandas as pd
		
		# setup
		df = pd.DataFrame({'num':[20,24,35,38]})
		val = 26 
		
		# subtract to get differences
		test = np.absolute(np.subtract(val, df["num"]))
		
		# get index
		idx = np.argmin(test)
		# Condition
		idx = np.where(df["num"][idx] > val, [idx-1, idx], [idx, idx+1])
		
		print(idx)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66126525
"link": https://stackoverflow.com/questions/66126525/pandas-clean-up-string-column-containing-single-quotes-and-brackets-using-regex
"question": {
	"title": Pandas: Clean up String column containing Single Quotes and Brackets using Regex?
	"desc": I want to clean the following Pandas dataframe column, but in a single and efficient statement than the way I am trying to achieve it in the code below. Input: Output: Code: 
}
"io": {
	"Frame-1": 
		                  string
		0  ['string', '#string']
		1            ['#string']
		2                     []
		
	"Frame-2":
		            string
		0  string, #string
		1          #string
		2              NaN
		
}
"answer": {
	"desc": %s You can use Details: - matches a string that only consists of zero or more , or whitespace chars - or - captures into Group 1 one or more chars at the start of string, or one or more chars at the end of string or any char. If Group 1 matches, the replacement is an empty string (the match is removed), else, the replacement is . 
	"code-snippets": [
		df['string'] = df['string'].astype(str).str.replace(r"^[][\s]*$|(^\[+|\]+$|')", lambda m: '' if m.group(1) else np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 44615807
"link": https://stackoverflow.com/questions/44615807/remove-double-quotes-in-pandas
"question": {
	"title": Remove double quotes in Pandas
	"desc": I have the following file: Which I read by: Then I print and see: Instead, I would like to see: How to remove the double quotes? 
}
"io": {
	"Frame-1": 
		   "j"  "x"  y
		0  "0"  "1"  5
		1  "1"  "2"  6
		2  "2"  "3"  7
		3  "3"  "4"  8
		4  "4"  "5"  3
		5  "5"  "5"  4
		
	"Frame-2":
		   j  x  y
		0  0  1  5
		1  1  2  6
		2  2  3  7
		3  3  4  8
		4  4  5  3
		5  5  5  4
		
}
"answer": {
	"desc": %s I did it with: 
	"code-snippets": [
		rm_quote = lambda x: x.replace('"', '')
		
		df = pd.read_csv('test.csv', delimiter='; ', engine='python', 
		     converters={'\"j\"': rm_quote, 
		                 '\"x\"': rm_quote})
		
		df = df.rename(columns=rm_quote)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66088290
"link": https://stackoverflow.com/questions/66088290/use-a-categorical-column-to-order-the-dataframe-according-to-an-array
"question": {
	"title": Use a categorical column to order the dataframe according to an array
	"desc": I have an array like this: I also have a dataframe like this: I want to use to order the column dataframe according to the array. The expected output is: 
}
"io": {
	"Frame-1": 
		BIN      CA      SUM
		100       B      B 100
		300       A      A 300
		300       B      B 300
		400       B      B 400
		400       A      A 400
		200       B      B 200
		100       A      A 100
		200       A      A 200
		
	"Frame-2":
		BIN      CA      SUM
		100       A      A 100
		200       A      A 200
		300       A      A 300
		400       A      A 400
		100       B      B 100
		200       B      B 200
		300       B      B 300
		400       B      B 400
		
}
"answer": {
	"desc": %s You can use to convert the column to categorical column having order, then the values: Alternatively you can create a dictionary that maps the items in to their sorting order then this dictionary on column and use to get the indices that would sort the dataframe: 
	"code-snippets": [
		df['SUM'] = pd.Categorical(df['SUM'], categories=arr, ordered=True)
		df.sort_values('SUM')
		
		----------------------------------------------------------------------
		dct = {v: i for i, v in enumerate(arr)}
		df.iloc[np.argsort(df['SUM'].map(dct))]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66075712
"link": https://stackoverflow.com/questions/66075712/create-a-pandas-column-of-numbers-from-1-to-3-and-repeat-again
"question": {
	"title": Create a pandas column of numbers from 1 to 3 and repeat again
	"desc": I have a dataframe: I want to add a column so that my final dataframe looks like: I don't want the new row to depend upon StoreNumber which looks ovious in example. I want to start numbering with 1 and when I reach 3, then again start with 1. How do I do it? 
}
"io": {
	"Frame-1": 
		   StoreNumber    Year  
		    1000          2000  
		    1000          2001  
		    1000          2002  
		    1001          2000  
		    1001          2001  
		    1001          2002  
		
	"Frame-2":
		StoreNumber       Year   New
		    1000          2000    1
		    1000          2001    2
		    1000          2002    3
		    1001          2000    1
		    1001          2001    2
		    1001          2002    3 
		
}
"answer": {
	"desc": %s You can use to create an iterator and use it to generate the target sequence: And the output will be 
	"code-snippets": [
		from itertools import cycle
		
		num_cycle = cycle([1, 2, 3])
		df['New'] = [next(num_cycle) for num in range(len(df))]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66075388
"link": https://stackoverflow.com/questions/66075388/create-df-columns-based-on-second-ddf
"question": {
	"title": Create DF Columns Based on Second DDF
	"desc": I have 2 dataframes with different columns: I would like to add the missing columns for the 2 dataframes - so each one will have each own columns + the other DFs columns (without column "number"). And the new columns will have initial number for our choice (let's say 0). So the final output: What's the best way to achieve this result? I've got messed up with getting the columns and trying to create new ones. Thank! 
}
"io": {
	"Frame-1": 
		DF A -                   DF B - 
		number | a  | b  | c  |||| a  | c  | d  | e  | f
		1      | 12 | 13 | 15 |||| 22 | 33 | 44 | 55 | 77
		
	"Frame-2":
		    DF A -                    
		    number | a  | b  | c  | d  | e  | f 
		    1      | 12 | 13 | 15 | 0  | 0  | 0 
		    DF B -
		    a  | b  | c  | d  | e  | f
		    22 | 0  | 33 | 44 | 55 | 77
		
}
"answer": {
	"desc": %s First, you need to create a superset of all the columns which are present in both the dataframes. This you can do using the below code. Then for each dataframes, you need to check which columns are missing that you can do using the below code. Then add the missing columns in both the dataframes Hope this solves your query! 
	"code-snippets": [
		all_columns = list(set(A.columns.to_list() + B.columns.to_list()))
		
		----------------------------------------------------------------------
		col_missing_from_A = [col for col in all_columns if col not in A.columns]
		col_missing_from_B = [col for col in all_columns if col not in B.columns]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66070517
"link": https://stackoverflow.com/questions/66070517/transpose-dataframe-based-on-column-list
"question": {
	"title": Transpose dataframe based on column list
	"desc": I have a dataframe in the following structure: I would like to transpose - create columns from the names in cNames. But I can't manage to achieve this with transpose because I want a column for each value in the list. The needed output: How can I achieve this result? Thanks! The code to create the DF: 
}
"io": {
	"Frame-1": 
		cNames  | cValues   |  number  
		[a,b,c] | [1,2,3]   |  10      
		[a,b,d] | [55,66,77]|  20
		
	"Frame-2":
		a   | b   | c   | d   |  number
		1   | 2   | 3   | NaN | 10
		55  | 66  | NaN | 77  | 20
		
}
"answer": {
	"desc": %s One option is : Or a DataFrame construction: Output: Update Performances sort by run time on sample data DataFrame concat: KJDII's new series Scott's apply(pd.Series.explode) wwnde's set_index.apply(explode) Celius' double explode 
	"code-snippets": [
		pd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) 
		               for idx, x in df.iterrows()], 
		          axis=1
		         ).T.join(df.iloc[:,2:])
		
		----------------------------------------------------------------------
		pd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )
		              for idx, x in df.iterrows()
		            }).T.join(df.iloc[:,2:])
		
		----------------------------------------------------------------------
		%%timeit
		pd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )
		              for idx, x in df.iterrows()
		            }).T.join(df.iloc[:,2:])
		1.29 ms ± 36.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
		%%timeit
		pd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) 
		               for idx, x in df.iterrows()], 
		          axis=1
		         ).T.join(df.iloc[:,2:])
		2.03 ms ± 86.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) 
		
		----------------------------------------------------------------------
		%%timeit
		df['series'] = df.apply(lambda x: dict(zip(x['cNames'], x['cValues'])), axis=1)
		pd.concat([df['number'], df['series'].apply(pd.Series)], axis=1)
		
		2.09 ms ± 65.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		----------------------------------------------------------------------
		%%timeit
		df.apply(pd.Series.explode)\
		  .set_index(['number', 'cNames'], append=True)['cValues']\
		  .unstack()\
		  .reset_index()\
		  .drop('level_0', axis=1)
		
		4.9 ms ± 135 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		----------------------------------------------------------------------
		%%timeit
		g=df.set_index('number').apply(lambda x: x.explode()).reset_index()
		g['cValues']=g['cValues'].astype(int)
		pd.pivot_table(g, index=["number"],values=["cValues"],columns=["cNames"]).droplevel(0, axis=1).reset_index()
		
		7.27 ms ± 162 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		----------------------------------------------------------------------
		%%timeit
		df1 = df.explode('cNames').explode('cValues')
		df1['cValues'] = pd.to_numeric(df1['cValues'])
		df1.pivot_table(columns='cNames',index='number',values='cValues')
		
		9.42 ms ± 189 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66060591
"link": https://stackoverflow.com/questions/66060591/join-two-pandas-dataframes-based-on-lists-columns
"question": {
	"title": Join two pandas dataframes based on lists columns
	"desc": I have 2 dataframes containing columns of lists. I would like to join them based on 2+ shared values on the lists. Example: In this case we can see that id1 matches id3 because there are 2+ shared values on the lists. So the output will be (columns name are not important and just for example): How can I achieve this result? I've tried to iterate each row in dataframe #1 but it doesn't seem a good idea. Thank you! 
}
"io": {
	"Frame-1": 
		ColumnA ColumnB        | ColumnA ColumnB        
		id1     ['a','b','c']  | id3     ['a','b','c','x','y', 'z']
		id2     ['a','d,'e']   | 
		
	"Frame-2":
		    ColumnA1 ColumnB1     ColumnA2   ColumnB2        
		    id1      ['a','b','c']  id3     ['a','b','c','x','y', 'z']
		    
		
}
"answer": {
	"desc": %s If you are using pandas 1.2.0 or newer (released on December 26, 2020), cartesian product (cross joint) can be simplified as follows: Also, if system performance (execution time) is a concern to you, it is advisable to use instead of the slower Using : while using : Notice that using is 3x times faster! Whole set of codes for your reference: 
	"code-snippets": [
		    df = df1.merge(df2, how='cross')         # simplified cross joint for pandas >= 1.2.0
		
		----------------------------------------------------------------------
		%%timeit
		df['overlap'] = df.apply(lambda x: 
		                         len(set(x['ColumnB1']).intersection(
		                             set(x['ColumnB2']))), axis=1)
		
		
		800 µs ± 59.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
		%%timeit
		df['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))
		
		217 µs ± 25.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
		    data = {'ColumnA1': ['id1', 'id2'], 'ColumnB1': [['a', 'b', 'c'], ['a', 'd', 'e']]}
		    df1 = pd.DataFrame(data)
		
		    data = {'ColumnA2': ['id3', 'id4'], 'ColumnB2': [['a','b','c','x','y', 'z'], ['d','e','f','p','q', 'r']]}
		    df2 = pd.DataFrame(data)
		
		    df = df1.merge(df2, how='cross')             # for pandas version >= 1.2.0
		
		    df['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))
		
		    df = df[df['overlap'] >= 2]
		    print (df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 66034318
"link": https://stackoverflow.com/questions/66034318/assigning-column-to-larger-dataframe-in-specific-positions
"question": {
	"title": Assigning column to larger DataFrame in specific positions
	"desc": I have two lists. I want to create C: Basically, where in there is a I want to have a value of , where there is a , a . In reality, contains around 10k elements, around 40k and I have many of them. I am working with a pandas.DataFrame (each "array" is a column of two different dataframes, I have to "fit" in by transforming it into ). I have done it with a for loop, but I am wondering how to do it in a better way. Thank you. 
}
"io": {
	"Frame-1": 
		A = [0, 1, 0, 0, 1, 1, 0, 1]
		B = [2, 3, 4, 5]
		
	"Frame-2":
		C = [NaN, 2, NaN, NaN, 3, 4, NaN, 5] 
		
}
"answer": {
	"desc": %s You can use and check with condition if it is or not. If it is then provide , else pop B from left side. output 
	"code-snippets": [
		[None if i==0 else B.pop(0) for i in A]
		
		----------------------------------------------------------------------
		[None, 2, None, None, 3, 4, None, 5]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65995268
"link": https://stackoverflow.com/questions/65995268/pandas-dataframe-average-non-0-value
"question": {
	"title": Pandas Dataframe, average non 0 value
	"desc": I have the following Pandas Dataframe 'df': How do I get the average value of "a" for from a1, a2, a3, ignoring the 0 value? I'm stuck with manual approach where I convert the value > 0 to 1 
}
"io": {
	"Frame-1": 
		a1  a2  a3  b1
		0   0   0   1
		1   2   0   2
		3   0   0   3
		2   4   0   4
		
	"Frame-2":
		a1  a2  a3  b1  avg(a)
		0   0   0   1   0
		1   2   0   2   1.5
		3   0   0   3   3.0
		2   4   0   4   3.0
		
}
"answer": {
	"desc": %s You can the like columns, then the zeros in these columns and take along : 
	"code-snippets": [
		a = df.filter(like='a')
		df['avg'] = a.mask(a.eq(0)).mean(1).fillna(0)
		
		# OR df['avg'] = a[a > 0].mean(1).fillna(0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65834585
"link": https://stackoverflow.com/questions/65834585/conditional-row-shift-in-pandas
"question": {
	"title": Conditional Row shift in Pandas
	"desc": I'm attempting to shift a row based on whether or not another column is not null. There's inconsistent spacing in the Description column so I can't do a .shift() Here's the original data And this is what I want my result to be Here's the code that I used from Align data in one column with another row, based on the last time some condition was true However when I run it, no errors and no changes in the dataframe. 
}
"io": {
	"Frame-1": 
		Permit Number    A      Description
		1234            NaN    NaN
		NaN             NaN    NaN
		NaN             NaN    foo
		3456            NaN    NaN
		NaN             NaN    bar
		
	"Frame-2":
		Permit Number    A      Description
		1234            NaN    foo
		NaN             NaN    NaN
		NaN             NaN    NaN
		3456            NaN    bar
		NaN             NaN    NaN
		
}
"answer": {
	"desc": %s FYI for anyone who sees this that might have a string in their column, this solutions works too I had a simple error of not "writing" the newly grouped dataframe back to df 
	"code-snippets": [
		mask = df['Description'].notnull()
		fmask = (df['Permit Number'].notnull() & df['Description'].isnull())
		df = df.assign(Description=df.groupby(mask[::-1].cumsum())['Description'].transform(lambda x: x.iloc[-1]).where(fmask))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65917323
"link": https://stackoverflow.com/questions/65917323/creating-new-columns-within-a-dataframe-based-on-the-latest-value-from-previous
"question": {
	"title": Creating new columns within a dataframe, based on the latest value from previous columns
	"desc": I've just completed a beginner's course in python, so please bear with me if the code below doesn't make sense or my issue is because of some rookie mistake. I've been trying to put the learning to use by working with college production of NFL players, with a view to understanding which statistics can be predictive or at least correlate to NFL production. It turns out that there's a lot of data out there so I have about 200 columns of data for 600 odd prospects from the last 20 years (just for running backs so far). However, one of the problems with this data is that each stat is only provided by the age the prospect was in that season giving me something like this: What I want to do at the moment is to be able to take the last year of college production and put it into a new column (for 17 different statistics). I've therefore defined the following function: Which I think should go backwards through the columns until I find a value which isn't NaN, and then take that value as the output. I've then defined the columns via a list: and have then run the function through a for loop based on this list: The result I'm getting back is a slightly bizarre one - the for loop appears to work, as all the new columns I'm expecting are created, however they are only populated with data where the player had an age 23 season. The remainder of indexes are filled with 'NaN': This suggests to me that the first 'if' statement in my function is working fine, but that all of the 'elif' statements aren't triggering and I can't work out why. I'm wondering whether it's because I need to be more explicit about why they would trigger, rather than just relying on a logical test of 'if the column is not, not equal to NaN, go to the next one', or if I'm misunderstanding the elif aspect all together. I've put the whole segment of code in also, just because when I've run into issues so far the problem has often not been where I originally thought. By all means tell me if you think I've gone about this in a weird way - this just seemed like a logical approach to the problem but open to other ways of getting the desired result. Thanks in advance! 
}
"io": {
	"Frame-1": 
		    GP 18  GP 19  GP 20  GP 21  GP 22  GP 23
		50   14.0   13.0   14.0    NaN    NaN    NaN
		51   14.0   14.0   14.0    NaN    NaN    NaN
		53   13.0   12.0   11.0    NaN    NaN    NaN
		56   10.0   13.0    9.0   13.0    NaN    NaN
		59   10.0   13.0   15.0    NaN    NaN    NaN
		61    NaN    NaN   11.0   11.0    NaN    NaN
		66    NaN   12.0   13.0   12.0    2.0   13.0
		
	"Frame-2":
		    GP Last
		50      NaN
		51      NaN
		53      NaN
		56      NaN
		59      NaN
		61      NaN
		66     13.0
		
}
"answer": {
	"desc": %s Use with regex parameter with for start of strings, then forward filling missing values and last select last value by position: Another alternative solution with for select columns: 
	"code-snippets": [
		stats_list = ['GP']
		for column in stats_list:
		    mask = df.columns.str.startswith(column)
		    df[f'{column} Last'] = df.loc[:, mask].ffill(axis=1).iloc[:, -1]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65905694
"link": https://stackoverflow.com/questions/65905694/pandas-multiindex-assign-all-elements-in-first-level
"question": {
	"title": pandas.MultiIndex: assign all elements in first level
	"desc": I have a dataframe with a multiindex, as per the following example: This gives me a dataframe like this: I have another 2-dimensional dataframe, as follows: Resulting in the following: I would like to assign to the cell, across all dates, and the cell, across all dates etc. Something akin to the following: Of course this doesn't work, because I am attempting to set a value on a copy of a slice : A value is trying to be set on a copy of a slice from a I tried several variations: How can I select index level 1 (eg: row 'a'), column 'a', across all index level 0 - and be able to set the values? 
}
"io": {
	"Frame-1": 
		                a   b   c
		2020-01-01  a   NaN NaN NaN
		            b   NaN NaN NaN
		            c   NaN NaN NaN
		2020-01-02  a   NaN NaN NaN
		            b   NaN NaN NaN
		            c   NaN NaN NaN
		2020-01-03  a   NaN NaN NaN
		            b   NaN NaN NaN
		            c   NaN NaN NaN
		2020-01-04  a   NaN NaN NaN
		            b   NaN NaN NaN
		            c   NaN NaN NaN
		
	"Frame-2":
		            a           b           c
		2020-01-01  0.540867    0.426181    0.220182
		2020-01-02  0.864340    0.432873    0.487878
		2020-01-03  0.017099    0.181050    0.373139
		2020-01-04  0.764557    0.097839    0.499788
		
}
"answer": {
	"desc": %s Try broadcasing on the values: Output: 
	"code-snippets": [
		a = df.to_numpy()
		
		panel = pd.DataFrame((a[...,None] * a[:,None,:]).reshape(-1, df.shape[1]), 
		                     index=panel.index, columns=panel.columns)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65893903
"link": https://stackoverflow.com/questions/65893903/how-can-i-use-split-in-a-string-when-broadcasting-a-dataframes-column
"question": {
	"title": How can I use split() in a string when broadcasting a dataframe&#39;s column?
	"desc": Take the following dataframe: Result: I need to create a 3rd column (broadcasting), using a condition on , and splitting the string on . This is ok to do: Result: But I need to specify dynamic indexes to split the string on , instead of (5, 8). When I try to run the following code it does not work, because is treated as a : I'm spending a huge time trying to solve this without needing to iterate the dataframe. 
}
"io": {
	"Frame-1": 
		   col_1     col_2
		0      0  here 123
		1      1  here 456
		
	"Frame-2":
		   col_1     col_2 col_3
		0      0  here 123   NaN
		1      1  here 456   456
		
}
"answer": {
	"desc": %s This one liner does the trick. 
	"code-snippets": [
		df['col_3']=[y.split(' ')[1] if x==1 else float('nan') for x,y in df[['col_1','col_2']].values]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65863722
"link": https://stackoverflow.com/questions/65863722/create-new-rows-in-pandas-by-adding-to-previous-row-looping-until-x-number-of
"question": {
	"title": Create new rows in Pandas, by adding to previous row, looping until x number of rows are made
	"desc": Input: Output: Desired Output: Starting from an initial reference row of list "tw" I need to add 1 to the starting value and keep going. How do I loop and keep creating rows so that the next row is the previous row +1, I need to do this for 3000 rows. A lot of solutions I've seen require me to create lists and add to the pandas dataframe however I cannot manually create 3000 lists and then manually add them to my dataframe. There must be a way to loop over this, please help! 
}
"io": {
	"Frame-1": 
		    1      2      3      4
		0   4      7      3      5
		
	"Frame-2":
		      1      2      3      4
		0     4      7      3      5
		1     5      8      4      6
		2     6      9      5      7
		...  ...    ...    ...    ...
		3000 3003   3006   3002   3004
		
}
"answer": {
	"desc": %s I would use numpy for this, that way we can perform a simple addition with braodcasting. Then create the DataFrame after you've made the array. Growing a DataFrame row by row is horribly inefficient. Another option is to create the one row DataFrame, to create all the rows, fill with the value you want to add () and . 
	"code-snippets": [
		df = (pd.DataFrame([tw], columns=[1,2,3,4])
		        .reindex(range(N))
		        .fillna(1, downcast='infer')
		        .cumsum())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65833007
"link": https://stackoverflow.com/questions/65833007/select-value-from-a-list-of-columns-that-is-close-to-another-value-in-pandas
"question": {
	"title": Select value from a list of columns that is close to another value in pandas
	"desc": I have the following data frame: I want to create another column which stores one value lower than the . Intended result: Here's what I have been trying: If the difference is greater than 3% it doesn't do the job right. Note that the s columns may vary so I would want to keep the Little help will be appreciated. THANKS! 
}
"io": {
	"Frame-1": 
		S0  S1  S2  S3  S4  S5... Price
		10  15  18  12  18  19     16
		55  45  44  66  58  45     64
		77  84  62  11  61  44     20    
		
	"Frame-2":
		S0  S1  S2  S3  S4  S5... Price  Sup
		10  15  18  12  18  19     16    15
		55  45  44  66  58  45     64    58
		77  84  62  11  61  44     20    11
		
}
"answer": {
	"desc": %s Try this: Output: 
	"code-snippets": [
		s = df.filter(like='S')
		
		# compare to the price
		mask = s.lt(df['Price'], axis=0)
		
		# `where(mask)` places `NaN` where mask==False
		# `max(1)` takes maximum along rows, ignoring `NaN`
		# rows with all `NaN` returns `NaN` after `max`, 
		# `fillna(0)` fills those with 0
		df['Price'] = s.where(mask).max(1).fillna(0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65811195
"link": https://stackoverflow.com/questions/65811195/pandas-replace-missing-dataframe-values-conditional-calculation-fillna
"question": {
	"title": Pandas: Replace missing dataframe values / conditional calculation: fillna
	"desc": I want to calculate a pandas dataframe, but some rows contain missing values. For those missing values, i want to use a diffent algorithm. Lets say: If column B contains a value, then substract A from B If column B does not contain a value, then subtract A from C results in: Approach 1: fill the NaN rows using : which results in SyntaxError: cannot assign to function call. Approach 2: fill the NaN rows using : is executed without errors and calculation is correct, these values are printed to the console: but the values are not written into , the datafram remains as is: What is the correct way of overwriting the values? 
}
"io": {
	"Frame-1": 
		print(df)
		   a    b  c  calc
		0  1  1.0  2   0.0
		1  2  1.0  2  -1.0
		2  3  NaN  2   NaN
		3  4  1.0  2  -3.0
		
	"Frame-2":
		print(df['calc'])
		0    0.0
		1   -1.0
		2    NaN
		3   -3.0
		
}
"answer": {
	"desc": %s Approach 2: you are assigning it to value. but this won't modify your original dataframe. also don't use it is too slow. Approach 1: it should be: but this will only find those row value where you have non zero value and fill that with the given value. Use: OR 
	"code-snippets": [
		for index, row in df.iterrows():
		    i = df['calc'].iloc[index]
		
		    if pd.isnull(row['b']):
		        i = row['c']-row['a']
		        print(i)
		    else:
		        i = row['b']-row['a']
		        print(i)
		    df.loc[index,'calc'] = i #<------------- here
		
		----------------------------------------------------------------------
		Pandas where() method is used to check a data frame for one or more condition and return the result accordingly. By default, The rows not satisfying the condition are filled with NaN value.
		----------------------------------------------------------------------
		df['calc'] = np.where(df['b'].isnull(), df['c']-df['a'], df['calc'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65793450
"link": https://stackoverflow.com/questions/65793450/pandas-create-new-column-based-on-conditions-of-multiple-columns
"question": {
	"title": Pandas: Create New Column Based on Conditions of Multiple Columns
	"desc": I have the following dataset: In the cell, ‘key’ is the date when someone is hospitalized and ‘value’ is the number of days. I need to create a new column for hospitalization ('Yes' or 'No'). The condition to be 'yes': The column [AAA or BBB] as well as the column [CCC or DDD] both should have filled-in dates. The date in the column [CCC or DDD] should be the next day of the date in the column [AAA or BBB]. For example, if [AAA or BBB] has a date of January 01, 2020. For 'yes', the date in [CCC or DDD] should be January 02, 2020. Desired output: I have tried the following code, but this captures if the dates are present but doesn't capture the timestamp. Any suggestions would be appreciated. Thanks! 
}
"io": {
	"Frame-1": 
		
		 ID            AAA                  BBB                  CCC                   DDD
		1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1} 
		                             '2016-02-15': 2}
		1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2017-01-06': 1} 
		        '2018-06-05': 1}  
		
		
	"Frame-2":
		 ID            AAA              BBB                  CCC                     DDD               Hospitalized
		1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1}            Yes
		                             '2016-02-15': 2}
		1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN                  NaN                      No
		        '2018-06-05': 1}  
		1236    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2018-01-06': 1}            Yes 
		        '2018-06-05': 1}  
		           
		
}
"answer": {
	"desc": %s df: Try: df: 
	"code-snippets": [
		df = pd.DataFrame([[1234, {'2015-01-01': 1}, {'2016-01-01': 1, '2016-02-15': 2}, {'2015-01-02': 1}, {'2016-01-02': 1}], [1235, {'2017-11-05': 1,'2018-06-05': 1}, {'2018-01-05': 1}, np.nan, np.nan]], columns= ['ID', 'AAA', 'BBB', 'CCC', 'DDD'])
		
		----------------------------------------------------------------------
		import itertools
		from dateutil import parser
		import datetime
		def func(x):
		    A_B_dates = list(map(parser.parse,list(itertools.chain(*[x['AAA'].keys()] + [x['BBB'].keys()]))))
		    C_D_dates = list(map(parser.parse,list(itertools.chain(*[x['CCC'].keys()] + [x['DDD'].keys()]))))
		    for date1 in A_B_dates:
		        if date1+datetime.timedelta(days=1) in C_D_dates:
		            return 'yes'
		    return 'no'
		
		df = df.where(df.notna(), lambda x: [{}])    
		df['Hospitalised'] = df.apply(func, axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65797595
"link": https://stackoverflow.com/questions/65797595/insert-complete-repeated-row-under-condition-pandas
"question": {
	"title": Insert complete repeated row under condition pandas
	"desc": Basically, I'm trying to consider the third column (df1[3]) if the value is higher or equal to 2 I want to repeat i.e insert the whole row to a new row, not to replace. Here is the dataframe: desired output: code for the DataFrame and attempt to solve it: Obviously, the above-stated approach doesn't create a new row with the same values from each column but replaces it. Append() wouldn't solve it either because I do have to preserve the exact same order of the data frame. Is there anything similar to insert/extend/add or slicing approach in list when it comes to pandas dataframe? 
}
"io": {
	"Frame-1": 
		    1           2       3    
		   
		0   5614    banana      1   
		1   4564    kiwi        1   
		2   3314    salsa       2   
		3   3144    avocado     1   
		4   1214    mix         3   
		5   4314    juice       1   
		
	"Frame-2":
		    1           2       3       
		1   5614    banana      1   
		2   4564    kiwi        1   
		3   3314    salsa       2   
		4   3314    salsa       2  
		5   3144    avocado     1   
		6   1214    mix         3   
		7   1214    mix         3 
		8   1214    mix         3 
		7   4314    juice       1   
		
}
"answer": {
	"desc": %s Try : Output: 
	"code-snippets": [
		count = pd.to_numeric(df['3'], errors='coerce').fillna(0).astype(int)
		
		# replace '3' with actual column name
		df.loc[df.index.repeat(count)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65750044
"link": https://stackoverflow.com/questions/65750044/adding-value-from-one-pandas-dataframe-to-another-dataframe-by-matching-a-variab
"question": {
	"title": Adding value from one pandas dataframe to another dataframe by matching a variable
	"desc": Suppose I have a pandas dataframe with 2 columns A second dataframe, contains c1, c2 and a few other columns. My goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. What is the best way to do this? 
}
"io": {
	"Frame-1": 
		           c1            c2
		    0      v1            b1
		    1      v2            b2
		    2      v3            b3
		    3      v4            b4
		    4      v5            b5
		
	"Frame-2":
		   c1 c2 c3  c4
		0  "" b5 500 3
		1  "" b2 420 7
		2  "" b1 380 5
		3  "" b2 470 9
		4  "" b3 290 2
		
}
"answer": {
	"desc": %s One easy way you can do is to use the merge of pandas based on similar column. 
	"code-snippets": [
		main_df = pd.merge(df2, df, on="c2", how="left")
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65749219
"link": https://stackoverflow.com/questions/65749219/add-multiple-dataframe-series-to-new-series-in-same-dataframe
"question": {
	"title": Add multiple DataFrame series to new series in same DataFrame
	"desc": I have a dataset in a .csv which I imported into a DataFrame using pandas, organized in the following manner (obviously not real numbers): What I want to achieve is to save the data in two extra columns G and H in the same DataFrame so I get the following: Where I would like to keep the same index for all data (so B belongs to A, D to C, F to E etc.). As you can see, the original dataset has some missing values, so I would also like to skip these if they are in there. Now, I have looked into pandas append and concat, however I do not see how I can achieve what I wanted, especially with skipping the empty values (presumbly via or some other function?). 
}
"io": {
	"Frame-1": 
		 A   B   C   D    E    F 
		 0  20   4   24   8    28
		 1  21   5   25   NA   NA 
		 NA  NA  6   26   10   30
		 3  23   NA  NA  11   31
		
	"Frame-2":
		A  B  C  D  E  F  G   H
		                  0   20
		                  1   21
		                  ...  ...
		                  11   31
		
}
"answer": {
	"desc": %s For example, your dataframe looks like this: Now lets assume that you need to add first three columns and add to another column. So, As I understood that your data contains basically string type data, the above code should work fine. Otherwise, you can convert the columns into string using the function. Older Solution: You can perform the operation and put it in a . For example: The output will be: Now, let's do some operations: The output is: Now save the new dataframe as excel file: 
	"code-snippets": [
		df = pd.read_excel('sample_excel.xlsx', engine='openpyxl') df
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65739584
"link": https://stackoverflow.com/questions/65739584/turn-columns-values-to-headers-of-columns-with-values-1-and-0-accordingly-p
"question": {
	"title": Turn columns&#39; values to headers of columns with values 1 and 0 ( accordingly) [python]
	"desc": I got a column of the form : The column represents the answers of users to a question of 5 choices (1-5). I want to turn this into a matrix of 5 columns where the indexes are the 5 possible answers and the values are 1 or 0 according to the user's given answer. Visualy i want a matrix of the form: 
}
"io": {
	"Frame-1": 
		0           q4
		1           4
		2           3   
		3           1
		4           2
		5           1
		6           5
		7           1
		8           3
		
	"Frame-2":
		0   q4_1  q4_2  q4_3  q4_4 q4_5
		1   Nan    Nan   Nan   1    Nan
		2   Nan    Nan   1    Nan   Nan
		3   1      Nan   Nan  Nan   Nan
		4   Nan    1     Nan  Nan   Nan
		5   1      Nan   Nan  Nan   Nan
		
}
"answer": {
	"desc": %s  Output: 
	"code-snippets": [
		for i in range(1,6):
		    df['q4_'+str(i)]=np.where(df.q4==i, 1, 0)
		
		def df['q4']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65738704
"link": https://stackoverflow.com/questions/65738704/combining-real-and-imag-columns-in-dataframe-into-complex-number-to-obtain-magni
"question": {
	"title": combining real and imag columns in dataframe into complex number to obtain magnitude using np.abs
	"desc": I have a data frame that has complex numbers split into a real and an imaginary column. I want to add a column (2, actually, one for each channel) to the dataframe that computes the log magnitude: If I try this: I get error: "TypeError: cannot convert the series to <class 'float'>", because I think cmath.complex cannot work on an array. So I then experimented using loc to pick out the first element of ch1_real, for example, to then work out how use it to accomplish what I'm trying to do, but couldn't figure out how to do it: This produces a KeyError. Brute forcing it works, but, I believe it is more legible to use np.abs to get the magnitude, plus I'm more interested in understanding how dataframes and indexing dataframes work and why what I initially attempted does not work. btw, what is the difference between df.ch1_real and df['ch1_real'] ? When do I use one vs. the other? Edit: more attempts at solution I tried using apply, since my understanding is that it "applies" the function passed to it to each row (by default): but this generates the same TypeError, since I think the issue is that complex cannot work on Series. Perhaps if I cast the series to float? After reading this post, I tried using pd.to_numeric to convert a series to type float: to no avail. 
}
"io": {
	"Frame-1": 
		 `    ch1_real  ch1_imag  ch2_real  ch2_imag  ch1_phase  ch2_phase  distance
		79   0.011960 -0.003418  0.005127 -0.019530     -15.95    -75.290       0.0
		78  -0.009766 -0.005371 -0.015870  0.010010    -151.20    147.800       1.0
		343  0.002197  0.010990  0.003662 -0.013180      78.69    -74.480       2.0
		80  -0.002686  0.010740  0.011960  0.013430     104.00     48.300       3.0
		341 -0.007080  0.009033  0.016600 -0.000977     128.10     -3.366       4.0
		
	"Frame-2":
		df['ch1_log_mag'] = 20 * np.log10(np.sqrt(df.ch1_real**2+ df.ch1_imag**2))
		
}
"answer": {
	"desc": %s You can do simple multiplication with which denotes the complex number , see imaginary literals: doesn't work as it needs a float argument, not a whole series. is not a valid expression, as the second argument must be a string, not a series (df.loc[79,'ch1_real'] would work for accessing an element). If you want to use it should be but as apply is just a disguised loop over the rows of the dataframe it's not recommended performancewise. There's no difference between and , it's a matter of personal preference. If your column name contains spaces or dots or the like you must use the latter form however. 
	"code-snippets": [
		df['ch1_log_mag'] = 20 * np.log10((df.ch1_real + 1j * df.ch1_imag).abs())
		
		----------------------------------------------------------------------
		20 * np.log10(df.apply(lambda x: complex(x.ch1_real, x.ch1_imag), 1).abs())
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65721916
"link": https://stackoverflow.com/questions/65721916/transform-a-pandas-dataframe-need-for-a-more-efficient-solution
"question": {
	"title": Transform a pandas dataframe: need for a more efficient solution
	"desc": I have a dataframe indexed by dates from a certain period. My columns are predictions about the value of a variable by the end of a given year. My original dataframe looks something like this: where NaN means that the prediction does not exist for that given year. Since I am working with 20+ years and most predictions are for the next 2-3 years, my real dataframe has 20+ columns mostly containing values. For instance, the column for the year 2005 has predictions made in 2003-2005, but in the range 2006-2020 it's all . I would like to transform my dataframe to something like this: where represents the prediction made for the . This way, I would have a dataframe with only 4 columns (Y_0, Y_1, Y_2, Y_3). I actually achieved this, but in what I think it is a very inefficient way: For a dataframe with only 1000 rows, this is taking almost 3 seconds to run. Can anyone think of a better solution? 
}
"io": {
	"Frame-1": 
		            2016  2017  2018
		2016-01-01   0.0     1   NaN
		2016-07-01   1.0     1   4.1
		2017-01-01   NaN     5   3.0
		2017-07-01   NaN     2   2.0
		
	"Frame-2":
		            Y_0  Y_1  Y_2
		2016-01-01    0    1  NaN
		2016-07-01    1    1  4.1
		2017-01,01    5    3  NaN
		2017-07-01    2    2  NaN
		
}
"answer": {
	"desc": %s You could use to convert it to the long format then pivot back based on the year differences. Using your DataFrame as an example: Long format: Convert back to the desired wide format: 
	"code-snippets": [
		df['year'] = pd.to_datetime(df['date']).dt.year
		df['dt'] = df['prediction_year'] - df['year']
		df = df.pivot(index = 'date', columns='dt', values='prediction').dropna(axis = 1, how = 'all').add_prefix('Y_')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65694203
"link": https://stackoverflow.com/questions/65694203/how-to-concat-two-or-more-data-frames-with-different-columns-names-in-pandas
"question": {
	"title": How to concat two or more data frames with different columns names in pandas
	"desc": I have hundreds csv files and I need join it to one file. I have it all load as pandas dataframes. Sample dataframes: I need this output: or How can I do that? Thanks EDIT: I have cca 500 csv files, this is my code to make one file from them: 
}
"io": {
	"Frame-1": 
		    a   x   y    z
		0  e1   4   7     
		1  e1   5   8     
		2  e1   6   9     
		3  e2  13  16  100
		4  e2  14  17  101
		5  e2  15  18  102
		
	"Frame-2":
		    a   x   y    z
		0  e1   4   7   na
		1  e1   5   8   na
		2  e1   6   9   na
		3  e2  13  16  100
		4  e2  14  17  101
		5  e2  15  18  102
		
}
"answer": {
	"desc": %s This should work Or if you really want values rather than you could do To get this to work in your edited question: 
	"code-snippets": [
		df1 = pd.DataFrame({'a':['e1','e1','e1'],'x':[4,5,6],'y':[7,8,9]})
		df2 = pd.DataFrame({'a':['e2','e2','e2'],'x':[13,14,15],'y':[16,17,18], 'z':[100,101,102]})
		newdf = df1.append(df2, ignore_index=True)
		
		----------------------------------------------------------------------
		import glob
		import pandas as pd
		
		path = r'C:/Users/Miro/data hist'
		all_files = glob.glob(path + "/*.csv")
		
		li = pd.DataFrame()
		
		for filename in all_files:
		    df = pd.read_csv(filename, index_col=None, sep='delimiter', header=None)
		    li = li.append(df, ignore_index=True)
		
		
		li.to_csv( "full.csv", index=False, encoding='utf-8-sig')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65692969
"link": https://stackoverflow.com/questions/65692969/how-to-spread-a-key-value-pair-across-multiple-columns-and-flatten-the-matrix-ba
"question": {
	"title": How to spread a key-value pair across multiple columns and flatten the matrix based on another column?
	"desc": Using Pandas 1.2.0, I want to transform this dataframe where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively: into: My attempt: What should I do next to flatten the diagonals of these sub-matrices and group by 'a'? 
}
"io": {
	"Frame-1": 
		     a  b     c
		0  x_1  1  6.00
		1  x_1  2  3.00
		2  x_1  3  0.00
		3  x_1  4  1.00
		4  x_1  5  3.40
		5  j_2  1  4.50
		6  j_2  2  0.10
		7  j_2  3  0.20
		8  j_2  5  0.88
		
	"Frame-2":
		     a    1    2    3    4     5
		0  x_1  6.0  3.0  0.0  1.0  3.40
		1  j_2  4.5  0.1  0.2  NaN  0.88
		
}
"answer": {
	"desc": %s EDIT: A typo in the sample data made not work because of duplicates. should work as well or : Tested on your specific version. 
	"code-snippets": [
		df = df.pivot(index='a', columns='b', values='c')
		----------------------------------------------------------------------
		df = df.pivot('a', 'b', 'c')
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65669445
"link": https://stackoverflow.com/questions/65669445/python-pandas-get-row-indices-for-a-particular-value-in-a-column
"question": {
	"title": Python - Pandas: get row indices for a particular value in a column
	"desc": Given a pandas dataframe, is there a way to get the indices of rows where a column has particular values? Consider the following toy example: CSV (save as test1.csv) What I currently have is this: Is there an option/functionality that can give me something like the following? (I want to be able to do this for large value lists, fast!) Desired output: 
}
"io": {
	"Frame-1": 
		id,val1,val2
		1,20,A
		1,19,A
		1,23,B
		2,10,B
		2,10,A
		2,14,A
		
	"Frame-2":
		   id  val1 val2
		0   1    20    A
		1   1    19    A
		2   1    23    B
		3   2    10    B
		4   2    10    A
		5   2    14    A
		[0, 1, 2]
		[3, 4, 5]
		
}
"answer": {
	"desc": %s Try : Output: 
	"code-snippets": [
		{k: list(d.index) for k, d in df.groupby('id')}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65679439
"link": https://stackoverflow.com/questions/65679439/pandas-add-an-empty-row-after-every-index-in-a-multiindex-dataframe
"question": {
	"title": Pandas: Add an empty row after every index in a MultiIndex dataframe
	"desc": Consider below : How can I add an empty row after each index? Expected output: 
}
"io": {
	"Frame-1": 
		              IA1  IA2  IA3
		Name Subject               
		Abc  DS        45   43   34
		     DMS       43   23   45
		     ADA       32   46   36
		Bcd  BA        45   35   37
		     EAD       23   45   12
		     DS        23   35   43
		Cdf  EAD       34   33   23
		     ADA       12   34   25
		
	"Frame-2":
		              IA1  IA2  IA3
		Name Subject               
		Abc  DS        45   43   34
		     DMS       43   23   45
		     ADA       32   46   36
		
		Bcd  BA        45   35   37
		     EAD       23   45   12
		     DS        23   35   43
		
		Cdf  EAD       34   33   23
		     ADA       12   34   25
		     
		
}
"answer": {
	"desc": %s Use custom function for add empty rows in : Or: 
	"code-snippets": [
		def f(x):
		    x.loc[('', ''), :] = ''
		    return x
		
		----------------------------------------------------------------------
		def f(x):
		    return x.append(pd.DataFrame('', columns=df.columns, index=[(x.name, '')]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65668962
"link": https://stackoverflow.com/questions/65668962/loop-which-every-iteration-will-replace-one-column-from-a-dataframe-with-zeros
"question": {
	"title": Loop which every iteration will replace one column from a dataframe with zeros
	"desc": I want to perform Sensitivity Analysis for classification models in Python. So I want to check how lack of every column will affect the metrics. I prepared function which returns metrics from original test set. I want perform the same but for X_test which with every iteration will have one column values replaced with 0. For example if this would be X_test: I would like to check those metrics for: and so on. And my question is to edit above code (or suggest something else) which help me to achieve it. Later I would like to have this outcome in Pandas DataFrame but it's enough to show me up to dictionary state. 
}
"io": {
	"Frame-1": 
		    A   B   C   D   E
		    5   7   11  12  6
		   11   32  11  13  6
		
	"Frame-2":
		    A   B   C   D   E
		    0   7  11  12   6
		    0  32  11  13   6
		
		    A   B   C   D   E
		    5   0  11  12   6
		   11   0  11  13   6
		
		    A   B   C   D   E
		    5   7   0   12  6
		   11   32  0   13  6
		
}
"answer": {
	"desc": %s So from your example, you actually dont want to drop the column, just give 0 value during the iteration. Then you can use: One option to integer this in your existing code: 
	"code-snippets": [
		for c in df.columns:
		    newDF = df.copy(deep=True)
		    newDF[c] = 0
		    # Here you opperate with the new DF in this instance
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65642545
"link": https://stackoverflow.com/questions/65642545/put-only-elements-into-a-list-with-a-certian-number
"question": {
	"title": put only elements into a list with a certian number
	"desc":  is the sales ID and is the sold itemid. I would like to use all unique i_ids to find all purchases that have interacted with i_id. I also implemented this in the loop. What I would like that I only want to add something to the list when the has more of a 1 item. How do I do that so that I only add the purchases to the list if it contains more than one item? Output But what I want means that the element does not exist, I only wrote for a better understanding 
}
"io": {
	"Frame-1": 
		[[1], [1, 2, 3], [1], [4, 1, 2]]
		[[1, 2, 3], [4, 1, 2]]
		[[1, 2, 3], [3, 5]]
		[[4, 1, 2]]
		[[3, 5]]
		
	"Frame-2":
		[[REMOVED], [1, 2, 3], [REMOVED], [4, 1, 2]] 
		[[1, 2, 3], [4, 1, 2]]
		[[1, 2, 3], [3, 5]]
		[[4, 1, 2]]
		[[3, 5]]
		
}
"answer": {
	"desc": %s A slightly different approach with two s. One for getting the items in an and the other for grouping the purchases happened along with particular s. Firstly, get the mapping of to list of Then grouping by to create list of list of items if length of "list of items" is greater than 1 Compared the timing, this approach () seems to be faster than approach in question () 
	"code-snippets": [
		6.71 ms ± 91 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		----------------------------------------------------------------------
		12.3 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65601285
"link": https://stackoverflow.com/questions/65601285/how-to-fill-multiple-list-by-0s-in-pandas-data-frame
"question": {
	"title": How to fill multiple list by 0&#39;s in Pandas data frame?
	"desc": I have Pandas data frame and I am trying to add 0's in those lists where numbers are missing. In the below data frame, the max length of the list is 4 which is in the 3rd position. accordingly, I will add 0's to the remaining lists. Input: Output: 
}
"io": {
	"Frame-1": 
		        Lists
		0     [158, 202]
		1     [609, 405]
		2     [544, 20]
		3     [90, 346, 130, 202]
		4     [6]
		
	"Frame-2":
		        Lists
		0     [158, 202, 0, 0]
		1     [609, 405, 0, 0]
		2     [544, 20, 0, 0]
		3     [90, 346, 130, 202]
		4     [6, 0, 0, 0]
		
}
"answer": {
	"desc": %s Another way to do this would be using apply with a lambda function - 
	"code-snippets": [
		maxlen = df['Lists'].str.len().max() #as suggested by Anky, better than an apply since vectorised
		f = lambda x: x + ([0] * (maxlen - len(x)))
		
		df['Padded'] = df['Lists'].apply(f)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65563873
"link": https://stackoverflow.com/questions/65563873/how-to-use-pandas-dataframe-as-condition-for-other-dataframe
"question": {
	"title": How to use pandas dataframe as condition for other dataframe
	"desc": Say I have dataframe A: and dataframe B: How can I use dataframe A as a condition for dataframe B, so that df B's cells are within the and values of df A. The ideal output is this: (first cell is explanatory) 
}
"io": {
	"Frame-1": 
		      A B  C
		lower 1 0 -5
		upper 2 2  0
		
	"Frame-2":
		     A  B  C
		sa   5  1 -2
		sb   3  0  2
		sc   1 -5  1
		
}
"answer": {
	"desc": %s You can chain mask with compare selected rows by in with and by for bitwise : Notice - columns names have to match in both s. 
	"code-snippets": [
		df = B.ge(A.loc['lower']) & B.le(A.loc['upper'])
		#alternative
		#df = (B>= A.loc['lower']) & (B <= A.loc['upper'])
		print (df)
		        A      B      C
		sa  False   True   True
		sb  False   True  False
		sc   True  False  False
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65550028
"link": https://stackoverflow.com/questions/65550028/count-the-number-of-specific-values-in-multiple-columns-pandas
"question": {
	"title": Count the number of specific values in multiple columns pandas
	"desc": I have a data frame: I want to count the number of times 'BUY' appears in each row. Intended result: I have tried the following but it simply gives 0 for all the rows: Note that BUY can only appear in B, C, D, E columns. I tried to find the solution online but shockingly found none. Little help will be appreciated. THANKS! 
}
"io": {
	"Frame-1": 
		A    B    C    D     E
		
		12  4.5  6.1   BUY  NaN
		12  BUY  BUY   5.6  NaN
		BUY  4.5  6.1  BUY  NaN
		12  4.5  6.1   0    NaN 
		
	"Frame-2":
		A    B    C    D     E   score
		
		12  4.5  6.1   BUY  NaN    1
		12  BUY  BUY   5.6  NaN    2
		15  4.5  6.1  BUY   NaN    1
		12  4.5  6.1   0    NaN    0
		
}
"answer": {
	"desc": %s Or you could use with : Output: 
	"code-snippets": [
		df['score'] = df.apply(lambda x: x.tolist().count('BUY'), axis=1)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65530626
"link": https://stackoverflow.com/questions/65530626/append-loop-output-in-column-pandas-python
"question": {
	"title": Append loop output in column pandas python
	"desc": I am working with the code below to append output to empty dataframe i am getting output as below but i want What i want the output to be How can i make 3 rows to 3 columns every time the loop repeats. 
}
"io": {
	"Frame-1": 
		        0
		0   30708
		1      15
		2    1800
		0   19200
		1      50
		2    1180
		
	"Frame-2":
		        0    1       2
		0   30708   15    1800
		1   19200   50    1180
		
}
"answer": {
	"desc": %s It perplexes me when you write without doing any manipulation on . Seems like a redundant operation. Another problem with your code is that is slow since it has to copy the backing memory. Repeatedly calling it in a loop is a guarantee of performance bottleneck. Try this instead: 
	"code-snippets": [
		# image_data starts as a list
		image_data = []
		
		for i in words:
		    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)
		    image_data.append(y)
		
		# And it ends as a DataFrame
		image_data = pd.DataFrame(image_data)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65515347
"link": https://stackoverflow.com/questions/65515347/fill-null-values-in-a-column-using-percent-change-from-a-second-column-while-gro
"question": {
	"title": Fill null values in a column using percent change from a second column while grouping by a third column
	"desc": I have a dataframe that looks like this: I want to fill in the gaps in the column by applying the same percent change as was calculated. However I also need to group using the column. I should end up with something like this: I only want to replace values that are null. Notice the 10 in row seven "resets" the forward fill. Without having to group, I could simply get the percent change in and multiply the previous row's cell by the current row's percent change cell wherever is not null. I was thinking that I could order the dataframe using , but then I would still have to worry about the edge case of when values change. 
}
"io": {
	"Frame-1": 
		grp    val    run
		a      5      10
		b      10     1
		a      NaN    8
		a      NaN    4
		b      NaN    5
		b      NaN    4
		a      10     6
		a      NaN    6
		
	"Frame-2":
		grp    val    run
		a      5      10
		b      10     1
		a      4      8
		a      2      4
		b      50     5
		b      40     4
		a      10     6
		a      10     6
		
}
"answer": {
	"desc": %s Let us try: Output ( the extra column that can be dropped): 
	"code-snippets": [
		# identify the na blocks and group by `grp` and these blocks
		na_blocks = df['val'].notna().groupby(df['grp']).cumsum()    
		g = df.groupby(['grp', na_blocks])
		
		# "pct change" on run
		df['x'] = df['run'] / g['run'].shift(fill_value=1)
		
		# cumprod() for cumulative change
		# `ffill` and `transform('first')` behave the same 
		# since we are grouping on non-nan following by consecutive nan's
		df['val'] = g['val'].ffill() * g['x'].cumprod() / g['run'].transform('first')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65483406
"link": https://stackoverflow.com/questions/65483406/replace-nan-values-with-the-means-of-other-cols-based-on-condition
"question": {
	"title": Replace NaN Values with the Means of other Cols based on Condition
	"desc": I have the following Pandas DataFrame I am writing the following function: I want to to replace the missing values present in columns with labels in the list . The value to be replaced is computed as the mean of the non missing values of the corresponding group. Groups are formed based on the values in the columns with labels in the list . When is applied to the above dataframe with arguments, it should yield: this is because the record on line 4 belongs to the group that has a mean of (1+3)/2 = 2. I tried using but it is giving me the error 
}
"io": {
	"Frame-1": 
		  Col1 Col2  Col3
		0    A    c   1.0
		1    A    c   3.0
		2    B    c   5.0
		3    A    d   6.0
		4    A    c   NaN
		
	"Frame-2":
		 Col1 Col2  Col3
		0    A    c   1.0
		1    A    c   3.0
		2    B    c   5.0
		3    A    d   6.0
		4    A    c   2.0
		
}
"answer": {
	"desc": %s You could implement the function like this: Output 
	"code-snippets": [
		def replace_missing_with_conditional_mean(df, condition_cols, cols):
		    s = df.groupby(condition_cols)[cols].transform('mean')
		    return df.fillna(s.to_dict('series'))
		
		
		res = replace_missing_with_conditional_mean(df, ['Col1', 'Col2'], ['Col3'])
		print(res)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65451097
"link": https://stackoverflow.com/questions/65451097/why-having-std-for-1-column-and-others-are-nan
"question": {
	"title": why having std for 1 column and others are nan?
	"desc": i have DataFrame looks something like this but with shape (345,5) like this and i want to get the std for the numeric columns ONLY with my manually std function and save in dictionary, the probelm is i am getting this result for the first column only: and here is my code: 
}
"io": {
	"Frame-1": 
		|something1|  something2|  numbers1| number2 |number3|
		|----------|------------|----------|---------|-------|
		| A        | str        |    45    | nan     |nan    |
		|B         | str2       |   6      |  nan    | nan   |
		| c        | str3       |   34     |  67     | 45    |
		|D         | str4       |    56    |  45     | 23    |
		
	"Frame-2":
		{'number1': 18.59267328815305,
		 'number2': nan,
		 'number3': nan,
		 'number4': nan}
		
}
"answer": {
	"desc": %s Pandas can handle this, try instead by default NaNs are skipped: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html if you want this in a dict then do: edit: if you are dead-set on using specifically your custom standard deviation function, just dropna from the column before applying: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dropna.html 
	"code-snippets": [
		std = {column:std_func(df[column].dropna().values) for column in df.columns}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65385779
"link": https://stackoverflow.com/questions/65385779/python-pandas-update-a-column-based-on-a-series-holding-sums-of-that-same-colum
"question": {
	"title": python/pandas: update a column based on a series holding sums of that same column
	"desc": I have a dataframe with a non-unique col1 like the following Some of the values of col1 repeat lots of times and others not so. I'd like to take the bottom (80%/50%/10%) and change the value to 'other' ahead of plotting. I've got a series which contains the codes in col1 (as the index) and the amount of times that they appear in the df in descending order by doing the following: I've also got my cut-off point (bottom 80%) I'd like to update col1 in df with the value 'others' when col1 appears after the cutOff in the index of the series df2. I don't know how to go about checking and updating. I figured that the best way would be to do a groupby on col1 and then loop through, but it starts to fall apart, should I create a new groupby object? Or do I call this as an .apply() for each row? Can you update a column that is being used as the index for a dataframe? I could do with some help about how to start. edit to add: So if the 'b's in col1 were not in the top 20% most populous values in col1 then I'd expect to see: 
}
"io": {
	"Frame-1": 
		    col1    col2
		0      a      1
		1      a      1
		2      a      2
		3      b      3
		4      b      3
		5      c      2
		6      c      2
		
	"Frame-2":
		    col1    col2
		0      a      1
		1      a      1
		2      a      2
		3 others      3
		4 others      3
		5      c      2
		6      c      2
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		data = [["a ", 1],
		        ["a ", 1],
		        ["a ", 2],
		        ["b ", 3],
		        ["b ", 3],
		        ["c ", 2],
		        ["c ", 2], ]
		df = pd.DataFrame(data, columns=["col1", "col2"])
		print(df)
		
		df2 = df.groupby(['col1']).size().sort_values(ascending=False)
		print(df2)
		
		cutOff = round(len(df2) / 5)
		others = df2.iloc[cutOff + 1:]
		print(others)
		
		result = df.copy()
		result.loc[result["col1"].isin(others.index), "col1"] = "others"
		print(result)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65392984
"link": https://stackoverflow.com/questions/65392984/pandas-advanced-problem-for-each-row-get-complex-info-from-another-dataframe
"question": {
	"title": Pandas advanced problem : For each row, get complex info from another dataframe
	"desc": Problem I have a dataframe : And I have another dataframe , with products like this : What I want is to add to a column, that will sum the Prices of the last products of each type known at the current date (with ). An example is the best way to explain : For the first row of The last A-Product known at this date bought by is this one : (since the 4th one is older, and the 5th one has a > ) The last B-Product known at this date bought by is this one : So the row in , after transformation, will look like that ( being , prices of the 2 products of interest) : The full after transformation will then be : As you can see, there are multiple possibilities : Buyers didn't necessary buy all products (see , who only bought A-products) Sometimes, no product is known at (see row 3 of the new , in 2015, we don't know any product bought by ) Sometimes, only one product is known at , and the value is the one of the product (see row 3 of the new , in 2015, we only have one product for , which is a B-product bought in 2014, whose price is ) What I did : I found a solution to this problem, but it's taking too much time to be used, since my dataframe is huge. For that, I iterate using iterrows on rows of , I then select the products linked to the Buyer, having on , then get the older grouping by and getting the max date, then I finally sum all my products prices. The fact I solve the problem iterating on each row (with a for iterrows), extracting for each row of a part of that I work on to finally get my sum, makes it really long. I'm almost sure there's a better way to solve the problem, with pandas functions ( for example), but I couldn't find the way. I've been searching a lot. Thanks in advance for your help Edit after Dani's answer Thanks a lot for your answer. It looks really good, I accepted it since you spent a lot of time on it. Execution is still pretty long, since I didn't specify something. In fact, are not shared through buyers : each buyers has its own multiple products types. The true way to see this is like this : As you can understand, product types are not shared through different buyers (in fact, it can happen, but in really rare situations that we won't consider here) The problem remains the same, since you want to sum prices, you'll add the prices of last occurences of johndoe-ID2 and johndoe-ID3 to have the same final result row But as you now understand, there are actually more than , so the step "get unique product types" from your answer, that looked pretty fast on the initial problem, actually takes a lot of time. Sorry for being unclear on this point, I didn't think of a possibility of creating a new df based on product types. 
}
"io": {
	"Frame-1": 
		3       A              2019-01-01     johndoe   600
		
	"Frame-2":
		7       B              2016-11-15     johndoe   300
		
}
"answer": {
	"desc": %s The main idea is to use merge_asof to fetch the last date for each Product-Type and Client_ID, so do the following: Output The problem is that merge_asof won't work with duplicate values, so we need to create unique values. These new values are the cartesian product of Client_ID and Product-Type, this part is done in: Finally do a groupby and sum the Price, not before doing a fillna to fill the missing values. UPDATE You could try: The idea here is to change how you generate the unique values. 
	"code-snippets": [
		# get unique product types
		product_types = list(df_prod['Product-Type'].unique())
		
		# create a new DataFrame with a row for each Product-Type for each Client_ID
		df['Product-Type'] = [product_types for _ in range(len(df))]
		df_with_prod = df.explode('Product-Type')
		
		# merge only the closest date by each client and product type
		merge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),
		                      df_prod.sort_values(['Product-Date', 'Buyer']),
		                      left_on='Date',
		                      right_on='Product-Date',
		                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])
		
		# fill na in prices
		merge['Price'] = merge['Price'].fillna(0)
		
		# sum Price by client and date
		res = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})
		print(res)
		
		----------------------------------------------------------------------
		# get unique product types
		product_types = list(df_prod['Product-Type'].unique())
		
		# create a new DataFrame with a row for each Product-Type for each Client_ID
		df['Product-Type'] = [product_types for _ in range(len(df))]
		df_with_prod = df.explode('Product-Type')
		
		----------------------------------------------------------------------
		# get unique product types
		product_types = df_prod.groupby('Buyer')['Product-Type'].apply(lambda x: list(set(x)))
		
		# create a new DataFrame with a row for each Product-Type for each Client_ID
		df['Product-Type'] = df['Client_ID'].map(product_types)
		df_with_prod = df.explode('Product-Type')
		
		# merge only the closest date by each client and product type
		merge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),
		                      df_prod.sort_values(['Product-Date', 'Buyer']),
		                      left_on='Date',
		                      right_on='Product-Date',
		                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])
		
		# fill na in prices
		merge['Price'] = merge['Price'].fillna(0)
		
		# sum Price by client and date
		res = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})
		
		print(res)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65356414
"link": https://stackoverflow.com/questions/65356414/how-to-apply-a-function-to-every-element-in-a-dataframe
"question": {
	"title": How to apply a function to every element in a dataframe?
	"desc": This is probably a very basic question but I can't find the answer in other questions. I have two lists that I have used to create a 2D dataframe, let's say: Which gives: I would like to go through all elements in the dataframe and use the values of and as inputs to some function, , that I have written. For example, in the 2rd row, 1st column (using zero indexing) position I have , so in this position I would like to apply and not . I think I should be able to use or somehow but I can't figure it out! 
}
"io": {
	"Frame-1": 
		     10.0 15.0 20.0 25.0
		0.00  NaN  NaN  NaN  NaN
		0.25  NaN  NaN  NaN  NaN
		0.50  NaN  NaN  NaN  NaN
		0.75  NaN  NaN  NaN  NaN
		1.00  NaN  NaN  NaN  NaN
		1.25  NaN  NaN  NaN  NaN
		1.50  NaN  NaN  NaN  NaN
		1.75  NaN  NaN  NaN  NaN
		2.00  NaN  NaN  NaN  NaN
		
	"Frame-2":
		(X, Y) = (0.5, 15.0)
}
"answer": {
	"desc": %s Since your problem requires access to both the index and column labels of your you probably want . has access to a representing each row/column (dependent on argument value) and you will have access to the column name and index; whereas utilises each individual value of at runtime - so you wouldn't necessarily have access to the index and column name as required. Example Output In the above example the column name and index of each Series constituting is passed to by way of . Within each value is defined by subtracting it's own index value from it's own column name value. Here you can see that the index value for each row is accessed using and the column value is accessed using within the call within . Update Many thanks to @SyntaxError for pointing out that and could be passed to within instead of feeding the entire Series () into the function and accessing the values manually therein. As mentioned, this seems to fit OP's use case in a much neater manner than my original response - which was largely the same but passed each series into which then had responsibility for extracting and . 
	"code-snippets": [
		import numpy as np
		import pandas as pd 
		
		def foo(name, index):
		    return name - index
		
		x = np.arange(0, 2.01, 0.25)
		y = np.arange(10, 30, 5.0) 
		
		df = pd.DataFrame(index = x, columns = y)
		
		df.apply(lambda x: foo(x.name, x.index))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65294879
"link": https://stackoverflow.com/questions/65294879/pandas-assigning-values-to-dataframe-conditional-on-values-in-another-with-the
"question": {
	"title": Pandas assigning values to dataframe, conditional on values in another with the same dimensions issue/question
	"desc": I'm trying to better understand Pandas/Python so I've been playing around with some stuff. I ran into an issue, I know some workarounds, but I'm wondering why it happened in the first place. Here's my full code, followed by an explanation: I create, 2 dataframes. The first with random numbers, the second dataframe is empty but has the same dimensions as the first. Based on the values in the first dataframe, I'd like to modify the values in the second. My first datframe I create looks like this: I create a second dataframe based on the dimensions of the second: What I would like to do now, is say that for values that are greater 0.6 in df1, I would like the corresponding value in df2 to be 1. And for values less than 0.6 I would like the values to be 0. I did that in the following way, by slicing df1 and then using that slice on df2, and then assigning the values. I thought this would work, but instead, the first row and first column are still NANs Now the reason this didn't work, I think, is because the column names and the row names don't align between the two indices, but what I'm trying to understand is why that's happening. I thought when I sliced df1 based on the conditional it created an array of trues/falses, that I could use on any other dataframe with the same dimension: r I thought that mapping of trues and falses above could be used anywhere, it seems like it can't. Is there a way around this doesn't involve renaming the columns/rows to match between the 2 dataframes? 
}
"io": {
	"Frame-1": 
		df1
		
		      1      2       3        4      5        6      7       8        9     10
		1   0.24    0.03    0.93    0.38    0.03    0.83    0.47    0.85    0.79    0.65
		2   0.66    0.25    0.01    0.28    0.19    0.26    0.25    0.48    0.33    0.92
		3   0.53    0.33    0.78    0.04    0.36    0.63    0.16    0.16    0.21    0.96
		4   0.76    0.03    0.89    0.15    0.24    0.90    0.59    0.41    0.92    0.98
		5   0.72    0.45    0.95    0.44    0.79    0.93    0.90    0.48    0.61    0.02
		
	"Frame-2":
		df2
		
		         0   1   2   3   4  5   6   7   8   9
		 0      NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN
		 1      NaN  0   0   1   0   0   1   0   1   1
		 2      NaN  1   0   0   0   0   0   0   0   0
		 3      NaN  0   0   1   0   0   1   0   0   0
		 4      NaN  1   0   1   0   0   1   0   0   1
		
}
"answer": {
	"desc": %s Just need to do: Output If need the output to be integer: Output (integer) If need to map to values to True and False, use np.where: Output (where) 
	"code-snippets": [
		df2 = (df1 > 0.6).astype(int)
		print(df2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65262268
"link": https://stackoverflow.com/questions/65262268/how-do-i-turn-categorical-column-values-into-different-column-names
"question": {
	"title": How do I turn categorical column values into different column names?
	"desc": I'm not sure how to approach this problem since I'm a beginner with pandas. I have this dataframe: and I want to turn it into a dataframe or a matrix like this: How should I approach this in Python? 
}
"io": {
	"Frame-1": 
		  col1 col2
		0    a    1 
		1    a    2 
		2    a    3 
		3    b    4
		4    b    5
		5    b    6
		6    c    7
		7    c    8
		8    c    9
		
	"Frame-2":
		   cola colb  colc
		0    1    4    7
		1    2    5    8
		2    3    6    9
		
}
"answer": {
	"desc": %s Let's the dataframe on and create key-value pairs inside comprehension: Alternatively, you can use + to create a sequential counter to distinguish different rows per group in , then use + to reshape: Another approach with with + : Result: 
	"code-snippets": [
		pd.DataFrame({k: [*g['col2']] for k, g in df.groupby('col1')})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65223498
"link": https://stackoverflow.com/questions/65223498/unprecise-values-when-using-pd-dataframe-values-tolist
"question": {
	"title": Unprecise values when using pd.Dataframe.values.tolist
	"desc": When converting a pd.DataFrame to a nested list, some values are unprecise. pd.DataFrame examplary row: pd.DataFrame.values.tolist() of this row: How can this be explained and avoided? 
}
"io": {
	"Frame-1": 
		1.0 -3.0 -3.0 0.01 -3.0 -1.0
		
	"Frame-2":
		[1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0]
		
}
"answer": {
	"desc": %s This is because this is the original value. When you display the pd.DataFrame it gets rounded: So it is not tolist()'s problem. It is pd.DataFrame that is rounding the numbers. Use to set display precision for DataFrame. 
	"code-snippets": [
		df.values.tolist()
		# [[1.0], [-3.0], [-3.0], [0.010000000000000009], [-3.0], [-1.0]]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65222196
"link": https://stackoverflow.com/questions/65222196/efficiently-relocate-elements-conditionally-in-a-panda-dataframe
"question": {
	"title": Efficiently relocate elements conditionally in a panda.Dataframe
	"desc": I am trying to sort the values of my data.frame in the following way: It is working, however it is very slow for my +40k rows. How can I do this more efficiently and more elegantly? I would prefer a solution that directly manipulates the original df, if possible. Example data: Desired output: 
}
"io": {
	"Frame-1": 
		x1  p1 x2   p2
		1  0.4  2  0.6
		2  0.2  1  0.8
		
	"Frame-2":
		x1  p1 x2   p2
		1  0.4  2  0.6
		1  0.8  2  0.2
		
}
"answer": {
	"desc": %s Here's one way that use a selection of the rows and then does a swap of the values using that selection 
	"code-snippets": [
		check = df["x1"] > df["x2"]
		df.loc[check, ["x2", "x1", "p2", "p1"]] = df.loc[check, ["x1", "x2", "p1", "p2"]].values
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 63282322
"link": https://stackoverflow.com/questions/63282322/how-to-read-list-of-json-objects-from-pandas-dataframe
"question": {
	"title": How to read list of json objects from Pandas DataFrame?
	"desc": I want just want to loop through the array of json objects, and get the values of 'box'..... I have a DataFrame which looks like this and the column 'facesJson' (dstype = object) contain array of json objects which look like this: when i run this code i get this error: 
}
"io": {
	"Frame-1": 
		       img                    facesJson
		0   2b26mn4.jpg [{'box': [57, 255, 91, 103], 'confidence': 0.7...
		1   cd7ntf.jpg  [{'box': [510, 85, 58, 87], 'confidence': 0.99...
		2   m9kf3e.jpg  [{'box': [328, 78, 93, 123], 'confidence': 0.9...
		3   b4hx0n.jpg  [{'box': [129, 30, 38, 54], 'confidence': 0.99...
		4   afx0fm.jpg  [{'box': [86, 126, 221, 298], 'confidence': 0....
		
	"Frame-2":
		[
		   {
		      "box":[ 158,115,84,112 ],
		      "confidence":0.9998929500579834,
		   },
		   {
		      "box":[ 404,105, 86,114 ],
		      "confidence":0.9996863603591919,
		   }
		]
		
}
"answer": {
	"desc": %s You can use this code on loop for each item of that column. 
	"code-snippets": [
		import ast
		
		line='[ { "box":[ 158,115,84,112 ], "confidence":0.9998929500579834, }, { "box":[ 404,105, 86,114 ], "confidence":0.9996863603591919, } ]'
		
		
		parsed=ast.literal_eval(line)
		
		print(parsed[0].keys())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65194434
"link": https://stackoverflow.com/questions/65194434/python-how-do-i-merge-rows-that-shares-the-same-name-of-ocode-or-ccode-in-da
"question": {
	"title": Python: How do I merge rows that shares the same name of &quot;Ocode&quot; or &quot;Ccode in dataframe
	"desc": I'm thinking of using pandas to merge several repetitive rows of "Ocode" and Ccode". Ideally I want only one "Ocode" or "Ccode" per row. When there are repetitive dates under c## (I.E. c21), only the latest date is kept. Separate dates under different column with the same "Ocode"/"Ccode" should also be merged. (For reference purpose: O and C code correspondingly represents Organization Code and Corporation code.) This is the heading of the dataframe. Which should become ----> Attempt: and perform the merge one by one. However, this method tends to delete information under other column when dealing with one of the c##(I.E. c21) column df.to_excel(ic + '.xlsx', index=False) 
}
"io": {
	"Frame-1": 
		Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75
		0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09
		1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN
		2    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN
		3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN
		4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN
		...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         
		
		2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN
		2385       NaN  CU0280002  2016-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN
		2386       NaN  CU0280002         NaN  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN
		2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN
		
	"Frame-2":
		Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75
		0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09
		1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN
		3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN
		4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN
		...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         
		2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN
		2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN
		
}
"answer": {
	"desc": %s One idea is grouping by both columns witt replace for avoid remove this groups in oldier pandas versions with for last non missing values per groups: For last versions of pandas: EDIT: Solution above test combination of both columns, if need prioritize it means set to if exist both use before solution above: EDIT1: One idea processing both codes separately: 
	"code-snippets": [
		df = (df.assign(Ocode = df['Ocode'].fillna('nan'),Ccode = df['Ccode'].fillna('nan'))
		        .groupby(['Ocode','Ccode'])
		        .last()
		        .reset_index()
		        .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))
		
		----------------------------------------------------------------------
		df = (df.groupby(['Ocode','Ccode'])
		        .last()
		        .reset_index())
		
		----------------------------------------------------------------------
		df.loc[df['Ocode'].notna() & df['Ocode'].notna(), 'Ccode'] = np.nan
		
		----------------------------------------------------------------------
		df1 = (df.assign(Ocode = df['Ocode'].fillna('nan'))
		         .drop('Ccode', axis=1)
		         .groupby('Ocode')
		         .last()
		         .reset_index())
		
		df2 = (df.assign(Ccode = df['Ccode'].fillna('nan'))
		         .drop('Ocode', axis=1)
		         .groupby('Ccode')
		         .last()
		         .reset_index())
		
		both = (pd.concat([df1, df2], sort=False)
		          .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65178805
"link": https://stackoverflow.com/questions/65178805/how-do-i-apply-a-function-to-the-groupby-sub-groups-that-depends-on-multiple-col
"question": {
	"title": How do I apply a function to the groupby sub-groups that depends on multiple columns?
	"desc": Take the following data frame and groupby object. How would I apply to the groupby object , multiplying each element of and together and then taking the sum. So for this example, for the group and for the group. So my desired output for the groupby object is: 
}
"io": {
	"Frame-1": 
		2*3 + 4*5 = 26
	"Frame-2":
		   a  f
		0  1  26
		2  2  30
		
}
"answer": {
	"desc": %s Do: Output 
	"code-snippets": [
		df = pd.DataFrame([[1, 2, 3],[1, 4, 5],[2, 5, 6]], columns=['a', 'b', 'c'])
		df['f'] = df['c'] * df['b']
		res = df.groupby('a', as_index=False)['f'].sum()
		print(res)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65169011
"link": https://stackoverflow.com/questions/65169011/parsing-a-txt-file-into-data-frame-filling-columns-based-on-the-multiple-separa
"question": {
	"title": Parsing a txt file into data frame, filling columns based on the multiple separators
	"desc": Having a .txt file structure as below trying to parse into dataframe of the following structure describing the rule: # i - 'i' is the row number n:data - 'n' is the column number to fill, 'data' is the value to fill into i'th row if the number of columns would be small enough it could be done manually, but txt considered has roughly 2000-3000 column values and some of them are missing. gives the following result I tried to remove the odd rows in data1 even in data2, then will hopefully figure out how to split the odd and merge the 2 df's, but there might be a faster and more beautiful method to do it, that's why asking here update, spent 3 hours figuring out how to work with dataframes, as I was not that familiar with them. now from that using It became this any suggestions on how to add unknown number of phantom columnsnd fill them using "n:value" from the list to fill the "n" column with the "value"? 
}
"io": {
	"Frame-1": 
		#n  1
		a 1:0.0002 3:0.0003...
		#n  2
		b 2:0.0002 3:0.0003...
		#n  3
		a 1:0.0002 2:0.0003... 
		...
		
	"Frame-2":
		#    type  1        2       3 
		1    a     0.0002   null    0.0003 ....
		2    b     null     0.0002  0.0003 ....
		3    a     0.0002   0.0003  null   ....
		...
		
}
"answer": {
	"desc": %s I think you are better off parsing the file yourself than relying on and then dealing with the mess. Here is how I would do it. Since I do not have access to your real file I am using a small example you have in your question. First, load the file. Then we read all lines, group them in pairs, parse and stick the results into a dict Now we have something that looks like this: this dict can now be used directly to create a dataframe, which we proceed to do and also order columns as they are in somewhat random order output 
	"code-snippets": [
		# read all lines
		lines = file.readlines()
		
		# here we will store the results, dictionary of dictionaries
		parsing_res = {}
		
		# a fancy way of processing two lines, odd and even, at the same time
		for line1,line2 in zip(lines[::2],lines[1::2]):
		    # line1 has the form '#n  1', we split on whitespace and take the second tokem
		    row_index = line1.split()[1]
		    # line2 is the other type of lines, split into tokens by whitespace
		    tokens = line2.split()
		    # first one is 'type'
		    t = tokens[0]
		
		    # the others are pairs 'x:y', split them into x,y and stick into a dictionary with label x and value y
		    row_dict = {token.split(':')[0]:token.split(':')[1] for token in tokens[1:]}
		
		    # add type
		    row_dict['type'] = t
		   
		    # store the result for these two lines into the main dictionary
		    parsing_res[row_index] = row_dict
		parsing_res
		
		----------------------------------------------------------------------
		df = pd.DataFrame.from_dict(parsing_res, orient='index')
		df.reindex(sorted(df.columns), axis=1).reindex(sorted(df.index), axis=0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65083476
"link": https://stackoverflow.com/questions/65083476/pandas-mapping-one-dataframe-onto-another
"question": {
	"title": Pandas, mapping one Dataframe onto another?
	"desc": I'm not sure how to tackle this problem. I have 3 data frames; one is a true/false table [3532x622], the other is a single series of integers[662x1], the other is my main dataframe[3532x8]. The true/false table was create by comparing a series of points to find which ones where inside a polygon, that is why is has the shape it does. I have outlined a diagram below as to what I am trying to accomplish. Convert to: Then map this onto the main dataframe This is what I have started I don't know where to go from here. 
}
"io": {
	"Frame-1": 
		df_2
		  0      1      2          8      9
		0 56489  np.nan np.nan ... np.nan 89641
		1 np.nan 86932  np.nan ... 45871  np.nan
		2 np.nan 86932  np.nan ... np.nan np.nan
		
	"Frame-2":
		df_3
		  0      1 
		0 poly_a 56489
		1 ploy_a 89641
		2 poly_b 86932
		3 poly_b 45871
		4 poly_c 86932
		
}
"answer": {
	"desc": %s This should do the trick: Output: 
	"code-snippets": [
		import numpy
		import pandas
		
		# Creating Example Dataframes
		df_1 = pandas.DataFrame([56489, 45872, 89657, 56895, 87456])
		df_2 = pandas.DataFrame(
		    [
		        [True, False, False, False, True],
		        [False, True, True, False, False],
		        [False, True, False, True, True],
		    ]
		)
		df_3 = pandas.DataFrame(["poly_a", "poly_b", "poly_c"])
		
		
		def replace_values(row: pandas.Series) -> pandas.Series:
		
		    # Make a copy of df_1 (first row) but flip it to become columns
		    c = df_1.T.copy().iloc[0]
		
		    # Use the boolean values from the row as index, replace False with NaN
		    c.loc[~row] = numpy.nan
		    return c
		
		
		# Combine 2 and 1
		combined = df_2.apply(replace_values, axis=1)
		
		# Add 3
		result = pandas.concat([df_3, combined], axis=1, ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65087280
"link": https://stackoverflow.com/questions/65087280/find-the-latest-occurrence-of-an-class-item-and-store-how-many-values-are-betwee
"question": {
	"title": Find the latest occurrence of an class item and store how many values are between these two in a pandas DataFrame
	"desc": I have a pandas DataFrame with some labels for classes. Now I want to add a column and store how many items are between two elements of the same class. and I want to get this: This is the code I used: This seems circuitous to me. Is there a more elegant way of producing this output? 
}
"io": {
	"Frame-1": 
		   Class
		0      0
		1      1
		2      1
		3      1
		4      0
		
	"Frame-2":
		    Class  Shift
		0      0    NaN
		1      1    NaN
		2      1    1.0
		3      1    1.0
		4      0    4.0
		
}
"answer": {
	"desc": %s You could do: Output As an alternative: The idea is to create a column with consecutive values, group by the Class column and compute the diff on the new column. 
	"code-snippets": [
		df['shift'] = np.arange(len(df))
		df['shift'] = df.groupby('Class')['shift'].diff()
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65077340
"link": https://stackoverflow.com/questions/65077340/how-to-filter-rows-in-a-dataframe-to-get-only-3-most-popular-and-delete-others-u
"question": {
	"title": How to filter rows in a dataframe to get only 3 most popular and delete others unused data?
	"desc": Theory I have some data on car brands in the US. I have to arrange them on the map of individual states and after hovering over with the mouse, I have to display the 3 most popular brands for a given state. Question I have the following dataframe I need to achieve something like that (structure is probably wrong - I am not sure what kind of structure would be best - I just need data about name of column and its value): Current situation I was able to create something like this: So the situation is identical to the example I gave at the top. Now I have to somehow "filter this data and keep information about the brand name and its percentage in a given state". It is a bit difficult for me, can someone please help me? 
}
"io": {
	"Frame-1": 
		    A   B   C   D   E
		1  20   0  40  10  30
		2   0  60  15   5  20 
		3  50  30  20   0   0
		
	"Frame-2":
		1  (C: 40) (E: 30) (A: 20)
		2  (B: 60) (E: 20) (C: 15)
		3  (A: 50) (B: 30) (C: 20) 
		
}
"answer": {
	"desc": %s You could use nlargest and convert it to a dictionary: Output If order is really important, return a list: Output 
	"code-snippets": [
		res = df.apply(lambda x: pd.Series(x).nlargest(3).to_dict(), axis=1)
		print(res)
		
		----------------------------------------------------------------------
		res = df.apply(lambda x: list(pd.Series(x).nlargest(3).items()), axis=1)
		print(res)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65043847
"link": https://stackoverflow.com/questions/65043847/organize-data-based-on-a-weird-column-distribution-in-pandas
"question": {
	"title": Organize data based on a weird column distribution in pandas
	"desc": Is there an elegant way of segment data in a dataframe in which the first row includes the name of the data owner, and the second row includes headers, with all the data organized below? I have this: I need to order that so that I can analyze it in something like: I though about making different dataframes, but that would be a waste of resources. Is there a more elegant way of doing this? Thanks. 
}
"io": {
	"Frame-1": 
		0   n_1 NaN NaN NaN NaN n_2 NaN NaN NaN NaN ... n_3 NaN NaN NaN NaN n_4 NaN NaN NaN NaN
		1   V1  V2  V3  V4  V5  V1  V2  V3  V4  V5  ... V1  V2  V3  V4  V5  V1  V2  V3  V4  V5
		2   45  43  30  32  NaN 45  52  47  47  NaN ... 45  57  51  50  NaN 45  51  47  50  NaN
		3   50  53  38  38  NaN 50  55  50  41  NaN ... 50  51  48  49  NaN 50  53  52  52  1
		4   50  54  37  41  NaN 50  53  49  49  1   ... 50  54  50  47  NaN 50  54  48  41  1
		5   50  51  40  39  NaN 50  53  50  48  NaN ... 50  53  50  49  NaN 50  51  49  50  NaN
		6   50  53  47  50  NaN 50  50  47  35  NaN ... 50  55  44  34  NaN 50  50  47  47  NaN
		7   50  51  47  45  NaN 50  52  48  48  1   ... 50  51  48  46  NaN 50  51  47  50  NaN
		8   50  52  50  50  NaN 50  50  47  50  NaN ... 50  51  47  48  NaN NaN NaN NaN NaN NaN
		9   NaN NaN NaN NaN NaN 50  54  51  53  NaN ... 50  52  48  51  NaN NaN NaN NaN NaN NaN
		
	"Frame-2":
		0   Own V1  V2  V3  V4  V5  
		1   n_1 45  43  30  32  NaN 
		2   n_1 50  53  38  38  NaN 
		3   n_1 50  54  37  41  NaN 
		4   n_1 50  51  40  39  NaN 
		5   n_1 50  53  47  50  NaN 
		6   n_1 50  51  47  45  NaN 
		7   n_1 50  52  50  50  NaN 
		8   n_2 45  52  47  47  NaN 
		9   n_2 50  55  50  41  NaN 
		10  n_2 50  53  49  49  1   
		11  n_2 50  53  50  48  NaN 
		12  n_2 50  50  47  35  NaN 
		13  n_2 50  52  48  48  1   
		14  n_2 50  50  47  50  NaN 
		15  n_2 50  54  51  53  NaN 
		16  n_3 45  57  51  50  NaN 
		17  n_3 50  51  48  49  NaN 
		18  n_3 50  54  50  47  NaN 
		19  n_3 50  53  50  49  NaN 
		20  n_3 50  55  44  34  NaN 
		21  n_3 50  51  48  46  NaN 
		22  n_3 50  51  47  48  NaN 
		23  n_3 50  52  48  51  NaN
		24  n_4 45  51  47  50  NaN
		25  n_4 50  53  52  52  1
		26  n_4 50  54  48  41  1
		27  n_4 50  51  49  50  NaN
		28  n_4 50  50  47  47  NaN
		29  n_4 50  50  51  47  NaN
		
}
"answer": {
	"desc": %s Personally I would use a multi index. from source this should work. The operative argument here is in which you tell the function what arguments are required for read. -- If your comes out malformed due to the source data we can fudge it by manually fixing it. which will yield the same as the above. 
	"code-snippets": [
		df = pd.read_csv('your_file.csv',headers=None)
		
		s = df.iloc[:2].T.replace('NaN',np.nan).ffill() # you may need to be smart with your replace here. 
		df.columns = pd.MultiIndex.from_frame(s)
		
		df1 = df.stack(0).reset_index(1).rename(columns={0 : 'own'}).iloc[2:]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 65035031
"link": https://stackoverflow.com/questions/65035031/filtering-dataframe-rows-which-have-overlapping-values-cross-columns
"question": {
	"title": Filtering DataFrame rows which have overlapping values cross-columns
	"desc": I have a dataframe that reflects rows with at least one conflict inside that row. Rows 0-3 and rows 4-5 have overlapping values with other rows, but the overlap occurs across various columns. How can I: drop all but the first row of each overlap group, in a table-wise or series-wise manner, ie without using down the rows This would be the output (though don't care about index): Below snippet for easy repro 
}
"io": {
	"Frame-1": 
		      email id1 id2 id3
		0      de@l  Z7  Q4  Q4
		1     sco@g  Q4  Z7  Q4
		2   alpha@n  Q4  Z7  Z7
		3   numer@o  Z7  Z7  Q4
		4    endo@c  D8  D8  L1
		5  chrono@k  L1  L1  D8
		
	"Frame-2":
		      email id1 id2 id3
		0      de@l  Z7  Q4  Q4
		4    endo@c  D8  D8  L1
		
}
"answer": {
	"desc": %s Idea is convert columns with values to hashable sets called frozensets, so possible filter by with inverted mask in : Alternative with list comprehension: 
	"code-snippets": [
		L = [frozenset(x) for x in df.filter(like='id').to_numpy()]
		df = df[~pd.Series(L, index=df.index).duplicated()]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 59340489
"link": https://stackoverflow.com/questions/59340489/splitting-dataframe-based-on-duplicate-values-into-multiple-csv-files
"question": {
	"title": Splitting Dataframe based on duplicate values into multiple csv files
	"desc": I have a dataset with multiple columns but only focusing on one column called 'VAL'. Every value in this column ranges from 0 to 4 so I would like to split this into 5 separate data frames based on those duplicate values and then export each of these data frames into individual csv files. I have been able to sort the numbers using pandas but now I need to divide up the values into smaller datasets keeping in mind that I have multiple files I would like to do this to so possibly a for loop? this is what I currently have as an output this is what I would like it to relatively look like 
}
"io": {
	"Frame-1": 
		 A       B      C      D      E      F      G         VAL   FILE
		954     380    158    166    431    201    769         0  001.csv
		1142    348    203    962      0    878   1023         0  001.csv
		1688    279    229      0    488   1007      0         0  001.csv
		4792    371    420     29    372      0    745         0  001.csv
		2106    352     76    196    388      0    695         0  001.csv
		    ...    ...    ...    ...    ...    ...       ...      ...
		5634    441    283    277    788     45    585         4  001.csv
		827     672    606     24   1023    463    742         4  001.csv
		6703    324    203      0    623    214    726         4  001.csv
		9056    604    398      0    981      0    633         4  001.csv
		0       574    338    144    942    608    793         4  001.csv
		
	"Frame-2":
		 A       B      C      D      E      F      G         VAL   FILE
		954     380    158    166    431    201    769         0  val_0.csv
		1142    348    203    962      0    878   1023         0  val_0.csv
		1688    279    229      0    488   1007      0         0  val_0.csv
		4792    371    420     29    372      0    745         0  val_0.csv
		2106    352     76    196    388      0    695         0  val_0.csv
		
		
		 A       B      C      D      E      F      G         VAL   FILE
		5634    441    283    277    788     45    585         4  val_4.csv
		827     672    606     24   1023    463    742         4  val_4.csv
		6703    324    203      0    623    214    726         4  val_4.csv
		9056    604    398      0    981      0    633         4  val_4.csv
		0       574    338    144    942    608    793         4  val_4.csv
		
		
}
"answer": {
	"desc": %s change your FILE to match your expected output. then groupby VAL and write your csv this writes two csv's for me from your data. 
	"code-snippets": [
		for group,data in df.groupby('VAL'):
		    data.to_csv(f"val_{group}.csv",index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64936694
"link": https://stackoverflow.com/questions/64936694/recovering-dataframe-multiindex-from-both-row-and-column-after-groupby
"question": {
	"title": Recovering DataFrame MultiIndex (from both row and column) after groupby
	"desc": I have a dataframe that is multi indexed in that manner. I want to apply a function to each of its columns, for each date (so the 89.583458 and 49.828466 go together, 9.328360 and 10.058943 go together, and so forth) This gives me But now I need to recover the lost indices (to get back the same structure as the original), but failed at setting , unstacking or using pd.MultiIndex.from_frame. Any idea? Perhaps there's a better to get exactly that from the call? 
}
"io": {
	"Frame-1": 
		                                  Value              Size
		                           A               B      Market Cap
		2019-07-01 AAPL         89.583458      9.328360  2.116356e+06
		           AMGN         49.828466     10.058943  1.395518e+05
		2019-10-01 AAPL         74.297570     11.237253  2.116356e+06
		           AMGN         56.841946     10.237481  1.395518e+05
		2019-12-31 AAPL         97.435257     14.736749  2.116356e+06
		           AMGN         71.400903     12.859612  1.395518e+05
		
	"Frame-2":
		                                              Market Cap  ...                             B
		2019-07-01  [[139551.76568603513], [139551.76568603513]]  ...  [[49.828465616227064], [49.828465616227064]]
		2019-10-01  [[139551.76568603513], [139551.76568603513]]  ...    [[56.84194615992103], [56.84194615992103]]
		2019-12-31  [[139551.76568603513], [139551.76568603513]]  ...    [[71.40090272484755], [71.40090272484755]]
		
}
"answer": {
	"desc": %s The problem is that returns a numpy array. So you're replacing a dataframe with a numpy array (wich is why you see in your output). Instead, you should replace the values of the dataframe. Here is an example: Output: 
	"code-snippets": [
		import pandas
		from scipy.stats.mstats import winsorize
		
		# Recreating your dataframe
		data = [
		    {"date": "2019-07-01", "group": "AAPL", "A": 89.583458, "B": 9.328360, "Market Cap": 2.116356e+06},
		    {"date": "2019-07-01", "group": "AMGN", "A": 49.828466, "B": 10.058943, "Market Cap": 1.395518e+05},
		    {"date": "2019-10-01", "group": "AAPL", "A": 74.297570, "B": 11.237253, "Market Cap": 2.116356e+06},
		    {"date": "2019-10-01", "group": "AMGN", "A": 56.841946, "B": 10.237481, "Market Cap": 1.395518e+05},
		    {"date": "2019-12-31", "group": "AAPL", "A": 97.435257, "B": 14.736749, "Market Cap": 2.116356e+06},
		    {"date": "2019-12-31", "group": "AMGN", "A": 71.400903, "B": 12.859612, "Market Cap": 1.395518e+05},
		]
		index = [
		    [pandas.to_datetime(line.get("date")) for line in data],
		    [line.get("group") for line in data],
		]
		columns = [
		    ["Value", "Value", "Size"],
		    ["A", "B", "Market Cap"]
		]
		df = pandas.DataFrame(data=[[line.get("A"), line.get("B"), line.get("Market Cap")] for line in data], index=index, columns=columns)
		
		
		# Your lambda function in a separate definition
		def process_group(group):
		
		    # Nested
		    def _sub(sub):
		        # winsorize returns an numpy array, sub is a dataframe; sub[:] replaces the "values" of the dataframe, not the dataframe itself
		        sub[:] = winsorize(a=sub, limits=[0.4, 0.6])  # I didn't know your limits so I've guessed...
		        return sub
		
		    # Return the result of the processing on the nested group
		    return group.groupby(level=1, axis=1).apply(_sub)
		
		# Process the groups
		df = df.groupby(level=0, axis=0).apply(process_group)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56437405
"link": https://stackoverflow.com/questions/56437405/explode-pandas-dataframe-singe-row-into-multiple-rows-across-multiple-columns-si
"question": {
	"title": Explode pandas dataframe singe row into multiple rows across multiple columns simultaneously
	"desc": I have a dataframe as I want to break each record in such a way that values in column and explode into multiple rows but such that the first value in after splitting upon corresponds to the first value in after splitting upon . So my should look like this: NOTE: this is not the same as Split (explode) pandas dataframe string entry to separate rows as here the exploding/splitting of one record is not just across one column but the need is to split or explode one row into multiple rows, in two columns simultaneously. Any help is appreciated. Thanks 
}
"io": {
	"Frame-1": 
		df
		       col1 act_id col2                                                                                                 
		   --------------------
		0  40;30;30   act1 A;B;C
		1  25;50;25   act2 D;E;F
		2     70;30   act3 G;H
		
	"Frame-2":
		desired_df
		       col1 act_id col2                                                                                                 
		       ---------------
		    0  40   act1   A
		    1  30   act1   B
		    2  30   act1   C
		    3  25   act2   D
		    4  50   act2   E
		    5  25   act2   F                                                                                                  
		    6  70   act3   G                                                                              
		    7  30   act3   H                                                                               
		
}
"answer": {
	"desc": %s one way to do this 
	"code-snippets": [
		df2.set_index('act_id').apply(lambda x: pd.Series(x.col1.split(';'),x.col2.split(';')), axis=1).stack().dropna().reset_index()
		
		df2.columns = ['col1','act_id','col2']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64972959
"link": https://stackoverflow.com/questions/64972959/choose-the-next-number-from-columns-in-pandas
"question": {
	"title": Choose the next number from columns in Pandas
	"desc":  I want to make a new column that shows the next biggest value after within the columns. Following is my intended result: I have been researching it a little but no luck. Some help will be appreciated, Thanks! 
}
"io": {
	"Frame-1": 
		ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5
		UN   22.85  22.86  8830500  0.21  25   34   12   87   105  102
		SS   55.01  52.67  6500     5.45  84   122  147  124  644  788   
		PN   90.00  90.99  1000     102   89   55   100  156  44   87     
		PI   184.99 182.38 15000    84    56   77   97   45   44   33    
		
	"Frame-2":
		ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5  X
		UN  22.85  22.86  8830500  0.21   25   34   12   87   105  102 25
		SS   55.01  52.67  6500     5.45  84   122  147  124  644  788 84  
		PN   90.00  90.99  1000     102   89   55   100  156  44   87 100   
		PI   184.99 182.38 15000    84    56   77   97   45   44   33 NaN  
		
}
"answer": {
	"desc": %s Depending on what do you mean by the next biggest. If you mean by the order from we can try : Output: If by next biggest, you mean in terms of the values, we can modify the above with : Then you pretty much have the same output (for this sample data), but of course with the said meaning. 
	"code-snippets": [
		# extract the `R` columns
		s = df.filter(like='R')
		
		# find out where these columns are larger than `Cl`:
		mask = s.gt(df['Cl'], axis='rows')
		
		# extract the values with `idxmax` and `lookup`:
		df['X'] = np.where(mask.any(1), s.lookup(s.index,mask.idxmax(1)), np.nan)
		
		----------------------------------------------------------------------
		# extract and sort by rows
		s = np.sort(df.filter(like='R').values, axis=1)
		
		# now we work with numpy data:
		mask = s > df['Cl'].values[:,None]
		
		# check and assign
		df['X'] = np.where(mask.any(1), s[np.arange(s.shape[0]),mask.argmax(1)], np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64969344
"link": https://stackoverflow.com/questions/64969344/merge-multiple-columns-into-one-by-placing-one-below-the-other-based-on-column-v
"question": {
	"title": Merge multiple columns into one by placing one below the other based on column value pandas dataframe
	"desc": I have the following dataframe df: Where the row is the row with the names of the columns in the dataframe (i.e. the row with bold font that states the names of each column). What I want is to rearrange this dataframe so that the output is this: I have tried searching different ways to append, concatenate, merge etc. the columns in the way that I want but I can't figure out how since there are multiple instances of each Video, i.e. multiple . So, for each of these multiple instances, I want to make one column of these with the Video number as the column name, and the rows be all the confidence values, as shown above. Is that possible? 
}
"io": {
	"Frame-1": 
		Video               1   1   1   1   1   1   1   1   1   1   ... 36  36  36  36  36  36  36  36  36  36
		Confidence Value    3   3   4   4   4   5   5   3   5   3   ... 3   3   3   2   4   2   3   3   3   3
		
	"Frame-2":
		Video 1 2 3 ... 36
		0     3 5 4 ... 3
		1     1 2 3 ... 2
		2     2 4 4 ... 5
		3     4 5 4 ... 3
		...
		
}
"answer": {
	"desc": %s A transpose-pivot construct may be what suits your need. Data Code This should work for indefinite number of confidence values per video: Note: If the number of confidence values per video are the same (5 in the example), then the step can be further simplified: Result 
	"code-snippets": [
		idx = df.transpose().groupby("Video").cumcount().values
		ans = df.transpose().set_index(idx).pivot(columns="Video", values="Confidence Value")
		
		----------------------------------------------------------------------
		ans = df.transpose().set_index(np.tile(range(5), 4)).pivot(columns="Video", values="Confidence Value")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64958453
"link": https://stackoverflow.com/questions/64958453/how-to-insert-values-of-a-dataframe-to-others-dataframe
"question": {
	"title": How to insert values of a dataframe to others dataframe
	"desc": This is the first dataframe: and in the other side I have other dataframes (CSV files) that have the same content. Here is two exemple: I want to add new column to each dataframe and to add each element of the first dataset to the others. Expected output: NB: I read CSV files as pandas so here the multiple dataframe are multiple csv files and I'm inserting new colum to each file and this column contain an element from the first dataframe.df1. This is my code it add the last element of df1 to all other df like that: 
}
"io": {
	"Frame-1": 
		
		df3=      df4=
		A B C     A B C 
		1 2 3     1 2 3
		4 5 6     4 5 6
		
	"Frame-2":
		df3=      df4=
		A B C D    A B C D
		1 2 3 a    1 2 3 b
		4 5 6 a    4 5 6 b
		
}
"answer": {
	"desc": %s You don't need to iterate through both and . That is where the problem originates. You are overriding each file multiple times, keeping only the last value from (where multiple is equal to ). Drop the outer loop and use to index into the instead. 
	"code-snippets": [
		list1=[]
		df=pd.read_csv('C:/dataframe_1.csv', sep=',')
		for elem in df['Name']:
		    list1.append(elem)
		
		os.chdir('C:/New')
		extension = 'csv'
		all_filenames = [i for i in glob.glob('*.{}'.format(extension))]
		
		for i, file in enumerate(all_filenames):
		    df1 = pd.read_csv(file, sep=',')
		    df1['new_column'] = list1[i]
		    
		df1.to_csv(file, index=False, na_rep='NaN')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64904669
"link": https://stackoverflow.com/questions/64904669/process-pandas-group-efficiently
"question": {
	"title": Process pandas group efficiently
	"desc": I have a dataframe df with columns a,b,c,d and e. What I want is, group by df on the basis of a,b and c. And tthen for each group I want to remove NULL value of column d and e with most frequent value of that column in that group. And then finally drop duplicates for each group. I am doing the following procesing: But the iteration is making my processing really very slow. Can someone suggest me better way to do it? Sample input: Sample output: 
}
"io": {
	"Frame-1": 
		a   b   c   d       e
		a1  b1  c1  NULL    e2
		a2  b2  c2  NULL    NULL
		a2  b2  c2  NULL    NULL
		a1  b1  c3  d4      e4
		a1  b1  c1  NULL    e2
		a1  b1  c1  d1      e2
		a1  b1  c1  d1     NULL
		
		
	"Frame-2":
		a   b   c   d         e
		a1  b1  c1  d1      e2
		a2  b2  c2  NULL    NULL
		a1  b1  c3  d4      e4
		
		
}
"answer": {
	"desc": %s You want with a catch when the data is all : Output: And if you want to fill your original dataframe with the mode: Output: 
	"code-snippets": [
		def get_mode(series):
		    out = series.mode()
		    return out.iloc[0] if len(out) else np.nan
		
		df.groupby(['a','b','c'], as_index=False, sort=False).agg(get_mode)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64880268
"link": https://stackoverflow.com/questions/64880268/mutate-several-columns-from-a-dataframe-based-on-a-mapping-between-values-from-a
"question": {
	"title": mutate several columns from a dataframe based on a mapping between values from another dataframe
	"desc": I have 2 dataframes: looks like this while looks like this what i want to obtain is the following so basically using the map between number and colours in rules to mutate df 
}
"io": {
	"Frame-1": 
		id   v1   v2   v3    v4   etc.
		1    1     4    2     5
		2    4     4    6     1
		3    2     1    3     4
		etc.
		
	"Frame-2":
		id        v1        v2        v3         v4   etc.
		1         red       green     blue       black
		2         green     green     gold       red
		3         blue      red       grey       green
		etc.
		
}
"answer": {
	"desc": %s You could do the following: With like and like the result is 
	"code-snippets": [
		results = df.applymap(lambda i: rules.loc[i, 'name'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64876318
"link": https://stackoverflow.com/questions/64876318/create-pd-dataframe-from-dictionary-with-multi-dimensional-array
"question": {
	"title": Create pd.DataFrame from dictionary with multi-dimensional array
	"desc": I've the following dictionary: I want to convert it to a , expecting this: How can I do that? I'm trying But it obviously doesn't work! 
}
"io": {
	"Frame-1": 
		dictA = {'A': [[1, 2, 3], [1, 2, 3], [1, 2, 3]],
		         'B': [[4, 4, 4], [4, 4, 4],],
		         'C': [[4, 6, 0]]
		        }
		
	"Frame-2":
		id       ColA        ColB        ColC
		0         1           4           4
		1         2           4           6
		2         3           4           0
		3         1           4           
		4         2           4
		5         3           4
		6         1
		7         2
		8         3
		
}
"answer": {
	"desc": %s Here is how: Output: Now, let's have a look at the problem one step at a time. The first thing we might try is simply: Which, of course, return this error: So now we need a way to be able to create dataframes from a with arrays of different lengths. For that, we can: Output: We want the dataframe to be vertical, so for each iteration, flatten out the lists with a list comprehension: Output: Now we want to replace all the s with blanks. For that, we need to , and do: Output: Finally use formatted string to convert the letters into letters: Output: 
	"code-snippets": [
		 ValueError: arrays must all be same length
		
		----------------------------------------------------------------------
		df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dictA.items()]))
		print(df)
		
		----------------------------------------------------------------------
		df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()]))
		print(df)
		
		----------------------------------------------------------------------
		df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()])).replace(np.nan, '')
		print(df)
		
		----------------------------------------------------------------------
		df = pd.DataFrame(dict([(f'Col{k}', pd.Series([a for b in v for a in b])) for k,v in dictA.items()])).replace(np.nan, '')
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64862482
"link": https://stackoverflow.com/questions/64862482/pandas-slice-dataframe-according-to-values-of-a-column
"question": {
	"title": Pandas: slice Dataframe according to values of a column
	"desc": I have to slice my Dataframe according to values (imported from a txt) that occur in one of my Dataframe' s column. This is what I have: This is what I need: drop rows whenever value in col2 is not among values in mytxt.txt Expected result must be: I tried: But it doesn' t work. Help would be very appreciated, thanks! 
}
"io": {
	"Frame-1": 
		>df
		col1 col2
		 a    1
		 b    2
		 c    3
		 d    4
		
		>'mytxt.txt'
		2
		3
		
	"Frame-2":
		>df
		col1 col2
		 b    2
		 c    3
		
}
"answer": {
	"desc": %s When you read , I would do it as a Series, and then convert it to a set, which will be more efficient for lookups: Then slicing will work: What was happening is you were reading in as a DataFrame rather than a Series, so the method was not behaving as you expected. 
	"code-snippets": [
		values = pd.read_csv('mytxt.txt', header=None, squeeze=True)
		values = set(values.tolist())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64828120
"link": https://stackoverflow.com/questions/64828120/pandas-fill-gaps-in-a-series-with-mean
"question": {
	"title": Pandas: Fill gaps in a series with mean
	"desc": Given df I want to replace the nans with the inbetween mean Expected output: I have seen this_answer but it's for a grouping which isn't my case and I couldn't find anything else. 
}
"io": {
	"Frame-1": 
		   distance
		0       0.0
		1       1.0
		2       2.0
		3       NaN
		4       3.0
		5       4.0
		6       5.0
		7       NaN
		8       NaN
		9       6.0
		
	"Frame-2":
		   distance
		0       0.0
		1       1.0
		2       2.0
		3       2.5
		4       3.0
		5       4.0
		6       5.0
		7       5.5
		8       5.5
		9       6.0
		
}
"answer": {
	"desc": %s If you don't want you can compute the mean of the surrounding values manually with and Out: 
	"code-snippets": [
		(df.ffill() + df.bfill()) / 2
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64826052
"link": https://stackoverflow.com/questions/64826052/how-to-reorder-rows-by-a-condition-in-pandas
"question": {
	"title": How to reorder rows by a condition in pandas?
	"desc": I have two dataframes and one of their orders is correct for me. I want to make the other's order the same as the correct one. Here is the point, it's not about index numbers, order depends on a variable. Like this df1 df2 I want the order of df2 to be same as df1, I put them in a for loop but it took long time (my real data is much greater than reproducible example) Is there any easier way to make my wish real ? Thanks in advice. 
}
"io": {
	"Frame-1": 
		A   B
		13  2
		20  5
		15  3
		.   .
		.   .
		
	"Frame-2":
		A   B
		15  3
		13  2
		20  5
		.   .
		.   .
		
}
"answer": {
	"desc": %s The best way I can think is to replace the index of by column and then call it with loc: output is: Is this ok for you? 
	"code-snippets": [
		df1 = pd.DataFrame({"A": [13,20,15], "B": [2,5,3]})
		df2 = pd.DataFrame({"A": [15,13,20], "B": [3,2,5]})
		df2 = df2.set_index("A")
		print(df2.loc[df1["A"]])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64789227
"link": https://stackoverflow.com/questions/64789227/how-to-replace-duplicate-dataframe-column-values-with-certain-conditions-in-pyth
"question": {
	"title": How to replace duplicate dataframe column values with certain conditions in python
	"desc": I have a dataframe with shape (10x401) having duplicate columns with same column names and values. Some of them have nulls while other have numeric values. The columns names are not in sorted order. A short example of dataframe is given below: By ignoring the null values, i need to replace each first occurrence of the numeric value (from 0 to 10) with 1 and the rest of the values with -1 for all 10 rows and 400 columns ignoring the ID column. The resulting dataframe will look like: I will be thankful for some help here. 
}
"io": {
	"Frame-1": 
		       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100
		   
		        1,         ,   ,   ,   ,  3,  3,  3,  3,   ,   ,   ,   ,.........,  0,    0,   0,   0   
		        2,        0,  0,  0,  0,   ,   ,   ,   , 10, 10, 10, 10,.........,   ,     ,    ,   
		        3,        9,  9,  9,  9,  1,  1,  1,  1,  4,  4,  4,  4,.........,  1,    1,   1,   1
		        .
		        .
		        .
		       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 6,    6,   6,   6
		
	"Frame-2":
		       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100
		   
		        1,         ,   ,   ,   ,  1, -1, -1, -1,   ,   ,   ,   ,.........,  1,   -1,   -1, -1   
		        2,        1, -1, -1, -1,   ,   ,   ,   ,  1, -1, -1, -1,.........,   ,     ,     ,   
		        3,        1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,.........,  1,   -1,   -1, -1
		        .
		        .
		        .
		       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 1,   -1,   -1, -1
		
}
"answer": {
	"desc": %s First, some example data: I recommend transposing the DataFrame, as it's more convenient to use the vectorized methods from pandas. Most of them can be used "horizontally" with specifying the . First you need to know all the cells where there are values: Secondly, you need to know all the starting positions of a new group. Shifting the whole DataFrame down by one row and checking for unequality helps. Combining that with your gives you the starting cells: Now you can set all the cells of value to and afterwards all the cells that are a the start of a group to Now you can always transpose it back: 
	"code-snippets": [
		import pandas as pd
		
		from io import StringIO
		
		df_string = '''
		ID;1;1;1;1;2;2;2;2;3;3;3;3
		1;;;;;3;3;3;3;;;;
		2;0;0;0;0;;;;;10;10;10;10
		3;9;9;9;9;1;1;1;1;4;4;4;4
		4;;;;;;;;;6;6;6;6
		'''
		
		df = pd.read_csv(StringIO(df_string), sep = ";", index_col="ID")
		
		# Removing the automatically added .1/.2/... suffixes. You don't need that for your data.
		df.columns = df.columns.str[0]
		
		----------------------------------------------------------------------
		StartMask = (df.shift() != df) & ValueMask
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64773001
"link": https://stackoverflow.com/questions/64773001/split-two-columns-in-a-pandas-dataframe-into-two-and-name-them
"question": {
	"title": Split two columns in a pandas dataframe into two and name them
	"desc": I have this pandas dataframe I would like to split the x and y columns and get an output with these given names on the columns. Is there a straight forward way to do this in python? 
}
"io": {
	"Frame-1": 
		         x           y       Values
		0       A B         C D       4.7
		1       A B         C D       10.9
		2       A B         C D       1.8
		3       A B         C D       6.5
		4       A B         C D       3.4
		
	"Frame-2":
		     x    f    y    g   Values
		0    A    B    C    D    4.7
		1    A    B    C    D    10.9
		2    A    B    C    D    1.8
		3    A    B    C    D    6.5
		4    A    B    C    D    3.4
		
}
"answer": {
	"desc": %s  You can make it scalable as well, defining a dict in which the keys are the columns, and the values a list with the desired new column names: 
	"code-snippets": [
		df[['x', 'f']] = df.x.str.split(" ", expand=True)
		df[['y', 'g']] = df.y.str.split(" ", expand=True)
		df[['x','f','y','g', 'Values']]
		
		----------------------------------------------------------------------
		# Define the target columns to split, and their new column names
		cols={
		    'x': ['x','f'],
		    'y': ['y','g']
		}
		# Apply the function to each target-column
		for k in cols:
		    df[cols[k]] = df[k].str.split(" ", expand=True)
		
		# Reorder the dataframe as you wish
		new_columns = sum(cols.values(),[])
		old_columns = set(df.columns) - set(new_columns)
		df[new_columns + list(old_columns)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64766331
"link": https://stackoverflow.com/questions/64766331/pandas-resample-column-based-on-other-column
"question": {
	"title": Pandas resample column based on other column
	"desc": I have a similar dataframe: And I want to resample this dataframe such that x values with the same y value is averaged. In other words: I've looked into the pandas.DataFrame.resample function, but not sure how to do this without timestamps. 
}
"io": {
	"Frame-1": 
		x | y
		1 | 1
		3 | 1
		3 | 1
		4 | 1
		5 | 2
		5 | 2
		9 | 2
		8 | 2
		
	"Frame-2":
		     x       |    y
		(1+3+3+4)/4  |    1
		(5+5+9+8)/4  |    2
		
}
"answer": {
	"desc": %s The following might return what you're looking for: 
	"code-snippets": [
		import pandas
		df = pandas.DataFrame({"x":[1,3,3,4,5,5,9,8],"y":[1,1,1,1,2,2,2,2]})
		df.groupby(["y"]).mean().reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64765350
"link": https://stackoverflow.com/questions/64765350/pandas-dataframe-select-list-value-from-another-column
"question": {
	"title": pandas dataframe select list value from another column
	"desc": Everyone! I have a pandas dataframe like this: as we can see, the A column is a list and the B column is an index value. I want to get a C column which is index by B from A: Is there any elegant method to solve this? Thank you! 
}
"io": {
	"Frame-1": 
		        A       B
		   0    [1,2,3] 0
		   1    [2,3,4] 1
		
	"Frame-2":
		        A       B     C
		   0    [1,2,3] 0     1
		   1    [2,3,4] 1     3
		
}
"answer": {
	"desc": %s Use list comprehension with indexing: Or , but it should be slowier if large DataFrame: 
	"code-snippets": [
		df['C'] = [x[y] for x, y in df[['A','B']].to_numpy()]
		
		----------------------------------------------------------------------
		df['C'] = df.apply(lambda x: x.A[x.B], axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64753531
"link": https://stackoverflow.com/questions/64753531/process-multiple-csv-files-on-pandas
"question": {
	"title": Process multiple csv files on pandas
	"desc": I have got three different .csv files which contain the grades for students in three different assignment. I would like to read them with pandas and calculate the average for each student. The template for each file is: For the second assignment: Desired output for the two doc for instance: the same for the last doc. I want to read these docs separately and for each student id to average the Mark. Now, my code for reading one file is the following: How can I process all three files simultaneously and extract the average grade? 
}
"io": {
	"Frame-1": 
		Student id, Mark, extra fields, ...
		4358975489, 9,  ... ...
		2345234523, 10,  ... ...
		7634565323, 7,  ... ...
		7653563366, 7,  ... ...
		...         ...,  ... ...
		
	"Frame-2":
		Student id, Mark, extra fields, ...
		4358975489, 6,  ... ...
		2345234523, 8,  ... ...
		7634565323, 4,  ... ...
		7653563366, 5,  ... ...
		...         ...,  ... ...
		
}
"answer": {
	"desc": %s If you want to process all together, you can do this: OR: If you want to do it simultaneously, you can use with and : Below is the of your : You can simultaneously find average of each student in a file and store the output in another : 
	"code-snippets": [
		df = df1.append([df2, df3]).groupby('Student id', as_index=False).mean()
		
		----------------------------------------------------------------------
		In [1223]: df = [i.groupby('Student_id', as_index=False).mean() for i in df_list]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64747375
"link": https://stackoverflow.com/questions/64747375/pandas-remove-characters-from-index
"question": {
	"title": Pandas remove characters from index
	"desc": I have the following dataframe: I want to remove the '-' character with the upper value in the index so I end up with the following dataframe: How do I do this? 
}
"io": {
	"Frame-1": 
		           A
		0-1.5      1
		1.5-3.3    2
		3.3-5.4    3
		5.4-7.9    4
		
	"Frame-2":
		     A
		0    1
		1.5  2
		3.3  3
		5.4  4
		
}
"answer": {
	"desc": %s You can use with seelct first lists by indexing: Or use with lambda function: 
	"code-snippets": [
		df = df.rename(lambda x: x.split('-')[0])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64722958
"link": https://stackoverflow.com/questions/64722958/concatenation-of-two-dataframe-after-onehotencoding
"question": {
	"title": Concatenation of two dataframe after onehotencoding
	"desc": Let us consider following code result of this code is following ( i am writing final dataframe) all others works fine, they are so my point is to remove commas in header part of the final dataframe, please help me 
}
"io": {
	"Frame-1": 
		 Alphabet  (A,)  (B,)  (C,)
		0        A   1.0   0.0   0.0
		1        B   0.0   1.0   0.0
		2        C   0.0   0.0   1.0
		3        A   1.0   0.0   0.0
		4        B   0.0   1.0   0.0
		
	"Frame-2":
		   A    B    C
		0  1.0  0.0  0.0
		1  0.0  1.0  0.0
		2  0.0  0.0  1.0
		3  1.0  0.0  0.0
		4  0.0  1.0  0.0
		
}
"answer": {
	"desc": %s Try adding: Your columns are a multi-index in . If you write: , you will get and see what I mean, so you need to change the column names with what I have suggested, by joining the multiple levels with an empty string, so that you have one string as a column name instead of a tuple. Also, you could use as an alternative to : Full Code: 
	"code-snippets": [
		Encoded_Dataframe.columns = [''.join(col) for col in Encoded_Dataframe.columns]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64715870
"link": https://stackoverflow.com/questions/64715870/max-for-dataframe-converts-object-type-to-float64
"question": {
	"title": .max() for dataframe converts object type to float64
	"desc": Have 3 columns namely Whenever I do , it ends up giving a float value. Something like Output: I wanted Tried using but no luck. 
}
"io": {
	"Frame-1": 
		   "A",         "B"
		 -7.480000e+01,-1.480000e+01
		-7.410000e+01,-1.410000e+01
		-7.370000e+01,-1.370000e+01
		-7.360000e+01,-1.360000e+01
		-7.370000e+01,-1.370000e+01
		-7.390000e+01,-1.390000e+01
		
	"Frame-2":
		       "C"     
		  -7.480000e+01,
		    -7.410000e+01
		    -7.370000e+01,
		    -7.360000e+01,
		    -7.370000e+01
		    -7.390000e+01
		
}
"answer": {
	"desc": %s  If values of 'A' and 'B' are strings: 
	"code-snippets": [
		df.apply(lambda x: '{:e}'.format(max(x['A'], x['B'])), axis=1)
		
		----------------------------------------------------------------------
		df.apply(lambda x: '{:e}'.format(max(float(x['A']), float(x['B']))), axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50367656
"link": https://stackoverflow.com/questions/50367656/python-pandas-pandas-to-datetime-is-switching-day-month-when-day-is-less-t
"question": {
	"title": Python Pandas : pandas.to_datetime() is switching day &amp; month when day is less than 13
	"desc": I wrote a code that reads multiple files, however on some of my files datetime swaps day & month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). I tried to fix it by doing this,but it doesn't work. My data frame looks like this: The actual datetime is from 12june2015 to 13june2015 when my I read my datetime column as a string the dates remain correct dd/mm/yyyy but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13. output: Here is my code : I loop through files : then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type by pd_datetime() it swaps the day and month when the day is less than 13. Post converting my datetime column the dates are correct (string type) But when I change the column type I get this: The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13? UPDATE This command swaps all the days and months of my column So in order to swap only the incorrect dates, I wrote a condition: But it doesn't work either 
}
"io": {
	"Frame-1": 
		tmp                     p1 p2 
		11/06/2015 00:56:55.060  0  1
		11/06/2015 04:16:38.060  0  1
		12/06/2015 16:13:30.060  0  1
		12/06/2015 21:24:03.060  0  1
		13/06/2015 02:31:44.060  0  1
		13/06/2015 02:37:49.060  0  1
		
	"Frame-2":
		print(df)
		tmp                  p1 p2 
		06/11/2015 00:56:55  0  1
		06/11/2015 04:16:38  0  1
		06/12/2015 16:13:30  0  1
		06/12/2015 21:24:03  0  1
		13/06/2015 02:31:44  0  1
		13/06/2015 02:37:49  0  1
		
}
"answer": {
	"desc": %s Well I solved my problem but in a memory consuming method, I split my tmp column first to a date and time columns then I re-split my date column to day month and year, that way I could look for the days that are less than 13 and replace them with the correspondent month # convert series to string type in order to merge them # merge time and date and place result in our column # drop the added columns 
	"code-snippets": [
		df['tmp'] = pd.to_datetime(df['tmp'], unit='ns')
		df['tmp'] = df['tmp'].apply(lambda x: x.replace(microsecond=0))
		df['date'] = [d.date() for d in df['tmp']]
		df['time'] = [d.time() for d in df['tmp']]
		df[['year','month','day']] = df['date'].apply(lambda x: pd.Series(x.strftime("%Y-%m-%d").split("-")))
		
		df['day'] = pd.to_numeric(df['day'], errors='coerce')
		df['month'] = pd.to_numeric(df['month'], errors='coerce')
		df['year'] = pd.to_numeric(df['year'], errors='coerce')
		
		
		#Loop to look for days less than 13 and then swap the day and month
		for index, d in enumerate(df['day']):
		        if(d <13): 
		 df.loc[index,'day'],df.loc[index,'month']=df.loc[index,'month'],df.loc[index,'day'] 
		
		----------------------------------------------------------------------
		 df['day'] = df['day'].astype(str)
		 df['month'] = df['month'].astype(str)
		 df['year'] = df['year'].astype(str)
		 df['date']=  pd.to_datetime(df[['year', 'month', 'day']])
		 df['date'] = df['date'].astype(str)
		 df['time'] = df['time'].astype(str)
		
		----------------------------------------------------------------------
		df.drop(df[['date','year', 'month', 'day','time']], axis=1, inplace = True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64663456
"link": https://stackoverflow.com/questions/64663456/generate-unique-key-from-multiple-dataframes-based-on-name
"question": {
	"title": Generate unique key from multiple dataframes based on name
	"desc": I have two data frames. As you can see, the function merges it correctly, but it is wrong. Because the carid must be unique and must not be assigned twice. How can I solve this problem? It can appear several times in a data frame, but it must remain unique over two data records. So across all data records and not What I want 
}
"io": {
	"Frame-1": 
		Carid = 1 = Mercedes-benz
	"Frame-2":
		Cardid = 1 = Mercedes-Benz & Citroen
}
"answer": {
	"desc": %s Method 1 Pandas Approach First method if you don't mind changing your keys to floats is to increment using Method 2 Functional Approach using Dictionaries. Assumptions. The logic of the function is predicated upon having a a unique per dataframe. Your IDs are in a sequential order so using the to generate the numbers makes the most sense. This may generate non sequential numbers if you have a list of Carids this would generate a new unique of for Citroen given that an ID of already exists and is owned by a car make. Function In Action Testing additional dataframe. 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		from collections import ChainMap
		
		
		def generate_new_keys(*args,key='Carid',name='Carname'):
		    """
		    Takes in a number of dataframes and returns any duplicates with a new unique id. 
		    groupby columns fixed to CarID and CarName.
		    """
		    # adds dictionaries into a single list.
		    dicts_ = [arg.groupby(key)[name].first().to_dict() for arg in args]
		    #merges dicts on unique key, this will exclude duplicates.
		    merged_dicts = dict(ChainMap(*dicts_))
		    #get the duplicate and pass the name into a list.
		    delta = [v for each_dict in dicts_ for k,v in each_dict.items() if v not in merged_dicts.values()]
		    # get the max sequence key
		    start_key =  max(merged_dicts.keys()) + 1
		    # create a new sequence
		    sequence = range(start_key, start_key + len(delta) + 1)
		    # return a dictionary.
		    return {name : number for name,number in zip(delta,sequence)}
		    
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64657045
"link": https://stackoverflow.com/questions/64657045/how-to-merge-multiple-columns-containing-numeric-data-in-pandas-but-ignore-empt
"question": {
	"title": How to merge multiple columns containing numeric data in Pandas, but ignore empty cells
	"desc": I have a table like this: where each column in the desired range has only one integer in its row. I want to merge these columns into a single new column that would look like this: I have been searching, but the closest solution I can find is doing something like: However, this also concatenates "NaN"s from the blank cells, which is obviously undesirable. How might I get my desired output? 
}
"io": {
	"Frame-1": 
		|-----|-----|-----|
		|  A  |  B  |  C  |
		|-----|-----|-----|
		|     |  5  |     |
		|-----|-----|-----|
		|  1  |     |     |
		|-----|-----|-----|
		|     |  5  |     |
		|-----|-----|-----|
		|     |     |  2  |
		|-----|-----|-----|
		|     |     |  2  |
		|-----|-----|-----|
		
	"Frame-2":
		|-----|-----|-----|    |-----|
		|  A  |  B  |  C  |    |  Z  |
		|-----|-----|-----|    |-----|
		|     |  5  |     | →  |  5  |
		|-----|-----|-----|    |-----|
		|  1  |     |     | →  |  1  |
		|-----|-----|-----|    |-----|
		|     |  5  |     | →  |  5  |
		|-----|-----|-----|    |-----|
		|     |     |  2  | →  |  2  |
		|-----|-----|-----|    |-----|
		|     |     |  2  | →  |  2  |
		|-----|-----|-----|    |-----|
		
}
"answer": {
	"desc": %s I think it is what you want. Alternatively, you can use Which might be safer if (per chance) you have multiple non-NULL values and just want one of them (the largest one in this case). 
	"code-snippets": [
		df['Z'] = df.max(axis = 1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64651683
"link": https://stackoverflow.com/questions/64651683/create-dataframe-column-that-increases-count-based-on-another-columns-value
"question": {
	"title": Create Dataframe column that increases count based on another columns value
	"desc": I need to add a column to my dataframe that will add a number each time a value in another column surpasses a limit. Example below: Original DF: Desired DF: Where value in ColB surpasses 10, ColC count = count +1 thank you for the help! 
}
"io": {
	"Frame-1": 
		ColA   ColB
		 4      1.4
		 10     0.5
		 1      2.3
		 3      12.2
		 8.8    8.5
		 2      5.2
		 0.6    0.33
		 9      3
		 4      144
		 33     8
		
	"Frame-2":
		ColA   ColB     ColC
		 4      1.4       1
		 10     0.5       1
		 1      2.3       1
		 3      12.2      2
		 8.8    8.5       2
		 2      5.2       2
		 0.6    0.33      2
		 9      3         2
		 4      144       3
		 33     8         3
		
}
"answer": {
	"desc": %s  Prints: 
	"code-snippets": [
		df['ColC'] = (df['ColB'] > 10).cumsum() + 1
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64646490
"link": https://stackoverflow.com/questions/64646490/calculate-similarity-between-rows-of-a-dataframe-count-values-in-common
"question": {
	"title": Calculate similarity between rows of a dataframe (count values in common)
	"desc": I want to calculate similarity between the rows of my dataframe. I have some columns with informations about some people. One row is one person. It looks like that : I want to count for each row the number of values in common with the other rows divided by the number of columns if at least 3 columns are completed. For example, between the row with the index 1 and the row with the index 2, there are 4 variables in common. So, my similarity will be 4/5 (id doesn't count) = 80% of similarity. My result has to be a similarity matrix, because after that I want to find the rows with a similarity higher than 0.6 to build a new dataframe. It could be something like that : Because the results are duplicated, half of that would be enough : I'm looking for a function that will automate that but I couldn't find. Does something like that exist? Thanks for reading, any advice or idea will be welcomed. 
}
"io": {
	"Frame-1": 
		 print(similarity)
		        0    1    2    3    4
		    0   1    0    0    0    0.2
		    1   0.2  1    0.8  0.2  0
		    2   0    0.8  1    0.2  0
		    3   0    0.2  0.2  1    0
		    4   0.2  0    0    0    1
		
	"Frame-2":
		 print(similarity)
		        0    1    2    3    4
		    0        0    0    0    0.2
		    1             0.8  0.2  0
		    2                  0.2  0
		    3                       0
		    4 
		
}
"answer": {
	"desc": %s You can use with a custom distance function Out: 
	"code-snippets": [
		from scipy.spatial.distance import pdist, squareform
		pd.DataFrame(1 - squareform(pdist(df.set_index('id'), lambda u,v: (u != v).mean())))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64640182
"link": https://stackoverflow.com/questions/64640182/how-to-modify-numercial-values-in-a-column-of-mixed-data-types-in-a-pandas-dataf
"question": {
	"title": How to modify numercial values in a column of mixed data types in a pandas dataframe?
	"desc": I have a pandas dataframe in pyhton that looks like this (my actual dataframe is MUCH bigger than this): How can I perform some operations on the numerical values of specific columns. For example, multiply the numerical values of col_2 by 10 to get something like this: Although it looks like a simple task I couldn't find a solution for it anywhere on internet. Thanks in advance. 
}
"io": {
	"Frame-1": 
		  col_1 col_2
		0   0.8   0.1
		1  nope   0.6
		2   0.4   0.7
		3  nope  nope
		
	"Frame-2":
		  col_1 col_2
		0   0.8   1
		1  nope   6
		2   0.4   7
		3  nope  nope
		
}
"answer": {
	"desc": %s First you need to convert the type columns to columns by using : converts all non-numeric type values in the column to . Then, multiply by 10: if you want to convert back to , you can use : 
	"code-snippets": [
		In [141]: df.col_2 = pd.to_numeric(df.col_2, errors='coerce')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64625342
"link": https://stackoverflow.com/questions/64625342/subtract-from-every-value-in-a-dataframe
"question": {
	"title": Subtract from every value in a DataFrame
	"desc": I have a dataframe that looks like this: I want to subtract the movie scores from each movie so the output would look like this: The actual dataframe has thousands of movies and the movies are referenced by name so im trying to find a solution to comply with that. I should have also mention that the movies are not listed in order like ["movie1", "movie2", "movie3"], they are listed by their titles instead like ["Star Wars", "Harry Potter", "Lord of the Rings"]. The dataset could be changed so I wont know what the last movie in the list is. 
}
"io": {
	"Frame-1": 
		userId   movie1   movie2   movie3   movie4   score
		0        4.1      2.1      1.0      NaN      2
		1        3.1      1.1      3.4      1.4      1
		2        2.8      NaN      1.7      NaN      3
		3        NaN      5.0      NaN      2.3      4
		4        NaN      NaN      NaN      NaN      1
		5        2.3      NaN      2.0      4.0      1
		
	"Frame-2":
		userId   movie1   movie2   movie3   movie4   score
		0        2.1      0.1     -1.0      NaN      2
		1        2.1      0.1      2.4      0.4      1
		2       -0.2      NaN     -2.3      NaN      3
		3        NaN      1.0      NaN     -1.7      4
		4        NaN      NaN      NaN      NaN      1
		5        1.3      NaN      1.0      3.0      1
		
}
"answer": {
	"desc": %s Use to identify the columns and then these columns from array: EDIT: When the movie column names are random. Select all columns except : 
	"code-snippets": [
		x = df.columns[~df.columns.isin(['userId', 'score'])]
		df[x] = df[x] - df.score.values[:, None]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64599227
"link": https://stackoverflow.com/questions/64599227/is-there-a-way-to-combine-9-12-or-15-columns-from-a-single-df-into-just-3
"question": {
	"title": Is there a way to combine 9,12 or 15 columns from a single df into just 3?
	"desc": I'm trying to convert a df that has the data divided every 3 columns into just three. An example is from this: To this: 
}
"io": {
	"Frame-1": 
		C1 C2 C3 C4 C5 C6  C7  C8  C9 
		1  6   9  A  D  G  1A  6A  9A
		2  7  10  B  E  H  2A  7A  10A
		3  8  11  C  F  I  3A  8A  11A
		
	"Frame-2":
		C1 C2 C3
		1  6   9
		2  7  10
		3  8  11
		C4 C5 C6
		A  D  G
		B  E  H
		C  F  I
		C7 C8 C9
		1A 6A 9A
		2A 7A 10A
		3A 8A 11A
		
}
"answer": {
	"desc": %s You can use : Output: 
	"code-snippets": [
		arr = np.hsplit(np.vstack([df.columns.values, df.values]), 3)
		pd.DataFrame(np.vstack(arr))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64592950
"link": https://stackoverflow.com/questions/64592950/pandas-shifting-a-rolling-sum-after-grouping-spills-over-to-following-groups
"question": {
	"title": Pandas - shifting a rolling sum after grouping spills over to following groups
	"desc": I might be doing something wrong, but I was trying to calculate a rolling average (let's use sum instead in this example for simplicity) after grouping the dataframe. Until here it all works well, but when I apply a shift I'm finding the values spill over to the group below. See example below: Expected result: Result I actually get: You can see the result of A2 gets passed to B3 and the result of B5 to C6. I'm not sure this is the intended behaviour and I'm doing something wrong or there is some bug in pandas? Thanks 
}
"io": {
	"Frame-1": 
		X   
		A  0    NaN
		   1    NaN
		   2    3.0
		B  3    NaN
		   4    NaN
		   5    3.0
		C  6    NaN
		   7    NaN
		   8    3.0
		
	"Frame-2":
		X   
		A  0    NaN
		   1    NaN
		   2    3.0
		B  3    5.0
		   4    NaN
		   5    3.0
		C  6    5.0
		   7    NaN
		   8    3.0
		
}
"answer": {
	"desc": %s The problem is that returns a new series, then when you chain with , you shift the series as a whole, not within the group. You need another to shift within the group: Or use : Output: 
	"code-snippets": [
		grouped_df = (df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum()
		                .groupby(level=0).shift(periods=1)
		             )
		
		----------------------------------------------------------------------
		grouped_df = (df.groupby('X')['Y']
		                .transform(lambda x: x.rolling(window=2, min_periods=2)
		                                      .sum().shift(periods=1))
		             )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64583504
"link": https://stackoverflow.com/questions/64583504/selectively-adding-values-of-columns-in-a-dataframe
"question": {
	"title": Selectively adding values of columns in a dataframe
	"desc": I have a pandas data frame like this I want to add all the values in the given columns like this: So basically I want to check if the column names fall in a five year range of the corresponding values in the column 'YEAR_OPENED' and create a new column with the sum of all the values. How should I proceed? 
}
"io": {
	"Frame-1": 
		YEAR_OPENED  2000 2001 2002 2003 2004 2005 2006 2007 2008 2009
		 1999          1    0    0   0    1    0     0   0     1    0
		 2000          1    1    2   0    3    0     0   0     0    0
		 2001          0    0    0   4    0    0     0   0     0    0
		
	"Frame-2":
		YEAR_OPENED   CLOSED_IN_5_YEARS
		 1999               2
		 2000               7
		 2001               4
		
}
"answer": {
	"desc": %s  Prints: 
	"code-snippets": [
		df['CLOSED_IN_5_YEARS'] = df.set_index('YEAR_OPENED').apply(
		        lambda x: sum(i for i, c in zip(x, x.index) if x.name <= int(c) <= x.name + 5), axis=1
		    ).values
		
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64577375
"link": https://stackoverflow.com/questions/64577375/merge-pandas-dataframes-under-new-index-level
"question": {
	"title": merge pandas dataframes under new index level
	"desc": I have 2 s and that I want to combine into a single dataframe : Dataframe should contain one more column index level than and , and contain each under its own level-0 identifier, like so: Any ideas as to how to do this? It's a bit like ing the two frames: ...but using an additional level, instead of a suffix, to prevent name collisions. I tried: I could use loops to build up the series-by-series, but that doesn't seem right. Many thanks. 
}
"io": {
	"Frame-1": 
		act #have
		
		          a         b
		0  0.853910  0.405463
		1  0.822641  0.255832
		2  0.673718  0.313768
		
		exp #have
		
		          a         c
		0  0.464781  0.325553
		1  0.565531  0.269678
		2  0.363693  0.775927
		
	"Frame-2":
		df  #want
		
		        act                 exp          
		          a         b         a         c
		0  0.853910  0.405463  0.464781  0.325553
		1  0.822641  0.255832  0.565531  0.269678
		2  0.673718  0.313768  0.363693  0.775927
		
}
"answer": {
	"desc": %s May be you can try using : Result: 
	"code-snippets": [
		pd.concat([act, exp], axis=1, keys=['act', 'exp'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64526205
"link": https://stackoverflow.com/questions/64526205/pandas-loop-to-numpy-numpy-count-occurrences-of-string-as-nonzero-in-array
"question": {
	"title": Pandas loop to numpy . Numpy count occurrences of string as nonzero in array
	"desc": Suppose I have the following dataframe with element types in brackets When using pandas loops I use the following code. If : I am trying to use NumPy and array methods for efficiency. I have tried translating the method but no luck. Expected output 
}
"io": {
	"Frame-1": 
		  Column1(int) Column2(str)  Column3(str)
		0     2             02            34
		1     2             34            02
		2     2             80            85
		3     2             91            09
		4     2             09            34
		
	"Frame-2":
		  Column1(int) Column2(str)  Column3(str)  Column4(int)
		0     2             02            34           1
		1     2             34            02           2
		2     2             80            85           0
		3     2             91            09           0
		4     2             09            34           1
		
}
"answer": {
	"desc": %s You can use on and use it as mapping for , you can pass object to , missing values with with Or you can use here. 
	"code-snippets": [
		s = df['Column2'].map(df['Column3'].value_counts()).fillna(0)
		df['Column4'] = np.where(df['Column1'].eq(2), s, 'F')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64524795
"link": https://stackoverflow.com/questions/64524795/how-to-create-a-new-column-based-on-matching-ids-and-strings-in-names-of-other
"question": {
	"title": How to create a new column based on matching ID&#39;s and string&#39;s in names of other columns in the same data frame?
	"desc": I have tried to find a solution online but I cannot. I have a dataframe with 10 separate id columns, and 10 separate corresponding value columns for each ID. A brief example is shown below Example: I want to create a new column that takes the value column from the corresponding match of ID's between the 'shooter_id' and any of the 'player_id' columns like below: I have really been struggling to make this work, I am not sure if I need to merge within itself, for loop through the dataframe, or .apply.. any insight would be very helpful! Thank you! 
}
"io": {
	"Frame-1": 
		player_id_1    player_1_x   player_id_2   player_2_x  shooter_id 
		
		300               10           301           12           301
		
		299               11           300           13           299
		
		
	"Frame-2":
		player_id_1    player_1_x   player_id_2   player_2_x  shooter_id  shooter_x
		
		300               10           301           12           301         12 
		
		299               11           300           13           299         11
		
		
}
"answer": {
	"desc": %s Let's the like columns, the use + to get the columns where the match is found, finally use to get values corresponding to : 
	"code-snippets": [
		c = df.filter(like='player_id')\
		      .eq(df['shooter_id'], axis=0)\
		      .idxmax(1).str.replace('_id', '').add('_x')
		
		df['shooter_x'] = df.lookup(df.index, c)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62778713
"link": https://stackoverflow.com/questions/62778713/drop-rows-and-sort-one-dataframe-according-to-another
"question": {
	"title": Drop rows and sort one dataframe according to another
	"desc": I have two pandas data frames ( and ): My goal is to append the corresponding from to each in . However, the relationship is not one-to-one (this is my client's fault and there's nothing I can do about this). To solve this problem, I want to sort by such that is identical to . So basically, for any row in 0 to : if then keep row in . if then drop row from and repeat. The desired result is: This way, I can use to assign to . Is there a standardized way to do this? Does have a method for doing this? Before this gets voted as a duplicate, please realize that , so threads like this are not quite what I'm looking for. 
}
"io": {
	"Frame-1": 
		# df1
		  ID  COL
		   1    A
		   2    F
		   2    A
		   3    A
		   3    S
		   3    D
		   4    D
		
		# df2
		  ID  VAL
		   1    1
		   2    0
		   3    0
		   3    1
		   4    0
		
	"Frame-2":
		  ID  COL
		   1    A
		   2    F
		   3    A
		   3    S
		   4    D
		
}
"answer": {
	"desc": %s This can be done with merge on both and the order within each : Output: 
	"code-snippets": [
		(df1.assign(idx=df1.groupby('ID').cumcount())
		    .merge(df2.assign(idx=df2.groupby('ID').cumcount()),
		           on=['ID','idx'],
		           suffixes=['','_drop'])
		    [df1.columns]
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62006098
"link": https://stackoverflow.com/questions/62006098/checking-for-nans-in-many-columns-in-pandas
"question": {
	"title": Checking for NaNs in many columns in Pandas
	"desc": I want to add a binary column to my dataframe based on whether given columns contain NaN or not. I have tried to do it with the below code. but I got a ValueError at the line before last. Sample input: Expected output: I want to check NaNs only for A, B, C columns. 
}
"io": {
	"Frame-1": 
		A     B     C     D
		10   NaN    40    NaN
		NaN  NaN    80    90
		20    45    NaN   89
		NaN  NaN    NaN   46
		
	"Frame-2":
		A     B     C     D     E
		10   NaN    40    NaN   0
		NaN  NaN    80    90    0
		20    45    NaN   89    0
		NaN  NaN    NaN   46    1
		
}
"answer": {
	"desc": %s You want to check whether a row with columns() has all or not. You can do this using : Performance comparison: Quang Hoang's answer: YOBEN_S's answer: anky's answer: My answer: As you can see, my answer with is the fastest. 
	"code-snippets": [
		In [1720]: %timeit df['ismissing'] = df[['A','B','C']].isna().all(axis=1)
		989 µs ± 70 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
		In [1719]: %timeit df['New']=~df.index.isin(df.drop('D',1).dropna(thresh=1).index)
		2.05 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
		In [1724]: %timeit df['all_nan'] = df[['A','B','C']].count(axis=1).eq(0).view('i1')
		1.48 ms ± 117 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
		In [1723]: %timeit dat['E'] = np.where(dat[['A','B','C']].isnull().all(1), 1, 0)
		914 µs ± 18.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64464184
"link": https://stackoverflow.com/questions/64464184/select-highest-member-of-close-coordinates-saved-in-pandas-dataframe
"question": {
	"title": Select highest member of close coordinates saved in pandas dataframe
	"desc": I have a dataframe that has following columns: X and Y are Cartesian coordinates and Value is the value of element at these coordinates. What I want to achieve is to select only one coordinates out of that are close to other, lets say coordinates are close if distance is lower than some value , so the initial DF looks like this (example): distance is count with following function: lets say if we want to , the output dataframe would look like this: What is to be done: So I need to go through dataframe row by row, check the rest, select best match and then continue. I can't think about any simple method how to achieve this, this cant be use case of , since they are not duplicates, but looping over the whole DF will be very inefficient. One method I could think about was to loop just once, for each of rows finds close ones (probably apply countdistance()), select the best fitting row and replace rest with its values, in the end use . The other idea was to create a recursive function that would create a new DF, then while original df will have rows select first, find close ones, best match append to new DF, remove first row and all close from original DF and continue until empty, then return same function with new DF as to remove possible uncaught close points. These ideas are all kind of inefficient, is there a nice and efficient pythonic way to achieve this? 
}
"io": {
	"Frame-1": 
		    X  Y  Value
		0   0  0      6
		1   0  1      7
		2   0  4      4
		3   1  2      5
		4   1  6      6
		5   5  5      5
		6   6  6      6
		7   7  4      4
		8   8  8      8
		
	"Frame-2":
		    X  Y  Value
		1   0  1      7
		4   1  6      6
		8   8  8      8
		
}
"answer": {
	"desc": %s For now, I have created simple code with recursion, the code works but is most likely not optimal. 
	"code-snippets": [
		def recModif(self,df):
		  #columns=['','X','Y','Value']
		  new_df = df.copy()
		  new_df = new_df[new_df['Value']<0] #create copy to work with
		  changed = False
		  while not df.empty: #for all the data
		    df = df.reset_index(drop=True) #need to reset so 0 is always accessible
		    x = df.loc[0,'X'] #first row x and y
		    y = df.loc[0,'Y']
		    df['dist'] = self.countDistance(x,y,df['X'],df['Y']) #add column with distances
		    select = df[df['dist']<10] #number of meters that two elements cant be next to other 
		    if(len(select.index)>1): #if there is more than one elem close
		      changed = True
		      #print(select,select['Value'].idxmax())
		    select = select.loc[[select['Value'].idxmax()]] #get the highest one
		    new_df = new_df.append(pd.DataFrame(select.iloc[:,:3]),ignore_index=True) #add it to new df
		    df = df[df['dist'] >= 10] #drop the elements now
		
		  if changed:
		    return self.recModif(new_df) #use recursion if possible overlaps
		  else: 
		    return new_df #return new df if all was OK
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64449526
"link": https://stackoverflow.com/questions/64449526/convert-dictionary-of-dictionaries-to-dataframe-with-data-types
"question": {
	"title": Convert dictionary of dictionaries to dataframe with data types
	"desc": What is the preferred way to convert dictionary of dictionaries into a data frame with data types? I have the following kind of dictionary which contains fact sets behind each key Converting this dictionary of dictionaries into a dataframe can be done in a quite straightforward way which yields the following version on the original dictionary of dictionaries and the following datatypes for columns However, I would like to have transposed version on . After doing so it seems like the expected representation on the data is shown in matrix form but the data types are all What is the preferred way to do such conversion from to so that would yield directly data types similar to converting to ? 
}
"io": {
	"Frame-1": 
		     1    2    3
		a    1  NaN  NaN
		b    2    1  NaN
		c    b    e  NaN
		d  NaN    1  NaN
		e  NaN  NaN  0.0
		
	"Frame-2":
		     a    b    c    d    e
		1    1    2    b  NaN  NaN
		2  NaN    1    e    1  NaN
		3  NaN  NaN  NaN  NaN    0
		
}
"answer": {
	"desc": %s Just set the right orientation (default is , you want ). 
	"code-snippets": [
		a    float64
		b    float64
		c     object
		d    float64
		e    float64
		dtype: object
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64444765
"link": https://stackoverflow.com/questions/64444765/how-to-find-the-average-of-each-cell-in-multiple-csvs
"question": {
	"title": How to find the average of each cell in multiple csv&#39;s
	"desc": I have several excel files with data in them in a format similar to this There are 3 csv's total, and I need to create a new csv with the average of each cell. So would be as follows So far I have the files imported but I am not sure how to proceed. 
}
"io": {
	"Frame-1": 
		csv1             csv1
		  a b c           a b c
		x 1 2 3         x 3 2 1
		y 4 5 6         y 6 5 4
		
	"Frame-2":
		  a       b        c
		x (3+1)/2) (2+2)/2  (3+1)/2
		y (6+4)/2  etc.
		
}
"answer": {
	"desc": %s Since you tagged , I'm assuming a numpy solution would work. 
	"code-snippets": [
		import numpy as np
		csv1 = np.genfromtxt('my_file1.csv', delimiter=',')
		csv2 = np.genfromtxt('my_file2.csv', delimiter=',')
		np.savetxt("foo.csv", (csv1+csv2)/2, delimiter=",")    
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64446862
"link": https://stackoverflow.com/questions/64446862/how-to-add-to-dataframe-column-a-dict
"question": {
	"title": How to add to dataframe column a dict?
	"desc": Input dataframe: Following dataframe want as output: It is giving wrong output! 
}
"io": {
	"Frame-1": 
		          Id               Score            Score1
		0        19138359    0.5347029367015973   0.832428474443
		1        12134001    0.9347094453553113   0.632535428479
		
	"Frame-2":
		          Id                             Scores
		0        19138359  {'Score': 0.5347029367015973, 'Score1': 0.832428474443}
		1        12134001  {'Score': 0.9347094453553113, 'Score1': 0.632535428479}
		
}
"answer": {
	"desc": %s Use argument in : [out] 
	"code-snippets": [
		# Get score columns
		score_columns = df.filter(like='Score').columns
		
		# Create dict of scores column
		df['Scores'] = df[score_columns].to_dict(orient='records')
		
		# Drop original score columns
		df.drop(columns=score_columns, inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64445512
"link": https://stackoverflow.com/questions/64445512/how-to-get-the-n-most-frequent-or-top-values-per-column-in-python-pandas
"question": {
	"title": How to get the n most frequent or top values per column in python pandas?
	"desc": my dataframe looks like: for top 2 most frequent values per column (n=2), the output should be: Thank you 
}
"io": {
	"Frame-1": 
		df:
		    A   B
		0   a   g
		1   f   g
		2   a   g
		3   a   d
		4   h   d
		5   f   a
		
		
	"Frame-2":
		top_df:
		    A   B
		0   a   g
		1   f   d
		
		
}
"answer": {
	"desc": %s This should work 
	"code-snippets": [
		n = 2
		df.apply(lambda x: pd.Series(x.value_counts().index[:n]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64405516
"link": https://stackoverflow.com/questions/64405516/find-the-number-of-mutual-friends-in-python
"question": {
	"title": Find the number of mutual friends in Python
	"desc": I have a dataframe of users and their friends that looks like: I want to write a function in to compute the number of mutual friends for each pair: Currently I have: It works fine for small datasets, but I'm running it on a dataset with millions of rows. It takes forever to run everything. I know it's not the ideal way to find the count. Is there a better algorithm in Python? Thanks in advance! 
}
"io": {
	"Frame-1": 
		user_id | friend_id
		1         3
		1         4
		2         3
		2         5
		3         4
		
	"Frame-2":
		user_id | friend_id | num_mutual
		1         3           1
		1         4           1
		2         3           0
		2         5           0
		3         4           1
		
}
"answer": {
	"desc": %s The [ugly] idea is to construct a 4 point path that starts with a and ends with the same . If such a path exists, then 2 starting points have mutual friends. We start with: Then you can do: A more elegant and straightforward way to use a graph approach Then you can identify number of mutual friends as number of paths for 2 adjacent nodes (2 friends) for which a 3 node path exists: 
	"code-snippets": [
		import networkx as nx
		g = nx.from_pandas_edgelist(df, "user_id","friend_id")
		nx.draw_networkx(g)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64404294
"link": https://stackoverflow.com/questions/64404294/pandas-series-add-previous-row-if-diff-negative
"question": {
	"title": pandas series add previous row if diff negative
	"desc": I have a df that contains some revenue values and I want to interpolate the values to the dates that are not included in the index. To do so, I am finding the difference between rows and interpolating: I have this in a function and it is looped over thousands of such calculations (each one creating such a df). This works for most cases, but there are a few where the 'checkout till' resets and thus the diff is negative: The above code will give out negative interpolating values, so I am wondering whether there is a quick way to take that into account when it happens, without putting too much toll on the execution time because it's called thousands of times. The end result for the revenue df (before the interpolation is carried out) should be: So basically if there is a 'reset', the diff should be added to the value in the row above. And that will happen for all rows below. I hope this makes sense. I am struggling to find a way of doing it which is not costly computationally. Thanks in advance. 
}
"io": {
	"Frame-1": 
		            revenue
		2015-10-19  203.0
		2016-04-03  271.0
		2016-06-13  301.0
		2016-06-13  0.0
		2016-09-27  30.0
		2017-03-14  77.0
		2017-09-19  128.0
		2018-09-19  0.0
		2018-03-19  10.0
		2019-03-22  287.0
		2020-03-20  398.0
		
	"Frame-2":
		            revenue
		2015-10-19  203.0
		2016-04-03  271.0
		2016-06-13  301.0
		2016-09-27  331.0
		2017-03-14  378.0
		2017-09-19  429.0
		2018-03-19  439.0
		2019-03-22  716.0   
		2020-03-20  827.0
		
}
"answer": {
	"desc": %s No magic. Steps: Identify the breakpoints by computing revenue difference. Populate the values to be added for subsequent data. Sum it up. Remove duplicate records. Code Result 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df.reset_index(inplace=True)
		
		# 1. compute difference
		df["rev_diff"] = 0.0
		df.loc[1:, "rev_diff"] = df["revenue"].values[1:] - df["revenue"].values[:-1]
		
		# get breakpoint locations
		breakpoints = df[df["rev_diff"] < 0].index.values
		
		# 2. accumulate the values to be added
		df["rev_add"] = 0.0
		for idx in breakpoints:
		    add_value = df.at[idx-1, "revenue"]
		    df.loc[idx:, "rev_add"] += add_value  # accumulate
		
		# 3. sum up
		df["rev_new"] = df["revenue"] + df["rev_add"]
		
		# 4. remove duplicate rows
		df_new = df[["index", "rev_new"]].drop_duplicates().set_index("index")
		df_new.index.name = None
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64368604
"link": https://stackoverflow.com/questions/64368604/shuffling-pandas-dataframe-columns
"question": {
	"title": Shuffling Pandas Dataframe Columns
	"desc": I need to shuffle dataframe columns. Currently I do it this way: Before: After: So it does the job, but there must be a better way to do this. Any ideas? 
}
"io": {
	"Frame-1": 
		          0         1         2         3         4
		0  0.472918  0.261734  0.987053  0.921826  0.144114
		
	"Frame-2":
		          0         1         2         3         4
		0  0.472918  0.921826  0.987053  0.144114  0.261734
		
}
"answer": {
	"desc": %s try: 
	"code-snippets": [
		def shuffle(df, n=1):
		    for _ in range(n):
		        df.apply(np.random.shuffle)
		        return df
		df = pd.DataFrame({'A':range(10), 'B':range(10)})
		
		shuffle(df)
		
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64326826
"link": https://stackoverflow.com/questions/64326826/plotting-a-data-frame-of-error-bars-onto-a-data-frame-in-matplotlib-python
"question": {
	"title": Plotting a data frame of error bars onto a data frame in matplotlib Python
	"desc": I've got a pandas data frame () of values as follows: I also have a data frame () with the error of each of those values: I have successfully been able to plot with matplotlib as I desired: However, I am struggling to get the error bars onto this graph. My code for plotting is currently as follows: As you can see, I am trying to pass the data frame into the flag, but it does not do anything, and I am getting the error: I have had a look online but it seems not many people are trying to add so many error bars like I am trying to. What do I need to change to allow this to work? 
}
"io": {
	"Frame-1": 
		            0           1           2
		0  100.000000  100.000000  100.000000
		1    0.412497    0.668880  136.019498
		2    5.144450   77.323610  163.496773
		3   31.078457   78.151325  146.772621
		
	"Frame-2":
		          0         1         2
		0  0.083579  0.048520  0.082328
		1  0.005855  0.005904  0.046494
		2  0.009907  0.080799  0.083671
		3  0.045831  0.075932  0.044581
		
}
"answer": {
	"desc": %s try casting deviation to list in (and multiply by 100 to see anything) 
	"code-snippets": [
		ax = df.T.plot(kind='bar', yerr=list(deviation.values*100) color=['C0', 'C3', 'C1', 'C2'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64275557
"link": https://stackoverflow.com/questions/64275557/rename-column-in-dataframe-that-contains-digits-in-the-middle
"question": {
	"title": Rename column in dataframe that contains digits in the middle
	"desc": Say I have a dataframe columns as such : To: This need to be done dynamically. My plan is: Use regex to find digits in the MIDDLE of string. Replace to the back of the column name, iteratively. My current code : 
}
"io": {
	"Frame-1": 
		 #   Column                       Non-Null Count  Dtype 
		---  ------                       --------------  ----- 
		 0   Action_3.@source             1 non-null      object
		 1   Description_3.#text          1 non-null      object
		 2   Code_3.@source               1 non-null      object
		 3   Others                       1 non-null      object
		 4   Animal_1                     1 non-null      object
		
	"Frame-2":
		 #   Column                       Non-Null Count  Dtype 
		---  ------                       --------------  ----- 
		 0   Action.@source_3             1 non-null      object
		 1   Description.#text_3          1 non-null      object
		 2   Code.@source_3               1 non-null      object
		 3   Others                       1 non-null      object
		 4   Animal_1                     1 non-null      object
		
}
"answer": {
	"desc": %s You could use a regular expression, with pandas' str.replace to dynamically change it : You can also define the function, if you wish to avoid lambda : Apply the function : 
	"code-snippets": [
		df = pd.DataFrame(
		    [],
		    columns=[
		        "Action_3.@source",
		        "Description_3.#text",
		        "Code_3.@source",
		        "Others",
		        "Animal_1",
		    ],
		)
		
		pat = r"(?P<first>.+)(?P<middle>_\d)(?P<last>.+)"
		repl = lambda m: f"{m.group('first')}{m.group('last')}{m.group('middle')}"
		
		df.columns.str.replace(pat, repl)
		
		Index(['Action.@source_3', 'Description.#text_3', 'Code.@source_3', 'Others',
		       'Animal_1'],
		      dtype='object')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64242508
"link": https://stackoverflow.com/questions/64242508/how-to-create-new-dataframe-by-combining-some-columns-together-of-existing-one
"question": {
	"title": how to create new dataframe by combining some columns together of existing one?
	"desc": I am having a dataframe df like shown: where the explanation of the columns as the following: the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3. I would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups. The new dataframe I need is like the shown: 
}
"io": {
	"Frame-1": 
		1-1    1-2    1-3    2-1    2-2    3-1    3-2    4-1    5-1
		10      3      9      1     3       9      33     10     11
		21      31     3      22    21      13     11     7      13
		33      22     61     31    35      34     8      10     16
		6       9      32     5      4      8      9      6      8
		
	"Frame-2":
		1    2    3    4    5
		10   3    33   10   11
		31   22   13   7    13
		61   35   34   10   16
		32   5    9    6    8
		
}
"answer": {
	"desc": %s You can aggregate by columns with and lambda function for split and select first values with and : This working correct if numbers of groups contains 2 or more digits. Alternative is pass splitted columns names: 
	"code-snippets": [
		df1 = df.groupby(lambda x: x.split('-')[0], axis=1).max()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64230159
"link": https://stackoverflow.com/questions/64230159/shifting-and-reverting-multiple-rows-in-pandas-dataframe
"question": {
	"title": Shifting and reverting multiple rows in pandas dataframe
	"desc": I have the following dataframe and wish to shift over the 0 values to the right and then revert each row: This is the result I would like to get: I've tried varius shift and apply combinations without any success. Is there a simple way of achieving this? 
}
"io": {
	"Frame-1": 
		    H00 H01 H02 H03 H04 H05 H06
		NR                          
		1   33  28  98  97  0   0   0
		2   29  24  22  98  97  0   0
		3   78  76  98  97  0   0   0
		4   16  15  98  97  0   0   0
		5   81  72  70  98  97  0   0
		
	"Frame-2":
		    H00 H01 H02 H03 H04 H05 H06
		NR                          
		1   97  98  28  33  0   0   0
		2   97  98  22  24  29  0   0
		3   97  98  76  78  0   0   0
		4   97  98  15  16  0   0   0
		5   97  98  70  72  81  0   0
		
}
"answer": {
	"desc": %s You can reverse the values that are greater than zero Out: 
	"code-snippets": [
		def reverse_part(series):
		  series[series > 0] = series[series > 0][::-1]
		  return series
		
		df.apply(reverse_part, axis=1, raw=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64229332
"link": https://stackoverflow.com/questions/64229332/using-pythons-max-to-return-two-equally-large-values-across-columns-of-a-data-f
"question": {
	"title": Using Python&#39;s max to return two equally large values across columns of a data frame
	"desc": I would like to find the column of a data frame with the maximum value per row and if there are multiple equally large values, then return all the column names where those values are. I would like to store all of these values in the last column of the data frame. I have been referencing the following post, and am unsure of how to modify it to handle data frames: Using Python's max to return two equally large values So if my data looked like this My goal is an output that looks like this: I know how to use idxmax(axis=1,skipna = True) to return the first max and know that if I change 0 to Nan in the dataframe it will populate the last row correctly, just not sure how to do this when there are multiple max values. Any help is greatly appreciated ! I am an R programmer and this is my first time in Python. 
}
"io": {
	"Frame-1": 
		Key    Column_1  Column_2  Column_3
		0          1        2         3
		1          1        1         0
		2          0        0         0
		
	"Frame-2":
		Key    Column_1  Column_2  Column_3  Column_4
		0          1        2         3      Column_3
		1          1        1         0      Column_1,Column_2
		2          0        0         0      NA
		
}
"answer": {
	"desc": %s Using as well, and combining it with : 
	"code-snippets": [
		d = df.set_index('Key').select_dtypes('number')
		v = d.eq(d.max(axis=1), axis=0).dot(d.columns + ',').str.rstrip(',')
		df['Column_4'] = v.mask(d.eq(0).all(axis=1)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64198934
"link": https://stackoverflow.com/questions/64198934/dropping-rows-from-pandas-dataframe-based-on-value-in-columns
"question": {
	"title": Dropping rows from pandas dataframe based on value in column(s)
	"desc": Suppose I have a dataframe which has Column 'A' and Column 'B' How do I drop rows where Column 'A' and 'B' are equal , but not in same row. I only wanto to drop rows where column 'B' is equal to column 'A' For example Column 'B' from Rows 4, 8 & 9 is equal to Rows 2,3&5 Column 'A'. I want to drop Rows 4, 8 & 9 Drop Rows 4, 8 & 9 since Column B from rows is equal to column A from row 2,3&5 Expected output Rows 4, 8 & 9 needs to be deleted Adding additional details: Column A and B will never be equal in same row. Multiple rows in Column B may have matching values in Column A. To illustrate I have expanded the dataframe Sorry if my originial row numbers are not matching. To summarize the requirement. Multiple rows will have column B matching with Column A and expectation is to delete all rows where column B is matching with Column A in any row. To reiterate Column A and Column B will not be equal in same row 
}
"io": {
	"Frame-1": 
		    Column A         Column B                                                 
		1        10               62 
		2        10               72
		3        20               75
		4        20               10
		5        30               35
		6        30               45               
		7        40               55    
		8        40               20
		9        40               30
		
	"Frame-2":
		    Column A         Column B                                                 
		1        10               62 
		2        10               72
		3        20               75
		
		5        30               35
		6        30               45               
		7        40               55    
		  
		
}
"answer": {
	"desc": %s This solution is assuming that unique values in should be dropped, too, when the condition is met in . I added a fifth row to test for the condition that equal values in the same row should not be dropped Check for all values in if they are in with but exclude rows with equal values. Out: Updated, as per the additional details, and the output matches the expected result. 
	"code-snippets": [
		df[~(df['Column B'].isin(df['Column A']) & (df['Column B'] != df['Column A']))]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64183163
"link": https://stackoverflow.com/questions/64183163/how-can-i-save-dataframe-as-list-and-not-as-string
"question": {
	"title": How can I save DataFrame as list and not as string
	"desc": I have this created. I want to create a and it is saving as this but by doing The problem is that each is now saved as . For example, row 3 is And for those skeptics, I've tried and it's . Column 2 is working well. How can I save as each row and not as ? That is, how can I save all rows of as and not as ? 
}
"io": {
	"Frame-1": 
		                                                         0  1
		0        [15921, 10, 82, 22, 202973, 368, 1055, 3135, 1...  0
		1        [609, 226, 413, 363, 211, 241, 988, 80, 12, 19...  0
		2        [22572, 3720, 233, 13, 827, 710, 512, 354, 1, ...  0
		3                             [345, 656, 25, 2589, 6, 866]  0
		4                                [29142, 8, 4141, 456, 24]  0
		                                                   ... ..
		1599995                         [256, 8, 80, 110, 25, 152]  4
		1599996  [609039, 22, 129, 184, 163, 9419, 769, 358, 10...  4
		1599997                       [140, 5715, 6540, 294, 1552]  4
		1599998  [59, 22771, 189, 387, 4483, 13, 10305, 112231,...  4
		1599999                [59, 15833, 200370, 609041, 609042]  4
		
	"Frame-2":
		"[345, 656, 25, 2589, 6, 866]"
		
}
"answer": {
	"desc": %s Try this: The data in column b is now an array. 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'a': ["[1,2,3,4]", "[6,7,8,9]"]})
		df['b'] = df['a'].apply(eval)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64168703
"link": https://stackoverflow.com/questions/64168703/pandas-new-column-with-index-of-unique-values-of-another-column
"question": {
	"title": Pandas : new column with index of unique values of another column
	"desc": My dataframe: Expected new dataframe: 
}
"io": {
	"Frame-1": 
		ID       Name_Identify  ColumnA  ColumnB  ColumnC
		1        POM-OPP        D43      D03      D59
		2        MIAN-ERP       D80      D74      E34
		3        POM-OPP        E97      B56      A01
		4        POM-OPP        A66      D04      C34
		5        DONP28         B55      A42      A80
		6        MIAN-ERP       E97      D59      C34
		
	"Frame-2":
		ID       Name_Identify ColumnA  ColumnB  ColumnC    NEW_ID
		1        POM-OPP       D43      D03      D59        1
		2        MIAN-ERP      D80      D74      E34        2
		3        POM-OPP       E97      B56      A01        1
		4        POM-OPP       A66      D04      C34        1
		5        DONP28        B55      A42      A80        3
		6        MIAN-ERP      E97      D59      C34        2
		
}
"answer": {
	"desc": %s  The explanation: In the first command we select unique names from the column and then create a dictionary from the enumerated sequence of them (the enumeration starts with ): In the second command we use this dictionary for creating a new column by converting all names in the column to appropriate numbers: 
	"code-snippets": [
		convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}
		df["NEW_ID"] = df.Name_Identify.map(convert)
		
		----------------------------------------------------------------------
		In[24]: convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}
		In[25]: convert
		
		----------------------------------------------------------------------
		In[26]: df["NEW_ID"] = df.Name_Identify.map(convert)
		In[27]: df
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64176921
"link": https://stackoverflow.com/questions/64176921/drop-rows-based-on-condition-pandas
"question": {
	"title": Drop rows based on condition pandas
	"desc": Consider I have a dataframe that looks like this: What I need to do is to sum the C and D and if the sum is higher than 10 remove the entire row. Howerver I can't acess the columns by their names, I need to do it by their position. How can I do it in pandas? EDIT: Another problem. How can I keep the rows that have at least two values in the columns B, C and D? 
}
"io": {
	"Frame-1": 
		   A  B   C   D
		0  0  1   2   3
		1  4  5   6   7
		2  8  9  10  11
		
	"Frame-2":
		   A  B   C   D
		0  0  NaN   2   3
		1  4  5   NaN   NaN
		2  8  9  10  11
		
}
"answer": {
	"desc": %s To keep the rows that have at least two values in the columns B, C and D. You can use this. output For your first question you should fill remaining values using , Documentation before moving with solution provided in earlier answers Output Now you can use to drop the rows where the sum of and is more than 10. Finally I recommend you to keep 1 post per question, this helps other people to search for it. 
	"code-snippets": [
		df = pd.DataFrame({'A': [0,4,8], 'B':[1, np.nan, 9], 'C':[2,np.nan, np.nan], 'D':[3, 7, 11]})
		mask = df.iloc[:,1:].isnull().sum(axis=1) < 2
		print(df[mask])
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64165393
"link": https://stackoverflow.com/questions/64165393/add-character-to-column-based-on-ascending-order-of-another-column-if-condition
"question": {
	"title": Add character to column based on ascending order of another column if condition met pandas
	"desc": Stuck on a data problem in pandas. See data below: The rules are: Only one Cost for each unique (Product, Level) combination. If multiple Cost for each unique (Product, Level) combination, add a letter to the Level value (L1 A, L1 B, etc) based on the Cost value (L1 A being the smallest Cost). If (Product, Level) combination has a unique Cost then do nothing. Desired output: 
}
"io": {
	"Frame-1": 
		| Product | Level | Cost |
		 --------- ------- ------
		| Prod_A  | L1    | 100  |
		| Prod_A  | L1    | 100  |
		| Prod_A  | L1    | 200  |
		| Prod_A  | L2    | 100  |
		| Prod_A  | L3    | 100  |
		| Prod_B  | L1    | 150  |
		| Prod_B  | L1    | 150  |
		| Prod_B  | L2    | 200  |
		| Prod_B  | L2    | 300  |
		| Prod_C  | L3    | 100  |
		
	"Frame-2":
		| Product | Level | Cost |
		 --------- ------- ------
		| Prod_A  | L1 A  | 100  |
		| Prod_A  | L1 A  | 100  |
		| Prod_A  | L1 B  | 200  |
		| Prod_A  | L2    | 100  |
		| Prod_A  | L3    | 100  |
		| Prod_B  | L1    | 150  |
		| Prod_B  | L1    | 150  |
		| Prod_B  | L2 A  | 200  |
		| Prod_B  | L2 B  | 300  |
		| Prod_C  | L3    | 100  |
		
}
"answer": {
	"desc": %s Here's one way: Output: Details: First, create dictionary of the characters to append. Then product and level using unique "encode" each Cost with if there is only one Cost amount then use -1. Lastly, map the results of the "encoded" cost using the dictionary and fillna with a blank string. 
	"code-snippets": [
		charlist='ABCDEFG'
		dd = {k:' '+v for k, v in enumerate(charlist)}
		df['Level'] += df.groupby(['Product', 'Level'])['Cost']\
		                 .transform(lambda x: x.factorize()[0] if x.nunique()>1 else -1)\
		                 .map(dd).fillna('')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 64159489
"link": https://stackoverflow.com/questions/64159489/divide-a-pandas-dataframe-by-the-sum-of-its-index-column-and-row
"question": {
	"title": Divide a pandas dataframe by the sum of its index column and row
	"desc": Here is what I currently have: How can i transform this to df1 so that we divide each element in the row by the sum of the index columns? The output of df1 should look like this: 
}
"io": {
	"Frame-1": 
		    print(df)
		
		    10   25  26
		10  530  1   46  
		25  1    61  61
		26  46   61  330
		
	"Frame-2":
		df1:
		
		
		    10             25               26
		10  530/(530)     1/(530+61)       46/(530+330)  
		25  1/(61+530)    61/(61)          61/(61+330)
		26  46/(330+530)  61/(330+61)      330/(330)
		
		    print(df1)
		
		    10      25        26
		10  1       0.0016    0.0534
		25  0.0016  1         0.1560
		26  0.0534  0.1560    1
		
}
"answer": {
	"desc": %s IIUC, try: Output: 
	"code-snippets": [
		a = np.diag(df)[None, :]
		b = np.diag(df)[:, None]
		
		c = a+b
		np.fill_diagonal(c, np.diag(df))
		
		df_out = df.div(c)
		df_out
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


