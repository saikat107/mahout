[
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 16373605,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/Am4kI.jpg?s=128&g=1",
            "display_name": "python noob",
            "link": "https://stackoverflow.com/users/16373605/python-noob"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 68315969,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625861541,
        "creation_date": 1625824728,
        "last_edit_date": 1625825340,
        "question_id": 68314886,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value",
        "title": "Pandas: Loop through rows to update column value",
        "body": "<p>Here is sample dataframe look like:</p>\n<pre><code>&gt;&gt;&gt; df\n  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.4   NaN    NaN\n4  0.5   NaN    1.0\n5  0.6   NaN    NaN\n6  0.7   1.0    1.0\n7  0.8   NaN    NaN\n8  0.9   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.5   NaN    NaN\n14 1.6   NaN    NaN\n15 1.7   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.7   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>From this dataFrame I want to update <code>point</code> value. Condition is when <code>x</code> or <code>y</code> is not <code>NaN</code> immediate next value of <code>point</code> will be replaced by previous <code>point</code> value afterthat next point value should be reindexed(<strong>cycle .1 to .6</strong>). eg. in row index(2) when <code>point=0.3, x=5.0</code> So, the next <code>point</code> value should be also <strong>0.3 instead of 0.4</strong>, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively)</p>\n<p>OUTPUT I want:</p>\n<pre><code>  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.3   NaN    NaN\n4  0.4   NaN    1.0\n5  0.4   NaN    NaN\n6  0.5   1.0    1.0\n7  0.5   NaN    NaN\n8  0.6   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.4   NaN    NaN\n14 1.5   NaN    NaN\n15 1.6   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.6   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>Code I tried:</p>\n<pre><code>import pandas as pd\ndf = pd.read_csv(&quot;data.csv&quot;)\ndf['point'] = df.groupby() #Don't know how should I approach\n</code></pre>\n",
        "answer_body": "<p>Can you try that:</p>\n<pre><code>mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)\npoint = df['point'].astype(int)\ngroup = point.sub(point.shift(1)).ne(0).cumsum()\n\ndf['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))\n</code></pre>\n<pre><code>&gt;&gt;&gt; df\n    point    x    y\n0     0.1  NaN  NaN\n1     0.2  NaN  NaN\n2     0.3  5.0  NaN\n3     0.3  NaN  NaN\n4     0.4  NaN  1.0\n5     0.4  NaN  NaN\n6     0.5  1.0  1.0\n7     0.5  NaN  NaN\n8     0.6  NaN  NaN\n9     1.1  NaN  NaN\n10    1.2  NaN  NaN\n11    1.3  NaN  NaN\n12    1.4  NaN  2.0\n13    1.4  NaN  NaN\n14    1.5  NaN  NaN\n15    1.6  NaN  NaN\n16    0.1  NaN  NaN\n17    0.2  NaN  NaN\n18    0.3  NaN  NaN\n19    0.4  NaN  NaN\n20    0.5  NaN  NaN\n21    0.6  2.0  NaN\n22    0.6  NaN  NaN\n23    1.1  NaN  NaN\n</code></pre>\n",
        "question_body": "<p>Here is sample dataframe look like:</p>\n<pre><code>&gt;&gt;&gt; df\n  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.4   NaN    NaN\n4  0.5   NaN    1.0\n5  0.6   NaN    NaN\n6  0.7   1.0    1.0\n7  0.8   NaN    NaN\n8  0.9   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.5   NaN    NaN\n14 1.6   NaN    NaN\n15 1.7   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.7   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>From this dataFrame I want to update <code>point</code> value. Condition is when <code>x</code> or <code>y</code> is not <code>NaN</code> immediate next value of <code>point</code> will be replaced by previous <code>point</code> value afterthat next point value should be reindexed(<strong>cycle .1 to .6</strong>). eg. in row index(2) when <code>point=0.3, x=5.0</code> So, the next <code>point</code> value should be also <strong>0.3 instead of 0.4</strong>, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively)</p>\n<p>OUTPUT I want:</p>\n<pre><code>  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.3   NaN    NaN\n4  0.4   NaN    1.0\n5  0.4   NaN    NaN\n6  0.5   1.0    1.0\n7  0.5   NaN    NaN\n8  0.6   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.4   NaN    NaN\n14 1.5   NaN    NaN\n15 1.6   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.6   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>Code I tried:</p>\n<pre><code>import pandas as pd\ndf = pd.read_csv(&quot;data.csv&quot;)\ndf['point'] = df.groupby() #Don't know how should I approach\n</code></pre>\n",
        "formatted_input": {
            "qid": 68314886,
            "link": "https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value",
            "question": {
                "title": "Pandas: Loop through rows to update column value",
                "ques_desc": "Here is sample dataframe look like: From this dataFrame I want to update value. Condition is when or is not immediate next value of will be replaced by previous value afterthat next point value should be reindexed(cycle .1 to .6). eg. in row index(2) when So, the next value should be also 0.3 instead of 0.4, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively) OUTPUT I want: Code I tried: "
            },
            "io": [
                ">>> df\n  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.4   NaN    NaN\n4  0.5   NaN    1.0\n5  0.6   NaN    NaN\n6  0.7   1.0    1.0\n7  0.8   NaN    NaN\n8  0.9   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.5   NaN    NaN\n14 1.6   NaN    NaN\n15 1.7   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.7   NaN    NaN\n23 1.1   NaN    NaN\n",
                "  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.3   NaN    NaN\n4  0.4   NaN    1.0\n5  0.4   NaN    NaN\n6  0.5   1.0    1.0\n7  0.5   NaN    NaN\n8  0.6   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.4   NaN    NaN\n14 1.5   NaN    NaN\n15 1.6   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.6   NaN    NaN\n23 1.1   NaN    NaN\n"
            ],
            "answer": {
                "ans_desc": "Can you try that: ",
                "code": [
                    "mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)\npoint = df['point'].astype(int)\ngroup = point.sub(point.shift(1)).ne(0).cumsum()\n\ndf['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "beautifulsoup"
        ],
        "owner": {
            "reputation": 651,
            "user_id": 9846358,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/n1efb.jpg?s=128&g=1",
            "display_name": "Mary",
            "link": "https://stackoverflow.com/users/9846358/mary"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 68264773,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625547795,
        "creation_date": 1625547085,
        "last_edit_date": 1625547795,
        "question_id": 68264711,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting",
        "title": "pd.read_html changed number formatting",
        "body": "<p>Cannot get <code>1,2,3,4,5,6</code> from the column of <code>CCCCCCC</code>, after <code>pd.read_html</code> format changed to <code>123456</code>, and my <strong>expected result</strong> should be keep <code>1,2,3,4,5,6</code></p>\n<p><strong>HTML code</strong></p>\n<pre><code>html = &quot;&quot;&quot;&lt;html&gt;\n&lt;body&gt;\n&lt;div id=&quot;MMMMMMMM&quot; class=&quot;MMMMMMMMMMM&quot; style=&quot;&quot;&gt;\n        &lt;table class=&quot;OOOOOOOO&quot; style=&quot;&quot;&gt;\n            &lt;thead&gt;\n                &lt;tr class=&quot;PPPPPPPPPP&quot;&gt;\n                    &lt;td colspan=&quot;3&quot; style=&quot;font-size:14px;font-weight:bold;&quot; class=&quot;QQQQQQQQQQ&quot;&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n                &lt;tr class=&quot;RRRRRRRRRR&quot;&gt;\n                    &lt;td&gt;BBBBBB&lt;/td&gt;\n                    &lt;td&gt;CCCCCCC&lt;/td&gt;\n                    &lt;td&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n                    &lt;tr class=&quot;SSSSSSSS&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;DDDDDD&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;3&quot;&gt;EEEEEEEEE&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;FFFFFFFFF&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;GGGGGGGGG&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;HHHHHHHHH&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;IIIIIIIIII&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;JJJJJJJJ&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;2&quot;&gt;KKKKKKKK&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;TTTTTT&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n            &lt;/tbody&gt;\n        &lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;&quot;&quot;&quot;\n</code></pre>\n<p><strong>Python Code</strong></p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1)\ndf_list\n</code></pre>\n<p><strong>Execution Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n<p><strong>Expected Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n</code></pre>\n",
        "answer_body": "<p>You need to add the <code>thousands</code> parameter and set it to <code>None</code> by default it's <code>','</code>.</p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n</code></pre>\n<h5>OUTPUT:</h5>\n<pre><code>[        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD  1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF  1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG  1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH  1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII  1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ  1,2,3,4,5,6  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n",
        "question_body": "<p>Cannot get <code>1,2,3,4,5,6</code> from the column of <code>CCCCCCC</code>, after <code>pd.read_html</code> format changed to <code>123456</code>, and my <strong>expected result</strong> should be keep <code>1,2,3,4,5,6</code></p>\n<p><strong>HTML code</strong></p>\n<pre><code>html = &quot;&quot;&quot;&lt;html&gt;\n&lt;body&gt;\n&lt;div id=&quot;MMMMMMMM&quot; class=&quot;MMMMMMMMMMM&quot; style=&quot;&quot;&gt;\n        &lt;table class=&quot;OOOOOOOO&quot; style=&quot;&quot;&gt;\n            &lt;thead&gt;\n                &lt;tr class=&quot;PPPPPPPPPP&quot;&gt;\n                    &lt;td colspan=&quot;3&quot; style=&quot;font-size:14px;font-weight:bold;&quot; class=&quot;QQQQQQQQQQ&quot;&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n                &lt;tr class=&quot;RRRRRRRRRR&quot;&gt;\n                    &lt;td&gt;BBBBBB&lt;/td&gt;\n                    &lt;td&gt;CCCCCCC&lt;/td&gt;\n                    &lt;td&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n                    &lt;tr class=&quot;SSSSSSSS&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;DDDDDD&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;3&quot;&gt;EEEEEEEEE&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;FFFFFFFFF&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;GGGGGGGGG&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;HHHHHHHHH&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;IIIIIIIIII&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;JJJJJJJJ&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;2&quot;&gt;KKKKKKKK&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;TTTTTT&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n            &lt;/tbody&gt;\n        &lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;&quot;&quot;&quot;\n</code></pre>\n<p><strong>Python Code</strong></p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1)\ndf_list\n</code></pre>\n<p><strong>Execution Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n<p><strong>Expected Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n</code></pre>\n",
        "formatted_input": {
            "qid": 68264711,
            "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting",
            "question": {
                "title": "pd.read_html changed number formatting",
                "ques_desc": "Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result "
            },
            "io": [
                " [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n",
                " [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n"
            ],
            "answer": {
                "ans_desc": "You need to add the parameter and set it to by default it's . OUTPUT: ",
                "code": [
                    "from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user16253345"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68243166,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625389829,
        "creation_date": 1625389469,
        "question_id": 68243146,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
        "title": "replace zero with value of an other column using pandas",
        "body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value from <strong>ref column</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n</code></pre>\n",
        "answer_body": "<p>via <code>mask()</code>:</p>\n<pre><code>df['id']=df['id'].mask(df['id'].eq(0),df['ref'])\n</code></pre>\n<p>OR</p>\n<p>via numpy's <code>where()</code>:</p>\n<pre><code>#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value from <strong>ref column</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n</code></pre>\n",
        "formatted_input": {
            "qid": 68243146,
            "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
            "question": {
                "title": "replace zero with value of an other column using pandas",
                "ques_desc": "I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: "
            },
            "io": [
                "    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n",
                "   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n"
            ],
            "answer": {
                "ans_desc": "via : OR via numpy's : ",
                "code": [
                    "#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "timestamp"
        ],
        "owner": {
            "reputation": 255,
            "user_id": 14073111,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-v9-fmI_5DnM/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckI-maisNcyjtLpr8roYSBRbOvrhw/photo.jpg?sz=128",
            "display_name": "user14073111",
            "link": "https://stackoverflow.com/users/14073111/user14073111"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68232113,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625301132,
        "creation_date": 1625261258,
        "last_edit_date": 1625262116,
        "question_id": 68231389,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
        "title": "Compare two columns that contains timestamps in pandas",
        "body": "<p>Lets say I have a dataframe like this one:</p>\n<pre><code>  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n</code></pre>\n<p>I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4).</p>\n<p>I tried this one:</p>\n<pre><code>df['Col1'] = pd.to_datetime(df['Col1'])\ndf['Col2'] = pd.to_datetime(df['Col2'])\ndf['Col3'] = pd.to_datetime(df['Col3'])\nk= (df['Col1'] &gt; df['Col2']).astype(int)\np=(df['Col2'] &gt; df['Col3']).astype(int)\n\nif k&gt;0:\n    df['Col2']=np.nan\n    df['Col3']=np.nan\n    df['Col4']=np.nan\nelif p&gt;0:\n    df['Col3']=np.nan\n    df['Col4']=np.nan \n</code></pre>\n<p>But it is showing me this error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>My desirable output would look like this:</p>\n<pre><code>  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n</code></pre>\n<p>EDITED:\nAdded Col0</p>\n",
        "answer_body": "<p>A straightforward way with boolean mask:</p>\n<pre><code>dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n</code></pre>\n<pre><code>&gt;&gt;&gt; df\n    Col0                Col1                Col2 Col3 Col4\n0  1.txt 2021-06-23 15:04:30                 NaT  NaT  NaT\n1  2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30  NaT  NaT\n</code></pre>\n",
        "question_body": "<p>Lets say I have a dataframe like this one:</p>\n<pre><code>  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n</code></pre>\n<p>I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4).</p>\n<p>I tried this one:</p>\n<pre><code>df['Col1'] = pd.to_datetime(df['Col1'])\ndf['Col2'] = pd.to_datetime(df['Col2'])\ndf['Col3'] = pd.to_datetime(df['Col3'])\nk= (df['Col1'] &gt; df['Col2']).astype(int)\np=(df['Col2'] &gt; df['Col3']).astype(int)\n\nif k&gt;0:\n    df['Col2']=np.nan\n    df['Col3']=np.nan\n    df['Col4']=np.nan\nelif p&gt;0:\n    df['Col3']=np.nan\n    df['Col4']=np.nan \n</code></pre>\n<p>But it is showing me this error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>My desirable output would look like this:</p>\n<pre><code>  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n</code></pre>\n<p>EDITED:\nAdded Col0</p>\n",
        "formatted_input": {
            "qid": 68231389,
            "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
            "question": {
                "title": "Compare two columns that contains timestamps in pandas",
                "ques_desc": "Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 "
            },
            "io": [
                "  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n",
                "  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n"
            ],
            "answer": {
                "ans_desc": "A straightforward way with boolean mask: ",
                "code": [
                    "dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 16317158,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyBUnbLv2PI4_Agtm14oXQyk_ozZyrNZkI9w4DE=k-s128",
            "display_name": "Jewel_R",
            "link": "https://stackoverflow.com/users/16317158/jewel-r"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68231157,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625259165,
        "creation_date": 1625258774,
        "question_id": 68231104,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
        "title": "Extract part of a 3 D dataframe",
        "body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html#pandas-index-isin\" rel=\"nofollow noreferrer\"><code>Index.isin</code></a> on the level 1 values of columns then select with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</p>\n<pre><code>filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n</code></pre>\n<p><code>filtered_df</code>:</p>\n<pre><code>   d1      d2    \n    A   B   A   B\n0   1   2   5   6\n1   9  10  13  14\n2  17  18  21  22\n</code></pre>\n<hr />\n<p>Sample Data Used:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n</code></pre>\n<pre><code>   d1              d2            \n    A   B   C   D   A   B   C   D\n0   1   2   3   4   5   6   7   8\n1   9  10  11  12  13  14  15  16\n2  17  18  19  20  21  22  23  24\n</code></pre>\n",
        "question_body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "formatted_input": {
            "qid": 68231104,
            "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
            "question": {
                "title": "Extract part of a 3 D dataframe",
                "ques_desc": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: "
            },
            "io": [
                "     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n",
                "    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n"
            ],
            "answer": {
                "ans_desc": "Use on the level 1 values of columns then select with : : Sample Data Used: ",
                "code": [
                    "filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n",
                    "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 11566142,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d2e69536157f140a0c82dc8f714c295d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ivan7",
            "link": "https://stackoverflow.com/users/11566142/ivan7"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 68229969,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1625254302,
        "creation_date": 1625250274,
        "last_edit_date": 1625251248,
        "question_id": 68229806,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
        "title": "Insert values from variable and DataFrame into another DataFrame",
        "body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "answer_body": "<p>Your mistake is on this string <code>df1[df1['id']==id]['col0']</code> when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value.</p>\n<p>To solve this issue is very very very simple, you just have to call the first item at the Series object like this: <code>df1[df1['id']==id]['col0'][0]</code></p>\n<p>Your code with the ajustment must look like this</p>\n<pre><code>import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n</code></pre>\n<p>Then your new df2 is like this:</p>\n<pre><code>   id  col0  col1  col2\n0   1     3    13    23\n1   1     3    14    24\n2   1     3    15    25\n</code></pre>\n",
        "question_body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "formatted_input": {
            "qid": 68229806,
            "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
            "question": {
                "title": "Insert values from variable and DataFrame into another DataFrame",
                "ques_desc": "On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code "
            },
            "io": [
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n",
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n"
            ],
            "answer": {
                "ans_desc": "Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: ",
                "code": [
                    "import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, \"id\", id)\ndf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "algorithm",
            "dataframe",
            "pairwise"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 16238148,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a87a2e4b04f8d0cd6d19f6e68eab5a4e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Eugene Zinder",
            "link": "https://stackoverflow.com/users/16238148/eugene-zinder"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68213724,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625157900,
        "creation_date": 1625155883,
        "question_id": 68213612,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
        "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
        "body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "answer_body": "<p>You can use <code>df.rolling</code> after grouping by <code>ID</code>:</p>\n<pre><code>out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n</code></pre>\n<pre><code>&gt;&gt;&gt; out\n       Val1  Val2\nid0_1  10.5  19.5\nid1_1   3.0   3.0\nid1_2   1.5   2.5\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "formatted_input": {
            "qid": 68213612,
            "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
            "question": {
                "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
                "ques_desc": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: "
            },
            "io": [
                "ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n",
                "ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n"
            ],
            "answer": {
                "ans_desc": "You can use after grouping by : ",
                "code": [
                    "out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 173,
            "user_id": 9254726,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9070bdb0341821aed239bfa229c8f43a?s=128&d=identicon&r=PG&f=1",
            "display_name": "5E4ME",
            "link": "https://stackoverflow.com/users/9254726/5e4me"
        },
        "is_answered": true,
        "view_count": 19,
        "accepted_answer_id": 68211988,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625149939,
        "creation_date": 1625149090,
        "question_id": 68211888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
        "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
        "body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "answer_body": "<p>Let's try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> then build out a <code>dict</code>:</p>\n<pre><code>dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>{'player1':       NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n 'player2':       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n 'player3':       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n 'player5':       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7}\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs['player1']</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7\n</code></pre>\n<hr />\n<p>Or as a <code>list</code>:</p>\n<pre><code>dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>[      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7]\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs[1]</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7\n</code></pre>\n",
        "question_body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "formatted_input": {
            "qid": 68211888,
            "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
            "question": {
                "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
                "ques_desc": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. "
            },
            "io": [
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n",
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n"
            ],
            "answer": {
                "ans_desc": "Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : ",
                "code": [
                    "dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n",
                    "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-munging"
        ],
        "owner": {
            "reputation": 752,
            "user_id": 6057371,
            "user_type": "registered",
            "accept_rate": 31,
            "profile_image": "https://www.gravatar.com/avatar/b744e3dbff9c7fdb801570de75f6eff7?s=128&d=identicon&r=PG&f=1",
            "display_name": "okuoub",
            "link": "https://stackoverflow.com/users/6057371/okuoub"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68193597,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625051576,
        "creation_date": 1625051004,
        "question_id": 68193558,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
        "title": "pandas group many columns to one column where every cell is a list of values",
        "body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "answer_body": "<p>Try:</p>\n<pre><code>#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n</code></pre>\n",
        "question_body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "formatted_input": {
            "qid": 68193558,
            "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
            "question": {
                "title": "pandas group many columns to one column where every cell is a list of values",
                "ques_desc": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? "
            },
            "io": [
                "df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n",
                "df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n"
            ],
            "answer": {
                "ans_desc": "Try: ",
                "code": [
                    "#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15759786,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/002b02baee3603027b66ed2361da99e2?s=128&d=identicon&r=PG&f=1",
            "display_name": "big sad",
            "link": "https://stackoverflow.com/users/15759786/big-sad"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68193581,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625051129,
        "creation_date": 1625050872,
        "question_id": 68193521,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
        "title": "Concatenate values and column names in a data frame to create a new data frame",
        "body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = df.melt(&quot;Value&quot;, value_name=&quot;Col 1&quot;)\nx.Value += &quot;_&quot; + x.variable\nx = x.drop(columns=&quot;variable&quot;)\nprint(x)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>     Value Col 1\n0   a_col1    aa\n1   b_col1    ba\n2   c_col1    ca\n3   d_col1    da\n4   e_col1    ea\n5   a_col2    ab\n6   b_col2    bb\n7   c_col2    cb\n8   d_col2    db\n9   e_col2    eb\n10  a_col3    ac\n11  b_col3    bc\n12  c_col3    cc\n13  d_col3    dc\n14  e_col3    ec\n</code></pre>\n<hr />\n<p>Optionally, you can sort values afterwards:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = x.sort_values(by=&quot;Value&quot;).reset_index(drop=True)\nprint(x)\n\n     Value Col 1\n0   a_col1    aa\n1   a_col2    ab\n2   a_col3    ac\n3   b_col1    ba\n4   b_col2    bb\n5   b_col3    bc\n6   c_col1    ca\n7   c_col2    cb\n8   c_col3    cc\n9   d_col1    da\n10  d_col2    db\n11  d_col3    dc\n12  e_col1    ea\n13  e_col2    eb\n14  e_col3    ec\n</code></pre>\n",
        "question_body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "formatted_input": {
            "qid": 68193521,
            "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
            "question": {
                "title": "Concatenate values and column names in a data frame to create a new data frame",
                "ques_desc": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used "
            },
            "io": [
                "  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n",
                "      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: Optionally, you can sort values afterwards: ",
                "code": [
                    "x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "data-structures"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 9079043,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/974eaeafce0b16f35dce0911a26ade93?s=128&d=identicon&r=PG&f=1",
            "display_name": "Isha",
            "link": "https://stackoverflow.com/users/9079043/isha"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 68174753,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624955956,
        "creation_date": 1624953161,
        "last_edit_date": 1624953363,
        "question_id": 68174614,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
        "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
        "body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "answer_body": "<p>The problem here is, when you call apply on <code>axis=1</code>, pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; s = pd.Series([1, 2, 3.2])\ns\n0    1.0\n1    2.0\n2    3.2\ndtype: float64\n</code></pre>\n<p>As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to :</p>\n<pre><code>df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n</code></pre>\n<p>There's already an issue <a href=\"https://github.com/pandas-dev/pandas/issues/23230\" rel=\"nofollow noreferrer\">DataFrame.apply unintuitively changes int to float because of another column</a> on github for this upcasting behavior of pandas <code>apply</code>.\nSo, one possible option for you is as I have mentioned in the comment, to call <code>to_json</code> on the entire dataframe as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt;df.to_json(orient='index')\n'{&quot;0&quot;:{&quot;A&quot;:2,&quot;B&quot;:6,&quot;C&quot;:5,&quot;D&quot;:8.0},&quot;1&quot;:{&quot;A&quot;:6,&quot;B&quot;:11,&quot;C&quot;:2,&quot;D&quot;:3.6},&quot;2&quot;:{&quot;A&quot;:1,&quot;B&quot;:5,&quot;C&quot;:7,&quot;D&quot;:5.2}}'\n</code></pre>\n<p>A working solution for you may be using python's <code>json</code> module alongwith <code>DataFrame.to_json()</code>, but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['total'] = list(map(json.dumps, [*json.loads(df.to_json(orient='index')).values()]))\n</code></pre>\n<p><strong>OUTPUT:</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>   A   B  C    D                                total\n0  2   6  5  8.0   {&quot;A&quot;: 2, &quot;B&quot;: 6, &quot;C&quot;: 5, &quot;D&quot;: 8.0}\n1  6  11  2  3.6  {&quot;A&quot;: 6, &quot;B&quot;: 11, &quot;C&quot;: 2, &quot;D&quot;: 3.6}\n2  1   5  7  5.2   {&quot;A&quot;: 1, &quot;B&quot;: 5, &quot;C&quot;: 7, &quot;D&quot;: 5.2}\n</code></pre>\n",
        "question_body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "formatted_input": {
            "qid": 68174614,
            "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
            "question": {
                "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
                "ques_desc": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? "
            },
            "io": [
                "A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n",
                "{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n"
            ],
            "answer": {
                "ans_desc": "The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: ",
                "code": [
                    "df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 4149213,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vBcjv.jpg?s=128&g=1",
            "display_name": "openwater",
            "link": "https://stackoverflow.com/users/4149213/openwater"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68174309,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624951870,
        "creation_date": 1624950821,
        "last_edit_date": 1624951870,
        "question_id": 68174113,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
        "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
        "body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "answer_body": "<p>A simpler solution would be to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.GroupBy.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> a custom function on each group. In this case, we can define a function <code>reclass</code> that obtains the correct bins and ids and then uses <code>pd.cut</code>:</p>\n<pre><code>def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n</code></pre>\n<p>Result:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "formatted_input": {
            "qid": 68174113,
            "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
            "question": {
                "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
                "ques_desc": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? "
            },
            "io": [
                "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
                "ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n"
            ],
            "answer": {
                "ans_desc": "A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: ",
                "code": [
                    "def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 121,
            "user_id": 2643948,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/3423e43b576ab333117f731daef5aad9?s=128&d=identicon&r=PG&f=1",
            "display_name": "RebeccaKennedy",
            "link": "https://stackoverflow.com/users/2643948/rebeccakennedy"
        },
        "is_answered": true,
        "view_count": 81,
        "accepted_answer_id": 68150849,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624808663,
        "creation_date": 1624788441,
        "question_id": 68150020,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
        "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
        "body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "answer_body": "<h3>Numpy approach</h3>\n<p>We can define a function <code>first_value</code> which takes a <code>2D</code> array and <code>offset</code> (n) as input arguments and returns <code>1D</code> array. Basically, for each row it returns the <code>nth</code> value after the first <code>non-nan</code> value</p>\n<pre><code>def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i &gt;= arr.shape[1])] = np.nan\n    return vals\n</code></pre>\n<h3>Pandas approach</h3>\n<p>We can <code>stack</code> the dataframe to reshape then group the dataframe on <code>level=0</code> and aggregate using <code>nth</code>, then <code>reindex</code> to conform the index of aggregated frame according to original frame</p>\n<pre><code>def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n</code></pre>\n<h3>Sample run</h3>\n<pre><code>&gt;&gt;&gt; first_valid(df, 0)\nDate\n1     25.0\n2     29.0\n3     33.0\n4     31.0\n5     30.0\n6     35.0\n7     31.0\n8     33.0\n9     26.0\n10    27.0\n11    35.0\n12    33.0\n13    28.0\n14    25.0\n15    25.0\n16    26.0\n17    34.0\n18    28.0\n19    34.0\n20    28.0\ndtype: float64\n\n\n&gt;&gt;&gt; first_valid(df, 1)\nDate\n1      NaN\n2      NaN\n3      NaN\n4     35.0\n5     34.0\n6     34.0\n7     26.0\n8     25.0\n9     31.0\n10    26.0\n11    25.0\n12    35.0\n13    25.0\n14    25.0\n15    26.0\n16    31.0\n17    29.0\n18    29.0\n19    26.0\n20    30.0\ndtype: float64\n\n&gt;&gt;&gt; first_valid(df, 2)\nDate\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n5      NaN\n6      NaN\n7      NaN\n8     31.0\n9      NaN\n10    28.0\n11    29.0\n12    28.0\n13    35.0\n14    28.0\n15    31.0\n16    27.0\n17    25.0\n18    31.0\n19     NaN\n20     NaN\ndtype: float64\n</code></pre>\n<h3>Performance</h3>\n<pre><code># Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p><strong>Numpy based approach is approximately <code>300x</code> faster than the <code>OP's</code> given approach</strong> while pandas based approach is approximately <code>22x</code> faster</p>\n",
        "question_body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "formatted_input": {
            "qid": 68150020,
            "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
            "question": {
                "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
                "ques_desc": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. "
            },
            "io": [
                "f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n",
                "f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n"
            ],
            "answer": {
                "ans_desc": "Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster ",
                "code": [
                    "def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n",
                    "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n",
                    "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 10082987,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/141965c0e67ba4408bdb2267fd7c04f2?s=128&d=identicon&r=PG&f=1",
            "display_name": "syedmfk",
            "link": "https://stackoverflow.com/users/10082987/syedmfk"
        },
        "is_answered": true,
        "view_count": 9806,
        "accepted_answer_id": 57033774,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624643356,
        "creation_date": 1563168421,
        "last_edit_date": 1563169986,
        "question_id": 57033657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
        "title": "How to Extract Month Name and Year from Date column of DataFrame",
        "body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "answer_body": "<p>Cast you date from object to actual datetime and use dt to access what you need.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n</code></pre>\n",
        "question_body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "formatted_input": {
            "qid": 57033657,
            "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
            "question": {
                "title": "How to Extract Month Name and Year from Date column of DataFrame",
                "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. "
            },
            "io": [
                "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
                "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
            ],
            "answer": {
                "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 197,
            "user_id": 12939325,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-ylhVvK83HnQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdDak9Ir2cS6IzUxpxSksMsp-B7tA/photo.jpg?sz=128",
            "display_name": "Steak",
            "link": "https://stackoverflow.com/users/12939325/steak"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 68107566,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624488621,
        "creation_date": 1624484332,
        "last_edit_date": 1624488621,
        "question_id": 68107298,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
        "title": "How to Create a Correlation Dataframe from already related data",
        "body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "answer_body": "<p>Use <code>crosstab</code> to create the all language combinations and fill with the existing data:</p>\n<pre><code>lg = pd.concat([df[0], df[1]]).unique()  # ['English', 'Spanish', 'Russian']\ncx = pd.crosstab(lg, lg)\n\ncx.update(df.set_index([0, 1]).squeeze().unstack())\ncx.update(df.set_index([0, 1]).squeeze().unstack().T)\n</code></pre>\n<pre><code>&gt;&gt;&gt; cx\ncol_0    English  Russian  Spanish\nrow_0\nEnglish     1.00     0.15      0.5\nRussian     0.15     1.00      0.0\nSpanish     0.50     0.00      1.0\n</code></pre>\n",
        "question_body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "formatted_input": {
            "qid": 68107298,
            "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
            "question": {
                "title": "How to Create a Correlation Dataframe from already related data",
                "ques_desc": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? "
            },
            "io": [
                "    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n",
                "        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n"
            ],
            "answer": {
                "ans_desc": "Use to create the all language combinations and fill with the existing data: ",
                "code": [
                    "lg = pd.concat([df[0], df[1]]).unique()  # ['English', 'Spanish', 'Russian']\ncx = pd.crosstab(lg, lg)\n\ncx.update(df.set_index([0, 1]).squeeze().unstack())\ncx.update(df.set_index([0, 1]).squeeze().unstack().T)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16124075,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxaHbBnxKKjuXo_wOrkhGQHbT5P4fY54mYRG-h6=k-s128",
            "display_name": "hang",
            "link": "https://stackoverflow.com/users/16124075/hang"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 68098622,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624446318,
        "creation_date": 1624444563,
        "question_id": 68098150,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
        "title": "set some rule to groupby in pandas",
        "body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "answer_body": "<p>IIUC:</p>\n<p>try:</p>\n<pre><code>c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') &amp; c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n</code></pre>\n",
        "question_body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "formatted_input": {
            "qid": 68098150,
            "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
            "question": {
                "title": "set some rule to groupby in pandas",
                "ques_desc": "I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have \"dup by\" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. "
            },
            "io": [
                "1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n",
                "1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n"
            ],
            "answer": {
                "ans_desc": "IIUC: try: ",
                "code": [
                    "c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 13920381,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgbR0vHurgfWnrlCJifphJzuFAkbDbJV3Ls033L6g=k-s128",
            "display_name": "dinn_",
            "link": "https://stackoverflow.com/users/13920381/dinn"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68090682,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624397667,
        "creation_date": 1624394378,
        "question_id": 68090463,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
        "title": "Merging more than two columns of the same dataframe in pandas",
        "body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df\n    VAR 1  VAR 2  VAR 3    GROUP  ANOTHER\n0     NaN    NaN    3.0   [0-10]  another\n1     1.0    NaN    3.0   [0-10]  another\n2     1.0    NaN    3.0   [0-10]  another\n3     1.0    2.0    NaN   [0-10]  another\n4     NaN    2.0    NaN   [0-10]  another\n5     3.0    NaN    3.0  [10-20]  another\n6     3.0    1.0    NaN  [10-20]  another\n7     NaN    1.0    NaN  [10-20]  another\n8     NaN    2.0    NaN  [10-20]  another\n9     NaN    NaN    2.0  [10-20]  another\n10    NaN    NaN    2.0  [10-20]  another\n</code></pre>\n<p>You can use <code>melt</code>. To fully understand, you can execute the code line by line (<code>df.melt(...)</code>, <code>df.melt(...).dropna()</code>, <code>df.melt(...).dropna.sort_values(...)</code> and so on):</p>\n<pre><code>id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n</code></pre>\n<p>Result output:</p>\n<pre><code>&gt;&gt;&gt; out\n    VAR_MERGED    GROUP  ANOTHER\n0          1.0   [0-10]  another\n1          1.0   [0-10]  another\n2          1.0   [0-10]  another\n3          2.0   [0-10]  another\n4          2.0   [0-10]  another\n5          3.0   [0-10]  another\n6          3.0   [0-10]  another\n7          3.0   [0-10]  another\n8          1.0  [10-20]  another\n9          1.0  [10-20]  another\n10         2.0  [10-20]  another\n11         2.0  [10-20]  another\n12         2.0  [10-20]  another\n13         3.0  [10-20]  another\n14         3.0  [10-20]  another\n15         3.0  [10-20]  another\n</code></pre>\n",
        "question_body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "formatted_input": {
            "qid": 68090463,
            "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
            "question": {
                "title": "Merging more than two columns of the same dataframe in pandas",
                "ques_desc": "Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? "
            },
            "io": [
                "VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n",
                "VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n"
            ],
            "answer": {
                "ans_desc": "Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: ",
                "code": [
                    "id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization"
        ],
        "owner": {
            "reputation": 1433,
            "user_id": 4263878,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/c95a66d30180a95353e7e655d0020f51?s=128&d=identicon&r=PG&f=1",
            "display_name": "MJS",
            "link": "https://stackoverflow.com/users/4263878/mjs"
        },
        "is_answered": true,
        "view_count": 3896,
        "accepted_answer_id": 33130915,
        "answer_count": 2,
        "score": 7,
        "last_activity_date": 1623919516,
        "creation_date": 1444839526,
        "last_edit_date": 1444840857,
        "question_id": 33130586,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
        "title": "python pandas - creating a column which keeps a running count of consecutive values",
        "body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "answer_body": "<p>You can use the compare-cumsum-groupby pattern (which I <em>really</em> need to getting around to writing up for the documentation), with a final <code>cumcount</code>:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"binary\": [0,1,1,1,0,0,1,1,0]})\n&gt;&gt;&gt; df[\"consec\"] = df[\"binary\"].groupby((df[\"binary\"] == 0).cumsum()).cumcount()\n&gt;&gt;&gt; df\n   binary  consec\n0       0       0\n1       1       1\n2       1       2\n3       1       3\n4       0       0\n5       0       0\n6       1       1\n7       1       2\n8       0       0\n</code></pre>\n\n<hr>\n\n<p>This works because first we get the positions where we want to reset the counter:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n</code></pre>\n\n<p>The cumulative sum of these gives us a different id for each group:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0).cumsum()\n0    1\n1    1\n2    1\n3    1\n4    2\n5    3\n6    3\n7    3\n8    4\nName: binary, dtype: int64\n</code></pre>\n\n<p>And then we can pass this to <code>groupby</code> and use <code>cumcount</code> to get an increasing index in each group.</p>\n",
        "question_body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "formatted_input": {
            "qid": 33130586,
            "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
            "question": {
                "title": "python pandas - creating a column which keeps a running count of consecutive values",
                "ques_desc": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. "
            },
            "io": [
                ".    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n",
                ".  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n"
            ],
            "answer": {
                "ans_desc": "You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. ",
                "code": [
                    ">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pyspark-dataframes"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9735423,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "learningtocode",
            "link": "https://stackoverflow.com/users/9735423/learningtocode"
        },
        "is_answered": true,
        "view_count": 607,
        "accepted_answer_id": 62099641,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1623911798,
        "creation_date": 1590822619,
        "last_edit_date": 1590926848,
        "question_id": 62099066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
        "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
        "body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "answer_body": "<p>Here you go:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n</code></pre>\n\n<p>Check <code>output.xlsx</code> for the output.</p>\n",
        "question_body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "formatted_input": {
            "qid": 62099066,
            "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
            "question": {
                "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
                "ques_desc": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: "
            },
            "io": [
                "(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n",
                "(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n"
            ],
            "answer": {
                "ans_desc": "Here you go: Check for the output. ",
                "code": [
                    "import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10340,
            "user_id": 1334761,
            "user_type": "registered",
            "accept_rate": 62,
            "profile_image": "https://www.gravatar.com/avatar/e98df35e5a847e4adee98cb544e5bf6e?s=128&d=identicon&r=PG",
            "display_name": "Jjang",
            "link": "https://stackoverflow.com/users/1334761/jjang"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 67990087,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1623830508,
        "creation_date": 1623772927,
        "last_edit_date": 1623830508,
        "question_id": 67989744,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
        "title": "Pandas replacing values in a column by values in another column",
        "body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "answer_body": "<p>Have a look at Jeremy Z answer on this post, for further explanation on solution <a href=\"https://stackoverflow.com/a/55631906/16235276\">https://stackoverflow.com/a/55631906/16235276</a></p>\n<pre><code>df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n</code></pre>\n",
        "question_body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "formatted_input": {
            "qid": 67989744,
            "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
            "question": {
                "title": "Pandas replacing values in a column by values in another column",
                "ques_desc": "Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: "
            },
            "io": [
                "    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n",
                "    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n"
            ],
            "answer": {
                "ans_desc": "Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 ",
                "code": [
                    "df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16117705,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b2f44431a658e5622efe2e5d2d6a84b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "WorkingPerson",
            "link": "https://stackoverflow.com/users/16117705/workingperson"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67987417,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623764393,
        "creation_date": 1623761108,
        "last_edit_date": 1623763345,
        "question_id": 67986537,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
        "title": "How do I check for conflict between columns in a pandas dataframe?",
        "body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "answer_body": "<p>IIUC, try:</p>\n<pre><code>df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      Yes\n1  Item2      6.0      7.0        5      Yes\n2  Item3      NaN      NaN        5      nan\n3  Item4      5.0      5.0        5      nan\n4  Item5      5.0      NaN        5      nan\n</code></pre>\n",
        "question_body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "formatted_input": {
            "qid": 67986537,
            "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
            "question": {
                "title": "How do I check for conflict between columns in a pandas dataframe?",
                "ques_desc": "I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. "
            },
            "io": [
                "    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n",
                "    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n"
            ],
            "answer": {
                "ans_desc": "IIUC, try: Output: ",
                "code": [
                    "df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 8581989,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/S7DTo.jpg?s=128&g=1",
            "display_name": "Steve Dallas",
            "link": "https://stackoverflow.com/users/8581989/steve-dallas"
        },
        "is_answered": true,
        "view_count": 11415,
        "accepted_answer_id": 46125692,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1623741805,
        "creation_date": 1504905625,
        "last_edit_date": 1527649748,
        "question_id": 46124699,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
        "title": "Splitting a dataframe into separate CSV files",
        "body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "answer_body": "<p>Why not just groupby <code>Column1</code> and save each group?</p>\n\n<pre><code>df = df.sort_values('Column1').assign(NewColumn='NewColumnValue')\nprint(df)\n\n   Column1  Column2       NewColumn\n0        1    93644  NewColumnValue\n5        1    19593  NewColumnValue\n6        1    12707  NewColumnValue\n1        2    63246  NewColumnValue\n7        2    53480  NewColumnValue\n2        3    47790  NewColumnValue\n3        3    39644  NewColumnValue\n4        3    32585  NewColumnValue\n</code></pre>\n\n<hr>\n\n<pre><code>for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n</code></pre>\n\n<p>Thanks to Unatiel for the <a href=\"https://stackoverflow.com/questions/46124699/splitting-csv-based-on-content-of-one-column-after-having-done-some-other-manipu/46125692?noredirect=1#comment79214220_46125692\">improvement</a>. <code>header=False</code> will not write headers and <code>index_label=False</code> will not write an index column.</p>\n\n<p>This creates 3 files:</p>\n\n<pre><code>1.csv\n2.csv\n3.csv\n</code></pre>\n\n<p>Each having data corresponding to each <code>Column1</code> group.</p>\n",
        "question_body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "formatted_input": {
            "qid": 46124699,
            "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
            "question": {
                "title": "Splitting a dataframe into separate CSV files",
                "ques_desc": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. "
            },
            "io": [
                "+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n",
                "+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n"
            ],
            "answer": {
                "ans_desc": "Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. ",
                "code": [
                    "for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 9235000,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1652134728176732/picture?type=large",
            "display_name": "Amruth Anand",
            "link": "https://stackoverflow.com/users/9235000/amruth-anand"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 67919490,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623322230,
        "creation_date": 1623321043,
        "question_id": 67919385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
        "title": "Find unique column values out of two different Dataframes",
        "body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "answer_body": "<p>TRY:</p>\n<pre><code>unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n</code></pre>\n<p><em>NOTE</em> : Replace <code>0</code> in <code>subset= [0]</code> with the first column name.</p>\n",
        "question_body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "formatted_input": {
            "qid": 67919385,
            "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
            "question": {
                "title": "Find unique column values out of two different Dataframes",
                "ques_desc": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read "
            },
            "io": [
                "67      Hij\n14      Xyz \n87      Pqr\n",
                "43      Def\n67      Lmn\n14      Xyz\n"
            ],
            "answer": {
                "ans_desc": "TRY: NOTE : Replace in with the first column name. ",
                "code": [
                    "unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 133,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 67917741,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623316126,
        "creation_date": 1623314044,
        "last_edit_date": 1623314922,
        "question_id": 67917573,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
        "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
        "body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "answer_body": "<p>Here is one way:</p>\n<pre><code>minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, &quot;-&quot;)\n</code></pre>\n<p>where we first flip the <code>df</code> over the columns, look where it is not <code>NaN</code> and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put <code>&quot;-&quot;</code> so we use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>mask</code></a> method to put minus signs there,</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n    L1   D1   L2   D2 L3\n0  1.0  ABC  1.1  4.1  -\n1  NaN  NaN  1.7    -  -\n2  NaN  4.1    -    -  -\n3  NaN  1.8  3.2  PQR  -\n4  NaN  NaN  1.6    -  -\n</code></pre>\n",
        "question_body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "formatted_input": {
            "qid": 67917573,
            "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
            "question": {
                "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
                "ques_desc": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! "
            },
            "io": [
                " L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n",
                " L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n"
            ],
            "answer": {
                "ans_desc": "Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get ",
                "code": [
                    "minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 133,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 67910764,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1623312939,
        "creation_date": 1623267501,
        "question_id": 67910688,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
        "title": "move column above and delete rows in pandas python dataframe",
        "body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "answer_body": "<p>You can shift back each column by the number of preceding missing values which is found with <code>first_valid_index</code>:</p>\n<pre><code>df.apply(lambda s: s.shift(-s.first_valid_index()))\n</code></pre>\n<p>to get</p>\n<pre><code>     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1  NaN  NaN  c.2  d.2  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n</code></pre>\n<p>To drop the rows full of <code>NaN</code>s and fill the rest with empty string:</p>\n<pre><code>out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=&quot;all&quot;)\n         .fillna(&quot;&quot;))\n</code></pre>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1            c.2  d.2\n</code></pre>\n<hr>\n<p>note: this assumes your index is <code>0..N-1</code>; so if it's not, you can store it beforehand and then restore back:</p>\n<pre><code>index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=&quot;all&quot;)\n        .fillna(&quot;&quot;))\ndf.index = index[:len(df)]\n</code></pre>\n<hr>\n<p>To make the pulling up specific to some columns:</p>\n<pre><code>def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n</code></pre>\n",
        "question_body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "formatted_input": {
            "qid": 67910688,
            "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
            "question": {
                "title": "move column above and delete rows in pandas python dataframe",
                "ques_desc": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be "
            },
            "io": [
                "A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n",
                "A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n"
            ],
            "answer": {
                "ans_desc": "You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: ",
                "code": [
                    "out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n",
                    "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n",
                    "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 166,
            "user_id": 16154762,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgA5126u43jbFmhmVe4Tv7Rq8KecxLwFi0ibsEU=k-s128",
            "display_name": "Vivek Singh",
            "link": "https://stackoverflow.com/users/16154762/vivek-singh"
        },
        "is_answered": true,
        "view_count": 70,
        "accepted_answer_id": 67905751,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1623247633,
        "creation_date": 1623247407,
        "last_edit_date": 1623247633,
        "question_id": 67905723,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
        "title": "Replicating the DataFrame row in a special manner",
        "body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "answer_body": "<p>Try with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html#pandas-series-str-split\" rel=\"noreferrer\"><code>str.split</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas-dataframe-explode\" rel=\"noreferrer\"><code>DataFrame.explode</code></a>:</p>\n<pre><code>df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n</code></pre>\n<pre><code>  col1      mob_no  col3\n0    a  9382949201    45\n0    a  3245622535    45\n1    b  8383459345    67\n1    b  4325562678    67\n2    c  8976247543    89\n2    c  1827472398    89\n3    d  7844329432    09\n</code></pre>\n",
        "question_body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "formatted_input": {
            "qid": 67905723,
            "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
            "question": {
                "title": "Replicating the DataFrame row in a special manner",
                "ques_desc": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: "
            },
            "io": [
                "col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n",
                "col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n"
            ],
            "answer": {
                "ans_desc": "Try with + : ",
                "code": [
                    "df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "dictionary"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67882312,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623132901,
        "creation_date": 1623131314,
        "question_id": 67882067,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
        "title": "Dictionary making for a transportation model from a Dataframe",
        "body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "answer_body": "<pre><code>df1 = df.set_index([&quot;Unnamed: 0&quot;, &quot;Unnamed: 1&quot;])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[&quot;demand&quot;].T.squeeze().dropna().iteritems())\nM = dict(plants[&quot;capacity&quot;].iteritems())\nI = list(plants.drop(columns=&quot;capacity&quot;).columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=&quot;capacity&quot;).stack().iteritems())\n</code></pre>\n<pre><code>&gt;&gt;&gt; d\n{'c1': 80.0, 'c2': 270.0, 'c3': 250.0, 'c4': 160.0, 'c5': 180.0}\n\n&gt;&gt;&gt; M\n{'p1': 500.0, 'p2': 500.0, 'p3': 500.0}\n\n&gt;&gt;&gt; I\n['c1', 'c2', 'c3', 'c4', 'c5']\n\n&gt;&gt;&gt; J\n['p1', 'p2', 'p3']\n\n&gt;&gt;&gt; cost\n{('p1', 'c1'): 4,\n ('p1', 'c2'): 5,\n ('p1', 'c3'): 6,\n ('p1', 'c4'): 8,\n ('p1', 'c5'): 10,\n ('p2', 'c1'): 6,\n ('p2', 'c2'): 4,\n ('p2', 'c3'): 3,\n ('p2', 'c4'): 5,\n ('p2', 'c5'): 8,\n ('p3', 'c1'): 9,\n ('p3', 'c2'): 7,\n ('p3', 'c3'): 4,\n ('p3', 'c4'): 2,\n ('p3', 'c5'): 4}\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "formatted_input": {
            "qid": 67882067,
            "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
            "question": {
                "title": "Dictionary making for a transportation model from a Dataframe",
                "ques_desc": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. "
            },
            "io": [
                " d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n",
                "  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67871572,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1623073970,
        "creation_date": 1623063317,
        "last_edit_date": 1623065349,
        "question_id": 67870323,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
        "title": "How to get list of previous n values of a column conditionally in DataFrame?",
        "body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "answer_body": "<p>Since rolling only supports production of numeric values, this has to be a work around.</p>\n<p>Try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> first then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\" rel=\"nofollow noreferrer\"><code>groupby rolling</code></a> on window + 1 and strip off the last element:</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n5         1      5         [15]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n11        1     12   [5, 20, 8]\n1         2      0           []\n3         2     30          [0]\n7         2      7      [0, 30]\n10        2      9   [0, 30, 7]\n2         3     18           []\n4         3     17         [18]\n6         4      9           []\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index\" rel=\"nofollow noreferrer\"><code>sort_index</code></a> to restore the initial order:</p>\n<pre><code>df = df.sort_index()\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [0, 30]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n10        2      9   [0, 30, 7]\n11        1     12   [5, 20, 8]\n</code></pre>\n<p>(Optional use extended slicing to reverse the lists and get elements in same order as expected output above):</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [30, 0]\n8         1     20      [5, 15]\n9         1      8  [20, 5, 15]\n10        2      9   [7, 30, 0]\n11        1     12   [8, 20, 5]\n</code></pre>\n<hr/>\n<p>Complete Working Example:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'Subject': [1, 2, 3, 2, 3, 1, 4, 2, 1, 1, 2, 1],\n    'Score': [15, 0, 18, 30, 17, 5, 9, 7, 20, 8, 9, 12]\n})\n\nwindow = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\ndf = df.sort_index()\n\nprint(df)\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "formatted_input": {
            "qid": 67870323,
            "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
            "question": {
                "title": "How to get list of previous n values of a column conditionally in DataFrame?",
                "ques_desc": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? "
            },
            "io": [
                "Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n",
                "Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n"
            ],
            "answer": {
                "ans_desc": "Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: ",
                "code": [
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n",
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "indexing"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 8970043,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uL3Er.jpg?s=128&g=1",
            "display_name": "Futurex",
            "link": "https://stackoverflow.com/users/8970043/futurex"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 67870693,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623064916,
        "creation_date": 1623064484,
        "question_id": 67870585,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
        "title": "Python dataframe create index column based on other id column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.ngroup.html#pandas-core-groupby-groupby-ngroup\" rel=\"nofollow noreferrer\"><code>groupby ngroup</code></a> + 1 :</p>\n<pre><code>df['ID_2'] = df.groupby('ID').ngroup() + 1\n</code></pre>\n<p>Or with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rank.html#pandas-series-rank\" rel=\"nofollow noreferrer\"><code>Rank</code></a>:</p>\n<pre><code>df['ID_2'] = df['ID'].rank(method='dense').astype(int)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>                 ID   Price  ID_2\n0  000afb96ded6677c  1514.5     1\n1  000afb96ded6677c    13.0     1\n2  000afb96ded6677c   611.0     1\n3  000afb96ded6677c   723.0     1\n4  000afb96ded6677c  2065.0     1\n5  ffea14e87a4e1269  2286.0     2\n6  ffea14e87a4e1269  1150.0     2\n7  ffea14e87a4e1269    80.0     2\n8  fff455057ad492da   650.0     3\n9  fff5fc66c1fd66c2   450.0     4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67870585,
            "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
            "question": {
                "title": "Python dataframe create index column based on other id column",
                "ques_desc": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: "
            },
            "io": [
                "ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n",
                "ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n"
            ],
            "answer": {
                "ans_desc": "Try + 1 : Or with : : ",
                "code": [
                    "df['ID_2'] = df.groupby('ID').ngroup() + 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 35,
            "user_id": 16132607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/234e69866fa5f5a2e96b28a4fd12597b?s=128&d=identicon&r=PG&f=1",
            "display_name": "homeboykeroro",
            "link": "https://stackoverflow.com/users/16132607/homeboykeroro"
        },
        "is_answered": true,
        "view_count": 92,
        "accepted_answer_id": 67857571,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1622976694,
        "creation_date": 1622966815,
        "last_edit_date": 1622970680,
        "question_id": 67856992,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
        "title": "Element wise numeric comparison in Pandas dataframe column value with list",
        "body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "answer_body": "<p>Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise).</p>\n<p>You can use <code>apply</code>:</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n</code></pre>\n<hr />\n<pre><code>res = df2.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<p><strong>res:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">A</th>\n<th style=\"text-align: center;\">B</th>\n<th style=\"text-align: center;\">C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[False, False, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n</tr>\n</tbody>\n</table>\n</div><h2>Update</h2>\n<p>(Complete Solution Based on the data you've provided):</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<h2>Time Comparison:</h2>\n<p><strong>Method 1 (Nk03's method1):</strong></p>\n<blockquote>\n<p>CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms</p>\n</blockquote>\n<p><strong>Method 2 (Nk03's method2):</strong></p>\n<blockquote>\n<p>CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms</p>\n</blockquote>\n<p><strong>Method 3 (Using numpy based comparison):</strong></p>\n<blockquote>\n<p>CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms</p>\n</blockquote>\n<p><strong>Nk03's Updated and Optimized Solution:</strong></p>\n<blockquote>\n<p>CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms</p>\n</blockquote>\n",
        "question_body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "formatted_input": {
            "qid": 67856992,
            "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
            "question": {
                "title": "Element wise numeric comparison in Pandas dataframe column value with list",
                "ques_desc": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: "
            },
            "io": [
                "  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n",
                "  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n"
            ],
            "answer": {
                "ans_desc": "Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms ",
                "code": [
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n",
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 10566774,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-8T7iqZlWN8Y/AAAAAAAAAAI/AAAAAAAACu0/hiVsQlXBjzQ/photo.jpg?sz=128",
            "display_name": "Salvatore Nedia",
            "link": "https://stackoverflow.com/users/10566774/salvatore-nedia"
        },
        "is_answered": true,
        "view_count": 109,
        "accepted_answer_id": 67845512,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1622859671,
        "creation_date": 1622854122,
        "last_edit_date": 1622854820,
        "question_id": 67845362,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
        "title": "Sort pandas df subset of rows (within a group) by specific column",
        "body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "answer_body": "<p>I think it should be as simple as this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.sort_values([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "formatted_input": {
            "qid": 67845362,
            "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
            "question": {
                "title": "Sort pandas df subset of rows (within a group) by specific column",
                "ques_desc": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? "
            },
            "io": [
                "\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n",
                "\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n"
            ],
            "answer": {
                "ans_desc": "I think it should be as simple as this: ",
                "code": [
                    "df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 35,
            "user_id": 12785115,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3484fd202fb5624044a4ed7b5181fab?s=128&d=identicon&r=PG&f=1",
            "display_name": "RamboJ",
            "link": "https://stackoverflow.com/users/12785115/ramboj"
        },
        "is_answered": true,
        "view_count": 91,
        "accepted_answer_id": 61625210,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1622820639,
        "creation_date": 1588722398,
        "question_id": 61624957,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
        "title": "Creating a table in pandas from json",
        "body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "answer_body": "<p>You need to massage the data a little bit to get what you want.</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: x['website'][0])\n    .to_frame('website')\n)\n\n    website\n1   https://bitcoin.org/\n2   https://litecoin.org/\n</code></pre>\n\n<p>To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. </p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n</code></pre>\n\n<p>To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls'</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n</code></pre>\n",
        "question_body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 61624957,
            "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
            "question": {
                "title": "Creating a table in pandas from json",
                "ques_desc": "I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! "
            },
            "io": [
                "0     website\n0  1  NaN\n1  2  NaN \n",
                "0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n"
            ],
            "answer": {
                "ans_desc": "You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' ",
                "code": [
                    "(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n",
                    "(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 355,
            "user_id": 5426588,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/068913af46eb9b38c9ffb53ba219c7c9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Baig",
            "link": "https://stackoverflow.com/users/5426588/baig"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67822589,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1622728725,
        "creation_date": 1622726951,
        "last_edit_date": 1622727694,
        "question_id": 67822403,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
        "title": "Replicate dataframe n number of times and increment another column by 1",
        "body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "answer_body": "<p>Something along these lines should work:</p>\n<pre><code>df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "formatted_input": {
            "qid": 67822403,
            "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
            "question": {
                "title": "Replicate dataframe n number of times and increment another column by 1",
                "ques_desc": "I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. "
            },
            "io": [
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n",
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n"
            ],
            "answer": {
                "ans_desc": "Something along these lines should work: ",
                "code": [
                    "df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16067894,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a6cf989fce509918dc286c22d013c519?s=128&d=identicon&r=PG&f=1",
            "display_name": "lasse3434",
            "link": "https://stackoverflow.com/users/16067894/lasse3434"
        },
        "is_answered": true,
        "view_count": 77,
        "accepted_answer_id": 67782978,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1622554424,
        "creation_date": 1622521690,
        "last_edit_date": 1622521880,
        "question_id": 67782727,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
        "title": "Python - Delete lines from dataframe (pandas)",
        "body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "answer_body": "<p>Make a clean dataframe and <strong>keep</strong> values you want:</p>\n<pre><code>data[['1', '2']] = data[['1', '2']].replace({&quot;///&quot;: np.nan, &quot;,&quot;: &quot;.&quot;}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; data\n               0      1      2\n0  9783630876672  12.35   2.62\n1  9783423282789  11.67   6.07\n2  9783833879500  17.25  12.40\n3  9783898798822   6.91   1.16\n4  9783453281417  12.93   2.84\n5  9783630876672  12.35   4.08\n6  9783423282789  11.67   6.07\n7  9783833879500  17.25   9.94\n8  9783898798822   6.91   2.96\n9  9783453281417  12.93   2.68\n</code></pre>\n<p><strong>Comments:</strong></p>\n<p><strong>1st line:</strong></p>\n<ul>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing\" rel=\"nofollow noreferrer\"><code>data[['1', '2']]</code></a> select columns named '1' and '2'</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\" rel=\"nofollow noreferrer\"><code>replace</code></a> change existing values ('///' and ',') by new ones ('nan' and '.')</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\" rel=\"nofollow noreferrer\"><code>astype(float)</code></a> convert your string columns to real numbers (float) since your dataframe is cleaned.</li>\n</ul>\n<p><strong>2nd line:</strong></p>\n<ul>\n<li><code>data.loc[...]</code> locate something in your dataframe</li>\n<li><code>data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)</code>: in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row.</li>\n</ul>\n",
        "question_body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "formatted_input": {
            "qid": 67782727,
            "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
            "question": {
                "title": "Python - Delete lines from dataframe (pandas)",
                "ques_desc": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. "
            },
            "io": [
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n",
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n"
            ],
            "answer": {
                "ans_desc": "Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. ",
                "code": [
                    "data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 13196248,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/--wvokyexUe4/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMneagY7ciQcziCtlVV3rVrRFH57A/photo.jpg?sz=128",
            "display_name": "Charles",
            "link": "https://stackoverflow.com/users/13196248/charles"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 67770609,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1622454286,
        "creation_date": 1622448681,
        "question_id": 67770056,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
        "title": "How to create columns from a string in a dataframe?",
        "body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "answer_body": "<pre><code>ndf = (df[&quot;long string&quot;]\n         .str.extractall(r&quot;(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)&quot;)\n         .droplevel(&quot;match&quot;)\n         .set_index(0, append=True)\n         .set_axis([&quot;tra&quot;, &quot;la&quot;], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(&quot;-&quot;.join)\n</code></pre>\n<ul>\n<li>Extract the desired parts with a <a href=\"https://regex101.com/r/F3EKrX/1\" rel=\"nofollow noreferrer\">regex</a></li>\n<li>Drop the index level induced by <code>extractall</code> called <code>match</code></li>\n<li>Append the <code>ha-hi-ho</code> matches as the index (<code>0</code> is first capturing group)</li>\n<li>Rename the remaining columns <code>tra</code> and <code>la</code></li>\n<li>Unstack the <code>ha-hi-ho</code> index to the columns</li>\n<li>Swap the <code>ha-hi-ho</code> and <code>tra-la</code> levels' order in columns so that <code>ha-hi-ho</code> is upper</li>\n<li>Lastly join these levels of columns' names with a hyphen</li>\n</ul>\n<p>to get</p>\n<pre><code>  ha-tra hi-tra ho-tra ha-la hi-la ho-la\n0      1      1      1     2     2     2\n1      1      1      1     2     2     2\n2      1      1      1     2     2     2\n</code></pre>\n",
        "question_body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "formatted_input": {
            "qid": 67770056,
            "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
            "question": {
                "title": "How to create columns from a string in a dataframe?",
                "ques_desc": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. "
            },
            "io": [
                "    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n",
                "    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n"
            ],
            "answer": {
                "ans_desc": " Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get ",
                "code": [
                    "ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15286348,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a1dfedef68132d2be690d3ce84d9aff5?s=128&d=identicon&r=PG",
            "display_name": "lynch1972",
            "link": "https://stackoverflow.com/users/15286348/lynch1972"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67697000,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621987482,
        "creation_date": 1621985797,
        "question_id": 67696814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
        "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
        "body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>Tr this. This would depend on how large your data is.</p>\n<pre><code>\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n</code></pre>\n",
        "question_body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 67696814,
            "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
            "question": {
                "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
                "ques_desc": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! "
            },
            "io": [
                "                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n",
                "        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n"
            ],
            "answer": {
                "ans_desc": "Tr this. This would depend on how large your data is. ",
                "code": [
                    "\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 548,
            "user_id": 5560529,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dEQgB.jpg?s=128&g=1",
            "display_name": "Peter",
            "link": "https://stackoverflow.com/users/5560529/peter"
        },
        "is_answered": true,
        "view_count": 80,
        "accepted_answer_id": 67672293,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1621860569,
        "creation_date": 1621859819,
        "question_id": 67672199,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
        "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
        "body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "answer_body": "<p>I think you just need <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> with boolean indexing on <code>df1</code>.</p>\n<pre><code>pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n</code></pre>\n<p>The <code>df1[df1['C']==43]]</code> part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2.</p>\n<p>Output:</p>\n<pre><code>    A   B   C\n0   4   c   12\n1   5   d   19\n2   2   b   43\n</code></pre>\n",
        "question_body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "formatted_input": {
            "qid": 67672199,
            "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
            "question": {
                "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
                "ques_desc": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? "
            },
            "io": [
                "A    B    C\n4    c    12\n5    d    19\n2    b    43\n",
                "    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n"
            ],
            "answer": {
                "ans_desc": "I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: ",
                "code": [
                    "pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 12537783,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4dec20a3cd9053e9e8a1751561acb545?s=128&d=identicon&r=PG&f=1",
            "display_name": "tester559",
            "link": "https://stackoverflow.com/users/12537783/tester559"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1621790691,
        "accepted_answer_id": 67662500,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621790367,
        "creation_date": 1621789806,
        "question_id": 67662415,
        "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "Working with list inside a Pandas dataframe",
        "body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "answer_body": "<p>I'd iterate through each element in <code>Column1</code>, get its length, save it in a list and then assign it to a new <code>Column2</code>. This would be summarized with:</p>\n<pre><code>df['Column2'] = [len(x) for x in df['Column1']]\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "formatted_input": {
            "qid": 67662415,
            "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
            "question": {
                "title": "Working with list inside a Pandas dataframe",
                "ques_desc": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? "
            },
            "io": [
                "ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n",
                "ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n"
            ],
            "answer": {
                "ans_desc": "I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: ",
                "code": [
                    "df['Column2'] = [len(x) for x in df['Column1']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 209,
            "user_id": 15313738,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/d6SVY.png?s=128&g=1",
            "display_name": "ganbaa",
            "link": "https://stackoverflow.com/users/15313738/ganbaa"
        },
        "is_answered": true,
        "view_count": 82,
        "accepted_answer_id": 67595963,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1621394938,
        "creation_date": 1621388779,
        "last_edit_date": 1621390133,
        "question_id": 67595888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
        "title": "how to extract each numbers from pandas string column to list?",
        "body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "answer_body": "<p>You can use <code>re</code> to find all the occurrences of the numbers either integer or float.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[['11.2', '17', '21'], ['25.2', '4.1', '53', '17', '78'], ['121.1', '14'], ['12']]\n</code></pre>\n<p>If you want, you can type cast them to <code>float</code>/<code>int</code> checking if the extracted string has <code>.</code> in them, something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[[11.2, 17, 21], [25.2, 4.1, 53, 17, 78], [121.1, 14], [12]]\n</code></pre>\n<p>As pointed by @Uts, we can directly call <code>findall</code> over <code>Series.str</code> as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA, listB, listC, listD = df.Column_A.str.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;)\n</code></pre>\n",
        "question_body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "formatted_input": {
            "qid": 67595888,
            "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
            "question": {
                "title": "how to extract each numbers from pandas string column to list?",
                "ques_desc": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: "
            },
            "io": [
                "Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n",
                "listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n"
            ],
            "answer": {
                "ans_desc": "You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: ",
                "code": [
                    "df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n",
                    "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2253,
            "user_id": 11901732,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/RiNDt.jpg?s=128&g=1",
            "display_name": "nilsinelabore",
            "link": "https://stackoverflow.com/users/11901732/nilsinelabore"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 67580063,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1621313364,
        "creation_date": 1621312500,
        "question_id": 67580031,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
        "title": "Groupby, counts in ranges and spread in Pandas",
        "body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "answer_body": "<p>You can use <code>pivot table</code>:</p>\n<pre><code>df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n</code></pre>\n<p><code>OUTPUT</code>:</p>\n<pre><code>bins  (0, 10]  (10, 20]  (20, 30]\nb                                \nAAA         1         0         1\nBBB         1         1         0\nCCC         0         2         0\n</code></pre>\n",
        "question_body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 67580031,
            "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
            "question": {
                "title": "Groupby, counts in ranges and spread in Pandas",
                "ques_desc": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output: "
            },
            "io": [
                "\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n",
                "    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n"
            ],
            "answer": {
                "ans_desc": "You can use : : ",
                "code": [
                    "df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 328,
            "user_id": 2030915,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/e99080a01128b3308d0c34114ff8470c?s=128&d=identicon&r=PG",
            "display_name": "Just_Some_Guy",
            "link": "https://stackoverflow.com/users/2030915/just-some-guy"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 67561636,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1621218414,
        "creation_date": 1621198511,
        "last_edit_date": 1621198851,
        "question_id": 67561501,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
        "title": "splitting list in dataframe columns to separate columns",
        "body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "answer_body": "<p>Here is a way using <code>applymap</code> and <code>map</code>:</p>\n<pre><code>df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n</code></pre>\n",
        "question_body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "formatted_input": {
            "qid": 67561501,
            "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
            "question": {
                "title": "splitting list in dataframe columns to separate columns",
                "ques_desc": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success "
            },
            "io": [
                "    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n",
                "   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n"
            ],
            "answer": {
                "ans_desc": "Here is a way using and : ",
                "code": [
                    "df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 405,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67506886,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1620834577,
        "creation_date": 1620833933,
        "question_id": 67506798,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
        "title": "Reformat Dataframe / Add rows when condition is met",
        "body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "answer_body": "<p>Assuming you only have positive integers in <code>'ColB'</code> You can re-create the DataFrame from scratch using <code>np.repeat</code>. The repeat takes care of the duplication, so we can assign ColB = 1.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n</code></pre>\n<p>Alternatively, if you have a non-duplicated Index, you can repeat that and use <code>loc</code> to get the repitition. Useful when you have more than a single column you want to repeat:</p>\n<pre><code>df = (df.loc[df.index.repeat(df.ColB)]\n        .assign(ColB=1))\n</code></pre>\n<hr />\n<pre><code>         ColA  ColB\n0  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n2  2021-03-10     1\n2  2021-03-10     1\n3  2021-03-10     1\n4  2021-03-10     1\n4  2021-03-10     1\n5  2021-03-11     1\n5  2021-03-11     1\n</code></pre>\n",
        "question_body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "formatted_input": {
            "qid": 67506798,
            "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
            "question": {
                "title": "Reformat Dataframe / Add rows when condition is met",
                "ques_desc": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! "
            },
            "io": [
                "   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n",
                "   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n"
            ],
            "answer": {
                "ans_desc": "Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 15817395,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyRFG42bJfN7WzpcIAmho4ZvAc1tTi_zySFK3_C=k-s128",
            "display_name": "Bernd Blase",
            "link": "https://stackoverflow.com/users/15817395/bernd-blase"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 67361868,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1619999183,
        "creation_date": 1619996303,
        "question_id": 67361824,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
        "title": "Convert dataframe objects to float by iterating over columns",
        "body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "answer_body": "<p>EDIT: Improved regex</p>\n<pre><code># Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n</code></pre>\n<p>Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals.</p>\n<p><strong>Output</strong></p>\n<pre><code>    c1      c2\n0   0.0     75.0\n1   -5.5    65.8\n2   0.0     6.9\n</code></pre>\n<p><strong>Explanation</strong></p>\n<p>We can use regex over complete df, to replace the required symbols, we are replacing <code>%</code> with empty string and if a row consists of <code>-</code> at the end then replace it with 0.0.</p>\n",
        "question_body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 67361824,
            "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
            "question": {
                "title": "Convert dataframe objects to float by iterating over columns",
                "ques_desc": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance "
            },
            "io": [
                "   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n",
                "   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n"
            ],
            "answer": {
                "ans_desc": "EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. ",
                "code": [
                    "# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-apply"
        ],
        "owner": {
            "reputation": 797,
            "user_id": 4014825,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/28ddcf15e4e55ae03685fcecee3c54f9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Inthu",
            "link": "https://stackoverflow.com/users/4014825/inthu"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 67358000,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1619969264,
        "creation_date": 1619967653,
        "last_edit_date": 1619968454,
        "question_id": 67357814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
        "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
        "body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "answer_body": "<p>you can fillna after calculating for the full dataframe and <code>set_axis</code></p>\n<pre><code>out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n</code></pre>\n<hr />\n<pre><code>print(out)\n\n   x   y   z\n0  a   1   2\n1  b   1   2\n2  c   1   2\n3  d  a1  a2\n4  e  b2  b1\n5  f  c3  c4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67357814,
            "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
            "question": {
                "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
                "ques_desc": "I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: "
            },
            "io": [
                "    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n",
                "   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n"
            ],
            "answer": {
                "ans_desc": "you can fillna after calculating for the full dataframe and ",
                "code": [
                    "out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 361,
            "user_id": 15492238,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5bb9116f49e2fd92ba919ad7f1fd1141?s=128&d=identicon&r=PG&f=1",
            "display_name": "tj judge ",
            "link": "https://stackoverflow.com/users/15492238/tj-judge"
        },
        "is_answered": true,
        "view_count": 50,
        "accepted_answer_id": 67336086,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1619843858,
        "creation_date": 1619793380,
        "last_edit_date": 1619843858,
        "question_id": 67335759,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
        "title": "Checking if column headers match PYTHON",
        "body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "answer_body": "<p>You can have access to the column names via <code>.columns</code> and then use set operations to check what you want:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [64],\n        &quot;High&quot;: [66],\n        &quot;Low&quot;: [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [33],\n        &quot;High&quot;: [45],\n        &quot;Volume&quot;: [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns &amp; df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(&quot;Columns only available in df1&quot;, df1_columns_only)\nprint(&quot;Columns only available in df2&quot;, df2_columns_only)\n</code></pre>\n<p>And it gives the expected output:</p>\n<pre><code>Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n</code></pre>\n",
        "question_body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "formatted_input": {
            "qid": 67335759,
            "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
            "question": {
                "title": "Checking if column headers match PYTHON",
                "ques_desc": "I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? "
            },
            "io": [
                "      ID  Open High Low  \n       1  64   66   52   \n",
                "      ID Open High  Volume\n      1   33   45   30043\n"
            ],
            "answer": {
                "ans_desc": "You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: ",
                "code": [
                    "import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [64],\n        \"High\": [66],\n        \"Low\": [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [33],\n        \"High\": [45],\n        \"Volume\": [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns & df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(\"Columns only available in df1\", df1_columns_only)\nprint(\"Columns only available in df2\", df2_columns_only)\n",
                    "Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 7028,
            "user_id": 1452759,
            "user_type": "registered",
            "accept_rate": 73,
            "profile_image": "https://www.gravatar.com/avatar/724bd9ac38fdf069f0b869b70ff0e753?s=128&d=identicon&r=PG",
            "display_name": "user1452759",
            "link": "https://stackoverflow.com/users/1452759/user1452759"
        },
        "is_answered": true,
        "view_count": 402423,
        "protected_date": 1562192230,
        "accepted_answer_id": 26838140,
        "answer_count": 8,
        "score": 291,
        "last_activity_date": 1619763697,
        "creation_date": 1415600966,
        "last_edit_date": 1540067939,
        "question_id": 26837998,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
        "title": "Pandas Replace NaN with blank/empty string",
        "body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "answer_body": "<pre><code>import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n</code></pre>\n\n<p>This might help. It will replace all NaNs with an empty string.</p>\n",
        "question_body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "formatted_input": {
            "qid": 26837998,
            "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
            "question": {
                "title": "Pandas Replace NaN with blank/empty string",
                "ques_desc": "I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: "
            },
            "io": [
                "    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n",
                "    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n"
            ],
            "answer": {
                "ans_desc": " This might help. It will replace all NaNs with an empty string. ",
                "code": [
                    "import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15778404,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GimYCkfz3pFkj7O8AJd9IIO7bm3ktBeyl9eehMGGQ=k-s128",
            "display_name": "rafazamp",
            "link": "https://stackoverflow.com/users/15778404/rafazamp"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67288740,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1619551535,
        "creation_date": 1619545844,
        "last_edit_date": 1619548270,
        "question_id": 67288220,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition",
        "title": "How can I add a new line in pandas dataframe based in a condition?",
        "body": "<p>I have this Dataframe that is populated from a file.\nThe first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition.</p>\n<pre><code>[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n</code></pre>\n<p>Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this..\nTaking for example the lines number 4 and 5:</p>\n<pre><code>  1     90  3\n  1    370  3\n 370 - 90 = 260 (260 is greater than 100)\n</code></pre>\n<p>So I need to add a new line with the last number + 100, and the last column needs to be zero:</p>\n<pre><code>  1     90  3\n  1    190  0\n  1    370  3\n</code></pre>\n<p>Any ideas how can I achieve that?\nThanks in advance.</p>\n<p>Edit: I just need to add the line once in the DataFrame.</p>\n",
        "answer_body": "<p>Try:</p>\n<pre><code>m = df[&quot;[2]&quot;].diff() &gt; 100\n\ndf.loc[m, &quot;[2]&quot;] = pd.Series(\n    [\n        [str(df.iloc[v - 1][&quot;[2]&quot;] + 100), df.iloc[v][&quot;[2]&quot;]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(&quot;[2]&quot;)\ndf[&quot;[3]&quot;] = np.where(\n    df[&quot;[2]&quot;].apply(lambda x: isinstance(x, str)), 0, df[&quot;[3]&quot;]\n)\ndf[&quot;[2]&quot;] = df[&quot;[2]&quot;].astype(int)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>    [1]    [2]  [3]\n0     1     30    2\n1     1     30    1\n2     1     30    3\n3     1     90    3\n4     1    190    0\n4     1    370    3\n5     1    430    3\n6     1    530    0\n6     1    705    3\n7     1    805    3\n8     1    880    2\n9     1    905    3\n10    1   1005    3\n11    1   1105    0\n11    1   1170    3\n12    1   1230    3\n13    1   1330    0\n13    1   1970    3\n14    1   2030    3\n15    1   2130    0\n15    1   2970    3\n16    1   3030    3\n17    1   3130    0\n17    1   3970    3\n18    1   4030    3\n19    1   4130    0\n19    1   4423    3\n20    1   4523    0\n20    1   4539    3\n21    1   4575    3\n22    1   4630    2\n23    1   4635    3\n24    1   4671    3\n25    1   4771    0\n25    1   4787    3\n26    1   4887    0\n26    1   4957    3\n27    1   5057    3\n28    1   5157    0\n28    1   5270    3\n29    1   5330    3\n30    1   5430    0\n30    1   5970    3\n31    1   6030    3\n32    1   6130    0\n32    1   6970    3\n33    1   7030    3\n34    1   7130    0\n34    1   7970    3\n35    1   8030    3\n36    1   8130    0\n36    1   8158    3\n37    1   8257    3\n38    1   8332    2\n39    1   8357    3\n40    1   8457    3\n41    1   8557    0\n41    1   8970    3\n42    1   9030    3\n43    1   9130    0\n43    1   9970    3\n44    1  10030    3\n45    1  10130    0\n45    1  10970    3\n46    1  11030    3\n47    1  11130    0\n47    1  11470    3\n48    1  11530    3\n49    1  11630    0\n49    1  11853    3\n50    1  11953    3\n</code></pre>\n<hr />\n<p>EDIT: To change only one value:</p>\n<pre><code>mask = df[&quot;[2]&quot;].diff() &gt; 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, &quot;[2]&quot;] = pd.Series(\n        [\n            [str(df.iloc[v - 1][&quot;[2]&quot;] + 100), df.iloc[v][&quot;[2]&quot;]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(&quot;[2]&quot;)\n    df[&quot;[3]&quot;] = np.where(\n        df[&quot;[2]&quot;].apply(lambda x: isinstance(x, str)), 0, df[&quot;[3]&quot;]\n    )\n    df[&quot;[2]&quot;] = df[&quot;[2]&quot;].astype(int)\n    print(df)\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe that is populated from a file.\nThe first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition.</p>\n<pre><code>[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n</code></pre>\n<p>Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this..\nTaking for example the lines number 4 and 5:</p>\n<pre><code>  1     90  3\n  1    370  3\n 370 - 90 = 260 (260 is greater than 100)\n</code></pre>\n<p>So I need to add a new line with the last number + 100, and the last column needs to be zero:</p>\n<pre><code>  1     90  3\n  1    190  0\n  1    370  3\n</code></pre>\n<p>Any ideas how can I achieve that?\nThanks in advance.</p>\n<p>Edit: I just need to add the line once in the DataFrame.</p>\n",
        "formatted_input": {
            "qid": 67288220,
            "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition",
            "question": {
                "title": "How can I add a new line in pandas dataframe based in a condition?",
                "ques_desc": "I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame. "
            },
            "io": [
                "[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n",
                "  1     90  3\n  1    190  0\n  1    370  3\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: EDIT: To change only one value: ",
                "code": [
                    "m = df[\"[2]\"].diff() > 100\n\ndf.loc[m, \"[2]\"] = pd.Series(\n    [\n        [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(\"[2]\")\ndf[\"[3]\"] = np.where(\n    df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n)\ndf[\"[2]\"] = df[\"[2]\"].astype(int)\nprint(df)\n",
                    "mask = df[\"[2]\"].diff() > 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, \"[2]\"] = pd.Series(\n        [\n            [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(\"[2]\")\n    df[\"[3]\"] = np.where(\n        df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n    )\n    df[\"[2]\"] = df[\"[2]\"].astype(int)\n    print(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 15415267,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/021c1306999edefd832f61302fe04f61?s=128&d=identicon&r=PG&f=1",
            "display_name": "muratmert41",
            "link": "https://stackoverflow.com/users/15415267/muratmert41"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 67258039,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1619423161,
        "creation_date": 1619381990,
        "last_edit_date": 1619423161,
        "question_id": 67257898,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
        "title": "How to add a value to a new column by referencing the values in a column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n</code></pre>\n<p>The <strong>xy</strong> column must be filled with the value of the column names in the <strong>reason</strong> column. Let's look at the first row. The <strong>reason column</strong> shows our value <strong>x1</strong>. So our value in column <strong>xy</strong>, will be the value of x1 column in the first row. Like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n</code></pre>\n<p>Is there a way to do this?</p>\n",
        "answer_body": "<pre><code>df[&quot;xy&quot;] = df.apply(lambda x: x[x[&quot;reason&quot;]], axis=1)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   id reason   x1  x2  x3  x4  x5   xy\n0   1     x1  100  15  10  20  25  100\n1   2     x1   15  16  14  10  10   15\n2   3     x4   10  50  40  30  25   30\n3   4     x3   12  15  60   5   1   60\n4   5     x1   80  15  10  20  25   80\n5   6     x1   15  19  84  10  10   15\n6   7     x4   90  40  90  30  25   30\n7   8     x4   12  85  60  50  10   50\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n</code></pre>\n<p>The <strong>xy</strong> column must be filled with the value of the column names in the <strong>reason</strong> column. Let's look at the first row. The <strong>reason column</strong> shows our value <strong>x1</strong>. So our value in column <strong>xy</strong>, will be the value of x1 column in the first row. Like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n</code></pre>\n<p>Is there a way to do this?</p>\n",
        "formatted_input": {
            "qid": 67257898,
            "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
            "question": {
                "title": "How to add a value to a new column by referencing the values in a column",
                "ques_desc": "I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? "
            },
            "io": [
                "id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n",
                "id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n"
            ],
            "answer": {
                "ans_desc": " Prints: ",
                "code": [
                    "df[\"xy\"] = df.apply(lambda x: x[x[\"reason\"]], axis=1)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15762077,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/853af71cd55ed204dddebe32bbc8a571?s=128&d=identicon&r=PG&f=1",
            "display_name": "kdfjlasjdflaj",
            "link": "https://stackoverflow.com/users/15762077/kdfjlasjdflaj"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 67255996,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1619369716,
        "creation_date": 1619368167,
        "last_edit_date": 1619369473,
        "question_id": 67255732,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe",
        "title": "Summation of operation in dataframe",
        "body": "<p>I want to implement a function that does the operation that you can see in the image:</p>\n<p><img src=\"https://i.stack.imgur.com/ocLbz.png\" alt=\"image\" /></p>\n<p>But i not sure how to implement the Summation for the moment i doing something like that:</p>\n<pre><code>def df_operation(df1: pd.DataFrame, df2: pd.DataFrame) -&gt; float:\n    return ((((abs(df1 - df2)).sum())/((df1+df2).sum()))*100)\n</code></pre>\n<p>And the problem is in the summation. If someone can help me.</p>\n<p>For example, i have two dataframes like that:</p>\n<pre><code>dataframe1 = pd.DataFrame({&quot;A&quot;:[8, 2],   \n               &quot;B&quot;:[26, 19]}) \n\ndataframe2 = pd.DataFrame({&quot;A&quot;:[3,6],   \n               &quot;B&quot;:[12,17]})  \n</code></pre>\n<p>Where for the abs operation we will obtain this:</p>\n<pre><code>   A  B\n0  5  14\n1  4  2\n</code></pre>\n<p>And for the sum:</p>\n<pre><code>   A   B\n0  11  38\n1  8   36\n</code></pre>\n<p>Finally we do the summation:</p>\n<pre><code>(25/93) * 100\n</code></pre>\n<p>where <code>25 = 5+14+4+2</code> and <code>93 = 11+8+38+36</code></p>\n",
        "answer_body": "<p>Use <code>.sum().sum()</code> to sum the dataframe across columns/rows:</p>\n<pre><code>result = (\n    dataframe1.sub(dataframe2).abs().sum().sum()\n    / dataframe1.add(dataframe2).sum().sum()\n) * 100\nprint(result)\n</code></pre>\n<p>Prints:</p>\n<pre><code>26.881720430107524\n</code></pre>\n",
        "question_body": "<p>I want to implement a function that does the operation that you can see in the image:</p>\n<p><img src=\"https://i.stack.imgur.com/ocLbz.png\" alt=\"image\" /></p>\n<p>But i not sure how to implement the Summation for the moment i doing something like that:</p>\n<pre><code>def df_operation(df1: pd.DataFrame, df2: pd.DataFrame) -&gt; float:\n    return ((((abs(df1 - df2)).sum())/((df1+df2).sum()))*100)\n</code></pre>\n<p>And the problem is in the summation. If someone can help me.</p>\n<p>For example, i have two dataframes like that:</p>\n<pre><code>dataframe1 = pd.DataFrame({&quot;A&quot;:[8, 2],   \n               &quot;B&quot;:[26, 19]}) \n\ndataframe2 = pd.DataFrame({&quot;A&quot;:[3,6],   \n               &quot;B&quot;:[12,17]})  \n</code></pre>\n<p>Where for the abs operation we will obtain this:</p>\n<pre><code>   A  B\n0  5  14\n1  4  2\n</code></pre>\n<p>And for the sum:</p>\n<pre><code>   A   B\n0  11  38\n1  8   36\n</code></pre>\n<p>Finally we do the summation:</p>\n<pre><code>(25/93) * 100\n</code></pre>\n<p>where <code>25 = 5+14+4+2</code> and <code>93 = 11+8+38+36</code></p>\n",
        "formatted_input": {
            "qid": 67255732,
            "link": "https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe",
            "question": {
                "title": "Summation of operation in dataframe",
                "ques_desc": "I want to implement a function that does the operation that you can see in the image: But i not sure how to implement the Summation for the moment i doing something like that: And the problem is in the summation. If someone can help me. For example, i have two dataframes like that: Where for the abs operation we will obtain this: And for the sum: Finally we do the summation: where and "
            },
            "io": [
                "   A  B\n0  5  14\n1  4  2\n",
                "   A   B\n0  11  38\n1  8   36\n"
            ],
            "answer": {
                "ans_desc": "Use to sum the dataframe across columns/rows: Prints: ",
                "code": [
                    "result = (\n    dataframe1.sub(dataframe2).abs().sum().sum()\n    / dataframe1.add(dataframe2).sum().sum()\n) * 100\nprint(result)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 772,
            "user_id": 5356096,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/sKX9g.png?s=128&g=1",
            "display_name": "Jack Avante",
            "link": "https://stackoverflow.com/users/5356096/jack-avante"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 67245812,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1619348036,
        "creation_date": 1619269385,
        "last_edit_date": 1619286390,
        "question_id": 67243081,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas",
        "title": "Best way to change column data for all rows over multiple dataframes in pandas?",
        "body": "<p>Consider dataframes <code>df1</code>, <code>df2</code>, and <code>df3</code>.</p>\n<p><code>df1</code> and <code>df2</code> have an <code>id</code> column, and <code>df3</code> has a <code>from_id</code> and <code>to_id</code> column.</p>\n<p>I need to iterate over all rows of <code>df3</code>, and replace <code>from_id</code> and <code>to_id</code> with new unique randomly generated UUIDs, and then update those in <code>df1</code> and <code>df2</code> where <code>(id == from_id) | (id == to_id)</code> (before the change to UUID).</p>\n<p>I originally wanted to iterate over all rows of <code>df3</code> and simply check both <code>df1</code> and <code>df2</code> if they contain the original <code>from_id</code> or <code>to_id</code> inside the <code>id</code> column before replacing both, but <a href=\"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\">I found that iterating over pandas rows is a bad idea and slow.</a></p>\n<p>I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes.</p>\n<p>My current method that I believe to be slow and inefficient:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid1()\n\ndef update_ids(df_places: pd.DataFrame, df_transitions: pd.DataFrame, df_arcs: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    for i in range(len(df_arcs)):\n        new_uuid_from = __rand_uuid()\n        new_uuid_to = __rand_uuid()\n        new_uuid_arc = __rand_uuid()\n\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_places.loc[df_places.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_places.loc[df_places.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_arcs.iloc[i]['sourceId'] = new_uuid_from\n        df_arcs.iloc[i]['destinationId'] = new_uuid_to\n        df_arcs.iloc[i]['id'] = new_uuid_arc\n\n    return df_places, df_transitions, df_arcs\n</code></pre>\n<p>Here <code>df_places</code> and <code>df_transitions</code> are above mentioned <code>df1</code> and <code>df2</code>, and <code>df_arcs</code> is <code>df3</code></p>\n<p>Example <code>df_places</code></p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_transitions</code>:</p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_arcs</code>:</p>\n<pre><code>+---+----------+---------------+\n|   | sourceId | destinationId |\n+---+----------+---------------+\n| 1 | a1       | b1            |\n+---+----------+---------------+\n| 2 | b1       | c1            |\n+---+----------+---------------+\n</code></pre>\n",
        "answer_body": "<p>A very simple approach:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n</code></pre>\n",
        "question_body": "<p>Consider dataframes <code>df1</code>, <code>df2</code>, and <code>df3</code>.</p>\n<p><code>df1</code> and <code>df2</code> have an <code>id</code> column, and <code>df3</code> has a <code>from_id</code> and <code>to_id</code> column.</p>\n<p>I need to iterate over all rows of <code>df3</code>, and replace <code>from_id</code> and <code>to_id</code> with new unique randomly generated UUIDs, and then update those in <code>df1</code> and <code>df2</code> where <code>(id == from_id) | (id == to_id)</code> (before the change to UUID).</p>\n<p>I originally wanted to iterate over all rows of <code>df3</code> and simply check both <code>df1</code> and <code>df2</code> if they contain the original <code>from_id</code> or <code>to_id</code> inside the <code>id</code> column before replacing both, but <a href=\"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\">I found that iterating over pandas rows is a bad idea and slow.</a></p>\n<p>I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes.</p>\n<p>My current method that I believe to be slow and inefficient:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid1()\n\ndef update_ids(df_places: pd.DataFrame, df_transitions: pd.DataFrame, df_arcs: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    for i in range(len(df_arcs)):\n        new_uuid_from = __rand_uuid()\n        new_uuid_to = __rand_uuid()\n        new_uuid_arc = __rand_uuid()\n\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_places.loc[df_places.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_places.loc[df_places.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_arcs.iloc[i]['sourceId'] = new_uuid_from\n        df_arcs.iloc[i]['destinationId'] = new_uuid_to\n        df_arcs.iloc[i]['id'] = new_uuid_arc\n\n    return df_places, df_transitions, df_arcs\n</code></pre>\n<p>Here <code>df_places</code> and <code>df_transitions</code> are above mentioned <code>df1</code> and <code>df2</code>, and <code>df_arcs</code> is <code>df3</code></p>\n<p>Example <code>df_places</code></p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_transitions</code>:</p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_arcs</code>:</p>\n<pre><code>+---+----------+---------------+\n|   | sourceId | destinationId |\n+---+----------+---------------+\n| 1 | a1       | b1            |\n+---+----------+---------------+\n| 2 | b1       | c1            |\n+---+----------+---------------+\n</code></pre>\n",
        "formatted_input": {
            "qid": 67243081,
            "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas",
            "question": {
                "title": "Best way to change column data for all rows over multiple dataframes in pandas?",
                "ques_desc": "Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example : "
            },
            "io": [
                "+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n",
                "+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n"
            ],
            "answer": {
                "ans_desc": "A very simple approach: ",
                "code": [
                    "import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 359,
            "user_id": 8546104,
            "user_type": "registered",
            "accept_rate": 78,
            "profile_image": "https://www.gravatar.com/avatar/d33f56c42c6594e69a0d6e8a83e5b91b?s=128&d=identicon&r=PG&f=1",
            "display_name": "codeDojo",
            "link": "https://stackoverflow.com/users/8546104/codedojo"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 67246882,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1619295624,
        "creation_date": 1619293827,
        "question_id": 67246859,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id",
        "title": "How to convert rows into columns and filter using the ID",
        "body": "<p>I have a CSV file that looks like this:</p>\n<pre><code>customer_id |  key_id.  |  quantity |\n1           |    777    |    3      |\n1           |    888    |    2      |\n1           |    999    |    3      |\n2           |    777    |    6      |\n2           |    888    |    1      |\n</code></pre>\n<p>and I would like to use simple python or pandas to:</p>\n<ol>\n<li>Make each unique customer id in a separate row</li>\n<li>convert key_id to the columns titles and the values are the quantity</li>\n</ol>\n<p>The output table should look like this:</p>\n<pre><code>            |  777    |  888  |   999  | \n1           |   3     |   2   |    3   |\n2           |   6     |   1   |    0   |\n</code></pre>\n<p>I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips?</p>\n",
        "answer_body": "<p>You can pivot into <code>key_id</code> columns using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html\" rel=\"nofollow noreferrer\"><strong><code>pivot_table()</code></strong></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.pivot_table(index='customer_id', columns='key_id', values='quantity').fillna(0)\n\n# key_id       777  888  999\n# customer_id               \n# 1            3.0  2.0  3.0\n# 2            6.0  1.0  0.0\n</code></pre>\n<hr />\n<p>To handle duplicates, <code>pivot_table()</code> averages them by default. To override this aggregation method, you can set the <code>aggfunc</code> param (<code>max</code>, <code>min</code>, <code>first</code>, <code>last</code>, <code>sum</code>, etc.):</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.pivot_table(\n    index='customer_id',\n    columns='key_id',\n    values='quantity',\n    aggfunc='max',\n).fillna(0)\n</code></pre>\n",
        "question_body": "<p>I have a CSV file that looks like this:</p>\n<pre><code>customer_id |  key_id.  |  quantity |\n1           |    777    |    3      |\n1           |    888    |    2      |\n1           |    999    |    3      |\n2           |    777    |    6      |\n2           |    888    |    1      |\n</code></pre>\n<p>and I would like to use simple python or pandas to:</p>\n<ol>\n<li>Make each unique customer id in a separate row</li>\n<li>convert key_id to the columns titles and the values are the quantity</li>\n</ol>\n<p>The output table should look like this:</p>\n<pre><code>            |  777    |  888  |   999  | \n1           |   3     |   2   |    3   |\n2           |   6     |   1   |    0   |\n</code></pre>\n<p>I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips?</p>\n",
        "formatted_input": {
            "qid": 67246859,
            "link": "https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id",
            "question": {
                "title": "How to convert rows into columns and filter using the ID",
                "ques_desc": "I have a CSV file that looks like this: and I would like to use simple python or pandas to: Make each unique customer id in a separate row convert key_id to the columns titles and the values are the quantity The output table should look like this: I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips? "
            },
            "io": [
                "customer_id |  key_id.  |  quantity |\n1           |    777    |    3      |\n1           |    888    |    2      |\n1           |    999    |    3      |\n2           |    777    |    6      |\n2           |    888    |    1      |\n",
                "            |  777    |  888  |   999  | \n1           |   3     |   2   |    3   |\n2           |   6     |   1   |    0   |\n"
            ],
            "answer": {
                "ans_desc": "You can pivot into columns using : To handle duplicates, averages them by default. To override this aggregation method, you can set the param (, , , , , etc.): ",
                "code": [
                    "df.pivot_table(\n    index='customer_id',\n    columns='key_id',\n    values='quantity',\n    aggfunc='max',\n).fillna(0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 17,
            "user_id": 14714831,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/af4c1da9e055882b5ec057200d936150?s=128&d=identicon&r=PG&f=1",
            "display_name": "csantos",
            "link": "https://stackoverflow.com/users/14714831/csantos"
        },
        "is_answered": true,
        "view_count": 50,
        "accepted_answer_id": 67215226,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1619110665,
        "creation_date": 1619097058,
        "question_id": 67213950,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf",
        "title": "How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?",
        "body": "<p>I have two dataframes:</p>\n<p>df1</p>\n<pre><code>     Code     Number\n0   ABC123      1\n1   DEF456      2\n2   GHI789      3\n3   DEA456      4\n</code></pre>\n<p>df2</p>\n<pre><code>     Code \n0   ABD123\n1   DEA458\n2   GHI789\n</code></pre>\n<p>df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be:</p>\n<p>ABD123 = 1 (because it has 1 different character from ABC123)</p>\n<p>DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one)</p>\n<p>GHI789 = 3 (because it has an equivalent at df1)</p>\n<p>I know how to check for the differences of each code individually and save the &quot;length&quot; of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way?</p>\n",
        "answer_body": "<blockquote>\n<p>don't know how to compare each row from df2 against all rows from df1.</p>\n</blockquote>\n<p>Nested loops will work.  If you had a function named <code>compare</code> it would look like this...</p>\n<pre><code>for index2, row2 in df2.iterrows():\n    for index1, row1 in df1.iterrows():\n        difference = compare(row2,row1)\n        #do something with the difference.\n</code></pre>\n<hr />\n<p>Nested loops are usually not ideal when working with Pandas or Numpy but they do work.  There may be better solutions.</p>\n<hr />\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html\" rel=\"nofollow noreferrer\" title=\"Pandas docs\">DataFrame.iterrows()</a></p>\n",
        "question_body": "<p>I have two dataframes:</p>\n<p>df1</p>\n<pre><code>     Code     Number\n0   ABC123      1\n1   DEF456      2\n2   GHI789      3\n3   DEA456      4\n</code></pre>\n<p>df2</p>\n<pre><code>     Code \n0   ABD123\n1   DEA458\n2   GHI789\n</code></pre>\n<p>df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be:</p>\n<p>ABD123 = 1 (because it has 1 different character from ABC123)</p>\n<p>DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one)</p>\n<p>GHI789 = 3 (because it has an equivalent at df1)</p>\n<p>I know how to check for the differences of each code individually and save the &quot;length&quot; of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way?</p>\n",
        "formatted_input": {
            "qid": 67213950,
            "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf",
            "question": {
                "title": "How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?",
                "ques_desc": "I have two dataframes: df1 df2 df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be: ABD123 = 1 (because it has 1 different character from ABC123) DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one) GHI789 = 3 (because it has an equivalent at df1) I know how to check for the differences of each code individually and save the \"length\" of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way? "
            },
            "io": [
                "     Code     Number\n0   ABC123      1\n1   DEF456      2\n2   GHI789      3\n3   DEA456      4\n",
                "     Code \n0   ABD123\n1   DEA458\n2   GHI789\n"
            ],
            "answer": {
                "ans_desc": " don't know how to compare each row from df2 against all rows from df1. Nested loops will work. If you had a function named it would look like this... Nested loops are usually not ideal when working with Pandas or Numpy but they do work. There may be better solutions. DataFrame.iterrows() ",
                "code": [
                    "for index2, row2 in df2.iterrows():\n    for index1, row1 in df1.iterrows():\n        difference = compare(row2,row1)\n        #do something with the difference.\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 565,
            "user_id": 13595636,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/98b802bd50d0b4dc923fcf51b1af45cd?s=128&d=identicon&r=PG&f=1",
            "display_name": "Abhinav Dhiman",
            "link": "https://stackoverflow.com/users/13595636/abhinav-dhiman"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 67181547,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1618931102,
        "creation_date": 1618922948,
        "question_id": 67179293,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67179293/moving-data-from-rows-to-columns-based-on-another-column",
        "title": "Moving data from rows to columns based on another column",
        "body": "<p>I have a huge dataset with contents such as given below:</p>\n<pre><code>+------+------------------------------------------------------------------+----------------------------------+--+\n| HHID |                             VAL_CD64                             |             VAL_CD32             |  |\n+------+------------------------------------------------------------------+----------------------------------+--+\n|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 373aeda34c0b4ab91a02ecf55af58e15 |  |\n|  203 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | 6f3606577eadacef1b956307558a1efd |  |\n|  203 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 332321ab150879e930869c15b1d10c83 |  |\n|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | 2c4f97a04f02db5a36a85f48dab39b5b |  |\n|  720 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | 2111293e946703652070968b224875c9 |  |\n|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 5c80a555fcda02d028fc60afa29c4a40 |  |\n|  348 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 6c10cd11b805fa57d2ca36df91654576 |  |\n|  348 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 6040b29107adf1a41c4f5964e0ff6dcb |  |\n|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | 71a91c4768bd314f3c9dc74e9c7937e8 |  |\n+------+------------------------------------------------------------------+----------------------------------+--+\n</code></pre>\n<p>HHID can be present in the file at a maximum of three times. If HHID is found once, then the VAL_CD64/VAL_CD32 should be moved to VAL1_CD64/VAL1_CD32 columns, if found 2nd time, second value should be moved to VAL2_CD64/VAL2_CD32 columns, and if found 3rd time, third value should be moved to VAL3_CD64/VAL3_CD32 columns. If value is not found, then these columns should be left blank.</p>\n<p>Output should look something like this:</p>\n<pre><code>+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n| HHID |                            VAL1_CD64                             |                            VAL2_CD64                             |                            VAL3_CD64                             |            VAL1_CD32             |            VAL2_CD32             |            VAL3_CD32             |  |\n+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 373aeda34c0b4ab91a02ecf55af58e15 | 6f3606577eadacef1b956307558a1efd | 332321ab150879e930869c15b1d10c83 |  |\n|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 |                                                                  | 2c4f97a04f02db5a36a85f48dab39b5b | 2111293e946703652070968b224875c9 |                                  |  |\n|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 5c80a555fcda02d028fc60afa29c4a40 | 6c10cd11b805fa57d2ca36df91654576 | 6040b29107adf1a41c4f5964e0ff6dcb |  |\n|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 |                                                                  |                                                                  | 71a91c4768bd314f3c9dc74e9c7937e8 |                                  |                                  |  |\n+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n</code></pre>\n<p>I tried using pivot/melt in pandas but unable to get an idea to implement it. Can anyone help in giving me a lead?</p>\n<p>Thanks</p>\n",
        "answer_body": "<p>One possible way is to combine <code>VAL_CD32</code> and <code>VAL_CD64</code> into list then split those list into columns:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df_ = df.groupby('HHID').agg({'VAL_CD32': list, 'VAL_CD64': list})\n\ndata = []\nfor col in df_.columns:\n    d = pd.DataFrame(df_[col].values.tolist(), index=df_.index)\n    d.columns = [f'{col}_{i}' for i in map(str, range(1, len(d.columns)+1))]\n    data.append(d)\n\nres = pd.concat(data, axis=1)\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>                            VAL_CD32_1                        VAL_CD32_2  \\\nHHID\n203   373aeda34c0b4ab91a02ecf55af58e15  6f3606577eadacef1b956307558a1efd\n348   5c80a555fcda02d028fc60afa29c4a40  6c10cd11b805fa57d2ca36df91654576\n403   71a91c4768bd314f3c9dc74e9c7937e8                              None\n720   2c4f97a04f02db5a36a85f48dab39b5b  2111293e946703652070968b224875c9\n\n                            VAL_CD32_3  \\\nHHID\n203   332321ab150879e930869c15b1d10c83\n348   6040b29107adf1a41c4f5964e0ff6dcb\n403                               None\n720                               None\n\n                                                            VAL_CD64_1  \\\nHHID\n203   8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5\n348   25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496\n403   3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636\n720   f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751\n\n                                                            VAL_CD64_2  \\\nHHID\n203   0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8\n348   67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9\n403                                                               None\n720   abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3\n\n                                                            VAL_CD64_3\nHHID\n203   a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7\n348   05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37\n403                                                               None\n720                                                               None\n</code></pre>\n",
        "question_body": "<p>I have a huge dataset with contents such as given below:</p>\n<pre><code>+------+------------------------------------------------------------------+----------------------------------+--+\n| HHID |                             VAL_CD64                             |             VAL_CD32             |  |\n+------+------------------------------------------------------------------+----------------------------------+--+\n|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 373aeda34c0b4ab91a02ecf55af58e15 |  |\n|  203 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | 6f3606577eadacef1b956307558a1efd |  |\n|  203 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 332321ab150879e930869c15b1d10c83 |  |\n|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | 2c4f97a04f02db5a36a85f48dab39b5b |  |\n|  720 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | 2111293e946703652070968b224875c9 |  |\n|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 5c80a555fcda02d028fc60afa29c4a40 |  |\n|  348 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 6c10cd11b805fa57d2ca36df91654576 |  |\n|  348 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 6040b29107adf1a41c4f5964e0ff6dcb |  |\n|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | 71a91c4768bd314f3c9dc74e9c7937e8 |  |\n+------+------------------------------------------------------------------+----------------------------------+--+\n</code></pre>\n<p>HHID can be present in the file at a maximum of three times. If HHID is found once, then the VAL_CD64/VAL_CD32 should be moved to VAL1_CD64/VAL1_CD32 columns, if found 2nd time, second value should be moved to VAL2_CD64/VAL2_CD32 columns, and if found 3rd time, third value should be moved to VAL3_CD64/VAL3_CD32 columns. If value is not found, then these columns should be left blank.</p>\n<p>Output should look something like this:</p>\n<pre><code>+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n| HHID |                            VAL1_CD64                             |                            VAL2_CD64                             |                            VAL3_CD64                             |            VAL1_CD32             |            VAL2_CD32             |            VAL3_CD32             |  |\n+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 373aeda34c0b4ab91a02ecf55af58e15 | 6f3606577eadacef1b956307558a1efd | 332321ab150879e930869c15b1d10c83 |  |\n|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 |                                                                  | 2c4f97a04f02db5a36a85f48dab39b5b | 2111293e946703652070968b224875c9 |                                  |  |\n|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 5c80a555fcda02d028fc60afa29c4a40 | 6c10cd11b805fa57d2ca36df91654576 | 6040b29107adf1a41c4f5964e0ff6dcb |  |\n|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 |                                                                  |                                                                  | 71a91c4768bd314f3c9dc74e9c7937e8 |                                  |                                  |  |\n+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n</code></pre>\n<p>I tried using pivot/melt in pandas but unable to get an idea to implement it. Can anyone help in giving me a lead?</p>\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 67179293,
            "link": "https://stackoverflow.com/questions/67179293/moving-data-from-rows-to-columns-based-on-another-column",
            "question": {
                "title": "Moving data from rows to columns based on another column",
                "ques_desc": "I have a huge dataset with contents such as given below: HHID can be present in the file at a maximum of three times. If HHID is found once, then the VAL_CD64/VAL_CD32 should be moved to VAL1_CD64/VAL1_CD32 columns, if found 2nd time, second value should be moved to VAL2_CD64/VAL2_CD32 columns, and if found 3rd time, third value should be moved to VAL3_CD64/VAL3_CD32 columns. If value is not found, then these columns should be left blank. Output should look something like this: I tried using pivot/melt in pandas but unable to get an idea to implement it. Can anyone help in giving me a lead? Thanks "
            },
            "io": [
                "+------+------------------------------------------------------------------+----------------------------------+--+\n| HHID |                             VAL_CD64                             |             VAL_CD32             |  |\n+------+------------------------------------------------------------------+----------------------------------+--+\n|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 373aeda34c0b4ab91a02ecf55af58e15 |  |\n|  203 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | 6f3606577eadacef1b956307558a1efd |  |\n|  203 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 332321ab150879e930869c15b1d10c83 |  |\n|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | 2c4f97a04f02db5a36a85f48dab39b5b |  |\n|  720 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | 2111293e946703652070968b224875c9 |  |\n|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 5c80a555fcda02d028fc60afa29c4a40 |  |\n|  348 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 6c10cd11b805fa57d2ca36df91654576 |  |\n|  348 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 6040b29107adf1a41c4f5964e0ff6dcb |  |\n|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | 71a91c4768bd314f3c9dc74e9c7937e8 |  |\n+------+------------------------------------------------------------------+----------------------------------+--+\n",
                "+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n| HHID |                            VAL1_CD64                             |                            VAL2_CD64                             |                            VAL3_CD64                             |            VAL1_CD32             |            VAL2_CD32             |            VAL3_CD32             |  |\n+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n|  203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 373aeda34c0b4ab91a02ecf55af58e15 | 6f3606577eadacef1b956307558a1efd | 332321ab150879e930869c15b1d10c83 |  |\n|  720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 |                                                                  | 2c4f97a04f02db5a36a85f48dab39b5b | 2111293e946703652070968b224875c9 |                                  |  |\n|  348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 5c80a555fcda02d028fc60afa29c4a40 | 6c10cd11b805fa57d2ca36df91654576 | 6040b29107adf1a41c4f5964e0ff6dcb |  |\n|  403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 |                                                                  |                                                                  | 71a91c4768bd314f3c9dc74e9c7937e8 |                                  |                                  |  |\n+------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+\n"
            ],
            "answer": {
                "ans_desc": "One possible way is to combine and into list then split those list into columns: ",
                "code": [
                    "df_ = df.groupby('HHID').agg({'VAL_CD32': list, 'VAL_CD64': list})\n\ndata = []\nfor col in df_.columns:\n    d = pd.DataFrame(df_[col].values.tolist(), index=df_.index)\n    d.columns = [f'{col}_{i}' for i in map(str, range(1, len(d.columns)+1))]\n    data.append(d)\n\nres = pd.concat(data, axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "mapping"
        ],
        "owner": {
            "reputation": 348,
            "user_id": 11932905,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/08225222c1c600624ca663a6203822fa?s=128&d=identicon&r=PG&f=1",
            "display_name": "Alex_Y",
            "link": "https://stackoverflow.com/users/11932905/alex-y"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 67111903,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1618510666,
        "creation_date": 1618501834,
        "last_edit_date": 1618508519,
        "question_id": 67111775,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67111775/python-pandas-check-each-element-in-list-values-of-column-to-exist-in-other-da",
        "title": "Python, Pandas: check each element in list values of column to exist in other dataframe",
        "body": "<p>I have dataframe column with values in lists, want to add new column with filtered values from list if they are in other dataframe.</p>\n<p>df:</p>\n<pre><code>df = pd.DataFrame({'a':[1,2,5,7,9],'b':[[10,1,'xxx'],[],[1,2,3],[5],[25,27]]})\n</code></pre>\n<pre><code>**a**|**b**\n:-----:|:-----:\n1|[10, 1, 'xxx']\n2|[]\n5|[1, 2, 3]\n7|[5]\n9|[25, 27]\n</code></pre>\n<p>df2:</p>\n<pre><code>df2 = pd.DataFrame({'d':[324,21,4353,345,4535,23],'e':[5,1,23,25,25,'xxx']})\n</code></pre>\n<p>I need to add new column with filtered column <code>b</code> in <code>df</code> so that it contains lists with only elements which are in <code>df2</code> column <code>e</code>.</p>\n<p>Result:</p>\n<pre><code>**a**|**b**|**c**\n:-----:|:-----:|:-----:\n1|[10, 1, 'xxx']|[1,'xxx']\n2|[]|[]\n5|[1, 2, 3]|[1]\n7|[5]|[5]\n</code></pre>\n<p>Speed is crucial, as there is a huge amount of records.</p>\n<p>What I did for now:</p>\n<ol>\n<li>created a set of possible values</li>\n</ol>\n<pre><code>l = list(df2['e'].unique())  \n</code></pre>\n<ol start=\"2\">\n<li>Try to use <code>df.assign</code> with comprehensive lists, but it's not quite working and too slow.</li>\n</ol>\n<pre><code>df.assign(mapped=[[x for x in row if x in l] for row in df.b])\n</code></pre>\n<p>Appreciate any help.</p>\n<h3>UPD</h3>\n<p>In lists and df2 not always integer values, sometimes it's strings.</p>\n",
        "answer_body": "<p>You can try casting to <code>str</code> and then <code>series.str.findall</code></p>\n<pre><code>l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall('|'.join([fr&quot;\\b{i}\\b&quot; for i in l]))\n</code></pre>\n<p>or shorter on the findall pattern courtesy @<a href=\"https://stackoverflow.com/users/12833166/shubham-sharma\">Shubham</a>:</p>\n<pre><code>l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall(fr&quot;\\b({'|'.join(l)})\\b&quot;)\n</code></pre>\n<hr />\n<pre><code>print(df)\n\n   a             b     c\n0  1  [10, 1, 100]   [1]\n1  2            []    []\n2  5     [1, 2, 3]   [1]\n3  7           [5]   [5]\n4  9      [25, 27]  [25]\n</code></pre>\n",
        "question_body": "<p>I have dataframe column with values in lists, want to add new column with filtered values from list if they are in other dataframe.</p>\n<p>df:</p>\n<pre><code>df = pd.DataFrame({'a':[1,2,5,7,9],'b':[[10,1,'xxx'],[],[1,2,3],[5],[25,27]]})\n</code></pre>\n<pre><code>**a**|**b**\n:-----:|:-----:\n1|[10, 1, 'xxx']\n2|[]\n5|[1, 2, 3]\n7|[5]\n9|[25, 27]\n</code></pre>\n<p>df2:</p>\n<pre><code>df2 = pd.DataFrame({'d':[324,21,4353,345,4535,23],'e':[5,1,23,25,25,'xxx']})\n</code></pre>\n<p>I need to add new column with filtered column <code>b</code> in <code>df</code> so that it contains lists with only elements which are in <code>df2</code> column <code>e</code>.</p>\n<p>Result:</p>\n<pre><code>**a**|**b**|**c**\n:-----:|:-----:|:-----:\n1|[10, 1, 'xxx']|[1,'xxx']\n2|[]|[]\n5|[1, 2, 3]|[1]\n7|[5]|[5]\n</code></pre>\n<p>Speed is crucial, as there is a huge amount of records.</p>\n<p>What I did for now:</p>\n<ol>\n<li>created a set of possible values</li>\n</ol>\n<pre><code>l = list(df2['e'].unique())  \n</code></pre>\n<ol start=\"2\">\n<li>Try to use <code>df.assign</code> with comprehensive lists, but it's not quite working and too slow.</li>\n</ol>\n<pre><code>df.assign(mapped=[[x for x in row if x in l] for row in df.b])\n</code></pre>\n<p>Appreciate any help.</p>\n<h3>UPD</h3>\n<p>In lists and df2 not always integer values, sometimes it's strings.</p>\n",
        "formatted_input": {
            "qid": 67111775,
            "link": "https://stackoverflow.com/questions/67111775/python-pandas-check-each-element-in-list-values-of-column-to-exist-in-other-da",
            "question": {
                "title": "Python, Pandas: check each element in list values of column to exist in other dataframe",
                "ques_desc": "I have dataframe column with values in lists, want to add new column with filtered values from list if they are in other dataframe. df: df2: I need to add new column with filtered column in so that it contains lists with only elements which are in column . Result: Speed is crucial, as there is a huge amount of records. What I did for now: created a set of possible values Try to use with comprehensive lists, but it's not quite working and too slow. Appreciate any help. UPD In lists and df2 not always integer values, sometimes it's strings. "
            },
            "io": [
                "**a**|**b**\n:-----:|:-----:\n1|[10, 1, 'xxx']\n2|[]\n5|[1, 2, 3]\n7|[5]\n9|[25, 27]\n",
                "**a**|**b**|**c**\n:-----:|:-----:|:-----:\n1|[10, 1, 'xxx']|[1,'xxx']\n2|[]|[]\n5|[1, 2, 3]|[1]\n7|[5]|[5]\n"
            ],
            "answer": {
                "ans_desc": "You can try casting to and then or shorter on the findall pattern courtesy @Shubham: ",
                "code": [
                    "l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall('|'.join([fr\"\\b{i}\\b\" for i in l]))\n",
                    "l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall(fr\"\\b({'|'.join(l)})\\b\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 394,
            "user_id": 14272509,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b242382a5118951007387b544b81822a?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dionisius Pratama",
            "link": "https://stackoverflow.com/users/14272509/dionisius-pratama"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 67106177,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1618480579,
        "creation_date": 1618479527,
        "question_id": 67105986,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67105986/how-to-edit-excel-file-using-dataframe-and-save-it-back-as-excel-file",
        "title": "How to edit Excel file using DataFrame and save it back as Excel file?",
        "body": "<p>I have <a href=\"https://drive.google.com/file/d/1_Mb1ccVDecbg2a-uIoFZtH6BlQQ0ifA-/view?usp=sharing\" rel=\"nofollow noreferrer\">this</a> Excel file. I also put the screenshot of my the file below.\n<a href=\"https://i.stack.imgur.com/IOQ2o.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/IOQ2o.jpg\" alt=\"Excel screenshot\" /></a></p>\n<p>I want to edit the data on <code>pitch-class</code> column with this 2 criteria:</p>\n<ol>\n<li>removing <code>' '</code> mark between the text.</li>\n<li>removing <code>0</code> values.</li>\n<li>removing <code>[]</code> mark.</li>\n</ol>\n<p>So, for example, from this text:</p>\n<pre><code>['0', 'E3', 'F3', 'F#3 / Gb3', 'G3', 'G#3 / Ab3', 'A3', 'A#3 / Bb3', 'B3', 'C4', 'C#4 / Db4', 'D4']\n</code></pre>\n<p>I want to make it look like this:</p>\n<pre><code>[E3, F3, F#3 / Gb3, G3, G#3 / Ab3, A3, A#3 / Bb3, B3, C4, C#4 / Db4, D4]\n</code></pre>\n<p>Of course, I can do this manually one by one, but unfortunately because I have about 20 similar files that I have to edit, I can't do it manually, so I think I might need help from Python.</p>\n<p>My idea to do it on Python is to load the Excel file to a DataFrame, edit the data row by row (maybe using <code>.remove()</code> and <code>.join()</code> method), and put the edit result back to original Excel file, or maybe generate a new one consisting an edited <code>pitch-class</code> data column.</p>\n<p>But, I kinda have no idea on how to do code it. So far, what I've tried to do is this:</p>\n<ol>\n<li>read the Excel files to Python.</li>\n<li>read <code>pitch-class</code> column in that Excel file.</li>\n<li>load it to a dataframe.\nBelow is my current code.</li>\n</ol>\n<pre><code>import pandas as pd \n\nfile = '014_twinkle_twinkle 300 0.0001 dataframe.xlsx' # file attached above\n\ndf = pd.read_excel(file, index_col=None, usecols=&quot;C&quot;) # read only pitch-class column\n\n# printing data\nfor row in df.iterrows():\n    print(df['pitch-class'].astype(str))\n\n</code></pre>\n<p>My question is how can I edit the <code>pitch-class</code> data per row and put the edit result back again to original or a new Excel file? I have difficulties accessing the <code>df['pitch-class']</code> data because I can't get the string value. Is there any way in Python to achieve it?</p>\n",
        "answer_body": "<p>In general you do not want to iterate over every row in a pandas dataframe, it is very slow. There are a lot of ways (that you can lean by practice over time) to apply functions over a column/row/the whole dataframe in pandas. In this example:</p>\n<p>Convert the column to type string, and replace the ' character with a blank space</p>\n<pre><code>df = pd.read_excel(&quot;014_twinkle_twinkle 300 0.0001 dataframe.xlsx&quot;)\ndf[&quot;pitch-class&quot;] = df[&quot;pitch-class&quot;].astype(str).str.replace(&quot;'0', &quot;, &quot;&quot;)\ndf[&quot;pitch-class&quot;] = df[&quot;pitch-class&quot;].astype(str).str.replace(&quot;'&quot;, &quot;&quot;)\ndf.to_excel(&quot;results.xlsx&quot;)\n</code></pre>\n",
        "question_body": "<p>I have <a href=\"https://drive.google.com/file/d/1_Mb1ccVDecbg2a-uIoFZtH6BlQQ0ifA-/view?usp=sharing\" rel=\"nofollow noreferrer\">this</a> Excel file. I also put the screenshot of my the file below.\n<a href=\"https://i.stack.imgur.com/IOQ2o.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/IOQ2o.jpg\" alt=\"Excel screenshot\" /></a></p>\n<p>I want to edit the data on <code>pitch-class</code> column with this 2 criteria:</p>\n<ol>\n<li>removing <code>' '</code> mark between the text.</li>\n<li>removing <code>0</code> values.</li>\n<li>removing <code>[]</code> mark.</li>\n</ol>\n<p>So, for example, from this text:</p>\n<pre><code>['0', 'E3', 'F3', 'F#3 / Gb3', 'G3', 'G#3 / Ab3', 'A3', 'A#3 / Bb3', 'B3', 'C4', 'C#4 / Db4', 'D4']\n</code></pre>\n<p>I want to make it look like this:</p>\n<pre><code>[E3, F3, F#3 / Gb3, G3, G#3 / Ab3, A3, A#3 / Bb3, B3, C4, C#4 / Db4, D4]\n</code></pre>\n<p>Of course, I can do this manually one by one, but unfortunately because I have about 20 similar files that I have to edit, I can't do it manually, so I think I might need help from Python.</p>\n<p>My idea to do it on Python is to load the Excel file to a DataFrame, edit the data row by row (maybe using <code>.remove()</code> and <code>.join()</code> method), and put the edit result back to original Excel file, or maybe generate a new one consisting an edited <code>pitch-class</code> data column.</p>\n<p>But, I kinda have no idea on how to do code it. So far, what I've tried to do is this:</p>\n<ol>\n<li>read the Excel files to Python.</li>\n<li>read <code>pitch-class</code> column in that Excel file.</li>\n<li>load it to a dataframe.\nBelow is my current code.</li>\n</ol>\n<pre><code>import pandas as pd \n\nfile = '014_twinkle_twinkle 300 0.0001 dataframe.xlsx' # file attached above\n\ndf = pd.read_excel(file, index_col=None, usecols=&quot;C&quot;) # read only pitch-class column\n\n# printing data\nfor row in df.iterrows():\n    print(df['pitch-class'].astype(str))\n\n</code></pre>\n<p>My question is how can I edit the <code>pitch-class</code> data per row and put the edit result back again to original or a new Excel file? I have difficulties accessing the <code>df['pitch-class']</code> data because I can't get the string value. Is there any way in Python to achieve it?</p>\n",
        "formatted_input": {
            "qid": 67105986,
            "link": "https://stackoverflow.com/questions/67105986/how-to-edit-excel-file-using-dataframe-and-save-it-back-as-excel-file",
            "question": {
                "title": "How to edit Excel file using DataFrame and save it back as Excel file?",
                "ques_desc": "I have this Excel file. I also put the screenshot of my the file below. I want to edit the data on column with this 2 criteria: removing mark between the text. removing values. removing mark. So, for example, from this text: I want to make it look like this: Of course, I can do this manually one by one, but unfortunately because I have about 20 similar files that I have to edit, I can't do it manually, so I think I might need help from Python. My idea to do it on Python is to load the Excel file to a DataFrame, edit the data row by row (maybe using and method), and put the edit result back to original Excel file, or maybe generate a new one consisting an edited data column. But, I kinda have no idea on how to do code it. So far, what I've tried to do is this: read the Excel files to Python. read column in that Excel file. load it to a dataframe. Below is my current code. My question is how can I edit the data per row and put the edit result back again to original or a new Excel file? I have difficulties accessing the data because I can't get the string value. Is there any way in Python to achieve it? "
            },
            "io": [
                "['0', 'E3', 'F3', 'F#3 / Gb3', 'G3', 'G#3 / Ab3', 'A3', 'A#3 / Bb3', 'B3', 'C4', 'C#4 / Db4', 'D4']\n",
                "[E3, F3, F#3 / Gb3, G3, G#3 / Ab3, A3, A#3 / Bb3, B3, C4, C#4 / Db4, D4]\n"
            ],
            "answer": {
                "ans_desc": "In general you do not want to iterate over every row in a pandas dataframe, it is very slow. There are a lot of ways (that you can lean by practice over time) to apply functions over a column/row/the whole dataframe in pandas. In this example: Convert the column to type string, and replace the ' character with a blank space ",
                "code": [
                    "df = pd.read_excel(\"014_twinkle_twinkle 300 0.0001 dataframe.xlsx\")\ndf[\"pitch-class\"] = df[\"pitch-class\"].astype(str).str.replace(\"'0', \", \"\")\ndf[\"pitch-class\"] = df[\"pitch-class\"].astype(str).str.replace(\"'\", \"\")\ndf.to_excel(\"results.xlsx\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 15268135,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-ht7hZdIK25o/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclGYRKPfPrMOpMqiMKs4a8V8VjbCA/s96-c/photo.jpg?sz=128",
            "display_name": "XYZ_2635",
            "link": "https://stackoverflow.com/users/15268135/xyz-2635"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67088160,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1618389013,
        "creation_date": 1618386037,
        "question_id": 67087432,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67087432/match-multiple-columns-on-python-to-a-single-value",
        "title": "Match multiple columns on Python to a single value",
        "body": "<p>I hope you are doing well.\nI am trying to perform a match based on multiple columns where my values of Column B of df1 is scattered in three to four columns in df2. The goal here is the the return the values of Column A of df2 if values of Column B matches any values in the columns C,D,E.\nWhat I did until now was actually to do multiple left merges (and changing the name of Column B to match the name of columns C,D,E of df2).\nI am trying to simplify the process but I am unsure how I am supposed to do this?\nMy dataset looks like that:</p>\n<p>Df1:</p>\n<pre><code>    ID\n0   77  \n1   4859    \n2   LSP\n</code></pre>\n<p>DF2:</p>\n<pre><code>    X           id1             id2             id3\n0   AAAAA_XX    889             77              BSP\n1   BBBBB_XX    4859            CC              998P\n2   CCCC_YY     YUI             TYU             LSP\n</code></pre>\n<p>My goal is to have in df1:</p>\n<pre><code>    ID     X\n0   77     AAAAA_XX\n1   4859   BBBBB_XX \n2   LSP    CCCC_YY\n</code></pre>\n<p>Thank you very much !</p>\n",
        "answer_body": "<p>you can get all the values in the columns to one first with <code>pd.concat</code>\nthen we merge the tables like this:</p>\n<pre><code>df3 = pd.concat([df2.id1, df2.id2]).reset_index()\ndf1 = df2.merge(df3, how=&quot;left&quot;, left_on = df1.ID, right_on = df3[0])\ndf1 = df1.iloc[:, :2]\ndf1 = df1.rename(columns={&quot;key_0&quot;: &quot;ID&quot;})\n</code></pre>\n<p>not the most beautiful code in the world, but it works.</p>\n<p>output:</p>\n<pre><code>    ID      X\n0   77      AAAAA_XX\n1   4859    BBBBB_XX\n2   LSP     CCCC_YY\n</code></pre>\n",
        "question_body": "<p>I hope you are doing well.\nI am trying to perform a match based on multiple columns where my values of Column B of df1 is scattered in three to four columns in df2. The goal here is the the return the values of Column A of df2 if values of Column B matches any values in the columns C,D,E.\nWhat I did until now was actually to do multiple left merges (and changing the name of Column B to match the name of columns C,D,E of df2).\nI am trying to simplify the process but I am unsure how I am supposed to do this?\nMy dataset looks like that:</p>\n<p>Df1:</p>\n<pre><code>    ID\n0   77  \n1   4859    \n2   LSP\n</code></pre>\n<p>DF2:</p>\n<pre><code>    X           id1             id2             id3\n0   AAAAA_XX    889             77              BSP\n1   BBBBB_XX    4859            CC              998P\n2   CCCC_YY     YUI             TYU             LSP\n</code></pre>\n<p>My goal is to have in df1:</p>\n<pre><code>    ID     X\n0   77     AAAAA_XX\n1   4859   BBBBB_XX \n2   LSP    CCCC_YY\n</code></pre>\n<p>Thank you very much !</p>\n",
        "formatted_input": {
            "qid": 67087432,
            "link": "https://stackoverflow.com/questions/67087432/match-multiple-columns-on-python-to-a-single-value",
            "question": {
                "title": "Match multiple columns on Python to a single value",
                "ques_desc": "I hope you are doing well. I am trying to perform a match based on multiple columns where my values of Column B of df1 is scattered in three to four columns in df2. The goal here is the the return the values of Column A of df2 if values of Column B matches any values in the columns C,D,E. What I did until now was actually to do multiple left merges (and changing the name of Column B to match the name of columns C,D,E of df2). I am trying to simplify the process but I am unsure how I am supposed to do this? My dataset looks like that: Df1: DF2: My goal is to have in df1: Thank you very much ! "
            },
            "io": [
                "    ID\n0   77  \n1   4859    \n2   LSP\n",
                "    ID     X\n0   77     AAAAA_XX\n1   4859   BBBBB_XX \n2   LSP    CCCC_YY\n"
            ],
            "answer": {
                "ans_desc": "you can get all the values in the columns to one first with then we merge the tables like this: not the most beautiful code in the world, but it works. output: ",
                "code": [
                    "df3 = pd.concat([df2.id1, df2.id2]).reset_index()\ndf1 = df2.merge(df3, how=\"left\", left_on = df1.ID, right_on = df3[0])\ndf1 = df1.iloc[:, :2]\ndf1 = df1.rename(columns={\"key_0\": \"ID\"})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "minimum"
        ],
        "owner": {
            "reputation": 93,
            "user_id": 15506607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7581bbef9e6804ec178260b9c022a60f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Michael Scofield",
            "link": "https://stackoverflow.com/users/15506607/michael-scofield"
        },
        "is_answered": true,
        "view_count": 72,
        "accepted_answer_id": 67053331,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1618209019,
        "creation_date": 1618207033,
        "question_id": 67053308,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67053308/how-to-replace-zero-by-one-for-particular-row-in-data-frame",
        "title": "How to replace &#39;Zero&#39; by &#39;One&#39; for particular row in data frame",
        "body": "<p>I've this dataframe:df1</p>\n<pre><code>                    DP1       DP2       DP3      DP4      DP5      DP6    DP7     DP8  DP9  DP10\nOP1             43239.0   46962.0   55858.0   9128.0  30372.0   5932.0  667.0   663.0  0.0   NaN\nOP2               146.0      73.0   16647.0   5596.0   1493.0   7175.0   45.0   438.0  NaN   NaN\nOP3            266279.0    1189.0       1.0  10939.0  17799.0   4702.0  235.0     NaN  NaN   NaN\nOP4            360547.0   56943.0  142271.0  38217.0   1141.0   6757.0    NaN     NaN  NaN   NaN\nOP5            380497.0   17946.0   19376.0      0.0   3974.0      NaN    NaN     NaN  NaN   NaN\nOP6              6151.0   16525.0   17046.0  11532.0      NaN      NaN    NaN     NaN  NaN   NaN\nOP7            142026.0   21999.0     820.0      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nOP8             76860.0  102580.0       NaN      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nOP9              6210.0       NaN       NaN      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nOP10                NaN       NaN       NaN      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nTotal         1281955.0  264217.0  252019.0  75412.0  54779.0  24566.0  947.0  1101.0  0.0   0.0\nVariance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  NaN  -0.0\nMack's Sigma      400.0     194.0     205.0    123.0    117.0     90.0   22.0    33.0  NaN  -0.0 \n</code></pre>\n<pre><code>Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  NaN  -0.0\n</code></pre>\n<p>I would like to Find the minimum value of last two entry of Variance row.\nI would like to last two entries and finding minimum , like in variance last two entries are <strong>474.0 and 1101.0</strong> and that should be added in Nan place.</p>\n<p><strong>Output look like</strong></p>\n<pre><code>Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  474.0 -0.0\n</code></pre>\n<p>I've tried this code:</p>\n<pre><code>minValuesObj = min(df1.loc('Variance'))\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html\" rel=\"nofollow noreferrer\"><code>DataFrame.iloc</code></a> with set values by min (there is selected by position, it means for last previous label <code>Variance</code> use <code>-2</code>):</p>\n<pre><code>df1.iloc[-2, -2] = df1.iloc[-2, -4:-2].min()\n</code></pre>\n<p>Or is possible use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.get_loc.html\" rel=\"nofollow noreferrer\"><code>Index.get_loc</code></a> for position by label name:</p>\n<pre><code>pos = df1.index.get_loc('Variance')\ndf1.iloc[pos, -2] = df1.iloc[pos, -4:-2].min()\n</code></pre>\n<p>Or if need select by <code>Variance</code> use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>DataFrame.loc</code></a> for seelct by labels, for dynamic columns names use indexing <code>df1.columns</code>:</p>\n<pre><code>df1.loc['Variance', df1.columns[-2]] = df1.loc['Variance', df1.columns[-4:-2]].min()\n</code></pre>\n<hr />\n<pre><code>                    DP1       DP2       DP3      DP4      DP5      DP6    DP7  \\\nOP1             43239.0   46962.0   55858.0   9128.0  30372.0   5932.0  667.0   \nOP2               146.0      73.0   16647.0   5596.0   1493.0   7175.0   45.0   \nOP3            266279.0    1189.0       1.0  10939.0  17799.0   4702.0  235.0   \nOP4            360547.0   56943.0  142271.0  38217.0   1141.0   6757.0    NaN   \nOP5            380497.0   17946.0   19376.0      0.0   3974.0      NaN    NaN   \nOP6              6151.0   16525.0   17046.0  11532.0      NaN      NaN    NaN   \nOP7            142026.0   21999.0     820.0      NaN      NaN      NaN    NaN   \nOP8             76860.0  102580.0       NaN      NaN      NaN      NaN    NaN   \nOP9              6210.0       NaN       NaN      NaN      NaN      NaN    NaN   \nOP10                NaN       NaN       NaN      NaN      NaN      NaN    NaN   \nTotal         1281955.0  264217.0  252019.0  75412.0  54779.0  24566.0  947.0   \nVariance       160244.0   37745.0   42003.0  15082.0  13695.0     89.0  474.0   \nMack's Sigma      400.0     194.0     205.0    123.0    117.0     90.0   22.0   \n\n                 DP8    DP9  DP10  \nOP1            663.0    0.0   NaN  \nOP2            438.0    NaN   NaN  \nOP3              NaN    NaN   NaN  \nOP4              NaN    NaN   NaN  \nOP5              NaN    NaN   NaN  \nOP6              NaN    NaN   NaN  \nOP7              NaN    NaN   NaN  \nOP8              NaN    NaN   NaN  \nOP9              NaN    NaN   NaN  \nOP10             NaN    NaN   NaN  \nTotal         1101.0    0.0   0.0  \nVariance      1101.0  474.0  -0.0  \nMack's Sigma    33.0    NaN  -0.0  \n</code></pre>\n",
        "question_body": "<p>I've this dataframe:df1</p>\n<pre><code>                    DP1       DP2       DP3      DP4      DP5      DP6    DP7     DP8  DP9  DP10\nOP1             43239.0   46962.0   55858.0   9128.0  30372.0   5932.0  667.0   663.0  0.0   NaN\nOP2               146.0      73.0   16647.0   5596.0   1493.0   7175.0   45.0   438.0  NaN   NaN\nOP3            266279.0    1189.0       1.0  10939.0  17799.0   4702.0  235.0     NaN  NaN   NaN\nOP4            360547.0   56943.0  142271.0  38217.0   1141.0   6757.0    NaN     NaN  NaN   NaN\nOP5            380497.0   17946.0   19376.0      0.0   3974.0      NaN    NaN     NaN  NaN   NaN\nOP6              6151.0   16525.0   17046.0  11532.0      NaN      NaN    NaN     NaN  NaN   NaN\nOP7            142026.0   21999.0     820.0      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nOP8             76860.0  102580.0       NaN      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nOP9              6210.0       NaN       NaN      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nOP10                NaN       NaN       NaN      NaN      NaN      NaN    NaN     NaN  NaN   NaN\nTotal         1281955.0  264217.0  252019.0  75412.0  54779.0  24566.0  947.0  1101.0  0.0   0.0\nVariance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  NaN  -0.0\nMack's Sigma      400.0     194.0     205.0    123.0    117.0     90.0   22.0    33.0  NaN  -0.0 \n</code></pre>\n<pre><code>Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  NaN  -0.0\n</code></pre>\n<p>I would like to Find the minimum value of last two entry of Variance row.\nI would like to last two entries and finding minimum , like in variance last two entries are <strong>474.0 and 1101.0</strong> and that should be added in Nan place.</p>\n<p><strong>Output look like</strong></p>\n<pre><code>Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  474.0 -0.0\n</code></pre>\n<p>I've tried this code:</p>\n<pre><code>minValuesObj = min(df1.loc('Variance'))\n</code></pre>\n",
        "formatted_input": {
            "qid": 67053308,
            "link": "https://stackoverflow.com/questions/67053308/how-to-replace-zero-by-one-for-particular-row-in-data-frame",
            "question": {
                "title": "How to replace &#39;Zero&#39; by &#39;One&#39; for particular row in data frame",
                "ques_desc": "I've this dataframe:df1 I would like to Find the minimum value of last two entry of Variance row. I would like to last two entries and finding minimum , like in variance last two entries are 474.0 and 1101.0 and that should be added in Nan place. Output look like I've tried this code: "
            },
            "io": [
                "Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  NaN  -0.0\n",
                "Variance       160244.0   37745.0   42003.0  15082.0  13695.0   89.0  474.0  1101.0  474.0 -0.0\n"
            ],
            "answer": {
                "ans_desc": "Use with set values by min (there is selected by position, it means for last previous label use ): Or is possible use for position by label name: Or if need select by use for seelct by labels, for dynamic columns names use indexing : ",
                "code": [
                    "df1.iloc[-2, -2] = df1.iloc[-2, -4:-2].min()\n",
                    "pos = df1.index.get_loc('Variance')\ndf1.iloc[pos, -2] = df1.iloc[pos, -4:-2].min()\n",
                    "df1.loc['Variance', df1.columns[-2]] = df1.loc['Variance', df1.columns[-4:-2]].min()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 447,
            "user_id": 9806325,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/10155495730706211/picture?type=large",
            "display_name": "Jack Rolph",
            "link": "https://stackoverflow.com/users/9806325/jack-rolph"
        },
        "is_answered": true,
        "view_count": 122,
        "accepted_answer_id": 62050868,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1618075666,
        "creation_date": 1590604488,
        "last_edit_date": 1590605432,
        "question_id": 62050416,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62050416/pandas-dataframe-calculate-shared-fraction",
        "title": "Pandas Dataframe: Calculate Shared Fraction",
        "body": "<p>Suppose I have a dataframe, <code>df</code>, consisting of a class of two objects, <code>S</code>, a set of co-ordinates associated with them, <code>X</code> and <code>Y</code>, and a value, <code>V</code>, that was measured there.</p>\n\n<p>The dataframe looks like this:</p>\n\n<pre><code>S X Y V\n0 1 1 1\n1 2 2 1\n1 9 9 2\n0 9 9 8\n</code></pre>\n\n<p>I would like to know the commands that allow me to go from this picture to the one where each <code>S</code> is converted to a series of columns where:</p>\n\n<ul>\n<li><code>V_s</code> represents the sum of all the shared <code>X-Y</code> coordinates;</li>\n<li><code>F0</code> and <code>F1</code> represent the fractions of the V for each possible class, <code>S</code>.</li>\n</ul>\n\n<p>For example:</p>\n\n<pre><code>X Y V_s  F0  F1\n1 1 1  1.0 0.0\n2 2 1  0.0 1.0\n9 9 10 0.2 0.8\n</code></pre>\n\n<p>I can sum and fraction calculate the fraction by using </p>\n\n<pre><code>df['V_s'] = df.groupby(['X', 'Y'])['V'].transform('sum')\ndf['F'] = df['V']/df['V_s']\n</code></pre>\n\n<p>What are the next steps?</p>\n",
        "answer_body": "<p>You could try this:</p>\n<pre><code>(df.groupby(['X','Y','S']).sum()\n   .unstack('S', fill_value=0)['V']\n   .rename(columns=lambda x: f&quot;F{x}&quot;)\n   .assign(V_s=lambda x: x.sum(1),\n           F0 =lambda x: x['F0']/x['V_s'],\n           F1 =lambda x: x['F1']/x['V_s'])\n   .reset_index()\n)\n</code></pre>\n<p>Output:</p>\n<pre><code>S  X  Y   F0   F1  V_s\n0  1  1  1.0  0.0    1\n1  2  2  0.0  1.0    1\n2  9  9  0.8  0.2   10\n</code></pre>\n<p><strong>Update</strong> for unknown/large number of classes in <code>S</code>:</p>\n<pre><code>new_df = (df.groupby(['X','Y','S']).sum()\n   .unstack('S', fill_value=0)['V']\n   .rename(columns=lambda x: f&quot;F{x}&quot;)\n)\n\nvs = new_df.sum(1)\nnew_df = (new_df.div(vs,axis='rows')\n                .assign(V_s=vs)\n                .reset_index()\n         )\n          \n</code></pre>\n<p>And you get same output.</p>\n",
        "question_body": "<p>Suppose I have a dataframe, <code>df</code>, consisting of a class of two objects, <code>S</code>, a set of co-ordinates associated with them, <code>X</code> and <code>Y</code>, and a value, <code>V</code>, that was measured there.</p>\n\n<p>The dataframe looks like this:</p>\n\n<pre><code>S X Y V\n0 1 1 1\n1 2 2 1\n1 9 9 2\n0 9 9 8\n</code></pre>\n\n<p>I would like to know the commands that allow me to go from this picture to the one where each <code>S</code> is converted to a series of columns where:</p>\n\n<ul>\n<li><code>V_s</code> represents the sum of all the shared <code>X-Y</code> coordinates;</li>\n<li><code>F0</code> and <code>F1</code> represent the fractions of the V for each possible class, <code>S</code>.</li>\n</ul>\n\n<p>For example:</p>\n\n<pre><code>X Y V_s  F0  F1\n1 1 1  1.0 0.0\n2 2 1  0.0 1.0\n9 9 10 0.2 0.8\n</code></pre>\n\n<p>I can sum and fraction calculate the fraction by using </p>\n\n<pre><code>df['V_s'] = df.groupby(['X', 'Y'])['V'].transform('sum')\ndf['F'] = df['V']/df['V_s']\n</code></pre>\n\n<p>What are the next steps?</p>\n",
        "formatted_input": {
            "qid": 62050416,
            "link": "https://stackoverflow.com/questions/62050416/pandas-dataframe-calculate-shared-fraction",
            "question": {
                "title": "Pandas Dataframe: Calculate Shared Fraction",
                "ques_desc": "Suppose I have a dataframe, , consisting of a class of two objects, , a set of co-ordinates associated with them, and , and a value, , that was measured there. The dataframe looks like this: I would like to know the commands that allow me to go from this picture to the one where each is converted to a series of columns where: represents the sum of all the shared coordinates; and represent the fractions of the V for each possible class, . For example: I can sum and fraction calculate the fraction by using What are the next steps? "
            },
            "io": [
                "S X Y V\n0 1 1 1\n1 2 2 1\n1 9 9 2\n0 9 9 8\n",
                "X Y V_s  F0  F1\n1 1 1  1.0 0.0\n2 2 1  0.0 1.0\n9 9 10 0.2 0.8\n"
            ],
            "answer": {
                "ans_desc": "You could try this: Output: Update for unknown/large number of classes in : And you get same output. ",
                "code": [
                    "(df.groupby(['X','Y','S']).sum()\n   .unstack('S', fill_value=0)['V']\n   .rename(columns=lambda x: f\"F{x}\")\n   .assign(V_s=lambda x: x.sum(1),\n           F0 =lambda x: x['F0']/x['V_s'],\n           F1 =lambda x: x['F1']/x['V_s'])\n   .reset_index()\n)\n",
                    "new_df = (df.groupby(['X','Y','S']).sum()\n   .unstack('S', fill_value=0)['V']\n   .rename(columns=lambda x: f\"F{x}\")\n)\n\nvs = new_df.sum(1)\nnew_df = (new_df.div(vs,axis='rows')\n                .assign(V_s=vs)\n                .reset_index()\n         )\n          \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 317,
            "user_id": 10506254,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/369d2c224e9e15df8b07d58e12c62fa7?s=128&d=identicon&r=PG&f=1",
            "display_name": "Amartin",
            "link": "https://stackoverflow.com/users/10506254/amartin"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 67027747,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1617999319,
        "creation_date": 1617998801,
        "question_id": 67027649,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67027649/how-to-unlist-a-list-with-one-value-inside-a-pandas-columns",
        "title": "How to unlist a list with one value inside a pandas columns?",
        "body": "<p>I have a pandas data frame:</p>\n<pre><code>Id       Col1\n1     ['string']\n2     ['string2']\n</code></pre>\n<p>Is possible to convert the data frame into another data frame that look like this?</p>\n<pre><code>Id     Col1\n1     string\n2     string2\n</code></pre>\n<p>I tried with this way but only get the <code>[</code>.</p>\n<pre><code>df.col1.apply(lambda x: x[0])\n</code></pre>\n<p>Thanks for your time!</p>\n",
        "answer_body": "<blockquote>\n<p>I tried with this way but only get the [.</p>\n</blockquote>\n<p>Then this means they are <code>str</code>ings, not <code>list</code>s. You can convert them to <code>list</code>s by <code>apply</code>ing <a href=\"https://docs.python.org/3/library/ast.html#ast.literal_eval\" rel=\"noreferrer\"><code>ast.literal_eval</code></a> and then <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.explode.html\" rel=\"noreferrer\"><code>explode</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import ast\n\ndf.Col1 = df.Col1.apply(ast.literal_eval).explode()\n</code></pre>\n<p>to get</p>\n<pre><code>   Id     Col1\n0   1   string\n1   2  string2\n</code></pre>\n",
        "question_body": "<p>I have a pandas data frame:</p>\n<pre><code>Id       Col1\n1     ['string']\n2     ['string2']\n</code></pre>\n<p>Is possible to convert the data frame into another data frame that look like this?</p>\n<pre><code>Id     Col1\n1     string\n2     string2\n</code></pre>\n<p>I tried with this way but only get the <code>[</code>.</p>\n<pre><code>df.col1.apply(lambda x: x[0])\n</code></pre>\n<p>Thanks for your time!</p>\n",
        "formatted_input": {
            "qid": 67027649,
            "link": "https://stackoverflow.com/questions/67027649/how-to-unlist-a-list-with-one-value-inside-a-pandas-columns",
            "question": {
                "title": "How to unlist a list with one value inside a pandas columns?",
                "ques_desc": "I have a pandas data frame: Is possible to convert the data frame into another data frame that look like this? I tried with this way but only get the . Thanks for your time! "
            },
            "io": [
                "Id       Col1\n1     ['string']\n2     ['string2']\n",
                "Id     Col1\n1     string\n2     string2\n"
            ],
            "answer": {
                "ans_desc": " I tried with this way but only get the [. Then this means they are ings, not s. You can convert them to s by ing and then : to get ",
                "code": [
                    "import ast\n\ndf.Col1 = df.Col1.apply(ast.literal_eval).explode()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15266944,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-J7t7C5-rIxU/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucmb4AL0SuzPw0bhZ_LlhHiOdDMzCQ/s96-c/photo.jpg?sz=128",
            "display_name": "Michael &#193;ngel",
            "link": "https://stackoverflow.com/users/15266944/michael-%c3%81ngel"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 67005490,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1617890087,
        "creation_date": 1617819693,
        "question_id": 66991966,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66991966/how-to-convert-this-dataframe-into-json",
        "title": "How to convert this DataFrame into Json",
        "body": "<p>I have this <code>DataFrame</code> with 2 columns</p>\n<pre><code>print(df)\n\n     a                b\n\n     10          {'A': 'foo', ...}\n     20          {'B': 'faa', ...}\n     30          {'C': 'fee', ...}\n     40          {'D': 'fii', ...}\n     50          {'E': 'foo', ...}\n\n</code></pre>\n<p>when I try to convert it into <code>json</code> it goes wrong:</p>\n<pre><code>df.to_json(&quot;test.json&quot;)\n\n# Output:\n\n\n{\n&quot;a&quot;:{10, 20, 30, 40, 50},\n&quot;b&quot;:{\n   &quot;1&quot;:{\n      &quot;A&quot;:&quot;foo&quot;,\n      ...\n      },\n   &quot;2&quot;:{\n      &quot;B&quot;:&quot;faa&quot;,\n      ...\n      },\n   &quot;3&quot;:{\n      &quot;B&quot;:&quot;faa&quot;,\n      ...\n      },\n\n   ... \n\n   &quot;5&quot;:{\n      &quot;E&quot;:&quot;foo&quot;,\n      ...\n      }\n}\n\n</code></pre>\n<p>I don't even know ehere the numbers come from.</p>\n<p>My desired <code>json</code>:</p>\n<pre><code>[{\n   'a': 10,\n   'b': {\n         'A': 'foo', \n         ...\n        }, \n    ...\n    'a': 50,\n    'b': {\n         'E': 'foo', \n         ...\n        }\n}\n]\n\n</code></pre>\n",
        "answer_body": "<p>You could try the following:</p>\n<pre><code>data = []\nfor i in df:\n    data.append({'a': df[i[0]], 'b': df(i[1])})\n</code></pre>\n<p>This should give you your desired output.</p>\n<p>If you want to convert this into a JSON file then you can do the following:</p>\n<pre><code>with open(&quot;myjson.json&quot;, &quot;w&quot;) as f:\n    json.dump(data, f, indent=4)\n</code></pre>\n",
        "question_body": "<p>I have this <code>DataFrame</code> with 2 columns</p>\n<pre><code>print(df)\n\n     a                b\n\n     10          {'A': 'foo', ...}\n     20          {'B': 'faa', ...}\n     30          {'C': 'fee', ...}\n     40          {'D': 'fii', ...}\n     50          {'E': 'foo', ...}\n\n</code></pre>\n<p>when I try to convert it into <code>json</code> it goes wrong:</p>\n<pre><code>df.to_json(&quot;test.json&quot;)\n\n# Output:\n\n\n{\n&quot;a&quot;:{10, 20, 30, 40, 50},\n&quot;b&quot;:{\n   &quot;1&quot;:{\n      &quot;A&quot;:&quot;foo&quot;,\n      ...\n      },\n   &quot;2&quot;:{\n      &quot;B&quot;:&quot;faa&quot;,\n      ...\n      },\n   &quot;3&quot;:{\n      &quot;B&quot;:&quot;faa&quot;,\n      ...\n      },\n\n   ... \n\n   &quot;5&quot;:{\n      &quot;E&quot;:&quot;foo&quot;,\n      ...\n      }\n}\n\n</code></pre>\n<p>I don't even know ehere the numbers come from.</p>\n<p>My desired <code>json</code>:</p>\n<pre><code>[{\n   'a': 10,\n   'b': {\n         'A': 'foo', \n         ...\n        }, \n    ...\n    'a': 50,\n    'b': {\n         'E': 'foo', \n         ...\n        }\n}\n]\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 66991966,
            "link": "https://stackoverflow.com/questions/66991966/how-to-convert-this-dataframe-into-json",
            "question": {
                "title": "How to convert this DataFrame into Json",
                "ques_desc": "I have this with 2 columns when I try to convert it into it goes wrong: I don't even know ehere the numbers come from. My desired : "
            },
            "io": [
                "print(df)\n\n     a                b\n\n     10          {'A': 'foo', ...}\n     20          {'B': 'faa', ...}\n     30          {'C': 'fee', ...}\n     40          {'D': 'fii', ...}\n     50          {'E': 'foo', ...}\n\n",
                "[{\n   'a': 10,\n   'b': {\n         'A': 'foo', \n         ...\n        }, \n    ...\n    'a': 50,\n    'b': {\n         'E': 'foo', \n         ...\n        }\n}\n]\n\n"
            ],
            "answer": {
                "ans_desc": "You could try the following: This should give you your desired output. If you want to convert this into a JSON file then you can do the following: ",
                "code": [
                    "data = []\nfor i in df:\n    data.append({'a': df[i[0]], 'b': df(i[1])})\n",
                    "with open(\"myjson.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 14026481,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1eff6235663a801ddff72a6f58619e8f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Blizzard2020",
            "link": "https://stackoverflow.com/users/14026481/blizzard2020"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 66978653,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1617793190,
        "creation_date": 1617752123,
        "last_edit_date": 1617793190,
        "question_id": 66977794,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66977794/splitting-by-indices-i-want-to-split-the-train-test-from-the-data-whose-indic",
        "title": "Splitting by indices: I want to split the train + test from the data whose indices have been given. How shall I get train/test df?",
        "body": "<p>for example= df is the data with features. I want to split the train + test from the data whose indices have been given. How shall I get train/test df.</p>\n<pre><code>df=\n0 2 0.3 0.5 0.5\n1 4 0.5 0.7 0.4\n2 2 0.5 0.1 0.4\n3 4 0.4 0.1 0.3\n4 2 0.3 0.1 0.5\n</code></pre>\n<p>where train.txt is</p>\n<pre><code>train=pd.read_csv(data_train.txt)\n</code></pre>\n<p>where in this dataframe indices are given. How should I get the training data from those indices?</p>\n<p>Contents in data_train.txt(there are 10000 of data in which train indices are given in this txt file)</p>\n<pre><code>0\n2\n4\n</code></pre>\n<p>I want these indices for training data with feature:- like</p>\n<p>final train should look like this (see the index):</p>\n<pre><code>0 2 0.3 0.5 0.5\n2 2 0.5 0.1 0.4\n4 2 0.3 0.1 0.5\n</code></pre>\n",
        "answer_body": "<p>If you have a df as given by:</p>\n<pre><code>   0  1    2    3    4\n0  0  2  0.3  0.5  0.5\n1  1  4  0.5  0.7  0.4\n2  2  2  0.5  0.1  0.4\n3  3  4  0.4  0.1  0.3\n4  4  2  0.3  0.1  0.5\n</code></pre>\n<p>and another train_indices as given by:</p>\n<pre><code>   0\n0  0\n1  2\n2  4\n</code></pre>\n<p>then all you need to do to get the corresponding rows of <code>df</code> depends on how the data is organised:</p>\n<pre><code>#if you're trying to match the index of the df itself\ntrain_df = df.iloc[train_indices]\n#if you're trying to match column 0, which might be important \n#if it's not aligned to the index\ntrain_df =  df.loc[df[0].isin(train_indices)]\n</code></pre>\n<p>Both of these (in this case) return:</p>\n<pre><code>   0  1    2    3    4\n0  0  2  0.3  0.5  0.5\n2  2  2  0.5  0.1  0.4\n4  4  2  0.3  0.1  0.5\n</code></pre>\n",
        "question_body": "<p>for example= df is the data with features. I want to split the train + test from the data whose indices have been given. How shall I get train/test df.</p>\n<pre><code>df=\n0 2 0.3 0.5 0.5\n1 4 0.5 0.7 0.4\n2 2 0.5 0.1 0.4\n3 4 0.4 0.1 0.3\n4 2 0.3 0.1 0.5\n</code></pre>\n<p>where train.txt is</p>\n<pre><code>train=pd.read_csv(data_train.txt)\n</code></pre>\n<p>where in this dataframe indices are given. How should I get the training data from those indices?</p>\n<p>Contents in data_train.txt(there are 10000 of data in which train indices are given in this txt file)</p>\n<pre><code>0\n2\n4\n</code></pre>\n<p>I want these indices for training data with feature:- like</p>\n<p>final train should look like this (see the index):</p>\n<pre><code>0 2 0.3 0.5 0.5\n2 2 0.5 0.1 0.4\n4 2 0.3 0.1 0.5\n</code></pre>\n",
        "formatted_input": {
            "qid": 66977794,
            "link": "https://stackoverflow.com/questions/66977794/splitting-by-indices-i-want-to-split-the-train-test-from-the-data-whose-indic",
            "question": {
                "title": "Splitting by indices: I want to split the train + test from the data whose indices have been given. How shall I get train/test df?",
                "ques_desc": "for example= df is the data with features. I want to split the train + test from the data whose indices have been given. How shall I get train/test df. where train.txt is where in this dataframe indices are given. How should I get the training data from those indices? Contents in data_train.txt(there are 10000 of data in which train indices are given in this txt file) I want these indices for training data with feature:- like final train should look like this (see the index): "
            },
            "io": [
                "df=\n0 2 0.3 0.5 0.5\n1 4 0.5 0.7 0.4\n2 2 0.5 0.1 0.4\n3 4 0.4 0.1 0.3\n4 2 0.3 0.1 0.5\n",
                "0 2 0.3 0.5 0.5\n2 2 0.5 0.1 0.4\n4 2 0.3 0.1 0.5\n"
            ],
            "answer": {
                "ans_desc": "If you have a df as given by: and another train_indices as given by: then all you need to do to get the corresponding rows of depends on how the data is organised: Both of these (in this case) return: ",
                "code": [
                    "#if you're trying to match the index of the df itself\ntrain_df = df.iloc[train_indices]\n#if you're trying to match column 0, which might be important \n#if it's not aligned to the index\ntrain_df =  df.loc[df[0].isin(train_indices)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "triangle"
        ],
        "owner": {
            "reputation": 93,
            "user_id": 15506607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7581bbef9e6804ec178260b9c022a60f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Michael Scofield",
            "link": "https://stackoverflow.com/users/15506607/michael-scofield"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 66939192,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1617607502,
        "creation_date": 1617522525,
        "question_id": 66939075,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66939075/how-to-divide-second-column-by-first-column-in-dataframe",
        "title": "How to divide second column by first column in dataframe?",
        "body": "<pre><code>     DP 1       DP 2        DP 3       DP 4        DP 5         DP 6        DP 7        DP 8       DP 9        DP 10\n 3,57,848    11,24,788   17,35,330   22,18,270   27,45,596   33,19,994   34,66,336   36,06,286   38,33,515   39,01,463 \n 3,52,118    12,36,139   21,70,033   33,53,322   37,99,067   41,20,063   46,47,867   49,14,039   53,39,085  \n 2,90,507    12,92,306   22,18,525   32,35,179   39,85,995   41,32,918   46,28,910   49,09,315      \n 3,10,608    14,18,858   21,95,047   37,57,447   40,29,929   43,81,982   45,88,268          \n 4,43,160    11,36,350   21,28,333   28,97,821   34,02,672   38,73,311              \n 3,96,132    13,33,217   21,80,715   29,85,752   36,91,712                  \n 4,40,832    12,88,463   24,19,861   34,83,130                      \n 3,59,480    14,21,128   28,64,498                          \n 3,76,686    13,63,294                              \n 3,44,014                                   \n</code></pre>\n<p>I've this triangle dataframe(df1) , i wanted to calculate new dataframe(df2) that contains the result:second_column(df2)/first_column(df2) and third_column(df2)/second_column(df2) and so on..</p>\n<p>i tried like this(i know its wrong).</p>\n<pre><code>for colname, col in df1.iteritems():\n            df1[colname7] = df1['second_column']/df1['first_column']\n</code></pre>\n<p>and i wanted df2 like this:</p>\n<pre><code>  DP 1   DP 2    DP 3    DP 4    DP 5    DP 6    DP 7    DP 8    DP 9   DP 10\n 3.14    1.54    1.28    1.24    1.21    1.04    1.04    1.06    1.02    -   \n 3.51    1.76    1.55    1.13    1.08    1.13    1.06    1.09    -      \n 4.45    1.72    1.46    1.23    1.04    1.12    1.06    -          \n 4.57    1.55    1.71    1.07    1.09    1.05    -              \n 2.56    1.87    1.36    1.17    1.14    -                  \n 3.37    1.64    1.37    1.24    -                      \n 2.92    1.88    1.44    -                          \n 3.95    2.02    -                              \n 3.62    -      \n\n                        \n</code></pre>\n<p>Thank You for your time..</p>\n",
        "answer_body": "<p>First of all you need to remove the commas and convert the dataframe to float type:</p>\n<pre><code>df2 = df1.replace(',', '', regex = True).astype(float)\n</code></pre>\n<p>Then you can divide directly by shifting the columns once:</p>\n<pre><code>df2 = df2.shift(-1, axis = 1).div(df2)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>df2\n       DP 1      DP 2      DP 3      DP 4      DP 5      DP 6      DP 7      DP 8      DP 9  DP 10\n0  3.143200  1.542806  1.278299  1.237719  1.209207  1.044079  1.040374  1.063009  1.017725    NaN\n1  3.510582  1.755493  1.545286  1.132926  1.084493  1.128106  1.057268  1.086496       NaN    NaN\n2  4.448450  1.716718  1.458257  1.232079  1.036860  1.120010  1.060577       NaN       NaN    NaN\n3  4.568002  1.547052  1.711784  1.072518  1.087360  1.047076       NaN       NaN       NaN    NaN\n4  2.564198  1.872956  1.361545  1.174217  1.138315       NaN       NaN       NaN       NaN    NaN\n5  3.365588  1.635679  1.369162  1.236443       NaN       NaN       NaN       NaN       NaN    NaN\n6  2.922798  1.878099  1.439393       NaN       NaN       NaN       NaN       NaN       NaN    NaN\n7  3.953288  2.015651       NaN       NaN       NaN       NaN       NaN       NaN       NaN    NaN\n8  3.619179       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN    NaN\n9       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN    NaN\n\n</code></pre>\n<hr />\n<p>Optionally you can round with:</p>\n<pre><code>df2 = df2.round(2)\n</code></pre>\n",
        "question_body": "<pre><code>     DP 1       DP 2        DP 3       DP 4        DP 5         DP 6        DP 7        DP 8       DP 9        DP 10\n 3,57,848    11,24,788   17,35,330   22,18,270   27,45,596   33,19,994   34,66,336   36,06,286   38,33,515   39,01,463 \n 3,52,118    12,36,139   21,70,033   33,53,322   37,99,067   41,20,063   46,47,867   49,14,039   53,39,085  \n 2,90,507    12,92,306   22,18,525   32,35,179   39,85,995   41,32,918   46,28,910   49,09,315      \n 3,10,608    14,18,858   21,95,047   37,57,447   40,29,929   43,81,982   45,88,268          \n 4,43,160    11,36,350   21,28,333   28,97,821   34,02,672   38,73,311              \n 3,96,132    13,33,217   21,80,715   29,85,752   36,91,712                  \n 4,40,832    12,88,463   24,19,861   34,83,130                      \n 3,59,480    14,21,128   28,64,498                          \n 3,76,686    13,63,294                              \n 3,44,014                                   \n</code></pre>\n<p>I've this triangle dataframe(df1) , i wanted to calculate new dataframe(df2) that contains the result:second_column(df2)/first_column(df2) and third_column(df2)/second_column(df2) and so on..</p>\n<p>i tried like this(i know its wrong).</p>\n<pre><code>for colname, col in df1.iteritems():\n            df1[colname7] = df1['second_column']/df1['first_column']\n</code></pre>\n<p>and i wanted df2 like this:</p>\n<pre><code>  DP 1   DP 2    DP 3    DP 4    DP 5    DP 6    DP 7    DP 8    DP 9   DP 10\n 3.14    1.54    1.28    1.24    1.21    1.04    1.04    1.06    1.02    -   \n 3.51    1.76    1.55    1.13    1.08    1.13    1.06    1.09    -      \n 4.45    1.72    1.46    1.23    1.04    1.12    1.06    -          \n 4.57    1.55    1.71    1.07    1.09    1.05    -              \n 2.56    1.87    1.36    1.17    1.14    -                  \n 3.37    1.64    1.37    1.24    -                      \n 2.92    1.88    1.44    -                          \n 3.95    2.02    -                              \n 3.62    -      \n\n                        \n</code></pre>\n<p>Thank You for your time..</p>\n",
        "formatted_input": {
            "qid": 66939075,
            "link": "https://stackoverflow.com/questions/66939075/how-to-divide-second-column-by-first-column-in-dataframe",
            "question": {
                "title": "How to divide second column by first column in dataframe?",
                "ques_desc": " I've this triangle dataframe(df1) , i wanted to calculate new dataframe(df2) that contains the result:second_column(df2)/first_column(df2) and third_column(df2)/second_column(df2) and so on.. i tried like this(i know its wrong). and i wanted df2 like this: Thank You for your time.. "
            },
            "io": [
                "     DP 1       DP 2        DP 3       DP 4        DP 5         DP 6        DP 7        DP 8       DP 9        DP 10\n 3,57,848    11,24,788   17,35,330   22,18,270   27,45,596   33,19,994   34,66,336   36,06,286   38,33,515   39,01,463 \n 3,52,118    12,36,139   21,70,033   33,53,322   37,99,067   41,20,063   46,47,867   49,14,039   53,39,085  \n 2,90,507    12,92,306   22,18,525   32,35,179   39,85,995   41,32,918   46,28,910   49,09,315      \n 3,10,608    14,18,858   21,95,047   37,57,447   40,29,929   43,81,982   45,88,268          \n 4,43,160    11,36,350   21,28,333   28,97,821   34,02,672   38,73,311              \n 3,96,132    13,33,217   21,80,715   29,85,752   36,91,712                  \n 4,40,832    12,88,463   24,19,861   34,83,130                      \n 3,59,480    14,21,128   28,64,498                          \n 3,76,686    13,63,294                              \n 3,44,014                                   \n",
                "  DP 1   DP 2    DP 3    DP 4    DP 5    DP 6    DP 7    DP 8    DP 9   DP 10\n 3.14    1.54    1.28    1.24    1.21    1.04    1.04    1.06    1.02    -   \n 3.51    1.76    1.55    1.13    1.08    1.13    1.06    1.09    -      \n 4.45    1.72    1.46    1.23    1.04    1.12    1.06    -          \n 4.57    1.55    1.71    1.07    1.09    1.05    -              \n 2.56    1.87    1.36    1.17    1.14    -                  \n 3.37    1.64    1.37    1.24    -                      \n 2.92    1.88    1.44    -                          \n 3.95    2.02    -                              \n 3.62    -      \n\n                        \n"
            ],
            "answer": {
                "ans_desc": "First of all you need to remove the commas and convert the dataframe to float type: Then you can divide directly by shifting the columns once: Output Optionally you can round with: ",
                "code": [
                    "df2 = df1.replace(',', '', regex = True).astype(float)\n",
                    "df2 = df2.shift(-1, axis = 1).div(df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nlp",
            "bert-language-model"
        ],
        "owner": {
            "reputation": 482,
            "user_id": 13540652,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GicrwNPIJUpO3GVW51Rveg-lCck7q2NA06hKtazrQ=k-s128",
            "display_name": "Ishan Dutta",
            "link": "https://stackoverflow.com/users/13540652/ishan-dutta"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 66941549,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1617541298,
        "creation_date": 1617535964,
        "question_id": 66940820,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function",
        "title": "Python: How to pass Dataframe Columns as parameters to a function?",
        "body": "<p>I have a dataframe <code>df</code> with 2 columns of text embeddings namely <code>embedding_1</code> and <code>embedding_2</code>. I want to create a third column in <code>df</code> named <code>distances</code> which should contain the cosine_similarity between every row of <code>embedding_1</code> and <code>embedding_2</code>.</p>\n<p>But when I try to implement this using the following code I get a <code>ValueError</code>.</p>\n<p>How to fix it?</p>\n<p><strong>Dataframe <code>df</code></strong></p>\n<pre><code>           embedding_1              |            embedding_2                                 \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]\n</code></pre>\n<p><strong>Code to Calculate Cosine Similarity</strong></p>\n<pre><code>df['distances'] = cosine_similarity(df['embeddings_1'], df['embeddings_2'])\n</code></pre>\n<p><strong>Error</strong></p>\n<pre><code>ValueError: setting an array element with a sequence.\n</code></pre>\n<p><strong>Required Dataframe</strong></p>\n<pre><code>       embedding_1              |            embedding_2                 |  distances                        \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312\n</code></pre>\n",
        "answer_body": "<p>You can use <code>apply()</code> to use <code>cosine_similarity()</code> on each row:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def cal_cosine_similarity(row):\n    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])\n\ndf['distances'] = df.apply(cal_cosine_similarity, axis=1)\n</code></pre>\n<p>or one liner</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe <code>df</code> with 2 columns of text embeddings namely <code>embedding_1</code> and <code>embedding_2</code>. I want to create a third column in <code>df</code> named <code>distances</code> which should contain the cosine_similarity between every row of <code>embedding_1</code> and <code>embedding_2</code>.</p>\n<p>But when I try to implement this using the following code I get a <code>ValueError</code>.</p>\n<p>How to fix it?</p>\n<p><strong>Dataframe <code>df</code></strong></p>\n<pre><code>           embedding_1              |            embedding_2                                 \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]\n</code></pre>\n<p><strong>Code to Calculate Cosine Similarity</strong></p>\n<pre><code>df['distances'] = cosine_similarity(df['embeddings_1'], df['embeddings_2'])\n</code></pre>\n<p><strong>Error</strong></p>\n<pre><code>ValueError: setting an array element with a sequence.\n</code></pre>\n<p><strong>Required Dataframe</strong></p>\n<pre><code>       embedding_1              |            embedding_2                 |  distances                        \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312\n</code></pre>\n",
        "formatted_input": {
            "qid": 66940820,
            "link": "https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function",
            "question": {
                "title": "Python: How to pass Dataframe Columns as parameters to a function?",
                "ques_desc": "I have a dataframe with 2 columns of text embeddings namely and . I want to create a third column in named which should contain the cosine_similarity between every row of and . But when I try to implement this using the following code I get a . How to fix it? Dataframe Code to Calculate Cosine Similarity Error Required Dataframe "
            },
            "io": [
                "           embedding_1              |            embedding_2                                 \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]\n",
                "       embedding_1              |            embedding_2                 |  distances                        \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312\n"
            ],
            "answer": {
                "ans_desc": "You can use to use on each row: or one liner ",
                "code": [
                    "def cal_cosine_similarity(row):\n    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])\n\ndf['distances'] = df.apply(cal_cosine_similarity, axis=1)\n",
                    "df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 1813,
            "user_id": 11622712,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/01f3989fa62907f1d89449c106d92fa0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Fluxy",
            "link": "https://stackoverflow.com/users/11622712/fluxy"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 66934536,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1617474570,
        "creation_date": 1617473468,
        "last_edit_date": 1617474309,
        "question_id": 66934423,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66934423/how-to-convert-pandas-dataframe-into-the-numpy-array-with-column-names",
        "title": "How to convert pandas dataframe into the numpy array with column names?",
        "body": "<p>How can I convert pandas <code>DataFrame</code> into the following Numpy array with column names?</p>\n<pre><code>array([('Heidi Mitchell', 'uboyd@hotmail.com', 74, 52, 'female', '1121', 'cancer', '03/06/2018'),\n       ('Kimberly Kent', 'wilsoncarla@mitchell-gree', 63, 51, 'male', '2003', 'cancer', '16/06/2017')],\n      dtype=[('name', '&lt;U16'), ('email', '&lt;U25'), ('age', '&lt;i4'), ('weight', '&lt;i4'), ('gender', '&lt;U10'), ('zipcode', '&lt;U6'), ('diagnosis', '&lt;U6'), ('dob', '&lt;U16')])\n</code></pre>\n<p>This is my pandas DataFrame <code>df</code>:</p>\n<pre><code>col1  col2\n3     5\n3     1\n4     5    \n1     5\n2     2\n</code></pre>\n<p>I tried to convert it as follows:</p>\n<pre><code>import numpy as np\n\ndt = np.dtype([('col1', np.int32), ('col2', np.int32)])\narr = np.array(df.values, dtype=dt)\n</code></pre>\n<p>But it gives me the output as follows:</p>\n<pre><code>array([[(3, 5), (3, 1)],\n      ...\n      dtype=[('col1', '&lt;i4'), ('col2', '&lt;i4')])\n</code></pre>\n<p>For some reason, the rows of data are grouped <code>[(3, 5), (3, 1)]</code> instead of <code>[(3, 5), (3, 1), (4, 5), (1, 5), (1, 2)]</code>.</p>\n",
        "answer_body": "<p>Use the pandas function <code>to_records()</code>, which converts a dataframe to a numpy record array. the link is the following: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_records.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_records.html</a></p>\n<p>Some examples given in the website are the following:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n                       index=['a', 'b'])\n&gt;&gt;&gt; df\n   A     B\na  1  0.50\nb  2  0.75\n&gt;&gt;&gt; df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('index', 'O'), ('A', '&lt;i8'), ('B', '&lt;f8')])\n</code></pre>\n<p>The index can be excluded from the record array:</p>\n<pre><code>&gt;&gt;&gt; df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '&lt;i8'), ('B', '&lt;f8')])\n</code></pre>\n",
        "question_body": "<p>How can I convert pandas <code>DataFrame</code> into the following Numpy array with column names?</p>\n<pre><code>array([('Heidi Mitchell', 'uboyd@hotmail.com', 74, 52, 'female', '1121', 'cancer', '03/06/2018'),\n       ('Kimberly Kent', 'wilsoncarla@mitchell-gree', 63, 51, 'male', '2003', 'cancer', '16/06/2017')],\n      dtype=[('name', '&lt;U16'), ('email', '&lt;U25'), ('age', '&lt;i4'), ('weight', '&lt;i4'), ('gender', '&lt;U10'), ('zipcode', '&lt;U6'), ('diagnosis', '&lt;U6'), ('dob', '&lt;U16')])\n</code></pre>\n<p>This is my pandas DataFrame <code>df</code>:</p>\n<pre><code>col1  col2\n3     5\n3     1\n4     5    \n1     5\n2     2\n</code></pre>\n<p>I tried to convert it as follows:</p>\n<pre><code>import numpy as np\n\ndt = np.dtype([('col1', np.int32), ('col2', np.int32)])\narr = np.array(df.values, dtype=dt)\n</code></pre>\n<p>But it gives me the output as follows:</p>\n<pre><code>array([[(3, 5), (3, 1)],\n      ...\n      dtype=[('col1', '&lt;i4'), ('col2', '&lt;i4')])\n</code></pre>\n<p>For some reason, the rows of data are grouped <code>[(3, 5), (3, 1)]</code> instead of <code>[(3, 5), (3, 1), (4, 5), (1, 5), (1, 2)]</code>.</p>\n",
        "formatted_input": {
            "qid": 66934423,
            "link": "https://stackoverflow.com/questions/66934423/how-to-convert-pandas-dataframe-into-the-numpy-array-with-column-names",
            "question": {
                "title": "How to convert pandas dataframe into the numpy array with column names?",
                "ques_desc": "How can I convert pandas into the following Numpy array with column names? This is my pandas DataFrame : I tried to convert it as follows: But it gives me the output as follows: For some reason, the rows of data are grouped instead of . "
            },
            "io": [
                "col1  col2\n3     5\n3     1\n4     5    \n1     5\n2     2\n",
                "[(3, 5), (3, 1), (4, 5), (1, 5), (1, 2)]"
            ],
            "answer": {
                "ans_desc": "Use the pandas function , which converts a dataframe to a numpy record array. the link is the following: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_records.html Some examples given in the website are the following: The index can be excluded from the record array: ",
                "code": [
                    ">>> df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '<i8'), ('B', '<f8')])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "indexing"
        ],
        "owner": {
            "reputation": 2461,
            "user_id": 6630397,
            "user_type": "registered",
            "accept_rate": 42,
            "profile_image": "https://i.stack.imgur.com/HbAYT.png?s=128&g=1",
            "display_name": "s.k",
            "link": "https://stackoverflow.com/users/6630397/s-k"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 66929825,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1617443987,
        "creation_date": 1617442948,
        "last_edit_date": 1617443458,
        "question_id": 66929674,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66929674/duplicate-rows-and-rename-dataframe-indexes-using-a-list-of-suffixes",
        "title": "Duplicate rows and rename DataFrame indexes using a list of suffixes",
        "body": "<p>I have a <a href=\"https://pandas.pydata.org/docs/reference/index.html\" rel=\"nofollow noreferrer\">pandas</a> DataFrame object <code>df</code> as follow:</p>\n<pre class=\"lang-py prettyprint-override\"><code>              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13\nobject                                                                  \nA            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nC            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nD            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nE            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\nF            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0\nG            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0\n</code></pre>\n<p>for which I'd like to duplicate each using a list of suffixes:</p>\n<pre class=\"lang-py prettyprint-override\"><code>              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13\nobject                                                                  \nA_XS         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_S          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_M          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_L          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_XL         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XS         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_S          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_M          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_L          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XL         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nC_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_S          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_M          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_L          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nD_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_S          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_M          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_L          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nE            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\nF            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0\nG            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0\n</code></pre>\n<p>The list of suffixes is: <code>list_of_suffixes = ['XS', 'S', 'M', 'L', 'XL', '']</code> and the list of indexes that must be changed is: <code>list_init = ['A', 'B', 'C', 'D']</code>.<br />\nNotice <code>E</code>, <code>F</code> and <code>G</code> are left untouched from the original DataFrame.</p>\n<p>From this answer: <a href=\"https://stackoverflow.com/a/50490890/6630397\">https://stackoverflow.com/a/50490890/6630397</a> I was able to duplicate the desired rows of my initial DataFrame <code>df</code> to the right number according to the length of the list <code>list_of_suffixes</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>list_init = ['A', 'B', 'C', 'D']\nlist_of_suffixes = ['XS', 'S', 'M', 'L', 'XL', '']\nreps = [len(list_of_suffixes) if k in list_init else 1 for k in df.index]\ndf = df.loc[np.repeat(df.index.values, reps)]\n</code></pre>\n<p>But now all my DataFrame indices are an array of 10 of each <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code> (except for <code>E</code>, <code>F</code> and <code>G</code> where there are only 1 row) where I'd like them to follow the pattern of suffixes from <code>list_of_suffixes</code> as shown here above.</p>\n<p>How could I <em>elegantly</em> achieve that with good performances?</p>\n<p>(note: if it's much better to work from a column containing the indexes it's also fine, because my objects <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code>, <code>F</code> and <code>G</code> in the index column previously come from a standalone column named <code>objects</code>).</p>\n",
        "answer_body": "<p>You should first compute a list containing the new indexes for each row, then explode that list:</p>\n<pre><code>newvals = [['{}{}'.format(i,j) for j in ('_' + k if k != '' else k\n                                         for k in list_of_suffixes)]\n           if i in list_init else [i] for i in df.index]\ndf.reset_index().assign(object=newvals).explode('object').set_index('object')\n</code></pre>\n<p>gives as expected:</p>\n<pre><code>        P0  P1  P2  P3  P4  P5   P6   P7   P8   P9  P10  P11  P12  P13\nobject                                                                \nA_XS   NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_S    NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_M    NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_L    NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_XL   NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA      NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XS   NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_S    NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_M    NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_L    NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XL   NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB      NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nC_XS   NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_S    NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_M    NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_L    NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_XL   NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC      NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nD_XS   NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_S    NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_M    NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_L    NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_XL   NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD      NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nE      NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\nF      NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0\nG      NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0\n</code></pre>\n",
        "question_body": "<p>I have a <a href=\"https://pandas.pydata.org/docs/reference/index.html\" rel=\"nofollow noreferrer\">pandas</a> DataFrame object <code>df</code> as follow:</p>\n<pre class=\"lang-py prettyprint-override\"><code>              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13\nobject                                                                  \nA            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nC            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nD            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nE            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\nF            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0\nG            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0\n</code></pre>\n<p>for which I'd like to duplicate each using a list of suffixes:</p>\n<pre class=\"lang-py prettyprint-override\"><code>              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13\nobject                                                                  \nA_XS         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_S          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_M          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_L          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_XL         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XS         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_S          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_M          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_L          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XL         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nC_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_S          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_M          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_L          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nD_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_S          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_M          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_L          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nE            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\nF            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0\nG            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0\n</code></pre>\n<p>The list of suffixes is: <code>list_of_suffixes = ['XS', 'S', 'M', 'L', 'XL', '']</code> and the list of indexes that must be changed is: <code>list_init = ['A', 'B', 'C', 'D']</code>.<br />\nNotice <code>E</code>, <code>F</code> and <code>G</code> are left untouched from the original DataFrame.</p>\n<p>From this answer: <a href=\"https://stackoverflow.com/a/50490890/6630397\">https://stackoverflow.com/a/50490890/6630397</a> I was able to duplicate the desired rows of my initial DataFrame <code>df</code> to the right number according to the length of the list <code>list_of_suffixes</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>list_init = ['A', 'B', 'C', 'D']\nlist_of_suffixes = ['XS', 'S', 'M', 'L', 'XL', '']\nreps = [len(list_of_suffixes) if k in list_init else 1 for k in df.index]\ndf = df.loc[np.repeat(df.index.values, reps)]\n</code></pre>\n<p>But now all my DataFrame indices are an array of 10 of each <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code> (except for <code>E</code>, <code>F</code> and <code>G</code> where there are only 1 row) where I'd like them to follow the pattern of suffixes from <code>list_of_suffixes</code> as shown here above.</p>\n<p>How could I <em>elegantly</em> achieve that with good performances?</p>\n<p>(note: if it's much better to work from a column containing the indexes it's also fine, because my objects <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code>, <code>F</code> and <code>G</code> in the index column previously come from a standalone column named <code>objects</code>).</p>\n",
        "formatted_input": {
            "qid": 66929674,
            "link": "https://stackoverflow.com/questions/66929674/duplicate-rows-and-rename-dataframe-indexes-using-a-list-of-suffixes",
            "question": {
                "title": "Duplicate rows and rename DataFrame indexes using a list of suffixes",
                "ques_desc": "I have a pandas DataFrame object as follow: for which I'd like to duplicate each using a list of suffixes: The list of suffixes is: and the list of indexes that must be changed is: . Notice , and are left untouched from the original DataFrame. From this answer: https://stackoverflow.com/a/50490890/6630397 I was able to duplicate the desired rows of my initial DataFrame to the right number according to the length of the list : But now all my DataFrame indices are an array of 10 of each , , and (except for , and where there are only 1 row) where I'd like them to follow the pattern of suffixes from as shown here above. How could I elegantly achieve that with good performances? (note: if it's much better to work from a column containing the indexes it's also fine, because my objects , , , , , and in the index column previously come from a standalone column named ). "
            },
            "io": [
                "              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13\nobject                                                                  \nA            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nC            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nD            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nE            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\nF            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0\nG            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0\n",
                "              P0  P1  P2  P3  P4  P5  P6   P7   P8   P9  P10  P11  P12  P13\nobject                                                                  \nA_XS         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_S          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_M          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_L          NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA_XL         NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nA            NaN NaN NaN NaN NaN NaN 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XS         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_S          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_M          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_L          NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB_XL         NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nB            NaN NaN NaN NaN NaN NaN NaN  1.0  1.0  1.0  1.0  1.0  1.0  1.0\nC_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_S          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_M          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_L          NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nC            NaN NaN NaN NaN NaN NaN NaN  NaN  1.0  1.0  1.0  1.0  1.0  1.0\nD_XS         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_S          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_M          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_L          NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD_XL         NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nD            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  1.0  1.0  1.0  1.0  1.0\nE            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\nF            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0\nG            NaN NaN NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0\n"
            ],
            "answer": {
                "ans_desc": "You should first compute a list containing the new indexes for each row, then explode that list: gives as expected: ",
                "code": [
                    "newvals = [['{}{}'.format(i,j) for j in ('_' + k if k != '' else k\n                                         for k in list_of_suffixes)]\n           if i in list_init else [i] for i in df.index]\ndf.reset_index().assign(object=newvals).explode('object').set_index('object')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 65,
            "user_id": 14675230,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7629c6fd8648a2ad5aac3feb2f915894?s=128&d=identicon&r=PG&f=1",
            "display_name": "Piotr",
            "link": "https://stackoverflow.com/users/14675230/piotr"
        },
        "is_answered": true,
        "view_count": 18,
        "accepted_answer_id": 66846577,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1616969024,
        "creation_date": 1616968250,
        "question_id": 66846548,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66846548/assign-unique-id-to-pandas-group-but-add-one-if-repeated",
        "title": "Assign unique ID to Pandas group but add one if repeated",
        "body": "<p>I couldn't find a solution and want something faster than what I already have. So, the idea is to assign a unique ID for 'fruit' column, e.g.</p>\n<pre><code>df = pd.DataFrame(['apple', 'apple', 'orange', 'orange', 'lemon', 'apple', 'apple', 'lemon', 'lemon'], columns=['fruit'])\n</code></pre>\n<p>However, if repeated, add 1 to the last result, so that instead of:</p>\n<pre><code>df['id'] = [0, 0, 1, 1, 2, 0, 0, 2, 2]\n</code></pre>\n<p>I will end up with:</p>\n<pre><code>df['id'] = [0, 0, 1, 1, 2, 3, 3, 4, 4]\n</code></pre>\n<p>So it adds up until the end, even if there may only be 4 fruits changing their positions.</p>\n<p>Here is my solution but it's really slow and I bet there is something that Pandas can do, inherently:</p>\n<pre><code>def create_ids(df):\n id_df = df.copy()\n i = 0\n last_row = None\n id_df['id'] = np.nan\n for row in id_df['fruits'].iteritems():\n    if row[1] == last_row:\n        id_df['id'].loc[row[0]] = i\n        last_row = row[1]\n    else:\n        i += 1\n        id_df['id'].loc[row[0]] = i\n        last_row = row[1]\n return id_df['id']\n</code></pre>\n<p>Any ideas?</p>\n",
        "answer_body": "<p>You can use <code>.groupby()</code> followed by <code>ngroup()</code>:</p>\n<pre><code>df[&quot;id&quot;] = df.groupby((df[&quot;fruit&quot;] != df[&quot;fruit&quot;].shift(1)).cumsum()).ngroup()\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre><code>    fruit  id\n0   apple   0\n1   apple   0\n2  orange   1\n3  orange   1\n4   lemon   2\n5   apple   3\n6   apple   3\n7   lemon   4\n8   lemon   4\n</code></pre>\n<hr />\n<p>Or if you prefer <code>itertools.groupby</code>:</p>\n<pre><code>from itertools import groupby\n\ndata, i = [], 0\nfor _, g in groupby(df[&quot;fruit&quot;]):\n    data.extend([i] * sum(1 for _ in g))\n    i += 1\n\ndf[&quot;id&quot;] = data\nprint(df)\n</code></pre>\n",
        "question_body": "<p>I couldn't find a solution and want something faster than what I already have. So, the idea is to assign a unique ID for 'fruit' column, e.g.</p>\n<pre><code>df = pd.DataFrame(['apple', 'apple', 'orange', 'orange', 'lemon', 'apple', 'apple', 'lemon', 'lemon'], columns=['fruit'])\n</code></pre>\n<p>However, if repeated, add 1 to the last result, so that instead of:</p>\n<pre><code>df['id'] = [0, 0, 1, 1, 2, 0, 0, 2, 2]\n</code></pre>\n<p>I will end up with:</p>\n<pre><code>df['id'] = [0, 0, 1, 1, 2, 3, 3, 4, 4]\n</code></pre>\n<p>So it adds up until the end, even if there may only be 4 fruits changing their positions.</p>\n<p>Here is my solution but it's really slow and I bet there is something that Pandas can do, inherently:</p>\n<pre><code>def create_ids(df):\n id_df = df.copy()\n i = 0\n last_row = None\n id_df['id'] = np.nan\n for row in id_df['fruits'].iteritems():\n    if row[1] == last_row:\n        id_df['id'].loc[row[0]] = i\n        last_row = row[1]\n    else:\n        i += 1\n        id_df['id'].loc[row[0]] = i\n        last_row = row[1]\n return id_df['id']\n</code></pre>\n<p>Any ideas?</p>\n",
        "formatted_input": {
            "qid": 66846548,
            "link": "https://stackoverflow.com/questions/66846548/assign-unique-id-to-pandas-group-but-add-one-if-repeated",
            "question": {
                "title": "Assign unique ID to Pandas group but add one if repeated",
                "ques_desc": "I couldn't find a solution and want something faster than what I already have. So, the idea is to assign a unique ID for 'fruit' column, e.g. However, if repeated, add 1 to the last result, so that instead of: I will end up with: So it adds up until the end, even if there may only be 4 fruits changing their positions. Here is my solution but it's really slow and I bet there is something that Pandas can do, inherently: Any ideas? "
            },
            "io": [
                "df['id'] = [0, 0, 1, 1, 2, 0, 0, 2, 2]\n",
                "df['id'] = [0, 0, 1, 1, 2, 3, 3, 4, 4]\n"
            ],
            "answer": {
                "ans_desc": "You can use followed by : Prints: Or if you prefer : ",
                "code": [
                    "df[\"id\"] = df.groupby((df[\"fruit\"] != df[\"fruit\"].shift(1)).cumsum()).ngroup()\nprint(df)\n",
                    "    fruit  id\n0   apple   0\n1   apple   0\n2  orange   1\n3  orange   1\n4   lemon   2\n5   apple   3\n6   apple   3\n7   lemon   4\n8   lemon   4\n",
                    "from itertools import groupby\n\ndata, i = [], 0\nfor _, g in groupby(df[\"fruit\"]):\n    data.extend([i] * sum(1 for _ in g))\n    i += 1\n\ndf[\"id\"] = data\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "format",
            "dataframe",
            "tostring"
        ],
        "owner": {
            "reputation": 10278,
            "user_id": 2342399,
            "user_type": "registered",
            "accept_rate": 95,
            "profile_image": "https://www.gravatar.com/avatar/da7b3bf63455228a889008a1cd76109a?s=128&d=identicon&r=PG",
            "display_name": "Boosted_d16",
            "link": "https://stackoverflow.com/users/2342399/boosted-d16"
        },
        "is_answered": true,
        "view_count": 1174,
        "accepted_answer_id": 33283357,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1616715484,
        "creation_date": 1445523419,
        "question_id": 33283249,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/33283249/pandas-write-df-to-text-file-indent-df-to-right-by-5-white-spaces",
        "title": "pandas: write df to text file - indent df to right by 5 white spaces",
        "body": "<p>I am writing a df to a text file like so:</p>\n\n<pre><code>file = open(\"pptx_comparision_log.txt\", \"w\")\ndf= df.to_string()\nfile.write(comp_df)\n</code></pre>\n\n<p>This works fine but how can I indent my df so it sits 5 white spaces to the right. </p>\n\n<p>so from this:</p>\n\n<pre><code>                    dim_pptx  qp_pptx\nAbsolute Radio        0.0739   0.0753\nBBC Asian Network     0.0013   0.0013\nBBC Radio 1           0.1441   0.1455\nBBC Radio 1Xtra       0.0057   0.0058\nBBC Radio 2           0.2336   0.2339\n</code></pre>\n\n<p>to:</p>\n\n<pre><code>                         dim_pptx  qp_pptx\n     Absolute Radio        0.0739   0.0753\n     BBC Asian Network     0.0013   0.0013\n     BBC Radio 1           0.1441   0.1455\n     BBC Radio 1Xtra       0.0057   0.0058\n     BBC Radio 2           0.2336   0.2339\n</code></pre>\n\n<p>Is this possible?</p>\n\n<p>Thanks.</p>\n",
        "answer_body": "<p>You can just do it with string manipulation after the conversion by replacing <code>df.to_string()</code> with <code>\" \"*5 + df.to_string().replace(\"\\n\", \"\\n     \")</code>.</p>\n",
        "question_body": "<p>I am writing a df to a text file like so:</p>\n\n<pre><code>file = open(\"pptx_comparision_log.txt\", \"w\")\ndf= df.to_string()\nfile.write(comp_df)\n</code></pre>\n\n<p>This works fine but how can I indent my df so it sits 5 white spaces to the right. </p>\n\n<p>so from this:</p>\n\n<pre><code>                    dim_pptx  qp_pptx\nAbsolute Radio        0.0739   0.0753\nBBC Asian Network     0.0013   0.0013\nBBC Radio 1           0.1441   0.1455\nBBC Radio 1Xtra       0.0057   0.0058\nBBC Radio 2           0.2336   0.2339\n</code></pre>\n\n<p>to:</p>\n\n<pre><code>                         dim_pptx  qp_pptx\n     Absolute Radio        0.0739   0.0753\n     BBC Asian Network     0.0013   0.0013\n     BBC Radio 1           0.1441   0.1455\n     BBC Radio 1Xtra       0.0057   0.0058\n     BBC Radio 2           0.2336   0.2339\n</code></pre>\n\n<p>Is this possible?</p>\n\n<p>Thanks.</p>\n",
        "formatted_input": {
            "qid": 33283249,
            "link": "https://stackoverflow.com/questions/33283249/pandas-write-df-to-text-file-indent-df-to-right-by-5-white-spaces",
            "question": {
                "title": "pandas: write df to text file - indent df to right by 5 white spaces",
                "ques_desc": "I am writing a df to a text file like so: This works fine but how can I indent my df so it sits 5 white spaces to the right. so from this: to: Is this possible? Thanks. "
            },
            "io": [
                "                    dim_pptx  qp_pptx\nAbsolute Radio        0.0739   0.0753\nBBC Asian Network     0.0013   0.0013\nBBC Radio 1           0.1441   0.1455\nBBC Radio 1Xtra       0.0057   0.0058\nBBC Radio 2           0.2336   0.2339\n",
                "                         dim_pptx  qp_pptx\n     Absolute Radio        0.0739   0.0753\n     BBC Asian Network     0.0013   0.0013\n     BBC Radio 1           0.1441   0.1455\n     BBC Radio 1Xtra       0.0057   0.0058\n     BBC Radio 2           0.2336   0.2339\n"
            ],
            "answer": {
                "ans_desc": "You can just do it with string manipulation after the conversion by replacing with . ",
                "code": [
                    "\" \"*5 + df.to_string().replace(\"\\n\", \"\\n     \")"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 446,
            "user_id": 3722117,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f384c7ad4980b01083daf342efb3b323?s=128&d=identicon&r=PG",
            "display_name": "konserw",
            "link": "https://stackoverflow.com/users/3722117/konserw"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 66801485,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1616682112,
        "creation_date": 1616682009,
        "question_id": 66801447,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66801447/merge-pandas-dataframes-by-timestamps",
        "title": "Merge pandas dataframes by timestamps",
        "body": "<p>I've got a few pandas dataframes indexed with timestamps and I would like to merge them into one dataframe, matching nearest timestamp. So I would like to have for example:</p>\n<pre><code>a = \n                         CPU\n2021-03-25 13:40:44.208  70.571797\n2021-03-25 13:40:44.723  14.126870\n2021-03-25 13:40:45.228  17.182844\n\nb = \n                          X   Y\n2021-03-25 13:40:44.193   45  1\n2021-03-25 13:40:44.707   46  1\n2021-03-25 13:40:45.216   50  2\n\na + b =\n                         CPU       X   Y\n2021-03-25 13:40:44.208  70.571797 45  1\n2021-03-25 13:40:44.723  14.126870 46  1\n2021-03-25 13:40:45.228  17.182844 50  2\n</code></pre>\n<p>What exact timestamp there is going to be in final DataFrame is not important to me.</p>\n<p>BTW. Is there an easy way to leter convert &quot;absolute&quot; timestamps into time from start (either in seconds or miliseconds)? So for this example:</p>\n<pre><code>\n     CPU       X   Y\n0.0  70.571797 45  1\n0.5  14.126870 46  1\n1.0  17.182844 50  2\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge_asof.html\" rel=\"nofollow noreferrer\"><code>merge_asof</code></a> with <code>direction='nearest'</code>:</p>\n<pre><code>pd.merge_asof(df1, df2, left_index=True, right_index=True, direction='nearest')\n</code></pre>\n",
        "question_body": "<p>I've got a few pandas dataframes indexed with timestamps and I would like to merge them into one dataframe, matching nearest timestamp. So I would like to have for example:</p>\n<pre><code>a = \n                         CPU\n2021-03-25 13:40:44.208  70.571797\n2021-03-25 13:40:44.723  14.126870\n2021-03-25 13:40:45.228  17.182844\n\nb = \n                          X   Y\n2021-03-25 13:40:44.193   45  1\n2021-03-25 13:40:44.707   46  1\n2021-03-25 13:40:45.216   50  2\n\na + b =\n                         CPU       X   Y\n2021-03-25 13:40:44.208  70.571797 45  1\n2021-03-25 13:40:44.723  14.126870 46  1\n2021-03-25 13:40:45.228  17.182844 50  2\n</code></pre>\n<p>What exact timestamp there is going to be in final DataFrame is not important to me.</p>\n<p>BTW. Is there an easy way to leter convert &quot;absolute&quot; timestamps into time from start (either in seconds or miliseconds)? So for this example:</p>\n<pre><code>\n     CPU       X   Y\n0.0  70.571797 45  1\n0.5  14.126870 46  1\n1.0  17.182844 50  2\n</code></pre>\n",
        "formatted_input": {
            "qid": 66801447,
            "link": "https://stackoverflow.com/questions/66801447/merge-pandas-dataframes-by-timestamps",
            "question": {
                "title": "Merge pandas dataframes by timestamps",
                "ques_desc": "I've got a few pandas dataframes indexed with timestamps and I would like to merge them into one dataframe, matching nearest timestamp. So I would like to have for example: What exact timestamp there is going to be in final DataFrame is not important to me. BTW. Is there an easy way to leter convert \"absolute\" timestamps into time from start (either in seconds or miliseconds)? So for this example: "
            },
            "io": [
                "a = \n                         CPU\n2021-03-25 13:40:44.208  70.571797\n2021-03-25 13:40:44.723  14.126870\n2021-03-25 13:40:45.228  17.182844\n\nb = \n                          X   Y\n2021-03-25 13:40:44.193   45  1\n2021-03-25 13:40:44.707   46  1\n2021-03-25 13:40:45.216   50  2\n\na + b =\n                         CPU       X   Y\n2021-03-25 13:40:44.208  70.571797 45  1\n2021-03-25 13:40:44.723  14.126870 46  1\n2021-03-25 13:40:45.228  17.182844 50  2\n",
                "\n     CPU       X   Y\n0.0  70.571797 45  1\n0.5  14.126870 46  1\n1.0  17.182844 50  2\n"
            ],
            "answer": {
                "ans_desc": "Use with : ",
                "code": [
                    "pd.merge_asof(df1, df2, left_index=True, right_index=True, direction='nearest')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science",
            "data-analysis"
        ],
        "owner": {
            "reputation": 55,
            "user_id": 14615061,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiL7z7UP62kOiaOBQUiBh5WSKmtf-Mm-j8q-fcKgQ=k-s128",
            "display_name": "Kishan Suresh",
            "link": "https://stackoverflow.com/users/14615061/kishan-suresh"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 66782386,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1616593815,
        "creation_date": 1616593155,
        "question_id": 66782284,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66782284/python-pandas-give-comma-separated-values-into-columns-with-title",
        "title": "python pandas give comma separated values into columns with &quot;title&quot;",
        "body": "<p>I have some comma-separated data in the same column and I wish to separate each value into different columns.</p>\n<pre><code>0          13.4119837, 42.082885, 13.4119837, 42.082885\n1        11.6285463, 42.4193742, 11.6285463, 42.4193742\n2            -3.606772, 39.460299, -3.606772, 39.460299\n3            -0.515639, 38.988847, -0.515639, 38.988847\n4            -2.403309, 37.241792, -2.403309, 37.241792\n</code></pre>\n<p>I have done separation using below</p>\n<pre><code>data['column_name'].str.split(&quot;,&quot;, n = 3, expand = True)\n</code></pre>\n<p>and the output I got is</p>\n<pre><code>     0           1           2           3\n0   13.4119837  42.082885   13.4119837  42.082885\n1   11.6285463  42.4193742  11.6285463  42.4193742\n2   -3.606772   39.460299   -3.606772   39.460299\n3   -0.515639   38.988847   -0.515639   38.988847\n4   -2.403309   37.241792   -2.403309   37.241792\n</code></pre>\n<p>but I need something like below (has to give some titles for each column)</p>\n<pre><code>    minLat      maxLat       minLong     maxLong\n0   13.4119837  42.082885   13.4119837  42.082885\n1   11.6285463  42.4193742  11.6285463  42.4193742\n2   -3.606772   39.460299   -3.606772   39.460299\n3   -0.515639   38.988847   -0.515639   38.988847\n4   -2.403309   37.241792   -2.403309   37.241792\n</code></pre>\n<p>How would I do that?</p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html\" rel=\"nofollow noreferrer\"><code>df.columns</code></a>:</p>\n<pre><code>data = data['column_name'].str.split(&quot;,&quot;, n = 3, expand = True)\ndata.columns = ['minLat', 'maxLat', 'minLong', 'maxLong']\n</code></pre>\n",
        "question_body": "<p>I have some comma-separated data in the same column and I wish to separate each value into different columns.</p>\n<pre><code>0          13.4119837, 42.082885, 13.4119837, 42.082885\n1        11.6285463, 42.4193742, 11.6285463, 42.4193742\n2            -3.606772, 39.460299, -3.606772, 39.460299\n3            -0.515639, 38.988847, -0.515639, 38.988847\n4            -2.403309, 37.241792, -2.403309, 37.241792\n</code></pre>\n<p>I have done separation using below</p>\n<pre><code>data['column_name'].str.split(&quot;,&quot;, n = 3, expand = True)\n</code></pre>\n<p>and the output I got is</p>\n<pre><code>     0           1           2           3\n0   13.4119837  42.082885   13.4119837  42.082885\n1   11.6285463  42.4193742  11.6285463  42.4193742\n2   -3.606772   39.460299   -3.606772   39.460299\n3   -0.515639   38.988847   -0.515639   38.988847\n4   -2.403309   37.241792   -2.403309   37.241792\n</code></pre>\n<p>but I need something like below (has to give some titles for each column)</p>\n<pre><code>    minLat      maxLat       minLong     maxLong\n0   13.4119837  42.082885   13.4119837  42.082885\n1   11.6285463  42.4193742  11.6285463  42.4193742\n2   -3.606772   39.460299   -3.606772   39.460299\n3   -0.515639   38.988847   -0.515639   38.988847\n4   -2.403309   37.241792   -2.403309   37.241792\n</code></pre>\n<p>How would I do that?</p>\n",
        "formatted_input": {
            "qid": 66782284,
            "link": "https://stackoverflow.com/questions/66782284/python-pandas-give-comma-separated-values-into-columns-with-title",
            "question": {
                "title": "python pandas give comma separated values into columns with &quot;title&quot;",
                "ques_desc": "I have some comma-separated data in the same column and I wish to separate each value into different columns. I have done separation using below and the output I got is but I need something like below (has to give some titles for each column) How would I do that? "
            },
            "io": [
                "0          13.4119837, 42.082885, 13.4119837, 42.082885\n1        11.6285463, 42.4193742, 11.6285463, 42.4193742\n2            -3.606772, 39.460299, -3.606772, 39.460299\n3            -0.515639, 38.988847, -0.515639, 38.988847\n4            -2.403309, 37.241792, -2.403309, 37.241792\n",
                "     0           1           2           3\n0   13.4119837  42.082885   13.4119837  42.082885\n1   11.6285463  42.4193742  11.6285463  42.4193742\n2   -3.606772   39.460299   -3.606772   39.460299\n3   -0.515639   38.988847   -0.515639   38.988847\n4   -2.403309   37.241792   -2.403309   37.241792\n"
            ],
            "answer": {
                "ans_desc": "Use : ",
                "code": [
                    "data = data['column_name'].str.split(\",\", n = 3, expand = True)\ndata.columns = ['minLat', 'maxLat', 'minLong', 'maxLong']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 635,
            "user_id": 12648789,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/yvQn8.jpg?s=128&g=1",
            "display_name": "YashvanderBamel",
            "link": "https://stackoverflow.com/users/12648789/yashvanderbamel"
        },
        "is_answered": true,
        "view_count": 64,
        "accepted_answer_id": 66638380,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1615868883,
        "creation_date": 1615812058,
        "last_edit_date": 1615868883,
        "question_id": 66638218,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66638218/group-a-dataframe-on-one-column-and-take-max-from-one-column-and-its-correspondi",
        "title": "Group a dataframe on one column and take max from one column and its corresponding value from the other col",
        "body": "<p>I have a large dataframe which has a similar pattern as below:</p>\n<pre><code>    X   Y   Z\n0   a   p   2\n1   a   q   5\n2   a   r   6\n3   a   s   3\n4   b   w   10\n5   b   z   20\n6   b   y   9\n7   b   x   20\n</code></pre>\n<p>And can be constructed as:</p>\n<pre><code>df = {\n    'X': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],\n    'Y': ['p', 'q', 'r', 's', 'w', 'x', 'y', 'z'],\n    'Z': [2, 5, 6, 3, 10, 20, 9, 5]\n}\n</code></pre>\n<p>Now I want to group this dataframe by the first column i.e., <code>X</code> and take <code>max</code> from the <code>Z</code> column and its corresponding value from <code>Y</code>. And if there are two max values in <code>Z</code>, then I would like to take alphabetically first value from <code>Y</code>.</p>\n<p>So my expected result would look like:</p>\n<pre><code>X   Y   Z\na   r   6\nb   x   20\n</code></pre>\n<p>I have tried <code>groupby('X', as_index=False).agg({'Z': 'max', 'Y': 'first'})</code> but this selects max from <code>Z</code> and first from <code>Y</code> both at the same time.</p>\n<p>Additionally I know there is a <code>pd.series.groupby.nlargest(1)</code> approach, but this would take a lot of time for my dataset.</p>\n<p>Any suggestions on how could I proceed would be appreciated.</p>\n<p>Thanks in advance:)</p>\n",
        "answer_body": "<p>Let us try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\" rel=\"nofollow noreferrer\"><code>drop_duplicates</code></a>:</p>\n<pre><code>df.sort_values(['X', 'Z', 'Y'], ascending=[True, False, True]).drop_duplicates('X')\n</code></pre>\n<hr />\n<pre><code>   X  Y   Z\n2  a  r   6\n5  b  x  20\n</code></pre>\n",
        "question_body": "<p>I have a large dataframe which has a similar pattern as below:</p>\n<pre><code>    X   Y   Z\n0   a   p   2\n1   a   q   5\n2   a   r   6\n3   a   s   3\n4   b   w   10\n5   b   z   20\n6   b   y   9\n7   b   x   20\n</code></pre>\n<p>And can be constructed as:</p>\n<pre><code>df = {\n    'X': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],\n    'Y': ['p', 'q', 'r', 's', 'w', 'x', 'y', 'z'],\n    'Z': [2, 5, 6, 3, 10, 20, 9, 5]\n}\n</code></pre>\n<p>Now I want to group this dataframe by the first column i.e., <code>X</code> and take <code>max</code> from the <code>Z</code> column and its corresponding value from <code>Y</code>. And if there are two max values in <code>Z</code>, then I would like to take alphabetically first value from <code>Y</code>.</p>\n<p>So my expected result would look like:</p>\n<pre><code>X   Y   Z\na   r   6\nb   x   20\n</code></pre>\n<p>I have tried <code>groupby('X', as_index=False).agg({'Z': 'max', 'Y': 'first'})</code> but this selects max from <code>Z</code> and first from <code>Y</code> both at the same time.</p>\n<p>Additionally I know there is a <code>pd.series.groupby.nlargest(1)</code> approach, but this would take a lot of time for my dataset.</p>\n<p>Any suggestions on how could I proceed would be appreciated.</p>\n<p>Thanks in advance:)</p>\n",
        "formatted_input": {
            "qid": 66638218,
            "link": "https://stackoverflow.com/questions/66638218/group-a-dataframe-on-one-column-and-take-max-from-one-column-and-its-correspondi",
            "question": {
                "title": "Group a dataframe on one column and take max from one column and its corresponding value from the other col",
                "ques_desc": "I have a large dataframe which has a similar pattern as below: And can be constructed as: Now I want to group this dataframe by the first column i.e., and take from the column and its corresponding value from . And if there are two max values in , then I would like to take alphabetically first value from . So my expected result would look like: I have tried but this selects max from and first from both at the same time. Additionally I know there is a approach, but this would take a lot of time for my dataset. Any suggestions on how could I proceed would be appreciated. Thanks in advance:) "
            },
            "io": [
                "    X   Y   Z\n0   a   p   2\n1   a   q   5\n2   a   r   6\n3   a   s   3\n4   b   w   10\n5   b   z   20\n6   b   y   9\n7   b   x   20\n",
                "df = {\n    'X': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],\n    'Y': ['p', 'q', 'r', 's', 'w', 'x', 'y', 'z'],\n    'Z': [2, 5, 6, 3, 10, 20, 9, 5]\n}\n"
            ],
            "answer": {
                "ans_desc": "Let us try + : ",
                "code": [
                    "df.sort_values(['X', 'Z', 'Y'], ascending=[True, False, True]).drop_duplicates('X')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "string",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13286,
            "user_id": 1893275,
            "user_type": "registered",
            "accept_rate": 77,
            "profile_image": "https://i.stack.imgur.com/i7osd.jpg?s=128&g=1",
            "display_name": "TheChymera",
            "link": "https://stackoverflow.com/users/1893275/thechymera"
        },
        "is_answered": true,
        "view_count": 203731,
        "accepted_answer_id": 20027386,
        "answer_count": 6,
        "score": 156,
        "last_activity_date": 1615839674,
        "creation_date": 1384649769,
        "last_edit_date": 1548706544,
        "question_id": 20025882,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas",
        "title": "add a string prefix to each value in a string column using Pandas",
        "body": "<p>I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).\nI already figured out how to kind-of do this and I am currently using:</p>\n\n<pre><code>df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n</code></pre>\n\n<p>This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)?</p>\n\n<p>In case this is yet unclear, I would like to turn:</p>\n\n<pre><code>    col \n1     a\n2     0\n</code></pre>\n\n<p>into:</p>\n\n<pre><code>       col \n1     stra\n2     str0\n</code></pre>\n",
        "answer_body": "<pre><code>df['col'] = 'str' + df['col'].astype(str)\n</code></pre>\n\n<p>Example:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col':['a',0]})\n&gt;&gt;&gt; df\n  col\n0   a\n1   0\n&gt;&gt;&gt; df['col'] = 'str' + df['col'].astype(str)\n&gt;&gt;&gt; df\n    col\n0  stra\n1  str0\n</code></pre>\n",
        "question_body": "<p>I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).\nI already figured out how to kind-of do this and I am currently using:</p>\n\n<pre><code>df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n</code></pre>\n\n<p>This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)?</p>\n\n<p>In case this is yet unclear, I would like to turn:</p>\n\n<pre><code>    col \n1     a\n2     0\n</code></pre>\n\n<p>into:</p>\n\n<pre><code>       col \n1     stra\n2     str0\n</code></pre>\n",
        "formatted_input": {
            "qid": 20025882,
            "link": "https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas",
            "question": {
                "title": "add a string prefix to each value in a string column using Pandas",
                "ques_desc": "I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly). I already figured out how to kind-of do this and I am currently using: This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)? In case this is yet unclear, I would like to turn: into: "
            },
            "io": [
                "    col \n1     a\n2     0\n",
                "       col \n1     stra\n2     str0\n"
            ],
            "answer": {
                "ans_desc": " Example: ",
                "code": [
                    "df['col'] = 'str' + df['col'].astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 143,
            "user_id": 11571109,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-EfkIpwdiLo4/AAAAAAAAAAI/AAAAAAAADvg/7lc6lQvJAf4/photo.jpg?sz=128",
            "display_name": "jainam shah",
            "link": "https://stackoverflow.com/users/11571109/jainam-shah"
        },
        "is_answered": true,
        "view_count": 75,
        "accepted_answer_id": 59225186,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1615525750,
        "creation_date": 1575715564,
        "last_edit_date": 1615525750,
        "question_id": 59225169,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59225169/how-to-convert-data-frame-column-list-value-to-element",
        "title": "How to convert data frame column list value to element",
        "body": "<p>Hi I have a dataframe like this:</p>\n<pre><code>        A\n   0    []\n   1    [1234] \n   2    []\n</code></pre>\n<p>I want to change it into:</p>\n<pre><code>        A\n   0    0\n   1    1234 \n   2    0\n</code></pre>\n<p>How can I do that?</p>\n<p>I tries with</p>\n<pre><code>df['A'] = [a[0] if len(a) &gt; 0 else 0 for a in df['A']]\n</code></pre>\n",
        "answer_body": "<p>First idea is select first value, replace missing values to <code>0</code> from empty lists and last convert to integers:</p>\n\n<pre><code>df['A'] = df['A'].str[0].fillna(0).astype(int)\n</code></pre>\n\n<p>Or use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\" rel=\"nofollow noreferrer\"><code>numpy.where</code></a> by mask with compare lenghts of lists:</p>\n\n<pre><code>df['A'] = np.where(df['A'].str.len() == 0, 0, df['A'].str[0]) \n</code></pre>\n",
        "question_body": "<p>Hi I have a dataframe like this:</p>\n<pre><code>        A\n   0    []\n   1    [1234] \n   2    []\n</code></pre>\n<p>I want to change it into:</p>\n<pre><code>        A\n   0    0\n   1    1234 \n   2    0\n</code></pre>\n<p>How can I do that?</p>\n<p>I tries with</p>\n<pre><code>df['A'] = [a[0] if len(a) &gt; 0 else 0 for a in df['A']]\n</code></pre>\n",
        "formatted_input": {
            "qid": 59225169,
            "link": "https://stackoverflow.com/questions/59225169/how-to-convert-data-frame-column-list-value-to-element",
            "question": {
                "title": "How to convert data frame column list value to element",
                "ques_desc": "Hi I have a dataframe like this: I want to change it into: How can I do that? I tries with "
            },
            "io": [
                "        A\n   0    []\n   1    [1234] \n   2    []\n",
                "        A\n   0    0\n   1    1234 \n   2    0\n"
            ],
            "answer": {
                "ans_desc": "First idea is select first value, replace missing values to from empty lists and last convert to integers: Or use by mask with compare lenghts of lists: ",
                "code": [
                    "df['A'] = np.where(df['A'].str.len() == 0, 0, df['A'].str[0]) \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 433,
            "user_id": 10155573,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/iEayN.png?s=128&g=1",
            "display_name": "csymvoul",
            "link": "https://stackoverflow.com/users/10155573/csymvoul"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 54669237,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1615478158,
        "creation_date": 1550057111,
        "last_edit_date": 1615478158,
        "question_id": 54669058,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54669058/find-average-of-every-column-in-a-dataframe-grouped-by-column-exluding-one-val",
        "title": "Find average of every column in a dataframe, grouped by column, exluding one value",
        "body": "<p>I have a Dataframe like the one presented below:</p>\n<pre><code>    CPU Memory Disk  Label\n0    21     28   29      0\n1    46     53   55      1\n2    48     45   49      2\n3    48     52   50      3\n4    51     54   55      4\n5    45     50   56      5\n6    50     83   44     -1 \n</code></pre>\n<p>What I want is to <code>grouby</code> and find the average of each label. So far I have this\n<code>dataset.groupby('Label')['CPU', 'Memory', 'Disk'].mean()</code> which works just fine and get the results as follows:</p>\n<pre><code>Label           CPU     Memory       Disk \n    -1     46.441176  53.882353  54.176471\n     0     48.500000  58.500000  60.750000\n     1     45.000000  51.000000  60.000000\n     2     54.000000  49.000000  56.000000\n     3     55.000000  71.500000  67.500000\n     4     53.000000  70.000000  71.000000\n     5     21.333333  30.000000  30.666667\n</code></pre>\n<p>The only thing I haven't yet found is how to exclude everything that is labeled as <code>-1</code>. Is there a way to do that?</p>\n",
        "answer_body": "<p>You could filter the dataframe before grouping:</p>\n<pre><code># Exclude rows with Label=-1\ndataset = dataset.loc[dataset['Label'] != -1]\n\n# Group by on filtered result\ndataset.groupby('Label')['CPU', 'Memory', 'Disk'].mean()\n</code></pre>\n",
        "question_body": "<p>I have a Dataframe like the one presented below:</p>\n<pre><code>    CPU Memory Disk  Label\n0    21     28   29      0\n1    46     53   55      1\n2    48     45   49      2\n3    48     52   50      3\n4    51     54   55      4\n5    45     50   56      5\n6    50     83   44     -1 \n</code></pre>\n<p>What I want is to <code>grouby</code> and find the average of each label. So far I have this\n<code>dataset.groupby('Label')['CPU', 'Memory', 'Disk'].mean()</code> which works just fine and get the results as follows:</p>\n<pre><code>Label           CPU     Memory       Disk \n    -1     46.441176  53.882353  54.176471\n     0     48.500000  58.500000  60.750000\n     1     45.000000  51.000000  60.000000\n     2     54.000000  49.000000  56.000000\n     3     55.000000  71.500000  67.500000\n     4     53.000000  70.000000  71.000000\n     5     21.333333  30.000000  30.666667\n</code></pre>\n<p>The only thing I haven't yet found is how to exclude everything that is labeled as <code>-1</code>. Is there a way to do that?</p>\n",
        "formatted_input": {
            "qid": 54669058,
            "link": "https://stackoverflow.com/questions/54669058/find-average-of-every-column-in-a-dataframe-grouped-by-column-exluding-one-val",
            "question": {
                "title": "Find average of every column in a dataframe, grouped by column, exluding one value",
                "ques_desc": "I have a Dataframe like the one presented below: What I want is to and find the average of each label. So far I have this which works just fine and get the results as follows: The only thing I haven't yet found is how to exclude everything that is labeled as . Is there a way to do that? "
            },
            "io": [
                "    CPU Memory Disk  Label\n0    21     28   29      0\n1    46     53   55      1\n2    48     45   49      2\n3    48     52   50      3\n4    51     54   55      4\n5    45     50   56      5\n6    50     83   44     -1 \n",
                "Label           CPU     Memory       Disk \n    -1     46.441176  53.882353  54.176471\n     0     48.500000  58.500000  60.750000\n     1     45.000000  51.000000  60.000000\n     2     54.000000  49.000000  56.000000\n     3     55.000000  71.500000  67.500000\n     4     53.000000  70.000000  71.000000\n     5     21.333333  30.000000  30.666667\n"
            ],
            "answer": {
                "ans_desc": "You could filter the dataframe before grouping: ",
                "code": [
                    "# Exclude rows with Label=-1\ndataset = dataset.loc[dataset['Label'] != -1]\n\n# Group by on filtered result\ndataset.groupby('Label')['CPU', 'Memory', 'Disk'].mean()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 15369326,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/aaa3f16ee88eb84a85d30798a238b84d?s=128&d=identicon&r=PG&f=1",
            "display_name": "RisingUp2",
            "link": "https://stackoverflow.com/users/15369326/risingup2"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 66568191,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1615391348,
        "creation_date": 1615390333,
        "question_id": 66567933,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t",
        "title": "Can I shift specific values in one data column to another column while keeping the other values unchanged?",
        "body": "<p>Here is an example dataset that I have:</p>\n<pre><code>C1      C2\n 1       1\nNaN      1\n 2       0\nNaN      0\nNaN      1\n 1       1\n 2       2\n 2       2\nNaN      1\n</code></pre>\n<p>I want to take all the values that have &quot;1&quot; in them in the Column &quot;C2&quot; and shift them to replace the adjacent values in column &quot;C1&quot;.</p>\n<p>So the output should look like:</p>\n<pre><code>C1      C2\n 1       1\n 1       1\n 2       0\n NaN     0\n 1       1\n 1       1\n 2       2\n 2       2\n 1       1\n\n</code></pre>\n<p>Alternatively, I could create a new column with these values replaced. Main point is, that I need all the &quot;1s&quot; in C2 TO replace the NaN values in C1.</p>\n<p>I can't do find all NaN and replace with 1, because there are some NaN values that should stay in C1.</p>\n<p>Is there a way to do this?</p>\n<p>Thanks for the help in advance.</p>\n",
        "answer_body": "<p>You could use the <code>DF.mask()</code> api to apply values from one column to another in rows where some condition is true (e.g. ==1).</p>\n<pre><code>value = 1\nsource_col = 1\ntarget_col = 0\n\ncondition = df[source_col] == value\n\ndf[target_col] = df[target_col].mask(condition,\n                                     df[source_col])\n</code></pre>\n",
        "question_body": "<p>Here is an example dataset that I have:</p>\n<pre><code>C1      C2\n 1       1\nNaN      1\n 2       0\nNaN      0\nNaN      1\n 1       1\n 2       2\n 2       2\nNaN      1\n</code></pre>\n<p>I want to take all the values that have &quot;1&quot; in them in the Column &quot;C2&quot; and shift them to replace the adjacent values in column &quot;C1&quot;.</p>\n<p>So the output should look like:</p>\n<pre><code>C1      C2\n 1       1\n 1       1\n 2       0\n NaN     0\n 1       1\n 1       1\n 2       2\n 2       2\n 1       1\n\n</code></pre>\n<p>Alternatively, I could create a new column with these values replaced. Main point is, that I need all the &quot;1s&quot; in C2 TO replace the NaN values in C1.</p>\n<p>I can't do find all NaN and replace with 1, because there are some NaN values that should stay in C1.</p>\n<p>Is there a way to do this?</p>\n<p>Thanks for the help in advance.</p>\n",
        "formatted_input": {
            "qid": 66567933,
            "link": "https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t",
            "question": {
                "title": "Can I shift specific values in one data column to another column while keeping the other values unchanged?",
                "ques_desc": "Here is an example dataset that I have: I want to take all the values that have \"1\" in them in the Column \"C2\" and shift them to replace the adjacent values in column \"C1\". So the output should look like: Alternatively, I could create a new column with these values replaced. Main point is, that I need all the \"1s\" in C2 TO replace the NaN values in C1. I can't do find all NaN and replace with 1, because there are some NaN values that should stay in C1. Is there a way to do this? Thanks for the help in advance. "
            },
            "io": [
                "C1      C2\n 1       1\nNaN      1\n 2       0\nNaN      0\nNaN      1\n 1       1\n 2       2\n 2       2\nNaN      1\n",
                "C1      C2\n 1       1\n 1       1\n 2       0\n NaN     0\n 1       1\n 1       1\n 2       2\n 2       2\n 1       1\n\n"
            ],
            "answer": {
                "ans_desc": "You could use the api to apply values from one column to another in rows where some condition is true (e.g. ==1). ",
                "code": [
                    "value = 1\nsource_col = 1\ntarget_col = 0\n\ncondition = df[source_col] == value\n\ndf[target_col] = df[target_col].mask(condition,\n                                     df[source_col])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-manipulation"
        ],
        "owner": {
            "reputation": 266,
            "user_id": 14291703,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5b33aa323a7e9a4873f79f4a9f494840?s=128&d=identicon&r=PG&f=1",
            "display_name": "royalewithcheese",
            "link": "https://stackoverflow.com/users/14291703/royalewithcheese"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 66545341,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1615357752,
        "creation_date": 1615285701,
        "last_edit_date": 1615355875,
        "question_id": 66545100,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66545100/how-to-transform-rows-of-other-columns-to-columns-on-the-basis-of-unique-values",
        "title": "How to transform rows of other columns to columns on the basis of unique values of a column?",
        "body": "<p>Suppose I have a df in the following structure,</p>\n<pre><code>column1 | column2 | column3 | column4 | column5 | column6 | column7\n   A    |    B    |    C    |    10   |    78   |   12    |  202001\n   A    |    B    |    D    |    21   |    64   |   87    |  202001\n   A    |    B    |    E    |    21   |    64   |   87    |  202001\n   X    |    K    |    C    |    54   |    23   |   23    |  202001\n   X    |    K    |    D    |    21   |    55   |   87    |  202001\n   X    |    K    |    E    |    21   |    43   |   22    |  202001\n   A    |    B    |    C    |    10   |    78   |   12    |  202002\n   A    |    B    |    D    |    23   |    64   |   87    |  202002\n   A    |    B    |    E    |    21   |    11   |   34    |  202002\n   Z    |    K    |    C    |    10   |    78   |   12    |  202002\n   Z    |    K    |    D    |    21   |    13   |   56    |  202002\n   Z    |    K    |    E    |    12   |    77   |   34    |  202002\n</code></pre>\n<p>relation between column1 to column2 - one to many</p>\n<p>relation between column2 to column1 - one to many</p>\n<p>Expected Output:</p>\n<pre><code>column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 | column4_202002 | column5_202002 | column6_202002 |\n   A    |    B    |    C    |      10        |       78       |       12       |      10        |      78        |      12        |\n   A    |    B    |    D    |      21        |       64       |       87       |      23        |      64        |      87        |\n   A    |    B    |    E    |      21        |       64       |       87       |      21        |      11        |      34        |   \n   X    |    K    |    C    |      54        |       23       |       23       |       0        |       0        |       0        |   \n   X    |    K    |    D    |      21        |       55       |       87       |       0        |       0        |       0        |   \n   X    |    K    |    E    |      21        |       43       |       22       |       0        |       0        |       0        |    \n   Z    |    K    |    C    |       0        |        0       |        0       |      10        |      78        |      12        |    \n   Z    |    K    |    D    |       0        |        0       |        0       |      21        |      13        |      56        |   \n   Z    |    K    |    E    |       0        |        0       |        0       |      12        |      77        |      34        |  \n</code></pre>\n<p>Also, while transforming, for every column7 can I create an empty column right beside column6_yyyymm?</p>\n<p>Final Output,</p>\n<pre><code>column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 |   empty_202001 | column4_202002 | column5_202002 | column6_202002 | empty_202002 ....\n   A    |    B    |    C    |      10        |       78       |       12       |                |      10        |      78        |      12        |\n   A    |    B    |    D    |      21        |       64       |       87       |                |      23        |      64        |      87        |\n   A    |    B    |    E    |      21        |       64       |       87       |                |      21        |      11        |      34        |   \n   X    |    K    |    C    |      54        |       23       |       23       |                |      0         |       0        |       0        |   \n   X    |    K    |    D    |      21        |       55       |       87       |                |      0         |       0        |       0        |   \n   X    |    K    |    E    |      21        |       43       |       22       |                |      0         |       0        |       0        |    \n   Z    |    K    |    C    |       0        |        0       |        0       |                |      10        |      78        |      12        |    \n   Z    |    K    |    D    |       0        |        0       |        0       |                |      21        |      13        |      56        |   \n   Z    |    K    |    E    |       0        |        0       |        0       |                |      12        |      77        |      34        | \n</code></pre>\n<p>How can I achieve Final Output using a python function and/or pandas library? If there is anything unclear please let me know.</p>\n<p>UPDATE:</p>\n<p>For all empty_yyyymm columns I want to implement the following function,</p>\n<pre><code>        def get_final(row):\n        if row['column2'].isin(['H', 'S', 'Z']):\n            return 0\n        elif row['column4_yyyymm'] + row['column5_yyyymm'] - row['column6_yyyymm'] &lt; 0 and not row['column2'].isin(['H', 'S', 'Z']):\n            return 0\n        else:\n            return row['column4_yyyymm'] + row['column5_yyyymm'] - row['column6_yyyymm']\n</code></pre>\n<p>How can achieve this too?</p>\n<p>Note: yyyymm is generic way of referring column7. It is not actually a column.</p>\n",
        "answer_body": "<p>First create empty column by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\" rel=\"nofollow noreferrer\"><code>DataFrame.assign</code></a>, then reshape by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.set_index</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html\" rel=\"nofollow noreferrer\"><code>DataFrame.unstack</code></a> and sorting datetimes in second level by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.sort_index</code></a>:</p>\n<pre><code>df = (df.assign(empty = np.nan)\n        .set_index(['column1','column2','column3','column7'])\n        .unstack(fill_value=0)\n        .sort_index(level=1, axis=1))\n</code></pre>\n<p>Then set values missing to all <code>empty</code> columns, flatten <code>MultiIndex in columns</code> by <code>map</code> and last convert <code>index</code> to columns by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.reset_index</code></a>:</p>\n<pre><code>df['empty'] = np.nan\n#if need fill by empty string\n#df['empty'] = ''\ndf.columns = df.columns.map(lambda x: f'{x[0]}_{x[1]}')\ndf = df.reset_index()\nprint (df)\n  column1 column2 column3  column4_202001  column5_202001  column6_202001  \\\n0       A       B       C              10              78              12   \n1       A       B       D              21              64              87   \n2       A       B       E              21              64              87   \n3       X       K       C              54              23              23   \n4       X       K       D              21              55              87   \n5       X       K       E              21              43              22   \n6       Z       K       C               0               0               0   \n7       Z       K       D               0               0               0   \n8       Z       K       E               0               0               0   \n\n   empty_202001  column4_202002  column5_202002  column6_202002  empty_202002  \n0           NaN              10              78              12           NaN  \n1           NaN              23              64              87           NaN  \n2           NaN              21              11              34           NaN  \n3           NaN               0               0               0           NaN  \n4           NaN               0               0               0           NaN  \n5           NaN               0               0               0           NaN  \n6           NaN              10              78              12           NaN  \n7           NaN              21              13              56           NaN  \n8           NaN              12              77              34           NaN  \n</code></pre>\n<p>EDIT: First count new column <code>empty</code> by conditions and then apply solution above without set <code>NaN</code> like:</p>\n<pre><code>m1 = df['column2'].isin(['H', 'S', 'Z'])\n\ns = df['column4'] + df['column5'] - df['column6']\nm2 = (s &lt; 0) &amp; ~m1\n\nout = np.where(m1 | m2, 0, s)\ndf = (df.assign(empty = out)\n        .set_index(['column1','column2','column3','column7'])\n        .unstack(fill_value=0)\n        .sort_index(level=1, axis=1))\n\ndf.columns = df.columns.map(lambda x: f'{x[0]}_{x[1]}')\ndf = df.reset_index()\n</code></pre>\n<hr />\n<pre><code>print (df)\n  column1 column2 column3  column4_202001  column5_202001  column6_202001  \\\n0       A       B       C              10              78              12   \n1       A       B       D              21              64              87   \n2       A       B       E              21              64              87   \n3       X       K       C              54              23              23   \n4       X       K       D              21              55              87   \n5       X       K       E              21              43              22   \n6       Z       K       C               0               0               0   \n7       Z       K       D               0               0               0   \n8       Z       K       E               0               0               0   \n\n   empty_202001  column4_202002  column5_202002  column6_202002  empty_202002  \n0            76              10              78              12            76  \n1             0              23              64              87             0  \n2             0              21              11              34             0  \n3            54               0               0               0             0  \n4             0               0               0               0             0  \n5            42               0               0               0             0  \n6             0              10              78              12            76  \n7             0              21              13              56             0  \n8             0              12              77              34            55  \n</code></pre>\n",
        "question_body": "<p>Suppose I have a df in the following structure,</p>\n<pre><code>column1 | column2 | column3 | column4 | column5 | column6 | column7\n   A    |    B    |    C    |    10   |    78   |   12    |  202001\n   A    |    B    |    D    |    21   |    64   |   87    |  202001\n   A    |    B    |    E    |    21   |    64   |   87    |  202001\n   X    |    K    |    C    |    54   |    23   |   23    |  202001\n   X    |    K    |    D    |    21   |    55   |   87    |  202001\n   X    |    K    |    E    |    21   |    43   |   22    |  202001\n   A    |    B    |    C    |    10   |    78   |   12    |  202002\n   A    |    B    |    D    |    23   |    64   |   87    |  202002\n   A    |    B    |    E    |    21   |    11   |   34    |  202002\n   Z    |    K    |    C    |    10   |    78   |   12    |  202002\n   Z    |    K    |    D    |    21   |    13   |   56    |  202002\n   Z    |    K    |    E    |    12   |    77   |   34    |  202002\n</code></pre>\n<p>relation between column1 to column2 - one to many</p>\n<p>relation between column2 to column1 - one to many</p>\n<p>Expected Output:</p>\n<pre><code>column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 | column4_202002 | column5_202002 | column6_202002 |\n   A    |    B    |    C    |      10        |       78       |       12       |      10        |      78        |      12        |\n   A    |    B    |    D    |      21        |       64       |       87       |      23        |      64        |      87        |\n   A    |    B    |    E    |      21        |       64       |       87       |      21        |      11        |      34        |   \n   X    |    K    |    C    |      54        |       23       |       23       |       0        |       0        |       0        |   \n   X    |    K    |    D    |      21        |       55       |       87       |       0        |       0        |       0        |   \n   X    |    K    |    E    |      21        |       43       |       22       |       0        |       0        |       0        |    \n   Z    |    K    |    C    |       0        |        0       |        0       |      10        |      78        |      12        |    \n   Z    |    K    |    D    |       0        |        0       |        0       |      21        |      13        |      56        |   \n   Z    |    K    |    E    |       0        |        0       |        0       |      12        |      77        |      34        |  \n</code></pre>\n<p>Also, while transforming, for every column7 can I create an empty column right beside column6_yyyymm?</p>\n<p>Final Output,</p>\n<pre><code>column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 |   empty_202001 | column4_202002 | column5_202002 | column6_202002 | empty_202002 ....\n   A    |    B    |    C    |      10        |       78       |       12       |                |      10        |      78        |      12        |\n   A    |    B    |    D    |      21        |       64       |       87       |                |      23        |      64        |      87        |\n   A    |    B    |    E    |      21        |       64       |       87       |                |      21        |      11        |      34        |   \n   X    |    K    |    C    |      54        |       23       |       23       |                |      0         |       0        |       0        |   \n   X    |    K    |    D    |      21        |       55       |       87       |                |      0         |       0        |       0        |   \n   X    |    K    |    E    |      21        |       43       |       22       |                |      0         |       0        |       0        |    \n   Z    |    K    |    C    |       0        |        0       |        0       |                |      10        |      78        |      12        |    \n   Z    |    K    |    D    |       0        |        0       |        0       |                |      21        |      13        |      56        |   \n   Z    |    K    |    E    |       0        |        0       |        0       |                |      12        |      77        |      34        | \n</code></pre>\n<p>How can I achieve Final Output using a python function and/or pandas library? If there is anything unclear please let me know.</p>\n<p>UPDATE:</p>\n<p>For all empty_yyyymm columns I want to implement the following function,</p>\n<pre><code>        def get_final(row):\n        if row['column2'].isin(['H', 'S', 'Z']):\n            return 0\n        elif row['column4_yyyymm'] + row['column5_yyyymm'] - row['column6_yyyymm'] &lt; 0 and not row['column2'].isin(['H', 'S', 'Z']):\n            return 0\n        else:\n            return row['column4_yyyymm'] + row['column5_yyyymm'] - row['column6_yyyymm']\n</code></pre>\n<p>How can achieve this too?</p>\n<p>Note: yyyymm is generic way of referring column7. It is not actually a column.</p>\n",
        "formatted_input": {
            "qid": 66545100,
            "link": "https://stackoverflow.com/questions/66545100/how-to-transform-rows-of-other-columns-to-columns-on-the-basis-of-unique-values",
            "question": {
                "title": "How to transform rows of other columns to columns on the basis of unique values of a column?",
                "ques_desc": "Suppose I have a df in the following structure, relation between column1 to column2 - one to many relation between column2 to column1 - one to many Expected Output: Also, while transforming, for every column7 can I create an empty column right beside column6_yyyymm? Final Output, How can I achieve Final Output using a python function and/or pandas library? If there is anything unclear please let me know. UPDATE: For all empty_yyyymm columns I want to implement the following function, How can achieve this too? Note: yyyymm is generic way of referring column7. It is not actually a column. "
            },
            "io": [
                "column1 | column2 | column3 | column4 | column5 | column6 | column7\n   A    |    B    |    C    |    10   |    78   |   12    |  202001\n   A    |    B    |    D    |    21   |    64   |   87    |  202001\n   A    |    B    |    E    |    21   |    64   |   87    |  202001\n   X    |    K    |    C    |    54   |    23   |   23    |  202001\n   X    |    K    |    D    |    21   |    55   |   87    |  202001\n   X    |    K    |    E    |    21   |    43   |   22    |  202001\n   A    |    B    |    C    |    10   |    78   |   12    |  202002\n   A    |    B    |    D    |    23   |    64   |   87    |  202002\n   A    |    B    |    E    |    21   |    11   |   34    |  202002\n   Z    |    K    |    C    |    10   |    78   |   12    |  202002\n   Z    |    K    |    D    |    21   |    13   |   56    |  202002\n   Z    |    K    |    E    |    12   |    77   |   34    |  202002\n",
                "column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 | column4_202002 | column5_202002 | column6_202002 |\n   A    |    B    |    C    |      10        |       78       |       12       |      10        |      78        |      12        |\n   A    |    B    |    D    |      21        |       64       |       87       |      23        |      64        |      87        |\n   A    |    B    |    E    |      21        |       64       |       87       |      21        |      11        |      34        |   \n   X    |    K    |    C    |      54        |       23       |       23       |       0        |       0        |       0        |   \n   X    |    K    |    D    |      21        |       55       |       87       |       0        |       0        |       0        |   \n   X    |    K    |    E    |      21        |       43       |       22       |       0        |       0        |       0        |    \n   Z    |    K    |    C    |       0        |        0       |        0       |      10        |      78        |      12        |    \n   Z    |    K    |    D    |       0        |        0       |        0       |      21        |      13        |      56        |   \n   Z    |    K    |    E    |       0        |        0       |        0       |      12        |      77        |      34        |  \n"
            ],
            "answer": {
                "ans_desc": "First create empty column by , then reshape by with and sorting datetimes in second level by : Then set values missing to all columns, flatten by and last convert to columns by : EDIT: First count new column by conditions and then apply solution above without set like: ",
                "code": [
                    "df = (df.assign(empty = np.nan)\n        .set_index(['column1','column2','column3','column7'])\n        .unstack(fill_value=0)\n        .sort_index(level=1, axis=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 5614047,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9b0c0cfa28e5d3b5ccb3680940926c4f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dark Star",
            "link": "https://stackoverflow.com/users/5614047/dark-star"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 66524094,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1615175659,
        "creation_date": 1615170465,
        "last_edit_date": 1615173197,
        "question_id": 66523605,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66523605/copy-the-last-seen-non-empty-value-of-a-column-based-on-a-condition-in-most-effi",
        "title": "Copy the last seen non empty value of a column based on a condition in most efficient way in Pandas/Python",
        "body": "<p>I need to copy and paste the previos non-empty value of a column based on a condition. I need to do it in the most efficient way because the number of rows is a couple of millions. Using for loop will be computationally costly.</p>\n<p>So it will be highly appreciated if somebody can help me in this regard.</p>\n<pre><code>|Col_A   |Col_B   |\n|--------|--------|\n|10.2.6.1| NaN    |\n|  NaN   | 51     |\n|  NaN   | NaN    |\n|10.2.6.1| NaN    |\n|  NaN   | 64     |\n|  NaN   | NaN    |\n|  NaN   | NaN    |\n|10.2.6.1| NaN    |\n</code></pre>\n<p>Based on the condition, whenever the Col_A will have any value (not null) 10.2.6.1 in this example, the last seen value in Col_B (51,61 respectively) will be paste on that corresponding row where the Col_A value is not null. And the dataset should look like this:</p>\n<pre><code>|Col_A   |Col_B   |\n|--------|--------|\n|10.2.6.1| NaN    |\n|  NaN   | 51     |\n|  NaN   | NaN    |\n|10.2.6.1| 51     |\n|  NaN   | 64     |\n|  NaN   | NaN    |\n|  NaN   | NaN    |\n|10.2.6.1| 64     |\n</code></pre>\n<p>I tried with this code below but it's not working:</p>\n<pre><code>df.loc[df[&quot;Col_A&quot;].notnull(),'Col_B'] = df.loc[df[&quot;Col_B&quot;].notnull(),'Col_B']\n</code></pre>\n",
        "answer_body": "<p>You can forward-fill the NaN values using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ffill.html\" rel=\"nofollow noreferrer\"><code>ffill</code></a> with the most recent non-NaN value.</p>\n<p>If you want to keep the NaNs in <code>Col_B</code> then simply create a new column (<code>Col_C</code>) as follows:</p>\n<pre><code>df['Col_C'] = df['Col_B'].ffill()\n</code></pre>\n<p>Then replace the value in <code>Col_B</code> where <code>Col_A</code> has a value:</p>\n<pre><code>df.loc[df['Col_A'].notnull(), 'Col_B'] = df.loc[df['Col_A'].notnull(), 'Col_C']\ndf = df.drop(columns=['Col_C'])\n</code></pre>\n<p>Result:</p>\n<pre><code>       Col_A    Col_B\n0   10.2.6.1      NaN\n1        NaN     51.0\n2        NaN      NaN\n3   10.2.6.1     51.0\n4        NaN     64.0\n5        NaN      NaN\n6        NaN      NaN\n7   10.2.6.1     64.0\n</code></pre>\n<hr />\n<p>The above can be simplified if you do not need to keep all NaN rows. For example, it's possible to do:</p>\n<pre><code>df['Col_B'] = df['Col_B'].ffill()\ndf = df.dropna()\n</code></pre>\n<p>Result:</p>\n<pre><code>       Col_A    Col_B\n3   10.2.6.1     51.0\n7   10.2.6.1     64.0\n</code></pre>\n",
        "question_body": "<p>I need to copy and paste the previos non-empty value of a column based on a condition. I need to do it in the most efficient way because the number of rows is a couple of millions. Using for loop will be computationally costly.</p>\n<p>So it will be highly appreciated if somebody can help me in this regard.</p>\n<pre><code>|Col_A   |Col_B   |\n|--------|--------|\n|10.2.6.1| NaN    |\n|  NaN   | 51     |\n|  NaN   | NaN    |\n|10.2.6.1| NaN    |\n|  NaN   | 64     |\n|  NaN   | NaN    |\n|  NaN   | NaN    |\n|10.2.6.1| NaN    |\n</code></pre>\n<p>Based on the condition, whenever the Col_A will have any value (not null) 10.2.6.1 in this example, the last seen value in Col_B (51,61 respectively) will be paste on that corresponding row where the Col_A value is not null. And the dataset should look like this:</p>\n<pre><code>|Col_A   |Col_B   |\n|--------|--------|\n|10.2.6.1| NaN    |\n|  NaN   | 51     |\n|  NaN   | NaN    |\n|10.2.6.1| 51     |\n|  NaN   | 64     |\n|  NaN   | NaN    |\n|  NaN   | NaN    |\n|10.2.6.1| 64     |\n</code></pre>\n<p>I tried with this code below but it's not working:</p>\n<pre><code>df.loc[df[&quot;Col_A&quot;].notnull(),'Col_B'] = df.loc[df[&quot;Col_B&quot;].notnull(),'Col_B']\n</code></pre>\n",
        "formatted_input": {
            "qid": 66523605,
            "link": "https://stackoverflow.com/questions/66523605/copy-the-last-seen-non-empty-value-of-a-column-based-on-a-condition-in-most-effi",
            "question": {
                "title": "Copy the last seen non empty value of a column based on a condition in most efficient way in Pandas/Python",
                "ques_desc": "I need to copy and paste the previos non-empty value of a column based on a condition. I need to do it in the most efficient way because the number of rows is a couple of millions. Using for loop will be computationally costly. So it will be highly appreciated if somebody can help me in this regard. Based on the condition, whenever the Col_A will have any value (not null) 10.2.6.1 in this example, the last seen value in Col_B (51,61 respectively) will be paste on that corresponding row where the Col_A value is not null. And the dataset should look like this: I tried with this code below but it's not working: "
            },
            "io": [
                "|Col_A   |Col_B   |\n|--------|--------|\n|10.2.6.1| NaN    |\n|  NaN   | 51     |\n|  NaN   | NaN    |\n|10.2.6.1| NaN    |\n|  NaN   | 64     |\n|  NaN   | NaN    |\n|  NaN   | NaN    |\n|10.2.6.1| NaN    |\n",
                "|Col_A   |Col_B   |\n|--------|--------|\n|10.2.6.1| NaN    |\n|  NaN   | 51     |\n|  NaN   | NaN    |\n|10.2.6.1| 51     |\n|  NaN   | 64     |\n|  NaN   | NaN    |\n|  NaN   | NaN    |\n|10.2.6.1| 64     |\n"
            ],
            "answer": {
                "ans_desc": "You can forward-fill the NaN values using with the most recent non-NaN value. If you want to keep the NaNs in then simply create a new column () as follows: Then replace the value in where has a value: Result: The above can be simplified if you do not need to keep all NaN rows. For example, it's possible to do: Result: ",
                "code": [
                    "df.loc[df['Col_A'].notnull(), 'Col_B'] = df.loc[df['Col_A'].notnull(), 'Col_C']\ndf = df.drop(columns=['Col_C'])\n",
                    "df['Col_B'] = df['Col_B'].ffill()\ndf = df.dropna()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 75,
            "user_id": 6098378,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d32fad1ed812d51d59c547aa02306776?s=128&d=identicon&r=PG&f=1",
            "display_name": "MaxUU",
            "link": "https://stackoverflow.com/users/6098378/maxuu"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 66517734,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1615130331,
        "creation_date": 1615126885,
        "last_edit_date": 1615127939,
        "question_id": 66517526,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66517526/fill-a-dataframe-column-with-list-of-values-if-condition-is-not-satisfied-based",
        "title": "Fill a Dataframe column with list of values if condition is not satisfied based on some other column",
        "body": "<p>I have a dataframe that looks like this -</p>\n<pre><code>col_1   |   col_2\n-------------------\n&quot;red&quot;   |    21\n-------------------\n&quot;blue&quot;  |    31\n-------------------\n&quot;red&quot;   |    12\n-------------------\n&quot;blue&quot;  |    99\n-------------------\n&quot;blue&quot;  |    102\n</code></pre>\n<p>I also have a list of values like this <code>label = [1,3,2]</code></p>\n<p>I want to construct a third column <code>col_3</code> which should have &quot;<strong>Yes</strong>&quot; if the colour is &quot;<strong>red</strong>&quot; in <code>col_1</code>, else should have 1,3,2 on respective rows. Basically, should have values from the label one after the other if colour is &quot;<strong>blue</strong>&quot;.</p>\n<p><strong>Expected output</strong> -</p>\n<pre><code>col_1   |   col_2    | col_3\n---------------------------\n&quot;red&quot;   |    21      |  &quot;Yes&quot;\n-----------------------------\n&quot;blue&quot;  |    31      |  &quot;1&quot;\n------------------------------\n&quot;red&quot;   |    12      | &quot;Yes&quot;\n------------------------------\n&quot;blue&quot;  |    99      |  &quot;3&quot;\n------------------------------\n&quot;blue&quot;  |    102     |  &quot;2&quot;\n</code></pre>\n<p><strong>My Approach</strong> -</p>\n<p>I have tried to impute using <code>np.where()</code> like this</p>\n<pre><code>np.where(df[&quot;col_1&quot;]==&quot;red&quot;,&quot;Yes&quot;,labels)\n</code></pre>\n<p>, but the</p>\n<pre><code>ValueError: operands could not be broadcast together with shapes\n</code></pre>\n<p>I believe this is due to the difference in the size of <code>df</code> and <code>labels</code> (5 vs 3).</p>\n<p>Can anyone help me out, please?</p>\n<p>Thanks</p>\n<p><strong>EDIT</strong>:</p>\n<ol>\n<li>Expected Output added</li>\n<li>Made some mistake in My Approach demonstration, corrected that.</li>\n</ol>\n",
        "answer_body": "<p>You can try this with boolean masking. First, preemptively assign <code>Yes</code> to the whole column, then create a boolean mask using <code>Series.ne</code>. In your case create a mask where <code>col_1</code> values are not equal to <code>Red</code> and use that mask to populate values.</p>\n<pre><code>df['col_3'] = 'Yes'\nm = df['col_1'].ne('Red') # ne -&gt; not equal to\ndf.loc[m, 'col_3'] = label\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like this -</p>\n<pre><code>col_1   |   col_2\n-------------------\n&quot;red&quot;   |    21\n-------------------\n&quot;blue&quot;  |    31\n-------------------\n&quot;red&quot;   |    12\n-------------------\n&quot;blue&quot;  |    99\n-------------------\n&quot;blue&quot;  |    102\n</code></pre>\n<p>I also have a list of values like this <code>label = [1,3,2]</code></p>\n<p>I want to construct a third column <code>col_3</code> which should have &quot;<strong>Yes</strong>&quot; if the colour is &quot;<strong>red</strong>&quot; in <code>col_1</code>, else should have 1,3,2 on respective rows. Basically, should have values from the label one after the other if colour is &quot;<strong>blue</strong>&quot;.</p>\n<p><strong>Expected output</strong> -</p>\n<pre><code>col_1   |   col_2    | col_3\n---------------------------\n&quot;red&quot;   |    21      |  &quot;Yes&quot;\n-----------------------------\n&quot;blue&quot;  |    31      |  &quot;1&quot;\n------------------------------\n&quot;red&quot;   |    12      | &quot;Yes&quot;\n------------------------------\n&quot;blue&quot;  |    99      |  &quot;3&quot;\n------------------------------\n&quot;blue&quot;  |    102     |  &quot;2&quot;\n</code></pre>\n<p><strong>My Approach</strong> -</p>\n<p>I have tried to impute using <code>np.where()</code> like this</p>\n<pre><code>np.where(df[&quot;col_1&quot;]==&quot;red&quot;,&quot;Yes&quot;,labels)\n</code></pre>\n<p>, but the</p>\n<pre><code>ValueError: operands could not be broadcast together with shapes\n</code></pre>\n<p>I believe this is due to the difference in the size of <code>df</code> and <code>labels</code> (5 vs 3).</p>\n<p>Can anyone help me out, please?</p>\n<p>Thanks</p>\n<p><strong>EDIT</strong>:</p>\n<ol>\n<li>Expected Output added</li>\n<li>Made some mistake in My Approach demonstration, corrected that.</li>\n</ol>\n",
        "formatted_input": {
            "qid": 66517526,
            "link": "https://stackoverflow.com/questions/66517526/fill-a-dataframe-column-with-list-of-values-if-condition-is-not-satisfied-based",
            "question": {
                "title": "Fill a Dataframe column with list of values if condition is not satisfied based on some other column",
                "ques_desc": "I have a dataframe that looks like this - I also have a list of values like this I want to construct a third column which should have \"Yes\" if the colour is \"red\" in , else should have 1,3,2 on respective rows. Basically, should have values from the label one after the other if colour is \"blue\". Expected output - My Approach - I have tried to impute using like this , but the I believe this is due to the difference in the size of and (5 vs 3). Can anyone help me out, please? Thanks EDIT: Expected Output added Made some mistake in My Approach demonstration, corrected that. "
            },
            "io": [
                "col_1   |   col_2\n-------------------\n\"red\"   |    21\n-------------------\n\"blue\"  |    31\n-------------------\n\"red\"   |    12\n-------------------\n\"blue\"  |    99\n-------------------\n\"blue\"  |    102\n",
                "col_1   |   col_2    | col_3\n---------------------------\n\"red\"   |    21      |  \"Yes\"\n-----------------------------\n\"blue\"  |    31      |  \"1\"\n------------------------------\n\"red\"   |    12      | \"Yes\"\n------------------------------\n\"blue\"  |    99      |  \"3\"\n------------------------------\n\"blue\"  |    102     |  \"2\"\n"
            ],
            "answer": {
                "ans_desc": "You can try this with boolean masking. First, preemptively assign to the whole column, then create a boolean mask using . In your case create a mask where values are not equal to and use that mask to populate values. ",
                "code": [
                    "df['col_3'] = 'Yes'\nm = df['col_1'].ne('Red') # ne -> not equal to\ndf.loc[m, 'col_3'] = label\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 266,
            "user_id": 14291703,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5b33aa323a7e9a4873f79f4a9f494840?s=128&d=identicon&r=PG&f=1",
            "display_name": "royalewithcheese",
            "link": "https://stackoverflow.com/users/14291703/royalewithcheese"
        },
        "is_answered": true,
        "view_count": 208,
        "accepted_answer_id": 66471981,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1615127027,
        "creation_date": 1614602187,
        "last_edit_date": 1614788652,
        "question_id": 66422275,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66422275/how-to-manipulate-data-cell-by-cell-in-pandas-df",
        "title": "How to manipulate data cell by cell in pandas df?",
        "body": "<p>Let the sample df (df1) be,</p>\n<p><a href=\"https://i.stack.imgur.com/gCfqM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/gCfqM.png\" alt=\"sample data\" /></a></p>\n<p>We can achieve df2 or final data-frame by manipulating the data of df1 in the following manner,</p>\n<p><strong>Step 1</strong>: Remove all positive numbers including zeros</p>\n<p>After Step 1 the sample data should look like,</p>\n<p><a href=\"https://i.stack.imgur.com/DhSbv.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/DhSbv.png\" alt=\"sample data after Step1\" /></a></p>\n<p><strong>Step 2</strong>: If <code>column4</code> A row is a negative number and <code>column4</code> B is blank, then remove the -ve number of <code>column4</code> A row</p>\n<p><strong>Step 3</strong>: If <code>column4</code> A row is blank and <code>column4</code> B is a negative number, then keep the -ve number of <code>column4</code> B row</p>\n<p><a href=\"https://i.stack.imgur.com/xsDht.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xsDht.png\" alt=\"sample data after step2 and 3\" /></a></p>\n<p>After Steps 1,2 and 3 are done,</p>\n<p><strong>Step 4</strong>: If both A and B of <code>column4</code> are negative then,</p>\n<p>For each A and B row of <code>column4</code>, check the left-side (LHS) value (for a given month) of the same A and B row of <code>column4</code></p>\n<p><strong>Step 4.1</strong>: If either of the LHS values of A or B is a -ve number, then delete the current row value of B <code>column4</code> and keep the current row value of A <code>column4</code></p>\n<p>After Step 4.1, the sample data should look like this,</p>\n<p><a href=\"https://i.stack.imgur.com/B6xzD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/B6xzD.png\" alt=\"sample data after step 4.1\" /></a></p>\n<p><strong>Step 4.2</strong>:</p>\n<p>If the LHS value of A and B <code>column4</code> is blank, then keep the current row value of B <code>column4</code> and delete the current row value of A <code>column4</code></p>\n<p>Sample data after Step 4.2 should look like,</p>\n<p><a href=\"https://i.stack.imgur.com/YGae4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/YGae4.png\" alt=\"sample data after step4.2\" /></a></p>\n<p>Since we see two negative numbers still, we perform Step 4.1 again and then the final data-frame or df2 will look like,</p>\n<p><a href=\"https://i.stack.imgur.com/zwWOI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zwWOI.png\" alt=\"sample data after Step4.1\" /></a></p>\n<p>How may I achieve the above using pandas? I was able to achieve till Step 1 but have no idea as to how to proceed further. Any help would be greatly appreciated.</p>\n<p>This is the approach that I took,</p>\n<pre><code>import pandas as pd\ndf = pd.read_excel('df1.xlsx', engine='openpyxl')\ndf.to_pickle('./df1.pkl')\nunpickled_df = pd.read_pickle('./df1.pkl')\nrem_cols = ['column2', 'column3', 'column5', 'column6', 'column7']\nunpickled_df['g'] = unpickled_df.groupby(['column1', 'column4'] ).cumcount()\ndf1 = unpickled_df.drop(rem_cols, axis=1)\ndf1 = df1.set_index(['column1','g', 'column4'])\ndf1.columns = pd.to_datetime(df1.columns, format='%b-%y').strftime('%b-%y')\nfirst_date = df1.columns[0]\ndf1 = df1.unstack(-1)\ndf1 = df1.mask(df1.ge(0))\nm1 = (df1.xs('A', level=1, axis=1, drop_level=False).notna() &amp; \n      df1.xs('B', level=1, axis=1, drop_level=False).rename(columns={'B':'A'}, level=1).isna())\nm2 = (df1.xs('B', level=1, axis=1, drop_level=False).notna() &amp;\n      df1.xs('A', level=1, axis=1, drop_level=False).rename(columns={'A':'B'}, level=1).isna())\n\nm = m1.join(m2)\ndf1 = df1.mask(m)\ndf2 = df1.groupby(level=1, axis=1).shift(1, axis=1)\nmask1 = df1.notna() &amp; df2.isna() &amp; (df1.columns.get_level_values(1) == 'A')[ None, :]\nmask1[first_date] = False\nmask2 = df1.notna() &amp; df2.notna() &amp; (df1.columns.get_level_values(1) == 'B')[ None, :]\ndf1 = df1.mask(mask1).mask(mask2).stack(dropna=False)\nunpickled_df = unpickled_df[rem_cols + ['column1','g', 'column4']].join(df1, on=['column1','g', 'column4'])\n#print(unpickled_df)\n</code></pre>\n<p>Small Test data:\ndf1,</p>\n<pre><code>{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [0, 10, 0, 0], 'Mar-21': [0, 0, 70, 70], 'Apr-21': [-10, -10, -8, 60], 'May-21': [-30, -60, -10, 40], 'Jun-21': [-20, 9, -40, -20], 'Jul-21': [30, -10, 0, -20], 'Aug-21': [-30, -20, 0, -20], 'Sep-21': [0, -15, 0, -20], 'Oct-21': [0, -15, 0, -20]}\n</code></pre>\n<p>df2 (expected output),</p>\n<pre><code>{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan], 'Apr-21': [nan, -10.0, nan, nan], 'May-21': [-30.0, nan, nan, nan], 'Jun-21': [nan, nan, nan, -20.0], 'Jul-21': [nan, -10.0, nan, -20.0], 'Aug-21': [-30.0, nan, nan, -20.0], 'Sep-21': [nan, -15.0, nan, -20.0], 'Oct-21': [nan, -15.0, nan, -20.0]}\n</code></pre>\n<p>Test data:</p>\n<p>df1</p>\n<pre><code>{'column1': ['CT', 'CT', 'NBB', 'NBB', 'CT', 'CT', 'NBB', 'NBB', 'HHH', 'HHH', 'TP1', 'TP1', 'TPR', 'TPR', 'PP1', 'PP1', 'PP1', 'PP1'], 'column2': ['POUPOU', 'POUPOU', 'PRPRP', 'PRPRP', 'STDD', 'STDD', 'STDD', 'STDD', 'STEVT', 'STEVT', 'SYSYS', 'SYSYS', 'SYSYS', 'SYSYS', 'SHW', 'SHW', 'JV', 'JV'], 'column3': ['V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV'], 'column4': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'], 'column5': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'column6': ['BBB', 'BBB', 'CCC', 'CCC', 'BBB', 'BBB', 'BBB', 'BBB', 'VVV', 'VVV', 'CHCH', 'CHCH', 'CHCH', 'CHCH', 'CCC', 'CCC', 'CHCH', 'CHCH'], 'column7': ['Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21'], 'Feb-21': [11655, 0, 0, 0, 121117, 0, 14948, 0, 0, 0, 0, 0, 0, 0, 1838, 0, 0, 0], 'Mar-21': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -16474.0, -16474.0, 7000.0, 7000.0, -19946.0, -19946.0, 16084.44444444444, 0.0, 0.0, 0.0], 'Apr-21': [104815.0, 104815.0, 17949.0, 17949.0, 96132.0, 96132.0, 0.0, 0.0, -17001.0, -33475.0, -878.0, 6122.0, 8398.0, -11548.0, -5297.073170731703, -5297.073170731703, -282.0, -282.0], 'May-21': [78260.0, 183075.0, 42557.0, 60506.0, -15265.0, 80867.0, -18.0, -18.0, 21084.0, -12391.0, -1831.0, 4291.0, 2862.0, -8686.0, 5261.25, -35.8231707317027, -369.0, -651.0], 'Jun-21': [-52480.0, 130595.0, -13258.0, 47248.0, -35577.0, 45290.0, 2434.0, 2416.0, 31147.0, 18756.0, -4310.0, -19.0, -4750.0, -13436.0, -92.0, -127.8231707317027, -280.0, -931.0], 'Jul-21': [-174544.0, -43949.0, -38127.0, 9121.0, -124986.0, -79696.0, -9707.0, -7291.0, 13577.0, 32333.0, 0.0, -19.0, -15746.0, -29182.0, 93.0, -34.8231707317027, -319.0, -1250.0], 'Aug-21': [35498.0, -8451.0, -37094.0, -27973.0, 79021.0, -675.0, -1423.0, -8714.0, 32168.0, 64501.0, 0.0, -19.0, 18702.0, -10480.0, 4347.634146341465, 4312.810975609762, -341.0, -1591.0], 'Sep-21': [44195.0, 35744.0, 2039.0, -25934.0, 70959.0, 70284.0, 2816.0, -5898.0, 38359.0, 102860.0, 0.0, -19.0, 18119.0, 7639.0, 5302.222222222219, 9615.033197831981, 0.0, -1591.0], 'Oct-21': [-13163.0, 22581.0, -4773.0, -30707.0, 205080.0, 275364.0, -709.0, -6607.0, -1397.0, 101463.0, 0.0, -19.0, 0.0, 7639.0, -34.0, 9581.033197831981, 0.0, -1591.0]}\n</code></pre>\n<p>df2 (expected output) ,</p>\n<pre><code>{'column1': ['CT', 'CT', 'NBB', 'NBB', 'CT', 'CT', 'NBB', 'NBB', 'HHH', 'HHH', 'TP1', 'TP1', 'TPR', 'TPR', 'PP1', 'PP1', 'PP1', 'PP1'], 'column2': ['POUPOU', 'POUPOU', 'PRPRP', 'PRPRP', 'STDD', 'STDD', 'STDD', 'STDD', 'STEVT', 'STEVT', 'SYSYS', 'SYSYS', 'SYSYS', 'SYSYS', 'SHW', 'SHW', 'JV', 'JV'], 'column3': ['V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV'], 'column4': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'], 'column5': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'column6': ['BBB', 'BBB', 'CCC', 'CCC', 'BBB', 'BBB', 'BBB', 'BBB', 'VVV', 'VVV', 'CHCH', 'CHCH', 'CHCH', 'CHCH', 'CCC', 'CCC', 'CHCH', 'CHCH'], 'column7': ['Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21'], 'Feb-21': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan, nan, nan, nan, nan, nan, -16474.0, nan, nan, nan, -19946.0, nan, nan, nan, nan], 'Apr-21': [nan, nan, nan, nan, nan, nan, nan, nan, -17001.0, nan, nan, nan, nan, -11548.0, nan, -5297.073170731703, nan, -282.0], 'May-21': [nan, nan, nan, nan, nan, nan, nan, -18.0, nan, -12391.0, nan, nan, nan, -8686.0, nan, -35.8231707317027, -369.0, nan], 'Jun-21': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -19.0, -4750.0, nan, -92.0, nan, -280.0, nan], 'Jul-21': [nan, -43949.0, nan, nan, nan, -79696.0, nan, -7291.0, nan, nan, nan, -19.0, -15746.0, nan, nan, -34.8231707317027, -319.0, nan], 'Aug-21': [nan, -8451.0, nan, -27973.0, nan, -675.0, -1423.0, nan, nan, nan, nan, -19.0, nan, -10480.0, nan, nan, -341.0, nan], 'Sep-21': [nan, nan, nan, -25934.0, nan, nan, nan, -5898.0, nan, nan, nan, -19.0, nan, nan, nan, nan, nan, -1591.0], 'Oct-21': [nan, nan, -4773.0, nan, nan, nan, -709.0, nan, nan, nan, nan, -19.0, nan, nan, nan, nan, nan, -1591.0]}\n</code></pre>\n<p>Note: I have implemented my code on the basis of the Test data provided. The sample data is merely to focus on the columns that are supposed to be manipulated.</p>\n",
        "answer_body": "<p>What about this ?</p>\n<p>For each step, I group on <code>column1</code>, then set <code>column4</code> as index and work on the transpose matrix with your criteria. Note that I've extrapoled a bit on your criteria to match your attended results (I hope it is correct but you will have to check that).</p>\n<p>Note also that I have kept each step separate to make it easier to read. But it would be more efficient to make the grouping/indexing/transposing in one shot and work on your algorithm from there.</p>\n<pre><code>import pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None)\n\n\ndf1 = pd.DataFrame(\n        {'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [0, 10, 0, 0], 'Mar-21': [0, 0, 70, 70], 'Apr-21': [-10, -10, -8, 60], 'May-21': [-30, -60, -10, 40], 'Jun-21': [-20, 9, -40, -20], 'Jul-21': [30, -10, 0, -20], 'Aug-21': [-30, -20, 0, -20], 'Sep-21': [0, -15, 0, -20], 'Oct-21': [0, -15, 0, -20]}\n        )\n\nprint(df1)\nprint('-'*50)\n\ndf = df1.copy()\n\n#step1 :\ndf.iloc[:, 2:] = df.iloc[:, 2:].where(df.iloc[:, 2:] &lt; 0, np.nan)\n\n\nprint(df)\nprint('-'*50)\n\n#step2 :\ndef step2(df):\n    df = df.set_index(&quot;column4&quot;).T\n    ix = df[(df.A&lt;=0) &amp; (df.B.isnull())].index\n    df.loc[ix, &quot;A&quot;] = np.nan\n    return df.T\ndf = df.groupby('column1').apply(step2)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n\n#step3 :\ndef step3(df):\n    df = df.set_index(&quot;column4&quot;).T\n    ix = df[(df.A.isnull()) &amp; (df.B&gt;=0)].index\n    df.loc[ix, &quot;B&quot;] = df.loc[ix, &quot;A&quot;]\n    return df.T\ndf = df.groupby('column1').apply(step3)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n#step4 :\ndef step4(df):\n    df = df.set_index(&quot;column4&quot;).T\n    a_pos = df.columns.get_loc('A')\n    b_pos = df.columns.get_loc('B')\n    #step 4.1\n    ix = df[(df.A&lt;0) &amp; (df.B&lt;0)].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n    \n    #step 4.2\n    ix = df[(df.A&lt;0) &amp; (df.B&lt;0)].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].isnull().all(axis=1)    \n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, a_pos] = np.nan\n    \n    #step 4.1 (again)\n    ix = df[(df.A&lt;0) &amp; (df.B&lt;0)].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n        \n    return df.T\n\ndf = df.groupby('column1').apply(step4)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n</code></pre>\n<p><strong>EDIT</strong></p>\n<p>I'll assume here (based on your previous comment) that your dataframe will always be composed of A/B rows in alternance (and that the order in the dataframe is valid). We will then need to compute an artificial index to indentify each pair of rows.</p>\n<p>Note that I used a <code>fillna</code> on your columns (mainly column5) as it is good practice with the <code>groupby</code> commands. Due to multiple levels of columns, I'm not sure it will have an impact anyway...</p>\n<p>Boolean indexing begins to be tricky when managing multiple levels of columns. You will see I will compute each &quot;column&quot; to a numpy.array (using the <code>.values</code> method). Somehow, pandas won't perform the boolean match on multiple columns, I'm not exactly sure why.</p>\n<p>So this goes :</p>\n<pre><code>df = pd.DataFrame(  ...  )\n\n#Compute the unique index for each pair of rows\ndf.reset_index(drop=False, inplace=True)\nix = df.index\nix = ix[ix%2==0]\ndf.loc[ix, 'index'] = df.shift(-1).loc[ix, 'index']\n\n#step1 :\ncols = [x for x in df.columns.tolist() if not x.startswith('column') and x != &quot;index&quot;]\ndf[cols] = df[cols].where(df[cols] &lt; 0, np.nan)\n\n\ncols_index = [&quot;column4&quot;, &quot;column1&quot;, &quot;column2&quot;, &quot;column3&quot;, &quot;column5&quot;, &quot;column6&quot;, &quot;column7&quot;]\ndf[cols_index] = df[cols_index].fillna(-1)\n\n#step2 :\ndef step2(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            (df.A&lt;=0).values\n            &amp; df.B.isnull().values\n         ].index\n    df.loc[ix, &quot;A&quot;] = np.nan\n    return df.T\ndf = df.groupby('index').apply(step2)\nprint(df)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n\n#step3 :\ndef step3(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            df.A.isnull().values \n            &amp; (df.B&gt;=0).values\n            ].index\n    df.loc[ix, &quot;B&quot;] = df.loc[ix, &quot;A&quot;]\n    return df.T\ndf = df.groupby('index').apply(step3)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n#step4 :\ndef step4(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    a_pos = df.columns.get_loc('A')\n    b_pos = df.columns.get_loc('B')\n    \n    #step 4.1\n    ix = df[\n            (df.A&lt;0).values\n            &amp; (df.B&lt;0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n    \n    #step 4.2\n    ix = df[\n            (df.A&lt;0).values\n            &amp; (df.B&lt;0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].isnull().all(axis=1)    \n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, a_pos] = np.nan\n    \n    #step 4.1 (again)\n    ix = df[\n            (df.A&lt;0).values\n            &amp; (df.B&lt;0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n        \n    return df.T\n\ndf = df.groupby('index').apply(step4)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n</code></pre>\n<p>And if you want to restore your column5 :</p>\n<pre><code>df[cols_index] = df[cols_index].replace(-1, np.nan)\n</code></pre>\n",
        "question_body": "<p>Let the sample df (df1) be,</p>\n<p><a href=\"https://i.stack.imgur.com/gCfqM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/gCfqM.png\" alt=\"sample data\" /></a></p>\n<p>We can achieve df2 or final data-frame by manipulating the data of df1 in the following manner,</p>\n<p><strong>Step 1</strong>: Remove all positive numbers including zeros</p>\n<p>After Step 1 the sample data should look like,</p>\n<p><a href=\"https://i.stack.imgur.com/DhSbv.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/DhSbv.png\" alt=\"sample data after Step1\" /></a></p>\n<p><strong>Step 2</strong>: If <code>column4</code> A row is a negative number and <code>column4</code> B is blank, then remove the -ve number of <code>column4</code> A row</p>\n<p><strong>Step 3</strong>: If <code>column4</code> A row is blank and <code>column4</code> B is a negative number, then keep the -ve number of <code>column4</code> B row</p>\n<p><a href=\"https://i.stack.imgur.com/xsDht.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xsDht.png\" alt=\"sample data after step2 and 3\" /></a></p>\n<p>After Steps 1,2 and 3 are done,</p>\n<p><strong>Step 4</strong>: If both A and B of <code>column4</code> are negative then,</p>\n<p>For each A and B row of <code>column4</code>, check the left-side (LHS) value (for a given month) of the same A and B row of <code>column4</code></p>\n<p><strong>Step 4.1</strong>: If either of the LHS values of A or B is a -ve number, then delete the current row value of B <code>column4</code> and keep the current row value of A <code>column4</code></p>\n<p>After Step 4.1, the sample data should look like this,</p>\n<p><a href=\"https://i.stack.imgur.com/B6xzD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/B6xzD.png\" alt=\"sample data after step 4.1\" /></a></p>\n<p><strong>Step 4.2</strong>:</p>\n<p>If the LHS value of A and B <code>column4</code> is blank, then keep the current row value of B <code>column4</code> and delete the current row value of A <code>column4</code></p>\n<p>Sample data after Step 4.2 should look like,</p>\n<p><a href=\"https://i.stack.imgur.com/YGae4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/YGae4.png\" alt=\"sample data after step4.2\" /></a></p>\n<p>Since we see two negative numbers still, we perform Step 4.1 again and then the final data-frame or df2 will look like,</p>\n<p><a href=\"https://i.stack.imgur.com/zwWOI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zwWOI.png\" alt=\"sample data after Step4.1\" /></a></p>\n<p>How may I achieve the above using pandas? I was able to achieve till Step 1 but have no idea as to how to proceed further. Any help would be greatly appreciated.</p>\n<p>This is the approach that I took,</p>\n<pre><code>import pandas as pd\ndf = pd.read_excel('df1.xlsx', engine='openpyxl')\ndf.to_pickle('./df1.pkl')\nunpickled_df = pd.read_pickle('./df1.pkl')\nrem_cols = ['column2', 'column3', 'column5', 'column6', 'column7']\nunpickled_df['g'] = unpickled_df.groupby(['column1', 'column4'] ).cumcount()\ndf1 = unpickled_df.drop(rem_cols, axis=1)\ndf1 = df1.set_index(['column1','g', 'column4'])\ndf1.columns = pd.to_datetime(df1.columns, format='%b-%y').strftime('%b-%y')\nfirst_date = df1.columns[0]\ndf1 = df1.unstack(-1)\ndf1 = df1.mask(df1.ge(0))\nm1 = (df1.xs('A', level=1, axis=1, drop_level=False).notna() &amp; \n      df1.xs('B', level=1, axis=1, drop_level=False).rename(columns={'B':'A'}, level=1).isna())\nm2 = (df1.xs('B', level=1, axis=1, drop_level=False).notna() &amp;\n      df1.xs('A', level=1, axis=1, drop_level=False).rename(columns={'A':'B'}, level=1).isna())\n\nm = m1.join(m2)\ndf1 = df1.mask(m)\ndf2 = df1.groupby(level=1, axis=1).shift(1, axis=1)\nmask1 = df1.notna() &amp; df2.isna() &amp; (df1.columns.get_level_values(1) == 'A')[ None, :]\nmask1[first_date] = False\nmask2 = df1.notna() &amp; df2.notna() &amp; (df1.columns.get_level_values(1) == 'B')[ None, :]\ndf1 = df1.mask(mask1).mask(mask2).stack(dropna=False)\nunpickled_df = unpickled_df[rem_cols + ['column1','g', 'column4']].join(df1, on=['column1','g', 'column4'])\n#print(unpickled_df)\n</code></pre>\n<p>Small Test data:\ndf1,</p>\n<pre><code>{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [0, 10, 0, 0], 'Mar-21': [0, 0, 70, 70], 'Apr-21': [-10, -10, -8, 60], 'May-21': [-30, -60, -10, 40], 'Jun-21': [-20, 9, -40, -20], 'Jul-21': [30, -10, 0, -20], 'Aug-21': [-30, -20, 0, -20], 'Sep-21': [0, -15, 0, -20], 'Oct-21': [0, -15, 0, -20]}\n</code></pre>\n<p>df2 (expected output),</p>\n<pre><code>{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan], 'Apr-21': [nan, -10.0, nan, nan], 'May-21': [-30.0, nan, nan, nan], 'Jun-21': [nan, nan, nan, -20.0], 'Jul-21': [nan, -10.0, nan, -20.0], 'Aug-21': [-30.0, nan, nan, -20.0], 'Sep-21': [nan, -15.0, nan, -20.0], 'Oct-21': [nan, -15.0, nan, -20.0]}\n</code></pre>\n<p>Test data:</p>\n<p>df1</p>\n<pre><code>{'column1': ['CT', 'CT', 'NBB', 'NBB', 'CT', 'CT', 'NBB', 'NBB', 'HHH', 'HHH', 'TP1', 'TP1', 'TPR', 'TPR', 'PP1', 'PP1', 'PP1', 'PP1'], 'column2': ['POUPOU', 'POUPOU', 'PRPRP', 'PRPRP', 'STDD', 'STDD', 'STDD', 'STDD', 'STEVT', 'STEVT', 'SYSYS', 'SYSYS', 'SYSYS', 'SYSYS', 'SHW', 'SHW', 'JV', 'JV'], 'column3': ['V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV'], 'column4': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'], 'column5': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'column6': ['BBB', 'BBB', 'CCC', 'CCC', 'BBB', 'BBB', 'BBB', 'BBB', 'VVV', 'VVV', 'CHCH', 'CHCH', 'CHCH', 'CHCH', 'CCC', 'CCC', 'CHCH', 'CHCH'], 'column7': ['Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21'], 'Feb-21': [11655, 0, 0, 0, 121117, 0, 14948, 0, 0, 0, 0, 0, 0, 0, 1838, 0, 0, 0], 'Mar-21': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -16474.0, -16474.0, 7000.0, 7000.0, -19946.0, -19946.0, 16084.44444444444, 0.0, 0.0, 0.0], 'Apr-21': [104815.0, 104815.0, 17949.0, 17949.0, 96132.0, 96132.0, 0.0, 0.0, -17001.0, -33475.0, -878.0, 6122.0, 8398.0, -11548.0, -5297.073170731703, -5297.073170731703, -282.0, -282.0], 'May-21': [78260.0, 183075.0, 42557.0, 60506.0, -15265.0, 80867.0, -18.0, -18.0, 21084.0, -12391.0, -1831.0, 4291.0, 2862.0, -8686.0, 5261.25, -35.8231707317027, -369.0, -651.0], 'Jun-21': [-52480.0, 130595.0, -13258.0, 47248.0, -35577.0, 45290.0, 2434.0, 2416.0, 31147.0, 18756.0, -4310.0, -19.0, -4750.0, -13436.0, -92.0, -127.8231707317027, -280.0, -931.0], 'Jul-21': [-174544.0, -43949.0, -38127.0, 9121.0, -124986.0, -79696.0, -9707.0, -7291.0, 13577.0, 32333.0, 0.0, -19.0, -15746.0, -29182.0, 93.0, -34.8231707317027, -319.0, -1250.0], 'Aug-21': [35498.0, -8451.0, -37094.0, -27973.0, 79021.0, -675.0, -1423.0, -8714.0, 32168.0, 64501.0, 0.0, -19.0, 18702.0, -10480.0, 4347.634146341465, 4312.810975609762, -341.0, -1591.0], 'Sep-21': [44195.0, 35744.0, 2039.0, -25934.0, 70959.0, 70284.0, 2816.0, -5898.0, 38359.0, 102860.0, 0.0, -19.0, 18119.0, 7639.0, 5302.222222222219, 9615.033197831981, 0.0, -1591.0], 'Oct-21': [-13163.0, 22581.0, -4773.0, -30707.0, 205080.0, 275364.0, -709.0, -6607.0, -1397.0, 101463.0, 0.0, -19.0, 0.0, 7639.0, -34.0, 9581.033197831981, 0.0, -1591.0]}\n</code></pre>\n<p>df2 (expected output) ,</p>\n<pre><code>{'column1': ['CT', 'CT', 'NBB', 'NBB', 'CT', 'CT', 'NBB', 'NBB', 'HHH', 'HHH', 'TP1', 'TP1', 'TPR', 'TPR', 'PP1', 'PP1', 'PP1', 'PP1'], 'column2': ['POUPOU', 'POUPOU', 'PRPRP', 'PRPRP', 'STDD', 'STDD', 'STDD', 'STDD', 'STEVT', 'STEVT', 'SYSYS', 'SYSYS', 'SYSYS', 'SYSYS', 'SHW', 'SHW', 'JV', 'JV'], 'column3': ['V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV', 'V', 'CV'], 'column4': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'], 'column5': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'column6': ['BBB', 'BBB', 'CCC', 'CCC', 'BBB', 'BBB', 'BBB', 'BBB', 'VVV', 'VVV', 'CHCH', 'CHCH', 'CHCH', 'CHCH', 'CCC', 'CCC', 'CHCH', 'CHCH'], 'column7': ['Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Mar-21', 'Apr-21', 'Apr-21', 'Mar-21', 'Mar-21'], 'Feb-21': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan, nan, nan, nan, nan, nan, -16474.0, nan, nan, nan, -19946.0, nan, nan, nan, nan], 'Apr-21': [nan, nan, nan, nan, nan, nan, nan, nan, -17001.0, nan, nan, nan, nan, -11548.0, nan, -5297.073170731703, nan, -282.0], 'May-21': [nan, nan, nan, nan, nan, nan, nan, -18.0, nan, -12391.0, nan, nan, nan, -8686.0, nan, -35.8231707317027, -369.0, nan], 'Jun-21': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -19.0, -4750.0, nan, -92.0, nan, -280.0, nan], 'Jul-21': [nan, -43949.0, nan, nan, nan, -79696.0, nan, -7291.0, nan, nan, nan, -19.0, -15746.0, nan, nan, -34.8231707317027, -319.0, nan], 'Aug-21': [nan, -8451.0, nan, -27973.0, nan, -675.0, -1423.0, nan, nan, nan, nan, -19.0, nan, -10480.0, nan, nan, -341.0, nan], 'Sep-21': [nan, nan, nan, -25934.0, nan, nan, nan, -5898.0, nan, nan, nan, -19.0, nan, nan, nan, nan, nan, -1591.0], 'Oct-21': [nan, nan, -4773.0, nan, nan, nan, -709.0, nan, nan, nan, nan, -19.0, nan, nan, nan, nan, nan, -1591.0]}\n</code></pre>\n<p>Note: I have implemented my code on the basis of the Test data provided. The sample data is merely to focus on the columns that are supposed to be manipulated.</p>\n",
        "formatted_input": {
            "qid": 66422275,
            "link": "https://stackoverflow.com/questions/66422275/how-to-manipulate-data-cell-by-cell-in-pandas-df",
            "question": {
                "title": "How to manipulate data cell by cell in pandas df?",
                "ques_desc": "Let the sample df (df1) be, We can achieve df2 or final data-frame by manipulating the data of df1 in the following manner, Step 1: Remove all positive numbers including zeros After Step 1 the sample data should look like, Step 2: If A row is a negative number and B is blank, then remove the -ve number of A row Step 3: If A row is blank and B is a negative number, then keep the -ve number of B row After Steps 1,2 and 3 are done, Step 4: If both A and B of are negative then, For each A and B row of , check the left-side (LHS) value (for a given month) of the same A and B row of Step 4.1: If either of the LHS values of A or B is a -ve number, then delete the current row value of B and keep the current row value of A After Step 4.1, the sample data should look like this, Step 4.2: If the LHS value of A and B is blank, then keep the current row value of B and delete the current row value of A Sample data after Step 4.2 should look like, Since we see two negative numbers still, we perform Step 4.1 again and then the final data-frame or df2 will look like, How may I achieve the above using pandas? I was able to achieve till Step 1 but have no idea as to how to proceed further. Any help would be greatly appreciated. This is the approach that I took, Small Test data: df1, df2 (expected output), Test data: df1 df2 (expected output) , Note: I have implemented my code on the basis of the Test data provided. The sample data is merely to focus on the columns that are supposed to be manipulated. "
            },
            "io": [
                "{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [0, 10, 0, 0], 'Mar-21': [0, 0, 70, 70], 'Apr-21': [-10, -10, -8, 60], 'May-21': [-30, -60, -10, 40], 'Jun-21': [-20, 9, -40, -20], 'Jul-21': [30, -10, 0, -20], 'Aug-21': [-30, -20, 0, -20], 'Sep-21': [0, -15, 0, -20], 'Oct-21': [0, -15, 0, -20]}\n",
                "{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan], 'Apr-21': [nan, -10.0, nan, nan], 'May-21': [-30.0, nan, nan, nan], 'Jun-21': [nan, nan, nan, -20.0], 'Jul-21': [nan, -10.0, nan, -20.0], 'Aug-21': [-30.0, nan, nan, -20.0], 'Sep-21': [nan, -15.0, nan, -20.0], 'Oct-21': [nan, -15.0, nan, -20.0]}\n"
            ],
            "answer": {
                "ans_desc": "What about this ? For each step, I group on , then set as index and work on the transpose matrix with your criteria. Note that I've extrapoled a bit on your criteria to match your attended results (I hope it is correct but you will have to check that). Note also that I have kept each step separate to make it easier to read. But it would be more efficient to make the grouping/indexing/transposing in one shot and work on your algorithm from there. EDIT I'll assume here (based on your previous comment) that your dataframe will always be composed of A/B rows in alternance (and that the order in the dataframe is valid). We will then need to compute an artificial index to indentify each pair of rows. Note that I used a on your columns (mainly column5) as it is good practice with the commands. Due to multiple levels of columns, I'm not sure it will have an impact anyway... Boolean indexing begins to be tricky when managing multiple levels of columns. You will see I will compute each \"column\" to a numpy.array (using the method). Somehow, pandas won't perform the boolean match on multiple columns, I'm not exactly sure why. So this goes : And if you want to restore your column5 : ",
                "code": [
                    "df = pd.DataFrame(  ...  )\n\n#Compute the unique index for each pair of rows\ndf.reset_index(drop=False, inplace=True)\nix = df.index\nix = ix[ix%2==0]\ndf.loc[ix, 'index'] = df.shift(-1).loc[ix, 'index']\n\n#step1 :\ncols = [x for x in df.columns.tolist() if not x.startswith('column') and x != \"index\"]\ndf[cols] = df[cols].where(df[cols] < 0, np.nan)\n\n\ncols_index = [\"column4\", \"column1\", \"column2\", \"column3\", \"column5\", \"column6\", \"column7\"]\ndf[cols_index] = df[cols_index].fillna(-1)\n\n#step2 :\ndef step2(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            (df.A<=0).values\n            & df.B.isnull().values\n         ].index\n    df.loc[ix, \"A\"] = np.nan\n    return df.T\ndf = df.groupby('index').apply(step2)\nprint(df)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n\n#step3 :\ndef step3(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            df.A.isnull().values \n            & (df.B>=0).values\n            ].index\n    df.loc[ix, \"B\"] = df.loc[ix, \"A\"]\n    return df.T\ndf = df.groupby('index').apply(step3)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n#step4 :\ndef step4(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    a_pos = df.columns.get_loc('A')\n    b_pos = df.columns.get_loc('B')\n    \n    #step 4.1\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n    \n    #step 4.2\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].isnull().all(axis=1)    \n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, a_pos] = np.nan\n    \n    #step 4.1 (again)\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n        \n    return df.T\n\ndf = df.groupby('index').apply(step4)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "multiple-columns"
        ],
        "owner": {
            "reputation": 17,
            "user_id": 13105397,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8575d28815ef5f76cb3d2171b8c2f979?s=128&d=identicon&r=PG&f=1",
            "display_name": "Gnai",
            "link": "https://stackoverflow.com/users/13105397/gnai"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 66516567,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1615123552,
        "creation_date": 1615113037,
        "question_id": 66515491,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66515491/how-to-convert-lists-array-entries-in-a-column-to-one-row-with-different-columns",
        "title": "how to convert lists/array entries in a column to one row with different columns for each entry",
        "body": "<p>I have a dataframe where one column called <code>features</code> has its entries as lists of numbers over 1064 rows. So each row contains 6 to 7 columns with the <code>features</code> column where over each row it contains a list of numbers. I want to take this list, and spread it over the columns till while the number of columns is equal to <code>len(list)</code>. Here's an example:</p>\n<pre><code>print(df1[0])&gt;&gt;&gt;\n[-0.03980884 -0.18056028  0.11624704  0.08659928  0.02749503 -0.23401791\n  0.10772136 -0.32243717 -0.09397306 -0.08458275  0.11873401  0.10531124\n  0.11620065 -0.1100786  -0.27929837 -0.06915713 -0.11539902  0.26890758\n -0.16375561  0.00525901  0.01196074  0.15442082  0.10281886 -0.15471214\n -0.22901823  0.11486725 -0.05937155 -0.00580112 -0.25958595 -0.27098128\n -0.03174639 -0.20656739 -0.13286862 -0.07104845 -0.04765386 -0.08396237\n  0.14032942 -0.15563552 -0.17417437  0.02441286  0.06222694 -0.08691377\n  0.08214904 -0.08121296 -0.079873    0.06362587  0.06934057  0.07980402\n -0.08373277 -0.08293616 -0.07830499 -0.08762348  0.07899728 -0.04922628\n -0.02680833 -0.0853695  -0.03179847  0.00792945  0.02782207]\n</code></pre>\n<p>That's the first entry of the column <code>features</code> let's say I want to spread it over the dataframe in a way where it would be formatted like the following:</p>\n<pre><code>col0        col1         col2        col3       ......col59      \n-0.03980884 -0.18056028  0.11624704  0.08659928 ......0.02782207\n</code></pre>\n<p>And most importantly I want to iterate this process over the 1064 rows.\nThank you for your help!</p>\n",
        "answer_body": "<p>You can generate another dataframe which contains the values you have in the lists of the column <code>features</code> by using the last row of the following code (first three rows are used to generate data):</p>\n<pre><code>import numpy as np \ndf = pd.DataFrame(columns = ['features']) \ndf['features'] = list(np.random.random((6,3))) + list(np.random.random((6,4)))\nres = pd.DataFrame(df['features'].values.tolist()).add_prefix('col')\n</code></pre>\n",
        "question_body": "<p>I have a dataframe where one column called <code>features</code> has its entries as lists of numbers over 1064 rows. So each row contains 6 to 7 columns with the <code>features</code> column where over each row it contains a list of numbers. I want to take this list, and spread it over the columns till while the number of columns is equal to <code>len(list)</code>. Here's an example:</p>\n<pre><code>print(df1[0])&gt;&gt;&gt;\n[-0.03980884 -0.18056028  0.11624704  0.08659928  0.02749503 -0.23401791\n  0.10772136 -0.32243717 -0.09397306 -0.08458275  0.11873401  0.10531124\n  0.11620065 -0.1100786  -0.27929837 -0.06915713 -0.11539902  0.26890758\n -0.16375561  0.00525901  0.01196074  0.15442082  0.10281886 -0.15471214\n -0.22901823  0.11486725 -0.05937155 -0.00580112 -0.25958595 -0.27098128\n -0.03174639 -0.20656739 -0.13286862 -0.07104845 -0.04765386 -0.08396237\n  0.14032942 -0.15563552 -0.17417437  0.02441286  0.06222694 -0.08691377\n  0.08214904 -0.08121296 -0.079873    0.06362587  0.06934057  0.07980402\n -0.08373277 -0.08293616 -0.07830499 -0.08762348  0.07899728 -0.04922628\n -0.02680833 -0.0853695  -0.03179847  0.00792945  0.02782207]\n</code></pre>\n<p>That's the first entry of the column <code>features</code> let's say I want to spread it over the dataframe in a way where it would be formatted like the following:</p>\n<pre><code>col0        col1         col2        col3       ......col59      \n-0.03980884 -0.18056028  0.11624704  0.08659928 ......0.02782207\n</code></pre>\n<p>And most importantly I want to iterate this process over the 1064 rows.\nThank you for your help!</p>\n",
        "formatted_input": {
            "qid": 66515491,
            "link": "https://stackoverflow.com/questions/66515491/how-to-convert-lists-array-entries-in-a-column-to-one-row-with-different-columns",
            "question": {
                "title": "how to convert lists/array entries in a column to one row with different columns for each entry",
                "ques_desc": "I have a dataframe where one column called has its entries as lists of numbers over 1064 rows. So each row contains 6 to 7 columns with the column where over each row it contains a list of numbers. I want to take this list, and spread it over the columns till while the number of columns is equal to . Here's an example: That's the first entry of the column let's say I want to spread it over the dataframe in a way where it would be formatted like the following: And most importantly I want to iterate this process over the 1064 rows. Thank you for your help! "
            },
            "io": [
                "print(df1[0])>>>\n[-0.03980884 -0.18056028  0.11624704  0.08659928  0.02749503 -0.23401791\n  0.10772136 -0.32243717 -0.09397306 -0.08458275  0.11873401  0.10531124\n  0.11620065 -0.1100786  -0.27929837 -0.06915713 -0.11539902  0.26890758\n -0.16375561  0.00525901  0.01196074  0.15442082  0.10281886 -0.15471214\n -0.22901823  0.11486725 -0.05937155 -0.00580112 -0.25958595 -0.27098128\n -0.03174639 -0.20656739 -0.13286862 -0.07104845 -0.04765386 -0.08396237\n  0.14032942 -0.15563552 -0.17417437  0.02441286  0.06222694 -0.08691377\n  0.08214904 -0.08121296 -0.079873    0.06362587  0.06934057  0.07980402\n -0.08373277 -0.08293616 -0.07830499 -0.08762348  0.07899728 -0.04922628\n -0.02680833 -0.0853695  -0.03179847  0.00792945  0.02782207]\n",
                "col0        col1         col2        col3       ......col59      \n-0.03980884 -0.18056028  0.11624704  0.08659928 ......0.02782207\n"
            ],
            "answer": {
                "ans_desc": "You can generate another dataframe which contains the values you have in the lists of the column by using the last row of the following code (first three rows are used to generate data): ",
                "code": [
                    "import numpy as np \ndf = pd.DataFrame(columns = ['features']) \ndf['features'] = list(np.random.random((6,3))) + list(np.random.random((6,4)))\nres = pd.DataFrame(df['features'].values.tolist()).add_prefix('col')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "jupyter"
        ],
        "owner": {
            "reputation": 11,
            "user_id": 15343038,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gha7ouwtoUlViVTwYTaNOPxIfJ35k1R03EVN2GqDg=k-s128",
            "display_name": "Akash Mishra",
            "link": "https://stackoverflow.com/users/15343038/akash-mishra"
        },
        "is_answered": true,
        "view_count": 97,
        "accepted_answer_id": 66507388,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1615074827,
        "creation_date": 1615043131,
        "last_edit_date": 1615045158,
        "question_id": 66507107,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66507107/select-only-those-rows-from-a-dataframe-where-certain-columns-with-suffix-have-v",
        "title": "Select only those rows from a Dataframe where certain columns with suffix have values not equal to zero",
        "body": "<p>I want to select only those rows from a dataframe where certain columns with suffix have values not equal to zero. Also the number of columns is more so I need a generalised solution.</p>\n<p>eg:</p>\n<pre><code>import pandas as pd\ndata = {\n    'ID' : [1,2,3,4,5],\n    'M_NEW':[10,12,14,16,18],\n    'M_OLD':[10,12,14,16,18],\n    'M_DIFF':[0,0,0,0,0],\n    'CA_NEW':[10,12,16,16,18],\n    'CA_OLD':[10,12,14,16,18],\n    'CA_DIFF':[0,0,2,0,0],\n    'BC_NEW':[10,12,14,16,18],\n    'BC_OLD':[10,12,14,16,17],\n    'BC_DIFF':[0,0,0,0,1]\n}\ndf = pd.DataFrame(data)\ndf\n</code></pre>\n<p>The dataframe would be :</p>\n<pre><code>   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF\n0   1     10     10       0      10      10        0      10      10        0\n1   2     12     12       0      12      12        0      12      12        0\n2   3     14     14       0      16      14        2      14      14        0\n3   4     16     16       0      16      16        0      16      16        0\n4   5     18     18       0      18      18        0      18      17        1\n</code></pre>\n<p>The desired output is : (because of 2 in CA_DIFF and 1 in BC_DIFF)</p>\n<pre><code>   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF\n0   3     14     14       0      16      14        2      14      14        0\n1   5     18     18       0      18      18        0      18      17        1\n</code></pre>\n<p>This works with using multiple conditions but what if the number of DIFF columns are more? Like 20? Can someone provide a general solution? Thanks.</p>\n",
        "answer_body": "<p>You can do this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\n...\n# get all columns with X_DIFF\ncolumns = df.columns[df.columns.str.contains('_DIFF')]\n\n# check if any has value greater than 0\ndf[df[columns].transform(lambda x: x &gt; 0).any(axis=1)]\n\n</code></pre>\n",
        "question_body": "<p>I want to select only those rows from a dataframe where certain columns with suffix have values not equal to zero. Also the number of columns is more so I need a generalised solution.</p>\n<p>eg:</p>\n<pre><code>import pandas as pd\ndata = {\n    'ID' : [1,2,3,4,5],\n    'M_NEW':[10,12,14,16,18],\n    'M_OLD':[10,12,14,16,18],\n    'M_DIFF':[0,0,0,0,0],\n    'CA_NEW':[10,12,16,16,18],\n    'CA_OLD':[10,12,14,16,18],\n    'CA_DIFF':[0,0,2,0,0],\n    'BC_NEW':[10,12,14,16,18],\n    'BC_OLD':[10,12,14,16,17],\n    'BC_DIFF':[0,0,0,0,1]\n}\ndf = pd.DataFrame(data)\ndf\n</code></pre>\n<p>The dataframe would be :</p>\n<pre><code>   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF\n0   1     10     10       0      10      10        0      10      10        0\n1   2     12     12       0      12      12        0      12      12        0\n2   3     14     14       0      16      14        2      14      14        0\n3   4     16     16       0      16      16        0      16      16        0\n4   5     18     18       0      18      18        0      18      17        1\n</code></pre>\n<p>The desired output is : (because of 2 in CA_DIFF and 1 in BC_DIFF)</p>\n<pre><code>   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF\n0   3     14     14       0      16      14        2      14      14        0\n1   5     18     18       0      18      18        0      18      17        1\n</code></pre>\n<p>This works with using multiple conditions but what if the number of DIFF columns are more? Like 20? Can someone provide a general solution? Thanks.</p>\n",
        "formatted_input": {
            "qid": 66507107,
            "link": "https://stackoverflow.com/questions/66507107/select-only-those-rows-from-a-dataframe-where-certain-columns-with-suffix-have-v",
            "question": {
                "title": "Select only those rows from a Dataframe where certain columns with suffix have values not equal to zero",
                "ques_desc": "I want to select only those rows from a dataframe where certain columns with suffix have values not equal to zero. Also the number of columns is more so I need a generalised solution. eg: The dataframe would be : The desired output is : (because of 2 in CA_DIFF and 1 in BC_DIFF) This works with using multiple conditions but what if the number of DIFF columns are more? Like 20? Can someone provide a general solution? Thanks. "
            },
            "io": [
                "   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF\n0   1     10     10       0      10      10        0      10      10        0\n1   2     12     12       0      12      12        0      12      12        0\n2   3     14     14       0      16      14        2      14      14        0\n3   4     16     16       0      16      16        0      16      16        0\n4   5     18     18       0      18      18        0      18      17        1\n",
                "   ID  M_NEW  M_OLD  M_DIFF  CA_NEW  CA_OLD  CA_DIFF  BC_NEW  BC_OLD  BC_DIFF\n0   3     14     14       0      16      14        2      14      14        0\n1   5     18     18       0      18      18        0      18      17        1\n"
            ],
            "answer": {
                "ans_desc": "You can do this: ",
                "code": [
                    "\n...\n# get all columns with X_DIFF\ncolumns = df.columns[df.columns.str.contains('_DIFF')]\n\n# check if any has value greater than 0\ndf[df[columns].transform(lambda x: x > 0).any(axis=1)]\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1397,
            "user_id": 7964098,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6a9225e0082406f55ee64c704ed3d069?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nerxis",
            "link": "https://stackoverflow.com/users/7964098/nerxis"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 66496672,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1614964364,
        "creation_date": 1614963296,
        "question_id": 66496528,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex",
        "title": "Repeating single DataFrame with changing DateTimeIndex",
        "body": "<p>Let's say I have very simple DataFrame like this:\n<code>df = pd.DataFrame(np.random.randint(0, 12, size=(12, 4)), columns=list('ABCD'), index=pd.date_range(&quot;2000-01-01&quot;, freq=&quot;M&quot;, periods=12))</code></p>\n<p>Output:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n</code></pre>\n<p>I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n2011-01-31   6  0   8  10\n2011-02-28   7  8  10   3\n2011-03-31  10  5   8  10\n2011-04-30   4  4   9   7\n2011-05-31   2  3   0  11\n2011-06-30   8  7  10   8\n2011-07-31  11  9   0   4\n2011-08-31   0  3   8   6\n2011-09-30   4  6   7   9\n2011-10-31   1  0  11   9\n2011-11-30   5  4   8   4\n2011-12-31   1  4   5   1\n2012-01-31   6  0   8  10\n2012-02-28   7  8  10   3\n2012-03-31  10  5   8  10\n2012-04-30   4  4   9   7\n2012-05-31   2  3   0  11\n2012-06-30   8  7  10   8\n2012-07-31  11  9   0   4\n2012-08-31   0  3   8   6\n2012-09-30   4  6   7   9\n2012-10-31   1  0  11   9\n2012-11-30   5  4   8   4\n2012-12-31   1  4   5   1\n</code></pre>\n<p>It's still the same DataFrame, repeating again and again, and year is incrementally changed.</p>\n<p>I could do something like this (example for 3 years):</p>\n<pre class=\"lang-py prettyprint-override\"><code>data_new = pd.concat([df] * 3)\ndata_new.index = np.ravel([df.index.map(lambda t: t.replace(year=year)) for year in [2010, 2011, 2012]])\n</code></pre>\n<p>I have mainly two questions:</p>\n<ul>\n<li>Is there a way how to do this in a single command?</li>\n<li>What is the best way how to deal with leap-year?</li>\n</ul>\n",
        "answer_body": "<p>Here's an option with applying <code>DateOffset</code> to the original index in list comprehension:</p>\n<pre><code>data_new = pd.concat([\n    df.set_index(df.index + pd.DateOffset(year=x)) for x in [2010, 2011, 2012]])\n</code></pre>\n",
        "question_body": "<p>Let's say I have very simple DataFrame like this:\n<code>df = pd.DataFrame(np.random.randint(0, 12, size=(12, 4)), columns=list('ABCD'), index=pd.date_range(&quot;2000-01-01&quot;, freq=&quot;M&quot;, periods=12))</code></p>\n<p>Output:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n</code></pre>\n<p>I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n2011-01-31   6  0   8  10\n2011-02-28   7  8  10   3\n2011-03-31  10  5   8  10\n2011-04-30   4  4   9   7\n2011-05-31   2  3   0  11\n2011-06-30   8  7  10   8\n2011-07-31  11  9   0   4\n2011-08-31   0  3   8   6\n2011-09-30   4  6   7   9\n2011-10-31   1  0  11   9\n2011-11-30   5  4   8   4\n2011-12-31   1  4   5   1\n2012-01-31   6  0   8  10\n2012-02-28   7  8  10   3\n2012-03-31  10  5   8  10\n2012-04-30   4  4   9   7\n2012-05-31   2  3   0  11\n2012-06-30   8  7  10   8\n2012-07-31  11  9   0   4\n2012-08-31   0  3   8   6\n2012-09-30   4  6   7   9\n2012-10-31   1  0  11   9\n2012-11-30   5  4   8   4\n2012-12-31   1  4   5   1\n</code></pre>\n<p>It's still the same DataFrame, repeating again and again, and year is incrementally changed.</p>\n<p>I could do something like this (example for 3 years):</p>\n<pre class=\"lang-py prettyprint-override\"><code>data_new = pd.concat([df] * 3)\ndata_new.index = np.ravel([df.index.map(lambda t: t.replace(year=year)) for year in [2010, 2011, 2012]])\n</code></pre>\n<p>I have mainly two questions:</p>\n<ul>\n<li>Is there a way how to do this in a single command?</li>\n<li>What is the best way how to deal with leap-year?</li>\n</ul>\n",
        "formatted_input": {
            "qid": 66496528,
            "link": "https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex",
            "question": {
                "title": "Repeating single DataFrame with changing DateTimeIndex",
                "ques_desc": "Let's say I have very simple DataFrame like this: Output: I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this: It's still the same DataFrame, repeating again and again, and year is incrementally changed. I could do something like this (example for 3 years): I have mainly two questions: Is there a way how to do this in a single command? What is the best way how to deal with leap-year? "
            },
            "io": [
                "             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n",
                "             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n2011-01-31   6  0   8  10\n2011-02-28   7  8  10   3\n2011-03-31  10  5   8  10\n2011-04-30   4  4   9   7\n2011-05-31   2  3   0  11\n2011-06-30   8  7  10   8\n2011-07-31  11  9   0   4\n2011-08-31   0  3   8   6\n2011-09-30   4  6   7   9\n2011-10-31   1  0  11   9\n2011-11-30   5  4   8   4\n2011-12-31   1  4   5   1\n2012-01-31   6  0   8  10\n2012-02-28   7  8  10   3\n2012-03-31  10  5   8  10\n2012-04-30   4  4   9   7\n2012-05-31   2  3   0  11\n2012-06-30   8  7  10   8\n2012-07-31  11  9   0   4\n2012-08-31   0  3   8   6\n2012-09-30   4  6   7   9\n2012-10-31   1  0  11   9\n2012-11-30   5  4   8   4\n2012-12-31   1  4   5   1\n"
            ],
            "answer": {
                "ans_desc": "Here's an option with applying to the original index in list comprehension: ",
                "code": [
                    "data_new = pd.concat([\n    df.set_index(df.index + pd.DateOffset(year=x)) for x in [2010, 2011, 2012]])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "tostring",
            "spaces"
        ],
        "owner": {
            "reputation": 674,
            "user_id": 5003606,
            "user_type": "registered",
            "accept_rate": 71,
            "profile_image": "https://www.gravatar.com/avatar/bb95a643bd0727ca91c0ede0f5faae59?s=128&d=identicon&r=PG&f=1",
            "display_name": "HaroldFinch",
            "link": "https://stackoverflow.com/users/5003606/haroldfinch"
        },
        "is_answered": true,
        "view_count": 742,
        "accepted_answer_id": 66376805,
        "answer_count": 1,
        "score": 8,
        "last_activity_date": 1614699208,
        "creation_date": 1614281528,
        "last_edit_date": 1614398942,
        "question_id": 66375262,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66375262/specify-number-of-spaces-between-pandas-dataframe-columns-when-printing",
        "title": "specify number of spaces between pandas DataFrame columns when printing",
        "body": "<p>When you print a pandas DataFrame, which calls DataFrame.to_string, it normally inserts a minimum of 2 spaces between the columns.  For example, this code</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame( {\n    &quot;c1&quot; : (&quot;a&quot;, &quot;bb&quot;, &quot;ccc&quot;, &quot;dddd&quot;, &quot;eeeeee&quot;),\n    &quot;c2&quot; : (11, 22, 33, 44, 55),\n    &quot;a3235235235&quot;: [1, 2, 3, 4, 5]\n} )\nprint(df)\n</code></pre>\n<p>outputs</p>\n<pre><code>       c1  c2  a3235235235\n0       a  11            1\n1      bb  22            2\n2     ccc  33            3\n3    dddd  44            4\n4  eeeeee  55            5\n</code></pre>\n<p>which has a minimum of 2 spaces between each column.</p>\n<p>I am copying DataFarames printed on the console and pasting it into documents, and I have received feedback that it is hard to read: people would like more spaces between the columns.</p>\n<p><em>Is there a standard way to do that?</em></p>\n<p>I see no option in either <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_string.html\" rel=\"nofollow noreferrer\">DataFrame.to_string</a> or <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html\" rel=\"nofollow noreferrer\">pandas.set_option</a>.</p>\n<p>I have done a web search, and not found an answer.  <a href=\"https://stackoverflow.com/questions/52030631/remove-the-automatic-two-spaces-between-columns-that-pandas-dataframe-to-string\">This question</a> asks how to remove those 2 spaces, while <a href=\"https://stackoverflow.com/questions/65772452/using-to-string-with-formatters-removes-one-space-between-columns-of-a-pandas\">this question</a> asks why sometimes only 1 space is between columns instead of 2 (I also have seen this bug, hope someone  answers that question).</p>\n<p>My hack solution is to define a function that converts a DataFrame's columns to type str, and then prepends each element with a string of the specified number of spaces.</p>\n<p>This code (added to the code above)</p>\n<pre><code>def prependSpacesToColumns(df: pd.DataFrame, n: int = 3):\n    spaces = ' ' * n\n    \n    # ensure every column name has the leading spaces:\n    if isinstance(df.columns, pd.MultiIndex):\n        for i in range(df.columns.nlevels):\n            levelNew = [spaces + str(s) for s in df.columns.levels[i]]\n            df.columns.set_levels(levelNew, level = i, inplace = True)\n    else:\n        df.columns = spaces + df.columns\n    \n    # ensure every element has the leading spaces:\n    df = df.astype(str)\n    df = spaces + df\n    \n    return df\n\ndfSp = prependSpacesToColumns(df, 3)\nprint(dfSp)\n</code></pre>\n<p>outputs</p>\n<pre><code>          c1     c2    a3235235235\n0          a     11              1\n1         bb     22              2\n2        ccc     33              3\n3       dddd     44              4\n4     eeeeee     55              5\n</code></pre>\n<p>which is the desired effect.</p>\n<p>But I think that pandas surely must have some <em>builtin</em> simple standard way to do this.  Did I miss how?</p>\n<p>Also, the solution needs to handle a DataFrame whose columns are a MultiIndex.  To continue the code example, consider this modification:</p>\n<pre><code>idx = ((&quot;Outer&quot;, &quot;Inner1&quot;), (&quot;Outer&quot;, &quot;Inner2&quot;), (&quot;Outer&quot;, &quot;a3235235235&quot;))\ndf.columns = pd.MultiIndex.from_tuples(idx)\n</code></pre>\n",
        "answer_body": "<p>You can accomplish this through <code>formatters</code>; it takes a bit of code to create the dictionary <code>{'col_name': format_string}</code>. Find the max character length in each column <strong>or the length of the column header</strong>, whichever is greater, add some padding, and then pass a formatting string.</p>\n<p>Use <code>partial</code> from <code>functools</code> as the formatters expect a one parameter function, yet we need to specify a different width for each column.</p>\n<h3>Sample Data</h3>\n<pre><code>import pandas as pd\ndf = pd.DataFrame({&quot;c1&quot;: (&quot;a&quot;, &quot;bb&quot;, &quot;ccc&quot;, &quot;dddd&quot;, 'eeeeee'),\n                   &quot;c2&quot;: (1, 22, 33, 44, 55),\n                   &quot;a3235235235&quot;: [1,2,3,4,5]})\n</code></pre>\n<hr />\n<h3>Code</h3>\n<pre><code>from functools import partial\n\n# Formatting string \ndef get_fmt_str(x, fill):\n    return '{message: &gt;{fill}}'.format(message=x, fill=fill)\n\n# Max character length per column\ns = df.astype(str).agg(lambda x: x.str.len()).max() \n\npad = 6  # How many spaces between \nfmts = {}\nfor idx, c_len in s.iteritems():\n    # Deal with MultIndex tuples or simple string labels. \n    if isinstance(idx, tuple):\n        lab_len = max([len(str(x)) for x in idx])\n    else:\n        lab_len = len(str(idx))\n\n    fill = max(lab_len, c_len) + pad - 1\n    fmts[idx] = partial(get_fmt_str, fill=fill)\n</code></pre>\n<hr />\n<pre><code>print(df.to_string(formatters=fmts))\n\n            c1      c2      a3235235235\n0            a      11                1\n1           bb      22                2\n2          ccc      33                3\n3         dddd      44                4\n4       eeeeee      55                5\n</code></pre>\n<hr />\n<pre><code># MultiIndex Output\n         Outer                             \n        Inner1      Inner2      a3235235235\n0            a          11                1\n1           bb          22                2\n2          ccc          33                3\n3         dddd          44                4\n4       eeeeee          55                5\n</code></pre>\n",
        "question_body": "<p>When you print a pandas DataFrame, which calls DataFrame.to_string, it normally inserts a minimum of 2 spaces between the columns.  For example, this code</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame( {\n    &quot;c1&quot; : (&quot;a&quot;, &quot;bb&quot;, &quot;ccc&quot;, &quot;dddd&quot;, &quot;eeeeee&quot;),\n    &quot;c2&quot; : (11, 22, 33, 44, 55),\n    &quot;a3235235235&quot;: [1, 2, 3, 4, 5]\n} )\nprint(df)\n</code></pre>\n<p>outputs</p>\n<pre><code>       c1  c2  a3235235235\n0       a  11            1\n1      bb  22            2\n2     ccc  33            3\n3    dddd  44            4\n4  eeeeee  55            5\n</code></pre>\n<p>which has a minimum of 2 spaces between each column.</p>\n<p>I am copying DataFarames printed on the console and pasting it into documents, and I have received feedback that it is hard to read: people would like more spaces between the columns.</p>\n<p><em>Is there a standard way to do that?</em></p>\n<p>I see no option in either <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_string.html\" rel=\"nofollow noreferrer\">DataFrame.to_string</a> or <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html\" rel=\"nofollow noreferrer\">pandas.set_option</a>.</p>\n<p>I have done a web search, and not found an answer.  <a href=\"https://stackoverflow.com/questions/52030631/remove-the-automatic-two-spaces-between-columns-that-pandas-dataframe-to-string\">This question</a> asks how to remove those 2 spaces, while <a href=\"https://stackoverflow.com/questions/65772452/using-to-string-with-formatters-removes-one-space-between-columns-of-a-pandas\">this question</a> asks why sometimes only 1 space is between columns instead of 2 (I also have seen this bug, hope someone  answers that question).</p>\n<p>My hack solution is to define a function that converts a DataFrame's columns to type str, and then prepends each element with a string of the specified number of spaces.</p>\n<p>This code (added to the code above)</p>\n<pre><code>def prependSpacesToColumns(df: pd.DataFrame, n: int = 3):\n    spaces = ' ' * n\n    \n    # ensure every column name has the leading spaces:\n    if isinstance(df.columns, pd.MultiIndex):\n        for i in range(df.columns.nlevels):\n            levelNew = [spaces + str(s) for s in df.columns.levels[i]]\n            df.columns.set_levels(levelNew, level = i, inplace = True)\n    else:\n        df.columns = spaces + df.columns\n    \n    # ensure every element has the leading spaces:\n    df = df.astype(str)\n    df = spaces + df\n    \n    return df\n\ndfSp = prependSpacesToColumns(df, 3)\nprint(dfSp)\n</code></pre>\n<p>outputs</p>\n<pre><code>          c1     c2    a3235235235\n0          a     11              1\n1         bb     22              2\n2        ccc     33              3\n3       dddd     44              4\n4     eeeeee     55              5\n</code></pre>\n<p>which is the desired effect.</p>\n<p>But I think that pandas surely must have some <em>builtin</em> simple standard way to do this.  Did I miss how?</p>\n<p>Also, the solution needs to handle a DataFrame whose columns are a MultiIndex.  To continue the code example, consider this modification:</p>\n<pre><code>idx = ((&quot;Outer&quot;, &quot;Inner1&quot;), (&quot;Outer&quot;, &quot;Inner2&quot;), (&quot;Outer&quot;, &quot;a3235235235&quot;))\ndf.columns = pd.MultiIndex.from_tuples(idx)\n</code></pre>\n",
        "formatted_input": {
            "qid": 66375262,
            "link": "https://stackoverflow.com/questions/66375262/specify-number-of-spaces-between-pandas-dataframe-columns-when-printing",
            "question": {
                "title": "specify number of spaces between pandas DataFrame columns when printing",
                "ques_desc": "When you print a pandas DataFrame, which calls DataFrame.to_string, it normally inserts a minimum of 2 spaces between the columns. For example, this code outputs which has a minimum of 2 spaces between each column. I am copying DataFarames printed on the console and pasting it into documents, and I have received feedback that it is hard to read: people would like more spaces between the columns. Is there a standard way to do that? I see no option in either DataFrame.to_string or pandas.set_option. I have done a web search, and not found an answer. This question asks how to remove those 2 spaces, while this question asks why sometimes only 1 space is between columns instead of 2 (I also have seen this bug, hope someone answers that question). My hack solution is to define a function that converts a DataFrame's columns to type str, and then prepends each element with a string of the specified number of spaces. This code (added to the code above) outputs which is the desired effect. But I think that pandas surely must have some builtin simple standard way to do this. Did I miss how? Also, the solution needs to handle a DataFrame whose columns are a MultiIndex. To continue the code example, consider this modification: "
            },
            "io": [
                "       c1  c2  a3235235235\n0       a  11            1\n1      bb  22            2\n2     ccc  33            3\n3    dddd  44            4\n4  eeeeee  55            5\n",
                "          c1     c2    a3235235235\n0          a     11              1\n1         bb     22              2\n2        ccc     33              3\n3       dddd     44              4\n4     eeeeee     55              5\n"
            ],
            "answer": {
                "ans_desc": "You can accomplish this through ; it takes a bit of code to create the dictionary . Find the max character length in each column or the length of the column header, whichever is greater, add some padding, and then pass a formatting string. Use from as the formatters expect a one parameter function, yet we need to specify a different width for each column. Sample Data Code ",
                "code": [
                    "from functools import partial\n\n# Formatting string \ndef get_fmt_str(x, fill):\n    return '{message: >{fill}}'.format(message=x, fill=fill)\n\n# Max character length per column\ns = df.astype(str).agg(lambda x: x.str.len()).max() \n\npad = 6  # How many spaces between \nfmts = {}\nfor idx, c_len in s.iteritems():\n    # Deal with MultIndex tuples or simple string labels. \n    if isinstance(idx, tuple):\n        lab_len = max([len(str(x)) for x in idx])\n    else:\n        lab_len = len(str(idx))\n\n    fill = max(lab_len, c_len) + pad - 1\n    fmts[idx] = partial(get_fmt_str, fill=fill)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "transpose"
        ],
        "owner": {
            "reputation": 361,
            "user_id": 11918314,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e0527f96ed53dc456ed90a908d5904aa?s=128&d=identicon&r=PG&f=1",
            "display_name": "wjie08",
            "link": "https://stackoverflow.com/users/11918314/wjie08"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 66422303,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1614605584,
        "creation_date": 1614602081,
        "last_edit_date": 1614605584,
        "question_id": 66422239,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list",
        "title": "Python transposing multiple dataframes in a list",
        "body": "<p>I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below</p>\n<pre><code>0    email           factor1_final   factor2_final   factor3_final\n1    john@abc.com    85%             90%             50%\n2    peter@abc.com   80%             60%             60%\n3    shelby@abc.com  50%             70%             60%\n4    jess@abc.com    60%             65%             50% \n5    mark@abc.com    98%             50%             60%\n</code></pre>\n<pre><code>0    email           item1           item2           \n1    john@abc.com    80%             60%             \n2    peter@abc.com   60%             90%             \n3    shelby@abc.com  900%            40%             \n4    jess@abc.com    70%             35%            \n5    mark@abc.com    92%             50%\n</code></pre>\n<p>my desired output is to have multiple dataframes with the email as column header and the factor or item as rows</p>\n<pre><code>\nemail     john@abc.com   peter@abc.com   shelby@abc.com   jess@abc.com   mark@abc.com\nfactor1     85%          80%             50%              60%            98%\nfactor2     90%          60%             70%              65%            50% \nfactor3     50%          60%             60%              50%            60%\n</code></pre>\n<p>I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose</p>\n<pre><code>#Set index to email and transpose\ndf1 = df1.set_index('email').T\ndf1\n</code></pre>\n<p>wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you.</p>\n<pre><code>#Create a list of all the dataframes\ndf_list = [df1, df2, df3, df4, df5, df6]\n\nfor df in df_list:\n    df = df.set_index('email').T\n\ndf1\n\n#tried this too but does not work \nfor i, df in enumerate(df_list):\n    df_list[i] = df_list[i].set_index('email').T\n</code></pre>\n",
        "answer_body": "<p>For me second solution working, here is small alternative:</p>\n<pre><code>df_list = [df1, df2]\n\nfor i, df in enumerate(df_list):\n    df_list[i] = df.set_index('email').T\n\nprint (df_list[0])\nemail         john@abc.com peter@abc.com shelby@abc.com jess@abc.com  \\\nfactor1_final          85%           80%            50%          60%   \nfactor2_final          90%           60%            70%          65%   \nfactor3_final          50%           60%            60%          50%   \n\nemail         mark@abc.com  \nfactor1_final          98%  \nfactor2_final          50%  \nfactor3_final          60%  \n\nprint (df_list[1])\nemail john@abc.com peter@abc.com shelby@abc.com jess@abc.com mark@abc.com\nitem1          80%           60%           900%          70%          92%\nitem2          60%           90%            40%          35%          50%\n</code></pre>\n<p>Solution with create new list of DataFrames:</p>\n<pre><code>dfs = [df.set_index('email').T for df in df_list]\n</code></pre>\n",
        "question_body": "<p>I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below</p>\n<pre><code>0    email           factor1_final   factor2_final   factor3_final\n1    john@abc.com    85%             90%             50%\n2    peter@abc.com   80%             60%             60%\n3    shelby@abc.com  50%             70%             60%\n4    jess@abc.com    60%             65%             50% \n5    mark@abc.com    98%             50%             60%\n</code></pre>\n<pre><code>0    email           item1           item2           \n1    john@abc.com    80%             60%             \n2    peter@abc.com   60%             90%             \n3    shelby@abc.com  900%            40%             \n4    jess@abc.com    70%             35%            \n5    mark@abc.com    92%             50%\n</code></pre>\n<p>my desired output is to have multiple dataframes with the email as column header and the factor or item as rows</p>\n<pre><code>\nemail     john@abc.com   peter@abc.com   shelby@abc.com   jess@abc.com   mark@abc.com\nfactor1     85%          80%             50%              60%            98%\nfactor2     90%          60%             70%              65%            50% \nfactor3     50%          60%             60%              50%            60%\n</code></pre>\n<p>I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose</p>\n<pre><code>#Set index to email and transpose\ndf1 = df1.set_index('email').T\ndf1\n</code></pre>\n<p>wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you.</p>\n<pre><code>#Create a list of all the dataframes\ndf_list = [df1, df2, df3, df4, df5, df6]\n\nfor df in df_list:\n    df = df.set_index('email').T\n\ndf1\n\n#tried this too but does not work \nfor i, df in enumerate(df_list):\n    df_list[i] = df_list[i].set_index('email').T\n</code></pre>\n",
        "formatted_input": {
            "qid": 66422239,
            "link": "https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list",
            "question": {
                "title": "Python transposing multiple dataframes in a list",
                "ques_desc": "I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you. "
            },
            "io": [
                "0    email           factor1_final   factor2_final   factor3_final\n1    john@abc.com    85%             90%             50%\n2    peter@abc.com   80%             60%             60%\n3    shelby@abc.com  50%             70%             60%\n4    jess@abc.com    60%             65%             50% \n5    mark@abc.com    98%             50%             60%\n",
                "\nemail     john@abc.com   peter@abc.com   shelby@abc.com   jess@abc.com   mark@abc.com\nfactor1     85%          80%             50%              60%            98%\nfactor2     90%          60%             70%              65%            50% \nfactor3     50%          60%             60%              50%            60%\n"
            ],
            "answer": {
                "ans_desc": "For me second solution working, here is small alternative: Solution with create new list of DataFrames: ",
                "code": [
                    "dfs = [df.set_index('email').T for df in df_list]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datatable"
        ],
        "owner": {
            "reputation": 19,
            "user_id": 14707062,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-EQilT7i2zIA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckl43fw_uOpaeZSxxLzdZFBwbNqnw/s96-c/photo.jpg?sz=128",
            "display_name": "omer ali20",
            "link": "https://stackoverflow.com/users/14707062/omer-ali20"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 66402563,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1614454127,
        "creation_date": 1614448567,
        "last_edit_date": 1614452650,
        "question_id": 66401782,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66401782/update-table-information-based-on-columns-of-another-table",
        "title": "update table information based on columns of another table",
        "body": "<p>I am new in python have two dataframes, df1 contains information about all students with their group and score, and df2 contains updated information about few students when they change their group and score. How could I update the information in df1 based on the values of df2 (group and score)?</p>\n<p>df1</p>\n<pre><code>   +----+----------+-----------+----------------+\n    |    |student No|   group   |       score    |\n    |----+----------+-----------+----------------|\n    |  0 |        0 |         0 |       0.839626 |\n    |  1 |        1 |         0 |       0.845435 |\n    |  2 |        2 |         3 |       0.830778 |\n    |  3 |        3 |         2 |       0.831565 |\n    |  4 |        4 |         3 |       0.823569 |\n    |  5 |        5 |         0 |       0.808109 |\n    |  6 |        6 |         4 |       0.831645 |\n    |  7 |        7 |         1 |       0.851048 |\n    |  8 |        8 |         3 |       0.843209 |\n    |  9 |        9 |         4 |       0.84902  |\n    | 10 |       10 |         0 |       0.835143 |\n    | 11 |       11 |         4 |       0.843228 |\n    | 12 |       12 |         2 |       0.826949 |\n    | 13 |       13 |         0 |       0.84196  |\n    | 14 |       14 |         1 |       0.821634 |\n    | 15 |       15 |         3 |       0.840702 |\n    | 16 |       16 |         0 |       0.828994 |\n    | 17 |       17 |         2 |       0.843043 |\n    | 18 |       18 |         4 |       0.809093 |\n    | 19 |       19 |         1 |       0.85426  |\n    +----+----------+-----------+----------------+\n\ndf2\n+----+-----------+----------+----------------+\n|    |   group   |student No|       score    |\n|----+-----------+----------+----------------|\n|  0 |         2 |        1 |       0.887435 |\n|  1 |         0 |       19 |       0.81214  |\n|  2 |         3 |       17 |       0.899041 |\n|  3 |         0 |        8 |       0.853333 |\n|  4 |         4 |        9 |       0.88512  |\n+----+-----------+----------+----------------+\n</code></pre>\n<p>The result</p>\n<p>df: 3</p>\n<pre><code>   +----+----------+-----------+----------------+\n    |    |student No|   group   |       score    |\n    |----+----------+-----------+----------------|\n    |  0 |        0 |         0 |       0.839626 |\n    |  1 |        1 |         2 |       0.887435 |\n    |  2 |        2 |         3 |       0.830778 |\n    |  3 |        3 |         2 |       0.831565 |\n    |  4 |        4 |         3 |       0.823569 |\n    |  5 |        5 |         0 |       0.808109 |\n    |  6 |        6 |         4 |       0.831645 |\n    |  7 |        7 |         1 |       0.851048 |\n    |  8 |        8 |         0 |       0.853333 |\n    |  9 |        9 |         4 |       0.88512  |\n    | 10 |       10 |         0 |       0.835143 |\n    | 11 |       11 |         4 |       0.843228 |\n    | 12 |       12 |         2 |       0.826949 |\n    | 13 |       13 |         0 |       0.84196  |\n    | 14 |       14 |         1 |       0.821634 |\n    | 15 |       15 |         3 |       0.840702 |\n    | 16 |       16 |         0 |       0.828994 |\n    | 17 |       17 |         3 |       0.899041 |\n    | 18 |       18 |         4 |       0.809093 |\n    | 19 |       19 |         0 |       0.81214  |\n    +----+----------+-----------+----------------+\n</code></pre>\n<p>my code to update df1 from df2</p>\n<pre><code>dfupdated = df1.merge(df2, how='left', on=['student No'], suffixes=('', '_new'))\ndfupdated['group'] = np.where(pd.notnull(dfupdated['group_new']), dfupdated['group_new'],\n                                         dfupdated['group'])\ndfupdated['score'] = np.where(pd.notnull(dfupdated['score_new']), dfupdated['score_new'],\n                                         dfupdated['score'])\ndfupdated.drop(['group_new', 'score_new'],axis=1, inplace=True)\ndfupdated.reset_index(drop=True, inplace=True)\n</code></pre>\n<p>but I face the following error</p>\n<pre><code>KeyError: &quot;['group'] not in index&quot;\n</code></pre>\n",
        "answer_body": "<p>I don't know what's wrong I ran same and got the answer\ngiving a different way to solve it</p>\n<p>try :</p>\n<pre><code>dfupdated = df1.merge(df2, on='student No', how='left')\ndfupdated['group'] = dfupdated['group_y'].fillna(dfupdated['group_x'])\ndfupdated['score'] = dfupdated['score_y'].fillna(dfupdated['score_x'])\ndfupdated.drop(['group_x', 'group_y','score_x', 'score_y'], axis=1,inplace=True)\n</code></pre>\n<p>will give you the solution you want.</p>\n<p>to get the max from each group</p>\n<p><code>dfupdated.groupby(['group'], sort=False)['score'].max()</code></p>\n",
        "question_body": "<p>I am new in python have two dataframes, df1 contains information about all students with their group and score, and df2 contains updated information about few students when they change their group and score. How could I update the information in df1 based on the values of df2 (group and score)?</p>\n<p>df1</p>\n<pre><code>   +----+----------+-----------+----------------+\n    |    |student No|   group   |       score    |\n    |----+----------+-----------+----------------|\n    |  0 |        0 |         0 |       0.839626 |\n    |  1 |        1 |         0 |       0.845435 |\n    |  2 |        2 |         3 |       0.830778 |\n    |  3 |        3 |         2 |       0.831565 |\n    |  4 |        4 |         3 |       0.823569 |\n    |  5 |        5 |         0 |       0.808109 |\n    |  6 |        6 |         4 |       0.831645 |\n    |  7 |        7 |         1 |       0.851048 |\n    |  8 |        8 |         3 |       0.843209 |\n    |  9 |        9 |         4 |       0.84902  |\n    | 10 |       10 |         0 |       0.835143 |\n    | 11 |       11 |         4 |       0.843228 |\n    | 12 |       12 |         2 |       0.826949 |\n    | 13 |       13 |         0 |       0.84196  |\n    | 14 |       14 |         1 |       0.821634 |\n    | 15 |       15 |         3 |       0.840702 |\n    | 16 |       16 |         0 |       0.828994 |\n    | 17 |       17 |         2 |       0.843043 |\n    | 18 |       18 |         4 |       0.809093 |\n    | 19 |       19 |         1 |       0.85426  |\n    +----+----------+-----------+----------------+\n\ndf2\n+----+-----------+----------+----------------+\n|    |   group   |student No|       score    |\n|----+-----------+----------+----------------|\n|  0 |         2 |        1 |       0.887435 |\n|  1 |         0 |       19 |       0.81214  |\n|  2 |         3 |       17 |       0.899041 |\n|  3 |         0 |        8 |       0.853333 |\n|  4 |         4 |        9 |       0.88512  |\n+----+-----------+----------+----------------+\n</code></pre>\n<p>The result</p>\n<p>df: 3</p>\n<pre><code>   +----+----------+-----------+----------------+\n    |    |student No|   group   |       score    |\n    |----+----------+-----------+----------------|\n    |  0 |        0 |         0 |       0.839626 |\n    |  1 |        1 |         2 |       0.887435 |\n    |  2 |        2 |         3 |       0.830778 |\n    |  3 |        3 |         2 |       0.831565 |\n    |  4 |        4 |         3 |       0.823569 |\n    |  5 |        5 |         0 |       0.808109 |\n    |  6 |        6 |         4 |       0.831645 |\n    |  7 |        7 |         1 |       0.851048 |\n    |  8 |        8 |         0 |       0.853333 |\n    |  9 |        9 |         4 |       0.88512  |\n    | 10 |       10 |         0 |       0.835143 |\n    | 11 |       11 |         4 |       0.843228 |\n    | 12 |       12 |         2 |       0.826949 |\n    | 13 |       13 |         0 |       0.84196  |\n    | 14 |       14 |         1 |       0.821634 |\n    | 15 |       15 |         3 |       0.840702 |\n    | 16 |       16 |         0 |       0.828994 |\n    | 17 |       17 |         3 |       0.899041 |\n    | 18 |       18 |         4 |       0.809093 |\n    | 19 |       19 |         0 |       0.81214  |\n    +----+----------+-----------+----------------+\n</code></pre>\n<p>my code to update df1 from df2</p>\n<pre><code>dfupdated = df1.merge(df2, how='left', on=['student No'], suffixes=('', '_new'))\ndfupdated['group'] = np.where(pd.notnull(dfupdated['group_new']), dfupdated['group_new'],\n                                         dfupdated['group'])\ndfupdated['score'] = np.where(pd.notnull(dfupdated['score_new']), dfupdated['score_new'],\n                                         dfupdated['score'])\ndfupdated.drop(['group_new', 'score_new'],axis=1, inplace=True)\ndfupdated.reset_index(drop=True, inplace=True)\n</code></pre>\n<p>but I face the following error</p>\n<pre><code>KeyError: &quot;['group'] not in index&quot;\n</code></pre>\n",
        "formatted_input": {
            "qid": 66401782,
            "link": "https://stackoverflow.com/questions/66401782/update-table-information-based-on-columns-of-another-table",
            "question": {
                "title": "update table information based on columns of another table",
                "ques_desc": "I am new in python have two dataframes, df1 contains information about all students with their group and score, and df2 contains updated information about few students when they change their group and score. How could I update the information in df1 based on the values of df2 (group and score)? df1 The result df: 3 my code to update df1 from df2 but I face the following error "
            },
            "io": [
                "   +----+----------+-----------+----------------+\n    |    |student No|   group   |       score    |\n    |----+----------+-----------+----------------|\n    |  0 |        0 |         0 |       0.839626 |\n    |  1 |        1 |         0 |       0.845435 |\n    |  2 |        2 |         3 |       0.830778 |\n    |  3 |        3 |         2 |       0.831565 |\n    |  4 |        4 |         3 |       0.823569 |\n    |  5 |        5 |         0 |       0.808109 |\n    |  6 |        6 |         4 |       0.831645 |\n    |  7 |        7 |         1 |       0.851048 |\n    |  8 |        8 |         3 |       0.843209 |\n    |  9 |        9 |         4 |       0.84902  |\n    | 10 |       10 |         0 |       0.835143 |\n    | 11 |       11 |         4 |       0.843228 |\n    | 12 |       12 |         2 |       0.826949 |\n    | 13 |       13 |         0 |       0.84196  |\n    | 14 |       14 |         1 |       0.821634 |\n    | 15 |       15 |         3 |       0.840702 |\n    | 16 |       16 |         0 |       0.828994 |\n    | 17 |       17 |         2 |       0.843043 |\n    | 18 |       18 |         4 |       0.809093 |\n    | 19 |       19 |         1 |       0.85426  |\n    +----+----------+-----------+----------------+\n\ndf2\n+----+-----------+----------+----------------+\n|    |   group   |student No|       score    |\n|----+-----------+----------+----------------|\n|  0 |         2 |        1 |       0.887435 |\n|  1 |         0 |       19 |       0.81214  |\n|  2 |         3 |       17 |       0.899041 |\n|  3 |         0 |        8 |       0.853333 |\n|  4 |         4 |        9 |       0.88512  |\n+----+-----------+----------+----------------+\n",
                "   +----+----------+-----------+----------------+\n    |    |student No|   group   |       score    |\n    |----+----------+-----------+----------------|\n    |  0 |        0 |         0 |       0.839626 |\n    |  1 |        1 |         2 |       0.887435 |\n    |  2 |        2 |         3 |       0.830778 |\n    |  3 |        3 |         2 |       0.831565 |\n    |  4 |        4 |         3 |       0.823569 |\n    |  5 |        5 |         0 |       0.808109 |\n    |  6 |        6 |         4 |       0.831645 |\n    |  7 |        7 |         1 |       0.851048 |\n    |  8 |        8 |         0 |       0.853333 |\n    |  9 |        9 |         4 |       0.88512  |\n    | 10 |       10 |         0 |       0.835143 |\n    | 11 |       11 |         4 |       0.843228 |\n    | 12 |       12 |         2 |       0.826949 |\n    | 13 |       13 |         0 |       0.84196  |\n    | 14 |       14 |         1 |       0.821634 |\n    | 15 |       15 |         3 |       0.840702 |\n    | 16 |       16 |         0 |       0.828994 |\n    | 17 |       17 |         3 |       0.899041 |\n    | 18 |       18 |         4 |       0.809093 |\n    | 19 |       19 |         0 |       0.81214  |\n    +----+----------+-----------+----------------+\n"
            ],
            "answer": {
                "ans_desc": "I don't know what's wrong I ran same and got the answer giving a different way to solve it try : will give you the solution you want. to get the max from each group ",
                "code": [
                    "dfupdated = df1.merge(df2, on='student No', how='left')\ndfupdated['group'] = dfupdated['group_y'].fillna(dfupdated['group_x'])\ndfupdated['score'] = dfupdated['score_y'].fillna(dfupdated['score_x'])\ndfupdated.drop(['group_x', 'group_y','score_x', 'score_y'], axis=1,inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "slice",
            "multi-index"
        ],
        "owner": {
            "reputation": 11910,
            "user_id": 1142881,
            "user_type": "registered",
            "accept_rate": 99,
            "profile_image": "https://www.gravatar.com/avatar/99836a8fe8fb744bfa64210a2bf7cc23?s=128&d=identicon&r=PG&f=1",
            "display_name": "SkyWalker",
            "link": "https://stackoverflow.com/users/1142881/skywalker"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 66398610,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1614427771,
        "creation_date": 1614426694,
        "last_edit_date": 1614427336,
        "question_id": 66398540,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66398540/how-to-set-new-columns-in-a-multi-column-index-from-a-dict-with-partially-specif",
        "title": "How to set new columns in a multi-column index from a dict with partially specified tuple keys?",
        "body": "<p>I have a pandas dataframe initialized in the following way:</p>\n<pre><code>import pandas as pd\n\nmy_multi_index = pd.MultiIndex.from_tuples([('a', 'a1'), ('a', 'a2'), \n                                            ('b', 'b1'), ('b', 'b2')],\n                                           names=['key1', 'key2']) \ndf = pd.DataFrame(data=[[1, 2], [3, 4], [5, 6], [7, 8]],\n                  columns=['col1', 'col2'],\n                  index=my_multi_index)\nprint(df)\n</code></pre>\n<p>which gives:</p>\n<pre><code>#            col1  col2\n# key1 key2            \n# a    a1       1     2\n#      a2       3     4\n# b    b1       5     6\n#      b2       7     8\n</code></pre>\n<p>Now I'd like to add a new column <code>desc1</code> to this dataframe using partial key slicing BUT not in code, I'd like to do this from configuration i.e. a dictionary with partial tuple keys:</p>\n<pre><code># i'd like to externalize this and not hardcode it i.e. easier maintenance\ndf.loc[pd.IndexSlice['a', :], 'desc1'] = 'x'\ndf.loc[pd.IndexSlice['b', 'b1'], 'desc1'] = 'y1'\ndf.loc[pd.IndexSlice['b', 'b2'], 'desc1'] = 'y2'\nprint(df)\n</code></pre>\n<p>which gives:</p>\n<pre><code># key1 key2                  \n# a    a1       1     2     x\n#      a2       3     4     x\n# b    b1       5     6    y1\n#      b2       7     8    y2\n</code></pre>\n<p>notice that setting 'x' doesn't depend on the second component of the <code>('a', _)</code> key and setting 'y1' and 'y2' do depend on the second component of the <code>('b', 'b1')</code> key. A possible solution is to fully specify the mapping but this is also not desirable if I have a 100 <code>(a, _)</code> whose assignment doesn't depend on the second component. I wish to reach the above result but not hard-coding the sliced assignments, instead I'd like to do it from an externalized dictionary:</p>\n<p>My configuration dictionary would look like this:</p>\n<pre><code>my_dict = {\n    ('a', None): 'x',\n    ('b', 'b1'): 'y1',\n    ('b', 'b2'): 'y2'\n}\n</code></pre>\n<p>Is there a pythonic and pandas-tonic way to apply this dictionary with partially specified keys to reach the sliced assignment produced before?</p>\n",
        "answer_body": "<p>We can leverage the fact that we can pass tuples as a MultiIndex slicer. Also we slightly adjust your <code>my_dict</code>. Then we apply a simple for loop:</p>\n<pre><code>my_dict = {\n    ('a',): 'x',\n    ('b', 'b1'): 'y1',\n    ('b', 'b2'): 'y2'\n}\n\nfor idx, value in my_dict.items():\n    df.loc[idx, 'desc1'] = value\n</code></pre>\n<pre><code>           col1  col2 desc1\nkey1 key2                  \na    a1       1     2     x\n     a2       3     4     x\nb    b1       5     6    y1\n     b2       7     8    y2\n</code></pre>\n<hr />\n<p>Second option would be to use <code>Index.map</code> and filling in the first value in your dict, so we can use <code>Series.ffill</code>:</p>\n<pre><code>my_dict = {\n    ('a', 'a1'): 'x',\n    ('b', 'b1'): 'y1',\n    ('b', 'b2'): 'y2'\n}\n\ndf['desc1'] = df.index.map(my_dict)\ndf['desc1'] = df['desc1'].ffill()\n\n\n           col1  col2 desc1\nkey1 key2                  \na    a1       1     2     x\n     a2       3     4     x\nb    b1       5     6    y1\n     b2       7     8    y2\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe initialized in the following way:</p>\n<pre><code>import pandas as pd\n\nmy_multi_index = pd.MultiIndex.from_tuples([('a', 'a1'), ('a', 'a2'), \n                                            ('b', 'b1'), ('b', 'b2')],\n                                           names=['key1', 'key2']) \ndf = pd.DataFrame(data=[[1, 2], [3, 4], [5, 6], [7, 8]],\n                  columns=['col1', 'col2'],\n                  index=my_multi_index)\nprint(df)\n</code></pre>\n<p>which gives:</p>\n<pre><code>#            col1  col2\n# key1 key2            \n# a    a1       1     2\n#      a2       3     4\n# b    b1       5     6\n#      b2       7     8\n</code></pre>\n<p>Now I'd like to add a new column <code>desc1</code> to this dataframe using partial key slicing BUT not in code, I'd like to do this from configuration i.e. a dictionary with partial tuple keys:</p>\n<pre><code># i'd like to externalize this and not hardcode it i.e. easier maintenance\ndf.loc[pd.IndexSlice['a', :], 'desc1'] = 'x'\ndf.loc[pd.IndexSlice['b', 'b1'], 'desc1'] = 'y1'\ndf.loc[pd.IndexSlice['b', 'b2'], 'desc1'] = 'y2'\nprint(df)\n</code></pre>\n<p>which gives:</p>\n<pre><code># key1 key2                  \n# a    a1       1     2     x\n#      a2       3     4     x\n# b    b1       5     6    y1\n#      b2       7     8    y2\n</code></pre>\n<p>notice that setting 'x' doesn't depend on the second component of the <code>('a', _)</code> key and setting 'y1' and 'y2' do depend on the second component of the <code>('b', 'b1')</code> key. A possible solution is to fully specify the mapping but this is also not desirable if I have a 100 <code>(a, _)</code> whose assignment doesn't depend on the second component. I wish to reach the above result but not hard-coding the sliced assignments, instead I'd like to do it from an externalized dictionary:</p>\n<p>My configuration dictionary would look like this:</p>\n<pre><code>my_dict = {\n    ('a', None): 'x',\n    ('b', 'b1'): 'y1',\n    ('b', 'b2'): 'y2'\n}\n</code></pre>\n<p>Is there a pythonic and pandas-tonic way to apply this dictionary with partially specified keys to reach the sliced assignment produced before?</p>\n",
        "formatted_input": {
            "qid": 66398540,
            "link": "https://stackoverflow.com/questions/66398540/how-to-set-new-columns-in-a-multi-column-index-from-a-dict-with-partially-specif",
            "question": {
                "title": "How to set new columns in a multi-column index from a dict with partially specified tuple keys?",
                "ques_desc": "I have a pandas dataframe initialized in the following way: which gives: Now I'd like to add a new column to this dataframe using partial key slicing BUT not in code, I'd like to do this from configuration i.e. a dictionary with partial tuple keys: which gives: notice that setting 'x' doesn't depend on the second component of the key and setting 'y1' and 'y2' do depend on the second component of the key. A possible solution is to fully specify the mapping but this is also not desirable if I have a 100 whose assignment doesn't depend on the second component. I wish to reach the above result but not hard-coding the sliced assignments, instead I'd like to do it from an externalized dictionary: My configuration dictionary would look like this: Is there a pythonic and pandas-tonic way to apply this dictionary with partially specified keys to reach the sliced assignment produced before? "
            },
            "io": [
                "#            col1  col2\n# key1 key2            \n# a    a1       1     2\n#      a2       3     4\n# b    b1       5     6\n#      b2       7     8\n",
                "# key1 key2                  \n# a    a1       1     2     x\n#      a2       3     4     x\n# b    b1       5     6    y1\n#      b2       7     8    y2\n"
            ],
            "answer": {
                "ans_desc": "We can leverage the fact that we can pass tuples as a MultiIndex slicer. Also we slightly adjust your . Then we apply a simple for loop: Second option would be to use and filling in the first value in your dict, so we can use : ",
                "code": [
                    "my_dict = {\n    ('a',): 'x',\n    ('b', 'b1'): 'y1',\n    ('b', 'b2'): 'y2'\n}\n\nfor idx, value in my_dict.items():\n    df.loc[idx, 'desc1'] = value\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 39,
            "user_id": 10896895,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-MziPEOj7XC4/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcaGdjjFIeDuzAY2m8Z8-fmzWPN2Qg/mo/photo.jpg?sz=128",
            "display_name": "Rushi",
            "link": "https://stackoverflow.com/users/10896895/rushi"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 66383938,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1614334724,
        "creation_date": 1614334554,
        "question_id": 66383901,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66383901/changing-value-of-adjacent-column-based-on-value-of-of-another-column",
        "title": "Changing Value of adjacent column based on value of of another column",
        "body": "<p>I have following dataframe:</p>\n<pre><code>    A1  A2  B1  B2\n 0  10  20  20   NA\n 1  20  40  30   No\n 2  50  No  50   10\n 3  40  NA  50   20\n</code></pre>\n<p>I want to change value in column A1 to NaN whenever corresponding value in column A2 is No or NA. Same for B1.<br />\n<strong>Note</strong>: NA here is a string objects not NaN.</p>\n<pre><code>     A1  A2  B1   B2\n 0  10   20  NaN  NA\n 1  20   40  NaN  No\n 2  NaN  No  50   10\n 3  NaN  NA  50   20\n</code></pre>\n",
        "answer_body": "<p>Use if <code>NA</code> and <code>No</code> are strings use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isin.html\" rel=\"nofollow noreferrer\"><code>Series.isin</code></a> in <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>DataFrame.loc</code></a> or :</p>\n<pre><code>df.loc[df.A2.isin(['NA','No']), 'A1'] = np.nan\n</code></pre>\n<p>Or <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mask.html\" rel=\"nofollow noreferrer\"><code>Series.mask</code></a>:</p>\n<pre><code>df['A1'] = df['A1'].mask(df.A2.isin(['NA','No']))\n</code></pre>\n<hr />\n<p>If <code>NA</code> is missing value test it by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isna.html\" rel=\"nofollow noreferrer\"><code>Series.isna</code></a>:</p>\n<pre><code>df.loc[df.A2.isna() | df.A2.eq('No'), 'A1'] = np.nan\n</code></pre>\n<p>Or:</p>\n<pre><code>df['A1'] = df['A1'].mask(df.A2.isna() | df.A2.eq('No'))\n</code></pre>\n",
        "question_body": "<p>I have following dataframe:</p>\n<pre><code>    A1  A2  B1  B2\n 0  10  20  20   NA\n 1  20  40  30   No\n 2  50  No  50   10\n 3  40  NA  50   20\n</code></pre>\n<p>I want to change value in column A1 to NaN whenever corresponding value in column A2 is No or NA. Same for B1.<br />\n<strong>Note</strong>: NA here is a string objects not NaN.</p>\n<pre><code>     A1  A2  B1   B2\n 0  10   20  NaN  NA\n 1  20   40  NaN  No\n 2  NaN  No  50   10\n 3  NaN  NA  50   20\n</code></pre>\n",
        "formatted_input": {
            "qid": 66383901,
            "link": "https://stackoverflow.com/questions/66383901/changing-value-of-adjacent-column-based-on-value-of-of-another-column",
            "question": {
                "title": "Changing Value of adjacent column based on value of of another column",
                "ques_desc": "I have following dataframe: I want to change value in column A1 to NaN whenever corresponding value in column A2 is No or NA. Same for B1. Note: NA here is a string objects not NaN. "
            },
            "io": [
                "    A1  A2  B1  B2\n 0  10  20  20   NA\n 1  20  40  30   No\n 2  50  No  50   10\n 3  40  NA  50   20\n",
                "     A1  A2  B1   B2\n 0  10   20  NaN  NA\n 1  20   40  NaN  No\n 2  NaN  No  50   10\n 3  NaN  NA  50   20\n"
            ],
            "answer": {
                "ans_desc": "Use if and are strings use in or : Or : If is missing value test it by : Or: ",
                "code": [
                    "df.loc[df.A2.isna() | df.A2.eq('No'), 'A1'] = np.nan\n",
                    "df['A1'] = df['A1'].mask(df.A2.isna() | df.A2.eq('No'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 103,
            "user_id": 14944419,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e3ffb851d33699da74bb4925b4e5620d?s=128&d=identicon&r=PG&f=1",
            "display_name": "user123",
            "link": "https://stackoverflow.com/users/14944419/user123"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 66371793,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1614268075,
        "creation_date": 1614267404,
        "question_id": 66371637,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66371637/sum-of-row-in-the-same-columns-in-pandas",
        "title": "sum of row in the same columns in pandas",
        "body": "<p>i have a dataframe something like this</p>\n<pre><code>   d1  d2    d3    d4\n  780  37.0  21.4  122840.0\n  784  38.1  21.4  122860.0\n  846  38.1  21.4  122880.0\n  843  38.0  21.5  122900.0\n  820  36.3  22.9  133220.0\n  819  36.3  22.9  133240.0\n  819  36.4  22.9  133260.0\n  820  36.3  22.9  133280.0\n  822  36.4  22.9  133300.0\n</code></pre>\n<p>how do i get the sum of values between the same column in a new column in a dataframe\nfor example:</p>\n<pre><code> d1    d2    d3    d4       d5 \n780  37.0  21.4  122840.0  1564\n784  38.1  21.4  122860.0  1630\n846  38.1  21.4  122880.0  1689\n</code></pre>\n<p>i want a new column with the sum of d1[i] + d1[i+1] .i know .sum() in pandas but i cant do sum between the same column</p>\n",
        "answer_body": "<p>Your question is not fully clear to me, but I think what you mean to do is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['d5'] = df['d1'] + df['d1'].shift(-1)\n</code></pre>\n<p>Now you have to decide what you want to happen for the last element of the series.</p>\n",
        "question_body": "<p>i have a dataframe something like this</p>\n<pre><code>   d1  d2    d3    d4\n  780  37.0  21.4  122840.0\n  784  38.1  21.4  122860.0\n  846  38.1  21.4  122880.0\n  843  38.0  21.5  122900.0\n  820  36.3  22.9  133220.0\n  819  36.3  22.9  133240.0\n  819  36.4  22.9  133260.0\n  820  36.3  22.9  133280.0\n  822  36.4  22.9  133300.0\n</code></pre>\n<p>how do i get the sum of values between the same column in a new column in a dataframe\nfor example:</p>\n<pre><code> d1    d2    d3    d4       d5 \n780  37.0  21.4  122840.0  1564\n784  38.1  21.4  122860.0  1630\n846  38.1  21.4  122880.0  1689\n</code></pre>\n<p>i want a new column with the sum of d1[i] + d1[i+1] .i know .sum() in pandas but i cant do sum between the same column</p>\n",
        "formatted_input": {
            "qid": 66371637,
            "link": "https://stackoverflow.com/questions/66371637/sum-of-row-in-the-same-columns-in-pandas",
            "question": {
                "title": "sum of row in the same columns in pandas",
                "ques_desc": "i have a dataframe something like this how do i get the sum of values between the same column in a new column in a dataframe for example: i want a new column with the sum of d1[i] + d1[i+1] .i know .sum() in pandas but i cant do sum between the same column "
            },
            "io": [
                "   d1  d2    d3    d4\n  780  37.0  21.4  122840.0\n  784  38.1  21.4  122860.0\n  846  38.1  21.4  122880.0\n  843  38.0  21.5  122900.0\n  820  36.3  22.9  133220.0\n  819  36.3  22.9  133240.0\n  819  36.4  22.9  133260.0\n  820  36.3  22.9  133280.0\n  822  36.4  22.9  133300.0\n",
                " d1    d2    d3    d4       d5 \n780  37.0  21.4  122840.0  1564\n784  38.1  21.4  122860.0  1630\n846  38.1  21.4  122880.0  1689\n"
            ],
            "answer": {
                "ans_desc": "Your question is not fully clear to me, but I think what you mean to do is: Now you have to decide what you want to happen for the last element of the series. ",
                "code": [
                    "df['d5'] = df['d1'] + df['d1'].shift(-1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "multi-index"
        ],
        "owner": {
            "reputation": 135,
            "user_id": 13288479,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cb424e8ea5bb5d49672edb281322ca80?s=128&d=identicon&r=PG&f=1",
            "display_name": "Occhima",
            "link": "https://stackoverflow.com/users/13288479/occhima"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 66357831,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1614209023,
        "creation_date": 1614195033,
        "question_id": 66357650,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns",
        "title": "transform a pandas dataframe in a pandas with multicolumns",
        "body": "<p>I have the following pandas dataframe, where the columna <code>id</code> is the dataframe index</p>\n<pre><code>+----+-----------+------------+-----------+------------+\n|    |   price_A |   amount_A |   price_B |   amount_b |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n</code></pre>\n<p>And i want to convert this datframe in to a multi column data frame, that looks like this</p>\n<pre><code>\n+----+-----------+------------+-----------+------------+\n|    |           A            |           B            |\n+----+-----------+------------+-----------+------------+\n| id |   price   |   amount   |   price   |   amount   |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n</code></pre>\n<p>I've tried transforming my old pandas dataframe in to a dict this way:</p>\n<pre><code>   dict = {&quot;A&quot;: df[[&quot;price_a&quot;,&quot;amount_a&quot;]], &quot;B&quot;:df[[&quot;price_b&quot;, &quot;amount_b&quot;]]}\n   df = pd.DataFrame(dict, index=df.index)\n\n</code></pre>\n<p>But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome.</p>\n",
        "answer_body": "<p>Try renaming columns manually:</p>\n<pre><code>df.columns=pd.MultiIndex.from_tuples([x.split('_')[::-1] for x in df.columns])\ndf.index.name='id'\n</code></pre>\n<p>Output:</p>\n<pre><code>           A                   B         b\n       price    amount     price    amount\nid                                        \n0   0.652826  0.941421  0.823048  0.728427\n1   0.400078  0.600585  0.194912  0.269842\n2   0.223524  0.146675  0.375459  0.177165\n3   0.330626  0.214981  0.389855  0.541666\n4   0.578132  0.304780  0.789573  0.268851\n5   0.094360  0.514878  0.419333  0.017010\n6   0.279122  0.401132  0.722363  0.337094\n7   0.444977  0.333254  0.643878  0.371528\n8   0.724673  0.063281  0.345225  0.935403\n9   0.905482  0.846500  0.585653  0.364495\n</code></pre>\n",
        "question_body": "<p>I have the following pandas dataframe, where the columna <code>id</code> is the dataframe index</p>\n<pre><code>+----+-----------+------------+-----------+------------+\n|    |   price_A |   amount_A |   price_B |   amount_b |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n</code></pre>\n<p>And i want to convert this datframe in to a multi column data frame, that looks like this</p>\n<pre><code>\n+----+-----------+------------+-----------+------------+\n|    |           A            |           B            |\n+----+-----------+------------+-----------+------------+\n| id |   price   |   amount   |   price   |   amount   |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n</code></pre>\n<p>I've tried transforming my old pandas dataframe in to a dict this way:</p>\n<pre><code>   dict = {&quot;A&quot;: df[[&quot;price_a&quot;,&quot;amount_a&quot;]], &quot;B&quot;:df[[&quot;price_b&quot;, &quot;amount_b&quot;]]}\n   df = pd.DataFrame(dict, index=df.index)\n\n</code></pre>\n<p>But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome.</p>\n",
        "formatted_input": {
            "qid": 66357650,
            "link": "https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns",
            "question": {
                "title": "transform a pandas dataframe in a pandas with multicolumns",
                "ques_desc": "I have the following pandas dataframe, where the columna is the dataframe index And i want to convert this datframe in to a multi column data frame, that looks like this I've tried transforming my old pandas dataframe in to a dict this way: But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome. "
            },
            "io": [
                "+----+-----------+------------+-----------+------------+\n|    |   price_A |   amount_A |   price_B |   amount_b |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n",
                "\n+----+-----------+------------+-----------+------------+\n|    |           A            |           B            |\n+----+-----------+------------+-----------+------------+\n| id |   price   |   amount   |   price   |   amount   |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n"
            ],
            "answer": {
                "ans_desc": "Try renaming columns manually: Output: ",
                "code": [
                    "df.columns=pd.MultiIndex.from_tuples([x.split('_')[::-1] for x in df.columns])\ndf.index.name='id'\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dictionary",
            "dataframe"
        ],
        "owner": {
            "reputation": 3021,
            "user_id": 3676196,
            "user_type": "registered",
            "accept_rate": 82,
            "profile_image": "https://i.stack.imgur.com/WC7PK.jpg?s=128&g=1",
            "display_name": "Prince Bhatti",
            "link": "https://stackoverflow.com/users/3676196/prince-bhatti"
        },
        "is_answered": true,
        "view_count": 378768,
        "protected_date": 1563294289,
        "accepted_answer_id": 26716774,
        "answer_count": 8,
        "score": 247,
        "last_activity_date": 1613982523,
        "creation_date": 1415026073,
        "last_edit_date": 1481476491,
        "question_id": 26716616,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary",
        "title": "Convert a Pandas DataFrame to a dictionary",
        "body": "<p>I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be <code>keys</code> and the elements of other columns in same row be <code>values</code>. </p>\n\n<p>DataFrame:   </p>\n\n<pre><code>    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n</code></pre>\n\n<p>Output should be like this:</p>\n\n<p>Dictionary:</p>\n\n<pre><code>{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n</code></pre>\n",
        "answer_body": "<p>The <a href=\"http://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.to_dict.html\" rel=\"noreferrer\"><code>to_dict()</code></a> method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this.</p>\n\n<p><code>to_dict()</code> also accepts an 'orient' argument which you'll need in order to output a <em>list</em> of values for each column. Otherwise, a dictionary of the form <code>{index: value}</code> will be returned for each column.</p>\n\n<p>These steps can be done with the following line:</p>\n\n<pre><code>&gt;&gt;&gt; df.set_index('ID').T.to_dict('list')\n{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n</code></pre>\n\n<hr>\n\n<p>In case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]})\n&gt;&gt;&gt; df\n        a      b\n0     red  0.500\n1  yellow  0.250\n2    blue  0.125\n</code></pre>\n\n<p>Then the options are as follows.</p>\n\n<p><strong>dict</strong> - the default: column names are keys, values are dictionaries of index:data pairs</p>\n\n<pre><code>&gt;&gt;&gt; df.to_dict('dict')\n{'a': {0: 'red', 1: 'yellow', 2: 'blue'}, \n 'b': {0: 0.5, 1: 0.25, 2: 0.125}}\n</code></pre>\n\n<p><strong>list</strong> - keys are column names, values are lists of column data</p>\n\n<pre><code>&gt;&gt;&gt; df.to_dict('list')\n{'a': ['red', 'yellow', 'blue'], \n 'b': [0.5, 0.25, 0.125]}\n</code></pre>\n\n<p><strong>series</strong> - like 'list', but values are Series</p>\n\n<pre><code>&gt;&gt;&gt; df.to_dict('series')\n{'a': 0       red\n      1    yellow\n      2      blue\n      Name: a, dtype: object, \n\n 'b': 0    0.500\n      1    0.250\n      2    0.125\n      Name: b, dtype: float64}\n</code></pre>\n\n<p><strong>split</strong> - splits columns/data/index as keys with values being column names, data values by row and index labels respectively</p>\n\n<pre><code>&gt;&gt;&gt; df.to_dict('split')\n{'columns': ['a', 'b'],\n 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],\n 'index': [0, 1, 2]}\n</code></pre>\n\n<p><strong>records</strong> - each row becomes a dictionary where key is column name and value is the data in the cell</p>\n\n<pre><code>&gt;&gt;&gt; df.to_dict('records')\n[{'a': 'red', 'b': 0.5}, \n {'a': 'yellow', 'b': 0.25}, \n {'a': 'blue', 'b': 0.125}]\n</code></pre>\n\n<p><strong>index</strong> - like 'records', but a dictionary of dictionaries with keys as index labels (rather than a list)</p>\n\n<pre><code>&gt;&gt;&gt; df.to_dict('index')\n{0: {'a': 'red', 'b': 0.5},\n 1: {'a': 'yellow', 'b': 0.25},\n 2: {'a': 'blue', 'b': 0.125}}\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be <code>keys</code> and the elements of other columns in same row be <code>values</code>. </p>\n\n<p>DataFrame:   </p>\n\n<pre><code>    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n</code></pre>\n\n<p>Output should be like this:</p>\n\n<p>Dictionary:</p>\n\n<pre><code>{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n</code></pre>\n",
        "formatted_input": {
            "qid": 26716616,
            "link": "https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary",
            "question": {
                "title": "Convert a Pandas DataFrame to a dictionary",
                "ques_desc": "I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be and the elements of other columns in same row be . DataFrame: Output should be like this: Dictionary: "
            },
            "io": [
                "    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n",
                "{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n"
            ],
            "answer": {
                "ans_desc": "The method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this. also accepts an 'orient' argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form will be returned for each column. These steps can be done with the following line: In case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame: Then the options are as follows. dict - the default: column names are keys, values are dictionaries of index:data pairs list - keys are column names, values are lists of column data series - like 'list', but values are Series split - splits columns/data/index as keys with values being column names, data values by row and index labels respectively records - each row becomes a dictionary where key is column name and value is the data in the cell index - like 'records', but a dictionary of dictionaries with keys as index labels (rather than a list) ",
                "code": [
                    ">>> df.to_dict('split')\n{'columns': ['a', 'b'],\n 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],\n 'index': [0, 1, 2]}\n",
                    ">>> df.to_dict('records')\n[{'a': 'red', 'b': 0.5}, \n {'a': 'yellow', 'b': 0.25}, \n {'a': 'blue', 'b': 0.125}]\n",
                    ">>> df.to_dict('index')\n{0: {'a': 'red', 'b': 0.5},\n 1: {'a': 'yellow', 'b': 0.25},\n 2: {'a': 'blue', 'b': 0.125}}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 66277487,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1613738975,
        "creation_date": 1613736096,
        "last_edit_date": 1613736508,
        "question_id": 66277212,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66277212/merge-padas-rows-if-the-difference-between-consecutive-rows-are-less-than-two",
        "title": "Merge padas rows if the difference between consecutive rows are less than two",
        "body": "<p>I have a data frame like this,</p>\n<pre><code>df\ncol1     col2         col3\n A       [p,s]         2\n A       [q]           3\n A       [r,t]         4\n A       [p,x]         7\n B       [x,y]         8\n C       [s]           4\n C       [t,v]         6\n C       [u,x]         7 \n</code></pre>\n<p>the df is sorted by col1. Now for each col1 values of the previous or/and next row if difference between consecutive col3 values are less than 2 then merge col2 values in a single row. So the data frame would look like,</p>\n<pre><code>df\ncol1    col2\n A      [p,s,q,r,t]\n A      [p,x]\n B      [x,y]\n C      [s]\n C      [t,v,u,x]\n</code></pre>\n<p>This could be done using for loop by filtering col1 values each time but it will take more time to execute, looking for some pandas shortcuts to do it most efficiently.</p>\n",
        "answer_body": "<p>You can create groups by compared if differency is greater of equal 2 with cumulative sums first:</p>\n<pre><code>df['g'] = df.groupby('col1')['col3'].apply(lambda x: x.diff().ge(2).cumsum())\n</code></pre>\n<p>And then use column for aggregate with flatten list of lists in lambda function:</p>\n<pre><code>f = lambda x: [z for y in x for z in y]\ndf = df.groupby(['col1','g'])['col2'].agg(f).reset_index(level=1, drop=True).reset_index()\nprint (df)\n  col1             col2\n0    A  [p, s, q, r, t]\n1    A           [p, x]\n2    B           [x, y]\n3    C              [s]\n4    C     [t, v, u, x]\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this,</p>\n<pre><code>df\ncol1     col2         col3\n A       [p,s]         2\n A       [q]           3\n A       [r,t]         4\n A       [p,x]         7\n B       [x,y]         8\n C       [s]           4\n C       [t,v]         6\n C       [u,x]         7 \n</code></pre>\n<p>the df is sorted by col1. Now for each col1 values of the previous or/and next row if difference between consecutive col3 values are less than 2 then merge col2 values in a single row. So the data frame would look like,</p>\n<pre><code>df\ncol1    col2\n A      [p,s,q,r,t]\n A      [p,x]\n B      [x,y]\n C      [s]\n C      [t,v,u,x]\n</code></pre>\n<p>This could be done using for loop by filtering col1 values each time but it will take more time to execute, looking for some pandas shortcuts to do it most efficiently.</p>\n",
        "formatted_input": {
            "qid": 66277212,
            "link": "https://stackoverflow.com/questions/66277212/merge-padas-rows-if-the-difference-between-consecutive-rows-are-less-than-two",
            "question": {
                "title": "Merge padas rows if the difference between consecutive rows are less than two",
                "ques_desc": "I have a data frame like this, the df is sorted by col1. Now for each col1 values of the previous or/and next row if difference between consecutive col3 values are less than 2 then merge col2 values in a single row. So the data frame would look like, This could be done using for loop by filtering col1 values each time but it will take more time to execute, looking for some pandas shortcuts to do it most efficiently. "
            },
            "io": [
                "df\ncol1     col2         col3\n A       [p,s]         2\n A       [q]           3\n A       [r,t]         4\n A       [p,x]         7\n B       [x,y]         8\n C       [s]           4\n C       [t,v]         6\n C       [u,x]         7 \n",
                "df\ncol1    col2\n A      [p,s,q,r,t]\n A      [p,x]\n B      [x,y]\n C      [s]\n C      [t,v,u,x]\n"
            ],
            "answer": {
                "ans_desc": "You can create groups by compared if differency is greater of equal 2 with cumulative sums first: And then use column for aggregate with flatten list of lists in lambda function: ",
                "code": [
                    "df['g'] = df.groupby('col1')['col3'].apply(lambda x: x.diff().ge(2).cumsum())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 394,
            "user_id": 10266922,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2dd2d4ee4235abf585cc9ba1deccc1b9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Boomshakalaka",
            "link": "https://stackoverflow.com/users/10266922/boomshakalaka"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 66267099,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1613677572,
        "creation_date": 1613674734,
        "question_id": 66266438,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66266438/python-how-to-combine-the-rows-into-a-single-row-in-pandas-not-group-by",
        "title": "Python - How to combine the rows into a single row in Pandas? (not group by)",
        "body": "<p>I am working with a weird dataframe using Pandas:</p>\n<pre><code>print(df)\n\nActive        Dead       Hold\nProduct1      n/a        n/a\nn/a           Product2   n/a\nn/a           n/a        Product3\n</code></pre>\n<p>I want to combine the three rows into 1 row and the expected output is:</p>\n<pre><code>Active        Dead       Hold\nProduct1      Product2   Product3\n</code></pre>\n<p>I really don't know how to do this and appreciate your help! Thank you.</p>\n",
        "answer_body": "<p>Here is one slightly faster alternative:</p>\n<pre><code>new = df.apply(lambda x: x.dropna().values)\n</code></pre>\n",
        "question_body": "<p>I am working with a weird dataframe using Pandas:</p>\n<pre><code>print(df)\n\nActive        Dead       Hold\nProduct1      n/a        n/a\nn/a           Product2   n/a\nn/a           n/a        Product3\n</code></pre>\n<p>I want to combine the three rows into 1 row and the expected output is:</p>\n<pre><code>Active        Dead       Hold\nProduct1      Product2   Product3\n</code></pre>\n<p>I really don't know how to do this and appreciate your help! Thank you.</p>\n",
        "formatted_input": {
            "qid": 66266438,
            "link": "https://stackoverflow.com/questions/66266438/python-how-to-combine-the-rows-into-a-single-row-in-pandas-not-group-by",
            "question": {
                "title": "Python - How to combine the rows into a single row in Pandas? (not group by)",
                "ques_desc": "I am working with a weird dataframe using Pandas: I want to combine the three rows into 1 row and the expected output is: I really don't know how to do this and appreciate your help! Thank you. "
            },
            "io": [
                "print(df)\n\nActive        Dead       Hold\nProduct1      n/a        n/a\nn/a           Product2   n/a\nn/a           n/a        Product3\n",
                "Active        Dead       Hold\nProduct1      Product2   Product3\n"
            ],
            "answer": {
                "ans_desc": "Here is one slightly faster alternative: ",
                "code": [
                    "new = df.apply(lambda x: x.dropna().values)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 29,
            "user_id": 10296584,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-vrR9kGkSvcA/AAAAAAAAAAI/AAAAAAAAOrQ/4VHSnMG2Fms/photo.jpg?sz=128",
            "display_name": "Atharva Katre",
            "link": "https://stackoverflow.com/users/10296584/atharva-katre"
        },
        "is_answered": true,
        "view_count": 79,
        "closed_date": 1613674498,
        "accepted_answer_id": 66266288,
        "answer_count": 4,
        "score": -1,
        "last_activity_date": 1613674381,
        "creation_date": 1613673715,
        "question_id": 66266188,
        "link": "https://stackoverflow.com/questions/66266188/how-to-stack-append-all-columns-into-one-column-in-pandas",
        "closed_reason": "Duplicate",
        "title": "How to stack/append all columns into one column in Pandas?",
        "body": "<p>I am working with a DF similar to this one:</p>\n<pre><code>    A   B   C\n0   1   4   7\n1   2   5   8\n2   3   6   9\n</code></pre>\n<p>I want the values of all columns to be stacked into the first column</p>\n<p>Desired Output:</p>\n<pre><code>    A\n0   1\n1   2\n2   3\n3   4\n4   5\n5   6\n6   7\n7   8\n8   9\n</code></pre>\n",
        "answer_body": "<p>Very simply with <code>melt</code>:</p>\n<pre><code>import pandas as pd\ndf.melt().drop('variable',axis=1).rename({'value':'A'},axis=1)\n</code></pre>\n<hr />\n<pre><code>   A\n0  1\n1  2\n2  3\n3  4\n4  5\n5  6\n6  7\n7  8\n8  9\n</code></pre>\n",
        "question_body": "<p>I am working with a DF similar to this one:</p>\n<pre><code>    A   B   C\n0   1   4   7\n1   2   5   8\n2   3   6   9\n</code></pre>\n<p>I want the values of all columns to be stacked into the first column</p>\n<p>Desired Output:</p>\n<pre><code>    A\n0   1\n1   2\n2   3\n3   4\n4   5\n5   6\n6   7\n7   8\n8   9\n</code></pre>\n",
        "formatted_input": {
            "qid": 66266188,
            "link": "https://stackoverflow.com/questions/66266188/how-to-stack-append-all-columns-into-one-column-in-pandas",
            "question": {
                "title": "How to stack/append all columns into one column in Pandas?",
                "ques_desc": "I am working with a DF similar to this one: I want the values of all columns to be stacked into the first column Desired Output: "
            },
            "io": [
                "    A   B   C\n0   1   4   7\n1   2   5   8\n2   3   6   9\n",
                "    A\n0   1\n1   2\n2   3\n3   4\n4   5\n5   6\n6   7\n7   8\n8   9\n"
            ],
            "answer": {
                "ans_desc": "Very simply with : ",
                "code": [
                    "import pandas as pd\ndf.melt().drop('variable',axis=1).rename({'value':'A'},axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 39,
            "user_id": 7600284,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-07xMhKHjzPg/AAAAAAAAAAI/AAAAAAAAEMw/zBxz6G-de6U/photo.jpg?sz=128",
            "display_name": "\u0394\u03ae\u03bc\u03b7\u03c4\u03c1\u03b1 \u0393\u03b5\u03c9\u03c1\u03b3\u03af\u03bf\u03c5",
            "link": "https://stackoverflow.com/users/7600284/%ce%94%ce%ae%ce%bc%ce%b7%cf%84%cf%81%ce%b1-%ce%93%ce%b5%cf%89%cf%81%ce%b3%ce%af%ce%bf%cf%85"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 66255616,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1613637443,
        "creation_date": 1613634106,
        "question_id": 66255574,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66255574/reverse-columns-of-dataframe-based-on-the-column-name",
        "title": "Reverse columns of dataframe based on the column name",
        "body": "<p>I have a dataframe:</p>\n<pre><code>     '1000'    '0100'    '0010'    '0001'    '1110'    '1101'    '1011'    '0111'\n0        0         1         2         3         4         5         6         7\n1       00        11        22        33        44        55        66        77\n</code></pre>\n<p>I would like to reverse the columns that their column names have their <em><strong>1st</strong></em> and <em><strong>2nd</strong></em> letters reversed and their <em><strong>3rd</strong></em> and <em><strong>4th</strong></em> as-is.</p>\n<p>i.e.</p>\n<p>1st col: 1000 \u2192 2nd col: 0100</p>\n<p>3rd col: 0010 \u2192 5th col: 1110</p>\n<p>4th col: 0001 \u2192 6th col: 1101</p>\n<p>7th col: 1011 \u2192 8th col: 0111</p>\n<p>I would like to have a dataframe like this:</p>\n<pre><code>     '0100'    '1000'    '1110'    '1101'    '0010'    '0001'    '1011'    '0111'\n0        1         0         4         5         2         3         7         6\n1       11        00        44        55        22        33        77        66\n</code></pre>\n<p>This is what I have for the reversion:\n<code>reversed = ''.join('1' if x == '0' else '0' for x in 1stand2ndletter)</code></p>\n",
        "answer_body": "<p>Use your function in list comprehenion with add next 2 values and then change order of columns by original ordering in subset:</p>\n<pre><code>df.columns = df.columns.str.strip(&quot;'&quot;)\ncols = df.columns\n\ndf.columns = [''.join('1' if x == '0' else '0' for x in y[:2])+y[2:] for y in df.columns]\n\ndf = df[cols]\nprint (df)\n   1000  0100  0010  0001  1110  1101  1011  0111\n0     1     0     4     5     2     3     7     6\n1    11     0    44    55    22    33    77    66\n</code></pre>\n<p>EDIT: More general solution is convert values to 2d numpy array filled by boolean:</p>\n<pre><code>df.columns = df.columns.str.strip(&quot;'&quot;)\ncols = df.columns\n\narr = np.array([[bool(int(y)) for y in x] for x in df.columns])\nprint (arr)\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n</code></pre>\n<p>And then for inverting use indexing, here <code>:</code> means all rows and columns positions form list and <code>~</code> invert mask:</p>\n<pre><code>pos = [0, 2]\narr[:, pos] = ~arr[:, pos]\nprint (arr)\n[[False False  True False]\n [ True  True  True False]\n [ True False False False]\n [ True False  True  True]\n [False  True False False]\n [False  True  True  True]\n [False False False  True]\n [ True  True False  True]]\n</code></pre>\n<p>Last join back to list of strings:</p>\n<pre><code>df.columns = [''.join(str(int(y)) for y in x) for x in arr]\nprint (df)\n   0010  1110  1000  1011  0100  0111  0001  1101\n0     0     1     2     3     4     5     6     7\n1     0    11    22    33    44    55    66    77\n\n# df = df[cols]\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n<pre><code>     '1000'    '0100'    '0010'    '0001'    '1110'    '1101'    '1011'    '0111'\n0        0         1         2         3         4         5         6         7\n1       00        11        22        33        44        55        66        77\n</code></pre>\n<p>I would like to reverse the columns that their column names have their <em><strong>1st</strong></em> and <em><strong>2nd</strong></em> letters reversed and their <em><strong>3rd</strong></em> and <em><strong>4th</strong></em> as-is.</p>\n<p>i.e.</p>\n<p>1st col: 1000 \u2192 2nd col: 0100</p>\n<p>3rd col: 0010 \u2192 5th col: 1110</p>\n<p>4th col: 0001 \u2192 6th col: 1101</p>\n<p>7th col: 1011 \u2192 8th col: 0111</p>\n<p>I would like to have a dataframe like this:</p>\n<pre><code>     '0100'    '1000'    '1110'    '1101'    '0010'    '0001'    '1011'    '0111'\n0        1         0         4         5         2         3         7         6\n1       11        00        44        55        22        33        77        66\n</code></pre>\n<p>This is what I have for the reversion:\n<code>reversed = ''.join('1' if x == '0' else '0' for x in 1stand2ndletter)</code></p>\n",
        "formatted_input": {
            "qid": 66255574,
            "link": "https://stackoverflow.com/questions/66255574/reverse-columns-of-dataframe-based-on-the-column-name",
            "question": {
                "title": "Reverse columns of dataframe based on the column name",
                "ques_desc": "I have a dataframe: I would like to reverse the columns that their column names have their 1st and 2nd letters reversed and their 3rd and 4th as-is. i.e. 1st col: 1000 \u2192 2nd col: 0100 3rd col: 0010 \u2192 5th col: 1110 4th col: 0001 \u2192 6th col: 1101 7th col: 1011 \u2192 8th col: 0111 I would like to have a dataframe like this: This is what I have for the reversion: "
            },
            "io": [
                "     '1000'    '0100'    '0010'    '0001'    '1110'    '1101'    '1011'    '0111'\n0        0         1         2         3         4         5         6         7\n1       00        11        22        33        44        55        66        77\n",
                "     '0100'    '1000'    '1110'    '1101'    '0010'    '0001'    '1011'    '0111'\n0        1         0         4         5         2         3         7         6\n1       11        00        44        55        22        33        77        66\n"
            ],
            "answer": {
                "ans_desc": "Use your function in list comprehenion with add next 2 values and then change order of columns by original ordering in subset: EDIT: More general solution is convert values to 2d numpy array filled by boolean: And then for inverting use indexing, here means all rows and columns positions form list and invert mask: Last join back to list of strings: ",
                "code": [
                    "df.columns = df.columns.str.strip(\"'\")\ncols = df.columns\n\narr = np.array([[bool(int(y)) for y in x] for x in df.columns])\nprint (arr)\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 66255201,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1613633064,
        "creation_date": 1613632100,
        "question_id": 66255188,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66255188/create-pandas-duplicate-rows-based-on-the-number-of-items-in-a-list-type-column",
        "title": "Create pandas duplicate rows based on the number of items in a list type column",
        "body": "<p>I have a data frame like this,</p>\n<pre><code>  df\n  col1     col2\n   A        [1]\n   B        [1,2]\n   A        [2,3,4]\n   C        [1,2]\n   B        [4]\n</code></pre>\n<p>Now I want to create new rows based on the number of values in the col2 list where the col1 values will be same so the final data frame would look like,</p>\n<pre><code>  df\n  col1    col2\n   A       [1]\n   B       [1]\n   B       [2]\n   A       [2]\n   A       [3]\n   A       [4]\n   C       [1]\n   C       [2]\n   B       [4]\n</code></pre>\n<p>I am looking for some pandas short cuts to do it more efficiently</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html\" rel=\"nofollow noreferrer\"><code>DataFrame.explode</code></a> and then create one element lists:</p>\n<pre><code>df2 = df.explode('col2')\n\ndf2['col2'] = df2['col2'].apply(lambda x: [x])\n</code></pre>\n<p>Another idea, I hope faster in large data is use numpy <code>np.repeat</code> with <code>chain.from_iterable</code> for flatten values:</p>\n<pre><code>from  itertools import chain\n\ndf2 = pd.DataFrame({\n        &quot;col1&quot;: np.repeat(df.col1.to_numpy(), df.col2.str.len()),\n        &quot;col2&quot;: [[x] for x in chain.from_iterable(df.col2)]})\n\nprint (df2)\n  col1 col2\n0    A  [1]\n1    B  [1]\n2    B  [2]\n3    A  [2]\n4    A  [3]\n5    A  [4]\n6    C  [1]\n7    C  [2]\n8    B  [4]\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this,</p>\n<pre><code>  df\n  col1     col2\n   A        [1]\n   B        [1,2]\n   A        [2,3,4]\n   C        [1,2]\n   B        [4]\n</code></pre>\n<p>Now I want to create new rows based on the number of values in the col2 list where the col1 values will be same so the final data frame would look like,</p>\n<pre><code>  df\n  col1    col2\n   A       [1]\n   B       [1]\n   B       [2]\n   A       [2]\n   A       [3]\n   A       [4]\n   C       [1]\n   C       [2]\n   B       [4]\n</code></pre>\n<p>I am looking for some pandas short cuts to do it more efficiently</p>\n",
        "formatted_input": {
            "qid": 66255188,
            "link": "https://stackoverflow.com/questions/66255188/create-pandas-duplicate-rows-based-on-the-number-of-items-in-a-list-type-column",
            "question": {
                "title": "Create pandas duplicate rows based on the number of items in a list type column",
                "ques_desc": "I have a data frame like this, Now I want to create new rows based on the number of values in the col2 list where the col1 values will be same so the final data frame would look like, I am looking for some pandas short cuts to do it more efficiently "
            },
            "io": [
                "  df\n  col1     col2\n   A        [1]\n   B        [1,2]\n   A        [2,3,4]\n   C        [1,2]\n   B        [4]\n",
                "  df\n  col1    col2\n   A       [1]\n   B       [1]\n   B       [2]\n   A       [2]\n   A       [3]\n   A       [4]\n   C       [1]\n   C       [2]\n   B       [4]\n"
            ],
            "answer": {
                "ans_desc": "Use and then create one element lists: Another idea, I hope faster in large data is use numpy with for flatten values: ",
                "code": [
                    "df2 = df.explode('col2')\n\ndf2['col2'] = df2['col2'].apply(lambda x: [x])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 289,
            "user_id": 15220414,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-AOsqVGYZSq8/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucl_UbFQEN7bk8lzRPAlYS-KMsCx-Q/s96-c/photo.jpg?sz=128",
            "display_name": "Johanna Marklund",
            "link": "https://stackoverflow.com/users/15220414/johanna-marklund"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 66229729,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1613499381,
        "creation_date": 1613496648,
        "last_edit_date": 1613497943,
        "question_id": 66229269,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66229269/in-panda-python-how-do-i-blacklist-or-whitelist-numbers-in-a-dataframe",
        "title": "In panda python how do I &quot;blacklist&quot; or &quot;whitelist&quot; numbers in a DataFrame",
        "body": "<p>I have two DataFrames, and I want to add a new column to the df with the first allowed numbers in df1, but only as many as are in each group, when it starts again at 1 it needs to look at the first number in Allowed_numbers.</p>\n<pre><code>df1 = pd.DataFrame({'Allowed_numbers': [3,4,5,6,8,9,12,15,17,24,28,29,30]})\ndf = pd.DataFrame({'order_out': [1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9]})\n</code></pre>\n<p>and want this:</p>\n<pre><code>      order_y  y_goal\n0         1       3\n1         2       4\n2         3       5\n3         4       6\n4         5       8\n5         6       9\n6         7      12\n7         8      15\n8         9      17\n9         1       3\n10        2       4\n11        3       5\n12        4       6\n13        5       8\n14        6       9\n15        7      12\n16        8      15\n17        9      17\n...\n</code></pre>\n<p>I have tried a few things</p>\n<pre><code>df= bags.sort_values(['order_out'])\ndf.loc[:, 'y_goal'] = pd.Series(Allowed_numbers)\n</code></pre>\n<p>and get this</p>\n<pre><code>    order_out y_goal\n0         1     3.0\n1         2     4.0\n2         3     5.0\n3         4     6.0\n4         5     8.0\n5         6     9.0\n6         7    12.0\n7         8    15.0\n8         9    17.0\n9         1    24.0\n10        2    28.0\n11        3    29.0\n12        4    30.0\n13        5     NaN\n14        6     NaN\n15        7     NaN\n16        8     NaN\n17        9     NaN\n...\n\n</code></pre>\n<p>I was also considering some kind of mapping numbers against eachother, since 1 in order_out will always be 3 in y_goals. the order_out might also have different lengths of number row, not always up to 9.</p>\n",
        "answer_body": "<p>You can use <code>order_out</code> as an index to df1 (we subtract 1 to convert from 1-indexing to 0-indexing):</p>\n<pre><code>y_goal = df1['Allowed_numbers'][df['order_out'] - 1]\n</code></pre>\n<p>Now we can reset the index on y_goal and neatly merge the two columns:</p>\n<pre><code>df['y_goal'] = y_goal.reset_index(drop = True)\n</code></pre>\n",
        "question_body": "<p>I have two DataFrames, and I want to add a new column to the df with the first allowed numbers in df1, but only as many as are in each group, when it starts again at 1 it needs to look at the first number in Allowed_numbers.</p>\n<pre><code>df1 = pd.DataFrame({'Allowed_numbers': [3,4,5,6,8,9,12,15,17,24,28,29,30]})\ndf = pd.DataFrame({'order_out': [1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9]})\n</code></pre>\n<p>and want this:</p>\n<pre><code>      order_y  y_goal\n0         1       3\n1         2       4\n2         3       5\n3         4       6\n4         5       8\n5         6       9\n6         7      12\n7         8      15\n8         9      17\n9         1       3\n10        2       4\n11        3       5\n12        4       6\n13        5       8\n14        6       9\n15        7      12\n16        8      15\n17        9      17\n...\n</code></pre>\n<p>I have tried a few things</p>\n<pre><code>df= bags.sort_values(['order_out'])\ndf.loc[:, 'y_goal'] = pd.Series(Allowed_numbers)\n</code></pre>\n<p>and get this</p>\n<pre><code>    order_out y_goal\n0         1     3.0\n1         2     4.0\n2         3     5.0\n3         4     6.0\n4         5     8.0\n5         6     9.0\n6         7    12.0\n7         8    15.0\n8         9    17.0\n9         1    24.0\n10        2    28.0\n11        3    29.0\n12        4    30.0\n13        5     NaN\n14        6     NaN\n15        7     NaN\n16        8     NaN\n17        9     NaN\n...\n\n</code></pre>\n<p>I was also considering some kind of mapping numbers against eachother, since 1 in order_out will always be 3 in y_goals. the order_out might also have different lengths of number row, not always up to 9.</p>\n",
        "formatted_input": {
            "qid": 66229269,
            "link": "https://stackoverflow.com/questions/66229269/in-panda-python-how-do-i-blacklist-or-whitelist-numbers-in-a-dataframe",
            "question": {
                "title": "In panda python how do I &quot;blacklist&quot; or &quot;whitelist&quot; numbers in a DataFrame",
                "ques_desc": "I have two DataFrames, and I want to add a new column to the df with the first allowed numbers in df1, but only as many as are in each group, when it starts again at 1 it needs to look at the first number in Allowed_numbers. and want this: I have tried a few things and get this I was also considering some kind of mapping numbers against eachother, since 1 in order_out will always be 3 in y_goals. the order_out might also have different lengths of number row, not always up to 9. "
            },
            "io": [
                "      order_y  y_goal\n0         1       3\n1         2       4\n2         3       5\n3         4       6\n4         5       8\n5         6       9\n6         7      12\n7         8      15\n8         9      17\n9         1       3\n10        2       4\n11        3       5\n12        4       6\n13        5       8\n14        6       9\n15        7      12\n16        8      15\n17        9      17\n...\n",
                "    order_out y_goal\n0         1     3.0\n1         2     4.0\n2         3     5.0\n3         4     6.0\n4         5     8.0\n5         6     9.0\n6         7    12.0\n7         8    15.0\n8         9    17.0\n9         1    24.0\n10        2    28.0\n11        3    29.0\n12        4    30.0\n13        5     NaN\n14        6     NaN\n15        7     NaN\n16        8     NaN\n17        9     NaN\n...\n\n"
            ],
            "answer": {
                "ans_desc": "You can use as an index to df1 (we subtract 1 to convert from 1-indexing to 0-indexing): Now we can reset the index on y_goal and neatly merge the two columns: ",
                "code": [
                    "df['y_goal'] = y_goal.reset_index(drop = True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 15207573,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/171e9f570b87b1444facda3d636823ff?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dipie",
            "link": "https://stackoverflow.com/users/15207573/dipie"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 66195584,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1613307021,
        "creation_date": 1613301005,
        "question_id": 66194784,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66194784/retrieve-rows-with-highest-value-with-condition",
        "title": "Retrieve rows with highest value with condition",
        "body": "<p>I have a dataframe that looks like this:</p>\n<pre><code>| Id | Label | Width |\n|----|-------| ------|\n| 0  |   A   |   5   |\n| 0  |   A   |   3   |\n| 0  |   B   |   4   |\n| 1  |   A   |   7   |\n| 1  |   A   |   9   |\n</code></pre>\n<p>I want to write a function that takes the rows with same id and label A and filter it based on the highest width</p>\n<p>so the after applying the function the dataframe would be:</p>\n<pre><code>| Id | Label | Width |\n|----|-------| ------|\n| 0  |   A   |   5   |\n| 0  |   B   |   4   |\n| 1  |   A   |   9   |\n</code></pre>\n",
        "answer_body": "<p>Let us try:</p>\n<pre><code>m = df['Label'].eq('A')\ndf_a = df.loc[df[m].groupby(['Id', 'Label'])['Width'].idxmax()]\n\ndf_out = pd.concat([df[~m], df_a]).sort_index()\n</code></pre>\n<p><strong>Details:</strong></p>\n<p>Create a boolean mask with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html\" rel=\"nofollow noreferrer\"><code>.eq</code></a> specifying the condition where <code>Label</code> equals <code>A</code>:</p>\n<pre><code>&gt;&gt;&gt; m\n\n0     True\n1     True\n2    False\n3     True\n4     True\nName: Label, dtype: bool\n</code></pre>\n<p>filter the rows using the above mask and group this dataframe on <code>Id</code> and <code>Label</code> and aggregate <code>Width</code> using <code>idxmax</code> to get the indices on max values:</p>\n<pre><code>&gt;&gt;&gt; df[m].groupby(['Id', 'Label'])['Width'].idxmax().tolist()\n[0, 4]\n\n&gt;&gt;&gt; df_a\n\n   Id Label  Width\n0   0     A      5\n4   1     A      9\n</code></pre>\n<p>finally <code>concat</code> the above dataframe with the dataframe containing labels other that <code>A</code> and <code>sort</code> the index to maintain the order:</p>\n<pre><code>&gt;&gt;&gt; df_out\n\n   Id Label  Width\n0   0     A      5\n2   0     B      4\n4   1     A      9\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like this:</p>\n<pre><code>| Id | Label | Width |\n|----|-------| ------|\n| 0  |   A   |   5   |\n| 0  |   A   |   3   |\n| 0  |   B   |   4   |\n| 1  |   A   |   7   |\n| 1  |   A   |   9   |\n</code></pre>\n<p>I want to write a function that takes the rows with same id and label A and filter it based on the highest width</p>\n<p>so the after applying the function the dataframe would be:</p>\n<pre><code>| Id | Label | Width |\n|----|-------| ------|\n| 0  |   A   |   5   |\n| 0  |   B   |   4   |\n| 1  |   A   |   9   |\n</code></pre>\n",
        "formatted_input": {
            "qid": 66194784,
            "link": "https://stackoverflow.com/questions/66194784/retrieve-rows-with-highest-value-with-condition",
            "question": {
                "title": "Retrieve rows with highest value with condition",
                "ques_desc": "I have a dataframe that looks like this: I want to write a function that takes the rows with same id and label A and filter it based on the highest width so the after applying the function the dataframe would be: "
            },
            "io": [
                "| Id | Label | Width |\n|----|-------| ------|\n| 0  |   A   |   5   |\n| 0  |   A   |   3   |\n| 0  |   B   |   4   |\n| 1  |   A   |   7   |\n| 1  |   A   |   9   |\n",
                "| Id | Label | Width |\n|----|-------| ------|\n| 0  |   A   |   5   |\n| 0  |   B   |   4   |\n| 1  |   A   |   9   |\n"
            ],
            "answer": {
                "ans_desc": "Let us try: Details: Create a boolean mask with specifying the condition where equals : filter the rows using the above mask and group this dataframe on and and aggregate using to get the indices on max values: finally the above dataframe with the dataframe containing labels other that and the index to maintain the order: ",
                "code": [
                    "m = df['Label'].eq('A')\ndf_a = df.loc[df[m].groupby(['Id', 'Label'])['Width'].idxmax()]\n\ndf_out = pd.concat([df[~m], df_a]).sort_index()\n",
                    ">>> m\n\n0     True\n1     True\n2    False\n3     True\n4     True\nName: Label, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 213,
            "user_id": 11666358,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/23f554cfcbd3a1d19a2e6e76aa84176f?s=128&d=identicon&r=PG&f=1",
            "display_name": "dxb",
            "link": "https://stackoverflow.com/users/11666358/dxb"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 66194667,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1613300169,
        "creation_date": 1613296632,
        "question_id": 66194236,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66194236/dataframe-pandas-how-ploting-same-columns-of-two-dataframe-for-visualising-t",
        "title": "Dataframe - Pandas - How ploting same columns of two dataframe for visualising the differences",
        "body": "<p>There are two dataframes</p>\n<pre><code>df1\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 5    |\n+-----+-----+-------+\n| 2   | 2   | 12    |\n+-----+-----+-------+\n| 3   | 3   | 34    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 90    |\n+-----+-----+-------+\n</code></pre>\n<p>and</p>\n<pre><code>df2\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 14    |\n+-----+-----+-------+\n| 2   | 2   | 15    |\n+-----+-----+-------+\n| 3   | 3   | 45    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 31    |\n+-----+-----+-------+\n</code></pre>\n<p>I would like to have a plot that shows the both price columns on X axis and sum on the Y axis to see how are the difference between these two dataframes.</p>\n<p>I tried the below but does nothing.</p>\n<pre><code>line1 = df1.plot.line()\nline2 = df2.plot.line()\nlines = df.plot.line(x=df1['price'], y=df2['price']\n</code></pre>\n<p>What is the best way to show the differences between the two patterns of the price in these two dataframes?</p>\n<p>I thought of something like this, but if there is a better way to show the differences please mention it.</p>\n<p><a href=\"https://i.stack.imgur.com/OXV7V.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/OXV7V.png\" alt=\"enter image description here\" /></a></p>\n",
        "answer_body": "<p>If you take first column from df1, and second column from df2, there will be no couple of lines, only one. For qualitatively compartion you can use matplotlib in simple way, because it automaticly creates a figure.</p>\n<pre><code>import pandas as pd \nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\n\ndf1 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\ndf2 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\n\nplt.plot(df1['col2'], label='first')\nplt.plot(df2['col2'], label='second')\nplt.legend()\n</code></pre>\n<p>Here is the result:<br>\n<a href=\"https://i.stack.imgur.com/Y3S7h.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Y3S7h.png\" alt=\"enter image description here\" /></a></p>\n<p>For every arg on x it shows point on df1[col1] and df2[col2]. But with this plot you can not compare it quantitatively.<br></p>\n<p>PS: Here is logic you tried to realize but with seaborn.</p>\n<pre><code>df3 = pd.merge(df1,df2, on='col1')\nsns.lineplot(x='col2_x', y='col2_y', data=df3)\n</code></pre>\n<p>Result:<br>\n<a href=\"https://i.stack.imgur.com/6y9Ie.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/6y9Ie.png\" alt=\"enter image description here\" /></a><br></p>\n<p><strong>Optional</strong></p>\n<p>Quantitative comparison.</p>\n<pre><code>df3['dif'] = abs(df3['col2_x'] - df3['col2_y'])\n\nsns.lineplot(x='col1', y='dif', data=df3)\nplt.xticks(df3['col1'])\n</code></pre>\n<p>Result:<br></p>\n<p><a href=\"https://i.stack.imgur.com/ZYIX8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ZYIX8.png\" alt=\"enter image description here\" /></a></p>\n",
        "question_body": "<p>There are two dataframes</p>\n<pre><code>df1\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 5    |\n+-----+-----+-------+\n| 2   | 2   | 12    |\n+-----+-----+-------+\n| 3   | 3   | 34    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 90    |\n+-----+-----+-------+\n</code></pre>\n<p>and</p>\n<pre><code>df2\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 14    |\n+-----+-----+-------+\n| 2   | 2   | 15    |\n+-----+-----+-------+\n| 3   | 3   | 45    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 31    |\n+-----+-----+-------+\n</code></pre>\n<p>I would like to have a plot that shows the both price columns on X axis and sum on the Y axis to see how are the difference between these two dataframes.</p>\n<p>I tried the below but does nothing.</p>\n<pre><code>line1 = df1.plot.line()\nline2 = df2.plot.line()\nlines = df.plot.line(x=df1['price'], y=df2['price']\n</code></pre>\n<p>What is the best way to show the differences between the two patterns of the price in these two dataframes?</p>\n<p>I thought of something like this, but if there is a better way to show the differences please mention it.</p>\n<p><a href=\"https://i.stack.imgur.com/OXV7V.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/OXV7V.png\" alt=\"enter image description here\" /></a></p>\n",
        "formatted_input": {
            "qid": 66194236,
            "link": "https://stackoverflow.com/questions/66194236/dataframe-pandas-how-ploting-same-columns-of-two-dataframe-for-visualising-t",
            "question": {
                "title": "Dataframe - Pandas - How ploting same columns of two dataframe for visualising the differences",
                "ques_desc": "There are two dataframes and I would like to have a plot that shows the both price columns on X axis and sum on the Y axis to see how are the difference between these two dataframes. I tried the below but does nothing. What is the best way to show the differences between the two patterns of the price in these two dataframes? I thought of something like this, but if there is a better way to show the differences please mention it. "
            },
            "io": [
                "df1\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 5    |\n+-----+-----+-------+\n| 2   | 2   | 12    |\n+-----+-----+-------+\n| 3   | 3   | 34    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 90    |\n+-----+-----+-------+\n",
                "df2\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 14    |\n+-----+-----+-------+\n| 2   | 2   | 15    |\n+-----+-----+-------+\n| 3   | 3   | 45    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 31    |\n+-----+-----+-------+\n"
            ],
            "answer": {
                "ans_desc": "If you take first column from df1, and second column from df2, there will be no couple of lines, only one. For qualitatively compartion you can use matplotlib in simple way, because it automaticly creates a figure. Here is the result: For every arg on x it shows point on df1[col1] and df2[col2]. But with this plot you can not compare it quantitatively. PS: Here is logic you tried to realize but with seaborn. Result: Optional Quantitative comparison. Result: ",
                "code": [
                    "import pandas as pd \nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\n\ndf1 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\ndf2 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\n\nplt.plot(df1['col2'], label='first')\nplt.plot(df2['col2'], label='second')\nplt.legend()\n",
                    "df3 = pd.merge(df1,df2, on='col1')\nsns.lineplot(x='col2_x', y='col2_y', data=df3)\n",
                    "df3['dif'] = abs(df3['col2_x'] - df3['col2_y'])\n\nsns.lineplot(x='col1', y='dif', data=df3)\nplt.xticks(df3['col1'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "apply"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 12898709,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/097cbb3d231ffff256af0bd980aa8c8f?s=128&d=identicon&r=PG&f=1",
            "display_name": "JuanMacD",
            "link": "https://stackoverflow.com/users/12898709/juanmacd"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 66147022,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1613002238,
        "creation_date": 1613000271,
        "question_id": 66146761,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66146761/is-there-a-way-to-apply-a-condition-while-using-apply-and-lambda-in-a-dataframe",
        "title": "Is there a way to apply a condition while using apply and lambda in a DataFrame?",
        "body": "<p>I have a Pandas dataframe that looks like this:</p>\n<pre><code>    ID                                               Dyn\n0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1\n1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200\n</code></pre>\n<p>And I'm looking for a way to iter trough the Dyn column, generating another one that sums only the numbers that are bigger than a cutoff, i.e.: 0.150, assigning all the values that pass it a value of one.\nThis is what the expected result should look like:</p>\n<pre><code>    ID                                               Dyn Sum\n0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1   2\n1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200   7\n</code></pre>\n<p>I thought I could use apply, while ittering trough all of the rows:</p>\n<pre><code>for index, rows in df.iterrows():\n   df['Sum'] = df['Dyn'].apply(lambda x: x = 1 if int(x) &gt; 0.150 ) \n</code></pre>\n<p>But I'm lost on how to apply the condition (only sum it if it's greater than 0.150) to all the values inside 'Dyn' and how to assign the value of 1 to them.\nAll advice is accepted. Thanks!</p>\n",
        "answer_body": "<pre><code>#Create temp column to hold Dyn convereted into list\ndf=df.assign(sum=df['Dyn'].str.split(','))\n\n#Explode DataFrame\ndf=df.explode('sum')\n#Convert to float\ndf['sum']=df['sum'].astype(float)\n#Filter out values greater that 0.015, groupby and sum\ndf[df['sum'].gt(0.150)].groupby(['ID','Dyn'])['sum'].sum().reset_index()\n</code></pre>\n",
        "question_body": "<p>I have a Pandas dataframe that looks like this:</p>\n<pre><code>    ID                                               Dyn\n0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1\n1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200\n</code></pre>\n<p>And I'm looking for a way to iter trough the Dyn column, generating another one that sums only the numbers that are bigger than a cutoff, i.e.: 0.150, assigning all the values that pass it a value of one.\nThis is what the expected result should look like:</p>\n<pre><code>    ID                                               Dyn Sum\n0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1   2\n1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200   7\n</code></pre>\n<p>I thought I could use apply, while ittering trough all of the rows:</p>\n<pre><code>for index, rows in df.iterrows():\n   df['Sum'] = df['Dyn'].apply(lambda x: x = 1 if int(x) &gt; 0.150 ) \n</code></pre>\n<p>But I'm lost on how to apply the condition (only sum it if it's greater than 0.150) to all the values inside 'Dyn' and how to assign the value of 1 to them.\nAll advice is accepted. Thanks!</p>\n",
        "formatted_input": {
            "qid": 66146761,
            "link": "https://stackoverflow.com/questions/66146761/is-there-a-way-to-apply-a-condition-while-using-apply-and-lambda-in-a-dataframe",
            "question": {
                "title": "Is there a way to apply a condition while using apply and lambda in a DataFrame?",
                "ques_desc": "I have a Pandas dataframe that looks like this: And I'm looking for a way to iter trough the Dyn column, generating another one that sums only the numbers that are bigger than a cutoff, i.e.: 0.150, assigning all the values that pass it a value of one. This is what the expected result should look like: I thought I could use apply, while ittering trough all of the rows: But I'm lost on how to apply the condition (only sum it if it's greater than 0.150) to all the values inside 'Dyn' and how to assign the value of 1 to them. All advice is accepted. Thanks! "
            },
            "io": [
                "    ID                                               Dyn\n0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1\n1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200\n",
                "    ID                                               Dyn Sum\n0 AA01   0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1   2\n1 BG54   0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200   7\n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "#Create temp column to hold Dyn convereted into list\ndf=df.assign(sum=df['Dyn'].str.split(','))\n\n#Explode DataFrame\ndf=df.explode('sum')\n#Convert to float\ndf['sum']=df['sum'].astype(float)\n#Filter out values greater that 0.015, groupby and sum\ndf[df['sum'].gt(0.150)].groupby(['ID','Dyn'])['sum'].sum().reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15183599,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c0da19c4316ed030c11fdb370f7aa598?s=128&d=identicon&r=PG&f=1",
            "display_name": "damien1991",
            "link": "https://stackoverflow.com/users/15183599/damien1991"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 66138417,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1612972387,
        "creation_date": 1612965251,
        "last_edit_date": 1612972387,
        "question_id": 66138245,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66138245/pandas-regex-comprehension-isolate-single-result",
        "title": "Pandas regex comprehension - isolate single result",
        "body": "<p>I have a dataframe that's been extracted from an SQL server, and I used regex to extract a string of three dimensions. I need all three dimensions in three new columns so I have used a regular expression for a number optionally separated by a period, and created a column from this findall result. But the result shows as a list and I am unable to index the second dimension. Due to the urgency I have been able to temporarily solve this with a lookaround. But how can I directly extract</p>\n<p>dimension column extract - not all are in this format</p>\n<pre><code>[1103.5 x 308.8 x 25.4 mm]\n[33.3 x 13 x 9.5mm]\n[136.5 x 15 x 12.7 mm]\n</code></pre>\n<p>code extract for finding dims</p>\n<pre><code>dims = re.compile(r'.*mm', flags=re.I)\nx_dim = re.compile(r'^([0-9]*\\.?[0-9]?)', flags=re.I)\ny_dim = re.compile(r'(\\d+\\.?\\d?)', flags=re.I)\n# y_dim = re.compile(r'(?&lt;=x\\s)(\\d+\\.?\\d?)(?=\\sx)', flags=re.I) # lookaround solution\ndf['Dimensions'] = df['Part_Extended_Desc'].str.findall(dims)\ndf['x'] = df['Part_Extended_Desc'].str.findall(x_dim)\ndf['y'] = df['Part_Extended_Desc'].str.findall(y_dim)\ndf['trial'] = df['y'][0][1] # does not work\ndf['second_dim'] = df.dim_list.apply(lambda x:x[1]) # also doesn't work (list index out of range)\nprint(df.tail(50))\n</code></pre>\n<p>sample output using the findall result</p>\n<pre><code>[1493.4, 204.2, 25.4, 0013, 900, 4]\n[136.5, 15, 12.7, 001, 900, 2]\n</code></pre>\n<p>I would need a column for 1493.4 and a second column for 204.2 - I can do the first one but how would I create a column for specific indexes in the regex results. I have tried lambda, list comprehension, and everything else I can think of.</p>\n<p>So far I cannot find a similar question online and I know it should be simple - but its taken me 2 days!</p>\n<p>Many thanks for all your help</p>\n<p>EDIT:</p>\n<p>To confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern.</p>\n<p>Additionally, my focus is on obtaining only list item n to a new column and not every item in the list</p>\n",
        "answer_body": "<p>If I understand correctly you are looking for way of expanding column holding lists, assuming that number of matches is equal in all record, you might do:</p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame({'dim':['10x10','12x10','10x12']})\ndf['dim_list'] = df.dim.str.findall(r'\\d+')\ndf[['width','height']] = df.dim_list.apply(pd.Series)\nprint(df)\n</code></pre>\n<p>output:</p>\n<pre><code>     dim  dim_list width height\n0  10x10  [10, 10]    10     10\n1  12x10  [12, 10]    12     10\n2  10x12  [10, 12]    10     12\n</code></pre>\n<p>If only n-th element is required you might use <code>lambda</code> following way:</p>\n<pre><code>df['second_dim'] = df.dim_list.apply(lambda x:x[1])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that's been extracted from an SQL server, and I used regex to extract a string of three dimensions. I need all three dimensions in three new columns so I have used a regular expression for a number optionally separated by a period, and created a column from this findall result. But the result shows as a list and I am unable to index the second dimension. Due to the urgency I have been able to temporarily solve this with a lookaround. But how can I directly extract</p>\n<p>dimension column extract - not all are in this format</p>\n<pre><code>[1103.5 x 308.8 x 25.4 mm]\n[33.3 x 13 x 9.5mm]\n[136.5 x 15 x 12.7 mm]\n</code></pre>\n<p>code extract for finding dims</p>\n<pre><code>dims = re.compile(r'.*mm', flags=re.I)\nx_dim = re.compile(r'^([0-9]*\\.?[0-9]?)', flags=re.I)\ny_dim = re.compile(r'(\\d+\\.?\\d?)', flags=re.I)\n# y_dim = re.compile(r'(?&lt;=x\\s)(\\d+\\.?\\d?)(?=\\sx)', flags=re.I) # lookaround solution\ndf['Dimensions'] = df['Part_Extended_Desc'].str.findall(dims)\ndf['x'] = df['Part_Extended_Desc'].str.findall(x_dim)\ndf['y'] = df['Part_Extended_Desc'].str.findall(y_dim)\ndf['trial'] = df['y'][0][1] # does not work\ndf['second_dim'] = df.dim_list.apply(lambda x:x[1]) # also doesn't work (list index out of range)\nprint(df.tail(50))\n</code></pre>\n<p>sample output using the findall result</p>\n<pre><code>[1493.4, 204.2, 25.4, 0013, 900, 4]\n[136.5, 15, 12.7, 001, 900, 2]\n</code></pre>\n<p>I would need a column for 1493.4 and a second column for 204.2 - I can do the first one but how would I create a column for specific indexes in the regex results. I have tried lambda, list comprehension, and everything else I can think of.</p>\n<p>So far I cannot find a similar question online and I know it should be simple - but its taken me 2 days!</p>\n<p>Many thanks for all your help</p>\n<p>EDIT:</p>\n<p>To confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern.</p>\n<p>Additionally, my focus is on obtaining only list item n to a new column and not every item in the list</p>\n",
        "formatted_input": {
            "qid": 66138245,
            "link": "https://stackoverflow.com/questions/66138245/pandas-regex-comprehension-isolate-single-result",
            "question": {
                "title": "Pandas regex comprehension - isolate single result",
                "ques_desc": "I have a dataframe that's been extracted from an SQL server, and I used regex to extract a string of three dimensions. I need all three dimensions in three new columns so I have used a regular expression for a number optionally separated by a period, and created a column from this findall result. But the result shows as a list and I am unable to index the second dimension. Due to the urgency I have been able to temporarily solve this with a lookaround. But how can I directly extract dimension column extract - not all are in this format code extract for finding dims sample output using the findall result I would need a column for 1493.4 and a second column for 204.2 - I can do the first one but how would I create a column for specific indexes in the regex results. I have tried lambda, list comprehension, and everything else I can think of. So far I cannot find a similar question online and I know it should be simple - but its taken me 2 days! Many thanks for all your help EDIT: To confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern. Additionally, my focus is on obtaining only list item n to a new column and not every item in the list "
            },
            "io": [
                "[1103.5 x 308.8 x 25.4 mm]\n[33.3 x 13 x 9.5mm]\n[136.5 x 15 x 12.7 mm]\n",
                "[1493.4, 204.2, 25.4, 0013, 900, 4]\n[136.5, 15, 12.7, 001, 900, 2]\n"
            ],
            "answer": {
                "ans_desc": "If I understand correctly you are looking for way of expanding column holding lists, assuming that number of matches is equal in all record, you might do: output: If only n-th element is required you might use following way: ",
                "code": [
                    "import pandas as pd\ndf = pd.DataFrame({'dim':['10x10','12x10','10x12']})\ndf['dim_list'] = df.dim.str.findall(r'\\d+')\ndf[['width','height']] = df.dim_list.apply(pd.Series)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1878,
            "user_id": 11922765,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ed69b624cdb86e52caf0010e274df7b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mainland",
            "link": "https://stackoverflow.com/users/11922765/mainland"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 66129132,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1612956318,
        "creation_date": 1612912429,
        "last_edit_date": 1612956318,
        "question_id": 66128637,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66128637/python-find-closest-neighbors-to-a-value-in-a-dataframe",
        "title": "Python find closest neighbors to a value in a dataframe",
        "body": "<p>I have a dataframe or list. I want to find the closest values and their index to a given value.</p>\n<p>My code:</p>\n<pre><code>df = pd.DataFrame({'num':[20,24,35,38]})\n\nval = 26 \n\nval_idx = df.iloc[(df['num']-val).abs().argsort()[:2]]\n</code></pre>\n<p>Present output (val_idx):</p>\n<pre><code>    num\n1   24\n0   20\n</code></pre>\n<p>Expected output (val_idx):</p>\n<pre><code>    num\n2   35\n1   24\n</code></pre>\n",
        "answer_body": "<pre><code>import numpy as np\nimport pandas as pd\n\n# setup\ndf = pd.DataFrame({'num':[20,24,35,38]})\nval = 26 \n\n# subtract to get differences\ntest = np.absolute(np.subtract(val, df[&quot;num&quot;]))\n\n# get index\nidx = np.argmin(test)\n# Condition\nidx = np.where(df[&quot;num&quot;][idx] &gt; val, [idx-1, idx], [idx, idx+1])\n\nprint(idx)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe or list. I want to find the closest values and their index to a given value.</p>\n<p>My code:</p>\n<pre><code>df = pd.DataFrame({'num':[20,24,35,38]})\n\nval = 26 \n\nval_idx = df.iloc[(df['num']-val).abs().argsort()[:2]]\n</code></pre>\n<p>Present output (val_idx):</p>\n<pre><code>    num\n1   24\n0   20\n</code></pre>\n<p>Expected output (val_idx):</p>\n<pre><code>    num\n2   35\n1   24\n</code></pre>\n",
        "formatted_input": {
            "qid": 66128637,
            "link": "https://stackoverflow.com/questions/66128637/python-find-closest-neighbors-to-a-value-in-a-dataframe",
            "question": {
                "title": "Python find closest neighbors to a value in a dataframe",
                "ques_desc": "I have a dataframe or list. I want to find the closest values and their index to a given value. My code: Present output (val_idx): Expected output (val_idx): "
            },
            "io": [
                "    num\n1   24\n0   20\n",
                "    num\n2   35\n1   24\n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "import numpy as np\nimport pandas as pd\n\n# setup\ndf = pd.DataFrame({'num':[20,24,35,38]})\nval = 26 \n\n# subtract to get differences\ntest = np.absolute(np.subtract(val, df[\"num\"]))\n\n# get index\nidx = np.argmin(test)\n# Condition\nidx = np.where(df[\"num\"][idx] > val, [idx-1, idx], [idx, idx+1])\n\nprint(idx)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 650,
            "user_id": 5709203,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ab035385a8931db29144b5d31de6dba6?s=128&d=identicon&r=PG&f=1",
            "display_name": "Shantanu",
            "link": "https://stackoverflow.com/users/5709203/shantanu"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 66126918,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1612903794,
        "creation_date": 1612902115,
        "question_id": 66126525,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66126525/pandas-clean-up-string-column-containing-single-quotes-and-brackets-using-regex",
        "title": "Pandas: Clean up String column containing Single Quotes and Brackets using Regex?",
        "body": "<p>I want to clean the following Pandas dataframe column, but in a single and efficient statement than the way I am trying to achieve it in the code below.</p>\n<p><strong>Input:</strong></p>\n<pre><code>                  string\n0  ['string', '#string']\n1            ['#string']\n2                     []\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>            string\n0  string, #string\n1          #string\n2              NaN\n</code></pre>\n<p><strong>Code:</strong></p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nd = {&quot;string&quot;: [&quot;['string', '#string']&quot;, &quot;['#string']&quot;, &quot;[]&quot;]}\ndf = pd.DataFrame(d)\n\ndf['string'] = df['string'].astype(str).str.strip('[]')\ndf['string'] = df['string'].replace(&quot;\\'&quot;, &quot;&quot;, regex=True)\ndf['string'] = df['string'].replace(r'^\\s*$', np.nan, regex=True)\n\nprint(df)\n</code></pre>\n",
        "answer_body": "<p>You can use</p>\n<pre><code>df['string'] = df['string'].astype(str).str.replace(r&quot;^[][\\s]*$|(^\\[+|\\]+$|')&quot;, lambda m: '' if m.group(1) else np.nan)\n</code></pre>\n<p><em>Details</em>:</p>\n<ul>\n<li><code>^[][\\s]*$</code> - matches a string that only consists of zero or more <code>[</code>, <code>]</code> or whitespace chars</li>\n<li><code>|</code> - or</li>\n<li><code>(^\\[+|\\]+$|')</code> - captures into Group 1 one or more <code>[</code> chars at the start of string, or one or more <code>]</code> chars at the end of string or any <code>'</code> char.</li>\n</ul>\n<p>If Group 1 matches, the replacement is an empty string (the match is removed), else, the replacement is <code>np.nan</code>.</p>\n",
        "question_body": "<p>I want to clean the following Pandas dataframe column, but in a single and efficient statement than the way I am trying to achieve it in the code below.</p>\n<p><strong>Input:</strong></p>\n<pre><code>                  string\n0  ['string', '#string']\n1            ['#string']\n2                     []\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>            string\n0  string, #string\n1          #string\n2              NaN\n</code></pre>\n<p><strong>Code:</strong></p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nd = {&quot;string&quot;: [&quot;['string', '#string']&quot;, &quot;['#string']&quot;, &quot;[]&quot;]}\ndf = pd.DataFrame(d)\n\ndf['string'] = df['string'].astype(str).str.strip('[]')\ndf['string'] = df['string'].replace(&quot;\\'&quot;, &quot;&quot;, regex=True)\ndf['string'] = df['string'].replace(r'^\\s*$', np.nan, regex=True)\n\nprint(df)\n</code></pre>\n",
        "formatted_input": {
            "qid": 66126525,
            "link": "https://stackoverflow.com/questions/66126525/pandas-clean-up-string-column-containing-single-quotes-and-brackets-using-regex",
            "question": {
                "title": "Pandas: Clean up String column containing Single Quotes and Brackets using Regex?",
                "ques_desc": "I want to clean the following Pandas dataframe column, but in a single and efficient statement than the way I am trying to achieve it in the code below. Input: Output: Code: "
            },
            "io": [
                "                  string\n0  ['string', '#string']\n1            ['#string']\n2                     []\n",
                "            string\n0  string, #string\n1          #string\n2              NaN\n"
            ],
            "answer": {
                "ans_desc": "You can use Details: - matches a string that only consists of zero or more , or whitespace chars - or - captures into Group 1 one or more chars at the start of string, or one or more chars at the end of string or any char. If Group 1 matches, the replacement is an empty string (the match is removed), else, the replacement is . ",
                "code": [
                    "df['string'] = df['string'].astype(str).str.replace(r\"^[][\\s]*$|(^\\[+|\\]+$|')\", lambda m: '' if m.group(1) else np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "string",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 3974,
            "user_id": 5082463,
            "user_type": "registered",
            "accept_rate": 74,
            "profile_image": "https://www.gravatar.com/avatar/877843e34372342a80c73f38ee79ad60?s=128&d=identicon&r=PG&f=1",
            "display_name": "KcFnMi",
            "link": "https://stackoverflow.com/users/5082463/kcfnmi"
        },
        "is_answered": true,
        "view_count": 12567,
        "accepted_answer_id": 44616445,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1612781737,
        "creation_date": 1497794746,
        "last_edit_date": 1612537826,
        "question_id": 44615807,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/44615807/remove-double-quotes-in-pandas",
        "title": "Remove double quotes in Pandas",
        "body": "<p>I have the following file:</p>\n\n<pre><code>\"j\"; \"x\"; y\n\"0\"; \"1\"; 5\n\"1\"; \"2\"; 6\n\"2\"; \"3\"; 7\n\"3\"; \"4\"; 8\n\"4\"; \"5\"; 3\n\"5\"; \"5\"; 4\n</code></pre>\n\n<p>Which I read by:</p>\n\n<pre><code>df = pd.read_csv('test.csv', delimiter='; ', engine='python')\n</code></pre>\n\n<p>Then I print <code>print df</code> and see:</p>\n\n<pre><code>   \"j\"  \"x\"  y\n0  \"0\"  \"1\"  5\n1  \"1\"  \"2\"  6\n2  \"2\"  \"3\"  7\n3  \"3\"  \"4\"  8\n4  \"4\"  \"5\"  3\n5  \"5\"  \"5\"  4\n</code></pre>\n\n<p>Instead, I would like to see:</p>\n\n<pre><code>   j  x  y\n0  0  1  5\n1  1  2  6\n2  2  3  7\n3  3  4  8\n4  4  5  3\n5  5  5  4\n</code></pre>\n\n<p>How to remove the double quotes?</p>\n",
        "answer_body": "<p>I did it with:</p>\n\n<pre><code>rm_quote = lambda x: x.replace('\"', '')\n\ndf = pd.read_csv('test.csv', delimiter='; ', engine='python', \n     converters={'\\\"j\\\"': rm_quote, \n                 '\\\"x\\\"': rm_quote})\n\ndf = df.rename(columns=rm_quote)\n</code></pre>\n",
        "question_body": "<p>I have the following file:</p>\n\n<pre><code>\"j\"; \"x\"; y\n\"0\"; \"1\"; 5\n\"1\"; \"2\"; 6\n\"2\"; \"3\"; 7\n\"3\"; \"4\"; 8\n\"4\"; \"5\"; 3\n\"5\"; \"5\"; 4\n</code></pre>\n\n<p>Which I read by:</p>\n\n<pre><code>df = pd.read_csv('test.csv', delimiter='; ', engine='python')\n</code></pre>\n\n<p>Then I print <code>print df</code> and see:</p>\n\n<pre><code>   \"j\"  \"x\"  y\n0  \"0\"  \"1\"  5\n1  \"1\"  \"2\"  6\n2  \"2\"  \"3\"  7\n3  \"3\"  \"4\"  8\n4  \"4\"  \"5\"  3\n5  \"5\"  \"5\"  4\n</code></pre>\n\n<p>Instead, I would like to see:</p>\n\n<pre><code>   j  x  y\n0  0  1  5\n1  1  2  6\n2  2  3  7\n3  3  4  8\n4  4  5  3\n5  5  5  4\n</code></pre>\n\n<p>How to remove the double quotes?</p>\n",
        "formatted_input": {
            "qid": 44615807,
            "link": "https://stackoverflow.com/questions/44615807/remove-double-quotes-in-pandas",
            "question": {
                "title": "Remove double quotes in Pandas",
                "ques_desc": "I have the following file: Which I read by: Then I print and see: Instead, I would like to see: How to remove the double quotes? "
            },
            "io": [
                "   \"j\"  \"x\"  y\n0  \"0\"  \"1\"  5\n1  \"1\"  \"2\"  6\n2  \"2\"  \"3\"  7\n3  \"3\"  \"4\"  8\n4  \"4\"  \"5\"  3\n5  \"5\"  \"5\"  4\n",
                "   j  x  y\n0  0  1  5\n1  1  2  6\n2  2  3  7\n3  3  4  8\n4  4  5  3\n5  5  5  4\n"
            ],
            "answer": {
                "ans_desc": "I did it with: ",
                "code": [
                    "rm_quote = lambda x: x.replace('\"', '')\n\ndf = pd.read_csv('test.csv', delimiter='; ', engine='python', \n     converters={'\\\"j\\\"': rm_quote, \n                 '\\\"x\\\"': rm_quote})\n\ndf = df.rename(columns=rm_quote)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "categorical-data"
        ],
        "owner": {
            "reputation": 85,
            "user_id": 15155674,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5ff3d2d966415a18e4b0d0ffd59a3f72?s=128&d=identicon&r=PG&f=1",
            "display_name": "alisonggg",
            "link": "https://stackoverflow.com/users/15155674/alisonggg"
        },
        "is_answered": true,
        "view_count": 142,
        "accepted_answer_id": 66088438,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1612705061,
        "creation_date": 1612703263,
        "last_edit_date": 1612704243,
        "question_id": 66088290,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66088290/use-a-categorical-column-to-order-the-dataframe-according-to-an-array",
        "title": "Use a categorical column to order the dataframe according to an array",
        "body": "<p>I have an array like this:</p>\n<pre><code>['A 100', 'A 200', 'A 300', 'A 400', 'A 500', 'B 100', 'B 200', 'B 300', 'B 400']\n</code></pre>\n<p>I also have a dataframe like this:</p>\n<pre><code>BIN      CA      SUM\n100       B      B 100\n300       A      A 300\n300       B      B 300\n400       B      B 400\n400       A      A 400\n200       B      B 200\n100       A      A 100\n200       A      A 200\n</code></pre>\n<p>I want to use <code>pd.Categorical</code> to order the column dataframe according to the array.</p>\n<p>The expected output is:</p>\n<pre><code>BIN      CA      SUM\n100       A      A 100\n200       A      A 200\n300       A      A 300\n400       A      A 400\n100       B      B 100\n200       B      B 200\n300       B      B 300\n400       B      B 400\n</code></pre>\n",
        "answer_body": "<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Categorical.html\" rel=\"nofollow noreferrer\"><code>pd.Categorical</code></a> to convert the <code>SUM</code> column to categorical column having order, then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\" rel=\"nofollow noreferrer\"><code>sort</code></a> the values:</p>\n<pre><code>df['SUM'] = pd.Categorical(df['SUM'], categories=arr, ordered=True)\ndf.sort_values('SUM')\n</code></pre>\n<p>Alternatively you can create a dictionary that maps the items in <code>arr</code> to their sorting order then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\" rel=\"nofollow noreferrer\"><code>.map</code></a> this dictionary on <code>SUM</code> column and use <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\" rel=\"nofollow noreferrer\"><code>np.argsort</code></a> to get the indices that would sort the dataframe:</p>\n<pre><code>dct = {v: i for i, v in enumerate(arr)}\ndf.iloc[np.argsort(df['SUM'].map(dct))]\n</code></pre>\n<hr />\n<pre><code>   BIN CA    SUM\n6  100  A  A 100\n7  200  A  A 200\n1  300  A  A 300\n4  400  A  A 400\n0  100  B  B 100\n5  200  B  B 200\n2  300  B  B 300\n3  400  B  B 400\n</code></pre>\n",
        "question_body": "<p>I have an array like this:</p>\n<pre><code>['A 100', 'A 200', 'A 300', 'A 400', 'A 500', 'B 100', 'B 200', 'B 300', 'B 400']\n</code></pre>\n<p>I also have a dataframe like this:</p>\n<pre><code>BIN      CA      SUM\n100       B      B 100\n300       A      A 300\n300       B      B 300\n400       B      B 400\n400       A      A 400\n200       B      B 200\n100       A      A 100\n200       A      A 200\n</code></pre>\n<p>I want to use <code>pd.Categorical</code> to order the column dataframe according to the array.</p>\n<p>The expected output is:</p>\n<pre><code>BIN      CA      SUM\n100       A      A 100\n200       A      A 200\n300       A      A 300\n400       A      A 400\n100       B      B 100\n200       B      B 200\n300       B      B 300\n400       B      B 400\n</code></pre>\n",
        "formatted_input": {
            "qid": 66088290,
            "link": "https://stackoverflow.com/questions/66088290/use-a-categorical-column-to-order-the-dataframe-according-to-an-array",
            "question": {
                "title": "Use a categorical column to order the dataframe according to an array",
                "ques_desc": "I have an array like this: I also have a dataframe like this: I want to use to order the column dataframe according to the array. The expected output is: "
            },
            "io": [
                "BIN      CA      SUM\n100       B      B 100\n300       A      A 300\n300       B      B 300\n400       B      B 400\n400       A      A 400\n200       B      B 200\n100       A      A 100\n200       A      A 200\n",
                "BIN      CA      SUM\n100       A      A 100\n200       A      A 200\n300       A      A 300\n400       A      A 400\n100       B      B 100\n200       B      B 200\n300       B      B 300\n400       B      B 400\n"
            ],
            "answer": {
                "ans_desc": "You can use to convert the column to categorical column having order, then the values: Alternatively you can create a dictionary that maps the items in to their sorting order then this dictionary on column and use to get the indices that would sort the dataframe: ",
                "code": [
                    "df['SUM'] = pd.Categorical(df['SUM'], categories=arr, ordered=True)\ndf.sort_values('SUM')\n",
                    "dct = {v: i for i, v in enumerate(arr)}\ndf.iloc[np.argsort(df['SUM'].map(dct))]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 175,
            "user_id": 14088584,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-RRC9038Ea5M/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclF9g9zltmH8LGdXkrjD8fk8fKq-Q/photo.jpg?sz=128",
            "display_name": "SHIVANSHU SAHOO",
            "link": "https://stackoverflow.com/users/14088584/shivanshu-sahoo"
        },
        "is_answered": true,
        "view_count": 61,
        "accepted_answer_id": 66075737,
        "answer_count": 4,
        "score": 3,
        "last_activity_date": 1612653698,
        "creation_date": 1612606437,
        "question_id": 66075712,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66075712/create-a-pandas-column-of-numbers-from-1-to-3-and-repeat-again",
        "title": "Create a pandas column of numbers from 1 to 3 and repeat again",
        "body": "<p>I have a dataframe:</p>\n<pre><code>   StoreNumber    Year  \n    1000          2000  \n    1000          2001  \n    1000          2002  \n    1001          2000  \n    1001          2001  \n    1001          2002  \n</code></pre>\n<p>I want to add a column so that my final dataframe looks like:</p>\n<pre><code>StoreNumber       Year   New\n    1000          2000    1\n    1000          2001    2\n    1000          2002    3\n    1001          2000    1\n    1001          2001    2\n    1001          2002    3 \n</code></pre>\n<p>I don't want the new row to depend upon StoreNumber which looks ovious in example. I want to start numbering with 1 and when I reach 3, then again start with 1.\nHow do I do it?</p>\n",
        "answer_body": "<p>You can use <a href=\"https://docs.python.org/3/library/itertools.html#itertools.cycle\" rel=\"nofollow noreferrer\"><code>itertools.cycle</code></a> to create an iterator and use it to generate the target sequence:</p>\n<pre><code>from itertools import cycle\n\nnum_cycle = cycle([1, 2, 3])\ndf['New'] = [next(num_cycle) for num in range(len(df))]\n</code></pre>\n<hr />\n<pre><code>import pandas as pd \nimport itertools \n\ndf = pd.DataFrame(\n    data = [\n        (1000, 2000),\n        (1000, 2001),\n        (1000, 2002),\n        (1001, 2000),\n        (1001, 2001),\n        (1001, 2002),\n    ],\n    columns=['StoreNumber', 'Year']\n)\n\nnum_cycle = itertools.cycle([1, 2, 3])\ndf['New'] = [next(num_cycle) for num in range(len(df))]\n\nprint(df)\n</code></pre>\n<p>And the output will be</p>\n<pre><code>   StoreNumber   Year  New\n0         1000   2000    1\n1         1000   2001    2\n2         1000   2002    3\n3         1001   2000    1\n4         1001   2001    2\n5         1001   2002    3\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n<pre><code>   StoreNumber    Year  \n    1000          2000  \n    1000          2001  \n    1000          2002  \n    1001          2000  \n    1001          2001  \n    1001          2002  \n</code></pre>\n<p>I want to add a column so that my final dataframe looks like:</p>\n<pre><code>StoreNumber       Year   New\n    1000          2000    1\n    1000          2001    2\n    1000          2002    3\n    1001          2000    1\n    1001          2001    2\n    1001          2002    3 \n</code></pre>\n<p>I don't want the new row to depend upon StoreNumber which looks ovious in example. I want to start numbering with 1 and when I reach 3, then again start with 1.\nHow do I do it?</p>\n",
        "formatted_input": {
            "qid": 66075712,
            "link": "https://stackoverflow.com/questions/66075712/create-a-pandas-column-of-numbers-from-1-to-3-and-repeat-again",
            "question": {
                "title": "Create a pandas column of numbers from 1 to 3 and repeat again",
                "ques_desc": "I have a dataframe: I want to add a column so that my final dataframe looks like: I don't want the new row to depend upon StoreNumber which looks ovious in example. I want to start numbering with 1 and when I reach 3, then again start with 1. How do I do it? "
            },
            "io": [
                "   StoreNumber    Year  \n    1000          2000  \n    1000          2001  \n    1000          2002  \n    1001          2000  \n    1001          2001  \n    1001          2002  \n",
                "StoreNumber       Year   New\n    1000          2000    1\n    1000          2001    2\n    1000          2002    3\n    1001          2000    1\n    1001          2001    2\n    1001          2002    3 \n"
            ],
            "answer": {
                "ans_desc": "You can use to create an iterator and use it to generate the target sequence: And the output will be ",
                "code": [
                    "from itertools import cycle\n\nnum_cycle = cycle([1, 2, 3])\ndf['New'] = [next(num_cycle) for num in range(len(df))]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 15150930,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c2098c2d8a8066bf882cd0346bc40fdc?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dave",
            "link": "https://stackoverflow.com/users/15150930/dave"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 66075551,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1612605300,
        "creation_date": 1612604040,
        "question_id": 66075388,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66075388/create-df-columns-based-on-second-ddf",
        "title": "Create DF Columns Based on Second DDF",
        "body": "<p>I have 2 dataframes with different columns:</p>\n<pre><code>DF A -                   DF B - \nnumber | a  | b  | c  |||| a  | c  | d  | e  | f\n1      | 12 | 13 | 15 |||| 22 | 33 | 44 | 55 | 77\n</code></pre>\n<p>I would like to add the <em>missing columns</em> for the 2 dataframes - so each one will have <strong>each own columns + the other DFs columns (without column &quot;number&quot;)</strong>. And the new columns will have initial number for our choice (let's say <em>0</em>).<br />\nSo the final output:</p>\n<pre><code>    DF A -                    \n    number | a  | b  | c  | d  | e  | f \n    1      | 12 | 13 | 15 | 0  | 0  | 0 \n    DF B -\n    a  | b  | c  | d  | e  | f\n    22 | 0  | 33 | 44 | 55 | 77\n</code></pre>\n<p>What's the best way to achieve this result? I've got messed up with getting the columns and trying to create new ones.</p>\n<p>Thank!</p>\n",
        "answer_body": "<p>First, you need to create a superset of all the columns which are present in both the dataframes. This you can do using the below code.</p>\n<pre><code>all_columns = list(set(A.columns.to_list() + B.columns.to_list()))\n</code></pre>\n<p>Then for each dataframes, you need to check which columns are missing that you can do using the below code.</p>\n<pre><code>col_missing_from_A = [col for col in all_columns if col not in A.columns]\ncol_missing_from_B = [col for col in all_columns if col not in B.columns]\n</code></pre>\n<p>Then add the missing columns in both the dataframes</p>\n<pre><code>A[col_missing_from_A] = 0\nA[col_missing_from_B] = 0\n</code></pre>\n<p>Hope this solves your query!</p>\n",
        "question_body": "<p>I have 2 dataframes with different columns:</p>\n<pre><code>DF A -                   DF B - \nnumber | a  | b  | c  |||| a  | c  | d  | e  | f\n1      | 12 | 13 | 15 |||| 22 | 33 | 44 | 55 | 77\n</code></pre>\n<p>I would like to add the <em>missing columns</em> for the 2 dataframes - so each one will have <strong>each own columns + the other DFs columns (without column &quot;number&quot;)</strong>. And the new columns will have initial number for our choice (let's say <em>0</em>).<br />\nSo the final output:</p>\n<pre><code>    DF A -                    \n    number | a  | b  | c  | d  | e  | f \n    1      | 12 | 13 | 15 | 0  | 0  | 0 \n    DF B -\n    a  | b  | c  | d  | e  | f\n    22 | 0  | 33 | 44 | 55 | 77\n</code></pre>\n<p>What's the best way to achieve this result? I've got messed up with getting the columns and trying to create new ones.</p>\n<p>Thank!</p>\n",
        "formatted_input": {
            "qid": 66075388,
            "link": "https://stackoverflow.com/questions/66075388/create-df-columns-based-on-second-ddf",
            "question": {
                "title": "Create DF Columns Based on Second DDF",
                "ques_desc": "I have 2 dataframes with different columns: I would like to add the missing columns for the 2 dataframes - so each one will have each own columns + the other DFs columns (without column \"number\"). And the new columns will have initial number for our choice (let's say 0). So the final output: What's the best way to achieve this result? I've got messed up with getting the columns and trying to create new ones. Thank! "
            },
            "io": [
                "DF A -                   DF B - \nnumber | a  | b  | c  |||| a  | c  | d  | e  | f\n1      | 12 | 13 | 15 |||| 22 | 33 | 44 | 55 | 77\n",
                "    DF A -                    \n    number | a  | b  | c  | d  | e  | f \n    1      | 12 | 13 | 15 | 0  | 0  | 0 \n    DF B -\n    a  | b  | c  | d  | e  | f\n    22 | 0  | 33 | 44 | 55 | 77\n"
            ],
            "answer": {
                "ans_desc": "First, you need to create a superset of all the columns which are present in both the dataframes. This you can do using the below code. Then for each dataframes, you need to check which columns are missing that you can do using the below code. Then add the missing columns in both the dataframes Hope this solves your query! ",
                "code": [
                    "all_columns = list(set(A.columns.to_list() + B.columns.to_list()))\n",
                    "col_missing_from_A = [col for col in all_columns if col not in A.columns]\ncol_missing_from_B = [col for col in all_columns if col not in B.columns]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 15150930,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c2098c2d8a8066bf882cd0346bc40fdc?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dave",
            "link": "https://stackoverflow.com/users/15150930/dave"
        },
        "is_answered": true,
        "view_count": 381,
        "accepted_answer_id": 66070910,
        "answer_count": 5,
        "score": 8,
        "last_activity_date": 1612561934,
        "creation_date": 1612558915,
        "last_edit_date": 1612559299,
        "question_id": 66070517,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66070517/transpose-dataframe-based-on-column-list",
        "title": "Transpose dataframe based on column list",
        "body": "<p>I have a dataframe in the following structure:</p>\n<pre><code>cNames  | cValues   |  number  \n[a,b,c] | [1,2,3]   |  10      \n[a,b,d] | [55,66,77]|  20\n</code></pre>\n<p>I would like to transpose - <strong>create columns from the names in <em>cNames</em></strong>.<br />\nBut I can't manage to achieve this with <em>transpose</em> because I want a column for each value in the list.<br />\nThe needed output:</p>\n<pre><code>a   | b   | c   | d   |  number\n1   | 2   | 3   | NaN | 10\n55  | 66  | NaN | 77  | 20\n</code></pre>\n<p>How can I achieve this result?<br />\nThanks!</p>\n<p><em>The code to create the DF:</em></p>\n<pre><code>d = {'cNames': [['a','b','c'], ['a','b','d']], 'cValues': [[1,2,3], \n[55,66,77]], 'number': [10,20]}\ndf = pd.DataFrame(data=d)\n</code></pre>\n",
        "answer_body": "<p>One option is <code>concat</code>:</p>\n<pre><code>pd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n</code></pre>\n<p>Or a DataFrame construction:</p>\n<pre><code>pd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n</code></pre>\n<p>Output:</p>\n<pre><code>      a     b    c     d  number\n0   1.0   2.0  3.0   NaN      10\n1  55.0  66.0  NaN  77.0      20\n</code></pre>\n<hr />\n<p><strong>Update</strong> Performances sort by run time on sample data</p>\n<p><strong>DataFrame</strong></p>\n<pre><code>%%timeit\npd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n1.29 ms \u00b1 36.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p><strong>concat</strong>:</p>\n<pre><code>%%timeit\npd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n2.03 ms \u00b1 86.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) \n</code></pre>\n<p><strong>KJDII's new series</strong></p>\n<pre><code>%%timeit\ndf['series'] = df.apply(lambda x: dict(zip(x['cNames'], x['cValues'])), axis=1)\npd.concat([df['number'], df['series'].apply(pd.Series)], axis=1)\n\n2.09 ms \u00b1 65.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n<p><strong>Scott's apply(pd.Series.explode)</strong></p>\n<pre><code>%%timeit\ndf.apply(pd.Series.explode)\\\n  .set_index(['number', 'cNames'], append=True)['cValues']\\\n  .unstack()\\\n  .reset_index()\\\n  .drop('level_0', axis=1)\n\n4.9 ms \u00b1 135 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n<p><strong>wwnde's set_index.apply(explode)</strong></p>\n<pre><code>%%timeit\ng=df.set_index('number').apply(lambda x: x.explode()).reset_index()\ng['cValues']=g['cValues'].astype(int)\npd.pivot_table(g, index=[&quot;number&quot;],values=[&quot;cValues&quot;],columns=[&quot;cNames&quot;]).droplevel(0, axis=1).reset_index()\n\n7.27 ms \u00b1 162 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n<p><strong>Celius' double explode</strong></p>\n<pre><code>%%timeit\ndf1 = df.explode('cNames').explode('cValues')\ndf1['cValues'] = pd.to_numeric(df1['cValues'])\ndf1.pivot_table(columns='cNames',index='number',values='cValues')\n\n9.42 ms \u00b1 189 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe in the following structure:</p>\n<pre><code>cNames  | cValues   |  number  \n[a,b,c] | [1,2,3]   |  10      \n[a,b,d] | [55,66,77]|  20\n</code></pre>\n<p>I would like to transpose - <strong>create columns from the names in <em>cNames</em></strong>.<br />\nBut I can't manage to achieve this with <em>transpose</em> because I want a column for each value in the list.<br />\nThe needed output:</p>\n<pre><code>a   | b   | c   | d   |  number\n1   | 2   | 3   | NaN | 10\n55  | 66  | NaN | 77  | 20\n</code></pre>\n<p>How can I achieve this result?<br />\nThanks!</p>\n<p><em>The code to create the DF:</em></p>\n<pre><code>d = {'cNames': [['a','b','c'], ['a','b','d']], 'cValues': [[1,2,3], \n[55,66,77]], 'number': [10,20]}\ndf = pd.DataFrame(data=d)\n</code></pre>\n",
        "formatted_input": {
            "qid": 66070517,
            "link": "https://stackoverflow.com/questions/66070517/transpose-dataframe-based-on-column-list",
            "question": {
                "title": "Transpose dataframe based on column list",
                "ques_desc": "I have a dataframe in the following structure: I would like to transpose - create columns from the names in cNames. But I can't manage to achieve this with transpose because I want a column for each value in the list. The needed output: How can I achieve this result? Thanks! The code to create the DF: "
            },
            "io": [
                "cNames  | cValues   |  number  \n[a,b,c] | [1,2,3]   |  10      \n[a,b,d] | [55,66,77]|  20\n",
                "a   | b   | c   | d   |  number\n1   | 2   | 3   | NaN | 10\n55  | 66  | NaN | 77  | 20\n"
            ],
            "answer": {
                "ans_desc": "One option is : Or a DataFrame construction: Output: Update Performances sort by run time on sample data DataFrame concat: KJDII's new series Scott's apply(pd.Series.explode) wwnde's set_index.apply(explode) Celius' double explode ",
                "code": [
                    "pd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n",
                    "pd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n",
                    "%%timeit\npd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n1.29 ms \u00b1 36.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n",
                    "%%timeit\npd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n2.03 ms \u00b1 86.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) \n",
                    "%%timeit\ndf['series'] = df.apply(lambda x: dict(zip(x['cNames'], x['cValues'])), axis=1)\npd.concat([df['number'], df['series'].apply(pd.Series)], axis=1)\n\n2.09 ms \u00b1 65.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n",
                    "%%timeit\ndf.apply(pd.Series.explode)\\\n  .set_index(['number', 'cNames'], append=True)['cValues']\\\n  .unstack()\\\n  .reset_index()\\\n  .drop('level_0', axis=1)\n\n4.9 ms \u00b1 135 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n",
                    "%%timeit\ng=df.set_index('number').apply(lambda x: x.explode()).reset_index()\ng['cValues']=g['cValues'].astype(int)\npd.pivot_table(g, index=[\"number\"],values=[\"cValues\"],columns=[\"cNames\"]).droplevel(0, axis=1).reset_index()\n\n7.27 ms \u00b1 162 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n",
                    "%%timeit\ndf1 = df.explode('cNames').explode('cValues')\ndf1['cValues'] = pd.to_numeric(df1['cValues'])\ndf1.pivot_table(columns='cNames',index='number',values='cValues')\n\n9.42 ms \u00b1 189 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 15150930,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c2098c2d8a8066bf882cd0346bc40fdc?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dave",
            "link": "https://stackoverflow.com/users/15150930/dave"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 66062197,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1612524355,
        "creation_date": 1612516717,
        "question_id": 66060591,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66060591/join-two-pandas-dataframes-based-on-lists-columns",
        "title": "Join two pandas dataframes based on lists columns",
        "body": "<p>I have <strong>2 dataframes</strong> containing columns of lists.<br />\nI would like to <em>join</em> them based on <strong>2+ shared values</strong> on the lists. Example:</p>\n<pre><code>ColumnA ColumnB        | ColumnA ColumnB        \nid1     ['a','b','c']  | id3     ['a','b','c','x','y', 'z']\nid2     ['a','d,'e']   | \n</code></pre>\n<p>In this case we can see that <strong>id1 matches id3</strong> because there are 2+ shared values on the lists. So the output will be (columns name are not important and just for example):</p>\n<pre><code>    ColumnA1 ColumnB1     ColumnA2   ColumnB2        \n    id1      ['a','b','c']  id3     ['a','b','c','x','y', 'z']\n    \n</code></pre>\n<p>How can I achieve this result? I've tried to iterate each row in dataframe #1 but it doesn't seem a good idea.<br />\nThank you!</p>\n",
        "answer_body": "<p>If you are using <strong>pandas 1.2.0 or newer</strong> (released on December 26, 2020), cartesian product (cross joint) can be simplified as follows:</p>\n<pre><code>    df = df1.merge(df2, how='cross')         # simplified cross joint for pandas &gt;= 1.2.0\n</code></pre>\n<p>Also, <strong>if system performance (execution time) is a concern</strong> to you, it is advisable to use <code>list(map... </code> instead of the slower <code>apply(... axis=1)</code></p>\n<p>Using <code>apply(... axis=1)</code>:</p>\n<pre><code>%%timeit\ndf['overlap'] = df.apply(lambda x: \n                         len(set(x['ColumnB1']).intersection(\n                             set(x['ColumnB2']))), axis=1)\n\n\n800 \u00b5s \u00b1 59.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p>while using <code>list(map(...</code>:</p>\n<pre><code>%%timeit\ndf['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n217 \u00b5s \u00b1 25.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p>Notice that <strong>using <code>list(map...</code> is 3x times faster!</strong></p>\n<p>Whole set of codes for your reference:</p>\n<pre><code>    data = {'ColumnA1': ['id1', 'id2'], 'ColumnB1': [['a', 'b', 'c'], ['a', 'd', 'e']]}\n    df1 = pd.DataFrame(data)\n\n    data = {'ColumnA2': ['id3', 'id4'], 'ColumnB2': [['a','b','c','x','y', 'z'], ['d','e','f','p','q', 'r']]}\n    df2 = pd.DataFrame(data)\n\n    df = df1.merge(df2, how='cross')             # for pandas version &gt;= 1.2.0\n\n    df['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n    df = df[df['overlap'] &gt;= 2]\n    print (df)\n</code></pre>\n",
        "question_body": "<p>I have <strong>2 dataframes</strong> containing columns of lists.<br />\nI would like to <em>join</em> them based on <strong>2+ shared values</strong> on the lists. Example:</p>\n<pre><code>ColumnA ColumnB        | ColumnA ColumnB        \nid1     ['a','b','c']  | id3     ['a','b','c','x','y', 'z']\nid2     ['a','d,'e']   | \n</code></pre>\n<p>In this case we can see that <strong>id1 matches id3</strong> because there are 2+ shared values on the lists. So the output will be (columns name are not important and just for example):</p>\n<pre><code>    ColumnA1 ColumnB1     ColumnA2   ColumnB2        \n    id1      ['a','b','c']  id3     ['a','b','c','x','y', 'z']\n    \n</code></pre>\n<p>How can I achieve this result? I've tried to iterate each row in dataframe #1 but it doesn't seem a good idea.<br />\nThank you!</p>\n",
        "formatted_input": {
            "qid": 66060591,
            "link": "https://stackoverflow.com/questions/66060591/join-two-pandas-dataframes-based-on-lists-columns",
            "question": {
                "title": "Join two pandas dataframes based on lists columns",
                "ques_desc": "I have 2 dataframes containing columns of lists. I would like to join them based on 2+ shared values on the lists. Example: In this case we can see that id1 matches id3 because there are 2+ shared values on the lists. So the output will be (columns name are not important and just for example): How can I achieve this result? I've tried to iterate each row in dataframe #1 but it doesn't seem a good idea. Thank you! "
            },
            "io": [
                "ColumnA ColumnB        | ColumnA ColumnB        \nid1     ['a','b','c']  | id3     ['a','b','c','x','y', 'z']\nid2     ['a','d,'e']   | \n",
                "    ColumnA1 ColumnB1     ColumnA2   ColumnB2        \n    id1      ['a','b','c']  id3     ['a','b','c','x','y', 'z']\n    \n"
            ],
            "answer": {
                "ans_desc": "If you are using pandas 1.2.0 or newer (released on December 26, 2020), cartesian product (cross joint) can be simplified as follows: Also, if system performance (execution time) is a concern to you, it is advisable to use instead of the slower Using : while using : Notice that using is 3x times faster! Whole set of codes for your reference: ",
                "code": [
                    "    df = df1.merge(df2, how='cross')         # simplified cross joint for pandas >= 1.2.0\n",
                    "%%timeit\ndf['overlap'] = df.apply(lambda x: \n                         len(set(x['ColumnB1']).intersection(\n                             set(x['ColumnB2']))), axis=1)\n\n\n800 \u00b5s \u00b1 59.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n",
                    "%%timeit\ndf['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n217 \u00b5s \u00b1 25.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n",
                    "    data = {'ColumnA1': ['id1', 'id2'], 'ColumnB1': [['a', 'b', 'c'], ['a', 'd', 'e']]}\n    df1 = pd.DataFrame(data)\n\n    data = {'ColumnA2': ['id3', 'id4'], 'ColumnB2': [['a','b','c','x','y', 'z'], ['d','e','f','p','q', 'r']]}\n    df2 = pd.DataFrame(data)\n\n    df = df1.merge(df2, how='cross')             # for pandas version >= 1.2.0\n\n    df['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n    df = df[df['overlap'] >= 2]\n    print (df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 1,
            "user_id": 15139949,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2849b613b8b946527edbbd12a4eabbd9?s=128&d=identicon&r=PG&f=1",
            "display_name": "g_strag",
            "link": "https://stackoverflow.com/users/15139949/g-strag"
        },
        "is_answered": true,
        "view_count": 27,
        "closed_date": 1612393813,
        "accepted_answer_id": 66034440,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1612381608,
        "creation_date": 1612380076,
        "question_id": 66034318,
        "link": "https://stackoverflow.com/questions/66034318/assigning-column-to-larger-dataframe-in-specific-positions",
        "closed_reason": "Needs more focus",
        "title": "Assigning column to larger DataFrame in specific positions",
        "body": "<p>I have two lists.</p>\n<pre><code>A = [0, 1, 0, 0, 1, 1, 0, 1]\nB = [2, 3, 4, 5]\n</code></pre>\n<p>I want to create C:</p>\n<pre><code>C = [NaN, 2, NaN, NaN, 3, 4, NaN, 5] \n</code></pre>\n<p>Basically, where in <code>A</code> there is a <code>1</code> I want to have a value of <code>B</code>, where there is a <code>0</code>, a <code>NaN</code>.\nIn reality, <code>A</code> contains around 10k elements, <code>B</code> around 40k and I have many of them. I am working with a pandas.DataFrame (each &quot;array&quot; is a column of two different dataframes, I have to &quot;fit&quot; <code>B</code> in <code>A</code> by transforming it into <code>C</code>).\nI have done it with a for loop, but I am wondering how to do it in a better way. Thank you.</p>\n",
        "answer_body": "<p>You can use <code>list comprehension</code> and check with <code>if</code> condition if it is <code>0</code> or not. If it is <code>0</code> then provide <code>None</code>, else pop B from left side.</p>\n<pre class=\"lang-py prettyprint-override\"><code>[None if i==0 else B.pop(0) for i in A]\n</code></pre>\n<p>output</p>\n<pre><code>[None, 2, None, None, 3, 4, None, 5]\n</code></pre>\n",
        "question_body": "<p>I have two lists.</p>\n<pre><code>A = [0, 1, 0, 0, 1, 1, 0, 1]\nB = [2, 3, 4, 5]\n</code></pre>\n<p>I want to create C:</p>\n<pre><code>C = [NaN, 2, NaN, NaN, 3, 4, NaN, 5] \n</code></pre>\n<p>Basically, where in <code>A</code> there is a <code>1</code> I want to have a value of <code>B</code>, where there is a <code>0</code>, a <code>NaN</code>.\nIn reality, <code>A</code> contains around 10k elements, <code>B</code> around 40k and I have many of them. I am working with a pandas.DataFrame (each &quot;array&quot; is a column of two different dataframes, I have to &quot;fit&quot; <code>B</code> in <code>A</code> by transforming it into <code>C</code>).\nI have done it with a for loop, but I am wondering how to do it in a better way. Thank you.</p>\n",
        "formatted_input": {
            "qid": 66034318,
            "link": "https://stackoverflow.com/questions/66034318/assigning-column-to-larger-dataframe-in-specific-positions",
            "question": {
                "title": "Assigning column to larger DataFrame in specific positions",
                "ques_desc": "I have two lists. I want to create C: Basically, where in there is a I want to have a value of , where there is a , a . In reality, contains around 10k elements, around 40k and I have many of them. I am working with a pandas.DataFrame (each \"array\" is a column of two different dataframes, I have to \"fit\" in by transforming it into ). I have done it with a for loop, but I am wondering how to do it in a better way. Thank you. "
            },
            "io": [
                "A = [0, 1, 0, 0, 1, 1, 0, 1]\nB = [2, 3, 4, 5]\n",
                "C = [NaN, 2, NaN, NaN, 3, 4, NaN, 5] \n"
            ],
            "answer": {
                "ans_desc": "You can use and check with condition if it is or not. If it is then provide , else pop B from left side. output ",
                "code": [
                    "[None if i==0 else B.pop(0) for i in A]\n",
                    "[None, 2, None, None, 3, 4, None, 5]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "jupyter-notebook",
            "google-colaboratory"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 15123783,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GguWir9Hc_mBju6EIwAlo1Ku4Xb_t-1SI0EMGsA=k-s128",
            "display_name": "ngakan alit",
            "link": "https://stackoverflow.com/users/15123783/ngakan-alit"
        },
        "is_answered": true,
        "view_count": 50,
        "accepted_answer_id": 65995575,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1612194312,
        "creation_date": 1612192728,
        "question_id": 65995268,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65995268/pandas-dataframe-average-non-0-value",
        "title": "Pandas Dataframe, average non 0 value",
        "body": "<p>I have the following Pandas Dataframe 'df':</p>\n<pre><code>a1  a2  a3  b1\n0   0   0   1\n1   2   0   2\n3   0   0   3\n2   4   0   4\n</code></pre>\n<p>How do I get the average value of &quot;a&quot; for from a1, a2, a3, ignoring the 0 value?</p>\n<pre><code>a1  a2  a3  b1  avg(a)\n0   0   0   1   0\n1   2   0   2   1.5\n3   0   0   3   3.0\n2   4   0   4   3.0\n</code></pre>\n<p>I'm stuck with manual approach where I convert the value &gt; 0 to 1</p>\n",
        "answer_body": "<p>You can <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html\" rel=\"nofollow noreferrer\"><code>.filter</code></a> the <code>a</code> like columns, then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>.mask</code></a> the zeros in these columns and take <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html\" rel=\"nofollow noreferrer\"><code>.mean</code></a> along <code>axis=1</code>:</p>\n<pre><code>a = df.filter(like='a')\ndf['avg'] = a.mask(a.eq(0)).mean(1).fillna(0)\n\n# OR df['avg'] = a[a &gt; 0].mean(1).fillna(0)\n</code></pre>\n<hr />\n<pre><code>   a1  a2  a3  b1  avg\n0   0   0   0   1  0.0\n1   1   2   0   2  1.5\n2   3   0   0   3  3.0\n3   2   4   0   4  3.0\n</code></pre>\n",
        "question_body": "<p>I have the following Pandas Dataframe 'df':</p>\n<pre><code>a1  a2  a3  b1\n0   0   0   1\n1   2   0   2\n3   0   0   3\n2   4   0   4\n</code></pre>\n<p>How do I get the average value of &quot;a&quot; for from a1, a2, a3, ignoring the 0 value?</p>\n<pre><code>a1  a2  a3  b1  avg(a)\n0   0   0   1   0\n1   2   0   2   1.5\n3   0   0   3   3.0\n2   4   0   4   3.0\n</code></pre>\n<p>I'm stuck with manual approach where I convert the value &gt; 0 to 1</p>\n",
        "formatted_input": {
            "qid": 65995268,
            "link": "https://stackoverflow.com/questions/65995268/pandas-dataframe-average-non-0-value",
            "question": {
                "title": "Pandas Dataframe, average non 0 value",
                "ques_desc": "I have the following Pandas Dataframe 'df': How do I get the average value of \"a\" for from a1, a2, a3, ignoring the 0 value? I'm stuck with manual approach where I convert the value > 0 to 1 "
            },
            "io": [
                "a1  a2  a3  b1\n0   0   0   1\n1   2   0   2\n3   0   0   3\n2   4   0   4\n",
                "a1  a2  a3  b1  avg(a)\n0   0   0   1   0\n1   2   0   2   1.5\n3   0   0   3   3.0\n2   4   0   4   3.0\n"
            ],
            "answer": {
                "ans_desc": "You can the like columns, then the zeros in these columns and take along : ",
                "code": [
                    "a = df.filter(like='a')\ndf['avg'] = a.mask(a.eq(0)).mean(1).fillna(0)\n\n# OR df['avg'] = a[a > 0].mean(1).fillna(0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 184,
            "user_id": 12270273,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/278f74eb98e458d2022132327d415ad8?s=128&d=identicon&r=PG&f=1",
            "display_name": "SpindriftSeltzer",
            "link": "https://stackoverflow.com/users/12270273/spindriftseltzer"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 65923311,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1611764981,
        "creation_date": 1611257808,
        "question_id": 65834585,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65834585/conditional-row-shift-in-pandas",
        "title": "Conditional Row shift in Pandas",
        "body": "<p>I'm attempting to shift a row based on whether or not another column is not null. There's inconsistent spacing in the Description column so I can't do a .shift()</p>\n<p>Here's the original data</p>\n<pre><code>Permit Number    A      Description\n1234            NaN    NaN\nNaN             NaN    NaN\nNaN             NaN    foo\n3456            NaN    NaN\nNaN             NaN    bar\n</code></pre>\n<p>And this is what I want my result to be</p>\n<pre><code>Permit Number    A      Description\n1234            NaN    foo\nNaN             NaN    NaN\nNaN             NaN    NaN\n3456            NaN    bar\nNaN             NaN    NaN\n</code></pre>\n<p>Here's the code that I used from <a href=\"https://stackoverflow.com/questions/45381840/align-data-in-one-column-with-another-row-based-on-the-last-time-some-condition\">Align data in one column with another row, based on the last time some condition was true</a></p>\n<pre><code>mask = df['Description'].notnull()\nfmask = (df['Permit Number'].notnull() &amp; df['Description'].isnull())\ndf.assign(Description=df.groupby(mask[::-1].cumsum())['Description'].transform(lambda x: x.iloc[-1]).where(fmask))\n</code></pre>\n<p>However when I run it, no errors and no changes in the dataframe.</p>\n",
        "answer_body": "<p>FYI for anyone who sees this that might have a string in their column, this solutions works too</p>\n<pre><code>mask = df['Description'].notnull()\nfmask = (df['Permit Number'].notnull() &amp; df['Description'].isnull())\ndf = df.assign(Description=df.groupby(mask[::-1].cumsum())['Description'].transform(lambda x: x.iloc[-1]).where(fmask))\n</code></pre>\n<p>I had a simple error of not &quot;writing&quot; the newly grouped dataframe back to df</p>\n",
        "question_body": "<p>I'm attempting to shift a row based on whether or not another column is not null. There's inconsistent spacing in the Description column so I can't do a .shift()</p>\n<p>Here's the original data</p>\n<pre><code>Permit Number    A      Description\n1234            NaN    NaN\nNaN             NaN    NaN\nNaN             NaN    foo\n3456            NaN    NaN\nNaN             NaN    bar\n</code></pre>\n<p>And this is what I want my result to be</p>\n<pre><code>Permit Number    A      Description\n1234            NaN    foo\nNaN             NaN    NaN\nNaN             NaN    NaN\n3456            NaN    bar\nNaN             NaN    NaN\n</code></pre>\n<p>Here's the code that I used from <a href=\"https://stackoverflow.com/questions/45381840/align-data-in-one-column-with-another-row-based-on-the-last-time-some-condition\">Align data in one column with another row, based on the last time some condition was true</a></p>\n<pre><code>mask = df['Description'].notnull()\nfmask = (df['Permit Number'].notnull() &amp; df['Description'].isnull())\ndf.assign(Description=df.groupby(mask[::-1].cumsum())['Description'].transform(lambda x: x.iloc[-1]).where(fmask))\n</code></pre>\n<p>However when I run it, no errors and no changes in the dataframe.</p>\n",
        "formatted_input": {
            "qid": 65834585,
            "link": "https://stackoverflow.com/questions/65834585/conditional-row-shift-in-pandas",
            "question": {
                "title": "Conditional Row shift in Pandas",
                "ques_desc": "I'm attempting to shift a row based on whether or not another column is not null. There's inconsistent spacing in the Description column so I can't do a .shift() Here's the original data And this is what I want my result to be Here's the code that I used from Align data in one column with another row, based on the last time some condition was true However when I run it, no errors and no changes in the dataframe. "
            },
            "io": [
                "Permit Number    A      Description\n1234            NaN    NaN\nNaN             NaN    NaN\nNaN             NaN    foo\n3456            NaN    NaN\nNaN             NaN    bar\n",
                "Permit Number    A      Description\n1234            NaN    foo\nNaN             NaN    NaN\nNaN             NaN    NaN\n3456            NaN    bar\nNaN             NaN    NaN\n"
            ],
            "answer": {
                "ans_desc": "FYI for anyone who sees this that might have a string in their column, this solutions works too I had a simple error of not \"writing\" the newly grouped dataframe back to df ",
                "code": [
                    "mask = df['Description'].notnull()\nfmask = (df['Permit Number'].notnull() & df['Description'].isnull())\ndf = df.assign(Description=df.groupby(mask[::-1].cumsum())['Description'].transform(lambda x: x.iloc[-1]).where(fmask))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 15090461,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f6dfaf5b1f43dce0bbcb44956b171250?s=128&d=identicon&r=PG&f=1",
            "display_name": "BewareTheHare",
            "link": "https://stackoverflow.com/users/15090461/bewarethehare"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 65917467,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1611744964,
        "creation_date": 1611743688,
        "question_id": 65917323,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65917323/creating-new-columns-within-a-dataframe-based-on-the-latest-value-from-previous",
        "title": "Creating new columns within a dataframe, based on the latest value from previous columns",
        "body": "<p>I've just completed a beginner's course in python, so please bear with me if the code below doesn't make sense or my issue is because of some rookie mistake.</p>\n<p>I've been trying to put the learning to use by working with college production of NFL players, with a view to understanding which statistics can be predictive or at least correlate to NFL production. It turns out that there's a lot of data out there so I have about 200 columns of data for 600 odd prospects from the last 20 years (just for running backs so far). However, one of the problems with this data is that each stat is only provided by the age the prospect was in that season giving me something like this:</p>\n<pre><code>    GP 18  GP 19  GP 20  GP 21  GP 22  GP 23\n50   14.0   13.0   14.0    NaN    NaN    NaN\n51   14.0   14.0   14.0    NaN    NaN    NaN\n53   13.0   12.0   11.0    NaN    NaN    NaN\n56   10.0   13.0    9.0   13.0    NaN    NaN\n59   10.0   13.0   15.0    NaN    NaN    NaN\n61    NaN    NaN   11.0   11.0    NaN    NaN\n66    NaN   12.0   13.0   12.0    2.0   13.0\n</code></pre>\n<p>What I want to do at the moment is to be able to take the last year of college production and put it into a new column (for 17 different statistics). I've therefore defined the following function:</p>\n<pre><code>def grab_latest(row):\n    if row[f'{column} 23'] != 'NaN':\n        return row[f'{column} 23']\n    elif row[f'{column} 22'] != 'NaN':\n        return row[f'{column} 22']\n    elif row[f'{column} 21'] != 'NaN':\n        return row[f'{column} 21']\n    elif row[f'{column} 20'] != 'NaN':\n        return row[f'{column} 20']\n    elif row[f'{column} 19'] != 'NaN':\n        return row[f'{column} 19']\n    elif row[f'{column} 18'] != 'NaN':\n        return row[f'{column} 18']\n</code></pre>\n<p>Which I think should go backwards through the columns until I find a value which isn't NaN, and then take that value as the output. I've then defined the columns via a list:</p>\n<pre><code>stats_list = ['% Yd Rec', 'PPR PPG', 'PPR', 'GP', 'Rush Att', 'Rush Yd', 'Rush TD', 'MS Rush Att', 'MS Rush Yd',\n              'MS Rush TD', 'REC', 'REC Yd', 'REC TD', 'MS REC', 'MS Rec Yd', 'MS REC TD', 'TOT DOM + TD MS']\n</code></pre>\n<p>and have then run the function through a for loop based on this list:</p>\n<pre><code>for column in stats_list:\n    RB_df[f'{column} Last'] = RB_df.apply(lambda row: grab_latest(row), axis=1)\n</code></pre>\n<p>The result I'm getting back is a slightly bizarre one - the for loop appears to work, as all the new columns I'm expecting are created, however they are only populated with data where the player had an age 23 season. The remainder of indexes are filled with 'NaN':</p>\n<pre><code>    GP Last\n50      NaN\n51      NaN\n53      NaN\n56      NaN\n59      NaN\n61      NaN\n66     13.0\n</code></pre>\n<p>This suggests to me that the first 'if' statement in my function is working fine, but that all of the 'elif' statements aren't triggering and I can't work out why. I'm wondering whether it's because I need to be more explicit about why they would trigger, rather than just relying on a logical test of 'if the column is not, not equal to NaN, go to the next one', or if I'm misunderstanding the elif aspect all together. I've put the whole segment of code in also, just because when I've run into issues so far the problem has often not been where I originally thought.</p>\n<p>By all means tell me if you think I've gone about this in a weird way - this just seemed like a logical approach to the problem but open to other ways of getting the desired result.</p>\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html\" rel=\"nofollow noreferrer\"><code>DataFrame.filter</code></a> with regex parameter with <code>^</code> for start of strings, then forward filling missing values and last select last value by position:</p>\n<pre><code>#for test one value in list\nstats_list = ['GP']\nfor column in stats_list:\n    df[f'{column} Last'] = df.filter(regex=rf'^{column}').ffill(axis=1).iloc[:, -1]\n\nprint (df)\n    GP 18  GP 19  GP 20  GP 21  GP 22  GP 23  GP Last\n50   14.0   13.0   14.0    NaN    NaN    NaN     14.0\n51   14.0   14.0   14.0    NaN    NaN    NaN     14.0\n53   13.0   12.0   11.0    NaN    NaN    NaN     11.0\n56   10.0   13.0    9.0   13.0    NaN    NaN     13.0\n59   10.0   13.0   15.0    NaN    NaN    NaN     15.0\n61    NaN    NaN   11.0   11.0    NaN    NaN     11.0\n66    NaN   12.0   13.0   12.0    2.0   13.0     13.0\n</code></pre>\n<p>Another alternative solution with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.startswith.html\" rel=\"nofollow noreferrer\"><code>Series.str.startswith</code></a> for select columns:</p>\n<pre><code>stats_list = ['GP']\nfor column in stats_list:\n    mask = df.columns.str.startswith(column)\n    df[f'{column} Last'] = df.loc[:, mask].ffill(axis=1).iloc[:, -1]\n</code></pre>\n",
        "question_body": "<p>I've just completed a beginner's course in python, so please bear with me if the code below doesn't make sense or my issue is because of some rookie mistake.</p>\n<p>I've been trying to put the learning to use by working with college production of NFL players, with a view to understanding which statistics can be predictive or at least correlate to NFL production. It turns out that there's a lot of data out there so I have about 200 columns of data for 600 odd prospects from the last 20 years (just for running backs so far). However, one of the problems with this data is that each stat is only provided by the age the prospect was in that season giving me something like this:</p>\n<pre><code>    GP 18  GP 19  GP 20  GP 21  GP 22  GP 23\n50   14.0   13.0   14.0    NaN    NaN    NaN\n51   14.0   14.0   14.0    NaN    NaN    NaN\n53   13.0   12.0   11.0    NaN    NaN    NaN\n56   10.0   13.0    9.0   13.0    NaN    NaN\n59   10.0   13.0   15.0    NaN    NaN    NaN\n61    NaN    NaN   11.0   11.0    NaN    NaN\n66    NaN   12.0   13.0   12.0    2.0   13.0\n</code></pre>\n<p>What I want to do at the moment is to be able to take the last year of college production and put it into a new column (for 17 different statistics). I've therefore defined the following function:</p>\n<pre><code>def grab_latest(row):\n    if row[f'{column} 23'] != 'NaN':\n        return row[f'{column} 23']\n    elif row[f'{column} 22'] != 'NaN':\n        return row[f'{column} 22']\n    elif row[f'{column} 21'] != 'NaN':\n        return row[f'{column} 21']\n    elif row[f'{column} 20'] != 'NaN':\n        return row[f'{column} 20']\n    elif row[f'{column} 19'] != 'NaN':\n        return row[f'{column} 19']\n    elif row[f'{column} 18'] != 'NaN':\n        return row[f'{column} 18']\n</code></pre>\n<p>Which I think should go backwards through the columns until I find a value which isn't NaN, and then take that value as the output. I've then defined the columns via a list:</p>\n<pre><code>stats_list = ['% Yd Rec', 'PPR PPG', 'PPR', 'GP', 'Rush Att', 'Rush Yd', 'Rush TD', 'MS Rush Att', 'MS Rush Yd',\n              'MS Rush TD', 'REC', 'REC Yd', 'REC TD', 'MS REC', 'MS Rec Yd', 'MS REC TD', 'TOT DOM + TD MS']\n</code></pre>\n<p>and have then run the function through a for loop based on this list:</p>\n<pre><code>for column in stats_list:\n    RB_df[f'{column} Last'] = RB_df.apply(lambda row: grab_latest(row), axis=1)\n</code></pre>\n<p>The result I'm getting back is a slightly bizarre one - the for loop appears to work, as all the new columns I'm expecting are created, however they are only populated with data where the player had an age 23 season. The remainder of indexes are filled with 'NaN':</p>\n<pre><code>    GP Last\n50      NaN\n51      NaN\n53      NaN\n56      NaN\n59      NaN\n61      NaN\n66     13.0\n</code></pre>\n<p>This suggests to me that the first 'if' statement in my function is working fine, but that all of the 'elif' statements aren't triggering and I can't work out why. I'm wondering whether it's because I need to be more explicit about why they would trigger, rather than just relying on a logical test of 'if the column is not, not equal to NaN, go to the next one', or if I'm misunderstanding the elif aspect all together. I've put the whole segment of code in also, just because when I've run into issues so far the problem has often not been where I originally thought.</p>\n<p>By all means tell me if you think I've gone about this in a weird way - this just seemed like a logical approach to the problem but open to other ways of getting the desired result.</p>\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 65917323,
            "link": "https://stackoverflow.com/questions/65917323/creating-new-columns-within-a-dataframe-based-on-the-latest-value-from-previous",
            "question": {
                "title": "Creating new columns within a dataframe, based on the latest value from previous columns",
                "ques_desc": "I've just completed a beginner's course in python, so please bear with me if the code below doesn't make sense or my issue is because of some rookie mistake. I've been trying to put the learning to use by working with college production of NFL players, with a view to understanding which statistics can be predictive or at least correlate to NFL production. It turns out that there's a lot of data out there so I have about 200 columns of data for 600 odd prospects from the last 20 years (just for running backs so far). However, one of the problems with this data is that each stat is only provided by the age the prospect was in that season giving me something like this: What I want to do at the moment is to be able to take the last year of college production and put it into a new column (for 17 different statistics). I've therefore defined the following function: Which I think should go backwards through the columns until I find a value which isn't NaN, and then take that value as the output. I've then defined the columns via a list: and have then run the function through a for loop based on this list: The result I'm getting back is a slightly bizarre one - the for loop appears to work, as all the new columns I'm expecting are created, however they are only populated with data where the player had an age 23 season. The remainder of indexes are filled with 'NaN': This suggests to me that the first 'if' statement in my function is working fine, but that all of the 'elif' statements aren't triggering and I can't work out why. I'm wondering whether it's because I need to be more explicit about why they would trigger, rather than just relying on a logical test of 'if the column is not, not equal to NaN, go to the next one', or if I'm misunderstanding the elif aspect all together. I've put the whole segment of code in also, just because when I've run into issues so far the problem has often not been where I originally thought. By all means tell me if you think I've gone about this in a weird way - this just seemed like a logical approach to the problem but open to other ways of getting the desired result. Thanks in advance! "
            },
            "io": [
                "    GP 18  GP 19  GP 20  GP 21  GP 22  GP 23\n50   14.0   13.0   14.0    NaN    NaN    NaN\n51   14.0   14.0   14.0    NaN    NaN    NaN\n53   13.0   12.0   11.0    NaN    NaN    NaN\n56   10.0   13.0    9.0   13.0    NaN    NaN\n59   10.0   13.0   15.0    NaN    NaN    NaN\n61    NaN    NaN   11.0   11.0    NaN    NaN\n66    NaN   12.0   13.0   12.0    2.0   13.0\n",
                "    GP Last\n50      NaN\n51      NaN\n53      NaN\n56      NaN\n59      NaN\n61      NaN\n66     13.0\n"
            ],
            "answer": {
                "ans_desc": "Use with regex parameter with for start of strings, then forward filling missing values and last select last value by position: Another alternative solution with for select columns: ",
                "code": [
                    "stats_list = ['GP']\nfor column in stats_list:\n    mask = df.columns.str.startswith(column)\n    df[f'{column} Last'] = df.loc[:, mask].ffill(axis=1).iloc[:, -1]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "multi-index"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 15085575,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-1iJ4_QEpV_s/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucm0Svi_DdQW5vS9j7-_zFDiDiB5jQ/s96-c/photo.jpg?sz=128",
            "display_name": "user987654",
            "link": "https://stackoverflow.com/users/15085575/user987654"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 65905793,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1611680563,
        "creation_date": 1611680211,
        "question_id": 65905694,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65905694/pandas-multiindex-assign-all-elements-in-first-level",
        "title": "pandas.MultiIndex: assign all elements in first level",
        "body": "<p>I have a dataframe with a multiindex, as per the following example:</p>\n<pre><code>dates = pandas.date_range(datetime.date(2020,1,1), datetime.date(2020,1,4))\ncolumns = ['a', 'b', 'c']\nindex = pandas.MultiIndex.from_product([dates,columns])\npanel = pandas.DataFrame(index=index, columns=columns)\n</code></pre>\n<p>This gives me a dataframe like this:</p>\n<pre><code>                a   b   c\n2020-01-01  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-02  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-03  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-04  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n</code></pre>\n<p>I have another 2-dimensional dataframe, as follows:</p>\n<pre><code>df = pandas.DataFrame(index=dates, columns=columns, data=numpy.random.rand(len(dates), len(columns)))\n</code></pre>\n<p>Resulting in the following:</p>\n<pre><code>            a           b           c\n2020-01-01  0.540867    0.426181    0.220182\n2020-01-02  0.864340    0.432873    0.487878\n2020-01-03  0.017099    0.181050    0.373139\n2020-01-04  0.764557    0.097839    0.499788\n</code></pre>\n<p>I would like to assign to the <code>[a,a]</code> cell, across all dates, and the <code>[a,b]</code> cell, across all dates etc.</p>\n<p>Something akin to the following:</p>\n<pre><code>for i in df.columns:\n    for j in df.columns:\n        panel.xs(i, level=1).loc[j] = df[i] * df[j]\n</code></pre>\n<p>Of course this doesn't work, because I am attempting to set a value on a copy of a slice</p>\n<blockquote>\n<p><code>SettingWithCopyWarning</code>:\nA value is trying to be set on a copy of a slice from a <code>DataFrame</code></p>\n</blockquote>\n<p>I tried several variations:</p>\n<pre><code>panel.loc[:,'a']         # selects all rows, and column a\npanel.loc[(:, 'a'), 'a'] # invalid syntax\netc...\n</code></pre>\n<p>How can I select index level 1 (eg: row 'a'), column 'a', across <strong>all</strong> index level 0 - and be able to <strong>set</strong> the values?</p>\n",
        "answer_body": "<p>Try broadcasing on the values:</p>\n<pre><code>a = df.to_numpy()\n\npanel = pd.DataFrame((a[...,None] * a[:,None,:]).reshape(-1, df.shape[1]), \n                     index=panel.index, columns=panel.columns)\n</code></pre>\n<p>Output:</p>\n<pre><code>                     a         b         c\n2020-01-01 a  0.292537  0.230507  0.119089\n           b  0.230507  0.181630  0.093837\n           c  0.119089  0.093837  0.048480\n2020-01-02 a  0.747084  0.374149  0.421692\n           b  0.374149  0.187379  0.211189\n           c  0.421692  0.211189  0.238025\n2020-01-03 a  0.000292  0.003096  0.006380\n           b  0.003096  0.032779  0.067557\n           c  0.006380  0.067557  0.139233\n2020-01-04 a  0.584547  0.074803  0.382116\n           b  0.074803  0.009572  0.048899\n           c  0.382116  0.048899  0.249788\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with a multiindex, as per the following example:</p>\n<pre><code>dates = pandas.date_range(datetime.date(2020,1,1), datetime.date(2020,1,4))\ncolumns = ['a', 'b', 'c']\nindex = pandas.MultiIndex.from_product([dates,columns])\npanel = pandas.DataFrame(index=index, columns=columns)\n</code></pre>\n<p>This gives me a dataframe like this:</p>\n<pre><code>                a   b   c\n2020-01-01  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-02  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-03  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-04  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n</code></pre>\n<p>I have another 2-dimensional dataframe, as follows:</p>\n<pre><code>df = pandas.DataFrame(index=dates, columns=columns, data=numpy.random.rand(len(dates), len(columns)))\n</code></pre>\n<p>Resulting in the following:</p>\n<pre><code>            a           b           c\n2020-01-01  0.540867    0.426181    0.220182\n2020-01-02  0.864340    0.432873    0.487878\n2020-01-03  0.017099    0.181050    0.373139\n2020-01-04  0.764557    0.097839    0.499788\n</code></pre>\n<p>I would like to assign to the <code>[a,a]</code> cell, across all dates, and the <code>[a,b]</code> cell, across all dates etc.</p>\n<p>Something akin to the following:</p>\n<pre><code>for i in df.columns:\n    for j in df.columns:\n        panel.xs(i, level=1).loc[j] = df[i] * df[j]\n</code></pre>\n<p>Of course this doesn't work, because I am attempting to set a value on a copy of a slice</p>\n<blockquote>\n<p><code>SettingWithCopyWarning</code>:\nA value is trying to be set on a copy of a slice from a <code>DataFrame</code></p>\n</blockquote>\n<p>I tried several variations:</p>\n<pre><code>panel.loc[:,'a']         # selects all rows, and column a\npanel.loc[(:, 'a'), 'a'] # invalid syntax\netc...\n</code></pre>\n<p>How can I select index level 1 (eg: row 'a'), column 'a', across <strong>all</strong> index level 0 - and be able to <strong>set</strong> the values?</p>\n",
        "formatted_input": {
            "qid": 65905694,
            "link": "https://stackoverflow.com/questions/65905694/pandas-multiindex-assign-all-elements-in-first-level",
            "question": {
                "title": "pandas.MultiIndex: assign all elements in first level",
                "ques_desc": "I have a dataframe with a multiindex, as per the following example: This gives me a dataframe like this: I have another 2-dimensional dataframe, as follows: Resulting in the following: I would like to assign to the cell, across all dates, and the cell, across all dates etc. Something akin to the following: Of course this doesn't work, because I am attempting to set a value on a copy of a slice : A value is trying to be set on a copy of a slice from a I tried several variations: How can I select index level 1 (eg: row 'a'), column 'a', across all index level 0 - and be able to set the values? "
            },
            "io": [
                "                a   b   c\n2020-01-01  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-02  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-03  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n2020-01-04  a   NaN NaN NaN\n            b   NaN NaN NaN\n            c   NaN NaN NaN\n",
                "            a           b           c\n2020-01-01  0.540867    0.426181    0.220182\n2020-01-02  0.864340    0.432873    0.487878\n2020-01-03  0.017099    0.181050    0.373139\n2020-01-04  0.764557    0.097839    0.499788\n"
            ],
            "answer": {
                "ans_desc": "Try broadcasing on the values: Output: ",
                "code": [
                    "a = df.to_numpy()\n\npanel = pd.DataFrame((a[...,None] * a[:,None,:]).reshape(-1, df.shape[1]), \n                     index=panel.index, columns=panel.columns)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 313,
            "user_id": 7669782,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/baf95d27299ef202c30ed924aab52b50?s=128&d=identicon&r=PG&f=1",
            "display_name": "Daniel Arges",
            "link": "https://stackoverflow.com/users/7669782/daniel-arges"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 65893952,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1611627038,
        "creation_date": 1611617707,
        "question_id": 65893903,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65893903/how-can-i-use-split-in-a-string-when-broadcasting-a-dataframes-column",
        "title": "How can I use split() in a string when broadcasting a dataframe&#39;s column?",
        "body": "<p>Take the following dataframe:</p>\n<p><code>df = pd.DataFrame({'col_1':[0, 1], 'col_2':['here 123', 'here 456']})</code></p>\n<p>Result:</p>\n<pre><code>   col_1     col_2\n0      0  here 123\n1      1  here 456\n</code></pre>\n<p>I need to create a 3rd column (broadcasting), using a condition on <code>col_1</code>, and splitting the string on <code>col_2</code>. This is ok to do:</p>\n<p><code>df['col_3'] = float('NaN')</code></p>\n<p><code>df.loc[df['col_1'] == 1, ['col_3']] = df['col_2'].str.slice(5, 8)</code></p>\n<p>Result:</p>\n<pre><code>   col_1     col_2 col_3\n0      0  here 123   NaN\n1      1  here 456   456\n</code></pre>\n<p>But I need to specify dynamic indexes to split the string on <code>col_2</code>, instead of (5, 8).</p>\n<p>When I try to run the following code it does not work, because <code>df['col_2']</code> is treated as a <code>Series</code>:</p>\n<p><code>df.loc[df['col_1'] == 1, ['col_3']] = df['col_2'].split(' ')[0]</code></p>\n<p>I'm spending a huge time trying to solve this without needing to iterate the dataframe.</p>\n",
        "answer_body": "<p>This one liner does the trick.</p>\n<pre><code>df['col_3']=[y.split(' ')[1] if x==1 else float('nan') for x,y in df[['col_1','col_2']].values]\n</code></pre>\n",
        "question_body": "<p>Take the following dataframe:</p>\n<p><code>df = pd.DataFrame({'col_1':[0, 1], 'col_2':['here 123', 'here 456']})</code></p>\n<p>Result:</p>\n<pre><code>   col_1     col_2\n0      0  here 123\n1      1  here 456\n</code></pre>\n<p>I need to create a 3rd column (broadcasting), using a condition on <code>col_1</code>, and splitting the string on <code>col_2</code>. This is ok to do:</p>\n<p><code>df['col_3'] = float('NaN')</code></p>\n<p><code>df.loc[df['col_1'] == 1, ['col_3']] = df['col_2'].str.slice(5, 8)</code></p>\n<p>Result:</p>\n<pre><code>   col_1     col_2 col_3\n0      0  here 123   NaN\n1      1  here 456   456\n</code></pre>\n<p>But I need to specify dynamic indexes to split the string on <code>col_2</code>, instead of (5, 8).</p>\n<p>When I try to run the following code it does not work, because <code>df['col_2']</code> is treated as a <code>Series</code>:</p>\n<p><code>df.loc[df['col_1'] == 1, ['col_3']] = df['col_2'].split(' ')[0]</code></p>\n<p>I'm spending a huge time trying to solve this without needing to iterate the dataframe.</p>\n",
        "formatted_input": {
            "qid": 65893903,
            "link": "https://stackoverflow.com/questions/65893903/how-can-i-use-split-in-a-string-when-broadcasting-a-dataframes-column",
            "question": {
                "title": "How can I use split() in a string when broadcasting a dataframe&#39;s column?",
                "ques_desc": "Take the following dataframe: Result: I need to create a 3rd column (broadcasting), using a condition on , and splitting the string on . This is ok to do: Result: But I need to specify dynamic indexes to split the string on , instead of (5, 8). When I try to run the following code it does not work, because is treated as a : I'm spending a huge time trying to solve this without needing to iterate the dataframe. "
            },
            "io": [
                "   col_1     col_2\n0      0  here 123\n1      1  here 456\n",
                "   col_1     col_2 col_3\n0      0  here 123   NaN\n1      1  here 456   456\n"
            ],
            "answer": {
                "ans_desc": "This one liner does the trick. ",
                "code": [
                    "df['col_3']=[y.split(' ')[1] if x==1 else float('nan') for x,y in df[['col_1','col_2']].values]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "loops",
            "iteration"
        ],
        "owner": {
            "reputation": 59,
            "user_id": 14913897,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/404c484fcfc3417419cf1c3e2c3c3290?s=128&d=identicon&r=PG&f=1",
            "display_name": "henryhong",
            "link": "https://stackoverflow.com/users/14913897/henryhong"
        },
        "is_answered": true,
        "view_count": 180,
        "accepted_answer_id": 65863879,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1611541139,
        "creation_date": 1611431560,
        "question_id": 65863722,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65863722/create-new-rows-in-pandas-by-adding-to-previous-row-looping-until-x-number-of",
        "title": "Create new rows in Pandas, by adding to previous row, looping until x number of rows are made",
        "body": "<p>Input:</p>\n<pre><code>mp = [1,2,3,4]\ntw = [4,7,3,5]\ncw = []\n\n# create the data frame:\n\ndf = pd.DataFrame((cw),\n    columns = [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;])\n\n# add list tw to the first row\n\ndf.loc[len(df)] = tw\ndf\n</code></pre>\n<p>Output:</p>\n<pre><code>    1      2      3      4\n0   4      7      3      5\n</code></pre>\n<p>Desired Output:</p>\n<pre><code>      1      2      3      4\n0     4      7      3      5\n1     5      8      4      6\n2     6      9      5      7\n...  ...    ...    ...    ...\n3000 3003   3006   3002   3004\n</code></pre>\n<p>Starting from an initial reference row of list &quot;tw&quot; I need to add 1 to the starting value and keep going.\nHow do I loop and keep creating rows so that the next row is the previous row +1, I need to do this for 3000 rows. A lot of solutions I've seen require me to create lists and add to the pandas dataframe however I cannot manually create 3000 lists and then manually add them to my dataframe. There must be a way to loop over this, please help!</p>\n",
        "answer_body": "<p>I would use numpy for this, that way we can perform a simple addition with braodcasting. Then create the DataFrame after you've made the array. Growing a DataFrame row by row is horribly inefficient.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nN = 3000           # Number of rows\ntw = [4, 7, 3, 5]  # Initial column values\n\ndf = pd.DataFrame(np.array(tw) + np.arange(N)[:, None],\n                  columns=[1,2,3,4])\n#         1     2     3     4\n#0        4     7     3     5\n#1        5     8     4     6\n#2        6     9     5     7\n#...    ...   ...   ...   ...\n#2997  3001  3004  3000  3002\n#2998  3002  3005  3001  3003\n#2999  3003  3006  3002  3004\n# \n#[3000 rows x 4 columns]\n</code></pre>\n<hr />\n<p>Another option is to create the one row DataFrame, <code>reindex</code> to create all the rows, fill with the value you want to add (<code>1</code>) and <code>cumsum</code>.</p>\n<pre><code>df = (pd.DataFrame([tw], columns=[1,2,3,4])\n        .reindex(range(N))\n        .fillna(1, downcast='infer')\n        .cumsum())\n</code></pre>\n",
        "question_body": "<p>Input:</p>\n<pre><code>mp = [1,2,3,4]\ntw = [4,7,3,5]\ncw = []\n\n# create the data frame:\n\ndf = pd.DataFrame((cw),\n    columns = [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;])\n\n# add list tw to the first row\n\ndf.loc[len(df)] = tw\ndf\n</code></pre>\n<p>Output:</p>\n<pre><code>    1      2      3      4\n0   4      7      3      5\n</code></pre>\n<p>Desired Output:</p>\n<pre><code>      1      2      3      4\n0     4      7      3      5\n1     5      8      4      6\n2     6      9      5      7\n...  ...    ...    ...    ...\n3000 3003   3006   3002   3004\n</code></pre>\n<p>Starting from an initial reference row of list &quot;tw&quot; I need to add 1 to the starting value and keep going.\nHow do I loop and keep creating rows so that the next row is the previous row +1, I need to do this for 3000 rows. A lot of solutions I've seen require me to create lists and add to the pandas dataframe however I cannot manually create 3000 lists and then manually add them to my dataframe. There must be a way to loop over this, please help!</p>\n",
        "formatted_input": {
            "qid": 65863722,
            "link": "https://stackoverflow.com/questions/65863722/create-new-rows-in-pandas-by-adding-to-previous-row-looping-until-x-number-of",
            "question": {
                "title": "Create new rows in Pandas, by adding to previous row, looping until x number of rows are made",
                "ques_desc": "Input: Output: Desired Output: Starting from an initial reference row of list \"tw\" I need to add 1 to the starting value and keep going. How do I loop and keep creating rows so that the next row is the previous row +1, I need to do this for 3000 rows. A lot of solutions I've seen require me to create lists and add to the pandas dataframe however I cannot manually create 3000 lists and then manually add them to my dataframe. There must be a way to loop over this, please help! "
            },
            "io": [
                "    1      2      3      4\n0   4      7      3      5\n",
                "      1      2      3      4\n0     4      7      3      5\n1     5      8      4      6\n2     6      9      5      7\n...  ...    ...    ...    ...\n3000 3003   3006   3002   3004\n"
            ],
            "answer": {
                "ans_desc": "I would use numpy for this, that way we can perform a simple addition with braodcasting. Then create the DataFrame after you've made the array. Growing a DataFrame row by row is horribly inefficient. Another option is to create the one row DataFrame, to create all the rows, fill with the value you want to add () and . ",
                "code": [
                    "df = (pd.DataFrame([tw], columns=[1,2,3,4])\n        .reindex(range(N))\n        .fillna(1, downcast='infer')\n        .cumsum())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 161,
            "user_id": 13929402,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c29c7515502b77402e3872707b8df4fb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ramsey",
            "link": "https://stackoverflow.com/users/13929402/ramsey"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 65833245,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1611252177,
        "creation_date": 1611251329,
        "question_id": 65833007,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65833007/select-value-from-a-list-of-columns-that-is-close-to-another-value-in-pandas",
        "title": "Select value from a list of columns that is close to another value in pandas",
        "body": "<p>I have the following data frame:</p>\n<pre><code>S0  S1  S2  S3  S4  S5... Price\n10  15  18  12  18  19     16\n55  45  44  66  58  45     64\n77  84  62  11  61  44     20    \n</code></pre>\n<p>I want to create another column <code>Sup</code> which stores one value lower than the <code>Price</code>.\nIntended result:</p>\n<pre><code>S0  S1  S2  S3  S4  S5... Price  Sup\n10  15  18  12  18  19     16    15\n55  45  44  66  58  45     64    58\n77  84  62  11  61  44     20    11\n</code></pre>\n<p>Here's what I have been trying:</p>\n<pre><code>s = np.sort(df.filter(like='S').values, axis=1)\nmask = (s*1.03) &gt; df['Close'].values[:,None]\ndf['Sup'] = np.where(mask.any(1), s[np.arange(s.shape[0]),mask.argmax(1)], 0)\n</code></pre>\n<p>If the difference is greater than 3% it doesn't do the job right. Note that the s columns may vary so I would want to keep the <code>np.sort(df.filter(like='S').values, axis=1)</code></p>\n<p>Little help will be appreciated. THANKS!</p>\n",
        "answer_body": "<p>Try this:</p>\n<pre><code>s = df.filter(like='S')\n\n# compare to the price\nmask = s.lt(df['Price'], axis=0)\n\n# `where(mask)` places `NaN` where mask==False\n# `max(1)` takes maximum along rows, ignoring `NaN`\n# rows with all `NaN` returns `NaN` after `max`, \n# `fillna(0)` fills those with 0\ndf['Price'] = s.where(mask).max(1).fillna(0)\n</code></pre>\n<p>Output:</p>\n<pre><code>   S0  S1  S2  S3  S4  S5  Price\n0  10  15  18  12  18  19   15.0\n1  55  45  44  66  58  45   58.0\n2  77  84  62  11  61  44   11.0\n</code></pre>\n",
        "question_body": "<p>I have the following data frame:</p>\n<pre><code>S0  S1  S2  S3  S4  S5... Price\n10  15  18  12  18  19     16\n55  45  44  66  58  45     64\n77  84  62  11  61  44     20    \n</code></pre>\n<p>I want to create another column <code>Sup</code> which stores one value lower than the <code>Price</code>.\nIntended result:</p>\n<pre><code>S0  S1  S2  S3  S4  S5... Price  Sup\n10  15  18  12  18  19     16    15\n55  45  44  66  58  45     64    58\n77  84  62  11  61  44     20    11\n</code></pre>\n<p>Here's what I have been trying:</p>\n<pre><code>s = np.sort(df.filter(like='S').values, axis=1)\nmask = (s*1.03) &gt; df['Close'].values[:,None]\ndf['Sup'] = np.where(mask.any(1), s[np.arange(s.shape[0]),mask.argmax(1)], 0)\n</code></pre>\n<p>If the difference is greater than 3% it doesn't do the job right. Note that the s columns may vary so I would want to keep the <code>np.sort(df.filter(like='S').values, axis=1)</code></p>\n<p>Little help will be appreciated. THANKS!</p>\n",
        "formatted_input": {
            "qid": 65833007,
            "link": "https://stackoverflow.com/questions/65833007/select-value-from-a-list-of-columns-that-is-close-to-another-value-in-pandas",
            "question": {
                "title": "Select value from a list of columns that is close to another value in pandas",
                "ques_desc": "I have the following data frame: I want to create another column which stores one value lower than the . Intended result: Here's what I have been trying: If the difference is greater than 3% it doesn't do the job right. Note that the s columns may vary so I would want to keep the Little help will be appreciated. THANKS! "
            },
            "io": [
                "S0  S1  S2  S3  S4  S5... Price\n10  15  18  12  18  19     16\n55  45  44  66  58  45     64\n77  84  62  11  61  44     20    \n",
                "S0  S1  S2  S3  S4  S5... Price  Sup\n10  15  18  12  18  19     16    15\n55  45  44  66  58  45     64    58\n77  84  62  11  61  44     20    11\n"
            ],
            "answer": {
                "ans_desc": "Try this: Output: ",
                "code": [
                    "s = df.filter(like='S')\n\n# compare to the price\nmask = s.lt(df['Price'], axis=0)\n\n# `where(mask)` places `NaN` where mask==False\n# `max(1)` takes maximum along rows, ignoring `NaN`\n# rows with all `NaN` returns `NaN` after `max`, \n# `fillna(0)` fills those with 0\ndf['Price'] = s.where(mask).max(1).fillna(0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 63,
            "user_id": 4395271,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c4917ca278a46bf680a4da0d7099061d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Jonas",
            "link": "https://stackoverflow.com/users/4395271/jonas"
        },
        "is_answered": true,
        "view_count": 373,
        "accepted_answer_id": 65811931,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1611154947,
        "creation_date": 1611151848,
        "question_id": 65811195,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65811195/pandas-replace-missing-dataframe-values-conditional-calculation-fillna",
        "title": "Pandas: Replace missing dataframe values / conditional calculation: fillna",
        "body": "<p>I want to calculate a pandas dataframe, but some rows contain missing values. For those missing values, i want to use a diffent algorithm. Lets say:</p>\n<ul>\n<li>If column B contains a value, then substract A from B</li>\n<li>If column B does <strong>not</strong> contain a value, then subtract A from C</li>\n</ul>\n<pre><code>import pandas as pd\ndf = pd.DataFrame({'a':[1,2,3,4], 'b':[1,1,None,1],'c':[2,2,2,2]})\ndf['calc'] = df['b']-df['a']\n</code></pre>\n<p>results in:</p>\n<pre><code>print(df)\n   a    b  c  calc\n0  1  1.0  2   0.0\n1  2  1.0  2  -1.0\n2  3  NaN  2   NaN\n3  4  1.0  2  -3.0\n</code></pre>\n<p><strong>Approach 1:</strong> fill the NaN rows using <code>.where</code>:</p>\n<pre><code>df['calc'].where(df['b'].isnull()) = df['c']-df['a']\n</code></pre>\n<p>which results in <strong>SyntaxError: cannot assign to function call</strong>.</p>\n<p><strong>Approach 2:</strong> fill the NaN rows using <code>.iterrows()</code>:</p>\n<pre><code>for index, row in df.iterrows():\n    i = df['calc'].iloc[index]\n\n    if pd.isnull(row['b']):\n        i = row['c']-row['a']\n        print(i)\n    else:\n        i = row['b']-row['a']\n        print(i)\n</code></pre>\n<p>is executed without errors and calculation is correct, these <code>i</code> values are printed to the console:</p>\n<pre><code>0.0\n-1.0\n-1.0\n-3.0\n</code></pre>\n<p>but the <strong>values are not written into <code>df['calc']</code></strong>, the datafram remains as is:</p>\n<pre><code>print(df['calc'])\n0    0.0\n1   -1.0\n2    NaN\n3   -3.0\n</code></pre>\n<p>What is the correct way of overwriting the <code>NaN</code> values?</p>\n",
        "answer_body": "<p><strong>Approach 2:</strong></p>\n<p>you are assigning it to <code>i</code> value. but this won't modify your original dataframe.</p>\n<pre><code>for index, row in df.iterrows():\n    i = df['calc'].iloc[index]\n\n    if pd.isnull(row['b']):\n        i = row['c']-row['a']\n        print(i)\n    else:\n        i = row['b']-row['a']\n        print(i)\n    df.loc[index,'calc'] = i #&lt;------------- here\n</code></pre>\n<p>also don't use <code>iterrows()</code> it is too slow.</p>\n<p><strong>Approach 1:</strong>\n<code>Pandas where() method is used to check a data frame for one or more condition and return the result accordingly. By default, The rows not satisfying the condition are filled with NaN value.</code></p>\n<p>it should be:</p>\n<pre><code>df['calc'] = df['calc'].where(df['b'].isnull(), df['c']-df['a'])\n</code></pre>\n<p>but this will only find those row value where you have non zero value and fill that with the given value.</p>\n<p>Use:</p>\n<pre><code>df['calc'] = df['calc'].where(~df['b'].isnull(), df['c']-df['a'])\n</code></pre>\n<p>OR</p>\n<pre><code>df['calc'] = np.where(df['b'].isnull(), df['c']-df['a'], df['calc'])\n</code></pre>\n",
        "question_body": "<p>I want to calculate a pandas dataframe, but some rows contain missing values. For those missing values, i want to use a diffent algorithm. Lets say:</p>\n<ul>\n<li>If column B contains a value, then substract A from B</li>\n<li>If column B does <strong>not</strong> contain a value, then subtract A from C</li>\n</ul>\n<pre><code>import pandas as pd\ndf = pd.DataFrame({'a':[1,2,3,4], 'b':[1,1,None,1],'c':[2,2,2,2]})\ndf['calc'] = df['b']-df['a']\n</code></pre>\n<p>results in:</p>\n<pre><code>print(df)\n   a    b  c  calc\n0  1  1.0  2   0.0\n1  2  1.0  2  -1.0\n2  3  NaN  2   NaN\n3  4  1.0  2  -3.0\n</code></pre>\n<p><strong>Approach 1:</strong> fill the NaN rows using <code>.where</code>:</p>\n<pre><code>df['calc'].where(df['b'].isnull()) = df['c']-df['a']\n</code></pre>\n<p>which results in <strong>SyntaxError: cannot assign to function call</strong>.</p>\n<p><strong>Approach 2:</strong> fill the NaN rows using <code>.iterrows()</code>:</p>\n<pre><code>for index, row in df.iterrows():\n    i = df['calc'].iloc[index]\n\n    if pd.isnull(row['b']):\n        i = row['c']-row['a']\n        print(i)\n    else:\n        i = row['b']-row['a']\n        print(i)\n</code></pre>\n<p>is executed without errors and calculation is correct, these <code>i</code> values are printed to the console:</p>\n<pre><code>0.0\n-1.0\n-1.0\n-3.0\n</code></pre>\n<p>but the <strong>values are not written into <code>df['calc']</code></strong>, the datafram remains as is:</p>\n<pre><code>print(df['calc'])\n0    0.0\n1   -1.0\n2    NaN\n3   -3.0\n</code></pre>\n<p>What is the correct way of overwriting the <code>NaN</code> values?</p>\n",
        "formatted_input": {
            "qid": 65811195,
            "link": "https://stackoverflow.com/questions/65811195/pandas-replace-missing-dataframe-values-conditional-calculation-fillna",
            "question": {
                "title": "Pandas: Replace missing dataframe values / conditional calculation: fillna",
                "ques_desc": "I want to calculate a pandas dataframe, but some rows contain missing values. For those missing values, i want to use a diffent algorithm. Lets say: If column B contains a value, then substract A from B If column B does not contain a value, then subtract A from C results in: Approach 1: fill the NaN rows using : which results in SyntaxError: cannot assign to function call. Approach 2: fill the NaN rows using : is executed without errors and calculation is correct, these values are printed to the console: but the values are not written into , the datafram remains as is: What is the correct way of overwriting the values? "
            },
            "io": [
                "print(df)\n   a    b  c  calc\n0  1  1.0  2   0.0\n1  2  1.0  2  -1.0\n2  3  NaN  2   NaN\n3  4  1.0  2  -3.0\n",
                "print(df['calc'])\n0    0.0\n1   -1.0\n2    NaN\n3   -3.0\n"
            ],
            "answer": {
                "ans_desc": "Approach 2: you are assigning it to value. but this won't modify your original dataframe. also don't use it is too slow. Approach 1: it should be: but this will only find those row value where you have non zero value and fill that with the given value. Use: OR ",
                "code": [
                    "for index, row in df.iterrows():\n    i = df['calc'].iloc[index]\n\n    if pd.isnull(row['b']):\n        i = row['c']-row['a']\n        print(i)\n    else:\n        i = row['b']-row['a']\n        print(i)\n    df.loc[index,'calc'] = i #<------------- here\n",
                    "Pandas where() method is used to check a data frame for one or more condition and return the result accordingly. By default, The rows not satisfying the condition are filled with NaN value.",
                    "df['calc'] = np.where(df['b'].isnull(), df['c']-df['a'], df['calc'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "date"
        ],
        "owner": {
            "reputation": 191,
            "user_id": 12210377,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/77cdb41b0330ef86a2c36388f6241f74?s=128&d=identicon&r=PG&f=1",
            "display_name": "Roy",
            "link": "https://stackoverflow.com/users/12210377/roy"
        },
        "is_answered": true,
        "view_count": 62,
        "accepted_answer_id": 65794994,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1611085420,
        "creation_date": 1611066234,
        "last_edit_date": 1611085420,
        "question_id": 65793450,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65793450/pandas-create-new-column-based-on-conditions-of-multiple-columns",
        "title": "Pandas: Create New Column Based on Conditions of Multiple Columns",
        "body": "<p>I have the following dataset:</p>\n<pre><code>\n ID            AAA                  BBB                  CCC                   DDD\n1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1} \n                             '2016-02-15': 2}\n1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2017-01-06': 1} \n        '2018-06-05': 1}  \n\n</code></pre>\n<p>In the cell, \u2018key\u2019 is the date when someone is hospitalized and \u2018value\u2019 is the number of days.</p>\n<p>I need to create a new column for hospitalization ('Yes' or 'No').</p>\n<p>The condition to be 'yes':</p>\n<ol>\n<li>The column [AAA or BBB] as well as the column [CCC or DDD] both should have filled-in dates.</li>\n<li>The date in the column [CCC or DDD] should be the next day of the date in the column [AAA or BBB].</li>\n</ol>\n<p>For example, if [AAA or BBB] has a date of January 01, 2020. For 'yes', the date in [CCC or DDD] should be January 02, 2020.</p>\n<p>Desired output:</p>\n<pre><code> ID            AAA              BBB                  CCC                     DDD               Hospitalized\n1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1}            Yes\n                             '2016-02-15': 2}\n1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN                  NaN                      No\n        '2018-06-05': 1}  \n1236    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2018-01-06': 1}            Yes \n        '2018-06-05': 1}  \n           \n</code></pre>\n<p>I have tried the following code, but this captures if the dates are present but doesn't capture the timestamp.</p>\n<pre><code>df['hospitalized'] = (df\n                     .apply(lambda r: 'yes' if (1 if pd.notna(r.loc[['AAA', 'BBB']]).any() else 0) + \n                                               (1 if pd.notna(r.loc[['CCC', 'DDD']]).any() else 0) &gt; 1 \n                            else 'no', axis=1))\n</code></pre>\n<p>Any suggestions would be appreciated. Thanks!</p>\n",
        "answer_body": "<p><strong>df:</strong></p>\n<pre><code>df = pd.DataFrame([[1234, {'2015-01-01': 1}, {'2016-01-01': 1, '2016-02-15': 2}, {'2015-01-02': 1}, {'2016-01-02': 1}], [1235, {'2017-11-05': 1,'2018-06-05': 1}, {'2018-01-05': 1}, np.nan, np.nan]], columns= ['ID', 'AAA', 'BBB', 'CCC', 'DDD'])\n</code></pre>\n<p>Try:</p>\n<pre><code>import itertools\nfrom dateutil import parser\nimport datetime\ndef func(x):\n    A_B_dates = list(map(parser.parse,list(itertools.chain(*[x['AAA'].keys()] + [x['BBB'].keys()]))))\n    C_D_dates = list(map(parser.parse,list(itertools.chain(*[x['CCC'].keys()] + [x['DDD'].keys()]))))\n    for date1 in A_B_dates:\n        if date1+datetime.timedelta(days=1) in C_D_dates:\n            return 'yes'\n    return 'no'\n\ndf = df.where(df.notna(), lambda x: [{}])    \ndf['Hospitalised'] = df.apply(func, axis=1)\n</code></pre>\n<hr />\n<p><strong>df:</strong></p>\n<pre><code>    ID       AAA                                BBB                                CCC                  DDD                 Hospitalised\n0   1234    {'2015-01-01': 1}                   {'2016-01-01': 1, '2016-02-15': 2}  {'2015-01-02': 1}   {'2016-01-02': 1}   yes\n1   1235    {'2017-11-05': 1, '2018-06-05': 1}  {'2018-01-05': 1}                   {}                  {'2017-01-06': 1}   no\n</code></pre>\n",
        "question_body": "<p>I have the following dataset:</p>\n<pre><code>\n ID            AAA                  BBB                  CCC                   DDD\n1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1} \n                             '2016-02-15': 2}\n1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2017-01-06': 1} \n        '2018-06-05': 1}  \n\n</code></pre>\n<p>In the cell, \u2018key\u2019 is the date when someone is hospitalized and \u2018value\u2019 is the number of days.</p>\n<p>I need to create a new column for hospitalization ('Yes' or 'No').</p>\n<p>The condition to be 'yes':</p>\n<ol>\n<li>The column [AAA or BBB] as well as the column [CCC or DDD] both should have filled-in dates.</li>\n<li>The date in the column [CCC or DDD] should be the next day of the date in the column [AAA or BBB].</li>\n</ol>\n<p>For example, if [AAA or BBB] has a date of January 01, 2020. For 'yes', the date in [CCC or DDD] should be January 02, 2020.</p>\n<p>Desired output:</p>\n<pre><code> ID            AAA              BBB                  CCC                     DDD               Hospitalized\n1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1}            Yes\n                             '2016-02-15': 2}\n1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN                  NaN                      No\n        '2018-06-05': 1}  \n1236    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2018-01-06': 1}            Yes \n        '2018-06-05': 1}  \n           \n</code></pre>\n<p>I have tried the following code, but this captures if the dates are present but doesn't capture the timestamp.</p>\n<pre><code>df['hospitalized'] = (df\n                     .apply(lambda r: 'yes' if (1 if pd.notna(r.loc[['AAA', 'BBB']]).any() else 0) + \n                                               (1 if pd.notna(r.loc[['CCC', 'DDD']]).any() else 0) &gt; 1 \n                            else 'no', axis=1))\n</code></pre>\n<p>Any suggestions would be appreciated. Thanks!</p>\n",
        "formatted_input": {
            "qid": 65793450,
            "link": "https://stackoverflow.com/questions/65793450/pandas-create-new-column-based-on-conditions-of-multiple-columns",
            "question": {
                "title": "Pandas: Create New Column Based on Conditions of Multiple Columns",
                "ques_desc": "I have the following dataset: In the cell, \u2018key\u2019 is the date when someone is hospitalized and \u2018value\u2019 is the number of days. I need to create a new column for hospitalization ('Yes' or 'No'). The condition to be 'yes': The column [AAA or BBB] as well as the column [CCC or DDD] both should have filled-in dates. The date in the column [CCC or DDD] should be the next day of the date in the column [AAA or BBB]. For example, if [AAA or BBB] has a date of January 01, 2020. For 'yes', the date in [CCC or DDD] should be January 02, 2020. Desired output: I have tried the following code, but this captures if the dates are present but doesn't capture the timestamp. Any suggestions would be appreciated. Thanks! "
            },
            "io": [
                "\n ID            AAA                  BBB                  CCC                   DDD\n1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1} \n                             '2016-02-15': 2}\n1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2017-01-06': 1} \n        '2018-06-05': 1}  \n\n",
                " ID            AAA              BBB                  CCC                     DDD               Hospitalized\n1234    {'2015-01-01': 1}    {'2016-01-01': 1,   {'2015-01-02': 1}     {'2016-01-02': 1}            Yes\n                             '2016-02-15': 2}\n1235    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN                  NaN                      No\n        '2018-06-05': 1}  \n1236    {'2017-11-05': 1,    {'2018-01-05': 1}         NaN             {'2018-01-06': 1}            Yes \n        '2018-06-05': 1}  \n           \n"
            ],
            "answer": {
                "ans_desc": "df: Try: df: ",
                "code": [
                    "df = pd.DataFrame([[1234, {'2015-01-01': 1}, {'2016-01-01': 1, '2016-02-15': 2}, {'2015-01-02': 1}, {'2016-01-02': 1}], [1235, {'2017-11-05': 1,'2018-06-05': 1}, {'2018-01-05': 1}, np.nan, np.nan]], columns= ['ID', 'AAA', 'BBB', 'CCC', 'DDD'])\n",
                    "import itertools\nfrom dateutil import parser\nimport datetime\ndef func(x):\n    A_B_dates = list(map(parser.parse,list(itertools.chain(*[x['AAA'].keys()] + [x['BBB'].keys()]))))\n    C_D_dates = list(map(parser.parse,list(itertools.chain(*[x['CCC'].keys()] + [x['DDD'].keys()]))))\n    for date1 in A_B_dates:\n        if date1+datetime.timedelta(days=1) in C_D_dates:\n            return 'yes'\n    return 'no'\n\ndf = df.where(df.notna(), lambda x: [{}])    \ndf['Hospitalised'] = df.apply(func, axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 105,
            "user_id": 14502733,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2cc5ece5559fe6187502e08b42fc38c3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ecko",
            "link": "https://stackoverflow.com/users/14502733/ecko"
        },
        "is_answered": true,
        "view_count": 13,
        "accepted_answer_id": 65797613,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1611083084,
        "creation_date": 1611081924,
        "question_id": 65797595,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65797595/insert-complete-repeated-row-under-condition-pandas",
        "title": "Insert complete repeated row under condition pandas",
        "body": "<p>Basically, I'm trying to consider the third column (df1[3]) if the value is higher or equal to 2 I want to repeat i.e insert the whole row to a new row, not to replace.</p>\n<p>Here is the dataframe:</p>\n<pre><code>    1           2       3    \n   \n0   5614    banana      1   \n1   4564    kiwi        1   \n2   3314    salsa       2   \n3   3144    avocado     1   \n4   1214    mix         3   \n5   4314    juice       1   \n</code></pre>\n<p>desired output:</p>\n<pre><code>    1           2       3       \n1   5614    banana      1   \n2   4564    kiwi        1   \n3   3314    salsa       2   \n4   3314    salsa       2  \n5   3144    avocado     1   \n6   1214    mix         3   \n7   1214    mix         3 \n8   1214    mix         3 \n7   4314    juice       1   \n</code></pre>\n<p>code for the DataFrame and attempt to solve it:</p>\n<pre><code>l = [5614,4564,3314,3144,1214,4314]\ni = ['banana','kiwi' ,'salsa','avocado','mix','juice']\nn = [1,1,2,1,3,1]\ndf1 = pd.DataFrame(columns = (1,2,3))\ndf1[1] = l\ndf1[2] = i\ndf1[3] = n\n\n    for indx,row in df.iterrows():\n        if row[3].isdigit() == True and int(row[3]) &gt;= 2:\n            df1.loc[indx] = [row * int(row[3])]\n</code></pre>\n<p>Obviously, the above-stated approach doesn't create a new row with the same values from each column but replaces it.</p>\n<p>Append() wouldn't solve it either because I do have to preserve the exact same order of the data frame.</p>\n<p>Is there anything similar to insert/extend/add or slicing approach in list when it comes to pandas dataframe?</p>\n",
        "answer_body": "<p>Try <code>repeat</code>:</p>\n<pre><code>count = pd.to_numeric(df['3'], errors='coerce').fillna(0).astype(int)\n\n# replace '3' with actual column name\ndf.loc[df.index.repeat(count)]\n</code></pre>\n<p>Output:</p>\n<pre><code>      1        2  3\n0  5614   banana  1\n1  4564     kiwi  1\n2  3314    salsa  2\n2  3314    salsa  2\n3  3144  avocado  1\n4  1214      mix  3\n4  1214      mix  3\n4  1214      mix  3\n5  4314    juice  1\n</code></pre>\n",
        "question_body": "<p>Basically, I'm trying to consider the third column (df1[3]) if the value is higher or equal to 2 I want to repeat i.e insert the whole row to a new row, not to replace.</p>\n<p>Here is the dataframe:</p>\n<pre><code>    1           2       3    \n   \n0   5614    banana      1   \n1   4564    kiwi        1   \n2   3314    salsa       2   \n3   3144    avocado     1   \n4   1214    mix         3   \n5   4314    juice       1   \n</code></pre>\n<p>desired output:</p>\n<pre><code>    1           2       3       \n1   5614    banana      1   \n2   4564    kiwi        1   \n3   3314    salsa       2   \n4   3314    salsa       2  \n5   3144    avocado     1   \n6   1214    mix         3   \n7   1214    mix         3 \n8   1214    mix         3 \n7   4314    juice       1   \n</code></pre>\n<p>code for the DataFrame and attempt to solve it:</p>\n<pre><code>l = [5614,4564,3314,3144,1214,4314]\ni = ['banana','kiwi' ,'salsa','avocado','mix','juice']\nn = [1,1,2,1,3,1]\ndf1 = pd.DataFrame(columns = (1,2,3))\ndf1[1] = l\ndf1[2] = i\ndf1[3] = n\n\n    for indx,row in df.iterrows():\n        if row[3].isdigit() == True and int(row[3]) &gt;= 2:\n            df1.loc[indx] = [row * int(row[3])]\n</code></pre>\n<p>Obviously, the above-stated approach doesn't create a new row with the same values from each column but replaces it.</p>\n<p>Append() wouldn't solve it either because I do have to preserve the exact same order of the data frame.</p>\n<p>Is there anything similar to insert/extend/add or slicing approach in list when it comes to pandas dataframe?</p>\n",
        "formatted_input": {
            "qid": 65797595,
            "link": "https://stackoverflow.com/questions/65797595/insert-complete-repeated-row-under-condition-pandas",
            "question": {
                "title": "Insert complete repeated row under condition pandas",
                "ques_desc": "Basically, I'm trying to consider the third column (df1[3]) if the value is higher or equal to 2 I want to repeat i.e insert the whole row to a new row, not to replace. Here is the dataframe: desired output: code for the DataFrame and attempt to solve it: Obviously, the above-stated approach doesn't create a new row with the same values from each column but replaces it. Append() wouldn't solve it either because I do have to preserve the exact same order of the data frame. Is there anything similar to insert/extend/add or slicing approach in list when it comes to pandas dataframe? "
            },
            "io": [
                "    1           2       3    \n   \n0   5614    banana      1   \n1   4564    kiwi        1   \n2   3314    salsa       2   \n3   3144    avocado     1   \n4   1214    mix         3   \n5   4314    juice       1   \n",
                "    1           2       3       \n1   5614    banana      1   \n2   4564    kiwi        1   \n3   3314    salsa       2   \n4   3314    salsa       2  \n5   3144    avocado     1   \n6   1214    mix         3   \n7   1214    mix         3 \n8   1214    mix         3 \n7   4314    juice       1   \n"
            ],
            "answer": {
                "ans_desc": "Try : Output: ",
                "code": [
                    "count = pd.to_numeric(df['3'], errors='coerce').fillna(0).astype(int)\n\n# replace '3' with actual column name\ndf.loc[df.index.repeat(count)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matching"
        ],
        "owner": {
            "reputation": 87,
            "user_id": 9956822,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-DTt1msgyStE/AAAAAAAAAAI/AAAAAAAAASM/9WkA5vkC7s8/photo.jpg?sz=128",
            "display_name": "Vadim Katsemba",
            "link": "https://stackoverflow.com/users/9956822/vadim-katsemba"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 65750216,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1610965504,
        "creation_date": 1610802840,
        "question_id": 65750044,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65750044/adding-value-from-one-pandas-dataframe-to-another-dataframe-by-matching-a-variab",
        "title": "Adding value from one pandas dataframe to another dataframe by matching a variable",
        "body": "<p>Suppose I have a pandas dataframe  <code>df</code> with 2 columns</p>\n<pre><code>           c1            c2\n    0      v1            b1\n    1      v2            b2\n    2      v3            b3\n    3      v4            b4\n    4      v5            b5\n</code></pre>\n<p>A second dataframe, <code>df2</code> contains c1, c2 and a few other columns.</p>\n<pre><code>   c1 c2 c3  c4\n0  &quot;&quot; b5 500 3\n1  &quot;&quot; b2 420 7\n2  &quot;&quot; b1 380 5\n3  &quot;&quot; b2 470 9\n4  &quot;&quot; b3 290 2\n</code></pre>\n<p>My goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. What is the best way to do this?</p>\n",
        "answer_body": "<p>One easy way you can do is to use the merge of pandas based on similar column.</p>\n<p><code>df2.drop('c1', axis=1, inplace=True)</code>\n<br>\n<code>main_df = pd.merge(df2, df, on=&quot;c2&quot;, how=&quot;left&quot;)</code>\n<br>\n<code>df2['c1'] = main_df['c1']</code>\n<br>\n<code>df2.columns = ['c1','c2','c3','c4']</code></p>\n",
        "question_body": "<p>Suppose I have a pandas dataframe  <code>df</code> with 2 columns</p>\n<pre><code>           c1            c2\n    0      v1            b1\n    1      v2            b2\n    2      v3            b3\n    3      v4            b4\n    4      v5            b5\n</code></pre>\n<p>A second dataframe, <code>df2</code> contains c1, c2 and a few other columns.</p>\n<pre><code>   c1 c2 c3  c4\n0  &quot;&quot; b5 500 3\n1  &quot;&quot; b2 420 7\n2  &quot;&quot; b1 380 5\n3  &quot;&quot; b2 470 9\n4  &quot;&quot; b3 290 2\n</code></pre>\n<p>My goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. What is the best way to do this?</p>\n",
        "formatted_input": {
            "qid": 65750044,
            "link": "https://stackoverflow.com/questions/65750044/adding-value-from-one-pandas-dataframe-to-another-dataframe-by-matching-a-variab",
            "question": {
                "title": "Adding value from one pandas dataframe to another dataframe by matching a variable",
                "ques_desc": "Suppose I have a pandas dataframe with 2 columns A second dataframe, contains c1, c2 and a few other columns. My goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. What is the best way to do this? "
            },
            "io": [
                "           c1            c2\n    0      v1            b1\n    1      v2            b2\n    2      v3            b3\n    3      v4            b4\n    4      v5            b5\n",
                "   c1 c2 c3  c4\n0  \"\" b5 500 3\n1  \"\" b2 420 7\n2  \"\" b1 380 5\n3  \"\" b2 470 9\n4  \"\" b3 290 2\n"
            ],
            "answer": {
                "ans_desc": "One easy way you can do is to use the merge of pandas based on similar column. ",
                "code": [
                    "main_df = pd.merge(df2, df, on=\"c2\", how=\"left\")"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15018168,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ab168c989f5a7443d4b3652fd1ffeb6b?s=128&d=identicon&r=PG&f=1",
            "display_name": "PythonNoob29",
            "link": "https://stackoverflow.com/users/15018168/pythonnoob29"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 65749366,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1610821151,
        "creation_date": 1610797146,
        "last_edit_date": 1610799009,
        "question_id": 65749219,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65749219/add-multiple-dataframe-series-to-new-series-in-same-dataframe",
        "title": "Add multiple DataFrame series to new series in same DataFrame",
        "body": "<p>I have a dataset in a .csv which I imported into a DataFrame using pandas, organized in the following manner (obviously not real numbers):</p>\n<pre><code> A   B   C   D    E    F \n 0  20   4   24   8    28\n 1  21   5   25   NA   NA \n NA  NA  6   26   10   30\n 3  23   NA  NA  11   31\n</code></pre>\n<p>What I want to achieve is to save the data in two extra columns G and H in the same DataFrame so I get the following:</p>\n<pre><code>A  B  C  D  E  F  G   H\n                  0   20\n                  1   21\n                  ...  ...\n                  11   31\n</code></pre>\n<p>Where I would like to keep the same index for all data (so B belongs to A, D to C, F to E etc.). As you can see, the original dataset has some missing values, so I would also like to skip these if they are in there.</p>\n<p>Now, I have looked into <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\" rel=\"nofollow noreferrer\">pandas append</a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\">concat</a>, however I do not see how I can achieve what I wanted, especially with skipping the empty values (presumbly via <code>data.dropna()</code> or some other function?).</p>\n",
        "answer_body": "<p>For example, your dataframe looks like this:</p>\n<pre><code>        keywords1   keywords2   keywords3   key1    key2    col2\n0       blue        red         green       1       5       NaN\n1       big         NaN         medium      2       6       3\n2       bat         ball        goal        3       7       0\n</code></pre>\n<p>Now lets assume that you need to add first three columns and add to another column. So,</p>\n<pre><code>col_1 = ['keywords1', 'keywords2', 'keywords3']\ndf[&quot;new_col_1&quot;] = df[col_1].apply(lambda x: ''.join(x.dropna()), axis=1)\n</code></pre>\n<p>As I understood that your data contains basically string type data, the above code should work fine. Otherwise, you can convert the columns into string using the<code>.apply(str)</code> function.</p>\n<p>Older Solution:</p>\n<p>You can perform the operation and put it in a <code>pandas.core.series.Series</code>. For example:</p>\n<pre><code>df = pd.read_excel('sample_excel.xlsx', engine='openpyxl') df\n</code></pre>\n<p>The output will be:</p>\n<blockquote>\n<pre><code>  key1    key2\n0  1      5\n1  2      6\n2  3      7\n</code></pre>\n</blockquote>\n<p>Now, let's do some operations:</p>\n<pre><code>df['key3'] = df['key1'] + df['key1']\ndf\n</code></pre>\n<p>The output is:</p>\n<blockquote>\n<pre><code>  key1    key2    key3\n0  1      5       6\n1  2      6       8\n2  3      7       10\n</code></pre>\n</blockquote>\n<p>Now save the new dataframe as excel file:</p>\n<pre><code>df.to_excel('new_sample.xlsx')\n</code></pre>\n",
        "question_body": "<p>I have a dataset in a .csv which I imported into a DataFrame using pandas, organized in the following manner (obviously not real numbers):</p>\n<pre><code> A   B   C   D    E    F \n 0  20   4   24   8    28\n 1  21   5   25   NA   NA \n NA  NA  6   26   10   30\n 3  23   NA  NA  11   31\n</code></pre>\n<p>What I want to achieve is to save the data in two extra columns G and H in the same DataFrame so I get the following:</p>\n<pre><code>A  B  C  D  E  F  G   H\n                  0   20\n                  1   21\n                  ...  ...\n                  11   31\n</code></pre>\n<p>Where I would like to keep the same index for all data (so B belongs to A, D to C, F to E etc.). As you can see, the original dataset has some missing values, so I would also like to skip these if they are in there.</p>\n<p>Now, I have looked into <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\" rel=\"nofollow noreferrer\">pandas append</a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\">concat</a>, however I do not see how I can achieve what I wanted, especially with skipping the empty values (presumbly via <code>data.dropna()</code> or some other function?).</p>\n",
        "formatted_input": {
            "qid": 65749219,
            "link": "https://stackoverflow.com/questions/65749219/add-multiple-dataframe-series-to-new-series-in-same-dataframe",
            "question": {
                "title": "Add multiple DataFrame series to new series in same DataFrame",
                "ques_desc": "I have a dataset in a .csv which I imported into a DataFrame using pandas, organized in the following manner (obviously not real numbers): What I want to achieve is to save the data in two extra columns G and H in the same DataFrame so I get the following: Where I would like to keep the same index for all data (so B belongs to A, D to C, F to E etc.). As you can see, the original dataset has some missing values, so I would also like to skip these if they are in there. Now, I have looked into pandas append and concat, however I do not see how I can achieve what I wanted, especially with skipping the empty values (presumbly via or some other function?). "
            },
            "io": [
                " A   B   C   D    E    F \n 0  20   4   24   8    28\n 1  21   5   25   NA   NA \n NA  NA  6   26   10   30\n 3  23   NA  NA  11   31\n",
                "A  B  C  D  E  F  G   H\n                  0   20\n                  1   21\n                  ...  ...\n                  11   31\n"
            ],
            "answer": {
                "ans_desc": "For example, your dataframe looks like this: Now lets assume that you need to add first three columns and add to another column. So, As I understood that your data contains basically string type data, the above code should work fine. Otherwise, you can convert the columns into string using the function. Older Solution: You can perform the operation and put it in a . For example: The output will be: Now, let's do some operations: The output is: Now save the new dataframe as excel file: ",
                "code": [
                    "df = pd.read_excel('sample_excel.xlsx', engine='openpyxl') df\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "database",
            "dataframe",
            "matrix"
        ],
        "owner": {
            "reputation": 99,
            "user_id": 10772750,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/ME2Xm.jpg?s=128&g=1",
            "display_name": "Alkis Ko",
            "link": "https://stackoverflow.com/users/10772750/alkis-ko"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 65739725,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1610728495,
        "creation_date": 1610727251,
        "question_id": 65739584,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65739584/turn-columns-values-to-headers-of-columns-with-values-1-and-0-accordingly-p",
        "title": "Turn columns&#39; values to headers of columns with values 1 and 0 ( accordingly) [python]",
        "body": "<p>I got a column of the form :</p>\n<pre><code>0           q4\n1           4\n2           3   \n3           1\n4           2\n5           1\n6           5\n7           1\n8           3\n</code></pre>\n<p>The column represents the answers of users to a question of 5 choices (1-5).</p>\n<p>I want to turn this into a matrix of 5 columns where the indexes are the 5 possible answers and the values are 1 or 0 according to the user's given answer.</p>\n<p>Visualy i want a matrix of the form:</p>\n<pre><code>0   q4_1  q4_2  q4_3  q4_4 q4_5\n1   Nan    Nan   Nan   1    Nan\n2   Nan    Nan   1    Nan   Nan\n3   1      Nan   Nan  Nan   Nan\n4   Nan    1     Nan  Nan   Nan\n5   1      Nan   Nan  Nan   Nan\n</code></pre>\n",
        "answer_body": "<pre><code>for i in range(1,6):\n    df['q4_'+str(i)]=np.where(df.q4==i, 1, 0)\n\ndef df['q4']\n</code></pre>\n<p>Output:</p>\n<pre><code>&gt;&gt;&gt; print(df)\n\n   q4_1  q4_2  q4_3  q4_4  q4_5\n0     0     0     0     1     0\n1     0     0     1     0     0\n2     1     0     0     0     0\n3     0     1     0     0     0\n4     1     0     0     0     0\n5     0     0     0     0     1\n6     1     0     0     0     0\n7     0     0     1     0     0\n</code></pre>\n",
        "question_body": "<p>I got a column of the form :</p>\n<pre><code>0           q4\n1           4\n2           3   \n3           1\n4           2\n5           1\n6           5\n7           1\n8           3\n</code></pre>\n<p>The column represents the answers of users to a question of 5 choices (1-5).</p>\n<p>I want to turn this into a matrix of 5 columns where the indexes are the 5 possible answers and the values are 1 or 0 according to the user's given answer.</p>\n<p>Visualy i want a matrix of the form:</p>\n<pre><code>0   q4_1  q4_2  q4_3  q4_4 q4_5\n1   Nan    Nan   Nan   1    Nan\n2   Nan    Nan   1    Nan   Nan\n3   1      Nan   Nan  Nan   Nan\n4   Nan    1     Nan  Nan   Nan\n5   1      Nan   Nan  Nan   Nan\n</code></pre>\n",
        "formatted_input": {
            "qid": 65739584,
            "link": "https://stackoverflow.com/questions/65739584/turn-columns-values-to-headers-of-columns-with-values-1-and-0-accordingly-p",
            "question": {
                "title": "Turn columns&#39; values to headers of columns with values 1 and 0 ( accordingly) [python]",
                "ques_desc": "I got a column of the form : The column represents the answers of users to a question of 5 choices (1-5). I want to turn this into a matrix of 5 columns where the indexes are the 5 possible answers and the values are 1 or 0 according to the user's given answer. Visualy i want a matrix of the form: "
            },
            "io": [
                "0           q4\n1           4\n2           3   \n3           1\n4           2\n5           1\n6           5\n7           1\n8           3\n",
                "0   q4_1  q4_2  q4_3  q4_4 q4_5\n1   Nan    Nan   Nan   1    Nan\n2   Nan    Nan   1    Nan   Nan\n3   1      Nan   Nan  Nan   Nan\n4   Nan    1     Nan  Nan   Nan\n5   1      Nan   Nan  Nan   Nan\n"
            ],
            "answer": {
                "ans_desc": " Output: ",
                "code": [
                    "for i in range(1,6):\n    df['q4_'+str(i)]=np.where(df.q4==i, 1, 0)\n\ndef df['q4']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "indexing",
            "complex-numbers"
        ],
        "owner": {
            "reputation": 132,
            "user_id": 7242713,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9db1c3078777aef4194935dac7db05f0?s=128&d=identicon&r=PG&f=1",
            "display_name": "jrive",
            "link": "https://stackoverflow.com/users/7242713/jrive"
        },
        "is_answered": true,
        "view_count": 73,
        "accepted_answer_id": 65739197,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1610726925,
        "creation_date": 1610723886,
        "last_edit_date": 1610725721,
        "question_id": 65738704,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65738704/combining-real-and-imag-columns-in-dataframe-into-complex-number-to-obtain-magni",
        "title": "combining real and imag columns in dataframe into complex number to obtain magnitude using np.abs",
        "body": "<p>I have a data frame that has complex numbers split into a real and an imaginary column.  I want to add a column (2, actually, one for each channel) to the dataframe that computes the log magnitude:</p>\n<pre><code> `    ch1_real  ch1_imag  ch2_real  ch2_imag  ch1_phase  ch2_phase  distance\n79   0.011960 -0.003418  0.005127 -0.019530     -15.95    -75.290       0.0\n78  -0.009766 -0.005371 -0.015870  0.010010    -151.20    147.800       1.0\n343  0.002197  0.010990  0.003662 -0.013180      78.69    -74.480       2.0\n80  -0.002686  0.010740  0.011960  0.013430     104.00     48.300       3.0\n341 -0.007080  0.009033  0.016600 -0.000977     128.10     -3.366       4.0\n</code></pre>\n<p>If I try this:</p>\n<pre><code>df['ch1_log_mag']=20*np.log10(np.abs(complex(df.ch1_real,df.ch1_imag)))\n</code></pre>\n<p>I get error: &quot;TypeError: cannot convert the series to &lt;class 'float'&gt;&quot;, because I think cmath.complex cannot work on an array.</p>\n<p>So I then experimented using loc to pick out the first element of ch1_real, for example, to then work out how use it to accomplish what I'm trying to do, but couldn't figure out how to do it:</p>\n<pre><code>df.loc[0,df['ch1_real']]\n</code></pre>\n<p>This produces a KeyError.</p>\n<p>Brute forcing it works,</p>\n<pre><code>df['ch1_log_mag'] = 20 * np.log10(np.sqrt(df.ch1_real**2+ df.ch1_imag**2))\n</code></pre>\n<p>but, I believe it is more legible to use np.abs to get the magnitude, plus I'm more interested in understanding how dataframes and indexing dataframes work and why what I initially attempted does not work.</p>\n<p>btw, what is the difference between df.ch1_real and df['ch1_real'] ?  When do I use one vs. the other?</p>\n<p><strong>Edit</strong>:  more attempts at solution\nI tried using apply, since my understanding is that it &quot;applies&quot; the function passed to it to each row (by default):</p>\n<pre><code>df.apply(complex(df['ch1_real'], df['ch1_imag']))\n</code></pre>\n<p>but this generates the same TypeError, since I think the issue is that complex cannot work on Series.  Perhaps if I cast the series to float?</p>\n<p>After reading this <a href=\"https://stackoverflow.com/questions/25952790/convert-pandas-series-from-dtype-object-to-float-and-errors-to-nans\">post</a>, I tried using pd.to_numeric to convert a series to  type float:</p>\n<pre><code> dfUnique.apply(complex(pd.to_numeric(dfUnique['ch1_real'],errors='coerce'), pd.to_numeric(dfUnique['ch1_imag'],errors='coerce')))\n</code></pre>\n<p>to no avail.</p>\n",
        "answer_body": "<p>You can do simple multiplication with <code>1j</code> which denotes the complex number <code>0+1j</code>, see <a href=\"https://docs.python.org/3/reference/lexical_analysis.html#imaginary-literals\" rel=\"nofollow noreferrer\">imaginary literals</a>:</p>\n<pre><code>df['ch1_log_mag'] = 20 * np.log10((df.ch1_real + 1j * df.ch1_imag).abs())\n</code></pre>\n<p><code>complex(df.ch1_real, df.ch1_imag)</code> doesn't work as it needs a float argument, not a whole series. <code>df.loc[0,df['ch1_real']]</code> is not a valid expression, as the second argument must be a string, not a series (df.loc[79,'ch1_real'] would work for accessing an element).<br />\nIf you want to use <code>apply</code> it should be <code>20 * np.log10(df.apply(lambda x: complex(x.ch1_real, x.ch1_imag), 1).abs())</code> but as apply is just a disguised loop over the rows of the dataframe it's not recommended performancewise.</p>\n<p>There's no difference between <code>df.ch1_real</code> and <code>df['ch1_real']</code>, it's a matter of personal preference. If your column name contains spaces or dots or the like you must use the latter form however.</p>\n",
        "question_body": "<p>I have a data frame that has complex numbers split into a real and an imaginary column.  I want to add a column (2, actually, one for each channel) to the dataframe that computes the log magnitude:</p>\n<pre><code> `    ch1_real  ch1_imag  ch2_real  ch2_imag  ch1_phase  ch2_phase  distance\n79   0.011960 -0.003418  0.005127 -0.019530     -15.95    -75.290       0.0\n78  -0.009766 -0.005371 -0.015870  0.010010    -151.20    147.800       1.0\n343  0.002197  0.010990  0.003662 -0.013180      78.69    -74.480       2.0\n80  -0.002686  0.010740  0.011960  0.013430     104.00     48.300       3.0\n341 -0.007080  0.009033  0.016600 -0.000977     128.10     -3.366       4.0\n</code></pre>\n<p>If I try this:</p>\n<pre><code>df['ch1_log_mag']=20*np.log10(np.abs(complex(df.ch1_real,df.ch1_imag)))\n</code></pre>\n<p>I get error: &quot;TypeError: cannot convert the series to &lt;class 'float'&gt;&quot;, because I think cmath.complex cannot work on an array.</p>\n<p>So I then experimented using loc to pick out the first element of ch1_real, for example, to then work out how use it to accomplish what I'm trying to do, but couldn't figure out how to do it:</p>\n<pre><code>df.loc[0,df['ch1_real']]\n</code></pre>\n<p>This produces a KeyError.</p>\n<p>Brute forcing it works,</p>\n<pre><code>df['ch1_log_mag'] = 20 * np.log10(np.sqrt(df.ch1_real**2+ df.ch1_imag**2))\n</code></pre>\n<p>but, I believe it is more legible to use np.abs to get the magnitude, plus I'm more interested in understanding how dataframes and indexing dataframes work and why what I initially attempted does not work.</p>\n<p>btw, what is the difference between df.ch1_real and df['ch1_real'] ?  When do I use one vs. the other?</p>\n<p><strong>Edit</strong>:  more attempts at solution\nI tried using apply, since my understanding is that it &quot;applies&quot; the function passed to it to each row (by default):</p>\n<pre><code>df.apply(complex(df['ch1_real'], df['ch1_imag']))\n</code></pre>\n<p>but this generates the same TypeError, since I think the issue is that complex cannot work on Series.  Perhaps if I cast the series to float?</p>\n<p>After reading this <a href=\"https://stackoverflow.com/questions/25952790/convert-pandas-series-from-dtype-object-to-float-and-errors-to-nans\">post</a>, I tried using pd.to_numeric to convert a series to  type float:</p>\n<pre><code> dfUnique.apply(complex(pd.to_numeric(dfUnique['ch1_real'],errors='coerce'), pd.to_numeric(dfUnique['ch1_imag'],errors='coerce')))\n</code></pre>\n<p>to no avail.</p>\n",
        "formatted_input": {
            "qid": 65738704,
            "link": "https://stackoverflow.com/questions/65738704/combining-real-and-imag-columns-in-dataframe-into-complex-number-to-obtain-magni",
            "question": {
                "title": "combining real and imag columns in dataframe into complex number to obtain magnitude using np.abs",
                "ques_desc": "I have a data frame that has complex numbers split into a real and an imaginary column. I want to add a column (2, actually, one for each channel) to the dataframe that computes the log magnitude: If I try this: I get error: \"TypeError: cannot convert the series to <class 'float'>\", because I think cmath.complex cannot work on an array. So I then experimented using loc to pick out the first element of ch1_real, for example, to then work out how use it to accomplish what I'm trying to do, but couldn't figure out how to do it: This produces a KeyError. Brute forcing it works, but, I believe it is more legible to use np.abs to get the magnitude, plus I'm more interested in understanding how dataframes and indexing dataframes work and why what I initially attempted does not work. btw, what is the difference between df.ch1_real and df['ch1_real'] ? When do I use one vs. the other? Edit: more attempts at solution I tried using apply, since my understanding is that it \"applies\" the function passed to it to each row (by default): but this generates the same TypeError, since I think the issue is that complex cannot work on Series. Perhaps if I cast the series to float? After reading this post, I tried using pd.to_numeric to convert a series to type float: to no avail. "
            },
            "io": [
                " `    ch1_real  ch1_imag  ch2_real  ch2_imag  ch1_phase  ch2_phase  distance\n79   0.011960 -0.003418  0.005127 -0.019530     -15.95    -75.290       0.0\n78  -0.009766 -0.005371 -0.015870  0.010010    -151.20    147.800       1.0\n343  0.002197  0.010990  0.003662 -0.013180      78.69    -74.480       2.0\n80  -0.002686  0.010740  0.011960  0.013430     104.00     48.300       3.0\n341 -0.007080  0.009033  0.016600 -0.000977     128.10     -3.366       4.0\n",
                "df['ch1_log_mag'] = 20 * np.log10(np.sqrt(df.ch1_real**2+ df.ch1_imag**2))\n"
            ],
            "answer": {
                "ans_desc": "You can do simple multiplication with which denotes the complex number , see imaginary literals: doesn't work as it needs a float argument, not a whole series. is not a valid expression, as the second argument must be a string, not a series (df.loc[79,'ch1_real'] would work for accessing an element). If you want to use it should be but as apply is just a disguised loop over the rows of the dataframe it's not recommended performancewise. There's no difference between and , it's a matter of personal preference. If your column name contains spaces or dots or the like you must use the latter form however. ",
                "code": [
                    "df['ch1_log_mag'] = 20 * np.log10((df.ch1_real + 1j * df.ch1_imag).abs())\n",
                    "20 * np.log10(df.apply(lambda x: complex(x.ch1_real, x.ch1_imag), 1).abs())"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 109,
            "user_id": 4843627,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-jU8LcnYQ-Mg/AAAAAAAAAAI/AAAAAAAAACw/I4KCeD0NHfs/photo.jpg?sz=128",
            "display_name": "Gustavo Rangel",
            "link": "https://stackoverflow.com/users/4843627/gustavo-rangel"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 65722115,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1610639080,
        "creation_date": 1610638189,
        "last_edit_date": 1610639028,
        "question_id": 65721916,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65721916/transform-a-pandas-dataframe-need-for-a-more-efficient-solution",
        "title": "Transform a pandas dataframe: need for a more efficient solution",
        "body": "<p>I have a dataframe indexed by dates from a certain period. My columns are predictions about the value of a variable by the end of a given year. My original dataframe looks something like this:</p>\n<pre><code>            2016  2017  2018\n2016-01-01   0.0     1   NaN\n2016-07-01   1.0     1   4.1\n2017-01-01   NaN     5   3.0\n2017-07-01   NaN     2   2.0\n</code></pre>\n<p>where NaN means that the prediction does not exist for that given year.</p>\n<p>Since I am working with 20+ years and most predictions are for the next 2-3 years, my real dataframe has 20+ columns mostly containing <code>NaN</code> values. For instance, the column for the year 2005 has predictions made in 2003-2005, but in the range 2006-2020 it's all <code>NaN</code>.</p>\n<p>I would like to transform my dataframe to something like this:</p>\n<pre><code>            Y_0  Y_1  Y_2\n2016-01-01    0    1  NaN\n2016-07-01    1    1  4.1\n2017-01,01    5    3  NaN\n2017-07-01    2    2  NaN\n</code></pre>\n<p>where <code>Y_j</code> represents the prediction made for the <code>year = index.year + j</code>. This way, I would have a dataframe with only 4 columns (Y_0, Y_1, Y_2, Y_3).</p>\n<p>I actually achieved this, but in what I think it is a very inefficient way:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfor i in range(4):\n    df[f'Y_{i}'] = numpy.nan  # create columns [Y_0, Y_1, Y_2, Y_3]\n\nfor index, row in df.iterrows():  # iterate through each row of df\n    \n    for year in row.dropna().index:  # iterate through each year where a prediction exists\n        \n        year_diff = int(year) - index.year # get the difference between the years for which the prediction was made and when it was made (possible values: 0, 1, 2 or 3)\n        \n        df.loc[index, f'Y_{year_diff}'] = df.loc[index, year]  # set  the values for the columns 'Y_0', 'Y_1', 'Y_2' and 'Y_3' cell by cell.\n\n        df = df.iloc[:, -4:]  # delete all but the new columns\n</code></pre>\n<p>For a dataframe with only 1000 rows, this is taking almost 3 seconds to run. Can anyone think of a better solution?</p>\n",
        "answer_body": "<p>You could use <code>melt</code> to convert it to the long format then pivot back based on the year differences.</p>\n<p>Using your DataFrame as an example:</p>\n<pre><code>df = pd.DataFrame({'date':[datetime.date(2016, 1, 1), datetime.date(2016, 7, 1),\n                      datetime.date(2017, 1, 1), datetime.date(2017, 7, 1)],\n             2016:[0,1,np.nan,np.nan],\n             2017:[1,1,5,2],\n             2018:[np.nan, 4.1, 3, 2]})\ndf = df.melt(id_vars = 'date', value_vars = [2016, 2017, 2018], var_name='prediction_year', value_name='prediction')\n</code></pre>\n<p>Long format:</p>\n<pre><code>    date        prediction_year prediction\n0   2016-01-01  2016    0.0\n1   2016-07-01  2016    1.0\n2   2017-01-01  2016    NaN\n3   2017-07-01  2016    NaN\n4   2016-01-01  2017    1.0\n5   2016-07-01  2017    1.0\n6   2017-01-01  2017    5.0\n7   2017-07-01  2017    2.0\n8   2016-01-01  2018    NaN\n9   2016-07-01  2018    4.1\n10  2017-01-01  2018    3.0\n11  2017-07-01  2018    2.0\n</code></pre>\n<p>Convert back to the desired wide format:</p>\n<pre><code>df['year'] = pd.to_datetime(df['date']).dt.year\ndf['dt'] = df['prediction_year'] - df['year']\ndf = df.pivot(index = 'date', columns='dt', values='prediction').dropna(axis = 1, how = 'all').add_prefix('Y_')\n</code></pre>\n<pre><code>            Y_0 Y_1 Y_2\ndate            \n2016-01-01  0.0 1.0 NaN\n2016-07-01  1.0 1.0 4.1\n2017-01-01  5.0 3.0 NaN\n2017-07-01  2.0 2.0 NaN\n</code></pre>\n",
        "question_body": "<p>I have a dataframe indexed by dates from a certain period. My columns are predictions about the value of a variable by the end of a given year. My original dataframe looks something like this:</p>\n<pre><code>            2016  2017  2018\n2016-01-01   0.0     1   NaN\n2016-07-01   1.0     1   4.1\n2017-01-01   NaN     5   3.0\n2017-07-01   NaN     2   2.0\n</code></pre>\n<p>where NaN means that the prediction does not exist for that given year.</p>\n<p>Since I am working with 20+ years and most predictions are for the next 2-3 years, my real dataframe has 20+ columns mostly containing <code>NaN</code> values. For instance, the column for the year 2005 has predictions made in 2003-2005, but in the range 2006-2020 it's all <code>NaN</code>.</p>\n<p>I would like to transform my dataframe to something like this:</p>\n<pre><code>            Y_0  Y_1  Y_2\n2016-01-01    0    1  NaN\n2016-07-01    1    1  4.1\n2017-01,01    5    3  NaN\n2017-07-01    2    2  NaN\n</code></pre>\n<p>where <code>Y_j</code> represents the prediction made for the <code>year = index.year + j</code>. This way, I would have a dataframe with only 4 columns (Y_0, Y_1, Y_2, Y_3).</p>\n<p>I actually achieved this, but in what I think it is a very inefficient way:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfor i in range(4):\n    df[f'Y_{i}'] = numpy.nan  # create columns [Y_0, Y_1, Y_2, Y_3]\n\nfor index, row in df.iterrows():  # iterate through each row of df\n    \n    for year in row.dropna().index:  # iterate through each year where a prediction exists\n        \n        year_diff = int(year) - index.year # get the difference between the years for which the prediction was made and when it was made (possible values: 0, 1, 2 or 3)\n        \n        df.loc[index, f'Y_{year_diff}'] = df.loc[index, year]  # set  the values for the columns 'Y_0', 'Y_1', 'Y_2' and 'Y_3' cell by cell.\n\n        df = df.iloc[:, -4:]  # delete all but the new columns\n</code></pre>\n<p>For a dataframe with only 1000 rows, this is taking almost 3 seconds to run. Can anyone think of a better solution?</p>\n",
        "formatted_input": {
            "qid": 65721916,
            "link": "https://stackoverflow.com/questions/65721916/transform-a-pandas-dataframe-need-for-a-more-efficient-solution",
            "question": {
                "title": "Transform a pandas dataframe: need for a more efficient solution",
                "ques_desc": "I have a dataframe indexed by dates from a certain period. My columns are predictions about the value of a variable by the end of a given year. My original dataframe looks something like this: where NaN means that the prediction does not exist for that given year. Since I am working with 20+ years and most predictions are for the next 2-3 years, my real dataframe has 20+ columns mostly containing values. For instance, the column for the year 2005 has predictions made in 2003-2005, but in the range 2006-2020 it's all . I would like to transform my dataframe to something like this: where represents the prediction made for the . This way, I would have a dataframe with only 4 columns (Y_0, Y_1, Y_2, Y_3). I actually achieved this, but in what I think it is a very inefficient way: For a dataframe with only 1000 rows, this is taking almost 3 seconds to run. Can anyone think of a better solution? "
            },
            "io": [
                "            2016  2017  2018\n2016-01-01   0.0     1   NaN\n2016-07-01   1.0     1   4.1\n2017-01-01   NaN     5   3.0\n2017-07-01   NaN     2   2.0\n",
                "            Y_0  Y_1  Y_2\n2016-01-01    0    1  NaN\n2016-07-01    1    1  4.1\n2017-01,01    5    3  NaN\n2017-07-01    2    2  NaN\n"
            ],
            "answer": {
                "ans_desc": "You could use to convert it to the long format then pivot back based on the year differences. Using your DataFrame as an example: Long format: Convert back to the desired wide format: ",
                "code": [
                    "df['year'] = pd.to_datetime(df['date']).dt.year\ndf['dt'] = df['prediction_year'] - df['year']\ndf = df.pivot(index = 'date', columns='dt', values='prediction').dropna(axis = 1, how = 'all').add_prefix('Y_')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 165,
            "user_id": 7676365,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/481646e71e405560984ec2f880fb8676?s=128&d=identicon&r=PG&f=1",
            "display_name": "314mip",
            "link": "https://stackoverflow.com/users/7676365/314mip"
        },
        "is_answered": true,
        "view_count": 67,
        "accepted_answer_id": 65694234,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1610539608,
        "creation_date": 1610499122,
        "last_edit_date": 1610533657,
        "question_id": 65694203,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65694203/how-to-concat-two-or-more-data-frames-with-different-columns-names-in-pandas",
        "title": "How to concat two or more data frames with different columns names in pandas",
        "body": "<p>I have hundreds csv files and I need join it to one file. I have it all load as pandas dataframes. Sample dataframes:</p>\n<pre><code>df1 = pd.DataFrame({'a':['e1','e1','e1'],'x':[4,5,6],'y':[7,8,9]})\ndf2 = pd.DataFrame({'a':['e2','e2','e2'],'x':[13,14,15],'y':[16,17,18], 'z':[100,101,102]})\n</code></pre>\n<p>I need this output:</p>\n<pre><code>    a   x   y    z\n0  e1   4   7     \n1  e1   5   8     \n2  e1   6   9     \n3  e2  13  16  100\n4  e2  14  17  101\n5  e2  15  18  102\n</code></pre>\n<p>or</p>\n<pre><code>    a   x   y    z\n0  e1   4   7   na\n1  e1   5   8   na\n2  e1   6   9   na\n3  e2  13  16  100\n4  e2  14  17  101\n5  e2  15  18  102\n</code></pre>\n<p>How can I do that?\nThanks</p>\n<p>EDIT:</p>\n<p>I have cca 500 csv files, this is my code to make one file from them:</p>\n<pre><code>import glob\nimport pandas as pd\n\npath = r'C:/Users/Miro/data hist'\nall_files = glob.glob(path + &quot;/*.csv&quot;)\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, sep='delimiter', header=None)\n    li.append(df)\n\nframe = pd.concat(li, axis=0, ignore_index=True)\n\nframe.to_csv( &quot;full.csv&quot;, index=False, encoding='utf-8-sig')\n</code></pre>\n",
        "answer_body": "<p>This should work</p>\n<pre><code>df1 = pd.DataFrame({'a':['e1','e1','e1'],'x':[4,5,6],'y':[7,8,9]})\ndf2 = pd.DataFrame({'a':['e2','e2','e2'],'x':[13,14,15],'y':[16,17,18], 'z':[100,101,102]})\nnewdf = df1.append(df2, ignore_index=True)\n</code></pre>\n<pre><code>    a   x   y      z\n0  e1   4   7    NaN\n1  e1   5   8    NaN\n2  e1   6   9    NaN\n3  e2  13  16  100.0\n4  e2  14  17  101.0\n5  e2  15  18  102.0\n</code></pre>\n<p>Or if you really want <code>na</code> values rather than <code>NaN</code> you could do</p>\n<pre><code>newdf = df1.append(df2, ignore_index=True).fillna(&quot;na&quot;)\n</code></pre>\n<pre><code>    a   x   y    z\n0  e1   4   7   na\n1  e1   5   8   na\n2  e1   6   9   na\n3  e2  13  16  100\n4  e2  14  17  101\n5  e2  15  18  102\n</code></pre>\n<p>To get this to work in your edited question:</p>\n<pre><code>import glob\nimport pandas as pd\n\npath = r'C:/Users/Miro/data hist'\nall_files = glob.glob(path + &quot;/*.csv&quot;)\n\nli = pd.DataFrame()\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, sep='delimiter', header=None)\n    li = li.append(df, ignore_index=True)\n\n\nli.to_csv( &quot;full.csv&quot;, index=False, encoding='utf-8-sig')\n</code></pre>\n",
        "question_body": "<p>I have hundreds csv files and I need join it to one file. I have it all load as pandas dataframes. Sample dataframes:</p>\n<pre><code>df1 = pd.DataFrame({'a':['e1','e1','e1'],'x':[4,5,6],'y':[7,8,9]})\ndf2 = pd.DataFrame({'a':['e2','e2','e2'],'x':[13,14,15],'y':[16,17,18], 'z':[100,101,102]})\n</code></pre>\n<p>I need this output:</p>\n<pre><code>    a   x   y    z\n0  e1   4   7     \n1  e1   5   8     \n2  e1   6   9     \n3  e2  13  16  100\n4  e2  14  17  101\n5  e2  15  18  102\n</code></pre>\n<p>or</p>\n<pre><code>    a   x   y    z\n0  e1   4   7   na\n1  e1   5   8   na\n2  e1   6   9   na\n3  e2  13  16  100\n4  e2  14  17  101\n5  e2  15  18  102\n</code></pre>\n<p>How can I do that?\nThanks</p>\n<p>EDIT:</p>\n<p>I have cca 500 csv files, this is my code to make one file from them:</p>\n<pre><code>import glob\nimport pandas as pd\n\npath = r'C:/Users/Miro/data hist'\nall_files = glob.glob(path + &quot;/*.csv&quot;)\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, sep='delimiter', header=None)\n    li.append(df)\n\nframe = pd.concat(li, axis=0, ignore_index=True)\n\nframe.to_csv( &quot;full.csv&quot;, index=False, encoding='utf-8-sig')\n</code></pre>\n",
        "formatted_input": {
            "qid": 65694203,
            "link": "https://stackoverflow.com/questions/65694203/how-to-concat-two-or-more-data-frames-with-different-columns-names-in-pandas",
            "question": {
                "title": "How to concat two or more data frames with different columns names in pandas",
                "ques_desc": "I have hundreds csv files and I need join it to one file. I have it all load as pandas dataframes. Sample dataframes: I need this output: or How can I do that? Thanks EDIT: I have cca 500 csv files, this is my code to make one file from them: "
            },
            "io": [
                "    a   x   y    z\n0  e1   4   7     \n1  e1   5   8     \n2  e1   6   9     \n3  e2  13  16  100\n4  e2  14  17  101\n5  e2  15  18  102\n",
                "    a   x   y    z\n0  e1   4   7   na\n1  e1   5   8   na\n2  e1   6   9   na\n3  e2  13  16  100\n4  e2  14  17  101\n5  e2  15  18  102\n"
            ],
            "answer": {
                "ans_desc": "This should work Or if you really want values rather than you could do To get this to work in your edited question: ",
                "code": [
                    "df1 = pd.DataFrame({'a':['e1','e1','e1'],'x':[4,5,6],'y':[7,8,9]})\ndf2 = pd.DataFrame({'a':['e2','e2','e2'],'x':[13,14,15],'y':[16,17,18], 'z':[100,101,102]})\nnewdf = df1.append(df2, ignore_index=True)\n",
                    "import glob\nimport pandas as pd\n\npath = r'C:/Users/Miro/data hist'\nall_files = glob.glob(path + \"/*.csv\")\n\nli = pd.DataFrame()\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, sep='delimiter', header=None)\n    li = li.append(df, ignore_index=True)\n\n\nli.to_csv( \"full.csv\", index=False, encoding='utf-8-sig')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2153,
            "user_id": 2515265,
            "user_type": "registered",
            "accept_rate": 92,
            "profile_image": "https://www.gravatar.com/avatar/a9d74d215ddc8239bd4fd0765375fda2?s=128&d=identicon&r=PG",
            "display_name": "Javide",
            "link": "https://stackoverflow.com/users/2515265/javide"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 65693134,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1610493002,
        "creation_date": 1610490425,
        "last_edit_date": 1610491228,
        "question_id": 65692969,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65692969/how-to-spread-a-key-value-pair-across-multiple-columns-and-flatten-the-matrix-ba",
        "title": "How to spread a key-value pair across multiple columns and flatten the matrix based on another column?",
        "body": "<p>Using Pandas 1.2.0, I want to transform this dataframe</p>\n<pre><code>df = pd.DataFrame({\n'a': ['x_1', 'x_1', 'x_1', 'x_1', 'x_1', 'j_2', 'j_2', 'j_2', 'j_2', ],\n'b': [1, 2, 3, 4, 5, 1, 2, 3, 5],\n'c': [6, 3, 0, 1, 3.4, 4.5, 0.1, 0.2, 0.88]})\n</code></pre>\n<p>where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively:</p>\n<pre><code>     a  b     c\n0  x_1  1  6.00\n1  x_1  2  3.00\n2  x_1  3  0.00\n3  x_1  4  1.00\n4  x_1  5  3.40\n5  j_2  1  4.50\n6  j_2  2  0.10\n7  j_2  3  0.20\n8  j_2  5  0.88\n</code></pre>\n<p>into:</p>\n<pre><code>     a    1    2    3    4     5\n0  x_1  6.0  3.0  0.0  1.0  3.40\n1  j_2  4.5  0.1  0.2  NaN  0.88\n</code></pre>\n<p>My attempt:</p>\n<pre><code>df.set_index(['b', 'a'], append=True).unstack('b')\n\n         c                     \nb        1    2    3    4     5\n  a                            \n0 x_1  6.0  NaN  NaN  NaN   NaN\n1 x_1  NaN  3.0  NaN  NaN   NaN\n2 x_1  NaN  NaN  0.0  NaN   NaN\n3 x_1  NaN  NaN  NaN  1.0   NaN\n4 x_1  NaN  NaN  NaN  NaN  3.40\n5 j_2  4.5  NaN  NaN  NaN   NaN\n6 j_2  NaN  0.1  NaN  NaN   NaN\n7 j_2  NaN  NaN  0.2  NaN   NaN\n8 j_2  NaN  NaN  NaN  NaN  0.88\n</code></pre>\n<p>What should I do next to flatten the diagonals of these sub-matrices and group by 'a'?</p>\n",
        "answer_body": "<p>EDIT: A typo in the sample data made <code>pivot</code> not work because of duplicates. <code>pivot</code> should work as well <code>df = df.pivot(index='a', columns='b', values='c')</code> or <code>df = df.pivot('a', 'b', 'c')</code>:</p>\n<p>Tested on your specific version.</p>\n<pre><code>#pip install pandas==1.2.0\n#both pivot and pivot_table should work on this version. However, this code for &quot;pivot&quot; would NOT work on earlier versions of pandas. Not sure the exact version but I think it was fixed in 2019.\nimport pandas as pd\ndf = pd.DataFrame({\n'a': ['x_1', 'x_1', 'x_1', 'x_1', 'x_1', 'j_2', 'j_2', 'j_2', 'j_2', ],\n'b': [1, 2, 3, 4, 5, 1, 2, 3, 5],\n'c': [6, 3, 0, 1, 3.4, 4.5, 0.1, 0.2, 0.88]})\ndf = df.pivot_table(index='a', columns='b', values='c')\ndf\nOut[1]: \nb      1    2    3    4     5\na                            \nj_2  4.5  0.1  0.2  NaN  0.88\nx_1  6.0  3.0  0.0  1.0  3.40\n</code></pre>\n",
        "question_body": "<p>Using Pandas 1.2.0, I want to transform this dataframe</p>\n<pre><code>df = pd.DataFrame({\n'a': ['x_1', 'x_1', 'x_1', 'x_1', 'x_1', 'j_2', 'j_2', 'j_2', 'j_2', ],\n'b': [1, 2, 3, 4, 5, 1, 2, 3, 5],\n'c': [6, 3, 0, 1, 3.4, 4.5, 0.1, 0.2, 0.88]})\n</code></pre>\n<p>where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively:</p>\n<pre><code>     a  b     c\n0  x_1  1  6.00\n1  x_1  2  3.00\n2  x_1  3  0.00\n3  x_1  4  1.00\n4  x_1  5  3.40\n5  j_2  1  4.50\n6  j_2  2  0.10\n7  j_2  3  0.20\n8  j_2  5  0.88\n</code></pre>\n<p>into:</p>\n<pre><code>     a    1    2    3    4     5\n0  x_1  6.0  3.0  0.0  1.0  3.40\n1  j_2  4.5  0.1  0.2  NaN  0.88\n</code></pre>\n<p>My attempt:</p>\n<pre><code>df.set_index(['b', 'a'], append=True).unstack('b')\n\n         c                     \nb        1    2    3    4     5\n  a                            \n0 x_1  6.0  NaN  NaN  NaN   NaN\n1 x_1  NaN  3.0  NaN  NaN   NaN\n2 x_1  NaN  NaN  0.0  NaN   NaN\n3 x_1  NaN  NaN  NaN  1.0   NaN\n4 x_1  NaN  NaN  NaN  NaN  3.40\n5 j_2  4.5  NaN  NaN  NaN   NaN\n6 j_2  NaN  0.1  NaN  NaN   NaN\n7 j_2  NaN  NaN  0.2  NaN   NaN\n8 j_2  NaN  NaN  NaN  NaN  0.88\n</code></pre>\n<p>What should I do next to flatten the diagonals of these sub-matrices and group by 'a'?</p>\n",
        "formatted_input": {
            "qid": 65692969,
            "link": "https://stackoverflow.com/questions/65692969/how-to-spread-a-key-value-pair-across-multiple-columns-and-flatten-the-matrix-ba",
            "question": {
                "title": "How to spread a key-value pair across multiple columns and flatten the matrix based on another column?",
                "ques_desc": "Using Pandas 1.2.0, I want to transform this dataframe where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively: into: My attempt: What should I do next to flatten the diagonals of these sub-matrices and group by 'a'? "
            },
            "io": [
                "     a  b     c\n0  x_1  1  6.00\n1  x_1  2  3.00\n2  x_1  3  0.00\n3  x_1  4  1.00\n4  x_1  5  3.40\n5  j_2  1  4.50\n6  j_2  2  0.10\n7  j_2  3  0.20\n8  j_2  5  0.88\n",
                "     a    1    2    3    4     5\n0  x_1  6.0  3.0  0.0  1.0  3.40\n1  j_2  4.5  0.1  0.2  NaN  0.88\n"
            ],
            "answer": {
                "ans_desc": "EDIT: A typo in the sample data made not work because of duplicates. should work as well or : Tested on your specific version. ",
                "code": [
                    "df = df.pivot(index='a', columns='b', values='c')",
                    "df = df.pivot('a', 'b', 'c')"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1258,
            "user_id": 14739759,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/601b1ef61f810049ddd0f6b0eb9aa860?s=128&d=identicon&r=PG&f=1",
            "display_name": "anurag",
            "link": "https://stackoverflow.com/users/14739759/anurag"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 65669534,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1610440081,
        "creation_date": 1610377956,
        "last_edit_date": 1610440081,
        "question_id": 65669445,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65669445/python-pandas-get-row-indices-for-a-particular-value-in-a-column",
        "title": "Python - Pandas: get row indices for a particular value in a column",
        "body": "<p>Given a pandas dataframe, is there a way to get the indices of rows where a column has particular values?</p>\n<p>Consider the following toy example:</p>\n<p><strong>CSV</strong> (<em>save as test1.csv</em>)</p>\n<pre class=\"lang-csv prettyprint-override\"><code>id,val1,val2\n1,20,A\n1,19,A\n1,23,B\n2,10,B\n2,10,A\n2,14,A\n</code></pre>\n<p>What I currently have is this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.read_csv('test1.csv')\nprint(df)\n\nprint(df[df['id']==1].index.to_list())\nprint(df[df['id']==2].index.to_list())\n</code></pre>\n<pre class=\"lang-bash prettyprint-override\"><code>   id  val1 val2\n0   1    20    A\n1   1    19    A\n2   1    23    B\n3   2    10    B\n4   2    10    A\n5   2    14    A\n[0, 1, 2]\n[3, 4, 5]\n</code></pre>\n<p>Is there an option/functionality that can give me something like the following?\n(I want to be able to do this for large value lists, <strong>fast</strong>!)</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(df['id'].someFn([1,2]))\n</code></pre>\n<p>Desired output:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>{1:[0,1,2], 2:[3,4,5]}\n</code></pre>\n",
        "answer_body": "<p>Try <code>groupby</code>:</p>\n<pre><code>{k: list(d.index) for k, d in df.groupby('id')}\n</code></pre>\n<p>Output:</p>\n<pre><code>{1: [0, 1, 2], 2: [3, 4, 5]}\n</code></pre>\n",
        "question_body": "<p>Given a pandas dataframe, is there a way to get the indices of rows where a column has particular values?</p>\n<p>Consider the following toy example:</p>\n<p><strong>CSV</strong> (<em>save as test1.csv</em>)</p>\n<pre class=\"lang-csv prettyprint-override\"><code>id,val1,val2\n1,20,A\n1,19,A\n1,23,B\n2,10,B\n2,10,A\n2,14,A\n</code></pre>\n<p>What I currently have is this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.read_csv('test1.csv')\nprint(df)\n\nprint(df[df['id']==1].index.to_list())\nprint(df[df['id']==2].index.to_list())\n</code></pre>\n<pre class=\"lang-bash prettyprint-override\"><code>   id  val1 val2\n0   1    20    A\n1   1    19    A\n2   1    23    B\n3   2    10    B\n4   2    10    A\n5   2    14    A\n[0, 1, 2]\n[3, 4, 5]\n</code></pre>\n<p>Is there an option/functionality that can give me something like the following?\n(I want to be able to do this for large value lists, <strong>fast</strong>!)</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(df['id'].someFn([1,2]))\n</code></pre>\n<p>Desired output:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>{1:[0,1,2], 2:[3,4,5]}\n</code></pre>\n",
        "formatted_input": {
            "qid": 65669445,
            "link": "https://stackoverflow.com/questions/65669445/python-pandas-get-row-indices-for-a-particular-value-in-a-column",
            "question": {
                "title": "Python - Pandas: get row indices for a particular value in a column",
                "ques_desc": "Given a pandas dataframe, is there a way to get the indices of rows where a column has particular values? Consider the following toy example: CSV (save as test1.csv) What I currently have is this: Is there an option/functionality that can give me something like the following? (I want to be able to do this for large value lists, fast!) Desired output: "
            },
            "io": [
                "id,val1,val2\n1,20,A\n1,19,A\n1,23,B\n2,10,B\n2,10,A\n2,14,A\n",
                "   id  val1 val2\n0   1    20    A\n1   1    19    A\n2   1    23    B\n3   2    10    B\n4   2    10    A\n5   2    14    A\n[0, 1, 2]\n[3, 4, 5]\n"
            ],
            "answer": {
                "ans_desc": "Try : Output: ",
                "code": [
                    "{k: list(d.index) for k, d in df.groupby('id')}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "multi-index"
        ],
        "owner": {
            "reputation": 27410,
            "user_id": 5820814,
            "user_type": "registered",
            "accept_rate": 43,
            "profile_image": "https://graph.facebook.com/1240609682619989/picture?type=large",
            "display_name": "Mayank Porwal",
            "link": "https://stackoverflow.com/users/5820814/mayank-porwal"
        },
        "is_answered": true,
        "view_count": 906,
        "accepted_answer_id": 65679487,
        "answer_count": 3,
        "score": 10,
        "last_activity_date": 1610437112,
        "creation_date": 1610435598,
        "question_id": 65679439,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65679439/pandas-add-an-empty-row-after-every-index-in-a-multiindex-dataframe",
        "title": "Pandas: Add an empty row after every index in a MultiIndex dataframe",
        "body": "<p>Consider below <code>df</code>:</p>\n<pre><code>              IA1  IA2  IA3\nName Subject               \nAbc  DS        45   43   34\n     DMS       43   23   45\n     ADA       32   46   36\nBcd  BA        45   35   37\n     EAD       23   45   12\n     DS        23   35   43\nCdf  EAD       34   33   23\n     ADA       12   34   25\n</code></pre>\n<p>How can I add an empty row after each <code>Name</code> index?</p>\n<p><strong>Expected output:</strong></p>\n<pre><code>              IA1  IA2  IA3\nName Subject               \nAbc  DS        45   43   34\n     DMS       43   23   45\n     ADA       32   46   36\n\nBcd  BA        45   35   37\n     EAD       23   45   12\n     DS        23   35   43\n\nCdf  EAD       34   33   23\n     ADA       12   34   25\n     \n</code></pre>\n",
        "answer_body": "<p>Use custom function for add empty rows in <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.apply.html\" rel=\"noreferrer\"><code>GroupBy.apply</code></a>:</p>\n<pre><code>def f(x):\n    x.loc[('', ''), :] = ''\n    return x\n</code></pre>\n<p>Or:</p>\n<pre><code>def f(x):\n    return x.append(pd.DataFrame('', columns=df.columns, index=[(x.name, '')]))\n</code></pre>\n<hr />\n<pre><code>df = df.groupby(level=0, group_keys=False).apply(f)\nprint (df)\n             IA1 IA2 IA3\nName Subject            \nAbc  DS       45  43  34\n     DMS      43  23  45\n     ADA      32  46  36\n                        \nBcd  BA       45  35  37\n     EAD      23  45  12\n     DS       23  35  43\n                        \nCdf  EAD      34  33  23\n     ADA      12  34  25\n                        \n</code></pre>\n",
        "question_body": "<p>Consider below <code>df</code>:</p>\n<pre><code>              IA1  IA2  IA3\nName Subject               \nAbc  DS        45   43   34\n     DMS       43   23   45\n     ADA       32   46   36\nBcd  BA        45   35   37\n     EAD       23   45   12\n     DS        23   35   43\nCdf  EAD       34   33   23\n     ADA       12   34   25\n</code></pre>\n<p>How can I add an empty row after each <code>Name</code> index?</p>\n<p><strong>Expected output:</strong></p>\n<pre><code>              IA1  IA2  IA3\nName Subject               \nAbc  DS        45   43   34\n     DMS       43   23   45\n     ADA       32   46   36\n\nBcd  BA        45   35   37\n     EAD       23   45   12\n     DS        23   35   43\n\nCdf  EAD       34   33   23\n     ADA       12   34   25\n     \n</code></pre>\n",
        "formatted_input": {
            "qid": 65679439,
            "link": "https://stackoverflow.com/questions/65679439/pandas-add-an-empty-row-after-every-index-in-a-multiindex-dataframe",
            "question": {
                "title": "Pandas: Add an empty row after every index in a MultiIndex dataframe",
                "ques_desc": "Consider below : How can I add an empty row after each index? Expected output: "
            },
            "io": [
                "              IA1  IA2  IA3\nName Subject               \nAbc  DS        45   43   34\n     DMS       43   23   45\n     ADA       32   46   36\nBcd  BA        45   35   37\n     EAD       23   45   12\n     DS        23   35   43\nCdf  EAD       34   33   23\n     ADA       12   34   25\n",
                "              IA1  IA2  IA3\nName Subject               \nAbc  DS        45   43   34\n     DMS       43   23   45\n     ADA       32   46   36\n\nBcd  BA        45   35   37\n     EAD       23   45   12\n     DS        23   35   43\n\nCdf  EAD       34   33   23\n     ADA       12   34   25\n     \n"
            ],
            "answer": {
                "ans_desc": "Use custom function for add empty rows in : Or: ",
                "code": [
                    "def f(x):\n    x.loc[('', ''), :] = ''\n    return x\n",
                    "def f(x):\n    return x.append(pd.DataFrame('', columns=df.columns, index=[(x.name, '')]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "loops"
        ],
        "owner": {
            "reputation": 250,
            "user_id": 13906221,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/caf10cef4b7f7319565a87081431de10?s=128&d=identicon&r=PG&f=1",
            "display_name": "Aly",
            "link": "https://stackoverflow.com/users/13906221/aly"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 65669232,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1610378530,
        "creation_date": 1610376200,
        "last_edit_date": 1610378530,
        "question_id": 65668962,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65668962/loop-which-every-iteration-will-replace-one-column-from-a-dataframe-with-zeros",
        "title": "Loop which every iteration will replace one column from a dataframe with zeros",
        "body": "<p>I want to perform Sensitivity Analysis for classification models in Python.</p>\n<p>So I want to check how lack of every column will affect the metrics.\nI prepared function which returns metrics from original test set.</p>\n<pre><code>def score_metrics(model, \n                  X_test,\n                  y_test):\n\ny_pred = model.predict(X_test) #predicted values from oryginal dataset\n\ncm_orig = confusion_matrix(y_test, y_pred)\n\ntp = cm_orig[1, 1]\nfp = cm_orig[0, 1]\nfn = cm_orig[1, 0]\ntn = cm_orig[0, 0]\n\nscore_orig_precision = precision_score(y_test, y_pred)\nscore_orig_accuracy = accuracy_score(y_test, y_pred)\nscore_orig_recall = recall_score(y_test, y_pred)\nscore_orig_specificity = tn/(tn+fp)\nscore_orig_F1 = f1_score(y_test, y_pred)\n\nresults = {'Feature': 'orginal',\n           'Precision': score_orig_precision,\n           'Accuracy': score_orig_accuracy,\n           'Recall': score_orig_recall,\n           'Specificity': score_orig_specificity,\n           'F1 score': score_orig_F1}\n\nreturn results\n</code></pre>\n<p>I want perform the same but for X_test which with every iteration will have one column values replaced with 0.</p>\n<p>For example if this would be X_test:</p>\n<pre><code>    A   B   C   D   E\n    5   7   11  12  6\n   11   32  11  13  6\n</code></pre>\n<p>I would like to check those metrics for:</p>\n<pre><code>    A   B   C   D   E\n    0   7  11  12   6\n    0  32  11  13   6\n\n    A   B   C   D   E\n    5   0  11  12   6\n   11   0  11  13   6\n\n    A   B   C   D   E\n    5   7   0   12  6\n   11   32  0   13  6\n</code></pre>\n<p>and so on.\nAnd my question is to edit above code (or suggest something else) which help me to achieve it.\nLater I would like to have this outcome in Pandas DataFrame but it's enough to show me up to dictionary state.</p>\n",
        "answer_body": "<p>So from your example, you actually dont want to drop the column, just give 0 value during the iteration. Then you can use:</p>\n<pre><code>for c in df.columns:\n    newDF = df.copy(deep=True)\n    newDF[c] = 0\n    # Here you opperate with the new DF in this instance\n</code></pre>\n<p>One option to integer this in your existing code:</p>\n<pre><code>def getting_results(y_pred, y_test):\n    cm_orig = confusion_matrix(y_test, y_pred)\n\n    tp = cm_orig[1, 1]\n    fp = cm_orig[0, 1]\n    fn = cm_orig[1, 0]\n    tn = cm_orig[0, 0]\n\n    score_orig_precision = precision_score(y_test, y_pred)\n    score_orig_accuracy = accuracy_score(y_test, y_pred)\n    score_orig_recall = recall_score(y_test, y_pred)\n    score_orig_specificity = tn/(tn+fp)\n    score_orig_F1 = f1_score(y_test, y_pred)\n\n    results = {'Feature': 'orginal',\n               'Precision': score_orig_precision,\n               'Accuracy': score_orig_accuracy,\n               'Recall': score_orig_recall,\n               'Specificity': score_orig_specificity,\n               'F1 score': score_orig_F1}\n\n    return results\n\n\ndef score_metrics(model, X_test, y_test):\n\n    for c in X_test.columns:\n        newX_test = X_test.copy(deep=True)\n        newX_test[c] = 0\n        \n        y_pred = model.predict(newX_test) #predicted values from oryginal dataset\n        getting_results(y_pred, y_test)\n</code></pre>\n",
        "question_body": "<p>I want to perform Sensitivity Analysis for classification models in Python.</p>\n<p>So I want to check how lack of every column will affect the metrics.\nI prepared function which returns metrics from original test set.</p>\n<pre><code>def score_metrics(model, \n                  X_test,\n                  y_test):\n\ny_pred = model.predict(X_test) #predicted values from oryginal dataset\n\ncm_orig = confusion_matrix(y_test, y_pred)\n\ntp = cm_orig[1, 1]\nfp = cm_orig[0, 1]\nfn = cm_orig[1, 0]\ntn = cm_orig[0, 0]\n\nscore_orig_precision = precision_score(y_test, y_pred)\nscore_orig_accuracy = accuracy_score(y_test, y_pred)\nscore_orig_recall = recall_score(y_test, y_pred)\nscore_orig_specificity = tn/(tn+fp)\nscore_orig_F1 = f1_score(y_test, y_pred)\n\nresults = {'Feature': 'orginal',\n           'Precision': score_orig_precision,\n           'Accuracy': score_orig_accuracy,\n           'Recall': score_orig_recall,\n           'Specificity': score_orig_specificity,\n           'F1 score': score_orig_F1}\n\nreturn results\n</code></pre>\n<p>I want perform the same but for X_test which with every iteration will have one column values replaced with 0.</p>\n<p>For example if this would be X_test:</p>\n<pre><code>    A   B   C   D   E\n    5   7   11  12  6\n   11   32  11  13  6\n</code></pre>\n<p>I would like to check those metrics for:</p>\n<pre><code>    A   B   C   D   E\n    0   7  11  12   6\n    0  32  11  13   6\n\n    A   B   C   D   E\n    5   0  11  12   6\n   11   0  11  13   6\n\n    A   B   C   D   E\n    5   7   0   12  6\n   11   32  0   13  6\n</code></pre>\n<p>and so on.\nAnd my question is to edit above code (or suggest something else) which help me to achieve it.\nLater I would like to have this outcome in Pandas DataFrame but it's enough to show me up to dictionary state.</p>\n",
        "formatted_input": {
            "qid": 65668962,
            "link": "https://stackoverflow.com/questions/65668962/loop-which-every-iteration-will-replace-one-column-from-a-dataframe-with-zeros",
            "question": {
                "title": "Loop which every iteration will replace one column from a dataframe with zeros",
                "ques_desc": "I want to perform Sensitivity Analysis for classification models in Python. So I want to check how lack of every column will affect the metrics. I prepared function which returns metrics from original test set. I want perform the same but for X_test which with every iteration will have one column values replaced with 0. For example if this would be X_test: I would like to check those metrics for: and so on. And my question is to edit above code (or suggest something else) which help me to achieve it. Later I would like to have this outcome in Pandas DataFrame but it's enough to show me up to dictionary state. "
            },
            "io": [
                "    A   B   C   D   E\n    5   7   11  12  6\n   11   32  11  13  6\n",
                "    A   B   C   D   E\n    0   7  11  12   6\n    0  32  11  13   6\n\n    A   B   C   D   E\n    5   0  11  12   6\n   11   0  11  13   6\n\n    A   B   C   D   E\n    5   7   0   12  6\n   11   32  0   13  6\n"
            ],
            "answer": {
                "ans_desc": "So from your example, you actually dont want to drop the column, just give 0 value during the iteration. Then you can use: One option to integer this in your existing code: ",
                "code": [
                    "for c in df.columns:\n    newDF = df.copy(deep=True)\n    newDF[c] = 0\n    # Here you opperate with the new DF in this instance\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 301,
            "user_id": 14721684,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/2NNp1.jpg?s=128&g=1",
            "display_name": "Ella",
            "link": "https://stackoverflow.com/users/14721684/ella"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 65643233,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1610199511,
        "creation_date": 1610194685,
        "last_edit_date": 1610196752,
        "question_id": 65642545,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65642545/put-only-elements-into-a-list-with-a-certian-number",
        "title": "put only elements into a list with a certian number",
        "body": "<p><code>S_id</code> is the sales ID and <code>i_id</code> is the sold itemid. I would like to use all unique i_ids to find all purchases that have interacted with i_id. I also implemented this in the loop. What I would like that I only want to add something to the list when the <code>s_id</code> has more of a 1 item.</p>\n<p>How do I do that so that I only add the purchases to the list if it contains more than one item?</p>\n<pre><code>import pandas as pd\n\nd = {'s_id': [1, 2, 2, 2, 3, 4, 4, 4, 5, 5],\n     'i_id': [1, 1, 2, 3, 1, 4, 1, 2, 3, 5]}\ndf = pd.DataFrame(data=d)\n\nprint(df)\n\n\nnumers_i = df.i_id.unique().tolist()\n\nfor i in numers_i:\n  buyers = df[df.i_id.eq(i)].s_id.unique()\n  df_new = df[df.s_id.isin(buyers)]\n  list_new = df_new.groupby(&quot;s_id&quot;)['i_id'].apply(list).tolist()\n  print(list_new)\n</code></pre>\n<p>Output</p>\n<pre><code>[[1], [1, 2, 3], [1], [4, 1, 2]]\n[[1, 2, 3], [4, 1, 2]]\n[[1, 2, 3], [3, 5]]\n[[4, 1, 2]]\n[[3, 5]]\n</code></pre>\n<p>But what I want</p>\n<pre><code>[[REMOVED], [1, 2, 3], [REMOVED], [4, 1, 2]] \n[[1, 2, 3], [4, 1, 2]]\n[[1, 2, 3], [3, 5]]\n[[4, 1, 2]]\n[[3, 5]]\n</code></pre>\n<p><code>[REMOVED]</code> means that the element does not exist, I only wrote for a better understanding</p>\n",
        "answer_body": "<p>A slightly different approach with two <code>groupby</code>s. One for getting the items in an <code>s_id</code> and the other for grouping the purchases happened along with particular <code>i_id</code>s.</p>\n<p>Firstly, get the mapping of <code>s_id</code> to list of <code>i_id</code></p>\n<pre><code>map_dict = df.groupby('s_id')['i_id'].apply(list).to_dict()\n\nmap_dict\n{1: [1], 2: [1, 2, 3], 3: [1], 4: [4, 1, 2], 5: [3, 5]}\n</code></pre>\n<p>Then grouping by <code>i_id</code> to create list of list of items if length of &quot;list of items&quot; is greater than 1</p>\n<pre><code>def func(df):\n    return ([items for items in df['s_id'].map(map_dict) if len(items) &gt; 1])\n\ndf.groupby('i_id').apply(lambda x: func(x))\n\ni_id\n1    [[1, 2, 3], [4, 1, 2]]\n2    [[1, 2, 3], [4, 1, 2]]\n3    [[1, 2, 3], [3, 5]]   \n4    [[4, 1, 2]]           \n5    [[3, 5]]              \ndtype: object\n</code></pre>\n<p>Compared the timing, this approach (<code>6.71 ms \u00b1 91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)</code>) seems to be faster than approach in question (<code>12.3 ms \u00b1 201 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)</code>)</p>\n",
        "question_body": "<p><code>S_id</code> is the sales ID and <code>i_id</code> is the sold itemid. I would like to use all unique i_ids to find all purchases that have interacted with i_id. I also implemented this in the loop. What I would like that I only want to add something to the list when the <code>s_id</code> has more of a 1 item.</p>\n<p>How do I do that so that I only add the purchases to the list if it contains more than one item?</p>\n<pre><code>import pandas as pd\n\nd = {'s_id': [1, 2, 2, 2, 3, 4, 4, 4, 5, 5],\n     'i_id': [1, 1, 2, 3, 1, 4, 1, 2, 3, 5]}\ndf = pd.DataFrame(data=d)\n\nprint(df)\n\n\nnumers_i = df.i_id.unique().tolist()\n\nfor i in numers_i:\n  buyers = df[df.i_id.eq(i)].s_id.unique()\n  df_new = df[df.s_id.isin(buyers)]\n  list_new = df_new.groupby(&quot;s_id&quot;)['i_id'].apply(list).tolist()\n  print(list_new)\n</code></pre>\n<p>Output</p>\n<pre><code>[[1], [1, 2, 3], [1], [4, 1, 2]]\n[[1, 2, 3], [4, 1, 2]]\n[[1, 2, 3], [3, 5]]\n[[4, 1, 2]]\n[[3, 5]]\n</code></pre>\n<p>But what I want</p>\n<pre><code>[[REMOVED], [1, 2, 3], [REMOVED], [4, 1, 2]] \n[[1, 2, 3], [4, 1, 2]]\n[[1, 2, 3], [3, 5]]\n[[4, 1, 2]]\n[[3, 5]]\n</code></pre>\n<p><code>[REMOVED]</code> means that the element does not exist, I only wrote for a better understanding</p>\n",
        "formatted_input": {
            "qid": 65642545,
            "link": "https://stackoverflow.com/questions/65642545/put-only-elements-into-a-list-with-a-certian-number",
            "question": {
                "title": "put only elements into a list with a certian number",
                "ques_desc": " is the sales ID and is the sold itemid. I would like to use all unique i_ids to find all purchases that have interacted with i_id. I also implemented this in the loop. What I would like that I only want to add something to the list when the has more of a 1 item. How do I do that so that I only add the purchases to the list if it contains more than one item? Output But what I want means that the element does not exist, I only wrote for a better understanding "
            },
            "io": [
                "[[1], [1, 2, 3], [1], [4, 1, 2]]\n[[1, 2, 3], [4, 1, 2]]\n[[1, 2, 3], [3, 5]]\n[[4, 1, 2]]\n[[3, 5]]\n",
                "[[REMOVED], [1, 2, 3], [REMOVED], [4, 1, 2]] \n[[1, 2, 3], [4, 1, 2]]\n[[1, 2, 3], [3, 5]]\n[[4, 1, 2]]\n[[3, 5]]\n"
            ],
            "answer": {
                "ans_desc": "A slightly different approach with two s. One for getting the items in an and the other for grouping the purchases happened along with particular s. Firstly, get the mapping of to list of Then grouping by to create list of list of items if length of \"list of items\" is greater than 1 Compared the timing, this approach () seems to be faster than approach in question () ",
                "code": [
                    "6.71 ms \u00b1 91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)",
                    "12.3 ms \u00b1 201 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 409,
            "user_id": 9282379,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f87e8e93f3acfa0f3a9272ae20a0ee27?s=128&d=identicon&r=PG&f=1",
            "display_name": "sheel",
            "link": "https://stackoverflow.com/users/9282379/sheel"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 65601614,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1609959047,
        "creation_date": 1609957295,
        "question_id": 65601285,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65601285/how-to-fill-multiple-list-by-0s-in-pandas-data-frame",
        "title": "How to fill multiple list by 0&#39;s in Pandas data frame?",
        "body": "<p>I have Pandas data frame and I am trying to add 0's in those lists where numbers are missing.\nIn the below data frame, the max length of the list is 4 which is in the 3rd position. accordingly, I will add 0's to the remaining lists.</p>\n<p><strong>Input:</strong></p>\n<pre><code>        Lists\n0     [158, 202]\n1     [609, 405]\n2     [544, 20]\n3     [90, 346, 130, 202]\n4     [6]\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>        Lists\n0     [158, 202, 0, 0]\n1     [609, 405, 0, 0]\n2     [544, 20, 0, 0]\n3     [90, 346, 130, 202]\n4     [6, 0, 0, 0]\n</code></pre>\n",
        "answer_body": "<p>Another way to do this would be using apply with a lambda function -</p>\n<pre><code>maxlen = df['Lists'].str.len().max() #as suggested by Anky, better than an apply since vectorised\nf = lambda x: x + ([0] * (maxlen - len(x)))\n\ndf['Padded'] = df['Lists'].apply(f)\nprint(df)\n</code></pre>\n<pre><code>                 Lists               Padded\n0           [158, 202]     [158, 202, 0, 0]\n1           [609, 405]     [609, 405, 0, 0]\n2            [544, 20]      [544, 20, 0, 0]\n3  [90, 346, 130, 202]  [90, 346, 130, 202]\n4                  [6]         [6, 0, 0, 0]\n</code></pre>\n",
        "question_body": "<p>I have Pandas data frame and I am trying to add 0's in those lists where numbers are missing.\nIn the below data frame, the max length of the list is 4 which is in the 3rd position. accordingly, I will add 0's to the remaining lists.</p>\n<p><strong>Input:</strong></p>\n<pre><code>        Lists\n0     [158, 202]\n1     [609, 405]\n2     [544, 20]\n3     [90, 346, 130, 202]\n4     [6]\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>        Lists\n0     [158, 202, 0, 0]\n1     [609, 405, 0, 0]\n2     [544, 20, 0, 0]\n3     [90, 346, 130, 202]\n4     [6, 0, 0, 0]\n</code></pre>\n",
        "formatted_input": {
            "qid": 65601285,
            "link": "https://stackoverflow.com/questions/65601285/how-to-fill-multiple-list-by-0s-in-pandas-data-frame",
            "question": {
                "title": "How to fill multiple list by 0&#39;s in Pandas data frame?",
                "ques_desc": "I have Pandas data frame and I am trying to add 0's in those lists where numbers are missing. In the below data frame, the max length of the list is 4 which is in the 3rd position. accordingly, I will add 0's to the remaining lists. Input: Output: "
            },
            "io": [
                "        Lists\n0     [158, 202]\n1     [609, 405]\n2     [544, 20]\n3     [90, 346, 130, 202]\n4     [6]\n",
                "        Lists\n0     [158, 202, 0, 0]\n1     [609, 405, 0, 0]\n2     [544, 20, 0, 0]\n3     [90, 346, 130, 202]\n4     [6, 0, 0, 0]\n"
            ],
            "answer": {
                "ans_desc": "Another way to do this would be using apply with a lambda function - ",
                "code": [
                    "maxlen = df['Lists'].str.len().max() #as suggested by Anky, better than an apply since vectorised\nf = lambda x: x + ([0] * (maxlen - len(x)))\n\ndf['Padded'] = df['Lists'].apply(f)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "conditional-statements"
        ],
        "owner": {
            "reputation": 307,
            "user_id": 11530164,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/892c29c358c8f3b6bd2d298c95c24c88?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mario",
            "link": "https://stackoverflow.com/users/11530164/mario"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 65563927,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1609768797,
        "creation_date": 1609768546,
        "question_id": 65563873,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65563873/how-to-use-pandas-dataframe-as-condition-for-other-dataframe",
        "title": "How to use pandas dataframe as condition for other dataframe",
        "body": "<p>Say I have dataframe A:</p>\n<pre><code>      A B  C\nlower 1 0 -5\nupper 2 2  0\n</code></pre>\n<p>and dataframe B:</p>\n<pre><code>     A  B  C\nsa   5  1 -2\nsb   3  0  2\nsc   1 -5  1\n</code></pre>\n<p>How can I use dataframe A as a condition for dataframe B, so that df B's cells are within the <code>upper</code> and <code>lower</code> values of df A. The ideal output is this: (first cell is explanatory)</p>\n<pre><code>     A                     B      C\nsa   (5&gt;=1)&amp;(5&lt;=2)=False   True   True\nsb   False                 True   False\nsc   True                  False  False\n</code></pre>\n",
        "answer_body": "<p>You can chain mask with compare selected rows by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>DataFrame.loc</code></a> in <code>A</code> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ge.html\" rel=\"nofollow noreferrer\"><code>DataFrame.ge</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.le.html\" rel=\"nofollow noreferrer\"><code>DataFrame.le</code></a> by <code>&amp;</code> for bitwise <code>AND</code>:</p>\n<p>Notice - columns names have to match in both <code>DataFrame</code>s.</p>\n<pre><code>df = B.ge(A.loc['lower']) &amp; B.le(A.loc['upper'])\n#alternative\n#df = (B&gt;= A.loc['lower']) &amp; (B &lt;= A.loc['upper'])\nprint (df)\n        A      B      C\nsa  False   True   True\nsb  False   True  False\nsc   True  False  False\n</code></pre>\n",
        "question_body": "<p>Say I have dataframe A:</p>\n<pre><code>      A B  C\nlower 1 0 -5\nupper 2 2  0\n</code></pre>\n<p>and dataframe B:</p>\n<pre><code>     A  B  C\nsa   5  1 -2\nsb   3  0  2\nsc   1 -5  1\n</code></pre>\n<p>How can I use dataframe A as a condition for dataframe B, so that df B's cells are within the <code>upper</code> and <code>lower</code> values of df A. The ideal output is this: (first cell is explanatory)</p>\n<pre><code>     A                     B      C\nsa   (5&gt;=1)&amp;(5&lt;=2)=False   True   True\nsb   False                 True   False\nsc   True                  False  False\n</code></pre>\n",
        "formatted_input": {
            "qid": 65563873,
            "link": "https://stackoverflow.com/questions/65563873/how-to-use-pandas-dataframe-as-condition-for-other-dataframe",
            "question": {
                "title": "How to use pandas dataframe as condition for other dataframe",
                "ques_desc": "Say I have dataframe A: and dataframe B: How can I use dataframe A as a condition for dataframe B, so that df B's cells are within the and values of df A. The ideal output is this: (first cell is explanatory) "
            },
            "io": [
                "      A B  C\nlower 1 0 -5\nupper 2 2  0\n",
                "     A  B  C\nsa   5  1 -2\nsb   3  0  2\nsc   1 -5  1\n"
            ],
            "answer": {
                "ans_desc": "You can chain mask with compare selected rows by in with and by for bitwise : Notice - columns names have to match in both s. ",
                "code": [
                    "df = B.ge(A.loc['lower']) & B.le(A.loc['upper'])\n#alternative\n#df = (B>= A.loc['lower']) & (B <= A.loc['upper'])\nprint (df)\n        A      B      C\nsa  False   True   True\nsb  False   True  False\nsc   True  False  False\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 83,
            "user_id": 13892664,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GhCAyQlgZ-fde5le27cT7C19bbHQw8JEqD8jpnX=k-s128",
            "display_name": "Johar Inam Unnar",
            "link": "https://stackoverflow.com/users/13892664/johar-inam-unnar"
        },
        "is_answered": true,
        "view_count": 146,
        "accepted_answer_id": 65550119,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1609695031,
        "creation_date": 1609678162,
        "question_id": 65550028,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65550028/count-the-number-of-specific-values-in-multiple-columns-pandas",
        "title": "Count the number of specific values in multiple columns pandas",
        "body": "<p>I have a data frame:</p>\n<pre><code>A    B    C    D     E\n\n12  4.5  6.1   BUY  NaN\n12  BUY  BUY   5.6  NaN\nBUY  4.5  6.1  BUY  NaN\n12  4.5  6.1   0    NaN \n</code></pre>\n<p>I want to count the number of times 'BUY' appears in each row. Intended result:</p>\n<pre><code>A    B    C    D     E   score\n\n12  4.5  6.1   BUY  NaN    1\n12  BUY  BUY   5.6  NaN    2\n15  4.5  6.1  BUY   NaN    1\n12  4.5  6.1   0    NaN    0\n</code></pre>\n<p>I have tried the following but it simply gives 0 for all the rows:</p>\n<pre><code>df['score'] = df[df == 'BUY'].sum(axis=1)\n</code></pre>\n<p>Note that BUY can only appear in B, C, D, E columns.</p>\n<p>I tried to find the solution online but shockingly found none.</p>\n<p>Little help will be appreciated. THANKS!</p>\n",
        "answer_body": "<p>Or you could use <code>apply</code> with <code>list.count</code>:</p>\n<pre><code>df['score'] = df.apply(lambda x: x.tolist().count('BUY'), axis=1)\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>     A    B    C    D   E  score\n0   12  4.5  6.1  BUY NaN      1\n1   12  BUY  BUY  5.6 NaN      2\n2  BUY  4.5  6.1  BUY NaN      2\n3   12  4.5  6.1    0 NaN      0\n</code></pre>\n",
        "question_body": "<p>I have a data frame:</p>\n<pre><code>A    B    C    D     E\n\n12  4.5  6.1   BUY  NaN\n12  BUY  BUY   5.6  NaN\nBUY  4.5  6.1  BUY  NaN\n12  4.5  6.1   0    NaN \n</code></pre>\n<p>I want to count the number of times 'BUY' appears in each row. Intended result:</p>\n<pre><code>A    B    C    D     E   score\n\n12  4.5  6.1   BUY  NaN    1\n12  BUY  BUY   5.6  NaN    2\n15  4.5  6.1  BUY   NaN    1\n12  4.5  6.1   0    NaN    0\n</code></pre>\n<p>I have tried the following but it simply gives 0 for all the rows:</p>\n<pre><code>df['score'] = df[df == 'BUY'].sum(axis=1)\n</code></pre>\n<p>Note that BUY can only appear in B, C, D, E columns.</p>\n<p>I tried to find the solution online but shockingly found none.</p>\n<p>Little help will be appreciated. THANKS!</p>\n",
        "formatted_input": {
            "qid": 65550028,
            "link": "https://stackoverflow.com/questions/65550028/count-the-number-of-specific-values-in-multiple-columns-pandas",
            "question": {
                "title": "Count the number of specific values in multiple columns pandas",
                "ques_desc": "I have a data frame: I want to count the number of times 'BUY' appears in each row. Intended result: I have tried the following but it simply gives 0 for all the rows: Note that BUY can only appear in B, C, D, E columns. I tried to find the solution online but shockingly found none. Little help will be appreciated. THANKS! "
            },
            "io": [
                "A    B    C    D     E\n\n12  4.5  6.1   BUY  NaN\n12  BUY  BUY   5.6  NaN\nBUY  4.5  6.1  BUY  NaN\n12  4.5  6.1   0    NaN \n",
                "A    B    C    D     E   score\n\n12  4.5  6.1   BUY  NaN    1\n12  BUY  BUY   5.6  NaN    2\n15  4.5  6.1  BUY   NaN    1\n12  4.5  6.1   0    NaN    0\n"
            ],
            "answer": {
                "ans_desc": "Or you could use with : Output: ",
                "code": [
                    "df['score'] = df.apply(lambda x: x.tolist().count('BUY'), axis=1)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 203,
            "user_id": 14642703,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/lj42q.jpg?s=128&g=1",
            "display_name": "Zac",
            "link": "https://stackoverflow.com/users/14642703/zac"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 65530890,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1609514120,
        "creation_date": 1609512208,
        "question_id": 65530626,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65530626/append-loop-output-in-column-pandas-python",
        "title": "Append loop output in column pandas python",
        "body": "<p>I am working with the code below to append output to empty dataframe</p>\n<pre><code>image_data = pd.DataFrame()\n\nfor i in words:\n    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)\n    x = y\n    image_data = image_data.append(x, ignore_index = True)\n</code></pre>\n<p>i am getting output as below but i want</p>\n<pre><code>        0\n0   30708\n1      15\n2    1800\n0   19200\n1      50\n2    1180\n</code></pre>\n<p>What i want the output to be</p>\n<pre><code>        0    1       2\n0   30708   15    1800\n1   19200   50    1180\n</code></pre>\n<p>How can i make 3 rows to  3 columns every time the loop repeats.</p>\n",
        "answer_body": "<p>It perplexes me when you write <code>x = y</code> without doing any manipulation on <code>x</code>. Seems like a redundant operation. Another problem with your code is that <code>image_data.append</code> is slow since it has to copy the backing memory. Repeatedly calling it in a loop is a guarantee of performance bottleneck.</p>\n<p>Try this instead:</p>\n<pre><code># image_data starts as a list\nimage_data = []\n\nfor i in words:\n    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)\n    image_data.append(y)\n\n# And it ends as a DataFrame\nimage_data = pd.DataFrame(image_data)\n</code></pre>\n",
        "question_body": "<p>I am working with the code below to append output to empty dataframe</p>\n<pre><code>image_data = pd.DataFrame()\n\nfor i in words:\n    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)\n    x = y\n    image_data = image_data.append(x, ignore_index = True)\n</code></pre>\n<p>i am getting output as below but i want</p>\n<pre><code>        0\n0   30708\n1      15\n2    1800\n0   19200\n1      50\n2    1180\n</code></pre>\n<p>What i want the output to be</p>\n<pre><code>        0    1       2\n0   30708   15    1800\n1   19200   50    1180\n</code></pre>\n<p>How can i make 3 rows to  3 columns every time the loop repeats.</p>\n",
        "formatted_input": {
            "qid": 65530626,
            "link": "https://stackoverflow.com/questions/65530626/append-loop-output-in-column-pandas-python",
            "question": {
                "title": "Append loop output in column pandas python",
                "ques_desc": "I am working with the code below to append output to empty dataframe i am getting output as below but i want What i want the output to be How can i make 3 rows to 3 columns every time the loop repeats. "
            },
            "io": [
                "        0\n0   30708\n1      15\n2    1800\n0   19200\n1      50\n2    1180\n",
                "        0    1       2\n0   30708   15    1800\n1   19200   50    1180\n"
            ],
            "answer": {
                "ans_desc": "It perplexes me when you write without doing any manipulation on . Seems like a redundant operation. Another problem with your code is that is slow since it has to copy the backing memory. Repeatedly calling it in a loop is a guarantee of performance bottleneck. Try this instead: ",
                "code": [
                    "# image_data starts as a list\nimage_data = []\n\nfor i in words:\n    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)\n    image_data.append(y)\n\n# And it ends as a DataFrame\nimage_data = pd.DataFrame(image_data)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 414,
            "user_id": 10816754,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/0gUNf.jpg?s=128&g=1",
            "display_name": "ahh_real_numbers",
            "link": "https://stackoverflow.com/users/10816754/ahh-real-numbers"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 65515522,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1609377513,
        "creation_date": 1609375382,
        "question_id": 65515347,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65515347/fill-null-values-in-a-column-using-percent-change-from-a-second-column-while-gro",
        "title": "Fill null values in a column using percent change from a second column while grouping by a third column",
        "body": "<p>I have a dataframe that looks like this:</p>\n<pre><code>grp    val    run\na      5      10\nb      10     1\na      NaN    8\na      NaN    4\nb      NaN    5\nb      NaN    4\na      10     6\na      NaN    6\n</code></pre>\n<p>I want to fill in the gaps in the <code>val</code> column by applying the same percent change as was calculated. However I also need to group using the <code>grp</code> column. I should end up with something like this:</p>\n<pre><code>grp    val    run\na      5      10\nb      10     1\na      4      8\na      2      4\nb      50     5\nb      40     4\na      10     6\na      10     6\n</code></pre>\n<p>I only want to replace values that are null. Notice the 10 in row seven &quot;resets&quot; the forward fill.</p>\n<p>Without having to group, I could simply get the percent change in <code>run</code> and multiply the previous row's <code>val</code> cell by the current row's percent change cell wherever <code>val</code> is not null.</p>\n<p>I was thinking that I could order the dataframe using <code>grp</code>, but then I would still have to worry about the edge case of when <code>grp</code> values change.</p>\n",
        "answer_body": "<p>Let us try:</p>\n<pre><code># identify the na blocks and group by `grp` and these blocks\nna_blocks = df['val'].notna().groupby(df['grp']).cumsum()    \ng = df.groupby(['grp', na_blocks])\n\n# &quot;pct change&quot; on run\ndf['x'] = df['run'] / g['run'].shift(fill_value=1)\n\n# cumprod() for cumulative change\n# `ffill` and `transform('first')` behave the same \n# since we are grouping on non-nan following by consecutive nan's\ndf['val'] = g['val'].ffill() * g['x'].cumprod() / g['run'].transform('first')\n</code></pre>\n<p>Output (<code>x</code> the extra column that can be dropped):</p>\n<pre><code>  grp   val  run     x\n0   a   5.0   10  10.0\n1   b  10.0    1   1.0\n2   a   4.0    8   0.8\n3   a   2.0    4   0.5\n4   b  50.0    5   5.0\n5   b  40.0    4   0.8\n6   a  10.0    6   6.0\n7   a  10.0    6   1.0\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like this:</p>\n<pre><code>grp    val    run\na      5      10\nb      10     1\na      NaN    8\na      NaN    4\nb      NaN    5\nb      NaN    4\na      10     6\na      NaN    6\n</code></pre>\n<p>I want to fill in the gaps in the <code>val</code> column by applying the same percent change as was calculated. However I also need to group using the <code>grp</code> column. I should end up with something like this:</p>\n<pre><code>grp    val    run\na      5      10\nb      10     1\na      4      8\na      2      4\nb      50     5\nb      40     4\na      10     6\na      10     6\n</code></pre>\n<p>I only want to replace values that are null. Notice the 10 in row seven &quot;resets&quot; the forward fill.</p>\n<p>Without having to group, I could simply get the percent change in <code>run</code> and multiply the previous row's <code>val</code> cell by the current row's percent change cell wherever <code>val</code> is not null.</p>\n<p>I was thinking that I could order the dataframe using <code>grp</code>, but then I would still have to worry about the edge case of when <code>grp</code> values change.</p>\n",
        "formatted_input": {
            "qid": 65515347,
            "link": "https://stackoverflow.com/questions/65515347/fill-null-values-in-a-column-using-percent-change-from-a-second-column-while-gro",
            "question": {
                "title": "Fill null values in a column using percent change from a second column while grouping by a third column",
                "ques_desc": "I have a dataframe that looks like this: I want to fill in the gaps in the column by applying the same percent change as was calculated. However I also need to group using the column. I should end up with something like this: I only want to replace values that are null. Notice the 10 in row seven \"resets\" the forward fill. Without having to group, I could simply get the percent change in and multiply the previous row's cell by the current row's percent change cell wherever is not null. I was thinking that I could order the dataframe using , but then I would still have to worry about the edge case of when values change. "
            },
            "io": [
                "grp    val    run\na      5      10\nb      10     1\na      NaN    8\na      NaN    4\nb      NaN    5\nb      NaN    4\na      10     6\na      NaN    6\n",
                "grp    val    run\na      5      10\nb      10     1\na      4      8\na      2      4\nb      50     5\nb      40     4\na      10     6\na      10     6\n"
            ],
            "answer": {
                "ans_desc": "Let us try: Output ( the extra column that can be dropped): ",
                "code": [
                    "# identify the na blocks and group by `grp` and these blocks\nna_blocks = df['val'].notna().groupby(df['grp']).cumsum()    \ng = df.groupby(['grp', na_blocks])\n\n# \"pct change\" on run\ndf['x'] = df['run'] / g['run'].shift(fill_value=1)\n\n# cumprod() for cumulative change\n# `ffill` and `transform('first')` behave the same \n# since we are grouping on non-nan following by consecutive nan's\ndf['val'] = g['val'].ffill() * g['x'].cumprod() / g['run'].transform('first')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 79,
            "user_id": 12857953,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d2e9bea2f86ea01d055865ba28e61723?s=128&d=identicon&r=PG&f=1",
            "display_name": "sinankoramaz",
            "link": "https://stackoverflow.com/users/12857953/sinankoramaz"
        },
        "is_answered": true,
        "view_count": 91,
        "accepted_answer_id": 65483740,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1609187580,
        "creation_date": 1609185740,
        "question_id": 65483406,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65483406/replace-nan-values-with-the-means-of-other-cols-based-on-condition",
        "title": "Replace NaN Values with the Means of other Cols based on Condition",
        "body": "<p>I have the following Pandas DataFrame</p>\n<pre><code>  Col1 Col2  Col3\n0    A    c   1.0\n1    A    c   3.0\n2    B    c   5.0\n3    A    d   6.0\n4    A    c   NaN\n</code></pre>\n<p>I am writing the following function:</p>\n<pre><code>replace_missing_with_conditional_mean(df, condition_cols, cols)\n</code></pre>\n<p>I want to to replace the missing values present in columns with labels in the list <code>cols</code>.</p>\n<p>The value to be replaced is computed as the mean of the non missing values of the corresponding group. Groups are formed based on the values in the columns with labels in the list <code>condition_cols</code>.</p>\n<p>When <code>replace_missing_with_conditional_mean(df, condition_cols=['Col1','Col2'], cols=['Col3'])</code> is applied to the above dataframe with arguments, it should yield:</p>\n<pre><code> Col1 Col2  Col3\n0    A    c   1.0\n1    A    c   3.0\n2    B    c   5.0\n3    A    d   6.0\n4    A    c   2.0\n</code></pre>\n<p>this is because the record on line 4 belongs to the group <code>A c</code> that has a mean of (1+3)/2 = 2.</p>\n<p>I tried using <code> df.fillna(df.groupby(condition_cols).transform('mean'))</code> but it is giving me the error</p>\n<pre><code>TypeError: Transform function invalid for data types\n</code></pre>\n",
        "answer_body": "<p>You could implement the function like this:</p>\n<pre><code>def replace_missing_with_conditional_mean(df, condition_cols, cols):\n    s = df.groupby(condition_cols)[cols].transform('mean')\n    return df.fillna(s.to_dict('series'))\n\n\nres = replace_missing_with_conditional_mean(df, ['Col1', 'Col2'], ['Col3'])\nprint(res)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>  Col1 Col2  Col3\n0    A    c   1.0\n1    A    c   3.0\n2    B    c   5.0\n3    A    d   6.0\n4    A    c   2.0\n</code></pre>\n",
        "question_body": "<p>I have the following Pandas DataFrame</p>\n<pre><code>  Col1 Col2  Col3\n0    A    c   1.0\n1    A    c   3.0\n2    B    c   5.0\n3    A    d   6.0\n4    A    c   NaN\n</code></pre>\n<p>I am writing the following function:</p>\n<pre><code>replace_missing_with_conditional_mean(df, condition_cols, cols)\n</code></pre>\n<p>I want to to replace the missing values present in columns with labels in the list <code>cols</code>.</p>\n<p>The value to be replaced is computed as the mean of the non missing values of the corresponding group. Groups are formed based on the values in the columns with labels in the list <code>condition_cols</code>.</p>\n<p>When <code>replace_missing_with_conditional_mean(df, condition_cols=['Col1','Col2'], cols=['Col3'])</code> is applied to the above dataframe with arguments, it should yield:</p>\n<pre><code> Col1 Col2  Col3\n0    A    c   1.0\n1    A    c   3.0\n2    B    c   5.0\n3    A    d   6.0\n4    A    c   2.0\n</code></pre>\n<p>this is because the record on line 4 belongs to the group <code>A c</code> that has a mean of (1+3)/2 = 2.</p>\n<p>I tried using <code> df.fillna(df.groupby(condition_cols).transform('mean'))</code> but it is giving me the error</p>\n<pre><code>TypeError: Transform function invalid for data types\n</code></pre>\n",
        "formatted_input": {
            "qid": 65483406,
            "link": "https://stackoverflow.com/questions/65483406/replace-nan-values-with-the-means-of-other-cols-based-on-condition",
            "question": {
                "title": "Replace NaN Values with the Means of other Cols based on Condition",
                "ques_desc": "I have the following Pandas DataFrame I am writing the following function: I want to to replace the missing values present in columns with labels in the list . The value to be replaced is computed as the mean of the non missing values of the corresponding group. Groups are formed based on the values in the columns with labels in the list . When is applied to the above dataframe with arguments, it should yield: this is because the record on line 4 belongs to the group that has a mean of (1+3)/2 = 2. I tried using but it is giving me the error "
            },
            "io": [
                "  Col1 Col2  Col3\n0    A    c   1.0\n1    A    c   3.0\n2    B    c   5.0\n3    A    d   6.0\n4    A    c   NaN\n",
                " Col1 Col2  Col3\n0    A    c   1.0\n1    A    c   3.0\n2    B    c   5.0\n3    A    d   6.0\n4    A    c   2.0\n"
            ],
            "answer": {
                "ans_desc": "You could implement the function like this: Output ",
                "code": [
                    "def replace_missing_with_conditional_mean(df, condition_cols, cols):\n    s = df.groupby(condition_cols)[cols].transform('mean')\n    return df.fillna(s.to_dict('series'))\n\n\nres = replace_missing_with_conditional_mean(df, ['Col1', 'Col2'], ['Col3'])\nprint(res)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan",
            "dictionary-comprehension"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 9608368,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "rawando",
            "link": "https://stackoverflow.com/users/9608368/rawando"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 65451174,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1608924949,
        "creation_date": 1608924320,
        "last_edit_date": 1608924521,
        "question_id": 65451097,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65451097/why-having-std-for-1-column-and-others-are-nan",
        "title": "why having std for 1 column and others are nan?",
        "body": "<p>i have DataFrame looks something like this but with shape (345,5) like this</p>\n<pre><code>|something1|  something2|  numbers1| number2 |number3|\n|----------|------------|----------|---------|-------|\n| A        | str        |    45    | nan     |nan    |\n|B         | str2       |   6      |  nan    | nan   |\n| c        | str3       |   34     |  67     | 45    |\n|D         | str4       |    56    |  45     | 23    |\n</code></pre>\n<p>and i want to get the std for the numeric columns ONLY with my manually std function and save in dictionary,  the probelm is i am getting this result for the first column only:</p>\n<pre><code>{'number1': 18.59267328815305,\n 'number2': nan,\n 'number3': nan,\n 'number4': nan}\n</code></pre>\n<p>and here is my code:</p>\n<pre><code>std = {column:std_func(df[column].values) for column in df.columns}\n</code></pre>\n",
        "answer_body": "<p>Pandas can handle this, try instead</p>\n<pre><code>df[['numbers1', 'numbers2', 'numbers3']].std()\n</code></pre>\n<p>by default NaNs are skipped:\n<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html</a></p>\n<p>if you want this in a dict then do:</p>\n<pre><code>df[['numbers1', 'numbers2', 'numbers3']].std().to_dict()\n</code></pre>\n<p>edit: if you are dead-set on using specifically your custom standard deviation function, just dropna from the column before applying:</p>\n<pre><code>std = {column:std_func(df[column].dropna().values) for column in df.columns}\n</code></pre>\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dropna.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dropna.html</a></p>\n",
        "question_body": "<p>i have DataFrame looks something like this but with shape (345,5) like this</p>\n<pre><code>|something1|  something2|  numbers1| number2 |number3|\n|----------|------------|----------|---------|-------|\n| A        | str        |    45    | nan     |nan    |\n|B         | str2       |   6      |  nan    | nan   |\n| c        | str3       |   34     |  67     | 45    |\n|D         | str4       |    56    |  45     | 23    |\n</code></pre>\n<p>and i want to get the std for the numeric columns ONLY with my manually std function and save in dictionary,  the probelm is i am getting this result for the first column only:</p>\n<pre><code>{'number1': 18.59267328815305,\n 'number2': nan,\n 'number3': nan,\n 'number4': nan}\n</code></pre>\n<p>and here is my code:</p>\n<pre><code>std = {column:std_func(df[column].values) for column in df.columns}\n</code></pre>\n",
        "formatted_input": {
            "qid": 65451097,
            "link": "https://stackoverflow.com/questions/65451097/why-having-std-for-1-column-and-others-are-nan",
            "question": {
                "title": "why having std for 1 column and others are nan?",
                "ques_desc": "i have DataFrame looks something like this but with shape (345,5) like this and i want to get the std for the numeric columns ONLY with my manually std function and save in dictionary, the probelm is i am getting this result for the first column only: and here is my code: "
            },
            "io": [
                "|something1|  something2|  numbers1| number2 |number3|\n|----------|------------|----------|---------|-------|\n| A        | str        |    45    | nan     |nan    |\n|B         | str2       |   6      |  nan    | nan   |\n| c        | str3       |   34     |  67     | 45    |\n|D         | str4       |    56    |  45     | 23    |\n",
                "{'number1': 18.59267328815305,\n 'number2': nan,\n 'number3': nan,\n 'number4': nan}\n"
            ],
            "answer": {
                "ans_desc": "Pandas can handle this, try instead by default NaNs are skipped: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html if you want this in a dict then do: edit: if you are dead-set on using specifically your custom standard deviation function, just dropna from the column before applying: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dropna.html ",
                "code": [
                    "std = {column:std_func(df[column].dropna().values) for column in df.columns}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 5,
            "user_id": 13716206,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/78fbf42f0eca6008b9f65880dd332505?s=128&d=identicon&r=PG&f=1",
            "display_name": "dodosaurusRex",
            "link": "https://stackoverflow.com/users/13716206/dodosaurusrex"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 65392371,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1608576348,
        "creation_date": 1608506349,
        "last_edit_date": 1608551633,
        "question_id": 65385779,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65385779/python-pandas-update-a-column-based-on-a-series-holding-sums-of-that-same-colum",
        "title": "python/pandas: update a column based on a series holding sums of that same column",
        "body": "<p>I have a dataframe with a non-unique col1 like the following</p>\n<pre><code>    col1    col2\n0      a      1\n1      a      1\n2      a      2\n3      b      3\n4      b      3\n5      c      2\n6      c      2\n</code></pre>\n<p>Some of the values of col1 repeat lots of times and others not so.  I'd like to take the bottom (80%/50%/10%) and change the value to 'other' ahead of plotting.</p>\n<p>I've got a series which contains the codes in col1 (as the index) and the amount of times that they appear in the df in descending order by doing the following:</p>\n<pre><code>df2 = df.groupby(['col1']).size().sort_values(ascending=False)\n</code></pre>\n<p>I've also got my cut-off point (bottom 80%)</p>\n<pre><code>cutOff = round(len(df2)/5)\n</code></pre>\n<p>I'd like to update col1 in df with the value 'others' when col1 appears after the cutOff in the index of the series df2.</p>\n<p>I don't know how to go about checking and updating. I figured that the best way would be to do a groupby on col1 and then loop through, but it starts to fall apart, should I create a new groupby object?  Or do I call this as an .apply() for each row? Can you update a column that is being used as the index for a dataframe? I could do with some help about how to start.</p>\n<p>edit to add:</p>\n<p>So if the 'b's in col1 were not in the top 20% most populous values in col1 then I'd expect to see:</p>\n<pre><code>    col1    col2\n0      a      1\n1      a      1\n2      a      2\n3 others      3\n4 others      3\n5      c      2\n6      c      2\n</code></pre>\n",
        "answer_body": "<pre><code>data = [[&quot;a &quot;, 1],\n        [&quot;a &quot;, 1],\n        [&quot;a &quot;, 2],\n        [&quot;b &quot;, 3],\n        [&quot;b &quot;, 3],\n        [&quot;c &quot;, 2],\n        [&quot;c &quot;, 2], ]\ndf = pd.DataFrame(data, columns=[&quot;col1&quot;, &quot;col2&quot;])\nprint(df)\n\ndf2 = df.groupby(['col1']).size().sort_values(ascending=False)\nprint(df2)\n\ncutOff = round(len(df2) / 5)\nothers = df2.iloc[cutOff + 1:]\nprint(others)\n\nresult = df.copy()\nresult.loc[result[&quot;col1&quot;].isin(others.index), &quot;col1&quot;] = &quot;others&quot;\nprint(result)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with a non-unique col1 like the following</p>\n<pre><code>    col1    col2\n0      a      1\n1      a      1\n2      a      2\n3      b      3\n4      b      3\n5      c      2\n6      c      2\n</code></pre>\n<p>Some of the values of col1 repeat lots of times and others not so.  I'd like to take the bottom (80%/50%/10%) and change the value to 'other' ahead of plotting.</p>\n<p>I've got a series which contains the codes in col1 (as the index) and the amount of times that they appear in the df in descending order by doing the following:</p>\n<pre><code>df2 = df.groupby(['col1']).size().sort_values(ascending=False)\n</code></pre>\n<p>I've also got my cut-off point (bottom 80%)</p>\n<pre><code>cutOff = round(len(df2)/5)\n</code></pre>\n<p>I'd like to update col1 in df with the value 'others' when col1 appears after the cutOff in the index of the series df2.</p>\n<p>I don't know how to go about checking and updating. I figured that the best way would be to do a groupby on col1 and then loop through, but it starts to fall apart, should I create a new groupby object?  Or do I call this as an .apply() for each row? Can you update a column that is being used as the index for a dataframe? I could do with some help about how to start.</p>\n<p>edit to add:</p>\n<p>So if the 'b's in col1 were not in the top 20% most populous values in col1 then I'd expect to see:</p>\n<pre><code>    col1    col2\n0      a      1\n1      a      1\n2      a      2\n3 others      3\n4 others      3\n5      c      2\n6      c      2\n</code></pre>\n",
        "formatted_input": {
            "qid": 65385779,
            "link": "https://stackoverflow.com/questions/65385779/python-pandas-update-a-column-based-on-a-series-holding-sums-of-that-same-colum",
            "question": {
                "title": "python/pandas: update a column based on a series holding sums of that same column",
                "ques_desc": "I have a dataframe with a non-unique col1 like the following Some of the values of col1 repeat lots of times and others not so. I'd like to take the bottom (80%/50%/10%) and change the value to 'other' ahead of plotting. I've got a series which contains the codes in col1 (as the index) and the amount of times that they appear in the df in descending order by doing the following: I've also got my cut-off point (bottom 80%) I'd like to update col1 in df with the value 'others' when col1 appears after the cutOff in the index of the series df2. I don't know how to go about checking and updating. I figured that the best way would be to do a groupby on col1 and then loop through, but it starts to fall apart, should I create a new groupby object? Or do I call this as an .apply() for each row? Can you update a column that is being used as the index for a dataframe? I could do with some help about how to start. edit to add: So if the 'b's in col1 were not in the top 20% most populous values in col1 then I'd expect to see: "
            },
            "io": [
                "    col1    col2\n0      a      1\n1      a      1\n2      a      2\n3      b      3\n4      b      3\n5      c      2\n6      c      2\n",
                "    col1    col2\n0      a      1\n1      a      1\n2      a      2\n3 others      3\n4 others      3\n5      c      2\n6      c      2\n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "data = [[\"a \", 1],\n        [\"a \", 1],\n        [\"a \", 2],\n        [\"b \", 3],\n        [\"b \", 3],\n        [\"c \", 2],\n        [\"c \", 2], ]\ndf = pd.DataFrame(data, columns=[\"col1\", \"col2\"])\nprint(df)\n\ndf2 = df.groupby(['col1']).size().sort_values(ascending=False)\nprint(df2)\n\ncutOff = round(len(df2) / 5)\nothers = df2.iloc[cutOff + 1:]\nprint(others)\n\nresult = df.copy()\nresult.loc[result[\"col1\"].isin(others.index), \"col1\"] = \"others\"\nprint(result)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 360,
            "user_id": 13877952,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/ZMCoC.png?s=128&g=1",
            "display_name": "BeamsAdept",
            "link": "https://stackoverflow.com/users/13877952/beamsadept"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 65393455,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1608563929,
        "creation_date": 1608554500,
        "last_edit_date": 1608562842,
        "question_id": 65392984,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65392984/pandas-advanced-problem-for-each-row-get-complex-info-from-another-dataframe",
        "title": "Pandas advanced problem : For each row, get complex info from another dataframe",
        "body": "<h2>Problem</h2>\n<p>I have a dataframe <code>df</code> :</p>\n<pre><code>Index  Client_ID   Date   \n1      johndoe     2019-01-15\n2      johndoe     2015-11-25\n3      pauldoe     2015-05-26\n</code></pre>\n<p>And I have another dataframe <code>df_prod</code>, with products like this :</p>\n<pre><code>Index   Product-Type   Product-Date   Buyer     Price\n1       A              2020-01-01     pauldoe   300\n2       A              2018-01-01     pauldoe   200\n3       A              2019-01-01     johndoe   600\n4       A              2017-01-01     johndoe   800\n5       A              2020-11-05     johndoe   100\n6       B              2014-12-12     johndoe   200\n7       B              2016-11-15     johndoe   300\n</code></pre>\n<p>What I want is to add to <code>df</code> a column, that will sum the Prices of the last products of each type known at the current date (with <code>Product-Date &lt;= df.Date</code>). An example is the best way to explain :</p>\n<p>For the first row of <code>df</code></p>\n<pre><code>1      johndoe     2019-01-01\n</code></pre>\n<p>The last A-Product known at this date bought by <code>johndoe</code> is this one :</p>\n<pre><code>3       A              2019-01-01     johndoe   600\n</code></pre>\n<p>(since the 4th one is older, and the 5th one has a <code>Product-Date</code> &gt; <code>Date</code>)\nThe last B-Product known at this date bought by <code>johndoe</code> is this one :</p>\n<pre><code>7       B              2016-11-15     johndoe   300\n</code></pre>\n<p>So the row in <code>df</code>, after transformation, will look like that (<code>900</code> being <code>600 + 300</code>, prices of the 2 products of interest) :</p>\n<pre><code>1      johndoe     2019-01-15   900\n</code></pre>\n<p>The full <code>df</code> after transformation will then be :</p>\n<pre><code>Index  Client_ID   Date         LastProdSum\n1      johndoe     2019-15-01   900\n2      johndoe     2015-11-25   200\n3      pauldoe     2015-05-26   0\n</code></pre>\n<p>As you can see, there are multiple possibilities :</p>\n<ul>\n<li>Buyers didn't necessary buy all products (see <code>pauldoe</code>, who only bought A-products)</li>\n<li>Sometimes, no product is known at <code>df.Date</code> (see row 3 of the new <code>df</code>, in 2015, we don't know any product bought by <code>pauldoe</code>)</li>\n<li>Sometimes, only one product is known at <code>df.Date</code>, and the value is the one of the product (see row 3 of the new <code>df</code>, in 2015, we only have one product for <code>johndoe</code>, which is a B-product bought in 2014, whose price is <code>200</code>)</li>\n</ul>\n<h2>What I did :</h2>\n<p>I found a solution to this problem, but it's taking too much time to be used, since my dataframe is huge.</p>\n<p>For that, I iterate using iterrows on rows of <code>df</code>, I then select the products linked to the Buyer, having <code>Product-Date &lt; Date</code> on <code>df_prod</code>, then get the older grouping by <code>Product-Type</code> and getting the max date, then I finally sum all my products prices.\nThe fact I solve the problem iterating on each row (with a for iterrows), extracting for each row of <code>df</code> a part of <code>df_prod</code> that I work on to finally get my sum, makes it really long.\nI'm almost sure there's a better way to solve the problem, with pandas functions (<code>pivot</code> for example), but I couldn't find the way. I've been searching a lot.</p>\n<p>Thanks in advance for your help</p>\n<h2>Edit after Dani's answer</h2>\n<p>Thanks a lot for your answer. It looks really good, I accepted it since you spent a lot of time on it.\nExecution is still pretty long, since I didn't specify something.\nIn fact, <code>Product-Types</code> are not shared through buyers : each buyers has its own multiple products types. The true way to see this is like this :</p>\n<pre><code>Index   Product-Type   Product-Date   Buyer     Price\n1       pauldoe-ID1    2020-01-01     pauldoe   300\n2       pauldoe-ID1    2018-01-01     pauldoe   200\n3       johndoe-ID2    2019-01-01     johndoe   600\n4       johndoe-ID2    2017-01-01     johndoe   800\n5       johndoe-ID2    2020-11-05     johndoe   100\n6       johndoe-ID3    2014-12-12     johndoe   200\n7       johndoe-ID3    2016-11-15     johndoe   300\n</code></pre>\n<p>As you can understand, product types are not shared through different buyers (in fact, it can happen, but in really rare situations that we won't consider here)</p>\n<p>The problem remains the same, since you want to sum prices, you'll add the prices of last occurences of johndoe-ID2 and johndoe-ID3 to have the same final result row</p>\n<pre><code>1      johndoe     2019-15-01   900\n</code></pre>\n<p>But as you now understand, there are actually more <code>Product-Types</code> than <code>Buyers</code>, so the step &quot;get unique product types&quot; from your answer, that looked pretty fast on the initial problem, actually takes <strong>a lot</strong> of time.</p>\n<p>Sorry for being unclear on this point, I didn't think of a possibility of creating a new df based on product types.</p>\n",
        "answer_body": "<p>The main idea is to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge_asof.html\" rel=\"nofollow noreferrer\">merge_asof</a> to fetch the last date for each <em>Product-Type</em> and <em>Client_ID</em>, so do the following:</p>\n<pre><code># get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\nprint(res)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>  Client_ID       Date  LastProdSum\n0   johndoe 2015-11-25        200.0\n1   johndoe 2019-01-15        900.0\n2   pauldoe 2015-05-26          0.0\n</code></pre>\n<p>The problem is that <em>merge_asof</em> won't work with duplicate values, so we need to create unique values. These new values are the cartesian product of <em>Client_ID</em> and <em>Product-Type</em>, this part is done in:</p>\n<pre><code># get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n</code></pre>\n<p>Finally do a groupby and sum the <em>Price</em>, not before doing a <em>fillna</em> to fill the missing values.</p>\n<p><strong>UPDATE</strong></p>\n<p>You could try:</p>\n<pre><code># get unique product types\nproduct_types = df_prod.groupby('Buyer')['Product-Type'].apply(lambda x: list(set(x)))\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = df['Client_ID'].map(product_types)\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\n\nprint(res)\n</code></pre>\n<p>The idea here is to change how you generate the <em>unique</em> values.</p>\n",
        "question_body": "<h2>Problem</h2>\n<p>I have a dataframe <code>df</code> :</p>\n<pre><code>Index  Client_ID   Date   \n1      johndoe     2019-01-15\n2      johndoe     2015-11-25\n3      pauldoe     2015-05-26\n</code></pre>\n<p>And I have another dataframe <code>df_prod</code>, with products like this :</p>\n<pre><code>Index   Product-Type   Product-Date   Buyer     Price\n1       A              2020-01-01     pauldoe   300\n2       A              2018-01-01     pauldoe   200\n3       A              2019-01-01     johndoe   600\n4       A              2017-01-01     johndoe   800\n5       A              2020-11-05     johndoe   100\n6       B              2014-12-12     johndoe   200\n7       B              2016-11-15     johndoe   300\n</code></pre>\n<p>What I want is to add to <code>df</code> a column, that will sum the Prices of the last products of each type known at the current date (with <code>Product-Date &lt;= df.Date</code>). An example is the best way to explain :</p>\n<p>For the first row of <code>df</code></p>\n<pre><code>1      johndoe     2019-01-01\n</code></pre>\n<p>The last A-Product known at this date bought by <code>johndoe</code> is this one :</p>\n<pre><code>3       A              2019-01-01     johndoe   600\n</code></pre>\n<p>(since the 4th one is older, and the 5th one has a <code>Product-Date</code> &gt; <code>Date</code>)\nThe last B-Product known at this date bought by <code>johndoe</code> is this one :</p>\n<pre><code>7       B              2016-11-15     johndoe   300\n</code></pre>\n<p>So the row in <code>df</code>, after transformation, will look like that (<code>900</code> being <code>600 + 300</code>, prices of the 2 products of interest) :</p>\n<pre><code>1      johndoe     2019-01-15   900\n</code></pre>\n<p>The full <code>df</code> after transformation will then be :</p>\n<pre><code>Index  Client_ID   Date         LastProdSum\n1      johndoe     2019-15-01   900\n2      johndoe     2015-11-25   200\n3      pauldoe     2015-05-26   0\n</code></pre>\n<p>As you can see, there are multiple possibilities :</p>\n<ul>\n<li>Buyers didn't necessary buy all products (see <code>pauldoe</code>, who only bought A-products)</li>\n<li>Sometimes, no product is known at <code>df.Date</code> (see row 3 of the new <code>df</code>, in 2015, we don't know any product bought by <code>pauldoe</code>)</li>\n<li>Sometimes, only one product is known at <code>df.Date</code>, and the value is the one of the product (see row 3 of the new <code>df</code>, in 2015, we only have one product for <code>johndoe</code>, which is a B-product bought in 2014, whose price is <code>200</code>)</li>\n</ul>\n<h2>What I did :</h2>\n<p>I found a solution to this problem, but it's taking too much time to be used, since my dataframe is huge.</p>\n<p>For that, I iterate using iterrows on rows of <code>df</code>, I then select the products linked to the Buyer, having <code>Product-Date &lt; Date</code> on <code>df_prod</code>, then get the older grouping by <code>Product-Type</code> and getting the max date, then I finally sum all my products prices.\nThe fact I solve the problem iterating on each row (with a for iterrows), extracting for each row of <code>df</code> a part of <code>df_prod</code> that I work on to finally get my sum, makes it really long.\nI'm almost sure there's a better way to solve the problem, with pandas functions (<code>pivot</code> for example), but I couldn't find the way. I've been searching a lot.</p>\n<p>Thanks in advance for your help</p>\n<h2>Edit after Dani's answer</h2>\n<p>Thanks a lot for your answer. It looks really good, I accepted it since you spent a lot of time on it.\nExecution is still pretty long, since I didn't specify something.\nIn fact, <code>Product-Types</code> are not shared through buyers : each buyers has its own multiple products types. The true way to see this is like this :</p>\n<pre><code>Index   Product-Type   Product-Date   Buyer     Price\n1       pauldoe-ID1    2020-01-01     pauldoe   300\n2       pauldoe-ID1    2018-01-01     pauldoe   200\n3       johndoe-ID2    2019-01-01     johndoe   600\n4       johndoe-ID2    2017-01-01     johndoe   800\n5       johndoe-ID2    2020-11-05     johndoe   100\n6       johndoe-ID3    2014-12-12     johndoe   200\n7       johndoe-ID3    2016-11-15     johndoe   300\n</code></pre>\n<p>As you can understand, product types are not shared through different buyers (in fact, it can happen, but in really rare situations that we won't consider here)</p>\n<p>The problem remains the same, since you want to sum prices, you'll add the prices of last occurences of johndoe-ID2 and johndoe-ID3 to have the same final result row</p>\n<pre><code>1      johndoe     2019-15-01   900\n</code></pre>\n<p>But as you now understand, there are actually more <code>Product-Types</code> than <code>Buyers</code>, so the step &quot;get unique product types&quot; from your answer, that looked pretty fast on the initial problem, actually takes <strong>a lot</strong> of time.</p>\n<p>Sorry for being unclear on this point, I didn't think of a possibility of creating a new df based on product types.</p>\n",
        "formatted_input": {
            "qid": 65392984,
            "link": "https://stackoverflow.com/questions/65392984/pandas-advanced-problem-for-each-row-get-complex-info-from-another-dataframe",
            "question": {
                "title": "Pandas advanced problem : For each row, get complex info from another dataframe",
                "ques_desc": "Problem I have a dataframe : And I have another dataframe , with products like this : What I want is to add to a column, that will sum the Prices of the last products of each type known at the current date (with ). An example is the best way to explain : For the first row of The last A-Product known at this date bought by is this one : (since the 4th one is older, and the 5th one has a > ) The last B-Product known at this date bought by is this one : So the row in , after transformation, will look like that ( being , prices of the 2 products of interest) : The full after transformation will then be : As you can see, there are multiple possibilities : Buyers didn't necessary buy all products (see , who only bought A-products) Sometimes, no product is known at (see row 3 of the new , in 2015, we don't know any product bought by ) Sometimes, only one product is known at , and the value is the one of the product (see row 3 of the new , in 2015, we only have one product for , which is a B-product bought in 2014, whose price is ) What I did : I found a solution to this problem, but it's taking too much time to be used, since my dataframe is huge. For that, I iterate using iterrows on rows of , I then select the products linked to the Buyer, having on , then get the older grouping by and getting the max date, then I finally sum all my products prices. The fact I solve the problem iterating on each row (with a for iterrows), extracting for each row of a part of that I work on to finally get my sum, makes it really long. I'm almost sure there's a better way to solve the problem, with pandas functions ( for example), but I couldn't find the way. I've been searching a lot. Thanks in advance for your help Edit after Dani's answer Thanks a lot for your answer. It looks really good, I accepted it since you spent a lot of time on it. Execution is still pretty long, since I didn't specify something. In fact, are not shared through buyers : each buyers has its own multiple products types. The true way to see this is like this : As you can understand, product types are not shared through different buyers (in fact, it can happen, but in really rare situations that we won't consider here) The problem remains the same, since you want to sum prices, you'll add the prices of last occurences of johndoe-ID2 and johndoe-ID3 to have the same final result row But as you now understand, there are actually more than , so the step \"get unique product types\" from your answer, that looked pretty fast on the initial problem, actually takes a lot of time. Sorry for being unclear on this point, I didn't think of a possibility of creating a new df based on product types. "
            },
            "io": [
                "3       A              2019-01-01     johndoe   600\n",
                "7       B              2016-11-15     johndoe   300\n"
            ],
            "answer": {
                "ans_desc": "The main idea is to use merge_asof to fetch the last date for each Product-Type and Client_ID, so do the following: Output The problem is that merge_asof won't work with duplicate values, so we need to create unique values. These new values are the cartesian product of Client_ID and Product-Type, this part is done in: Finally do a groupby and sum the Price, not before doing a fillna to fill the missing values. UPDATE You could try: The idea here is to change how you generate the unique values. ",
                "code": [
                    "# get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\nprint(res)\n",
                    "# get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n",
                    "# get unique product types\nproduct_types = df_prod.groupby('Buyer')['Product-Type'].apply(lambda x: list(set(x)))\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = df['Client_ID'].map(product_types)\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\n\nprint(res)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 49,
            "user_id": 14828964,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4ce5caf634152b59cf71e6c8e5668aff?s=128&d=identicon&r=PG&f=1",
            "display_name": "havingaball",
            "link": "https://stackoverflow.com/users/14828964/havingaball"
        },
        "is_answered": true,
        "view_count": 62,
        "accepted_answer_id": 65356759,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1608294108,
        "creation_date": 1608291584,
        "last_edit_date": 1608292145,
        "question_id": 65356414,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65356414/how-to-apply-a-function-to-every-element-in-a-dataframe",
        "title": "How to apply a function to every element in a dataframe?",
        "body": "<p>This is probably a very basic question but I can't find the answer in other questions. I have two lists that I have used to create a 2D dataframe, let's say:</p>\n<pre class=\"lang-py prettyprint-override\"><code>X= np.arange(0, 2.01, 0.25)\nY= np.arange(10, 30, 5.0) \n\ndf = pd.DataFrame(index = X, columns = Y)\nprint(df)\n</code></pre>\n<p>Which gives:</p>\n<pre class=\"lang-py prettyprint-override\"><code>     10.0 15.0 20.0 25.0\n0.00  NaN  NaN  NaN  NaN\n0.25  NaN  NaN  NaN  NaN\n0.50  NaN  NaN  NaN  NaN\n0.75  NaN  NaN  NaN  NaN\n1.00  NaN  NaN  NaN  NaN\n1.25  NaN  NaN  NaN  NaN\n1.50  NaN  NaN  NaN  NaN\n1.75  NaN  NaN  NaN  NaN\n2.00  NaN  NaN  NaN  NaN\n</code></pre>\n<p>I would like to go through all elements in the dataframe and use the values of <code>X</code> and <code>Y</code> as inputs to some function, <code>foo</code>, that I have written. For example, in the 2rd row, 1st column (using zero indexing) position I have <code>(X, Y) = (0.5, 15.0)</code>, so in this position I would like to apply <code>foo(0.5, 15.0)</code> and not <code>foo(2, 1)</code>.</p>\n<p>I think I should be able to use <code>df.apply()</code> or <code>df.applymap()</code> somehow but I can't figure it out!</p>\n",
        "answer_body": "<p>Since your problem requires access to both the index and column labels of your <code>df</code> you probably want <code>df.apply()</code>.</p>\n<p><code>df.apply()</code> has access to a <code>pandas.Series</code> representing each row/column (dependent on <code>axis</code> argument value) and you will have access to the column name and index; whereas <code>df.applymap()</code> utilises each individual value of <code>df</code> at runtime - so you wouldn't necessarily have access to the index and column name as required.</p>\n<p><strong>Example</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd \n\ndef foo(name, index):\n    return name - index\n\nx = np.arange(0, 2.01, 0.25)\ny = np.arange(10, 30, 5.0) \n\ndf = pd.DataFrame(index = x, columns = y)\n\ndf.apply(lambda x: foo(x.name, x.index))\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>       10.0   15.0   20.0   25.0\n0.00  10.00  15.00  20.00  25.00\n0.25   9.75  14.75  19.75  24.75\n0.50   9.50  14.50  19.50  24.50\n0.75   9.25  14.25  19.25  24.25\n1.00   9.00  14.00  19.00  24.00\n1.25   8.75  13.75  18.75  23.75\n1.50   8.50  13.50  18.50  23.50\n1.75   8.25  13.25  18.25  23.25\n2.00   8.00  13.00  18.00  23.00\n</code></pre>\n<p>In the above example the column name and index of each Series constituting <code>df</code> is passed to <code>foo()</code> by way of <code>df.apply()</code>. Within <code>foo()</code> each value is defined by subtracting it's own index value from it's own column name value. Here you can see that the index value for each row is accessed using <code>x.index</code> and the column value is accessed using <code>x.name</code> within the call within <code>df.apply()</code>.</p>\n<p><strong>Update</strong></p>\n<p>Many thanks to @SyntaxError for pointing out that <code>x.index</code> and <code>x.name</code> could be passed to <code>foo()</code> within <code>df.apply()</code> instead of feeding the entire Series (<code>x</code>) into the function and accessing the values manually therein. As mentioned, this seems to fit OP's use case in a much neater manner than my original response - which was largely the same but passed each <code>x</code> series into <code>foo()</code> which then had responsibility for extracting <code>x.name</code> and <code>x.column</code>.</p>\n",
        "question_body": "<p>This is probably a very basic question but I can't find the answer in other questions. I have two lists that I have used to create a 2D dataframe, let's say:</p>\n<pre class=\"lang-py prettyprint-override\"><code>X= np.arange(0, 2.01, 0.25)\nY= np.arange(10, 30, 5.0) \n\ndf = pd.DataFrame(index = X, columns = Y)\nprint(df)\n</code></pre>\n<p>Which gives:</p>\n<pre class=\"lang-py prettyprint-override\"><code>     10.0 15.0 20.0 25.0\n0.00  NaN  NaN  NaN  NaN\n0.25  NaN  NaN  NaN  NaN\n0.50  NaN  NaN  NaN  NaN\n0.75  NaN  NaN  NaN  NaN\n1.00  NaN  NaN  NaN  NaN\n1.25  NaN  NaN  NaN  NaN\n1.50  NaN  NaN  NaN  NaN\n1.75  NaN  NaN  NaN  NaN\n2.00  NaN  NaN  NaN  NaN\n</code></pre>\n<p>I would like to go through all elements in the dataframe and use the values of <code>X</code> and <code>Y</code> as inputs to some function, <code>foo</code>, that I have written. For example, in the 2rd row, 1st column (using zero indexing) position I have <code>(X, Y) = (0.5, 15.0)</code>, so in this position I would like to apply <code>foo(0.5, 15.0)</code> and not <code>foo(2, 1)</code>.</p>\n<p>I think I should be able to use <code>df.apply()</code> or <code>df.applymap()</code> somehow but I can't figure it out!</p>\n",
        "formatted_input": {
            "qid": 65356414,
            "link": "https://stackoverflow.com/questions/65356414/how-to-apply-a-function-to-every-element-in-a-dataframe",
            "question": {
                "title": "How to apply a function to every element in a dataframe?",
                "ques_desc": "This is probably a very basic question but I can't find the answer in other questions. I have two lists that I have used to create a 2D dataframe, let's say: Which gives: I would like to go through all elements in the dataframe and use the values of and as inputs to some function, , that I have written. For example, in the 2rd row, 1st column (using zero indexing) position I have , so in this position I would like to apply and not . I think I should be able to use or somehow but I can't figure it out! "
            },
            "io": [
                "     10.0 15.0 20.0 25.0\n0.00  NaN  NaN  NaN  NaN\n0.25  NaN  NaN  NaN  NaN\n0.50  NaN  NaN  NaN  NaN\n0.75  NaN  NaN  NaN  NaN\n1.00  NaN  NaN  NaN  NaN\n1.25  NaN  NaN  NaN  NaN\n1.50  NaN  NaN  NaN  NaN\n1.75  NaN  NaN  NaN  NaN\n2.00  NaN  NaN  NaN  NaN\n",
                "(X, Y) = (0.5, 15.0)"
            ],
            "answer": {
                "ans_desc": "Since your problem requires access to both the index and column labels of your you probably want . has access to a representing each row/column (dependent on argument value) and you will have access to the column name and index; whereas utilises each individual value of at runtime - so you wouldn't necessarily have access to the index and column name as required. Example Output In the above example the column name and index of each Series constituting is passed to by way of . Within each value is defined by subtracting it's own index value from it's own column name value. Here you can see that the index value for each row is accessed using and the column value is accessed using within the call within . Update Many thanks to @SyntaxError for pointing out that and could be passed to within instead of feeding the entire Series () into the function and accessing the values manually therein. As mentioned, this seems to fit OP's use case in a much neater manner than my original response - which was largely the same but passed each series into which then had responsibility for extracting and . ",
                "code": [
                    "import numpy as np\nimport pandas as pd \n\ndef foo(name, index):\n    return name - index\n\nx = np.arange(0, 2.01, 0.25)\ny = np.arange(10, 30, 5.0) \n\ndf = pd.DataFrame(index = x, columns = y)\n\ndf.apply(lambda x: foo(x.name, x.index))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 47,
            "user_id": 13376090,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-NvbgJW0vMeM/AAAAAAAAAAI/AAAAAAAAAAA/AKF05nBKbF2gKl2nAjJ7J6vB9oc9ouN3pA/photo.jpg?sz=128",
            "display_name": "MSCRN",
            "link": "https://stackoverflow.com/users/13376090/mscrn"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 65294893,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1607974313,
        "creation_date": 1607972313,
        "last_edit_date": 1607972399,
        "question_id": 65294879,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65294879/pandas-assigning-values-to-dataframe-conditional-on-values-in-another-with-the",
        "title": "Pandas assigning values to dataframe, conditional on values in another with the same dimensions issue/question",
        "body": "<p>I'm trying to better understand Pandas/Python so I've been playing around with some stuff. I ran into an issue, I know some workarounds, but I'm wondering why it happened in the first place.</p>\n<p>Here's my full code, followed by an explanation:</p>\n<pre><code>df1 = pd.DataFrame(np.random.rand(5, 10).round(2), index = list(range(1,6)), columns = list(range(1, 11)) )\ndf2 = pd.DataFrame(index = range(df1.shape[0]), columns = range(df1.shape[1]) )\n\ndf2[df1.iloc[:]&gt;0.6] = 1\ndf2[df1.iloc[:]&lt;0.6] = 0\n</code></pre>\n<p>I create, 2 dataframes. The first with random numbers, the second dataframe is empty but has the same dimensions as the first. Based on the values in the first dataframe, I'd like to modify the values in the second.</p>\n<pre><code>df1 = pd.DataFrame(np.random.rand(5, 10), index = list(range(1,6)), columns = list(range(1, 11)) )\n</code></pre>\n<p>My first datframe I create looks like this:</p>\n<pre><code>df1\n\n      1      2       3        4      5        6      7       8        9     10\n1   0.24    0.03    0.93    0.38    0.03    0.83    0.47    0.85    0.79    0.65\n2   0.66    0.25    0.01    0.28    0.19    0.26    0.25    0.48    0.33    0.92\n3   0.53    0.33    0.78    0.04    0.36    0.63    0.16    0.16    0.21    0.96\n4   0.76    0.03    0.89    0.15    0.24    0.90    0.59    0.41    0.92    0.98\n5   0.72    0.45    0.95    0.44    0.79    0.93    0.90    0.48    0.61    0.02\n</code></pre>\n<p>I create a second dataframe based on the dimensions of the second:</p>\n<pre><code>df2 = pd.DataFrame(index = range(df1.shape[0]), columns = range(df1.shape[1]) )\n</code></pre>\n<p>What I would like to do now, is say that for values that are greater 0.6 in df1, I would like the corresponding value in df2 to be 1. And for values less than 0.6 I would like the values to be 0.</p>\n<p>I did that in the following way, by slicing df1 and then using that slice on df2, and then assigning the values.</p>\n<pre><code>df2[df1.loc[:]&gt;0.6] = 1\ndf2[df1.loc[:]&lt;0.6] = 0\n</code></pre>\n<p>I thought this would work, but instead, the first row and first column are still NANs</p>\n<pre><code>df2\n\n         0   1   2   3   4  5   6   7   8   9\n 0      NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n 1      NaN  0   0   1   0   0   1   0   1   1\n 2      NaN  1   0   0   0   0   0   0   0   0\n 3      NaN  0   0   1   0   0   1   0   0   0\n 4      NaN  1   0   1   0   0   1   0   0   1\n</code></pre>\n<p>Now the reason this didn't work, I think, is because the column names and the row names don't align between the two indices, but what I'm trying to understand is why that's happening.</p>\n<p>I thought when I sliced df1 based on the conditional it created an array  of trues/falses, that I could use on any other dataframe with the same dimension:</p>\n<pre><code>[df1.loc[:]&gt;0.6]\n</code></pre>\n<p>r</p>\n<pre><code>       1      2      3      4      5      6      7      8      9      10\n  1  False  False   True  False  False   True  False   True   True   True\n  2   True  False  False  False  False  False  False  False  False   True\n  3  False  False   True  False  False   True  False  False  False   True\n  4   True  False   True  False  False   True  False  False   True   True\n  5   True  False   True  False   True   True   True  False   True  False\n</code></pre>\n<p>I thought that mapping of trues and falses above could be used anywhere, it seems like it can't. Is there a way around this doesn't involve renaming the columns/rows to match between the 2 dataframes?</p>\n",
        "answer_body": "<p>Just need to do:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nnp.random.seed(42)  # for reproducibility\ndf1 = pd.DataFrame(np.random.rand(5, 10), index = list(range(1,6)), columns = list(range(1, 11)))\ndf2 = df1 &gt; 0.6\nprint(df2)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>      1      2      3      4      5      6      7      8      9      10\n1  False   True   True  False  False  False  False   True   True   True\n2  False   True   True  False  False  False  False  False  False  False\n3   True  False  False  False  False   True  False  False  False  False\n4   True  False  False   True   True   True  False  False   True  False\n5  False  False  False   True  False   True  False  False  False  False\n</code></pre>\n<p>If need the output to be integer:</p>\n<pre><code>df2 = (df1 &gt; 0.6).astype(int)\nprint(df2)\n</code></pre>\n<p><strong>Output</strong> <em>(integer)</em></p>\n<pre><code>   1   2   3   4   5   6   7   8   9   10\n1   0   1   1   0   0   0   0   1   1   1\n2   0   1   1   0   0   0   0   0   0   0\n3   1   0   0   0   0   1   0   0   0   0\n4   1   0   0   1   1   1   0   0   1   0\n5   0   0   0   1   0   1   0   0   0   0\n</code></pre>\n<p>If need to map to values to True and False, use <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.where.html\" rel=\"nofollow noreferrer\">np.where</a>:</p>\n<pre><code>df2 = (df1 &gt; 0.6)\ndf2[:] = np.where(df2, 'M', 'F')\nprint(df2)\n</code></pre>\n<p><strong>Output</strong> (<em>where</em>)</p>\n<pre><code>  1  2  3  4  5  6  7  8  9  10\n1  F  M  M  F  F  F  F  M  M  M\n2  F  M  M  F  F  F  F  F  F  F\n3  M  F  F  F  F  M  F  F  F  F\n4  M  F  F  M  M  M  F  F  M  F\n5  F  F  F  M  F  M  F  F  F  F\n</code></pre>\n",
        "question_body": "<p>I'm trying to better understand Pandas/Python so I've been playing around with some stuff. I ran into an issue, I know some workarounds, but I'm wondering why it happened in the first place.</p>\n<p>Here's my full code, followed by an explanation:</p>\n<pre><code>df1 = pd.DataFrame(np.random.rand(5, 10).round(2), index = list(range(1,6)), columns = list(range(1, 11)) )\ndf2 = pd.DataFrame(index = range(df1.shape[0]), columns = range(df1.shape[1]) )\n\ndf2[df1.iloc[:]&gt;0.6] = 1\ndf2[df1.iloc[:]&lt;0.6] = 0\n</code></pre>\n<p>I create, 2 dataframes. The first with random numbers, the second dataframe is empty but has the same dimensions as the first. Based on the values in the first dataframe, I'd like to modify the values in the second.</p>\n<pre><code>df1 = pd.DataFrame(np.random.rand(5, 10), index = list(range(1,6)), columns = list(range(1, 11)) )\n</code></pre>\n<p>My first datframe I create looks like this:</p>\n<pre><code>df1\n\n      1      2       3        4      5        6      7       8        9     10\n1   0.24    0.03    0.93    0.38    0.03    0.83    0.47    0.85    0.79    0.65\n2   0.66    0.25    0.01    0.28    0.19    0.26    0.25    0.48    0.33    0.92\n3   0.53    0.33    0.78    0.04    0.36    0.63    0.16    0.16    0.21    0.96\n4   0.76    0.03    0.89    0.15    0.24    0.90    0.59    0.41    0.92    0.98\n5   0.72    0.45    0.95    0.44    0.79    0.93    0.90    0.48    0.61    0.02\n</code></pre>\n<p>I create a second dataframe based on the dimensions of the second:</p>\n<pre><code>df2 = pd.DataFrame(index = range(df1.shape[0]), columns = range(df1.shape[1]) )\n</code></pre>\n<p>What I would like to do now, is say that for values that are greater 0.6 in df1, I would like the corresponding value in df2 to be 1. And for values less than 0.6 I would like the values to be 0.</p>\n<p>I did that in the following way, by slicing df1 and then using that slice on df2, and then assigning the values.</p>\n<pre><code>df2[df1.loc[:]&gt;0.6] = 1\ndf2[df1.loc[:]&lt;0.6] = 0\n</code></pre>\n<p>I thought this would work, but instead, the first row and first column are still NANs</p>\n<pre><code>df2\n\n         0   1   2   3   4  5   6   7   8   9\n 0      NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n 1      NaN  0   0   1   0   0   1   0   1   1\n 2      NaN  1   0   0   0   0   0   0   0   0\n 3      NaN  0   0   1   0   0   1   0   0   0\n 4      NaN  1   0   1   0   0   1   0   0   1\n</code></pre>\n<p>Now the reason this didn't work, I think, is because the column names and the row names don't align between the two indices, but what I'm trying to understand is why that's happening.</p>\n<p>I thought when I sliced df1 based on the conditional it created an array  of trues/falses, that I could use on any other dataframe with the same dimension:</p>\n<pre><code>[df1.loc[:]&gt;0.6]\n</code></pre>\n<p>r</p>\n<pre><code>       1      2      3      4      5      6      7      8      9      10\n  1  False  False   True  False  False   True  False   True   True   True\n  2   True  False  False  False  False  False  False  False  False   True\n  3  False  False   True  False  False   True  False  False  False   True\n  4   True  False   True  False  False   True  False  False   True   True\n  5   True  False   True  False   True   True   True  False   True  False\n</code></pre>\n<p>I thought that mapping of trues and falses above could be used anywhere, it seems like it can't. Is there a way around this doesn't involve renaming the columns/rows to match between the 2 dataframes?</p>\n",
        "formatted_input": {
            "qid": 65294879,
            "link": "https://stackoverflow.com/questions/65294879/pandas-assigning-values-to-dataframe-conditional-on-values-in-another-with-the",
            "question": {
                "title": "Pandas assigning values to dataframe, conditional on values in another with the same dimensions issue/question",
                "ques_desc": "I'm trying to better understand Pandas/Python so I've been playing around with some stuff. I ran into an issue, I know some workarounds, but I'm wondering why it happened in the first place. Here's my full code, followed by an explanation: I create, 2 dataframes. The first with random numbers, the second dataframe is empty but has the same dimensions as the first. Based on the values in the first dataframe, I'd like to modify the values in the second. My first datframe I create looks like this: I create a second dataframe based on the dimensions of the second: What I would like to do now, is say that for values that are greater 0.6 in df1, I would like the corresponding value in df2 to be 1. And for values less than 0.6 I would like the values to be 0. I did that in the following way, by slicing df1 and then using that slice on df2, and then assigning the values. I thought this would work, but instead, the first row and first column are still NANs Now the reason this didn't work, I think, is because the column names and the row names don't align between the two indices, but what I'm trying to understand is why that's happening. I thought when I sliced df1 based on the conditional it created an array of trues/falses, that I could use on any other dataframe with the same dimension: r I thought that mapping of trues and falses above could be used anywhere, it seems like it can't. Is there a way around this doesn't involve renaming the columns/rows to match between the 2 dataframes? "
            },
            "io": [
                "df1\n\n      1      2       3        4      5        6      7       8        9     10\n1   0.24    0.03    0.93    0.38    0.03    0.83    0.47    0.85    0.79    0.65\n2   0.66    0.25    0.01    0.28    0.19    0.26    0.25    0.48    0.33    0.92\n3   0.53    0.33    0.78    0.04    0.36    0.63    0.16    0.16    0.21    0.96\n4   0.76    0.03    0.89    0.15    0.24    0.90    0.59    0.41    0.92    0.98\n5   0.72    0.45    0.95    0.44    0.79    0.93    0.90    0.48    0.61    0.02\n",
                "df2\n\n         0   1   2   3   4  5   6   7   8   9\n 0      NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n 1      NaN  0   0   1   0   0   1   0   1   1\n 2      NaN  1   0   0   0   0   0   0   0   0\n 3      NaN  0   0   1   0   0   1   0   0   0\n 4      NaN  1   0   1   0   0   1   0   0   1\n"
            ],
            "answer": {
                "ans_desc": "Just need to do: Output If need the output to be integer: Output (integer) If need to map to values to True and False, use np.where: Output (where) ",
                "code": [
                    "df2 = (df1 > 0.6).astype(int)\nprint(df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matrix"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 14696453,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d16c75071394b0501842f6ed20375ac5?s=128&d=identicon&r=PG&f=1",
            "display_name": "americ998",
            "link": "https://stackoverflow.com/users/14696453/americ998"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 65262327,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1607755993,
        "creation_date": 1607755052,
        "question_id": 65262268,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65262268/how-do-i-turn-categorical-column-values-into-different-column-names",
        "title": "How do I turn categorical column values into different column names?",
        "body": "<p>I'm not sure how to approach this problem since I'm a beginner with pandas.</p>\n<p>I have this dataframe:</p>\n<pre><code>  col1 col2\n0    a    1 \n1    a    2 \n2    a    3 \n3    b    4\n4    b    5\n5    b    6\n6    c    7\n7    c    8\n8    c    9\n</code></pre>\n<p>and I want to turn it into a dataframe or a matrix like this:</p>\n<pre><code>   cola colb  colc\n0    1    4    7\n1    2    5    8\n2    3    6    9\n</code></pre>\n<p>How should I approach this in Python?</p>\n",
        "answer_body": "<p>Let's <code>groupby</code> the dataframe on <code>col1</code> and create key-value pairs inside <code>dict</code> comprehension:</p>\n<pre><code>pd.DataFrame({k: [*g['col2']] for k, g in df.groupby('col1')})\n</code></pre>\n<p>Alternatively, you can use <code>groupby</code> + <code>cumcount</code> to create a sequential counter to distinguish different rows per group in <code>col1</code>, then use <code>set_index</code> + <code>unstack</code> to reshape:</p>\n<pre><code>df.set_index([df.groupby('col1').cumcount(), 'col1'])['col2'].unstack()\n</code></pre>\n<p>Another approach with <code>pivot_table</code> with <code>groupby</code> + <code>cumcount</code>:</p>\n<pre><code>df.pivot_table(index=df.groupby('col1').cumcount(), columns='col1', values='col2')\n</code></pre>\n<p>Result:</p>\n<pre><code>   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n</code></pre>\n",
        "question_body": "<p>I'm not sure how to approach this problem since I'm a beginner with pandas.</p>\n<p>I have this dataframe:</p>\n<pre><code>  col1 col2\n0    a    1 \n1    a    2 \n2    a    3 \n3    b    4\n4    b    5\n5    b    6\n6    c    7\n7    c    8\n8    c    9\n</code></pre>\n<p>and I want to turn it into a dataframe or a matrix like this:</p>\n<pre><code>   cola colb  colc\n0    1    4    7\n1    2    5    8\n2    3    6    9\n</code></pre>\n<p>How should I approach this in Python?</p>\n",
        "formatted_input": {
            "qid": 65262268,
            "link": "https://stackoverflow.com/questions/65262268/how-do-i-turn-categorical-column-values-into-different-column-names",
            "question": {
                "title": "How do I turn categorical column values into different column names?",
                "ques_desc": "I'm not sure how to approach this problem since I'm a beginner with pandas. I have this dataframe: and I want to turn it into a dataframe or a matrix like this: How should I approach this in Python? "
            },
            "io": [
                "  col1 col2\n0    a    1 \n1    a    2 \n2    a    3 \n3    b    4\n4    b    5\n5    b    6\n6    c    7\n7    c    8\n8    c    9\n",
                "   cola colb  colc\n0    1    4    7\n1    2    5    8\n2    3    6    9\n"
            ],
            "answer": {
                "ans_desc": "Let's the dataframe on and create key-value pairs inside comprehension: Alternatively, you can use + to create a sequential counter to distinguish different rows per group in , then use + to reshape: Another approach with with + : Result: ",
                "code": [
                    "pd.DataFrame({k: [*g['col2']] for k, g in df.groupby('col1')})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "floating-point"
        ],
        "owner": {
            "reputation": 99,
            "user_id": 9606753,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/11757f7d3a23ce660cb11dde53d82fd9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Max J.",
            "link": "https://stackoverflow.com/users/9606753/max-j"
        },
        "is_answered": true,
        "view_count": 78,
        "accepted_answer_id": 65223541,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1607541820,
        "creation_date": 1607541614,
        "question_id": 65223498,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65223498/unprecise-values-when-using-pd-dataframe-values-tolist",
        "title": "Unprecise values when using pd.Dataframe.values.tolist",
        "body": "<p>When converting a pd.DataFrame to a nested list, some values are unprecise.</p>\n<p>pd.DataFrame examplary row:</p>\n<pre><code>1.0 -3.0 -3.0 0.01 -3.0 -1.0\n</code></pre>\n<p>pd.DataFrame.values.tolist() of this row:</p>\n<pre><code>[1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0]\n</code></pre>\n<p>How can this be explained and avoided?</p>\n",
        "answer_body": "<p>This is because this <em>is</em> the original value. When you display the pd.DataFrame it gets rounded:</p>\n<pre><code>df = pd.DataFrame({'a':[1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0]})\n\n    a\n0   1.00\n1   -3.00\n2   -3.00\n3   0.01\n4   -3.00\n5   -1.00\n</code></pre>\n<pre><code>df.values.tolist()\n# [[1.0], [-3.0], [-3.0], [0.010000000000000009], [-3.0], [-1.0]]\n</code></pre>\n<p>So it is not tolist()'s problem. It is pd.DataFrame that is rounding the numbers.</p>\n<p>Use <code>pandas.set_option(&quot;display.precision&quot;, x)</code> to set display precision for DataFrame.</p>\n",
        "question_body": "<p>When converting a pd.DataFrame to a nested list, some values are unprecise.</p>\n<p>pd.DataFrame examplary row:</p>\n<pre><code>1.0 -3.0 -3.0 0.01 -3.0 -1.0\n</code></pre>\n<p>pd.DataFrame.values.tolist() of this row:</p>\n<pre><code>[1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0]\n</code></pre>\n<p>How can this be explained and avoided?</p>\n",
        "formatted_input": {
            "qid": 65223498,
            "link": "https://stackoverflow.com/questions/65223498/unprecise-values-when-using-pd-dataframe-values-tolist",
            "question": {
                "title": "Unprecise values when using pd.Dataframe.values.tolist",
                "ques_desc": "When converting a pd.DataFrame to a nested list, some values are unprecise. pd.DataFrame examplary row: pd.DataFrame.values.tolist() of this row: How can this be explained and avoided? "
            },
            "io": [
                "1.0 -3.0 -3.0 0.01 -3.0 -1.0\n",
                "[1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0]\n"
            ],
            "answer": {
                "ans_desc": "This is because this is the original value. When you display the pd.DataFrame it gets rounded: So it is not tolist()'s problem. It is pd.DataFrame that is rounding the numbers. Use to set display precision for DataFrame. ",
                "code": [
                    "df.values.tolist()\n# [[1.0], [-3.0], [-3.0], [0.010000000000000009], [-3.0], [-1.0]]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 99,
            "user_id": 9606753,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/11757f7d3a23ce660cb11dde53d82fd9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Max J.",
            "link": "https://stackoverflow.com/users/9606753/max-j"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 65222412,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1607536920,
        "creation_date": 1607536061,
        "question_id": 65222196,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65222196/efficiently-relocate-elements-conditionally-in-a-panda-dataframe",
        "title": "Efficiently relocate elements conditionally in a panda.Dataframe",
        "body": "<p>I am trying to sort the values of my data.frame in the following way:</p>\n<pre><code>for i in range(len(df.index)):\n    if df.at[i, &quot;x1&quot;] &lt;= df.at[i, &quot;x2&quot;]:\n        df2.at[i, &quot;p1&quot;] = df.at[i, &quot;p1&quot;]\n        df2.at[i, &quot;x1&quot;] = df.at[i, &quot;x1&quot;]\n        df2.at[i, &quot;x2&quot;] = df.at[i, &quot;x2&quot;]\n    else:\n        df2.at[i, &quot;p1&quot;] = df.at[i, &quot;p2&quot;]\n        df2.at[i, &quot;x1&quot;] = df.at[i, &quot;x2&quot;]\n        df2.at[i, &quot;x2&quot;] = df.at[i, &quot;x1&quot;]\n</code></pre>\n<p>It is working, however it is very slow for my +40k rows. How can I do this more efficiently and more elegantly? I would prefer a solution that directly manipulates the original df, if possible.</p>\n<p>Example data:</p>\n<pre><code>x1  p1 x2   p2\n1  0.4  2  0.6\n2  0.2  1  0.8\n</code></pre>\n<p>Desired output:</p>\n<pre><code>x1  p1 x2   p2\n1  0.4  2  0.6\n1  0.8  2  0.2\n</code></pre>\n",
        "answer_body": "<p>Here's one way that use a selection of the rows and then does a swap of the values using that selection</p>\n<pre><code>check = df[&quot;x1&quot;] &gt; df[&quot;x2&quot;]\ndf.loc[check, [&quot;x2&quot;, &quot;x1&quot;, &quot;p2&quot;, &quot;p1&quot;]] = df.loc[check, [&quot;x1&quot;, &quot;x2&quot;, &quot;p1&quot;, &quot;p2&quot;]].values\n</code></pre>\n",
        "question_body": "<p>I am trying to sort the values of my data.frame in the following way:</p>\n<pre><code>for i in range(len(df.index)):\n    if df.at[i, &quot;x1&quot;] &lt;= df.at[i, &quot;x2&quot;]:\n        df2.at[i, &quot;p1&quot;] = df.at[i, &quot;p1&quot;]\n        df2.at[i, &quot;x1&quot;] = df.at[i, &quot;x1&quot;]\n        df2.at[i, &quot;x2&quot;] = df.at[i, &quot;x2&quot;]\n    else:\n        df2.at[i, &quot;p1&quot;] = df.at[i, &quot;p2&quot;]\n        df2.at[i, &quot;x1&quot;] = df.at[i, &quot;x2&quot;]\n        df2.at[i, &quot;x2&quot;] = df.at[i, &quot;x1&quot;]\n</code></pre>\n<p>It is working, however it is very slow for my +40k rows. How can I do this more efficiently and more elegantly? I would prefer a solution that directly manipulates the original df, if possible.</p>\n<p>Example data:</p>\n<pre><code>x1  p1 x2   p2\n1  0.4  2  0.6\n2  0.2  1  0.8\n</code></pre>\n<p>Desired output:</p>\n<pre><code>x1  p1 x2   p2\n1  0.4  2  0.6\n1  0.8  2  0.2\n</code></pre>\n",
        "formatted_input": {
            "qid": 65222196,
            "link": "https://stackoverflow.com/questions/65222196/efficiently-relocate-elements-conditionally-in-a-panda-dataframe",
            "question": {
                "title": "Efficiently relocate elements conditionally in a panda.Dataframe",
                "ques_desc": "I am trying to sort the values of my data.frame in the following way: It is working, however it is very slow for my +40k rows. How can I do this more efficiently and more elegantly? I would prefer a solution that directly manipulates the original df, if possible. Example data: Desired output: "
            },
            "io": [
                "x1  p1 x2   p2\n1  0.4  2  0.6\n2  0.2  1  0.8\n",
                "x1  p1 x2   p2\n1  0.4  2  0.6\n1  0.8  2  0.2\n"
            ],
            "answer": {
                "ans_desc": "Here's one way that use a selection of the rows and then does a swap of the values using that selection ",
                "code": [
                    "check = df[\"x1\"] > df[\"x2\"]\ndf.loc[check, [\"x2\", \"x1\", \"p2\", \"p1\"]] = df.loc[check, [\"x1\", \"x2\", \"p1\", \"p2\"]].values\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "arrays",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 8103036,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-DINuVDq6RCA/AAAAAAAAAAI/AAAAAAAAAs8/w5f19ubSbXo/photo.jpg?sz=128",
            "display_name": "Khizar",
            "link": "https://stackoverflow.com/users/8103036/khizar"
        },
        "is_answered": true,
        "view_count": 771,
        "accepted_answer_id": 63282418,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1607423369,
        "creation_date": 1596712259,
        "last_edit_date": 1605267729,
        "question_id": 63282322,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63282322/how-to-read-list-of-json-objects-from-pandas-dataframe",
        "title": "How to read list of json objects from Pandas DataFrame?",
        "body": "<p>I want just want to loop through the array of json objects, and get the values of 'box'.....</p>\n<p>I have a DataFrame which looks like this</p>\n<pre><code>       img                    facesJson\n0   2b26mn4.jpg [{'box': [57, 255, 91, 103], 'confidence': 0.7...\n1   cd7ntf.jpg  [{'box': [510, 85, 58, 87], 'confidence': 0.99...\n2   m9kf3e.jpg  [{'box': [328, 78, 93, 123], 'confidence': 0.9...\n3   b4hx0n.jpg  [{'box': [129, 30, 38, 54], 'confidence': 0.99...\n4   afx0fm.jpg  [{'box': [86, 126, 221, 298], 'confidence': 0....\n</code></pre>\n<p>and the column 'facesJson' (dstype = object) contain array of json objects which look like this:</p>\n<pre><code>[\n   {\n      &quot;box&quot;:[ 158,115,84,112 ],\n      &quot;confidence&quot;:0.9998929500579834,\n   },\n   {\n      &quot;box&quot;:[ 404,105, 86,114 ],\n      &quot;confidence&quot;:0.9996863603591919,\n   }\n]\n</code></pre>\n<p>when i run this code</p>\n<pre><code>for index,row in df.iterrows():\n  df = pd.json_normalize(row['facesJson'])\n  print(len(df))\n</code></pre>\n<p>i get this error:</p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-53-8be848c6d44a&gt; in &lt;module&gt;()\n      1 for index,row in savedList.iterrows():\n----&gt; 2   df = pd.json_normalize(row['facesJson'])\n      3   print(len(df))\n\n1 frames\n/usr/local/lib/python3.6/dist-packages/pandas/io/json/_normalize.py in &lt;genexpr&gt;(.0)\n    272 \n    273     if record_path is None:\n--&gt; 274         if any([isinstance(x, dict) for x in y.values()] for y in data):\n    275             # naive normalization, this is idempotent for flat records\n    276             # and potentially will inflate the data considerably for\n\nAttributeError: 'str' object has no attribute 'values'\n</code></pre>\n",
        "answer_body": "<p>You can use this code on loop for each item of that column.</p>\n<pre><code>import ast\n\nline='[ { &quot;box&quot;:[ 158,115,84,112 ], &quot;confidence&quot;:0.9998929500579834, }, { &quot;box&quot;:[ 404,105, 86,114 ], &quot;confidence&quot;:0.9996863603591919, } ]'\n\n\nparsed=ast.literal_eval(line)\n\nprint(parsed[0].keys())\n</code></pre>\n",
        "question_body": "<p>I want just want to loop through the array of json objects, and get the values of 'box'.....</p>\n<p>I have a DataFrame which looks like this</p>\n<pre><code>       img                    facesJson\n0   2b26mn4.jpg [{'box': [57, 255, 91, 103], 'confidence': 0.7...\n1   cd7ntf.jpg  [{'box': [510, 85, 58, 87], 'confidence': 0.99...\n2   m9kf3e.jpg  [{'box': [328, 78, 93, 123], 'confidence': 0.9...\n3   b4hx0n.jpg  [{'box': [129, 30, 38, 54], 'confidence': 0.99...\n4   afx0fm.jpg  [{'box': [86, 126, 221, 298], 'confidence': 0....\n</code></pre>\n<p>and the column 'facesJson' (dstype = object) contain array of json objects which look like this:</p>\n<pre><code>[\n   {\n      &quot;box&quot;:[ 158,115,84,112 ],\n      &quot;confidence&quot;:0.9998929500579834,\n   },\n   {\n      &quot;box&quot;:[ 404,105, 86,114 ],\n      &quot;confidence&quot;:0.9996863603591919,\n   }\n]\n</code></pre>\n<p>when i run this code</p>\n<pre><code>for index,row in df.iterrows():\n  df = pd.json_normalize(row['facesJson'])\n  print(len(df))\n</code></pre>\n<p>i get this error:</p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-53-8be848c6d44a&gt; in &lt;module&gt;()\n      1 for index,row in savedList.iterrows():\n----&gt; 2   df = pd.json_normalize(row['facesJson'])\n      3   print(len(df))\n\n1 frames\n/usr/local/lib/python3.6/dist-packages/pandas/io/json/_normalize.py in &lt;genexpr&gt;(.0)\n    272 \n    273     if record_path is None:\n--&gt; 274         if any([isinstance(x, dict) for x in y.values()] for y in data):\n    275             # naive normalization, this is idempotent for flat records\n    276             # and potentially will inflate the data considerably for\n\nAttributeError: 'str' object has no attribute 'values'\n</code></pre>\n",
        "formatted_input": {
            "qid": 63282322,
            "link": "https://stackoverflow.com/questions/63282322/how-to-read-list-of-json-objects-from-pandas-dataframe",
            "question": {
                "title": "How to read list of json objects from Pandas DataFrame?",
                "ques_desc": "I want just want to loop through the array of json objects, and get the values of 'box'..... I have a DataFrame which looks like this and the column 'facesJson' (dstype = object) contain array of json objects which look like this: when i run this code i get this error: "
            },
            "io": [
                "       img                    facesJson\n0   2b26mn4.jpg [{'box': [57, 255, 91, 103], 'confidence': 0.7...\n1   cd7ntf.jpg  [{'box': [510, 85, 58, 87], 'confidence': 0.99...\n2   m9kf3e.jpg  [{'box': [328, 78, 93, 123], 'confidence': 0.9...\n3   b4hx0n.jpg  [{'box': [129, 30, 38, 54], 'confidence': 0.99...\n4   afx0fm.jpg  [{'box': [86, 126, 221, 298], 'confidence': 0....\n",
                "[\n   {\n      \"box\":[ 158,115,84,112 ],\n      \"confidence\":0.9998929500579834,\n   },\n   {\n      \"box\":[ 404,105, 86,114 ],\n      \"confidence\":0.9996863603591919,\n   }\n]\n"
            ],
            "answer": {
                "ans_desc": "You can use this code on loop for each item of that column. ",
                "code": [
                    "import ast\n\nline='[ { \"box\":[ 158,115,84,112 ], \"confidence\":0.9998929500579834, }, { \"box\":[ 404,105, 86,114 ], \"confidence\":0.9996863603591919, } ]'\n\n\nparsed=ast.literal_eval(line)\n\nprint(parsed[0].keys())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 81,
            "user_id": 12853501,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/19d33582b6ac848a77a98ed34054e02d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Joe",
            "link": "https://stackoverflow.com/users/12853501/joe"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 65194496,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1607415762,
        "creation_date": 1607410803,
        "last_edit_date": 1607415762,
        "question_id": 65194434,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65194434/python-how-do-i-merge-rows-that-shares-the-same-name-of-ocode-or-ccode-in-da",
        "title": "Python: How do I merge rows that shares the same name of &quot;Ocode&quot; or &quot;Ccode in dataframe",
        "body": "<p>I'm thinking of using pandas to merge several repetitive rows of &quot;Ocode&quot; and Ccode&quot;. Ideally I want only one &quot;Ocode&quot; or &quot;Ccode&quot; per row. When there are repetitive dates under c## (I.E. c21), only the latest date is kept. Separate dates under different column with the same &quot;Ocode&quot;/&quot;Ccode&quot; should also be merged.</p>\n<p>(For reference purpose: O and C code correspondingly represents Organization Code and Corporation code.)</p>\n<p>This is the heading of the dataframe.</p>\n<pre><code>Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75\n0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09\n1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n2    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN\n4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN\n...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         \n\n2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN\n2385       NaN  CU0280002  2016-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN\n2386       NaN  CU0280002         NaN  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN\n2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN\n</code></pre>\n<p>Which should become ----&gt;</p>\n<pre><code>Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75\n0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09\n1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN\n4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN\n...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         \n2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN\n2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN\n</code></pre>\n<p>Attempt:</p>\n<pre><code>import os\nimport xlsxwriter\nimport pandas as pd\nimport numpy as np\n\n# Pandas and XR\n\ndf = pd.read_excel('Result - Company.xlsx')\n\n#ic = 'c21'#IndustryCode\nic = 'c57'\n#ic = 'c58'\n#ic = 'c59'\n#ic = 'c70'\n#ic = 'c71'\n#ic = 'c74'\n#ic = 'c75'\n\ndf[ic] = pd.to_datetime(df[ic]) # , errors='coerce'\ndf = df.sort_values(ic).drop_duplicates('Ccode', keep='last')\n</code></pre>\n<p>and perform the merge one by one. However, this method tends to delete information under other column when dealing with one of the c##(I.E. c21) column</p>\n<p>df.to_excel(ic + '.xlsx', index=False)</p>\n",
        "answer_body": "<p>One idea is grouping by both columns witt replace <code>NaN</code> for avoid remove this groups in oldier pandas versions with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.last.html\" rel=\"nofollow noreferrer\"><code>GroupBy.last</code></a> for last non missing values per groups:</p>\n<pre><code>df = (df.assign(Ocode = df['Ocode'].fillna('nan'),Ccode = df['Ccode'].fillna('nan'))\n        .groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index()\n        .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n</code></pre>\n<p>For last versions of pandas:</p>\n<pre><code>df = (df.groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index())\n</code></pre>\n<hr />\n<pre><code>print (df)\n       Ocode      Ccode         c21  c57         c58  c59  c70         c71  \\\n0  BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN   \n1  CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31   \n2  CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN   \n3  CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN   \n4        NaN  CU0280002  2016-12-31  NaN         NaN  NaN  NaN         NaN   \n5        NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN   \n\n          c74         c75  \n0         NaN  2021-02-09  \n1         NaN         NaN  \n2         NaN         NaN  \n3  2020-06-04         NaN  \n4  2017-12-31         NaN  \n5  2022-05-31         NaN  \n</code></pre>\n<p>EDIT:</p>\n<p>Solution above test combination of both columns, if need prioritize <code>Ocode</code> it means set <code>NaN</code> to <code>Ccode</code> if exist both use before solution above:</p>\n<pre><code>df.loc[df['Ocode'].notna() &amp; df['Ocode'].notna(), 'Ccode'] = np.nan\n</code></pre>\n<p>EDIT1: One idea processing both codes separately:</p>\n<pre><code>df1 = (df.assign(Ocode = df['Ocode'].fillna('nan'))\n         .drop('Ccode', axis=1)\n         .groupby('Ocode')\n         .last()\n         .reset_index())\n\ndf2 = (df.assign(Ccode = df['Ccode'].fillna('nan'))\n         .drop('Ocode', axis=1)\n         .groupby('Ccode')\n         .last()\n         .reset_index())\n\nboth = (pd.concat([df1, df2], sort=False)\n          .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n</code></pre>\n",
        "question_body": "<p>I'm thinking of using pandas to merge several repetitive rows of &quot;Ocode&quot; and Ccode&quot;. Ideally I want only one &quot;Ocode&quot; or &quot;Ccode&quot; per row. When there are repetitive dates under c## (I.E. c21), only the latest date is kept. Separate dates under different column with the same &quot;Ocode&quot;/&quot;Ccode&quot; should also be merged.</p>\n<p>(For reference purpose: O and C code correspondingly represents Organization Code and Corporation code.)</p>\n<p>This is the heading of the dataframe.</p>\n<pre><code>Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75\n0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09\n1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n2    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN\n4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN\n...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         \n\n2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN\n2385       NaN  CU0280002  2016-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN\n2386       NaN  CU0280002         NaN  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN\n2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN\n</code></pre>\n<p>Which should become ----&gt;</p>\n<pre><code>Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75\n0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09\n1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN\n4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN\n...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         \n2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN\n2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN\n</code></pre>\n<p>Attempt:</p>\n<pre><code>import os\nimport xlsxwriter\nimport pandas as pd\nimport numpy as np\n\n# Pandas and XR\n\ndf = pd.read_excel('Result - Company.xlsx')\n\n#ic = 'c21'#IndustryCode\nic = 'c57'\n#ic = 'c58'\n#ic = 'c59'\n#ic = 'c70'\n#ic = 'c71'\n#ic = 'c74'\n#ic = 'c75'\n\ndf[ic] = pd.to_datetime(df[ic]) # , errors='coerce'\ndf = df.sort_values(ic).drop_duplicates('Ccode', keep='last')\n</code></pre>\n<p>and perform the merge one by one. However, this method tends to delete information under other column when dealing with one of the c##(I.E. c21) column</p>\n<p>df.to_excel(ic + '.xlsx', index=False)</p>\n",
        "formatted_input": {
            "qid": 65194434,
            "link": "https://stackoverflow.com/questions/65194434/python-how-do-i-merge-rows-that-shares-the-same-name-of-ocode-or-ccode-in-da",
            "question": {
                "title": "Python: How do I merge rows that shares the same name of &quot;Ocode&quot; or &quot;Ccode in dataframe",
                "ques_desc": "I'm thinking of using pandas to merge several repetitive rows of \"Ocode\" and Ccode\". Ideally I want only one \"Ocode\" or \"Ccode\" per row. When there are repetitive dates under c## (I.E. c21), only the latest date is kept. Separate dates under different column with the same \"Ocode\"/\"Ccode\" should also be merged. (For reference purpose: O and C code correspondingly represents Organization Code and Corporation code.) This is the heading of the dataframe. Which should become ----> Attempt: and perform the merge one by one. However, this method tends to delete information under other column when dealing with one of the c##(I.E. c21) column df.to_excel(ic + '.xlsx', index=False) "
            },
            "io": [
                "Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75\n0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09\n1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n2    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN\n4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN\n...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         \n\n2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN\n2385       NaN  CU0280002  2016-12-31  NaN         NaN  NaN  NaN         NaN         NaN         NaN\n2386       NaN  CU0280002         NaN  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN\n2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN\n",
                "Num      Ocode      Ccode         c21  c57         c58  c59  c70         c71         c74         c75\n0    BK0001000        NaN         NaN  NaN         NaN  NaN  NaN         NaN         NaN  2021-02-09\n1    CU0030000        NaN         NaN  NaN         NaN  NaN  NaN  2021-12-31         NaN         NaN\n3    CU0048000        NaN         NaN  NaN  2018-06-19  NaN  NaN         NaN         NaN         NaN\n4    CU0056000        NaN         NaN  NaN         NaN  NaN  NaN         NaN  2020-06-04         NaN\n...        ...        ...         ...  ...         ...  ...  ...         ...         ...         ...         \n2384       NaN  CU0280002  2017-12-31  NaN         NaN  NaN  NaN         NaN  2017-12-31         NaN\n2387       NaN  CU0659001         NaN  NaN         NaN  NaN  NaN         NaN  2022-05-31         NaN\n"
            ],
            "answer": {
                "ans_desc": "One idea is grouping by both columns witt replace for avoid remove this groups in oldier pandas versions with for last non missing values per groups: For last versions of pandas: EDIT: Solution above test combination of both columns, if need prioritize it means set to if exist both use before solution above: EDIT1: One idea processing both codes separately: ",
                "code": [
                    "df = (df.assign(Ocode = df['Ocode'].fillna('nan'),Ccode = df['Ccode'].fillna('nan'))\n        .groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index()\n        .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n",
                    "df = (df.groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index())\n",
                    "df.loc[df['Ocode'].notna() & df['Ocode'].notna(), 'Ccode'] = np.nan\n",
                    "df1 = (df.assign(Ocode = df['Ocode'].fillna('nan'))\n         .drop('Ccode', axis=1)\n         .groupby('Ocode')\n         .last()\n         .reset_index())\n\ndf2 = (df.assign(Ccode = df['Ccode'].fillna('nan'))\n         .drop('Ocode', axis=1)\n         .groupby('Ccode')\n         .last()\n         .reset_index())\n\nboth = (pd.concat([df1, df2], sort=False)\n          .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 241,
            "user_id": 4758057,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/cefea0480f74fd07afb86141dc739753?s=128&d=identicon&r=PG&f=1",
            "display_name": "AndrewK",
            "link": "https://stackoverflow.com/users/4758057/andrewk"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 65178883,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1607336272,
        "creation_date": 1607332137,
        "question_id": 65178805,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65178805/how-do-i-apply-a-function-to-the-groupby-sub-groups-that-depends-on-multiple-col",
        "title": "How do I apply a function to the groupby sub-groups that depends on multiple columns?",
        "body": "<p>Take the following data frame and groupby object.</p>\n<pre><code>df = pd.DataFrame([[1, 2, 3],[1, 4, 5],[2, 5, 6]], columns=['a', 'b', 'c'])\n\nprint(df)\n   a  b  c\n0  1  2  3\n1  1  4  5\n2  2  5  6\n\ndfGrouped = df.groupby(['a'])\n</code></pre>\n<p>How would I apply to the groupby object <code>dfGrouped</code>, multiplying each element of <code>b</code> and <code>c</code> together and then taking the sum. So for this example, <code>2*3 + 4*5 = 26</code> for the <code>1</code> group and <code>5*6 = 30</code> for the <code>0</code> group.</p>\n<p>So my desired output for the groupby object is:</p>\n<pre><code>   a  f\n0  1  26\n2  2  30\n</code></pre>\n",
        "answer_body": "<p>Do:</p>\n<pre><code>df = pd.DataFrame([[1, 2, 3],[1, 4, 5],[2, 5, 6]], columns=['a', 'b', 'c'])\ndf['f'] = df['c'] * df['b']\nres = df.groupby('a', as_index=False)['f'].sum()\nprint(res)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>   a   f\n0  1  26\n1  2  30\n</code></pre>\n",
        "question_body": "<p>Take the following data frame and groupby object.</p>\n<pre><code>df = pd.DataFrame([[1, 2, 3],[1, 4, 5],[2, 5, 6]], columns=['a', 'b', 'c'])\n\nprint(df)\n   a  b  c\n0  1  2  3\n1  1  4  5\n2  2  5  6\n\ndfGrouped = df.groupby(['a'])\n</code></pre>\n<p>How would I apply to the groupby object <code>dfGrouped</code>, multiplying each element of <code>b</code> and <code>c</code> together and then taking the sum. So for this example, <code>2*3 + 4*5 = 26</code> for the <code>1</code> group and <code>5*6 = 30</code> for the <code>0</code> group.</p>\n<p>So my desired output for the groupby object is:</p>\n<pre><code>   a  f\n0  1  26\n2  2  30\n</code></pre>\n",
        "formatted_input": {
            "qid": 65178805,
            "link": "https://stackoverflow.com/questions/65178805/how-do-i-apply-a-function-to-the-groupby-sub-groups-that-depends-on-multiple-col",
            "question": {
                "title": "How do I apply a function to the groupby sub-groups that depends on multiple columns?",
                "ques_desc": "Take the following data frame and groupby object. How would I apply to the groupby object , multiplying each element of and together and then taking the sum. So for this example, for the group and for the group. So my desired output for the groupby object is: "
            },
            "io": [
                "2*3 + 4*5 = 26",
                "   a  f\n0  1  26\n2  2  30\n"
            ],
            "answer": {
                "ans_desc": "Do: Output ",
                "code": [
                    "df = pd.DataFrame([[1, 2, 3],[1, 4, 5],[2, 5, 6]], columns=['a', 'b', 'c'])\ndf['f'] = df['c'] * df['b']\nres = df.groupby('a', as_index=False)['f'].sum()\nprint(res)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "separator",
            "txt"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 14774341,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/zZtwZ.jpg?s=128&g=1",
            "display_name": "2Napasa",
            "link": "https://stackoverflow.com/users/14774341/2napasa"
        },
        "is_answered": true,
        "view_count": 93,
        "accepted_answer_id": 65172306,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1607283803,
        "creation_date": 1607264059,
        "last_edit_date": 1607277007,
        "question_id": 65169011,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65169011/parsing-a-txt-file-into-data-frame-filling-columns-based-on-the-multiple-separa",
        "title": "Parsing a txt file into data frame, filling columns based on the multiple separators",
        "body": "<p>Having a .txt file</p>\n<p>structure as below</p>\n<pre><code>#n  1\na 1:0.0002 3:0.0003...\n#n  2\nb 2:0.0002 3:0.0003...\n#n  3\na 1:0.0002 2:0.0003... \n...\n</code></pre>\n<p>trying to parse into dataframe of the following structure</p>\n<pre><code>#    type  1        2       3 \n1    a     0.0002   null    0.0003 ....\n2    b     null     0.0002  0.0003 ....\n3    a     0.0002   0.0003  null   ....\n...\n</code></pre>\n<p>describing the rule:</p>\n<p># i - 'i' is the row number<br />\nn:data  - 'n' is the column number to fill, 'data' is the value to fill into i'th row</p>\n<p>if the number of columns would be small enough it could be done manually, but txt considered has roughly 2000-3000 column values and some of them are missing.</p>\n<pre><code>import pandas as pd\ndata = pd.read_csv(&quot;filename.txt&quot;, sep = &quot;#&quot;, header = None)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/u4jM0.png\" rel=\"nofollow noreferrer\">gives the following result</a></p>\n<pre><code>data1 = data.iloc[1::2]\ndata2 = data.iloc[::2]\n</code></pre>\n<p>I tried to remove the odd rows in data1 even in data2, then will hopefully figure out how to split the odd and merge the 2 df's, but there might be a faster and more beautiful method to do it, that's why asking here</p>\n<p>update, spent 3 hours figuring out how to work with dataframes, as I was not that familiar with them.\nnow from\n<a href=\"https://i.stack.imgur.com/4i1Pd.png\" rel=\"nofollow noreferrer\">that</a></p>\n<p>using</p>\n<pre><code>import pandas as pd\ndf = pd.read_csv(&quot;myfile.txt&quot;, sep = &quot;#&quot;, header = None)\nfor index, col in df.iterrows():\n    if index%2 == 0:\n        col[1] = int(col[1].split('\\t')[1])\nfor index, col in df.iterrows():\n    if index%2 == 1:\n#         print(col[0])\n        col[0] = col[0].split(' ')\ndf[0] = df[0].shift(-1)\ndf = df.iloc[::2]\ndf = df[[1,0]]\ndf = df.rename(columns={0: 1, 1: 0})\ndf.index = range(len(df))\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/AsnN1.png\" rel=\"nofollow noreferrer\">It became this</a></p>\n<p>any suggestions on how to add unknown number of phantom columnsnd fill them using &quot;n:value&quot; from the list to fill the &quot;n&quot; column with the &quot;value&quot;?</p>\n",
        "answer_body": "<p>I think you are better off parsing the file yourself than relying on <code>read_csv</code> and then dealing with the mess. Here is how I would do it. Since I do not have access to your real file I am using a small example you have in your question.\nFirst, load the file.</p>\n<pre><code>from io import StringIO\nfile = StringIO(\n&quot;&quot;&quot;\\\n#n  1\na 1:0.0002 3:0.0003\n#n  2\nb 2:0.0002 3:0.0003\n#n  3\na 1:0.0002 2:0.0003\n&quot;&quot;&quot;)\n# You would just use file = open('myfile.txt','r) instead of the above\n</code></pre>\n<p>Then we read all lines, group them in pairs, parse and stick the results into a dict</p>\n<pre><code># read all lines\nlines = file.readlines()\n\n# here we will store the results, dictionary of dictionaries\nparsing_res = {}\n\n# a fancy way of processing two lines, odd and even, at the same time\nfor line1,line2 in zip(lines[::2],lines[1::2]):\n    # line1 has the form '#n  1', we split on whitespace and take the second tokem\n    row_index = line1.split()[1]\n    # line2 is the other type of lines, split into tokens by whitespace\n    tokens = line2.split()\n    # first one is 'type'\n    t = tokens[0]\n\n    # the others are pairs 'x:y', split them into x,y and stick into a dictionary with label x and value y\n    row_dict = {token.split(':')[0]:token.split(':')[1] for token in tokens[1:]}\n\n    # add type\n    row_dict['type'] = t\n   \n    # store the result for these two lines into the main dictionary\n    parsing_res[row_index] = row_dict\nparsing_res\n</code></pre>\n<p>Now we have something that looks like this:</p>\n<pre><code>{'1': {'1': '0.0002', '3': '0.0003', 'type': 'a'},\n '2': {'2': '0.0002', '3': '0.0003', 'type': 'b'},\n '3': {'1': '0.0002', '2': '0.0003', 'type': 'a'}}\n</code></pre>\n<p>this dict can now be used directly to create a dataframe, which we proceed to do and also order columns as they are in somewhat random order</p>\n<pre><code>df = pd.DataFrame.from_dict(parsing_res, orient='index')\ndf.reindex(sorted(df.columns), axis=1).reindex(sorted(df.index), axis=0)\n</code></pre>\n<p>output</p>\n<pre><code>    1       2       3       type\n1   0.0002  NaN     0.0003  a\n2   NaN     0.0002  0.0003  b\n3   0.0002  0.0003  NaN     a\n</code></pre>\n",
        "question_body": "<p>Having a .txt file</p>\n<p>structure as below</p>\n<pre><code>#n  1\na 1:0.0002 3:0.0003...\n#n  2\nb 2:0.0002 3:0.0003...\n#n  3\na 1:0.0002 2:0.0003... \n...\n</code></pre>\n<p>trying to parse into dataframe of the following structure</p>\n<pre><code>#    type  1        2       3 \n1    a     0.0002   null    0.0003 ....\n2    b     null     0.0002  0.0003 ....\n3    a     0.0002   0.0003  null   ....\n...\n</code></pre>\n<p>describing the rule:</p>\n<p># i - 'i' is the row number<br />\nn:data  - 'n' is the column number to fill, 'data' is the value to fill into i'th row</p>\n<p>if the number of columns would be small enough it could be done manually, but txt considered has roughly 2000-3000 column values and some of them are missing.</p>\n<pre><code>import pandas as pd\ndata = pd.read_csv(&quot;filename.txt&quot;, sep = &quot;#&quot;, header = None)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/u4jM0.png\" rel=\"nofollow noreferrer\">gives the following result</a></p>\n<pre><code>data1 = data.iloc[1::2]\ndata2 = data.iloc[::2]\n</code></pre>\n<p>I tried to remove the odd rows in data1 even in data2, then will hopefully figure out how to split the odd and merge the 2 df's, but there might be a faster and more beautiful method to do it, that's why asking here</p>\n<p>update, spent 3 hours figuring out how to work with dataframes, as I was not that familiar with them.\nnow from\n<a href=\"https://i.stack.imgur.com/4i1Pd.png\" rel=\"nofollow noreferrer\">that</a></p>\n<p>using</p>\n<pre><code>import pandas as pd\ndf = pd.read_csv(&quot;myfile.txt&quot;, sep = &quot;#&quot;, header = None)\nfor index, col in df.iterrows():\n    if index%2 == 0:\n        col[1] = int(col[1].split('\\t')[1])\nfor index, col in df.iterrows():\n    if index%2 == 1:\n#         print(col[0])\n        col[0] = col[0].split(' ')\ndf[0] = df[0].shift(-1)\ndf = df.iloc[::2]\ndf = df[[1,0]]\ndf = df.rename(columns={0: 1, 1: 0})\ndf.index = range(len(df))\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/AsnN1.png\" rel=\"nofollow noreferrer\">It became this</a></p>\n<p>any suggestions on how to add unknown number of phantom columnsnd fill them using &quot;n:value&quot; from the list to fill the &quot;n&quot; column with the &quot;value&quot;?</p>\n",
        "formatted_input": {
            "qid": 65169011,
            "link": "https://stackoverflow.com/questions/65169011/parsing-a-txt-file-into-data-frame-filling-columns-based-on-the-multiple-separa",
            "question": {
                "title": "Parsing a txt file into data frame, filling columns based on the multiple separators",
                "ques_desc": "Having a .txt file structure as below trying to parse into dataframe of the following structure describing the rule: # i - 'i' is the row number n:data - 'n' is the column number to fill, 'data' is the value to fill into i'th row if the number of columns would be small enough it could be done manually, but txt considered has roughly 2000-3000 column values and some of them are missing. gives the following result I tried to remove the odd rows in data1 even in data2, then will hopefully figure out how to split the odd and merge the 2 df's, but there might be a faster and more beautiful method to do it, that's why asking here update, spent 3 hours figuring out how to work with dataframes, as I was not that familiar with them. now from that using It became this any suggestions on how to add unknown number of phantom columnsnd fill them using \"n:value\" from the list to fill the \"n\" column with the \"value\"? "
            },
            "io": [
                "#n  1\na 1:0.0002 3:0.0003...\n#n  2\nb 2:0.0002 3:0.0003...\n#n  3\na 1:0.0002 2:0.0003... \n...\n",
                "#    type  1        2       3 \n1    a     0.0002   null    0.0003 ....\n2    b     null     0.0002  0.0003 ....\n3    a     0.0002   0.0003  null   ....\n...\n"
            ],
            "answer": {
                "ans_desc": "I think you are better off parsing the file yourself than relying on and then dealing with the mess. Here is how I would do it. Since I do not have access to your real file I am using a small example you have in your question. First, load the file. Then we read all lines, group them in pairs, parse and stick the results into a dict Now we have something that looks like this: this dict can now be used directly to create a dataframe, which we proceed to do and also order columns as they are in somewhat random order output ",
                "code": [
                    "# read all lines\nlines = file.readlines()\n\n# here we will store the results, dictionary of dictionaries\nparsing_res = {}\n\n# a fancy way of processing two lines, odd and even, at the same time\nfor line1,line2 in zip(lines[::2],lines[1::2]):\n    # line1 has the form '#n  1', we split on whitespace and take the second tokem\n    row_index = line1.split()[1]\n    # line2 is the other type of lines, split into tokens by whitespace\n    tokens = line2.split()\n    # first one is 'type'\n    t = tokens[0]\n\n    # the others are pairs 'x:y', split them into x,y and stick into a dictionary with label x and value y\n    row_dict = {token.split(':')[0]:token.split(':')[1] for token in tokens[1:]}\n\n    # add type\n    row_dict['type'] = t\n   \n    # store the result for these two lines into the main dictionary\n    parsing_res[row_index] = row_dict\nparsing_res\n",
                    "df = pd.DataFrame.from_dict(parsing_res, orient='index')\ndf.reindex(sorted(df.columns), axis=1).reindex(sorted(df.index), axis=0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 109,
            "user_id": 8669991,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3f2b45ed2573d8946d4195b6840f74c?s=128&d=identicon&r=PG&f=1",
            "display_name": "TylerSingleton",
            "link": "https://stackoverflow.com/users/8669991/tylersingleton"
        },
        "is_answered": true,
        "view_count": 67,
        "accepted_answer_id": 65091076,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1606845044,
        "creation_date": 1606785845,
        "question_id": 65083476,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65083476/pandas-mapping-one-dataframe-onto-another",
        "title": "Pandas, mapping one Dataframe onto another?",
        "body": "<p>I'm not sure how to tackle this problem. I have 3 data frames; one is a true/false table [3532x622], the other is a single series of integers[662x1], the other is my main dataframe[3532x8]. The true/false table was create by comparing a series of points to find which ones where inside a polygon, that is why is has the shape it does. I have outlined a diagram below as to what I am trying to accomplish.</p>\n<pre><code>df_1       df_2                                   df_3    \n  0          0      1     2         8     9         0\n0 56489    0 True   False False ... False True    0 poly_a\n1 45872    1 False  True  False ... True  False   1 poly_b\n2 86932    2 False  True  False ... False False   2 poly_c\n...        \n8 45871\n9 89641\n</code></pre>\n<p>Convert to:</p>\n<pre><code>df_2\n  0      1      2          8      9\n0 56489  np.nan np.nan ... np.nan 89641\n1 np.nan 86932  np.nan ... 45871  np.nan\n2 np.nan 86932  np.nan ... np.nan np.nan\n</code></pre>\n<p>Then map this onto the main dataframe</p>\n<pre><code>df_3\n  0      1 \n0 poly_a 56489\n1 ploy_a 89641\n2 poly_b 86932\n3 poly_b 45871\n4 poly_c 86932\n</code></pre>\n<p>This is what I have started</p>\n<pre><code># Creating Example Dataframes\ndf_1 = pd.DataFrame([56489, 45872, 89657, 56895, 87456])\ndf_2 = pd.DataFrame([[True, False, False, False, True],\n                     [False, True, True, False, False],\n                     [False, True, False, True, True]])\ndf_3 = pd.DataFrame(['poly_a', 'poly_b', 'poly_c'])\n\n# Mapping dataframe 1 onto 2\nfor i in list(np.where(df_2 == True))[1]:\n    df_new = pd.DataFrame(np.where(df_2 == True, df_1.iloc[i], np.nan))\n    \ndf = pd.concat([df_3, df_new], axis=1, ignore_index=True)\n</code></pre>\n<p>I don't know where to go from here.</p>\n",
        "answer_body": "<p>This should do the trick:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy\nimport pandas\n\n# Creating Example Dataframes\ndf_1 = pandas.DataFrame([56489, 45872, 89657, 56895, 87456])\ndf_2 = pandas.DataFrame(\n    [\n        [True, False, False, False, True],\n        [False, True, True, False, False],\n        [False, True, False, True, True],\n    ]\n)\ndf_3 = pandas.DataFrame([&quot;poly_a&quot;, &quot;poly_b&quot;, &quot;poly_c&quot;])\n\n\ndef replace_values(row: pandas.Series) -&gt; pandas.Series:\n\n    # Make a copy of df_1 (first row) but flip it to become columns\n    c = df_1.T.copy().iloc[0]\n\n    # Use the boolean values from the row as index, replace False with NaN\n    c.loc[~row] = numpy.nan\n    return c\n\n\n# Combine 2 and 1\ncombined = df_2.apply(replace_values, axis=1)\n\n# Add 3\nresult = pandas.concat([df_3, combined], axis=1, ignore_index=True)\n</code></pre>\n<p>Output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(result)\n        0        1        2        3        4        5\n0  poly_a  56489.0      NaN      NaN      NaN  87456.0\n1  poly_b      NaN  45872.0  89657.0      NaN      NaN\n2  poly_c      NaN  45872.0      NaN  56895.0  87456.0\n</code></pre>\n",
        "question_body": "<p>I'm not sure how to tackle this problem. I have 3 data frames; one is a true/false table [3532x622], the other is a single series of integers[662x1], the other is my main dataframe[3532x8]. The true/false table was create by comparing a series of points to find which ones where inside a polygon, that is why is has the shape it does. I have outlined a diagram below as to what I am trying to accomplish.</p>\n<pre><code>df_1       df_2                                   df_3    \n  0          0      1     2         8     9         0\n0 56489    0 True   False False ... False True    0 poly_a\n1 45872    1 False  True  False ... True  False   1 poly_b\n2 86932    2 False  True  False ... False False   2 poly_c\n...        \n8 45871\n9 89641\n</code></pre>\n<p>Convert to:</p>\n<pre><code>df_2\n  0      1      2          8      9\n0 56489  np.nan np.nan ... np.nan 89641\n1 np.nan 86932  np.nan ... 45871  np.nan\n2 np.nan 86932  np.nan ... np.nan np.nan\n</code></pre>\n<p>Then map this onto the main dataframe</p>\n<pre><code>df_3\n  0      1 \n0 poly_a 56489\n1 ploy_a 89641\n2 poly_b 86932\n3 poly_b 45871\n4 poly_c 86932\n</code></pre>\n<p>This is what I have started</p>\n<pre><code># Creating Example Dataframes\ndf_1 = pd.DataFrame([56489, 45872, 89657, 56895, 87456])\ndf_2 = pd.DataFrame([[True, False, False, False, True],\n                     [False, True, True, False, False],\n                     [False, True, False, True, True]])\ndf_3 = pd.DataFrame(['poly_a', 'poly_b', 'poly_c'])\n\n# Mapping dataframe 1 onto 2\nfor i in list(np.where(df_2 == True))[1]:\n    df_new = pd.DataFrame(np.where(df_2 == True, df_1.iloc[i], np.nan))\n    \ndf = pd.concat([df_3, df_new], axis=1, ignore_index=True)\n</code></pre>\n<p>I don't know where to go from here.</p>\n",
        "formatted_input": {
            "qid": 65083476,
            "link": "https://stackoverflow.com/questions/65083476/pandas-mapping-one-dataframe-onto-another",
            "question": {
                "title": "Pandas, mapping one Dataframe onto another?",
                "ques_desc": "I'm not sure how to tackle this problem. I have 3 data frames; one is a true/false table [3532x622], the other is a single series of integers[662x1], the other is my main dataframe[3532x8]. The true/false table was create by comparing a series of points to find which ones where inside a polygon, that is why is has the shape it does. I have outlined a diagram below as to what I am trying to accomplish. Convert to: Then map this onto the main dataframe This is what I have started I don't know where to go from here. "
            },
            "io": [
                "df_2\n  0      1      2          8      9\n0 56489  np.nan np.nan ... np.nan 89641\n1 np.nan 86932  np.nan ... 45871  np.nan\n2 np.nan 86932  np.nan ... np.nan np.nan\n",
                "df_3\n  0      1 \n0 poly_a 56489\n1 ploy_a 89641\n2 poly_b 86932\n3 poly_b 45871\n4 poly_c 86932\n"
            ],
            "answer": {
                "ans_desc": "This should do the trick: Output: ",
                "code": [
                    "import numpy\nimport pandas\n\n# Creating Example Dataframes\ndf_1 = pandas.DataFrame([56489, 45872, 89657, 56895, 87456])\ndf_2 = pandas.DataFrame(\n    [\n        [True, False, False, False, True],\n        [False, True, True, False, False],\n        [False, True, False, True, True],\n    ]\n)\ndf_3 = pandas.DataFrame([\"poly_a\", \"poly_b\", \"poly_c\"])\n\n\ndef replace_values(row: pandas.Series) -> pandas.Series:\n\n    # Make a copy of df_1 (first row) but flip it to become columns\n    c = df_1.T.copy().iloc[0]\n\n    # Use the boolean values from the row as index, replace False with NaN\n    c.loc[~row] = numpy.nan\n    return c\n\n\n# Combine 2 and 1\ncombined = df_2.apply(replace_values, axis=1)\n\n# Add 3\nresult = pandas.concat([df_3, combined], axis=1, ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1921,
            "user_id": 14058726,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/qfNgz.png?s=128&g=1",
            "display_name": "mosc9575",
            "link": "https://stackoverflow.com/users/14058726/mosc9575"
        },
        "is_answered": true,
        "view_count": 23,
        "accepted_answer_id": 65087356,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1606813570,
        "creation_date": 1606812605,
        "question_id": 65087280,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65087280/find-the-latest-occurrence-of-an-class-item-and-store-how-many-values-are-betwee",
        "title": "Find the latest occurrence of an class item and store how many values are between these two in a pandas DataFrame",
        "body": "<p>I have a pandas DataFrame with some labels for <code>n</code> classes. Now I want to add a column and store how many items are between two elements of the same class.</p>\n<pre><code>   Class\n0      0\n1      1\n2      1\n3      1\n4      0\n</code></pre>\n<p>and I want to get this:</p>\n<pre><code>    Class  Shift\n0      0    NaN\n1      1    NaN\n2      1    1.0\n3      1    1.0\n4      0    4.0\n</code></pre>\n<p>This is the code I used:</p>\n<pre><code>df = pd.DataFrame({'Class':[0,1,1,1,0]})\ndf['Shift'] = np.nan\nfor item in df.Class.unique():\n    _df = df[df['Class'] == item]\n    _df = _df.reset_index().rename({'index':'idx'}, axis=1)\n    df.loc[_df.idx, 'Shift'] = _df['idx'].diff().values\ndf\n</code></pre>\n<p>This seems circuitous to me. Is there a more elegant way of producing this output?</p>\n",
        "answer_body": "<p>You could do:</p>\n<pre><code>df['shift'] = np.arange(len(df))\ndf['shift'] = df.groupby('Class')['shift'].diff()\nprint(df)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>   Class  shift\n0      0    NaN\n1      1    NaN\n2      1    1.0\n3      1    1.0\n4      0    4.0\n</code></pre>\n<p>As an alternative:</p>\n<pre><code>df['shift'] = df.assign(shift=np.arange(len(df))).groupby('Class')['shift'].diff()\n</code></pre>\n<p>The idea is to create a column with consecutive values, group by the <em>Class</em> column and compute the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.diff.html\" rel=\"nofollow noreferrer\">diff</a> on the new column.</p>\n",
        "question_body": "<p>I have a pandas DataFrame with some labels for <code>n</code> classes. Now I want to add a column and store how many items are between two elements of the same class.</p>\n<pre><code>   Class\n0      0\n1      1\n2      1\n3      1\n4      0\n</code></pre>\n<p>and I want to get this:</p>\n<pre><code>    Class  Shift\n0      0    NaN\n1      1    NaN\n2      1    1.0\n3      1    1.0\n4      0    4.0\n</code></pre>\n<p>This is the code I used:</p>\n<pre><code>df = pd.DataFrame({'Class':[0,1,1,1,0]})\ndf['Shift'] = np.nan\nfor item in df.Class.unique():\n    _df = df[df['Class'] == item]\n    _df = _df.reset_index().rename({'index':'idx'}, axis=1)\n    df.loc[_df.idx, 'Shift'] = _df['idx'].diff().values\ndf\n</code></pre>\n<p>This seems circuitous to me. Is there a more elegant way of producing this output?</p>\n",
        "formatted_input": {
            "qid": 65087280,
            "link": "https://stackoverflow.com/questions/65087280/find-the-latest-occurrence-of-an-class-item-and-store-how-many-values-are-betwee",
            "question": {
                "title": "Find the latest occurrence of an class item and store how many values are between these two in a pandas DataFrame",
                "ques_desc": "I have a pandas DataFrame with some labels for classes. Now I want to add a column and store how many items are between two elements of the same class. and I want to get this: This is the code I used: This seems circuitous to me. Is there a more elegant way of producing this output? "
            },
            "io": [
                "   Class\n0      0\n1      1\n2      1\n3      1\n4      0\n",
                "    Class  Shift\n0      0    NaN\n1      1    NaN\n2      1    1.0\n3      1    1.0\n4      0    4.0\n"
            ],
            "answer": {
                "ans_desc": "You could do: Output As an alternative: The idea is to create a column with consecutive values, group by the Class column and compute the diff on the new column. ",
                "code": [
                    "df['shift'] = np.arange(len(df))\ndf['shift'] = df.groupby('Class')['shift'].diff()\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 12496193,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c237a8fb2c7e1df957157a85b8990825?s=128&d=identicon&r=PG&f=1",
            "display_name": "Harord",
            "link": "https://stackoverflow.com/users/12496193/harord"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 65077509,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1606754161,
        "creation_date": 1606753555,
        "question_id": 65077340,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65077340/how-to-filter-rows-in-a-dataframe-to-get-only-3-most-popular-and-delete-others-u",
        "title": "How to filter rows in a dataframe to get only 3 most popular and delete others unused data?",
        "body": "<p><strong>Theory</strong><br />\nI have some data on car brands in the US. I have to arrange them on the map of individual states and after hovering over with the mouse, I have to display the 3 most popular brands for a given state.</p>\n<hr />\n<p><strong>Question</strong><br />\nI have the following dataframe</p>\n<pre><code>    A   B   C   D   E\n1  20   0  40  10  30\n2   0  60  15   5  20 \n3  50  30  20   0   0\n</code></pre>\n<p>I need to achieve something like that (structure is probably wrong - I am not sure what kind of structure would be best - I just need data about name of column and its value):</p>\n<pre><code>1  (C: 40) (E: 30) (A: 20)\n2  (B: 60) (E: 20) (C: 15)\n3  (A: 50) (B: 30) (C: 20) \n</code></pre>\n<hr />\n<p><strong>Current situation</strong><br />\nI was able to create something like this:</p>\n<pre><code>pd.crosstab(df['code'],df['brand'],normalize='index').reset_index(drop=True).apply(lambda x: x*100)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/DvQz5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/DvQz5.png\" alt=\"enter image description here\" /></a></p>\n<p>So the situation is identical to the example I gave at the top. Now I have to somehow &quot;filter this data and keep information about the brand name and its percentage in a given state&quot;.</p>\n<hr />\n<p>It is a bit difficult for me, can someone please help me?</p>\n",
        "answer_body": "<p>You could use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.nlargest.html\" rel=\"nofollow noreferrer\">nlargest</a> and convert it to a dictionary:</p>\n<pre><code>res = df.apply(lambda x: pd.Series(x).nlargest(3).to_dict(), axis=1)\nprint(res)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>0    {'C': 40, 'E': 30, 'A': 20}\n1    {'B': 60, 'E': 20, 'C': 15}\n2    {'A': 50, 'B': 30, 'C': 20}\ndtype: object\n</code></pre>\n<p>If order is <em>really</em> important, return a list:</p>\n<pre><code>res = df.apply(lambda x: list(pd.Series(x).nlargest(3).items()), axis=1)\nprint(res)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>0    [(C, 40), (E, 30), (A, 20)]\n1    [(B, 60), (E, 20), (C, 15)]\n2    [(A, 50), (B, 30), (C, 20)]\ndtype: object\n</code></pre>\n",
        "question_body": "<p><strong>Theory</strong><br />\nI have some data on car brands in the US. I have to arrange them on the map of individual states and after hovering over with the mouse, I have to display the 3 most popular brands for a given state.</p>\n<hr />\n<p><strong>Question</strong><br />\nI have the following dataframe</p>\n<pre><code>    A   B   C   D   E\n1  20   0  40  10  30\n2   0  60  15   5  20 \n3  50  30  20   0   0\n</code></pre>\n<p>I need to achieve something like that (structure is probably wrong - I am not sure what kind of structure would be best - I just need data about name of column and its value):</p>\n<pre><code>1  (C: 40) (E: 30) (A: 20)\n2  (B: 60) (E: 20) (C: 15)\n3  (A: 50) (B: 30) (C: 20) \n</code></pre>\n<hr />\n<p><strong>Current situation</strong><br />\nI was able to create something like this:</p>\n<pre><code>pd.crosstab(df['code'],df['brand'],normalize='index').reset_index(drop=True).apply(lambda x: x*100)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/DvQz5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/DvQz5.png\" alt=\"enter image description here\" /></a></p>\n<p>So the situation is identical to the example I gave at the top. Now I have to somehow &quot;filter this data and keep information about the brand name and its percentage in a given state&quot;.</p>\n<hr />\n<p>It is a bit difficult for me, can someone please help me?</p>\n",
        "formatted_input": {
            "qid": 65077340,
            "link": "https://stackoverflow.com/questions/65077340/how-to-filter-rows-in-a-dataframe-to-get-only-3-most-popular-and-delete-others-u",
            "question": {
                "title": "How to filter rows in a dataframe to get only 3 most popular and delete others unused data?",
                "ques_desc": "Theory I have some data on car brands in the US. I have to arrange them on the map of individual states and after hovering over with the mouse, I have to display the 3 most popular brands for a given state. Question I have the following dataframe I need to achieve something like that (structure is probably wrong - I am not sure what kind of structure would be best - I just need data about name of column and its value): Current situation I was able to create something like this: So the situation is identical to the example I gave at the top. Now I have to somehow \"filter this data and keep information about the brand name and its percentage in a given state\". It is a bit difficult for me, can someone please help me? "
            },
            "io": [
                "    A   B   C   D   E\n1  20   0  40  10  30\n2   0  60  15   5  20 \n3  50  30  20   0   0\n",
                "1  (C: 40) (E: 30) (A: 20)\n2  (B: 60) (E: 20) (C: 15)\n3  (A: 50) (B: 30) (C: 20) \n"
            ],
            "answer": {
                "ans_desc": "You could use nlargest and convert it to a dictionary: Output If order is really important, return a list: Output ",
                "code": [
                    "res = df.apply(lambda x: pd.Series(x).nlargest(3).to_dict(), axis=1)\nprint(res)\n",
                    "res = df.apply(lambda x: list(pd.Series(x).nlargest(3).items()), axis=1)\nprint(res)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 17,
            "user_id": 13939478,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8870156a0dda4b844830452ce16b522d?s=128&d=identicon&r=PG&f=1",
            "display_name": "pmonasterio",
            "link": "https://stackoverflow.com/users/13939478/pmonasterio"
        },
        "is_answered": true,
        "view_count": 50,
        "accepted_answer_id": 65044629,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1606517102,
        "creation_date": 1606511081,
        "question_id": 65043847,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65043847/organize-data-based-on-a-weird-column-distribution-in-pandas",
        "title": "Organize data based on a weird column distribution in pandas",
        "body": "<p>Is there an elegant way of segment data in a dataframe in which the first row includes the name of the data owner, and the second row includes headers, with all the data organized below?</p>\n<p>I have this:</p>\n<pre><code>0   n_1 NaN NaN NaN NaN n_2 NaN NaN NaN NaN ... n_3 NaN NaN NaN NaN n_4 NaN NaN NaN NaN\n1   V1  V2  V3  V4  V5  V1  V2  V3  V4  V5  ... V1  V2  V3  V4  V5  V1  V2  V3  V4  V5\n2   45  43  30  32  NaN 45  52  47  47  NaN ... 45  57  51  50  NaN 45  51  47  50  NaN\n3   50  53  38  38  NaN 50  55  50  41  NaN ... 50  51  48  49  NaN 50  53  52  52  1\n4   50  54  37  41  NaN 50  53  49  49  1   ... 50  54  50  47  NaN 50  54  48  41  1\n5   50  51  40  39  NaN 50  53  50  48  NaN ... 50  53  50  49  NaN 50  51  49  50  NaN\n6   50  53  47  50  NaN 50  50  47  35  NaN ... 50  55  44  34  NaN 50  50  47  47  NaN\n7   50  51  47  45  NaN 50  52  48  48  1   ... 50  51  48  46  NaN 50  51  47  50  NaN\n8   50  52  50  50  NaN 50  50  47  50  NaN ... 50  51  47  48  NaN NaN NaN NaN NaN NaN\n9   NaN NaN NaN NaN NaN 50  54  51  53  NaN ... 50  52  48  51  NaN NaN NaN NaN NaN NaN\n</code></pre>\n<p>I need to order that so that I can analyze it in something like:</p>\n<pre><code>0   Own V1  V2  V3  V4  V5  \n1   n_1 45  43  30  32  NaN \n2   n_1 50  53  38  38  NaN \n3   n_1 50  54  37  41  NaN \n4   n_1 50  51  40  39  NaN \n5   n_1 50  53  47  50  NaN \n6   n_1 50  51  47  45  NaN \n7   n_1 50  52  50  50  NaN \n8   n_2 45  52  47  47  NaN \n9   n_2 50  55  50  41  NaN \n10  n_2 50  53  49  49  1   \n11  n_2 50  53  50  48  NaN \n12  n_2 50  50  47  35  NaN \n13  n_2 50  52  48  48  1   \n14  n_2 50  50  47  50  NaN \n15  n_2 50  54  51  53  NaN \n16  n_3 45  57  51  50  NaN \n17  n_3 50  51  48  49  NaN \n18  n_3 50  54  50  47  NaN \n19  n_3 50  53  50  49  NaN \n20  n_3 50  55  44  34  NaN \n21  n_3 50  51  48  46  NaN \n22  n_3 50  51  47  48  NaN \n23  n_3 50  52  48  51  NaN\n24  n_4 45  51  47  50  NaN\n25  n_4 50  53  52  52  1\n26  n_4 50  54  48  41  1\n27  n_4 50  51  49  50  NaN\n28  n_4 50  50  47  47  NaN\n29  n_4 50  50  51  47  NaN\n</code></pre>\n<p>I though about making different dataframes, but that would be a waste of resources. Is there a more elegant way of doing this?</p>\n<p>Thanks.</p>\n",
        "answer_body": "<p>Personally I would use a multi index.</p>\n<p>from source this should work. The operative argument here is <code>headers</code> in which you tell the <code>read_csv</code> function what arguments are required for read.</p>\n<pre><code>df = pd.read_csv('your_file.csv',headers=[0,1])\ndf = df.stack(0).reset_index(1).rename(columns={0 : 'own'})\n\nprint(df)\n\n1  own  V1  V2  V3  V4   V5\n2  n_1  45  43  30  32  NaN\n2  n_2  45  52  47  47  NaN\n2  n_3  45  57  51  50  NaN\n2  n_4  45  51  47  50  NaN\n3  n_1  50  53  38  38  NaN\n3  n_2  50  55  50  41  NaN\n3  n_3  50  51  48  49  NaN\n3  n_4  50  53  52  52    1\n4  n_1  50  54  37  41  NaN\n4  n_2  50  53  49  49    1\n4  n_3  50  54  50  47  NaN\n4  n_4  50  54  48  41    1\n5  n_1  50  51  40  39  NaN\n5  n_2  50  53  50  48  NaN\n5  n_3  50  53  50  49  NaN\n5  n_4  50  51  49  50  NaN\n6  n_1  50  53  47  50  NaN\n6  n_2  50  50  47  35  NaN\n6  n_3  50  55  44  34  NaN\n6  n_4  50  50  47  47  NaN\n7  n_1  50  51  47  45  NaN\n7  n_2  50  52  48  48    1\n7  n_3  50  51  48  46  NaN\n7  n_4  50  51  47  50  NaN\n8  n_1  50  52  50  50  NaN\n8  n_2  50  50  47  50  NaN\n8  n_3  50  51  47  48  NaN\n9  n_2  50  54  51  53  NaN\n9  n_3  50  52  48  51  NaN\n</code></pre>\n<p>--</p>\n<p>If your <code>MultiIndex</code> comes out malformed due to the source data we can fudge it by manually fixing it.</p>\n<pre><code>df = pd.read_csv('your_file.csv',headers=None)\n\ns = df.iloc[:2].T.replace('NaN',np.nan).ffill() # you may need to be smart with your replace here. \ndf.columns = pd.MultiIndex.from_frame(s)\n\ndf1 = df.stack(0).reset_index(1).rename(columns={0 : 'own'}).iloc[2:]\n</code></pre>\n<p>which will yield the same as the above.</p>\n",
        "question_body": "<p>Is there an elegant way of segment data in a dataframe in which the first row includes the name of the data owner, and the second row includes headers, with all the data organized below?</p>\n<p>I have this:</p>\n<pre><code>0   n_1 NaN NaN NaN NaN n_2 NaN NaN NaN NaN ... n_3 NaN NaN NaN NaN n_4 NaN NaN NaN NaN\n1   V1  V2  V3  V4  V5  V1  V2  V3  V4  V5  ... V1  V2  V3  V4  V5  V1  V2  V3  V4  V5\n2   45  43  30  32  NaN 45  52  47  47  NaN ... 45  57  51  50  NaN 45  51  47  50  NaN\n3   50  53  38  38  NaN 50  55  50  41  NaN ... 50  51  48  49  NaN 50  53  52  52  1\n4   50  54  37  41  NaN 50  53  49  49  1   ... 50  54  50  47  NaN 50  54  48  41  1\n5   50  51  40  39  NaN 50  53  50  48  NaN ... 50  53  50  49  NaN 50  51  49  50  NaN\n6   50  53  47  50  NaN 50  50  47  35  NaN ... 50  55  44  34  NaN 50  50  47  47  NaN\n7   50  51  47  45  NaN 50  52  48  48  1   ... 50  51  48  46  NaN 50  51  47  50  NaN\n8   50  52  50  50  NaN 50  50  47  50  NaN ... 50  51  47  48  NaN NaN NaN NaN NaN NaN\n9   NaN NaN NaN NaN NaN 50  54  51  53  NaN ... 50  52  48  51  NaN NaN NaN NaN NaN NaN\n</code></pre>\n<p>I need to order that so that I can analyze it in something like:</p>\n<pre><code>0   Own V1  V2  V3  V4  V5  \n1   n_1 45  43  30  32  NaN \n2   n_1 50  53  38  38  NaN \n3   n_1 50  54  37  41  NaN \n4   n_1 50  51  40  39  NaN \n5   n_1 50  53  47  50  NaN \n6   n_1 50  51  47  45  NaN \n7   n_1 50  52  50  50  NaN \n8   n_2 45  52  47  47  NaN \n9   n_2 50  55  50  41  NaN \n10  n_2 50  53  49  49  1   \n11  n_2 50  53  50  48  NaN \n12  n_2 50  50  47  35  NaN \n13  n_2 50  52  48  48  1   \n14  n_2 50  50  47  50  NaN \n15  n_2 50  54  51  53  NaN \n16  n_3 45  57  51  50  NaN \n17  n_3 50  51  48  49  NaN \n18  n_3 50  54  50  47  NaN \n19  n_3 50  53  50  49  NaN \n20  n_3 50  55  44  34  NaN \n21  n_3 50  51  48  46  NaN \n22  n_3 50  51  47  48  NaN \n23  n_3 50  52  48  51  NaN\n24  n_4 45  51  47  50  NaN\n25  n_4 50  53  52  52  1\n26  n_4 50  54  48  41  1\n27  n_4 50  51  49  50  NaN\n28  n_4 50  50  47  47  NaN\n29  n_4 50  50  51  47  NaN\n</code></pre>\n<p>I though about making different dataframes, but that would be a waste of resources. Is there a more elegant way of doing this?</p>\n<p>Thanks.</p>\n",
        "formatted_input": {
            "qid": 65043847,
            "link": "https://stackoverflow.com/questions/65043847/organize-data-based-on-a-weird-column-distribution-in-pandas",
            "question": {
                "title": "Organize data based on a weird column distribution in pandas",
                "ques_desc": "Is there an elegant way of segment data in a dataframe in which the first row includes the name of the data owner, and the second row includes headers, with all the data organized below? I have this: I need to order that so that I can analyze it in something like: I though about making different dataframes, but that would be a waste of resources. Is there a more elegant way of doing this? Thanks. "
            },
            "io": [
                "0   n_1 NaN NaN NaN NaN n_2 NaN NaN NaN NaN ... n_3 NaN NaN NaN NaN n_4 NaN NaN NaN NaN\n1   V1  V2  V3  V4  V5  V1  V2  V3  V4  V5  ... V1  V2  V3  V4  V5  V1  V2  V3  V4  V5\n2   45  43  30  32  NaN 45  52  47  47  NaN ... 45  57  51  50  NaN 45  51  47  50  NaN\n3   50  53  38  38  NaN 50  55  50  41  NaN ... 50  51  48  49  NaN 50  53  52  52  1\n4   50  54  37  41  NaN 50  53  49  49  1   ... 50  54  50  47  NaN 50  54  48  41  1\n5   50  51  40  39  NaN 50  53  50  48  NaN ... 50  53  50  49  NaN 50  51  49  50  NaN\n6   50  53  47  50  NaN 50  50  47  35  NaN ... 50  55  44  34  NaN 50  50  47  47  NaN\n7   50  51  47  45  NaN 50  52  48  48  1   ... 50  51  48  46  NaN 50  51  47  50  NaN\n8   50  52  50  50  NaN 50  50  47  50  NaN ... 50  51  47  48  NaN NaN NaN NaN NaN NaN\n9   NaN NaN NaN NaN NaN 50  54  51  53  NaN ... 50  52  48  51  NaN NaN NaN NaN NaN NaN\n",
                "0   Own V1  V2  V3  V4  V5  \n1   n_1 45  43  30  32  NaN \n2   n_1 50  53  38  38  NaN \n3   n_1 50  54  37  41  NaN \n4   n_1 50  51  40  39  NaN \n5   n_1 50  53  47  50  NaN \n6   n_1 50  51  47  45  NaN \n7   n_1 50  52  50  50  NaN \n8   n_2 45  52  47  47  NaN \n9   n_2 50  55  50  41  NaN \n10  n_2 50  53  49  49  1   \n11  n_2 50  53  50  48  NaN \n12  n_2 50  50  47  35  NaN \n13  n_2 50  52  48  48  1   \n14  n_2 50  50  47  50  NaN \n15  n_2 50  54  51  53  NaN \n16  n_3 45  57  51  50  NaN \n17  n_3 50  51  48  49  NaN \n18  n_3 50  54  50  47  NaN \n19  n_3 50  53  50  49  NaN \n20  n_3 50  55  44  34  NaN \n21  n_3 50  51  48  46  NaN \n22  n_3 50  51  47  48  NaN \n23  n_3 50  52  48  51  NaN\n24  n_4 45  51  47  50  NaN\n25  n_4 50  53  52  52  1\n26  n_4 50  54  48  41  1\n27  n_4 50  51  49  50  NaN\n28  n_4 50  50  47  47  NaN\n29  n_4 50  50  51  47  NaN\n"
            ],
            "answer": {
                "ans_desc": "Personally I would use a multi index. from source this should work. The operative argument here is in which you tell the function what arguments are required for read. -- If your comes out malformed due to the source data we can fudge it by manually fixing it. which will yield the same as the above. ",
                "code": [
                    "df = pd.read_csv('your_file.csv',headers=None)\n\ns = df.iloc[:2].T.replace('NaN',np.nan).ffill() # you may need to be smart with your replace here. \ndf.columns = pd.MultiIndex.from_frame(s)\n\ndf1 = df.stack(0).reset_index(1).rename(columns={0 : 'own'}).iloc[2:]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 376,
            "user_id": 9466397,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/02b09909edace168153db4294f5e1488?s=128&d=identicon&r=PG&f=1",
            "display_name": "defraggled",
            "link": "https://stackoverflow.com/users/9466397/defraggled"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 65035118,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1606473651,
        "creation_date": 1606470000,
        "question_id": 65035031,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65035031/filtering-dataframe-rows-which-have-overlapping-values-cross-columns",
        "title": "Filtering DataFrame rows which have overlapping values cross-columns",
        "body": "<p>I have a dataframe that reflects rows with at least one <code>id</code> conflict inside that row.</p>\n<pre><code>      email id1 id2 id3\n0      de@l  Z7  Q4  Q4\n1     sco@g  Q4  Z7  Q4\n2   alpha@n  Q4  Z7  Z7\n3   numer@o  Z7  Z7  Q4\n4    endo@c  D8  D8  L1\n5  chrono@k  L1  L1  D8\n</code></pre>\n<p>Rows 0-3 and rows 4-5 have overlapping <code>id</code> values with other rows, but the overlap occurs across various columns.</p>\n<p>How can I:</p>\n<ol>\n<li>drop all but the <strong>first row</strong> of each overlap group,</li>\n<li>in a table-wise or series-wise manner, ie <strong>without using <code>df.apply()</code> down the rows</strong></li>\n</ol>\n<p>This would be the output (though don't care about index):</p>\n<pre><code>      email id1 id2 id3\n0      de@l  Z7  Q4  Q4\n4    endo@c  D8  D8  L1\n</code></pre>\n<hr />\n<p>Below snippet for easy repro</p>\n<pre><code>pd.DataFrame([\n    {'email':'de@l', 'id1':'Z7', 'id2':'Q4', 'id3':'Q4'},\n    {'email':'sco@g', 'id1':'Q4', 'id2':'Z7', 'id3':'Q4'},\n    {'email':'alpha@n', 'id1':'Q4', 'id2':'Z7', 'id3':'Z7'},\n    {'email':'numer@o', 'id1':'Z7', 'id2':'Z7', 'id3':'Q4'},\n    {'email':'endo@c', 'id1':'D8', 'id2':'D8', 'id3':'L1'},\n    {'email':'chrono@k','id1':'L1', 'id2':'L1', 'id3':'D8'},\n])\n</code></pre>\n",
        "answer_body": "<p>Idea is convert columns with <code>id</code> values to hashable sets called frozensets, so possible filter by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.duplicated.html\" rel=\"nofollow noreferrer\"><code>Series.duplicated</code></a> with inverted mask in <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a>:</p>\n<pre><code>df = df[~df.filter(like='id').apply(frozenset, axis=1).duplicated()]\n#for test all columns without first\n#df = df[~df.iloc[:, 1:].apply(frozenset, axis=1).duplicated()]\nprint (df)\n    email id1 id2 id3\n0    de@l  Z7  Q4  Q4\n4  endo@c  D8  D8  L1\n</code></pre>\n<p>Alternative with list comprehension:</p>\n<pre><code>L = [frozenset(x) for x in df.filter(like='id').to_numpy()]\ndf = df[~pd.Series(L, index=df.index).duplicated()]\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that reflects rows with at least one <code>id</code> conflict inside that row.</p>\n<pre><code>      email id1 id2 id3\n0      de@l  Z7  Q4  Q4\n1     sco@g  Q4  Z7  Q4\n2   alpha@n  Q4  Z7  Z7\n3   numer@o  Z7  Z7  Q4\n4    endo@c  D8  D8  L1\n5  chrono@k  L1  L1  D8\n</code></pre>\n<p>Rows 0-3 and rows 4-5 have overlapping <code>id</code> values with other rows, but the overlap occurs across various columns.</p>\n<p>How can I:</p>\n<ol>\n<li>drop all but the <strong>first row</strong> of each overlap group,</li>\n<li>in a table-wise or series-wise manner, ie <strong>without using <code>df.apply()</code> down the rows</strong></li>\n</ol>\n<p>This would be the output (though don't care about index):</p>\n<pre><code>      email id1 id2 id3\n0      de@l  Z7  Q4  Q4\n4    endo@c  D8  D8  L1\n</code></pre>\n<hr />\n<p>Below snippet for easy repro</p>\n<pre><code>pd.DataFrame([\n    {'email':'de@l', 'id1':'Z7', 'id2':'Q4', 'id3':'Q4'},\n    {'email':'sco@g', 'id1':'Q4', 'id2':'Z7', 'id3':'Q4'},\n    {'email':'alpha@n', 'id1':'Q4', 'id2':'Z7', 'id3':'Z7'},\n    {'email':'numer@o', 'id1':'Z7', 'id2':'Z7', 'id3':'Q4'},\n    {'email':'endo@c', 'id1':'D8', 'id2':'D8', 'id3':'L1'},\n    {'email':'chrono@k','id1':'L1', 'id2':'L1', 'id3':'D8'},\n])\n</code></pre>\n",
        "formatted_input": {
            "qid": 65035031,
            "link": "https://stackoverflow.com/questions/65035031/filtering-dataframe-rows-which-have-overlapping-values-cross-columns",
            "question": {
                "title": "Filtering DataFrame rows which have overlapping values cross-columns",
                "ques_desc": "I have a dataframe that reflects rows with at least one conflict inside that row. Rows 0-3 and rows 4-5 have overlapping values with other rows, but the overlap occurs across various columns. How can I: drop all but the first row of each overlap group, in a table-wise or series-wise manner, ie without using down the rows This would be the output (though don't care about index): Below snippet for easy repro "
            },
            "io": [
                "      email id1 id2 id3\n0      de@l  Z7  Q4  Q4\n1     sco@g  Q4  Z7  Q4\n2   alpha@n  Q4  Z7  Z7\n3   numer@o  Z7  Z7  Q4\n4    endo@c  D8  D8  L1\n5  chrono@k  L1  L1  D8\n",
                "      email id1 id2 id3\n0      de@l  Z7  Q4  Q4\n4    endo@c  D8  D8  L1\n"
            ],
            "answer": {
                "ans_desc": "Idea is convert columns with values to hashable sets called frozensets, so possible filter by with inverted mask in : Alternative with list comprehension: ",
                "code": [
                    "L = [frozenset(x) for x in df.filter(like='id').to_numpy()]\ndf = df[~pd.Series(L, index=df.index).duplicated()]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe",
            "split"
        ],
        "owner": {
            "reputation": 65,
            "user_id": 11924976,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/67836a88689ce15582c85bf74c6ef7c2?s=128&d=identicon&r=PG&f=1",
            "display_name": "user11924976",
            "link": "https://stackoverflow.com/users/11924976/user11924976"
        },
        "is_answered": true,
        "view_count": 733,
        "accepted_answer_id": 59340656,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1606468213,
        "creation_date": 1576372516,
        "question_id": 59340489,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/59340489/splitting-dataframe-based-on-duplicate-values-into-multiple-csv-files",
        "title": "Splitting Dataframe based on duplicate values into multiple csv files",
        "body": "<p>I have a dataset with multiple columns but only focusing on one column called 'VAL'. Every value in this column ranges from 0 to 4 so I would like to split this into 5 separate data frames based on those duplicate values and then export each of these data frames into individual csv files. </p>\n\n<p>I have been able to sort the numbers using pandas but now I need to divide up the values into smaller datasets keeping in mind that I have multiple files I would like to do this to so possibly a for loop?</p>\n\n<blockquote>\n  <p>this is what I currently have as an output</p>\n</blockquote>\n\n<pre><code> A       B      C      D      E      F      G         VAL   FILE\n954     380    158    166    431    201    769         0  001.csv\n1142    348    203    962      0    878   1023         0  001.csv\n1688    279    229      0    488   1007      0         0  001.csv\n4792    371    420     29    372      0    745         0  001.csv\n2106    352     76    196    388      0    695         0  001.csv\n    ...    ...    ...    ...    ...    ...       ...      ...\n5634    441    283    277    788     45    585         4  001.csv\n827     672    606     24   1023    463    742         4  001.csv\n6703    324    203      0    623    214    726         4  001.csv\n9056    604    398      0    981      0    633         4  001.csv\n0       574    338    144    942    608    793         4  001.csv\n</code></pre>\n\n<blockquote>\n  <p>this is what I would like it to relatively look like</p>\n</blockquote>\n\n<pre><code> A       B      C      D      E      F      G         VAL   FILE\n954     380    158    166    431    201    769         0  val_0.csv\n1142    348    203    962      0    878   1023         0  val_0.csv\n1688    279    229      0    488   1007      0         0  val_0.csv\n4792    371    420     29    372      0    745         0  val_0.csv\n2106    352     76    196    388      0    695         0  val_0.csv\n\n\n A       B      C      D      E      F      G         VAL   FILE\n5634    441    283    277    788     45    585         4  val_4.csv\n827     672    606     24   1023    463    742         4  val_4.csv\n6703    324    203      0    623    214    726         4  val_4.csv\n9056    604    398      0    981      0    633         4  val_4.csv\n0       574    338    144    942    608    793         4  val_4.csv\n\n</code></pre>\n",
        "answer_body": "<p>change your FILE to match your expected output.</p>\n<pre><code>df = pd.read_clipboard(sep'\\s+')\n</code></pre>\n<p>then groupby VAL and write your csv</p>\n<pre><code>for group,data in df.groupby('VAL'):\n    data.to_csv(f&quot;val_{group}.csv&quot;,index=False)\n</code></pre>\n<p>this writes two csv's for me from your data.</p>\n<p><a href=\"https://i.stack.imgur.com/Ky5XX.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Ky5XX.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>for group,data in df.groupby('VAL'):\n    print(data)\n          A    B    C    D    E     F     G VAL       FILE\n0   954  380  158  166  431   201   769   0  val_0.csv\n1  1142  348  203  962    0   878  1023   0  val_0.csv\n2  1688  279  229    0  488  1007     0   0  val_0.csv\n3  4792  371  420   29  372     0   745   0  val_0.csv\n4  2106  352   76  196  388     0   695   0  val_0.csv\n       A    B    C    D     E    F    G VAL       FILE\n6   5634  441  283  277   788   45  585   4  val_4.csv\n7    827  672  606   24  1023  463  742   4  val_4.csv\n8   6703  324  203    0   623  214  726   4  val_4.csv\n9   9056  604  398    0   981    0  633   4  val_4.csv\n10     0  574  338  144   942  608  793   4  val_4.csv\n</code></pre>\n",
        "question_body": "<p>I have a dataset with multiple columns but only focusing on one column called 'VAL'. Every value in this column ranges from 0 to 4 so I would like to split this into 5 separate data frames based on those duplicate values and then export each of these data frames into individual csv files. </p>\n\n<p>I have been able to sort the numbers using pandas but now I need to divide up the values into smaller datasets keeping in mind that I have multiple files I would like to do this to so possibly a for loop?</p>\n\n<blockquote>\n  <p>this is what I currently have as an output</p>\n</blockquote>\n\n<pre><code> A       B      C      D      E      F      G         VAL   FILE\n954     380    158    166    431    201    769         0  001.csv\n1142    348    203    962      0    878   1023         0  001.csv\n1688    279    229      0    488   1007      0         0  001.csv\n4792    371    420     29    372      0    745         0  001.csv\n2106    352     76    196    388      0    695         0  001.csv\n    ...    ...    ...    ...    ...    ...       ...      ...\n5634    441    283    277    788     45    585         4  001.csv\n827     672    606     24   1023    463    742         4  001.csv\n6703    324    203      0    623    214    726         4  001.csv\n9056    604    398      0    981      0    633         4  001.csv\n0       574    338    144    942    608    793         4  001.csv\n</code></pre>\n\n<blockquote>\n  <p>this is what I would like it to relatively look like</p>\n</blockquote>\n\n<pre><code> A       B      C      D      E      F      G         VAL   FILE\n954     380    158    166    431    201    769         0  val_0.csv\n1142    348    203    962      0    878   1023         0  val_0.csv\n1688    279    229      0    488   1007      0         0  val_0.csv\n4792    371    420     29    372      0    745         0  val_0.csv\n2106    352     76    196    388      0    695         0  val_0.csv\n\n\n A       B      C      D      E      F      G         VAL   FILE\n5634    441    283    277    788     45    585         4  val_4.csv\n827     672    606     24   1023    463    742         4  val_4.csv\n6703    324    203      0    623    214    726         4  val_4.csv\n9056    604    398      0    981      0    633         4  val_4.csv\n0       574    338    144    942    608    793         4  val_4.csv\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 59340489,
            "link": "https://stackoverflow.com/questions/59340489/splitting-dataframe-based-on-duplicate-values-into-multiple-csv-files",
            "question": {
                "title": "Splitting Dataframe based on duplicate values into multiple csv files",
                "ques_desc": "I have a dataset with multiple columns but only focusing on one column called 'VAL'. Every value in this column ranges from 0 to 4 so I would like to split this into 5 separate data frames based on those duplicate values and then export each of these data frames into individual csv files. I have been able to sort the numbers using pandas but now I need to divide up the values into smaller datasets keeping in mind that I have multiple files I would like to do this to so possibly a for loop? this is what I currently have as an output this is what I would like it to relatively look like "
            },
            "io": [
                " A       B      C      D      E      F      G         VAL   FILE\n954     380    158    166    431    201    769         0  001.csv\n1142    348    203    962      0    878   1023         0  001.csv\n1688    279    229      0    488   1007      0         0  001.csv\n4792    371    420     29    372      0    745         0  001.csv\n2106    352     76    196    388      0    695         0  001.csv\n    ...    ...    ...    ...    ...    ...       ...      ...\n5634    441    283    277    788     45    585         4  001.csv\n827     672    606     24   1023    463    742         4  001.csv\n6703    324    203      0    623    214    726         4  001.csv\n9056    604    398      0    981      0    633         4  001.csv\n0       574    338    144    942    608    793         4  001.csv\n",
                " A       B      C      D      E      F      G         VAL   FILE\n954     380    158    166    431    201    769         0  val_0.csv\n1142    348    203    962      0    878   1023         0  val_0.csv\n1688    279    229      0    488   1007      0         0  val_0.csv\n4792    371    420     29    372      0    745         0  val_0.csv\n2106    352     76    196    388      0    695         0  val_0.csv\n\n\n A       B      C      D      E      F      G         VAL   FILE\n5634    441    283    277    788     45    585         4  val_4.csv\n827     672    606     24   1023    463    742         4  val_4.csv\n6703    324    203      0    623    214    726         4  val_4.csv\n9056    604    398      0    981      0    633         4  val_4.csv\n0       574    338    144    942    608    793         4  val_4.csv\n\n"
            ],
            "answer": {
                "ans_desc": "change your FILE to match your expected output. then groupby VAL and write your csv this writes two csv's for me from your data. ",
                "code": [
                    "for group,data in df.groupby('VAL'):\n    data.to_csv(f\"val_{group}.csv\",index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 95,
            "user_id": 9935285,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-0wnoj65-v9k/AAAAAAAAAAI/AAAAAAAAABc/iAsarmEhefM/photo.jpg?sz=128",
            "display_name": "Alain Daccache",
            "link": "https://stackoverflow.com/users/9935285/alain-daccache"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 65007649,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1606317760,
        "creation_date": 1605905292,
        "question_id": 64936694,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64936694/recovering-dataframe-multiindex-from-both-row-and-column-after-groupby",
        "title": "Recovering DataFrame MultiIndex (from both row and column) after groupby",
        "body": "<p>I have a dataframe that is multi indexed in that manner.</p>\n<pre><code>                                  Value              Size\n                           A               B      Market Cap\n2019-07-01 AAPL         89.583458      9.328360  2.116356e+06\n           AMGN         49.828466     10.058943  1.395518e+05\n2019-10-01 AAPL         74.297570     11.237253  2.116356e+06\n           AMGN         56.841946     10.237481  1.395518e+05\n2019-12-31 AAPL         97.435257     14.736749  2.116356e+06\n           AMGN         71.400903     12.859612  1.395518e+05\n</code></pre>\n<p>I want to apply a function to each of its columns, for each date (so the 89.583458 and 49.828466 go together, 9.328360 and 10.058943 go together, and so forth)</p>\n<pre><code>winsorized_df = pipeline_df.groupby(level=0, axis=0).apply(\n                lambda level_0_col: level_0_col.groupby(level=1, axis=1).apply(\n                    lambda series: mstats.winsorize(a=series, limits=winsorize_bounds))\n            )\n</code></pre>\n<p>This gives me</p>\n<pre><code>                                              Market Cap  ...                             B\n2019-07-01  [[139551.76568603513], [139551.76568603513]]  ...  [[49.828465616227064], [49.828465616227064]]\n2019-10-01  [[139551.76568603513], [139551.76568603513]]  ...    [[56.84194615992103], [56.84194615992103]]\n2019-12-31  [[139551.76568603513], [139551.76568603513]]  ...    [[71.40090272484755], [71.40090272484755]]\n</code></pre>\n<p>But now I need to recover the lost indices (to get back the same structure as the original), but failed at setting <code>as_index=False</code>, unstacking or using pd.MultiIndex.from_frame. Any idea? Perhaps there's a better to get exactly that from the <code>groupby</code> call?</p>\n",
        "answer_body": "<p>The problem is that <code>winsorize</code> returns a numpy array. So you're replacing a dataframe with a numpy array (wich is why you see <code>[[...]]</code> in your output). Instead, you should replace the values of the dataframe. Here is an example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas\nfrom scipy.stats.mstats import winsorize\n\n# Recreating your dataframe\ndata = [\n    {&quot;date&quot;: &quot;2019-07-01&quot;, &quot;group&quot;: &quot;AAPL&quot;, &quot;A&quot;: 89.583458, &quot;B&quot;: 9.328360, &quot;Market Cap&quot;: 2.116356e+06},\n    {&quot;date&quot;: &quot;2019-07-01&quot;, &quot;group&quot;: &quot;AMGN&quot;, &quot;A&quot;: 49.828466, &quot;B&quot;: 10.058943, &quot;Market Cap&quot;: 1.395518e+05},\n    {&quot;date&quot;: &quot;2019-10-01&quot;, &quot;group&quot;: &quot;AAPL&quot;, &quot;A&quot;: 74.297570, &quot;B&quot;: 11.237253, &quot;Market Cap&quot;: 2.116356e+06},\n    {&quot;date&quot;: &quot;2019-10-01&quot;, &quot;group&quot;: &quot;AMGN&quot;, &quot;A&quot;: 56.841946, &quot;B&quot;: 10.237481, &quot;Market Cap&quot;: 1.395518e+05},\n    {&quot;date&quot;: &quot;2019-12-31&quot;, &quot;group&quot;: &quot;AAPL&quot;, &quot;A&quot;: 97.435257, &quot;B&quot;: 14.736749, &quot;Market Cap&quot;: 2.116356e+06},\n    {&quot;date&quot;: &quot;2019-12-31&quot;, &quot;group&quot;: &quot;AMGN&quot;, &quot;A&quot;: 71.400903, &quot;B&quot;: 12.859612, &quot;Market Cap&quot;: 1.395518e+05},\n]\nindex = [\n    [pandas.to_datetime(line.get(&quot;date&quot;)) for line in data],\n    [line.get(&quot;group&quot;) for line in data],\n]\ncolumns = [\n    [&quot;Value&quot;, &quot;Value&quot;, &quot;Size&quot;],\n    [&quot;A&quot;, &quot;B&quot;, &quot;Market Cap&quot;]\n]\ndf = pandas.DataFrame(data=[[line.get(&quot;A&quot;), line.get(&quot;B&quot;), line.get(&quot;Market Cap&quot;)] for line in data], index=index, columns=columns)\n\n\n# Your lambda function in a separate definition\ndef process_group(group):\n\n    # Nested\n    def _sub(sub):\n        # winsorize returns an numpy array, sub is a dataframe; sub[:] replaces the &quot;values&quot; of the dataframe, not the dataframe itself\n        sub[:] = winsorize(a=sub, limits=[0.4, 0.6])  # I didn't know your limits so I've guessed...\n        return sub\n\n    # Return the result of the processing on the nested group\n    return group.groupby(level=1, axis=1).apply(_sub)\n\n# Process the groups\ndf = df.groupby(level=0, axis=0).apply(process_group)\n</code></pre>\n<p>Output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>                     Value                  Size\n                         A          B Market Cap\n2019-07-01 AAPL  49.828466   9.328360   139551.8\n           AMGN  49.828466   9.328360   139551.8\n2019-10-01 AAPL  56.841946  10.237481   139551.8\n           AMGN  56.841946  10.237481   139551.8\n2019-12-31 AAPL  71.400903  12.859612   139551.8\n           AMGN  71.400903  12.859612   139551.8\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that is multi indexed in that manner.</p>\n<pre><code>                                  Value              Size\n                           A               B      Market Cap\n2019-07-01 AAPL         89.583458      9.328360  2.116356e+06\n           AMGN         49.828466     10.058943  1.395518e+05\n2019-10-01 AAPL         74.297570     11.237253  2.116356e+06\n           AMGN         56.841946     10.237481  1.395518e+05\n2019-12-31 AAPL         97.435257     14.736749  2.116356e+06\n           AMGN         71.400903     12.859612  1.395518e+05\n</code></pre>\n<p>I want to apply a function to each of its columns, for each date (so the 89.583458 and 49.828466 go together, 9.328360 and 10.058943 go together, and so forth)</p>\n<pre><code>winsorized_df = pipeline_df.groupby(level=0, axis=0).apply(\n                lambda level_0_col: level_0_col.groupby(level=1, axis=1).apply(\n                    lambda series: mstats.winsorize(a=series, limits=winsorize_bounds))\n            )\n</code></pre>\n<p>This gives me</p>\n<pre><code>                                              Market Cap  ...                             B\n2019-07-01  [[139551.76568603513], [139551.76568603513]]  ...  [[49.828465616227064], [49.828465616227064]]\n2019-10-01  [[139551.76568603513], [139551.76568603513]]  ...    [[56.84194615992103], [56.84194615992103]]\n2019-12-31  [[139551.76568603513], [139551.76568603513]]  ...    [[71.40090272484755], [71.40090272484755]]\n</code></pre>\n<p>But now I need to recover the lost indices (to get back the same structure as the original), but failed at setting <code>as_index=False</code>, unstacking or using pd.MultiIndex.from_frame. Any idea? Perhaps there's a better to get exactly that from the <code>groupby</code> call?</p>\n",
        "formatted_input": {
            "qid": 64936694,
            "link": "https://stackoverflow.com/questions/64936694/recovering-dataframe-multiindex-from-both-row-and-column-after-groupby",
            "question": {
                "title": "Recovering DataFrame MultiIndex (from both row and column) after groupby",
                "ques_desc": "I have a dataframe that is multi indexed in that manner. I want to apply a function to each of its columns, for each date (so the 89.583458 and 49.828466 go together, 9.328360 and 10.058943 go together, and so forth) This gives me But now I need to recover the lost indices (to get back the same structure as the original), but failed at setting , unstacking or using pd.MultiIndex.from_frame. Any idea? Perhaps there's a better to get exactly that from the call? "
            },
            "io": [
                "                                  Value              Size\n                           A               B      Market Cap\n2019-07-01 AAPL         89.583458      9.328360  2.116356e+06\n           AMGN         49.828466     10.058943  1.395518e+05\n2019-10-01 AAPL         74.297570     11.237253  2.116356e+06\n           AMGN         56.841946     10.237481  1.395518e+05\n2019-12-31 AAPL         97.435257     14.736749  2.116356e+06\n           AMGN         71.400903     12.859612  1.395518e+05\n",
                "                                              Market Cap  ...                             B\n2019-07-01  [[139551.76568603513], [139551.76568603513]]  ...  [[49.828465616227064], [49.828465616227064]]\n2019-10-01  [[139551.76568603513], [139551.76568603513]]  ...    [[56.84194615992103], [56.84194615992103]]\n2019-12-31  [[139551.76568603513], [139551.76568603513]]  ...    [[71.40090272484755], [71.40090272484755]]\n"
            ],
            "answer": {
                "ans_desc": "The problem is that returns a numpy array. So you're replacing a dataframe with a numpy array (wich is why you see in your output). Instead, you should replace the values of the dataframe. Here is an example: Output: ",
                "code": [
                    "import pandas\nfrom scipy.stats.mstats import winsorize\n\n# Recreating your dataframe\ndata = [\n    {\"date\": \"2019-07-01\", \"group\": \"AAPL\", \"A\": 89.583458, \"B\": 9.328360, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-07-01\", \"group\": \"AMGN\", \"A\": 49.828466, \"B\": 10.058943, \"Market Cap\": 1.395518e+05},\n    {\"date\": \"2019-10-01\", \"group\": \"AAPL\", \"A\": 74.297570, \"B\": 11.237253, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-10-01\", \"group\": \"AMGN\", \"A\": 56.841946, \"B\": 10.237481, \"Market Cap\": 1.395518e+05},\n    {\"date\": \"2019-12-31\", \"group\": \"AAPL\", \"A\": 97.435257, \"B\": 14.736749, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-12-31\", \"group\": \"AMGN\", \"A\": 71.400903, \"B\": 12.859612, \"Market Cap\": 1.395518e+05},\n]\nindex = [\n    [pandas.to_datetime(line.get(\"date\")) for line in data],\n    [line.get(\"group\") for line in data],\n]\ncolumns = [\n    [\"Value\", \"Value\", \"Size\"],\n    [\"A\", \"B\", \"Market Cap\"]\n]\ndf = pandas.DataFrame(data=[[line.get(\"A\"), line.get(\"B\"), line.get(\"Market Cap\")] for line in data], index=index, columns=columns)\n\n\n# Your lambda function in a separate definition\ndef process_group(group):\n\n    # Nested\n    def _sub(sub):\n        # winsorize returns an numpy array, sub is a dataframe; sub[:] replaces the \"values\" of the dataframe, not the dataframe itself\n        sub[:] = winsorize(a=sub, limits=[0.4, 0.6])  # I didn't know your limits so I've guessed...\n        return sub\n\n    # Return the result of the processing on the nested group\n    return group.groupby(level=1, axis=1).apply(_sub)\n\n# Process the groups\ndf = df.groupby(level=0, axis=0).apply(process_group)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 926,
            "user_id": 2397119,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://i.stack.imgur.com/ZAwet.jpg?s=128&g=1",
            "display_name": "Aman Singh",
            "link": "https://stackoverflow.com/users/2397119/aman-singh"
        },
        "is_answered": true,
        "view_count": 1367,
        "accepted_answer_id": 56437497,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1606301283,
        "creation_date": 1559622904,
        "question_id": 56437405,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56437405/explode-pandas-dataframe-singe-row-into-multiple-rows-across-multiple-columns-si",
        "title": "Explode pandas dataframe singe row into multiple rows across multiple columns simultaneously",
        "body": "<p>I have a dataframe <code>df</code> as</p>\n\n<pre><code>df\n       col1 act_id col2                                                                                                 \n   --------------------\n0  40;30;30   act1 A;B;C\n1  25;50;25   act2 D;E;F\n2     70;30   act3 G;H\n</code></pre>\n\n<p>I want to break each record in such a way that values in column <code>col1</code> and <code>col2</code> explode into multiple rows but such that the first value in <code>col1</code> after splitting upon <code>';'</code> corresponds to the first value in <code>col2</code> after splitting upon <code>';'</code>. So my <code>desired_df</code> should look like this:</p>\n\n<pre><code>desired_df\n       col1 act_id col2                                                                                                 \n       ---------------\n    0  40   act1   A\n    1  30   act1   B\n    2  30   act1   C\n    3  25   act2   D\n    4  50   act2   E\n    5  25   act2   F                                                                                                  \n    6  70   act3   G                                                                              \n    7  30   act3   H                                                                               \n</code></pre>\n\n<p><strong>NOTE:</strong> this is not the same as <a href=\"https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\">Split (explode) pandas dataframe string entry to separate rows\n</a> as here the exploding/splitting of one record is not just across one column but the need is to split or explode one row into multiple rows, in two columns simultaneously.</p>\n\n<p>Any help is appreciated. Thanks</p>\n",
        "answer_body": "<p>one way to do this</p>\n\n<pre><code>df2.set_index('act_id').apply(lambda x: pd.Series(x.col1.split(';'),x.col2.split(';')), axis=1).stack().dropna().reset_index()\n\ndf2.columns = ['col1','act_id','col2']\n</code></pre>\n\n<p></p>\n\n<pre><code>  col1 act_id col2\n0  A    act1   40 \n1  B    act1   30 \n2  C    act1   30 \n3  D    act2   25 \n4  E    act2   50 \n5  F    act2   25 \n6  G    act3   70 \n7  H    act3   30 \n</code></pre>\n",
        "question_body": "<p>I have a dataframe <code>df</code> as</p>\n\n<pre><code>df\n       col1 act_id col2                                                                                                 \n   --------------------\n0  40;30;30   act1 A;B;C\n1  25;50;25   act2 D;E;F\n2     70;30   act3 G;H\n</code></pre>\n\n<p>I want to break each record in such a way that values in column <code>col1</code> and <code>col2</code> explode into multiple rows but such that the first value in <code>col1</code> after splitting upon <code>';'</code> corresponds to the first value in <code>col2</code> after splitting upon <code>';'</code>. So my <code>desired_df</code> should look like this:</p>\n\n<pre><code>desired_df\n       col1 act_id col2                                                                                                 \n       ---------------\n    0  40   act1   A\n    1  30   act1   B\n    2  30   act1   C\n    3  25   act2   D\n    4  50   act2   E\n    5  25   act2   F                                                                                                  \n    6  70   act3   G                                                                              \n    7  30   act3   H                                                                               \n</code></pre>\n\n<p><strong>NOTE:</strong> this is not the same as <a href=\"https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\">Split (explode) pandas dataframe string entry to separate rows\n</a> as here the exploding/splitting of one record is not just across one column but the need is to split or explode one row into multiple rows, in two columns simultaneously.</p>\n\n<p>Any help is appreciated. Thanks</p>\n",
        "formatted_input": {
            "qid": 56437405,
            "link": "https://stackoverflow.com/questions/56437405/explode-pandas-dataframe-singe-row-into-multiple-rows-across-multiple-columns-si",
            "question": {
                "title": "Explode pandas dataframe singe row into multiple rows across multiple columns simultaneously",
                "ques_desc": "I have a dataframe as I want to break each record in such a way that values in column and explode into multiple rows but such that the first value in after splitting upon corresponds to the first value in after splitting upon . So my should look like this: NOTE: this is not the same as Split (explode) pandas dataframe string entry to separate rows as here the exploding/splitting of one record is not just across one column but the need is to split or explode one row into multiple rows, in two columns simultaneously. Any help is appreciated. Thanks "
            },
            "io": [
                "df\n       col1 act_id col2                                                                                                 \n   --------------------\n0  40;30;30   act1 A;B;C\n1  25;50;25   act2 D;E;F\n2     70;30   act3 G;H\n",
                "desired_df\n       col1 act_id col2                                                                                                 \n       ---------------\n    0  40   act1   A\n    1  30   act1   B\n    2  30   act1   C\n    3  25   act2   D\n    4  50   act2   E\n    5  25   act2   F                                                                                                  \n    6  70   act3   G                                                                              \n    7  30   act3   H                                                                               \n"
            ],
            "answer": {
                "ans_desc": "one way to do this ",
                "code": [
                    "df2.set_index('act_id').apply(lambda x: pd.Series(x.col1.split(';'),x.col2.split(';')), axis=1).stack().dropna().reset_index()\n\ndf2.columns = ['col1','act_id','col2']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 161,
            "user_id": 13929402,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c29c7515502b77402e3872707b8df4fb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ramsey",
            "link": "https://stackoverflow.com/users/13929402/ramsey"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 64973145,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1606153841,
        "creation_date": 1606151294,
        "question_id": 64972959,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64972959/choose-the-next-number-from-columns-in-pandas",
        "title": "Choose the next number from columns in Pandas",
        "body": "<pre><code>ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5\nUN   22.85  22.86  8830500  0.21  25   34   12   87   105  102\nSS   55.01  52.67  6500     5.45  84   122  147  124  644  788   \nPN   90.00  90.99  1000     102   89   55   100  156  44   87     \nPI   184.99 182.38 15000    84    56   77   97   45   44   33    \n</code></pre>\n<p>I want to make a new column that shows the next biggest value after <code>'Cl'</code> within the <code>R0,R1,R2,R3,R4,R5</code> columns. Following is my intended result:</p>\n<pre><code>ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5  X\nUN  22.85  22.86  8830500  0.21   25   34   12   87   105  102 25\nSS   55.01  52.67  6500     5.45  84   122  147  124  644  788 84  \nPN   90.00  90.99  1000     102   89   55   100  156  44   87 100   \nPI   184.99 182.38 15000    84    56   77   97   45   44   33 NaN  \n</code></pre>\n<p>I have been researching it a little but no luck. Some help will be appreciated, Thanks!</p>\n",
        "answer_body": "<p>Depending on what do you mean by the next biggest. If you mean by the order from <code>R0-&gt;R5</code> we can try <code>idxmax</code>:</p>\n<pre><code># extract the `R` columns\ns = df.filter(like='R')\n\n# find out where these columns are larger than `Cl`:\nmask = s.gt(df['Cl'], axis='rows')\n\n# extract the values with `idxmax` and `lookup`:\ndf['X'] = np.where(mask.any(1), s.lookup(s.index,mask.idxmax(1)), np.nan)\n</code></pre>\n<p>Output:</p>\n<pre><code>   ID      Op      Cl        V       C  R0   R1   R2   R3   R4   R5      X\n0  UN   22.85   22.86  8830500    0.21  25   34   12   87  105  102   25.0\n1  SS   55.01   52.67     6500    5.45  84  122  147  124  644  788   84.0\n2  PN   90.00   90.99     1000  102.00  89   55  100  156   44   87  100.0\n3  PI  184.99  182.38    15000   84.00  56   77   97   45   44   33    NaN\n</code></pre>\n<hr />\n<p>If by next biggest, you mean in terms of the values, we can modify the above with <code>sort</code>:</p>\n<pre><code># extract and sort by rows\ns = np.sort(df.filter(like='R').values, axis=1)\n\n# now we work with numpy data:\nmask = s &gt; df['Cl'].values[:,None]\n\n# check and assign\ndf['X'] = np.where(mask.any(1), s[np.arange(s.shape[0]),mask.argmax(1)], np.nan)\n</code></pre>\n<p>Then you pretty much have the same output (for this sample data), but of course with the said meaning.</p>\n",
        "question_body": "<pre><code>ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5\nUN   22.85  22.86  8830500  0.21  25   34   12   87   105  102\nSS   55.01  52.67  6500     5.45  84   122  147  124  644  788   \nPN   90.00  90.99  1000     102   89   55   100  156  44   87     \nPI   184.99 182.38 15000    84    56   77   97   45   44   33    \n</code></pre>\n<p>I want to make a new column that shows the next biggest value after <code>'Cl'</code> within the <code>R0,R1,R2,R3,R4,R5</code> columns. Following is my intended result:</p>\n<pre><code>ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5  X\nUN  22.85  22.86  8830500  0.21   25   34   12   87   105  102 25\nSS   55.01  52.67  6500     5.45  84   122  147  124  644  788 84  \nPN   90.00  90.99  1000     102   89   55   100  156  44   87 100   \nPI   184.99 182.38 15000    84    56   77   97   45   44   33 NaN  \n</code></pre>\n<p>I have been researching it a little but no luck. Some help will be appreciated, Thanks!</p>\n",
        "formatted_input": {
            "qid": 64972959,
            "link": "https://stackoverflow.com/questions/64972959/choose-the-next-number-from-columns-in-pandas",
            "question": {
                "title": "Choose the next number from columns in Pandas",
                "ques_desc": " I want to make a new column that shows the next biggest value after within the columns. Following is my intended result: I have been researching it a little but no luck. Some help will be appreciated, Thanks! "
            },
            "io": [
                "ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5\nUN   22.85  22.86  8830500  0.21  25   34   12   87   105  102\nSS   55.01  52.67  6500     5.45  84   122  147  124  644  788   \nPN   90.00  90.99  1000     102   89   55   100  156  44   87     \nPI   184.99 182.38 15000    84    56   77   97   45   44   33    \n",
                "ID     Op     Cl     V        C   R0   R1   R2   R3   R4   R5  X\nUN  22.85  22.86  8830500  0.21   25   34   12   87   105  102 25\nSS   55.01  52.67  6500     5.45  84   122  147  124  644  788 84  \nPN   90.00  90.99  1000     102   89   55   100  156  44   87 100   \nPI   184.99 182.38 15000    84    56   77   97   45   44   33 NaN  \n"
            ],
            "answer": {
                "ans_desc": "Depending on what do you mean by the next biggest. If you mean by the order from we can try : Output: If by next biggest, you mean in terms of the values, we can modify the above with : Then you pretty much have the same output (for this sample data), but of course with the said meaning. ",
                "code": [
                    "# extract the `R` columns\ns = df.filter(like='R')\n\n# find out where these columns are larger than `Cl`:\nmask = s.gt(df['Cl'], axis='rows')\n\n# extract the values with `idxmax` and `lookup`:\ndf['X'] = np.where(mask.any(1), s.lookup(s.index,mask.idxmax(1)), np.nan)\n",
                    "# extract and sort by rows\ns = np.sort(df.filter(like='R').values, axis=1)\n\n# now we work with numpy data:\nmask = s > df['Cl'].values[:,None]\n\n# check and assign\ndf['X'] = np.where(mask.any(1), s[np.arange(s.shape[0]),mask.argmax(1)], np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 165,
            "user_id": 13566536,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ccf16318a209649db20090fa31cafed2?s=128&d=identicon&r=PG&f=1",
            "display_name": "Oam",
            "link": "https://stackoverflow.com/users/13566536/oam"
        },
        "is_answered": true,
        "view_count": 138,
        "accepted_answer_id": 64969827,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1606140173,
        "creation_date": 1606137954,
        "question_id": 64969344,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64969344/merge-multiple-columns-into-one-by-placing-one-below-the-other-based-on-column-v",
        "title": "Merge multiple columns into one by placing one below the other based on column value pandas dataframe",
        "body": "<p>I have the following dataframe df:</p>\n<pre><code>Video               1   1   1   1   1   1   1   1   1   1   ... 36  36  36  36  36  36  36  36  36  36\nConfidence Value    3   3   4   4   4   5   5   3   5   3   ... 3   3   3   2   4   2   3   3   3   3\n</code></pre>\n<p>Where the row <code>Video</code> is the row with the names of the columns in the dataframe (i.e. the row with bold font that states the names of each column).</p>\n<p>What I want is to rearrange this dataframe so that the output is this:</p>\n<pre><code>Video 1 2 3 ... 36\n0     3 5 4 ... 3\n1     1 2 3 ... 2\n2     2 4 4 ... 5\n3     4 5 4 ... 3\n...\n</code></pre>\n<p>I have tried searching different ways to append, concatenate, merge etc. the columns in the way that I want but I can't figure out how since there are multiple instances of each Video, i.e. multiple <code>1, 2, .. 36</code>.</p>\n<p>So, for each of these multiple instances, I want to make one column of these with the Video number as the column name, and the rows be all the confidence values, as shown above.</p>\n<p>Is that possible?</p>\n",
        "answer_body": "<p>A transpose-pivot construct may be what suits your need.</p>\n<h2>Data</h2>\n<pre><code>df = pd.read_csv(io.StringIO(&quot;&quot;&quot;\nVideo               1   1   1   1   1   2   2   2   2   2   35  35  35  35  35  36  36  36  36  36\nConfidence Value    3   3   4   4   4   5   5   3   5   3   3   3   3   2   4   2   3   3   3   3\n&quot;&quot;&quot;), sep=r&quot;\\s{2,}&quot;, engine=&quot;python&quot;, header=None, index_col=0)\n\nprint(df)\n                  1   2   3   4   5   6   7   ...  14  15  16  17  18  19  20\n0                                             ...                            \nVideo              1   1   1   1   1   2   2  ...  35  35  36  36  36  36  36\nConfidence Value   3   3   4   4   4   5   5  ...   2   4   2   3   3   3   3\n[2 rows x 20 columns]\n</code></pre>\n<h2>Code</h2>\n<p>This should work for indefinite number of confidence values per video:</p>\n<pre><code>idx = df.transpose().groupby(&quot;Video&quot;).cumcount().values\nans = df.transpose().set_index(idx).pivot(columns=&quot;Video&quot;, values=&quot;Confidence Value&quot;)\n</code></pre>\n<p>Note: <strong>If the number of confidence values per video are the same</strong> (5 in the example), then the <code>groupby-cumcount</code> step can be further simplified:</p>\n<pre><code>ans = df.transpose().set_index(np.tile(range(5), 4)).pivot(columns=&quot;Video&quot;, values=&quot;Confidence Value&quot;)\n</code></pre>\n<h2>Result</h2>\n<pre><code>print(ans)\n\nVideo  1   2   35  36\n0       3   5   3   2\n1       3   5   3   3\n2       4   3   3   3\n3       4   5   2   3\n4       4   3   4   3\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe df:</p>\n<pre><code>Video               1   1   1   1   1   1   1   1   1   1   ... 36  36  36  36  36  36  36  36  36  36\nConfidence Value    3   3   4   4   4   5   5   3   5   3   ... 3   3   3   2   4   2   3   3   3   3\n</code></pre>\n<p>Where the row <code>Video</code> is the row with the names of the columns in the dataframe (i.e. the row with bold font that states the names of each column).</p>\n<p>What I want is to rearrange this dataframe so that the output is this:</p>\n<pre><code>Video 1 2 3 ... 36\n0     3 5 4 ... 3\n1     1 2 3 ... 2\n2     2 4 4 ... 5\n3     4 5 4 ... 3\n...\n</code></pre>\n<p>I have tried searching different ways to append, concatenate, merge etc. the columns in the way that I want but I can't figure out how since there are multiple instances of each Video, i.e. multiple <code>1, 2, .. 36</code>.</p>\n<p>So, for each of these multiple instances, I want to make one column of these with the Video number as the column name, and the rows be all the confidence values, as shown above.</p>\n<p>Is that possible?</p>\n",
        "formatted_input": {
            "qid": 64969344,
            "link": "https://stackoverflow.com/questions/64969344/merge-multiple-columns-into-one-by-placing-one-below-the-other-based-on-column-v",
            "question": {
                "title": "Merge multiple columns into one by placing one below the other based on column value pandas dataframe",
                "ques_desc": "I have the following dataframe df: Where the row is the row with the names of the columns in the dataframe (i.e. the row with bold font that states the names of each column). What I want is to rearrange this dataframe so that the output is this: I have tried searching different ways to append, concatenate, merge etc. the columns in the way that I want but I can't figure out how since there are multiple instances of each Video, i.e. multiple . So, for each of these multiple instances, I want to make one column of these with the Video number as the column name, and the rows be all the confidence values, as shown above. Is that possible? "
            },
            "io": [
                "Video               1   1   1   1   1   1   1   1   1   1   ... 36  36  36  36  36  36  36  36  36  36\nConfidence Value    3   3   4   4   4   5   5   3   5   3   ... 3   3   3   2   4   2   3   3   3   3\n",
                "Video 1 2 3 ... 36\n0     3 5 4 ... 3\n1     1 2 3 ... 2\n2     2 4 4 ... 5\n3     4 5 4 ... 3\n...\n"
            ],
            "answer": {
                "ans_desc": "A transpose-pivot construct may be what suits your need. Data Code This should work for indefinite number of confidence values per video: Note: If the number of confidence values per video are the same (5 in the example), then the step can be further simplified: Result ",
                "code": [
                    "idx = df.transpose().groupby(\"Video\").cumcount().values\nans = df.transpose().set_index(idx).pivot(columns=\"Video\", values=\"Confidence Value\")\n",
                    "ans = df.transpose().set_index(np.tile(range(5), 4)).pivot(columns=\"Video\", values=\"Confidence Value\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 5,
            "user_id": 14586990,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f19d0a90a882aee360f3cd5412ca7386?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ahmed Ahmed",
            "link": "https://stackoverflow.com/users/14586990/ahmed-ahmed"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 64958631,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1606073626,
        "creation_date": 1606072662,
        "question_id": 64958453,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64958453/how-to-insert-values-of-a-dataframe-to-others-dataframe",
        "title": "How to insert values of a dataframe to others dataframe",
        "body": "<p>This is the first dataframe:</p>\n<pre><code>df1=\n\nName\na\nb\n</code></pre>\n<p>and in the other side I have other dataframes (CSV files) that have the same content. Here is two exemple:</p>\n<pre><code>\ndf3=      df4=\nA B C     A B C \n1 2 3     1 2 3\n4 5 6     4 5 6\n</code></pre>\n<p>I want to add new column to each dataframe and to add each element of the first dataset to the others. Expected output:</p>\n<pre><code>df3=      df4=\nA B C D    A B C D\n1 2 3 a    1 2 3 b\n4 5 6 a    4 5 6 b\n</code></pre>\n<p><strong>NB</strong>: I read CSV files as pandas so here the multiple dataframe are multiple csv files and I'm inserting new colum to each file and this column contain an element from the first dataframe.df1.</p>\n<p>This is my code it add the last element of df1 to all other df like that:</p>\n<pre><code>df3=      df4=\nA B C D    A B C D\n1 2 3 b    1 2 3 b\n4 5 6 b    4 5 6 b\n\nimport csv\nimport glob\nimport pandas as pd\n\nlist1=[]\ndf=pd.read_csv('C:/dataframe_1.csv', sep=',')\nfor elem in df['Name']:\n    list1.append(elem)\n\nfor elm in list1:\n    os.chdir('C:/New')\n    extension = 'csv'\n    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n    for file in all_filenames:\n        df1 = pd.read_csv(file, sep=',')\n        df1['new_column']=elm\n    df1.to_csv(file, index=False, na_rep='NaN')\n</code></pre>\n",
        "answer_body": "<p>You don't need to iterate through both <code>list1</code> and <code>all_filenames</code>. That is where the problem originates. You are overriding each file <em>multiple</em> times, keeping only the last value from <code>list1</code> (where <em>multiple</em> is equal to <code>len(list1)</code>).</p>\n<p>Drop the outer loop and use <code>enumerate</code> to index into the <code>list1</code> instead.</p>\n<pre><code>list1=[]\ndf=pd.read_csv('C:/dataframe_1.csv', sep=',')\nfor elem in df['Name']:\n    list1.append(elem)\n\nos.chdir('C:/New')\nextension = 'csv'\nall_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n\nfor i, file in enumerate(all_filenames):\n    df1 = pd.read_csv(file, sep=',')\n    df1['new_column'] = list1[i]\n    \ndf1.to_csv(file, index=False, na_rep='NaN')\n</code></pre>\n",
        "question_body": "<p>This is the first dataframe:</p>\n<pre><code>df1=\n\nName\na\nb\n</code></pre>\n<p>and in the other side I have other dataframes (CSV files) that have the same content. Here is two exemple:</p>\n<pre><code>\ndf3=      df4=\nA B C     A B C \n1 2 3     1 2 3\n4 5 6     4 5 6\n</code></pre>\n<p>I want to add new column to each dataframe and to add each element of the first dataset to the others. Expected output:</p>\n<pre><code>df3=      df4=\nA B C D    A B C D\n1 2 3 a    1 2 3 b\n4 5 6 a    4 5 6 b\n</code></pre>\n<p><strong>NB</strong>: I read CSV files as pandas so here the multiple dataframe are multiple csv files and I'm inserting new colum to each file and this column contain an element from the first dataframe.df1.</p>\n<p>This is my code it add the last element of df1 to all other df like that:</p>\n<pre><code>df3=      df4=\nA B C D    A B C D\n1 2 3 b    1 2 3 b\n4 5 6 b    4 5 6 b\n\nimport csv\nimport glob\nimport pandas as pd\n\nlist1=[]\ndf=pd.read_csv('C:/dataframe_1.csv', sep=',')\nfor elem in df['Name']:\n    list1.append(elem)\n\nfor elm in list1:\n    os.chdir('C:/New')\n    extension = 'csv'\n    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n    for file in all_filenames:\n        df1 = pd.read_csv(file, sep=',')\n        df1['new_column']=elm\n    df1.to_csv(file, index=False, na_rep='NaN')\n</code></pre>\n",
        "formatted_input": {
            "qid": 64958453,
            "link": "https://stackoverflow.com/questions/64958453/how-to-insert-values-of-a-dataframe-to-others-dataframe",
            "question": {
                "title": "How to insert values of a dataframe to others dataframe",
                "ques_desc": "This is the first dataframe: and in the other side I have other dataframes (CSV files) that have the same content. Here is two exemple: I want to add new column to each dataframe and to add each element of the first dataset to the others. Expected output: NB: I read CSV files as pandas so here the multiple dataframe are multiple csv files and I'm inserting new colum to each file and this column contain an element from the first dataframe.df1. This is my code it add the last element of df1 to all other df like that: "
            },
            "io": [
                "\ndf3=      df4=\nA B C     A B C \n1 2 3     1 2 3\n4 5 6     4 5 6\n",
                "df3=      df4=\nA B C D    A B C D\n1 2 3 a    1 2 3 b\n4 5 6 a    4 5 6 b\n"
            ],
            "answer": {
                "ans_desc": "You don't need to iterate through both and . That is where the problem originates. You are overriding each file multiple times, keeping only the last value from (where multiple is equal to ). Drop the outer loop and use to index into the instead. ",
                "code": [
                    "list1=[]\ndf=pd.read_csv('C:/dataframe_1.csv', sep=',')\nfor elem in df['Name']:\n    list1.append(elem)\n\nos.chdir('C:/New')\nextension = 'csv'\nall_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n\nfor i, file in enumerate(all_filenames):\n    df1 = pd.read_csv(file, sep=',')\n    df1['new_column'] = list1[i]\n    \ndf1.to_csv(file, index=False, na_rep='NaN')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 12142978,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-G1-cMXZ6m5w/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfDDAl2SW5AGkDuHdqHO_8MAbF_2A/photo.jpg?sz=128",
            "display_name": "aman jain",
            "link": "https://stackoverflow.com/users/12142978/aman-jain"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 64904698,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1605754324,
        "creation_date": 1605754108,
        "question_id": 64904669,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64904669/process-pandas-group-efficiently",
        "title": "Process pandas group efficiently",
        "body": "<p>I have a dataframe df with columns a,b,c,d and e. What I want is, group by df on the basis of a,b and c. And tthen for each group I want to remove NULL value of column d and e with most frequent value of that column in that group. And then finally drop duplicates for each group.</p>\n<p>I am doing the following procesing:</p>\n<pre><code>        final_df = pd.DataFrame()\n        grouped = df.groupby(['a', 'b', 'c'])\n        for _, group in grouped:\n            group = group.replace('', np.nan)\n            group = group.fillna(group.mode().iloc[0])\n            group.drop_duplicates(keep='first', inplace=True)\n            final_df = pd.concat([rows_to_insert, final_df])\n</code></pre>\n<p>But the iteration is making my processing really very slow. Can someone suggest me better way to do it?</p>\n<p>Sample input:</p>\n<pre><code>a   b   c   d       e\na1  b1  c1  NULL    e2\na2  b2  c2  NULL    NULL\na2  b2  c2  NULL    NULL\na1  b1  c3  d4      e4\na1  b1  c1  NULL    e2\na1  b1  c1  d1      e2\na1  b1  c1  d1     NULL\n\n</code></pre>\n<p>Sample output:</p>\n<pre><code>a   b   c   d         e\na1  b1  c1  d1      e2\na2  b2  c2  NULL    NULL\na1  b1  c3  d4      e4\n\n</code></pre>\n",
        "answer_body": "<p>You want <code>groupby().mode</code> with a catch when the data is all <code>NaN</code>:</p>\n<pre><code>def get_mode(series):\n    out = series.mode()\n    return out.iloc[0] if len(out) else np.nan\n\ndf.groupby(['a','b','c'], as_index=False, sort=False).agg(get_mode)\n</code></pre>\n<p>Output:</p>\n<pre><code>    a   b   c    d    e\n0  a1  b1  c1   d1   e2\n1  a2  b2  c2  NaN  NaN\n2  a1  b1  c3   d4   e4\n</code></pre>\n<p>And if you want to fill your original dataframe with the mode:</p>\n<pre><code>df[['d','e']] = df.groupby(['a','b','c']).transform(get_mode)\n</code></pre>\n<p>Output:</p>\n<pre><code>    a   b   c    d    e\n0  a1  b1  c1   d1   e2\n1  a2  b2  c2  NaN  NaN\n2  a2  b2  c2  NaN  NaN\n3  a1  b1  c3   d4   e4\n4  a1  b1  c1   d1   e2\n5  a1  b1  c1   d1   e2\n6  a1  b1  c1   d1   e2\n</code></pre>\n",
        "question_body": "<p>I have a dataframe df with columns a,b,c,d and e. What I want is, group by df on the basis of a,b and c. And tthen for each group I want to remove NULL value of column d and e with most frequent value of that column in that group. And then finally drop duplicates for each group.</p>\n<p>I am doing the following procesing:</p>\n<pre><code>        final_df = pd.DataFrame()\n        grouped = df.groupby(['a', 'b', 'c'])\n        for _, group in grouped:\n            group = group.replace('', np.nan)\n            group = group.fillna(group.mode().iloc[0])\n            group.drop_duplicates(keep='first', inplace=True)\n            final_df = pd.concat([rows_to_insert, final_df])\n</code></pre>\n<p>But the iteration is making my processing really very slow. Can someone suggest me better way to do it?</p>\n<p>Sample input:</p>\n<pre><code>a   b   c   d       e\na1  b1  c1  NULL    e2\na2  b2  c2  NULL    NULL\na2  b2  c2  NULL    NULL\na1  b1  c3  d4      e4\na1  b1  c1  NULL    e2\na1  b1  c1  d1      e2\na1  b1  c1  d1     NULL\n\n</code></pre>\n<p>Sample output:</p>\n<pre><code>a   b   c   d         e\na1  b1  c1  d1      e2\na2  b2  c2  NULL    NULL\na1  b1  c3  d4      e4\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 64904669,
            "link": "https://stackoverflow.com/questions/64904669/process-pandas-group-efficiently",
            "question": {
                "title": "Process pandas group efficiently",
                "ques_desc": "I have a dataframe df with columns a,b,c,d and e. What I want is, group by df on the basis of a,b and c. And tthen for each group I want to remove NULL value of column d and e with most frequent value of that column in that group. And then finally drop duplicates for each group. I am doing the following procesing: But the iteration is making my processing really very slow. Can someone suggest me better way to do it? Sample input: Sample output: "
            },
            "io": [
                "a   b   c   d       e\na1  b1  c1  NULL    e2\na2  b2  c2  NULL    NULL\na2  b2  c2  NULL    NULL\na1  b1  c3  d4      e4\na1  b1  c1  NULL    e2\na1  b1  c1  d1      e2\na1  b1  c1  d1     NULL\n\n",
                "a   b   c   d         e\na1  b1  c1  d1      e2\na2  b2  c2  NULL    NULL\na1  b1  c3  d4      e4\n\n"
            ],
            "answer": {
                "ans_desc": "You want with a catch when the data is all : Output: And if you want to fill your original dataframe with the mode: Output: ",
                "code": [
                    "def get_mode(series):\n    out = series.mode()\n    return out.iloc[0] if len(out) else np.nan\n\ndf.groupby(['a','b','c'], as_index=False, sort=False).agg(get_mode)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 760,
            "user_id": 10957279,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/10217868241928586/picture?type=large",
            "display_name": "Carbo",
            "link": "https://stackoverflow.com/users/10957279/carbo"
        },
        "is_answered": true,
        "view_count": 37,
        "closed_date": 1605948145,
        "accepted_answer_id": 64883875,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1605649916,
        "creation_date": 1605634055,
        "last_edit_date": 1605636160,
        "question_id": 64880268,
        "link": "https://stackoverflow.com/questions/64880268/mutate-several-columns-from-a-dataframe-based-on-a-mapping-between-values-from-a",
        "closed_reason": "Needs more focus",
        "title": "mutate several columns from a dataframe based on a mapping between values from another dataframe",
        "body": "<p>I have 2 dataframes:</p>\n<p><code>df</code> looks like this</p>\n<pre><code>id   v1   v2   v3    v4   etc.\n1    1     4    2     5\n2    4     4    6     1\n3    2     1    3     4\netc.\n</code></pre>\n<p>while <code>rules</code> looks like this</p>\n<pre><code>id       name\n1        red\n2        blue  \n3        grey \n4        green  \n5        black  \n6        gold \netc\n</code></pre>\n<p>what i want to obtain is the following</p>\n<pre><code>id        v1        v2        v3         v4   etc.\n1         red       green     blue       black\n2         green     green     gold       red\n3         blue      red       grey       green\netc.\n</code></pre>\n<p>so basically using the map between number and colours in rules to mutate df</p>\n",
        "answer_body": "<p>You could do the following:</p>\n<pre><code>results = df.applymap(lambda i: rules.loc[i, 'name'])\n</code></pre>\n<p>With <code>df</code> like</p>\n<pre><code>    v1  v2  v3  v4\nid                \n1    1   4   2   5\n2    4   4   6   1\n3    2   1   3   4\n</code></pre>\n<p>and <code>rules</code> like</p>\n<pre><code>     name\nid       \n1     red\n2    blue\n3    grey\n4   green\n5   black\n6    gold\n</code></pre>\n<p>the result is</p>\n<pre><code>       v1     v2    v3     v4\nid                           \n1     red  green  blue  black\n2   green  green  gold    red\n3    blue    red  grey  green\n</code></pre>\n",
        "question_body": "<p>I have 2 dataframes:</p>\n<p><code>df</code> looks like this</p>\n<pre><code>id   v1   v2   v3    v4   etc.\n1    1     4    2     5\n2    4     4    6     1\n3    2     1    3     4\netc.\n</code></pre>\n<p>while <code>rules</code> looks like this</p>\n<pre><code>id       name\n1        red\n2        blue  \n3        grey \n4        green  \n5        black  \n6        gold \netc\n</code></pre>\n<p>what i want to obtain is the following</p>\n<pre><code>id        v1        v2        v3         v4   etc.\n1         red       green     blue       black\n2         green     green     gold       red\n3         blue      red       grey       green\netc.\n</code></pre>\n<p>so basically using the map between number and colours in rules to mutate df</p>\n",
        "formatted_input": {
            "qid": 64880268,
            "link": "https://stackoverflow.com/questions/64880268/mutate-several-columns-from-a-dataframe-based-on-a-mapping-between-values-from-a",
            "question": {
                "title": "mutate several columns from a dataframe based on a mapping between values from another dataframe",
                "ques_desc": "I have 2 dataframes: looks like this while looks like this what i want to obtain is the following so basically using the map between number and colours in rules to mutate df "
            },
            "io": [
                "id   v1   v2   v3    v4   etc.\n1    1     4    2     5\n2    4     4    6     1\n3    2     1    3     4\netc.\n",
                "id        v1        v2        v3         v4   etc.\n1         red       green     blue       black\n2         green     green     gold       red\n3         blue      red       grey       green\netc.\n"
            ],
            "answer": {
                "ans_desc": "You could do the following: With like and like the result is ",
                "code": [
                    "results = df.applymap(lambda i: rules.loc[i, 'name'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 109,
            "user_id": 13846828,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-0_V-y_-hs5M/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclZNje_nr1pZIiV6pOfSmUN24goKw/photo.jpg?sz=128",
            "display_name": "yoyoyo",
            "link": "https://stackoverflow.com/users/13846828/yoyoyo"
        },
        "is_answered": true,
        "view_count": 113,
        "accepted_answer_id": 64876444,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1605638760,
        "creation_date": 1605620160,
        "question_id": 64876318,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64876318/create-pd-dataframe-from-dictionary-with-multi-dimensional-array",
        "title": "Create pd.DataFrame from dictionary with multi-dimensional array",
        "body": "<p>I've the following dictionary:</p>\n<pre><code>dictA = {'A': [[1, 2, 3], [1, 2, 3], [1, 2, 3]],\n         'B': [[4, 4, 4], [4, 4, 4],],\n         'C': [[4, 6, 0]]\n        }\n</code></pre>\n<p>I want to convert it to a <code>pd.DataFrame()</code>, expecting this:</p>\n<pre><code>id       ColA        ColB        ColC\n0         1           4           4\n1         2           4           6\n2         3           4           0\n3         1           4           \n4         2           4\n5         3           4\n6         1\n7         2\n8         3\n</code></pre>\n<p>How can I do that?\nI'm trying</p>\n<pre><code>pd.DataFrame(dictAll.items(), columns=['ColA', 'ColB', 'ColC'])\n</code></pre>\n<p>But it obviously doesn't work!</p>\n",
        "answer_body": "<p>Here is how:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndictA = {'A': [[1, 2, 3], [1, 2, 3], [1, 2, 3]],\n         'B': [[4, 4, 4], [4, 4, 4],],\n         'C': [[4, 6, 0]]}\n\ndf = pd.DataFrame(dict([(f'Col{k}', pd.Series([a for b in v for a in b])) for k,v in dictA.items()])).replace(np.nan, '')\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>   ColA ColB ColC\n0     1    4    4\n1     2    4    6\n2     3    4    0\n3     1    4     \n4     2    4     \n5     3    4     \n6     1          \n7     2          \n8     3  \n</code></pre>\n<hr />\n<p>Now, let's have a look at the problem one step at a time.</p>\n<ol>\n<li><p>The first thing we might try is simply:</p>\n<pre><code>df = pd.DataFrame(dictA)\nprint(df)\n</code></pre>\n<p>Which, of course, return this error:</p>\n<pre><code> ValueError: arrays must all be same length\n</code></pre>\n</li>\n<li><p>So now we need a way to be able to create dataframes from a <code>dict</code> with arrays of different lengths. For that, we can:</p>\n<pre><code>df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dictA.items()]))\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>           A          B          C\n0  [1, 2, 3]  [4, 4, 4]  [4, 6, 0]\n1  [1, 2, 3]  [4, 4, 4]        NaN\n2  [1, 2, 3]        NaN        NaN\n</code></pre>\n</li>\n<li><p>We want the dataframe to be vertical, so for each iteration, flatten out the lists with a list comprehension:</p>\n<pre><code>df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()]))\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>   A    B    C\n0  1  4.0  4.0\n1  2  4.0  6.0\n2  3  4.0  0.0\n3  1  4.0  NaN\n4  2  4.0  NaN\n5  3  4.0  NaN\n6  1  NaN  NaN\n7  2  NaN  NaN\n8  3  NaN  NaN\n</code></pre>\n</li>\n<li><p>Now we want to replace all the <code>NaN</code>s with blanks. For that, we need to <code>import numpy as np</code>, and do:</p>\n<pre><code>df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()])).replace(np.nan, '')\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>   A  B  C\n0  1  4  4\n1  2  4  6\n2  3  4  0\n3  1  4   \n4  2  4   \n5  3  4   \n6  1      \n7  2      \n8  3     \n</code></pre>\n</li>\n<li><p>Finally use formatted string to convert the letters into <code>&quot;Col&quot;</code> letters:</p>\n<pre><code>df = pd.DataFrame(dict([(f'Col{k}', pd.Series([a for b in v for a in b])) for k,v in dictA.items()])).replace(np.nan, '')\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>   ColA ColB ColC\n0     1    4    4\n1     2    4    6\n2     3    4    0\n3     1    4     \n4     2    4     \n5     3    4     \n6     1          \n7     2          \n8     3  \n</code></pre>\n</li>\n</ol>\n",
        "question_body": "<p>I've the following dictionary:</p>\n<pre><code>dictA = {'A': [[1, 2, 3], [1, 2, 3], [1, 2, 3]],\n         'B': [[4, 4, 4], [4, 4, 4],],\n         'C': [[4, 6, 0]]\n        }\n</code></pre>\n<p>I want to convert it to a <code>pd.DataFrame()</code>, expecting this:</p>\n<pre><code>id       ColA        ColB        ColC\n0         1           4           4\n1         2           4           6\n2         3           4           0\n3         1           4           \n4         2           4\n5         3           4\n6         1\n7         2\n8         3\n</code></pre>\n<p>How can I do that?\nI'm trying</p>\n<pre><code>pd.DataFrame(dictAll.items(), columns=['ColA', 'ColB', 'ColC'])\n</code></pre>\n<p>But it obviously doesn't work!</p>\n",
        "formatted_input": {
            "qid": 64876318,
            "link": "https://stackoverflow.com/questions/64876318/create-pd-dataframe-from-dictionary-with-multi-dimensional-array",
            "question": {
                "title": "Create pd.DataFrame from dictionary with multi-dimensional array",
                "ques_desc": "I've the following dictionary: I want to convert it to a , expecting this: How can I do that? I'm trying But it obviously doesn't work! "
            },
            "io": [
                "dictA = {'A': [[1, 2, 3], [1, 2, 3], [1, 2, 3]],\n         'B': [[4, 4, 4], [4, 4, 4],],\n         'C': [[4, 6, 0]]\n        }\n",
                "id       ColA        ColB        ColC\n0         1           4           4\n1         2           4           6\n2         3           4           0\n3         1           4           \n4         2           4\n5         3           4\n6         1\n7         2\n8         3\n"
            ],
            "answer": {
                "ans_desc": "Here is how: Output: Now, let's have a look at the problem one step at a time. The first thing we might try is simply: Which, of course, return this error: So now we need a way to be able to create dataframes from a with arrays of different lengths. For that, we can: Output: We want the dataframe to be vertical, so for each iteration, flatten out the lists with a list comprehension: Output: Now we want to replace all the s with blanks. For that, we need to , and do: Output: Finally use formatted string to convert the letters into letters: Output: ",
                "code": [
                    " ValueError: arrays must all be same length\n",
                    "df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dictA.items()]))\nprint(df)\n",
                    "df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()]))\nprint(df)\n",
                    "df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()])).replace(np.nan, '')\nprint(df)\n",
                    "df = pd.DataFrame(dict([(f'Col{k}', pd.Series([a for b in v for a in b])) for k,v in dictA.items()])).replace(np.nan, '')\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "import",
            "slice"
        ],
        "owner": {
            "reputation": 29,
            "user_id": 14354708,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-UvrfsZPYaQs/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucnWZcXIta3LAn2xHGhZiDLCRWlKjQ/photo.jpg?sz=128",
            "display_name": "luca giovanni voglino",
            "link": "https://stackoverflow.com/users/14354708/luca-giovanni-voglino"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 64862556,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1605546873,
        "creation_date": 1605546596,
        "question_id": 64862482,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64862482/pandas-slice-dataframe-according-to-values-of-a-column",
        "title": "Pandas: slice Dataframe according to values of a column",
        "body": "<p>I have to slice my Dataframe according to values (imported from a txt) that occur in one of my Dataframe' s column. This is what I have:</p>\n<pre><code>&gt;df\ncol1 col2\n a    1\n b    2\n c    3\n d    4\n\n&gt;'mytxt.txt'\n2\n3\n</code></pre>\n<p>This is what I need: drop rows whenever value in <em>col2</em> is not among values in <em>mytxt.txt</em></p>\n<p>Expected result must be:</p>\n<pre><code>&gt;df\ncol1 col2\n b    2\n c    3\n</code></pre>\n<p>I tried:</p>\n<pre><code>values = pd.read_csv('mytxt.txt', header=None)\ndf = df.col2.isin(values)\n</code></pre>\n<p>But it doesn' t work. Help would be very appreciated, thanks!</p>\n",
        "answer_body": "<p>When you read <code>values</code>, I would do it as a Series, and then convert it to a set, which will be more efficient for lookups:</p>\n<pre class=\"lang-py prettyprint-override\"><code>values = pd.read_csv('mytxt.txt', header=None, squeeze=True)\nvalues = set(values.tolist())\n</code></pre>\n<p>Then slicing will work:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; df[df.col2.isin(values)]\n  col1  col2\n1    b     2\n2    c     3\n</code></pre>\n<p>What was happening is you were reading <code>values</code> in as a DataFrame rather than a Series, so the <code>.isin</code> method was not behaving as you expected.</p>\n",
        "question_body": "<p>I have to slice my Dataframe according to values (imported from a txt) that occur in one of my Dataframe' s column. This is what I have:</p>\n<pre><code>&gt;df\ncol1 col2\n a    1\n b    2\n c    3\n d    4\n\n&gt;'mytxt.txt'\n2\n3\n</code></pre>\n<p>This is what I need: drop rows whenever value in <em>col2</em> is not among values in <em>mytxt.txt</em></p>\n<p>Expected result must be:</p>\n<pre><code>&gt;df\ncol1 col2\n b    2\n c    3\n</code></pre>\n<p>I tried:</p>\n<pre><code>values = pd.read_csv('mytxt.txt', header=None)\ndf = df.col2.isin(values)\n</code></pre>\n<p>But it doesn' t work. Help would be very appreciated, thanks!</p>\n",
        "formatted_input": {
            "qid": 64862482,
            "link": "https://stackoverflow.com/questions/64862482/pandas-slice-dataframe-according-to-values-of-a-column",
            "question": {
                "title": "Pandas: slice Dataframe according to values of a column",
                "ques_desc": "I have to slice my Dataframe according to values (imported from a txt) that occur in one of my Dataframe' s column. This is what I have: This is what I need: drop rows whenever value in col2 is not among values in mytxt.txt Expected result must be: I tried: But it doesn' t work. Help would be very appreciated, thanks! "
            },
            "io": [
                ">df\ncol1 col2\n a    1\n b    2\n c    3\n d    4\n\n>'mytxt.txt'\n2\n3\n",
                ">df\ncol1 col2\n b    2\n c    3\n"
            ],
            "answer": {
                "ans_desc": "When you read , I would do it as a Series, and then convert it to a set, which will be more efficient for lookups: Then slicing will work: What was happening is you were reading in as a DataFrame rather than a Series, so the method was not behaving as you expected. ",
                "code": [
                    "values = pd.read_csv('mytxt.txt', header=None, squeeze=True)\nvalues = set(values.tolist())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10361,
            "user_id": 6031995,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/KAcWh.png?s=128&g=1",
            "display_name": "Kenan",
            "link": "https://stackoverflow.com/users/6031995/kenan"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 64828273,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1605305168,
        "creation_date": 1605302111,
        "question_id": 64828120,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64828120/pandas-fill-gaps-in-a-series-with-mean",
        "title": "Pandas: Fill gaps in a series with mean",
        "body": "<p>Given df</p>\n<p><code>df = pd.DataFrame({'distance': [0,1,2,np.nan,3,4,5,np.nan,np.nan,6]})</code></p>\n<pre><code>   distance\n0       0.0\n1       1.0\n2       2.0\n3       NaN\n4       3.0\n5       4.0\n6       5.0\n7       NaN\n8       NaN\n9       6.0\n</code></pre>\n<p>I want to replace the nans with the inbetween mean</p>\n<p>Expected output:</p>\n<pre><code>   distance\n0       0.0\n1       1.0\n2       2.0\n3       2.5\n4       3.0\n5       4.0\n6       5.0\n7       5.5\n8       5.5\n9       6.0\n</code></pre>\n<p>I have seen <a href=\"https://stackoverflow.com/questions/19966018/pandas-filling-missing-values-by-mean-in-each-group\">this_answer</a> but it's for a grouping which isn't my case and I couldn't find anything else.</p>\n",
        "answer_body": "<p>If you don't want <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html\" rel=\"nofollow noreferrer\"><code>df.interpolate</code></a> you can compute the mean of the surrounding values manually with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.bfill.html\" rel=\"nofollow noreferrer\"><code>df.bfill</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ffill.html\" rel=\"nofollow noreferrer\"><code>df.ffill</code></a></p>\n<pre><code>(df.ffill() + df.bfill()) / 2\n</code></pre>\n<p>Out:</p>\n<pre><code>   distance\n0       0.0\n1       1.0\n2       2.0\n3       2.5\n4       3.0\n5       4.0\n6       5.0\n7       5.5\n8       5.5\n9       6.0\n</code></pre>\n",
        "question_body": "<p>Given df</p>\n<p><code>df = pd.DataFrame({'distance': [0,1,2,np.nan,3,4,5,np.nan,np.nan,6]})</code></p>\n<pre><code>   distance\n0       0.0\n1       1.0\n2       2.0\n3       NaN\n4       3.0\n5       4.0\n6       5.0\n7       NaN\n8       NaN\n9       6.0\n</code></pre>\n<p>I want to replace the nans with the inbetween mean</p>\n<p>Expected output:</p>\n<pre><code>   distance\n0       0.0\n1       1.0\n2       2.0\n3       2.5\n4       3.0\n5       4.0\n6       5.0\n7       5.5\n8       5.5\n9       6.0\n</code></pre>\n<p>I have seen <a href=\"https://stackoverflow.com/questions/19966018/pandas-filling-missing-values-by-mean-in-each-group\">this_answer</a> but it's for a grouping which isn't my case and I couldn't find anything else.</p>\n",
        "formatted_input": {
            "qid": 64828120,
            "link": "https://stackoverflow.com/questions/64828120/pandas-fill-gaps-in-a-series-with-mean",
            "question": {
                "title": "Pandas: Fill gaps in a series with mean",
                "ques_desc": "Given df I want to replace the nans with the inbetween mean Expected output: I have seen this_answer but it's for a grouping which isn't my case and I couldn't find anything else. "
            },
            "io": [
                "   distance\n0       0.0\n1       1.0\n2       2.0\n3       NaN\n4       3.0\n5       4.0\n6       5.0\n7       NaN\n8       NaN\n9       6.0\n",
                "   distance\n0       0.0\n1       1.0\n2       2.0\n3       2.5\n4       3.0\n5       4.0\n6       5.0\n7       5.5\n8       5.5\n9       6.0\n"
            ],
            "answer": {
                "ans_desc": "If you don't want you can compute the mean of the surrounding values manually with and Out: ",
                "code": [
                    "(df.ffill() + df.bfill()) / 2\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 301,
            "user_id": 14587041,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-inOlDAR_mP0/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuck20CVtIA9skvRyZHDHocGPw4poTg/s96-c/photo.jpg?sz=128",
            "display_name": "Samet S&#246;kel",
            "link": "https://stackoverflow.com/users/14587041/samet-s%c3%b6kel"
        },
        "is_answered": true,
        "view_count": 73,
        "accepted_answer_id": 64826230,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1605292348,
        "creation_date": 1605291556,
        "question_id": 64826052,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64826052/how-to-reorder-rows-by-a-condition-in-pandas",
        "title": "How to reorder rows by a condition in pandas?",
        "body": "<p>I have two dataframes and one of their orders is correct for me. I want to make the other's order the same as the correct one. Here is the point, it's not about index numbers, order depends on a variable. Like this</p>\n<p>df1</p>\n<pre><code>A   B\n13  2\n20  5\n15  3\n.   .\n.   .\n</code></pre>\n<p>df2</p>\n<pre><code>A   B\n15  3\n13  2\n20  5\n.   .\n.   .\n</code></pre>\n<p>I want the order of df2 to be same as df1, I put them in a for loop but it took long time (my real data is much greater than reproducible example)</p>\n<p>Is there any easier way to make my wish real ? Thanks in advice.</p>\n",
        "answer_body": "<p>The best way I can think is to replace the index of <code>df2</code> by column <code>A</code> and then call it with loc:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [13,20,15], &quot;B&quot;: [2,5,3]})\ndf2 = pd.DataFrame({&quot;A&quot;: [15,13,20], &quot;B&quot;: [3,2,5]})\ndf2 = df2.set_index(&quot;A&quot;)\nprint(df2.loc[df1[&quot;A&quot;]])\n</code></pre>\n<p>output is:</p>\n<pre><code>    B\nA    \n13  2\n20  5\n15  3\n</code></pre>\n<p>Is this ok for you?</p>\n",
        "question_body": "<p>I have two dataframes and one of their orders is correct for me. I want to make the other's order the same as the correct one. Here is the point, it's not about index numbers, order depends on a variable. Like this</p>\n<p>df1</p>\n<pre><code>A   B\n13  2\n20  5\n15  3\n.   .\n.   .\n</code></pre>\n<p>df2</p>\n<pre><code>A   B\n15  3\n13  2\n20  5\n.   .\n.   .\n</code></pre>\n<p>I want the order of df2 to be same as df1, I put them in a for loop but it took long time (my real data is much greater than reproducible example)</p>\n<p>Is there any easier way to make my wish real ? Thanks in advice.</p>\n",
        "formatted_input": {
            "qid": 64826052,
            "link": "https://stackoverflow.com/questions/64826052/how-to-reorder-rows-by-a-condition-in-pandas",
            "question": {
                "title": "How to reorder rows by a condition in pandas?",
                "ques_desc": "I have two dataframes and one of their orders is correct for me. I want to make the other's order the same as the correct one. Here is the point, it's not about index numbers, order depends on a variable. Like this df1 df2 I want the order of df2 to be same as df1, I put them in a for loop but it took long time (my real data is much greater than reproducible example) Is there any easier way to make my wish real ? Thanks in advice. "
            },
            "io": [
                "A   B\n13  2\n20  5\n15  3\n.   .\n.   .\n",
                "A   B\n15  3\n13  2\n20  5\n.   .\n.   .\n"
            ],
            "answer": {
                "ans_desc": "The best way I can think is to replace the index of by column and then call it with loc: output is: Is this ok for you? ",
                "code": [
                    "df1 = pd.DataFrame({\"A\": [13,20,15], \"B\": [2,5,3]})\ndf2 = pd.DataFrame({\"A\": [15,13,20], \"B\": [3,2,5]})\ndf2 = df2.set_index(\"A\")\nprint(df2.loc[df1[\"A\"]])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 17,
            "user_id": 14584633,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4ec1bc9056a4d2d46d3e4cdf76bb30ae?s=128&d=identicon&r=PG&f=1",
            "display_name": "codingXP",
            "link": "https://stackoverflow.com/users/14584633/codingxp"
        },
        "is_answered": true,
        "view_count": 80,
        "accepted_answer_id": 64790004,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1605176098,
        "creation_date": 1605108085,
        "last_edit_date": 1605176098,
        "question_id": 64789227,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64789227/how-to-replace-duplicate-dataframe-column-values-with-certain-conditions-in-pyth",
        "title": "How to replace duplicate dataframe column values with certain conditions in python",
        "body": "<p>I have a dataframe with shape (10x401) having duplicate columns with same column names and values. Some of them have nulls while other have numeric values. The columns names are not in sorted order. A short example of dataframe is given below:</p>\n<pre><code>       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100\n   \n        1,         ,   ,   ,   ,  3,  3,  3,  3,   ,   ,   ,   ,.........,  0,    0,   0,   0   \n        2,        0,  0,  0,  0,   ,   ,   ,   , 10, 10, 10, 10,.........,   ,     ,    ,   \n        3,        9,  9,  9,  9,  1,  1,  1,  1,  4,  4,  4,  4,.........,  1,    1,   1,   1\n        .\n        .\n        .\n       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 6,    6,   6,   6\n</code></pre>\n<p>By ignoring the null values, i need to replace each first occurrence of the numeric value (from 0 to 10) with 1 and the rest of the values with -1 for all 10 rows and 400 columns ignoring the ID column. The resulting dataframe will look like:</p>\n<pre><code>       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100\n   \n        1,         ,   ,   ,   ,  1, -1, -1, -1,   ,   ,   ,   ,.........,  1,   -1,   -1, -1   \n        2,        1, -1, -1, -1,   ,   ,   ,   ,  1, -1, -1, -1,.........,   ,     ,     ,   \n        3,        1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,.........,  1,   -1,   -1, -1\n        .\n        .\n        .\n       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 1,   -1,   -1, -1\n</code></pre>\n<p>I will be thankful for some help here.</p>\n",
        "answer_body": "<p>First, some example data:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\nfrom io import StringIO\n\ndf_string = '''\nID;1;1;1;1;2;2;2;2;3;3;3;3\n1;;;;;3;3;3;3;;;;\n2;0;0;0;0;;;;;10;10;10;10\n3;9;9;9;9;1;1;1;1;4;4;4;4\n4;;;;;;;;;6;6;6;6\n'''\n\ndf = pd.read_csv(StringIO(df_string), sep = &quot;;&quot;, index_col=&quot;ID&quot;)\n\n# Removing the automatically added .1/.2/... suffixes. You don't need that for your data.\ndf.columns = df.columns.str[0]\n</code></pre>\n<pre><code>      1    1    1    1    2    2    2    2     3     3     3     3\nID                                                                \n1   NaN  NaN  NaN  NaN  3.0  3.0  3.0  3.0   NaN   NaN   NaN   NaN\n2   0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN  10.0  10.0  10.0  10.0\n3   9.0  9.0  9.0  9.0  1.0  1.0  1.0  1.0   4.0   4.0   4.0   4.0\n4   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   6.0   6.0   6.0   6.0\n</code></pre>\n<p>I recommend transposing the DataFrame, as it's more convenient to use the vectorized methods from pandas. Most of them can be used &quot;horizontally&quot; with specifying the <code>axis=1</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.T\n</code></pre>\n<pre><code>ID    1     2    3    4\n1   NaN   0.0  9.0  NaN\n1   NaN   0.0  9.0  NaN\n1   NaN   0.0  9.0  NaN\n1   NaN   0.0  9.0  NaN\n2   3.0   NaN  1.0  NaN\n2   3.0   NaN  1.0  NaN\n2   3.0   NaN  1.0  NaN\n2   3.0   NaN  1.0  NaN\n3   NaN  10.0  4.0  6.0\n3   NaN  10.0  4.0  6.0\n3   NaN  10.0  4.0  6.0\n3   NaN  10.0  4.0  6.0\n</code></pre>\n<p>First you need to know all the cells where there are values:</p>\n<pre class=\"lang-py prettyprint-override\"><code>ValueMask = ~df.isna()\n</code></pre>\n<pre><code>ID      1      2     3      4\n1   False   True  True  False\n1   False   True  True  False\n1   False   True  True  False\n1   False   True  True  False\n2    True  False  True  False\n2    True  False  True  False\n2    True  False  True  False\n2    True  False  True  False\n3   False   True  True   True\n3   False   True  True   True\n3   False   True  True   True\n3   False   True  True   True\n</code></pre>\n<p>Secondly, you need to know all the starting positions of a new group. Shifting the whole DataFrame down by one row and checking for unequality helps. Combining that with your <code>ValueMask</code> gives you the starting cells:</p>\n<pre class=\"lang-py prettyprint-override\"><code>StartMask = (df.shift() != df) &amp; ValueMask\n</code></pre>\n<pre><code>ID      1      2      3      4\n1   False   True   True  False\n1   False  False  False  False\n1   False  False  False  False\n1   False  False  False  False\n2    True  False   True  False\n2   False  False  False  False\n2   False  False  False  False\n2   False  False  False  False\n3   False   True   True   True\n3   False  False  False  False\n3   False  False  False  False\n3   False  False  False  False\n</code></pre>\n<p>Now you can set all the cells of value to <code>-1</code> and afterwards all the cells that are a the start of a group to <code>1</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>df[ValueMask] = -1\ndf[StartMask] = 1\n</code></pre>\n<pre><code>ID    1    2    3    4\n1   NaN  1.0  1.0  NaN\n1   NaN -1.0 -1.0  NaN\n1   NaN -1.0 -1.0  NaN\n1   NaN -1.0 -1.0  NaN\n2   1.0  NaN  1.0  NaN\n2  -1.0  NaN -1.0  NaN\n2  -1.0  NaN -1.0  NaN\n2  -1.0  NaN -1.0  NaN\n3   NaN  1.0  1.0  1.0\n3   NaN -1.0 -1.0 -1.0\n3   NaN -1.0 -1.0 -1.0\n3   NaN -1.0 -1.0 -1.0\n</code></pre>\n<p>Now you can always transpose it back:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.T\n</code></pre>\n<pre><code>      1    1    1    1    2    2    2    2    3    3    3    3\nID                                                            \n1   NaN  NaN  NaN  NaN  1.0 -1.0 -1.0 -1.0  NaN  NaN  NaN  NaN\n2   1.0 -1.0 -1.0 -1.0  NaN  NaN  NaN  NaN  1.0 -1.0 -1.0 -1.0\n3   1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0\n4   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  1.0 -1.0 -1.0 -1.0\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with shape (10x401) having duplicate columns with same column names and values. Some of them have nulls while other have numeric values. The columns names are not in sorted order. A short example of dataframe is given below:</p>\n<pre><code>       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100\n   \n        1,         ,   ,   ,   ,  3,  3,  3,  3,   ,   ,   ,   ,.........,  0,    0,   0,   0   \n        2,        0,  0,  0,  0,   ,   ,   ,   , 10, 10, 10, 10,.........,   ,     ,    ,   \n        3,        9,  9,  9,  9,  1,  1,  1,  1,  4,  4,  4,  4,.........,  1,    1,   1,   1\n        .\n        .\n        .\n       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 6,    6,   6,   6\n</code></pre>\n<p>By ignoring the null values, i need to replace each first occurrence of the numeric value (from 0 to 10) with 1 and the rest of the values with -1 for all 10 rows and 400 columns ignoring the ID column. The resulting dataframe will look like:</p>\n<pre><code>       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100\n   \n        1,         ,   ,   ,   ,  1, -1, -1, -1,   ,   ,   ,   ,.........,  1,   -1,   -1, -1   \n        2,        1, -1, -1, -1,   ,   ,   ,   ,  1, -1, -1, -1,.........,   ,     ,     ,   \n        3,        1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,.........,  1,   -1,   -1, -1\n        .\n        .\n        .\n       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 1,   -1,   -1, -1\n</code></pre>\n<p>I will be thankful for some help here.</p>\n",
        "formatted_input": {
            "qid": 64789227,
            "link": "https://stackoverflow.com/questions/64789227/how-to-replace-duplicate-dataframe-column-values-with-certain-conditions-in-pyth",
            "question": {
                "title": "How to replace duplicate dataframe column values with certain conditions in python",
                "ques_desc": "I have a dataframe with shape (10x401) having duplicate columns with same column names and values. Some of them have nulls while other have numeric values. The columns names are not in sorted order. A short example of dataframe is given below: By ignoring the null values, i need to replace each first occurrence of the numeric value (from 0 to 10) with 1 and the rest of the values with -1 for all 10 rows and 400 columns ignoring the ID column. The resulting dataframe will look like: I will be thankful for some help here. "
            },
            "io": [
                "       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100\n   \n        1,         ,   ,   ,   ,  3,  3,  3,  3,   ,   ,   ,   ,.........,  0,    0,   0,   0   \n        2,        0,  0,  0,  0,   ,   ,   ,   , 10, 10, 10, 10,.........,   ,     ,    ,   \n        3,        9,  9,  9,  9,  1,  1,  1,  1,  4,  4,  4,  4,.........,  1,    1,   1,   1\n        .\n        .\n        .\n       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 6,    6,   6,   6\n",
                "       ID#,       1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,.........,100,  100, 100, 100\n   \n        1,         ,   ,   ,   ,  1, -1, -1, -1,   ,   ,   ,   ,.........,  1,   -1,   -1, -1   \n        2,        1, -1, -1, -1,   ,   ,   ,   ,  1, -1, -1, -1,.........,   ,     ,     ,   \n        3,        1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,.........,  1,   -1,   -1, -1\n        .\n        .\n        .\n       10,         ,   ,   ,   ,   ,   ,    ,  ,   ,   ,    ,   ,........., 1,   -1,   -1, -1\n"
            ],
            "answer": {
                "ans_desc": "First, some example data: I recommend transposing the DataFrame, as it's more convenient to use the vectorized methods from pandas. Most of them can be used \"horizontally\" with specifying the . First you need to know all the cells where there are values: Secondly, you need to know all the starting positions of a new group. Shifting the whole DataFrame down by one row and checking for unequality helps. Combining that with your gives you the starting cells: Now you can set all the cells of value to and afterwards all the cells that are a the start of a group to Now you can always transpose it back: ",
                "code": [
                    "import pandas as pd\n\nfrom io import StringIO\n\ndf_string = '''\nID;1;1;1;1;2;2;2;2;3;3;3;3\n1;;;;;3;3;3;3;;;;\n2;0;0;0;0;;;;;10;10;10;10\n3;9;9;9;9;1;1;1;1;4;4;4;4\n4;;;;;;;;;6;6;6;6\n'''\n\ndf = pd.read_csv(StringIO(df_string), sep = \";\", index_col=\"ID\")\n\n# Removing the automatically added .1/.2/... suffixes. You don't need that for your data.\ndf.columns = df.columns.str[0]\n",
                    "StartMask = (df.shift() != df) & ValueMask\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 41,
            "user_id": 14613864,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d1796dd6ff1b9d71e29f13a8e7513c87?s=128&d=identicon&r=PG&f=1",
            "display_name": "Martynelius",
            "link": "https://stackoverflow.com/users/14613864/martynelius"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 64773223,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1605028495,
        "creation_date": 1605026439,
        "question_id": 64773001,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64773001/split-two-columns-in-a-pandas-dataframe-into-two-and-name-them",
        "title": "Split two columns in a pandas dataframe into two and name them",
        "body": "<p>I have this pandas dataframe</p>\n<pre><code>         x           y       Values\n0       A B         C D       4.7\n1       A B         C D       10.9\n2       A B         C D       1.8\n3       A B         C D       6.5\n4       A B         C D       3.4\n</code></pre>\n<p>I would like to split the x and y columns and get an output with these given names on the columns.</p>\n<pre><code>     x    f    y    g   Values\n0    A    B    C    D    4.7\n1    A    B    C    D    10.9\n2    A    B    C    D    1.8\n3    A    B    C    D    6.5\n4    A    B    C    D    3.4\n</code></pre>\n<p>Is there a straight forward way to do this in python?</p>\n",
        "answer_body": "<pre><code>df[['x', 'f']] = df.x.str.split(&quot; &quot;, expand=True)\ndf[['y', 'g']] = df.y.str.split(&quot; &quot;, expand=True)\ndf[['x','f','y','g', 'Values']]\n</code></pre>\n<p>You can make it scalable as well, defining a dict in which the keys are the columns, and the values a list with the desired new column names:</p>\n<pre><code># Define the target columns to split, and their new column names\ncols={\n    'x': ['x','f'],\n    'y': ['y','g']\n}\n# Apply the function to each target-column\nfor k in cols:\n    df[cols[k]] = df[k].str.split(&quot; &quot;, expand=True)\n\n# Reorder the dataframe as you wish\nnew_columns = sum(cols.values(),[])\nold_columns = set(df.columns) - set(new_columns)\ndf[new_columns + list(old_columns)]\n</code></pre>\n",
        "question_body": "<p>I have this pandas dataframe</p>\n<pre><code>         x           y       Values\n0       A B         C D       4.7\n1       A B         C D       10.9\n2       A B         C D       1.8\n3       A B         C D       6.5\n4       A B         C D       3.4\n</code></pre>\n<p>I would like to split the x and y columns and get an output with these given names on the columns.</p>\n<pre><code>     x    f    y    g   Values\n0    A    B    C    D    4.7\n1    A    B    C    D    10.9\n2    A    B    C    D    1.8\n3    A    B    C    D    6.5\n4    A    B    C    D    3.4\n</code></pre>\n<p>Is there a straight forward way to do this in python?</p>\n",
        "formatted_input": {
            "qid": 64773001,
            "link": "https://stackoverflow.com/questions/64773001/split-two-columns-in-a-pandas-dataframe-into-two-and-name-them",
            "question": {
                "title": "Split two columns in a pandas dataframe into two and name them",
                "ques_desc": "I have this pandas dataframe I would like to split the x and y columns and get an output with these given names on the columns. Is there a straight forward way to do this in python? "
            },
            "io": [
                "         x           y       Values\n0       A B         C D       4.7\n1       A B         C D       10.9\n2       A B         C D       1.8\n3       A B         C D       6.5\n4       A B         C D       3.4\n",
                "     x    f    y    g   Values\n0    A    B    C    D    4.7\n1    A    B    C    D    10.9\n2    A    B    C    D    1.8\n3    A    B    C    D    6.5\n4    A    B    C    D    3.4\n"
            ],
            "answer": {
                "ans_desc": " You can make it scalable as well, defining a dict in which the keys are the columns, and the values a list with the desired new column names: ",
                "code": [
                    "df[['x', 'f']] = df.x.str.split(\" \", expand=True)\ndf[['y', 'g']] = df.y.str.split(\" \", expand=True)\ndf[['x','f','y','g', 'Values']]\n",
                    "# Define the target columns to split, and their new column names\ncols={\n    'x': ['x','f'],\n    'y': ['y','g']\n}\n# Apply the function to each target-column\nfor k in cols:\n    df[cols[k]] = df[k].str.split(\" \", expand=True)\n\n# Reorder the dataframe as you wish\nnew_columns = sum(cols.values(),[])\nold_columns = set(df.columns) - set(new_columns)\ndf[new_columns + list(old_columns)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 79,
            "user_id": 9670181,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5dc6d1b4a4162c4d9361a2afb1cf8920?s=128&d=identicon&r=PG&f=1",
            "display_name": "fendrbud",
            "link": "https://stackoverflow.com/users/9670181/fendrbud"
        },
        "is_answered": true,
        "view_count": 30,
        "closed_date": 1605002661,
        "accepted_answer_id": 64766518,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1605001861,
        "creation_date": 1605001219,
        "question_id": 64766331,
        "link": "https://stackoverflow.com/questions/64766331/pandas-resample-column-based-on-other-column",
        "closed_reason": "Duplicate",
        "title": "Pandas resample column based on other column",
        "body": "<p>I have a similar dataframe:</p>\n<pre><code>x | y\n1 | 1\n3 | 1\n3 | 1\n4 | 1\n5 | 2\n5 | 2\n9 | 2\n8 | 2\n</code></pre>\n<p>And I want to resample this dataframe such that x values with the same y value is averaged. In other words:</p>\n<pre><code>     x       |    y\n(1+3+3+4)/4  |    1\n(5+5+9+8)/4  |    2\n</code></pre>\n<p>I've looked into the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\" rel=\"nofollow noreferrer\">pandas.DataFrame.resample</a> function, but not sure how to do this without timestamps.</p>\n",
        "answer_body": "<p>The following might return what you're looking for:</p>\n<pre><code>import pandas\ndf = pandas.DataFrame({&quot;x&quot;:[1,3,3,4,5,5,9,8],&quot;y&quot;:[1,1,1,1,2,2,2,2]})\ndf.groupby([&quot;y&quot;]).mean().reset_index()\n</code></pre>\n",
        "question_body": "<p>I have a similar dataframe:</p>\n<pre><code>x | y\n1 | 1\n3 | 1\n3 | 1\n4 | 1\n5 | 2\n5 | 2\n9 | 2\n8 | 2\n</code></pre>\n<p>And I want to resample this dataframe such that x values with the same y value is averaged. In other words:</p>\n<pre><code>     x       |    y\n(1+3+3+4)/4  |    1\n(5+5+9+8)/4  |    2\n</code></pre>\n<p>I've looked into the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\" rel=\"nofollow noreferrer\">pandas.DataFrame.resample</a> function, but not sure how to do this without timestamps.</p>\n",
        "formatted_input": {
            "qid": 64766331,
            "link": "https://stackoverflow.com/questions/64766331/pandas-resample-column-based-on-other-column",
            "question": {
                "title": "Pandas resample column based on other column",
                "ques_desc": "I have a similar dataframe: And I want to resample this dataframe such that x values with the same y value is averaged. In other words: I've looked into the pandas.DataFrame.resample function, but not sure how to do this without timestamps. "
            },
            "io": [
                "x | y\n1 | 1\n3 | 1\n3 | 1\n4 | 1\n5 | 2\n5 | 2\n9 | 2\n8 | 2\n",
                "     x       |    y\n(1+3+3+4)/4  |    1\n(5+5+9+8)/4  |    2\n"
            ],
            "answer": {
                "ans_desc": "The following might return what you're looking for: ",
                "code": [
                    "import pandas\ndf = pandas.DataFrame({\"x\":[1,3,3,4,5,5,9,8],\"y\":[1,1,1,1,2,2,2,2]})\ndf.groupby([\"y\"]).mean().reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 115,
            "user_id": 10819640,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/56e4c23202e990d869cf7d47f1645748?s=128&d=identicon&r=PG",
            "display_name": "lazy_frog",
            "link": "https://stackoverflow.com/users/10819640/lazy-frog"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 64765410,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1605000010,
        "creation_date": 1604997082,
        "question_id": 64765350,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64765350/pandas-dataframe-select-list-value-from-another-column",
        "title": "pandas dataframe select list value from another column",
        "body": "<p>Everyone! I have a pandas dataframe like this:</p>\n<pre><code>        A       B\n   0    [1,2,3] 0\n   1    [2,3,4] 1\n</code></pre>\n<p>as we can see, the A column is a list and the B column is an index value. I want to get a C column which is index by B from A:</p>\n<pre><code>        A       B     C\n   0    [1,2,3] 0     1\n   1    [2,3,4] 1     3\n</code></pre>\n<p>Is there any elegant method to solve this? Thank you!</p>\n",
        "answer_body": "<p>Use list comprehension with indexing:</p>\n<pre><code>df['C'] = [x[y] for x, y in df[['A','B']].to_numpy()]\n</code></pre>\n<p>Or <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\" rel=\"nofollow noreferrer\"><code>DataFrame.apply</code></a>, but it should be slowier if large DataFrame:</p>\n<pre><code>df['C'] = df.apply(lambda x: x.A[x.B], axis=1)\n</code></pre>\n<hr />\n<pre><code>print (df)\n           A  B  C\n0  [1, 2, 3]  0  1\n1  [2, 3, 4]  1  3\n</code></pre>\n",
        "question_body": "<p>Everyone! I have a pandas dataframe like this:</p>\n<pre><code>        A       B\n   0    [1,2,3] 0\n   1    [2,3,4] 1\n</code></pre>\n<p>as we can see, the A column is a list and the B column is an index value. I want to get a C column which is index by B from A:</p>\n<pre><code>        A       B     C\n   0    [1,2,3] 0     1\n   1    [2,3,4] 1     3\n</code></pre>\n<p>Is there any elegant method to solve this? Thank you!</p>\n",
        "formatted_input": {
            "qid": 64765350,
            "link": "https://stackoverflow.com/questions/64765350/pandas-dataframe-select-list-value-from-another-column",
            "question": {
                "title": "pandas dataframe select list value from another column",
                "ques_desc": "Everyone! I have a pandas dataframe like this: as we can see, the A column is a list and the B column is an index value. I want to get a C column which is index by B from A: Is there any elegant method to solve this? Thank you! "
            },
            "io": [
                "        A       B\n   0    [1,2,3] 0\n   1    [2,3,4] 1\n",
                "        A       B     C\n   0    [1,2,3] 0     1\n   1    [2,3,4] 1     3\n"
            ],
            "answer": {
                "ans_desc": "Use list comprehension with indexing: Or , but it should be slowier if large DataFrame: ",
                "code": [
                    "df['C'] = [x[y] for x, y in df[['A','B']].to_numpy()]\n",
                    "df['C'] = df.apply(lambda x: x.A[x.B], axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 5144,
            "user_id": 1194864,
            "user_type": "registered",
            "accept_rate": 89,
            "profile_image": "https://i.stack.imgur.com/uley8.jpg?s=128&g=1",
            "display_name": "Jose Ramon",
            "link": "https://stackoverflow.com/users/1194864/jose-ramon"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 64753750,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1604933797,
        "creation_date": 1604932138,
        "last_edit_date": 1604933797,
        "question_id": 64753531,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64753531/process-multiple-csv-files-on-pandas",
        "title": "Process multiple csv files on pandas",
        "body": "<p>I have got three different .csv files which contain the grades for students in three different assignment. I would like to read them with pandas and calculate the average for each student. The template for each file is:</p>\n<pre><code>Student id, Mark, extra fields, ...\n4358975489, 9,  ... ...\n2345234523, 10,  ... ...\n7634565323, 7,  ... ...\n7653563366, 7,  ... ...\n...         ...,  ... ...\n</code></pre>\n<p>For the second assignment:</p>\n<pre><code>Student id, Mark, extra fields, ...\n4358975489, 6,  ... ...\n2345234523, 8,  ... ...\n7634565323, 4,  ... ...\n7653563366, 5,  ... ...\n...         ...,  ... ...\n</code></pre>\n<p>Desired output for the two doc for instance:</p>\n<pre><code>Student id, average, extra fields, ...\n4358975489, 7.5,  ... ...\n2345234523, 9,  ... ...\n7634565323, 5.5,  ... ...\n7653563366, 6,  ... ...\n...         ...,  ... ...\n</code></pre>\n<p>the same for the last doc. I want to read these docs separately and for each student id to average the Mark.</p>\n<p>Now, my code for reading one file is the following:</p>\n<pre><code>i_df1 = pandas.read_csv('first.csv')\ni_df2 = pandas.read_csv('second.csv')\ni_df3 = pandas.read_csv('third.csv')\n\nprint (o_df.keys())\nfor i, row in i_df1.iterrows():\n    pdb.set_trace()\n</code></pre>\n<p>How can I process all three files simultaneously and extract the average grade?</p>\n",
        "answer_body": "<p>If you want to process all <code>dfs</code> together, you can do this:</p>\n<pre><code>df = df1.append([df2, df3]).groupby('Student id', as_index=False).mean()\n</code></pre>\n<p><strong>OR:</strong></p>\n<p>If you want to do it simultaneously, you can use <code>list comprehension</code> with <code>df.append</code> and <code>mean</code>:</p>\n<p>Below is the <code>list</code> of your <code>dfs</code>:</p>\n<pre><code>In [1220]: df_list = [i_df1, i_df2, i_df3]\n</code></pre>\n<p>You can simultaneously find average of each student in a file and store the output in another <code>list</code>:</p>\n<pre><code>In [1223]: df = [i.groupby('Student_id', as_index=False).mean() for i in df_list]\n</code></pre>\n",
        "question_body": "<p>I have got three different .csv files which contain the grades for students in three different assignment. I would like to read them with pandas and calculate the average for each student. The template for each file is:</p>\n<pre><code>Student id, Mark, extra fields, ...\n4358975489, 9,  ... ...\n2345234523, 10,  ... ...\n7634565323, 7,  ... ...\n7653563366, 7,  ... ...\n...         ...,  ... ...\n</code></pre>\n<p>For the second assignment:</p>\n<pre><code>Student id, Mark, extra fields, ...\n4358975489, 6,  ... ...\n2345234523, 8,  ... ...\n7634565323, 4,  ... ...\n7653563366, 5,  ... ...\n...         ...,  ... ...\n</code></pre>\n<p>Desired output for the two doc for instance:</p>\n<pre><code>Student id, average, extra fields, ...\n4358975489, 7.5,  ... ...\n2345234523, 9,  ... ...\n7634565323, 5.5,  ... ...\n7653563366, 6,  ... ...\n...         ...,  ... ...\n</code></pre>\n<p>the same for the last doc. I want to read these docs separately and for each student id to average the Mark.</p>\n<p>Now, my code for reading one file is the following:</p>\n<pre><code>i_df1 = pandas.read_csv('first.csv')\ni_df2 = pandas.read_csv('second.csv')\ni_df3 = pandas.read_csv('third.csv')\n\nprint (o_df.keys())\nfor i, row in i_df1.iterrows():\n    pdb.set_trace()\n</code></pre>\n<p>How can I process all three files simultaneously and extract the average grade?</p>\n",
        "formatted_input": {
            "qid": 64753531,
            "link": "https://stackoverflow.com/questions/64753531/process-multiple-csv-files-on-pandas",
            "question": {
                "title": "Process multiple csv files on pandas",
                "ques_desc": "I have got three different .csv files which contain the grades for students in three different assignment. I would like to read them with pandas and calculate the average for each student. The template for each file is: For the second assignment: Desired output for the two doc for instance: the same for the last doc. I want to read these docs separately and for each student id to average the Mark. Now, my code for reading one file is the following: How can I process all three files simultaneously and extract the average grade? "
            },
            "io": [
                "Student id, Mark, extra fields, ...\n4358975489, 9,  ... ...\n2345234523, 10,  ... ...\n7634565323, 7,  ... ...\n7653563366, 7,  ... ...\n...         ...,  ... ...\n",
                "Student id, Mark, extra fields, ...\n4358975489, 6,  ... ...\n2345234523, 8,  ... ...\n7634565323, 4,  ... ...\n7653563366, 5,  ... ...\n...         ...,  ... ...\n"
            ],
            "answer": {
                "ans_desc": "If you want to process all together, you can do this: OR: If you want to do it simultaneously, you can use with and : Below is the of your : You can simultaneously find average of each student in a file and store the output in another : ",
                "code": [
                    "df = df1.append([df2, df3]).groupby('Student id', as_index=False).mean()\n",
                    "In [1223]: df = [i.groupby('Student_id', as_index=False).mean() for i in df_list]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 537,
            "user_id": 12844452,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f69ae695fbf53f2a10b174a85f418145?s=128&d=identicon&r=PG&f=1",
            "display_name": "Jeroen",
            "link": "https://stackoverflow.com/users/12844452/jeroen"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 64747387,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1604924175,
        "creation_date": 1604906944,
        "last_edit_date": 1604924175,
        "question_id": 64747375,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64747375/pandas-remove-characters-from-index",
        "title": "Pandas remove characters from index",
        "body": "<p>I have the following dataframe:</p>\n<pre><code>           A\n0-1.5      1\n1.5-3.3    2\n3.3-5.4    3\n5.4-7.9    4\n</code></pre>\n<p>I want to remove the '-' character with the upper value in the index so I end up with the following dataframe:</p>\n<pre><code>     A\n0    1\n1.5  2\n3.3  3\n5.4  4\n</code></pre>\n<p>How do I do this?</p>\n",
        "answer_body": "<p>You can use <code>split</code> with seelct first lists by indexing:</p>\n<pre><code>df.index = df.index.str.split('-').str[0]\n</code></pre>\n<p>Or use <code>rename</code> with lambda function:</p>\n<pre><code>df = df.rename(lambda x: x.split('-')[0])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe:</p>\n<pre><code>           A\n0-1.5      1\n1.5-3.3    2\n3.3-5.4    3\n5.4-7.9    4\n</code></pre>\n<p>I want to remove the '-' character with the upper value in the index so I end up with the following dataframe:</p>\n<pre><code>     A\n0    1\n1.5  2\n3.3  3\n5.4  4\n</code></pre>\n<p>How do I do this?</p>\n",
        "formatted_input": {
            "qid": 64747375,
            "link": "https://stackoverflow.com/questions/64747375/pandas-remove-characters-from-index",
            "question": {
                "title": "Pandas remove characters from index",
                "ques_desc": "I have the following dataframe: I want to remove the '-' character with the upper value in the index so I end up with the following dataframe: How do I do this? "
            },
            "io": [
                "           A\n0-1.5      1\n1.5-3.3    2\n3.3-5.4    3\n5.4-7.9    4\n",
                "     A\n0    1\n1.5  2\n3.3  3\n5.4  4\n"
            ],
            "answer": {
                "ans_desc": "You can use with seelct first lists by indexing: Or use with lambda function: ",
                "code": [
                    "df = df.rename(lambda x: x.split('-')[0])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 17017,
            "user_id": 466534,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/966493b7357dc065e054f17c10821420?s=128&d=identicon&r=PG&f=1",
            "display_name": "dato datuashvili",
            "link": "https://stackoverflow.com/users/466534/dato-datuashvili"
        },
        "is_answered": true,
        "view_count": 23,
        "accepted_answer_id": 64723013,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1604706657,
        "creation_date": 1604705887,
        "question_id": 64722958,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64722958/concatenation-of-two-dataframe-after-onehotencoding",
        "title": "Concatenation of two dataframe after onehotencoding",
        "body": "<p>Let us consider following code</p>\n<pre><code>import pandas  as pd\nimport numpy as np\nNames =pd.Series([&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;,&quot;C&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;],name=&quot;Alphabet&quot;)\nStrings =pd.DataFrame(Names)\nprint(Strings.head(12))\nfrom sklearn.preprocessing import OneHotEncoder\nMyEncoder=OneHotEncoder(sparse=False)\nencoded =MyEncoder.fit_transform(Strings[[&quot;Alphabet&quot;]])\nprint(encoded)\nprint(MyEncoder.categories_)\nEncoded_Dataframe =pd.DataFrame(encoded)\nEncoded_Dataframe.columns =list(MyEncoder.categories_)\nprint(Encoded_Dataframe.head())\nStrings =pd.concat([Strings,Encoded_Dataframe],axis=1)\nprint(Strings.head())\n</code></pre>\n<p>result of this code is following ( i am writing final dataframe)</p>\n<pre><code> Alphabet  (A,)  (B,)  (C,)\n0        A   1.0   0.0   0.0\n1        B   0.0   1.0   0.0\n2        C   0.0   0.0   1.0\n3        A   1.0   0.0   0.0\n4        B   0.0   1.0   0.0\n</code></pre>\n<p>all others works fine, they are</p>\n<pre><code>   A    B    C\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n4  0.0  1.0  0.0\n</code></pre>\n<p>so my  point is to remove commas in  header part of the final dataframe, please help me</p>\n",
        "answer_body": "<p>Try adding:</p>\n<pre><code>Encoded_Dataframe.columns = [''.join(col) for col in Encoded_Dataframe.columns]\n</code></pre>\n<p>Your columns are a multi-index in <code>Encoded_Dataframe</code>. If you write: <code>Encoded_Dataframe.columns</code>, you will get <code>MultiIndex([('A',),('B',),('C',)],)</code> and see what I mean, so you need to change the column names with what I have suggested, by joining the multiple levels with an empty string, so that you have one string as a column name instead of a tuple.</p>\n<p>Also, you could use <code>Strings = Strings.join(Encoded_Dataframe)</code> as an alternative to <code>Strings = pd.concat([Strings,Encoded_Dataframe],axis=1)</code>:</p>\n<p>Full Code:</p>\n<pre><code>import pandas  as pd\nimport numpy as np\nNames =pd.Series([&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;,&quot;C&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;],name=&quot;Alphabet&quot;)\nStrings =pd.DataFrame(Names)\nprint(Strings.head(12))\nfrom sklearn.preprocessing import OneHotEncoder\nMyEncoder=OneHotEncoder(sparse=False)\nencoded =MyEncoder.fit_transform(Strings[[&quot;Alphabet&quot;]])\nprint(encoded)\nprint(MyEncoder.categories_)\nEncoded_Dataframe =pd.DataFrame(encoded)\nEncoded_Dataframe.columns =list(MyEncoder.categories_)\nprint(Encoded_Dataframe.head())\nEncoded_Dataframe.columns = [''.join(col) for col in Encoded_Dataframe.columns]\nStrings =pd.concat([Strings,Encoded_Dataframe],axis=1)\nprint(Strings.head())\n   Alphabet\n0         A\n1         B\n2         C\n3         A\n4         B\n5         B\n6         A\n7         C\n8         C\n9         B\n10        A\n11        C\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\n[array(['A', 'B', 'C'], dtype=object)]\n     A    B    C\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n4  0.0  1.0  0.0\n  Alphabet    A    B    C\n0        A  1.0  0.0  0.0\n1        B  0.0  1.0  0.0\n2        C  0.0  0.0  1.0\n3        A  1.0  0.0  0.0\n4        B  0.0  1.0  0.0\n</code></pre>\n",
        "question_body": "<p>Let us consider following code</p>\n<pre><code>import pandas  as pd\nimport numpy as np\nNames =pd.Series([&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;,&quot;C&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;],name=&quot;Alphabet&quot;)\nStrings =pd.DataFrame(Names)\nprint(Strings.head(12))\nfrom sklearn.preprocessing import OneHotEncoder\nMyEncoder=OneHotEncoder(sparse=False)\nencoded =MyEncoder.fit_transform(Strings[[&quot;Alphabet&quot;]])\nprint(encoded)\nprint(MyEncoder.categories_)\nEncoded_Dataframe =pd.DataFrame(encoded)\nEncoded_Dataframe.columns =list(MyEncoder.categories_)\nprint(Encoded_Dataframe.head())\nStrings =pd.concat([Strings,Encoded_Dataframe],axis=1)\nprint(Strings.head())\n</code></pre>\n<p>result of this code is following ( i am writing final dataframe)</p>\n<pre><code> Alphabet  (A,)  (B,)  (C,)\n0        A   1.0   0.0   0.0\n1        B   0.0   1.0   0.0\n2        C   0.0   0.0   1.0\n3        A   1.0   0.0   0.0\n4        B   0.0   1.0   0.0\n</code></pre>\n<p>all others works fine, they are</p>\n<pre><code>   A    B    C\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n4  0.0  1.0  0.0\n</code></pre>\n<p>so my  point is to remove commas in  header part of the final dataframe, please help me</p>\n",
        "formatted_input": {
            "qid": 64722958,
            "link": "https://stackoverflow.com/questions/64722958/concatenation-of-two-dataframe-after-onehotencoding",
            "question": {
                "title": "Concatenation of two dataframe after onehotencoding",
                "ques_desc": "Let us consider following code result of this code is following ( i am writing final dataframe) all others works fine, they are so my point is to remove commas in header part of the final dataframe, please help me "
            },
            "io": [
                " Alphabet  (A,)  (B,)  (C,)\n0        A   1.0   0.0   0.0\n1        B   0.0   1.0   0.0\n2        C   0.0   0.0   1.0\n3        A   1.0   0.0   0.0\n4        B   0.0   1.0   0.0\n",
                "   A    B    C\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n4  0.0  1.0  0.0\n"
            ],
            "answer": {
                "ans_desc": "Try adding: Your columns are a multi-index in . If you write: , you will get and see what I mean, so you need to change the column names with what I have suggested, by joining the multiple levels with an empty string, so that you have one string as a column name instead of a tuple. Also, you could use as an alternative to : Full Code: ",
                "code": [
                    "Encoded_Dataframe.columns = [''.join(col) for col in Encoded_Dataframe.columns]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 867,
            "user_id": 3719940,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/bd2798bbb5bd797bd4eaafe8c1ea994c?s=128&d=identicon&r=PG&f=1",
            "display_name": "Rahul",
            "link": "https://stackoverflow.com/users/3719940/rahul"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 64715980,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1604673806,
        "creation_date": 1604670652,
        "question_id": 64715870,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64715870/max-for-dataframe-converts-object-type-to-float64",
        "title": ".max() for dataframe converts object type to float64",
        "body": "<p>Have 3 columns namely</p>\n<pre><code>   &quot;A&quot;,         &quot;B&quot;\n -7.480000e+01,-1.480000e+01\n-7.410000e+01,-1.410000e+01\n-7.370000e+01,-1.370000e+01\n-7.360000e+01,-1.360000e+01\n-7.370000e+01,-1.370000e+01\n-7.390000e+01,-1.390000e+01\n</code></pre>\n<p>Whenever I do <code>df[[&quot;A&quot;,&quot;B&quot;]].max(axis=1)</code>, it ends up giving a float value. Something like\nOutput:</p>\n<pre><code>   &quot;C&quot;     \n 7.12\n9.12\n1.1231\n6.1231\n1.123\n8.421\n</code></pre>\n<p>I wanted</p>\n<pre><code>       &quot;C&quot;     \n  -7.480000e+01,\n    -7.410000e+01\n    -7.370000e+01,\n    -7.360000e+01,\n    -7.370000e+01\n    -7.390000e+01\n</code></pre>\n<p>Tried using <code>.astype(str)</code> but no luck.</p>\n",
        "answer_body": "<pre class=\"lang-py prettyprint-override\"><code>df.apply(lambda x: '{:e}'.format(max(x['A'], x['B'])), axis=1)\n</code></pre>\n<p>If values of 'A' and 'B' are strings:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.apply(lambda x: '{:e}'.format(max(float(x['A']), float(x['B']))), axis=1)\n</code></pre>\n",
        "question_body": "<p>Have 3 columns namely</p>\n<pre><code>   &quot;A&quot;,         &quot;B&quot;\n -7.480000e+01,-1.480000e+01\n-7.410000e+01,-1.410000e+01\n-7.370000e+01,-1.370000e+01\n-7.360000e+01,-1.360000e+01\n-7.370000e+01,-1.370000e+01\n-7.390000e+01,-1.390000e+01\n</code></pre>\n<p>Whenever I do <code>df[[&quot;A&quot;,&quot;B&quot;]].max(axis=1)</code>, it ends up giving a float value. Something like\nOutput:</p>\n<pre><code>   &quot;C&quot;     \n 7.12\n9.12\n1.1231\n6.1231\n1.123\n8.421\n</code></pre>\n<p>I wanted</p>\n<pre><code>       &quot;C&quot;     \n  -7.480000e+01,\n    -7.410000e+01\n    -7.370000e+01,\n    -7.360000e+01,\n    -7.370000e+01\n    -7.390000e+01\n</code></pre>\n<p>Tried using <code>.astype(str)</code> but no luck.</p>\n",
        "formatted_input": {
            "qid": 64715870,
            "link": "https://stackoverflow.com/questions/64715870/max-for-dataframe-converts-object-type-to-float64",
            "question": {
                "title": ".max() for dataframe converts object type to float64",
                "ques_desc": "Have 3 columns namely Whenever I do , it ends up giving a float value. Something like Output: I wanted Tried using but no luck. "
            },
            "io": [
                "   \"A\",         \"B\"\n -7.480000e+01,-1.480000e+01\n-7.410000e+01,-1.410000e+01\n-7.370000e+01,-1.370000e+01\n-7.360000e+01,-1.360000e+01\n-7.370000e+01,-1.370000e+01\n-7.390000e+01,-1.390000e+01\n",
                "       \"C\"     \n  -7.480000e+01,\n    -7.410000e+01\n    -7.370000e+01,\n    -7.360000e+01,\n    -7.370000e+01\n    -7.390000e+01\n"
            ],
            "answer": {
                "ans_desc": " If values of 'A' and 'B' are strings: ",
                "code": [
                    "df.apply(lambda x: '{:e}'.format(max(x['A'], x['B'])), axis=1)\n",
                    "df.apply(lambda x: '{:e}'.format(max(float(x['A']), float(x['B']))), axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "datetime",
            "dataframe",
            "series"
        ],
        "owner": {
            "reputation": 478,
            "user_id": 9423134,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b51721129833d356415143a02bc65e76?s=128&d=identicon&r=PG&f=1",
            "display_name": "Oumab10",
            "link": "https://stackoverflow.com/users/9423134/oumab10"
        },
        "is_answered": true,
        "view_count": 12612,
        "accepted_answer_id": 50486911,
        "answer_count": 3,
        "score": 12,
        "last_activity_date": 1604577843,
        "creation_date": 1526464352,
        "last_edit_date": 1531481526,
        "question_id": 50367656,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50367656/python-pandas-pandas-to-datetime-is-switching-day-month-when-day-is-less-t",
        "title": "Python Pandas : pandas.to_datetime() is switching day &amp; month when day is less than 13",
        "body": "<p>I wrote a code that reads multiple files, however on some of my files datetime swaps day &amp; month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). \nI tried to fix it by doing this,but it doesn't work.</p>\n\n<p>My data frame looks like this:\nThe actual datetime is from 12june2015 to 13june2015\nwhen my I read my datetime column as a string the dates remain correct dd/mm/yyyy</p>\n\n<pre><code>tmp                     p1 p2 \n11/06/2015 00:56:55.060  0  1\n11/06/2015 04:16:38.060  0  1\n12/06/2015 16:13:30.060  0  1\n12/06/2015 21:24:03.060  0  1\n13/06/2015 02:31:44.060  0  1\n13/06/2015 02:37:49.060  0  1\n</code></pre>\n\n<p>but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13.</p>\n\n<p>output:</p>\n\n<pre><code>print(df)\ntmp                  p1 p2 \n06/11/2015 00:56:55  0  1\n06/11/2015 04:16:38  0  1\n06/12/2015 16:13:30  0  1\n06/12/2015 21:24:03  0  1\n13/06/2015 02:31:44  0  1\n13/06/2015 02:37:49  0  1\n</code></pre>\n\n<p><strong>Here is my code :</strong></p>\n\n<p>I loop through files :</p>\n\n<pre><code>df = pd.read_csv(PATH+file, header = None,error_bad_lines=False , sep = '\\t')\n</code></pre>\n\n<p>then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type  by pd_datetime() it swaps the day and month when the day is less than 13.</p>\n\n<p>Post converting my datetime column the dates are correct (string type)</p>\n\n<pre><code>print(tmp) # as a result I get 11.06.2015 12:56:05 (11june2015)\n</code></pre>\n\n<p>But when I change the column type I get this:</p>\n\n<pre><code>tmp = pd.to_datetime(tmp, unit = \"ns\")\ntmp = temps_absolu.apply(lambda x: x.replace(microsecond=0))\nprint(tmp) # I get 06-11-2016 12:56:05 (06november2015 its not the right date)\n</code></pre>\n\n<p>The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13?</p>\n\n<p><strong>UPDATE</strong>\nThis command swaps all the days and months of my column</p>\n\n<pre><code>tmp =  pd.to_datetime(tmp, unit='s').dt.strftime('%#m/%#d/%Y %H:%M:%S') \n</code></pre>\n\n<p>So in order to swap only the incorrect dates, I wrote a condition:</p>\n\n<pre><code>for t in tmp:\n        if (t.day &lt; 13):\n            t = datetime(year=t.year, month=t.day, day=t.month, hour=t.hour, minute=t.minute, second = t.second)\n</code></pre>\n\n<p>But it doesn't work either</p>\n",
        "answer_body": "<p>Well I solved my problem but in a memory consuming method, I split my tmp column first to a date and time columns then I re-split my date column to day month and year, that way I could look for the days that are less than 13 and replace them with the correspondent month</p>\n\n<pre><code>df['tmp'] = pd.to_datetime(df['tmp'], unit='ns')\ndf['tmp'] = df['tmp'].apply(lambda x: x.replace(microsecond=0))\ndf['date'] = [d.date() for d in df['tmp']]\ndf['time'] = [d.time() for d in df['tmp']]\ndf[['year','month','day']] = df['date'].apply(lambda x: pd.Series(x.strftime(\"%Y-%m-%d\").split(\"-\")))\n\ndf['day'] = pd.to_numeric(df['day'], errors='coerce')\ndf['month'] = pd.to_numeric(df['month'], errors='coerce')\ndf['year'] = pd.to_numeric(df['year'], errors='coerce')\n\n\n#Loop to look for days less than 13 and then swap the day and month\nfor index, d in enumerate(df['day']):\n        if(d &lt;13): \n df.loc[index,'day'],df.loc[index,'month']=df.loc[index,'month'],df.loc[index,'day'] \n</code></pre>\n\n<p># convert series to string type in order to merge them</p>\n\n<pre><code> df['day'] = df['day'].astype(str)\n df['month'] = df['month'].astype(str)\n df['year'] = df['year'].astype(str)\n df['date']=  pd.to_datetime(df[['year', 'month', 'day']])\n df['date'] = df['date'].astype(str)\n df['time'] = df['time'].astype(str)\n</code></pre>\n\n<p># merge time and date and place result in our column</p>\n\n<pre><code>df['tmp'] =pd.to_datetime(df['date']+ ' '+df['time'])\n</code></pre>\n\n<p># drop the added columns</p>\n\n<pre><code>df.drop(df[['date','year', 'month', 'day','time']], axis=1, inplace = True)\n</code></pre>\n",
        "question_body": "<p>I wrote a code that reads multiple files, however on some of my files datetime swaps day &amp; month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). \nI tried to fix it by doing this,but it doesn't work.</p>\n\n<p>My data frame looks like this:\nThe actual datetime is from 12june2015 to 13june2015\nwhen my I read my datetime column as a string the dates remain correct dd/mm/yyyy</p>\n\n<pre><code>tmp                     p1 p2 \n11/06/2015 00:56:55.060  0  1\n11/06/2015 04:16:38.060  0  1\n12/06/2015 16:13:30.060  0  1\n12/06/2015 21:24:03.060  0  1\n13/06/2015 02:31:44.060  0  1\n13/06/2015 02:37:49.060  0  1\n</code></pre>\n\n<p>but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13.</p>\n\n<p>output:</p>\n\n<pre><code>print(df)\ntmp                  p1 p2 \n06/11/2015 00:56:55  0  1\n06/11/2015 04:16:38  0  1\n06/12/2015 16:13:30  0  1\n06/12/2015 21:24:03  0  1\n13/06/2015 02:31:44  0  1\n13/06/2015 02:37:49  0  1\n</code></pre>\n\n<p><strong>Here is my code :</strong></p>\n\n<p>I loop through files :</p>\n\n<pre><code>df = pd.read_csv(PATH+file, header = None,error_bad_lines=False , sep = '\\t')\n</code></pre>\n\n<p>then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type  by pd_datetime() it swaps the day and month when the day is less than 13.</p>\n\n<p>Post converting my datetime column the dates are correct (string type)</p>\n\n<pre><code>print(tmp) # as a result I get 11.06.2015 12:56:05 (11june2015)\n</code></pre>\n\n<p>But when I change the column type I get this:</p>\n\n<pre><code>tmp = pd.to_datetime(tmp, unit = \"ns\")\ntmp = temps_absolu.apply(lambda x: x.replace(microsecond=0))\nprint(tmp) # I get 06-11-2016 12:56:05 (06november2015 its not the right date)\n</code></pre>\n\n<p>The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13?</p>\n\n<p><strong>UPDATE</strong>\nThis command swaps all the days and months of my column</p>\n\n<pre><code>tmp =  pd.to_datetime(tmp, unit='s').dt.strftime('%#m/%#d/%Y %H:%M:%S') \n</code></pre>\n\n<p>So in order to swap only the incorrect dates, I wrote a condition:</p>\n\n<pre><code>for t in tmp:\n        if (t.day &lt; 13):\n            t = datetime(year=t.year, month=t.day, day=t.month, hour=t.hour, minute=t.minute, second = t.second)\n</code></pre>\n\n<p>But it doesn't work either</p>\n",
        "formatted_input": {
            "qid": 50367656,
            "link": "https://stackoverflow.com/questions/50367656/python-pandas-pandas-to-datetime-is-switching-day-month-when-day-is-less-t",
            "question": {
                "title": "Python Pandas : pandas.to_datetime() is switching day &amp; month when day is less than 13",
                "ques_desc": "I wrote a code that reads multiple files, however on some of my files datetime swaps day & month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). I tried to fix it by doing this,but it doesn't work. My data frame looks like this: The actual datetime is from 12june2015 to 13june2015 when my I read my datetime column as a string the dates remain correct dd/mm/yyyy but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13. output: Here is my code : I loop through files : then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type by pd_datetime() it swaps the day and month when the day is less than 13. Post converting my datetime column the dates are correct (string type) But when I change the column type I get this: The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13? UPDATE This command swaps all the days and months of my column So in order to swap only the incorrect dates, I wrote a condition: But it doesn't work either "
            },
            "io": [
                "tmp                     p1 p2 \n11/06/2015 00:56:55.060  0  1\n11/06/2015 04:16:38.060  0  1\n12/06/2015 16:13:30.060  0  1\n12/06/2015 21:24:03.060  0  1\n13/06/2015 02:31:44.060  0  1\n13/06/2015 02:37:49.060  0  1\n",
                "print(df)\ntmp                  p1 p2 \n06/11/2015 00:56:55  0  1\n06/11/2015 04:16:38  0  1\n06/12/2015 16:13:30  0  1\n06/12/2015 21:24:03  0  1\n13/06/2015 02:31:44  0  1\n13/06/2015 02:37:49  0  1\n"
            ],
            "answer": {
                "ans_desc": "Well I solved my problem but in a memory consuming method, I split my tmp column first to a date and time columns then I re-split my date column to day month and year, that way I could look for the days that are less than 13 and replace them with the correspondent month # convert series to string type in order to merge them # merge time and date and place result in our column # drop the added columns ",
                "code": [
                    "df['tmp'] = pd.to_datetime(df['tmp'], unit='ns')\ndf['tmp'] = df['tmp'].apply(lambda x: x.replace(microsecond=0))\ndf['date'] = [d.date() for d in df['tmp']]\ndf['time'] = [d.time() for d in df['tmp']]\ndf[['year','month','day']] = df['date'].apply(lambda x: pd.Series(x.strftime(\"%Y-%m-%d\").split(\"-\")))\n\ndf['day'] = pd.to_numeric(df['day'], errors='coerce')\ndf['month'] = pd.to_numeric(df['month'], errors='coerce')\ndf['year'] = pd.to_numeric(df['year'], errors='coerce')\n\n\n#Loop to look for days less than 13 and then swap the day and month\nfor index, d in enumerate(df['day']):\n        if(d <13): \n df.loc[index,'day'],df.loc[index,'month']=df.loc[index,'month'],df.loc[index,'day'] \n",
                    " df['day'] = df['day'].astype(str)\n df['month'] = df['month'].astype(str)\n df['year'] = df['year'].astype(str)\n df['date']=  pd.to_datetime(df[['year', 'month', 'day']])\n df['date'] = df['date'].astype(str)\n df['time'] = df['time'].astype(str)\n",
                    "df.drop(df[['date','year', 'month', 'day','time']], axis=1, inplace = True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user14540992"
        },
        "is_answered": true,
        "view_count": 75,
        "accepted_answer_id": 64664557,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1604427244,
        "creation_date": 1604408887,
        "last_edit_date": 1604416019,
        "question_id": 64663456,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64663456/generate-unique-key-from-multiple-dataframes-based-on-name",
        "title": "Generate unique key from multiple dataframes based on name",
        "body": "<p>I have two data frames. As you can see, the function merges it correctly, but it is wrong. Because the carid must be unique and must not be assigned twice. How can I solve this problem? It can appear several times in a data frame, but it must remain unique over two data records. So <code>Carid = 1 = Mercedes-benz</code> across all data records and <strong>not</strong> <code>Cardid = 1 = Mercedes-Benz &amp; Citroen</code></p>\n<pre><code>import pandas as pd\n\nd = {'Carid ': [1, 2, 3, 1], 'Carname': ['Mercedes-Benz', 'Audi', 'BMW', 'Mercedes-Benz'], 'model': ['S-Klasse AMG 63s', 'S6', 'X6 M-Power', 'Maybach']}\ndf = pd.DataFrame(data=d)\ndisplay(df.head())\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/1YSTU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/1YSTU.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>d2 = {'Carid ': [4, 1, 5], 'Carname': ['VW', 'Citroen', 'Opel'], 'model': ['GTI', 'S', 'Corsa']}\ndf2 = pd.DataFrame(data=d2)\ndisplay(df2.head())\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/ilyT2.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ilyT2.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>dfs = []\ndfs.append(df)\ndfs.append(df2)\npd.concat(dfs)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/vXa0d.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vXa0d.png\" alt=\"enter image description here\" /></a></p>\n<p>What I want</p>\n<p><a href=\"https://i.stack.imgur.com/EQPmr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/EQPmr.png\" alt=\"enter image description here\" /></a></p>\n",
        "answer_body": "<h2>Method 1 Pandas Approach</h2>\n<p>First method if you don't mind changing your keys to floats is to increment using <code>cumcount</code></p>\n<pre><code>df3 = pd.concat([df,df2])\n\ns = df3.groupby('Carname',sort=False)['Carid'].first().to_frame()\ns['Carid'] = s['Carid']  + s.groupby('Carid').cumcount() / 10\n\nnew_ids = s.to_dict(orient='dict')['Carid']\n\ndf3['Carid'] = df3['Carname'].map(new_ids)\n\n  Carid        Carname             model\n0    1.0  Mercedes-Benz  S-Klasse AMG 63s\n1    2.0           Audi                S6\n2    3.0            BMW        X6 M-Power\n3    1.0  Mercedes-Benz           Maybach\n0    4.0             VW               GTI\n1    1.1        Citroen                 S\n2    5.0           Opel             Corsa\n</code></pre>\n<h1>Method 2 Functional Approach using Dictionaries.</h1>\n<h2>Assumptions.</h2>\n<p>The logic of the function is predicated upon having a a unique <code>carid</code> per dataframe.</p>\n<p>Your IDs are in a sequential order so using the <code>max</code> <code>carid</code> to generate the numbers makes the most sense. This may generate non sequential numbers if you have a list of Carids <code>[1,2,3,200]</code></p>\n<p>this would generate a new unique <code>Carid</code>  of <code>201</code> for Citroen given that an ID of <code>200</code> already exists and is owned by a car make.</p>\n<h2>Function</h2>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom collections import ChainMap\n\n\ndef generate_new_keys(*args,key='Carid',name='Carname'):\n    &quot;&quot;&quot;\n    Takes in a number of dataframes and returns any duplicates with a new unique id. \n    groupby columns fixed to CarID and CarName.\n    &quot;&quot;&quot;\n    # adds dictionaries into a single list.\n    dicts_ = [arg.groupby(key)[name].first().to_dict() for arg in args]\n    #merges dicts on unique key, this will exclude duplicates.\n    merged_dicts = dict(ChainMap(*dicts_))\n    #get the duplicate and pass the name into a list.\n    delta = [v for each_dict in dicts_ for k,v in each_dict.items() if v not in merged_dicts.values()]\n    # get the max sequence key\n    start_key =  max(merged_dicts.keys()) + 1\n    # create a new sequence\n    sequence = range(start_key, start_key + len(delta) + 1)\n    # return a dictionary.\n    return {name : number for name,number in zip(delta,sequence)}\n    \n</code></pre>\n<h2>In Action</h2>\n<pre><code>new_keys = generate_new_keys(df,df2)\n\nprint(new_keys)\n{'Citroen': 6}\n\ndf3 = pd.concat([df,df2])\n\ndf3['Carid'] = np.where(df3['Carname'].isin(new_keys.keys()),\n         df3['Carname'].map(new_keys), df3['Carid'])\n\nprint(df3)\n\n   Carid        Carname             model\n0    1.0  Mercedes-Benz  S-Klasse AMG 63s\n1    2.0           Audi                S6\n2    3.0            BMW        X6 M-Power\n0    4.0             VW               GTI\n1    6.0        Citroen                 S\n2    5.0           Opel             Corsa\n</code></pre>\n<h2>Testing additional dataframe.</h2>\n<pre><code>new_df = pd.DataFrame({'Carid' : [1,2,3],\n             'Carname' : ['Mercedes-Benz', 'Toyota','BMW'] })\n\n\nnew_keys = generate_new_keys(df,df2,new_df)\n{'Citroen': 6, 'Toyota': 7}\n\ndf3 = pd.concat([df1,df2,new_df])\n\ndf3['Carid'] = np.where(df3['Carname'].isin(new_keys.keys()),\n         df3['Carname'].map(new_keys), df3['Carid'])\n\nprint(df3)\n\n  Carid        Carname             model\n0    1.0  Mercedes-Benz  S-Klasse AMG 63s\n1    2.0           Audi                S6\n2    3.0            BMW        X6 M-Power\n0    4.0             VW               GTI\n1    6.0        Citroen                 S #&lt; new id\n2    5.0           Opel             Corsa\n0    1.0  Mercedes-Benz               NaN\n1    7.0         Toyota               NaN #&lt; new id\n2    3.0            BMW               NaN\n</code></pre>\n",
        "question_body": "<p>I have two data frames. As you can see, the function merges it correctly, but it is wrong. Because the carid must be unique and must not be assigned twice. How can I solve this problem? It can appear several times in a data frame, but it must remain unique over two data records. So <code>Carid = 1 = Mercedes-benz</code> across all data records and <strong>not</strong> <code>Cardid = 1 = Mercedes-Benz &amp; Citroen</code></p>\n<pre><code>import pandas as pd\n\nd = {'Carid ': [1, 2, 3, 1], 'Carname': ['Mercedes-Benz', 'Audi', 'BMW', 'Mercedes-Benz'], 'model': ['S-Klasse AMG 63s', 'S6', 'X6 M-Power', 'Maybach']}\ndf = pd.DataFrame(data=d)\ndisplay(df.head())\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/1YSTU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/1YSTU.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>d2 = {'Carid ': [4, 1, 5], 'Carname': ['VW', 'Citroen', 'Opel'], 'model': ['GTI', 'S', 'Corsa']}\ndf2 = pd.DataFrame(data=d2)\ndisplay(df2.head())\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/ilyT2.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ilyT2.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>dfs = []\ndfs.append(df)\ndfs.append(df2)\npd.concat(dfs)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/vXa0d.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vXa0d.png\" alt=\"enter image description here\" /></a></p>\n<p>What I want</p>\n<p><a href=\"https://i.stack.imgur.com/EQPmr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/EQPmr.png\" alt=\"enter image description here\" /></a></p>\n",
        "formatted_input": {
            "qid": 64663456,
            "link": "https://stackoverflow.com/questions/64663456/generate-unique-key-from-multiple-dataframes-based-on-name",
            "question": {
                "title": "Generate unique key from multiple dataframes based on name",
                "ques_desc": "I have two data frames. As you can see, the function merges it correctly, but it is wrong. Because the carid must be unique and must not be assigned twice. How can I solve this problem? It can appear several times in a data frame, but it must remain unique over two data records. So across all data records and not What I want "
            },
            "io": [
                "Carid = 1 = Mercedes-benz",
                "Cardid = 1 = Mercedes-Benz & Citroen"
            ],
            "answer": {
                "ans_desc": "Method 1 Pandas Approach First method if you don't mind changing your keys to floats is to increment using Method 2 Functional Approach using Dictionaries. Assumptions. The logic of the function is predicated upon having a a unique per dataframe. Your IDs are in a sequential order so using the to generate the numbers makes the most sense. This may generate non sequential numbers if you have a list of Carids this would generate a new unique of for Citroen given that an ID of already exists and is owned by a car make. Function In Action Testing additional dataframe. ",
                "code": [
                    "import pandas as pd\nimport numpy as np\nfrom collections import ChainMap\n\n\ndef generate_new_keys(*args,key='Carid',name='Carname'):\n    \"\"\"\n    Takes in a number of dataframes and returns any duplicates with a new unique id. \n    groupby columns fixed to CarID and CarName.\n    \"\"\"\n    # adds dictionaries into a single list.\n    dicts_ = [arg.groupby(key)[name].first().to_dict() for arg in args]\n    #merges dicts on unique key, this will exclude duplicates.\n    merged_dicts = dict(ChainMap(*dicts_))\n    #get the duplicate and pass the name into a list.\n    delta = [v for each_dict in dicts_ for k,v in each_dict.items() if v not in merged_dicts.values()]\n    # get the max sequence key\n    start_key =  max(merged_dicts.keys()) + 1\n    # create a new sequence\n    sequence = range(start_key, start_key + len(delta) + 1)\n    # return a dictionary.\n    return {name : number for name,number in zip(delta,sequence)}\n    \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numeric"
        ],
        "owner": {
            "reputation": 767,
            "user_id": 7387269,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e7866ca86d9249650281908977f75469?s=128&d=identicon&r=PG&f=1",
            "display_name": "Raj",
            "link": "https://stackoverflow.com/users/7387269/raj"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 64657056,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1604380422,
        "creation_date": 1604379663,
        "last_edit_date": 1604380413,
        "question_id": 64657045,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64657045/how-to-merge-multiple-columns-containing-numeric-data-in-pandas-but-ignore-empt",
        "title": "How to merge multiple columns containing numeric data in Pandas, but ignore empty cells",
        "body": "<p>I have a table like this:</p>\n<pre><code>|-----|-----|-----|\n|  A  |  B  |  C  |\n|-----|-----|-----|\n|     |  5  |     |\n|-----|-----|-----|\n|  1  |     |     |\n|-----|-----|-----|\n|     |  5  |     |\n|-----|-----|-----|\n|     |     |  2  |\n|-----|-----|-----|\n|     |     |  2  |\n|-----|-----|-----|\n</code></pre>\n<p>where each column in the desired range has only one integer in its row. I want to merge these columns into a single new column that would look like this:</p>\n<pre><code>|-----|-----|-----|    |-----|\n|  A  |  B  |  C  |    |  Z  |\n|-----|-----|-----|    |-----|\n|     |  5  |     | \u2192  |  5  |\n|-----|-----|-----|    |-----|\n|  1  |     |     | \u2192  |  1  |\n|-----|-----|-----|    |-----|\n|     |  5  |     | \u2192  |  5  |\n|-----|-----|-----|    |-----|\n|     |     |  2  | \u2192  |  2  |\n|-----|-----|-----|    |-----|\n|     |     |  2  | \u2192  |  2  |\n|-----|-----|-----|    |-----|\n</code></pre>\n<p>I have been searching, but the closest solution I can find is doing something like:</p>\n<pre><code>df.iloc[:,some_column:another_column].apply( lambda x: &quot;&quot;.join(x.astype(str)), axis=1)\n</code></pre>\n<p>However, this also concatenates &quot;NaN&quot;s from the blank cells, which is obviously undesirable.</p>\n<p>How might I get my desired output?</p>\n",
        "answer_body": "<p>I think it is what you want.</p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame({&quot;A&quot;:[np.nan, 1, np.nan, np.nan, np.nan],\n                   &quot;B&quot;: [5, np.nan, 5, np.nan, np.nan]})\ndf['Z'] = df.sum(axis = 1)\n</code></pre>\n<p>Alternatively, you can use</p>\n<pre><code>df['Z'] = df.max(axis = 1)\n</code></pre>\n<p>Which might be safer if (per chance) you have multiple non-NULL values and just want one of them (the largest one in this case).</p>\n",
        "question_body": "<p>I have a table like this:</p>\n<pre><code>|-----|-----|-----|\n|  A  |  B  |  C  |\n|-----|-----|-----|\n|     |  5  |     |\n|-----|-----|-----|\n|  1  |     |     |\n|-----|-----|-----|\n|     |  5  |     |\n|-----|-----|-----|\n|     |     |  2  |\n|-----|-----|-----|\n|     |     |  2  |\n|-----|-----|-----|\n</code></pre>\n<p>where each column in the desired range has only one integer in its row. I want to merge these columns into a single new column that would look like this:</p>\n<pre><code>|-----|-----|-----|    |-----|\n|  A  |  B  |  C  |    |  Z  |\n|-----|-----|-----|    |-----|\n|     |  5  |     | \u2192  |  5  |\n|-----|-----|-----|    |-----|\n|  1  |     |     | \u2192  |  1  |\n|-----|-----|-----|    |-----|\n|     |  5  |     | \u2192  |  5  |\n|-----|-----|-----|    |-----|\n|     |     |  2  | \u2192  |  2  |\n|-----|-----|-----|    |-----|\n|     |     |  2  | \u2192  |  2  |\n|-----|-----|-----|    |-----|\n</code></pre>\n<p>I have been searching, but the closest solution I can find is doing something like:</p>\n<pre><code>df.iloc[:,some_column:another_column].apply( lambda x: &quot;&quot;.join(x.astype(str)), axis=1)\n</code></pre>\n<p>However, this also concatenates &quot;NaN&quot;s from the blank cells, which is obviously undesirable.</p>\n<p>How might I get my desired output?</p>\n",
        "formatted_input": {
            "qid": 64657045,
            "link": "https://stackoverflow.com/questions/64657045/how-to-merge-multiple-columns-containing-numeric-data-in-pandas-but-ignore-empt",
            "question": {
                "title": "How to merge multiple columns containing numeric data in Pandas, but ignore empty cells",
                "ques_desc": "I have a table like this: where each column in the desired range has only one integer in its row. I want to merge these columns into a single new column that would look like this: I have been searching, but the closest solution I can find is doing something like: However, this also concatenates \"NaN\"s from the blank cells, which is obviously undesirable. How might I get my desired output? "
            },
            "io": [
                "|-----|-----|-----|\n|  A  |  B  |  C  |\n|-----|-----|-----|\n|     |  5  |     |\n|-----|-----|-----|\n|  1  |     |     |\n|-----|-----|-----|\n|     |  5  |     |\n|-----|-----|-----|\n|     |     |  2  |\n|-----|-----|-----|\n|     |     |  2  |\n|-----|-----|-----|\n",
                "|-----|-----|-----|    |-----|\n|  A  |  B  |  C  |    |  Z  |\n|-----|-----|-----|    |-----|\n|     |  5  |     | \u2192  |  5  |\n|-----|-----|-----|    |-----|\n|  1  |     |     | \u2192  |  1  |\n|-----|-----|-----|    |-----|\n|     |  5  |     | \u2192  |  5  |\n|-----|-----|-----|    |-----|\n|     |     |  2  | \u2192  |  2  |\n|-----|-----|-----|    |-----|\n|     |     |  2  | \u2192  |  2  |\n|-----|-----|-----|    |-----|\n"
            ],
            "answer": {
                "ans_desc": "I think it is what you want. Alternatively, you can use Which might be safer if (per chance) you have multiple non-NULL values and just want one of them (the largest one in this case). ",
                "code": [
                    "df['Z'] = df.max(axis = 1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 405,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 19,
        "accepted_answer_id": 64651715,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1604343741,
        "creation_date": 1604343550,
        "question_id": 64651683,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64651683/create-dataframe-column-that-increases-count-based-on-another-columns-value",
        "title": "Create Dataframe column that increases count based on another columns value",
        "body": "<p>I need to add a column to my dataframe that will add a number each time a value in another column surpasses a limit. Example below:</p>\n<p>Original DF:</p>\n<pre><code>ColA   ColB\n 4      1.4\n 10     0.5\n 1      2.3\n 3      12.2\n 8.8    8.5\n 2      5.2\n 0.6    0.33\n 9      3\n 4      144\n 33     8\n</code></pre>\n<p>Desired DF: Where value in ColB surpasses 10, ColC count = count +1</p>\n<pre><code>ColA   ColB     ColC\n 4      1.4       1\n 10     0.5       1\n 1      2.3       1\n 3      12.2      2\n 8.8    8.5       2\n 2      5.2       2\n 0.6    0.33      2\n 9      3         2\n 4      144       3\n 33     8         3\n</code></pre>\n<p>thank you for the help!</p>\n",
        "answer_body": "<pre><code>df['ColC'] = (df['ColB'] &gt; 10).cumsum() + 1\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre><code>   ColA    ColB  ColC\n0   4.0    1.40     1\n1  10.0    0.50     1\n2   1.0    2.30     1\n3   3.0   12.20     2\n4   8.8    8.50     2\n5   2.0    5.20     2\n6   0.6    0.33     2\n7   9.0    3.00     2\n8   4.0  144.00     3\n9  33.0    8.00     3\n</code></pre>\n",
        "question_body": "<p>I need to add a column to my dataframe that will add a number each time a value in another column surpasses a limit. Example below:</p>\n<p>Original DF:</p>\n<pre><code>ColA   ColB\n 4      1.4\n 10     0.5\n 1      2.3\n 3      12.2\n 8.8    8.5\n 2      5.2\n 0.6    0.33\n 9      3\n 4      144\n 33     8\n</code></pre>\n<p>Desired DF: Where value in ColB surpasses 10, ColC count = count +1</p>\n<pre><code>ColA   ColB     ColC\n 4      1.4       1\n 10     0.5       1\n 1      2.3       1\n 3      12.2      2\n 8.8    8.5       2\n 2      5.2       2\n 0.6    0.33      2\n 9      3         2\n 4      144       3\n 33     8         3\n</code></pre>\n<p>thank you for the help!</p>\n",
        "formatted_input": {
            "qid": 64651683,
            "link": "https://stackoverflow.com/questions/64651683/create-dataframe-column-that-increases-count-based-on-another-columns-value",
            "question": {
                "title": "Create Dataframe column that increases count based on another columns value",
                "ques_desc": "I need to add a column to my dataframe that will add a number each time a value in another column surpasses a limit. Example below: Original DF: Desired DF: Where value in ColB surpasses 10, ColC count = count +1 thank you for the help! "
            },
            "io": [
                "ColA   ColB\n 4      1.4\n 10     0.5\n 1      2.3\n 3      12.2\n 8.8    8.5\n 2      5.2\n 0.6    0.33\n 9      3\n 4      144\n 33     8\n",
                "ColA   ColB     ColC\n 4      1.4       1\n 10     0.5       1\n 1      2.3       1\n 3      12.2      2\n 8.8    8.5       2\n 2      5.2       2\n 0.6    0.33      2\n 9      3         2\n 4      144       3\n 33     8         3\n"
            ],
            "answer": {
                "ans_desc": " Prints: ",
                "code": [
                    "df['ColC'] = (df['ColB'] > 10).cumsum() + 1\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 135,
            "user_id": 12410784,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d65f64d4e9bb2c4c58e8f886c0e290b8?s=128&d=identicon&r=PG&f=1",
            "display_name": "mathilde",
            "link": "https://stackoverflow.com/users/12410784/mathilde"
        },
        "is_answered": true,
        "view_count": 140,
        "accepted_answer_id": 64646913,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1604325316,
        "creation_date": 1604323690,
        "question_id": 64646490,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64646490/calculate-similarity-between-rows-of-a-dataframe-count-values-in-common",
        "title": "Calculate similarity between rows of a dataframe (count values in common)",
        "body": "<p>I want to calculate similarity between the rows of my dataframe. I have some columns with informations about some people. One row is one person. It looks like that :</p>\n<pre><code> print(df)\n        id  name      firstname  email                town    age\n    0    1  martin    pierre     truc@machin.com      Paris   na\n    1    2  dupond    sarah      bidule@machin.com    London  32\n    2    3  dupond    sarah      bidule@machin.com    Berlin  32\n    3    4  dupond    john       na                   Madrid  45\n    4    5  smith     na         something@thing.com  Paris   28\n\n</code></pre>\n<p>I want to count for each row the number of values in common with the other rows divided by the number of columns if at least 3 columns are completed.\nFor example, between the row with the index 1 and the row with the index 2, there are 4 variables in common. So, my similarity will be 4/5 (id doesn't count) = 80% of similarity.\nMy result has to be a similarity matrix, because after that I want to find the rows with a similarity higher than 0.6 to build a new dataframe.\nIt could be something like that :</p>\n<pre><code> print(similarity)\n        0    1    2    3    4\n    0   1    0    0    0    0.2\n    1   0.2  1    0.8  0.2  0\n    2   0    0.8  1    0.2  0\n    3   0    0.2  0.2  1    0\n    4   0.2  0    0    0    1\n</code></pre>\n<p>Because the results are duplicated, half of that would be enough :</p>\n<pre><code> print(similarity)\n        0    1    2    3    4\n    0        0    0    0    0.2\n    1             0.8  0.2  0\n    2                  0.2  0\n    3                       0\n    4 \n</code></pre>\n<p>I'm looking for a function that will automate that but I couldn't find. Does something like that exist?\nThanks for reading, any advice or idea will be welcomed.</p>\n",
        "answer_body": "<p>You can use <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\" rel=\"nofollow noreferrer\"><code>scipy.spatial.distance.pdist</code></a> with a custom distance function</p>\n<pre><code>from scipy.spatial.distance import pdist, squareform\npd.DataFrame(1 - squareform(pdist(df.set_index('id'), lambda u,v: (u != v).mean())))\n</code></pre>\n<p>Out:</p>\n<pre><code>     0    1    2    3    4\n0  1.0  0.0  0.0  0.0  0.2\n1  0.0  1.0  0.8  0.2  0.0\n2  0.0  0.8  1.0  0.2  0.0\n3  0.0  0.2  0.2  1.0  0.0\n4  0.2  0.0  0.0  0.0  1.0\n</code></pre>\n",
        "question_body": "<p>I want to calculate similarity between the rows of my dataframe. I have some columns with informations about some people. One row is one person. It looks like that :</p>\n<pre><code> print(df)\n        id  name      firstname  email                town    age\n    0    1  martin    pierre     truc@machin.com      Paris   na\n    1    2  dupond    sarah      bidule@machin.com    London  32\n    2    3  dupond    sarah      bidule@machin.com    Berlin  32\n    3    4  dupond    john       na                   Madrid  45\n    4    5  smith     na         something@thing.com  Paris   28\n\n</code></pre>\n<p>I want to count for each row the number of values in common with the other rows divided by the number of columns if at least 3 columns are completed.\nFor example, between the row with the index 1 and the row with the index 2, there are 4 variables in common. So, my similarity will be 4/5 (id doesn't count) = 80% of similarity.\nMy result has to be a similarity matrix, because after that I want to find the rows with a similarity higher than 0.6 to build a new dataframe.\nIt could be something like that :</p>\n<pre><code> print(similarity)\n        0    1    2    3    4\n    0   1    0    0    0    0.2\n    1   0.2  1    0.8  0.2  0\n    2   0    0.8  1    0.2  0\n    3   0    0.2  0.2  1    0\n    4   0.2  0    0    0    1\n</code></pre>\n<p>Because the results are duplicated, half of that would be enough :</p>\n<pre><code> print(similarity)\n        0    1    2    3    4\n    0        0    0    0    0.2\n    1             0.8  0.2  0\n    2                  0.2  0\n    3                       0\n    4 \n</code></pre>\n<p>I'm looking for a function that will automate that but I couldn't find. Does something like that exist?\nThanks for reading, any advice or idea will be welcomed.</p>\n",
        "formatted_input": {
            "qid": 64646490,
            "link": "https://stackoverflow.com/questions/64646490/calculate-similarity-between-rows-of-a-dataframe-count-values-in-common",
            "question": {
                "title": "Calculate similarity between rows of a dataframe (count values in common)",
                "ques_desc": "I want to calculate similarity between the rows of my dataframe. I have some columns with informations about some people. One row is one person. It looks like that : I want to count for each row the number of values in common with the other rows divided by the number of columns if at least 3 columns are completed. For example, between the row with the index 1 and the row with the index 2, there are 4 variables in common. So, my similarity will be 4/5 (id doesn't count) = 80% of similarity. My result has to be a similarity matrix, because after that I want to find the rows with a similarity higher than 0.6 to build a new dataframe. It could be something like that : Because the results are duplicated, half of that would be enough : I'm looking for a function that will automate that but I couldn't find. Does something like that exist? Thanks for reading, any advice or idea will be welcomed. "
            },
            "io": [
                " print(similarity)\n        0    1    2    3    4\n    0   1    0    0    0    0.2\n    1   0.2  1    0.8  0.2  0\n    2   0    0.8  1    0.2  0\n    3   0    0.2  0.2  1    0\n    4   0.2  0    0    0    1\n",
                " print(similarity)\n        0    1    2    3    4\n    0        0    0    0    0.2\n    1             0.8  0.2  0\n    2                  0.2  0\n    3                       0\n    4 \n"
            ],
            "answer": {
                "ans_desc": "You can use with a custom distance function Out: ",
                "code": [
                    "from scipy.spatial.distance import pdist, squareform\npd.DataFrame(1 - squareform(pdist(df.set_index('id'), lambda u,v: (u != v).mean())))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 10162135,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-4zSjGHCq3UQ/AAAAAAAAAAI/AAAAAAAAAFA/bQ0rIMjjPQE/photo.jpg?sz=128",
            "display_name": "Ali Mahmoudi",
            "link": "https://stackoverflow.com/users/10162135/ali-mahmoudi"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 64640228,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1604295874,
        "creation_date": 1604294662,
        "question_id": 64640182,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64640182/how-to-modify-numercial-values-in-a-column-of-mixed-data-types-in-a-pandas-dataf",
        "title": "How to modify numercial values in a column of mixed data types in a pandas dataframe?",
        "body": "<p>I have a pandas dataframe in pyhton that looks like this (my actual dataframe is MUCH bigger than this):</p>\n<pre><code>  col_1 col_2\n0   0.8   0.1\n1  nope   0.6\n2   0.4   0.7\n3  nope  nope\n</code></pre>\n<p>How can I perform some operations on the numerical values of specific columns. For example, multiply the numerical values of col_2 by 10 to get something like this:</p>\n<pre><code>  col_1 col_2\n0   0.8   1\n1  nope   6\n2   0.4   7\n3  nope  nope\n</code></pre>\n<p>Although it looks like a simple task I couldn't find a solution for it anywhere on internet.</p>\n<p>Thanks in advance.</p>\n",
        "answer_body": "<p>First you need to convert the <code>object</code> type columns to <code>numeric</code> columns by using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html\" rel=\"nofollow noreferrer\"><code>pd.to_numeric</code></a>:</p>\n<pre><code>In [141]: df.col_2 = pd.to_numeric(df.col_2, errors='coerce')\n</code></pre>\n<p><code>errors='coerce'</code> converts all non-numeric type values in the column to <code>NaN</code>.</p>\n<p>Then, multiply by 10:</p>\n<pre><code>In [144]: df.col_2 = df.col_2 * 10\n\nIn [145]: df\nOut[145]: \n  col_1  col_2\n0   0.8    1.0\n1  nope    6.0\n2   0.4    7.0\n3  nope    NaN\n</code></pre>\n<p>if you want to convert <code>NaN</code> back to <code>nope</code>, you can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\" rel=\"nofollow noreferrer\"><code>df.fillna</code></a>:</p>\n<pre><code>In [177]: df.fillna('nope', inplace=True)\n\nIn [178]: df\nOut[178]: \n  col_1 col_2\n0   0.8     1\n1  nope     6\n2   0.4     7\n3  nope  nope\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe in pyhton that looks like this (my actual dataframe is MUCH bigger than this):</p>\n<pre><code>  col_1 col_2\n0   0.8   0.1\n1  nope   0.6\n2   0.4   0.7\n3  nope  nope\n</code></pre>\n<p>How can I perform some operations on the numerical values of specific columns. For example, multiply the numerical values of col_2 by 10 to get something like this:</p>\n<pre><code>  col_1 col_2\n0   0.8   1\n1  nope   6\n2   0.4   7\n3  nope  nope\n</code></pre>\n<p>Although it looks like a simple task I couldn't find a solution for it anywhere on internet.</p>\n<p>Thanks in advance.</p>\n",
        "formatted_input": {
            "qid": 64640182,
            "link": "https://stackoverflow.com/questions/64640182/how-to-modify-numercial-values-in-a-column-of-mixed-data-types-in-a-pandas-dataf",
            "question": {
                "title": "How to modify numercial values in a column of mixed data types in a pandas dataframe?",
                "ques_desc": "I have a pandas dataframe in pyhton that looks like this (my actual dataframe is MUCH bigger than this): How can I perform some operations on the numerical values of specific columns. For example, multiply the numerical values of col_2 by 10 to get something like this: Although it looks like a simple task I couldn't find a solution for it anywhere on internet. Thanks in advance. "
            },
            "io": [
                "  col_1 col_2\n0   0.8   0.1\n1  nope   0.6\n2   0.4   0.7\n3  nope  nope\n",
                "  col_1 col_2\n0   0.8   1\n1  nope   6\n2   0.4   7\n3  nope  nope\n"
            ],
            "answer": {
                "ans_desc": "First you need to convert the type columns to columns by using : converts all non-numeric type values in the column to . Then, multiply by 10: if you want to convert back to , you can use : ",
                "code": [
                    "In [141]: df.col_2 = pd.to_numeric(df.col_2, errors='coerce')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 132,
            "user_id": 9650258,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "Cameron Cheung",
            "link": "https://stackoverflow.com/users/9650258/cameron-cheung"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 64625455,
        "answer_count": 6,
        "score": 3,
        "last_activity_date": 1604172678,
        "creation_date": 1604170340,
        "last_edit_date": 1604171787,
        "question_id": 64625342,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64625342/subtract-from-every-value-in-a-dataframe",
        "title": "Subtract from every value in a DataFrame",
        "body": "<p>I have a dataframe that looks like this:</p>\n<pre><code>userId   movie1   movie2   movie3   movie4   score\n0        4.1      2.1      1.0      NaN      2\n1        3.1      1.1      3.4      1.4      1\n2        2.8      NaN      1.7      NaN      3\n3        NaN      5.0      NaN      2.3      4\n4        NaN      NaN      NaN      NaN      1\n5        2.3      NaN      2.0      4.0      1\n</code></pre>\n<p>I want to subtract the movie scores from each movie so the output would look like this:</p>\n<pre><code>userId   movie1   movie2   movie3   movie4   score\n0        2.1      0.1     -1.0      NaN      2\n1        2.1      0.1      2.4      0.4      1\n2       -0.2      NaN     -2.3      NaN      3\n3        NaN      1.0      NaN     -1.7      4\n4        NaN      NaN      NaN      NaN      1\n5        1.3      NaN      1.0      3.0      1\n</code></pre>\n<p>The actual dataframe has thousands of movies and the movies are referenced by name so im trying to find a solution to comply with that.</p>\n<p><strong>I should have also mention that the movies are not listed in order like [&quot;movie1&quot;, &quot;movie2&quot;, &quot;movie3&quot;], they are listed by their titles instead like [&quot;Star Wars&quot;, &quot;Harry Potter&quot;, &quot;Lord of the Rings&quot;]. The dataset could be changed so I wont know what the last movie in the list is.</strong></p>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html\" rel=\"nofollow noreferrer\"><code>df.filter</code></a> to identify the <code>movie</code> columns and then <code>subtract</code> these columns from <code>score</code> array:</p>\n<pre><code>In [35]: x = df.filter(like='movie', axis=1).columns.tolist()\n\nIn [36]: df[x] = df.filter(like='movie', axis=1) - df.score.values[:, None]\n\nIn [37]: df\nOut[37]: \n   userId  movie1  movie2  movie3  movie4  score\n0       0     2.1     0.1    -1.0     NaN      2\n1       1     2.1     0.1     2.4     0.4      1\n2       2    -0.2     NaN    -1.3     NaN      3\n3       3     NaN     1.0     NaN    -1.7      4\n4       4     NaN     NaN     NaN     NaN      5\n5       5    -3.7     NaN    -4.0    -2.0      6\n</code></pre>\n<p><strong>EDIT: When the movie column names are random. Select all columns except <code>'userId', 'score'</code>:</strong></p>\n<pre><code>x = df.columns[~df.columns.isin(['userId', 'score'])]\ndf[x] = df[x] - df.score.values[:, None]\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like this:</p>\n<pre><code>userId   movie1   movie2   movie3   movie4   score\n0        4.1      2.1      1.0      NaN      2\n1        3.1      1.1      3.4      1.4      1\n2        2.8      NaN      1.7      NaN      3\n3        NaN      5.0      NaN      2.3      4\n4        NaN      NaN      NaN      NaN      1\n5        2.3      NaN      2.0      4.0      1\n</code></pre>\n<p>I want to subtract the movie scores from each movie so the output would look like this:</p>\n<pre><code>userId   movie1   movie2   movie3   movie4   score\n0        2.1      0.1     -1.0      NaN      2\n1        2.1      0.1      2.4      0.4      1\n2       -0.2      NaN     -2.3      NaN      3\n3        NaN      1.0      NaN     -1.7      4\n4        NaN      NaN      NaN      NaN      1\n5        1.3      NaN      1.0      3.0      1\n</code></pre>\n<p>The actual dataframe has thousands of movies and the movies are referenced by name so im trying to find a solution to comply with that.</p>\n<p><strong>I should have also mention that the movies are not listed in order like [&quot;movie1&quot;, &quot;movie2&quot;, &quot;movie3&quot;], they are listed by their titles instead like [&quot;Star Wars&quot;, &quot;Harry Potter&quot;, &quot;Lord of the Rings&quot;]. The dataset could be changed so I wont know what the last movie in the list is.</strong></p>\n",
        "formatted_input": {
            "qid": 64625342,
            "link": "https://stackoverflow.com/questions/64625342/subtract-from-every-value-in-a-dataframe",
            "question": {
                "title": "Subtract from every value in a DataFrame",
                "ques_desc": "I have a dataframe that looks like this: I want to subtract the movie scores from each movie so the output would look like this: The actual dataframe has thousands of movies and the movies are referenced by name so im trying to find a solution to comply with that. I should have also mention that the movies are not listed in order like [\"movie1\", \"movie2\", \"movie3\"], they are listed by their titles instead like [\"Star Wars\", \"Harry Potter\", \"Lord of the Rings\"]. The dataset could be changed so I wont know what the last movie in the list is. "
            },
            "io": [
                "userId   movie1   movie2   movie3   movie4   score\n0        4.1      2.1      1.0      NaN      2\n1        3.1      1.1      3.4      1.4      1\n2        2.8      NaN      1.7      NaN      3\n3        NaN      5.0      NaN      2.3      4\n4        NaN      NaN      NaN      NaN      1\n5        2.3      NaN      2.0      4.0      1\n",
                "userId   movie1   movie2   movie3   movie4   score\n0        2.1      0.1     -1.0      NaN      2\n1        2.1      0.1      2.4      0.4      1\n2       -0.2      NaN     -2.3      NaN      3\n3        NaN      1.0      NaN     -1.7      4\n4        NaN      NaN      NaN      NaN      1\n5        1.3      NaN      1.0      3.0      1\n"
            ],
            "answer": {
                "ans_desc": "Use to identify the columns and then these columns from array: EDIT: When the movie column names are random. Select all columns except : ",
                "code": [
                    "x = df.columns[~df.columns.isin(['userId', 'score'])]\ndf[x] = df[x] - df.score.values[:, None]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 14543409,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-Yx0j89A-Cgs/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuck2CgR5vnG0PSC6nlaphQaxgMoa4A/s96-c/photo.jpg?sz=128",
            "display_name": "Hector Castro",
            "link": "https://stackoverflow.com/users/14543409/hector-castro"
        },
        "is_answered": true,
        "view_count": 46,
        "closed_date": 1604015932,
        "accepted_answer_id": 64599603,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1604009171,
        "creation_date": 1604005659,
        "last_edit_date": 1604009171,
        "question_id": 64599227,
        "link": "https://stackoverflow.com/questions/64599227/is-there-a-way-to-combine-9-12-or-15-columns-from-a-single-df-into-just-3",
        "closed_reason": "Needs details or clarity",
        "title": "Is there a way to combine 9,12 or 15 columns from a single df into just 3?",
        "body": "<p>I'm trying to convert a df that has the data divided every 3 columns into just three.\nAn example is from this:</p>\n<pre><code>C1 C2 C3 C4 C5 C6  C7  C8  C9 \n1  6   9  A  D  G  1A  6A  9A\n2  7  10  B  E  H  2A  7A  10A\n3  8  11  C  F  I  3A  8A  11A\n</code></pre>\n<p>To this:</p>\n<pre><code>C1 C2 C3\n1  6   9\n2  7  10\n3  8  11\nC4 C5 C6\nA  D  G\nB  E  H\nC  F  I\nC7 C8 C9\n1A 6A 9A\n2A 7A 10A\n3A 8A 11A\n</code></pre>\n",
        "answer_body": "<p>You can use <code>numpy</code>:</p>\n<pre><code>arr = np.hsplit(np.vstack([df.columns.values, df.values]), 3)\npd.DataFrame(np.vstack(arr))\n</code></pre>\n<p>Output:</p>\n<pre><code>     0   1    2\n0   C1  C2   C3\n1    1   6    9\n2    2   7   10\n3    3   8   11\n4   C4  C5   C6\n5    A   D    G\n6    B   E    H\n7    C   F    I\n8   C7  C8   C9\n9   1A  6A   9A\n10  2A  7A  10A\n11  3A  8A  11A\n</code></pre>\n",
        "question_body": "<p>I'm trying to convert a df that has the data divided every 3 columns into just three.\nAn example is from this:</p>\n<pre><code>C1 C2 C3 C4 C5 C6  C7  C8  C9 \n1  6   9  A  D  G  1A  6A  9A\n2  7  10  B  E  H  2A  7A  10A\n3  8  11  C  F  I  3A  8A  11A\n</code></pre>\n<p>To this:</p>\n<pre><code>C1 C2 C3\n1  6   9\n2  7  10\n3  8  11\nC4 C5 C6\nA  D  G\nB  E  H\nC  F  I\nC7 C8 C9\n1A 6A 9A\n2A 7A 10A\n3A 8A 11A\n</code></pre>\n",
        "formatted_input": {
            "qid": 64599227,
            "link": "https://stackoverflow.com/questions/64599227/is-there-a-way-to-combine-9-12-or-15-columns-from-a-single-df-into-just-3",
            "question": {
                "title": "Is there a way to combine 9,12 or 15 columns from a single df into just 3?",
                "ques_desc": "I'm trying to convert a df that has the data divided every 3 columns into just three. An example is from this: To this: "
            },
            "io": [
                "C1 C2 C3 C4 C5 C6  C7  C8  C9 \n1  6   9  A  D  G  1A  6A  9A\n2  7  10  B  E  H  2A  7A  10A\n3  8  11  C  F  I  3A  8A  11A\n",
                "C1 C2 C3\n1  6   9\n2  7  10\n3  8  11\nC4 C5 C6\nA  D  G\nB  E  H\nC  F  I\nC7 C8 C9\n1A 6A 9A\n2A 7A 10A\n3A 8A 11A\n"
            ],
            "answer": {
                "ans_desc": "You can use : Output: ",
                "code": [
                    "arr = np.hsplit(np.vstack([df.columns.values, df.values]), 3)\npd.DataFrame(np.vstack(arr))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 316,
            "user_id": 7388758,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/274dedbcb986e6f2ba60c3ea79f53bbb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Miguel",
            "link": "https://stackoverflow.com/users/7388758/miguel"
        },
        "is_answered": true,
        "view_count": 75,
        "accepted_answer_id": 64592985,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1603981161,
        "creation_date": 1603981034,
        "question_id": 64592950,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64592950/pandas-shifting-a-rolling-sum-after-grouping-spills-over-to-following-groups",
        "title": "Pandas - shifting a rolling sum after grouping spills over to following groups",
        "body": "<p>I might be doing something wrong, but I was trying to calculate a rolling average (let's use sum instead in this example for simplicity) after grouping the dataframe. Until here it all works well, but when I apply a shift I'm finding the values spill over to the group below. See example below:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'X': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n                   'Y': [1, 2, 3, 1, 2, 3, 1, 2, 3]})\n\ngrouped_df = df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum().shift(periods=1)\nprint(grouped_df)\n</code></pre>\n<p>Expected result:</p>\n<pre><code>X   \nA  0    NaN\n   1    NaN\n   2    3.0\nB  3    NaN\n   4    NaN\n   5    3.0\nC  6    NaN\n   7    NaN\n   8    3.0\n</code></pre>\n<p>Result I actually get:</p>\n<pre><code>X   \nA  0    NaN\n   1    NaN\n   2    3.0\nB  3    5.0\n   4    NaN\n   5    3.0\nC  6    5.0\n   7    NaN\n   8    3.0\n</code></pre>\n<p>You can see the result of A2 gets passed to B3 and the result of B5 to C6. I'm not sure this is the intended behaviour and I'm doing something wrong or there is some bug in pandas?</p>\n<p>Thanks</p>\n",
        "answer_body": "<p>The problem is that</p>\n<pre><code>df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum()\n</code></pre>\n<p>returns a new series, then when you chain with <code>shift()</code>, you shift the series as a whole, not within the group.</p>\n<p>You need another <code>groupby</code> to shift within the group:</p>\n<pre><code>grouped_df = (df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum()\n                .groupby(level=0).shift(periods=1)\n             )\n</code></pre>\n<p>Or use <code>groupby.transform</code>:</p>\n<pre><code>grouped_df = (df.groupby('X')['Y']\n                .transform(lambda x: x.rolling(window=2, min_periods=2)\n                                      .sum().shift(periods=1))\n             )\n</code></pre>\n<p>Output:</p>\n<pre><code>X   \nA  0    NaN\n   1    NaN\n   2    3.0\nB  3    NaN\n   4    NaN\n   5    3.0\nC  6    NaN\n   7    NaN\n   8    3.0\nName: Y, dtype: float64\n</code></pre>\n",
        "question_body": "<p>I might be doing something wrong, but I was trying to calculate a rolling average (let's use sum instead in this example for simplicity) after grouping the dataframe. Until here it all works well, but when I apply a shift I'm finding the values spill over to the group below. See example below:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'X': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n                   'Y': [1, 2, 3, 1, 2, 3, 1, 2, 3]})\n\ngrouped_df = df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum().shift(periods=1)\nprint(grouped_df)\n</code></pre>\n<p>Expected result:</p>\n<pre><code>X   \nA  0    NaN\n   1    NaN\n   2    3.0\nB  3    NaN\n   4    NaN\n   5    3.0\nC  6    NaN\n   7    NaN\n   8    3.0\n</code></pre>\n<p>Result I actually get:</p>\n<pre><code>X   \nA  0    NaN\n   1    NaN\n   2    3.0\nB  3    5.0\n   4    NaN\n   5    3.0\nC  6    5.0\n   7    NaN\n   8    3.0\n</code></pre>\n<p>You can see the result of A2 gets passed to B3 and the result of B5 to C6. I'm not sure this is the intended behaviour and I'm doing something wrong or there is some bug in pandas?</p>\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 64592950,
            "link": "https://stackoverflow.com/questions/64592950/pandas-shifting-a-rolling-sum-after-grouping-spills-over-to-following-groups",
            "question": {
                "title": "Pandas - shifting a rolling sum after grouping spills over to following groups",
                "ques_desc": "I might be doing something wrong, but I was trying to calculate a rolling average (let's use sum instead in this example for simplicity) after grouping the dataframe. Until here it all works well, but when I apply a shift I'm finding the values spill over to the group below. See example below: Expected result: Result I actually get: You can see the result of A2 gets passed to B3 and the result of B5 to C6. I'm not sure this is the intended behaviour and I'm doing something wrong or there is some bug in pandas? Thanks "
            },
            "io": [
                "X   \nA  0    NaN\n   1    NaN\n   2    3.0\nB  3    NaN\n   4    NaN\n   5    3.0\nC  6    NaN\n   7    NaN\n   8    3.0\n",
                "X   \nA  0    NaN\n   1    NaN\n   2    3.0\nB  3    5.0\n   4    NaN\n   5    3.0\nC  6    5.0\n   7    NaN\n   8    3.0\n"
            ],
            "answer": {
                "ans_desc": "The problem is that returns a new series, then when you chain with , you shift the series as a whole, not within the group. You need another to shift within the group: Or use : Output: ",
                "code": [
                    "grouped_df = (df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum()\n                .groupby(level=0).shift(periods=1)\n             )\n",
                    "grouped_df = (df.groupby('X')['Y']\n                .transform(lambda x: x.rolling(window=2, min_periods=2)\n                                      .sum().shift(periods=1))\n             )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 14370867,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgIrtu8Ao5Ae9PVTC4f4RL_lKPDnnfiJXZSnqL1=k-s128",
            "display_name": "Soham",
            "link": "https://stackoverflow.com/users/14370867/soham"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 64583570,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1603934225,
        "creation_date": 1603933694,
        "question_id": 64583504,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64583504/selectively-adding-values-of-columns-in-a-dataframe",
        "title": "Selectively adding values of columns in a dataframe",
        "body": "<p>I have a pandas data frame like this</p>\n<pre><code>YEAR_OPENED  2000 2001 2002 2003 2004 2005 2006 2007 2008 2009\n 1999          1    0    0   0    1    0     0   0     1    0\n 2000          1    1    2   0    3    0     0   0     0    0\n 2001          0    0    0   4    0    0     0   0     0    0\n</code></pre>\n<p>I want to add all the values in the given columns like this:</p>\n<pre><code>YEAR_OPENED   CLOSED_IN_5_YEARS\n 1999               2\n 2000               7\n 2001               4\n</code></pre>\n<p>So basically I want to check if the column names fall in a five year range of the corresponding values in the column 'YEAR_OPENED' and create a new column with the sum of all the values. How should I proceed?</p>\n",
        "answer_body": "<pre><code>df['CLOSED_IN_5_YEARS'] = df.set_index('YEAR_OPENED').apply(\n        lambda x: sum(i for i, c in zip(x, x.index) if x.name &lt;= int(c) &lt;= x.name + 5), axis=1\n    ).values\n\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre><code>   YEAR_OPENED  2000  2001  2002  ...  2007  2008  2009  CLOSED_IN_5_YEARS\n0         1999     1     0     0  ...     0     1     0                  2\n1         2000     1     1     2  ...     0     0     0                  7\n2         2001     0     0     0  ...     0     0     0                  4\n</code></pre>\n",
        "question_body": "<p>I have a pandas data frame like this</p>\n<pre><code>YEAR_OPENED  2000 2001 2002 2003 2004 2005 2006 2007 2008 2009\n 1999          1    0    0   0    1    0     0   0     1    0\n 2000          1    1    2   0    3    0     0   0     0    0\n 2001          0    0    0   4    0    0     0   0     0    0\n</code></pre>\n<p>I want to add all the values in the given columns like this:</p>\n<pre><code>YEAR_OPENED   CLOSED_IN_5_YEARS\n 1999               2\n 2000               7\n 2001               4\n</code></pre>\n<p>So basically I want to check if the column names fall in a five year range of the corresponding values in the column 'YEAR_OPENED' and create a new column with the sum of all the values. How should I proceed?</p>\n",
        "formatted_input": {
            "qid": 64583504,
            "link": "https://stackoverflow.com/questions/64583504/selectively-adding-values-of-columns-in-a-dataframe",
            "question": {
                "title": "Selectively adding values of columns in a dataframe",
                "ques_desc": "I have a pandas data frame like this I want to add all the values in the given columns like this: So basically I want to check if the column names fall in a five year range of the corresponding values in the column 'YEAR_OPENED' and create a new column with the sum of all the values. How should I proceed? "
            },
            "io": [
                "YEAR_OPENED  2000 2001 2002 2003 2004 2005 2006 2007 2008 2009\n 1999          1    0    0   0    1    0     0   0     1    0\n 2000          1    1    2   0    3    0     0   0     0    0\n 2001          0    0    0   4    0    0     0   0     0    0\n",
                "YEAR_OPENED   CLOSED_IN_5_YEARS\n 1999               2\n 2000               7\n 2001               4\n"
            ],
            "answer": {
                "ans_desc": " Prints: ",
                "code": [
                    "df['CLOSED_IN_5_YEARS'] = df.set_index('YEAR_OPENED').apply(\n        lambda x: sum(i for i, c in zip(x, x.index) if x.name <= int(c) <= x.name + 5), axis=1\n    ).values\n\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1529,
            "user_id": 2302262,
            "user_type": "registered",
            "accept_rate": 79,
            "profile_image": "https://i.stack.imgur.com/Z6iVa.jpg?s=128&g=1",
            "display_name": "ElRudi",
            "link": "https://stackoverflow.com/users/2302262/elrudi"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 64577500,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1603903927,
        "creation_date": 1603903066,
        "question_id": 64577375,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64577375/merge-pandas-dataframes-under-new-index-level",
        "title": "merge pandas dataframes under new index level",
        "body": "<p>I have 2 <code>pandas</code> <code>DataFrame</code>s <code>act</code> and <code>exp</code> that I want to combine into a single dataframe <code>df</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom numpy.random import rand\nact = pd.DataFrame(rand(3,2), columns=['a', 'b'])\nexp = pd.DataFrame(rand(3,2), columns=['a', 'c'])\n</code></pre>\n<pre><code>act #have\n\n          a         b\n0  0.853910  0.405463\n1  0.822641  0.255832\n2  0.673718  0.313768\n\nexp #have\n\n          a         c\n0  0.464781  0.325553\n1  0.565531  0.269678\n2  0.363693  0.775927\n</code></pre>\n<p>Dataframe <code>df</code> should contain one more column index level than <code>act</code> and <code>exp</code>, and contain each under its own level-0 identifier, like so:</p>\n<pre><code>df  #want\n\n        act                 exp          \n          a         b         a         c\n0  0.853910  0.405463  0.464781  0.325553\n1  0.822641  0.255832  0.565531  0.269678\n2  0.673718  0.313768  0.363693  0.775927\n</code></pre>\n<p>Any ideas as to how to do this?</p>\n<hr />\n<p>It's a bit like <code>merge</code>ing the two frames:</p>\n<pre><code>act.merge(exp, left_index=True, right_index=True, suffixes=['_act', '_exp'])\n\n      a_act         b     a_exp         c\n0  0.853910  0.405463  0.464781  0.325553\n1  0.822641  0.255832  0.565531  0.269678\n2  0.673718  0.313768  0.363693  0.775927\n</code></pre>\n<p>...but using an additional level, instead of a suffix, to prevent name collisions.</p>\n<p>I tried:</p>\n<pre><code>#not working\npd.DataFrame({'act': act, 'exp':exp})  \n</code></pre>\n<p>I could use loops to build up the <code>df</code> series-by-series, but that doesn't seem right.</p>\n<p>Many thanks.</p>\n",
        "answer_body": "<p>May be you can try using <code>concat</code>:</p>\n<pre><code>pd.concat([act, exp], axis=1, keys=['act', 'exp'])\n</code></pre>\n<p>Result:</p>\n<pre><code>          act                      exp\n       a           b             a           c\n0   0.604027    0.933399    0.830059    0.317602\n1   0.992192    0.991513    0.397223    0.904166\n2   0.382579    0.981182    0.862077    0.239373\n</code></pre>\n",
        "question_body": "<p>I have 2 <code>pandas</code> <code>DataFrame</code>s <code>act</code> and <code>exp</code> that I want to combine into a single dataframe <code>df</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom numpy.random import rand\nact = pd.DataFrame(rand(3,2), columns=['a', 'b'])\nexp = pd.DataFrame(rand(3,2), columns=['a', 'c'])\n</code></pre>\n<pre><code>act #have\n\n          a         b\n0  0.853910  0.405463\n1  0.822641  0.255832\n2  0.673718  0.313768\n\nexp #have\n\n          a         c\n0  0.464781  0.325553\n1  0.565531  0.269678\n2  0.363693  0.775927\n</code></pre>\n<p>Dataframe <code>df</code> should contain one more column index level than <code>act</code> and <code>exp</code>, and contain each under its own level-0 identifier, like so:</p>\n<pre><code>df  #want\n\n        act                 exp          \n          a         b         a         c\n0  0.853910  0.405463  0.464781  0.325553\n1  0.822641  0.255832  0.565531  0.269678\n2  0.673718  0.313768  0.363693  0.775927\n</code></pre>\n<p>Any ideas as to how to do this?</p>\n<hr />\n<p>It's a bit like <code>merge</code>ing the two frames:</p>\n<pre><code>act.merge(exp, left_index=True, right_index=True, suffixes=['_act', '_exp'])\n\n      a_act         b     a_exp         c\n0  0.853910  0.405463  0.464781  0.325553\n1  0.822641  0.255832  0.565531  0.269678\n2  0.673718  0.313768  0.363693  0.775927\n</code></pre>\n<p>...but using an additional level, instead of a suffix, to prevent name collisions.</p>\n<p>I tried:</p>\n<pre><code>#not working\npd.DataFrame({'act': act, 'exp':exp})  \n</code></pre>\n<p>I could use loops to build up the <code>df</code> series-by-series, but that doesn't seem right.</p>\n<p>Many thanks.</p>\n",
        "formatted_input": {
            "qid": 64577375,
            "link": "https://stackoverflow.com/questions/64577375/merge-pandas-dataframes-under-new-index-level",
            "question": {
                "title": "merge pandas dataframes under new index level",
                "ques_desc": "I have 2 s and that I want to combine into a single dataframe : Dataframe should contain one more column index level than and , and contain each under its own level-0 identifier, like so: Any ideas as to how to do this? It's a bit like ing the two frames: ...but using an additional level, instead of a suffix, to prevent name collisions. I tried: I could use loops to build up the series-by-series, but that doesn't seem right. Many thanks. "
            },
            "io": [
                "act #have\n\n          a         b\n0  0.853910  0.405463\n1  0.822641  0.255832\n2  0.673718  0.313768\n\nexp #have\n\n          a         c\n0  0.464781  0.325553\n1  0.565531  0.269678\n2  0.363693  0.775927\n",
                "df  #want\n\n        act                 exp          \n          a         b         a         c\n0  0.853910  0.405463  0.464781  0.325553\n1  0.822641  0.255832  0.565531  0.269678\n2  0.673718  0.313768  0.363693  0.775927\n"
            ],
            "answer": {
                "ans_desc": "May be you can try using : Result: ",
                "code": [
                    "pd.concat([act, exp], axis=1, keys=['act', 'exp'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "count"
        ],
        "owner": {
            "reputation": 105,
            "user_id": 14260968,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d457b5fa03be9a666bface317e85715b?s=128&d=identicon&r=PG&f=1",
            "display_name": "Moshe",
            "link": "https://stackoverflow.com/users/14260968/moshe"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 64526509,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1603646713,
        "creation_date": 1603644006,
        "last_edit_date": 1603646713,
        "question_id": 64526205,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64526205/pandas-loop-to-numpy-numpy-count-occurrences-of-string-as-nonzero-in-array",
        "title": "Pandas loop to numpy . Numpy count occurrences of string as nonzero in array",
        "body": "<p>Suppose I have the following dataframe with element types in brackets</p>\n<pre><code>  Column1(int) Column2(str)  Column3(str)\n0     2             02            34\n1     2             34            02\n2     2             80            85\n3     2             91            09\n4     2             09            34\n</code></pre>\n<p>When using pandas loops I use the following code. If <code>Column1 = 2, count how many times Column2  occurs in Column 3 and assign the count() to Column4</code> :</p>\n<pre><code>import pandas as pd\n\nfor index in df.index:\n    if df.loc[index, &quot;Column&quot;] == 2:\n        df.loc[index, &quot;Column4&quot;] = df.loc[\n            df.Column3 == df.loc[index, &quot;Column2&quot;], &quot;Column3&quot;\n        ].count()\n</code></pre>\n<p>I am trying to use NumPy and array methods for efficiency. I have tried translating the method but no luck.</p>\n<pre><code>import numpy as np\n\n# turn Column3 to array\narray = df.loc[:, &quot;Column3&quot;].values\n\nindex = df.index\ndf.assign(\n    Column4=lambda x: np.where(\n        (x[&quot;Column1&quot;] == 2), np.count_nonzero(array == df.loc[index, &quot;Column2&quot;]), &quot;F&quot;\n    )\n)\n</code></pre>\n<blockquote>\n<p>Expected output</p>\n</blockquote>\n<pre><code>  Column1(int) Column2(str)  Column3(str)  Column4(int)\n0     2             02            34           1\n1     2             34            02           2\n2     2             80            85           0\n3     2             91            09           0\n4     2             09            34           1\n</code></pre>\n",
        "answer_body": "<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\" rel=\"nofollow noreferrer\"><strong><code>pd.Series.value_counts</code></strong></a> on <code>Column3</code> and use it as mapping for <code>Column2</code>, you can pass <code>Series</code> object to <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\" rel=\"nofollow noreferrer\"><strong><code>pd.Series.map</code></strong></a>, missing values with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html\" rel=\"nofollow noreferrer\"><strong><code>pd.Series.fillna</code></strong></a> with <code>0</code></p>\n<pre><code>s = df['Column2'].map(df['Column3'].value_counts()).fillna(0)\ndf.loc[df['Column1'].eq(2), 'Column4'] = s\ndf['Column4'] = df['Column4'].fillna('F') \n# Fills with 'F' where `Column1` is not equal to 2.\n\n   Column1  Column2  Column3  Column4\n0        2        2       34      1.0\n1        2       34        2      2.0\n2        2       80       85      0.0\n3        2       91        9      0.0\n4        2        9       34      1.0\n</code></pre>\n<p>Or you can use <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.where.html\" rel=\"nofollow noreferrer\"><strong><code>np.where</code></strong></a> here.</p>\n<pre><code>s = df['Column2'].map(df['Column3'].value_counts()).fillna(0)\ndf['Column4'] = np.where(df['Column1'].eq(2), s, 'F')\n</code></pre>\n",
        "question_body": "<p>Suppose I have the following dataframe with element types in brackets</p>\n<pre><code>  Column1(int) Column2(str)  Column3(str)\n0     2             02            34\n1     2             34            02\n2     2             80            85\n3     2             91            09\n4     2             09            34\n</code></pre>\n<p>When using pandas loops I use the following code. If <code>Column1 = 2, count how many times Column2  occurs in Column 3 and assign the count() to Column4</code> :</p>\n<pre><code>import pandas as pd\n\nfor index in df.index:\n    if df.loc[index, &quot;Column&quot;] == 2:\n        df.loc[index, &quot;Column4&quot;] = df.loc[\n            df.Column3 == df.loc[index, &quot;Column2&quot;], &quot;Column3&quot;\n        ].count()\n</code></pre>\n<p>I am trying to use NumPy and array methods for efficiency. I have tried translating the method but no luck.</p>\n<pre><code>import numpy as np\n\n# turn Column3 to array\narray = df.loc[:, &quot;Column3&quot;].values\n\nindex = df.index\ndf.assign(\n    Column4=lambda x: np.where(\n        (x[&quot;Column1&quot;] == 2), np.count_nonzero(array == df.loc[index, &quot;Column2&quot;]), &quot;F&quot;\n    )\n)\n</code></pre>\n<blockquote>\n<p>Expected output</p>\n</blockquote>\n<pre><code>  Column1(int) Column2(str)  Column3(str)  Column4(int)\n0     2             02            34           1\n1     2             34            02           2\n2     2             80            85           0\n3     2             91            09           0\n4     2             09            34           1\n</code></pre>\n",
        "formatted_input": {
            "qid": 64526205,
            "link": "https://stackoverflow.com/questions/64526205/pandas-loop-to-numpy-numpy-count-occurrences-of-string-as-nonzero-in-array",
            "question": {
                "title": "Pandas loop to numpy . Numpy count occurrences of string as nonzero in array",
                "ques_desc": "Suppose I have the following dataframe with element types in brackets When using pandas loops I use the following code. If : I am trying to use NumPy and array methods for efficiency. I have tried translating the method but no luck. Expected output "
            },
            "io": [
                "  Column1(int) Column2(str)  Column3(str)\n0     2             02            34\n1     2             34            02\n2     2             80            85\n3     2             91            09\n4     2             09            34\n",
                "  Column1(int) Column2(str)  Column3(str)  Column4(int)\n0     2             02            34           1\n1     2             34            02           2\n2     2             80            85           0\n3     2             91            09           0\n4     2             09            34           1\n"
            ],
            "answer": {
                "ans_desc": "You can use on and use it as mapping for , you can pass object to , missing values with with Or you can use here. ",
                "code": [
                    "s = df['Column2'].map(df['Column3'].value_counts()).fillna(0)\ndf['Column4'] = np.where(df['Column1'].eq(2), s, 'F')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "join",
            "merge"
        ],
        "owner": {
            "reputation": 75,
            "user_id": 12842105,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/78138cda66af1337dc98d5e0e96d16dc?s=128&d=identicon&r=PG&f=1",
            "display_name": "Neel",
            "link": "https://stackoverflow.com/users/12842105/neel"
        },
        "is_answered": true,
        "view_count": 58,
        "accepted_answer_id": 64525111,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1603637674,
        "creation_date": 1603635948,
        "last_edit_date": 1603636537,
        "question_id": 64524795,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64524795/how-to-create-a-new-column-based-on-matching-ids-and-strings-in-names-of-other",
        "title": "How to create a new column based on matching ID&#39;s and string&#39;s in names of other columns in the same data frame?",
        "body": "<p>I have tried to find a solution online but I cannot. I have a dataframe with 10 separate id columns, and 10 separate corresponding value columns for each ID. A brief example is shown below</p>\n<p>Example:</p>\n<pre><code>player_id_1    player_1_x   player_id_2   player_2_x  shooter_id \n\n300               10           301           12           301\n\n299               11           300           13           299\n\n</code></pre>\n<p>I want to create a new column that takes the value column from the corresponding match of ID's between the 'shooter_id' and any of the 'player_id' columns like below:</p>\n<pre><code>player_id_1    player_1_x   player_id_2   player_2_x  shooter_id  shooter_x\n\n300               10           301           12           301         12 \n\n299               11           300           13           299         11\n\n</code></pre>\n<p>I have really been struggling to make this work, I am not sure if I need to merge within itself, for loop through the dataframe, or .apply.. any insight would be very helpful!</p>\n<p>Thank you!</p>\n",
        "answer_body": "<p>Let's <code>filter</code> the <code>player_id</code> like columns, the use <code>.eq</code> + <code>idxmax</code> to  get the <code>player_id</code> columns where the match is found, finally use <code>lookup</code> to get values corresponding to <code>player_id's</code>:</p>\n<pre><code>c = df.filter(like='player_id')\\\n      .eq(df['shooter_id'], axis=0)\\\n      .idxmax(1).str.replace('_id', '').add('_x')\n\ndf['shooter_x'] = df.lookup(df.index, c)\n</code></pre>\n<hr />\n<pre><code>   player_id_1  player_1_x  player_id_2  player_2_x  shooter_id  shooter_x\n0          300          10          301          12         301         12\n1          299          11          300          13         299         11\n</code></pre>\n",
        "question_body": "<p>I have tried to find a solution online but I cannot. I have a dataframe with 10 separate id columns, and 10 separate corresponding value columns for each ID. A brief example is shown below</p>\n<p>Example:</p>\n<pre><code>player_id_1    player_1_x   player_id_2   player_2_x  shooter_id \n\n300               10           301           12           301\n\n299               11           300           13           299\n\n</code></pre>\n<p>I want to create a new column that takes the value column from the corresponding match of ID's between the 'shooter_id' and any of the 'player_id' columns like below:</p>\n<pre><code>player_id_1    player_1_x   player_id_2   player_2_x  shooter_id  shooter_x\n\n300               10           301           12           301         12 \n\n299               11           300           13           299         11\n\n</code></pre>\n<p>I have really been struggling to make this work, I am not sure if I need to merge within itself, for loop through the dataframe, or .apply.. any insight would be very helpful!</p>\n<p>Thank you!</p>\n",
        "formatted_input": {
            "qid": 64524795,
            "link": "https://stackoverflow.com/questions/64524795/how-to-create-a-new-column-based-on-matching-ids-and-strings-in-names-of-other",
            "question": {
                "title": "How to create a new column based on matching ID&#39;s and string&#39;s in names of other columns in the same data frame?",
                "ques_desc": "I have tried to find a solution online but I cannot. I have a dataframe with 10 separate id columns, and 10 separate corresponding value columns for each ID. A brief example is shown below Example: I want to create a new column that takes the value column from the corresponding match of ID's between the 'shooter_id' and any of the 'player_id' columns like below: I have really been struggling to make this work, I am not sure if I need to merge within itself, for loop through the dataframe, or .apply.. any insight would be very helpful! Thank you! "
            },
            "io": [
                "player_id_1    player_1_x   player_id_2   player_2_x  shooter_id \n\n300               10           301           12           301\n\n299               11           300           13           299\n\n",
                "player_id_1    player_1_x   player_id_2   player_2_x  shooter_id  shooter_x\n\n300               10           301           12           301         12 \n\n299               11           300           13           299         11\n\n"
            ],
            "answer": {
                "ans_desc": "Let's the like columns, the use + to get the columns where the match is found, finally use to get values corresponding to : ",
                "code": [
                    "c = df.filter(like='player_id')\\\n      .eq(df['shooter_id'], axis=0)\\\n      .idxmax(1).str.replace('_id', '').add('_x')\n\ndf['shooter_x'] = df.lookup(df.index, c)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "sorting"
        ],
        "owner": {
            "reputation": 3332,
            "user_id": 9795817,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/5ivtG.png?s=128&g=1",
            "display_name": "Arturo Sbr",
            "link": "https://stackoverflow.com/users/9795817/arturo-sbr"
        },
        "is_answered": true,
        "view_count": 50,
        "accepted_answer_id": 62778802,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1603571712,
        "creation_date": 1594135729,
        "question_id": 62778713,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62778713/drop-rows-and-sort-one-dataframe-according-to-another",
        "title": "Drop rows and sort one dataframe according to another",
        "body": "<p>I have two pandas data frames (<code>df1</code> and <code>df2</code>):</p>\n<pre class=\"lang-py prettyprint-override\"><code># df1\n  ID  COL\n   1    A\n   2    F\n   2    A\n   3    A\n   3    S\n   3    D\n   4    D\n\n# df2\n  ID  VAL\n   1    1\n   2    0\n   3    0\n   3    1\n   4    0\n</code></pre>\n<p>My goal is to append the corresponding <code>val</code> from <code>df2</code> to each <code>ID</code> in <code>df1</code>. However, the relationship is not one-to-one (this is my client's fault and there's nothing I can do about this). To solve this problem, I want to sort <code>df1</code> by <code>df2['ID']</code> such that <code>df1['ID']</code> is identical to <code>df2['ID']</code>.</p>\n<p>So basically, for any row <code>i</code> in 0 to <code>len(df2)</code>:</p>\n<ul>\n<li>if <code>df1.loc[i, 'ID'] == df2.loc[i, 'ID']</code> then keep row <code>i</code> in <code>df1</code>.</li>\n<li>if <code>df1.loc[i, 'ID'] != df2.loc[i, 'ID']</code> then drop row <code>i</code> from <code>df1</code> and repeat.</li>\n</ul>\n<p>The desired result is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>  ID  COL\n   1    A\n   2    F\n   3    A\n   3    S\n   4    D\n</code></pre>\n<p>This way, I can use <code>pandas.concat([df1, df2['ID']], axis=0)</code> to assign <code>df2[VAL]</code> to <code>df1</code>.</p>\n<p>Is there a standardized way to do this? Does <code>pandas.merge()</code> have a method for doing this?</p>\n<p>Before this gets voted as a duplicate, please realize that <code>len(df1) != len(df2)</code>, so <a href=\"https://stackoverflow.com/questions/45576800/how-to-sort-dataframe-based-on-a-column-in-another-dataframe-in-pandas\">threads like this</a> are not quite what I'm looking for.</p>\n",
        "answer_body": "<p>This can be done with merge on both <code>ID</code> and the order within each <code>ID</code>:</p>\n<pre><code>(df1.assign(idx=df1.groupby('ID').cumcount())\n    .merge(df2.assign(idx=df2.groupby('ID').cumcount()),\n           on=['ID','idx'],\n           suffixes=['','_drop'])\n    [df1.columns]\n)\n</code></pre>\n<p>Output:</p>\n<pre><code>   ID COL\n0   1   A\n1   2   F\n2   3   A\n3   3   S\n4   4   D\n</code></pre>\n",
        "question_body": "<p>I have two pandas data frames (<code>df1</code> and <code>df2</code>):</p>\n<pre class=\"lang-py prettyprint-override\"><code># df1\n  ID  COL\n   1    A\n   2    F\n   2    A\n   3    A\n   3    S\n   3    D\n   4    D\n\n# df2\n  ID  VAL\n   1    1\n   2    0\n   3    0\n   3    1\n   4    0\n</code></pre>\n<p>My goal is to append the corresponding <code>val</code> from <code>df2</code> to each <code>ID</code> in <code>df1</code>. However, the relationship is not one-to-one (this is my client's fault and there's nothing I can do about this). To solve this problem, I want to sort <code>df1</code> by <code>df2['ID']</code> such that <code>df1['ID']</code> is identical to <code>df2['ID']</code>.</p>\n<p>So basically, for any row <code>i</code> in 0 to <code>len(df2)</code>:</p>\n<ul>\n<li>if <code>df1.loc[i, 'ID'] == df2.loc[i, 'ID']</code> then keep row <code>i</code> in <code>df1</code>.</li>\n<li>if <code>df1.loc[i, 'ID'] != df2.loc[i, 'ID']</code> then drop row <code>i</code> from <code>df1</code> and repeat.</li>\n</ul>\n<p>The desired result is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>  ID  COL\n   1    A\n   2    F\n   3    A\n   3    S\n   4    D\n</code></pre>\n<p>This way, I can use <code>pandas.concat([df1, df2['ID']], axis=0)</code> to assign <code>df2[VAL]</code> to <code>df1</code>.</p>\n<p>Is there a standardized way to do this? Does <code>pandas.merge()</code> have a method for doing this?</p>\n<p>Before this gets voted as a duplicate, please realize that <code>len(df1) != len(df2)</code>, so <a href=\"https://stackoverflow.com/questions/45576800/how-to-sort-dataframe-based-on-a-column-in-another-dataframe-in-pandas\">threads like this</a> are not quite what I'm looking for.</p>\n",
        "formatted_input": {
            "qid": 62778713,
            "link": "https://stackoverflow.com/questions/62778713/drop-rows-and-sort-one-dataframe-according-to-another",
            "question": {
                "title": "Drop rows and sort one dataframe according to another",
                "ques_desc": "I have two pandas data frames ( and ): My goal is to append the corresponding from to each in . However, the relationship is not one-to-one (this is my client's fault and there's nothing I can do about this). To solve this problem, I want to sort by such that is identical to . So basically, for any row in 0 to : if then keep row in . if then drop row from and repeat. The desired result is: This way, I can use to assign to . Is there a standardized way to do this? Does have a method for doing this? Before this gets voted as a duplicate, please realize that , so threads like this are not quite what I'm looking for. "
            },
            "io": [
                "# df1\n  ID  COL\n   1    A\n   2    F\n   2    A\n   3    A\n   3    S\n   3    D\n   4    D\n\n# df2\n  ID  VAL\n   1    1\n   2    0\n   3    0\n   3    1\n   4    0\n",
                "  ID  COL\n   1    A\n   2    F\n   3    A\n   3    S\n   4    D\n"
            ],
            "answer": {
                "ans_desc": "This can be done with merge on both and the order within each : Output: ",
                "code": [
                    "(df1.assign(idx=df1.groupby('ID').cumcount())\n    .merge(df2.assign(idx=df2.groupby('ID').cumcount()),\n           on=['ID','idx'],\n           suffixes=['','_drop'])\n    [df1.columns]\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 151,
            "user_id": 9527637,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/8SWLS.jpg?s=128&g=1",
            "display_name": "Ayesh Weerasinghe",
            "link": "https://stackoverflow.com/users/9527637/ayesh-weerasinghe"
        },
        "is_answered": true,
        "view_count": 126,
        "accepted_answer_id": 62006174,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1603552011,
        "creation_date": 1590421927,
        "last_edit_date": 1590422172,
        "question_id": 62006098,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62006098/checking-for-nans-in-many-columns-in-pandas",
        "title": "Checking for NaNs in many columns in Pandas",
        "body": "<p>I want to add a binary column to my dataframe based on whether given columns contain NaN or not.</p>\n\n<p>I have tried to do it with the below code.</p>\n\n<pre><code>import pandas as pd\n\ndat = pd.DataFrame({'A': [12,34,56,78, 23,None, None], 'B': [90,80,70,23,None, 78, None], 'C': [90,80,70,23,None, 78, None], 'D': [12,34,56,78, 23,None, None]})\ndat['A1'] = dat['A'].isnull()\ndat['B1'] = dat['B'].isnull()\ndat['C1'] = dat['C'].isnull()\ndat['ismissing'] = 1 if dat['A1'] == True and dat['B1'] == True and dat['C1'] == True else 0\ndat\n</code></pre>\n\n<p>but I got a ValueError at the line before last.</p>\n\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n\n<p>Sample input:</p>\n\n<pre><code>A     B     C     D\n10   NaN    40    NaN\nNaN  NaN    80    90\n20    45    NaN   89\nNaN  NaN    NaN   46\n</code></pre>\n\n<p>Expected output:</p>\n\n<pre><code>A     B     C     D     E\n10   NaN    40    NaN   0\nNaN  NaN    80    90    0\n20    45    NaN   89    0\nNaN  NaN    NaN   46    1\n</code></pre>\n\n<p>I want to check NaNs only for A, B, C columns.</p>\n",
        "answer_body": "<p>You want to check whether a row with columns(<code>A,B,C</code>) has all <code>nan</code> or not.</p>\n<p>You can do this using <a href=\"https://numpy.org/doc/1.18/reference/generated/numpy.where.html\" rel=\"nofollow noreferrer\"><code>numpy.where</code></a>:</p>\n<pre><code>In [1711]: import numpy as np\n\nIn [1710]: dat['E'] = np.where(dat[['A','B','C']].isnull().all(1), 1, 0)    \nIn [1711]: dat\nOut[1711]: \n      A     B     C     D  E\n0  12.0  90.0  90.0  12.0  0\n1  34.0  80.0  80.0  34.0  0\n2  56.0  70.0  70.0  56.0  0\n3  78.0  23.0  23.0  78.0  0\n4  23.0   NaN   NaN  23.0  0\n5   NaN  78.0  78.0   NaN  0\n6   NaN   NaN   NaN   NaN  1\n</code></pre>\n<p>Performance comparison:</p>\n<p>Quang Hoang's answer:</p>\n<pre><code>In [1720]: %timeit df['ismissing'] = df[['A','B','C']].isna().all(axis=1)\n989 \u00b5s \u00b1 70 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p>YOBEN_S's answer:</p>\n<pre><code>In [1719]: %timeit df['New']=~df.index.isin(df.drop('D',1).dropna(thresh=1).index)\n2.05 ms \u00b1 113 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p>anky's answer:</p>\n<pre><code>In [1724]: %timeit df['all_nan'] = df[['A','B','C']].count(axis=1).eq(0).view('i1')\n1.48 ms \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p>My answer:</p>\n<pre><code>In [1723]: %timeit dat['E'] = np.where(dat[['A','B','C']].isnull().all(1), 1, 0)\n914 \u00b5s \u00b1 18.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p>As you can see, my answer with <code>np.where</code> is the fastest.</p>\n",
        "question_body": "<p>I want to add a binary column to my dataframe based on whether given columns contain NaN or not.</p>\n\n<p>I have tried to do it with the below code.</p>\n\n<pre><code>import pandas as pd\n\ndat = pd.DataFrame({'A': [12,34,56,78, 23,None, None], 'B': [90,80,70,23,None, 78, None], 'C': [90,80,70,23,None, 78, None], 'D': [12,34,56,78, 23,None, None]})\ndat['A1'] = dat['A'].isnull()\ndat['B1'] = dat['B'].isnull()\ndat['C1'] = dat['C'].isnull()\ndat['ismissing'] = 1 if dat['A1'] == True and dat['B1'] == True and dat['C1'] == True else 0\ndat\n</code></pre>\n\n<p>but I got a ValueError at the line before last.</p>\n\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n\n<p>Sample input:</p>\n\n<pre><code>A     B     C     D\n10   NaN    40    NaN\nNaN  NaN    80    90\n20    45    NaN   89\nNaN  NaN    NaN   46\n</code></pre>\n\n<p>Expected output:</p>\n\n<pre><code>A     B     C     D     E\n10   NaN    40    NaN   0\nNaN  NaN    80    90    0\n20    45    NaN   89    0\nNaN  NaN    NaN   46    1\n</code></pre>\n\n<p>I want to check NaNs only for A, B, C columns.</p>\n",
        "formatted_input": {
            "qid": 62006098,
            "link": "https://stackoverflow.com/questions/62006098/checking-for-nans-in-many-columns-in-pandas",
            "question": {
                "title": "Checking for NaNs in many columns in Pandas",
                "ques_desc": "I want to add a binary column to my dataframe based on whether given columns contain NaN or not. I have tried to do it with the below code. but I got a ValueError at the line before last. Sample input: Expected output: I want to check NaNs only for A, B, C columns. "
            },
            "io": [
                "A     B     C     D\n10   NaN    40    NaN\nNaN  NaN    80    90\n20    45    NaN   89\nNaN  NaN    NaN   46\n",
                "A     B     C     D     E\n10   NaN    40    NaN   0\nNaN  NaN    80    90    0\n20    45    NaN   89    0\nNaN  NaN    NaN   46    1\n"
            ],
            "answer": {
                "ans_desc": "You want to check whether a row with columns() has all or not. You can do this using : Performance comparison: Quang Hoang's answer: YOBEN_S's answer: anky's answer: My answer: As you can see, my answer with is the fastest. ",
                "code": [
                    "In [1720]: %timeit df['ismissing'] = df[['A','B','C']].isna().all(axis=1)\n989 \u00b5s \u00b1 70 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n",
                    "In [1719]: %timeit df['New']=~df.index.isin(df.drop('D',1).dropna(thresh=1).index)\n2.05 ms \u00b1 113 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n",
                    "In [1724]: %timeit df['all_nan'] = df[['A','B','C']].count(axis=1).eq(0).view('i1')\n1.48 ms \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n",
                    "In [1723]: %timeit dat['E'] = np.where(dat[['A','B','C']].isnull().all(1), 1, 0)\n914 \u00b5s \u00b1 18.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2203,
            "user_id": 3157428,
            "user_type": "registered",
            "accept_rate": 88,
            "profile_image": "https://i.stack.imgur.com/NdasB.jpg?s=128&g=1",
            "display_name": "Ruli",
            "link": "https://stackoverflow.com/users/3157428/ruli"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 64483469,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1603373829,
        "creation_date": 1603285453,
        "question_id": 64464184,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64464184/select-highest-member-of-close-coordinates-saved-in-pandas-dataframe",
        "title": "Select highest member of close coordinates saved in pandas dataframe",
        "body": "<p>I have a dataframe that has following columns: X and Y are Cartesian coordinates and Value is the value of element at these coordinates. What I want to achieve is to select only one coordinates out of <code>n</code> that are close to other, lets say coordinates are close if distance is lower than some value <code>m</code>, so the initial DF looks like this (example):</p>\n<pre><code>data = {'X':[0,0,0,1,1,5,6,7,8],'Y':[0,1,4,2,6,5,6,4,8],'Value':[6,7,4,5,6,5,6,4,8]}\ndf = pd.DataFrame(data)\n</code></pre>\n<blockquote>\n<pre><code>    X  Y  Value\n0   0  0      6\n1   0  1      7\n2   0  4      4\n3   1  2      5\n4   1  6      6\n5   5  5      5\n6   6  6      6\n7   7  4      4\n8   8  8      8\n</code></pre>\n</blockquote>\n<p>distance is count with following function:</p>\n<pre><code>def countDistance(lat1, lon1, lat2, lon2):\n    #use basic knowledge about triangles - values are in meters\n    distance = sqrt(pow(lat1-lat2,2)+pow(lon1-lon2,2))\n    return distance\n</code></pre>\n<p>lets say if we want to <code>m&lt;=3</code>, the output dataframe would look like this:</p>\n<blockquote>\n<pre><code>    X  Y  Value\n1   0  1      7\n4   1  6      6\n8   8  8      8\n</code></pre>\n</blockquote>\n<p>What is to be done:</p>\n<blockquote>\n<pre><code>rows 0,1,3 are close, highest value is in row 1, continue\nrows 2 and 4 (from  original df) are close, keep row 4\nrows 5,6,7 are close, keep row 6\nleft over row 6 is close to row 8, keep row 8, has higher value\n</code></pre>\n</blockquote>\n<p>So I need to go through dataframe row by row, check the rest, select best match and then continue. I can't think about any simple method how to achieve this, this cant be use case of <code>drop_duplicates</code>, since they are not duplicates, but looping over the whole DF will be very inefficient. One method I could think about was to loop just once, for each of rows finds close ones (probably apply countdistance()), select the best fitting row and replace rest with its values, in the end use <code>drop_duplicates</code>. The other idea was to create a recursive function that would create a new DF, then while original df will have rows select first, find close ones, best match append to new DF, remove first row and all close from original DF and continue until empty, then return same function with new DF as to remove possible uncaught close points.</p>\n<p>These ideas are all kind of inefficient, is there a nice and efficient pythonic way to achieve this?</p>\n",
        "answer_body": "<p>For now, I have created simple code with recursion, the code works but is most likely not optimal.</p>\n<pre><code>def recModif(self,df):\n  #columns=['','X','Y','Value']\n  new_df = df.copy()\n  new_df = new_df[new_df['Value']&lt;0] #create copy to work with\n  changed = False\n  while not df.empty: #for all the data\n    df = df.reset_index(drop=True) #need to reset so 0 is always accessible\n    x = df.loc[0,'X'] #first row x and y\n    y = df.loc[0,'Y']\n    df['dist'] = self.countDistance(x,y,df['X'],df['Y']) #add column with distances\n    select = df[df['dist']&lt;10] #number of meters that two elements cant be next to other \n    if(len(select.index)&gt;1): #if there is more than one elem close\n      changed = True\n      #print(select,select['Value'].idxmax())\n    select = select.loc[[select['Value'].idxmax()]] #get the highest one\n    new_df = new_df.append(pd.DataFrame(select.iloc[:,:3]),ignore_index=True) #add it to new df\n    df = df[df['dist'] &gt;= 10] #drop the elements now\n\n  if changed:\n    return self.recModif(new_df) #use recursion if possible overlaps\n  else: \n    return new_df #return new df if all was OK\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that has following columns: X and Y are Cartesian coordinates and Value is the value of element at these coordinates. What I want to achieve is to select only one coordinates out of <code>n</code> that are close to other, lets say coordinates are close if distance is lower than some value <code>m</code>, so the initial DF looks like this (example):</p>\n<pre><code>data = {'X':[0,0,0,1,1,5,6,7,8],'Y':[0,1,4,2,6,5,6,4,8],'Value':[6,7,4,5,6,5,6,4,8]}\ndf = pd.DataFrame(data)\n</code></pre>\n<blockquote>\n<pre><code>    X  Y  Value\n0   0  0      6\n1   0  1      7\n2   0  4      4\n3   1  2      5\n4   1  6      6\n5   5  5      5\n6   6  6      6\n7   7  4      4\n8   8  8      8\n</code></pre>\n</blockquote>\n<p>distance is count with following function:</p>\n<pre><code>def countDistance(lat1, lon1, lat2, lon2):\n    #use basic knowledge about triangles - values are in meters\n    distance = sqrt(pow(lat1-lat2,2)+pow(lon1-lon2,2))\n    return distance\n</code></pre>\n<p>lets say if we want to <code>m&lt;=3</code>, the output dataframe would look like this:</p>\n<blockquote>\n<pre><code>    X  Y  Value\n1   0  1      7\n4   1  6      6\n8   8  8      8\n</code></pre>\n</blockquote>\n<p>What is to be done:</p>\n<blockquote>\n<pre><code>rows 0,1,3 are close, highest value is in row 1, continue\nrows 2 and 4 (from  original df) are close, keep row 4\nrows 5,6,7 are close, keep row 6\nleft over row 6 is close to row 8, keep row 8, has higher value\n</code></pre>\n</blockquote>\n<p>So I need to go through dataframe row by row, check the rest, select best match and then continue. I can't think about any simple method how to achieve this, this cant be use case of <code>drop_duplicates</code>, since they are not duplicates, but looping over the whole DF will be very inefficient. One method I could think about was to loop just once, for each of rows finds close ones (probably apply countdistance()), select the best fitting row and replace rest with its values, in the end use <code>drop_duplicates</code>. The other idea was to create a recursive function that would create a new DF, then while original df will have rows select first, find close ones, best match append to new DF, remove first row and all close from original DF and continue until empty, then return same function with new DF as to remove possible uncaught close points.</p>\n<p>These ideas are all kind of inefficient, is there a nice and efficient pythonic way to achieve this?</p>\n",
        "formatted_input": {
            "qid": 64464184,
            "link": "https://stackoverflow.com/questions/64464184/select-highest-member-of-close-coordinates-saved-in-pandas-dataframe",
            "question": {
                "title": "Select highest member of close coordinates saved in pandas dataframe",
                "ques_desc": "I have a dataframe that has following columns: X and Y are Cartesian coordinates and Value is the value of element at these coordinates. What I want to achieve is to select only one coordinates out of that are close to other, lets say coordinates are close if distance is lower than some value , so the initial DF looks like this (example): distance is count with following function: lets say if we want to , the output dataframe would look like this: What is to be done: So I need to go through dataframe row by row, check the rest, select best match and then continue. I can't think about any simple method how to achieve this, this cant be use case of , since they are not duplicates, but looping over the whole DF will be very inefficient. One method I could think about was to loop just once, for each of rows finds close ones (probably apply countdistance()), select the best fitting row and replace rest with its values, in the end use . The other idea was to create a recursive function that would create a new DF, then while original df will have rows select first, find close ones, best match append to new DF, remove first row and all close from original DF and continue until empty, then return same function with new DF as to remove possible uncaught close points. These ideas are all kind of inefficient, is there a nice and efficient pythonic way to achieve this? "
            },
            "io": [
                "    X  Y  Value\n0   0  0      6\n1   0  1      7\n2   0  4      4\n3   1  2      5\n4   1  6      6\n5   5  5      5\n6   6  6      6\n7   7  4      4\n8   8  8      8\n",
                "    X  Y  Value\n1   0  1      7\n4   1  6      6\n8   8  8      8\n"
            ],
            "answer": {
                "ans_desc": "For now, I have created simple code with recursion, the code works but is most likely not optimal. ",
                "code": [
                    "def recModif(self,df):\n  #columns=['','X','Y','Value']\n  new_df = df.copy()\n  new_df = new_df[new_df['Value']<0] #create copy to work with\n  changed = False\n  while not df.empty: #for all the data\n    df = df.reset_index(drop=True) #need to reset so 0 is always accessible\n    x = df.loc[0,'X'] #first row x and y\n    y = df.loc[0,'Y']\n    df['dist'] = self.countDistance(x,y,df['X'],df['Y']) #add column with distances\n    select = df[df['dist']<10] #number of meters that two elements cant be next to other \n    if(len(select.index)>1): #if there is more than one elem close\n      changed = True\n      #print(select,select['Value'].idxmax())\n    select = select.loc[[select['Value'].idxmax()]] #get the highest one\n    new_df = new_df.append(pd.DataFrame(select.iloc[:,:3]),ignore_index=True) #add it to new df\n    df = df[df['dist'] >= 10] #drop the elements now\n\n  if changed:\n    return self.recModif(new_df) #use recursion if possible overlaps\n  else: \n    return new_df #return new df if all was OK\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary",
            "column-types"
        ],
        "owner": {
            "reputation": 2150,
            "user_id": 3874475,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://www.gravatar.com/avatar/63aa718c1aba9a69f4aa8636f0865523?s=128&d=identicon&r=PG&f=1",
            "display_name": "Heikki",
            "link": "https://stackoverflow.com/users/3874475/heikki"
        },
        "is_answered": true,
        "view_count": 290,
        "accepted_answer_id": 64452377,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1603223395,
        "creation_date": 1603211712,
        "question_id": 64449526,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64449526/convert-dictionary-of-dictionaries-to-dataframe-with-data-types",
        "title": "Convert dictionary of dictionaries to dataframe with data types",
        "body": "<p>What is the preferred way to convert dictionary of dictionaries into a data frame with data types?</p>\n<p>I have the following kind of dictionary <code>r</code> which contains fact sets behind each key</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\nr = { 1:{'a':1,'b':2,'c':'b'},\n      2:{'d':1,'b':1,'c':'b'},\n      3:{'e':0} }\n</code></pre>\n<p>Converting this dictionary of dictionaries into a dataframe can be done in a quite straightforward way</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = pd.DataFrame(r)\nx\nx.dtypes\n</code></pre>\n<p>which yields the following version on the original dictionary of dictionaries</p>\n<pre><code>     1    2    3\na    1  NaN  NaN\nb    2    1  NaN\nc    b    e  NaN\nd  NaN    1  NaN\ne  NaN  NaN  0.0\n</code></pre>\n<p>and the following datatypes for columns</p>\n<pre><code>1     object\n2     object\n3    float64\ndtype: object\n</code></pre>\n<p>However, I would like to have transposed version on <code>x</code>. After doing so</p>\n<pre class=\"lang-py prettyprint-override\"><code>y = x.transpose()\ny\ny.dtypes\n</code></pre>\n<p>it seems like the expected representation on the data is shown in matrix form</p>\n<pre><code>     a    b    c    d    e\n1    1    2    b  NaN  NaN\n2  NaN    1    e    1  NaN\n3  NaN  NaN  NaN  NaN    0\n</code></pre>\n<p>but the data types are all <code>object</code></p>\n<pre><code>a    object\nb    object\nc    object\nd    object\ne    object\ndtype: object\n</code></pre>\n<p>What is the preferred way to do such conversion from <code>r</code> to <code>y</code> so that <code>y.dtypes</code> would yield directly data types</p>\n<pre><code>a    float64\nb    float64\nc    object\nd    float64\ne    float64\ndtype: object\n</code></pre>\n<p>similar to converting <code>r</code> to <code>x</code>?</p>\n",
        "answer_body": "<p>Just set the right orientation (default is <code>columns</code>, you want <code>index</code>).</p>\n<pre><code>df = pd.DataFrame.from_dict(r, orient='index')\n</code></pre>\n<hr />\n<pre><code>a    float64\nb    float64\nc     object\nd    float64\ne    float64\ndtype: object\n</code></pre>\n",
        "question_body": "<p>What is the preferred way to convert dictionary of dictionaries into a data frame with data types?</p>\n<p>I have the following kind of dictionary <code>r</code> which contains fact sets behind each key</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\nr = { 1:{'a':1,'b':2,'c':'b'},\n      2:{'d':1,'b':1,'c':'b'},\n      3:{'e':0} }\n</code></pre>\n<p>Converting this dictionary of dictionaries into a dataframe can be done in a quite straightforward way</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = pd.DataFrame(r)\nx\nx.dtypes\n</code></pre>\n<p>which yields the following version on the original dictionary of dictionaries</p>\n<pre><code>     1    2    3\na    1  NaN  NaN\nb    2    1  NaN\nc    b    e  NaN\nd  NaN    1  NaN\ne  NaN  NaN  0.0\n</code></pre>\n<p>and the following datatypes for columns</p>\n<pre><code>1     object\n2     object\n3    float64\ndtype: object\n</code></pre>\n<p>However, I would like to have transposed version on <code>x</code>. After doing so</p>\n<pre class=\"lang-py prettyprint-override\"><code>y = x.transpose()\ny\ny.dtypes\n</code></pre>\n<p>it seems like the expected representation on the data is shown in matrix form</p>\n<pre><code>     a    b    c    d    e\n1    1    2    b  NaN  NaN\n2  NaN    1    e    1  NaN\n3  NaN  NaN  NaN  NaN    0\n</code></pre>\n<p>but the data types are all <code>object</code></p>\n<pre><code>a    object\nb    object\nc    object\nd    object\ne    object\ndtype: object\n</code></pre>\n<p>What is the preferred way to do such conversion from <code>r</code> to <code>y</code> so that <code>y.dtypes</code> would yield directly data types</p>\n<pre><code>a    float64\nb    float64\nc    object\nd    float64\ne    float64\ndtype: object\n</code></pre>\n<p>similar to converting <code>r</code> to <code>x</code>?</p>\n",
        "formatted_input": {
            "qid": 64449526,
            "link": "https://stackoverflow.com/questions/64449526/convert-dictionary-of-dictionaries-to-dataframe-with-data-types",
            "question": {
                "title": "Convert dictionary of dictionaries to dataframe with data types",
                "ques_desc": "What is the preferred way to convert dictionary of dictionaries into a data frame with data types? I have the following kind of dictionary which contains fact sets behind each key Converting this dictionary of dictionaries into a dataframe can be done in a quite straightforward way which yields the following version on the original dictionary of dictionaries and the following datatypes for columns However, I would like to have transposed version on . After doing so it seems like the expected representation on the data is shown in matrix form but the data types are all What is the preferred way to do such conversion from to so that would yield directly data types similar to converting to ? "
            },
            "io": [
                "     1    2    3\na    1  NaN  NaN\nb    2    1  NaN\nc    b    e  NaN\nd  NaN    1  NaN\ne  NaN  NaN  0.0\n",
                "     a    b    c    d    e\n1    1    2    b  NaN  NaN\n2  NaN    1    e    1  NaN\n3  NaN  NaN  NaN  NaN    0\n"
            ],
            "answer": {
                "ans_desc": "Just set the right orientation (default is , you want ). ",
                "code": [
                    "a    float64\nb    float64\nc     object\nd    float64\ne    float64\ndtype: object\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 12169025,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-XvlTe9xEFJ8/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rehDdRM1HW4cs0f89rdyNyovUZVhg/photo.jpg?sz=128",
            "display_name": "waleed khalid",
            "link": "https://stackoverflow.com/users/12169025/waleed-khalid"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 64444953,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1603222590,
        "creation_date": 1603195370,
        "question_id": 64444765,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64444765/how-to-find-the-average-of-each-cell-in-multiple-csvs",
        "title": "How to find the average of each cell in multiple csv&#39;s",
        "body": "<p>I have several excel files with data in them in a format similar to this</p>\n<pre><code>csv1             csv1\n  a b c           a b c\nx 1 2 3         x 3 2 1\ny 4 5 6         y 6 5 4\n</code></pre>\n<p>There are 3 csv's total, and I need to create a new csv with the average of each cell.\nSo <code>csv3</code> would be as follows</p>\n<pre><code>  a       b        c\nx (3+1)/2) (2+2)/2  (3+1)/2\ny (6+4)/2  etc.\n</code></pre>\n<p>So far I have the files imported but I am not sure how to proceed.</p>\n<pre><code>import pandas as pd\n\ndef Averager(fileA,fileB,fileC):\n    csvA=pd.read_csv(fileA)\n    csvB=pd.read_csv(fileB)\n    csvC=pd.read_csv(fileC)\n    g=pd.concat([csvA, csvB, csvC]).groupby(level=0).mean()\n    print(g)                                                   \nprint(Averager('a.csv','b.csv','c.csv'))\n</code></pre>\n",
        "answer_body": "<p>Since you tagged <code>numpy</code>, I'm assuming a numpy solution would work.</p>\n<pre><code>import numpy as np\ncsv1 = np.genfromtxt('my_file1.csv', delimiter=',')\ncsv2 = np.genfromtxt('my_file2.csv', delimiter=',')\nnp.savetxt(&quot;foo.csv&quot;, (csv1+csv2)/2, delimiter=&quot;,&quot;)    \n</code></pre>\n",
        "question_body": "<p>I have several excel files with data in them in a format similar to this</p>\n<pre><code>csv1             csv1\n  a b c           a b c\nx 1 2 3         x 3 2 1\ny 4 5 6         y 6 5 4\n</code></pre>\n<p>There are 3 csv's total, and I need to create a new csv with the average of each cell.\nSo <code>csv3</code> would be as follows</p>\n<pre><code>  a       b        c\nx (3+1)/2) (2+2)/2  (3+1)/2\ny (6+4)/2  etc.\n</code></pre>\n<p>So far I have the files imported but I am not sure how to proceed.</p>\n<pre><code>import pandas as pd\n\ndef Averager(fileA,fileB,fileC):\n    csvA=pd.read_csv(fileA)\n    csvB=pd.read_csv(fileB)\n    csvC=pd.read_csv(fileC)\n    g=pd.concat([csvA, csvB, csvC]).groupby(level=0).mean()\n    print(g)                                                   \nprint(Averager('a.csv','b.csv','c.csv'))\n</code></pre>\n",
        "formatted_input": {
            "qid": 64444765,
            "link": "https://stackoverflow.com/questions/64444765/how-to-find-the-average-of-each-cell-in-multiple-csvs",
            "question": {
                "title": "How to find the average of each cell in multiple csv&#39;s",
                "ques_desc": "I have several excel files with data in them in a format similar to this There are 3 csv's total, and I need to create a new csv with the average of each cell. So would be as follows So far I have the files imported but I am not sure how to proceed. "
            },
            "io": [
                "csv1             csv1\n  a b c           a b c\nx 1 2 3         x 3 2 1\ny 4 5 6         y 6 5 4\n",
                "  a       b        c\nx (3+1)/2) (2+2)/2  (3+1)/2\ny (6+4)/2  etc.\n"
            ],
            "answer": {
                "ans_desc": "Since you tagged , I'm assuming a numpy solution would work. ",
                "code": [
                    "import numpy as np\ncsv1 = np.genfromtxt('my_file1.csv', delimiter=',')\ncsv2 = np.genfromtxt('my_file2.csv', delimiter=',')\nnp.savetxt(\"foo.csv\", (csv1+csv2)/2, delimiter=\",\")    \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 14485907,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-_nbSmyB0u3A/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckG5grNqpUZGJhVuNh4U53eHsE6Rg/s96-c/photo.jpg?sz=128",
            "display_name": "Sneha",
            "link": "https://stackoverflow.com/users/14485907/sneha"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 64447212,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1603203855,
        "creation_date": 1603202707,
        "last_edit_date": 1603203037,
        "question_id": 64446862,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64446862/how-to-add-to-dataframe-column-a-dict",
        "title": "How to add to dataframe column a dict?",
        "body": "<p>Input dataframe:</p>\n<pre><code>          Id               Score            Score1\n0        19138359    0.5347029367015973   0.832428474443\n1        12134001    0.9347094453553113   0.632535428479\n</code></pre>\n<p>Following dataframe want as output:</p>\n<pre><code>          Id                             Scores\n0        19138359  {'Score': 0.5347029367015973, 'Score1': 0.832428474443}\n1        12134001  {'Score': 0.9347094453553113, 'Score1': 0.632535428479}\n</code></pre>\n<pre><code> Scores = ['Score', 'Score1']      \n d, l = {}, []\n for i, row in publish.iterrows():\n     d['Id'] = row['Id']\n     d['Scores'] = row[Scores].to_dict()\n     l.append(d)\n</code></pre>\n<p>It is giving wrong output!</p>\n",
        "answer_body": "<p>Use <code>orient=&quot;records&quot;</code> argument in <code>to_dict</code>:</p>\n<pre><code># Get score columns\nscore_columns = df.filter(like='Score').columns\n\n# Create dict of scores column\ndf['Scores'] = df[score_columns].to_dict(orient='records')\n\n# Drop original score columns\ndf.drop(columns=score_columns, inplace=True)\n</code></pre>\n<p>[out]</p>\n<pre><code>         Id                                                       Scores\n0  19138359  {'Score': 0.5347029367015973, 'Score1': 0.8324284744429999}\n1  12134001      {'Score': 0.9347094453553112, 'Score1': 0.632535428479}\n</code></pre>\n",
        "question_body": "<p>Input dataframe:</p>\n<pre><code>          Id               Score            Score1\n0        19138359    0.5347029367015973   0.832428474443\n1        12134001    0.9347094453553113   0.632535428479\n</code></pre>\n<p>Following dataframe want as output:</p>\n<pre><code>          Id                             Scores\n0        19138359  {'Score': 0.5347029367015973, 'Score1': 0.832428474443}\n1        12134001  {'Score': 0.9347094453553113, 'Score1': 0.632535428479}\n</code></pre>\n<pre><code> Scores = ['Score', 'Score1']      \n d, l = {}, []\n for i, row in publish.iterrows():\n     d['Id'] = row['Id']\n     d['Scores'] = row[Scores].to_dict()\n     l.append(d)\n</code></pre>\n<p>It is giving wrong output!</p>\n",
        "formatted_input": {
            "qid": 64446862,
            "link": "https://stackoverflow.com/questions/64446862/how-to-add-to-dataframe-column-a-dict",
            "question": {
                "title": "How to add to dataframe column a dict?",
                "ques_desc": "Input dataframe: Following dataframe want as output: It is giving wrong output! "
            },
            "io": [
                "          Id               Score            Score1\n0        19138359    0.5347029367015973   0.832428474443\n1        12134001    0.9347094453553113   0.632535428479\n",
                "          Id                             Scores\n0        19138359  {'Score': 0.5347029367015973, 'Score1': 0.832428474443}\n1        12134001  {'Score': 0.9347094453553113, 'Score1': 0.632535428479}\n"
            ],
            "answer": {
                "ans_desc": "Use argument in : [out] ",
                "code": [
                    "# Get score columns\nscore_columns = df.filter(like='Score').columns\n\n# Create dict of scores column\ndf['Scores'] = df[score_columns].to_dict(orient='records')\n\n# Drop original score columns\ndf.drop(columns=score_columns, inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 12492734,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c9083adf6d461b31c069610413fc813d?s=128&d=identicon&r=PG&f=1",
            "display_name": "jujujune",
            "link": "https://stackoverflow.com/users/12492734/jujujune"
        },
        "is_answered": true,
        "view_count": 102,
        "accepted_answer_id": 64445600,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1603199374,
        "creation_date": 1603198069,
        "question_id": 64445512,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64445512/how-to-get-the-n-most-frequent-or-top-values-per-column-in-python-pandas",
        "title": "How to get the n most frequent or top values per column in python pandas?",
        "body": "<p>my dataframe looks like:</p>\n<pre><code>df:\n    A   B\n0   a   g\n1   f   g\n2   a   g\n3   a   d\n4   h   d\n5   f   a\n\n</code></pre>\n<p>for top 2 most frequent values per column (n=2), the output should be:</p>\n<pre><code>top_df:\n    A   B\n0   a   g\n1   f   d\n\n</code></pre>\n<p>Thank you</p>\n",
        "answer_body": "<p>This should work</p>\n<pre><code>n = 2\ndf.apply(lambda x: pd.Series(x.value_counts().index[:n]))\n</code></pre>\n",
        "question_body": "<p>my dataframe looks like:</p>\n<pre><code>df:\n    A   B\n0   a   g\n1   f   g\n2   a   g\n3   a   d\n4   h   d\n5   f   a\n\n</code></pre>\n<p>for top 2 most frequent values per column (n=2), the output should be:</p>\n<pre><code>top_df:\n    A   B\n0   a   g\n1   f   d\n\n</code></pre>\n<p>Thank you</p>\n",
        "formatted_input": {
            "qid": 64445512,
            "link": "https://stackoverflow.com/questions/64445512/how-to-get-the-n-most-frequent-or-top-values-per-column-in-python-pandas",
            "question": {
                "title": "How to get the n most frequent or top values per column in python pandas?",
                "ques_desc": "my dataframe looks like: for top 2 most frequent values per column (n=2), the output should be: Thank you "
            },
            "io": [
                "df:\n    A   B\n0   a   g\n1   f   g\n2   a   g\n3   a   d\n4   h   d\n5   f   a\n\n",
                "top_df:\n    A   B\n0   a   g\n1   f   d\n\n"
            ],
            "answer": {
                "ans_desc": "This should work ",
                "code": [
                    "n = 2\ndf.apply(lambda x: pd.Series(x.value_counts().index[:n]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "mutual-friendship"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 14469059,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/84bf7c28924b24425dad5f2439b06ce6?s=128&d=identicon&r=PG&f=1",
            "display_name": "CWuu",
            "link": "https://stackoverflow.com/users/14469059/cwuu"
        },
        "is_answered": true,
        "view_count": 498,
        "accepted_answer_id": 64406567,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1602967043,
        "creation_date": 1602956817,
        "question_id": 64405516,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64405516/find-the-number-of-mutual-friends-in-python",
        "title": "Find the number of mutual friends in Python",
        "body": "<p>I have a dataframe of users and their friends that looks like:</p>\n<pre><code>user_id | friend_id\n1         3\n1         4\n2         3\n2         5\n3         4\n</code></pre>\n<p>I want to write a function in <code>python</code> to compute the number of mutual friends for each pair:</p>\n<pre><code>user_id | friend_id | num_mutual\n1         3           1\n1         4           1\n2         3           0\n2         5           0\n3         4           1\n</code></pre>\n<p>Currently I have:</p>\n<pre><code>def find_mutual(df):\n    num_mutual = []\n    for i in range(len(df)):\n        user, friend = df.loc[i, 'user_id'], df.loc[i, 'friend_id']\n        user_list = df[df.user_id == user].friend_id.tolist() + df[df.friend_id == user].user_id.tolist()\n        friend_list = df[df.user_id == friend].friend_id.tolist() + df[df.friend_id == friend].user_id.tolist()\n        mutual = len(list(set(user_list) &amp; set(friend_list)))\n        num_mutual.append(mutual)\n    return num_mutual\n</code></pre>\n<p>It works fine for small datasets, but I'm running it on a dataset with millions of rows. It takes forever to run everything. I know it's not the ideal way to find the count. Is there a better algorithm in Python? Thanks in advance!</p>\n",
        "answer_body": "<p>The [ugly] idea is to construct a 4 point path that starts with a <code>user_id</code> and ends with the same <code>user_id</code>. If such a path exists, then 2 starting points have mutual friends.</p>\n<p>We start with:</p>\n<pre><code>df\n          user_id  friend_id\n0        1          3\n1        1          4\n2        2          3\n3        2          5\n4        3          4\n</code></pre>\n<p>Then you can do:</p>\n<pre><code>dff = df.append(df.rename(columns={&quot;user_id&quot;:&quot;friend_id&quot;,&quot;friend_id&quot;:&quot;user_id&quot;}))\ndf_new = dff.merge(dff, on=&quot;friend_id&quot;, how=&quot;outer&quot;)\ndf_new = df_new[df_new[&quot;user_id_x&quot;]!= df_new[&quot;user_id_y&quot;]]\ndf_new = df_new.merge(dff, left_on= &quot;user_id_y&quot;, right_on=&quot;user_id&quot;)\ndf_new = df_new[df_new[&quot;user_id_x&quot;]==df_new[&quot;friend_id_y&quot;]]\ndf_out = df.merge(df_new, left_on=[&quot;user_id&quot;,&quot;friend_id&quot;], right_on=[&quot;user_id_x&quot;,&quot;friend_id_x&quot;], how=&quot;left&quot;,suffixes=(&quot;__&quot;,&quot;_&quot;))\ndf_out[&quot;count&quot;] = (~df_out[&quot;user_id_x&quot;].isnull()).astype(int)\ndf_out[[&quot;user_id__&quot;,&quot;friend_id&quot;,&quot;count&quot;]]\n\n   user_id__  friend_id  count\n0          1          3      1\n1          1          4      1\n2          2          3      0\n3          2          5      0\n4          3          4      1\n</code></pre>\n<p>A more elegant and straightforward way to use a graph approach</p>\n<pre><code>import networkx as nx\ng = nx.from_pandas_edgelist(df, &quot;user_id&quot;,&quot;friend_id&quot;)\nnx.draw_networkx(g)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/XasaD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/XasaD.png\" alt=\"enter image description here\" /></a></p>\n<p>Then you can identify number of mutual friends as number of paths for 2 adjacent nodes (2 friends) for which a 3 node path exists:</p>\n<pre><code>from networkx.algorithms.simple_paths import all_simple_paths\nfor row in df.itertuples():\n    df.at[row[0],&quot;count&quot;] = sum([len(l)==3 for l in list(all_simple_paths(g, row[1], row[2]))])\nprint(df)\n   user_id  friend_id  count\n0        1          3    1.0\n1        1          4    1.0\n2        2          3    0.0\n3        2          5    0.0\n4        3          4    1.0\n</code></pre>\n",
        "question_body": "<p>I have a dataframe of users and their friends that looks like:</p>\n<pre><code>user_id | friend_id\n1         3\n1         4\n2         3\n2         5\n3         4\n</code></pre>\n<p>I want to write a function in <code>python</code> to compute the number of mutual friends for each pair:</p>\n<pre><code>user_id | friend_id | num_mutual\n1         3           1\n1         4           1\n2         3           0\n2         5           0\n3         4           1\n</code></pre>\n<p>Currently I have:</p>\n<pre><code>def find_mutual(df):\n    num_mutual = []\n    for i in range(len(df)):\n        user, friend = df.loc[i, 'user_id'], df.loc[i, 'friend_id']\n        user_list = df[df.user_id == user].friend_id.tolist() + df[df.friend_id == user].user_id.tolist()\n        friend_list = df[df.user_id == friend].friend_id.tolist() + df[df.friend_id == friend].user_id.tolist()\n        mutual = len(list(set(user_list) &amp; set(friend_list)))\n        num_mutual.append(mutual)\n    return num_mutual\n</code></pre>\n<p>It works fine for small datasets, but I'm running it on a dataset with millions of rows. It takes forever to run everything. I know it's not the ideal way to find the count. Is there a better algorithm in Python? Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 64405516,
            "link": "https://stackoverflow.com/questions/64405516/find-the-number-of-mutual-friends-in-python",
            "question": {
                "title": "Find the number of mutual friends in Python",
                "ques_desc": "I have a dataframe of users and their friends that looks like: I want to write a function in to compute the number of mutual friends for each pair: Currently I have: It works fine for small datasets, but I'm running it on a dataset with millions of rows. It takes forever to run everything. I know it's not the ideal way to find the count. Is there a better algorithm in Python? Thanks in advance! "
            },
            "io": [
                "user_id | friend_id\n1         3\n1         4\n2         3\n2         5\n3         4\n",
                "user_id | friend_id | num_mutual\n1         3           1\n1         4           1\n2         3           0\n2         5           0\n3         4           1\n"
            ],
            "answer": {
                "ans_desc": "The [ugly] idea is to construct a 4 point path that starts with a and ends with the same . If such a path exists, then 2 starting points have mutual friends. We start with: Then you can do: A more elegant and straightforward way to use a graph approach Then you can identify number of mutual friends as number of paths for 2 adjacent nodes (2 friends) for which a 3 node path exists: ",
                "code": [
                    "import networkx as nx\ng = nx.from_pandas_edgelist(df, \"user_id\",\"friend_id\")\nnx.draw_networkx(g)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-wrangling"
        ],
        "owner": {
            "reputation": 93,
            "user_id": 9479122,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a148c2014d9a2330544490f6f2f214f0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dsto4",
            "link": "https://stackoverflow.com/users/9479122/dsto4"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 64405563,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1602957388,
        "creation_date": 1602949458,
        "last_edit_date": 1602954323,
        "question_id": 64404294,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64404294/pandas-series-add-previous-row-if-diff-negative",
        "title": "pandas series add previous row if diff negative",
        "body": "<p>I have a df that contains some revenue values and I want to interpolate the values to the dates that are not included in the index. To do so, I am finding the difference between rows and interpolating:</p>\n<pre><code>rev_diff = df.revenue.diff().fillna(0)\ndf = df.resample(&quot;M&quot;).mean()\ndf[&quot;revenue&quot;] = df.revenue.interpolate().diff()\n</code></pre>\n<p>I have this in a function and it is looped over thousands of such calculations (each one creating such a df). This works for most cases, but there are a few where the 'checkout till' resets and thus the diff is negative:</p>\n<pre><code>            revenue\n2015-10-19  203.0\n2016-04-03  271.0\n2016-06-13  301.0\n2016-06-13  0.0\n2016-09-27  30.0\n2017-03-14  77.0\n2017-09-19  128.0\n2018-09-19  0.0\n2018-03-19  10.0\n2019-03-22  287.0\n2020-03-20  398.0\n</code></pre>\n<p>The above code will give out negative interpolating values, so I am wondering whether there is a quick way to take that into account when it happens, without putting too much toll on the execution time because it's called thousands of times. The end result for the revenue df (before the interpolation is carried out) should be:</p>\n<pre><code>            revenue\n2015-10-19  203.0\n2016-04-03  271.0\n2016-06-13  301.0\n2016-09-27  331.0\n2017-03-14  378.0\n2017-09-19  429.0\n2018-03-19  439.0\n2019-03-22  716.0   \n2020-03-20  827.0\n</code></pre>\n<p>So basically if there is a 'reset', the diff should be added to the value in the row above. And that will happen for all rows below.</p>\n<p>I hope this makes sense. I am struggling to find a way of doing it which is not costly computationally.</p>\n<p>Thanks in advance.</p>\n",
        "answer_body": "<p>No magic. Steps:</p>\n<ol>\n<li>Identify the breakpoints by computing revenue difference.</li>\n<li>Populate the <code>revenue</code> values to be added for subsequent data.</li>\n<li>Sum it up.</li>\n<li>Remove duplicate records.</li>\n</ol>\n<h2>Code</h2>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf.reset_index(inplace=True)\n\n# 1. compute difference\ndf[&quot;rev_diff&quot;] = 0.0\ndf.loc[1:, &quot;rev_diff&quot;] = df[&quot;revenue&quot;].values[1:] - df[&quot;revenue&quot;].values[:-1]\n\n# get breakpoint locations\nbreakpoints = df[df[&quot;rev_diff&quot;] &lt; 0].index.values\n\n# 2. accumulate the values to be added\ndf[&quot;rev_add&quot;] = 0.0\nfor idx in breakpoints:\n    add_value = df.at[idx-1, &quot;revenue&quot;]\n    df.loc[idx:, &quot;rev_add&quot;] += add_value  # accumulate\n\n# 3. sum up\ndf[&quot;rev_new&quot;] = df[&quot;revenue&quot;] + df[&quot;rev_add&quot;]\n\n# 4. remove duplicate rows\ndf_new = df[[&quot;index&quot;, &quot;rev_new&quot;]].drop_duplicates().set_index(&quot;index&quot;)\ndf_new.index.name = None\n</code></pre>\n<h2>Result</h2>\n<pre><code>df_new\nOut[85]:\n            rev_new\n2015-10-19    203.0\n2016-04-03    271.0\n2016-06-13    301.0\n2016-09-27    331.0\n2017-03-14    378.0\n2017-09-19    429.0\n2018-03-19    439.0\n2019-03-22    716.0\n2020-03-20    827.0\n</code></pre>\n",
        "question_body": "<p>I have a df that contains some revenue values and I want to interpolate the values to the dates that are not included in the index. To do so, I am finding the difference between rows and interpolating:</p>\n<pre><code>rev_diff = df.revenue.diff().fillna(0)\ndf = df.resample(&quot;M&quot;).mean()\ndf[&quot;revenue&quot;] = df.revenue.interpolate().diff()\n</code></pre>\n<p>I have this in a function and it is looped over thousands of such calculations (each one creating such a df). This works for most cases, but there are a few where the 'checkout till' resets and thus the diff is negative:</p>\n<pre><code>            revenue\n2015-10-19  203.0\n2016-04-03  271.0\n2016-06-13  301.0\n2016-06-13  0.0\n2016-09-27  30.0\n2017-03-14  77.0\n2017-09-19  128.0\n2018-09-19  0.0\n2018-03-19  10.0\n2019-03-22  287.0\n2020-03-20  398.0\n</code></pre>\n<p>The above code will give out negative interpolating values, so I am wondering whether there is a quick way to take that into account when it happens, without putting too much toll on the execution time because it's called thousands of times. The end result for the revenue df (before the interpolation is carried out) should be:</p>\n<pre><code>            revenue\n2015-10-19  203.0\n2016-04-03  271.0\n2016-06-13  301.0\n2016-09-27  331.0\n2017-03-14  378.0\n2017-09-19  429.0\n2018-03-19  439.0\n2019-03-22  716.0   \n2020-03-20  827.0\n</code></pre>\n<p>So basically if there is a 'reset', the diff should be added to the value in the row above. And that will happen for all rows below.</p>\n<p>I hope this makes sense. I am struggling to find a way of doing it which is not costly computationally.</p>\n<p>Thanks in advance.</p>\n",
        "formatted_input": {
            "qid": 64404294,
            "link": "https://stackoverflow.com/questions/64404294/pandas-series-add-previous-row-if-diff-negative",
            "question": {
                "title": "pandas series add previous row if diff negative",
                "ques_desc": "I have a df that contains some revenue values and I want to interpolate the values to the dates that are not included in the index. To do so, I am finding the difference between rows and interpolating: I have this in a function and it is looped over thousands of such calculations (each one creating such a df). This works for most cases, but there are a few where the 'checkout till' resets and thus the diff is negative: The above code will give out negative interpolating values, so I am wondering whether there is a quick way to take that into account when it happens, without putting too much toll on the execution time because it's called thousands of times. The end result for the revenue df (before the interpolation is carried out) should be: So basically if there is a 'reset', the diff should be added to the value in the row above. And that will happen for all rows below. I hope this makes sense. I am struggling to find a way of doing it which is not costly computationally. Thanks in advance. "
            },
            "io": [
                "            revenue\n2015-10-19  203.0\n2016-04-03  271.0\n2016-06-13  301.0\n2016-06-13  0.0\n2016-09-27  30.0\n2017-03-14  77.0\n2017-09-19  128.0\n2018-09-19  0.0\n2018-03-19  10.0\n2019-03-22  287.0\n2020-03-20  398.0\n",
                "            revenue\n2015-10-19  203.0\n2016-04-03  271.0\n2016-06-13  301.0\n2016-09-27  331.0\n2017-03-14  378.0\n2017-09-19  429.0\n2018-03-19  439.0\n2019-03-22  716.0   \n2020-03-20  827.0\n"
            ],
            "answer": {
                "ans_desc": "No magic. Steps: Identify the breakpoints by computing revenue difference. Populate the values to be added for subsequent data. Sum it up. Remove duplicate records. Code Result ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf.reset_index(inplace=True)\n\n# 1. compute difference\ndf[\"rev_diff\"] = 0.0\ndf.loc[1:, \"rev_diff\"] = df[\"revenue\"].values[1:] - df[\"revenue\"].values[:-1]\n\n# get breakpoint locations\nbreakpoints = df[df[\"rev_diff\"] < 0].index.values\n\n# 2. accumulate the values to be added\ndf[\"rev_add\"] = 0.0\nfor idx in breakpoints:\n    add_value = df.at[idx-1, \"revenue\"]\n    df.loc[idx:, \"rev_add\"] += add_value  # accumulate\n\n# 3. sum up\ndf[\"rev_new\"] = df[\"revenue\"] + df[\"rev_add\"]\n\n# 4. remove duplicate rows\ndf_new = df[[\"index\", \"rev_new\"]].drop_duplicates().set_index(\"index\")\ndf_new.index.name = None\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 13946736,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "Christian Sch.",
            "link": "https://stackoverflow.com/users/13946736/christian-sch"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 64368741,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1602754626,
        "creation_date": 1602754145,
        "question_id": 64368604,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64368604/shuffling-pandas-dataframe-columns",
        "title": "Shuffling Pandas Dataframe Columns",
        "body": "<p>I need to shuffle dataframe columns. Currently I do it this way:</p>\n<pre><code>import random\nimport pandas as pd\nimport numpy\n\ndf = pd.DataFrame(numpy.random.rand(1,5))\nprint (df)\ndf_as_list = df.values.tolist()[0]\nrandom.shuffle(df_as_list)\ndf_shuffled = pd.DataFrame(df_as_list).transpose()\nprint (df_shuffled)\n</code></pre>\n<p>Before:</p>\n<pre><code>          0         1         2         3         4\n0  0.472918  0.261734  0.987053  0.921826  0.144114\n</code></pre>\n<p>After:</p>\n<pre><code>          0         1         2         3         4\n0  0.472918  0.921826  0.987053  0.144114  0.261734\n</code></pre>\n<p>So it does the job, but there must be a better way to do this. Any ideas?</p>\n",
        "answer_body": "<p><strong>try</strong>:</p>\n<pre><code>def shuffle(df, n=1):\n    for _ in range(n):\n        df.apply(np.random.shuffle)\n        return df\ndf = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nshuffle(df)\n\nprint(df)\n</code></pre>\n",
        "question_body": "<p>I need to shuffle dataframe columns. Currently I do it this way:</p>\n<pre><code>import random\nimport pandas as pd\nimport numpy\n\ndf = pd.DataFrame(numpy.random.rand(1,5))\nprint (df)\ndf_as_list = df.values.tolist()[0]\nrandom.shuffle(df_as_list)\ndf_shuffled = pd.DataFrame(df_as_list).transpose()\nprint (df_shuffled)\n</code></pre>\n<p>Before:</p>\n<pre><code>          0         1         2         3         4\n0  0.472918  0.261734  0.987053  0.921826  0.144114\n</code></pre>\n<p>After:</p>\n<pre><code>          0         1         2         3         4\n0  0.472918  0.921826  0.987053  0.144114  0.261734\n</code></pre>\n<p>So it does the job, but there must be a better way to do this. Any ideas?</p>\n",
        "formatted_input": {
            "qid": 64368604,
            "link": "https://stackoverflow.com/questions/64368604/shuffling-pandas-dataframe-columns",
            "question": {
                "title": "Shuffling Pandas Dataframe Columns",
                "ques_desc": "I need to shuffle dataframe columns. Currently I do it this way: Before: After: So it does the job, but there must be a better way to do this. Any ideas? "
            },
            "io": [
                "          0         1         2         3         4\n0  0.472918  0.261734  0.987053  0.921826  0.144114\n",
                "          0         1         2         3         4\n0  0.472918  0.921826  0.987053  0.144114  0.261734\n"
            ],
            "answer": {
                "ans_desc": "try: ",
                "code": [
                    "def shuffle(df, n=1):\n    for _ in range(n):\n        df.apply(np.random.shuffle)\n        return df\ndf = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nshuffle(df)\n\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matplotlib"
        ],
        "owner": {
            "reputation": 123,
            "user_id": 10367596,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/fbf814903a7a747e37e420c940d745f7?s=128&d=identicon&r=PG&f=1",
            "display_name": "Rocco",
            "link": "https://stackoverflow.com/users/10367596/rocco"
        },
        "is_answered": true,
        "view_count": 53,
        "accepted_answer_id": 64331506,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1602577887,
        "creation_date": 1602547404,
        "question_id": 64326826,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64326826/plotting-a-data-frame-of-error-bars-onto-a-data-frame-in-matplotlib-python",
        "title": "Plotting a data frame of error bars onto a data frame in matplotlib Python",
        "body": "<p>I've got a pandas data frame (<code>df</code>) of values as follows:</p>\n<pre><code>            0           1           2\n0  100.000000  100.000000  100.000000\n1    0.412497    0.668880  136.019498\n2    5.144450   77.323610  163.496773\n3   31.078457   78.151325  146.772621\n</code></pre>\n<p>I also have a data frame (<code>deviation</code>) with the error of each of those values:</p>\n<pre><code>          0         1         2\n0  0.083579  0.048520  0.082328\n1  0.005855  0.005904  0.046494\n2  0.009907  0.080799  0.083671\n3  0.045831  0.075932  0.044581\n</code></pre>\n<p>I have successfully been able to plot <code>df</code> with matplotlib as I desired:</p>\n<p><a href=\"https://i.stack.imgur.com/H0s0q.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/H0s0q.png\" alt=\"enter image description here\" /></a></p>\n<p>However, I am struggling to get the error bars onto this graph. My code for plotting is currently as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>drugCount = 3\ndf = pd.DataFrame(drug)\ndeviation = pd.DataFrame(deviation)\ndf.columns = list(string.ascii_uppercase[0:drugCount])\nax = df.T.plot(kind='bar', yerr=deviation, color=['C0', 'C3', 'C1', 'C2'])\n\nplt.xlabel('Drug')\nplt.ylabel('% Cell viability')\nplt.legend(labels=['Control', 'High', 'Medium', 'Low'])\nplt.title('Viability of HeLa cells against various drugs')\n\nplt.show()\n</code></pre>\n<p>As you can see, I am trying to pass the <code>deviation</code> data frame into the <code>yerr</code> flag, but it does not do anything, and I am getting the error:</p>\n<pre><code>/usr/local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: UserWarning: Warning: converting a masked element to nan.\n  return array(a, dtype, copy=False, order=order)\n</code></pre>\n<p>I have had a look online but it seems not many people are trying to add so many error bars like I am trying to. What do I need to change to allow this to work?</p>\n",
        "answer_body": "<p>try casting deviation to list in (and multiply by 100 to see anything)</p>\n<pre><code>ax = df.T.plot(kind='bar', yerr=list(deviation.values*100) color=['C0', 'C3', 'C1', 'C2'])\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/rccyF.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/rccyF.png\" alt=\"enter image description here\" /></a></p>\n",
        "question_body": "<p>I've got a pandas data frame (<code>df</code>) of values as follows:</p>\n<pre><code>            0           1           2\n0  100.000000  100.000000  100.000000\n1    0.412497    0.668880  136.019498\n2    5.144450   77.323610  163.496773\n3   31.078457   78.151325  146.772621\n</code></pre>\n<p>I also have a data frame (<code>deviation</code>) with the error of each of those values:</p>\n<pre><code>          0         1         2\n0  0.083579  0.048520  0.082328\n1  0.005855  0.005904  0.046494\n2  0.009907  0.080799  0.083671\n3  0.045831  0.075932  0.044581\n</code></pre>\n<p>I have successfully been able to plot <code>df</code> with matplotlib as I desired:</p>\n<p><a href=\"https://i.stack.imgur.com/H0s0q.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/H0s0q.png\" alt=\"enter image description here\" /></a></p>\n<p>However, I am struggling to get the error bars onto this graph. My code for plotting is currently as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>drugCount = 3\ndf = pd.DataFrame(drug)\ndeviation = pd.DataFrame(deviation)\ndf.columns = list(string.ascii_uppercase[0:drugCount])\nax = df.T.plot(kind='bar', yerr=deviation, color=['C0', 'C3', 'C1', 'C2'])\n\nplt.xlabel('Drug')\nplt.ylabel('% Cell viability')\nplt.legend(labels=['Control', 'High', 'Medium', 'Low'])\nplt.title('Viability of HeLa cells against various drugs')\n\nplt.show()\n</code></pre>\n<p>As you can see, I am trying to pass the <code>deviation</code> data frame into the <code>yerr</code> flag, but it does not do anything, and I am getting the error:</p>\n<pre><code>/usr/local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: UserWarning: Warning: converting a masked element to nan.\n  return array(a, dtype, copy=False, order=order)\n</code></pre>\n<p>I have had a look online but it seems not many people are trying to add so many error bars like I am trying to. What do I need to change to allow this to work?</p>\n",
        "formatted_input": {
            "qid": 64326826,
            "link": "https://stackoverflow.com/questions/64326826/plotting-a-data-frame-of-error-bars-onto-a-data-frame-in-matplotlib-python",
            "question": {
                "title": "Plotting a data frame of error bars onto a data frame in matplotlib Python",
                "ques_desc": "I've got a pandas data frame () of values as follows: I also have a data frame () with the error of each of those values: I have successfully been able to plot with matplotlib as I desired: However, I am struggling to get the error bars onto this graph. My code for plotting is currently as follows: As you can see, I am trying to pass the data frame into the flag, but it does not do anything, and I am getting the error: I have had a look online but it seems not many people are trying to add so many error bars like I am trying to. What do I need to change to allow this to work? "
            },
            "io": [
                "            0           1           2\n0  100.000000  100.000000  100.000000\n1    0.412497    0.668880  136.019498\n2    5.144450   77.323610  163.496773\n3   31.078457   78.151325  146.772621\n",
                "          0         1         2\n0  0.083579  0.048520  0.082328\n1  0.005855  0.005904  0.046494\n2  0.009907  0.080799  0.083671\n3  0.045831  0.075932  0.044581\n"
            ],
            "answer": {
                "ans_desc": "try casting deviation to list in (and multiply by 100 to see anything) ",
                "code": [
                    "ax = df.T.plot(kind='bar', yerr=list(deviation.values*100) color=['C0', 'C3', 'C1', 'C2'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "rename"
        ],
        "owner": {
            "reputation": 197,
            "user_id": 12753208,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cb5107aa10e5217667273fffe75546b9?s=128&d=identicon&r=PG&f=1",
            "display_name": "cna",
            "link": "https://stackoverflow.com/users/12753208/cna"
        },
        "is_answered": true,
        "view_count": 160,
        "accepted_answer_id": 64275785,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1602229190,
        "creation_date": 1602227919,
        "question_id": 64275557,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64275557/rename-column-in-dataframe-that-contains-digits-in-the-middle",
        "title": "Rename column in dataframe that contains digits in the middle",
        "body": "<p>Say I have a dataframe columns as such <code>df.info()</code>:</p>\n<pre><code> #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Action_3.@source             1 non-null      object\n 1   Description_3.#text          1 non-null      object\n 2   Code_3.@source               1 non-null      object\n 3   Others                       1 non-null      object\n 4   Animal_1                     1 non-null      object\n</code></pre>\n<p>To:</p>\n<pre><code> #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Action.@source_3             1 non-null      object\n 1   Description.#text_3          1 non-null      object\n 2   Code.@source_3               1 non-null      object\n 3   Others                       1 non-null      object\n 4   Animal_1                     1 non-null      object\n</code></pre>\n<p>This need to be done dynamically. My plan is:</p>\n<ol>\n<li>Use regex to find digits in the MIDDLE of string.</li>\n<li>Replace <code>_{digit}</code> to the back of the column name, iteratively.</li>\n</ol>\n<p>My current code :</p>\n<pre><code>def check_number_in_column(column_name):\n\n    return any(i.isdigit() for i in column_name)\n\n\n# List out the column names then loop\ntemp_column_name = temp_df.columns\n\nfor j, name in enumerate(temp_column_name):            \n\n    if check_number_in_column(name) is True :\n    #TODO\n</code></pre>\n",
        "answer_body": "<p>You could use a regular expression, with pandas' <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.replace.html\" rel=\"nofollow noreferrer\">str.replace</a> to dynamically change it :</p>\n<pre><code>df = pd.DataFrame(\n    [],\n    columns=[\n        &quot;Action_3.@source&quot;,\n        &quot;Description_3.#text&quot;,\n        &quot;Code_3.@source&quot;,\n        &quot;Others&quot;,\n        &quot;Animal_1&quot;,\n    ],\n)\n\npat = r&quot;(?P&lt;first&gt;.+)(?P&lt;middle&gt;_\\d)(?P&lt;last&gt;.+)&quot;\nrepl = lambda m: f&quot;{m.group('first')}{m.group('last')}{m.group('middle')}&quot;\n\ndf.columns.str.replace(pat, repl)\n\nIndex(['Action.@source_3', 'Description.#text_3', 'Code.@source_3', 'Others',\n       'Animal_1'],\n      dtype='object')\n</code></pre>\n<p>You can also define the function, if you wish to avoid lambda :</p>\n<pre><code>def repl(m):\n    return f&quot;{m.group('first')}{m.group('last')}{m.group('middle')}&quot;\n</code></pre>\n<p>Apply the function :</p>\n<pre><code>df.columns.str.replace(pat, repl=repl)\n</code></pre>\n",
        "question_body": "<p>Say I have a dataframe columns as such <code>df.info()</code>:</p>\n<pre><code> #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Action_3.@source             1 non-null      object\n 1   Description_3.#text          1 non-null      object\n 2   Code_3.@source               1 non-null      object\n 3   Others                       1 non-null      object\n 4   Animal_1                     1 non-null      object\n</code></pre>\n<p>To:</p>\n<pre><code> #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Action.@source_3             1 non-null      object\n 1   Description.#text_3          1 non-null      object\n 2   Code.@source_3               1 non-null      object\n 3   Others                       1 non-null      object\n 4   Animal_1                     1 non-null      object\n</code></pre>\n<p>This need to be done dynamically. My plan is:</p>\n<ol>\n<li>Use regex to find digits in the MIDDLE of string.</li>\n<li>Replace <code>_{digit}</code> to the back of the column name, iteratively.</li>\n</ol>\n<p>My current code :</p>\n<pre><code>def check_number_in_column(column_name):\n\n    return any(i.isdigit() for i in column_name)\n\n\n# List out the column names then loop\ntemp_column_name = temp_df.columns\n\nfor j, name in enumerate(temp_column_name):            \n\n    if check_number_in_column(name) is True :\n    #TODO\n</code></pre>\n",
        "formatted_input": {
            "qid": 64275557,
            "link": "https://stackoverflow.com/questions/64275557/rename-column-in-dataframe-that-contains-digits-in-the-middle",
            "question": {
                "title": "Rename column in dataframe that contains digits in the middle",
                "ques_desc": "Say I have a dataframe columns as such : To: This need to be done dynamically. My plan is: Use regex to find digits in the MIDDLE of string. Replace to the back of the column name, iteratively. My current code : "
            },
            "io": [
                " #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Action_3.@source             1 non-null      object\n 1   Description_3.#text          1 non-null      object\n 2   Code_3.@source               1 non-null      object\n 3   Others                       1 non-null      object\n 4   Animal_1                     1 non-null      object\n",
                " #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Action.@source_3             1 non-null      object\n 1   Description.#text_3          1 non-null      object\n 2   Code.@source_3               1 non-null      object\n 3   Others                       1 non-null      object\n 4   Animal_1                     1 non-null      object\n"
            ],
            "answer": {
                "ans_desc": "You could use a regular expression, with pandas' str.replace to dynamically change it : You can also define the function, if you wish to avoid lambda : Apply the function : ",
                "code": [
                    "df = pd.DataFrame(\n    [],\n    columns=[\n        \"Action_3.@source\",\n        \"Description_3.#text\",\n        \"Code_3.@source\",\n        \"Others\",\n        \"Animal_1\",\n    ],\n)\n\npat = r\"(?P<first>.+)(?P<middle>_\\d)(?P<last>.+)\"\nrepl = lambda m: f\"{m.group('first')}{m.group('last')}{m.group('middle')}\"\n\ndf.columns.str.replace(pat, repl)\n\nIndex(['Action.@source_3', 'Description.#text_3', 'Code.@source_3', 'Others',\n       'Animal_1'],\n      dtype='object')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 14406696,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/bfa023d25d5be1109a43aa89d21c7a36?s=128&d=identicon&r=PG&f=1",
            "display_name": "moham",
            "link": "https://stackoverflow.com/users/14406696/moham"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 64242533,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1602068754,
        "creation_date": 1602067819,
        "question_id": 64242508,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64242508/how-to-create-new-dataframe-by-combining-some-columns-together-of-existing-one",
        "title": "how to create new dataframe by combining some columns together of existing one?",
        "body": "<p>I am having a dataframe df like shown:</p>\n<pre><code>1-1    1-2    1-3    2-1    2-2    3-1    3-2    4-1    5-1\n10      3      9      1     3       9      33     10     11\n21      31     3      22    21      13     11     7      13\n33      22     61     31    35      34     8      10     16\n6       9      32     5      4      8      9      6      8\n</code></pre>\n<p>where the explanation of the columns as the following:</p>\n<p>the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3.</p>\n<p>I would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups.</p>\n<p>The new dataframe I need is like the shown:</p>\n<pre><code>1    2    3    4    5\n10   3    33   10   11\n31   22   13   7    13\n61   35   34   10   16\n32   5    9    6    8\n</code></pre>\n",
        "answer_body": "<p>You can aggregate by columns with <code>axis=1</code> and lambda function for split and select first values with <code>max</code> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>DataFrame.groupby</code></a>:</p>\n<p>This working correct if numbers of groups contains 2 or more digits.</p>\n<pre><code>df1 = df.groupby(lambda x: x.split('-')[0], axis=1).max()\n</code></pre>\n<p>Alternative is pass splitted columns names:</p>\n<pre><code>df1 = df.groupby(df.columns.str.split('-').str[0], axis=1).max()\n\nprint (df1)\n    1   2   3   4   5\n0  10   3  33  10  11\n1  31  22  13   7  13\n2  61  35  34  10  16\n3  32   5   9   6   8\n</code></pre>\n",
        "question_body": "<p>I am having a dataframe df like shown:</p>\n<pre><code>1-1    1-2    1-3    2-1    2-2    3-1    3-2    4-1    5-1\n10      3      9      1     3       9      33     10     11\n21      31     3      22    21      13     11     7      13\n33      22     61     31    35      34     8      10     16\n6       9      32     5      4      8      9      6      8\n</code></pre>\n<p>where the explanation of the columns as the following:</p>\n<p>the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3.</p>\n<p>I would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups.</p>\n<p>The new dataframe I need is like the shown:</p>\n<pre><code>1    2    3    4    5\n10   3    33   10   11\n31   22   13   7    13\n61   35   34   10   16\n32   5    9    6    8\n</code></pre>\n",
        "formatted_input": {
            "qid": 64242508,
            "link": "https://stackoverflow.com/questions/64242508/how-to-create-new-dataframe-by-combining-some-columns-together-of-existing-one",
            "question": {
                "title": "how to create new dataframe by combining some columns together of existing one?",
                "ques_desc": "I am having a dataframe df like shown: where the explanation of the columns as the following: the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3. I would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups. The new dataframe I need is like the shown: "
            },
            "io": [
                "1-1    1-2    1-3    2-1    2-2    3-1    3-2    4-1    5-1\n10      3      9      1     3       9      33     10     11\n21      31     3      22    21      13     11     7      13\n33      22     61     31    35      34     8      10     16\n6       9      32     5      4      8      9      6      8\n",
                "1    2    3    4    5\n10   3    33   10   11\n31   22   13   7    13\n61   35   34   10   16\n32   5    9    6    8\n"
            ],
            "answer": {
                "ans_desc": "You can aggregate by columns with and lambda function for split and select first values with and : This working correct if numbers of groups contains 2 or more digits. Alternative is pass splitted columns names: ",
                "code": [
                    "df1 = df.groupby(lambda x: x.split('-')[0], axis=1).max()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 14254117,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/10160411997357796/picture?type=large",
            "display_name": "Bur&#231;in Kermen",
            "link": "https://stackoverflow.com/users/14254117/bur%c3%a7in-kermen"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 64230490,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1602003546,
        "creation_date": 1602002193,
        "question_id": 64230159,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64230159/shifting-and-reverting-multiple-rows-in-pandas-dataframe",
        "title": "Shifting and reverting multiple rows in pandas dataframe",
        "body": "<p>I have the following dataframe and wish to shift over the 0 values to the right and then revert each row:</p>\n<pre><code>    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   33  28  98  97  0   0   0\n2   29  24  22  98  97  0   0\n3   78  76  98  97  0   0   0\n4   16  15  98  97  0   0   0\n5   81  72  70  98  97  0   0\n</code></pre>\n<p>This is the result I would like to get:</p>\n<pre><code>    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   97  98  28  33  0   0   0\n2   97  98  22  24  29  0   0\n3   97  98  76  78  0   0   0\n4   97  98  15  16  0   0   0\n5   97  98  70  72  81  0   0\n</code></pre>\n<p>I've tried varius shift and apply combinations without any success. Is there a simple way of achieving this?</p>\n",
        "answer_body": "<p>You can reverse the values that are greater than zero</p>\n<pre><code>def reverse_part(series):\n  series[series &gt; 0] = series[series &gt; 0][::-1]\n  return series\n\ndf.apply(reverse_part, axis=1, raw=True)\n</code></pre>\n<p>Out:</p>\n<pre><code>    H00  H01  H02  H03  H04  H05  H06\nNR                                   \n1    97   98   28   33    0    0    0\n2    97   98   22   24   29    0    0\n3    97   98   76   78    0    0    0\n4    97   98   15   16    0    0    0\n5    97   98   70   72   81    0    0\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe and wish to shift over the 0 values to the right and then revert each row:</p>\n<pre><code>    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   33  28  98  97  0   0   0\n2   29  24  22  98  97  0   0\n3   78  76  98  97  0   0   0\n4   16  15  98  97  0   0   0\n5   81  72  70  98  97  0   0\n</code></pre>\n<p>This is the result I would like to get:</p>\n<pre><code>    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   97  98  28  33  0   0   0\n2   97  98  22  24  29  0   0\n3   97  98  76  78  0   0   0\n4   97  98  15  16  0   0   0\n5   97  98  70  72  81  0   0\n</code></pre>\n<p>I've tried varius shift and apply combinations without any success. Is there a simple way of achieving this?</p>\n",
        "formatted_input": {
            "qid": 64230159,
            "link": "https://stackoverflow.com/questions/64230159/shifting-and-reverting-multiple-rows-in-pandas-dataframe",
            "question": {
                "title": "Shifting and reverting multiple rows in pandas dataframe",
                "ques_desc": "I have the following dataframe and wish to shift over the 0 values to the right and then revert each row: This is the result I would like to get: I've tried varius shift and apply combinations without any success. Is there a simple way of achieving this? "
            },
            "io": [
                "    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   33  28  98  97  0   0   0\n2   29  24  22  98  97  0   0\n3   78  76  98  97  0   0   0\n4   16  15  98  97  0   0   0\n5   81  72  70  98  97  0   0\n",
                "    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   97  98  28  33  0   0   0\n2   97  98  22  24  29  0   0\n3   97  98  76  78  0   0   0\n4   97  98  15  16  0   0   0\n5   97  98  70  72  81  0   0\n"
            ],
            "answer": {
                "ans_desc": "You can reverse the values that are greater than zero Out: ",
                "code": [
                    "def reverse_part(series):\n  series[series > 0] = series[series > 0][::-1]\n  return series\n\ndf.apply(reverse_part, axis=1, raw=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "max"
        ],
        "owner": {
            "reputation": 75,
            "user_id": 6432037,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1b2086b5781116272b5445e9fd288d29?s=128&d=identicon&r=PG&f=1",
            "display_name": "CjV",
            "link": "https://stackoverflow.com/users/6432037/cjv"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 64229596,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1602003002,
        "creation_date": 1601999219,
        "question_id": 64229332,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64229332/using-pythons-max-to-return-two-equally-large-values-across-columns-of-a-data-f",
        "title": "Using Python&#39;s max to return two equally large values across columns of a data frame",
        "body": "<p>I would like to find the column of a data frame with the maximum value per row and if there are multiple equally large values, then return all the column names where those values are. I would like to store all of these values in the last column of the data frame. I have been referencing the following post, and am unsure of how to modify it to handle data frames:</p>\n<p><a href=\"https://stackoverflow.com/questions/9853302/using-pythons-max-to-return-two-equally-large-values\">Using Python&#39;s max to return two equally large values</a></p>\n<p>So if my data looked like this</p>\n<pre><code>Key    Column_1  Column_2  Column_3\n0          1        2         3\n1          1        1         0\n2          0        0         0\n</code></pre>\n<p>My goal is an output that looks like this:</p>\n<pre><code>Key    Column_1  Column_2  Column_3  Column_4\n0          1        2         3      Column_3\n1          1        1         0      Column_1,Column_2\n2          0        0         0      NA\n</code></pre>\n<p>I know how to use idxmax(axis=1,skipna = True) to return the first max and know that if I change 0 to Nan in the dataframe it will populate the last row correctly, just not sure how to do this when there are multiple max values.</p>\n<p>Any help is greatly appreciated ! I am an R programmer and this is my first time in Python.</p>\n",
        "answer_body": "<p>Using <code>dot</code> as well, and combining it with <code>mask</code>:</p>\n<pre><code>d = df.set_index('Key').select_dtypes('number')\nv = d.eq(d.max(axis=1), axis=0).dot(d.columns + ',').str.rstrip(',')\ndf['Column_4'] = v.mask(d.eq(0).all(axis=1)))\n</code></pre>\n<pre><code>   Key  Column_1  Column_2  Column_3           Column_4\n0    0         1         2         3           Column_3\n1    1         1         1         0  Column_1,Column_2\n2    2         0         0         0                NaN\n</code></pre>\n",
        "question_body": "<p>I would like to find the column of a data frame with the maximum value per row and if there are multiple equally large values, then return all the column names where those values are. I would like to store all of these values in the last column of the data frame. I have been referencing the following post, and am unsure of how to modify it to handle data frames:</p>\n<p><a href=\"https://stackoverflow.com/questions/9853302/using-pythons-max-to-return-two-equally-large-values\">Using Python&#39;s max to return two equally large values</a></p>\n<p>So if my data looked like this</p>\n<pre><code>Key    Column_1  Column_2  Column_3\n0          1        2         3\n1          1        1         0\n2          0        0         0\n</code></pre>\n<p>My goal is an output that looks like this:</p>\n<pre><code>Key    Column_1  Column_2  Column_3  Column_4\n0          1        2         3      Column_3\n1          1        1         0      Column_1,Column_2\n2          0        0         0      NA\n</code></pre>\n<p>I know how to use idxmax(axis=1,skipna = True) to return the first max and know that if I change 0 to Nan in the dataframe it will populate the last row correctly, just not sure how to do this when there are multiple max values.</p>\n<p>Any help is greatly appreciated ! I am an R programmer and this is my first time in Python.</p>\n",
        "formatted_input": {
            "qid": 64229332,
            "link": "https://stackoverflow.com/questions/64229332/using-pythons-max-to-return-two-equally-large-values-across-columns-of-a-data-f",
            "question": {
                "title": "Using Python&#39;s max to return two equally large values across columns of a data frame",
                "ques_desc": "I would like to find the column of a data frame with the maximum value per row and if there are multiple equally large values, then return all the column names where those values are. I would like to store all of these values in the last column of the data frame. I have been referencing the following post, and am unsure of how to modify it to handle data frames: Using Python's max to return two equally large values So if my data looked like this My goal is an output that looks like this: I know how to use idxmax(axis=1,skipna = True) to return the first max and know that if I change 0 to Nan in the dataframe it will populate the last row correctly, just not sure how to do this when there are multiple max values. Any help is greatly appreciated ! I am an R programmer and this is my first time in Python. "
            },
            "io": [
                "Key    Column_1  Column_2  Column_3\n0          1        2         3\n1          1        1         0\n2          0        0         0\n",
                "Key    Column_1  Column_2  Column_3  Column_4\n0          1        2         3      Column_3\n1          1        1         0      Column_1,Column_2\n2          0        0         0      NA\n"
            ],
            "answer": {
                "ans_desc": "Using as well, and combining it with : ",
                "code": [
                    "d = df.set_index('Key').select_dtypes('number')\nv = d.eq(d.max(axis=1), axis=0).dot(d.columns + ',').str.rstrip(',')\ndf['Column_4'] = v.mask(d.eq(0).all(axis=1)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 165,
            "user_id": 11329454,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/59ec761034d2bd4def5196c0d8f2ec58?s=128&d=identicon&r=PG&f=1",
            "display_name": "MLNLPEnhusiast",
            "link": "https://stackoverflow.com/users/11329454/mlnlpenhusiast"
        },
        "is_answered": true,
        "view_count": 662,
        "accepted_answer_id": 64199000,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1601934689,
        "creation_date": 1601839470,
        "last_edit_date": 1601904470,
        "question_id": 64198934,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64198934/dropping-rows-from-pandas-dataframe-based-on-value-in-columns",
        "title": "Dropping rows from pandas dataframe based on value in column(s)",
        "body": "<p>Suppose I have a dataframe which has Column 'A' and Column 'B' How do I drop rows where Column 'A' and 'B' are equal , <strong>but not in same row</strong>. I only wanto to drop rows where column 'B' is equal to column 'A'</p>\n<p>For example Column 'B' from Rows 4, 8 &amp; 9 is equal to Rows 2,3&amp;5 Column 'A'.\nI want to drop Rows 4, 8 &amp; 9</p>\n<pre><code>    Column A         Column B                                                 \n1        10               62 \n2        10               72\n3        20               75\n4        20               10\n5        30               35\n6        30               45               \n7        40               55    \n8        40               20\n9        40               30\n</code></pre>\n<p>Drop Rows 4, 8 &amp; 9 since Column B from rows is equal to column A from row 2,3&amp;5</p>\n<p>Expected output</p>\n<pre><code>    Column A         Column B                                                 \n1        10               62 \n2        10               72\n3        20               75\n\n5        30               35\n6        30               45               \n7        40               55    \n  \n</code></pre>\n<p>Rows 4, 8 &amp; 9 needs to be deleted</p>\n<p><strong>Adding additional details:</strong>\nColumn A and B will never be equal in same row.\nMultiple rows in Column B may have matching values in Column A. To illustrate I have expanded the dataframe\nSorry if my originial row numbers are not matching.\nTo summarize the requirement.</p>\n<p>Multiple rows will have column B matching with Column A and expectation is to delete all rows where column B is matching with Column A in any row.</p>\n<p>To reiterate Column A and Column B will not be equal in same row</p>\n",
        "answer_body": "<p>This solution is assuming that unique values in <code>column A</code> should be dropped, too, when the condition is met in <code>column B</code>.</p>\n<p>I added a fifth row to test for the condition that equal values in the same row should not be dropped</p>\n<pre><code>   Column A  Column B\n1        10        62\n2        20        75\n3        30        45\n4        45        55\n5        65        65\n</code></pre>\n<p>Check for all values in <code>column B</code> if they are in <code>column A</code> with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html\" rel=\"nofollow noreferrer\"><code>isin</code></a> but exclude rows with equal values.</p>\n<pre><code>df[~(df['Column B'].isin(df['Column A']) &amp; (df['Column B'] != df['Column A']))]\n</code></pre>\n<p>Out:</p>\n<pre><code>   Column A  Column B\n1        10        62\n2        20        75\n4        45        55\n5        65        65\n</code></pre>\n<ul>\n<li>Updated, as per the additional details, and the output matches the expected result.</li>\n</ul>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\n# sample\ndf = pd.DataFrame({'colA': [10, 10, 20, 20, 30, 30, 40, 40, 40], 'colB': [62, 72, 75, 10, 35, 45, 55, 20, 30]})\n\n# display(df)\n   colA  colB\n0    10    62\n1    10    72\n2    20    75\n3    20    10\n4    30    35\n5    30    45\n6    40    55\n7    40    20\n8    40    30\n\ndf[~(df['colB'].isin(df['colA']) &amp; (df['colB'] != df['colA']))]\n\n[out]:\n   colA  colB\n0    10    62\n1    10    72\n2    20    75\n4    30    35\n5    30    45\n6    40    55\n</code></pre>\n",
        "question_body": "<p>Suppose I have a dataframe which has Column 'A' and Column 'B' How do I drop rows where Column 'A' and 'B' are equal , <strong>but not in same row</strong>. I only wanto to drop rows where column 'B' is equal to column 'A'</p>\n<p>For example Column 'B' from Rows 4, 8 &amp; 9 is equal to Rows 2,3&amp;5 Column 'A'.\nI want to drop Rows 4, 8 &amp; 9</p>\n<pre><code>    Column A         Column B                                                 \n1        10               62 \n2        10               72\n3        20               75\n4        20               10\n5        30               35\n6        30               45               \n7        40               55    \n8        40               20\n9        40               30\n</code></pre>\n<p>Drop Rows 4, 8 &amp; 9 since Column B from rows is equal to column A from row 2,3&amp;5</p>\n<p>Expected output</p>\n<pre><code>    Column A         Column B                                                 \n1        10               62 \n2        10               72\n3        20               75\n\n5        30               35\n6        30               45               \n7        40               55    \n  \n</code></pre>\n<p>Rows 4, 8 &amp; 9 needs to be deleted</p>\n<p><strong>Adding additional details:</strong>\nColumn A and B will never be equal in same row.\nMultiple rows in Column B may have matching values in Column A. To illustrate I have expanded the dataframe\nSorry if my originial row numbers are not matching.\nTo summarize the requirement.</p>\n<p>Multiple rows will have column B matching with Column A and expectation is to delete all rows where column B is matching with Column A in any row.</p>\n<p>To reiterate Column A and Column B will not be equal in same row</p>\n",
        "formatted_input": {
            "qid": 64198934,
            "link": "https://stackoverflow.com/questions/64198934/dropping-rows-from-pandas-dataframe-based-on-value-in-columns",
            "question": {
                "title": "Dropping rows from pandas dataframe based on value in column(s)",
                "ques_desc": "Suppose I have a dataframe which has Column 'A' and Column 'B' How do I drop rows where Column 'A' and 'B' are equal , but not in same row. I only wanto to drop rows where column 'B' is equal to column 'A' For example Column 'B' from Rows 4, 8 & 9 is equal to Rows 2,3&5 Column 'A'. I want to drop Rows 4, 8 & 9 Drop Rows 4, 8 & 9 since Column B from rows is equal to column A from row 2,3&5 Expected output Rows 4, 8 & 9 needs to be deleted Adding additional details: Column A and B will never be equal in same row. Multiple rows in Column B may have matching values in Column A. To illustrate I have expanded the dataframe Sorry if my originial row numbers are not matching. To summarize the requirement. Multiple rows will have column B matching with Column A and expectation is to delete all rows where column B is matching with Column A in any row. To reiterate Column A and Column B will not be equal in same row "
            },
            "io": [
                "    Column A         Column B                                                 \n1        10               62 \n2        10               72\n3        20               75\n4        20               10\n5        30               35\n6        30               45               \n7        40               55    \n8        40               20\n9        40               30\n",
                "    Column A         Column B                                                 \n1        10               62 \n2        10               72\n3        20               75\n\n5        30               35\n6        30               45               \n7        40               55    \n  \n"
            ],
            "answer": {
                "ans_desc": "This solution is assuming that unique values in should be dropped, too, when the condition is met in . I added a fifth row to test for the condition that equal values in the same row should not be dropped Check for all values in if they are in with but exclude rows with equal values. Out: Updated, as per the additional details, and the output matches the expected result. ",
                "code": [
                    "df[~(df['Column B'].isin(df['Column A']) & (df['Column B'] != df['Column A']))]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 109,
            "user_id": 13846828,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-0_V-y_-hs5M/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclZNje_nr1pZIiV6pOfSmUN24goKw/photo.jpg?sz=128",
            "display_name": "yoyoyo",
            "link": "https://stackoverflow.com/users/13846828/yoyoyo"
        },
        "is_answered": true,
        "view_count": 91,
        "closed_date": 1601728612,
        "accepted_answer_id": 64183280,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1601721482,
        "creation_date": 1601720602,
        "question_id": 64183163,
        "link": "https://stackoverflow.com/questions/64183163/how-can-i-save-dataframe-as-list-and-not-as-string",
        "closed_reason": "Duplicate",
        "title": "How can I save DataFrame as list and not as string",
        "body": "<p>I have this <code>pd.DataFrame()</code> created. I want to create a <code>.csv</code> and it is saving as this</p>\n<pre><code>                                                         0  1\n0        [15921, 10, 82, 22, 202973, 368, 1055, 3135, 1...  0\n1        [609, 226, 413, 363, 211, 241, 988, 80, 12, 19...  0\n2        [22572, 3720, 233, 13, 827, 710, 512, 354, 1, ...  0\n3                             [345, 656, 25, 2589, 6, 866]  0\n4                                [29142, 8, 4141, 456, 24]  0\n                                                   ... ..\n1599995                         [256, 8, 80, 110, 25, 152]  4\n1599996  [609039, 22, 129, 184, 163, 9419, 769, 358, 10...  4\n1599997                       [140, 5715, 6540, 294, 1552]  4\n1599998  [59, 22771, 189, 387, 4483, 13, 10305, 112231,...  4\n1599999                [59, 15833, 200370, 609041, 609042]  4\n</code></pre>\n<p>but by doing</p>\n<pre><code>data.to_csv(&quot;foo.csv&quot;, index=True)\n</code></pre>\n<p>The problem is that each <code>list</code> is now saved as <code>str</code>. For example, row 3 is</p>\n<pre><code>&quot;[345, 656, 25, 2589, 6, 866]&quot;\n</code></pre>\n<p>And for those skeptics, I've tried <code>type(row 3)</code> and it's <code>str</code>. Column 2 is working well.</p>\n<p>How can I save as <code>list</code> each row and not as <code>str</code>? That is, how can I save all rows of <code>col 1</code>as <code>DataFrame</code> and not as <code>str</code>?</p>\n",
        "answer_body": "<p>Try this:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'a': [&quot;[1,2,3,4]&quot;, &quot;[6,7,8,9]&quot;]})\ndf['b'] = df['a'].apply(eval)\nprint(df)\n</code></pre>\n<p>The data in column b is now an array.</p>\n<pre><code>           a             b\n0  [1,2,3,4]  [1, 2, 3, 4]\n1  [6,7,8,9]  [6, 7, 8, 9]\n</code></pre>\n",
        "question_body": "<p>I have this <code>pd.DataFrame()</code> created. I want to create a <code>.csv</code> and it is saving as this</p>\n<pre><code>                                                         0  1\n0        [15921, 10, 82, 22, 202973, 368, 1055, 3135, 1...  0\n1        [609, 226, 413, 363, 211, 241, 988, 80, 12, 19...  0\n2        [22572, 3720, 233, 13, 827, 710, 512, 354, 1, ...  0\n3                             [345, 656, 25, 2589, 6, 866]  0\n4                                [29142, 8, 4141, 456, 24]  0\n                                                   ... ..\n1599995                         [256, 8, 80, 110, 25, 152]  4\n1599996  [609039, 22, 129, 184, 163, 9419, 769, 358, 10...  4\n1599997                       [140, 5715, 6540, 294, 1552]  4\n1599998  [59, 22771, 189, 387, 4483, 13, 10305, 112231,...  4\n1599999                [59, 15833, 200370, 609041, 609042]  4\n</code></pre>\n<p>but by doing</p>\n<pre><code>data.to_csv(&quot;foo.csv&quot;, index=True)\n</code></pre>\n<p>The problem is that each <code>list</code> is now saved as <code>str</code>. For example, row 3 is</p>\n<pre><code>&quot;[345, 656, 25, 2589, 6, 866]&quot;\n</code></pre>\n<p>And for those skeptics, I've tried <code>type(row 3)</code> and it's <code>str</code>. Column 2 is working well.</p>\n<p>How can I save as <code>list</code> each row and not as <code>str</code>? That is, how can I save all rows of <code>col 1</code>as <code>DataFrame</code> and not as <code>str</code>?</p>\n",
        "formatted_input": {
            "qid": 64183163,
            "link": "https://stackoverflow.com/questions/64183163/how-can-i-save-dataframe-as-list-and-not-as-string",
            "question": {
                "title": "How can I save DataFrame as list and not as string",
                "ques_desc": "I have this created. I want to create a and it is saving as this but by doing The problem is that each is now saved as . For example, row 3 is And for those skeptics, I've tried and it's . Column 2 is working well. How can I save as each row and not as ? That is, how can I save all rows of as and not as ? "
            },
            "io": [
                "                                                         0  1\n0        [15921, 10, 82, 22, 202973, 368, 1055, 3135, 1...  0\n1        [609, 226, 413, 363, 211, 241, 988, 80, 12, 19...  0\n2        [22572, 3720, 233, 13, 827, 710, 512, 354, 1, ...  0\n3                             [345, 656, 25, 2589, 6, 866]  0\n4                                [29142, 8, 4141, 456, 24]  0\n                                                   ... ..\n1599995                         [256, 8, 80, 110, 25, 152]  4\n1599996  [609039, 22, 129, 184, 163, 9419, 769, 358, 10...  4\n1599997                       [140, 5715, 6540, 294, 1552]  4\n1599998  [59, 22771, 189, 387, 4483, 13, 10305, 112231,...  4\n1599999                [59, 15833, 200370, 609041, 609042]  4\n",
                "\"[345, 656, 25, 2589, 6, 866]\"\n"
            ],
            "answer": {
                "ans_desc": "Try this: The data in column b is now an array. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'a': [\"[1,2,3,4]\", \"[6,7,8,9]\"]})\ndf['b'] = df['a'].apply(eval)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 41,
            "user_id": 14378300,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/133988098437910/picture?type=large",
            "display_name": "Fum Jimoji",
            "link": "https://stackoverflow.com/users/14378300/fum-jimoji"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 64169938,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1601717292,
        "creation_date": 1601628663,
        "last_edit_date": 1601630558,
        "question_id": 64168703,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64168703/pandas-new-column-with-index-of-unique-values-of-another-column",
        "title": "Pandas : new column with index of unique values of another column",
        "body": "<p>My dataframe:</p>\n<pre><code>ID       Name_Identify  ColumnA  ColumnB  ColumnC\n1        POM-OPP        D43      D03      D59\n2        MIAN-ERP       D80      D74      E34\n3        POM-OPP        E97      B56      A01\n4        POM-OPP        A66      D04      C34\n5        DONP28         B55      A42      A80\n6        MIAN-ERP       E97      D59      C34\n</code></pre>\n<p>Expected new dataframe:</p>\n<pre><code>ID       Name_Identify ColumnA  ColumnB  ColumnC    NEW_ID\n1        POM-OPP       D43      D03      D59        1\n2        MIAN-ERP      D80      D74      E34        2\n3        POM-OPP       E97      B56      A01        1\n4        POM-OPP       A66      D04      C34        1\n5        DONP28        B55      A42      A80        3\n6        MIAN-ERP      E97      D59      C34        2\n</code></pre>\n",
        "answer_body": "<pre><code>convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\ndf[&quot;NEW_ID&quot;] = df.Name_Identify.map(convert)\n</code></pre>\n<hr />\n<p><em>The explanation:</em></p>\n<p>In the first command we select <em>unique names</em> from the <code>Name_Identify</code> column</p>\n<pre><code>In[23]: df.Name_Identify.unique()\n</code></pre>\n<blockquote>\n<pre><code>array(['POM-OPP', 'MIAN-ERP', 'DONP28'], dtype=object)\n</code></pre>\n</blockquote>\n<p>and then create a dictionary from the <em>enumerated sequence of them</em> (the enumeration starts with <code>1</code>):</p>\n<pre><code>In[24]: convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\nIn[25]: convert\n</code></pre>\n<blockquote>\n<pre><code>{'POM-OPP': 1, 'MIAN-ERP': 2, 'DONP28': 3}\n</code></pre>\n</blockquote>\n<p>In the second command <em>we use this dictionary</em> for creating a new column by <em>converting all names in the <code>Name_Identify</code> column to appropriate numbers:</em></p>\n<pre><code>In[26]: df[&quot;NEW_ID&quot;] = df.Name_Identify.map(convert)\nIn[27]: df\n</code></pre>\n<blockquote>\n<pre><code>   D Name_Identify ColumnA ColumnB ColumnC  NEW_ID\n0  1       POM-OPP     D43     D03     D59       1\n1  2      MIAN-ERP     D80     D74     E34       2\n2  3       POM-OPP     E97     B56     A01       1\n3  4       POM-OPP     A66     D04     C34       1\n4  5        DONP28     B55     A42     A80       3\n5  6      MIAN-ERP     E97     D59     C34       2\n</code></pre>\n</blockquote>\n",
        "question_body": "<p>My dataframe:</p>\n<pre><code>ID       Name_Identify  ColumnA  ColumnB  ColumnC\n1        POM-OPP        D43      D03      D59\n2        MIAN-ERP       D80      D74      E34\n3        POM-OPP        E97      B56      A01\n4        POM-OPP        A66      D04      C34\n5        DONP28         B55      A42      A80\n6        MIAN-ERP       E97      D59      C34\n</code></pre>\n<p>Expected new dataframe:</p>\n<pre><code>ID       Name_Identify ColumnA  ColumnB  ColumnC    NEW_ID\n1        POM-OPP       D43      D03      D59        1\n2        MIAN-ERP      D80      D74      E34        2\n3        POM-OPP       E97      B56      A01        1\n4        POM-OPP       A66      D04      C34        1\n5        DONP28        B55      A42      A80        3\n6        MIAN-ERP      E97      D59      C34        2\n</code></pre>\n",
        "formatted_input": {
            "qid": 64168703,
            "link": "https://stackoverflow.com/questions/64168703/pandas-new-column-with-index-of-unique-values-of-another-column",
            "question": {
                "title": "Pandas : new column with index of unique values of another column",
                "ques_desc": "My dataframe: Expected new dataframe: "
            },
            "io": [
                "ID       Name_Identify  ColumnA  ColumnB  ColumnC\n1        POM-OPP        D43      D03      D59\n2        MIAN-ERP       D80      D74      E34\n3        POM-OPP        E97      B56      A01\n4        POM-OPP        A66      D04      C34\n5        DONP28         B55      A42      A80\n6        MIAN-ERP       E97      D59      C34\n",
                "ID       Name_Identify ColumnA  ColumnB  ColumnC    NEW_ID\n1        POM-OPP       D43      D03      D59        1\n2        MIAN-ERP      D80      D74      E34        2\n3        POM-OPP       E97      B56      A01        1\n4        POM-OPP       A66      D04      C34        1\n5        DONP28        B55      A42      A80        3\n6        MIAN-ERP      E97      D59      C34        2\n"
            ],
            "answer": {
                "ans_desc": " The explanation: In the first command we select unique names from the column and then create a dictionary from the enumerated sequence of them (the enumeration starts with ): In the second command we use this dictionary for creating a new column by converting all names in the column to appropriate numbers: ",
                "code": [
                    "convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\ndf[\"NEW_ID\"] = df.Name_Identify.map(convert)\n",
                    "In[24]: convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\nIn[25]: convert\n",
                    "In[26]: df[\"NEW_ID\"] = df.Name_Identify.map(convert)\nIn[27]: df\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 53,
            "user_id": 13315406,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-KEpv8VkbHHE/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJOwMc4f4bJGF-uUd93OcWsuiYQdiw/photo.jpg?sz=128",
            "display_name": "Lucas Lazari",
            "link": "https://stackoverflow.com/users/13315406/lucas-lazari"
        },
        "is_answered": true,
        "view_count": 873,
        "closed_date": 1601703653,
        "accepted_answer_id": 64177276,
        "answer_count": 3,
        "score": -1,
        "last_activity_date": 1601667675,
        "creation_date": 1601665160,
        "last_edit_date": 1601666414,
        "question_id": 64176921,
        "link": "https://stackoverflow.com/questions/64176921/drop-rows-based-on-condition-pandas",
        "closed_reason": "Duplicate",
        "title": "Drop rows based on condition pandas",
        "body": "<p>Consider I have a dataframe that looks like this:</p>\n<pre><code>   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n</code></pre>\n<p>What I need to do is to sum the C and D and if the sum is higher than 10 remove the entire row. Howerver I can't acess the columns by their names, I need to do it by their position.</p>\n<p>How can I do it in pandas?</p>\n<p>EDIT:\nAnother problem.</p>\n<pre><code>   A  B   C   D\n0  0  NaN   2   3\n1  4  5   NaN   NaN\n2  8  9  10  11\n</code></pre>\n<p>How can I keep the rows that have at least two values in the columns B, C and D?</p>\n",
        "answer_body": "<p>To keep  the rows that have at least two values in the columns B, C and D.\nYou can use this.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame({'A': [0,4,8], 'B':[1, np.nan, 9], 'C':[2,np.nan, np.nan], 'D':[3, 7, 11]})\nmask = df.iloc[:,1:].isnull().sum(axis=1) &lt; 2\nprint(df[mask])\n\n</code></pre>\n<p><strong>output</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>    A   B   C   D\n0   0   1.0 2.0 3\n2   8   9.0 NaN 11\n</code></pre>\n<p>For your first question you should fill remaining <code>nan</code> values using <code>df.fillna()</code>,\n<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\" rel=\"nofollow noreferrer\">Documentation</a> before moving with solution provided in earlier answers</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = new_df.fillna(0)\nprint(df)\n</code></pre>\n<p><strong>Output</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>   A    B    C   D\n0  0  1.0  2.0   3\n2  8  9.0  0.0  11\n</code></pre>\n<p>Now you can use <code>df.loc[df.iloc[:, 2:].sum(axis=1) &lt;= 10]</code> to drop the rows where the sum of <code>C</code> and  <code>D</code> is more than 10.</p>\n<p>Finally I recommend you to keep 1 post per question, this helps other people to search for it.</p>\n",
        "question_body": "<p>Consider I have a dataframe that looks like this:</p>\n<pre><code>   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n</code></pre>\n<p>What I need to do is to sum the C and D and if the sum is higher than 10 remove the entire row. Howerver I can't acess the columns by their names, I need to do it by their position.</p>\n<p>How can I do it in pandas?</p>\n<p>EDIT:\nAnother problem.</p>\n<pre><code>   A  B   C   D\n0  0  NaN   2   3\n1  4  5   NaN   NaN\n2  8  9  10  11\n</code></pre>\n<p>How can I keep the rows that have at least two values in the columns B, C and D?</p>\n",
        "formatted_input": {
            "qid": 64176921,
            "link": "https://stackoverflow.com/questions/64176921/drop-rows-based-on-condition-pandas",
            "question": {
                "title": "Drop rows based on condition pandas",
                "ques_desc": "Consider I have a dataframe that looks like this: What I need to do is to sum the C and D and if the sum is higher than 10 remove the entire row. Howerver I can't acess the columns by their names, I need to do it by their position. How can I do it in pandas? EDIT: Another problem. How can I keep the rows that have at least two values in the columns B, C and D? "
            },
            "io": [
                "   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n",
                "   A  B   C   D\n0  0  NaN   2   3\n1  4  5   NaN   NaN\n2  8  9  10  11\n"
            ],
            "answer": {
                "ans_desc": "To keep the rows that have at least two values in the columns B, C and D. You can use this. output For your first question you should fill remaining values using , Documentation before moving with solution provided in earlier answers Output Now you can use to drop the rows where the sum of and is more than 10. Finally I recommend you to keep 1 post per question, this helps other people to search for it. ",
                "code": [
                    "df = pd.DataFrame({'A': [0,4,8], 'B':[1, np.nan, 9], 'C':[2,np.nan, np.nan], 'D':[3, 7, 11]})\nmask = df.iloc[:,1:].isnull().sum(axis=1) < 2\nprint(df[mask])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 307,
            "user_id": 8961082,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/956f7a116659efc476e1a9bbecaae32d?s=128&d=identicon&r=PG&f=1",
            "display_name": "d789w",
            "link": "https://stackoverflow.com/users/8961082/d789w"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 64165702,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1601608195,
        "creation_date": 1601604520,
        "question_id": 64165393,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64165393/add-character-to-column-based-on-ascending-order-of-another-column-if-condition",
        "title": "Add character to column based on ascending order of another column if condition met pandas",
        "body": "<p>Stuck on a data problem in pandas. See data below:</p>\n<pre><code>| Product | Level | Cost |\n --------- ------- ------\n| Prod_A  | L1    | 100  |\n| Prod_A  | L1    | 100  |\n| Prod_A  | L1    | 200  |\n| Prod_A  | L2    | 100  |\n| Prod_A  | L3    | 100  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L2    | 200  |\n| Prod_B  | L2    | 300  |\n| Prod_C  | L3    | 100  |\n</code></pre>\n<p>The rules are:</p>\n<ol>\n<li>Only one Cost for each unique (Product, Level) combination.</li>\n<li>If multiple Cost for each unique (Product, Level) combination, add a letter to the Level value (L1 A, L1 B, etc) based on the Cost value (L1 A being the smallest Cost).</li>\n<li>If (Product, Level) combination has a unique Cost then do nothing.</li>\n</ol>\n<p>Desired output:</p>\n<pre><code>| Product | Level | Cost |\n --------- ------- ------\n| Prod_A  | L1 A  | 100  |\n| Prod_A  | L1 A  | 100  |\n| Prod_A  | L1 B  | 200  |\n| Prod_A  | L2    | 100  |\n| Prod_A  | L3    | 100  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L2 A  | 200  |\n| Prod_B  | L2 B  | 300  |\n| Prod_C  | L3    | 100  |\n</code></pre>\n",
        "answer_body": "<p>Here's one way:</p>\n<pre><code>charlist='ABCDEFG'\ndd = {k:' '+v for k, v in enumerate(charlist)}\ndf['Level'] += df.groupby(['Product', 'Level'])['Cost']\\\n                 .transform(lambda x: x.factorize()[0] if x.nunique()&gt;1 else -1)\\\n                 .map(dd).fillna('')\n</code></pre>\n<p>Output:</p>\n<pre><code>  Product Level  Cost\n0  Prod_A  L1 A   100\n1  Prod_A  L1 A   100\n2  Prod_A  L1 B   200\n3  Prod_A    L2   100\n4  Prod_A    L3   100\n5  Prod_B    L1   150\n6  Prod_B    L1   150\n7  Prod_B  L2 A   200\n8  Prod_B  L2 B   300\n9  Prod_C    L3   100\n</code></pre>\n<p><em><strong>Details:</strong></em></p>\n<p>First, create dictionary of the characters to append.</p>\n<p>Then <code>groupby</code> product and level using <code>transform</code> unique &quot;encode&quot; each Cost with <code>pd.Series.factorize</code> if there is only one Cost amount then use -1.</p>\n<p>Lastly, map the results of the  &quot;encoded&quot; cost using the dictionary and fillna with a blank string.</p>\n",
        "question_body": "<p>Stuck on a data problem in pandas. See data below:</p>\n<pre><code>| Product | Level | Cost |\n --------- ------- ------\n| Prod_A  | L1    | 100  |\n| Prod_A  | L1    | 100  |\n| Prod_A  | L1    | 200  |\n| Prod_A  | L2    | 100  |\n| Prod_A  | L3    | 100  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L2    | 200  |\n| Prod_B  | L2    | 300  |\n| Prod_C  | L3    | 100  |\n</code></pre>\n<p>The rules are:</p>\n<ol>\n<li>Only one Cost for each unique (Product, Level) combination.</li>\n<li>If multiple Cost for each unique (Product, Level) combination, add a letter to the Level value (L1 A, L1 B, etc) based on the Cost value (L1 A being the smallest Cost).</li>\n<li>If (Product, Level) combination has a unique Cost then do nothing.</li>\n</ol>\n<p>Desired output:</p>\n<pre><code>| Product | Level | Cost |\n --------- ------- ------\n| Prod_A  | L1 A  | 100  |\n| Prod_A  | L1 A  | 100  |\n| Prod_A  | L1 B  | 200  |\n| Prod_A  | L2    | 100  |\n| Prod_A  | L3    | 100  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L2 A  | 200  |\n| Prod_B  | L2 B  | 300  |\n| Prod_C  | L3    | 100  |\n</code></pre>\n",
        "formatted_input": {
            "qid": 64165393,
            "link": "https://stackoverflow.com/questions/64165393/add-character-to-column-based-on-ascending-order-of-another-column-if-condition",
            "question": {
                "title": "Add character to column based on ascending order of another column if condition met pandas",
                "ques_desc": "Stuck on a data problem in pandas. See data below: The rules are: Only one Cost for each unique (Product, Level) combination. If multiple Cost for each unique (Product, Level) combination, add a letter to the Level value (L1 A, L1 B, etc) based on the Cost value (L1 A being the smallest Cost). If (Product, Level) combination has a unique Cost then do nothing. Desired output: "
            },
            "io": [
                "| Product | Level | Cost |\n --------- ------- ------\n| Prod_A  | L1    | 100  |\n| Prod_A  | L1    | 100  |\n| Prod_A  | L1    | 200  |\n| Prod_A  | L2    | 100  |\n| Prod_A  | L3    | 100  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L2    | 200  |\n| Prod_B  | L2    | 300  |\n| Prod_C  | L3    | 100  |\n",
                "| Product | Level | Cost |\n --------- ------- ------\n| Prod_A  | L1 A  | 100  |\n| Prod_A  | L1 A  | 100  |\n| Prod_A  | L1 B  | 200  |\n| Prod_A  | L2    | 100  |\n| Prod_A  | L3    | 100  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L1    | 150  |\n| Prod_B  | L2 A  | 200  |\n| Prod_B  | L2 B  | 300  |\n| Prod_C  | L3    | 100  |\n"
            ],
            "answer": {
                "ans_desc": "Here's one way: Output: Details: First, create dictionary of the characters to append. Then product and level using unique \"encode\" each Cost with if there is only one Cost amount then use -1. Lastly, map the results of the \"encoded\" cost using the dictionary and fillna with a blank string. ",
                "code": [
                    "charlist='ABCDEFG'\ndd = {k:' '+v for k, v in enumerate(charlist)}\ndf['Level'] += df.groupby(['Product', 'Level'])['Cost']\\\n                 .transform(lambda x: x.factorize()[0] if x.nunique()>1 else -1)\\\n                 .map(dd).fillna('')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "for-loop",
            "data-cleaning"
        ],
        "owner": {
            "reputation": 237,
            "user_id": 5795833,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/ea6f2a5e8969798e3dbd9b665744ef06?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mitchell",
            "link": "https://stackoverflow.com/users/5795833/mitchell"
        },
        "is_answered": true,
        "view_count": 83,
        "accepted_answer_id": 64160239,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1601574354,
        "creation_date": 1601569467,
        "question_id": 64159489,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/64159489/divide-a-pandas-dataframe-by-the-sum-of-its-index-column-and-row",
        "title": "Divide a pandas dataframe by the sum of its index column and row",
        "body": "<p>Here is what I currently have:</p>\n<pre><code>    print(df)\n\n    10   25  26\n10  530  1   46  \n25  1    61  61\n26  46   61  330\n</code></pre>\n<p>How can i transform this to df1 so that we divide each element in the row by the sum of the index columns? The output of df1 should look like this:</p>\n<pre><code>df1:\n\n\n    10             25               26\n10  530/(530)     1/(530+61)       46/(530+330)  \n25  1/(61+530)    61/(61)          61/(61+330)\n26  46/(330+530)  61/(330+61)      330/(330)\n\n    print(df1)\n\n    10      25        26\n10  1       0.0016    0.0534\n25  0.0016  1         0.1560\n26  0.0534  0.1560    1\n</code></pre>\n",
        "answer_body": "<p>IIUC, try:</p>\n<pre><code>a = np.diag(df)[None, :]\nb = np.diag(df)[:, None]\n\nc = a+b\nnp.fill_diagonal(c, np.diag(df))\n\ndf_out = df.div(c)\ndf_out\n</code></pre>\n<p>Output:</p>\n<pre><code>          10        25        26\n10  1.000000  0.001692  0.053488\n25  0.001692  1.000000  0.156010\n26  0.053488  0.156010  1.000000\n</code></pre>\n",
        "question_body": "<p>Here is what I currently have:</p>\n<pre><code>    print(df)\n\n    10   25  26\n10  530  1   46  \n25  1    61  61\n26  46   61  330\n</code></pre>\n<p>How can i transform this to df1 so that we divide each element in the row by the sum of the index columns? The output of df1 should look like this:</p>\n<pre><code>df1:\n\n\n    10             25               26\n10  530/(530)     1/(530+61)       46/(530+330)  \n25  1/(61+530)    61/(61)          61/(61+330)\n26  46/(330+530)  61/(330+61)      330/(330)\n\n    print(df1)\n\n    10      25        26\n10  1       0.0016    0.0534\n25  0.0016  1         0.1560\n26  0.0534  0.1560    1\n</code></pre>\n",
        "formatted_input": {
            "qid": 64159489,
            "link": "https://stackoverflow.com/questions/64159489/divide-a-pandas-dataframe-by-the-sum-of-its-index-column-and-row",
            "question": {
                "title": "Divide a pandas dataframe by the sum of its index column and row",
                "ques_desc": "Here is what I currently have: How can i transform this to df1 so that we divide each element in the row by the sum of the index columns? The output of df1 should look like this: "
            },
            "io": [
                "    print(df)\n\n    10   25  26\n10  530  1   46  \n25  1    61  61\n26  46   61  330\n",
                "df1:\n\n\n    10             25               26\n10  530/(530)     1/(530+61)       46/(530+330)  \n25  1/(61+530)    61/(61)          61/(61+330)\n26  46/(330+530)  61/(330+61)      330/(330)\n\n    print(df1)\n\n    10      25        26\n10  1       0.0016    0.0534\n25  0.0016  1         0.1560\n26  0.0534  0.1560    1\n"
            ],
            "answer": {
                "ans_desc": "IIUC, try: Output: ",
                "code": [
                    "a = np.diag(df)[None, :]\nb = np.diag(df)[:, None]\n\nc = a+b\nnp.fill_diagonal(c, np.diag(df))\n\ndf_out = df.div(c)\ndf_out\n"
                ]
            }
        }
    }
]