"qid": 58085046
"link": https://stackoverflow.com/questions/58085046/pandas-create-a-column-and-assign-values-to-it-from-a-dictionary
"question": {
	"title": pandas create a column and assign values to it from a dictionary
	"desc": I have a dictionary looks like this, I have a df looks like this, I like to create a column in whose values will be based on the values in , so the result will look like, I am wondering whats the best way to do this. 
}
"io": {
	"Frame-1": 
		id    code
		1     SA01
		2     SA02
		3     SA03
		4     AP01
		5     AP02
		6     AP03
		
	"Frame-2":
		id    code    region
		1     SA01    South America
		2     SA02    South America
		3     SA03    South America
		4     AP01    Asia Pacific
		5     AP02    Asia Pacific
		6     AP03    Asia Pacific
		
}
"answer": {
	"desc": %s You could redefine your dictionary ( here) to have an individual entry for each code that appears in the strings and use it to map the values in the column: 
	"code-snippets": [
		d_ = {code:sd['name'] for sd in d['regions'] for code in sd['code'].split(',')} 
		# {'SA01': 'South America', 'SA02': 'South America', 'SA03': 'South America',...
		df['region'] = df.code.map(d_)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58086144
"link": https://stackoverflow.com/questions/58086144/replicate-multiple-rows-of-events-for-specific-ids-multiple-times
"question": {
	"title": Replicate multiple rows of events for specific IDs multiple times
	"desc": I have a call log data made on customers. Which looks something like below, where ID is customer ID and A and B are log attributes: I want to replicate each set of event for each ID based on some slots. For e.g. if slot value is 2 then all events for ID "A" should be replicated slot-1 times. and a new Index should be created indicating which slot does replicated values belong to: I have tried following solution: it gives me the expected output but is not scalable when slots are increased and number of customers increases in order of 10k. I think its taking a long time because of the loop. Any solution which is vectorized will be really helpful. 
}
"io": {
	"Frame-1": 
		  ID   A   B
		A  A  46  31
		A  A  99  54
		A  A  34   9
		B  B  46  48
		B  B   7  75
		C  C   1  25
		C  C  71  40
		C  C  74  53
		D  D  57  17
		D  D  19  78
		
	"Frame-2":
		  ID   A   B
		A  A  46  31
		A  A  99  54
		A  A  34   9
		
		A  A  46  31
		A  A  99  54
		A  A  34   9
		
}
"answer": {
	"desc": %s You can try: Output: 
	"code-snippets": [
		slots = 2
		new_df = pd.concat(df.assign(Index=f'_{i}') for i in range(1, slots+1))
		
		new_df['Index'] = new_df['ID'] + new_df['Index']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 58060206
"link": https://stackoverflow.com/questions/58060206/how-to-change-values-of-rows-based-on-conditions-in-dataframe
"question": {
	"title": How to change values of rows based on conditions in dataframe?
	"desc": I have dataframe that is shown below, (Need some magic to be done) The change should be made in type column such that, in a row if is and is then next row of should be assigned . Full dataframe should look like this: 
}
"io": {
	"Frame-1": 
		    type  label
		0      0      0
		1      0      0
		2      0      0
		3      0      0
		4      2      1
		5      2      1
		6      2      1
		7      2      1
		8      2      1
		9      2      1
		10     0      0
		11     0      0
		12     0      0
		13     0      0
		14     0      0
		15     0      0
		16     0      0
		17     0      0
		18     0      0
		19     0      0 
		
	"Frame-2":
		    type  label
		0      0      0
		1      2      0
		2      2      0
		3      0      0
		4      2      1
		5      2      1
		6      2      1
		7      2      1
		8      2      1
		9      2      1
		10     0      0
		11     2      0
		12     2      0
		13     2      0
		14     2      0
		15     2      0
		16     2      0
		17     2      0
		18     2      0
		19     2      0 
		
}
"answer": {
	"desc": %s Using to mask row which has and columns value is 0 and to shift index by desired number of periods with an optional time freq. Ex. O/P: 
	"code-snippets": [
		m = df['type'].eq(0) & df['label'].eq(0)
		df.loc[m == m.shift(1),'type'] = 2
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 38445974
"link": https://stackoverflow.com/questions/38445974/count-of-rows-where-given-columns-of-a-dataframe-are-non-zero
"question": {
	"title": Count of rows where given columns of a DataFrame are non-zero
	"desc": I have a that looks like this: I would like to have another matrix which gives me the number of non-zero elements for the intersection of every column except for . For example, the intersection of columns and would be 2 (because 1 and 3 have non-zero values for and ), intersection of and would be 2 as well (because 1 and 3 have non-zero values for and ). The final matrix would look like this: As we can see, it should be a symmetric matrix, similar to a correlation matrix, but not the correlation matrix. Intersection of any 2 columns = # of having non-zero values in both columns. I would show some initial code here but I feel like there would be a simple function to do this task that I don't know of. Here's the code to create the : Any pointers would be appreciated. TIA. 
}
"io": {
	"Frame-1": 
		MemberID    A    B    C    D
		1           0.3  0.5 0.1   0
		2           0    0.2 0.9   0.3
		3           0.4  0.2 0.5   0.3
		4           0.1  0   0     0.7
		
	"Frame-2":
		    A    B    C    D
		A   3    2    2    2
		B   2    3    3    2
		C   2    3    3    2
		D   2    2    2    3
		
}
"answer": {
	"desc": %s This should to it: 
	"code-snippets": [
		z = (df != 0) * 1
		z.T.dot(z)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57926657
"link": https://stackoverflow.com/questions/57926657/conditional-pairwise-calculations-in-pandas
"question": {
	"title": Conditional pairwise calculations in pandas
	"desc": For example, I have 2 dfs: df1 and another df is df2 I want to calculate first pairwise subtraction from df2 to df1. I am using using a function Now, I want to check, these new columns name like and . I am checking if there is any values in this new less than 5. If there is, then I want to do further calculations. Like this. For example, here for columns name , less than 5 value is 4 which is at . Now in this case, I want to subtract columns name of but at row 3, in this case it would be value . I want to subtract this value 2 with but at row 1 (because column name ) was from value at row 1 in . My is so complex for this. It would be great, if there would be some easier way in pandas. Any help, suggestions would be great. The expected new dataframe is this 
}
"io": {
	"Frame-1": 
		ID,col1,col2
		1,5,9
		2,6,3
		3,7,2
		4,8,5
		
	"Frame-2":
		0,1,2
		Nan,Nan,Nan
		Nan,Nan,Nan
		(2-9)=-7,Nan,Nan
		(5-9)=-4,(5-7)=-2,Nan
		
}
"answer": {
	"desc": %s Similar to Ben's answer, but with : Output: 
	"code-snippets": [
		pd.DataFrame(np.where(dist_df<5, df1.col2.values[:,None] - df2.col2.values, np.nan),
		             index=dist_df.index,
		             columns=dist_df.columns)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57857068
"link": https://stackoverflow.com/questions/57857068/pandas-dataframe-column-headers-to-labels-for-data
"question": {
	"title": Pandas dataframe column headers to labels for data
	"desc": SUMMARY: The output of my code gives me a dataframe of the following format. The column headers of the dataframe are the labels for the text in the column . The labels will be used as training data for a multilabel classifier in the next step. This is a snippet of actual data which is much larger. Since they are columns titles, it is not possible to use them as mapped to the text they are the labels for. UPDATE: Converting the df to csv shows the empty cells are blank( vs ): Where is the column where the text is, and , , , , and are the column headers that need to be turned into the labels. Only columns with 1s or 2s are relevant. The column with empty cells are not relevant and thus don't need to be converted as labels. UPDATE: After some digging, maybe the numbers might not be ints, but strings. I know that when entering the text + labels into a classifier for processing, the length of both arrays needs to be equal, else it is not accepted as valid input. Is there a way I can convert the columns titles to labels for the text in in the DF? EXPECTED OUTPUT: 
}
"io": {
	"Frame-1": 
		Content  A  B  C  D  E
		    zxy  1  2     1   
		    wvu  1     2  1   
		    tsr  1  2        2
		    qpo     1  1  1   
		    nml        2  2   
		    kji  1     1     2
		    hgf        1     2
		    edc  1  2     1              
		
	"Frame-2":
		>>Content  A  B  C  D  E     Labels
		0   zxy    1  2     1        A, B, D  
		1   wvu    1     2  1        A, C, D
		2   tsr    1  2        2     A, B, E
		3   qpo       1  1  1        B, C, D
		4   nml          2  2        C, D    
		5   kji    1     1     2     A, C, E
		6   hgf          1     2     C, E
		7   edc    1  2     1        A, B, D   
		
}
"answer": {
	"desc": %s Full Solution: We can do , notice here, I treat the blanks as , if that is a real blank in your data, change the last line 
	"code-snippets": [
		# make certain the label names match the appropriate columns 
		s=df.loc[:, ['A', 'B', 'C', 'D', 'E']]  
		# or
		s=df.loc[:,'A':]
		
		df['Labels']=(s>0).dot(s.columns+',').str[:-1]  # column A:E need to be numeric, not str
		# df['Labels']=(~s.isin(['']).dot(s.columns+',').str[:-1]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57873651
"link": https://stackoverflow.com/questions/57873651/identify-common-elements-between-df-rows-to-create-a-new-column
"question": {
	"title": identify common elements between df rows to create a new column
	"desc": My df is shown below. I want to create a new column called common which contains which other key has the same value as my current key. The final dataframe would look like: The only way I can think of is to create a column with empty dictionaries and then have two loops to get the result. I wanted to know if there is an easy way to do this. Thanks 
}
"io": {
	"Frame-1": 
		    key       val    
		0   A1  [1, 2, 3, 4]
		1   A2  [1, 2, 7, 9]    
		2   A3  [1, 3, 5]   
		3   A4  [6, 9]  
		4   A5  [8] 
		
	"Frame-2":
		   key        val      common
		0   A1  [1, 2, 3, 4]   {'A2':[1, 2], 'A3':[1, 3]} 
		1   A2  [1, 2, 7, 9]   {'A1':[1, 2], 'A3':[1], 'A4':[9], 'A5':[7]}
		2   A3  [1, 3, 5]      {'A1':[1, 3], 'A2':[1]}
		3   A4  [6, 9]         {'A2':[9]}
		4   A5  [8]            {}
		
}
"answer": {
	"desc": %s Here is one way using first then Update 
	"code-snippets": [
		l={x: y.reset_index(level=0,drop=True).to_dict()for x , y in s.groupby(level=0)}
		
		df['common']=pd.Series(l).reindex(df.Key).values
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57864979
"link": https://stackoverflow.com/questions/57864979/print-dataframe-without-float-precision
"question": {
	"title": Print dataframe without float precision
	"desc": I would like to print a dataframe in my console without displaying any float decimal precision. The output I got after printing is: Whereas what I expected is: There seems to be an issue to display the tuple without float precision. Any idea why ? Cheers 
}
"io": {
	"Frame-1": 
		0   (a, 2.01212121212)
		
		1                1.12
		
		2                8.12
		
	"Frame-2":
		0            (a, 2.01)
		
		1                1.12
		
		2                8.12
		
}
"answer": {
	"desc": %s You can do like this (very specific to this problem): Output: 
	"code-snippets": [
		pd_tmp = pd.DataFrame()
		pd_tmp["new_column"] = [("a", 2.01212121212), 1.123123, 8.1212]
		
		
		def foo(pd_col):
		    pd_new_col = []
		    for i in pd_col:
		        if not isinstance(i, float):
		            lst = []
		            for j in i:
		                if not isinstance(j, float):
		                    k = j
		                    lst.append(k)
		                else:
		                    k = round(j, 2)
		                    lst.append(k)
		            pd_new_col.append(tuple(lst))
		
		        else:
		            pd_new_col.append(round(i, 2))
		
		   return pd_new_col
		
		
		pd_tmp["new_column"] = foo(pd_tmp["new_column"])
		print(pd_tmp)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57842073
"link": https://stackoverflow.com/questions/57842073/pandas-how-to-drop-rows-when-all-float-columns-are-nan
"question": {
	"title": pandas how to drop rows when all float columns are NaN
	"desc": I have the following df With the following dtypes Is there a way to drop rows only when ALL float columns are NaN? output: I can't do it with df.dropna(subset=['ID1','ID2','ID3','ID4']) because my real df has several dynamic floating columns. Thanks 
}
"io": {
	"Frame-1": 
		  AAA BBB CCC DDD  ID1  ID2  ID3  ID4
		0 txt txt txt txt  10   NaN  12   NaN
		1 txt txt txt txt  10   NaN  12   13
		2 txt txt txt txt  NaN  NaN  NaN  NaN
		
	"Frame-2":
		  AAA BBB CCC DDD  ID1  ID2  ID3  ID4
		0 txt txt txt txt  10   NaN  12   NaN
		1 txt txt txt txt  10   NaN  12   13
		
}
"answer": {
	"desc": %s Use for get all float columns, then test for non missing values and select by for at least one non misisng value per row - so misising floats rows are removed: Your solution with should be changed for pass float columns and parameter for test if all s per rows: If possible multiple types of floats check by : 
	"code-snippets": [
		df1 = df.dropna(subset=df.select_dtypes(float).columns, how='all')
		#for return same dataframe 
		#df.dropna(subset=df.select_dtypes(float).columns, how='all', inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57801048
"link": https://stackoverflow.com/questions/57801048/subtract-previous-row-value-from-the-current-row-value-in-a-pandas-column
"question": {
	"title": Subtract previous row value from the current row value in a Pandas column
	"desc": I have a pandas column with the name 'values' containing respective values . I want to subtract the each value from the next value so that I get the following format: I've tried to solve this using a for loop that loops over all the data-frame but this method was time consuming. Is there a straightforward method the dataframe functions to solve this problem quickly? The output we desire is the following: 
}
"io": {
	"Frame-1": 
		10 15 36 95 99
	"Frame-2":
		10 5 21 59 4
}
"answer": {
	"desc": %s Use with : Output: 
	"code-snippets": [
		import pandas as pd
		
		s = pd.Series([11,15,22,27,36,69,77])
		s.diff().fillna(s)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57726939
"link": https://stackoverflow.com/questions/57726939/pandas-integers-to-negative-integer-powers-are-not-allowed
"question": {
	"title": pandas Integers to negative integer powers are not allowed
	"desc": I have a , I want to create a new column based on the following calculation: but I got the following error, I am wondering how to get around this, so the result will look like, 
}
"io": {
	"Frame-1": 
		decimal_places    amount
		 2                10
		 3                100
		 1                1000
		
	"Frame-2":
		decimal_places    amount    converted_amount
		   2                10           10
		   3                100          10
		   1                1000         10000
		
}
"answer": {
	"desc": %s There are two problems here. Firstly as the error suggests, the values must be floats. But secondly, by setting , the values in are cast to integer as all decimal places are , and hence you'll get the same error. So you want: For a faster approach you could work with the underlying numpy arrays: 
	"code-snippets": [
		df = df.astype(float)
		df = (df.assign(
		      converted_amount=lambda x: x.amount * 10 ** (2 - x.decimal_places.fillna(2))))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57650114
"link": https://stackoverflow.com/questions/57650114/pandas-dataframe-uniformly-scale-down-values-when-column-sum-exceeds-treshold
"question": {
	"title": Pandas dataframe: uniformly scale down values when column sum exceeds treshold
	"desc": Initial Situation Consider the following example dataframe: which in printed form looks like: Desired Result I would now like to do the following for each column of this dataframe: Calculate the sum of the column's values (ignoring any NaN values). If the sum exceeds 10.0, then I want to uniformly scale down all values in the column such that the new sum is exactly 10.0 (again ignoring any NaN values). Basically I'd like to obtain a result dataframe that looks like this: Tried thus far The following code obtains the desired result. However this code feels a bit verbose and inefficient to me. Based on my experience with pandas thus far I'd suspect that a more vectorized solution is still possible. Would anyone be able to help me find this? 
}
"io": {
	"Frame-1": 
		     A    B    C    D
		0  3.0  7.0  4.0  1.0
		1  2.0  NaN  5.0  0.0
		2  1.0  1.0  1.0  2.0
		3  NaN  3.0  2.0  3.0
		
	"Frame-2":
		     A         B         C    D
		0  3.0  6.363636  3.333333  1.0
		1  2.0       NaN  4.166667  0.0
		2  1.0  0.909091  0.833333  2.0
		3  NaN  2.727273  1.666667  3.0
		
}
"answer": {
	"desc": %s Here is one method: 
	"code-snippets": [
		thres = 10
		result = df * thres / df.sum().clip(lower=thres)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57589538
"link": https://stackoverflow.com/questions/57589538/identical-dictionaries-in-dataframe-all-changing-at-once
"question": {
	"title": Identical dictionaries in DataFrame all changing at once
	"desc": I'm working on a project and currently experiencing an issue with populating some dictionaries within a DataFrame. The problem is more complicated but essentially the main issue can be simplified as follows: I have a DataFrame of dictionaries, all of which initially empty, say When I attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one I'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a': I'm assuming this behaviour is caused by using in my DataFrame initialization, but I'm not familiar enough with Python to understand why. Is Python creating one dictionary and passing references to it to populate the DataFrame? If so, how could I initialise it to create individual dictionaries? I found that I can create a deep copy of each dictionary before processing them, i.e. , but I'm curious if there is a way of doing it without resorting to that. 
}
"io": {
	"Frame-1": 
		    0   1   
		
		0   {}  {}  
		1   {}  {}  
		2   {}  {}  
		
	"Frame-2":
		          0               1         
		0   {'char': 'a'}   {'char': 'a'}   
		1   {'char': 'a'}   {'char': 'a'}
		2   {'char': 'a'}   {'char': 'a'}   
		
}
"answer": {
	"desc": %s  I have a DataFrame of dictionaries, all of which initially empty If you multiply a list with an integer, you do not make a deep copy, you just copy the references to a new list. This thus means that you made a single dictionary, not two identical ones. So you have one dictionary, and all the six cells in the dataframe refer to the same dictionary. If you modify that dictionary, of course all the cells see that change. You can use list comprehension to make different dictionaries, for example: or for an arbitrary length rows and columns: 
	"code-snippets": [
		example = pd.DataFrame([[{} for _ in range(2)] for _ in range(3)], index=range(0, 3))
		----------------------------------------------------------------------
		example = pd.DataFrame([[{} for _ in range(n)] for _ in range(m)])
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57533654
"link": https://stackoverflow.com/questions/57533654/ignore-nulls-in-pandas-map-dictionary
"question": {
	"title": Ignore Nulls in pandas map dictionary
	"desc": My Dataframe looks like this : I am trying to label encode with nulls as such. My result should look like: The code i tried : Achieved Result : Expected Result : 
}
"io": {
	"Frame-1": 
		COL1    COL2    COL3
		A        M       X
		B        F       Y
		NaN      M       Y
		A        nan     Y
		
	"Frame-2":
		COL1_    COL2_    COL3_
		0        0       0
		1        1       1
		NaN      0       1
		0        nan     1
		
}
"answer": {
	"desc": %s Try using the below code, I first use the function, than I drop the NaNs, then I convert it into a list then I use the method for each value in the new list, and gives the index of the first occurence of the value, after that convert it into the Series, and make the the index of the series without NaNs, I am doing that since after I drop the NaNs it will turn from index 0, 1, 2, 3 to 0, 2, 3 or something like that, whereas the missing index will be NaN again, after that I add a underscore to each column, and I it with the original dataframe: Output: 
	"code-snippets": [
		print(df.join(df.apply(lambda x: pd.Series(map(x.dropna().tolist().index, x.dropna()), index=x.dropna().index)).add_suffix('_')))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57515994
"link": https://stackoverflow.com/questions/57515994/changing-the-increment-value-in-data-frame-at-certain-row
"question": {
	"title": Changing the Increment Value in Data Frame at Certain Row
	"desc": I have this data frame: I wanted to create another column called 'Seconds' that increments by 10 every row so I wrote this code: This produces the data frame: I now want to make the Seconds column increment by 10 until the 5th row. At the 5th row, I want to change the increment to 0.1 until the 7th row. At the 7th row, I want to change the increment back to 10. So, I want the data frame to look like this: How would I go about doing this? Should I change the index and multiply the index.values by a different value when I get to the row where the increment needs to change? Thanks in advance for your help. 
}
"io": {
	"Frame-1": 
		Seconds   Power
		  10       15
		  20       15
		  30       10
		  40       30
		  50       15
		  60       90
		  70       100
		  80       22
		  90       15
		
	"Frame-2":
		Seconds   Power
		  10       15
		  20       15
		  30       10
		  40       30
		  50       15
		  50.1     90
		  50.2     100
		  60.2      22
		  70.2      15
		
}
"answer": {
	"desc": %s You can use + In a more general sense, it is probably more intuitive to not have to calculate out the repeats by hand, and it would be easier to define them not by their difference with the previous value, but by absolute location in the array. Using some arithmetic, we can create a function that does all of the grunt-work for you. 
	"code-snippets": [
		np.repeat([10, 0.1, 10], [5, 2, 2]).cumsum()
		#                         ^  ^  ^
		#                        5th ^  ^
		#                           7th ^
		#                               Until end -> df.shape - increments[:-1].sum() more generally
		
		----------------------------------------------------------------------
		array([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])
		
		----------------------------------------------------------------------
		def cumsum_custom_increment(increments, points, n):
		    points[1:] = np.diff(points)
		    d = np.r_[points, n - np.sum(points)]
		    return np.repeat(increments, d).cumsum()
		
		>>> cumsum_custom_increment([10, 0.1, 10], [5, 7], df.shape[0])
		array([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57481676
"link": https://stackoverflow.com/questions/57481676/in-a-dataframe-how-can-i-count-a-specific-value-and-then-select-the-value-with-t
"question": {
	"title": In a dataframe how can I count a specific value and then select the value with the highest count to create another dataframe?
	"desc": I am looking for a way to select specific rows of data from a dataframe. Here is an example of the dataframe. I am looking for an output frame like this: Note, ID 006DE4E3 is not in the output because there the counts of the value was equal. Thank You! 
}
"io": {
	"Frame-1": 
		Id  \  Value
		0    002D85EF   5
		1    002D85EF   1
		2    002D85EF   5
		3    00557D1B   1
		4    00557D1B   1
		5    00557D1B   5
		6    0063EAFB   5
		7    0063EAFB   5
		8    0063EAFB   5
		9    006DE4E3   1
		10   006DE4E3   5
		11   006DE4E3   1
		12   006DE4E3   5
		
	"Frame-2":
		Id  \  Value
		0    002D85EF   5
		1    00557D1B   1
		2    0063EAFB   5
		
}
"answer": {
	"desc": %s (Since i can not comment, i directly try to give You a hint, probably not the answer.) try: or btw. the given row ids in Your answer do not match the above frame row ids and values. Also i don't understand why You do not select the row edit after clarification, i think You want is: return the first of each occurrence of the Id (in a sorted frame). But only, if the all values of the ids group are not distributed equally. For that my answer is: In this example the frame c is unsorted... 
	"code-snippets": [
		import numpy as np
		import pandas as pd
		a = np.random.randint(5, high=10, size=(20, 1))
		b = np.random.choice(['a', 'b', 'c', 'd'], 20)[:, None]
		c = pd.DataFrame(np.hstack([b,a]), columns=['id', 'value'])
		
		
		def first_or_none(grp, col_name):
		    cnts = grp.groupby(col_name).count()
		    if len(cnts) == len(cnts.nunique()):
		        return None
		    else:
		        return grp.iloc[0]
		
		c.groupby(['id']).apply(first_or_none, 'value').dropna()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57479385
"link": https://stackoverflow.com/questions/57479385/how-to-rearrange-reorder-the-rows-and-columns-in-python-dataframe
"question": {
	"title": How to rearrange/reorder the rows and columns in python dataframe?
	"desc": SCREEN SHOT OF ACTUAL DATA FRAMEDataframe of 5000 rows and 192 columns I want to change the size of my data frame of m rows and n columns (m= 5000 and n = 192) into a size of n/3 rows(64 rows) and m*5000 columns(15000 columns)?? existing data frame DESIRED data frame 
}
"io": {
	"Frame-1": 
		0 A1 A2 A3 A4 A5 A6 A7 A8 A9.....A192
		1 B1 B2 B3 B4 B5 B6 B7 B8 B9.....B192
		.
		.
		.
		5000 192 X1 X2 X3 X4 X5 X6 X7 X8 X9.....X192
		
	"Frame-2":
		0 A1 A2 A3 B1 B2 B3.....X1 X2 X3
		1 A4 A5 A6 B4 B5 B6.....X4 X5 X6
		2 A7 A8 A9 B7 B8 B9.....X7 X8 X9
		.
		.
		64 A190 A191 A192 B190 B191 B192.....X190 X191 X192
		
}
"answer": {
	"desc": %s IIUC, can use a loop 
	"code-snippets": [
		pd.DataFrame([df.iloc[:, e:e+3].values.flatten() for e in range(0, 192, 3)])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57428633
"link": https://stackoverflow.com/questions/57428633/change-value-of-datetime-in-pandas-dataframe
"question": {
	"title": Change value of datetime in pandas Dataframe
	"desc": i want to change the value of a pandas dataframe column with datetime format. Basically i want to add always 20 seconds to a row. Im new to python/pandas so i dont know any ways to solve that issue. Here is my code so far: Dataframe df_date: original: ... expected: ... Any help on this would be appreciated! 
}
"io": {
	"Frame-1": 
		0  2017-03-10 01:00:00
		1  2017-03-10 01:00:00
		2  2017-03-10 01:00:00
		3  2017-03-10 01:00:00
		4  2017-03-10 01:00:00
		
	"Frame-2":
		0  2017-03-10 01:00:20
		1  2017-03-10 01:00:40
		2  2017-03-10 01:01:00
		3  2017-03-10 01:00:20
		4  2017-03-10 01:00:40
		
}
"answer": {
	"desc": %s Add timedeltas created by multiple by with : Or created by : 
	"code-snippets": [
		td = pd.to_timedelta(np.arange(1, len(dataframe) + 1) * 20, unit='s')
		
		----------------------------------------------------------------------
		td = pd.timedelta_range(0, periods=len(dataframe), freq='20s') 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57411679
"link": https://stackoverflow.com/questions/57411679/pandas-concatenate-levels-in-multiindex
"question": {
	"title": Pandas concatenate levels in multiindex
	"desc": I do have following excel file: I would like to create following dataframe: What I tried: The new dataframe: This approach works but is kind of tedious: Which gives me: Is there a simpler solution available ? 
}
"io": {
	"Frame-1": 
		{0: {0: nan, 1: nan, 2: nan, 3: 'A', 4: 'A', 5: 'B', 6: 'B', 7: 'C', 8: 'C'},
		 1: {0: nan, 1: nan, 2: nan, 3: 1.0, 4: 2.0, 5: 1.0, 6: 2.0, 7: 1.0, 8: 2.0},
		 2: {0: 'AA1', 1: 'a', 2: 'ng/mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},
		 3: {0: 'AA2', 1: 'a', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},
		 4: {0: 'BB1', 1: 'b', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},
		 5: {0: 'BB2', 1: 'b', 2: 'mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},
		 6: {0: 'CC1', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},
		 7: {0: 'CC2', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}}
		
	"Frame-2":
		      AA1 AA2 CB1 BB2 CC1 CC2
		        a   a   b   b   c   c
		    ng/mL N/A N/A  mL N/A N/A
		0 1                          
		A 1     1   1   1   1   1   1
		  2     1   1   1   1   1   1
		B 1     1   1   1   1   1   1
		  2     1   1   1   1   1   1
		C 1     1   1   1   1   1   1
		  2     1   1   1   1   1   1
		
}
"answer": {
	"desc": %s Use: First create with , then create by first 2 columns with and remove index names by : Then loop by each level in list comprehension and join second with third level if not , last use : Another idea is use: 
	"code-snippets": [
		df = pd.read_excel('file.xlsx', header=[0,1,2])
		
		df = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))
		
		----------------------------------------------------------------------
		df = pd.read_excel('file.xlsx', header=[0,1,2])
		df = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))
		
		lv1 = df.columns.get_level_values(0)
		lv2 = df.columns.get_level_values(1)
		lv3 = df.columns.get_level_values(2)
		lv3 = lv3.where(~lv3.str.startswith('Unnamed'),'N/A')
		
		df.columns = [lv1, lv2.to_series() + ' ' + lv3]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57376287
"link": https://stackoverflow.com/questions/57376287/identify-increasing-features-in-a-data-frame
"question": {
	"title": Identify increasing features in a data frame
	"desc": I have a data frame that present some features with cumulative values. I need to identify those features in order to revert the cumulative values. This is how my dataset looks (plus about 50 variables): What I wish to achieve is: I've seem this answer, but it first revert the values and then try to identify the columns. Can't I do the other way around? First identify the features and then revert the values? Finding cumulative features in dataframe? What I do at the moment is run the following code in order to give me the feature's names with cumulative values: Afterwards, I save these features names manually in a list called cum_features and revert the values, creating the desired dataset: Is there a better way to solve my problem? 
}
"io": {
	"Frame-1": 
		a      b     
		346    17    
		76     52    
		459    70    
		680    96    
		679    167   
		246    180   
		
	"Frame-2":
		a      b     
		346    17    
		76     35    
		459    18    
		680    26    
		679    71   
		246    13   
		
}
"answer": {
	"desc": %s To identify which columns have increasing* values throughout the whole column, you will need to apply conditions on all the values. So in that sense, you have to use the values first to figure out what columns fit the conditions. With that out of the way, given a dataframe such as: Figuring out which columns contain increasing values is just a question of using diff on all values in the dataframe, and checking which ones are increasing throughout the whole column. That can be written as: Then, you can just use the column names to select only those with in them *(the term cumulative doesn't really represent the conditions you used.Did you want it to be cumulative or just increasing? Cumulative implies that the value in a particular row/index was the sum of all previous values upto that index, while increasing is just that, the value in current row/index is greater than previous.) 
	"code-snippets": [
		out = (df.diff().dropna()>0).all()
		#Output:
		a     True
		b    False
		dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 35682719
"link": https://stackoverflow.com/questions/35682719/drop-rows-with-a-question-mark-value-in-any-column-in-a-pandas-dataframe
"question": {
	"title": Drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe
	"desc": I want to remove all rows (or take all rows without) a question mark symbol in any column. I also want to change the elements to float type. Input: Output: Preferably using pandas dataframe operations. 
}
"io": {
	"Frame-1": 
		X Y Z
		0 1 ?
		1 2 3
		? ? 4
		4 4 4
		? 2 5
		
	"Frame-2":
		X Y Z
		1 2 3
		4 4 4
		
}
"answer": {
	"desc": %s You can try first find string in columns, create boolean mask and last filter rows - use boolean indexing. If you need convert columns to , use : Or you can try: Better is use: Or: 
	"code-snippets": [
		df = df[(df != '?').all(axis=1)]
		
		----------------------------------------------------------------------
		df = df[~(df == '?').any(axis=1)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57264388
"link": https://stackoverflow.com/questions/57264388/dataframe-to-dictionary-values-came-out-scrambled
"question": {
	"title": Dataframe to dictionary, values came out scrambled
	"desc": I have a dataframe that contains two columns that I would like to convert into a dictionary to use as a map. I have tried multiple ways of converting, but my dictionary values always comes up in the wrong order. My python version is 3 and Pandas version is 0.24.2. This is what the first few rows of my dataframe looks like: I would like my dictionary to look like this: But instead my outputs came up with the wrong order for the values. I tried this first but the dictionary came out unordered: Then I tried coverting the two columns into list first then convert to dictionary, but same problem occurred: I tried converting to OrderedDict and that didn't work either. Then I've tried: I've also tried: I've also tried the answers suggested: panda dataframe to ordered dictionary None of these work. Really not sure what is going on? 
}
"io": {
	"Frame-1": 
		{100100: 36276,
		100124: 36310,
		100460: 35005,
		100460: 35062,
		100460: 35214,...}
		
	"Frame-2":
		{100100: 98520,
		 100124: 36310,
		 100460: 57520,
		 100484: 35540,
		 100676: 19018,
		 100820: 57311,
		 100988: 15483,
		 101132: 36861,...}
		
}
"answer": {
	"desc": %s try this and if same have multiple values you can provide those as list 
	"code-snippets": [
		defaultdict(<class 'list'>, {100100: [36276], 100124: [36310], 100460: [35005, 35062, 35214]})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57262708
"link": https://stackoverflow.com/questions/57262708/plot-partial-dependency-with-model-built-from-pandas-data-frame
"question": {
	"title": Plot partial dependency with model built from pandas data frame
	"desc": I have a model that is trained from a pandas dataframe. It can predict dataframe input without problem: However, when I use the exact data and model to plot the partial dependency graph, I have the following error: The code I used was: I understand that I can convert X_train to numpy.ndarray before training the model, and it solves the problem. However, as the actual classifier is very large and it took a long time to train already, I would like to re-use the classifier that was trained with pandas dataframe. Is there a way to do that? Thank you very much! Edit the OP to include some sample data: X_train.head(10): y_train.head(10): 
}
"io": {
	"Frame-1": 
		    a        b        c    d           e
		0  34   226830  5249738  409  1186.78850
		1  36    38940  8210911   76  2326.72880
		2  36    38940  8210911   76  2326.72880
		3  34   761188  5074516  698   370.27365
		4  36  1097060  9072727  296   576.91693
		5  36  1097060  9072727  296   576.91693
		6  25    62240   881740  102   194.59651
		7  25    62240   881740  102   194.59651
		8  25    62240   881740  102   194.59651
		9  28    65484  1391620  105   259.25095
		
	"Frame-2":
		0    1
		1    1
		2    1
		3    1
		4    1
		5    1
		6    1
		7    1
		8    1
		9    1
		
}
"answer": {
	"desc": %s Congrats! You found a deficiency between and . Using the traceback to guide me, I stuck a as the first line in . When I run your method (with dummy data I created), I get output like this: The first several lines where the feature names are correct are from fitting the model. When fitting, apparently, it is possible to set the feature names. The last line is from calling the . Seemingly, there is no way for sklearn to propagate the column names to xgboost using this method and so the latter defaults to 'f0', 'f1', etc. WARNING: I am uncertain if disabling feature validation in the manner described below has adverse affects (namely, that feature names are confused). It is hard to tell when using dummy data as I have. Take the resulting partial dependence plots with a grain of salt. You may want to check the results from XGBClassifier against those from sklearn's GradientBoostingClassifier as a precaution. Or, rename the columns to ['f0', 'f1', 'f2', 'f3', 'f4'] prior to training. On the plus side, you can get around this without having to change your column names. Ideally, the function would allow us to specify a list of keyword arguments to pass to the (i.e. ) because we would ideally pass . In lieu of this interface, I propose the following hack: 
	"code-snippets": [
		# store keyword argument default values
		tmpdefaults = XGBClassifier.predict_proba.__defaults__
		# change default value of validate_features to False
		XGBClassifier.predict_proba.__defaults__ = (None, False)
		
		# plot
		plot_partial_dependence(estimator=clf, X=X_train, features=[0, 1], feature_names=X_train.columns.tolist())
		plt.show()
		
		# reset default keyword argument values to original
		XGBClassifier.predict_proba.__defaults = tmpdefaults
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57234137
"link": https://stackoverflow.com/questions/57234137/group-rows-dates-and-summarize-serveral-columns-several-measured-values-for-e
"question": {
	"title": group rows (dates) and summarize serveral columns (several measured values for eacht date) in Python Pandas
	"desc": I use Python Pandas and load a table like this from Postgres: I want to group the Date rows using Pandas and summarize the values. The result should look like this I can group the dates and summarize the values separately, but not in one view. My results are And Thats the Code: 
}
"io": {
	"Frame-1": 
		2001-01-01 00:00:00   500
		2001-02-01 00:00:00   160
		
	"Frame-2":
		1 220
		2 280
		3 160
		
}
"answer": {
	"desc": %s If is column first aggregate sum and then per : Or create by column, then sum all rows and last sum per index (aggregate sum per index): If is index solution above is simplify: 
	"code-snippets": [
		df1 = df.sum(axis=1).sum(level=0).reset_index(name='sum')
		
		df1 = df.sum(level=0).sum(axis=1).reset_index(name='sum')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57143033
"link": https://stackoverflow.com/questions/57143033/using-pandas-interpolate
"question": {
	"title": Using pandas.interpolate()
	"desc": Suppose I would like to apply following command: which returns Question: How can i apply a restriction on the minimum number of valid numbers (i.e not NaN) before AND after a group of NaNs, so as to apply the interpolation In this example, i would like to fill first group of NaNs because there are minimum 3 valid numbers before AND after, but NOT interpolate the second group of NaNs, as there are only two valid numbers after the NaNs (and not 3 as i would prefer) Expected result: 
}
"io": {
	"Frame-1": 
		           0
		0   1.000000
		1   2.000000
		2   3.000000
		3   3.166667
		4   3.333333
		5        NaN
		6   3.666667
		7   3.833333
		8   4.000000
		9   5.000000
		10  6.000000
		11  5.500000
		12  5.000000
		13       NaN
		14  4.000000
		15  3.500000
		16  3.000000
		17  4.000000
		18       NaN
		
	"Frame-2":
		           0
		0   1.000000
		1   2.000000
		2   3.000000
		3   3.166667
		4   3.333333
		5        NaN
		6   3.666667
		7   3.833333
		8   4.000000
		9   5.000000
		10  6.000000
		11       NaN
		12       NaN
		13       NaN
		14       NaN
		15       NaN
		16  3.000000
		17  4.000000
		18       NaN
		
}
"answer": {
	"desc": %s EDIT 1: revised my first answer. One more go with some sort of mask approach based on this Q&A. EDIT 2: added copy back to pd df using to avoid the copy-by-reference issue. now call the interpolation and write back values from 'too small' blocks: is then to improve this hack, you could put the masking in a function and get rid of the for loop. 
	"code-snippets": [
		    import numpy as np
		    import pandas as pd
		    from copy import deepcopy
		
		    a = np.array([1,2,3,np.nan,np.nan,np.nan,np.nan,np.nan,4,5,6,np.nan,np.nan,np.nan,np.nan,np.nan,3,4,np.nan,1])
		
		    df = pd.DataFrame(a)
		    # store values for later, to keep information from blocks that are below size limit:
		    temp = deepcopy(df[df[0].notnull()]) 
		
		    mask = np.concatenate(([False],np.isfinite(a),[False]))
		    idx = np.nonzero(mask[1:] != mask[:-1])[0] # start and stop indices of your blocks of finite numbers
		    counts = (np.flatnonzero(mask[1:] < mask[:-1]) - np.flatnonzero(mask[1:] > mask[:-1])) # n finite numbers per block
		
		    sz_limit = 2 # set limit, exclusive in this case
		    for i, size in enumerate(counts):
		        if size <= sz_limit:
		            a[idx[i*2]:idx[i*2+1]] = np.nan
		
		
		----------------------------------------------------------------------
		
		    a_inter = pd.DataFrame(a).interpolate(limit = 2, limit_direction = 'both', limit_area = 'inside') 
		    a_inter.update(other = temp)  
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57140598
"link": https://stackoverflow.com/questions/57140598/add-columns-to-a-dataset-without-any-columns
"question": {
	"title": Add columns to a dataset without any columns
	"desc": I want to load a dataset into a dataframe and then add columns to the dataset. Right now when I add columns, it removes the first line of data. To visualize for happens; Let's assume the following data from a csv is loaded to the dataframe 21,5,14 456,47,1 47,89,66 It will look like this So basically the first line of data is now shown as the columns, if your visualize the dataframe. When, I try to add columns Where, file_structure, is a list with the columns Does now look like this; 
}
"io": {
	"Frame-1": 
		   21  5  14
		0  456 47 1
		1  47  89 66
		
	"Frame-2":
		   x   y  z
		0  456 47 1
		1  47  89 66
		
}
"answer": {
	"desc": %s  names lets you set column names header sets an index to use as colum names https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv 
	"code-snippets": [
		    df = pd.read_csv(
		            io.StringIO(decoded.decode('utf-8')), index_col=False, low_memory=False, header=None, names=file_structure
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57050068
"link": https://stackoverflow.com/questions/57050068/pandas-list-of-tuples-to-multiindex
"question": {
	"title": Pandas list of tuples to MultiIndex
	"desc": I have a that looks like this: I need to return a that looks like this: What is the best approach to this? 
}
"io": {
	"Frame-1": 
		    id    t_l
		0   100   [('a', 1), ('b', 2)]
		1   151   [('x', 4), ('y', 3)]
		
	"Frame-2":
		    id    f    g
		0   100  'a'   1
		1        'b'   2
		2   151  'x'   4
		3        'y'   3
		
}
"answer": {
	"desc": %s Edit: @ALollz makes a good point about speed of vs. . I and it is true. Therefore, I added solution using Original: I would construct a new using for data and for index. Finally, to put back to column 
	"code-snippets": [
		from itertools import chain
		pd.DataFrame(chain.from_iterable(df.t_l), index=np.repeat(df.id, df.t_l.str.len()), \
		                                          columns=['f', 'g']).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57043695
"link": https://stackoverflow.com/questions/57043695/pandas-downsample-a-more-frequent-dataframe-to-the-frequency-of-a-less-frequen
"question": {
	"title": pandas - downsample a more frequent DataFrame to the frequency of a less frequent DataFrame
	"desc": I have two DataFrames that have different data measured at different frequencies, as in those csv examples: df1: df2: I would like to obtain a single df that have all the measures of both dfs at the times of the first one (which get data less frequently). I tried to do that with a for loop averaging over the df2 measures between two timestamps of df1 but it was extremely slow. 
}
"io": {
	"Frame-1": 
		i,m1,m2,t
		0,0.556529,6.863255,43564.844
		1,0.5565576199999884,6.86327749999999,43564.863999999994
		2,0.5565559400000003,6.8632764,43564.884
		3,0.5565699799999941,6.863286799999996,43564.903999999995
		4,0.5565570200000007,6.863277200000001,43564.924
		5,0.5565316400000097,6.863257100000007,43564.944
		...
		
	"Frame-2":
		i,m3,m4,t
		0,306.81162500000596,-1.2126870045404683,43564.878125
		1,306.86175000000725,-1.1705838272666433,43564.928250000004
		2,306.77552454544787,-1.1240195386446195,43564.97837499999
		3,306.85900545454086,-1.0210345363692084,43565.0285
		4,306.8354250000052,-1.0052431772666657,43565.078625
		5,306.88397499999286,-0.9468344809917896,43565.12875
		...
		
}
"answer": {
	"desc": %s IIUC, is index column and you want to put in bins and averaging the other columns. So you can use : Then is: which you can merge with on : and get: 
	"code-snippets": [
		groups =pd.cut(df2.t, bins= list(df1.t) + [np.inf],
		               right=False,
		               labels=df1['t'])
		
		# cols to copy
		cols = [col for col in df2.columns if col != 't']
		
		# groupby and get the average
		new_df = (df2[cols].groupby(groups)
		                   .mean()
		                   .reset_index()
		         )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56976142
"link": https://stackoverflow.com/questions/56976142/error-when-trying-to-insert-into-dataframe
"question": {
	"title": Error when trying to .insert() into dataframe
	"desc": I have some code that takes a csv file, that finds the min/max each day, then tells me what time that happens. I also have 2 variables to find the percentage for both max/min. This is currently the output for the dataframe Then I have 2 varables for the % of High/Lows.. (Just ph shown) I've tried to do an .insert(), but recieve this error. TypeError: insert() takes from 4 to 5 positional arguments but 6 were given This was my code I would like the output to show the % in columns 3 &4 
}
"io": {
	"Frame-1": 
		>Out
		       High   Low
		10:00   6.0  10.0
		10:05  10.0   3.0
		10:10   1.0   7.0
		10:15   1.0   NaN
		10:20   4.0   4.0
		10:25   4.0   4.0
		10:30   5.0   1.0
		10:35   5.0   6.0
		10:40   3.0   2.0
		10:45   4.0   5.0
		10:50   4.0   1.0
		10:55   3.0   4.0
		11:00   4.0   5.0
		>
		
	"Frame-2":
		>Out
		       High   Low    % High    %Low
		10:00   6.0  10.0    .015306   
		10:05  10.0   3.0    .025510
		10:10   1.0   7.0    .002551
		10:15   1.0   NaN    .002551
		10:20   4.0   4.0    .010204
		10:25   4.0   4.0    .010204
		
		>
		
}
"answer": {
	"desc": %s You can only add one column at a time with insert. And as you intend to add the new columns at the end of the dataframe you do not even need insert: If you insist on using insert the correct syntax would be: 
	"code-snippets": [
		#adding % to end of dataframe
		result["High %"] = ph
		result["Low %"] = pl
		
		----------------------------------------------------------------------
		result.insert(2, "High %", ph)
		result.insert(3, "Low %", pl)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56929277
"link": https://stackoverflow.com/questions/56929277/can-not-make-desired-pandas-dataframe
"question": {
	"title": Can not make desired pandas dataframe
	"desc": I am trying to make a pandas dataframe using 2 paramters as columns. But it makes a dataframe transpose of what I need. I have and as column parameters as follows: This gives the following dataframe: However, I want the dataframe as: 
}
"io": {
	"Frame-1": 
		
		    0   1   2   3   4 
		0   1   2   3   4   5
		1   11  22  33  44  55
		
	"Frame-2":
		    0   1 
		0   1   11
		1   2   22
		2   3   33
		3   4   44
		4   5   55
		
}
"answer": {
	"desc": %s One solution is transpose by : Or use : Also is possible specify column names by dictionary: 
	"code-snippets": [
		df = pd.DataFrame(list(zip(a, b)))
		#pandas 0.24+
		#df = pd.DataFrame(zip(a, b))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56915574
"link": https://stackoverflow.com/questions/56915574/python-populating-tuples-in-tuples-over-dataframe-range
"question": {
	"title": python: populating tuples in tuples over dataframe range
	"desc": I have 4 portfolios a,b,c,d which can take on values either "no" or "own" over a period of time. (code included below to facilitate replication) Summary of schedule: What I have tried: create a holding dataframe and filling in values based on the schedule. Unfortunately the first portfolio 'a' gets overridden desired output: I am sure there's an easier way of achieving this but probably this is an example I haven't encountered before. Many thanks in advance! 
}
"io": {
	"Frame-1": 
		    portf   base    st          end
		0   a       no      2018-01-01  2018-01-02
		1   a       own     2018-01-03  2018-01-04
		2   b       no      2018-01-01  2018-01-05
		3   b       own     2018-01-06  2018-01-07
		4   c       own     2018-01-09  2018-01-10
		5   d       own     2018-01-09  2018-01-09  
		
	"Frame-2":
		2018-01-01  (('a','no'), ('b','no'))
		2018-01-02  (('a','no'), ('b','no'))
		2018-01-03  (('a','own'), ('b','no'))
		2018-01-04  (('a','own'), ('b','no'))
		2018-01-05  ('b','no')
		...
		
}
"answer": {
	"desc": %s I would organize the data differently, index is date, columns for portf and the values are base. First we need to reshape the data and resample to daily fields. Then it's a simple pivot. Output If you need your other output, we can get there with some less than ideal calls. This will be very bad for a large DataFrame; I would seriously consider keeping the above. 
	"code-snippets": [
		cols = ['portf', 'base']
		s = (df.reset_index()
		       .melt(cols+['index'], value_name='date')
		       .set_index('date')
		       .groupby(cols+['index'], group_keys=False)
		       .resample('D').ffill()
		       .drop(columns=['variable', 'index'])
		       .reset_index())
		
		res = s.pivot(index='date', columns='portf')
		res = res.resample('D').first()  # Recover missing dates between
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56891370
"link": https://stackoverflow.com/questions/56891370/how-to-manipulate-column-entries-using-only-one-specific-output-of-a-function-th
"question": {
	"title": How to manipulate column entries using only one specific output of a function that returns several values?
	"desc": I have a dataframe like this: and I have a function that returns several values. Here I just use a dummy function that returns the minimum and maximum for a certain input iterable: Now I want to e.g. add the maximum of each column to each value in the respective column. So gives and then yields the desired outcome I am wondering whether there is a more straightforward way that avoids the two chained 's. Just to make sure: I am NOT interested in a type solution. I highlighted the to illustrate that this not my actual function but just serves as a minimal example function that has several outputs. 
}
"io": {
	"Frame-1": 
		a    (0, 3)
		b    (2, 5)
		
	"Frame-2":
		   a   b
		0  3   7
		1  4   8
		2  5   9
		3  6  10
		
}
"answer": {
	"desc": %s  will returns a Series of the column-wise maximum values. will then add this , aligning on columns. If you're real function is much more complicated, there are a few alternatives. Keep it as is, use to access the max element. Consider returning a Series with the index being descriptive about what is returned: Or if the returns can be separated (in this case and really don't need to be done in the same function), it's simpler to have them separated: 
	"code-snippets": [
		def return_min_max(x):
		    return (np.min(x), np.max(x))
		
		df.add(df.apply(return_min_max).str[1])
		
		----------------------------------------------------------------------
		def return_min_max(x):
		    return pd.Series([np.min(x), np.max(x)], index=['min', 'max'])
		
		df.add(df.apply(return_min_max).loc['max'])
		
		----------------------------------------------------------------------
		def return_max(x):
		    return np.max(x)
		
		df.add(df.apply(return_max))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56869643
"link": https://stackoverflow.com/questions/56869643/pandas-remove-index-entry-and-all-its-rows-from-multilevel-index-when-all-da
"question": {
	"title": Pandas: Remove index entry (and all it&#39;s rows) from multilevel index when all data in a column is NaN
	"desc": I'd like to clean up some data I have in a dataframe with a multilevel index. I'd like to loose the complete group indexed by bar, because all of the data in column A is NaN. I'd like to keep foo, because only some of the data in column A is NaN (column B is not important here, even if it's all NaN). I'd like to keep baz, because not all of column Ais NaN. So my result should look like this: What's the best way to do this with pandas and python? I suppose there is a better way than looping through the data... 
}
"io": {
	"Frame-1": 
		                | A   | B   | 
		----------------+-----+-----+
		foo  2019-01-01 | x   | NaN |
		     2019-01-02 | x   | NaN |
		     2019-01-03 | NaN | NaN |
		................+.....+.....+
		bar  2019-01-01 | NaN | x   |
		     2019-01-02 | NaN | y   |
		     2019-01-03 | NaN | z   |
		................+.....+.....+
		baz  2019-01-01 | x   | x   |
		     2019-01-02 | x   | x   |
		     2019-01-03 | x   | x   |
		
		
	"Frame-2":
		                | A   | B   | 
		----------------+-----+-----+
		foo  2019-01-01 | x   | NaN |
		     2019-01-02 | x   | NaN |
		     2019-01-03 | NaN | NaN |
		................+.....+.....+
		baz  2019-01-01 | x   | x   |
		     2019-01-02 | x   | x   |
		     2019-01-03 | x   | x   |
		
		
}
"answer": {
	"desc": %s , & We can on your first level index and then check if any of the values in column A are not . We use to get the same shaped boolean array back so we can use to filter out the correct rows. What does return? 
	"code-snippets": [
		m = df['A'].notna().groupby(level=0).transform('any')
		print(m)
		
		idx  idx2      
		foo  2019-01-01     True
		     2019-01-02     True
		     2019-01-03     True
		bar  2019-01-01    False
		     2019-01-02    False
		     2019-01-03    False
		baz  2019-01-01     True
		     2019-01-02     True
		     2019-01-03     True
		Name: A, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 48864923
"link": https://stackoverflow.com/questions/48864923/pythonpandas-select-certain-rows-by-index-of-another-dataframe
"question": {
	"title": Python[pandas]: Select certain rows by index of another dataframe
	"desc": I have a dataframe and I would select only rows that contain index value into df1.index. for Example: and these indexes I would like this output: thanks 
}
"io": {
	"Frame-1": 
		In [96]: df
		Out[96]:
		   A  B  C  D
		1  1  4  9  1
		2  4  5  0  2
		3  5  5  1  0
		22 1  3  9  6
		
	"Frame-2":
		In [96]: df
		Out[96]:
		   A  B  C  D
		1  1  4  9  1
		3  5  5  1  0
		22 1  3  9  6
		
}
"answer": {
	"desc": %s Use : Or get all intersectioned indices and select by : EDIT: I tried solution: df = df.loc[df1.index]. Do you think that this solution is correct? Solution is incorrect: 
	"code-snippets": [
		df = df.loc[df.index & df1.index]
		df = df.loc[np.intersect1d(df.index, df1.index)]
		df = df.loc[df.index.intersection(df1.index)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56813691
"link": https://stackoverflow.com/questions/56813691/how-to-append-value-counts-output-to-the-original-df
"question": {
	"title": How to append value_counts() output to the original df?
	"desc": So I have the following : when I running this line: I get the following output: I was wonder how I could append this output to the original to make it look like this: Thank you very much for your help in advance. 
}
"io": {
	"Frame-1": 
		       Open      High       Low     Close  
		0    0.001268  0.001277  0.001266  0.001271   
		1    0.001268  0.001269  0.001265  0.001266   
		2    0.001265  0.001265  0.001242  0.001254   
		3    0.001253  0.001271  0.001244  0.001251   
		4    0.001253  0.001259  0.001249  0.001257   
		5    0.001257  0.001260  0.001241  0.001248
		
	"Frame-2":
		0.001253    2
		0.001268    2
		0.001265    1
		0.001257    1
		
}
"answer": {
	"desc": %s  is your friend: Output: 
	"code-snippets": [
		s = df["Open"].value_counts()
		
		df.merge(s, left_on='Open', right_index=True,
		         suffixes=['','_count'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56810353
"link": https://stackoverflow.com/questions/56810353/pandas-keep-values-in-a-set-of-columns-if-they-exist-in-another-set-of-columns
"question": {
	"title": Pandas: Keep values in a set of columns if they exist in another set of columns in the same row, otherwise set it to NaN
	"desc": I have a couple of columns in my dataframe that have values in them. I want to only keep those values in those columns if they exist in another set of columns in the same row. Otherwise, I want to set the value to . Here's an example dataframe: In this case, I want and to be changed based on and : It's been difficult to form a query to google this, and the closest I've gotten is to use like this: Which gives me this: Which appears to be somewhat useful, but I'm not sure if this is the right path or what to do with it from here. 
}
"io": {
	"Frame-1": 
		    A   B   C   D
		0   1  30   1  29
		1   5  42  99   5
		2  64  67  12  22
		3   2  22  22   0
		4  43   6   9  43
		
	"Frame-2":
		    A   B     C     D
		0   1  30   1.0   NaN
		1   5  42   NaN   5.0
		2  64  67   NaN   NaN
		3   2  22  22.0   NaN
		4  43   6   NaN  43.0
		
}
"answer": {
	"desc": %s Numpy broadcasting and Less Numpy Same thing but more specific with the columns 
	"code-snippets": [
		df[['C', 'D']] = [
		    (c if c in ab else np.nan, d if d in ab else np.nan)
		    for *ab, c, d in zip(*map(df.get, ['A', 'B', 'C', 'D']))
		]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56776539
"link": https://stackoverflow.com/questions/56776539/how-to-i-convert-a-pandas-dataframe-row-into-multiple-rows
"question": {
	"title": How to I convert a pandas dataframe row into multiple rows
	"desc": I have a pandas dataframe that has one row per object. Within that object, there are subobjects. I want to create a dataframe which contains one row per subobject. I've read stuff on melt but can't begin to work out how to use it for what I want to do. I want to go from to 
}
"io": {
	"Frame-1": 
		ObjectID    Sub1_ID Sub1_Var1   Sub1_Var2   Sub1_Var3   Sub2_ID Sub2_Var1   Sub2_Var2   Sub2_Var3
		1           98398   3           10          9           19231           6           7           5
		2           87868   8           5           4               
		3           4579    5           6           6           24833           6           2           2
		4           2514    1           6           9   
		
	"Frame-2":
		ObjectID    Sub_ID  Var1    Var2    Var3
		1           98398   3       10      9
		1           19231   6       7       5
		2           87868   8       5       4
		3           4579    5       6       6
		3           24833   6       2       2
		4           2514    1       6       9
		
}
"answer": {
	"desc": %s One way you can do this is using MultiIndex with and then use to reshape the dataframe: Output: 
	"code-snippets": [
		df1 = df.set_index('ObjectID')
		
		df1.columns = pd.MultiIndex.from_arrays(zip(*df1.columns.str.split('_')))
		
		df1.stack(0).reset_index().drop('level_1', axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56757624
"link": https://stackoverflow.com/questions/56757624/increase-specific-rows-by-multiplication-until-sum-of-columns-fulfils-criteria
"question": {
	"title": Increase specific rows by multiplication until sum of columns fulfils criteria
	"desc": I have a dataframe with 4 columns and I want to do the following steps (ideally in one code): - Filter rows where the sum of the 4 columns is lower than 0.9 - Multiply each cell in each row so that the sum of the row is 0.9 - In case there is a 0 in any cell, this cell stays unchanged (as multiplying 0 with anything remains 0) - At the end display all rows, also the ones that were not changed Here is an example dataframe: Now only rows 0 and 1 should be affected by the algorithm I had used this one which worked partly here: But I do now know how to work only with the columns A to C and how I can first filter by the rows where the sum is below 0.9 and then how to show all rows again. So my desired outcome is something like this: Important, all columns (including product column) and rows should still be there and the format be a dataframe with all of the rows. I only added the sum function below to see that they add up to 0.9 or more. 
}
"io": {
	"Frame-1": 
		print (df)
		   Name     A         B         C
		0  Bread    0.0414  0.170292  0.690000
		1  Butter   0.0000  0.452000  0.452000
		2  Cheese   0.70    0.3330   0.0333
		
	"Frame-2":
		Sum = df["A"]+df["B"]+df["C"]
		    print (Sum)
		
		0    0.9
		1    0.9
		2    1.0663
		
}
"answer": {
	"desc": %s To save the intermediate values in a new dataframe : is: And if you do: you get: 
	"code-snippets": [
		df2 = df.apply(lambda x : x if x.sum() > 0.9 else x.mul(0.9/x.sum()), axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56762170
"link": https://stackoverflow.com/questions/56762170/converting-dataframe-to-dictionary-with-header-as-key-and-column-as-array-with-v
"question": {
	"title": Converting DataFrame to dictionary with header as key and column as array with values
	"desc": I have a dataframe as follows: I want a dictionary like this: I have tried using the normal and it is no where close. If I use the transposed dataframe, hence it gets close, but I have something like this: The questions in stack overflow are limited to the dictionary having one value per key, not an array. It would be very valuable for me to use and avoid any for loop, since the database I am using is quite big and I want the computational complexity to be as low as possible. 
}
"io": {
	"Frame-1": 
		A B C 
		1 6 1 
		2 5 7 
		3 4 9
		4 2 2
		
	"Frame-2":
		{0: {A: 1, B: 6, C:1} , ... , 4:{A: 4, B: 2, C: 2 } }
		
}
"answer": {
	"desc": %s  Or: the method actually just checks the first character If you want to preserve the numpy array 
	"code-snippets": [
		{k: v.to_numpy() for k, v in df.items()}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56738691
"link": https://stackoverflow.com/questions/56738691/is-there-a-way-to-use-previous-row-value-in-pandas-apply-function-when-previous
"question": {
	"title": Is there a way to use previous row value in pandas&#39; apply function when previous value is iteratively summed ?or an efficient way?
	"desc": I have a dataframe with some columns and I would like to apply the following transformation in an efficient manner. Given the Dataframe below: It should be transformed in such a way I can get the following output: Note that: C[i] = C[i] + C[i - 1] + ... + C[0] and D[i] = D[i] + C[i - 1] NaN values should be filtered. Thx! 
}
"io": {
	"Frame-1": 
		   C    D
		 ===========
		  Nan  10
		  0  22 
		  2  280
		  4  250
		  6  270
		
	"Frame-2":
		   C    D
		 ===========
		  Nan  10
		  0  22 
		  2  280
		  6  252
		  12 276 
		
}
"answer": {
	"desc": %s  Output: 
	"code-snippets": [
		df['C'] = df['C'].cumsum()
		df['D'] = df['D'].add(df['C'].shift(1).fillna(0))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56681714
"link": https://stackoverflow.com/questions/56681714/how-should-i-construct-this-json-return-from-a-pandas-dataframe
"question": {
	"title": How should I construct this json return from a pandas dataframe
	"desc": I have some data, organised by date, as a datetime index. I then subset it so it is effectively irregular: In my service (this is not for interactive use) I am given a string which I pass to to aggregate my time data to any level. The string is supplied directly to the argument, and can be values like , , , I would like to use the same string to create an json which is similar to the following structure: The array will be 'ragged' i.e. not all records will be present, and not all keys in the json will have the same length of array. The goals for a good solution are: is the level set by the same string parameter as given to the array in the aggregate level will always be the raw, datetime hourly values Ideally the string parameter is not associated with a bunch of 'translatation rules' such as "If then use like this, but if use this and if use this would just return a single array, of the raw values So in practice if a is supplied: Note that there are two keys at daily level, with the values in the array split to the correct day. If is supplied: Note that this means the contents of the value array will be 3 in this example, as the 3 datetimes are all in the same month Things I've tried/looked at that I haven't made work well: , they look like they aggregate only, based on some rules. I specifically need to return the actual records Parseing a new column based on the argument would technically work, but it seems wrong as I would have to start converting each to a or similar. I have not yet found a function that accepts the same character string and does not also perform aggregations Is setting a multi-index a solution to this? It might be but I'm not certain how to populate it in regards to the point above about the , , etc. custom resampler: Which is not working. I understand, it may be that this is not solvable with the rules above, but I am probably not good enough at yet to realise it. 
}
"io": {
	"Frame-1": 
		{'2018-01-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'},
		'2018-01-08':{'2018-01-08 03:00:00'}}
		
	"Frame-2":
		{'2018-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00', '2018-01-08 03:00:00'}}
		
}
"answer": {
	"desc": %s UPDATE: The return structure is infact a series, and as such can be filtered for empty list index points using this method: ORIGINAL: I believe I have worked out a solution. Specifically, I was using custom resamplers wrong, and in fact they can do the json conversion itself. I need to process a little to get to the result I need, but for now this is the answer: Very happy to be corrected/refined/improved on this. 
	"code-snippets": [
		def custom_resampler(array_like):
		    return array_like.to_json()
		
		df.resample('D').apply(custom_resampler)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56723207
"link": https://stackoverflow.com/questions/56723207/negating-column-values-and-adding-particular-values-in-only-some-columns-in-a-pa
"question": {
	"title": Negating column values and adding particular values in only some columns in a Pandas Dataframe
	"desc": Taking a Pandas dataframe df I would like to be able to both take away the value in the particular column for all rows/entries and also add another value. This value to be added is a fixed additive for each of the columns. I believe I could reproduce df, say dfcopy=df, set all cell values in dfcopy to the particular numbers and then subtract df from dfcopy but am hoping for a simpler way. I am thinking that I need to somehow modify So for example of how this should look: Then negating only those values in columns (0,3,4) and then adding 10 (for example) we would have: Thanks. 
}
"io": {
	"Frame-1": 
		    A   B   C   D   E
		0   1.0 3.0 1.0 2.0 7.0
		1   2.0 1.0 8.0 5.0 3.0
		2   1.0 1.0 1.0 1.0 6.0
		
	"Frame-2":
		    A   B   C   D   E
		0   9.0 3.0 1.0 8.0 3.0
		1   8.0 1.0 8.0 5.0 7.0
		2   9.0 1.0 1.0 9.0 4.0
		
}
"answer": {
	"desc": %s pandas is very intuitive in letting you perform these operations, negate: add a constant c: or change to constant value c: and any other arithmetics you can think of 
	"code-snippets": [
		df.iloc[:, [0,2,7,10,11] = -df.iloc[:, [0,2,7,10,11]
		
		----------------------------------------------------------------------
		df.iloc[:, [0,2,7,10,11] = df.iloc[:, [0,2,7,10,11]+c
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56666271
"link": https://stackoverflow.com/questions/56666271/how-to-find-the-last-non-zero-element-in-every-column-throughout-dataframe
"question": {
	"title": How to find the last non zero element in every column throughout dataframe?
	"desc": How can one go about finding the last occurring non zero element in every column of a dataframe? Input Output 
}
"io": {
	"Frame-1": 
		    A  B
		0   0  1
		1   0  2
		2   9  0
		3  10  0
		4   0  0
		5   0  0
		
	"Frame-2":
		    A  B
		0  10  2
		
}
"answer": {
	"desc": %s Here's one approach using and advanced indexing: Update In order to find the last nonzero: 
	"code-snippets": [
		first_max = df.values[df.ne(0).values.argmax(0), range(df.shape[1])]
		out = pd.DataFrame([first_max], columns=df.columns)
		
		----------------------------------------------------------------------
		row_ix = df.shape[0]-df.ne(0).values[::-1].argmax(0)-1
		first_max = df.values[row_ix, range(df.shape[1])]
		out = pd.DataFrame([first_max], columns=df.columns)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56693262
"link": https://stackoverflow.com/questions/56693262/re-arranging-a-single-column-of-strings-based-on-text-containing-different-dates
"question": {
	"title": Re-arranging a single column of strings based on text containing different dates, by date
	"desc": I am looking to arrange a dataframe by dates, however, the dates are a part of a string within each row. The rows must be rearranged in order by day. Other solutions from stack overflow show how to sort based on a column of dates alone, this example is different because other information is a part of each string and is mixed with the dates. The dataframe is one column with an index, but the rows are not arranged in order from the dates contained on the far right side of each string. The score numbers are random and do not require any attention. The expected dataframe should look like this (repeated dates have no preference for order between each other and index doesn't matter). The respective scores must stay with their associated dates. What is a way to do this? 
}
"io": {
	"Frame-1": 
		                        0
		__________________________
		0     score17 6-20-19.xlsx
		1     score23 6-7-19.xlsx
		2     score4  6-17-19.xlsx      
		3     score34 6-8-19.xlsx
		4     score10 6-7-19.xlsx
		
	"Frame-2":
		                         0
		__________________________
		1     score23 6-7-19.xlsx
		4     score10 6-7-19.xlsx
		3     score34 6-8-19.xlsx
		2     score4  6-17-19.xlsx
		0     score17 6-20-19.xlsx
		
}
"answer": {
	"desc": %s A somewhat crude way using common string expressions to create a few columns, then sort accordingly. First, I'd suggest "stripping" your column to ensure leading/lagging whitespace is not an issue, your example hand non standard spacing. You can then split your column at the first "space" (' ') like so, note this creates two columns: You can then split your "date" column at the period ('.') to get rid of the extension: Then cast that "date" column as datetime: Now you can sort your dataframe based on this "date" column, setting ascending = True/False based on your desired format. 
	"code-snippets": [
		df[['score', 'date']] = df['column_name'].str.split(' ', n=1, expand=True)
		
		----------------------------------------------------------------------
		df['date'] = df['date'].str.split('.', expand = True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56677321
"link": https://stackoverflow.com/questions/56677321/pandas-concatenation-not-working-properly
"question": {
	"title": Pandas Concatenation not working properly
	"desc": So I've been setting up a label archive on my deep learning classifier and I wanted to concatenate the labels of an already existing 2D archive into one I just made. The one that exists is 'y_trainvalid' (39209, 43), which stands for 39209 images in 43 classes. The new label archive I'm trying to add is 'new_file_label' (23, 43). On these archives, the number set to 1 if it matches the class and 0 if it doesn't. Here's a sample of both of them: When I tried to concatenate using this command: Something like this appeared: As if it doubled the amount of columns to fit the data instead of putting the new data just below it. I'm not sure why this is happening cause I'm pretty sure both label archives have the same number of columns. When I print use the 'y_trainvalid2.head().to_dict()' command, this appears: How do I solve this problem? 
}
"io": {
	"Frame-1": 
		 0    1    2    3    4    5    6  ...   41   42    5    6    7    8    9
		39204  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN
		39205  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN
		39206  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN
		39207  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN
		39208  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN
		39209  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39210  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39211  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39212  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39213  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39214  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39215  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39216  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39217  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39218  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39219  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39220  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39221  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39222  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39223  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39224  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39225  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39226  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39227  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39228  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39229  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39230  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		39231  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0
		
	"Frame-2":
		{0: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '0': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 1: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '1': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 10: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '10': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '11': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 12: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '12': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 13: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '13': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 14: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '14': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '15': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 16: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '16': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 17: {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '17': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 18: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '18': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 19: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '19': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 2: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '2': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 20: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '20': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 21: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '21': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 22: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '22': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 23: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '23': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 24: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '24': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 25: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '25': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 26: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '26': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 27: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '27': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 28: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '28': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 29: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '29': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '3': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 30: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '30': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 31: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '31': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 32: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0},
		 '32': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 33: {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0},
		 '33': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 34: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '34': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 35: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '35': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 36: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '36': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 37: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '37': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 38: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0},
		 '38': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 39: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '39': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 4: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '4': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 40: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '40': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 41: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '41': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 42: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '42': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '5': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 6: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '6': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '7': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 8: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '8': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
		 9: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},
		 '9': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}}
		
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		y_trainvalid.columns = [str(x) for x in y_trainvalid.columns]
		new_file_label.columns = [str(x) for x in new_file_label.columns]
		y_trainvalid2 = pd.concat([y_trainvalid, new_file_label])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56662690
"link": https://stackoverflow.com/questions/56662690/how-to-find-the-last-value-of-consecutive-values-in-pandas-dataframe
"question": {
	"title": how to find the last value of consecutive values in pandas dataframe?
	"desc": I have a data frame like this I want to group by this data frame on col1 where there is consecutive values, and take the last value for each consecutive groups, The final data frame should look like: I have tried something like: But its missing the consecutive condition. How to implement it in most effective way using pandas/ python 
}
"io": {
	"Frame-1": 
		df:
		col1     col2
		 1        10
		 1        20
		 2        11
		 3        33
		 1        20
		 1        10
		 2        24
		 3        21
		 3        28
		
	"Frame-2":
		df
		col1    col2
		 1       20
		 2       11
		 3       33
		 1       10
		 2       24
		 3       28
		
}
"answer": {
	"desc": %s Use with filtering by with ed Series with for last dupe consecutive rows: Detail: 
	"code-snippets": [
		print (df['col1'].ne(df['col1'].shift(-1)))
		0    False
		1     True
		2     True
		3     True
		4    False
		5     True
		6     True
		7    False
		8     True
		Name: col1, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 27513890
"link": https://stackoverflow.com/questions/27513890/how-do-you-stack-two-pandas-dataframe-columns-on-top-of-each-other
"question": {
	"title": How do you stack two Pandas Dataframe columns on top of each other?
	"desc": Is there a library function or correct way of stacking two Pandas data frame columns on top of each other? For example make 4 columns into 2: to The documentation for Pandas Data Frames that I read for the most part only deal with concatenating rows and doing row manipulation, but I'm sure there has to be a way to do what I described and I am sure it's very simple. Any help would be great. 
}
"io": {
	"Frame-1": 
		a1  b1  a2  b2
		 1   2   3   4
		 5   6   7   8
		
	"Frame-2":
		c   d
		1   2
		5   6
		3   4
		7   8
		
}
"answer": {
	"desc": %s You can select the first two and second two columns using . Then, change the column name of both parts to and . Afterwards, you can just join them using . This gives you: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = pd.DataFrame(np.arange(1, 9).reshape((2, 4)),
		        columns=["a1", "b1", "a2", "b2"])
		
		part1 = df.iloc[:,0:2]
		part2 = df.iloc[:,2:4]
		
		new_columns = ["c", "d"]
		part1.columns = new_columns
		part2.columns = new_columns
		
		print pd.concat([part1, part2], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56592574
"link": https://stackoverflow.com/questions/56592574/how-to-create-a-json-from-pandas-data-frame-where-columns-are-the-key
"question": {
	"title": How to create a json from pandas data frame where columns are the key
	"desc": I have a data frame df The json I am looking for is: I have tries df.to_json but its not working This is not the output i was looking for How to do it in most effective way using pandas/python ? 
}
"io": {
	"Frame-1": 
		df:
		col1    col2  col3
		 1        2     3
		 4        5     6
		 7        8     9
		
	"Frame-2":
		 {
		            "col1": 1,
		            "col1": 4,
		            "col1": 7,
		        },
		        {
		            "col2": 2,
		            "col2": 5,
		            "col2": 8
		        },
		        {
		            "col3": 3,
		            "col3": 6,
		            "col3": 9,
		        }
		
}
"answer": {
	"desc": %s JSON files are treated as dicts in python, the JSON file you specified has duplicate keys and could only be parsed as a string (and not using the python json library). The following code: will produce the following JSON: which you could later load to python. alternatively, this script will produce exatcly the string you want: and the output would be: edit: fixed brackets 
	"code-snippets": [
		import json
		from io import StringIO
		
		df = pd.DataFrame(np.arange(1,10).reshape((3,3)), columns=['col1','col2','col3'])
		io = StringIO()
		df.to_json(io, orient='columns')
		parsed = json.loads(io.getvalue())
		with open("pretty.json", '+w') as of:
		    json.dump(parsed, of, indent=4)
		
		----------------------------------------------------------------------
		with open("exact.json", "w+") as of:
		    of.write('[\n\t{\n' + '\t},\n\t{\n'.join(["".join(["\t\t\"%s\": %s,\n"%(c, df[c][i]) for i in df.index]) for c in df.columns])+'\t}\n]')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56572543
"link": https://stackoverflow.com/questions/56572543/combine-pandas-dataframes-eliminating-common-columns-with-python
"question": {
	"title": Combine pandas dataframes eliminating common columns with python
	"desc": I have 3 dataframes: I want to combine them together to get the following results: When I try to combine them, I keep getting: The common column (A) is duplicated once for each dataframe used in the concat call. I have tried various combinations on: Some variations have been disastrous while some keep giving the undesired result. Any suggestions would be much appreciated. Thanks. 
}
"io": {
	"Frame-1": 
		    A   B   C   D   E   F
		0  A0  B0  C0  D0  E0  F0
		1  A1  B1  C1  D1  E1  F1
		2  A2  B2  C2  D2  E2  F2
		3  A3  B3  C3  D3  E3  F3
		
	"Frame-2":
		    A   B   C   D   A   E   A   F
		0  A0  B0  C0  D0  A0  E0  A0  F0
		1  A1  B1  C1  D1  A1  E1  A1  F1
		2  A2  B2  C2  D2  A2  E2  A2  F2
		3  A3  B3  C3  D3  A3  E3  A3  F3
		
}
"answer": {
	"desc": %s Try Output: 
	"code-snippets": [
		df4 = (pd.concat((df.set_index('A') for df in (df1,df2,df3)), axis=1)
		         .reset_index()
		      )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56488071
"link": https://stackoverflow.com/questions/56488071/how-to-compress-dataframe-by-removing-columns-that-contains-nan-value-in-betwe
"question": {
	"title": How to compress dataframe by removing columns that contains &#39;NaN&#39; value in between columns that has a value?
	"desc": I am currently following the answer here. It mostly worked but when I viewed the whole dataframe, I saw that there are columns that contains 'NaN' values in between columns that do contain a value. For example I keep getting a result of something like this: Is there a way to remove those cells that contains NaN such that the output would be like this: 
}
"io": {
	"Frame-1": 
		     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9
		300 1001|1001|1002|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN   
		301 1010|1010|NaN |  NaN | 1000 | 2000|1234| NaN|  NaN | 1213 | 1415
		302 1100|1234|5678| 9101 | 1121 | 3141|2345|6789| 1011 | 1617 | 1819
		303 1000|2001|9876|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN  
		
	"Frame-2":
		     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9
		300 1001|1001|1002|  1001| 1002 | NaN |NaN | NaN|  NaN | NaN  | NaN   
		301 1010|1010|1000|  2000| 1234 | 1213|1415| NaN|  NaN | NaN  | NaN
		302 1100|1234|5678|  9101| 1121 | 3141|2345|6789| 1011 | 1617 | 1819
		303 1000|2001|9876|  1001| 1002 | NaN |NaN |NaN |  NaN | NaN  | NaN 
		
}
"answer": {
	"desc": %s Using with : Output: 
	"code-snippets": [
		import pandas as pd
		
		df[df.columns] = pd.concat([s.dropna().reset_index(drop=True) for i,s in df.iterrows()], 1).T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 45239357
"link": https://stackoverflow.com/questions/45239357/leading-zero-issues-with-pandas-read-csv-function
"question": {
	"title": Leading zero issues with pandas read_csv function
	"desc": I have a column of values such as this: When I do or they both produce values like I was searching through stackexchange, and people say that you should use , but it doesn't work for me.. 
}
"io": {
	"Frame-1": 
		123, 234, 345, 456, 567
		
	"Frame-2":
		00123, 00234, 00345, 00456, 00567.
		
}
"answer": {
	"desc": %s If you want to read in your data as integers, drop the : If you want to convert the fields to strings, you can apply a transformation with : Another option would be to use a converter: 
	"code-snippets": [
		df = pd.read_csv('data.csv', converters={'ColName': str})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56328404
"link": https://stackoverflow.com/questions/56328404/how-to-apply-a-method-to-a-pandas-dataframe
"question": {
	"title": How to apply a method to a Pandas Dataframe
	"desc": I have this dataframe I would like to convert it to I know how to create a dataframe (with indexes) for 1 column, but not for multiple columns This code produces this result how can I amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally I wouln't want to have to enter the same code for every column) 
}
"io": {
	"Frame-1": 
		   Col1              Col2
		
		0  A (1000 EUR)  C ( 3000 USD)
		
		1  B (2000 CHF)  D ( 4000 GBP)
		
	"Frame-2":
		   Col1  Col2
		
		0  1000  3000
		
		1  2000  4000
		
}
"answer": {
	"desc": %s You can use the apply function to apply your operation to all elements in both rows. Does the trick for me 
	"code-snippets": [
		# creates your dataframe
		df = pd.DataFrame({'Col1':['A (1000 EUR)','B (2000 CHF)'], 'Col2':['C (3000 USD)', 'D (4000 GBP)']})
		
		# use the apply function to  apply your code to all elements of both columns
		df = df.apply(lambda x: x.str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56289734
"link": https://stackoverflow.com/questions/56289734/iterate-over-columns-in-python-dataframe-to-do-calculations-and-insert-new-colum
"question": {
	"title": Iterate over columns in python dataframe to do calculations and insert new columns between existing columns
	"desc": I'm new to python and programming in general and can't seem to find a solution to my problem. I have a dataframe imported from an excel sheet with 15 rows of species and their number and 3 columns which are locations where they are found. That is a species by station matrix: I want to calculate for each column the top-10 species (index), their value, percentage of total in column, cumulative percentage and insert the new columns after each exististing column and return in one dataframe. This is the result I'm looking for (example with two first columns): I have managed to do this by calculating each column and make new data frames and using concat to merge the data frames together in the end using the following code: This code works, but my datasets are much larger with often more then 50 columns so I'm wondering if it possible to an iteration for each column that results in the same dataframe as shown above. Sorry for the long read. 
}
"io": {
	"Frame-1": 
		              A1    A2    A3
		Species 1   1259   600   151
		Species 2    912  1820   899
		Species 3   1288  1491   631
		Species 4     36   609  1946
		Species 5   1639   819  1864
		Species 6   1989   748   843
		Species 7    688   271  1206
		Species 8   1031   341   756
		Species 9   1517  1164   138
		Species 10  1290   669   811
		Species 11    16   409  1686
		Species 12   329   521   954
		Species 13  1782   958  1727
		Species 14   464  1804  1105
		Species 15  1002  1483   109
		
	"Frame-2":
		     Species    A1  pct  cum_pct     Species    A2  pct  cum_pct   
		0   Species 6  1989   13       13   Species 2  1820   13       13  
		1  Species 13  1782   11       24  Species 14  1804   13       26   
		2   Species 5  1639   10       35   Species 3  1491   10       37   
		3   Species 9  1517    9       45  Species 15  1483   10       48  
		4  Species 10  1290    8       53   Species 9  1164    8       56   
		5   Species 3  1288    8       62  Species 13   958    6       63    
		6   Species 1  1259    8       70   Species 5   819    5       69  
		7   Species 8  1031    6       77   Species 6   748    5       75    
		8  Species 15  1002    6       83  Species 10   669    4       79   
		9   Species 2   912    5       89   Species 4   609    4       84    
		
}
"answer": {
	"desc": %s Using a loop, , with functions to calculate and and to combine for final output frame: [out] If it's necessary to format the calculated fields and as , then instead use: [out] 
	"code-snippets": [
		frames = []
		for col in df:
		    frames.append(df[col].nlargest(10).to_frame()
		                  .assign(pct=lambda x: x[col] / df[col].sum(),
		                          cum_pct=lambda x: x['pct'].cumsum())
		                  .rename_axis('Species').reset_index())
		
		
		df_new = pd.concat(frames, axis=1)
		
		----------------------------------------------------------------------
		frames = []
		for col in df:
		    frames.append(df[col].nlargest(10).to_frame()
		                  .assign(pct=lambda x: x[col] / df[col].sum(),
		                          cum_pct=lambda x: x['pct'].cumsum())
		                  .assign(pct=lambda x: x['pct'].mul(100).astype(int),
		                          cum_pct=lambda x: x['cum_pct'].mul(100).astype(int))
		                  .rename_axis('Species').reset_index())
		
		
		df_new = pd.concat(frames, axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56271520
"link": https://stackoverflow.com/questions/56271520/pandas-create-multiple-dataframes-based-on-duplicate-index-dataframe
"question": {
	"title": pandas create multiple dataframes based on duplicate index dataframe
	"desc": If I have a dataframe with duplicates in the index, how would I create a set of dataframes with no duplicates in the index? More precisely, given the dataframe: I would want as output, a list of dataframes: This needs to be scalable to as many dataframes as needed based on the number of duplicates. 
}
"io": {
	"Frame-1": 
		   a  b
		1  1  6
		1  2  7
		2  3  8
		2  4  9
		2  5  0
		
	"Frame-2":
		   a  b
		1  1  6
		2  3  8
		
		
		   a  b
		1  2  7
		2  4  9
		
		
		   a  b
		2  5  0
		
}
"answer": {
	"desc": %s Another approach is to use : Output: 
	"code-snippets": [
		import numpy as np
		
		g = df.groupby(df.index)
		cnt = np.bincount(df.index).max()
		dfs = [g.nth(i) for i in range(cnt)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50529459
"link": https://stackoverflow.com/questions/50529459/append-rows-to-groups-in-pandas
"question": {
	"title": Append rows to groups in pandas
	"desc": I'm trying to append a number of NaN rows to each group in a pandas dataframe. Essentially I want to pad each group to be 5 rows long. Ordering is important. I have: I want: 
}
"io": {
	"Frame-1": 
		    Rank id
		0   1  a
		1   2  a
		2   3  a
		3   4  a
		4   5  a
		5   1  c
		6   2  c
		7   1  e
		8   2  e
		9   3  e
		
	"Frame-2":
		    Rank id
		0   1    a
		1   2    a
		2   3    a
		3   4    a
		4   5    a
		5   1    c
		6   2    c
		7   NaN  c
		8   NaN  c
		9   NaN  c
		10  1    e
		11  2    e
		12  3    e
		13  NaN  e
		14  NaN  e
		
}
"answer": {
	"desc": %s Using : Output: Another approach, assuming the maximum group size in is exactly 5. Full explanation: 
	"code-snippets": [
		df = pd.crosstab(df.Rank, df.ID).iloc[:5].unstack().reset_index()
		df.loc[(df[0]==0),'Rank'] = np.nan
		del df[0]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56122209
"link": https://stackoverflow.com/questions/56122209/how-to-slice-a-dataframe-and-reassemble-it-into-a-new-dataframe
"question": {
	"title": how to slice a dataframe and reassemble it into a new dataframe
	"desc": I get a dataframe like this: Slice every two columns and then reorganize to form a new dataframe, as follows: I have tried but somethings wrong happened! Thank you. 
}
"io": {
	"Frame-1": 
		A   YEAR2000    B   YEAR2001    C   YEAR2002
		a      1        b     3         a      7
		b      3        c     5         e      6
		c      6        d     2         f      3
		                e     1         g      0
		
	"Frame-2":
		type    YEAR2000    YEAR2001    YEAR2002
		a         1                         7
		b         3            3    
		c         6            5    
		d                      2    
		e                      1            6
		f                                   3
		g                                   0
		
}
"answer": {
	"desc": %s Using merge twice will achieve it. for cleaner look: if you have mutiple dataframe, put them into a list and use comprehension with reduce. 
	"code-snippets": [
		df1 = pd.DataFrame([['a', 1], ['b', 3], ['c', 6]],columns=['letter', 'number'])
		df2 = pd.DataFrame([['b', 3], ['c', 5], ['d', 2], ['e', 1]],columns=['letter', 'number'])
		df3 = pd.DataFrame([['a', 7], ['e', 6], ['f', 3], ['g', 0]],columns=['letter', 'number'])
		pd.merge(pd.merge(df1, df2, how='outer', on='letter'), df3, how='outer', on='letter')
		
		----------------------------------------------------------------------
		df1.merge(df2, how='outer', on='letter').merge(df3, how='outer', on='letter')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56086666
"link": https://stackoverflow.com/questions/56086666/how-does-pandas-convert-one-column-of-data-into-another
"question": {
	"title": How does pandas convert one column of data into another?
	"desc": I have a dataframe generated by pandas, as follows I want to convert the CODE column data to get the NUM column. The encoding rules are as follows: thank you 
}
"io": {
	"Frame-1": 
		NO  CODE
		1   a
		2   a
		3   a
		4   a
		5   a
		6   a
		7   b
		8   b
		9   a
		10  a
		11  a
		12  a
		13  b
		14  a
		15  a
		16  a
		
	"Frame-2":
		NO CODE NUM
		1   a   1
		2   a   2
		3   a   3
		4   a   4
		5   a   5
		6   a   6
		7   b   b
		8   b   b
		9   a   1
		10  a   2
		11  a   3
		12  a   4
		13  b   b
		14  a   1
		15  a   2
		16  a   3
		
		
}
"answer": {
	"desc": %s Try: on yields 
	"code-snippets": [
		a_group = df.CODE.eq('a')
		
		df['NUM'] = np.where(a_group, 
		                     df.groupby(a_group.ne(a_group.shift()).cumsum())
		                       .CODE.cumcount()+1, 
		                     df.CODE)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 17426292
"link": https://stackoverflow.com/questions/17426292/what-is-the-most-efficient-way-to-create-a-dictionary-of-two-pandas-dataframe-co
"question": {
	"title": What is the most efficient way to create a dictionary of two pandas Dataframe columns?
	"desc": What is the most efficient way to organise the following pandas Dataframe: data = into a dictionary like ? 
}
"io": {
	"Frame-1": 
		Position    Letter
		1           a
		2           b
		3           c
		4           d
		5           e
		
	"Frame-2":
		alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']
}
"answer": {
	"desc": %s  Speed comparion (using Wouter's method) 
	"code-snippets": [
		In [9]: pd.Series(df.Letter.values,index=df.Position).to_dict()
		Out[9]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56044354
"link": https://stackoverflow.com/questions/56044354/function-on-dataframe-rows-to-reduce-duplicate-pairs-python
"question": {
	"title": Function on dataframe rows to reduce duplicate pairs Python
	"desc": I've got a dataframe that looks like: Each 'layer'/row has pairs that are duplicates that I want to reduce. The one problem is that there are repeating 0s as well so I cannot just simply remove duplicates per row or it will leave an uneven number of rows. My desired output would be a lambda function that I could apply to all rows of this dataframe to get this: Is there a simple function I could write to do this? 
}
"io": {
	"Frame-1": 
		0     1      2      3       4       5       6       7      8     9     10     11
		12    13     13     13.4    13.4    12.4    12.4    16     0     0     0      0
		14    12.2   12.2   13.4    13.4    12.6    12.6    19     5     5     6.7    6.7
		.
		.
		.
		
	"Frame-2":
		0     1      2      3       4       5      6 
		12    13     13.4   12.4    16      0      0
		14    12.2   13.4   12.6    19      5      6.7
		.
		.
		.
		
}
"answer": {
	"desc": %s Method 1 using As mentioned by Yuca in the comments: Method 2 using with even numbers We can make a list of even numbers and then select those columns based on their index: 
	"code-snippets": [
		idxcols = [x-1 for x in range(len(df.columns)) if x % 2]
		
		df = df.iloc[:, idxcols]
		
		df.columns = range(len(df.columns))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56036178
"link": https://stackoverflow.com/questions/56036178/how-can-i-sort-numbers-in-a-string-in-pandas
"question": {
	"title": How can I sort numbers in a string in pandas?
	"desc": I have a dataframe like this: I want to sort it in desending order like this I tried this: The above function do some kind of sorting but not the way I want it. 
}
"io": {
	"Frame-1": 
		id   String
		1    345 -456 -13  879
		2    158 -926 -81  249 35 -4 -53  9
		3    945 -506 -103
		
	"Frame-2":
		id   String
		1    879 345 -13 -457
		2    249 158 35 9 -4 -53 -81 -926
		3    945 -103 -506
		
}
"answer": {
	"desc": %s First is necessary convert values to integers, sorting and then convert to strings back: Or: 
	"code-snippets": [
		f = lambda x: ' '.join(map(str, sorted(map(int, x.split()), reverse=True)))
		df['string'] = df['string'].map(f)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 56020272
"link": https://stackoverflow.com/questions/56020272/convert-rows-per-unique-id-into-all-comma-separated-possibilities
"question": {
	"title": Convert Rows per Unique Id into all comma separated possibilities
	"desc": i have some data in the following format, at the moment this is in a Pandas Dataframe. What i require is all of the possible combinations of the Lenders columns for each Uid so the output would be something like this And the same for Uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. Thanks, 
}
"io": {
	"Frame-1": 
		Row   Uid    Lender
		1     1      HSBC
		2     1      Lloyds
		3     1      Barclays
		4     2      Lloyds
		5     2      Barclays
		6     2      Santander
		7     2      RBS
		8     2      HSBC
		
	"Frame-2":
		Row   Uid   LenderCombo
		1     1     Barclays
		2     1     Lloyds
		3     1     HSBC
		4     1     Barclays, HSBC
		5     1     Barclays, Lloyds
		6     1     HSBC, Lloyds 
		7     1     Barclays, HSBC, Lloyds
		
}
"answer": {
	"desc": %s Use with custom function and join tuples by : 
	"code-snippets": [
		from itertools import chain, combinations
		
		#https://stackoverflow.com/a/5898031
		def all_subsets(ss):
		    return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))
		
		df = (df.groupby('Uid')['Lender']
		       .apply(lambda x: pd.Series([', '.join(y) for y in all_subsets(x)]))
		       .reset_index()
		       .rename(columns={'level_1':'Row'}))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55986653
"link": https://stackoverflow.com/questions/55986653/is-there-a-way-to-get-the-mean-value-of-previous-two-columns-in-pandas
"question": {
	"title": Is there a way to get the mean value of previous two columns in pandas?
	"desc": I want to calculate the mean value of previous two rows and fill the NAN's in my dataframe. There are only few rows with missing values in the column. I tried using and but it only captures the previous or next row/column value and fill NAN. My example data set has 7 columns as below: The output I want: 
}
"io": {
	"Frame-1": 
		X       1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19
		Hyderabad    10          20       NAN         1         3           NAN
		
	"Frame-2":
		X          1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19
		Hyderabad    10          20          15         1           3         2
		
}
"answer": {
	"desc": %s To use row-wise in this way, an easy solution is to provide an pandas series as argument to . This will replace values depending on the index. Since the column names have duplicates the below code uses the column indices. Assuming a dataframe called : This will fill the values with the mean of the two columns to the left of each column in . 
	"code-snippets": [
		col_indices = [3, 6]
		
		for i in col_indices:
		    means = df.iloc[:, [i-1, i-2]].mean(axis=1)
		    df.iloc[:, i].fillna(means, inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 43096821
"link": https://stackoverflow.com/questions/43096821/operation-on-pandas-dataframe-columns-using-its-index
"question": {
	"title": Operation on Pandas Dataframe columns using its Index
	"desc": This should be relatively easy. I have a pandas dataframe (Dates): I would like to take the difference between Dates.index and Dates. The output would be like so: Naturally, I tried this: But I receive this lovely TypeError: Instead, I've written a loop to go column by column, but that just seems silly. Can anyone suggest a pythonic way to do this? EDIT 
}
"io": {
	"Frame-1": 
		    A   B   C
		1/8/2017    1/11/2017   1/20/2017   1/25/2017
		1/9/2017    1/11/2017   1/20/2017   1/25/2017
		1/10/2017   1/11/2017   1/20/2017   1/25/2017
		1/11/2017   1/20/2017   1/25/2017   1/31/2017
		1/12/2017   1/20/2017   1/25/2017   1/31/2017
		1/13/2017   1/20/2017   1/25/2017   1/31/2017
		
	"Frame-2":
		    A   B   C
		1/8/2017     3   12      17 
		1/9/2017     2   11      16 
		1/10/2017    1   10      15 
		1/11/2017    9   14      20 
		1/12/2017    8   13      19 
		1/13/2017    7   12      18 
		
}
"answer": {
	"desc": %s I think a more explicit and elegant way to do this is to simply use . 
	"code-snippets": [
		df = df.apply(pd.to_datetime, axis="columns")  # just to make sure values are datetime
		df.apply(lambda x: x - df.index.to_series(), axis="rows)
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55920437
"link": https://stackoverflow.com/questions/55920437/how-to-set-pandas-to-extract-certain-rows-of-certain-columns-and-stack-them-on-t
"question": {
	"title": how to set Pandas to extract certain rows of certain columns and stack them on top of each other?
	"desc": How can I extract certain columns and rows to stack them together? I created a simple exemplary dataframe with this data: This is what I would like to get: Another Format I would like to get is in two columns: The headers in the desired results are just for explanation 
}
"io": {
	"Frame-1": 
		   d1_d2_d3-t1-t2
		1             101
		2             201
		3             102
		4             202
		5             103
		6             203
		
	"Frame-2":
		   d1_d2_d3-t1-t2  d1_d2_d3-t3-t4
		1             101             301
		2             201             401
		3             102             302
		4             202             402
		5             103             303
		6             203             403
		
}
"answer": {
	"desc": %s Use for filtering with for reshape: Another solution is convert values to numpy array, flatten by and create new DataFrame by constructor: Or: 
	"code-snippets": [
		idx = ['t1','t2']
		cols = ['d1','d2', 'd3']
		df = dfa.loc[idx, cols].melt(value_name='data')[['data']]
		
		----------------------------------------------------------------------
		df = pd.DataFrame({'data': dfa.loc[idx, cols].values.ravel()})
		
		----------------------------------------------------------------------
		df = pd.DataFrame({'data1': dfa.loc[idx1, cols].values.ravel(),
		                   'data2': dfa.loc[idx2, cols].values.ravel()})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 39359272
"link": https://stackoverflow.com/questions/39359272/add-new-columns-to-pandas-dataframe-based-on-other-dataframe
"question": {
	"title": Add new columns to pandas dataframe based on other dataframe
	"desc": I'm trying to set a new column (two columns in fact) in a pandas dataframe, with the data comes from other dataframe. I have the following two dataframes (they are example for this purpose, the original dataframes are so much bigger): And I want to have a new dataframe (or added to df0, whatever), as: As you can see, in the resulting dataframe isn't present the row with A=6 which is present in df1 but not in df0. Also the row with A=0 is duplicated in df1, but not in the result df2. Actually, I'm having trouble with the selection method. I can do this: But I'm not sure how to apply the part of keep with unique data (remember that df1 can contain duplicated data) and add the two columns to the df2 dataset (or add them to df0). I've search here and I don't know see how to apply something like groupby, or even map. Any idea? Thanks! 
}
"io": {
	"Frame-1": 
		In [116]: df0
		Out[116]:     
		   A  B  C
		0  0  1  0
		1  2  3  2
		2  4  5  4
		3  5  5  5
		
		
		In [118]: df1
		Out[118]: 
		   A  D  E
		0  2  7  2
		1  6  5  5
		2  4  3  2
		3  0  1  0
		4  5  4  6
		5  0  1  0
		
	"Frame-2":
		df2: 
		   A  B  C  D  E
		0  0  1  0  1  0
		1  2  3  2  7  2
		2  4  5  4  3  2
		3  5  5  5  4  6
		
}
"answer": {
	"desc": %s This is a basic application of (docs): 
	"code-snippets": [
		import pandas as pd
		df2 = pd.merge(df0,df1, left_index=True, right_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55859947
"link": https://stackoverflow.com/questions/55859947/how-to-concatenate-pandas-dataframe-date-and-different-time-formats-to-single-ti
"question": {
	"title": How to concatenate pandas dataframe date and different time formats to single timestamp?
	"desc": I have two columns in a data frame as outlined below. Notice how some of the is in , some is in format. When running... ...I can get in a consumable (for my purposes) format (e.g. ). But when running... ... is added to the times and is not being applied to all rows. How do I concatenate the date and times (which include multiple time formats) in a single timestamp? Edit1: @Wen-Ben 's solution got me here: Then to concatenate EVENT_DATE and EVENT_TIME, I found this (which works): ...results in: Next I want to get this into ISO8601 format. So I found this (which works): ...results in: HERES MY NEW PROBLEM: Running still shows the concatenated versions (e.g. ) instead of the ISO version (e.g.) How do I get the ISO8601 column "added" to the dataframe? Ideally, I want it to take the place of . I want it as a transformation of the data, not tacked on as a new column. 
}
"io": {
	"Frame-1": 
		1      19:53:00
		11     14:30:00
		15     16:30:00
		
	"Frame-2":
		1     1999-07-28 19:53:00
		11    2001-07-28 14:30:00
		15    2002-06-07 16:30:00
		
}
"answer": {
	"desc": %s Using 
	"code-snippets": [
		s1=pd.to_datetime(df['EVENT_TIME'], format='%H.%M.%S', errors='coerce')
		s2=pd.to_datetime(df['EVENT_TIME'], format='%I:%M:%S %p', errors='coerce')
		df['EVENT_TIME']=s1.fillna(s2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 48821092
"link": https://stackoverflow.com/questions/48821092/predicting-values-in-movie-recommendations
"question": {
	"title": Predicting Values in Movie Recommendations
	"desc": I've been trying to create a recommendation system using the movielens dataset in python. My goal is to determine the similarity between users and then output the top five recommended movies for each user in this format: The data I am using for now is this ratings dataset. Here is the code so far: I am trying to implement the prediction function. I want to predict the missing values and add them to c1. I am trying to implement this. The formula as well as an example of how it should be used is in the picture. As you can see it uses the similarity scores of the most similar users. The output of similarity looks like this: For example here is user1's similarity: I need help using these similarities in the prediction function to predict missing movie ratings. If that is solved I will then have to find the top 5 recommended movies for each user and output them in the format above. I currently need help with the prediction function. Any advice helps. Please let me know if you need any more information or clarification. Thank you for reading 
}
"io": {
	"Frame-1": 
		User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5
		User-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5
		
	"Frame-2":
		[(34, 0.19269904365720053) (196, 0.19187531680008307)
		 (538, 0.14932027335788825) (67, 0.14093020024386654)
		 (419, 0.11034407313683092) (319, 0.10055810007385564)]
		
}
"answer": {
	"desc": %s First of all, vectorisation makes complex problems much easier. here are a few suggestion to improve what you already have use the userID as as columns in the pivot table, this makes the example for the prediction easier to see NaN stands for missing value, which is conceptually not the same as 0, in this particular case a high negative number will do and will only bee needed when using the cosine similarity function take advantage of pandas advanced features, e.g. to keep the original values but add the predictions, fillna can be used when constructing the dataframe be sure to keep track of the useIds, you can do so by setting the index and columns to Here is my edited version of the code including predict implementation: ``` ``` here is a sample of the output Edit The code above will recommends the top 5 regardless of whether the user already watched/rated them. to fix this we can reset the values of the original ratings to 0 when choosing the recommendations as shown below\ The output 
	"code-snippets": [
		import pandas as pd
		from sklearn.metrics.pairwise import cosine_similarity
		from sklearn.preprocessing import scale
		
		
		def predict(l):
		    # finds the userIds corresponding to the top 5 similarities
		    # calculate the prediction according to the formula
		    return (df[l.index] * l).sum(axis=1) / l.sum()
		
		
		# use userID as columns for convinience when interpretering the forumla
		df = pd.read_csv('ratings.csv').pivot(columns='userId',
		                                                index='movieId',
		                                                values='rating')
		
		similarity = pd.DataFrame(cosine_similarity(
		    scale(df.T.fillna(-1000))),
		    index=df.columns,
		    columns=df.columns)
		# iterate each column (userID),
		# for each userID find the highest five similarities
		# and use to calculate the prediction for that user,
		# use fillna so that original ratings dont change
		
		res = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in col.fillna(
		    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))
		print(res)
		
		----------------------------------------------------------------------
		res = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in (0 * col).fillna(
		    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55829605
"link": https://stackoverflow.com/questions/55829605/split-list-element-in-dataframe-over-multiple-rows
"question": {
	"title": Split list element in dataframe over multiple rows
	"desc": I have a df. I would like to get to a second dataframe, with only scalar values in . If and only if the original values was a list, I would like to spread it over multiple new rows, with the other values duplicated. e.g from: to: In Split set values from Pandas dataframe cell over multiple rows, I have learned how to do this for one columns, but how to handle the case where df has multiple columns which need to be duplicated as ? 
}
"io": {
	"Frame-1": 
		 id     foo
		0  301     [a]
		1  301  [b, c]
		2  302  [e, f,33,'Z']
		3  303   42
		
	"Frame-2":
		 id     foo
		0  301     a
		1  301     b
		1  301     c 
		2  302     e
		2  302     f
		2  302     33
		2  302     Z
		3  303     42
		
}
"answer": {
	"desc": %s If you want avoid use because slow, here is another solution - convert non lists values to one element list first and then apply solution: Or: If small data or performance is not important - solution with for extract column : Or solution with : 
	"code-snippets": [
		df['foo']  = [x if isinstance(x, list) else [x] for x in df['foo']]
		
		from itertools import chain
		
		df = pd.DataFrame({
		    'id' : df['id'].values.repeat(df['foo'].str.len()),
		    'foo' : list(chain.from_iterable(df['foo'].tolist()))
		
		})
		
		----------------------------------------------------------------------
		s = df.pop('foo').apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')
		df = df.join(s).reset_index(drop=True)
		
		----------------------------------------------------------------------
		s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')
		df = df.drop('foo', axis=1).join(s).reset_index(drop=True)
		
		----------------------------------------------------------------------
		df=pd.DataFrame(data=[[301,301,302,303],[['a'],['b','c'],['e','f',33,'Z'],42]],index=['id','foo']).T
		
		df = pd.concat([df] * 1000, ignore_index=True)
		
		def f(df):
		    s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')
		    return df.drop('foo', axis=1).join(s).reset_index(drop=True)
		
		
		In [241]: %timeit (f(df))
		814 ms  11.2 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
		
		In [242]: %%timeit
		     ...: L  = [x if isinstance(x, list) else [x] for x in df['foo']]
		     ...: 
		     ...: from itertools import chain
		     ...: 
		     ...: pd.DataFrame({
		     ...:     'id' : df['id'].values.repeat([len(x) for x in L]),
		     ...:     'foo' : list(chain.from_iterable(L))
		     ...: 
		     ...: })
		     ...: 
		2.6 ms  15.6 s per loop (mean  std. dev. of 7 runs, 100 loops each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55803354
"link": https://stackoverflow.com/questions/55803354/add-values-to-existing-rows-dataframe
"question": {
	"title": Add values to existing rows -DataFrame
	"desc": I'm appending some weather data (from json- dict) - in Japanese to DataFrame. I would like to have something like this But I have this How Could I change the Codes to make it like that? Here is the code 
}
"io": {
	"Frame-1": 
		                     
		 0  : Clouds    : 2.1m
		 1     NaN          : 230
		
	"Frame-2":
		                     
		 0  : Clouds        NaN
		 1      NaN          : 2.1m
		 2      NaN           : 230
		
}
"answer": {
	"desc": %s One way could be: And print without index 
	"code-snippets": [
		df = pd.DataFrame(columns=['',''])
		df = df.append({'': weather_status, '': wind_speed}, ignore_index=True)
		df = df.append({'': wind_deg}, ignore_index=True)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55777300
"link": https://stackoverflow.com/questions/55777300/how-to-print-rows-if-a-list-of-values-appear-in-any-column-of-pandas-dataframe
"question": {
	"title": How to print rows if a list of values appear in any column of pandas dataframe
	"desc": How to print rows if values appear in any column of pandas dataframe I would like to print all rows of a dataframe where I find some values from a list of values in any of the columns. The dataframe follows this structure: First: I have a Series of values with size 3 that I get from a combinatory of 6 different values. Second: I have a dataframe with 2143 rows. I want to check if in any of these rows, I have those three values in any sort of order in the columns. Gave me this: I just tried the query command and this is what I've got: df_ordered.query('_1 == 2 & _2 == 12') Now, I want to expand the same thing, but I want to look at all those columns and find any of those values. I also didn't know how to plug those series into a loop to find the values into the query statement. EDIT: I tried the command, but I have no ideia how to expand it to the 6 columns I have. 
}
"io": {
	"Frame-1": 
		1476 13/03/2013  4 10 26 37 47 57
		1475 09/03/2013 12 13 37 44 48 51
		1474 06/03/2013  1  2  3 11 28 43
		1473 02/03/2013  2 12 33 57 58 60
		1472 27/02/2013 12 18 23 25 45 50
		1471 23/02/2013 10 25 33 36 40 58
		1470 20/02/2013  2 34 36 38 51 55
		1469 16/02/2013  4 13 35 54 56 58
		1468 13/02/2013  1  2 10 19 20 37
		1467 09/02/2013 23 24 26 41 52 53
		1466 06/02/2013  4  6 13 34 37 51
		1465 02/02/2013  6 11 16 26 44 53
		1464 30/01/2013  2 24 32 50 54 59
		1463 26/01/2013 13 22 28 29 40 48
		1462 23/01/2013  5  9 25 27 38 40
		1461 19/01/2013 31 36 44 47 49 54
		1460 16/01/2013  4 14 27 38 50 52
		1459 12/01/2013  2  6 30 34 35 52
		1458 09/01/2013  2  4 16 33 44 51
		1457 05/01/2013 15 16 34 42 46 59
		1456 02/01/2013  6  8 14 26 36 40
		1455 31/12/2012 14 32 33 36 41 52
		1454 22/12/2012  4 27 29 41 48 52
		1453 20/12/2012  6 13 25 32 47 57
		
	"Frame-2":
		0    [(2, 12, 35), (2, 12, 51), (2, 12, 57), (2, 12...
		1    [(12, 35, 51), (12, 35, 57), (12, 35, 58), (12...
		2           [(35, 51, 57), (35, 51, 58), (35, 57, 58)]
		3                                       [(51, 57, 58)]
		
		
}
"answer": {
	"desc": %s IIUC, you could try creating a with a list comprehension using , and : [out] 
	"code-snippets": [
		import numpy as np
		from itertools import combinations
		
		inputlist = [2,12,35,51,57,58]
		combined = np.array(list(combinations(inputlist, 3)))
		
		mask = (np.array([set(row).issuperset(c) for row in df.values for c in combined])
		        .reshape(len(df), -1).any(1))
		
		print(df[mask])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55756126
"link": https://stackoverflow.com/questions/55756126/choose-a-value-from-a-set-of-columns-based-on-value-and-create-new-column-with-t
"question": {
	"title": Choose a value from a set of columns based on value and create new column with the value?
	"desc": so if I have a pandas Dataframe like: and want to insert row 'E' by choosing from columns 'A', 'B', or 'C' based on conditions in column 'D', how would I go about doing this? For example: if D == a, choose 'A', else choose 'B', outputting: Thanks in advance! 
}
"io": {
	"Frame-1": 
		   A  B  C  D
		0  1  2  3  a 
		1  2  4  6  a
		2  4  8  8  b
		3  2  3  5  c
		
	"Frame-2":
		   A  B  C  D  E
		0  1  2  3  a  1
		1  2  4  6  a  2
		2  4  8  8  b  8
		3  2  3  5  c  3
		
}
"answer": {
	"desc": %s This is 
	"code-snippets": [
		df.lookup(df.index,df.D.str.upper())
		Out[749]: array([1, 2, 8, 5], dtype=int64)
		
		df['E']=df.lookup(df.index,df.D.str.upper())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49266390
"link": https://stackoverflow.com/questions/49266390/python-count-combinations-of-values-within-two-columns-and-find-max-frequency-o
"question": {
	"title": Python: Count combinations of values within two columns and find max frequency of each combination
	"desc": My pandas dataframe looks like this: First, I need to add another column containing the frequency of each combination of Section and Group. It is important to keep all rows. Desired output: The second step would be marking the highest value within Count for each Section. For example, with a column like this: The original data frame has lots of rows. That is why I'm asking for an efficient way because I cannot think of one. Thank you very much! 
}
"io": {
	"Frame-1": 
		+-----+---------+-------+
		| No. | Section | Group |
		+-----+---------+-------+
		| 123 |     222 |     1 |
		| 234 |     222 |     1 |
		| 345 |     222 |     1 |
		| 456 |     222 |     3 |
		| 567 |     241 |     1 |
		| 678 |     241 |     2 |
		| 789 |     241 |     2 |
		| 890 |     241 |     3 |
		+-----+---------+-------+
		
	"Frame-2":
		+-----+---------+-------+-------+
		| No. | Section | Group | Count |
		+-----+---------+-------+-------+
		| 123 |     222 |     1 |     3 |
		| 234 |     222 |     1 |     3 |
		| 345 |     222 |     1 |     3 |
		| 456 |     222 |     3 |     1 |
		| 567 |     241 |     1 |     1 |
		| 678 |     241 |     2 |     2 |
		| 789 |     241 |     2 |     2 |
		| 890 |     241 |     3 |     1 |
		+-----+---------+-------+-------+
		
}
"answer": {
	"desc": %s Look at 
	"code-snippets": [
		df['Count']=df.groupby(['Section','Group'])['Group'].transform('size')
		df['Max']=df.groupby(['Section'])['Count'].transform('max')==df['Count']
		df
		Out[508]: 
		    No  Section  Group  Count    Max
		0  123      222      1      3   True
		1  234      222      1      3   True
		2  345      222      1      3   True
		3  456      222      3      1  False
		4  567      241      1      1  False
		5  678      241      2      2   True
		6  789      241      2      2   True
		7  890      241      3      1  False
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55672247
"link": https://stackoverflow.com/questions/55672247/sort-dataframe-column-with-given-input-list
"question": {
	"title": Sort DataFrame column with given input list
	"desc": Hi I want to sort DataFrame column with given input list values. My list looks like : And DataFrame is : Here I want to sort DataFrame column 'val' on basis of given 'inputlist'. I am expecting following output : 
}
"io": {
	"Frame-1": 
		  val    kaywords
		
		195    keyword3
		221    keyword5
		307    keyword8
		309    keyword9
		354    keyword0
		426    keyword1
		585    keyword2
		698    keyword4
		789    keyword33
		
	"Frame-2":
		val    kaywords
		
		309    keyword9
		585    keyword2
		221    keyword5
		789    keyword33
		195    keyword3
		354    keyword0
		307    keyword8
		698    keyword4
		426    keyword1
		
}
"answer": {
	"desc": %s Use ordered , but first convert values of list to integers: Another idea if all values from exist in : 
	"code-snippets": [
		inputlist = [int(x) for x in inputlist]
		df = df.set_index('val').reindex(inputlist).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55661343
"link": https://stackoverflow.com/questions/55661343/how-to-convert-one-column-in-dataframe-into-a-2d-array-in-python
"question": {
	"title": how to convert one column in dataframe into a 2D array in python
	"desc": I have an dataframe which contain the observed data as: how can I get an array from the value to form a 2D with shape:(3,3) I try but it gives me which is not what I want 
}
"io": {
	"Frame-1": 
		 [[1, 2, 1],
		 [5, 4, 6],
		 [7, 20, 9]]
		
	"Frame-2":
		[list([1, 2, 1]) list([5, 4, 6]) list([7, 20, 9])]
		
}
"answer": {
	"desc": %s You can extract the column lists, then array-ify using a couple of methods below. If the lists are un-evenly sized, this will return a regular 2D array with nans in missing positions. 
	"code-snippets": [
		# pd.DataFrame(df['Value'].tolist()).values   #  < v0.24
		pd.DataFrame(df['Value'].tolist()).to_numpy() #  v0.24+
		
		array([[ 1.,  2., nan],
		       [ 3., nan, nan],
		       [ 4.,  5.,  6.]])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55654105
"link": https://stackoverflow.com/questions/55654105/pandas-get-the-min-value-between-2-dataframe-columns
"question": {
	"title": Pandas: get the min value between 2 dataframe columns
	"desc": I have 2 columns and I want a 3rd column to be the minimum value between them. My data looks like this: And I want to get a column C in the following way: Some helping code: Thanks! 
}
"io": {
	"Frame-1": 
		   A  B
		0  2  1
		1  2  1
		2  2  4
		3  2  4
		4  3  5
		5  3  5
		6  3  6
		7  3  6
		
	"Frame-2":
		   A  B   C
		0  2  1   1
		1  2  1   1
		2  2  4   2
		3  2  4   2
		4  3  5   3
		5  3  5   3
		6  3  6   3
		7  3  6   3
		
}
"answer": {
	"desc": %s Use This returns the min row-wise (when passing ) For non-heterogenous dtypes and large dfs you can use which will be quicker: timings: So for a 8K row df: You can see that the numpy version is nearly 10x quicker (note I pass so we pass a numpy array), this will become more of a factor when we get to even larger dfs Note for versions or greater, use so the above becomes: Timings: There is a minor discrepancy between and , it depends on whether you know upfront that the dtype is not mixed, and that the likely dtype is a factor e.g. vs see that link for further explanation. Pandas is doing a little more checking when calling 
	"code-snippets": [
		%timeit df.min(axis=1)
		%timeit np.min(df.values,axis=1)
		314 s  3.63 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
		34.4 s  161 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)
		
		----------------------------------------------------------------------
		%timeit df.min(axis=1)
		%timeit np.min(df.values,axis=1)
		%timeit np.min(df.to_numpy(),axis=1)
		314 s  3 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
		35.2 s  680 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)
		35.5 s  262 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55420231
"link": https://stackoverflow.com/questions/55420231/how-to-shift-pandas-column-elements-for-given-index-based-on-condition
"question": {
	"title": How to shift pandas column elements for given index based on condition?
	"desc": I have recently started using python and pandas, please bear with me on this. I have two columns (A, B) of data (dataframe) that should be arranged in particular sequence based on certain relation between two columns (let's say elements of column A should be smaller than elements column B for a given index), if relation is not satisfied data should shifted (only for A) by a row starting from the index where condition is not satisfied throughout the length of a column. And it should be replaced by NaN where is condition is not met. I have tried shift(1) function. This works only if the first element doesn't meet the condition but if there is any other element or multiple elements don't meet criteria it creates multiple NaNs at the start of column A instead of at the place where criteria is not met. Actual result Expected result 
}
"io": {
	"Frame-1": 
		A   B
		NaN 2
		NaN 4
		3.0 6
		5.0 7
		8.0 9
		10.0    11
		
	"Frame-2":
		A   B
		NaN 2
		3.0 4
		5.0 6
		NaN 7
		8.0 9
		10.0    11
		
}
"answer": {
	"desc": %s I don't understand what you want to do exactly. but by simply altering your code i get the expected results: shift(1) shifts the whole column/dataframe by one row, thus you need to start shifting from the index you're at to get what you want. 
	"code-snippets": [
		for xt in range (0,len(mdf1)):
		if mdf1.A[xt]>mdf1.B[xt]:
		    mdf1.loc[xt:,'A'] = mdf1[xt:]['A'].shift(1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55388727
"link": https://stackoverflow.com/questions/55388727/pandas-filter-rows-based-on-condition-but-always-retain-the-first-row
"question": {
	"title": Pandas filter rows based on condition, but always retain the first row
	"desc": I would like to drop some rows that meets certain conditions but I do not want to drop the first row even if the first row meets that criteria. I tried dropping rows by using the df.drop function but it will erase the first row if the first row meets that condition. I do not want that. Data looks something like this: I want to do it in a way that if a row has a value of 3 in column2 then drop it. And I want the new data to be like this (after dropping but keeping the first one even though the first row had a value of 3 in column 2): 
}
"io": {
	"Frame-1": 
		Column1 Column2 Column3
		  1        3      A
		  2        1      B
		  3        3      C
		  4        1      D
		  5        1      E
		  6        3      F
		
	"Frame-2":
		Column1 Column2 Column3
		  1        3      A
		  2        1      B
		  4        1      D
		  5        1      E
		
}
"answer": {
	"desc": %s You can make "retain first row" a part of your condition for dropping/keeping rows. The condition for keeping the rows would be . In code, this is Inversely, using DeMorgan's Laws, the condition for dropping rows would be . We then invert the condition to get our expected output, These work assuming your index is a . If not, use as the second condition instead. 
	"code-snippets": [
		<ORIGINAL CONDITION> or <CONDITION TO KEEP FIRST ROW>
		----------------------------------------------------------------------
		(df['Column2'] == 3) & (df.index != 0)
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55356991
"link": https://stackoverflow.com/questions/55356991/add-row-values-of-all-columns-when-a-particular-column-value-is-null-until-it-ge
"question": {
	"title": Add row values of all columns when a particular column value is null until it gets the not null values?
	"desc": I have a data frame like this: If col1 value is null I want to add all the values of other columns untill it finds the notnull element in col1 The data frame i am looking for should look like: How to do in in most efficient way using python/pandas 
}
"io": {
	"Frame-1": 
		df
		col1      col2      col3      col4
		 A         12        34        XX
		 B         20        25        PP
		 B         nan       nan       nan
		 nan       P         54        nan
		 nan       R         nan       nan
		 nan       nan       nan       PQ
		  C         D         32       SS
		  R         S         32       RS
		
	"Frame-2":
		col1     col2     col3     col4
		 A         12       34       XX
		 B         20       25       PP
		 B         PR       54       PQ
		 C          D       32       SS
		 R          S       32       RS        
		
}
"answer": {
	"desc": %s If want processes all columns like strings first forward filling missing values in , replace s to empty strings, convert all values to and use : If need processes only numeric columns with aggregate method, e.g. use lambda function with : EDIT: New helper column is used: 
	"code-snippets": [
		df['new'] = df['col1'].notna().cumsum()
		df['col1'] = df['col1'].ffill()
		c = df.select_dtypes(object).columns
		df[c] = df[c].fillna('')
		
		f = lambda x: x.mean() if np.issubdtype(x.dtype, np.number) else ''.join(x)
		df = df.groupby(['col1', 'new']).agg(f).reset_index(level=1, drop=True).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55214456
"link": https://stackoverflow.com/questions/55214456/pandas-ignoring-empty-values-when-concatenating-grouped-rows
"question": {
	"title": Pandas - Ignoring empty values when concatenating grouped rows
	"desc": I'm trying to group a dataframe based on a column value, and I want to concatenate (join) the values in the other columns. I'm doing something like - But, this gives me some values where the columns has no values. So the result looks like How can I get rid of these nan values in the column? Also, is there a way to get a third column that has the number of values present in column. For eg. for the above, Edit: Sample Data - Thanks! :) 
}
"io": {
	"Frame-1": 
		K Code
		
		K0016, K0068, nan, nan, A0046
		
		nan, nan, nan
		
	"Frame-2":
		UC      LO Number      K Code
		C001     C001.1        K0068
		C001     C001.2        K0372
		C002     C002.1        
		C002     C002.3        K0032
		C002     C002.5          
		
}
"answer": {
	"desc": %s You can try using with , however this will create the multiple index Since you nan is do replace before run below If you do not want the multiple index 
	"code-snippets": [
		df=df.replace({'nan':np.nan})
		
		
		df_combined.groupby('UC').agg({'LO Number': ', '.join,
		                                             'K Code': [lambda x : ', '.join(y for y in x if y==y),'count']})
		
		----------------------------------------------------------------------
		df_combined.assign(count=df_combined['K Code']).
		         groupby('UC').agg({'LO Number': ', '.join,
		                           'K Code': lambda x : ', '.join(y for y in x if y==y),
		                            'count':'count'})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 55132744
"link": https://stackoverflow.com/questions/55132744/aggregate-values-of-same-name-pandas-dataframe-columns-to-single-column
"question": {
	"title": Aggregate values of same name pandas dataframe columns to single column
	"desc": I have multiple csv files that were produced by tokenizing code. These files contain keywords in uppercase and lowercase. I would like to merge all those files in one single dataframe which contains all the unique values (summed) in lowercase. What would you suggest to get the result below? Initial DF: Result I don't have access to the raw data from which the csv files where created so I cannot correct this at an earlier step. At the moment I have tried mapping .lower() to the dataframe headers that I create, but it returns seperate columns with the same name like so: Using pandas is not essential. I have thought of converting the csv files to dictionaries and then trying the above procedure (turns out it is much more complicated than I thought), or using lists. Also, group by does not do the job as it will remove non duplicate column names. Any approach is welcome. 
}
"io": {
	"Frame-1": 
		+---+---+----+-----+
		| a | b |  A |  B  |
		+---+---+----+-----+
		| 1 | 2 |  3 |   1 |
		| 2 | 1 |  3 |   1 |
		+---+---+----+-----+
		
	"Frame-2":
		+---+---+
		| a | b |
		+---+---+
		| 4 | 3 |
		| 5 | 2 |
		+---+---+
		
}
"answer": {
	"desc": %s Code: You could iterate through the columns summing those that have the same lowercase representation: Example: 
	"code-snippets": [
		def sumDupeColumns(df):
		    """Return dataframe with columns with the same lowercase spelling summed."""
		
		    # Get list of unique lowercase column headers
		    columns = set(map(str.lower, df.columns))
		    # Create new (zero-initialised) dataframe for output
		    df1 = pd.DataFrame(data=np.zeros((len(df), len(columns))), columns=columns)
		
		    # Sum matching columns
		    for col in df.columns:
		        df1[col.lower()] += df[col]
		
		    return df1
		
		----------------------------------------------------------------------
		import pandas as pd
		import numpy as np
		
		np.random.seed(seed=42)
		
		# Generate DataFrame with random int input and 'duplicate' columns to sum
		df = pd.DataFrame(columns = ['a','A','b','B','Cc','cC','d','eEe','eeE','Eee'], 
		                  data = np.random.randint(9, size=(5,10))
		
		df = sumDupeColumns(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54944513
"link": https://stackoverflow.com/questions/54944513/transforming-pandas-dataframe-where-column-entries-are-column-headers
"question": {
	"title": Transforming pandas dataframe, where column entries are column headers
	"desc": My dataset has 12 columns, X1-X6 and Y1-Y6. The variables X and Y match to each other - the first record means: 80 parts of A, 10 parts of C, 2 parts of J and 8 parts of K (each row has 100 total). I would like to be able to transform my dataset into a dataset in which the entries in columns X1-X6 are now the headers. See before and after datasets below. My dataset (before): The dataset I'd like to transform to: 
}
"io": {
	"Frame-1": 
		   X1 X2 X3   X4   X5   X6    Y1    Y2    Y3    Y4    Y5   Y6
		0   A  C  J    K  NaN  NaN  80.0  10.0   2.0   8.0   NaN  NaN
		1   F  N  O  NaN  NaN  NaN   2.0  25.0  73.0   NaN   NaN  NaN
		2   A  H  J    M  NaN  NaN  70.0   6.0  15.0   9.0   NaN  NaN
		3   B  I  K    P  NaN  NaN   0.5   1.5   2.0  96.0   NaN  NaN
		4   A  B  F    H    O    P  83.0   4.0   9.0   2.0   1.0  1.0
		5   A  B  F    G  NaN  NaN   1.0  16.0   9.0  74.0   NaN  NaN
		6   A  B  D    F    L  NaN  95.0   2.0   1.0   1.0   1.0  NaN
		7   B  F  H    P  NaN  NaN   0.2   0.4   0.4  99.0   NaN  NaN
		8   A  D  F    L  NaN  NaN  35.0  12.0  30.0  23.0   NaN  NaN
		9   A  B  F    I    O  NaN  95.0   0.3   0.1   1.6   3.0  NaN
		10  B  E  G  NaN  NaN  NaN  10.0  31.0  59.0   NaN   NaN  NaN
		11  A  F  G    L  NaN  NaN  24.0   6.0  67.0   3.0   NaN  NaN
		12  A  C  I  NaN  NaN  NaN  65.0  30.0   5.0   NaN   NaN  NaN
		13  A  F  G    L  NaN  NaN  55.0   6.0   4.0  35.0   NaN  NaN
		14  A  F  J    K    L  NaN  22.0   3.0  12.0   0.8  62.2  NaN
		15  B  F  I    P  NaN  NaN   0.6   1.2   0.2  98.0   NaN  NaN
		16  A  B  F    H    O  NaN  27.0   6.0  46.0  13.0   8.0  NaN
		
	"Frame-2":
		   A     B     C     D     E     F     G     H    I     J    K     L    M  \
		0 80.0  NaN  10.0   NaN   NaN   NaN   NaN   NaN  NaN   2.0  8.0   NaN  NaN   
		1 NaN   NaN   NaN   NaN   NaN   2.0   NaN   NaN  NaN   NaN  NaN   NaN  NaN   
		2 70.0  NaN   NaN   NaN   NaN   NaN   NaN   6.0  NaN  15.0  NaN   NaN  9.0   
		3 NaN   0.5   NaN   NaN   NaN   NaN   NaN   NaN  1.5   NaN  2.0   NaN  NaN   
		4 83.0  4.0   NaN   NaN   NaN   9.0   NaN   2.0  NaN   NaN  NaN   NaN  NaN   
		5 1.0   16.0   NaN   NaN   NaN   9.0  74.0   NaN  NaN   NaN  NaN   NaN  NaN   
		6 95.0   2.0   NaN   1.0   NaN   1.0   NaN   NaN  NaN   NaN  NaN   1.0  NaN   
		7 NaN   0.2   NaN   NaN   NaN   0.4   NaN   0.4  NaN   NaN  NaN   NaN  NaN   
		8 35.0   NaN   NaN  12.0   NaN  30.0   NaN   NaN  NaN   NaN  NaN  23.0  NaN   
		9 95.0   0.3   NaN   NaN   NaN   0.1   NaN   NaN  1.6   NaN  NaN   NaN  NaN   
		10 NaN  10.0   NaN   NaN  31.0  NaN   59.0   NaN  NaN   NaN  NaN   NaN  NaN   
		11 24.0  NaN   NaN   NaN   NaN   6.0  67.0   NaN  NaN   NaN  NaN   3.0  NaN   
		12 65.0  NaN  30.0   NaN   NaN   NaN   NaN   NaN  5.0   NaN  NaN   NaN  NaN   
		13 55.0  NaN   NaN   NaN   NaN   6.0   4.0   NaN  NaN   NaN  NaN  35.0  NaN   
		14 22.0  NaN   NaN   NaN   NaN   3.0   NaN   NaN  NaN  12.0  0.8  62.2  NaN   
		15 NaN   0.6   NaN   NaN   NaN   1.2   NaN   NaN  0.2   NaN  NaN   NaN  NaN   
		16 27.0  6.0   NaN   NaN   NaN  46.0   NaN  13.0  NaN   NaN  NaN   NaN  NaN   
		
		      N     O     P  
		0    NaN   NaN   NaN  
		1   25.0  73.0   NaN  
		2    NaN   NaN   NaN  
		3    NaN   NaN  96.0  
		4    NaN   1.0   1.0  
		5    NaN   NaN   NaN  
		6    NaN   NaN   NaN  
		7    NaN   NaN  99.0  
		8    NaN   NaN   NaN  
		9    NaN   3.0   NaN  
		10   NaN   NaN   NaN  
		11   NaN   NaN   NaN  
		12   NaN   NaN   NaN  
		13   NaN   NaN   NaN  
		14   NaN   NaN   NaN  
		15   NaN   NaN  98.0  
		16   NaN   8.0   NaN 
		
}
"answer": {
	"desc": %s As you know that you want the Xi part to contain the column names for the new dataframe, while the Yi part would be the value, it is enough to change every line in a dict where Xi is the key and Yi the value. Then you use the list of that dictionnaries to feed the new dataframe: gives: 
	"code-snippets": [
		data = list(df.apply(lambda x: {x['X'+ str(i)]: x['Y'+str(i)] for i in range(1,7)
		                                if x['X'+str(i)]!= 'NaN'}, axis=1))
		
		resul = pd.DataFrame(data)
		print(resul)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54923349
"link": https://stackoverflow.com/questions/54923349/top-3-values-per-row-in-pandas
"question": {
	"title": Top 3 Values Per Row in Pandas
	"desc": I have a large Pandas dataframe that is in the vein of: and I would like to generate output that looks like this, taking the column names of the 3 highest values for each row: I have tried using df.idmax(axis=1) which returns the 1st maximum column name but am unsure how to compute the other two? Any help on this would be truly appreciated, thanks! 
}
"io": {
	"Frame-1": 
		| ID | Var1 | Var2 | Var3 | Var4 | Var5 |
		|----|------|------|------|------|------|
		| 1  | 1    | 2    | 3    | 4    | 5    |
		| 2  | 10   | 9    | 8    | 7    | 6    |
		| 3  | 25   | 37   | 41   | 24   | 21   |
		| 4  | 102  | 11   | 72   | 56   | 151  |
		...
		
	"Frame-2":
		| ID | 1st Max | 2nd Max | 3rd Max |
		|----|---------|---------|---------|
		| 1  | Var5    | Var4    | Var3    |
		| 2  | Var1    | Var2    | Var3    |
		| 3  | Var3    | Var2    | Var1    |
		| 4  | Var5    | Var1    | Var3    |
		...
		
}
"answer": {
	"desc": %s Use for positions of sorted values with select by indexing, last pass it to constructor: Or if performance is not important use with per each row: 
	"code-snippets": [
		c = ['1st Max','2nd Max','3rd Max']
		df = (df.set_index('ID')
		        .apply(lambda x: pd.Series(x.nlargest(3).index, index=c), axis=1)
		        .reset_index())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 42142756
"link": https://stackoverflow.com/questions/42142756/how-can-i-change-a-specific-row-label-in-a-pandas-dataframe
"question": {
	"title": How can I change a specific row label in a Pandas dataframe?
	"desc": I have a dataframe such as: Where the final row contains averages. I would like to rename the final row label to so that the dataframe will look like this: I understand columns can be done with . But how can I do this with a specific row label? 
}
"io": {
	"Frame-1": 
		      0     1    2    3    4    5
		0  41.0  22.0  9.0  4.0  2.0  1.0
		1   6.0   1.0  2.0  1.0  1.0  1.0
		2   4.0   2.0  4.0  1.0  0.0  1.0
		3   1.0   2.0  1.0  1.0  1.0  1.0
		4   5.0   1.0  0.0  1.0  0.0  1.0
		5  11.4   5.6  3.2  1.6  0.8  1.0
		
	"Frame-2":
		      0     1    2    3    4    5
		0  41.0  22.0  9.0  4.0  2.0  1.0
		1   6.0   1.0  2.0  1.0  1.0  1.0
		2   4.0   2.0  4.0  1.0  0.0  1.0
		3   1.0   2.0  1.0  1.0  1.0  1.0
		4   5.0   1.0  0.0  1.0  0.0  1.0
		A  11.4   5.6  3.2  1.6  0.8  1.0
		
}
"answer": {
	"desc": %s You can get the last index using negative indexing similar to that in Python Then Edit: If you are looking for a one-liner, 
	"code-snippets": [
		df.index = df.index[:-1].tolist() + ['a']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54891381
"link": https://stackoverflow.com/questions/54891381/change-day-to-specific-entries-in-pandas-dataframe
"question": {
	"title": Change day to specific entries in pandas dataframe
	"desc": I have a dataframe in pandas which has an error in the index: each entry between 23:00:00 and 23:59:59 has a wrong date. I would need to subtract one day (i.e. 24 hours) to each entry between those two times. I know that I can obtain the entries between those two times as , where is my dataframe. However, can I modify the day only for those specific entries of the dataframe index? Resetting would take me more time, since my dataframe index is not evenly spaced as you can see from the figure below (the step between two consecutive entries is once 15 minutes and once 30 minutes). Note also from the figure the wrong date in the last three entries: it should be 2018-02-05 and not 2018-02-06. I tried to do this but I get Sample data: Expected output: 
}
"io": {
	"Frame-1": 
		2018-02-05 22:00:00    271.8000
		2018-02-05 22:30:00    271.5600
		2018-02-05 22:45:00    271.4400
		2018-02-06 23:15:00    271.3750
		2018-02-06 23:30:00    271.3425
		2018-02-06 00:00:00    271.2700
		2018-02-06 00:15:00    271.2300
		2018-02-06 00:45:00    271.1500
		2018-02-06 01:00:00    271.1475
		2018-02-06 01:30:00    271.1425
		2018-02-06 01:45:00    271.1400
		
	"Frame-2":
		2018-02-05 22:00:00    271.8000
		2018-02-05 22:30:00    271.5600
		2018-02-05 22:45:00    271.4400
		2018-02-05 23:15:00    271.3750
		2018-02-05 23:30:00    271.3425
		2018-02-06 00:00:00    271.2700
		2018-02-06 00:15:00    271.2300
		2018-02-06 00:45:00    271.1500
		2018-02-06 01:00:00    271.1475
		2018-02-06 01:30:00    271.1425
		2018-02-06 01:45:00    271.1400
		
}
"answer": {
	"desc": %s I solved the issue myself by using this answer. This is my code: 
	"code-snippets": [
		as_list = df.index.tolist()
		new_index = []
		for idx,entry in enumerate(as_list):
		    if entry.hour == 23:
		        if entry.day != 1:            
		            new_index.append(as_list[idx].replace(day = as_list[idx].day - 1))
		        else:
		            new_day = calendar.monthrange(as_list[idx].year, as_list[idx].month -1)[1]
		            new_index.append(as_list[idx].replace(day = new_day, month = entry.month -1))
		    else:
		        new_index.append(entry)
		df.index = new_index
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54884153
"link": https://stackoverflow.com/questions/54884153/remove-rows-when-the-occurrence-of-a-column-value-in-the-data-frame-is-less-than
"question": {
	"title": Remove rows when the occurrence of a column value in the data frame is less than a certain number using pandas/python?
	"desc": I have a data frame like this: I have seen that col1 values with B and D occurs more than one times in the data frame. I want to keep those values with occurrence more than one, the final data frame will look like: How to do this in most efficient way using pandas/python ? 
}
"io": {
	"Frame-1": 
		df
		col1    col2
		A         1
		B         1
		C         2
		D         3
		D         2
		B         1
		D         5
		
	"Frame-2":
		col1     col2
		 B         1
		 D         3
		 D         2
		 B         1
		 D         5
		
}
"answer": {
	"desc": %s Use with specify column for search dupes with for return s for all dupe rows, last filter by : If need specify threshold use with and filter same way like first solution: Alternative solution with and : If performance is not important use : 
	"code-snippets": [
		df = df[df['col1'].map(df['col1'].value_counts()) > 1]
		
		----------------------------------------------------------------------
		df = df.groupby('col1').filter(lambda x: len(x) > 1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54806672
"link": https://stackoverflow.com/questions/54806672/pandascalculate-mean-of-a-group-of-n-values-of-each-columns-of-a-dataframe
"question": {
	"title": Pandas:Calculate mean of a group of n values of each columns of a dataframe
	"desc": I have a dataframe of the following type: I want to calculate the mean of the first 3 element of each column and then next 3 elements and so on and then store in a dataframe. Desired Output- Using Group By was one of the approach I thought of but I am unable to figure out how to use Group by in this case. 
}
"io": {
	"Frame-1": 
		     A      B    
		0    1      2    
		1    4      5    
		2    7      8    
		3    10    11   
		4    13    14   
		5    16    17   
		
	"Frame-2":
		      A      B    
		0     4      5
		1     12    14
		
}
"answer": {
	"desc": %s If default then use integer division and pass to : Detail: General solution with array created by length of DataFrame - working with all index values: Detail: 
	"code-snippets": [
		print (df.index // 3)
		Int64Index([0, 0, 0, 1, 1, 1], dtype='int64')
		
		----------------------------------------------------------------------
		df = df.groupby(np.arange(len(df)) // 3).mean()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54777760
"link": https://stackoverflow.com/questions/54777760/pd-dataframe-prints-output-in-a-single-column
"question": {
	"title": pd.DataFrame prints output in a single column
	"desc": I have a file which I'm trying to convert into data frame using pandas. It is inside a loop and returns me the output as shown below. Here is the code I'm using: Here the tbl file is not tab seperated and to create a tab separation, I've to convert it into list. Here is the reult I'm getting: The expected output is like this: Any help will be highly appreciated. 
}
"io": {
	"Frame-1": 
		0    B
		1  244
		2    S
		3    0
		     0
		0    B
		1  245
		2    A
		3    0
		
	"Frame-2":
		0    B   244  S  0
		
		0    B   245  A  0
		
}
"answer": {
	"desc": %s Try this: 
	"code-snippets": [
		import pandas as pd
		import csv
		df=pd.DataFrame()
		with open('File.tbl', 'r') as  f:
		    P=list(f)
		    del P[0]
		for o in P:
		    M=o.split()
		    B= M[:4]             #selecting specific columns only
		    df = pd.concat([df,pd.DataFrame(B).T])    #converting into DataFrame
		df.to_csv('para.csv', sep=',')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54783721
"link": https://stackoverflow.com/questions/54783721/add-a-fix-value-to-a-dataframe-accumulating-to-future-ones
"question": {
	"title": Add a fix value to a dataframe (accumulating to future ones)
	"desc": I am trying to simulate inventory level during the next 6 months: 1- I have the expected accumulated demand for each day of next 6 months. So, with no reorder, my balance would be more negative everyday. 2- My idea is: Everytime the inventory level is lower than 3000, I would send an order to buy 10000, and after 3 days, my level would increase again: How is the best way to add this value into all the future values ? I started doing like this : But it just adds to the actual row, not for the accumulated future ones. 
}
"io": {
	"Frame-1": 
		       ds         saldo
		0 2019-01-01  10200.839819
		1 2019-01-02   5219.412952
		2 2019-01-03      3.161876
		3 2019-01-04  -5507.506201
		4 2019-01-05 -10730.291221
		5 2019-01-06 -14406.833593
		6 2019-01-07 -17781.500396
		7 2019-01-08 -21545.503098
		8 2019-01-09 -25394.427708
		
	"Frame-2":
		print(row['ds'], row['saldo'])
		9 2019-01-10 -29277.647817
		
}
"answer": {
	"desc": %s The problem is that you are only adding 1000 to the , not to every entry that follows. Try replacing with This will add 1000 to every following entry. 
	"code-snippets": [
		forecast_data['saldo'][index:] += forecast_data['saldo'][index:] + 1000
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54752268
"link": https://stackoverflow.com/questions/54752268/pandas-complex-merge-of-dataframes-with-date-basis
"question": {
	"title": Pandas: Complex merge of DataFrames with date basis
	"desc": I have 2 pandas 's that I need to merge in a bit of a complex manner so I am in need of some help. DataFrame to be inserted: DataFrame to receive insertion 1) The needs to establish a common basis for (notice column is different), thus the needs to be organized into , , , where the dates of , , and are used for assembling to correctly reflect the values in , , at a certain date. Sample output from step 1: 2) will need to be sorted by date according to and the columns , , will be added as columns in the . I imagine this will follow similar methods as in 1). Sample output from step 2: 3) The new columns , , will need to be filled forward with cumsum but I think I got that down: ~ Sample output from step 3: So the end goal would result in the values and shares held according to the index. For 's that will not have a column value (since column is "missing" some dates) in the resulting , it would be best to make that value but 0 would suffice. Happy to clarify anything, I appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! EDIT: Trying to merge in a loop now since number of - pairs may vary. I now have a list of separate for the - pairs: . Since number of pairs may vary, it seems best not to based on column labels hence . 
}
"io": {
	"Frame-1": 
		               0      1           2        3           4       5
		0     1998-01-02  16.25  2014-03-27   558.46  1998-01-02  131.13
		1     1998-01-05  15.88  2014-03-28   559.99  1998-01-05  130.38
		2     1998-01-06  18.94  2014-03-31   556.97  1998-01-06  131.13
		3     1998-01-07  17.50  2014-04-01   567.16  1998-01-07  129.56
		4     1998-01-08  18.19  2014-04-02   567.00  1998-01-08  130.50
		5     1998-01-09  18.19  2014-04-03   569.74  1998-01-09  127.00
		6     1998-01-12  18.25  2014-04-04   543.14  1998-01-12  129.50
		7     1998-01-13  19.50  2014-04-07   538.15  1998-01-13  132.13
		8     1998-01-14  19.75  2014-04-08   554.90  1998-01-14  131.13
		9     1998-01-15  19.19  2014-04-09   564.14  1998-01-15  132.31
		10    1998-01-16  18.81  2014-04-10   540.95  1998-01-16  135.25
		11    1998-01-20  19.06  2014-04-11   530.60  1998-01-20  137.81
		12    1998-01-21  18.91  2014-04-14   532.52  1998-01-21  137.00
		13    1998-01-22  19.25  2014-04-15   536.44  1998-01-22  138.63
		14    1998-01-23  19.50  2014-04-16   556.54  1998-01-23  138.25
		15    1998-01-26  19.44  2014-04-17   536.10  1998-01-26  141.75
		
	"Frame-2":
		               0      1       3       5
		0     1998-01-02  16.25      NA  131.13
		1     1998-01-05  15.88      NA  130.38
		2     1998-01-06  18.94      NA  131.13
		3     1998-01-07  17.50      NA  129.56
		4     1998-01-08  18.19      NA  130.50
		5     1998-01-09  18.19      NA  127.00
		6     1998-01-12  18.25      NA  129.50
		7     1998-01-13  19.50      NA  132.13
		8     1998-01-14  19.75      NA  131.13
		...
		10    2014-04-10  18.81  558.46  135.25
		11    2014-04-11  19.06  559.99  137.81
		12    2014-04-14  18.91  556.97  137.00
		13    2014-04-15  19.25  567.16  138.63
		14    2014-04-16  19.50  567.00  138.25
		15    2014-04-17  19.44  569.74  141.75
		...
		
}
"answer": {
	"desc": %s Based on your clarifications, here's what I understand a solution to be. Given the dataframes given, I added column names to make things more clear, and renamed to just to save on typing: Now, here's where I couldn't test, since your insert df has no matching date indices from your receive df, but that should look like Of course, this assumes that the column is set as the index already as in your sample. If you want a numerical index at the end, you can , or you can leave the date as the index. It looks like you already have a handle on the last part of the task, and I can't test it anyway with the given data. And note also that you can reorder the columns however you want depending how you want your output to look () 
	"code-snippets": [
		final_df=pd.merge(left=rec_df, right=insert_df, left_index=True, right_index=True, dropna=False)
		
		----------------------------------------------------------------------
		df=df[[list of columns in order]]
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 44606413
"link": https://stackoverflow.com/questions/44606413/grouping-columns-by-unique-values-in-python
"question": {
	"title": Grouping columns by unique values in Python
	"desc": I have a data set with two columns and I need to change it from this format: to this I need every unique value in the first column to be on its own row. I am a beginner with Python and beyond reading in my text file, I'm at a loss for how to proceed. 
}
"io": {
	"Frame-1": 
		10  1 
		10  5
		10  3
		11  5
		11  4
		12  6
		12  2
		
	"Frame-2":
		10  1  5  3
		11  5  4
		12  6  2
		
}
"answer": {
	"desc": %s You can use Pandas dataframes. Output: Let's use and : Output: 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'A':[10,10,10,11,11,12,12],'B':[1,5,3,5,4,6,2]})
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46723310
"link": https://stackoverflow.com/questions/46723310/conditional-mean-and-sum-of-previous-n-rows-in-pandas-dataframe
"question": {
	"title": Conditional mean and sum of previous N rows in pandas dataframe
	"desc": Concerned is this exemplary pandas dataframe: Whenever is , I wish to calculate sum and mean of the last 3 (starting from current) valid measurements. Measurements are considered valid, if the column is . So let's clarify using the two examples in the above dataframe: : Indices should be used. Expected : Indices should be used. Expected I have tried and creating new, shifted columns, but was not successful. See the following excerpt from my tests (which should directly run): Any help or solution is greatly appreciated. Thanks and Cheers! EDIT: Clarification: This is the resulting dataframe I expect: EDIT2: Another clarification: I did indeed not miscalculate, but rather I did not make my intentions as clear as I could have. Here's another try using the same dataframe: Let's first look at the column: We find the first in index 3 (green rectangle). So index 3 is the point, where we start looking. There is no valid measurement at index 3 (Column is ; red rectangle). So, we start to go further back in time, until we have accumulated three lines, where is . This happens for indices 2,1 and 0. For these three indices, we calculate the sum and mean of the column (blue rectangle): SUM: 2.0 + 4.0 + 3.0 = 9.0 MEAN: (2.0 + 4.0 + 3.0) / 3 = 3.0 Now we start the next iteration of this little algorithm: Look again for the next in the column. We find it at index 7 (green rectangle). There is also a valid measuremnt at index 7, so we include it this time. For our calculation, we use indices 7,6 and 5 (green rectangle), and thus get: SUM: 1.0 + 2.0 + 3.0 = 6.0 MEAN: (1.0 + 2.0 + 3.0) / 3 = 2.0 I hope, this sheds more light on this little problem. 
}
"io": {
	"Frame-1": 
		Sum = 9.0, Mean = 3.0
	"Frame-2":
		Sum = 6.0, Mean = 2.0
}
"answer": {
	"desc": %s Heres an option, take the 3 period rolling mean and sum Now set False Triggers equals to yields Edit, updated to reflect valid argument Yields 
	"code-snippets": [
		df['RollM'] = df.Measurement.rolling(window=3,min_periods=0).mean()
		
		df['RollS'] = df.Measurement.rolling(window=3,min_periods=0).sum()
		
		----------------------------------------------------------------------
		df.loc[df.Trigger == False,['RollS','RollM']] = np.nan
		
		----------------------------------------------------------------------
		   Measurement  Trigger  Valid     RollM  RollS
		0          2.0    False   True       NaN    NaN
		1          4.0    False   True       NaN    NaN
		2          3.0    False   True       NaN    NaN
		3          0.0     True  False  2.333333    7.0
		4        100.0    False   True       NaN    NaN
		5          3.0    False   True       NaN    NaN
		6          2.0    False   True       NaN    NaN
		7          1.0     True   True  2.000000    6.0
		
		----------------------------------------------------------------------
		df['mean'],df['sum'] = np.nan,np.nan
		
		roller = df.Measurement.rolling(window=3,min_periods=0).agg(['mean','sum'])
		
		df.loc[(df.Trigger == True) & (df.Valid == True),['mean','sum']] = roller
		
		df.loc[(df.Trigger == True) & (df.Valid == False),['mean','sum']] = roller.shift(1)
		
		----------------------------------------------------------------------
		  Measurement  Trigger  Valid  mean  sum
		0          2.0    False   True   NaN  NaN
		1          4.0    False   True   NaN  NaN
		2          3.0    False   True   NaN  NaN
		3          0.0     True  False   3.0  9.0
		4        100.0    False   True   NaN  NaN
		5          3.0    False   True   NaN  NaN
		6          2.0    False   True   NaN  NaN
		7          1.0     True   True   2.0  6.0
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54737891
"link": https://stackoverflow.com/questions/54737891/convert-the-last-non-zero-value-to-0-for-each-row-in-a-pandas-dataframe
"question": {
	"title": Convert the last non-zero value to 0 for each row in a pandas DataFrame
	"desc": I'm trying to modify my data frame in a way that the last variable of a label encoded feature is converted to 0. For example, I have this data frame, top row being the labels and the first column as the index: Columns 1-10 are the ones that have been encoded. What I want to convert this data frame to, without changing anything else is: So the last values occurring in each row should be converted to 0. I was thinking of using the last_valid_index method, but that would take in the other remaining columns and change that as well, which I don't want. Any help is appreciated 
}
"io": {
	"Frame-1": 
		df
		   1  2  3  4  5  6  7  8  9  10
		0  0  1  0  0  0  0  0  0  1   1
		1  0  0  0  1  0  0  0  0  0   0
		2  0  0  0  0  0  0  0  0  1   0
		
	"Frame-2":
		   1  2  3  4  5  6  7  8  9  10
		0  0  1  0  0  0  0  0  0  1   0
		1  0  0  0  0  0  0  0  0  0   0
		2  0  0  0  0  0  0  0  0  0   0
		
}
"answer": {
	"desc": %s You can use to build a boolean mask, and set to zero. Another similar option is reversing before calling , you can now do this in a single line. If you have more columns, just apply these operations on the slice. Later, assign back. 
	"code-snippets": [
		u = df.iloc[:, :10]
		df[u.columns] = u[~u.iloc[:, ::-1].cumsum(1).le(1)].fillna(0, downcast='infer')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50418202
"link": https://stackoverflow.com/questions/50418202/pandas-remove-all-nan-values-in-all-columns
"question": {
	"title": Pandas: Remove all NaN values in all columns
	"desc": I have a data frame with many null records: I want to remove all NaN values in all rows of columns . As you could see, each column has different number of rows. So, I want to get something like this: I tried But it removes all records in the dataframe. How may I do that ? 
}
"io": {
	"Frame-1": 
		Col_1    Col_2      Col_3
		10         5          2
		22         7          7
		3         9          5       
		4         NaN       NaN
		5         NaN       NaN
		6         4         NaN
		7         6          7
		8         10        NaN
		12        NaN        1
		
	"Frame-2":
		Col_1    Col_2      Col_3
		10         5          2
		22         7          7
		3          9          5       
		4          4          7
		6          6          1
		7         10          
		8                 
		12    
		
}
"answer": {
	"desc": %s  As you could see, each column has different number of rows. A DataFrame is a tabular data structure: you can look up an index and a column, and find the value. If the number of rows is different per columns, then the index is meaningless and misleading. A might be a better alternative: or 
	"code-snippets": [
		{c: df[c].dropna().values for c in df.columns}
		
		----------------------------------------------------------------------
		{c: list(df[c]) for c in df.columns}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54733869
"link": https://stackoverflow.com/questions/54733869/how-to-get-the-rank-of-current-row-compared-to-previous-rows
"question": {
	"title": How to get the Rank of current row compared to previous rows
	"desc": How to get the Rank of current row compared to previous rows I have a dataframe like: I want to get the rank of current row compared to all previous rows for Volume Column. Desired Dataframe Data: pandas.DataFrame.rank Function doesnot serve my purpose. 
}
"io": {
	"Frame-1": 
		Instru Price Volume
		ABCD   1000  100258
		ABCD   1000  100252
		ABCD   1000  100168
		ABCD   1000  100390
		ABCD   1000  100470
		ABCD   1000  100420
		
	"Frame-2":
		Instru Price Volume  Rank
		ABCD   1000  100258  1     => 1st Row so Rank 1
		ABCD   1000  100252  2     => Rank 2 (Compare 100258,100252)
		ABCD   1000  100168  3     => Rank 3 (Compare 100258,100252,100168)
		ABCD   1000  100390  1     => Rank 1 (Compare 100390,100258,100252,100168)
		ABCD   1000  100470  1     => Rank 1 (Compare 100470,100390,100258,100252,100168)
		ABCD   1000  100420  2     => Rank 2 (Compare 100470,100420,100390,100258,100252,100168)
		
}
"answer": {
	"desc": %s Use np.searchsorted after a cumulative sort: Output 
	"code-snippets": [
		df['Rank'] = np.array([i - np.searchsorted(sorted(df.Volume[:i]), v) for i, v in enumerate(df.Volume)]) + 1
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54621203
"link": https://stackoverflow.com/questions/54621203/efficient-python-pandas-equivalent-implementation-of-r-sweep-with-multiple-argum
"question": {
	"title": Efficient python pandas equivalent/implementation of R sweep with multiple arguments
	"desc": Other questions attempting to provide the equivalent to 's function (like here) do not really address the case of multiple arguments where it is most useful. Say I wish to apply a 2 argument function to each row of a Dataframe with the matching element from a column of another DataFrame: In I got the equivalent using on what is basically a loop through the row counts. I highly doubt this is efficient in , what is a better way of doing this? Both bits of code should result in a Dataframe/matrix of 6 numbers when applying : I should state clearly that the aim is to insert one's own function into this like behavior say: resulting in: What is a good way of doing that in python pandas? 
}
"io": {
	"Frame-1": 
		   A   B
		1 10 110
		2 22 132
		3 36 156
		
	"Frame-2":
		 A B
		[1,] 3 4
		[2,] 3 4
		[3,] 3 5
		
}
"answer": {
	"desc": %s If I understand this correctly, you are looking to apply a binary function f(x,y) to a dataframe (for the x) row-wise with arguments from a series for y. One way to do this is to borrow the implementation from pandas internals itself. If you want to extend this function (e.g. apply along columns, it can be done in a similar manner, as long as f is binary. If you need more arguments, you can simply do a on f to make it binary Hope this helps. 
	"code-snippets": [
		import pandas as pd
		from pandas.core.dtypes.generic import ABCSeries
		
		def sweep(df, series, FUN):
		    assert isinstance(series, ABCSeries)
		
		    # row-wise application
		    assert len(df) == len(series)
		    return df._combine_match_index(series, FUN)
		
		
		# define your binary operator
		def f(x, y):
		    return x*y    
		
		# the input data frames
		df = pd.DataFrame( { "A" : range(1,4),"B" : range(11,14) } )
		df2 = pd.DataFrame( { "X" : range(10,13),"Y" : range(10000,10003) } )
		
		# apply
		test1 = sweep(df, df2.X, f)
		
		# performance
		# %timeit sweep(df, df2.X, f)
		# 155 s  1.27 s per loop (mean  std. dev. of 7 runs, 10000 loops each)#
		
		# another method
		import numpy as np
		test2 = pd.Series(range(df.shape[0])).apply(lambda row_count: np.multiply(df.iloc[row_count,:],df2.iloc[row_count,df2.columns.get_loc('X')]))
		
		# %timeit performance
		# 1.54 ms  56.4 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
		
		assert all(test1 == test2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54674956
"link": https://stackoverflow.com/questions/54674956/transfer-pandas-dataframe-column-names-to-dictionary
"question": {
	"title": Transfer pandas dataframe column names to dictionary
	"desc": I'm trying to convert a pandas dataframe column names into a dictionary. Not so worried about the actual data in the dataframe. Say I have an example dataframe like this and I'm not too worried about index just now: I'd like to get an output of a dictionary like: Not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order. 
}
"io": {
	"Frame-1": 
		Col1 Col2 Col3 Col4
		--------------------
		 a    b    c    a
		 b    d    e    c
		
	"Frame-2":
		{'Col1': 0, 'Col2': 1, 'Col3': 2, 'Col4': 3}
		
}
"answer": {
	"desc": %s That is straight forward with a comprehension as: Code: Test Code: Results: 
	"code-snippets": [
		{c: i for i, c in enumerate(df.columns)}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54629663
"link": https://stackoverflow.com/questions/54629663/how-to-drop-a-percentage-of-rows-where-the-column-value-is-nan
"question": {
	"title": How to drop a percentage of rows where the column value is NaN
	"desc": Let's say I have a where a certain column has 50% missing values. How can I drop let's say 10% of the rows which are missing values in respect to the column? Basically how can I reduce the percentage of missing values of a column from 50% to 40%? Input (50% of values are missing (6/12)): Output (40% of values are missing (4/10)): We dropped the last 2 NaN rows with ID's of 8 and 10 
}
"io": {
	"Frame-1": 
		         0
		    0  1.0
		    1  1.0
		    2  NaN
		    3  NaN
		    4  NaN
		    5  1.0
		    6  NaN
		    7  1.0
		    8  NaN
		    9  1.0
		   10  NaN
		   11  1.0
		
	"Frame-2":
		         0
		    0  1.0
		    1  1.0
		    2  NaN
		    3  NaN
		    4  NaN
		    5  1.0
		    6  NaN
		    7  1.0
		    9  1.0
		   11  1.0
		
}
"answer": {
	"desc": %s To get the array with the indices with nan values in your column, use: To drop, say, the first 20%, use: 
	"code-snippets": [
		df.drop(nan_indices[:int(len(nan_indices) * 0.2)])   #to create a new DataFrame, if you want to modify the original one, put inplace=True
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54640734
"link": https://stackoverflow.com/questions/54640734/faster-way-to-update-a-column-in-a-pandas-data-frame-based-on-the-value-of-anoth
"question": {
	"title": Faster way to update a column in a pandas data frame based on the value of another column
	"desc": I have a pandas data frame with columns = [A, B, C, D, ...I, Z]. There are around ~80000 rows in the dataframe, and columns A, B, C, D, ..., I have value 0 for all these rows. Z has a value between [0, 9]. What I am trying to do is update the value of the x'th column for all rows in the data frame, where x is the current value of Z. If value of x is 0, then ignore. The data frame looks like - This is what I have so far. This is way too slow, and causes the script to stop executing midway. Is there a faster or better way to do it? I tried looking at np.where and np.apply, but I am not able to figure out the syntax. This is what I tried using np.apply - The desired output for the above sample is - 
}
"io": {
	"Frame-1": 
		    A    B    C    D  ...  Z
		0   0    0    0    0  ...  9
		1   0    0    0    0  ...  1
		2   0    0    0    0  ...  2
		3   0    0    0    0  ...  3    
		
	"Frame-2":
		    A    B    C    D  ...  Z
		0   0    0    0    0  ...  9
		1   0    1    0    0  ...  1
		2   0    0    1    0  ...  2
		3   0    0    0    1  ...  3 
		
}
"answer": {
	"desc": %s  yields Pandas has a function, pd.get_dummies, which does exactly what you want: By making a NumPy array, you can use NumPy integer array indexing to generate the desired column labels. (The purpose of the column is explained below): So that generates this DataFrame: copies non-NaN values from the DataFrame into . Since does not have a column labeled , the values in that column are ignored. Alternatively, construct by concatenating with : yields Notice that some columns may be missing if there is no value in the column which corresponds to it. 
	"code-snippets": [
		In [276]: cols[df['Z']]
		Out[276]: array(['temp', 'B', 'C', 'D', 'B', 'F', 'E'], dtype='<U3')
		
		----------------------------------------------------------------------
		import numpy as np
		import pandas as pd
		cols = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'temp'])
		df = pd.DataFrame({'Z':[9,1,2,3,1,5,4]})
		
		df = pd.concat([pd.get_dummies(cols[df['Z']]), df['Z']], axis=1)
		df = df.drop('temp', axis=1)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54607098
"link": https://stackoverflow.com/questions/54607098/how-to-create-a-join-in-dataframe-based-on-the-other-dataframe
"question": {
	"title": How to create a join in Dataframe based on the other dataframe?
	"desc": I have 2 dataframes. One containing student batch details and another one with points. I want to join 2 dataframes. Dataframe1 contains Dataframe2 contains I am trying to map the mark in the same dataset for each student. I tried below code but it is replacing the values one by one. 
}
"io": {
	"Frame-1": 
		+-------+-------+-------+--+
		|  s1   |  s2   |  s3   |  |
		+-------+-------+-------+--+
		| Stud1 | Stud2 | Stud3 |  |
		| Stud2 | Stud4 | Stud1 |  |
		| Stud1 | Stud3 | Stud4 |  |
		+-------+-------+-------+--+
		
	"Frame-2":
		+-------+-------+-------+----+----+----+
		| Stud1 | Stud2 | Stud3 | 90 | 80 | 95 |
		| Stud2 | Stud4 | Stud1 | 80 | 55 | 90 |
		| Stud1 | Stud3 | Stud4 | 90 | 95 | 55 |
		+-------+-------+-------+----+----+----+
		
}
"answer": {
	"desc": %s Solution working same if all values from exist in column : Or: Or: If not, output is different - for not exist values () get s: And for get original value: 
	"code-snippets": [
		s = dfnamepoints.set_index('Name')['Point']
		df = df3.join(df3.replace(s).add_prefix('new_'))
		
		----------------------------------------------------------------------
		df = df3.join(df3.apply(lambda x: x.map(s)).add_prefix('new_'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54577752
"link": https://stackoverflow.com/questions/54577752/pandas-dataframe-from-dictionary
"question": {
	"title": Pandas DataFrame from Dictionary
	"desc": Let's assume that I have a JSON file like below, and I want to convert this file into a data frame with 2 columns: This is what I already tried, but I am unable to create DF and probably there is a more efficient way of doing this task: Expecting output like this for all elements in a Json file Also I want to create a directed graph for each parent node with child nodes as clusters because parent nodes has the same child nodes which are also present in other parent nodes. Thanks in Advance 
}
"io": {
	"Frame-1": 
		{"1087": [4,5,6,7,8,9,10,12,13,21,22,23,24,25,26,27,28,34,35 ,37,39,40,42,44,45,46,47,48,51,52,54,55,56,59,60,61,63,64,65,66,67,68,72,73,74,75,78,80,81,82,83,84,85,87,88,92,94,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,149,180,181,196,198,200,202,206,222,223,226,227,230,231,232,233,234,235,242,255,257,258,259,261,263,264,265,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,302,303,304,305,306,307,308,309,310,311,313,314,316,318,319,320,323,325,326,327,328,330,334,336,337,339,340,342,343,350,351,354,355,362,363,365,366,367,368,369,370,371,372,374,375,376,377,378,379,380,383,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,427,428,429,430,431,432,433,434,435,437,438,444,446,449,451,455,457,461,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,494,496,498,499,500,502,504,506,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,559,560,561,565,567,569,571,573,574,576,579,580,581,583,586,587,588,590,593,594,597,598,600,601,602,604,605,606,607,608,609,611,612,613,614,615,616,617,620,621,622,624,625,626,629,631,633,634,636,638,639,640,641,643,644,647,649,650,651,652,653,654,657,658,664,665,666,667,669,671,674,675,676,677,678,682,683,685,686,687,688,692,694,695,702,703,705,708,712,713,714,715,716,717,718,720,728,732,734,735,739,740,742,743,745,746,751,752,759,769,770,772,778,779,780,783,784,786,792,805,815,823,831,832,834,835,836,837,838,839,852,854,855,856,867,875,877,879,888,890,891,896,900,908,909,910,911,912,913,914,915,916,917,918,919,934,935,936,937,938,939,944,945,946,950,951,952,953,957,958,959,960,964,965,966,967,971,975,977,978,980,981,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1027,1028,1033,1034,1035,1036,1037,1038,1039,1049,1061,1063,1065,1067,1069,1070,1071,1072,1073,1074,1076,1077,1078,1080,1081,1084,1088,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1127,1128,1129,1130,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1151,1155,1156,1201,1202,1203,1204,1207,1208,1209,1213,1214,1215,1216,1217,1220,1221,1222,1223,1224,1232,1233,1235,1237,1238,1241,1243,1244,1245,1248,1249,1251,1254,1269,1271,1273,1274,1275,1284,1289,1298,1301,1302,1303,1470,1495,1500,1501,1508,1509,1517,1518,1572,1573,1574,1575,1614,1619,1620,1625,1633,1639,1661,1669,1670,1671,1692,1693,1694,1695,1696,1698,1699,1700,1701,1706,1707,1708,1709,1711,1712,1713,1715,1720,1726,1728,1729,1730,1731,1732,1734,1755,1771,1780,1781,1785,1788,1794,1795,1797,1801,1802,1803,1805,1827,1829,1830,1836,1838,1843,1845,1847,1849,1851,1852,1853,1854,1855,1897,1899,1901,1920,1922,1923,1974,1987,1988,1989,1990,1991,1993,1994,2013,2014,2038,2039,2040,2044,2057,2086,2108,2144,2150,2215,2216,2218,2219,2220,2227,2228,2229,2230,2250,2258,2271,2279,2283,2285,2286,2287,2295,2302,2327,2390,2397,2406,2407,2411,2413,2414,2415,2419,2421,2429,2441,2471,2472,2490,2493,2507,2514,2519,2524,2525,2531,2532,2535,2538,2541,2551,2552,2553,2555,2560,2561,2562,2564,2570,2577,2578,2579,2580,2581,2586,2587,2588,2589,2591,2592,2594,2595,2596,2597,2599,2600,2601,2602,2603,2604,2605,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2625,2626,2627,2628,2629,2630,2631,2634,2635,2665,2668,2669,2671,2673,2681,2682,2683,2684,2705,2706,2707,2708,2709,2710,2711,2712,2713,2750,2766,2768,2769,2770,2798,2804,2817,2821,2822,2823,2824,2825,2826,2844,2847,2853,2855,2858,2860,2861,2862,2863,2864,2865,2880,2900,2901,2902,2903,2906,2911,2912,2913,2916,2918,2922,2925,2926,2932,2935,2941,2943,2945,2947,2948,2949,2950,2958,2959,2966,2967,2972,2976,2977,2978,2979,2980,2981,2982,2987,2988,2991,2992,2993,2994,2995,2996,2999,3001,3003,3007,3008,3011,3012,3015,3016,3017,3018,3024,3030,3031,3033,3034,3045,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3093,3099,3105,3112,3113,3114,3115,3116,3117,3127,3128,3154,3155,3156,3157,3297,3299,3300,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3336,3339,3416,3417,3420,3424,3550,3587,3588,3589,3590,3591,3592,3593,3598,3599,3600,3602,3603,3604,3605,3606,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3625,3655,3656,3657,3718,3721,3724,3725,3726,3730,3732,3733,3736,3738,3741,3743,3744,3747,3748,3750,3752,3754,3756,3762,3763,3764,3765,3766,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3795,3797,3798,3800,3806,3811,3864,3866,3867,3871,3881,3883,3884,3885,3886,3925,3926,3929,3930,3935,3936,3940,4018,4030,4045,4049,4050,4051,4054,4058,4059,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4104,4109,4110,4113,4116,4117,4118,4119,4161,4267,4285,4310,4317,4335,4358,4359,4365,4366,4467,4471,4475,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4638,4639,4764,4765,4766,4782,4803,4824,4827,4828,4830,4888,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4926,4990,6998,7026,7027,7028],"1096": [25,26,27,28,45,46,63,64,65,66,67,80,81,82,83,84,85,128,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,330,366,367,376,385,386,387,388,391,417,418,419,420,437,449,451,553,555,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,641,643,644,779,780,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1273,1274,1275,1278,1280,1289,1292,1670,1671,1713,1730,1731,1847,1849,1993,2086,2218,2219,2220,2258,2421,2586,2587,2608,2610,2611,2629,2631,2673,2708,2709,2710,2711,2712,2713,2821,2822,2823,2825,2844,2847,2858,2860,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2959,2976,2977,2978,2979,2980,2981,2982,2991,2992,2993,2994,2995,3001,3003,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3112,3113,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3416,3417,3589,3590,3591,3592,3593,3598,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3656,3657,3732,3738,3743,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3797,3798,3871,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4267,4358,4359,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4764,4765,4766,7026,7027,7028],"1144": [25,26,27,28,144,372,374,422,768,1005,1051,1052,1053,1054,1057,1058,1060,1062,1064,1066,1068,1098,1101,1146,1703,1704,1705,1713,1716,1994,2086,2382,3095,3096,3097,3114,3115,3116,3339,3619,3620,3621,3732,3738,3743,3881,3883,3884,3885,3886,4113,4116,4117,4118,4119,4267,4285,4365,4370,4371,4372,4373,4374,4375,4471,4764,4765,4766,4803,4824,4828,4830,4990],"-1": [40,63,64,65,66,67,68,80,81,82,83,84,85,87,130,131,132,133,134,135,136,137,138,139,140,141,234,261,263,264,267,268,269,271,272,293,308,314,318,319,337,366,367,375,376,385,386,387,388,391,407,416,417,418,419,420,435,461,489,559,561,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,623,624,625,626,632,634,636,644,666,682,683,685,686,687,688,694,695,696,720,737,854,855,870,882,883,888,896,916,917,918,919,930,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,971,978,982,986,987,988,993,994,995,996,1000,1001,1002,1036,1037,1038,1039,1081,1132,1133,1134,1135,1136,1210,1317,1321,1341,1347,1377,1378,1380,1383,1384,1386,1396,1398,1408,1410,1432,1456,1458,1473,1500,1501,1525,1614,1670,1730,1808,1838,1982,1983,1984,1985,1993,2033,2034,2038,2039,2040,2069,2150,2151,2258,2355,2356,2571,2596,2692,2729,2737,2821,2822,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2976,2977,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3150,3151,3316,3416,3417,3594,3744,3769,3770,3773,3776,3785,3786,3787,3871,4064,4066,4068,4070,4072,4074,4076,4078,4080,4082,4084,4086,4088,4092,4094,4096,4098,4109,4288,4321,4330,4466,4991,5567,6913],"2060": [47,65,67,80,81,148,155,156,166,167,168,226,227,267,268,269,580,594,597,600,601,602,604,607,608,609,611,614,634,636,682,683,685,686,687,688,694,695,696,728,733,738,744,944,945,946,993,994,995,1317,1321,1341,1347,1377,1378,1380,1383,1384,1385,1386,1387,1396,1398,1408,1410,1432,1456,1458,1473,1525,1696,1736,1737,1738,1739,1754,1808,1859,1865,1873,1879,1885,1892,1922,1982,1983,1984,1985,1993,1994,2038,2039,2040,2150,2151,2254,2300,2355,2356,2377,2391,2448,2478,2530,2564,2723,2742,2745,2746,2747,2882,2922,2925,2947,2948,2949,2950,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3341,3383,3482,3493,3494,3506,3530,3672,3675,3821,4332,4439,4440,4459,4908,4913,4914,4915,4916,4917,4919,4920,4921,4922,4923,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5177,5178,5179,5180,5181,5182,5183,5184,5188,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5217,5218,5219,5220,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5486,5487,5488,5489,5490,5491,5492,5493,5494,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5559,5566,5567,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,7037],"1742": [47,60,61,63,64,65,66,67,80,81,82,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,226,227,232,233,242,267,268,269,271,314,319,323,366,367,376,385,388,417,418,419,420,554,559,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,633,634,636,643,700,701,702,703,705,717,728,745,746,834,835,836,837,838,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1088,1119,1120,1121,1122,1123,1207,1208,1209,1213,1214,1215,1216,1269,1386,1508,1509,1692,1693,1694,1695,1698,1699,1700,1701,1720,1726,1727,1729,1780,1781,1830,1851,1920,1993,2127,2216,2258,2295,2390,2564,2621,2821,2823,2844,2847,2862,2863,2864,2865,2911,2912,2913,2916,2922,2925,2935,2943,2945,2947,2948,2949,2950,2978,2979,2980,2981,2982,2987,2988,2996,3007,3008,3011,3012,3015,3016,3017,3018,3072,3099,3112,3113,3154,3155,3156,3157,3311,3312,3315,3318,3353,3355,3420,3422,3423,3590,3591,3592,3625,3732,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3794,3801,3871,3940,4049,4050,4058,4063,4169,4170,4476,4477,4482,4616,4906,6864,6865],"1125": [47,53,65,67,80,81,172,174,187,190,196,198,200,202,246,267,268,269,309,313,316,319,320,323,324,325,326,370,372,374,421,448,594,597,600,634,636,657,658,673,679,692,708,735,860,944,945,946,993,994,995,1061,1063,1065,1067,1220,1222,1223,1277,1502,1517,1518,1572,1573,1574,1575,1621,1622,1623,1632,1635,1637,1661,1696,1849,1897,1899,1901,1968,1993,2032,2033,2034,2069,2283,2421,2422,2423,2445,2471,2472,2490,2493,2527,2529,2609,2623,2627,2669,2671,2729,2738,2739,2804,2825,2826,2853,2854,2855,2856,2894,2895,2901,2902,2903,2904,2918,2926,2932,3024,3107,3114,3115,3116,3299,3353,3354,3355,3356,3364,3365,3373,3390,3391,3392,3393,3415,3422,3423,3425,3541,3550,3715,3719,3720,3721,3792,3793,3794,3795,3800,3835,3836,3844,3845,3846,3847,3864,3866,3867,3920,3925,3926,3929,3930,3935,3936,4030,4059,4090,4111,4112,4138,4143,4145,4161,4162,4165,4306,4311,4351,4361,4368,4397,4457,4467,4471,4480,4638,4639,4754,4764,4765,4766,4770,4803,4806,4807,4808,4824,4830,4888,4904,4905,4911,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,6892,6893],"1095": [64,65,66,67,80,81,187,190,196,198,200,202,219,267,268,269,319,320,324,385,388,559,573,576,580,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,644,679,689,690,691,751,756,842,843,844,845,846,847,848,937,938,939,944,945,946,951,952,953,958,959,960,964,965,966,967,986,987,988,993,994,995,1000,1001,1002,1993,2098,2250,2258,2354,2421,2495,2496,2821,2823,2839,2844,2847,2854,2856,2862,2863,2864,2865,2922,2925,2947,2948,2949,2950,2978,2980,3007,3011,3015,3016,3017,3018,3113,3299,3306,3310,3315,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3368,3422,3423,3590,3591,3592,3748,3750,3752,3754,3756,3762,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3792,3793,3794,3795,3801,3871],"1145": [64,65,66,67,80,81,82,84,85,125,129,130,131,132,133,134,135,140,141,263,264,267,268,269,327,334,351,367,388,446,553,594,597,600,601,602,604,607,608,609,611,614,620,622,629,631,634,643,692,735,786,867,896,937,938,939,944,945,946,950,951,952,953,958,959,960,975,977,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1077,1088,1119,1120,1121,1122,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1204,1224,1232,1233,1244,1245,1275,1517,1518,1572,1573,1574,1575,1661,1669,1670,1671,1709,1711,1729,1730,1731,1843,1845,1897,1899,1901,1993,1994,2013,2057,2218,2219,2220,2227,2228,2229,2230,2250,2258,2283,2327,2471,2472,2490,2493,2560,2562,2587,2596,2608,2609,2610,2611,2630,2631,2673,2682,2683,2684,2705,2706,2708,2709,2710,2711,2712,2713,2817,2821,2825,2844,2862,2863,2880,2900,2901,2902,2916,2918,2922,2925,2926,2941,2947,2948,2949,2950,2977,2978,2982,2991,2992,2993,3007,3011,3015,3016,3017,3018,3033,3034,3072,3297,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3417,3588,3721,3744,3864,3866,3867,4030,4034,4059,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4161,4358,4359,4366,4638,4639],"1966": [64,65,66,67,80,81,82,83,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,366,367,376,385,386,387,388,391,417,418,419,420,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,616,624,625,626,634,636,639,640,641,644,778,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1075,1114,1115,1116,1117,1118,1728,1993,2258,2596,2673,2821,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3112,3113,3590,3591,3592,3744,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3800,3871,4062,4764,4765,4766],"2148": [159,220,696,1050,1277,1278,1280,1696,1994,2032,2033,2034,2151,2319,2429,2432,2433,2434,2435,2436,2437,2441,2445,3299,3310,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3622,4913,6945,6946,6947,6948,6949,6950,6951,6952,6953,6990,6991],"2387": [159,1050,1994,2708,2709,2710,2711,2712,2713,2976,2977,3416,3417,3622,4358,4359],"865": [216,217,851,860,2053,2054,2055,2056,2131,2132,2422,2423],"2442": [220,1277,1621,1622,1623,1632,1635,1637,1859,1865,1873,1879,1885,1892,1994,2032,2033,2034,2432,2433,2434,2435,2436,2437,2445,2780,2789,3299],"2370": [321,692,1245,1517,1518,1572,1573,1574,1575,1661,1897,1899,1901,2068,2094,2095,2096,2106,2109,2263,2264,2270,2283,2284,2303,2327,2366,2367,2390,2428,2490,2493,2719,2722,2726,2735,2736,2738,2739,2740,2741,2765,2827,2894,2895,2901,2902,2903,2926,3019,3024,3031,3077,3078,3079,3081,3083,3084,3085,3349,3590,3591,3592,3605,3606,3715,3716,3853,3854,3855,3856,3857,3861,4112,4120,4284,4306,4398,4620,4621,6902,6903],"1950": [684,816,1285,1286,1656,1657,2405,2512,2527,3651],"3852": [779,780,1213,1214,1289,1847,1849,3339,3732,3738,3743,3790,3797,3800,4054,4113,4765,4766],"2381": [781,782,810,2023,2024,2632,2633,4365],"1108": [920,921],"1105": [1276,1502,1994,2032,2269,2319,2342,2343,2344,2348,2349,2350,2420,2421,2429,2441,3310,3321,3322,3323,3326,3327,3328],"2725": [2723,4161],"2727": [2729],"2728": [2730],"2820": [2858,2860]}
		
	"Frame-2":
		Parent   Child
		1087       4
		1087       5
		1087       6
		.....           ......
		1096       25
		1096       26
		1096       27
		1096       28
		......        ......
		1144      25
		1144      26
		1144      27
		.....         .....
		
}
"answer": {
	"desc": %s Pandas will not always have a way to magically parse a data structure directly (whether you are using just the constructor or one of its classmethods such as ). Here, you can just pass a modified version of the native Python structure directly: The expression combines a dict comprehension and a list comprehension. It looks like: 
	"code-snippets": [
		[[k, i] for k, v in data.items() for i in v]
		----------------------------------------------------------------------
		In [14]: [[k, i] for k, v in data.items() for i in v]                                                                                                                                                                                                        
		Out[14]:
		[['1108', 920],
		 ['1108', 921],
		 ['2381', 781],
		 ['2381', 782],
		 ['2381', 810],
		 ['2381', 2023],
		 ['2381', 2024],
		 ['2381', 2632],
		 ['2381', 2633],
		 ['2381', 4365],
		 ['2728', 2730]]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54518504
"link": https://stackoverflow.com/questions/54518504/check-if-group-contains-same-value-in-pandas
"question": {
	"title": Check if group contains same value in Pandas
	"desc": I am curious if there is a pre-built function in Pandas to check if all members of a group (factors in a column) contain the same value in another column. i.e. if my dataframe was similar to below it would return an empty list. However, if my dataframe appeared as such (notice the 1 in Col1): Then the output would be a list containing the object "B" since the group B has different values in Col1. 
}
"io": {
	"Frame-1": 
		Col1    Col2
		2        A
		2        A
		0        B
		0        B
		
	"Frame-2":
		Col1    Col2
		2        A
		2        A
		0        B
		1        B 
		
}
"answer": {
	"desc": %s Use groupby nunique and index of unique values > 1 Output: 
	"code-snippets": [
		a = df.groupby('Col2').Col1.nunique() > 1
		a[a].index.tolist()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54460092
"link": https://stackoverflow.com/questions/54460092/matching-dictionaries-with-columns-and-indices-in-dataframe-python
"question": {
	"title": Matching dictionaries with columns and indices in DataFrame | python
	"desc": I have a DataFrame with column names as on example and the indices from 0 to 1000. The dataframe is filled with zeros. Then, I have dictionary, e.g.: Dictionary name edited in order not to confuse anyone later. My goal is to: for every dict key match it with the column for every number in the list as dictionary value to match with the index if there is a match of index and column, then change the cell to 1 else: leave zero How would you do that? 
}
"io": {
	"Frame-1": 
		House 1 | House 2 | House 5 | House 8 | ...
		0
		1
		2
		3
		4...
		
	"Frame-2":
		dict_of_houses = {'House 1':[100,201,306,387,500,900],'House 2':[31,87,254,675,987],'House 5':[23,45,67,123,345,654,789,808,864,987,999],'House 8':[23,675,786,858,868,912,934]}
		
}
"answer": {
	"desc": %s You can use a loop: 
	"code-snippets": [
		for house, indices in dict_.items():
		    df.loc[indices, house] = 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 48749201
"link": https://stackoverflow.com/questions/48749201/how-to-sum-n-columns-in-python
"question": {
	"title": How to sum N columns in python?
	"desc": I've a pandas df and I'd like to sum N of the columns. The df might look like this: I'd like to get a df like this: The A variable is not an index, but a variable. 
}
"io": {
	"Frame-1": 
		A B C D ... X
		
		1 4 2 6     3
		2 3 1 2     2 
		3 1 1 2     4
		4 2 3 5 ... 1
		
	"Frame-2":
		A Z
		
		1 15
		2 8
		3 8
		4 11
		
}
"answer": {
	"desc": %s Use for new created by all columns without : Or extract column first by : If want select columns by positions use : 
	"code-snippets": [
		df = df.iloc[:, [0]].join(df.iloc[:, 1:].sum(axis=1).rename('Z'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54407275
"link": https://stackoverflow.com/questions/54407275/add-column-to-dataframe-in-a-loop
"question": {
	"title": Add column to DataFrame in a loop
	"desc": Let's say I have a very simple pandas dataframe, containing a single indexed column with "initial values". I want to read in a loop N other dataframes to fill a single "comparison" column, with matching indices. For instance, with my inital dataframe as and the following two dataframes to read in a loop I would like to produce the following result Using , or , I only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with . What's the most pandas-pythonic way of achieving this? Below an example from the proposed duplicate solution: Second edit: @W-B, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. It also requires turning off warnings, which might be dangerous... 
}
"io": {
	"Frame-1": 
		   Initial
		0        a
		1        b
		2        c
		3        d
		
	"Frame-2":
		    Initial Comparison
		0        a           e
		1        b           f
		2        c           g
		3        d           h
		
}
"answer": {
	"desc": %s Ok , since Op need more info Data input Solution 
	"code-snippets": [
		import functools
		df1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])
		df1['Compare']=np.nan
		df2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])
		df3 = pd.DataFrame(np.array(['g','h','i']), columns=['Compare'],index=[2,3,4])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54372352
"link": https://stackoverflow.com/questions/54372352/python-pandas-calculate-percentage-change-using-last-non-na-value
"question": {
	"title": python pandas - calculate percentage change using last non-na value
	"desc": I am pretty new to python (mostly I use R) and I would like to perform a simple calculation but keep getting errors and incorrect results. I would like to calculate the percentage change for a column in a pandas df using the latest non-na value. A toy example is below. I keep getting a weird result: I guess this has to do with the Nan values. How do I tell python to use the latest non-na value. The desired result is as follows: Since I don't know very much python at all, any suggestions would be welcome, even more convoluted ones. 
}
"io": {
	"Frame-1": 
		price_chg = [Nan, -0.2307, 0, 0, 0.4444, NaN] 
		
	"Frame-2":
		price_chg = [Nan, -0.2307, 0.4444, 0, 0, NaN]
		
}
"answer": {
	"desc": %s I believe what you're looking for is to employ backfill when calling the function. This results in: This page describes the options you have when calling , including the . You can learn more about the fill methods available in pandas here 
	"code-snippets": [
		df['price_chg'] = df.price.pct_change(periods = -1, fill_method='backfill')
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54333620
"link": https://stackoverflow.com/questions/54333620/select-columns-in-pandas-dataframe-that-have-an-integer-header
"question": {
	"title": Select columns in panda&#39;s dataframe that have an integer header
	"desc": I have a dataframe in pandas that looks like this: What I want to do is select specific columns from this data frame. But when I try the following code (the df_matrix being the dataframe displayed at the top) : It does not work and from what I can tell is because it is an integer. I tried to force it with str(100) but gave the same error as before: Does anyone know how to get around this? Thanks! EDIT 1: After trying to use it worked as expecte. Btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use: and the output will be: 
}
"io": {
	"Frame-1": 
		   100  200  300  400
		0    1    1    0    1
		1    1    1    1    0
		
	"Frame-2":
		   100  300
		0    1    0
		1    1    1
		
}
"answer": {
	"desc": %s Simply use below as in this case as your columns are . If you want your columns to be accessed as , Use: and then Output 
	"code-snippets": [
		df.columns = [str(x) for x in df.columns]
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54273695
"link": https://stackoverflow.com/questions/54273695/how-can-i-remove-the-nan-not-removing-the-data
"question": {
	"title": How can I remove the &#39;NaN&#39; not removing the data?
	"desc": I'm trying to remove the 'NaN'. In detail, there is data on one line and 'NaN'. My data looks like the one below. I want to eliminate the NAN between the data and make one data for every 18 lines. I tried option 'dropna()' (using 'how = 'all'' or 'thread = '10''). But these are not what I want. How can I remove NaN and merge data? Add This is the code that I using(python2). The is the data that have NaN. If you look at the data, there are data in the 0th line from 1 to 10, and data in the 1st line from 11th to 21st. That is, there are two lines of data. I want to wrap this in a single line without NaN. Like this result. I tried to re-index the row to time to using resampling. And I save the start and end index. And I save the index_time to use resampling time. The result of 'df_time_merge' is like this. enter image description here It's working!! But if I have data(starting with Nan) like this, the code didn't working. enter image description here If I run same code, the and . Where did I miss? 
}
"io": {
	"Frame-1": 
		     01   02   03   04   05   06     07     08   09    10 ...      12   13  \
		0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0
		1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 
		
		          14         15      16   17   18        19   20   21   
		0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  
		1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  
		
		
	"Frame-2":
		     01   02   03   04   05   06     07     08   09    10 ...      12   13  \
		0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0
		1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 
		
		          14         15      16   17   18        19   20   21   
		0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  
		1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  
		
		
}
"answer": {
	"desc": %s Does this do what you want? Create an array to identify which rows are NOT all nans Use tat array to compress test to rows that are not all nans Set nans to zero Add odd to even rows 
	"code-snippets": [
		def make_sample():
		    test=np.full((8,12), np.nan)
		    test[0,:6]=np.arange(6)
		    test[1,6:]=np.arange(6,18,2)
		    test[4:6,:]=2*test[:2,:]
		    return test
		
		test=make_sample()
		
		In [74]: test
		Out[74]: 
		array([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],
		       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],
		       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
		       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
		       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],
		       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.],
		       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
		       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])
		
		----------------------------------------------------------------------
		filt=1^np.isnan(test).all(axis=1)
		
		In [78]: filt
		Out[78]: array([1, 1, 0, 0, 1, 1, 0, 0])
		
		----------------------------------------------------------------------
		compress=np.compress(filt, test, axis=0)
		
		In [80]: compress
		Out[80]: 
		array([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],
		       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],
		       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],
		       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.]])
		
		----------------------------------------------------------------------
		compress[np.isnan(compress)]=0
		
		In [83]: compress
		Out[83]: 
		array([[ 0.,  1.,  2.,  3.,  4.,  5.,  0.,  0.,  0.,  0.,  0.,  0.],
		       [ 0.,  0.,  0.,  0.,  0.,  0.,  6.,  8., 10., 12., 14., 16.],
		       [ 0.,  2.,  4.,  6.,  8., 10.,  0.,  0.,  0.,  0.,  0.,  0.],
		       [ 0.,  0.,  0.,  0.,  0.,  0., 12., 16., 20., 24., 28., 32.]])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54020869
"link": https://stackoverflow.com/questions/54020869/how-to-delete-continuous-four-digits-from-a-column-value-in-pandas-dataframe
"question": {
	"title": How to delete continuous four digits from a column value in pandas dataframe
	"desc": I have a data frame like this: I want to remove where the digits occurring for exact four times The result will look like: What is the best way to do it using python 
}
"io": {
	"Frame-1": 
		col1         col2                col3
		 A        12134 tea2014           2
		 B        2013 coffee 1           1
		 C        green 2015 tea          4
		
	"Frame-2":
		 col1         col2                col3
		 A        12134 tea                 2
		 B         coffee 1                 1
		 C        green tea                 4
		
}
"answer": {
	"desc": %s You will need with a carefully applied regex pattern: Regex Breakdown The pattern will look for exactly 4 digits that are not preceeded by digits before or after (so strings of less/more than 4 digits are left alone). 
	"code-snippets": [
		(
		    ?<!   # negative lookbehind 
		    \d    # any single digit
		)
		\d{4}     # match exactly 4 digits
		(
		    ?!    # negative lookahead
		    \d
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54219106
"link": https://stackoverflow.com/questions/54219106/dropping-duplicate-rows-but-keeping-certain-values-pandas
"question": {
	"title": Dropping duplicate rows but keeping certain values Pandas
	"desc": I have 2 similar dataframes that I concatenated that have a lot of repeated values because they are basically the same data set but for different years. The problem is that one of the sets has some values missing whereas the other sometimes has these values. For example: I want to drop duplicates on the since some repetitions don't have years. However, I'm left with the data that has no and I'd like to keep the data with these values: How do I keep these values rather than the blanks? 
}
"io": {
	"Frame-1": 
		Name        Unit       Year      Level
		Nik         1          2000      12
		Nik         1                    12
		John        2          2001      11
		John        2          2001      11
		Stacy       1                    8
		Stacy       1          1999      8
		.
		.
		
	"Frame-2":
		Name        Unit       Year      Level
		Nik         1          2000      12
		John        2          2001      11
		Stacy       1          1999      8
		.
		.
		
}
"answer": {
	"desc": %s Use with default parameter , so should be omit, and then : Or: Another solution with for return first non missing value of per groups: 
	"code-snippets": [
		df = df.sort_values(subset + ['Year']).drop_duplicates(subset)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54105419
"link": https://stackoverflow.com/questions/54105419/add-numbers-with-duplicate-values-for-columns-in-pandas
"question": {
	"title": Add numbers with duplicate values for columns in pandas
	"desc": I have a data frame like this: I found that there is duplicate value and its pqr. I want to add 1,2,3 where pqr occurs. The final data frame I want to achieve is: How to do it in efficient way 
}
"io": {
	"Frame-1": 
		df:
		col1     col2
		 1        pqr
		 3        abc
		 2        pqr
		 4        xyz
		 1        pqr
		
	"Frame-2":
		df1
		col1      col2
		 1        pqr1
		 3        abc
		 2        pqr2
		 4        xyz
		 1        pqr3
		
}
"answer": {
	"desc": %s Use with for all dupe rows and add counter created by : Or: If need same only for values: 
	"code-snippets": [
		mask = df['col2'].duplicated(keep=False)
		df.loc[mask, 'col2'] += df.groupby('col2').cumcount().add(1).astype(str)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54059466
"link": https://stackoverflow.com/questions/54059466/report-difference-change-in-values-between-two-dataframes-of-identical-shape
"question": {
	"title": Report difference/change in values between two dataFrames of identical shape
	"desc": The context is I want to compare two df's and find the difference. Here's df and df2 with a small difference: Comparing them yields a 2D boolean df of the same shape: I tried to extract the elements corresponding to the True's, but other elements (that I don't want) still occurs as NaN How to extract only the elements corresponding to the True's and the indices (so I know where in the df): update: the above example has only one True. In a general situation with multiple True's, I think are two cases: df is small and one may want to see: df is large and one may want to see: @U9-Forward's answer works nicely for case 1, and when there's only one True. @coldspeed provided a comprehensive solution. Thanks! 
}
"io": {
	"Frame-1": 
		df[df != df2]
		Out[29]: 
		    a    b
		0 NaN  NaN
		1 NaN  1.0
		2 NaN  NaN
		
	"Frame-2":
		df[df != df2] # somehow?
		Out[30]: 
		    b
		1 1.0
		
}
"answer": {
	"desc": %s Mask on the values: How should this case be handled? Which is the required output? If you just the values in each column of that differ, something simple like and will do. If you want the indices and columns, use : Or, use , to do this with numpy - True values are nonzero, the indices of these are returned. 
	"code-snippets": [
		df.values[df != df2]
		# array([1])
		
		----------------------------------------------------------------------
		df[df != df2].agg(lambda x: x.dropna().tolist())
		
		a    [0.0]
		b    [1.0]
		dtype: object
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53673872
"link": https://stackoverflow.com/questions/53673872/pandas-dataframe-to-sparse-representation
"question": {
	"title": Pandas dataframe to sparse representation
	"desc": I have a dense pandas dataframe. I would like to get a sparse dataframe out of it where each value of the original dataframe would be the column of a 1 in the resulting sparse dataframe. Example: Original df: Sparse df: I do not care if in case of collision it is a 1 or the number of collision I will then pass this df to sklearn.linear_model.LogisticRegression fit function (I am not sure which kind of sparse matrix would be accepted here) What would be the appropriate approach ? I can create it by hand (iterating over the row) but the dataframe is quite big so I am trying to find an efficient way of doing it. Thanks 
}
"io": {
	"Frame-1": 
		    a b
		0   5 3
		1   2 6
		
	"Frame-2":
		(0,3): 1
		(0,5): 1
		(1,2): 1
		(1,6): 1
		
}
"answer": {
	"desc": %s A much faster solution than the one proposed by @Dark is to use the csr_matrix constructor but the ones will sum up in case of redundancy which is ok for my case: 
	"code-snippets": [
		nrow = len(df.index)
		ncol = len(df.columns)
		indices = df.values.flatten()
		data = np.full_like(indices, 1)
		nelement = len(indices)
		indptr = range(0, nelement+ncol, ncol)
		result = csr_matrix((data, indices, indptr))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54037912
"link": https://stackoverflow.com/questions/54037912/merge-order-with-items-in-columns
"question": {
	"title": Merge order with items in columns
	"desc": I have a dataset with all the order, customer and orderitem information. I wandt to expand my orderitems in new columns, but without losing the information about the customer And the result should be somehow: I tried 
}
"io": {
	"Frame-1": 
		CustomerId    OrderId    Item
		1    1    CD
		1    1    DVD
		2    2    CD
		
	"Frame-2":
		CustomerId    OrderId    CD    DVD
		1    1    1    1
		2    2    1    0
		
}
"answer": {
	"desc": %s Simpler is ; Or, if performance is important. If you want to use dummies, is another option: If you need the indicator, 
	"code-snippets": [
		(df.set_index(['CustomerId', 'OrderId'])
		   .Item.str.get_dummies()
		   .max(level=[0, 1])
		   .reset_index())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 54005974
"link": https://stackoverflow.com/questions/54005974/change-values-in-columns-of-dataframe-depending-on-values-of-other-columns-valu
"question": {
	"title": Change values in columns of dataframe depending on values of other columns (values come from lists)
	"desc": I have a Data Frame in python, for example this one: The code for the creation of the dataframe: Let the list1 be ['A','C','E'] and list2 be ['B','D','F']. What I want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). I have tried which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. Could someone help please? This is my expected output: 
}
"io": {
	"Frame-1": 
		  col1 col2 col3 col4
		0    A    C    B    D
		1    C    E    E    A
		2    E    A    E    A
		3    A    D    D    D
		4    B    B    B    B
		5    D    D    D    D
		6    F    F    A    F
		7    E    E    E    E
		8    B    B    B    B
		
	"Frame-2":
		  col1 col2 col3 col4
		0    A         B    D
		1    C    E    E    A
		2    E    A    E    A
		3    A         D    D
		4    B    B    B    B
		5    D    D    D    D
		6    F    F    A    F
		7    E    E    E    E
		8    B    B    B    B
		
}
"answer": {
	"desc": %s  is a method of , so use it with your dataframe, not with a series. In addition, you can test criteria on multiple series via : Result: 
	"code-snippets": [
		m1 = df['col1'].isin(list1)
		m2 = df[['col2', 'col3', 'col4']].isin(list2).any(1)
		
		df.loc[m1 & m2, 'col2'] = ''
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53976903
"link": https://stackoverflow.com/questions/53976903/find-a-shared-time-overlap-in-time-columns-in-python
"question": {
	"title": Find a shared time (overlap) in time columns in python
	"desc": I'm trying to find a shared time where two datetime columns representing a start and an end time overlap with other records. for example if we have these two columns: I want to check the overlap between them, the output would be: is there an efficient way to achieve it? 
}
"io": {
	"Frame-1": 
		Start                    End 
		2016-08-22 20:20:00      2016-08-22 20:30:00   
		2016-08-22 20:55:00      2016-08-22 21:53:00   
		2016-08-22 21:38:00      2016-08-22 21:58:00
		
	"Frame-2":
		 Start                    End                   Overlap
		2016-08-22 20:20:00      2016-08-22 20:30:00    NaN
		2016-08-22 20:55:00      2016-08-22 21:53:00   2016-08-22 21:38:00
		2016-08-22 21:38:00      2016-08-22 21:58:00   2016-08-22 21:38:00
		
}
"answer": {
	"desc": %s Here's a possible approach. You could define the following function: What it does is look for other rows with time overlap and assigns the time in from the overlapping row (which will not be the current row, as your sample output suggests). If you apply this along you get: If what you want is to have the index of the row with time overlapping you can instead use: Which will in this case give: 
	"code-snippets": [
		def common_row(x):
		    rows = df.loc[df.index != x.name,:]
		    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() > 0 for 
		             y in rows.itertuples()]
		    shared = rows.index[s].values
		    if shared.size > 0:
		        return df.loc[shared[0], 'Start']
		
		----------------------------------------------------------------------
		def common_row(x):
		    rows = df.loc[df.index != x.name,:]
		    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() > 0 for 
		             y in rows.itertuples()]
		    shared = rows.index[s].values
		    if shared.size > 0:
		        return int(shared[0])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53973711
"link": https://stackoverflow.com/questions/53973711/how-to-merge-rows-in-a-dataframe-based-on-a-column-value
"question": {
	"title": How to merge rows in a dataframe based on a column value?
	"desc": I have a data-set that is in the shape of this, where each row represents a in a specific match that is specified by the . The thing I want to do is create a function that takes the rows with the same and joins them. As you can see in data example below, the two rows represents one game that is split up into a home team (row_1 ) and an away team (row_2). I want these two rows to sit on one row only. How do I get this result? EDIT: I created too much confusion, posting my code so you can get a better grasp of the problem I want to solve. 
}
"io": {
	"Frame-1": 
		  gameID          Won/Lost   Home   Away  metric2 metric3 metric4   team1 team2 team3 team4
		2017020001         1          1      0      10      10      10      1     0     0      0
		2017020001         0          0      1      10      10      10      0     1     0      0
		
	"Frame-2":
		Won/Lost  h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4
		1            10       10         10        10         10        10      1       0        0      0         0      1        0      0
		
}
"answer": {
	"desc": %s This is under the assumption that you have exactly two rows per and that you want to group by that ID. (It also assumes that I understand the question.) Improved solution Given a dataframe such as you can use (and some data munging) like this: (I kept the prefix for because it indicates that it's the statistic for the home team. Also, if anybody knows how to add the prefixes more elegantly without having to re-rename the please leave a comment.) Original Attempt You can apply the following function after grouping ... like this: 
	"code-snippets": [
		def munge(group): 
		     is_home = group.Home == 1 
		     wonlost = group.loc[is_home, 'Won/Lost'].reset_index(drop=True) 
		     group = group.loc[:, 'metric2':] 
		     home = group[is_home].add_prefix('h_').reset_index(drop=True) 
		     away = group[~is_home].add_prefix('a_').reset_index(drop=True) 
		     return pd.concat([wonlost, home, away], axis=1) 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53961914
"link": https://stackoverflow.com/questions/53961914/create-a-dataframe-from-arrays-python
"question": {
	"title": Create a dataframe from arrays python
	"desc": I'm try to construct a dataframe (I'm using Pandas library) from some arrays and one matrix. in particular, if I have two array like this: And one matrix like this : Can i create a dataset like this? Maybe is a stupid question, but i m very new with Python and Pandas. I seen this : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html but specify only 'colums'. I should read the matrix row for row and paste in my dataset, but I m think that exist a more easy solution with Pandas. 
}
"io": {
	"Frame-1": 
		1 2 2
		3 3 3
		4 4 4
		
	"Frame-2":
		  A B C
		D 1 2 2
		E 3 3 3
		F 4 4 4
		
}
"answer": {
	"desc": %s This should do the trick for you. 
	"code-snippets": [
		columns = ["A", "B", "C"]
		rows = ["D", "E", "F"]
		data = np.array([[1, 2, 2], [3, 3, 3],[4, 4, 4]])
		df = pd.DataFrame(data=data, index=rows, columns=columns)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 47402346
"link": https://stackoverflow.com/questions/47402346/ranking-groups-based-on-size
"question": {
	"title": Ranking groups based on size
	"desc": Sample Data: What I would like to do is replace the largest cluster id with and the second largest with and so on and so forth. Output would be as shown below. I'm not quite sure where to start with this. Any help would be much appreciated. 
}
"io": {
	"Frame-1": 
		id cluster
		1 3
		2 3
		3 3
		4 3
		5 1
		6 1
		7 2
		8 2
		9 2
		10 4
		11 4
		12 5
		13 6
		
	"Frame-2":
		id cluster
		1 0
		2 0
		3 0
		4 0
		5 2
		6 2
		7 1
		8 1
		9 1
		10 3
		11 3
		12 4
		13 5
		
}
"answer": {
	"desc": %s The objective is to relabel groups defined in the column by the corresponding rank of that group's total value count within the column. We'll break this down into several steps: Integer factorization. Find an integer representation where each unique value in the column gets its own integer. We'll start with zero. We then need the counts of each of these unique values. We need to rank the unique values by their counts. We assign the ranks back to the positions of the original column. Approach 1 Using Numpy's + TL;DR Turns out, performs the task of integer factorization and counting values in one go. In the process, we get unique values as well, but we don't really need those. Also, the integer factorization isn't obvious. That's because per the function, the return value we're looking for is called the . It's called the inverse because it was intended to act as a way to get back the original array given the array of unique values. So if we let You'll see looks like: And if we did we get back the original But we are going to use it as integer factorization. Next, we need the counts I'm going to propose the use of but it's confusing. So I'll try to show it: What does in general is to place the top spot (position 0), the position to draw from in the originating array. What this allows us to do is to slice this result with our integer factorization to arrive at a remapping of the ranks. We can then drop it into the column with Approach 2 I'm going to leverage the same concepts. However, I'll use Pandas to get integer factorization with to count values. The reason to use this approach is because Numpy's actually sorts the values in the midst of factorizing and counting. does not. For larger data sets, big oh is our friend as this remains while the Numpy approach is . 
	"code-snippets": [
		u, i, c = np.unique(
		    df.cluster.values,
		    return_inverse=True,
		    return_counts=True
		)
		(-c).argsort()[i]
		
		----------------------------------------------------------------------
		u, i, c = np.unique(
		    df.cluster.values,
		    return_inverse=True,
		    return_couns=True
		)
		
		----------------------------------------------------------------------
		array([2, 2, 2, 2, 0, 0, 1, 1, 1, 3, 3, 4, 5])
		
		----------------------------------------------------------------------
		array([3, 3, 3, 3, 1, 1, 2, 2, 2, 4, 4, 5, 6])
		
		----------------------------------------------------------------------
		array([2, 3, 4, 2, 1, 1])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53953121
"link": https://stackoverflow.com/questions/53953121/dataframe-summary-math-based-on-condition-from-another-dataframe
"question": {
	"title": Dataframe summary math based on condition from another dataframe?
	"desc": I have what amounts to 3D data but can't install the Pandas recommended xarray package. df_values df_condition I know I can get the average of all values in like this. Question...  What is the simplest way to find the where ? 
}
"io": {
	"Frame-1": 
		   | a    b    c
		-----------------
		0  | 5    9    2
		1  | 6    9    5
		2  | 1    6    8  
		
	"Frame-2":
		   | a    b    c
		-----------------
		0  | y    y    y
		1  | y    n    y
		2  | n    n    y
		
}
"answer": {
	"desc": %s Assuming you wish to find the mean of all values where : Using NumPy is substantially cheaper than Pandas or : 
	"code-snippets": [
		# Pandas 0.23.0, NumPy 1.14.3
		n = 10**5
		df_values = pd.concat([df_values]*n, ignore_index=True)
		df_condition = pd.concat([df_condition]*n, ignore_index=True)
		
		%timeit np.nanmean(df_values.values[df_condition.eq('y')])       # 32 ms
		%timeit np.nanmean(df_values.where(df_condition == 'y').values)  # 88 ms
		%timeit df_values[df_condition.eq('y')].stack().mean()           # 107 ms
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53939072
"link": https://stackoverflow.com/questions/53939072/column-dupe-renaming-in-pandas
"question": {
	"title": Column dupe renaming in pandas
	"desc": I have the following csv file of data: Pandas currently renames this to: Is there a way to customize how this is renamed? For example, I would prefer: 
}
"io": {
	"Frame-1": 
		       id number id.1
		0  132605      1    1
		1  132750      2    1
		
	"Frame-2":
		           id number id2
		0  132605      1    1
		1  132750      2    1
		
}
"answer": {
	"desc": %s : use period delimiter Assuming duplicate column labels are the only instances where a column name contains a period (), you can use a custom function with : : robust solution A robust solution is possible with the module from the standard library: 
	"code-snippets": [
		from collections import defaultdict
		import csv
		
		# replace StringIO(file) with open('file.csv', 'r')
		with StringIO(file) as fin:
		    headers = next(csv.reader(fin))
		
		def rename_duplicates(original_cols):
		    count = defaultdict(int)
		    for x in original_cols:
		        count[x] += 1
		        yield f'{x}{count[x]}' if count[x] > 1 else x
		
		df.columns = rename_duplicates(headers)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53932155
"link": https://stackoverflow.com/questions/53932155/how-to-append-ndarray-values-into-dataframe-rows-of-particular-columns
"question": {
	"title": How to append ndarray values into dataframe rows of particular columns?
	"desc": I have a function that returns an like this Now, I have a data frame with columns A,B,C,...,Z ; but the array we are getting has only 20 values. Hence I want to find a way such that for every array I get as output, I am able to store it in like this (A,B,W,X,Y,Z are to be left blank): 
}
"io": {
	"Frame-1": 
		[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0] 
		
	"Frame-2":
		__| A | B | C | D | E | F | ...
		0 |nan|nan| 0 | 1 | 0 | 0 | ...
		1 |nan|nan| 1 | 1 | 0 | 1 | ...
		.
		. 
		.
		
}
"answer": {
	"desc": %s I couldn't get what I wanted through the suggestions posted here. However, I did figure it out myself. I'm sharing it here for the community's reference. 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = pd.DataFrame(columns=[chr(i) for i in range(ord('A'),ord('Z')+1)])
		
		print(df)
		
		----------------------------------------------------------------------
		Empty DataFrame
		Columns: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z]
		Index: []
		
		[0 rows x 26 columns]
		
		----------------------------------------------------------------------
		list1 = [i for i in range(101,121)]
		arr1d = np.array(list1)
		
		arr1d
		
		----------------------------------------------------------------------
		# Create alphabet list of uppercase letters
		alphabet = []
		for letter in range(ord('C'),ord('W')):
		    alphabet.append(chr(letter))
		alphabet
		
		----------------------------------------------------------------------
		['C',
		 'D',
		 'E',
		 'F',
		 'G',
		 'H',
		 'I',
		 'J',
		 'K',
		 'L',
		 'M',
		 'N',
		 'O',
		 'P',
		 'Q',
		 'R',
		 'S',
		 'T',
		 'U',
		 'V']
		
		----------------------------------------------------------------------
		df = df.append(pd.Series(arr1d, index=alphabet), ignore_index=True)
		#This line of code can be used for every new value of arr1d 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53885404
"link": https://stackoverflow.com/questions/53885404/python-setting-values-without-loop
"question": {
	"title": Python Setting Values Without Loop
	"desc": I have a time series dataframe where there is 1 or 0 in it (true/false). I wrote a function that loops through all rows with values 1 in them. Given user defined integer parameter called , I will set values 1 to n rows forward from the initial row. For example, in the dataframe below I will be loop to row . If , then I will set both and to 1 too.: The resulting will then is The problem I have is this is being run 10s of thousands of times and my current solution where I am looping over rows where there are ones and subsetting is way too slow. I was wondering if there are any solutions to the above problem that is really fast. Here is my (slow) solution, is the initial signal dataframe: 
}
"io": {
	"Frame-1": 
		2016-08-03    0
		2016-08-04    0
		2016-08-05    1
		2016-08-08    0
		2016-08-09    0
		2016-08-10    0
		
	"Frame-2":
		2016-08-03    0
		2016-08-04    0
		2016-08-05    1
		2016-08-08    1
		2016-08-09    1
		2016-08-10    0
		
}
"answer": {
	"desc": %s Completely changed answer, because working differently with consecutive values: Explanation: Solution remove each consecutive first by with chained boolean mask by comparing with (not equal ) with to s, forward filling them by with parameter and last replace back: Timings and comparing outputs: Timings: 
	"code-snippets": [
		n_hold = 2
		s = x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')
		
		----------------------------------------------------------------------
		np.random.seed(123)
		x = pd.Series(np.random.choice([0,1], p=(.8,.2), size=1000))
		x1 = x.copy()
		#print (x)
		
		
		def orig(x):
		    n_hold = 2
		    entry_sig_diff = x.diff()
		    entry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index
		    final_signal = x * 0
		    for i in range(0, len(entry_sig_dt)):
		        row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])
		
		        if (row_idx + n_hold) >= len(x):
		            break
		
		        final_signal[row_idx:(row_idx + n_hold + 1)] = 1
		    return final_signal
		
		#print (orig(x))
		
		----------------------------------------------------------------------
		%timeit (orig(x))
		24.8 ms  653 s per loop (mean  std. dev. of 7 runs, 10 loops each)
		
		%timeit x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')
		1.36 ms  12.7 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51645934
"link": https://stackoverflow.com/questions/51645934/using-iterrows-with-series-nlargest-to-get-the-highest-number-in-a-row-in-a
"question": {
	"title": Using .iterrows() with series.nlargest() to get the highest number in a row in a Dataframe
	"desc": I am trying to create a function that uses and . I want to iterate over each row and find the largest number and then mark it as a . This is the data frame: Here is the output I wish to have: This is the function I wish to use here: I get the following error: AttributeError: 'tuple' object has no attribute 'nlargest' Help would be appreciated on how to re-write my function in a neater way and to actually work! Thanks in advance 
}
"io": {
	"Frame-1": 
		A   B    C
		9   6    5
		3   7    2
		
	"Frame-2":
		A    B   C
		1    0   0
		0    1   0
		
}
"answer": {
	"desc": %s Add variable, because return indices with for each row: General solution with for positions in descending order, then compare and convert boolean array to integers: EDIT: Solution with is possible, but not recommended, because slow: 
	"code-snippets": [
		for i, row in df.iterrows():
		    top_numbers = row.nlargest(top_n).sum()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53820131
"link": https://stackoverflow.com/questions/53820131/how-to-create-a-dataframe-from-numpy-arrays
"question": {
	"title": How to create a dataframe from numpy arrays?
	"desc": I am trying to create a matrix / DataFrame with the numbers stored in 2 variables and I would like them to look like this: I would like it to be in a so I can work with it with the library. Thanks in advance 
}
"io": {
	"Frame-1": 
		x = np.linspace(0,50)
		y = np.exp(x)
		
	"Frame-2":
		x    |     y
		___________________
		0    |     1.0...
		1    |     2.77...             
		2    |     7.6...                      
		...  |     ...             
		50   |     5.18e+21...    
		
}
"answer": {
	"desc": %s With : You can issue to get the x and y values and then build your dataframe with In this case, you can also use the x-values as the index of a series without losing any information. Without : Consider if you truly need a dataframe or series. You could just leave it at and then index into with the integers , , , ... to get the values of , , , ... 
	"code-snippets": [
		>>> xs = np.arange(51)                                                                                                 
		>>> ys = np.exp(xs) 
		
		----------------------------------------------------------------------
		>>> xs = np.arange(51)                                                                                                 
		>>> ys = np.exp(xs)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53733422
"link": https://stackoverflow.com/questions/53733422/replace-nan-value-by-last-valid-value-for-only-one-column-in-a-dataframe-with
"question": {
	"title": Replace &quot;NaN&quot; value by last valid value for only one column in a dataframe with column multi-index (df.fillna)
	"desc": I'm working with Python 3.6.5. Here is a little script to generate a multi index dataframe with some "NaN" value. I get this dataframe And I would like to replace the "NaN" value with the last valid value, BUT ONLY FOR ONE COLUMN. For example, I would like to get this (for column named 'X','b') I tried this : But I get this error "A value is trying to be set on a copy of a slice from a DataFrame" I can not find any solution for a dataframe with multi-index of column. I found this link that gives me no hope. (https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.MultiIndex.fillna.html) Does anyone have an idea to help me? 
}
"io": {
	"Frame-1": 
		print(df)
		       X           Y      
		       a     b     a     b
		10  17.0  17.0   NaN   NaN
		20  15.0  11.0  20.0  28.0
		25   NaN   NaN  23.0  24.0
		30  12.0  16.0   NaN   NaN
		35  10.0  10.0   NaN   NaN
		40  15.0  14.0  25.0  28.0
		50   NaN   NaN  22.0  22.0
		80   NaN   NaN  23.0  21.0
		
	"Frame-2":
		print(df)
		       X           Y      
		       a     b     a     b
		10  17.0  17.0   NaN   NaN
		20  15.0  11.0  20.0  28.0
		25   NaN  11.0  23.0  24.0
		30  12.0  16.0   NaN   NaN
		35  10.0  10.0   NaN   NaN
		40  15.0  14.0  25.0  28.0
		50   NaN  14.0  22.0  22.0
		80   NaN  14.0  23.0  21.0
		
}
"answer": {
	"desc": %s After some digging, I found that there's a more appropriate way of referencing columns that we want to edit specifically. Check How to deal with SettingWithCopyWarning in Pandas? out for more info. Another resource: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy. It is recommended that we use to alter columns. Using the line below removed any errors. However, here I am using the max of the column to replace every with. I'm uncertain on what is meant by the last valid value. 
	"code-snippets": [
		df.loc[df['X']['b'].isnull(), ('X', 'b')] = df['X']['b'].ffill()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53769189
"link": https://stackoverflow.com/questions/53769189/rearrange-rows-of-dataframe-alternatively
"question": {
	"title": Rearrange rows of Dataframe alternatively
	"desc": I have a dataframe looks like this: and I want to make it look like this: My own way to do it seems to take quite a few lines, aka not pythonic. My code: Is there any more pythonic way to accomplish the same result? Thank you in advance. EDIT: I'd like a more general method to deal with dataframe like this 
}
"io": {
	"Frame-1": 
		   col1           col2
		0     1  random string
		1    -1  random string
		2     2  random string
		3    -2  random string
		4     3  random string
		5    -3  random string
		6     4  random string
		7    -4  random string
		8     5  random string
		9    -5  random string
		10    6  random string
		11   -6  random string
		12    7  random string
		13   -7  random string
		14    8  random string
		15   -8  random string
		16    9  random string
		17   -9  random string
		18   10  random string
		19  -10  random string
		
	"Frame-2":
		   col1           col2
		0     1  random string
		1     2  random string
		2     3  random string
		3     4  random string
		4     5  random string
		5    1x  random string
		6    2x  random string
		7    3x  random string
		8    4x  random string
		9    5x  random string
		10   1y  random string
		11   2y  random string
		12   3y  random string
		13   4y  random string
		14   5y  random string
		
}
"answer": {
	"desc": %s If the size of the subgroups is known, let's call it , and your is chunked with each group following the other, we just need some math: Output: 
	"code-snippets": [
		n=5
		
		df.index = df.index%n + (df.index//n)/(len(df)/n)
		df = df.sort_index().reset_index(drop=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53687204
"link": https://stackoverflow.com/questions/53687204/check-one-on-one-relationship-between-two-columns
"question": {
	"title": Check one-on-one relationship between two columns
	"desc": I have two columns A and B in a pandas dataframe, where values are repeated multiple times. For a unique value in A, B is expected to have "another" unique value too. And each unique value of A has a corresponding unique value in B (See example below in the form of two lists). But since each value in each column is repeated multiple times, I would like to check if any one-to-one relationship exists between two columns or not. Is there any inbuilt function in pandas to check that? If not, is there an efficient way of achieving that task? Example: Here, for each 1 in A, the corresponding value in B is always 5, and nothing else. Similarly, for 2-->10, and for 3-->12. Hence, each number in A has only one/unique corresponding number in B (and no other number). I have called this one-on-one relationship. Now I want to check if such relationship exists between two columns in pandas dataframe or not. An example where this relationship is not satisfied: Here, 1 in A doesn't have a unique corresponding value in B. It has two corresponding values - 5 and 7. Hence, the relationship is not satisfied. 
}
"io": {
	"Frame-1": 
		A = [1, 3, 3, 2, 1, 2, 1, 1]
		B = [5, 12, 12, 10, 5, 10, 5, 5]
		
	"Frame-2":
		A = [1, 3, 3, 2, 1, 2, 1, 1]
		B = [5, 12, 12, 10, 5, 10, 7, 5]
		
}
"answer": {
	"desc": %s Consider you have some dataframe: has method, which returns object. This is the interface to group some rows by equal column value, for example. On grouped rows you could perform an aggregation. Lets find min and max value in every group. Now we just should check that and are equal in every group, so every group consists of equal fields: If this check is OK, then for every you have unique . Now you should check the same criteria for and columns swapped. 
	"code-snippets": [
		 gb = d.groupby('A')
		 grouped_b_column = gb['B']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53654729
"link": https://stackoverflow.com/questions/53654729/pandas-dataframe-remove-secondary-upcoming-same-value
"question": {
	"title": Pandas dataframe: Remove secondary upcoming same value
	"desc": I have a dataframe: On I want to keep only the first from the top and replace every below the first one with a , such that the output is: Thank you very much. 
}
"io": {
	"Frame-1": 
		col1  col2
		 a     0
		 b     1
		 c     1
		 d     0
		 c     1
		 d     0
		
	"Frame-2":
		col1  col2
		 a     0
		 b     1
		 c     0
		 d     0
		 c     0
		 d     0
		
}
"answer": {
	"desc": %s You can find the index of the first and set others to : For better performance, see Efficiently return the index of the first value satisfying condition in array. 
	"code-snippets": [
		mask = df['col2'].eq(1)
		df.loc[mask & (df.index != mask.idxmax()), 'col2'] = 0
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 48831802
"link": https://stackoverflow.com/questions/48831802/outputting-pandas-series-to-txt-file
"question": {
	"title": Outputting pandas series to txt file
	"desc": I have a pandas series object that look like this: What is the best way to go about outputting this to a txt file (e.g. output.txt) such that the format look like this? The values on the far left are the userId's and the other values are the movieId's. Here is the code that generated the above: I tried implementing @emmet02 solution but got this error, I do not understand why I got it though: Any advice is appreciated, please let me know if you need any more information or clarification. 
}
"io": {
	"Frame-1": 
		userId
		1          3072 1196 838 2278 1259
		2               648 475 1 151 1035
		3               457 150 300 21 339
		4          1035 7153 953 4993 2571
		5           260 671 1210 2628 7153
		6          4993 1210 2291 589 1196
		7               150 457 111 246 25
		8       1221 8132 30749 44191 1721
		9           296 377 2858 3578 3256
		10          2762 377 2858 1617 858
		11           527 593 2396 318 1258
		12        3578 2683 2762 2571 2580
		13        7153 150 5952 35836 2028
		14        1197 2580 2712 2762 1968
		15        1245 1090 1080 2529 1261
		16         296 2324 4993 7153 1203
		17       1208 1234 6796 55820 1060
		18            1377 1 1073 1356 592
		19           778 1173 272 3022 909
		20              329 534 377 73 272
		21            608 904 903 1204 111
		22       1221 1136 1258 4973 48516
		23        1214 1200 1148 2761 2791
		24             593 318 162 480 733
		25               314 969 25 85 766
		26        293 253 4878 46578 64614
		27          1193 2716 24 2959 2841
		28         318 260 58559 8961 4226
		29            318 260 1196 2959 50
		30        1077 1136 1230 1203 3481
		
		642            123 593 750 1212 50
		643         750 671 1663 2427 5618
		644            780 3114 1584 11 62
		645         912 2858 1617 1035 903
		646           608 527 21 2710 1704
		647         1196 720 5060 2599 594
		648         46578 50 745 1223 5995
		649            318 300 110 529 246
		650            733 110 151 318 364
		651         1240 1210 541 589 1247
		652      4993 296 95510 122900 736
		653            858 1225 1961 25 36
		654        333 1221 3039 1610 4011
		655           318 47 6377 527 2028
		656          527 1193 1073 1265 73
		657             527 349 454 357 97
		658            457 590 480 589 329
		659              474 508 1 288 477
		660         904 1197 1247 858 1221
		661           780 1527 3 1376 5481
		662             110 590 50 593 733
		663          2028 919 527 2791 110
		664    1201 64839 1228 122886 1203
		665        1197 858 7153 1221 6539
		666            318 300 161 500 337
		667            527 260 318 593 223
		668            161 527 151 110 300
		669          50 2858 4993 318 2628
		670          296 5952 508 272 1196
		671         1210 1200 7153 593 110
		
	"Frame-2":
		User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5
		User-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5
		
}
"answer": {
	"desc": %s I would suggest turning your pd.Series into a pd.DataFrame first. So long as the Series has the same number of values (for every entry!), separated by a space, this will return a dataframe in this format Next I would name the columns appropriately Finally, the dataframe is indexed by customer id (I am supposing this based upon your series index). We want to move that into the dataframe, and then reorganise the columns. This now leaves you with a dataframe like this which I would recommend you write to disk as a csv using If however you want to write it as a text file, with only spaces separating, you can use the same function but pass the separator. I don't think that the Series object is the correct choice of data structure for the problem you want to solve. Treating numerical data as numerical data (and in a DataFrame) is far easier than maintaining 'space delimited string' conversions imo. 
	"code-snippets": [
		df = pd.DataFrame.from_items(zip(series.index, series.str.split(' '))).T
		
		----------------------------------------------------------------------
		df.columns = ['movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']
		
		----------------------------------------------------------------------
		df['customer_id'] = df.index
		df = df[['customer_id', 'movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53611917
"link": https://stackoverflow.com/questions/53611917/swapping-of-elements-in-a-pandas-dataframe
"question": {
	"title": Swapping of elements in a PANDAS dataframe
	"desc": Given below is a table : This was originally an excel sheet which has been converted into a dataframe. I wish to swap some of the elements such that the A number column has only 9999574081. Therefore the output should look like : This is the code I have used : However, I am not getting the desired result. Please help me out. Thanks:) 
}
"io": {
	"Frame-1": 
		    A NUMBER    B NUMBER
		    7042967611  9999574081
		    12320       9999574081
		    9999574081  9810256463
		    9999574081  9716551924
		    9716551924  9999574081  
		    9999574081  8130945859
		
	"Frame-2":
		    A NUMBER    B NUMBER
		    9999574081  7042967611  
		    9999574081  12320       
		    9999574081  9810256463
		    9999574081  9716551924
		    9999574081  9716551924  
		    9999574081  8130945859
		
}
"answer": {
	"desc": %s Use for swap only rows matched boolean mask, is necessary for avoid align index values: Another solution with : 
	"code-snippets": [
		m = df['A NUMBER'] != 9999574081
		
		df.loc[m, ['A NUMBER','B NUMBER']] = df.loc[m, ['B NUMBER','A NUMBER']].values
		
		----------------------------------------------------------------------
		df['B NUMBER'] = np.where(df['A NUMBER'] != 9999574081, df['A NUMBER'], df['B NUMBER'])
		df['A NUMBER'] = 9999574081
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53602854
"link": https://stackoverflow.com/questions/53602854/how-to-identify-string-repetition-throughout-rows-of-a-column-in-a-pandas-datafr
"question": {
	"title": How to identify string repetition throughout rows of a column in a Pandas DataFrame?
	"desc": I'm trying to think of a way to best handle this. If I have a data frame like this: How would I go about setting up a search and find to locate and identify repetition in the middle or on edges or complete strings? Sorry the formatting looks bad Basically I have the module, line item, and formula columns filled in, but I need to figure out some sort of search function that I can apply to each of the last 3 columns. I'm not sure where to start with this. I want to match any repetition that occurs between 3 or more words, including if for example a formula was and that occurred 4 times in the Formula column, I'd want to give a yes to the boolean column "repetition" return on the "Where repeated" column and a list of every module/line item combination where it occurred on the last column. I'm sure I can tweak it more to fit my needs once I get started. 
}
"io": {
	"Frame-1": 
		1 + 2 + 3 + 4
	"Frame-2":
		1 + 2 + 3 + 4
}
"answer": {
	"desc": %s This one was a bit messy, is surely some more straight forward way to do some of the steps, but it worked for your data. Step 1: I just reset_index() (assuming index uses row numbers) to get row numbers into a column. I then wrote a for loop which aim was to check for each given value, if that value is at any place in the given column (using the function, and if so, where. And then store that information in a dictionary. Note that here I used to split the various values you search by as that looked to be a valid separator in your dataset, but you can adjust this accordingly We can now unpack the dictionary into list, where we had a match: now contains one row per match, however, we want to have one row per original row in the df, so we combine all matches for each main row into one lastly we can now get the result by merging the result with our original dataframe and lastly we can add the column like this: 
	"code-snippets": [
		#the dictionary will have a key containing row number and the value we searched for
		#the value will contain the module and line item values
		result = {}
		#create a rownumber variable so we know where in the dataset we are
		rownumber = -1
		#now we just iterate over every row of the Formula series
		for row in df['Formula']:
		    rownumber +=1
		    #and also every relevant value within that cell
		    for value in row.split('+'):
		        #we clean the value from trailing/preceding whitespace
		        value = value.strip()
		        #and then we return our key and value and update our dictionary
		        key = 'row:|:'+str(rownumber)+':|:'+value
		        value = (df.loc[((df.Formula.str.contains(value,regex=False))) & (df.index!=rownumber),['Module','Line Item']])
		        result.update({key:value})
		
		----------------------------------------------------------------------
		where_raw = []
		what_raw = []
		rows_raw = []
		for key,value in zip(result.keys(),result.values()):
		    if 'Empty' in str(value):
		        continue
		    else:
		        where_raw.append(list(value['Module']+' '+value['Line Item']))
		        what_raw.append(key.split(':|:')[2])
		        rows_raw.append(int(key.split(':|:')[1]))
		
		tempdf = pd.DataFrame({'row':rows_raw,'where':where_raw,'what':what_raw})
		
		----------------------------------------------------------------------
		where = []
		what = []
		rows = []        
		
		for row in tempdf.row.unique():
		    where.append(list(tempdf.loc[tempdf.row==row,'where']))
		    what.append(list(tempdf.loc[tempdf.row==row,'what']))
		    rows.append(row)
		result = df.merge(pd.DataFrame({'index':rows,'where':where,'what':what}))
		
		----------------------------------------------------------------------
		 print(result)
		 Module     Line Item   Formula                                         what                                                 where
		 Module 1   Line Item 1 hello[SUM: hello2]                              ['hello[SUM: hello2]']                               [['Module 1 Line Item 2']]
		 Module 1   Line Item 2 goodbye[LOOKUP: blue123] + hello[SUM: hello2]   ['goodbye[LOOKUP: blue123]', 'hello[SUM: hello2]']   [['Module 2 Line Item 1'], ['Module 1 Line Item 1']]
		 Module 2   Line Item 1 goodbye[LOOKUP: blue123] + some other line item ['goodbye[LOOKUP: blue123]']                         [['Module 1 Line Item 2']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53592186
"link": https://stackoverflow.com/questions/53592186/modify-and-flatten-values-from-pandas-dataframe
"question": {
	"title": Modify and flatten values from Pandas dataframe
	"desc": Here is the dataframe I am working with: dtypes gives this: You can get a sample of the data by click on the link below: https://ufile.io/x534q What I would like to do now is to get rid of the header, the first column (0 to 6) and to flatten the rest of values so that the end result looks like this: Could you please help me? Thanks in advance. 
}
"io": {
	"Frame-1": 
		            0
		0  380.143752
		1  379.942595
		2  379.589472
		3  379.816187
		4  379.622086
		5  379.299071
		6  379.559615
		
	"Frame-2":
		380.143752 379.942595 379.589472 379.816187 379.622086 379.299071 379.559615
		
}
"answer": {
	"desc": %s A Transpose of the dataframe should give you what you need. This will still have headers though but you can use the data in your normal operations ignoring the header and index e.g. write to a csv file 
	"code-snippets": [
		newdf.to_csv(file,mode='w', sep=',',header=False,index=False) # you can use any separator here 'tabs' if you don't want commas.
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53589894
"link": https://stackoverflow.com/questions/53589894/how-to-handle-missing-data-with-respect-to-type-of-dataset
"question": {
	"title": How to handle missing data with respect to type of dataset?
	"desc": I have a dataset where has column types that has type like , . df I want to replace missing value with for each type. Such as- result_df How can do it with Python? 
}
"io": {
	"Frame-1": 
		   ID    types        C   D
		0  101   Primary      2   3
		1  103   Primary      6   3
		2  108   Primary     10   ?
		3  109   Primary      3  12
		4  118   Secondary    5   2
		5  122   Secondary    ?   6
		6  123   Secondary    5   6
		7  125   Secondary    2   5 
		
	"Frame-2":
		   ID    types        C   D
		0  101   Primary      2   3
		1  103   Primary      6   3
		2  108   Primary     10   3
		3  109   Primary      3  12
		4  118   Secondary    5   2
		5  122   Secondary    5   6
		6  123   Secondary    5   6
		7  125   Secondary    2   5 
		
}
"answer": {
	"desc": %s Something like this should work: First replace in your df with actual values: For me the is showing as for columns and . Hence, I convert these into numeric before finding median. If this is not applicable for you, skip this step and directly run the below command with function. In order to find , convert columns and into pandas numeric type: Now, you can fill nulls with of types in both columns and like below, using and functions: Let me know if this helps. 
	"code-snippets": [
		In [1274]: df.dtypes
		Out[1274]: 
		ID        int64
		types    object
		C        object
		D        object
		dtype: object
		
		----------------------------------------------------------------------
		In [1256]: df.C = df.C.apply(pd.to_numeric)
		In [1258]: df.D = df.D.apply(pd.to_numeric)
		
		In [1279]: df.dtypes
		Out[1279]: 
		ID         int64
		types     object
		C        float64
		D        float64
		dtype: object
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52466844
"link": https://stackoverflow.com/questions/52466844/pandas-corr-returning-nan-too-often
"question": {
	"title": Pandas corr() returning NaN too often
	"desc": I'm attempting to run what I think should be a simple correlation function on a dataframe but it is returning NaN in places where I don't believe it should. Code: Subject DataFrame: corr() Result: According to the (limited) documentation on the function, it should exclude "NA/null values". Since there are overlapping values for each column, should the result not all be non-NaN? There are good discussions here and here, but neither answered my question. I've tried the idea discussed here, but that failed as well. @hellpanderr's comment brought up a good point, I'm using 0.22.0 Bonus question - I'm no mathematician, but how is there a 1:1 correlation between B and C in this result? 
}
"io": {
	"Frame-1": 
		       A      B      C
		0    NaN  500.0    NaN
		1   99.0  100.0  100.0
		2    NaN  100.0    NaN
		3  100.0    NaN  100.0
		4  100.0    NaN    NaN
		5  100.0  100.0    NaN
		6    NaN  500.0  300.0
		7    NaN  500.0    NaN
		8    NaN  100.0    NaN
		
	"Frame-2":
		    A    B    C
		A  1.0  NaN  NaN
		B  NaN  1.0  1.0
		C  NaN  1.0  1.0
		
}
"answer": {
	"desc": %s The result seems to be an artefact of the data you work with. As you write, s are ignored, so it basically boils down to: So, there are only two values per column left for the calculation which should therefore lead to to correlation coefficients of : So, where do the s then come from for the remaining combinations? So, also here you end up with only two values per column. The difference is that the columns and contain only one value () which gives a standard deviation of : When the correlation coefficient is calculated, you divide by the standard deviation, which leads to a . 
	"code-snippets": [
		df[['A', 'C']].dropna().std()
		
		A    0.707107
		C    0.000000
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53471422
"link": https://stackoverflow.com/questions/53471422/drop-group-by-number-of-occurrence
"question": {
	"title": drop group by number of occurrence
	"desc": Hi I want to delete the rows with the entries whose number of occurrence is smaller than a number, for example: Here I want to delete all the rows if the number of occurrence in column 'a' is less than twice. Wanted output: What I know: we can find the number of occurrence by , and it will give me something like: But I don't know how I should approach from here to delete the rows. Thanks in advance! 
}
"io": {
	"Frame-1": 
		   a  b  c
		0  1  4  0
		1  2  5  1
		2  3  6  3
		3  2  7  2
		
	"Frame-2":
		   a  b  c
		1  2  5  1
		3  2  7  2
		
}
"answer": {
	"desc": %s  + The method maps to aligned with . + 
	"code-snippets": [
		res = df[df.groupby('a')['b'].transform('size') >= 2]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53357204
"link": https://stackoverflow.com/questions/53357204/how-to-do-a-calculation-only-at-some-rows-of-my-dataframe
"question": {
	"title": How to do a calculation only at some rows of my dataframe?
	"desc": Let's say I have a dataframe with only two columns and 20 rows, where all values from the first column are equal to 10, and all values from the second row are random percentage numbers. Now, I want to multiply the first column with the percentage values of the second column +1, but only at some intervals, and copy the last value to the next row. E.g. I want to do this multiplication operation from row 5 to 10. The problem Is that I don't know to start and end the calculation in arbitrary spots based on the df's index. Example input data: Which produces: An output I would like to get, is where the first row go thorugh a comulative multiplication only at sow rows, like this: Thank you! 
}
"io": {
	"Frame-1": 
		      A     B
		0   10  0.07
		1   10  0.02
		2   10  0.05
		3   10  0.00
		4   10  0.01
		5   10  0.09
		6   10  0.00
		7   10  0.02
		8   10  0.03
		9   10  0.05
		10  10  0.05
		11  10  0.03
		12  10  0.01
		13  10  0.09
		14  10  0.06
		15  10  0.07
		16  10  0.01
		17  10  0.01
		18  10  0.01
		19  10  0.07
		
	"Frame-2":
		      C       B
		0   10     0.07
		1   10     0.02
		2   10     0.05
		3   10     0.00
		4   10     0.01
		5   10,9   0.09
		6   10,9   0.00
		7   11,11  0.02
		8   11,45  0.03
		9   12,02  0.05
		10  12,62  0.05
		11  12,62  0.03
		12  12,62  0.01
		13  12,62  0.09
		14  12,62  0.06
		15  12,62  0.07
		16  12,62  0.01
		17  12,62  0.01
		18  12,62  0.01
		19  12,62  0.07
		
}
"answer": {
	"desc": %s To get the recursive product you can do the following: Output: Explanation: Calculate the cumulative product of , which is the factor to mulitply by to obtain the recursive product. Do this only over the range specified. and fill the the rows before with 1, so the value remains constant before this range. Multiply by to get the actual value, forward filling values after the range you specify. 
	"code-snippets": [
		start = 5
		end = 10
		
		df['C'] = ((1+df.B)[start:end+1].cumprod().reindex(df.index[:end+1]).fillna(1)*df.A).ffill()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53327932
"link": https://stackoverflow.com/questions/53327932/creating-a-pandas-dataframe-based-on-cell-content-of-two-other-dataframes
"question": {
	"title": creating a pandas dataframe based on cell content of two other dataframes
	"desc": I have wo dataframes with the same number of rows and columns. I would like to create a third dataframe based on these two dataframes that has the same dimensions as the other two dataframes. Each cell in the third dataframe should be the result by a function applied to the corresponding cell values in df1 and df2 respectively. i.e. if I have then df3 should be like this I have a way to do this that I do not think is very pythonic nor appropriate for large dataframes and would like to know if there is an efficient way to do such a thing? The function I wish to apply is: It can be used to produce a single scalar value OR an array of values. In my use case above the input to the function would be two scalar values. So smape(1, 5) = 0.66. 
}
"io": {
	"Frame-1": 
		df1 = | 1 | 2 |
		      | 3 | 4 |
		
		df2 = | 5 | 6 |
		      | 7 | 8 |
		
	"Frame-2":
		df3 = | func(1, 5) | func(2, 6) |
		      | func(3, 7) | func(4, 8) |
		
}
"answer": {
	"desc": %s You can use a vectorised approach: Or if you want to separate some of the logic in a function: 
	"code-snippets": [
		def smape3(df1, df2):
		    return (df2 - df1).abs() / (df2 + df1).abs()
		
		df = pd.DataFrame(np.where(df1.eq(0) & df2.eq(0), 0, smape3(df1, df2)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53314441
"link": https://stackoverflow.com/questions/53314441/python-pandas-explode-rows-by-turns
"question": {
	"title": Python - pandas explode rows by turns
	"desc": I have a dataframe as below. Now I extracted letters from B1-B3 and add to new columns U1-U3 get: and I want to let the row to explode like this: Thanks in advance 
}
"io": {
	"Frame-1": 
		    B1   B2  B3  U1 U2 U3 
		0   1C            C         
		1   3A   1A       A  A      
		2  41A  28A  3A   A  A  A   
		
	"Frame-2":
		    B1   B2  B3   U1  U2  U3 
		0   1C            C         
		1   3A   1A       A         
		2   3A   1A            A      
		3  41A  28A  3A   A         
		4  41A  28A  3A        A      
		5  41A  28A  3A            A    
		
}
"answer": {
	"desc": %s I think, it needs 3 step solution of 1) extracting the Alphabates from data and creating new columns, 2) duplicating the rows w.r.t values and 3) masking with identity matrix. 1) Extracting the Alphabates from the rows and assigning as columns Out: 2) You can try of of dataframe wherever it has more than 1 value -> 3) then by grouping with index, pick only (np.identity) w.r.t length of each group. Out: 
	"code-snippets": [
		df = df.merge(df.apply(lambda x: x.str.extract('([A-Za-z])')).add_prefix('U_'), left_index=True,right_index=True,how='outer')
		
		----------------------------------------------------------------------
		# Duplicating the rows of dataframe
		val = df[['U_B1','U_B2','U_B3']].notnull().sum(axis=1)
		df1 = df.loc[np.repeat(val.index,val)]
		
		----------------------------------------------------------------------
		masked values of identity matrix
		----------------------------------------------------------------------
		df1[['U_B1','U_B2','U_B3']] = df1.groupby(df1.index)['U_B1','U_B2','U_B3'].apply(lambda x: x.dropna(axis=1).mask(np.identity(len(x))==0))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53280650
"link": https://stackoverflow.com/questions/53280650/pandas-dataframe-interpreting-columns-as-float-instead-of-string
"question": {
	"title": Pandas Dataframe interpreting columns as float instead of String
	"desc": I want to import a csv file into a pandas dataframe. There is a column with IDs, which consist of only numbers, but not every row has an ID. I want to read this column as String, but even if I specifiy it with I get Is there an easy way get the ID as a string without decimal like without having to edit the Strings after importing the table? 
}
"io": {
	"Frame-1": 
		   ID      xyz
		0  12345     4.56
		1           45.60
		2  54231   987.00
		
	"Frame-2":
		   ID         xyz
		0  '12345.0'    4.56
		1   NaN        45.60
		2  '54231.0'  987.00
		
}
"answer": {
	"desc": %s A solution could be this, but after you have imported the df: Or since there are with: 
	"code-snippets": [
		df = pd.read_csv(filename)
		df['ID'] = df['ID'].astype(int).astype(str)
		
		----------------------------------------------------------------------
		df['ID'] = df['ID'].apply(lambda x: x if pd.isnull(x) else str(int(x)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53277378
"link": https://stackoverflow.com/questions/53277378/estimate-the-mean-of-a-dataframegroupby-by-only-considering-values-in-a-percenti
"question": {
	"title": Estimate the mean of a DataFrameGroupBy by only considering values in a percentile range
	"desc": I need to estimate the mean of a pandas DataFrameGroupBy by only considering the values between a given percentile range. For instance, given the snippet the result is However, if a percentile range is picked to exclude the maximum and minimum values the result should be How can I filter, for each group, the values between an arbitrary percentile range before estimating the mean? For instance, only considering the values between the 20th and 80th percentiles. 
}
"io": {
	"Frame-1": 
		m1 =            1
		      0          
		      1  2.333333
		      2  2.333333
		
	"Frame-2":
		m1 =     1
		      0          
		      1  2
		      2  2
		
}
"answer": {
	"desc": %s You can use a custom function with either or . The performance difference is marginal. The below example includes values only above the 20th and below the 80th percentile in calculating groupwise mean. 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		a = np.matrix('1 1; 1 2; 1 4; 2 1; 2 2; 2 4')
		data = pd.DataFrame(a)
		
		def jpp_np(df):
		    def meaner(x, lowperc, highperc):
		        low, high = np.percentile(x, [lowperc, highperc])
		        return x[(x > low) & (x < high)].mean()
		    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()
		
		def jpp_pd(df):
		    def meaner(x, lowperc, highperc):
		        low, high = x.quantile([lowperc/100, highperc/100]).values
		        return x[x.between(low, high, inclusive=False)].mean()
		    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()
		
		data = pd.concat([data]*10000)
		
		assert np.array_equal(jpp_np(data), jpp_pd(data))
		
		%timeit jpp_np(data)  # 11.2 ms per loop
		%timeit jpp_pd(data)  # 12.5 ms per loop
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53252056
"link": https://stackoverflow.com/questions/53252056/how-to-drop-rows-with-respect-to-a-column-values-in-python
"question": {
	"title": How to drop rows with respect to a column values in Python?
	"desc": I wantto remove rows with respect column values. df Here the list of value those I want to remove. I want to output like following: df How can I do this? 
}
"io": {
	"Frame-1": 
		   ID    B   C   D
		0  101   1   2   3
		1  103   5   6   7
		2  108   9  10  11
		3  109   5   3  12
		4  118  11  15   2
		5  121   2   5   6
		
	"Frame-2":
		   ID    B   C   D
		0  101   1   2   3
		3  109   5   3  12
		4  118  11  15   2
		
}
"answer": {
	"desc": %s You can check which IDs are in with the method, negate the result with and use the resulting for boolean indexing. Details: 
	"code-snippets": [
		>>> df['ID'].isin(remove_id)
		>>> 
		0    False
		1     True
		2     True
		3    False
		4    False
		5     True
		Name: ID, dtype: bool
		>>> ~df['ID'].isin(remove_id)
		>>> 
		0     True
		1    False
		2    False
		3     True
		4     True
		5    False
		Name: ID, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53184917
"link": https://stackoverflow.com/questions/53184917/interpolation-of-a-dataframe-with-immediate-data-appearing-before-and-after-it
"question": {
	"title": Interpolation of a dataframe with immediate data appearing before and after it - Pandas
	"desc": Let's say I've a dataframe like this - I want the dataframe to be transormed to - It takes the average of the immediate data available before and after a NaN and updates the missing/NaN value accordingly. 
}
"io": {
	"Frame-1": 
		ID  Weight Height
		1   80.0   180.0
		2   60.0   170.0
		3   NaN    NaN
		4   NaN    NaN
		5   82.0   185.0
		
	"Frame-2":
		ID  Weight  Height
		1   80.0    180.0
		2   60.0    170.0
		3   71.0    177.5
		4   76.5    181.25
		5   82.0    185.0
		
}
"answer": {
	"desc": %s You can use interpolation from the library by using the following: Check the arguments on the documentation for the method of interpolation to tune this to your problem case: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html 
	"code-snippets": [
		df['Weight'], df['Height'] = df.Weight.interpolate(), df.Height.interpolate()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53149552
"link": https://stackoverflow.com/questions/53149552/functools-reduce-in-place-modifies-original-dataframe
"question": {
	"title": functools reduce In-Place modifies original dataframe
	"desc": I currently facing the issue that "functools.reduce(operator.iadd,...)" alters the original input. E.g. I have a simple dataframe df = pd.DataFrame([[['A', 'B']], [['C', 'D']]]) Applying the iadd operator leads to following result: Now, the original df changed to Also copying the df using df.copy(deep=True) beforehand does not help. Has anyone an idea to overcome this issue? THX, Lazloo 
}
"io": {
	"Frame-1": 
		        0
		0  [A, B]
		1  [C, D]
		
	"Frame-2":
		              0
		0  [A, B, C, D]
		1        [C, D]
		
}
"answer": {
	"desc": %s Use instead of : After all, is the same as . So it modifies . In contrast, returns , so there is no modification of . Or, you could compute the same quantity using : The docs for warns: When , data is copied but actual Python objects will not be copied recursively, only the reference to the object. Since contains Python lists, the lists are not copied even with . This is why modifying the copy still affects . 
	"code-snippets": [
		In [39]: df[0].sum()
		Out[39]: ['A', 'B', 'C', 'D']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52107572
"link": https://stackoverflow.com/questions/52107572/swap-contents-of-columns-inside-dataframe
"question": {
	"title": Swap contents of columns inside dataframe
	"desc": I have a pandas dataframe with this contents; I would like to swap the contents between Column 1 and Column 2. The output dataframe should look like this; I am using python v3.6 
}
"io": {
	"Frame-1": 
		Column1   Column2    Column3  
		C11         C21        C31
		C12         C22        C32
		C13         C23        C33
		
	"Frame-2":
		Column1   Column2    Column3  
		C21         C11        C31
		C22         C12        C32
		C23         C13        C33
		
}
"answer": {
	"desc": %s I'm sure there's a better answer, but you can swap the column names and then reorder: 
	"code-snippets": [
		df = pd.DataFrame({"Column1": ["C11", "C12", "C13"],
		                   "Column2": ["C21", "C22", "C23"],
		                   "Column3": ["C31", "C32", "C33"]})
		df.columns = ["Column2", "Column1", "Column3"]
		df[["Column1", "Column2", "Column3"]]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 53062225
"link": https://stackoverflow.com/questions/53062225/how-can-check-the-duplication-on-the-group-level
"question": {
	"title": How can check the duplication on the group level?
	"desc": How can I check for duplicated groups and remove them? Here is my data frame: In this data frame group A and B are duplicate where as C is not because its forth element is different and thus it is deeper to be unique not duplicate, the resultant data frame should look like this: I tried to groupby and check for duplicates, but this will check the values on the observational level. How can check the duplication on the group level? 
}
"io": {
	"Frame-1": 
		Group     Value_1      Value_2
		 A          17           0.1
		 A          20           0.8
		 A          22           0.9
		 A          24           0.13
		
		 B          17           0.1
		 B          20           0.8
		 B          22           0.9
		 B          24           0.13
		
		 C          17           0.1
		 C          20           0.8
		 C          22           0.9
		 C          26           0.11    
		
	"Frame-2":
		Group     Value_1      Value_2
		 A          17           0.1
		 A          20           0.8
		 A          22           0.9
		 A          24           0.13
		
		
		 C          17           0.1
		 C          20           0.8
		 C          22           0.9
		 C          26           0.11    
		
}
"answer": {
	"desc": %s You can use and aggregate by with , then remove duplicates by (by default by all columns) and get indices - all groups names: Or reshape to by with and : Last filter by with : 
	"code-snippets": [
		idx = df.groupby('Group').agg(frozenset).drop_duplicates().index
		#alternative solution
		idx = df.groupby('Group').agg(tuple).drop_duplicates().index
		
		----------------------------------------------------------------------
		g = df.groupby('Group').cumcount()
		idx = df.set_index(['Group',g]).unstack().drop_duplicates().index
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52676653
"link": https://stackoverflow.com/questions/52676653/create-a-column-that-has-the-same-length-of-the-longest-column-in-the-data-at-th
"question": {
	"title": Create a column that has the same length of the longest column in the data at the same time
	"desc": I have the following data: Output: Is it possible to create a 4th column AT THE SAME TIME the others columns are created in data, which has the same length as the longest column of this dataframe (3rd one)? The data of this column doesn't matter. Assume it's 8. So this is the desired output can be: In my script the dataframe keeps changing every time. This means the longest columns keeps changing with it. Thanks for reading 
}
"io": {
	"Frame-1": 
		     0    1    2
		0  1.0  1.0  1.0
		1  2.0  2.0  2.0
		2  3.0  3.0  3.0
		3  NaN  4.0  4.0
		4  NaN  5.0  5.0
		5  NaN  NaN  6.0
		6  NaN  NaN  7.0
		
	"Frame-2":
		     0    1    2    3
		0  1.0  1.0  1.0  8.0
		1  2.0  2.0  2.0  8.0
		2  3.0  3.0  3.0  8.0
		3  NaN  4.0  4.0  8.0
		4  NaN  5.0  5.0  8.0
		5  NaN  NaN  6.0  8.0
		6  NaN  NaN  7.0  8.0
		
}
"answer": {
	"desc": %s This is quite similar to answers from @jpp, @Cleb, and maybe some other answers here, just slightly simpler: This will automatically give you a column of NaNs that is the same length as the longest columnn, so you don't need the extra work of calculating the length of the longest column. Resulting dataframe: Note that this answer is less general than some others here (such as by @jpp & @Cleb) in that it will only fill with NaNs. If you want some default fill values other than NaN, you should use one of their answers. 
	"code-snippets": [
		data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]] + [[]]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52985798
"link": https://stackoverflow.com/questions/52985798/get-minimum-value-from-index-in-data-frame
"question": {
	"title": Get minimum value from index in data frame
	"desc": I have a dataframe like this. I would like to get minimum values for each value in column1. So my output would be When I try the code It gives me an empty dataframe and if I try it deletes some values, for reasons I don't understand. I use python 2.7 
}
"io": {
	"Frame-1": 
		column1 column2
		
		1         2
		1         3
		1         4
		2         3
		2         1
		2         4
		
	"Frame-2":
		column1    column2
		1             2
		2             1
		
}
"answer": {
	"desc": %s Function return index values of minimal values of column per groups, so is necessary for selecting: Another solution is use with : EDIT: If possible multiple minimal values and want select all of them use with : 
	"code-snippets": [
		df = df.sort_values('column2', ascending=False).drop_duplicates('column1', keep='last')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52940317
"link": https://stackoverflow.com/questions/52940317/pandas-variable-shifting-within-groups
"question": {
	"title": Pandas variable shifting within groups
	"desc": I have a dataframe: I want to create a new field val2 such that each value in val2 is the value in val2 shifted by Lag number of rows. The tricky part here is that the shift should happen within the groups defined in field c1, such that the output looks something like I have been trying with along the lines of to no avail and getting a "The truth value of a Series is ambiguous." error. Appreciate any help. Thanks! 
}
"io": {
	"Frame-1": 
		c1   Lag  Val1  
		A    3    10
		A    1    5
		A    2    20
		A    2    15
		A    1    10
		B    1    25
		B    2    10
		
	"Frame-2":
		c1   Lag  Val1  Val2
		A    3    10    15
		A    1    5     20
		A    2    20    10
		A    2    15    NaN
		A    1    10    NaN
		B    1    25    10
		B    2    10    NaN
		
}
"answer": {
	"desc": %s You can can accomplish this with self- and a little manipulation of the index: Output: 
	"code-snippets": [
		# Copy and keep only the columns that are relevant
		df2 = df.rename(columns={'Val1': 'Val2'}).drop(columns='Lag').copy()
		
		# Shift the index
		df.index = df.index+df.Lag
		
		# Merge, requiring match on shifted index and within group.
		df.reset_index().merge(df2.reset_index(), on=['index', 'c1'], how='left').drop(columns='index')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52902568
"link": https://stackoverflow.com/questions/52902568/series-calculation-based-on-shifted-values-recursive-algorithm
"question": {
	"title": Series calculation based on shifted values / recursive algorithm
	"desc": I have the following: This lines basically only take in df['Alpha'] but not the df['PositionLong'].shift(1).. It cannot recognize it but I dont understand why? It produces this: However what I wanted the code to do is this: I believe the solution is to loop each row, but this will take very long. Can you help me please? 
}
"io": {
	"Frame-1": 
		df['Alpha']  df['Bravo']   df['PositionLong']
		0               0             0
		1               1             1
		0               1             0
		1               1             1
		1               1             1
		
	"Frame-2":
		df['Alpha']  df['Bravo']   df['PositionLong']
		0               0             0
		1               1             1
		0               1             1
		1               1             1
		1               1             1
		
}
"answer": {
	"desc": %s You are looking for a recursive function, since a previous value depends on , which itself is used to determine . But is a regular function, so is evaluated as a series of values, since you initialise the series with . A manual loop need not be expensive. You can use to efficiently implement your recursive algorithm: Result: 
	"code-snippets": [
		from numba import njit
		
		@njit
		def rec_algo(alpha, bravo):
		    res = np.empty(alpha.shape)
		    res[0] = 1 if alpha[0] == 1 else 0
		    for i in range(1, len(res)):
		        if (alpha[i] == 1) or ((res[i-1] == 1) and bravo[i] == 1):
		            res[i] = 1
		        else:
		            res[i] = 0
		    return res
		
		df['PositionLong'] = rec_algo(df['Alpha'].values, df['Bravo'].values).astype(int)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52873547
"link": https://stackoverflow.com/questions/52873547/calculating-grid-values-given-the-distance-in-python
"question": {
	"title": Calculating grid values given the distance in python
	"desc": I have a cell grid of big dimensions. Each cell has an ID (), cell value () and coordinates in actual measures (, ). This is how first 10 rows/cells look like Neighbouring cells of cell in the can be determined as (, , , , , ). For example: of 5 has neighbours - 4,6,504,505,506. (these are the ID of rows in the upper table - ). What I am trying to is: For the chosen value/row in , I would like to know all neighbours in the chosen distance from and sum all their values. I tried to apply this solution (link), but I don't know how to incorporate the distance parameter. The cell value can be taken with , but the steps before this are a bit tricky for me. Can you give me any advice? EDIT: Using the solution from Thomas and having df called : I'd like to add another column and use the values from columns But it doesn't work. If I add a number instead of a reference to row it works like charm. But how can I use values from p3 column automatically in function? SOLVED: It worked with: 
}
"io": {
	"Frame-1": 
		      p1     p2          p3     X  Y
		0      0     0.0         0.0    0  0
		1      1     0.0         0.0  100  0
		2      2     0.0        12.0  200  0
		3      3     0.0         0.0  300  0
		4      4     0.0        70.0  400  0
		5      5     0.0        40.0  500  0
		6      6     0.0        20.0  600  0
		7      7     0.0         0.0  700  0
		8      8     0.0         0.0  800  0
		9      9     0.0         0.0  900  0
		
	"Frame-2":
		      p3
		0     45
		1    580
		2  12000
		3  12531
		4  22456
		
}
"answer": {
	"desc": %s Solution: Output: Notes: I assumed was as seemed to be implied (so we don't need it at all). I don't think that is a neighbour of given the list of neighbors of defined by the OP. 
	"code-snippets": [
		import numpy as np
		import pandas
		
		# Generating toy data
		N = 10
		data = pandas.DataFrame({'p3': np.random.randn(N)})
		print(data)
		
		# Finding neighbours
		get_candidates = lambda i: [i-500+1, i-500-1, i-1, i+1, i+500+1, i+500-1]
		filter = lambda neighbors, N: [n for n in neighbors if 0<=n<N]
		get_neighbors = lambda i, N: filter(get_candidates(i), N)
		
		print("Neighbors of 5: {}".format(get_neighbors(5, len(data))))
		
		# Summing p3 on neighbors
		def sum_neighbors(data, i, col='p3'):
		  return data.iloc[get_neighbors(i, len(data))][col].sum()
		
		print("p3 sum on neighbors of 5: {}".format(sum_neighbors(data, 5)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52800453
"link": https://stackoverflow.com/questions/52800453/how-to-remove-unwanted-data-from-python-panda-data-frame
"question": {
	"title": How to remove unwanted data from python panda data frame?
	"desc": After reading my txt file: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale The panda datadframe like as below: But I need the data as below( Space and extra letter should be removed) 
}
"io": {
	"Frame-1": 
		1                 2                     3      4            
		
		-0.4302012 2     -0.3233208 3    0.576837 4   0.426791 5 
		
	"Frame-2":
		1                 2                     3      4            
		
		-0.4302012      -0.3233208     0.576837   0.426791 
		
}
"answer": {
	"desc": %s I got the answer: Thanks for all trying 
	"code-snippets": [
		data = pd.read_csv(filepath,delimiter=':',header= None)
		data = data.drop([0],axis=1)
		for i in range(1,9):
		   search_string = str(i+1)
		   data[i] = data[i].map(lambda x: x.replace(search_string.rjust(1),''))
		print(data)
		data.sample(n=5)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52744334
"link": https://stackoverflow.com/questions/52744334/creating-a-function-to-perform-grouping-and-sorting-based-on-columns-in-pandas-d
"question": {
	"title": Creating a function to perform grouping and sorting based on columns in Pandas dataframe and Labeling
	"desc":  I am wanting to group the data into two groups based on the Col2 group. However the first match should be assigned one value and the rest of the matches should be assigned a different value. Rahlf helped me to get a function created and then do However, I need two modifications to the function. Instead of the val, it will take the corresponding value from the Col 4 and then return one value (like 'low' to the first match within a group (based on the sorted col1) and then say 'low_red' for the rest of the matches in the group. So my question is how can I modify the function to do that? My Input: Expected Output: 
}
"io": {
	"Frame-1": 
		   Col1 Col2  Col3  Col4    
		   100   m1     1     4    
		   200   m2     7     5    
		   120   m1     4     4   
		   240   m2     8     5   
		   300   m3     5     4   
		   330   m3     2     4    
		   350   m3    11     4    
		   200   m4     9     4
		
	"Frame-2":
		   Col1 Col2  Col3  Col4   Col 5    
		   100   m1     1     4    low    
		   200   m2     7     5    med    
		   120   m1     4     4    low_red    
		   240   m2     8     5    med_red    
		   300   m3     5     4    high    
		   330   m3     2     4    high_red    
		   350   m3    11     4    high_red    
		   200   m4     9     4    high
		
}
"answer": {
	"desc": %s You can create a higher level function (let's call it ) that is called by , which then calls a lower level function (let's call it ) that applies the previous logic outlined in your question, like so: This yields: Note that operates on series and returns a like-indexed NDFrame, which is the result that we want (i.e. retain the index of the original dataframe). Therefore, we can call with our column, and then extract the corresponding column values from the original index using in the function being called from . 
	"code-snippets": [
		def my_function(group):
		
		    val = df.iloc[group.index]['Col4']
		
		    value = deeper_logic(group.iloc[0], val.iloc[0], group)
		
		    return [value if i==0 else value + '_red' for i in range(group.shape[0])]
		
		def deeper_logic(x, val, group):
		
		    if group.shape[0]==1:
		        if x>val:
		            return 'high'
		        else:
		            return 'low'
		
		    if x>val and any(i<=val for i in group.iloc[1:]):
		        return 'high'
		    elif x>val:
		        return 'med'
		    elif x<=val:
		        return 'low'
		    else:
		        return np.nan
		
		df['Col5'] = df.sort_values(['Col2','Col1']).groupby('Col2')['Col3'].transform(my_function)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52745166
"link": https://stackoverflow.com/questions/52745166/how-to-take-first-one-or-two-digits-from-float-number
"question": {
	"title": How to take first one or two digits from float number?
	"desc": I have a Pandas dataframe. In a series, I have time in hour and minute represented as , as below. I want only the hours: Example of time column values from (1 to 12) : Example of time column values from (13 to 24) : I have tried this code but it takes very long time until I close the editor so I didn't see the result 
}
"io": {
	"Frame-1": 
		1000.0 -> 10
		901.0 ->  9
		
	"Frame-2":
		1850.0 -> 18
		2301.0 -> 23
		
}
"answer": {
	"desc": %s With Pandas, don't iterate rows when vectorised methods are available. In this case, you can use floor division followed by : 
	"code-snippets": [
		df['hour'] = (df['dep_time'] // 100).astype(int)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52705423
"link": https://stackoverflow.com/questions/52705423/match-a-value-in-the-column-and-return-another-column-in-pandas-python
"question": {
	"title": Match a value in the column and return another column in pandas | python
	"desc": I have an of two columns(tab-separted): And which have one column: what I want is to search an element from in , and return the corresponding value in . If there is more than one matched value, then return all together in one column. Here is what should the output file be like: would be left empty because it does not exist. Any suggestion will be helpful. 
}
"io": {
	"Frame-1": 
		c1\tc2
		aaa\t232 65 19 32
		bbew\t32 22 20
		jhsi\t986 1 32 463 221
		
	"Frame-2":
		19       aaa
		1        jhsi
		32       aaa bbew jhsi
		277      
		
}
"answer": {
	"desc": %s This isn't easily vectorisable. For performance, I suggest you perform your transformation before you put data in a Pandas dataframe. Here is a solution using : Result Setup 
	"code-snippets": [
		# use set for O(1) lookup
		scope_set = set(df2['c1'])
		
		# initialise defualtdict of lists
		dd = defaultdict(list)
		
		# iterate and create dictionary mapping numbers to keys
		for row in df1.itertuples(index=False):
		    for num in map(int, row.c2.split()):
		        if num in scope_set:
		            dd[num].append(row.c1)
		
		# construct dataframe from defaultdict
		df = pd.DataFrame({'num': list(dd), 'keys': list(map(' '.join, dd.values()))})
		
		# reindex to include blanks
		df = df.set_index('num').reindex(sorted(scope_set)).reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52591303
"link": https://stackoverflow.com/questions/52591303/pandas-dataframe-filter
"question": {
	"title": Pandas DataFrame filter
	"desc": My question is about the pandas.DataFrame.filter command. It seems that pandas creates a copy of the data frame to write any changes. How am I able to write on the data frame itself? In other words: Output: Desired Output: 
}
"io": {
	"Frame-1": 
		   col1  col2
		0     1     3
		1     2     4
		
	"Frame-2":
		   col1  col2
		0    10     3
		1     2     4
		
}
"answer": {
	"desc": %s I think you need extract columns names and then use or functions: Or: You cannnot use function, because subset returns a Series/DataFrame which may have its data as a view. That's why is possible there (or raise if you set the option). 
	"code-snippets": [
		cols = df.filter(regex='col1').columns 
		df.loc[0, cols]=10
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52545438
"link": https://stackoverflow.com/questions/52545438/convert-a-dense-vector-to-a-dataframe-using-pyspark
"question": {
	"title": Convert a Dense Vector to a Dataframe using Pyspark
	"desc": Firstly I tried everything in the link below to fix my error but none of them worked. How to convert RDD of dense vector into DataFrame in pyspark? I am trying to convert a dense vector into a dataframe (Spark preferably) along with column names and running into issues. My column in spark dataframe is a vector that was created using Vector Assembler and I now want to convert it back to a dataframe as I would like to create plots on some of the variables in the vector. Approach 1: Below is the Error Approach 2: Error: I also tried to convert the dataframe into a Pandas dataframe and after that I am not able to split the values into separate columns Approach 3: Above code runs fine but I still have only one column in my dataframe with all the values separated by commas as a list. Any help is greatly appreciated! EDIT: Here is how my temp dataframe looks like. It just has one column all_features. I am trying to create a dataframe that splits all of these values into separate columns (all_features is a vector that was created using 200 columns) Expected output is a dataframe with all 200 columns separated out in a dataframe Here is how my Pandas DF output looks like 
}
"io": {
	"Frame-1": 
		+----------------------------+
		|        col1| col2| col3|...
		+----------------------------+
		|0.01193689934723|0.0|0.5049431301173817...
		|0.04774759738895|0.0|0.1657316216149636...
		|0.0|0.0|7.213126372469...
		|0.02387379869447|0.0|0.1866693496827619|...
		|1.89796699621085|0.0|0.3192169213385746|...
		+----------------------------+
		only showing top 5 rows
		
	"Frame-2":
		              0
		0   [0.011936899347238104, 0.0, 0.5049431301173817...
		1   [0.047747597388952415, 0.0, 0.1657316216149636...
		2   [0.0, 0.0, 0.19441761495525278, 7.213126372469...
		3   [0.023873798694476207, 0.0, 0.1866693496827619...
		4   [1.8979669962108585, 0.0, 0.3192169213385746, ...
		
}
"answer": {
	"desc": %s Since you want all the features in separate columns (as I got from your EDIT), the link to the answer you provided is not your solution. Try this, EDIT: Since your is originally a dataframe, you can also use this method without converting it to , 
	"code-snippets": [
		#column_names
		temp = temp.rdd.map(lambda x:[float(y) for y in x['all_features']]).toDF(column_names)
		
		----------------------------------------------------------------------
		import pyspark.sql.functions as F
		from pyspark.sql.types import *
		
		splits = [F.udf(lambda val: float(val[i].item()),FloatType()) for i in range(200)]
		temp = temp.select(*[s(F.col('all_features')).alias(c) for c,s in zip(column_names,splits)])
		temp.show()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52506702
"link": https://stackoverflow.com/questions/52506702/how-to-select-and-replace-specific-values-with-nan-in-pandas-dataframe-how-to
"question": {
	"title": How to select, and replace specific values with NaN in pandas dataframe. How to remove a column from each level 1 multiindex
	"desc": I have a csv file, which I read into a pandas frame: This is the view of the csv file (print(csv_file)): The resulting dataframe is MultiIndexed with two levels: print(df.column()): If a coordinate has a lower likelihood, I wan't the coordinates to be replaced by NaN. The new dataframe shan't have a likelihood column(s). An example with the first row from 'nose': After function should be like this: Note that outstanding values remain unchanged during this process! 
}
"io": {
	"Frame-1": 
		coords           x           y    likelihood
		0       197.486369    4.545954  3.890000e-07
		
	"Frame-2":
		coords           x           y
		0              NaN         NaN
		
}
"answer": {
	"desc": %s Assuming you have a threshold for defining "lower" likelihood: I also think there might be a more optimal way to do this (without looping through the columns) but this should work too. 
	"code-snippets": [
		for col in df.columns.levels[0]:
		    df.loc[df[(col, 'likelihood')] < threshold, [(col, 'x'), (col, 'y')]] = np.nan
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52502179
"link": https://stackoverflow.com/questions/52502179/split-pandas-dataframe-into-multiple-dataframes-based-on-null-columns
"question": {
	"title": Split pandas dataframe into multiple dataframes based on null columns
	"desc": I have a pandas dataframe as follows: Is there a simple way to split the dataframe into multiple dataframes based on non-null values? 
}
"io": {
	"Frame-1": 
		        a       b       c
		    0   1.0     NaN     NaN
		    1   NaN     7.0     5.0
		    2   3.0     8.0     3.0
		    3   4.0     9.0     2.0
		    4   5.0     0.0     NaN
		
	"Frame-2":
		        a   
		    0   1.0     
		
		         b      c
		    1    7.0    5.0
		
		        a       b       c
		    2   3.0     8.0     3.0
		    3   4.0     9.0     2.0
		
		        a       b      
		    4   5.0     0.0
		
}
"answer": {
	"desc": %s Using with We can save them in dict More Info using the to get the null column , if they are same we should combine them together 
	"code-snippets": [
		d = {y : x.dropna(1) for y, x in df.groupby(df.isnull().dot(df.columns))}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52492961
"link": https://stackoverflow.com/questions/52492961/compare-each-of-the-column-values-and-return-final-value-based-on-conditions
"question": {
	"title": Compare each of the column values and return final value based on conditions
	"desc": I currently have a dataframe which looks like this: What I want to do is apply some condition to the column values and return the final result in a new column. The condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4] I tried to define a function to append the final results but wasnt really getting anywhere...any thoughts? The desired outcome would look something like: where col4 is the new column created. Thanks 
}
"io": {
	"Frame-1": 
		col1  col2  col3
		 1      2     3
		 2      3     NaN
		 3      4     NaN
		 2      NaN   NaN
		 0      2     NaN
		
	"Frame-2":
		col1  col2  col3  col4
		 1     2     3     2
		 2     3     NaN   2
		 3     4    NaN    3
		 2     NaN   NaN   2
		 0     2    NaN    2
		
}
"answer": {
	"desc": %s first you may want to get ride of the NaNs: and then apply a function to every row to find your value: Output: 
	"code-snippets": [
		def func(x,l=[2,1,3,0,4,5]):
		    for j in l:
		      if(j in x):
		         return j
		
		df['new'] = df.apply(lambda x: func(list(x)),axis =1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52491327
"link": https://stackoverflow.com/questions/52491327/pandas-dataframe-data-are-same-or-new
"question": {
	"title": Pandas Dataframe data are same or new?
	"desc": In Python, Pandas dataframes are used : dataframe_1 : dataframe_2 : Here, dataframe_2 contains AB20, AB10 and AB17 same as dataframe_1 in random order. How to check which elements in dataframe_2 are new and which are same as dataframe_1 ??? 
}
"io": {
	"Frame-1": 
		     id
		0  AB17
		1  AB18
		2  AB19
		3  AB20
		4  AB10
		
	"Frame-2":
		     id
		0  AB20
		1  AB10
		2  AB17
		3  AB21
		4  AB09
		
}
"answer": {
	"desc": %s I think need for boolean mask and filter by with , if necessary convert output to : 
	"code-snippets": [
		mask = dataframe_2['id'].isin(dataframe_1['id'])
		print (mask)
		0     True
		1     True
		2     True
		3    False
		4    False
		Name: id, dtype: bool
		
		same = dataframe_2.loc[mask, 'id'].tolist()
		diff = dataframe_2.loc[~mask, 'id'].tolist()
		
		#if want unique values
		#same = dataframe_2.loc[mask, 'id'].unique().tolist()
		#diff = dataframe_2.loc[~mask, 'id'].unique().tolist()
		
		print (same)
		['AB20', 'AB10', 'AB17']
		
		print (diff)
		['AB21', 'AB09']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52489565
"link": https://stackoverflow.com/questions/52489565/convert-list-of-dict-in-dataframe-to-csv
"question": {
	"title": Convert list of dict in dataframe to CSV
	"desc": I have a dataframe that looks like this (df1): The detail column contains a list of dictionaries and each dictionary looks like this: I need to extract this information into a CSV with this format: In addition, the x2 and y2 in the extracted data should be like this: Expected output (Assuming the dict provided is in the first row of df1): I have tried this code to create a new df based off the detail column but I got an error: Error: 
}
"io": {
	"Frame-1": 
		{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}
		
	"Frame-2":
		1054, 78, 602, 549, 1232, 1113, 1, 5, 0.0
		
}
"answer": {
	"desc": %s  For each dict, you should use . Actually, I don't exactly that you want to print it out? or convert it into multiple dataframes. Here is some simple solutions. 
	"code-snippets": [
		a = pd.DataFrame(index=[78],columns=["detail"])
		a.loc[78,"detail"] = [{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]
		a.loc[188,"detail"] = [{'y1': 649, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52439222
"link": https://stackoverflow.com/questions/52439222/how-to-get-the-top-frequency-elements-after-grouping-by-columns
"question": {
	"title": How to get the top frequency elements after grouping by columns?
	"desc": I have a DataFrame named , and I want to count the top frequency elements in column , and on different . As you see, the of both and is . For , appears the most in column , and , appears the second most. So for and , the most frequency element is , and the second most is . 
}
"io": {
	"Frame-1": 
		df
		    id app_0 app_1 app_2  sex
		0   1     a     b     c    0
		1   2     b     c     b    0
		2   3     c     d     a    1
		3   4     d   NaN     a    1
		
	"Frame-2":
		df
		    id app_0 app_1 app_2  sex  top_1  top_2
		0   1     a     b     c    0      b      c
		1   2     b     c     b    0      b      c
		2   3     c     d     a    1      a      d
		3   4     d   NaN     a    1      a      d
		
}
"answer": {
	"desc": %s Use custom function with and : Or use with flattened values with : EDIT: More general solution working if second top values not exist with : 
	"code-snippets": [
		def f(x):
		    s = x.stack().value_counts()
		    return pd.Series([s.index[0], s.index[1]], index=['top_1','top_2'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52354096
"link": https://stackoverflow.com/questions/52354096/pandas-dataframe-take-rows-before-certain-indexes
"question": {
	"title": pandas dataframe take rows before certain indexes
	"desc": I have a dataframe and a list of indexes, and I want to get a new dataframe such that for each index (from the given last), I will take the all the preceding rows that matches in the value of the given column at the index. The column c3 the indexes (row numbers) 2, 4 , 5 my new dataframe will be: Explanation: For index 2, rows 0,1,2 were selected because C3 equals in all of them. For index 4, no preceding row is valid. And for index 5 also no preceding row is valid, and row 6 is irrelevant because it is not preceding. What is the best way to do so? 
}
"io": {
	"Frame-1": 
		      C1 C2 C3
		0     1  2  A
		1     3  4  A
		2     5  4  A
		3     7  5  B
		4     9  7  C
		5     2  3  D
		6     1  1  D
		
	"Frame-2":
		     C1 C2 C3
		0     1  2  A
		1     3  4  A
		2     5  4  A
		4     9  7  C
		5     2  3  D
		
}
"answer": {
	"desc": %s you can make conditions to filter data,if you want just preceding rows match to condition. Out: appying on other column Out: 
	"code-snippets": [
		ind= 2
		col ='C3'
		# ".loc[np.arange(ind+1)]" creates indexes till preceding row, so rest of matching conditions can be ignored 
		df.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()
		
		----------------------------------------------------------------------
		ind= 2
		col ='C2'
		df.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52312211
"link": https://stackoverflow.com/questions/52312211/mean-of-only-two-consecutive-rows-with-conditional-statements
"question": {
	"title": Mean of only two consecutive rows with conditional statements
	"desc": After having searched for similar questions I found out with this and this questions. Unfortunately neither of them works with me. The first works on all the columns, the second does not work on my column of and and returns error (I also have not understood it completely). Here's a description of the problem: I am working with a dataframe of ~54k rows. Here's an example of 24 values: is the solar hour angle in radians. It ranges from -pi/2 to +pi/2 for the hours 00:00 and 24:00 respectively. At midday its value is 0. is the hour angle to which the sunset occurs. Due to the symmetry of the sun-earth system, . These values are constant along one day, but change for every day. The column is a result of a conditional expression: when then it's day and further calculations can be made. In order to do further calculations I need to associate for each hour the midpoint of the time span that the measure covers. So, for example, the midday measure was recorded at 12:00 but in order to represent all of that hour I want to have the hour angle of 12:30. Therefore I need a where represents the index. But here a new problem arises: if the sunset occurs, let's say, at 6:40 am then the midpoint hour has to be calculated like this: Thus the hourly radian angle will correspond to 6:50am. I created the column to help perform this task, but unfortunately I can't really use it. Thank you. EDIT: The solution proposed by @Mabel Villaba is not correct, for the column only has sunrise and sunset values. A coorect column would be: I hope that it is clear enough EDIT2: Thank you again, but the values are still not correct: the mean values are still calculated wrongly. I have calculated manually the correct values, I will post them here: EDIT3: I think the column obtained thanks to the boolean mask might be misleading. In fact: the sunrise always occurs between two rows, let them be called and , whom belong to and respectively. The same happens with the sunset, but with and . What happens is that the correct of is calculated as: but hase a attribute in the column. For the sunset it's the opposite: occurring between and it is calculated as: and has a attribute. 
}
"io": {
	"Frame-1": 
		     omegam
		
		...
		7    -1.530775
		8    -1.359855
		9    -1.098058
		...
		13   -0.05256705
		...
		19   1.47992
		...
		
	"Frame-2":
		omegam(row3) = (omega3 + omegass)/2
		
}
"answer": {
	"desc": %s EDIT In the case you mention, it is a little more complicated but I think I came to a workaround. There is some misleading, since the operation at sunrise and sunset is not always done in the same direction. Let us create two omegas, that does and another that does : Then, we need to create a mask that tells us whether it is sunset or sunrise, or none of them: This way, corresponds to sunrise, to sunset and corresponds to the rest. Based on these conditions, we can create : OLD SOLUTION: As you mention, since , then you could create a new column in your pandas based on the hour so you can get the you need for the mean operation (if sunrise (hour<12): omegasr, else: - omegasr): Data in 'new_omega' is shifted to comply with Then, can be obtained just by applying the mean to the columns and when the condition is satisfied or the when : Hope it serves. 
	"code-snippets": [
		df['omega1'] = .5*((df['omega'] + df['omegasr'].shift(-1)))   
		df['omega2'] = .5*((df['omega'].shift(1) + df['omegass']))
		
		----------------------------------------------------------------------
		df['mask'] =  (df['isday'] * 1).diff().bfill()
		
		>> df[['date','mask', 'isday']]
		
		                     date  mask  isday
		0    2012-03-27 00:00:00    0.0  False
		1    2012-03-27 01:00:00    0.0  False
		2    2012-03-27 02:00:00    0.0  False
		3    2012-03-27 03:00:00    0.0  False
		4    2012-03-27 04:00:00    0.0  False
		5    2012-03-27 05:00:00    0.0  False
		6    2012-03-27 06:00:00    0.0  False
		7    2012-03-27 07:00:00    1.0   True
		8    2012-03-27 08:00:00    0.0   True
		9    2012-03-27 09:00:00    0.0   True
		10   2012-03-27 10:00:00    0.0   True
		11   2012-03-27 11:00:00    0.0   True
		12   2012-03-27 12:00:00    0.0   True
		13   2012-03-27 13:00:00    0.0   True
		14   2012-03-27 14:00:00    0.0   True
		15   2012-03-27 15:00:00    0.0   True
		16   2012-03-27 16:00:00    0.0   True
		17   2012-03-27 17:00:00    0.0   True
		18   2012-03-27 18:00:00    0.0   True
		19   2012-03-27 19:00:00   -1.0  False
		20   2012-03-27 20:00:00    0.0  False
		21   2012-03-27 21:00:00    0.0  False
		22   2012-03-27 22:00:00    0.0  False
		23   2012-03-27 23:00:00    0.0  False
		
		----------------------------------------------------------------------
		df['new_omega'] = df.apply(lambda x: x['omegasr'] if pd.to_datetime(x['date']).hour < 12 else -x['omegasr'], axis=1).shift(-1)
		
		>> df
		
		                     date   omegasr     omega   omegass  isday  new_omega
		
		1    2012-03-27 00:00:00  -1.570796 -3.323350  1.570796  False  -1.570796
		2    2012-03-27 01:00:00  -1.570796 -3.061551  1.570796  False  -1.570796
		3    2012-03-27 02:00:00  -1.570796 -2.799752  1.570796  False  -1.570796
		4    2012-03-27 03:00:00  -1.570796 -2.537952  1.570796  False  -1.570796
		5    2012-03-27 04:00:00  -1.570796 -2.276153  1.570796  False  -1.570796
		6    2012-03-27 05:00:00  -1.570796 -2.014353  1.570796  False  -1.570796
		7    2012-03-27 06:00:00  -1.570796 -1.752554  1.570796  False  -1.570796
		8    2012-03-27 07:00:00  -1.570796 -1.490755  1.570796   True  -1.570796
		9    2012-03-27 08:00:00  -1.570796 -1.228955  1.570796   True  -1.570796
		10   2012-03-27 09:00:00  -1.570796 -0.967156  1.570796   True  -1.570796
		11   2012-03-27 10:00:00  -1.570796 -0.705356  1.570796   True  -1.570796
		12   2012-03-27 11:00:00  -1.570796 -0.443557  1.570796   True   1.570796
		13   2012-03-27 12:00:00  -1.570796 -0.181758  1.570796   True   1.570796
		14   2012-03-27 13:00:00  -1.570796  0.080042  1.570796   True   1.570796
		15   2012-03-27 14:00:00  -1.570796  0.341841  1.570796   True   1.570796
		16   2012-03-27 15:00:00  -1.570796  0.603640  1.570796   True   1.570796
		17   2012-03-27 16:00:00  -1.570796  0.865440  1.570796   True   1.570796
		18   2012-03-27 17:00:00  -1.570796  1.127239  1.570796   True   1.570796
		19   2012-03-27 18:00:00  -1.570796  1.389039  1.570796   True   1.570796
		20   2012-03-27 19:00:00  -1.570796  1.650838  1.570796  False   1.570796
		21   2012-03-27 20:00:00  -1.570796  1.912637  1.570796  False   1.570796
		22   2012-03-27 21:00:00  -1.570796  2.174437  1.570796  False   1.570796
		23   2012-03-27 22:00:00  -1.570796  2.436236  1.570796  False   1.570796
		24   2012-03-27 23:00:00  -1.570796  2.698036  1.570796  False        NaN
		
		----------------------------------------------------------------------
		omegam[i] = (omegasr[i],omega[i+1]).mean() #sunrise
		omegam[i] = (omega[i],omegass[i+1]).mean() #sunset
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52151831
"link": https://stackoverflow.com/questions/52151831/repeating-a-series-to-create-a-df-in-python
"question": {
	"title": Repeating a series to create a df in python
	"desc": I have a series, How do I repeat this column 9 times to create a dataframe. Expected output: is I can use but this is a bit messy. I tried but it wouldn't work along , and isn't what I need Thanks 
}
"io": {
	"Frame-1": 
		       A   
		0     1.5         
		1     2.5                  
		2     1.3          
		
	"Frame-2":
		      A             A         ...       A
		0     1.5          1.5        ...      1.5         
		1     2.5          2.5        ...      2.5                 
		2     1.3          1.3        ...      1.3         
		
}
"answer": {
	"desc": %s Not sure, what you meant by messy but handles it pretty well here: you can pass to to make distinct column names. 
	"code-snippets": [
		pd.concat([s for i in range(9)], axis=1)
		
		----------------------------------------------------------------------
		keys=[f'A{i}' for i in range(9)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52155125
"link": https://stackoverflow.com/questions/52155125/rearranging-python-data-frame-index-and-columns
"question": {
	"title": Rearranging python data frame index and columns
	"desc": I want to convert this dataframe (note that 'ABC' is the index name): to this dataframe: What's the best way to perform this function? 
}
"io": {
	"Frame-1": 
		       t1    t2    t3 
		ABC
		gp      7    11    26
		fp      6    14    23
		pm      3    -1     7
		wm      2    -2     9
		
	"Frame-2":
		     s1   tx    gp   fp   pm   wm 
		0   ABC   t1     7    6    3    2
		1   ABC   t2    11   14   -1   -2
		2   ABC   t3    26   23    7    9
		
}
"answer": {
	"desc": %s How about this: 
	"code-snippets": [
		df = df.T
		df.reset_index(inplace=True)
		df.rename(columns={"index":"tx"}, inplace=True)
		df["s1"] = "ABC"
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52143288
"link": https://stackoverflow.com/questions/52143288/how-to-calculate-dictionaries-of-lists-using-pandas-dataframe
"question": {
	"title": How to calculate dictionaries of lists using pandas DataFrame?
	"desc": I have two strings in Python3.x, which are defined to be the same length: I am also given an integer which is meant to represent the "starting index" of . In this case, . The goal is to create a dictionary based on the indices. So, begins at , begins at . The dictionary "converting" these coordinates is as follows: which can be constructed (give the variables above) with: I currently have this data in the form of a pandas DataFrame: There are multiple entries of the same string in column . In this case, the dictionary for the coordinates with should be: I would like to take this DataFrame and calculate similar dictionaries of the coordinates. Such a statement looks like one should somehow use ? I'm not sure how to populate dictionary lists like this... Here is the correct output (keeping the DataFrame structure). Here the DataFrame has the column such that it looks like the following: 
}
"io": {
	"Frame-1": 
		{0: 51, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61}
		
	"Frame-2":
		{0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], 
		     4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]}
		
}
"answer": {
	"desc": %s Use - Output Explanation The reuses your code to create the dict for every row and then the zips the dicts together to form a list of dicts. The then creates the output out of the interim output. The last piece that I haven't included is the part where if the length of the list is 1 then you can include the first element only, taking the output from to 
	"code-snippets": [
		column1
		LJNVTJOY      {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54,...
		MXRBMVQDHF    {0: [79], 1: [80], 2: [81], 3: [82], 4: [83], ...
		WHLAOECVQR    {0: [18], 1: [19], 2: [20], 3: [21], 4: [22], ...
		Name: val, dtype: object
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52113834
"link": https://stackoverflow.com/questions/52113834/conditional-removal-of-duplicates-entries-in-pandas
"question": {
	"title": Conditional removal of duplicates entries in Pandas
	"desc": How can I remove the duplicate entries in the Pandas DataFrame given below. Desired result: The condition is: remove if and . 
}
"io": {
	"Frame-1": 
		a   b   c   d
		11216   08-08-2018  2000    SIP
		40277   28-08-2018  1000    SIP
		44165   02-08-2018  8000    Lump
		44165   03-08-2018  5000    Lump
		45845   16-08-2018  25000   Lump
		45845   18-08-2018  50000   Lump
		52730   13-08-2018  10000   Lump
		52730   27-08-2018  10000   Lump
		53390   20-08-2018  400000  Lump
		56180   02-08-2018  1000    Lump
		58537   11-07-2018  5000    Lump
		58537   22-08-2018  2000    SIP
		912813  15-08-2018  160001  Lump
		912813  15-08-2018  6000    SIP
		85606   16-08-2018  3500    SIP
		88327   06-08-2018  5000    SIP
		90240   07-08-2018  2000    SIP
		
	"Frame-2":
		a   b   c   d
		11216   08-08-2018  2000    SIP
		40277   28-08-2018  1000    SIP
		44165   02-08-2018  8000    Lump
		45845   16-08-2018  25000   Lump
		52730   13-08-2018  10000   Lump
		53390   20-08-2018  400000  Lump
		58537   11-07-2018  5000    Lump
		912813  15-08-2018  160001  Lump
		912813  15-08-2018  6000    SIP
		85606   16-08-2018  3500    SIP
		88327   06-08-2018  5000    SIP
		90240   07-08-2018  2000    SIP
		
}
"answer": {
	"desc": %s First you need to add them to a list and then this code can remove the duplicated items with your conditions. 
	"code-snippets": [
		i = 0 
		while i < len(a)-1 :
		    if a[i] == a[i+1] and if b[i] != b[s] :
		        del a[i]
		        del b[i]
		        del c[i]
		        del d[i]
		        i -= 1 
		    i += 1 
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52118251
"link": https://stackoverflow.com/questions/52118251/easy-tool-to-filtering-columns-with-specific-conditions-using-pandas
"question": {
	"title": easy tool to filtering columns with specific conditions using pandas
	"desc": I'm wondering if exist a tool in python to filter data between columns that follow an specific condition. I need to generate a clean dataframe where all the data in column 'A' must have the same consecutive number in column 'E'(and this number is repeated at least twice). Here an example: The output will be: 
}
"io": {
	"Frame-1": 
		df
		Out[30]: 
		         A         B           C          D           E
		6        1       2.366       8.621      10.835        1
		7        1       2.489       8.586      10.890        2
		8        1       2.279       8.460      10.945        2
		9        1       2.296       8.559      11.000        2
		10       2       2.275       8.620      11.055        2
		11       2       2.539       8.528      11.110        2
		50       2      3.346       5.979      10.175         5
		51       3       3.359       5.910      10.230        1
		52       3       3.416       5.936      10.285        1 
		
	"Frame-2":
		df
		Out[31]: 
		         A         B           C          D           E
		7        1       2.489       8.586      10.890        2
		8        1       2.279       8.460      10.945        2
		9        1       2.296       8.559      11.000        2
		10       2       2.275       8.620      11.055        2
		11       2       2.539       8.528      11.110        2
		51       3       3.359       5.910      10.230        1
		52       3       3.416       5.936      10.285        1 
		
}
"answer": {
	"desc": %s What you are looking for is: Output: Explanation: You want to keep all records where there is a consecutive group in which has a size of more than 2. The first part allows you to label consecutive groups in column , and then you group by that label and filter the , keeping only groups where the size is 2 or more. 
	"code-snippets": [
		import numpy as np
		
		df.groupby((df.E != df.E.shift(1)).cumsum()).filter(lambda x: np.size(x.E) >= 2)
		# or
		df[df.groupby((df.E != df.E.shift(1)).cumsum()).E.transform('size') >= 2]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52085200
"link": https://stackoverflow.com/questions/52085200/in-dataframe-of-strings-and-floats-cast-floats-to-integer-then-string
"question": {
	"title": In dataframe of strings AND floats, cast floats to integer then string
	"desc": I have the following dataframe of dtype : I would like to cast all the floats to integers then convert everything to strings, so the output would be of dtype : Is there something that allows me to cast a dataframe to an int but ignore errors? Or achieve this in a different way? (using the , seems to ignore the entire thing) 
}
"io": {
	"Frame-1": 
		  col1 col2  col3
		0  1.1  3.3  spam
		1  2.2  foo  eggs
		2  bar  4.4   5.5
		
	"Frame-2":
		  col1 col2  col3
		0    1    3  spam
		1    2  foo  eggs
		2  bar    4     5
		
}
"answer": {
	"desc": %s You can use a helper function that: tries to convert what's in your object to a - so "2.5" and "2" will be able to be translated (as well as anything that Python's function can interpret as a value), but "hello there. how are you?" won't... then tries to convert that to an then returns its value If the conversion fails - then it'll just return your original object as technically only the conversation can fail, as if that succeeds you can always and will always work (failing some weird custom class - that deliberately causes it to fail). eg: Then use it with 
	"code-snippets": [
		def try_to_int(obj):
		    try:
		        return str(int(float(obj)))
		    except (ValueError, TypeError):
		        return obj
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52064324
"link": https://stackoverflow.com/questions/52064324/add-a-different-item-from-a-list-to-each-cell-in-a-dataframe-with-pandas
"question": {
	"title": Add a different item from a list to each cell in a dataframe with Pandas
	"desc": Given a dataframe I want to make a list of sequential that has as many elements as there are rows in the dataframe And then add each element of the list (as a string) onto the end of a single column in the dataframe. Does not work, as it adds the whole list as a string to each value as opposed to one value of the list per value in the dataframe. Essentially, I want to transform to So anyway to do that would be fine. Thanks for the help! 
}
"io": {
	"Frame-1": 
		A    B    C
		23   16   85
		 9   74   12
		99   24   83
		
	"Frame-2":
		A     B    C
		231   16   85
		 92   74   12
		993   24   83  
		
}
"answer": {
	"desc": %s Try: That's assuming your dataframe has a regular . If not, you can use: Or 
	"code-snippets": [
		df['A'] = df.A.astype(str) + (df.reset_index().index + 1).astype(str)
		
		----------------------------------------------------------------------
		df['A'] = df.A.astype(str) + (pd.RangeIndex(len(df)) + 1).astype(str)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 52033359
"link": https://stackoverflow.com/questions/52033359/transform-a-large-dataframe-takes-too-long
"question": {
	"title": Transform a large dataframe - takes too long
	"desc": I have a dataframe loaded from a CSV in the following format: I want to transform it to the following format: This is my function ( is the original data frame) that does the transformation, but it takes 7 minutes for 547500 row dataframe. Is there a way to speed it up? 
}
"io": {
	"Frame-1": 
		           stock_code    price 
		20180827     001          10
		20180827     002          11
		20180827     003          12
		20180827     004          13
		20180826     001          14
		20180826     002          15
		20180826     003          11
		20180826     004          10
		20180826     005          19
		
	"Frame-2":
		            001     002     003     004     005
		20180827    10      11      12      13      nan
		20180826    14      15      11      10      19
		
}
"answer": {
	"desc": %s I believe here is possible use : 
	"code-snippets": [
		df = pd.pivot(df.index, df['stock_code'], df['price'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51902887
"link": https://stackoverflow.com/questions/51902887/python-subplot-plot-bar-from-one-dataframe-and-legend-from-a-different-dataframe
"question": {
	"title": python subplot plot.bar from one dataframe and legend from a different dataframe
	"desc": I have two data sets below Df1: I draw a bar diagram. (This is not an exact code of mine, but it will give you an idea) Now I want a legend based on a second data set of centroids. Df2: HPE subplot should have HPE column values (19.282091,1790.500000,1500.000000) as below. How can I do that? 
}
"io": {
	"Frame-1": 
		    Cluster     HPE     FRE     UNE
		0        0  176617  255282   55881
		1        1  126130    7752  252045
		2        2   12613   52326    7434
		
	"Frame-2":
		   Cluster          HPE         FRE          UNE
		0        0    19.282091  106.470162  1620.005037
		1        1  1790.500000  367.625000   537.856177
		2        2  1500.000000  180.148148  4729.275913
		
}
"answer": {
	"desc": %s Explicitly set the legend to strings obtained from second dataframe (if you want the color boxes of the bars): Or just use a table: 
	"code-snippets": [
		subp.legend([str(a) + ' - ' + str(b) for a, b in zip(df2['Cluster'].tolist(), df2['HPE'].tolist())])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51872125
"link": https://stackoverflow.com/questions/51872125/creating-list-from-dataframe
"question": {
	"title": creating list from dataframe
	"desc": I am a newbie to python. I am trying iterate over rows of individual columns of a dataframe in python. I am trying to create an adjacency list using the first two columns of the dataframe taken from csv data (which has 3 columns). The following is the code to iterate over the dataframe and create a dictionary for adjacency list: and the following is the output I am getting: I see that I am not getting the entire list when I use the constructor. Hence I am not able to loop over the entire data. Could anyone tell me where I am going wrong? To summarize, Here is the input data: the output that I am expecting: 
}
"io": {
	"Frame-1": 
		A,B,C
		933,4139,20100313073721718
		933,6597069777240,20100920094243187
		933,10995116284808,20110102064341955
		933,32985348833579,20120907011130195
		933,32985348838375,20120717080449463
		1129,1242,20100202163844119
		1129,2199023262543,20100331220757321
		1129,6597069771886,20100724111548162
		1129,6597069776731,20100804033836982
		
	"Frame-2":
		933: [4139,6597069777240, 10995116284808, 32985348833579, 32985348838375]
		1129: [1242, 2199023262543, 6597069771886, 6597069776731]
		
}
"answer": {
	"desc": %s Use and create of s and then : 
	"code-snippets": [
		#selecting by columns names
		d = df1.groupby('A')['B'].apply(list).to_dict()
		
		#seelcting columns by positions
		d = df1.iloc[:, 1].groupby(df1.iloc[:, 0]).apply(list).to_dict()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51850165
"link": https://stackoverflow.com/questions/51850165/pandas-dataframe-filter-column-a-depending-on-if-column-b-contains-x-for-group-o
"question": {
	"title": Pandas DataFrame filter column A depending on if column B contains x for group of values in A
	"desc": I would like to filter the below DataFrame on column , based on if for the value in , column contains the value . Here, values 1, 3, and 4 contain at least one row with value in column , while 2 and 5 do not. I am trying to filter out any rows with 2 and 5 so that the final output is: How could I do this (preferably in one step)? 
}
"io": {
	"Frame-1": 
		In [32]: df
		Out[32]: 
		   ref type
		0    1    P
		1    1    C
		2    1    A
		3    2    C
		4    3    P
		5    3    P
		6    4    P
		7    4    A
		8    5    C
		9    5    A
		
	"Frame-2":
		In [34]: df
		Out[34]: 
		   ref type
		0    1    P
		1    1    C
		2    1    A
		4    3    P
		5    3    P
		6    4    P
		7    4    A
		
}
"answer": {
	"desc": %s Use and : returns: 
	"code-snippets": [
		df.groupby('ref').filter(lambda x : ('P' in x['type'].values))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51784855
"link": https://stackoverflow.com/questions/51784855/remove-decimals-in-pandas-column-names
"question": {
	"title": Remove decimals in Pandas column names
	"desc": I have a dataframe with as the column names. I'd like to change them to be strings and remove the decimal places: I've tried saving the columns out to a list with and changing them there but I've had no luck. Any help would be greatly appreciated! 
}
"io": {
	"Frame-1": 
		df =
		     2006.0   2007.0   2008.0   2009.0
		0       foo      foo      bar      bar
		1       foo      foo      bar      bar
		
	"Frame-2":
		df =
		       2006     2007     2008     2009
		0       foo      foo      bar      bar
		1       foo      foo      bar      bar
		
}
"answer": {
	"desc": %s You can convert the type with Or use and convert to string type. 
	"code-snippets": [
		In [338]: df.columns.map('{:g}'.format)
		Out[338]: Index(['2006', '2007', '2008', '2009'], dtype='object')
		
		In [319]: df.columns.map(int)
		Out[319]: Int64Index([2006, 2007, 2008, 2009], dtype='int64')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51780710
"link": https://stackoverflow.com/questions/51780710/how-to-slice-rows-from-two-pandas-dataframes-then-merge-them-with-some-other-val
"question": {
	"title": How to slice rows from two pandas dataframes then merge them with some other value
	"desc": I got two pandas dataframes and two indexes, and one datetime variable. What I would like to do is: slice the dataframes with the indexes, then I got two rows. combine the two rows to one row. add the variable to the row. then I can get new indexes and datetime values to form more rows, and assemble the rows to a new dataframe. Example: df1: df2: index: 3, 5, datetime: Output: 
}
"io": {
	"Frame-1": 
		    A   B
		0   0   10
		1   1   11
		2   2   12
		3   3   13
		4   4   14
		5   5   15
		6   6   16
		7   7   17
		8   8   18
		9   9   19
		
	"Frame-2":
		    C   D
		0   10  110
		1   11  111
		2   12  112
		3   13  113
		4   14  114
		5   15  115
		6   16  116
		7   17  117
		8   18  118
		9   19  119
		
}
"answer": {
	"desc": %s You can try : Output: 
	"code-snippets": [
		index = [3,5]
		data = np.r_[df1.iloc[index[0]].values,df2.iloc[index[1]].values]
		df = pd.DataFrame([data],columns = list('ABCD'))
		dt = datetime.datetime(2018, 8, 10, 16, 53, 52, 760014)
		df['date'] = dt
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51734814
"link": https://stackoverflow.com/questions/51734814/combination-of-two-dataframes-without-duplicate-and-reversion-in-efficient-way
"question": {
	"title": Combination of two dataframes without duplicate and reversion in efficient way | python
	"desc": I have two dataframes with thousands of rows, I need to combine both into one dataframe without duplicate and reversion. for example: Dataframe 1 Dataframe 2 So, the output dataframe will be: output-dataframe I don't want the output combination containing something like: I actually try it using but it return duplicate and reversion and also took long time because I have thousands in Dataframes 1 and 2 Any help please ? 
}
"io": {
	"Frame-1": 
		drug1 disease1
		drug1 disease2
		drug1 disease3
		drug2 disease1
		drug2 disease2
		drug2 disease3 
		drug3 disease1
		drug3 disease2
		drug3 disease3
		
	"Frame-2":
		disease1 drug1
		drug1 drug1
		disease1 disease1 
		
}
"answer": {
	"desc": %s Try this solution: 
	"code-snippets": [
		from pandas import DataFrame, merge
		
		df1['key'] = 1
		df2['key'] = 1
		
		result = df1.merge(df2, on='key').drop('key', axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51735106
"link": https://stackoverflow.com/questions/51735106/pandas-assign-random-numbers-in-given-range-to-equal-column-values
"question": {
	"title": pandas: assign random numbers in given range to equal column values
	"desc": I am working with a large dataset, and one of the columns has very long integers, like below: What is important here is not the actual number in Column_2, but when those numbers are the same while Column_1 is different. I would like to reassign the values of Column_2 randomly from a range of smaller numbers, say (1, 999). My issue is figuring a way to describe in a lambda function that each equal value in Column_2 needs the same random number. 
}
"io": {
	"Frame-1": 
		       Column_1        Column_2
		  1     A              12345123451
		  2     B              12345123451
		  3     C              12345123451
		  4     D              23456789234
		  5     E              23456789234
		  6     F              34567893456
		
	"Frame-2":
		       Column_1        Column_2
		  1     A              120
		  2     B              120
		  3     C              120
		  4     D              54
		  5     E              54
		  6     F              567
		
}
"answer": {
	"desc": %s You can create an array of random numbers between 1 and 999 using , making sure to say so you don't get any duplicates, and then map to a dictionary mapping of unique values with your array of random numbers: Explanation: Your array of numbers looks like: And your mapping dictionary looks like: So when you map, you are saying to replace with , with , and so on... 
	"code-snippets": [
		>>> nums
		array([274, 842, 860])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51587685
"link": https://stackoverflow.com/questions/51587685/pandas-dataframe-replace-multiple-rows-based-on-values-in-another-column
"question": {
	"title": Pandas dataframe: Replace multiple rows based on values in another column
	"desc": I'm trying to replace some values in one dataframe's column with values from another data frame's column. Here's what the data frames look like. has a lot of rows and columns. The final df should look like this So basically what needs to happen is that and need to be matched and then needs to have values replaced by the corresponding row in for the rows that matched. I don't want to lose any values in which are not in I believe the module in python can do that? This is what I have so far: But it definitely doesn't work. I could also use merge as in but that doesn't replace the values inline. 
}
"io": {
	"Frame-1": 
		df1
		
		    0                   1029
		0   aaaaa               Green
		1   bbbbb               Green
		2   fffff               Blue
		3   xxxxx               Blue
		4   zzzzz               Green
		
		df2
		    0       1   2     3  ....    1029
		0   aaaaa   1   NaN   14         NaN
		1   bbbbb   1   NaN   14         NaN
		2   ccccc   1   NaN   14         Blue
		3   ddddd   1   NaN   14         Blue
		...    
		25  yyyyy   1   NaN   14         Blue
		26  zzzzz   1   NaN   14         Blue
		
	"Frame-2":
		    0       1   2     3  ....    1029
		0   aaaaa   1   NaN   14         Green 
		1   bbbbb   1   NaN   14         Green
		2   ccccc   1   NaN   14         Blue
		3   ddddd   1   NaN   14         Blue
		...    
		25  yyyyy   1   NaN   14         Blue
		26  zzzzz   1   NaN   14         Green
		
}
"answer": {
	"desc": %s You need: Output: 
	"code-snippets": [
		df1 = pd.DataFrame({'0':['aaaaa','bbbbb','fffff','xxxxx','zzzzz'], '1029':['Green','Green','Blue','Blue','Green']})
		
		df2 = pd.DataFrame({'0':['aaaa','bbbb','ccccc','ddddd','yyyyy','zzzzz',], '1029':[None,None,'Blue','Blue','Blue','Blue']})
		
		
		# Fill NaNs
		df2['1029'] = df2['1029'].fillna(df1['1029'])
		
		# Merge the dataframes 
		df_ = df2.merge(df1, how='left', on=['0'])
		
		df_['1029'] = np.where(df_['1029_y'].isna(), df_['1029_x'], df_['1029_y'])
		
		df_.drop(['1029_y','1029_x'],1,inplace=True)
		print(df_)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51578845
"link": https://stackoverflow.com/questions/51578845/indexing-with-multidimensional-key-pandas-error
"question": {
	"title": indexing with multidimensional key pandas error
	"desc": I have a dataframe as the following: I want to filter out g2 & g4 where all of 'B', 'C', 'D' values are 0 I tried returns output same as input, also returns output same as input, returns 'Cannot index with multi-dimensional key. The expected output is, What is the error here? Thanks in Advance. 
}
"io": {
	"Frame-1": 
		A   B  C  D  E
		g1  1 -10 20 text1
		g2  0  0  0  text2
		g3  0  1  0  text3
		g4  0  0  0  text4
		
	"Frame-2":
		A   B  C  D  E
		g1  1 -10 20 text1
		g3  0  1  0  text3
		
}
"answer": {
	"desc": %s You were closest with your second attempt. Compare to zero before you use . Then take the negative via the operator: To understand how this logic works, notice that comparing a dataframe to a scalar returns a Boolean dataframe: You can then collapse by dimension 1 via . The syntax and methods are borrowed from NumPy array functionality. 
	"code-snippets": [
		print(df[['B', 'C', 'D']] == 0)
		
		       B      C      D
		0  False  False  False
		2   True  False   True
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51357485
"link": https://stackoverflow.com/questions/51357485/combine-multi-columns-to-one-column-pandas
"question": {
	"title": Combine multi columns to one column Pandas
	"desc": Hi I have the following dataframe I would like to create a new column d to do something like this For the numbers, it is not in integer. It is in np.float64. The integers are for clear example. you may assume the numbers are like 32065431243556.62, 763835218962767.8 Thank you for your help 
}
"io": {
	"Frame-1": 
		   z  a   b   c 
		   a  1   NaN NaN
		   ss NaN 2   NaN
		   cc 3   NaN NaN
		   aa NaN 4   NaN
		   ww NaN 5   NaN
		   ss NaN NaN 6
		   aa NaN NaN 7
		   g  NaN NaN 8
		   j  9   NaN NaN
		
	"Frame-2":
		z  a   b   c    d
		a  1   NaN NaN  1
		ss NaN 2   NaN  2
		cc 3  NaN NaN  3
		aa NaN 4   NaN  4
		ww NaN 5   NaN  5
		ss NaN NaN 6    6
		aa NaN NaN 7    7
		g  NaN NaN 8    8
		j  9   NaN NaN  9
		
}
"answer": {
	"desc": %s We can replace the NA by 0 and sum up the rows. 
	"code-snippets": [
		df['d'] = df[['a', 'b', 'c']].fillna(0).sum(axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51408990
"link": https://stackoverflow.com/questions/51408990/how-to-fill-column-value-with-another-column-and-keep-existing
"question": {
	"title": How to fill column&#39; value with another column and keep existing?
	"desc": I have this dataframe with two column, and I want to fill with its corresponding if exist, also if already have a value keep it don't change it with . Here is an example: input output It's look easy, but I'm beginner in python :( Any help please. 
}
"io": {
	"Frame-1": 
		c1             c2
		HP_0003470  
		HP_8362789    HP_0093723
		              MP_0000231
		              MP_0000231
		
	"Frame-2":
		c1             c2
		HP_0003470  
		HP_8362789    HP_0093723
		MP_0000231    MP_0000231
		MP_0000231    MP_0000231
		
}
"answer": {
	"desc": %s You can do so: Output: 
	"code-snippets": [
		df['c1'] = df['c1'].replace('',np.NaN).fillna(df['c2'])
		df['c2'] = df['c2'].replace('',np.NaN).fillna(df['c1'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51371507
"link": https://stackoverflow.com/questions/51371507/pulling-multiple-ranges-from-dataframe-pandas
"question": {
	"title": Pulling Multiple Ranges from Dataframe Pandas
	"desc": Lets say I have the following data set: I have a the following list of ranges. How do I efficiently pull rows that lie in those ranges? Wanted Output Edit: Needs to work for floating points 
}
"io": {
	"Frame-1": 
		A      B    
		10.1   53
		12.5   42
		16.0   37
		20.7   03
		25.6   16
		30.1   01
		40.9   19
		60.5   99  
		
	"Frame-2":
		A      B    
		10.1   53
		12.5   42
		20.7   03
		40.9   19
		
}
"answer": {
	"desc": %s Update for modified question For floats, you can construct a mask using NumPy array operations: Previous answer to original question For integers, you can use with : An alternative solution with and : 
	"code-snippets": [
		from itertools import chain
		
		vals = set(chain.from_iterable(range(i, j) for i, j in L))
		
		res = df[df['A'].isin(vals)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51374862
"link": https://stackoverflow.com/questions/51374862/how-to-map-new-variable-in-pandas-in-effective-way
"question": {
	"title": How to map new variable in pandas in effective way
	"desc": Here's my data What I need, is to map : if is more than , is . But,if is less than , is What I did It works, but not highly configurable and not effective. 
}
"io": {
	"Frame-1": 
		Id  Amount
		1   6
		2   2
		3   0
		4   6
		
	"Frame-2":
		Id  Amount   Map
		1   6        1
		2   2        0
		3   0        0
		4   5        1
		
}
"answer": {
	"desc": %s Convert boolean mask to integer: Performance: 
	"code-snippets": [
		#[400000 rows x 3 columns]
		df = pd.concat([df] * 100000, ignore_index=True)
		
		In [133]: %timeit df['Map'] = (df['Amount'].values >= 3).astype(int)
		2.44 ms  97.4 s per loop (mean  std. dev. of 7 runs, 100 loops each)
		
		In [134]: %timeit df['Map'] = (df['Amount'] >= 3).astype(int)
		2.6 ms  66.4 s per loop (mean  std. dev. of 7 runs, 100 loops each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51357253
"link": https://stackoverflow.com/questions/51357253/how-to-find-which-row-items-are-appearing-most-in-a-pandas-dataframe
"question": {
	"title": How to find which row items are appearing most in a pandas dataframe
	"desc": I have a dataframe something like this : How to find which row is appearing the most number of times and unique items count? Here this is appearing most times in rows . I tried ,but it is giving me 100+ rules if my data is big. .NB : My real data is not and . This is mock data. 
}
"io": {
	"Frame-1": 
		    a   b   c   d   e   f
		  ------------------------
		0   0   0   1   1   0   1
		1   1   0   1   1   0   0
		2   0   0   1   1   0   1
		3   1   0   1   0   0   0
		4   0   0   1   1   0   1
		5   0   1   1   0   0   0
		6   1   0   1   0   1   1
		7   0   0   1   1   0   1
		8   1   0   1   1   1   0
		9   0   0   1   1   0   1
		
	"Frame-2":
		0  0  1   1   0   1
}
"answer": {
	"desc": %s Use by all columns with and for index by max value add : And for index values with compare by value: 
	"code-snippets": [
		s = df.groupby(df.columns.tolist())[df.columns[0]].transform('size')
		idx = s.index[s == s.max()]
		print (idx)
		Int64Index([0, 2, 4, 7, 9], dtype='int64')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51279903
"link": https://stackoverflow.com/questions/51279903/pandas-apply-and-applymap-functions-are-taking-long-time-to-run-on-large-dataset
"question": {
	"title": pandas apply and applymap functions are taking long time to run on large dataset
	"desc": I have two functions applied on a dataframe {{Update}} Dataframe has got almost 700 000 rows. This is taking much time to run. How to reduce the running time? Sample data : output: This line of code takes items from a list and fill one by one to each column as shown above. There will be almost 38 columns. 
}
"io": {
	"Frame-1": 
		   A        
		 ----------
		0 [1,4,3,c] 
		1 [t,g,h,j]  
		2 [d,g,e,w]  
		3 [f,i,j,h] 
		4 [m,z,s,e] 
		5 [q,f,d,s] 
		
	"Frame-2":
		   A         B   C   D  E
		-------------------------
		0 [1,4,3,c]  1   4   3  c
		1 [t,g,h,j]  t   g   h  j
		2 [d,g,e,w]  d   g   e  w
		3 [f,i,j,h]  f   i   j  h
		4 [m,z,s,e]  m   z   s  e
		5 [q,f,d,s]  q   f   d  s
		
}
"answer": {
	"desc": %s I think: should be changed to: And second if not mixed columns values - numeric with strings: 
	"code-snippets": [
		cols = res.select_dtypes(object).columns
		res[cols] = res[cols].apply(lambda x: x.str.strip('"'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51265888
"link": https://stackoverflow.com/questions/51265888/python-how-to-sum-unique-elements-respectively-of-a-dataframe-column-based-on-a
"question": {
	"title": python: how to sum unique elements respectively of a dataframe column based on another column
	"desc": For example, I have a df with two columns. Input Output I want to count the element in group by user_id respectively. The expected output is shown as follow. Expected Briefly, in column , I count the number of in column based on column . Hopefully for help! 
}
"io": {
	"Frame-1": 
		df
		    label user_id
		0      0       a
		1      0       a
		2      1       a
		3      0       b
		4      0       b
		5      2       b
		6      0       c
		7      1       c
		8      2       c
		
	"Frame-2":
		  df
		    label user_id  label_0  label_1  label_2
		0      0       a        2         1         0
		1      0       a        2         1         0
		2      1       a        2         1         0
		3      0       b        2         0         1
		4      0       b        2         0         1
		5      2       b        2         0         1
		6      0       c        1         1         1 
		7      1       c        1         1         1
		8      2       c        1         1         1
		
}
"answer": {
	"desc": %s Idea is create helper by with or and then and to original : Or using and with left join: 
	"code-snippets": [
		df = (df.join(df.groupby(['user_id', 'label'])
		                .size()
		                .unstack(fill_value=0)
		                .add_prefix('label_'), 'user_id'))
		
		----------------------------------------------------------------------
		df = (df.join(df.groupby('user_id')['label']
		                .value_counts()
		                .unstack(fill_value=0)
		                .add_prefix('label_'), 'user_id'))
		
		----------------------------------------------------------------------
		df = (df.merge(pd.crosstab(df['user_id'], df['label'])
		                 .add_prefix('label_'), on='user_id', how='left'))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51259229
"link": https://stackoverflow.com/questions/51259229/how-to-select-specific-column-items-as-list-from-pandas-dataframe
"question": {
	"title": How to select specific column items as list from pandas dataframe?
	"desc": I have a dataframe like this : How to convert it into this form (All zeros appearing are not considered) : And finally into a set of items like : 
}
"io": {
	"Frame-1": 
		  A  B  C   D 
		---------------
		0  A  0  C  D
		1  A  0  C  D
		2  0  B  C  0
		3  A  0  0  D
		4  0  B  C  0
		5  A  0  0  0
		
	"Frame-2":
		   A  B  C  D    E
		----------------------
		0  A  0  C  D  [A,C,D]
		1  A  0  C  D  [A,C,D]
		2  0  A  C  0  [A,C]
		3  A  0  0  D  [A,D]
		4  0  A  C  0  [A,C]
		5  A  0  0  0  [A]
		
}
"answer": {
	"desc": %s Use nested list comprehension with filtering : And for s: 
	"code-snippets": [
		s = [set([y for y in x if y != '0']) for x in df.values.tolist()]
		print (s)
		[{'A', 'D', 'C'}, {'A', 'D', 'C'}, {'C', 'B'}, {'A', 'D'}, {'C', 'B'}, {'A'}]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51186619
"link": https://stackoverflow.com/questions/51186619/convert-list-of-dictionaries-to-dataframe-with-one-column-for-keys-and-one-for-v
"question": {
	"title": Convert list of dictionaries to dataframe with one column for keys and one for values
	"desc": Let's suppose I have the following list: Which I want to convert it to a panda dataframe that have two columns: one for the keys, and one for the values. To do so, I have tried to use and also , but, in both cases, I get a dataframe like: Is there any way to specify what I want? By doing research I could only find the way I am describing above. 
}
"io": {
	"Frame-1": 
		list1 = [{'a': 1}, {'b': 2}, {'c': 3}]
	"Frame-2":
		     a    b    c
		0  1.0  NaN  NaN
		1  NaN  2.0  NaN
		2  NaN  NaN  3.0
		
}
"answer": {
	"desc": %s Use with flattening for list of tuples: Detail: 
	"code-snippets": [
		print ([(i, j) for a in list1 for i, j in a.items()])
		
		[('a', 1), ('b', 2), ('c', 3)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51143187
"link": https://stackoverflow.com/questions/51143187/create-new-column-depending-on-values-from-other-column
"question": {
	"title": Create new column depending on values from other column
	"desc": I have a DataFrame that looks something like this: df I want to give indexes the values that are between and in column and create a new columns as : 
}
"io": {
	"Frame-1": 
		         A  B  C
		0 vt 40462  5  6
		1        5  6  6
		2        5  5  8
		3        4  3  1
		4  vl 6450  5  6
		5        5  6  7
		6        1  2  3
		7  vt 40462  5  6
		8        5  5  8
		9   vl 658  6  7
		10       5  5  8
		11       4  3  1
		12 vt 40461 5  6
		13        5 5  8
		14        7 8  5
		
	"Frame-2":
		         A  B  C  D
		0 vt 40462  5  6  vt 40462
		1        5  6  6  vt 40462
		2        5  5  8  vt 40462
		3        4  3  1  vt 40462
		4  vl 6450  5  6  vl 6450
		5        5  6  7  vl 6450
		6        1  2  3  vl 6450
		7 vt 40462  5  6  vt 40462
		8        5  5  8  vt 40462
		9   vl 658  6  7  vl 658
		10       5  5  8  vl 658
		11       4  3  1  vl 658
		12 vt 40461 5  6  vt 40461
		13        5 5  8  vt 40461
		14        7 8  5  vt 40461
		
}
"answer": {
	"desc": %s Another way would be to column to be all values of that start with a letter, and then use to get rid of s: Or, more or less equivalently, but in 2 steps: 
	"code-snippets": [
		df.loc[df.A.astype(str).str.contains('^[A-Za-z]'), 'D'] = df.A
		
		df.ffill()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51141459
"link": https://stackoverflow.com/questions/51141459/map-filter-and-reduce-procedures-in-python
"question": {
	"title": Map, Filter and Reduce procedures in Python
	"desc": I am working through understanding the concepts of map, filter and reduce in Python. I am working in Spyder IDE with Python v3.6. I have a data frame: I want to select Cap records in increments of .005. Please see below: In this case, wouldn't a map function work in this case? Any other option would be great. I ideally need it to work in a way where I can incrementally select the records based on a certain value. 
}
"io": {
	"Frame-1": 
		Cap    OC_y       GMWB         PE        Acc
		0.01    0.0065  0.560840708 0.646683673 0.515243902
		0.0105  0.0068  0.586725664 0.676530612 0.53902439
		0.011   0.0071  0.612610619 0.706377551 0.562804878
		0.0115  0.0073  0.629867257 0.72627551  0.578658537
		0.012   0.0076  0.655752212 0.756122449 0.602439024
		0.0125  0.0079  0.681637168 0.785969388 0.626219512
		0.013   0.0082  0.707522124 0.815816327 0.65
		0.0135  0.0085  0.73340708  0.845663265 0.673780488
		0.014   0.0087  0.750663717 0.865561224 0.689634146
		0.0145  0.009   0.776548673 0.895408163 0.713414634
		0.015   0.0093  0.802433628 0.925255102 0.737195122
		
	"Frame-2":
		Cap    OC_y        GMWB          PE         Acc
		0.01    0.0065  0.560840708 0.646683673 0.515243902
		0.015   0.0093  0.802433628 0.925255102 0.737195122
		
}
"answer": {
	"desc": %s Assuming these are multiples of 0.005, you can divide and then use to select. If this isn't the case, then I recommend subtracting the delta until it is, and then repeating the above: 
	"code-snippets": [
		v = df.Cap / 0.005
		out = df[np.isclose(v, v.astype(int))]
		
		----------------------------------------------------------------------
		v = (df.Cap - (df.Cap % 0.005).iat[0]) / 0.005
		out = df[np.isclose(v, v.astype(int))]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51054844
"link": https://stackoverflow.com/questions/51054844/python-pandas-find-maximum-value-only-from-a-specific-part-of-a-column
"question": {
	"title": Python Pandas | Find maximum value only from a specific part of a column
	"desc": I have been trying to do this. Pandas max() would find the maximum value from the entire column. What I need is: My input csv file: Output needed: I am not sure how to select/group values from Val1 column with the same Id and then find their maximum value. Also, I have some blanks in the Val1 column, rendering its datatype as object. I don't know how to go about this. Any help would be most welcome. 
}
"io": {
	"Frame-1": 
		Id  Param1          Param2              Val1
		1  -5.00138282776   2.04990620034e-08   1.738e-05
		1  -4.80147838593   2.01516989762e-08   1.628e-05
		1  -4.60159301758   1.98263165885e-08   1.671e-05
		1  -4.40133094788   1.94918392538e-08   1.576e-05
		1  -4.20143127441   1.91767686175e-08   
		2  -5.00141859055   6.88369405921e-09   5.512e-06
		2  -4.80152130126   6.77335965093e-09   5.964e-06
		2  -4.60163593292   6.65415056389e-09
		3  -5.00138044357   1.16316911658e-08   4.008e-06
		3  -4.80148792267   1.15515588206e-08   7.347e-06
		3  -4.60160970681   1.14048361866e-08   8.446e-06
		3  -4.40137386322   1.12357021465e-08   
		
	"Frame-2":
		Id  Param1          Param2              Val1        Max_Val1_for_each_Id
		1  -5.00138282776   2.04990620034e-08   1.738e-05   1.738e-05
		1  -4.80147838593   2.01516989762e-08   1.628e-05
		1  -4.60159301758   1.98263165885e-08   1.671e-05
		1  -4.40133094788   1.94918392538e-08   1.576e-05
		1  -4.20143127441   1.91767686175e-08   
		2  -5.00141859055   6.88369405921e-09   5.512e-06   5.964e-06
		2  -4.80152130126   6.77335965093e-09   5.964e-06
		2  -4.60163593292   6.65415056389e-09
		3  -5.00138044357   1.16316911658e-08   4.008e-06   8.446e-06
		3  -4.80148792267   1.15515588206e-08   7.347e-06
		3  -4.60160970681   1.14048361866e-08   8.446e-06
		3  -4.40137386322   1.12357021465e-08 
		
}
"answer": {
	"desc": %s Use for new column of values per group: And then if need only first value add with mask created by with for invert mask: EDIT: If have no values and solution above raise error: TypeError: '>=' not supported between instances of 'float' and 'str' first step is convert non numeric to s: 
	"code-snippets": [
		df['Val1'] = pd.to_numeric(df['Val1'], errors='coerce')
		df['Max_Val1_for_each_Id'] = df.groupby('Id')['Val1'].transform('max')
		df['Max_Val1_for_each_Id'] = df['Max_Val1_for_each_Id'].where(~df['Id'].duplicated())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51045848
"link": https://stackoverflow.com/questions/51045848/improve-pandas-filter-speed-by-storing-indices
"question": {
	"title": Improve pandas filter speed by storing indices?
	"desc": I have the following df: I accumulate the AREA column as so: For each in , the is summed where == or , and it is always run when is sorted on . The real dataframe I'm working on though is 150,000 records, each row belonging to a unique ID1. Running the above on this dataframe takes 2.5 hours. Since this operation will take place repeatedly for the foreseeable future, I decided to store the indices of the True values in and in a DB with the following schema. Table ID1: Table ID2: The next time I run the accumulation on the column (now filled with different values) I read in the sql tables and the convert them to dicts. I then use these dicts to grab the records I need during the summing loop. When run this way it only takes 6 minutes! My question: Is there a better/standard way to handle this scenario, i.e storing dataframe selections for later use? Side note, I have set an index on the SQL table's ID columns and tried getting the indices by querying the table for each id, and it works well, but still takes a little longer than the above (9 mins). 
}
"io": {
	"Frame-1": 
		ID_,INDEX_
		1  ,   0
		2  ,   1
		etc, ect
		
	"Frame-2":
		ID_,INDEX_
		1  ,   0
		1  ,   4
		2  ,   0
		2  ,   1
		2  ,   3
		2  ,   5
		etc, etc
		
}
"answer": {
	"desc": %s One way to do it is like this: and you get the result expected Now on a bigger like: The method presented here turns in about 0.76 s on my computer while your first is running in 6.5 s. Ultimately, you could create a such as: to keep somewhere the information that linked ID1 and ID2: here you can see the id is equal to 2 in the column ID2, where the value of ID1 = 1, 4 and 6 and then you can run to not re-create the , with a small difference in the code: Hope it's faster 
	"code-snippets": [
		df = df.set_index('ID1') 
		for row in df.join(df.groupby('ID2')['AREA'].apply(lambda x: x.index.tolist()),rsuffix='_').dropna().itertuples():
		    df.loc[row[0],'AREA'] += df.loc[row[3],'AREA'].sum()
		df = df.reset_index()
		
		----------------------------------------------------------------------
		df = pd.DataFrame( {'ID1':range(1,1501),'ID2': np.random.randint(1,1501,(1500,)),'AREA':[1]*1500}, 
		                   columns = ['ID1','ID2','AREA'])
		
		----------------------------------------------------------------------
		df_list = (df.set_index('ID1')
		             .join(df.set_index('ID1').groupby('ID2')['AREA']
		                     .apply(lambda x: x.index.tolist()),rsuffix='_ID2')
		             .dropna().drop(['AREA','ID2'],1))
		
		----------------------------------------------------------------------
		df = df.set_index('ID1') 
		for row in df_list.itertuples():
		    df.loc[row[0],'AREA'] += df.loc[row[1],'AREA'].sum()
		df = df.reset_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51039857
"link": https://stackoverflow.com/questions/51039857/pandas-count-values-greater-than-current-row-in-the-last-n-rows
"question": {
	"title": Pandas count values greater than current row in the last n rows
	"desc": How to get count of values greater than current row in the last n rows? Imagine we have a dataframe as following: I am trying to get a table such as following where n=3. Thanks in advance. 
}
"io": {
	"Frame-1": 
		    col_a
		0    8.4
		1   11.3
		2    7.2
		3    6.5
		4    4.5
		5    8.9
		
	"Frame-2":
		    col_a   col_b
		0     8.4       0
		1    11.3       0
		2     7.2       2
		3     6.5       3
		4     4.5       3
		5     8.9       0
		
}
"answer": {
	"desc": %s In pandas is best dont loop because slow, here is better use with custom function: If performance is important, use strides: Performance: Here is used in small window : Also I was curious about performance in large window, : 
	"code-snippets": [
		np.random.seed(1256)
		n = 3
		
		def rolling_window(a, window):
		    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
		    strides = a.strides + (a.strides[-1],)
		    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)
		
		def roll(df):
		    df['new'] = (df['col_a'].rolling(n+1, min_periods=1).apply(lambda x: (x[-1] < x[:-1]).sum(), raw=True).astype(int))
		    return df
		
		def list_comp(df):
		    df['count'] = [(j < df['col_a'].iloc[max(0, i-3):i]).sum() for i, j in df['col_a'].items()]
		    return df
		
		def strides(df):
		    x = np.concatenate([[np.nan] * (n), df['col_a'].values])
		    arr = rolling_window(x, n + 1)
		    df['new1'] = (arr[:, :-1] > arr[:, [-1]]).sum(axis=1)
		    return df
		
		
		def make_df(n):
		    df = pd.DataFrame(np.random.randint(20, size=n), columns=['col_a'])
		    return df
		
		perfplot.show(
		    setup=make_df,
		    kernels=[list_comp, roll, strides],
		    n_range=[2**k for k in range(2, 15)],
		    logx=True,
		    logy=True,
		    xlabel='len(df)')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 51018104
"link": https://stackoverflow.com/questions/51018104/how-to-group-phone-number-with-and-without-country-code
"question": {
	"title": How to group phone number with and without country code
	"desc": I am trying to detect phone number, my country code is but some phone manufacturer or operator use and , after query and pivoting I get pivoted data. But, the pivoted data is out of context Here's the pivoted data Here's what I need to group, but I don't want to group manually ( and is same, etc) 
}
"io": {
	"Frame-1": 
		Id    +623684682   03684682   +623684684   03684684
		1              1          0            1          1
		2              1          1            2          1
		
	"Frame-2":
		Id      03684682   03684684
		1              1          2
		2              2          3
		
}
"answer": {
	"desc": %s I think need with aggregate : Or columns names and : 
	"code-snippets": [
		df = df.groupby(lambda x: x.replace('+62','0'), axis=1).sum()
		
		----------------------------------------------------------------------
		df.columns = df.columns.str.replace('\+62','0')
		df = df.sum(level=0, axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50957993
"link": https://stackoverflow.com/questions/50957993/fill-all-values-in-a-group-with-the-first-non-null-value-in-that-group
"question": {
	"title": Fill all values in a group with the first non-null value in that group
	"desc": The following is the pandas dataframe I have: If we look into the data, cluster 1 has Value 'A' for one row and remain all are NA values. I want to fill 'A' value for all the rows of cluster 1. Similarly for all the clusters. Based on one of the values of the cluster, I want to fill the remaining rows of the cluster. The output should be like, I am new to python and not sure how to proceed with this. Can anybody help with this ? 
}
"io": {
	"Frame-1": 
		cluster Value
		1         A
		1        NaN
		1        NaN
		1        NaN
		1        NaN
		2        NaN
		2        NaN
		2         B
		2        NaN
		3        NaN
		3        NaN
		3         C
		3        NaN
		4        NaN
		4         S
		4        NaN
		5        NaN
		5         A
		5        NaN
		5        NaN
		
	"Frame-2":
		cluster Value
		1         A
		1         A
		1         A
		1         A
		1         A
		2         B
		2         B
		2         B
		2         B
		3         C
		3         C
		3         C
		3         C
		4         S
		4         S
		4         S
		5         A
		5         A
		5         A
		5         A
		
}
"answer": {
	"desc": %s Edit The following seems better: Original I can't think of a better way to do this than iterate over all the rows, but one might exist. First I built your DataFrame: Now here's an approach that first creates a dict, then sets the values in as specified in the dict. Output: cluster Value 0 1 A 1 1 A 2 1 A 3 1 A 4 1 A 5 2 B 6 2 B 7 2 B 8 2 B 9 3 C 10 3 C 11 3 C 12 3 C 13 4 S 14 4 S 15 4 S 16 5 A 17 5 A 18 5 A 19 5 A Note: This sets all values based on the cluster and doesn't check for NaN-ness. You may want to experiment with something like: to see which is more efficient (my guess is the former, without the checks). 
	"code-snippets": [
		nan_map = df.dropna().set_index('cluster').to_dict()['Value']
		df['Value'] = df['cluster'].map(nan_map)
		
		print(df)
		
		----------------------------------------------------------------------
		# Create a dict to map clusters to unique values
		nan_map = df.dropna().set_index('cluster').to_dict()['Value']
		# nan_map: {1: 'A', 2: 'B', 3: 'C', 4: 'S', 5: 'A'}
		
		# Apply
		for i, row in df.iterrows():
		    df.at[i,'Value'] = nan_map[row['cluster']]
		
		print(df)
		
		----------------------------------------------------------------------
		# Apply
		for i, row in df.iterrows():
		    if isinstance(df.at[i,'Value'], float) and math.isnan(df.at[i,'Value']):
		        df.at[i,'Value'] = nan_map[row['cluster']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50983386
"link": https://stackoverflow.com/questions/50983386/how-do-i-sum-time-series-data-by-day-in-python-resample-sum-has-no-effect
"question": {
	"title": How do I sum time series data by day in Python? resample.sum() has no effect
	"desc": I am new to Python. How do I sum data based on date and plot the result? I have a Series object with data like: I have the following code: This gives me the following line(?) graph: It's a start; now I want to sum the doses by date. However, this code fails to effect any change: The resulting plot is the same. What is wrong? I have also tried , , , but there is no change in the plot. Is even the correct function? I understand resampling to be sampling from the data, e.g. randomly taking one point per day, whereas I want to sum each day's values. Namely, I'm hoping for some result (based on the above data) like: 
}
"io": {
	"Frame-1": 
		2017-11-03 07:30:00      NaN
		2017-11-03 09:18:00      NaN
		2017-11-03 10:00:00      NaN
		2017-11-03 11:08:00      NaN
		2017-11-03 14:39:00      NaN
		2017-11-03 14:53:00      NaN
		2017-11-03 15:00:00      NaN
		2017-11-03 16:00:00      NaN
		2017-11-03 17:03:00      NaN
		2017-11-03 17:42:00    800.0
		2017-11-04 07:27:00    600.0
		2017-11-04 10:10:00      NaN
		2017-11-04 11:48:00      NaN
		2017-11-04 12:58:00    500.0
		2017-11-04 13:40:00      NaN
		2017-11-04 15:15:00      NaN
		2017-11-04 16:21:00      NaN
		2017-11-04 17:37:00    500.0
		2017-11-04 21:37:00      NaN
		2017-11-05 03:00:00      NaN
		2017-11-05 06:30:00      NaN
		2017-11-05 07:19:00      NaN
		2017-11-05 08:31:00    200.0
		2017-11-05 09:31:00    500.0
		2017-11-05 12:03:00      NaN
		2017-11-05 12:25:00    200.0
		2017-11-05 13:11:00    500.0
		2017-11-05 16:31:00      NaN
		2017-11-05 19:00:00    500.0
		2017-11-06 08:08:00      NaN
		
	"Frame-2":
		2017-11-03 800
		2017-11-04 1600
		2017-11-05 1900
		2017-11-06 NaN
		
}
"answer": {
	"desc": %s This answer helped me see that I needed to assign it to a new object (if that's the right terminology): That produces the following plots: Is this method not better than the 'groupby' function? Now how do I make a scatter or bar plot instead of this line plot...? 
	"code-snippets": [
		import pandas as pd
		import matplotlib.pyplot as plt
		df = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')
		# plot bar graph of date and painkiller amount
		times = pd.to_datetime(df.loc[:,'Time'])
		
		# raw plot of data
		ts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,
		               name = 'Painkiller over Time')
		fig1 = ts.plot()
		
		# combine data by day
		test2 = ts.resample('D').sum()
		fig2 = test2.plot()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50890844
"link": https://stackoverflow.com/questions/50890844/select-individual-rows-from-multiindex-pandas-dataframe
"question": {
	"title": select individual rows from multiindex pandas dataframe
	"desc": I am trying to select individual rows from a multiindex dataframe using a list of multiindices. For example. I have got the following dataframe: I would like to select all 'C' with (A,B) = [(1,1), (2,2)] My flawed code for this is as follows: 
}
"io": {
	"Frame-1": 
		           Col1
		A B C
		1 1 1 -0.148593
		    2  2.043589
		  2 3 -1.696572
		    4 -0.249049
		2 1 5  2.012294
		    6 -1.756410
		  2 7  0.476035
		    8 -0.531612
		
	"Frame-2":
		           Col1
		A B C
		1 1 1 -0.148593
		    2  2.043589
		2 2 7  0.476035
		    8 -0.531612
		
}
"answer": {
	"desc": %s One option is to use : For a more generic solution, you can use f-strings (Python 3.6+), which should perform better than or manual concatenation. 
	"code-snippets": [
		filters = [(1,1), (2,2)]
		filterstr = '|'.join(f'(A=={i})&(B=={j})' for i, j in filters)
		res = df.query(filterstr)
		
		print(filterstr)
		
		(A==1)&(B==1)|(A==2)&(B==2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50860328
"link": https://stackoverflow.com/questions/50860328/python-add-a-column-to-a-dataframe-containing-a-value-from-another-row-based-o
"question": {
	"title": Python - Add a column to a dataframe containing a value from another row based on condition
	"desc": My dataframe looks like this: Now I need to add another column which conatains the difference between the value of column from the current row and the value of column from the row with the same number () and the group () that is written in . Exeption: If equals , and are the same. So the result should be: Explanation for the first two rows: First row: equals -> = Second row: search for the row with the same (123) and as (A1) and calculate of the current row minus of the referenced row (7.3 - 5.0 = 2.3). I thought I might need to use groupby() and apply(), but how? Hope my example is detailed enough, if you need any further information, please ask :) 
}
"io": {
	"Frame-1": 
		+-----+-------+----------+-------+
		| No  | Group | refGroup | Value |
		+-----+-------+----------+-------+
		| 123 | A1    | A1       |   5.0 |
		| 123 | B1    | A1       |   7.3 |
		| 123 | B2    | A1       |   8.9 |
		| 123 | B3    | B1       |   7.9 |
		| 465 | A1    | A1       |   1.4 |
		| 465 | B1    | A1       |   4.5 |
		| 465 | B2    | B1       |   7.3 |
		+-----+-------+----------+-------+
		
	"Frame-2":
		+-----+-------+----------+-------+----------+
		| No  | Group | refGroup | Value | refValue |
		+-----+-------+----------+-------+----------+
		| 123 | A1    | A1       |   5.0 |      5.0 |
		| 123 | B1    | A1       |   7.3 |      2.3 |
		| 123 | B2    | A1       |   8.9 |      3.9 |
		| 123 | B3    | B1       |   7.9 |      0.6 |
		| 465 | A1    | A1       |   1.4 |      1.4 |
		| 465 | B1    | A1       |   4.5 |      3.1 |
		| 465 | B2    | B1       |   7.3 |      2.8 |
		+-----+-------+----------+-------+----------+
		
}
"answer": {
	"desc": %s One way is to use a database SQL like technique; use 'self-join' with . You merge/join a dataframe to itself using and to line up 'Group' with 'refGroup' then subtract the value from each dataframe record: Output: 
	"code-snippets": [
		df_out = df.merge(df, 
		                  left_on=['No','refGroup'], 
		                  right_on=['No','Group'], 
		                  suffixes=('','_ref'))
		
		df['refValue'] = np.where(df_out['Group'] == df_out['refGroup'],
		                          df_out['value'],
		                          df_out['value'] - df_out['value_ref'])
		
		df
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50717904
"link": https://stackoverflow.com/questions/50717904/pandas-delete-all-rows-which-contains-required-value-in-all-column
"question": {
	"title": Pandas delete all rows which contains &quot;required value&quot; in all column
	"desc": I have the following dataframe I want to delete all rows which contains all column a V or N Output data frame will be :- 
}
"io": {
	"Frame-1": 
		 A     B    C    D
		BUY   150   Q   2018
		SELL  63    Q   2018
		N      N    N    N
		
		V      v    v    v
		SELL  53    Q   2018
		
	"Frame-2":
		    A     B     C    D
		   BUY   150    Q   2018
		   SELL  63     Q   2018
		
		   SELL  53     Q   2018
		
}
"answer": {
	"desc": %s Use : Detail: First compare by : Get rows if s per rows: Invert condition by : 
	"code-snippets": [
		print (df.isin(['V', 'v', 'N', 'n']))
		       A      B      C      D
		0  False  False  False  False
		1  False  False  False  False
		2   True   True   True   True
		3   True   True   True   True
		4  False  False  False  False
		
		----------------------------------------------------------------------
		print (df.isin(['V', 'v', 'N', 'n']).all(axis=1))
		0    False
		1    False
		2     True
		3     True
		4    False
		dtype: bool
		
		----------------------------------------------------------------------
		print (~df.isin(['V', 'v', 'N', 'n']).all(axis=1))
		0     True
		1     True
		2    False
		3    False
		4     True
		dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50644315
"link": https://stackoverflow.com/questions/50644315/merge-dictionaries-to-dataframe-get-dummies
"question": {
	"title": Merge dictionaries to dataframe get_dummies
	"desc": In a dictionary with information about a string in a text file, where keys are the strings and values are the names of the files. 'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'} Another dictionary contains information about the best match for the strings from the text. The third dictionary contains information about the percentage of occurrence of the string in the text. Now my aims are to use the information of these different dictionaries to generate a dataframe like this: To this, I started the script but I am stuck: This block was removed from script: And substituted to: But the final result was not very exact: But the expected table is: Where the string in file4 where not detected. Blosk removed Here relies the problem But the only result that I get is this: As I can understand, pd.getdummies groups the field 'file' from my df_origin and uses 'text' to check their presence. How can I redirect the command to plot the columns 'percent' in my df_origin dataframe? 
}
"io": {
	"Frame-1": 
		Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}
		
	"Frame-2":
		Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}
		
}
"answer": {
	"desc": %s Try this: Output: 
	"code-snippets": [
		       fly jump  run swim
		file1   60   90  NaN   30
		file2   30   70   40   60
		file3   90   10   90   70
		file4  NaN  NaN  NaN  NaN
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50533522
"link": https://stackoverflow.com/questions/50533522/copying-columns-within-pandas-dataframe
"question": {
	"title": Copying columns within pandas dataframe
	"desc": I want to slice and copy columns in a Python Dataframe. My data frame looks like the following: I want to make it of the form Which basically means that I want to shift the values in Columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1' For the same, I wrote the code as No copying takes places within the columns. How shall I modify my code?? 
}
"io": {
	"Frame-1": 
		     1928  1928.1  1929  1929.1  1930  1930.1
		 0    0     0       0     0       0     0
		 1    1     3       3     2       2     2
		 2    4     1       3     0       1     2
		
	"Frame-2":
		     1928  1928.1  1929 1929.1 1930 1930.1
		 0   0     0            
		 1   1     3          
		 2   4     1                    
		 3   0     0
		 4   3     2
		 5   3     0
		 6   0     0
		 7   2     2
		 8   1     2 
		
}
"answer": {
	"desc": %s Setup with : 
	"code-snippets": [
		cols = ['1929', '1929.1', '1930', '1930.1']
		vals = df[cols].values.reshape(-1, 2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 18942506
"link": https://stackoverflow.com/questions/18942506/add-new-column-in-pandas-dataframe-python
"question": {
	"title": Add new column in Pandas DataFrame Python
	"desc": I have dataframe in Pandas for example: Now if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be: Any idea on how to achieve this? 
}
"io": {
	"Frame-1": 
		Col1 Col2
		A     1 
		B     2
		C     3
		
	"Frame-2":
		Col1 Col2 Col3
		A    1    1
		B    2    0
		C    3    0
		
}
"answer": {
	"desc": %s You just do an opposite comparison. . This will return a boolean Series with values for those greater than 1 and values for the other. If you convert it to an dtype, becomes and become , If you want a more general solution, where you can assign any number to depending on the value of you should do something like: Or: 
	"code-snippets": [
		df['Col3'] = (df['Col2'] <= 1).astype(int)
		
		----------------------------------------------------------------------
		df['Col3'] = df['Col2'].map(lambda x: 42 if x > 1 else 55)
		
		----------------------------------------------------------------------
		df['Col3'] = 0
		condition = df['Col2'] > 1
		df.loc[condition, 'Col3'] = 42
		df.loc[~condition, 'Col3'] = 55
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50642233
"link": https://stackoverflow.com/questions/50642233/creating-single-row-pandas-dataframe
"question": {
	"title": Creating single row pandas dataframe
	"desc": I have a dataframe like this- I want to create a new dataframe which looks like this- 
}
"io": {
	"Frame-1": 
		                 0
		0            a  43
		1            b  630
		2            r  587
		3            i  462
		4            g  153
		5            t  266
		
	"Frame-2":
		     a         b     r       i     g        t
		0    43       630   587    462    153      266
		
}
"answer": {
	"desc": %s If you have just one column with this format , this should work: Input: Output: 
	"code-snippets": [
		df.columns = ['col']
		df = pd.DataFrame(df.col.str.split(' ',1).tolist(), columns = ['col1','col2']).T.reset_index(drop=True)
		df = df.rename(columns=df.iloc[0]).drop(df.index[0])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50593948
"link": https://stackoverflow.com/questions/50593948/python-cbind-previous-and-next-row-to-current-row
"question": {
	"title": Python - Cbind previous and next row to current row
	"desc": I have a Pandas data frame like so: Which looks like: I'd like to bind the previous row and the next next row to each column like so (accounting for "doc" and "sent" column in my example, which count as indices that nothing can come before or after as seen below): 
}
"io": {
	"Frame-1": 
		  doc  sent col1 col2 col3
		0   0    0    5   4    8
		1   0    1    6   3    2
		2   0    2    1   2    9
		3   1    0    6   1    6
		4   1    1    5   1    5
		
	"Frame-2":
		  doc  sent col1 col2 col3 p_col1 p_col2 p_col3 n_col1 n_col2 n_col3
		0   0    0    5   4    8    0      0      0      6       3      2  
		1   0    1    6   3    2    5      4      8      1       2      9
		2   0    2    1   2    9    6      3      2      6       1      6
		3   1    0    6   1    6    0      0      0      5       1      5
		4   1    1    5   1    5    6      1      6      0       0      0
		
}
"answer": {
	"desc": %s use to get the prev / next rows, to merge the dataframes & to set nulls to zero The presence of nulls upcasts the ints to floats, since numpy integer arrays cannot contain null values, which are cast back to ints after replacing nulls with 0. outputs: 
	"code-snippets": [
		cs = ['col1', 'col2', 'col3']
		g = df.groupby('doc')
		
		pd.concat([
		   df, 
		   g[cs].shift(-1).add_prefix('n'), 
		   g[cs].shift().add_prefix('p')
		], axis=1).fillna(0).astype(int)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50592595
"link": https://stackoverflow.com/questions/50592595/merge-dataframes-including-extreme-values
"question": {
	"title": Merge dataframes including extreme values
	"desc": I have 2 data frames, df1 and df2: I want to merge dataframes but at the same time including the first and/or last value of the set in column A. This is an example of the desired outcome: I'm trying to use but that only slice the portion of data frames that coincides. Someone have an idea to deal with this? thanks! 
}
"io": {
	"Frame-1": 
		df1
		Out[66]: 
		    A   B
		0   1  11
		1   1   2
		2   1  32
		3   1  42
		4   1  54
		5   1  66
		6   2  16
		7   2  23
		8   3  13
		9   3  24
		10  3  35
		11  3  46
		12  3  51
		13  4  12
		14  4  28
		15  4  39
		16  4  49
		
		df2
		Out[80]: 
		    B
		0  32
		1  42
		2  13
		3  24
		4  35
		5  39
		6  49
		
	"Frame-2":
		df3
		Out[93]: 
		    A   B
		0   1   2
		1   1  32
		2   1  42
		3   1  54
		4   3  13
		5   3  24
		6   3  35
		7   3  46
		8   4  28
		9   4  39
		10  4  49
		
}
"answer": {
	"desc": %s Here's one way to do it using with indicator, , and : Output: 
	"code-snippets": [
		df[df.merge(df2, on='B', how='left', indicator='Ind').eval('Found=Ind == "both"')
		     .groupby('A')['Found']
		     .apply(lambda x: x.rolling(3, center=True, min_periods=2).max()).astype(bool)]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50545075
"link": https://stackoverflow.com/questions/50545075/create-extra-columns-in-pandas-time-indexed-dataframe
"question": {
	"title": Create extra columns in pandas time-indexed DataFrame
	"desc": I currenty have a Datetime-Indexed dataframe with three columns: I would like to create three extra columns that hold the values indexed one hour from the current index to end up with something like this: I have already defined a function that creates a dataframe with the columns 'Glucosa1', 'Insulina1', 'Carbs1' but it is very poorly performing and I would like to make it run faster. I profile the time used by different functions on my code using the following: This outputs a time of 8.331165 seconds (on average) for the function nn_format_df() compared to similar functions (which iterate over the rows of the dataframe) wicht output 0.366158 seconds . After creating a new dataframe calling my function on the original I merge them to get the desired dataframe. The function: To sum it up: I would appreciate any comments on how to improve the function so that it doesn't take so long. A better, more Pythonic or pandas-y way of getting the desired dataframe is welcome. I am new to pandas and I understand my implementation of the function is a completely nave approach. 
}
"io": {
	"Frame-1": 
		                     Glucosa   Insulina  Carbs
		Hour
		2018-05-16 06:43:00    156.0       7.0   65.0
		2018-05-16 07:43:00    170.0       0.0   65.0
		2018-05-16 08:45:00    185.0       2.0    0.0
		2018-05-16 09:45:00    150.0       0.0    0.0
		2018-05-16 10:45:00     80.0       0.0    0.0
		     ...
		
	"Frame-2":
		                     Glucosa   Insulina  Carbs  Glucosa1  Insulina1  Carbs1
		Hour
		2018-05-16 06:43:00    156.0       7.0   65.0      170.0        0.0   65.0
		2018-05-16 07:43:00    170.0       0.0   65.0      185.0        2.0    0.0
		2018-05-16 08:45:00    185.0       2.0    0.0      150.0        0.0    0.0
		2018-05-16 09:45:00    150.0       0.0    0.0       80.0        0.0    0.0
		2018-05-16 10:45:00     80.0       0.0    0.0       ...         ...    ...
		     ...
		
}
"answer": {
	"desc": %s You can accomplish this very quickly with , which shifts an entire . Just use to combine them together; the argument specifies that you want to append new columns instead of rows. The above code gives you the following output: 
	"code-snippets": [
		import pandas as pd
		pd.concat([df, df.shift(-1).rename(columns=dict((elem, elem+'1') for elem in df.columns))], axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50481372
"link": https://stackoverflow.com/questions/50481372/lengthening-a-dataframe-based-on-stacking-columns-within-it-in-pandas
"question": {
	"title": Lengthening a DataFrame based on stacking columns within it in Pandas
	"desc": I am looking for a function that achieves the following. It is best shown in an example. Consider: which looks like: I would like to collapase the and columns, lengthening the DataFame where necessary, so that the output is: That is, one row for each combination between either and , or and . I am looking for a function that does this relatively efficiently, as I have multiple s and many rows. 
}
"io": {
	"Frame-1": 
		   x  y1   y2
		0  1   2  3
		1  4   5  NaN
		
	"Frame-2":
		   x  y
		0  1   2  
		1  1   3  
		2  4   5  
		
}
"answer": {
	"desc": %s Here's one based on NumPy, as you were looking for performance - Sample run - If the columns are always starting from the second column onwards until the end, we can simply slice the dataframe and hence get further performance boost, like so - 
	"code-snippets": [
		def gather_columns(df):
		    col_mask = [i.startswith('y') for i in df.columns]
		    ally_vals = df.iloc[:,col_mask].values
		    y_valid_mask = ~np.isnan(ally_vals)
		
		    reps = np.count_nonzero(y_valid_mask, axis=1)
		    x_vals = np.repeat(df.x.values, reps)
		    y_vals = ally_vals[y_valid_mask]
		    return pd.DataFrame({'x':x_vals, 'y':y_vals})
		
		----------------------------------------------------------------------
		def gather_columns_v2(df):
		    ally_vals = df.iloc[:,1:].values
		    y_valid_mask = ~np.isnan(ally_vals)
		
		    reps = np.count_nonzero(y_valid_mask, axis=1)
		    x_vals = np.repeat(df.x.values, reps)
		    y_vals = ally_vals[y_valid_mask]
		    return pd.DataFrame({'x':x_vals, 'y':y_vals})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50409452
"link": https://stackoverflow.com/questions/50409452/python-pandas-apply-on-separated-values
"question": {
	"title": Python pandas: apply on separated values
	"desc": How can I sum values in dataframe that a separated by semicolon? Got: Need: 
}
"io": {
	"Frame-1": 
		                  col1            col2
		2018-03-05         2.1               8
		2018-03-06           8           3.1;2
		2018-03-07         1;1             8;1
		
	"Frame-2":
		                  col1            col2
		2018-03-05         2.1               8
		2018-03-06           8             5.1
		2018-03-07           2               9
		
}
"answer": {
	"desc": %s You can use for processes each column with , cast to and per columns: Or process each value separately by : EDIT: If numeric with strings columns is possible use for exclude numeric and working only with s columns with : 
	"code-snippets": [
		df = df.apply(lambda x: x.str.split(';', expand=True).astype(float).sum(axis=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50336251
"link": https://stackoverflow.com/questions/50336251/split-dataframe-entries-at-midnight
"question": {
	"title": split dataframe entries at midnight
	"desc": I have a , with and datatime. and can be expected to be sorted interally, but gaps/overlaps may occur between consecutive rows. I would like to create a new dataframe with the difference that if row contains midnight (e.g. midnight is contained in [,]), the row is then split in two parts before and after midnight ex: should be 
}
"io": {
	"Frame-1": 
		 Start                 End
		0 2010-02-01 00:00:00 2010-02-01 04:00:00
		1 2010-02-01 05:03:00 2010-02-01 09:03:00
		2 2010-02-01 10:06:00 2010-02-01 14:06:00
		3 2010-02-01 15:09:00 2010-02-01 19:09:00
		4 2010-02-01 20:12:00 2010-02-02 00:12:00
		5 2010-02-02 01:15:00 2010-02-02 05:15:00
		
	"Frame-2":
		Start                 End
		    0 2010-02-01 00:00:00 2010-02-01 04:00:00
		    1 2010-02-01 05:03:00 2010-02-01 09:03:00
		    2 2010-02-01 10:06:00 2010-02-01 14:06:00
		    3 2010-02-01 15:09:00 2010-02-01 19:09:00
		    -----------------------------------------
		    4 2010-02-01 20:12:00 2010-02-01 23:59:00
		    5 2010-02-02 00:00:00 2010-02-02 00:12:00
		    -----------------------------------------
		    6 2010-02-02 01:15:00 2010-02-02 05:15:00
		
}
"answer": {
	"desc": %s You can concat the DataFrame of new pairs, then erase the old ones. First find the splits: Now concatenate and drop: 
	"code-snippets": [
		splits = df[df.End.dt.date > df.Start.dt.date].copy()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50269584
"link": https://stackoverflow.com/questions/50269584/append-list-from-pandas-column-to-python-list
"question": {
	"title": Append list from pandas column to python list
	"desc": I have values in a list in pandas column, for example: df But when I append col1 to list I got quote around the first element in each sublist. And I got: But I need: 
}
"io": {
	"Frame-1": 
		id       col1
		1       [51.97559, 4.12565]
		2       [52.97559, 3.12565]
		3       [49.97559, 5.12565]
		
	"Frame-2":
		[[51.97559, 4.12565]
		[52.97559, 3.12565]
		[49.97559, 5.12565]]
		
}
"answer": {
	"desc": %s You can cast all the elements of the list into 
	"code-snippets": [
		new_list = []
		for val in df['col1'].values:
		    new_list.append(list(map(float, val)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50152137
"link": https://stackoverflow.com/questions/50152137/how-to-extract-a-2d-array-encoded-in-a-list-of-strings-in-a-pandas-dataframe
"question": {
	"title": how to extract a 2D array encoded in a list of strings in a pandas dataframe?
	"desc": I have messed up a dataframe. I have a columns which contain strings which encode a list of numbers e.g. EDIT: actually, the commas are missing as well Each of the strings encodes a list with a fixed number of elements. I would like to convert this into 3 (in general N, where each of them numeric, containing one element from the original list in mycol I have tried the following, without success 
}
"io": {
	"Frame-1": 
		df=
		                                    mycol
		0   '[ 0.5497076,   0.59722222,  0.42361111]'  
		1   '[ 0.8030303,   0.69090909,  0.52727273]'  
		2   '[ 0.51461988,  0.38194444,  0.66666667]'
		
	"Frame-2":
		df=
		                                    mycol
		0   '[ 0.5497076   0.59722222  0.42361111]'  
		1   '[ 0.8030303   0.69090909  0.52727273]'  
		2   '[ 0.51461988  0.38194444  0.66666667]'
		
}
"answer": {
	"desc": %s This should help. Ex: Output: 
	"code-snippets": [
		import pandas as pd
		df = pd.DataFrame({"mycol": ['[ 0.5497076   0.59722222  0.42361111]', '[ 0.8030303   0.69090909  0.52727273]']})
		df[['mycol1','mycol2','mycol3']]  = df["mycol"].apply(lambda x: x.replace("[", "").replace("]", "").split()).apply(pd.Series)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50150090
"link": https://stackoverflow.com/questions/50150090/pandas-create-dataframe-from-data-and-column-order
"question": {
	"title": Pandas: Create dataframe from data and column order
	"desc": what i'm asking must be something very easy, but i honestly can't see it.... :( I have an array, lets say and i want to put it in a dataframe. I do aiming for: but i am getting: (notice the discrepancy between column names and data) I know i can re-arrange the column names order in the dataframe creation, but i'm trying to understand how it works. Am i doing something wrong, or it's normal behaviour? (why though?) 
}
"io": {
	"Frame-1": 
		col1 col2 col3
		1    2    3
		4    5    6
		7    8    9
		10   11   12
		
	"Frame-2":
		col3 col1 col2
		1    2    3
		4    5    6
		7    8    9
		10   11   12
		
}
"answer": {
	"desc": %s You are are using a of columns, which is NOT an ordered collection (neither are dictionaries). Try with a , o simply a 
	"code-snippets": [
		df = pd.Dataframe(data, columns=['col1', 'col2', 'col3'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50067957
"link": https://stackoverflow.com/questions/50067957/saving-a-pandas-dataframe-in-fixed-format-with-different-column-widths
"question": {
	"title": Saving a Pandas dataframe in fixed format with different column widths
	"desc": I have a pandas dataframe (df) that looks like this: I want to save this dataframe in a fixed format. The fixed format I have in mind has different column width and is as follows: "one space for column A's value then a comma then four spaces for column B's values and a comma and then five spaces for column C's values" Or symbolically: My dataframe above (df) would look like the following in my desired fixed format: How can I write a command in Python that saves my dataframe into this format? 
}
"io": {
	"Frame-1": 
		   A   B  C
		0  1  10  1234
		1  2  20  0
		
	"Frame-2":
		1,  10, 1234
		2,  20,    0
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		df['B'] = df['B'].apply(lambda t: (' '*(4-len(str(t)))+str(t)))
		df['C'] = df['C'].apply(lambda t: (' '*(5-len(str(t)))+str(t)))
		df.to_csv('path_to_file.csv', index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 50003791
"link": https://stackoverflow.com/questions/50003791/removing-random-rows-from-a-data-frame-until-count-is-equal-some-criteria
"question": {
	"title": Removing random rows from a data frame until count is equal some criteria
	"desc": I have a dataframe with data that I feed to a ML library in python. The data I have is categorized into 5 different tasks, t1,t2,t3,t4,t5. The data I have right now for every task is uneven, to simplify things here is an example. In the case above, I want to remove random rows with the task label of "t1" until there is an equal amount of "t1" as there is "t2" So after the code is run, it should look like this: What is the most clean way to do this? I could of course just do for loops and if conditions and use random numbers and count the occurances for each iteration, but that solution would not be very elegant. Surely there must be a way using functions of dataframe? So far, this is what I got: 
}
"io": {
	"Frame-1": 
		task, someValue
		t1,   XXX
		t1,   XXX
		t1,   XXX
		t1,   XXX
		t2,   XXX
		t2,   XXX
		
	"Frame-2":
		task, someValue
		t1,   XXX
		t1,   XXX
		t2,   XXX
		t2,   XXX
		
}
"answer": {
	"desc": %s You can calculate the smallest number of tasks in your dataFrame, and then use + to get the top N rows per task. 
	"code-snippets": [
		v = df['task'].value_counts().min()
		df = df.groupby('task', as_index=False).head(v)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49925888
"link": https://stackoverflow.com/questions/49925888/pandas-get-the-least-number-of-records-so-all-columns-have-at-least-one-non-nu
"question": {
	"title": Pandas : Get the least number of records so all columns have at least one non null value
	"desc": I have a dataframe with 62 columns that are largely null. Some records have multiple columns with non-null values, others just a single non-null. I'm wondering if there's a way to use .dropna or other strategy to return the least number of rows with each column having at least one non-null value. For a simplified example Would return ... 
}
"io": {
	"Frame-1": 
		       a          b         c
		      NaN        1         NaN
		      1          NaN       NaN
		      NaN        NaN       NaN
		      NaN        1         1
		
	"Frame-2":
		      a          b         c
		      1          NaN       NaN
		      NaN        1         1
		
}
"answer": {
	"desc": %s Here's a simple greedy solution that should do the job but won't guarantee you have the lowest number of rows (as @chthonicdaemon said the problem is NP-hard) Out: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		# sample dataframe
		df = pd.DataFrame({'a':[np.nan,1,np.nan,np.nan],'b':[1, np.nan, np.nan, 1],'c':[np.nan, np.nan, np.nan, 1]})
		df_orig = df
		cols = df.columns.tolist()
		
		rows = []
		while not df.empty:
		    ## Find the row with most non-null column entries
		    x = df.notnull().sum(axis=1).idxmax() # edit - fix for null/nonnull
		    ## Add the row to our list and continue
		    rows.append(x)
		    ## Remove the columns from our dataframe
		    df = df.drop(columns=df.columns[df.loc[[x]].notnull().any()].tolist())
		
		## Access the dataframe with only 'essential' rows
		df_orig.loc[rows]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49885060
"link": https://stackoverflow.com/questions/49885060/cross-reference-list-of-ids-to-index
"question": {
	"title": Cross reference list of ids to index
	"desc": I have grouped together a list of ids that are associated with a certain value and placed all these lists of ids into a dataframe. It looks like this: (with index = id) I want to iterate through these lists and cross reference them to the id index where phase equals either a 2 or 3, then just keep the ids that match within the original list (or if not possible, create a new column with modified lists). Something like this below: If possible I'd like to do this within the dataframe object as there are multiple features/dependencies for each row. Any tips on how to go about this? My actual data: And the dtypes: And the good_ids output: 
}
"io": {
	"Frame-1": 
		    phase  list_ids
		id  
		a1  1      [a1,a2,c3] 
		a2  3      [a1,b2,c3]  
		b1  3      [a2,b2] 
		b2  2      [b1,b2,c1] 
		b3  3      [b2,c1] 
		c1  1      [a1,a2,c3] 
		c2  1      [a1,b1,c4] 
		c3  2      [c1,c2,c4] 
		c4  1      [c1,c2]
		
	"Frame-2":
		               phase  ids
		Study_id             
		ACP-103-006    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... 
		ACP-103-008    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10...  
		ACP-103-010    2.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  
		ACP-103-012    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  
		ACP-103-014    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...   
		
}
"answer": {
	"desc": %s Assuming that each element in the column is a list of strings, you could do the following: First get a of the "good" (where phase is 2 or 3): Next use to filter the using : 
	"code-snippets": [
		good_ids = set(df[df["phase"].isin([2,3])].index)
		print(good_ids)
		#{'a2', 'b1', 'b2', 'b3', 'c3'}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49073547
"link": https://stackoverflow.com/questions/49073547/sum-values-in-third-column-while-putting-together-corespondinng-values-in-first
"question": {
	"title": Sum values in third column while putting together corespondinng values in first and second columns
	"desc": I have 3 columns of data. I have data stored in three columns (k, v, t) in csv. For instance, Data: I want to get as the following data. Basically, sum all the values of t that has the same k and v. this is the code I have so far: and it keeps going until the end. I use "for loop" and "if" but it is too long. Can I use numpy in a short and clean way? or any other better way? 
}
"io": {
	"Frame-1": 
		k v t    
		
		a 1 2    
		b 2 3    
		c 3 4    
		a 2 3    
		b 3 2    
		b 3 4    
		c 3 5    
		b 2 3
		
	"Frame-2":
		a 1 5
		b 2 6
		b 3 6
		c 3 9
		
}
"answer": {
	"desc": %s Here is one solution using . First create a dataframe, then perform a operation. The below code assumes your data is stored in a csv file. Result 
	"code-snippets": [
		df = pd.read_csv('file.csv')
		
		g = df.groupby(['k', 'v'], as_index=False)['t'].sum()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49352279
"link": https://stackoverflow.com/questions/49352279/calculating-the-duration-an-event-in-a-time-series-python
"question": {
	"title": Calculating the duration an event in a time series python
	"desc": I have a dataframe as show below: The index is datetime and have column record the rainfall value(unit:mm) in each hour,I would like to calculate the "Average wet spell duration", which means the average of continuous hours that exist values (not zero) in a day, so the calculation is and the "average wet spell amount", which means the average of sum of the values in continuous hours in a day. The datafame above is just a example, the dataframe which I have have more longer time series (more than one year for example), how can I write a function so it could calculate the two value mentioned above in a better way? thanks in advance! P.S. the values may be NaN, and I would like to just ignore it. 
}
"io": {
	"Frame-1": 
		2 + 4 + 1 + 1 + 2 + 5 / 6 (events) = 2.5 (hr)
		
	"Frame-2":
		{ (14.5 + 15.8) + ( 13.6 + 4.3 + 13.7 + 14.4 ) + (17.2) + (5.3) + (2 + 4)+ (3.9 + 7.2 + 1 + 1 + 10) } /  6 (events) = 21.32 (mm)
		
}
"answer": {
	"desc": %s I believe this is what you are looking for. I have added explanations to the code for each step. Result 
	"code-snippets": [
		# create helper columns defining contiguous blocks and day
		df['block'] = (df['value'].astype(bool).shift() != df['value'].astype(bool)).cumsum()
		df['day'] = df['index'].dt.normalize()
		
		# group by day to get unique block count and value count
		session_map = df[df['value'].astype(bool)].groupby('day')['block'].nunique()
		hour_map = df[df['value'].astype(bool)].groupby('day')['value'].count()
		
		# map to original dataframe
		df['sessions'] = df['day'].map(session_map)
		df['hours'] = df['day'].map(hour_map)
		
		# calculate result
		res = df.groupby(['day', 'hours', 'sessions'], as_index=False)['value'].sum()
		res['duration'] = res['hours'] / res['sessions']
		res['amount'] = res['value'] / res['sessions']
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49857470
"link": https://stackoverflow.com/questions/49857470/replace-value-in-pandas-dataframe-based-on-condition
"question": {
	"title": Replace value in Pandas Dataframe based on condition
	"desc": I have a dataframe column with some numeric values. I want that these values get replaced by 1 and 0 based on a given condition. The condition is that if the value is above the mean of the column, then change the numeric value to 1, else set it to 0. Here is the code I have now: The target is the dataframe y. y is like so: and so on. mean_y is equal to 3.55. Therefore, I need that all values greater than 3.55 to become ones, and the rest 0. I applied this loop, but without success: The output is the following: What am I doing wrong? Can someone please explain me the mistake? Thank you! 
}
"io": {
	"Frame-1": 
		      0
		0    16
		1    13
		2    12.5
		3    12
		
	"Frame-2":
		      0
		0    16
		1    13
		2    0
		3    12
		
}
"answer": {
	"desc": %s Try this vectorized approach: 
	"code-snippets": [
		dataset.myCol = np.where(dataset.myCol > dataset.myCol.mean(), 1, 0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 45970751
"link": https://stackoverflow.com/questions/45970751/shift-nans-to-the-end-of-their-respective-rows
"question": {
	"title": Shift NaNs to the end of their respective rows
	"desc": I have a DataFrame like : What I want to get is This is my approach as of now. Is there any efficient way to achieve this ? Here is way to slow . Thank you for your assistant!:) My real data size 
}
"io": {
	"Frame-1": 
		     0    1    2
		0  0.0  1.0  2.0
		1  NaN  1.0  2.0
		2  NaN  NaN  2.0
		
	"Frame-2":
		Out[116]: 
		     0    1    2
		0  0.0  1.0  2.0
		1  1.0  2.0  NaN
		2  2.0  NaN  NaN
		
}
"answer": {
	"desc": %s Here's a NumPy solution using - If you want to save memory, assign it back instead - 
	"code-snippets": [
		df[:] = justify(df.values, invalid_val=np.nan, axis=1, side='left')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49717720
"link": https://stackoverflow.com/questions/49717720/how-to-append-multiple-columns-into-two
"question": {
	"title": How to append multiple columns into two?
	"desc": The data I am working with is in a large set of columns, with related values - for example: In order to join this data with another data set, I need to append these values into one column, preserving the column data, as well as the data: I've tried using and , but am so far unable to get the required result .. which pandas function should I be using here? 
}
"io": {
	"Frame-1": 
		| YearQ  | Area A | Area B | Area C |
		+--------+--------+--------+--------+
		| 2017Q1 | 1234.0 | 9252.0 | 3421.0 |
		| 2017Q2 | 1245.0 | 9368.0 | 3321.0 |
		| 2017Q3 | 1350.0 | 9440.0 | 3225.0 |
		| 2017Q4 | 1333.0 | 9501.0 | 3625.0 |
		
	"Frame-2":
		| YearQ  |  Area  |  Value  |
		+--------+--------+---------+
		| 2017Q1 | Area A | 1234.0  |
		| 2017Q1 | Area B | 9252.0  |
		| 2017Q1 | Area C | 3421.0  |
		| 2017Q2 | Area A | 1245.0  |
		| 2017Q2 | Area B | 9368.0  |
		| 2017Q2 | Area C | 3321.0  |
		
}
"answer": {
	"desc": %s Use with : A bit slowier alternative with , and : 
	"code-snippets": [
		df = df.melt('YearQ', var_name='Area', value_name='Value').sort_values(['YearQ','Area'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49715104
"link": https://stackoverflow.com/questions/49715104/dynamically-accessing-subset-of-pandas-dataf-rame-perform-calculation-and-write
"question": {
	"title": Dynamically accessing subset of pandas dataf rame, perform calculation and write to new data frame
	"desc": I have a very large data frame from which I would like to pull a subsample, perform some calculation and then write these results into a new data frame. For the sample, please consider: returning this: Now I would like "extract" always 3 rows, rolling from the beginning and calculate the averages (as an example, other calculations would work too) of each column: the result data frame is then How can I do that? 
}
"io": {
	"Frame-1": 
		    a   b   c   d   e
		0   1   9   0   3   0
		1   5   4   1   0   3
		2   9   3   6   3   5
		3   6   2   5   9   7
		4   9   0   7   9   5
		
	"Frame-2":
		df_1
		    a   b   c   d   e
		0   1   9   0   3   0
		1   5   4   1   0   3
		2   9   3   6   3   5
		
		df_2 
		    a   b   c   d   e
		1   5   4   1   0   3
		2   9   3   6   3   5
		3   6   2   5   9   7
		
		df_3 
		    a   b   c   d   e
		2   9   3   6   3   5
		3   6   2   5   9   7
		4   9   0   7   9   5
		
}
"answer": {
	"desc": %s Use and remove first s rows by or : If need also of first, first + second rows add parameter : EDIT: Manual aproach should be create one line s and then join all together: 
	"code-snippets": [
		N = 3
		df = df.rolling(N).mean().iloc[N-1:]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49274575
"link": https://stackoverflow.com/questions/49274575/merge-2-csv-files-with-mapped-values-in-another-file-separated-by-comma
"question": {
	"title": Merge 2 CSV files with mapped values in another file separated by comma
	"desc": here is my problem: I have tow csv files as follows: Book1.csv Book2.csv I want merge above files and get an output file as this: The code I am using right now is: what I get from this is : All the Attributes and Products are merged correctly. But what I want is merge Attibutes into one string and separate by comma (not line by line). How do I do this? Thank you in advance! 
}
"io": {
	"Frame-1": 
		Id  Product
		0   aaaa
		1   bbbb
		2   cccc
		3   dddd
		
	"Frame-2":
		Id  Attribute
		0   aaad
		0   sssd
		1   fffd
		1   gggd
		1   cccd
		2   bbbd
		3   hhhd
		3   bbbd
		
}
"answer": {
	"desc": %s This is one way. Result If you want to just combine to , you can use this instead, though it won't print nicely: Explanation Group Attributes by Id and aggregate to list. Map to column in via . 
	"code-snippets": [
		g = df2.groupby('Id')['Attribute'].apply(', '.join)
		df1['Attributes'] = df1['Id'].map(g)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49525337
"link": https://stackoverflow.com/questions/49525337/python-pandas-how-to-fast-process-the-value-in-columns
"question": {
	"title": Python pandas: how to fast process the value in columns
	"desc": Hi there is a dataframe like the following dataframes df1. The data type is string. I want to get the dataframe like the following dataframe. The eye data is divided into eye_x, eye_y, the other columns is the same, the data type is float. Until now I know how to get the (x, y) value together with the following code: 
}
"io": {
	"Frame-1": 
		 eye_x   eye_y    nose_x   nose_y     mouse_x  mouse_y     ear_x   ear_y        
		    34       35       45       66         45        64        78       87
		    35       38       75       76         95        37        38       79
		    64       43       85       66         65        45        87       45
		
	"Frame-2":
		 eye           nose       mouse       ear
		  (34, 35)      (45,66)    (45,64)     (78,87)
		  (35, 38)      (75,76)    (95,37)     (38,79)
		  (64, 43)      (85,66)    (65,45)     (87,45)
		
}
"answer": {
	"desc": %s You may try ing and ing again. 
	"code-snippets": [
		v = df.stack().str.split('_', expand=True).iloc[:, :-1]
		v.columns = ['x', 'y']
		
		v = v.unstack().swaplevel(0, 1, axis=1)
		v.columns = v.columns.map('_'.join)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49513688
"link": https://stackoverflow.com/questions/49513688/check-if-string-contains-sub-string-from-the-same-column-in-pandas-dataframe
"question": {
	"title": check if string contains sub string from the same column in pandas dataframe
	"desc": Hi I have the following dataframe: i want to check for the strings that contain sub string from this column and create a new column that holds the bigger strings if the condition is full filled something like this: Thanks in advance 
}
"io": {
	"Frame-1": 
		> df1
		  col1  
		0 donald     
		1 mike
		2 donald trump
		3 trump
		4 mike pence
		5 pence
		6 jarred
		
	"Frame-2":
		> df1
		  col1           col2
		0 donald        donald trump
		1 mike          mike pence
		2 donald trump  donald trump
		3 trump         donald trump
		4 mike pence    mike pence
		5 pence         mike pence
		6 jarred        jarred
		
}
"answer": {
	"desc": %s This should do it: 
	"code-snippets": [
		df['Col2'] = df['Col1'].apply(lambda x: max([i for i in df['Col1'] if x in i], key=len))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49499352
"link": https://stackoverflow.com/questions/49499352/insert-characters-at-multiple-positions-in-a-dataframe-column
"question": {
	"title": Insert characters at multiple positions in a dataframe column
	"desc": I have a table with a lot of rows, and i need to change all the first " " space for "h" the second space for "m" and add at last char "s" I have made this code, but for my use is getting a slow. With this code I change my data frame from: To This: Is there any function that i can do this? I tried using df.replace, but it replaces all " " in the table... thanks for now 
}
"io": {
	"Frame-1": 
		     DEC             RA
		+26 01 09.559  11 50 10.4747 
		+26 01 10.770  11 50 10.2641  
		+26 01 11.980  11 50 10.0534 
		+26 01 13.191  11 50 09.8428
		
	"Frame-2":
		         DEC              RA
		0  +26d01m09.559s  11h50m10.4747s
		1  +26d01m10.770s  11h50m10.2641s
		2  +26d01m11.980s  11h50m10.0534s
		3  +26d01m13.191s  11h50m09.8428s
		
}
"answer": {
	"desc": %s Define a function to handle splitting and recombination. You may use , followed by some super simple string concatenation, respectively. Now, call it with your appropriate parameters. 
	"code-snippets": [
		def split_combine(v, letters=list('dms')):
		    v = v.str.split(expand=True)
		    return (
		       v[0] + letters[0] 
		     + v[1] + letters[1] 
		     + v[2] + letters[2]
		    )
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49493720
"link": https://stackoverflow.com/questions/49493720/merging-combining-dataframes-in-pandas
"question": {
	"title": Merging/Combining Dataframes in Pandas
	"desc": I have a df1, example: ,and a df2, example: The column and row 'C' is common in both dataframes. I would like to combine these dataframes such that I get, Is there an easy way to do this? pd.concat and pd.append do not seem to work. Thanks! Edit: df1.combine_first(df2) works (thanks @jezarel), but can we keep the original ordering? 
}
"io": {
	"Frame-1": 
		    C    E    D
		C        2    3
		E             1
		D   2
		
	"Frame-2":
		    B    A    C    D    E
		B        1
		A             1
		C   2              2    3
		D                       1
		E   2  
		
}
"answer": {
	"desc": %s There is problem always sorted columns namd index, so need with combine columns names: More general solution: 
	"code-snippets": [
		c = df1.columns.append(df2.columns).unique()
		i = df1.index.append(df2.index).unique()
		
		df = df1.combine_first(df2).reindex(index=i, columns=c)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49367519
"link": https://stackoverflow.com/questions/49367519/select-consecultive-times-in-a-dataframe
"question": {
	"title": Select consecultive times in a dataframe
	"desc": I have a data frame where I would like to select the consecutive timestamp. I mean times that happen one after the other, in this case, that happened 15 minutes consecutively. For example, I would select only those from the above dataframe I have This is just an example but I am dealing with many timestamps. How can I do this with python commands? 
}
"io": {
	"Frame-1": 
		2017-07-19 17:45:00+02:00    16
		2017-07-23 02:45:00+02:00    23
		2017-07-25 14:15:00+02:00    23
		2017-07-27 07:00:00+02:00    25
		2017-07-28 09:30:00+02:00    22
		2017-07-28 18:00:00+02:00    17
		2017-07-29 04:00:00+02:00    28
		2017-07-29 04:15:00+02:00    19
		2017-07-29 11:30:00+02:00    20
		2017-07-30 09:00:00+02:00    11
		2017-08-03 02:45:00+02:00    22
		2017-08-04 06:45:00+02:00    27
		2017-08-06 01:45:00+02:00    21
		2017-08-08 19:30:00+02:00    27
		2017-08-08 19:45:00+02:00    27
		2017-08-08 20:00:00+02:00    15
		2017-08-08 21:45:00+02:00    25
		
	"Frame-2":
		2017-07-29 04:00:00+02:00    28
		2017-07-29 04:15:00+02:00    19
		2017-08-08 19:30:00+02:00    27
		2017-08-08 19:45:00+02:00    27
		2017-08-08 20:00:00+02:00    15 
		
}
"answer": {
	"desc": %s You could 
	"code-snippets": [
		In [205]: df.time.diff().dt.total_seconds().eq(900)
		Out[205]:
		0     False
		1     False
		2     False
		3     False
		4     False
		5     False
		6     False
		7      True
		8     False
		9     False
		10    False
		11    False
		12    False
		13    False
		14     True
		15     True
		16    False
		Name: time, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 43828124
"link": https://stackoverflow.com/questions/43828124/how-can-i-change-the-structure-of-a-dataframe
"question": {
	"title": How can I change the structure of a dataframe?
	"desc": How can I change the structure of a dataframe? I need series data of each row. I tried unstack but failed. Example dataframe: Output Series: 
}
"io": {
	"Frame-1": 
		    df:
		
		        c1   c2     c3
		
		   0     a     b     c
		   1     d     e     f
		
	"Frame-2":
		S1 
		0   a
		1   b
		2   c
		
		S2
		0   d
		1   e
		2   f
		
}
"answer": {
	"desc": %s You can transpose the data frame, and, if you so like, rename the indices, to a range: 
	"code-snippets": [
		dfT = df.T
		dfT.index = pd.RangeIndex(dfT.shape[0]) # Rename the indicies
		dfT.columns=['S{}'.format(i+1) for i in range(dfT.shape[1])] # Rename the columns
		dfT.unstack()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49352824
"link": https://stackoverflow.com/questions/49352824/python-pandas-delete-every-row-after-specific-value-in-cell
"question": {
	"title": Python Pandas delete every row after specific value in cell
	"desc": I have a Pandas Dataframe that looks as follows: I want to delete every row after the first in the column. The result should look like this: 
}
"io": {
	"Frame-1": 
		    streak 
		0      1.0 
		1      2.0 
		2      0.0 
		3      1.0 
		4      2.0 
		5      0.0 
		6      0.0 
		
	"Frame-2":
		    streak 
		0      1.0 
		1      2.0 
		
}
"answer": {
	"desc": %s Get index of first by and slice by , only need default unique indices: Detail: EDIT: For more general solution is necessary use - get position by : 
	"code-snippets": [
		print (df)
		   streak
		a     1.0
		b     2.0
		c     0.0
		d     1.0
		e     2.0
		f     0.0
		g     0.0
		
		df = df.iloc[:df['streak'].eq(0).values.argmax()]
		print (df)
		   streak
		a     1.0
		b     2.0
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49358261
"link": https://stackoverflow.com/questions/49358261/compare-2-consecutive-cells-in-a-dataframe
"question": {
	"title": Compare 2 consecutive cells in a dataframe
	"desc": I have a dataframe (more than 150 rows and 16 columns) with like this: What I would like is to have only the last numbers per column before the 0 in a following row: I started to try with , but then I got stucked Can anyone help in this direction or with any other solution? Thanks in advance 
}
"io": {
	"Frame-1": 
		              a001          a002          a003        a004         a005  
		Year Week                                                                    
		2017  1          0            1            1            3            0   
		      2          1            2            2            4            0   
		      3          2            0            3            5            0   
		      4          0            0            4            0            0   
		      5          0            1            5            0            0   
		      6          0            2            6            1            0   
		      7          0            0            7            2            0   
		      8          1            0            0            3            0   
		      9          2            0            0            0            0   
		     10          3            2            0            0            0  
		
	"Frame-2":
		              a001          a002          a003        a004         a005  
		Year Week                                                                    
		2017  1          0            0            0            0            0   
		      2          0            0            0            0            0   
		      3          0            2            0            0            0   
		      4          2            0            0            5            0   
		      5          0            0            0            0            0   
		      6          0            0            0            0            0   
		      7          0            2            0            0            0   
		      8          0            0            7            0            0   
		      9          0            0            0            3            0   
		     10          0            0            0            0            0  
		
}
"answer": {
	"desc": %s I think need compare 2 consecutive , replace another values to by , , convert s to by and last to : EDIT: Thanks @Joe for simplify code: 
	"code-snippets": [
		df1 = df.where((df != 0) & (df.shift(-1) == 0)).shift().fillna(0).astype(int)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49324988
"link": https://stackoverflow.com/questions/49324988/pandas-sum-the-differences-between-two-columns-in-each-group
"question": {
	"title": pandas sum the differences between two columns in each group
	"desc": I have a looks like, the of and are , and are of ; I like to and and get the differences between and , but I don't know how to sum such differences in each group and assign the values to a new column say , possibly in a new , 
}
"io": {
	"Frame-1": 
		A               B              C    D
		2017-10-01      2017-10-11     M    2017-10
		2017-10-02      2017-10-03     M    2017-10
		2017-11-01      2017-11-04     B    2017-11
		2017-11-08      2017-11-09     B    2017-11
		2018-01-01      2018-01-03     A    2018-01
		
	"Frame-2":
		C    D          E
		M    2017-10    11
		M    2017-10    11
		B    2017-11    4
		B    2017-11    4
		A    2018-01    2
		
}
"answer": {
	"desc": %s Base on you code 
	"code-snippets": [
		df.merge(df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A']).sum(level=[0,1]).reset_index())
		Out[292]: 
		           A          B  C        D       0
		0 2017-10-01 2017-10-11  M  2017-10 11 days
		1 2017-10-02 2017-10-03  M  2017-10 11 days
		2 2017-11-01 2017-11-04  B  2017-11  4 days
		3 2017-11-08 2017-11-09  B  2017-11  4 days
		4 2018-01-01 2018-01-03  A  2018-01  2 days
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 49301344
"link": https://stackoverflow.com/questions/49301344/pass-different-columns-in-pandas-dataframe-in-a-custom-function-in-df-apply
"question": {
	"title": Pass Different Columns in Pandas DataFrame in a Custom Function in df.apply()
	"desc": Say I have a dataframe : I wanna have two new columns that are x * y and x * z: So I define a function (just for example) that takes either the string or the string as an argument to indicate which column I want to multiply with the column x: And apply the function to the dataframe : Apparently it is wrong here because I didn't specify the , or . Question is, just takes function name, how do I tell it to take the two arguments? 
}
"io": {
	"Frame-1": 
		  x y z
		0 1 2 3
		1 4 5 6
		2 7 8 9
		
	"Frame-2":
		  x y z xy xz
		0 1 2 3  2  3
		1 4 5 6 20 24
		2 7 8 9 56 63
		
}
"answer": {
	"desc": %s You can use lambda function with specify columns, but also is necessary change : If is not possible change : 
	"code-snippets": [
		def func(row, colName):
		    return row * colName
		
		cols = ['y', 'z']
		for c in cols:
		    df['x' + c] = df.apply(lambda x: func(x['x'], x[c]), axis=1)
		
		----------------------------------------------------------------------
		def func(row, colName):
		    return row['x'] * row[colName]
		
		cols = ['y', 'z']
		for c in cols:
		    df['x' + c] = df.apply(lambda x: func(x, c), axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 48563186
"link": https://stackoverflow.com/questions/48563186/python-dataframe-get-the-nan-columns-for-each-row
"question": {
	"title": Python Dataframe get the NaN columns for each row
	"desc": I have a pandas dataframe which look like the following: I would like to add a column that gives me something like a summary of Null values. So I need a command which gives me for every row which columns are NULL. Something like this: I could not find anything which satisfies my need on the internet. 
}
"io": {
	"Frame-1": 
		  a          b       c     
		 NaN         2       165     
		 NaN         9       NaN     
		 NaN         NaN     NaN    
		  15        15       NaN      
		   5         NaN     11 
		
	"Frame-2":
		  a          b       c     Summary
		 NaN         2       165      a
		 NaN         9       NaN     a + c
		 NaN         NaN     NaN    a + b + c
		  15        15       NaN      c      
		   5         NaN     11       b
		
}
"answer": {
	"desc": %s Here is one way. 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = pd.DataFrame([[np.nan, 2, 165], [np.nan, 9, np.nan], [np.nan, np.nan, np.nan],
		                   [15, 15, np.nan], [5, np.nan, 11]], columns=['a', 'b', 'c'])
		
		df['Errors'] = df.apply(lambda row: ' + '.join(i for i in ['a', 'b', 'c'] if np.isnan(row[i])), axis=1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


