[
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 6234,
            "user_id": 766708,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/55fd0f7d95a4938975026ff886c3a563?s=128&d=identicon&r=PG",
            "display_name": "daiyue",
            "link": "https://stackoverflow.com/users/766708/daiyue"
        },
        "is_answered": true,
        "view_count": 131,
        "accepted_answer_id": 58085118,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1569364530,
        "creation_date": 1569344336,
        "question_id": 58085046,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58085046/pandas-create-a-column-and-assign-values-to-it-from-a-dictionary",
        "title": "pandas create a column and assign values to it from a dictionary",
        "body": "<p>I have a dictionary looks like this,</p>\n\n<pre><code>{\"regions\":[\n   {\"name\": \"South America\", \"code\": \"SA01,SA02,SA03\"},\n   {\"name\": \"Asia Pacific\", \"code\": \"AP01,AP02,AP03\"}\n]}\n</code></pre>\n\n<p>I have a df looks like this,</p>\n\n<pre><code>id    code\n1     SA01\n2     SA02\n3     SA03\n4     AP01\n5     AP02\n6     AP03\n</code></pre>\n\n<p>I like to create a column <code>region</code> in <code>df</code> whose values will be based on the <code>code</code> values in <code>regions</code>, so the result will look like,</p>\n\n<pre><code>id    code    region\n1     SA01    South America\n2     SA02    South America\n3     SA03    South America\n4     AP01    Asia Pacific\n5     AP02    Asia Pacific\n6     AP03    Asia Pacific\n</code></pre>\n\n<p>I am wondering whats the best way to do this.</p>\n",
        "answer_body": "<p>You could redefine your dictionary (<code>d</code> here) to have an individual <code>code:region</code> entry for each code that appears in the strings and use it to map the values in the <code>code</code> column:</p>\n\n<pre><code>d_ = {code:sd['name'] for sd in d['regions'] for code in sd['code'].split(',')} \n# {'SA01': 'South America', 'SA02': 'South America', 'SA03': 'South America',...\ndf['region'] = df.code.map(d_)\n</code></pre>\n\n<hr>\n\n<pre><code>print(df)\n\n   id  code         region\n0   1  SA01  South America\n1   2  SA02  South America\n2   3  SA03  South America\n3   4  AP01   Asia Pacific\n4   5  AP02   Asia Pacific\n5   6  AP03   Asia Pacific\n</code></pre>\n",
        "question_body": "<p>I have a dictionary looks like this,</p>\n\n<pre><code>{\"regions\":[\n   {\"name\": \"South America\", \"code\": \"SA01,SA02,SA03\"},\n   {\"name\": \"Asia Pacific\", \"code\": \"AP01,AP02,AP03\"}\n]}\n</code></pre>\n\n<p>I have a df looks like this,</p>\n\n<pre><code>id    code\n1     SA01\n2     SA02\n3     SA03\n4     AP01\n5     AP02\n6     AP03\n</code></pre>\n\n<p>I like to create a column <code>region</code> in <code>df</code> whose values will be based on the <code>code</code> values in <code>regions</code>, so the result will look like,</p>\n\n<pre><code>id    code    region\n1     SA01    South America\n2     SA02    South America\n3     SA03    South America\n4     AP01    Asia Pacific\n5     AP02    Asia Pacific\n6     AP03    Asia Pacific\n</code></pre>\n\n<p>I am wondering whats the best way to do this.</p>\n",
        "formatted_input": {
            "qid": 58085046,
            "link": "https://stackoverflow.com/questions/58085046/pandas-create-a-column-and-assign-values-to-it-from-a-dictionary",
            "question": {
                "title": "pandas create a column and assign values to it from a dictionary",
                "ques_desc": "I have a dictionary looks like this, I have a df looks like this, I like to create a column in whose values will be based on the values in , so the result will look like, I am wondering whats the best way to do this. "
            },
            "io": [
                "id    code\n1     SA01\n2     SA02\n3     SA03\n4     AP01\n5     AP02\n6     AP03\n",
                "id    code    region\n1     SA01    South America\n2     SA02    South America\n3     SA03    South America\n4     AP01    Asia Pacific\n5     AP02    Asia Pacific\n6     AP03    Asia Pacific\n"
            ],
            "answer": {
                "ans_desc": "You could redefine your dictionary ( here) to have an individual entry for each code that appears in the strings and use it to map the values in the column: ",
                "code": [
                    "d_ = {code:sd['name'] for sd in d['regions'] for code in sd['code'].split(',')} \n# {'SA01': 'South America', 'SA02': 'South America', 'SA03': 'South America',...\ndf['region'] = df.code.map(d_)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization",
            "data-manipulation"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 7879423,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://lh3.googleusercontent.com/-6RzJHu_Ow0Q/AAAAAAAAAAI/AAAAAAAAAmo/SLYcUg5ZNZQ/photo.jpg?sz=128",
            "display_name": "siddhesh tiwari",
            "link": "https://stackoverflow.com/users/7879423/siddhesh-tiwari"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 58086302,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1569356133,
        "creation_date": 1569349230,
        "question_id": 58086144,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58086144/replicate-multiple-rows-of-events-for-specific-ids-multiple-times",
        "title": "Replicate multiple rows of events for specific IDs multiple times",
        "body": "<p>I have a call log data made on customers. Which looks something like below, where ID is customer ID and A and B are log attributes:</p>\n\n<pre><code>import pandas as pd \nimport numpy as np\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 2)), columns=list('AB'),\n                  index = ['A','A','A','B','B','C','C','C','D','D'])\ndf['ID']=df.index\ndf = df[['ID','A','B']]\n</code></pre>\n\n<pre><code>  ID   A   B\nA  A  46  31\nA  A  99  54\nA  A  34   9\nB  B  46  48\nB  B   7  75\nC  C   1  25\nC  C  71  40\nC  C  74  53\nD  D  57  17\nD  D  19  78\n</code></pre>\n\n<p>I want to replicate each set of event for each ID based on some slots. For e.g. if slot value is 2 then all events for ID \"A\" should be replicated slot-1 times.</p>\n\n<pre><code>  ID   A   B\nA  A  46  31\nA  A  99  54\nA  A  34   9\n\nA  A  46  31\nA  A  99  54\nA  A  34   9\n</code></pre>\n\n<p>and a new Index should be created indicating which slot does replicated values belong to:</p>\n\n<pre><code>ID   A   B Index\n A  46  31  A-1\n A  99  54  A-1\n A  34   9  A-1\n\n A  46  31  A-2\n A  99  54  A-2\n A  34   9  A-2\n</code></pre>\n\n<p>I have tried following solution:</p>\n\n<pre><code>slots = 2\nnba_data = pd.DataFrame()\nidx = pd.Index(list(range(1,slots+1))) \n\nfor i in unique_rec_counts_dict:\n    b = df.loc[df.ID==i,:]\n    b = b.append([b]*(slots-1),ignore_index=True)\n    b['Index'] = str(i)+'-'+idx.repeat(unique_rec_counts_dict[i]).astype(str)\n    nba_data = nba_data.append(b)\n</code></pre>\n\n<p>it gives me the expected output but is not scalable when slots are increased and number of customers increases in order of 10k. </p>\n\n<pre><code>  ID   A   B Index\n0  A  46  31   A-1\n1  A  99  54   A-1\n2  A  34   9   A-1\n3  A  46  31   A-2\n4  A  99  54   A-2\n5  A  34   9   A-2\n0  B  46  48   B-1\n1  B   7  75   B-1\n2  B  46  48   B-2\n3  B   7  75   B-2\n0  C   1  25   C-1\n1  C  71  40   C-1\n2  C  74  53   C-1\n3  C   1  25   C-2\n4  C  71  40   C-2\n5  C  74  53   C-2\n0  D  57  17   D-1\n1  D  19  78   D-1\n2  D  57  17   D-2\n3  D  19  78   D-2\n</code></pre>\n\n<p>I think its taking a long time because of the loop. Any solution which is vectorized will be really helpful.</p>\n",
        "answer_body": "<p>You can try:</p>\n\n<pre><code>slots = 2\nnew_df = pd.concat(df.assign(Index=f'_{i}') for i in range(1, slots+1))\n\nnew_df['Index'] = new_df['ID'] + new_df['Index']\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>  ID   A   B Index\nA  A  48  61   A_1\nA  A  70  13   A_1\nA  A  36  23   A_1\nB  B  22  66   B_1\nB  B  92  95   B_1\nC  C  53   9   C_1\nC  C  41  57   C_1\nC  C  88  93   C_1\nD  D  76  82   D_1\nD  D  11  36   D_1\nA  A  48  61   A_2\nA  A  70  13   A_2\nA  A  36  23   A_2\nB  B  22  66   B_2\nB  B  92  95   B_2\nC  C  53   9   C_2\nC  C  41  57   C_2\nC  C  88  93   C_2\nD  D  76  82   D_2\nD  D  11  36   D_2\n</code></pre>\n",
        "question_body": "<p>I have a call log data made on customers. Which looks something like below, where ID is customer ID and A and B are log attributes:</p>\n\n<pre><code>import pandas as pd \nimport numpy as np\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 2)), columns=list('AB'),\n                  index = ['A','A','A','B','B','C','C','C','D','D'])\ndf['ID']=df.index\ndf = df[['ID','A','B']]\n</code></pre>\n\n<pre><code>  ID   A   B\nA  A  46  31\nA  A  99  54\nA  A  34   9\nB  B  46  48\nB  B   7  75\nC  C   1  25\nC  C  71  40\nC  C  74  53\nD  D  57  17\nD  D  19  78\n</code></pre>\n\n<p>I want to replicate each set of event for each ID based on some slots. For e.g. if slot value is 2 then all events for ID \"A\" should be replicated slot-1 times.</p>\n\n<pre><code>  ID   A   B\nA  A  46  31\nA  A  99  54\nA  A  34   9\n\nA  A  46  31\nA  A  99  54\nA  A  34   9\n</code></pre>\n\n<p>and a new Index should be created indicating which slot does replicated values belong to:</p>\n\n<pre><code>ID   A   B Index\n A  46  31  A-1\n A  99  54  A-1\n A  34   9  A-1\n\n A  46  31  A-2\n A  99  54  A-2\n A  34   9  A-2\n</code></pre>\n\n<p>I have tried following solution:</p>\n\n<pre><code>slots = 2\nnba_data = pd.DataFrame()\nidx = pd.Index(list(range(1,slots+1))) \n\nfor i in unique_rec_counts_dict:\n    b = df.loc[df.ID==i,:]\n    b = b.append([b]*(slots-1),ignore_index=True)\n    b['Index'] = str(i)+'-'+idx.repeat(unique_rec_counts_dict[i]).astype(str)\n    nba_data = nba_data.append(b)\n</code></pre>\n\n<p>it gives me the expected output but is not scalable when slots are increased and number of customers increases in order of 10k. </p>\n\n<pre><code>  ID   A   B Index\n0  A  46  31   A-1\n1  A  99  54   A-1\n2  A  34   9   A-1\n3  A  46  31   A-2\n4  A  99  54   A-2\n5  A  34   9   A-2\n0  B  46  48   B-1\n1  B   7  75   B-1\n2  B  46  48   B-2\n3  B   7  75   B-2\n0  C   1  25   C-1\n1  C  71  40   C-1\n2  C  74  53   C-1\n3  C   1  25   C-2\n4  C  71  40   C-2\n5  C  74  53   C-2\n0  D  57  17   D-1\n1  D  19  78   D-1\n2  D  57  17   D-2\n3  D  19  78   D-2\n</code></pre>\n\n<p>I think its taking a long time because of the loop. Any solution which is vectorized will be really helpful.</p>\n",
        "formatted_input": {
            "qid": 58086144,
            "link": "https://stackoverflow.com/questions/58086144/replicate-multiple-rows-of-events-for-specific-ids-multiple-times",
            "question": {
                "title": "Replicate multiple rows of events for specific IDs multiple times",
                "ques_desc": "I have a call log data made on customers. Which looks something like below, where ID is customer ID and A and B are log attributes: I want to replicate each set of event for each ID based on some slots. For e.g. if slot value is 2 then all events for ID \"A\" should be replicated slot-1 times. and a new Index should be created indicating which slot does replicated values belong to: I have tried following solution: it gives me the expected output but is not scalable when slots are increased and number of customers increases in order of 10k. I think its taking a long time because of the loop. Any solution which is vectorized will be really helpful. "
            },
            "io": [
                "  ID   A   B\nA  A  46  31\nA  A  99  54\nA  A  34   9\nB  B  46  48\nB  B   7  75\nC  C   1  25\nC  C  71  40\nC  C  74  53\nD  D  57  17\nD  D  19  78\n",
                "  ID   A   B\nA  A  46  31\nA  A  99  54\nA  A  34   9\n\nA  A  46  31\nA  A  99  54\nA  A  34   9\n"
            ],
            "answer": {
                "ans_desc": "You can try: Output: ",
                "code": [
                    "slots = 2\nnew_df = pd.concat(df.assign(Index=f'_{i}') for i in range(1, slots+1))\n\nnew_df['Index'] = new_df['ID'] + new_df['Index']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 12106290,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-m9YL1ijMHAQ/AAAAAAAAAAI/AAAAAAAAAAc/JxfZe5zpRM0/photo.jpg?sz=128",
            "display_name": "Fortune Seeker",
            "link": "https://stackoverflow.com/users/12106290/fortune-seeker"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 58060345,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1569251885,
        "creation_date": 1569233853,
        "question_id": 58060206,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58060206/how-to-change-values-of-rows-based-on-conditions-in-dataframe",
        "title": "How to change values of rows based on conditions in dataframe?",
        "body": "<p>I have dataframe that is shown below,</p>\n\n<pre><code>    type  label\n0      0      0\n1      0      0\n2      0      0\n3      0      0\n4      2      1\n5      2      1\n6      2      1\n7      2      1\n8      2      1\n9      2      1\n10     0      0\n11     0      0\n12     0      0\n13     0      0\n14     0      0\n15     0      0\n16     0      0\n17     0      0\n18     0      0\n19     0      0 \n</code></pre>\n\n<p><strong>(Need some magic to be done)</strong></p>\n\n<p>The change should be made in type column such that, in a row if <code>label</code> is <code>0</code> and <code>type</code> is <code>0</code> then next row of <code>type</code> should be assigned <code>2</code>.</p>\n\n<p>Full dataframe should look like this:</p>\n\n<pre><code>    type  label\n0      0      0\n1      2      0\n2      2      0\n3      0      0\n4      2      1\n5      2      1\n6      2      1\n7      2      1\n8      2      1\n9      2      1\n10     0      0\n11     2      0\n12     2      0\n13     2      0\n14     2      0\n15     2      0\n16     2      0\n17     2      0\n18     2      0\n19     2      0 \n</code></pre>\n",
        "answer_body": "<p>Using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.eq.html\" rel=\"nofollow noreferrer\"><code>.eq()</code></a> to mask row which has <code>type</code> and <code>label</code> columns value is 0 and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html\" rel=\"nofollow noreferrer\"><code>.shift()</code></a> to shift index by desired number of periods with an optional time freq.</p>\n\n<p><strong>Ex.</strong></p>\n\n<pre><code>m = df['type'].eq(0) &amp; df['label'].eq(0)\ndf.loc[m == m.shift(1),'type'] = 2\nprint(df)\n</code></pre>\n\n<p><strong>O/P:</strong></p>\n\n<pre><code>    type  label\n0      0      0\n1      2      0\n2      2      0\n3      2      0\n4      2      1\n5      2      1\n6      2      1\n7      2      1\n8      2      1\n9      2      1\n10     0      0\n11     2      0\n12     2      0\n13     2      0\n14     2      0\n15     2      0\n16     2      0\n17     2      0\n18     2      0\n19     2      0\n</code></pre>\n",
        "question_body": "<p>I have dataframe that is shown below,</p>\n\n<pre><code>    type  label\n0      0      0\n1      0      0\n2      0      0\n3      0      0\n4      2      1\n5      2      1\n6      2      1\n7      2      1\n8      2      1\n9      2      1\n10     0      0\n11     0      0\n12     0      0\n13     0      0\n14     0      0\n15     0      0\n16     0      0\n17     0      0\n18     0      0\n19     0      0 \n</code></pre>\n\n<p><strong>(Need some magic to be done)</strong></p>\n\n<p>The change should be made in type column such that, in a row if <code>label</code> is <code>0</code> and <code>type</code> is <code>0</code> then next row of <code>type</code> should be assigned <code>2</code>.</p>\n\n<p>Full dataframe should look like this:</p>\n\n<pre><code>    type  label\n0      0      0\n1      2      0\n2      2      0\n3      0      0\n4      2      1\n5      2      1\n6      2      1\n7      2      1\n8      2      1\n9      2      1\n10     0      0\n11     2      0\n12     2      0\n13     2      0\n14     2      0\n15     2      0\n16     2      0\n17     2      0\n18     2      0\n19     2      0 \n</code></pre>\n",
        "formatted_input": {
            "qid": 58060206,
            "link": "https://stackoverflow.com/questions/58060206/how-to-change-values-of-rows-based-on-conditions-in-dataframe",
            "question": {
                "title": "How to change values of rows based on conditions in dataframe?",
                "ques_desc": "I have dataframe that is shown below, (Need some magic to be done) The change should be made in type column such that, in a row if is and is then next row of should be assigned . Full dataframe should look like this: "
            },
            "io": [
                "    type  label\n0      0      0\n1      0      0\n2      0      0\n3      0      0\n4      2      1\n5      2      1\n6      2      1\n7      2      1\n8      2      1\n9      2      1\n10     0      0\n11     0      0\n12     0      0\n13     0      0\n14     0      0\n15     0      0\n16     0      0\n17     0      0\n18     0      0\n19     0      0 \n",
                "    type  label\n0      0      0\n1      2      0\n2      2      0\n3      0      0\n4      2      1\n5      2      1\n6      2      1\n7      2      1\n8      2      1\n9      2      1\n10     0      0\n11     2      0\n12     2      0\n13     2      0\n14     2      0\n15     2      0\n16     2      0\n17     2      0\n18     2      0\n19     2      0 \n"
            ],
            "answer": {
                "ans_desc": "Using to mask row which has and columns value is 0 and to shift index by desired number of periods with an optional time freq. Ex. O/P: ",
                "code": [
                    "m = df['type'].eq(0) & df['label'].eq(0)\ndf.loc[m == m.shift(1),'type'] = 2\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "matrix",
            "dataframe"
        ],
        "owner": {
            "reputation": 4013,
            "user_id": 2735009,
            "user_type": "registered",
            "accept_rate": 69,
            "profile_image": "https://www.gravatar.com/avatar/38a795fbd4a70b92d3ea5596dc8beb6e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Patthebug",
            "link": "https://stackoverflow.com/users/2735009/patthebug"
        },
        "is_answered": true,
        "view_count": 300,
        "accepted_answer_id": 38446273,
        "answer_count": 1,
        "score": 5,
        "last_activity_date": 1568677576,
        "creation_date": 1468875921,
        "last_edit_date": 1568677576,
        "question_id": 38445974,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/38445974/count-of-rows-where-given-columns-of-a-dataframe-are-non-zero",
        "title": "Count of rows where given columns of a DataFrame are non-zero",
        "body": "<p>I have a <code>Pandas</code> <code>DataFrame</code> that looks like this:</p>\n\n<pre><code>MemberID    A    B    C    D\n1           0.3  0.5 0.1   0\n2           0    0.2 0.9   0.3\n3           0.4  0.2 0.5   0.3\n4           0.1  0   0     0.7\n</code></pre>\n\n<p>I would like to have another matrix which gives me the <strong>number of non-zero elements for the intersection of every column</strong> except for <code>MemberID</code>. </p>\n\n<p>For example, the intersection of columns <code>A</code> and <code>B</code> would be 2 (because <code>MemberID</code> 1 and 3 have non-zero values for <code>A</code> and <code>B</code>), intersection of <code>A</code> and <code>C</code> would be 2 as well (because <code>MemberID</code> 1 and 3 have non-zero values for <code>A</code> and <code>C</code>).</p>\n\n<p>The final matrix would look like this:</p>\n\n<pre><code>    A    B    C    D\nA   3    2    2    2\nB   2    3    3    2\nC   2    3    3    2\nD   2    2    2    3\n</code></pre>\n\n<p>As we can see, it should be a symmetric matrix, similar to a correlation matrix, but not the correlation matrix.</p>\n\n<p><strong>Intersection of any 2 columns = # of <code>MemberID</code> having non-zero values in both columns.</strong></p>\n\n<p>I would show some initial code here but I feel like there would be a simple function to do this task that I don't know of. </p>\n\n<p>Here's the code to create the <code>DataFrame</code>:</p>\n\n<pre><code>df = pd.DataFrame([[0.3, 0.5,  0.1, 0],\n                   [0,  0.2,  0.9, 0.3],\n                   [ 0.4,  0.2,  0.5, 0.3],\n                   [ 0.1, 0, 0,  0.7]],\n                  columns=list('ABCD'))\n</code></pre>\n\n<p>Any pointers would be appreciated. TIA.</p>\n",
        "answer_body": "<p>This should to it:</p>\n\n<pre><code>z = (df != 0) * 1\nz.T.dot(z)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/cjkQS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/cjkQS.png\" alt=\"enter image description here\"></a></p>\n",
        "question_body": "<p>I have a <code>Pandas</code> <code>DataFrame</code> that looks like this:</p>\n\n<pre><code>MemberID    A    B    C    D\n1           0.3  0.5 0.1   0\n2           0    0.2 0.9   0.3\n3           0.4  0.2 0.5   0.3\n4           0.1  0   0     0.7\n</code></pre>\n\n<p>I would like to have another matrix which gives me the <strong>number of non-zero elements for the intersection of every column</strong> except for <code>MemberID</code>. </p>\n\n<p>For example, the intersection of columns <code>A</code> and <code>B</code> would be 2 (because <code>MemberID</code> 1 and 3 have non-zero values for <code>A</code> and <code>B</code>), intersection of <code>A</code> and <code>C</code> would be 2 as well (because <code>MemberID</code> 1 and 3 have non-zero values for <code>A</code> and <code>C</code>).</p>\n\n<p>The final matrix would look like this:</p>\n\n<pre><code>    A    B    C    D\nA   3    2    2    2\nB   2    3    3    2\nC   2    3    3    2\nD   2    2    2    3\n</code></pre>\n\n<p>As we can see, it should be a symmetric matrix, similar to a correlation matrix, but not the correlation matrix.</p>\n\n<p><strong>Intersection of any 2 columns = # of <code>MemberID</code> having non-zero values in both columns.</strong></p>\n\n<p>I would show some initial code here but I feel like there would be a simple function to do this task that I don't know of. </p>\n\n<p>Here's the code to create the <code>DataFrame</code>:</p>\n\n<pre><code>df = pd.DataFrame([[0.3, 0.5,  0.1, 0],\n                   [0,  0.2,  0.9, 0.3],\n                   [ 0.4,  0.2,  0.5, 0.3],\n                   [ 0.1, 0, 0,  0.7]],\n                  columns=list('ABCD'))\n</code></pre>\n\n<p>Any pointers would be appreciated. TIA.</p>\n",
        "formatted_input": {
            "qid": 38445974,
            "link": "https://stackoverflow.com/questions/38445974/count-of-rows-where-given-columns-of-a-dataframe-are-non-zero",
            "question": {
                "title": "Count of rows where given columns of a DataFrame are non-zero",
                "ques_desc": "I have a that looks like this: I would like to have another matrix which gives me the number of non-zero elements for the intersection of every column except for . For example, the intersection of columns and would be 2 (because 1 and 3 have non-zero values for and ), intersection of and would be 2 as well (because 1 and 3 have non-zero values for and ). The final matrix would look like this: As we can see, it should be a symmetric matrix, similar to a correlation matrix, but not the correlation matrix. Intersection of any 2 columns = # of having non-zero values in both columns. I would show some initial code here but I feel like there would be a simple function to do this task that I don't know of. Here's the code to create the : Any pointers would be appreciated. TIA. "
            },
            "io": [
                "MemberID    A    B    C    D\n1           0.3  0.5 0.1   0\n2           0    0.2 0.9   0.3\n3           0.4  0.2 0.5   0.3\n4           0.1  0   0     0.7\n",
                "    A    B    C    D\nA   3    2    2    2\nB   2    3    3    2\nC   2    3    3    2\nD   2    2    2    3\n"
            ],
            "answer": {
                "ans_desc": "This should to it: ",
                "code": [
                    "z = (df != 0) * 1\nz.T.dot(z)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1150,
            "user_id": 3280146,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2c4f2d0cfdd2ef32ac2535401d781e7a?s=128&d=identicon&r=PG&f=1",
            "display_name": "user96564",
            "link": "https://stackoverflow.com/users/3280146/user96564"
        },
        "is_answered": true,
        "view_count": 98,
        "accepted_answer_id": 57927250,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1568395020,
        "creation_date": 1568389404,
        "last_edit_date": 1568391639,
        "question_id": 57926657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57926657/conditional-pairwise-calculations-in-pandas",
        "title": "Conditional pairwise calculations in pandas",
        "body": "<p>For example, I have 2 dfs:</p>\n\n<p>df1</p>\n\n<pre><code>ID,col1,col2\n1,5,9\n2,6,3\n3,7,2\n4,8,5\n</code></pre>\n\n<p>and another df is</p>\n\n<p>df2</p>\n\n<pre><code>ID,col1,col2\n1,11,9\n2,12,7\n3,13,2\n</code></pre>\n\n<p>I want to calculate first pairwise subtraction from df2 to df1. I am using <code>scipy.spatial.distance</code> using a function <code>subtract_</code></p>\n\n<pre><code>def subtract_(a, b):\n    return abs(a - b)\n\nd1_s = df1[['col1']]\nd2_s = df2[['col1']]\n\ndist = cdist(d1_s, d2_s, metric=subtract_)\n\ndist_df = pd.DataFrame(dist, columns= d2_s.values.ravel())\nprint(dist_df)\n\n 11   12   13\n6.0  7.0  8.0\n5.0  6.0  7.0\n4.0  5.0  6.0\n3.0  4.0  5.0\n</code></pre>\n\n<p>Now, I want to check, these new columns name like <code>11,12</code> and <code>13</code>. I am checking if there is any values in this new <code>dataframe</code> less than 5. If there is, then I want to do further calculations. Like this.\nFor example, here for columns name <code>'11'</code>, less than 5 value is 4 which is at <code>rows 3</code>. Now in this case, I want to subtract columns name <code>('col2')</code> of <code>df1</code> but at row 3, in this case it would be value <code>2</code>. I want to subtract this value 2 with <code>df2(col2)</code> but at row 1 (because column name <code>'11'</code>) was from value at row 1 in <code>df2</code>.</p>\n\n<p>My <code>for loop</code> is so complex for this. It would be great, if there would be some easier way in pandas. \nAny help, suggestions would be great.</p>\n\n<p>The expected new dataframe is this</p>\n\n<pre><code>0,1,2\nNan,Nan,Nan\nNan,Nan,Nan\n(2-9)=-7,Nan,Nan\n(5-9)=-4,(5-7)=-2,Nan\n</code></pre>\n",
        "answer_body": "<p>Similar to Ben's answer, but with <code>np.where</code>:</p>\n\n<pre><code>pd.DataFrame(np.where(dist_df&lt;5, df1.col2.values[:,None] - df2.col2.values, np.nan),\n             index=dist_df.index,\n             columns=dist_df.columns)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>    11   12  13\n0  NaN  NaN NaN\n1  NaN  NaN NaN\n2 -7.0  NaN NaN\n3 -4.0 -2.0 NaN\n</code></pre>\n",
        "question_body": "<p>For example, I have 2 dfs:</p>\n\n<p>df1</p>\n\n<pre><code>ID,col1,col2\n1,5,9\n2,6,3\n3,7,2\n4,8,5\n</code></pre>\n\n<p>and another df is</p>\n\n<p>df2</p>\n\n<pre><code>ID,col1,col2\n1,11,9\n2,12,7\n3,13,2\n</code></pre>\n\n<p>I want to calculate first pairwise subtraction from df2 to df1. I am using <code>scipy.spatial.distance</code> using a function <code>subtract_</code></p>\n\n<pre><code>def subtract_(a, b):\n    return abs(a - b)\n\nd1_s = df1[['col1']]\nd2_s = df2[['col1']]\n\ndist = cdist(d1_s, d2_s, metric=subtract_)\n\ndist_df = pd.DataFrame(dist, columns= d2_s.values.ravel())\nprint(dist_df)\n\n 11   12   13\n6.0  7.0  8.0\n5.0  6.0  7.0\n4.0  5.0  6.0\n3.0  4.0  5.0\n</code></pre>\n\n<p>Now, I want to check, these new columns name like <code>11,12</code> and <code>13</code>. I am checking if there is any values in this new <code>dataframe</code> less than 5. If there is, then I want to do further calculations. Like this.\nFor example, here for columns name <code>'11'</code>, less than 5 value is 4 which is at <code>rows 3</code>. Now in this case, I want to subtract columns name <code>('col2')</code> of <code>df1</code> but at row 3, in this case it would be value <code>2</code>. I want to subtract this value 2 with <code>df2(col2)</code> but at row 1 (because column name <code>'11'</code>) was from value at row 1 in <code>df2</code>.</p>\n\n<p>My <code>for loop</code> is so complex for this. It would be great, if there would be some easier way in pandas. \nAny help, suggestions would be great.</p>\n\n<p>The expected new dataframe is this</p>\n\n<pre><code>0,1,2\nNan,Nan,Nan\nNan,Nan,Nan\n(2-9)=-7,Nan,Nan\n(5-9)=-4,(5-7)=-2,Nan\n</code></pre>\n",
        "formatted_input": {
            "qid": 57926657,
            "link": "https://stackoverflow.com/questions/57926657/conditional-pairwise-calculations-in-pandas",
            "question": {
                "title": "Conditional pairwise calculations in pandas",
                "ques_desc": "For example, I have 2 dfs: df1 and another df is df2 I want to calculate first pairwise subtraction from df2 to df1. I am using using a function Now, I want to check, these new columns name like and . I am checking if there is any values in this new less than 5. If there is, then I want to do further calculations. Like this. For example, here for columns name , less than 5 value is 4 which is at . Now in this case, I want to subtract columns name of but at row 3, in this case it would be value . I want to subtract this value 2 with but at row 1 (because column name ) was from value at row 1 in . My is so complex for this. It would be great, if there would be some easier way in pandas. Any help, suggestions would be great. The expected new dataframe is this "
            },
            "io": [
                "ID,col1,col2\n1,5,9\n2,6,3\n3,7,2\n4,8,5\n",
                "0,1,2\nNan,Nan,Nan\nNan,Nan,Nan\n(2-9)=-7,Nan,Nan\n(5-9)=-4,(5-7)=-2,Nan\n"
            ],
            "answer": {
                "ans_desc": "Similar to Ben's answer, but with : Output: ",
                "code": [
                    "pd.DataFrame(np.where(dist_df<5, df1.col2.values[:,None] - df2.col2.values, np.nan),\n             index=dist_df.index,\n             columns=dist_df.columns)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 190,
            "user_id": 11900800,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/4om9t.jpg?s=128&g=1",
            "display_name": "mvx",
            "link": "https://stackoverflow.com/users/11900800/mvx"
        },
        "is_answered": true,
        "view_count": 1936,
        "accepted_answer_id": 57857093,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1568216187,
        "creation_date": 1568043117,
        "last_edit_date": 1568203839,
        "question_id": 57857068,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57857068/pandas-dataframe-column-headers-to-labels-for-data",
        "title": "Pandas dataframe column headers to labels for data",
        "body": "<p><strong>SUMMARY:</strong> The output of my code gives me a dataframe of the following format. The column headers of the dataframe are the labels for the text in the column <code>Content</code>. The labels will be used as training data for a multilabel classifier in the next step. This is a snippet of actual data which is much larger.</p>\n\n<p>Since they are columns titles, it is not possible to use them as mapped to the text they are the labels for.  </p>\n\n<pre><code>Content  A  B  C  D  E\n    zxy  1  2     1   \n    wvu  1     2  1   \n    tsr  1  2        2\n    qpo     1  1  1   \n    nml        2  2   \n    kji  1     1     2\n    hgf        1     2\n    edc  1  2     1              \n</code></pre>\n\n<p>UPDATE: Converting the df to csv shows the empty cells are blank(<code>''</code> vs <code>' '</code>):\n<a href=\"https://i.stack.imgur.com/zMpi4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zMpi4.png\" alt=\"enter image description here\"></a> </p>\n\n<p>Where <code>Content</code> is the column where the text is, and <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, and <code>E</code> are the column headers that need to be turned into the labels. Only columns with 1s or 2s are relevant. The column with empty cells are not relevant and thus don't need to be converted as labels. </p>\n\n<p>UPDATE: After some digging, maybe the numbers might not be ints, but strings.</p>\n\n<p>I know that when entering the text + labels into a classifier for processing, the length of both arrays needs to be equal, else it is not accepted as valid input. </p>\n\n<p>Is there a way I can convert the columns titles to labels for the text in <code>Content</code> in the DF?</p>\n\n<p><strong>EXPECTED OUTPUT:</strong></p>\n\n<pre><code>&gt;&gt;Content  A  B  C  D  E     Labels\n0   zxy    1  2     1        A, B, D  \n1   wvu    1     2  1        A, C, D\n2   tsr    1  2        2     A, B, E\n3   qpo       1  1  1        B, C, D\n4   nml          2  2        C, D    \n5   kji    1     1     2     A, C, E\n6   hgf          1     2     C, E\n7   edc    1  2     1        A, B, D   \n</code></pre>\n",
        "answer_body": "<h2>Full Solution:</h2>\n\n<pre class=\"lang-py prettyprint-override\"><code># first: clear all whitespace before and after a char, fine for all columns\nfor col in df.columns:\n    df[col] = df[col].str.strip()\n\n# fill na with 0\ndf.fillna(0, inplace=True)\n\n# replace '' with 0\ndf.replace('', 0, inplace=True)\n\n# convert to int, this must only be done on the specific columns with the numeric data\n# this list is the column names as you've presented them, if they are different in the real data,\n# replace them\nfor col in ['A', 'B', 'C', 'D', 'E']:\n    df = df.astype({col: 'int16'})\n\nprint(df.info())\n\n# you should end up with something like this.\n\"\"\"\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8 entries, 0 to 7\nData columns (total 6 columns):\nContent    8 non-null object\nA          8 non-null int16\nB          8 non-null int16\nC          8 non-null int16\nD          8 non-null int16\nE          8 non-null int16\ndtypes: int16(5), object(1)\nmemory usage: 272.0+ bytes\n\"\"\"\n</code></pre>\n\n<p>We can do <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dot.html\" rel=\"nofollow noreferrer\"><code>dot</code></a>, notice here, I treat the blanks as <code>np.nan</code>, if that is a real blank in your data, change the last line </p>\n\n<pre class=\"lang-py prettyprint-override\"><code># make certain the label names match the appropriate columns \ns=df.loc[:, ['A', 'B', 'C', 'D', 'E']]  \n# or\ns=df.loc[:,'A':]\n\ndf['Labels']=(s&gt;0).dot(s.columns+',').str[:-1]  # column A:E need to be numeric, not str\n# df['Labels']=(~s.isin(['']).dot(s.columns+',').str[:-1]\n</code></pre>\n",
        "question_body": "<p><strong>SUMMARY:</strong> The output of my code gives me a dataframe of the following format. The column headers of the dataframe are the labels for the text in the column <code>Content</code>. The labels will be used as training data for a multilabel classifier in the next step. This is a snippet of actual data which is much larger.</p>\n\n<p>Since they are columns titles, it is not possible to use them as mapped to the text they are the labels for.  </p>\n\n<pre><code>Content  A  B  C  D  E\n    zxy  1  2     1   \n    wvu  1     2  1   \n    tsr  1  2        2\n    qpo     1  1  1   \n    nml        2  2   \n    kji  1     1     2\n    hgf        1     2\n    edc  1  2     1              \n</code></pre>\n\n<p>UPDATE: Converting the df to csv shows the empty cells are blank(<code>''</code> vs <code>' '</code>):\n<a href=\"https://i.stack.imgur.com/zMpi4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zMpi4.png\" alt=\"enter image description here\"></a> </p>\n\n<p>Where <code>Content</code> is the column where the text is, and <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, and <code>E</code> are the column headers that need to be turned into the labels. Only columns with 1s or 2s are relevant. The column with empty cells are not relevant and thus don't need to be converted as labels. </p>\n\n<p>UPDATE: After some digging, maybe the numbers might not be ints, but strings.</p>\n\n<p>I know that when entering the text + labels into a classifier for processing, the length of both arrays needs to be equal, else it is not accepted as valid input. </p>\n\n<p>Is there a way I can convert the columns titles to labels for the text in <code>Content</code> in the DF?</p>\n\n<p><strong>EXPECTED OUTPUT:</strong></p>\n\n<pre><code>&gt;&gt;Content  A  B  C  D  E     Labels\n0   zxy    1  2     1        A, B, D  \n1   wvu    1     2  1        A, C, D\n2   tsr    1  2        2     A, B, E\n3   qpo       1  1  1        B, C, D\n4   nml          2  2        C, D    \n5   kji    1     1     2     A, C, E\n6   hgf          1     2     C, E\n7   edc    1  2     1        A, B, D   \n</code></pre>\n",
        "formatted_input": {
            "qid": 57857068,
            "link": "https://stackoverflow.com/questions/57857068/pandas-dataframe-column-headers-to-labels-for-data",
            "question": {
                "title": "Pandas dataframe column headers to labels for data",
                "ques_desc": "SUMMARY: The output of my code gives me a dataframe of the following format. The column headers of the dataframe are the labels for the text in the column . The labels will be used as training data for a multilabel classifier in the next step. This is a snippet of actual data which is much larger. Since they are columns titles, it is not possible to use them as mapped to the text they are the labels for. UPDATE: Converting the df to csv shows the empty cells are blank( vs ): Where is the column where the text is, and , , , , and are the column headers that need to be turned into the labels. Only columns with 1s or 2s are relevant. The column with empty cells are not relevant and thus don't need to be converted as labels. UPDATE: After some digging, maybe the numbers might not be ints, but strings. I know that when entering the text + labels into a classifier for processing, the length of both arrays needs to be equal, else it is not accepted as valid input. Is there a way I can convert the columns titles to labels for the text in in the DF? EXPECTED OUTPUT: "
            },
            "io": [
                "Content  A  B  C  D  E\n    zxy  1  2     1   \n    wvu  1     2  1   \n    tsr  1  2        2\n    qpo     1  1  1   \n    nml        2  2   \n    kji  1     1     2\n    hgf        1     2\n    edc  1  2     1              \n",
                ">>Content  A  B  C  D  E     Labels\n0   zxy    1  2     1        A, B, D  \n1   wvu    1     2  1        A, C, D\n2   tsr    1  2        2     A, B, E\n3   qpo       1  1  1        B, C, D\n4   nml          2  2        C, D    \n5   kji    1     1     2     A, C, E\n6   hgf          1     2     C, E\n7   edc    1  2     1        A, B, D   \n"
            ],
            "answer": {
                "ans_desc": "Full Solution: We can do , notice here, I treat the blanks as , if that is a real blank in your data, change the last line ",
                "code": [
                    "# make certain the label names match the appropriate columns \ns=df.loc[:, ['A', 'B', 'C', 'D', 'E']]  \n# or\ns=df.loc[:,'A':]\n\ndf['Labels']=(s>0).dot(s.columns+',').str[:-1]  # column A:E need to be numeric, not str\n# df['Labels']=(~s.isin(['']).dot(s.columns+',').str[:-1]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 485,
            "user_id": 11740864,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/186b8ced1838e9982a3a8a95bb0f0d37?s=128&d=identicon&r=PG&f=1",
            "display_name": "Moshee",
            "link": "https://stackoverflow.com/users/11740864/moshee"
        },
        "is_answered": true,
        "view_count": 166,
        "accepted_answer_id": 57874108,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1568143937,
        "creation_date": 1568128112,
        "last_edit_date": 1568139290,
        "question_id": 57873651,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57873651/identify-common-elements-between-df-rows-to-create-a-new-column",
        "title": "identify common elements between df rows to create a new column",
        "body": "<p>My df is shown below.<br> </p>\n\n<pre><code>    key       val    \n0   A1  [1, 2, 3, 4]\n1   A2  [1, 2, 7, 9]    \n2   A3  [1, 3, 5]   \n3   A4  [6, 9]  \n4   A5  [8] \n</code></pre>\n\n<p>I want to create a new column called common which contains which other key has the same value as my current key. The final dataframe would look like:</p>\n\n<pre><code>   key        val      common\n0   A1  [1, 2, 3, 4]   {'A2':[1, 2], 'A3':[1, 3]} \n1   A2  [1, 2, 7, 9]   {'A1':[1, 2], 'A3':[1], 'A4':[9], 'A5':[7]}\n2   A3  [1, 3, 5]      {'A1':[1, 3], 'A2':[1]}\n3   A4  [6, 9]         {'A2':[9]}\n4   A5  [8]            {}\n</code></pre>\n\n<p>The only way I can think of is to create a column with empty dictionaries and then have two loops to get the result. I wanted to know if there is an easy way to do this. <br>\nThanks</p>\n",
        "answer_body": "<p>Here is one way using <code>explode</code> first then <code>merge</code> </p>\n\n<pre><code>s=df.explode('Val')\ns=s.merge(s,on='Val').query('Key_x ! = Key_y').groupby(['Key_x','Key_y']).Val.apply(list)\nl=[y.reset_index(level=0,drop=True).to_dict()for x , y in s.groupby(level=0)]\nOut[73]: \n[{'A2': [1, 2], 'A3': [1, 3]},\n {'A1': [1, 2], 'A3': [1], 'A4': [9], 'A5': [7]},\n {'A1': [1, 3], 'A2': [1]},\n {'A2': [9]},\n {'A2': [7]}]\ndf['common']=l\n</code></pre>\n\n<p>Update </p>\n\n<pre><code>l={x: y.reset_index(level=0,drop=True).to_dict()for x , y in s.groupby(level=0)}\n\ndf['common']=pd.Series(l).reindex(df.Key).values\n</code></pre>\n",
        "question_body": "<p>My df is shown below.<br> </p>\n\n<pre><code>    key       val    \n0   A1  [1, 2, 3, 4]\n1   A2  [1, 2, 7, 9]    \n2   A3  [1, 3, 5]   \n3   A4  [6, 9]  \n4   A5  [8] \n</code></pre>\n\n<p>I want to create a new column called common which contains which other key has the same value as my current key. The final dataframe would look like:</p>\n\n<pre><code>   key        val      common\n0   A1  [1, 2, 3, 4]   {'A2':[1, 2], 'A3':[1, 3]} \n1   A2  [1, 2, 7, 9]   {'A1':[1, 2], 'A3':[1], 'A4':[9], 'A5':[7]}\n2   A3  [1, 3, 5]      {'A1':[1, 3], 'A2':[1]}\n3   A4  [6, 9]         {'A2':[9]}\n4   A5  [8]            {}\n</code></pre>\n\n<p>The only way I can think of is to create a column with empty dictionaries and then have two loops to get the result. I wanted to know if there is an easy way to do this. <br>\nThanks</p>\n",
        "formatted_input": {
            "qid": 57873651,
            "link": "https://stackoverflow.com/questions/57873651/identify-common-elements-between-df-rows-to-create-a-new-column",
            "question": {
                "title": "identify common elements between df rows to create a new column",
                "ques_desc": "My df is shown below. I want to create a new column called common which contains which other key has the same value as my current key. The final dataframe would look like: The only way I can think of is to create a column with empty dictionaries and then have two loops to get the result. I wanted to know if there is an easy way to do this. Thanks "
            },
            "io": [
                "    key       val    \n0   A1  [1, 2, 3, 4]\n1   A2  [1, 2, 7, 9]    \n2   A3  [1, 3, 5]   \n3   A4  [6, 9]  \n4   A5  [8] \n",
                "   key        val      common\n0   A1  [1, 2, 3, 4]   {'A2':[1, 2], 'A3':[1, 3]} \n1   A2  [1, 2, 7, 9]   {'A1':[1, 2], 'A3':[1], 'A4':[9], 'A5':[7]}\n2   A3  [1, 3, 5]      {'A1':[1, 3], 'A2':[1]}\n3   A4  [6, 9]         {'A2':[9]}\n4   A5  [8]            {}\n"
            ],
            "answer": {
                "ans_desc": "Here is one way using first then Update ",
                "code": [
                    "l={x: y.reset_index(level=0,drop=True).to_dict()for x , y in s.groupby(level=0)}\n\ndf['common']=pd.Series(l).reindex(df.Key).values\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 41,
            "user_id": 11200770,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-9YU_Wc9XEhQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfxZCBCC1gJllt5Xf6JhpjJtc1m_w/mo/photo.jpg?sz=128",
            "display_name": "fqntom",
            "link": "https://stackoverflow.com/users/11200770/fqntom"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 57865812,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1568100882,
        "creation_date": 1568096141,
        "question_id": 57864979,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57864979/print-dataframe-without-float-precision",
        "title": "Print dataframe without float precision",
        "body": "<p>I would like to print a dataframe in my console without displaying any float decimal precision.</p>\n\n<pre><code>pd.options.display.float_format = '{:,.2f}'.format\npd_tmp = pd.DataFrame()\npd_tmp[\"new_column\"] = [(\"a\",2.01212121212),1.123123,8.1212]\n\nprint(pd_tmp)\n</code></pre>\n\n<p>The output I got after printing is:</p>\n\n<pre><code>0   (a, 2.01212121212)\n\n1                1.12\n\n2                8.12\n</code></pre>\n\n<p>Whereas what I expected is:</p>\n\n<pre><code>0            (a, 2.01)\n\n1                1.12\n\n2                8.12\n</code></pre>\n\n<p>There seems to be an issue to display the tuple without float precision. Any idea why ?</p>\n\n<p>Cheers</p>\n",
        "answer_body": "<p>You can do like this (very specific to this problem):</p>\n\n<pre><code>pd_tmp = pd.DataFrame()\npd_tmp[\"new_column\"] = [(\"a\", 2.01212121212), 1.123123, 8.1212]\n\n\ndef foo(pd_col):\n    pd_new_col = []\n    for i in pd_col:\n        if not isinstance(i, float):\n            lst = []\n            for j in i:\n                if not isinstance(j, float):\n                    k = j\n                    lst.append(k)\n                else:\n                    k = round(j, 2)\n                    lst.append(k)\n            pd_new_col.append(tuple(lst))\n\n        else:\n            pd_new_col.append(round(i, 2))\n\n   return pd_new_col\n\n\npd_tmp[\"new_column\"] = foo(pd_tmp[\"new_column\"])\nprint(pd_tmp)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>       new_column\n0       (a, 2.01)\n1            1.12\n2            8.12\n</code></pre>\n",
        "question_body": "<p>I would like to print a dataframe in my console without displaying any float decimal precision.</p>\n\n<pre><code>pd.options.display.float_format = '{:,.2f}'.format\npd_tmp = pd.DataFrame()\npd_tmp[\"new_column\"] = [(\"a\",2.01212121212),1.123123,8.1212]\n\nprint(pd_tmp)\n</code></pre>\n\n<p>The output I got after printing is:</p>\n\n<pre><code>0   (a, 2.01212121212)\n\n1                1.12\n\n2                8.12\n</code></pre>\n\n<p>Whereas what I expected is:</p>\n\n<pre><code>0            (a, 2.01)\n\n1                1.12\n\n2                8.12\n</code></pre>\n\n<p>There seems to be an issue to display the tuple without float precision. Any idea why ?</p>\n\n<p>Cheers</p>\n",
        "formatted_input": {
            "qid": 57864979,
            "link": "https://stackoverflow.com/questions/57864979/print-dataframe-without-float-precision",
            "question": {
                "title": "Print dataframe without float precision",
                "ques_desc": "I would like to print a dataframe in my console without displaying any float decimal precision. The output I got after printing is: Whereas what I expected is: There seems to be an issue to display the tuple without float precision. Any idea why ? Cheers "
            },
            "io": [
                "0   (a, 2.01212121212)\n\n1                1.12\n\n2                8.12\n",
                "0            (a, 2.01)\n\n1                1.12\n\n2                8.12\n"
            ],
            "answer": {
                "ans_desc": "You can do like this (very specific to this problem): Output: ",
                "code": [
                    "pd_tmp = pd.DataFrame()\npd_tmp[\"new_column\"] = [(\"a\", 2.01212121212), 1.123123, 8.1212]\n\n\ndef foo(pd_col):\n    pd_new_col = []\n    for i in pd_col:\n        if not isinstance(i, float):\n            lst = []\n            for j in i:\n                if not isinstance(j, float):\n                    k = j\n                    lst.append(k)\n                else:\n                    k = round(j, 2)\n                    lst.append(k)\n            pd_new_col.append(tuple(lst))\n\n        else:\n            pd_new_col.append(round(i, 2))\n\n   return pd_new_col\n\n\npd_tmp[\"new_column\"] = foo(pd_tmp[\"new_column\"])\nprint(pd_tmp)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 165,
            "user_id": 11886203,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/68dd01470afa93018c537c81f5e2c2a0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Matteo",
            "link": "https://stackoverflow.com/users/11886203/matteo"
        },
        "is_answered": true,
        "view_count": 1491,
        "accepted_answer_id": 57842085,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1567947973,
        "creation_date": 1567946575,
        "question_id": 57842073,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57842073/pandas-how-to-drop-rows-when-all-float-columns-are-nan",
        "title": "pandas how to drop rows when all float columns are NaN",
        "body": "<p>I have the following df</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>  AAA BBB CCC DDD  ID1  ID2  ID3  ID4\n0 txt txt txt txt  10   NaN  12   NaN\n1 txt txt txt txt  10   NaN  12   13\n2 txt txt txt txt  NaN  NaN  NaN  NaN\n</code></pre>\n\n<p>With the following dtypes</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>AAA          object\nBBB          object\nCCC          object\nDDD          object\nID1          float64\nID2          float64\nID3          float64\nID4          float64\n</code></pre>\n\n<p>Is there a way to drop rows only when ALL float columns are NaN?</p>\n\n<p>output:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>  AAA BBB CCC DDD  ID1  ID2  ID3  ID4\n0 txt txt txt txt  10   NaN  12   NaN\n1 txt txt txt txt  10   NaN  12   13\n</code></pre>\n\n<p>I can't do it with df.dropna(subset=['ID1','ID2','ID3','ID4']) because my real df has several dynamic floating columns.</p>\n\n<p>Thanks</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\" rel=\"nofollow noreferrer\"><code>DataFrame.select_dtypes</code></a> for get all float columns, then test for non missing values and select by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.any.html\" rel=\"nofollow noreferrer\"><code>DataFrame.any</code></a> for at least one non misisng value per row - so misising floats rows are removed:</p>\n\n<pre><code>df1 = df[df.select_dtypes(float).notna().any(axis=1)]\nprint (df1)\n   AAA  BBB  CCC  DDD   ID1  ID2   ID3   ID4\n0  txt  txt  txt  txt  10.0  NaN  12.0   NaN\n1  txt  txt  txt  txt  10.0  NaN  12.0  13.0\n</code></pre>\n\n<p>Your solution with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\" rel=\"nofollow noreferrer\"><code>DataFrame.dropna</code></a> should be changed for pass float columns and parameter <code>how='all'</code> for test if all <code>NaN</code>s per rows:</p>\n\n<pre><code>df1 = df.dropna(subset=df.select_dtypes(float).columns, how='all')\n#for return same dataframe \n#df.dropna(subset=df.select_dtypes(float).columns, how='all', inplace=True)\n</code></pre>\n\n<p>If possible multiple types of floats check by <code>np.floating</code>:</p>\n\n<pre><code>df1 = df.dropna(subset=df.select_dtypes(np.floating).columns, how='all')\n</code></pre>\n",
        "question_body": "<p>I have the following df</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>  AAA BBB CCC DDD  ID1  ID2  ID3  ID4\n0 txt txt txt txt  10   NaN  12   NaN\n1 txt txt txt txt  10   NaN  12   13\n2 txt txt txt txt  NaN  NaN  NaN  NaN\n</code></pre>\n\n<p>With the following dtypes</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>AAA          object\nBBB          object\nCCC          object\nDDD          object\nID1          float64\nID2          float64\nID3          float64\nID4          float64\n</code></pre>\n\n<p>Is there a way to drop rows only when ALL float columns are NaN?</p>\n\n<p>output:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>  AAA BBB CCC DDD  ID1  ID2  ID3  ID4\n0 txt txt txt txt  10   NaN  12   NaN\n1 txt txt txt txt  10   NaN  12   13\n</code></pre>\n\n<p>I can't do it with df.dropna(subset=['ID1','ID2','ID3','ID4']) because my real df has several dynamic floating columns.</p>\n\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 57842073,
            "link": "https://stackoverflow.com/questions/57842073/pandas-how-to-drop-rows-when-all-float-columns-are-nan",
            "question": {
                "title": "pandas how to drop rows when all float columns are NaN",
                "ques_desc": "I have the following df With the following dtypes Is there a way to drop rows only when ALL float columns are NaN? output: I can't do it with df.dropna(subset=['ID1','ID2','ID3','ID4']) because my real df has several dynamic floating columns. Thanks "
            },
            "io": [
                "  AAA BBB CCC DDD  ID1  ID2  ID3  ID4\n0 txt txt txt txt  10   NaN  12   NaN\n1 txt txt txt txt  10   NaN  12   13\n2 txt txt txt txt  NaN  NaN  NaN  NaN\n",
                "  AAA BBB CCC DDD  ID1  ID2  ID3  ID4\n0 txt txt txt txt  10   NaN  12   NaN\n1 txt txt txt txt  10   NaN  12   13\n"
            ],
            "answer": {
                "ans_desc": "Use for get all float columns, then test for non missing values and select by for at least one non misisng value per row - so misising floats rows are removed: Your solution with should be changed for pass float columns and parameter for test if all s per rows: If possible multiple types of floats check by : ",
                "code": [
                    "df1 = df.dropna(subset=df.select_dtypes(float).columns, how='all')\n#for return same dataframe \n#df.dropna(subset=df.select_dtypes(float).columns, how='all', inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 303,
            "user_id": 4427584,
            "user_type": "registered",
            "accept_rate": 46,
            "profile_image": "https://lh4.googleusercontent.com/-iAZ3geOb9UY/AAAAAAAAAAI/AAAAAAAAAIY/j6F83yet-OY/photo.jpg?sz=128",
            "display_name": "pentanol",
            "link": "https://stackoverflow.com/users/4427584/pentanol"
        },
        "is_answered": true,
        "view_count": 4365,
        "accepted_answer_id": 57801078,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1567676815,
        "creation_date": 1567670778,
        "last_edit_date": 1567673454,
        "question_id": 57801048,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57801048/subtract-previous-row-value-from-the-current-row-value-in-a-pandas-column",
        "title": "Subtract previous row value from the current row value in a Pandas column",
        "body": "<p>I have a pandas column with the name 'values' containing respective values <code>10 15 36 95 99</code>. I want to subtract the each value from the next value so that I get the following format: <code>10 5 21 59 4</code></p>\n\n<p>I've tried to solve this using a for loop that loops over all the data-frame but this method was time consuming.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>for i in range(1,length_colulmn):\n    df['value'].iloc[i] = df['value'].iloc[i]-df['value'].iloc[i-1]\n</code></pre>\n\n<p>Is there a straightforward method the dataframe functions to solve this problem quickly? \nThe output we desire is the following: </p>\n\n<pre><code>['input']                                       \n11\n15\n22\n27\n36\n69\n77\n\n['output']                                        \n11\n4\n7\n5\n9\n33\n8\n</code></pre>\n\n\n",
        "answer_body": "<p>Use <code>pandas.Series.diff</code> with <code>fillna</code>:</p>\n\n<pre><code>import pandas as pd\n\ns = pd.Series([11,15,22,27,36,69,77])\ns.diff().fillna(s)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>0    11.0\n1     4.0\n2     7.0\n3     5.0\n4     9.0\n5    33.0\n6     8.0\ndtype: float64\n</code></pre>\n",
        "question_body": "<p>I have a pandas column with the name 'values' containing respective values <code>10 15 36 95 99</code>. I want to subtract the each value from the next value so that I get the following format: <code>10 5 21 59 4</code></p>\n\n<p>I've tried to solve this using a for loop that loops over all the data-frame but this method was time consuming.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>for i in range(1,length_colulmn):\n    df['value'].iloc[i] = df['value'].iloc[i]-df['value'].iloc[i-1]\n</code></pre>\n\n<p>Is there a straightforward method the dataframe functions to solve this problem quickly? \nThe output we desire is the following: </p>\n\n<pre><code>['input']                                       \n11\n15\n22\n27\n36\n69\n77\n\n['output']                                        \n11\n4\n7\n5\n9\n33\n8\n</code></pre>\n\n\n",
        "formatted_input": {
            "qid": 57801048,
            "link": "https://stackoverflow.com/questions/57801048/subtract-previous-row-value-from-the-current-row-value-in-a-pandas-column",
            "question": {
                "title": "Subtract previous row value from the current row value in a Pandas column",
                "ques_desc": "I have a pandas column with the name 'values' containing respective values . I want to subtract the each value from the next value so that I get the following format: I've tried to solve this using a for loop that loops over all the data-frame but this method was time consuming. Is there a straightforward method the dataframe functions to solve this problem quickly? The output we desire is the following: "
            },
            "io": [
                "10 15 36 95 99",
                "10 5 21 59 4"
            ],
            "answer": {
                "ans_desc": "Use with : Output: ",
                "code": [
                    "import pandas as pd\n\ns = pd.Series([11,15,22,27,36,69,77])\ns.diff().fillna(s)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 6234,
            "user_id": 766708,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/55fd0f7d95a4938975026ff886c3a563?s=128&d=identicon&r=PG",
            "display_name": "daiyue",
            "link": "https://stackoverflow.com/users/766708/daiyue"
        },
        "is_answered": true,
        "view_count": 248,
        "accepted_answer_id": 57726990,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1567169020,
        "creation_date": 1567168779,
        "question_id": 57726939,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57726939/pandas-integers-to-negative-integer-powers-are-not-allowed",
        "title": "pandas Integers to negative integer powers are not allowed",
        "body": "<p>I have a <code>df</code>,</p>\n\n<pre><code>decimal_places    amount\n 2                10\n 3                100\n 1                1000\n</code></pre>\n\n<p>I want to create a new column <code>converted_amount</code> based on the following calculation: </p>\n\n<pre><code>df = (df.assign(\n        converted_amount=lambda x: x.amount * 10 ** (2 - x.decimal_places.fillna(2, downcast='infer'))))\n</code></pre>\n\n<p>but I got the following error,</p>\n\n<pre><code>ValueError: Integers to negative integer powers are not allowed.\n</code></pre>\n\n<p>I am wondering how to get around this, so the result will look like,</p>\n\n<pre><code>decimal_places    amount    converted_amount\n   2                10           10\n   3                100          10\n   1                1000         10000\n</code></pre>\n",
        "answer_body": "<p>There are two problems here. Firstly as the error suggests, the values must be floats. But secondly, by setting <code>downcast='infer'</code>, the values in <code>decimal_places</code> are cast to integer as all decimal places are <code>0</code>, and hence you'll get the same error. So you want:</p>\n\n<pre><code>df = df.astype(float)\ndf = (df.assign(\n      converted_amount=lambda x: x.amount * 10 ** (2 - x.decimal_places.fillna(2))))\n</code></pre>\n\n<hr>\n\n<p>For a faster approach you could work with the underlying numpy arrays:</p>\n\n<pre><code>df['converted_amount'] = df.amount.values*(10**(2-df.decimal_places.fillna(2).values))\n</code></pre>\n\n<hr>\n\n<pre><code>print(df)\n    decimal_places  amount  converted_amount\n0             2.0    10.0              10.0\n1             3.0   100.0              10.0\n2             1.0  1000.0           10000.0\n</code></pre>\n",
        "question_body": "<p>I have a <code>df</code>,</p>\n\n<pre><code>decimal_places    amount\n 2                10\n 3                100\n 1                1000\n</code></pre>\n\n<p>I want to create a new column <code>converted_amount</code> based on the following calculation: </p>\n\n<pre><code>df = (df.assign(\n        converted_amount=lambda x: x.amount * 10 ** (2 - x.decimal_places.fillna(2, downcast='infer'))))\n</code></pre>\n\n<p>but I got the following error,</p>\n\n<pre><code>ValueError: Integers to negative integer powers are not allowed.\n</code></pre>\n\n<p>I am wondering how to get around this, so the result will look like,</p>\n\n<pre><code>decimal_places    amount    converted_amount\n   2                10           10\n   3                100          10\n   1                1000         10000\n</code></pre>\n",
        "formatted_input": {
            "qid": 57726939,
            "link": "https://stackoverflow.com/questions/57726939/pandas-integers-to-negative-integer-powers-are-not-allowed",
            "question": {
                "title": "pandas Integers to negative integer powers are not allowed",
                "ques_desc": "I have a , I want to create a new column based on the following calculation: but I got the following error, I am wondering how to get around this, so the result will look like, "
            },
            "io": [
                "decimal_places    amount\n 2                10\n 3                100\n 1                1000\n",
                "decimal_places    amount    converted_amount\n   2                10           10\n   3                100          10\n   1                1000         10000\n"
            ],
            "answer": {
                "ans_desc": "There are two problems here. Firstly as the error suggests, the values must be floats. But secondly, by setting , the values in are cast to integer as all decimal places are , and hence you'll get the same error. So you want: For a faster approach you could work with the underlying numpy arrays: ",
                "code": [
                    "df = df.astype(float)\ndf = (df.assign(\n      converted_amount=lambda x: x.amount * 10 ** (2 - x.decimal_places.fillna(2))))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 6025,
            "user_id": 7306999,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://i.stack.imgur.com/9dNzL.jpg?s=128&g=1",
            "display_name": "Xukrao",
            "link": "https://stackoverflow.com/users/7306999/xukrao"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 57650205,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1566774381,
        "creation_date": 1566771610,
        "last_edit_date": 1566773408,
        "question_id": 57650114,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57650114/pandas-dataframe-uniformly-scale-down-values-when-column-sum-exceeds-treshold",
        "title": "Pandas dataframe: uniformly scale down values when column sum exceeds treshold",
        "body": "<h2>Initial Situation</h2>\n\n<p>Consider the following example dataframe:</p>\n\n<pre><code>df = pd.DataFrame({\n    'A': [3., 2., 1., np.nan],\n    'B': [7., np.nan, 1., 3.],\n    'C': [4., 5., 1., 2.],\n    'D': [1., 0., 2., 3.]    \n})\n</code></pre>\n\n<p>which in printed form looks like:</p>\n\n<pre><code>     A    B    C    D\n0  3.0  7.0  4.0  1.0\n1  2.0  NaN  5.0  0.0\n2  1.0  1.0  1.0  2.0\n3  NaN  3.0  2.0  3.0\n</code></pre>\n\n<h2>Desired Result</h2>\n\n<p>I would now like to do the following for each column of this dataframe:</p>\n\n<ol>\n<li>Calculate the sum of the column's values (ignoring any NaN values).</li>\n<li>If the sum exceeds 10.0, then I want to uniformly scale down all values in the column such that the new sum is exactly 10.0 (again ignoring any NaN values).</li>\n</ol>\n\n<p>Basically I'd like to obtain a result dataframe that looks like this:</p>\n\n<pre><code>     A         B         C    D\n0  3.0  6.363636  3.333333  1.0\n1  2.0       NaN  4.166667  0.0\n2  1.0  0.909091  0.833333  2.0\n3  NaN  2.727273  1.666667  3.0\n</code></pre>\n\n<h2>Tried thus far</h2>\n\n<p>The following code obtains the desired result.</p>\n\n<pre><code>def helper_func(s):\n    if s.sum() &gt; 10.:\n        return s * 10. / s.sum()\n    else:\n        return s\n\nresult_df = df.apply(helper_func)\n</code></pre>\n\n<p>However this code feels a bit verbose and inefficient to me. Based on my experience with pandas thus far I'd suspect that a more vectorized solution is still possible. Would anyone be able to help me find this?</p>\n",
        "answer_body": "<p>Here is one method:</p>\n\n<pre><code>thres = 10\nresult = df * thres / df.sum().clip(lower=thres)\n</code></pre>\n",
        "question_body": "<h2>Initial Situation</h2>\n\n<p>Consider the following example dataframe:</p>\n\n<pre><code>df = pd.DataFrame({\n    'A': [3., 2., 1., np.nan],\n    'B': [7., np.nan, 1., 3.],\n    'C': [4., 5., 1., 2.],\n    'D': [1., 0., 2., 3.]    \n})\n</code></pre>\n\n<p>which in printed form looks like:</p>\n\n<pre><code>     A    B    C    D\n0  3.0  7.0  4.0  1.0\n1  2.0  NaN  5.0  0.0\n2  1.0  1.0  1.0  2.0\n3  NaN  3.0  2.0  3.0\n</code></pre>\n\n<h2>Desired Result</h2>\n\n<p>I would now like to do the following for each column of this dataframe:</p>\n\n<ol>\n<li>Calculate the sum of the column's values (ignoring any NaN values).</li>\n<li>If the sum exceeds 10.0, then I want to uniformly scale down all values in the column such that the new sum is exactly 10.0 (again ignoring any NaN values).</li>\n</ol>\n\n<p>Basically I'd like to obtain a result dataframe that looks like this:</p>\n\n<pre><code>     A         B         C    D\n0  3.0  6.363636  3.333333  1.0\n1  2.0       NaN  4.166667  0.0\n2  1.0  0.909091  0.833333  2.0\n3  NaN  2.727273  1.666667  3.0\n</code></pre>\n\n<h2>Tried thus far</h2>\n\n<p>The following code obtains the desired result.</p>\n\n<pre><code>def helper_func(s):\n    if s.sum() &gt; 10.:\n        return s * 10. / s.sum()\n    else:\n        return s\n\nresult_df = df.apply(helper_func)\n</code></pre>\n\n<p>However this code feels a bit verbose and inefficient to me. Based on my experience with pandas thus far I'd suspect that a more vectorized solution is still possible. Would anyone be able to help me find this?</p>\n",
        "formatted_input": {
            "qid": 57650114,
            "link": "https://stackoverflow.com/questions/57650114/pandas-dataframe-uniformly-scale-down-values-when-column-sum-exceeds-treshold",
            "question": {
                "title": "Pandas dataframe: uniformly scale down values when column sum exceeds treshold",
                "ques_desc": "Initial Situation Consider the following example dataframe: which in printed form looks like: Desired Result I would now like to do the following for each column of this dataframe: Calculate the sum of the column's values (ignoring any NaN values). If the sum exceeds 10.0, then I want to uniformly scale down all values in the column such that the new sum is exactly 10.0 (again ignoring any NaN values). Basically I'd like to obtain a result dataframe that looks like this: Tried thus far The following code obtains the desired result. However this code feels a bit verbose and inefficient to me. Based on my experience with pandas thus far I'd suspect that a more vectorized solution is still possible. Would anyone be able to help me find this? "
            },
            "io": [
                "     A    B    C    D\n0  3.0  7.0  4.0  1.0\n1  2.0  NaN  5.0  0.0\n2  1.0  1.0  1.0  2.0\n3  NaN  3.0  2.0  3.0\n",
                "     A         B         C    D\n0  3.0  6.363636  3.333333  1.0\n1  2.0       NaN  4.166667  0.0\n2  1.0  0.909091  0.833333  2.0\n3  NaN  2.727273  1.666667  3.0\n"
            ],
            "answer": {
                "ans_desc": "Here is one method: ",
                "code": [
                    "thres = 10\nresult = df * thres / df.sum().clip(lower=thres)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 153,
            "user_id": 5997528,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/joq7s.jpg?s=128&g=1",
            "display_name": "Ionut Deaconu",
            "link": "https://stackoverflow.com/users/5997528/ionut-deaconu"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 57589650,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1566383658,
        "creation_date": 1566383167,
        "question_id": 57589538,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57589538/identical-dictionaries-in-dataframe-all-changing-at-once",
        "title": "Identical dictionaries in DataFrame all changing at once",
        "body": "<p>I'm working on a project and currently experiencing an issue with populating some dictionaries within a DataFrame. The problem is more complicated but essentially the main issue can be simplified as follows:</p>\n\n<p>I have a DataFrame of dictionaries, all of which initially empty, say</p>\n\n<pre><code>example = pd.DataFrame([[dict()] * 2], index=range(0, 3))\n</code></pre>\n\n<pre><code>    0   1   \n\n0   {}  {}  \n1   {}  {}  \n2   {}  {}  \n</code></pre>\n\n<p>When I attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one I'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a':</p>\n\n<pre><code>example.iloc[0][0]['char'] = 'a'\n</code></pre>\n\n<pre><code>          0               1         \n0   {'char': 'a'}   {'char': 'a'}   \n1   {'char': 'a'}   {'char': 'a'}\n2   {'char': 'a'}   {'char': 'a'}   \n</code></pre>\n\n<p>I'm assuming this behaviour is caused by using <code>[[dict()] * 2]</code> in my DataFrame initialization, but I'm not familiar enough with Python to understand why. Is Python creating one dictionary and passing references to it to populate the DataFrame? If so, how could I initialise it to create individual dictionaries? I found that I can create a deep copy of each dictionary before processing them, i.e. <code>example.applymap(lambda d: copy.deepcopy(d))</code>, but I'm curious if there is a way of doing it without resorting to that.</p>\n",
        "answer_body": "<blockquote>\n  <p>I have a DataFrame of dictionaries, all of which initially empty</p>\n</blockquote>\n\n<p>If you multiply a list with an integer, you do <em>not</em> make a deep copy, you just copy the references to a new list. This thus means that you made a <em>single</em> dictionary, not two identical ones. So you have one dictionary, and all the six cells in the dataframe refer to the <em>same</em> dictionary. If you modify that dictionary, of course all the cells see that change.</p>\n\n<p>You can use list comprehension to make different dictionaries, for example:</p>\n\n<pre><code>example = pd.DataFrame([<b>[{} for _ in range(2)]</b> for _ in range(3)], index=range(0, 3))</code></pre>\n\n<p>or for an arbitrary length <code>m</code> rows and <code>n</code> columns:</p>\n\n<pre><code>example = pd.DataFrame([<b>[{} for _ in range(n)]</b> for _ in range(m)])</code></pre>\n",
        "question_body": "<p>I'm working on a project and currently experiencing an issue with populating some dictionaries within a DataFrame. The problem is more complicated but essentially the main issue can be simplified as follows:</p>\n\n<p>I have a DataFrame of dictionaries, all of which initially empty, say</p>\n\n<pre><code>example = pd.DataFrame([[dict()] * 2], index=range(0, 3))\n</code></pre>\n\n<pre><code>    0   1   \n\n0   {}  {}  \n1   {}  {}  \n2   {}  {}  \n</code></pre>\n\n<p>When I attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one I'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a':</p>\n\n<pre><code>example.iloc[0][0]['char'] = 'a'\n</code></pre>\n\n<pre><code>          0               1         \n0   {'char': 'a'}   {'char': 'a'}   \n1   {'char': 'a'}   {'char': 'a'}\n2   {'char': 'a'}   {'char': 'a'}   \n</code></pre>\n\n<p>I'm assuming this behaviour is caused by using <code>[[dict()] * 2]</code> in my DataFrame initialization, but I'm not familiar enough with Python to understand why. Is Python creating one dictionary and passing references to it to populate the DataFrame? If so, how could I initialise it to create individual dictionaries? I found that I can create a deep copy of each dictionary before processing them, i.e. <code>example.applymap(lambda d: copy.deepcopy(d))</code>, but I'm curious if there is a way of doing it without resorting to that.</p>\n",
        "formatted_input": {
            "qid": 57589538,
            "link": "https://stackoverflow.com/questions/57589538/identical-dictionaries-in-dataframe-all-changing-at-once",
            "question": {
                "title": "Identical dictionaries in DataFrame all changing at once",
                "ques_desc": "I'm working on a project and currently experiencing an issue with populating some dictionaries within a DataFrame. The problem is more complicated but essentially the main issue can be simplified as follows: I have a DataFrame of dictionaries, all of which initially empty, say When I attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one I'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a': I'm assuming this behaviour is caused by using in my DataFrame initialization, but I'm not familiar enough with Python to understand why. Is Python creating one dictionary and passing references to it to populate the DataFrame? If so, how could I initialise it to create individual dictionaries? I found that I can create a deep copy of each dictionary before processing them, i.e. , but I'm curious if there is a way of doing it without resorting to that. "
            },
            "io": [
                "    0   1   \n\n0   {}  {}  \n1   {}  {}  \n2   {}  {}  \n",
                "          0               1         \n0   {'char': 'a'}   {'char': 'a'}   \n1   {'char': 'a'}   {'char': 'a'}\n2   {'char': 'a'}   {'char': 'a'}   \n"
            ],
            "answer": {
                "ans_desc": " I have a DataFrame of dictionaries, all of which initially empty If you multiply a list with an integer, you do not make a deep copy, you just copy the references to a new list. This thus means that you made a single dictionary, not two identical ones. So you have one dictionary, and all the six cells in the dataframe refer to the same dictionary. If you modify that dictionary, of course all the cells see that change. You can use list comprehension to make different dictionaries, for example: or for an arbitrary length rows and columns: ",
                "code": [
                    "example = pd.DataFrame([[{} for _ in range(2)] for _ in range(3)], index=range(0, 3))",
                    "example = pd.DataFrame([[{} for _ in range(n)] for _ in range(m)])"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 385,
            "user_id": 11049287,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4778d97b12d9d78d70a2fc3ffb10602f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Madhanlal",
            "link": "https://stackoverflow.com/users/11049287/madhanlal"
        },
        "is_answered": true,
        "view_count": 516,
        "accepted_answer_id": 57533748,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1566023952,
        "creation_date": 1566021061,
        "question_id": 57533654,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57533654/ignore-nulls-in-pandas-map-dictionary",
        "title": "Ignore Nulls in pandas map dictionary",
        "body": "<p>My Dataframe looks like this :</p>\n\n<pre><code>COL1    COL2    COL3\nA        M       X\nB        F       Y\nNaN      M       Y\nA        nan     Y\n</code></pre>\n\n<p>I am trying to label encode with nulls as such. My result should look like:</p>\n\n<pre><code>COL1_    COL2_    COL3_\n0        0       0\n1        1       1\nNaN      0       1\n0        nan     1\n</code></pre>\n\n<p>The code i tried :</p>\n\n<pre><code>modified_l2 = {}\nfor val in list(df_obj.columns): \n    modified_l2[val] = {k: i for i,k in enumerate(df_obj[val].unique(),0)}\n\nfor cols in modified_l2.keys():\n    df_obj[cols+'_']=df_obj[cols].map(modified_l2[cols],na_action='ignore')\n</code></pre>\n\n<p>Achieved Result :</p>\n\n<p><a href=\"https://i.stack.imgur.com/bYRDD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/bYRDD.png\" alt=\"enter image description here\"></a></p>\n\n<p>Expected Result :</p>\n\n<p><a href=\"https://i.stack.imgur.com/PVkRd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/PVkRd.png\" alt=\"enter image description here\"></a></p>\n",
        "answer_body": "<p>Try using the below code, I first use the <code>apply</code> function, than I drop the NaNs, then I convert it into a list then I use the <code>list.index</code> method for each value in the new list, and <code>list.index</code> gives the index of the first occurence of the value, after that convert it into the Series, and make the <code>index</code> the index of the series without NaNs, I am doing that since after I drop the NaNs it will turn from index 0, 1, 2, 3 to 0, 2, 3 or something like that, whereas the missing index will be NaN again, after that I add a underscore to each column, and I <code>join</code> it with the original dataframe:</p>\n\n<pre><code>print(df.join(df.apply(lambda x: pd.Series(map(x.dropna().tolist().index, x.dropna()), index=x.dropna().index)).add_suffix('_')))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>  COL1 COL2 COL3  COL1_  COL2_  COL3_\n0    A    M    X    0.0    0.0      0\n1    B    F    Y    1.0    1.0      1\n2  NaN    M    Y    NaN    0.0      1\n3    A  NaN    Y    0.0    NaN      1\n</code></pre>\n",
        "question_body": "<p>My Dataframe looks like this :</p>\n\n<pre><code>COL1    COL2    COL3\nA        M       X\nB        F       Y\nNaN      M       Y\nA        nan     Y\n</code></pre>\n\n<p>I am trying to label encode with nulls as such. My result should look like:</p>\n\n<pre><code>COL1_    COL2_    COL3_\n0        0       0\n1        1       1\nNaN      0       1\n0        nan     1\n</code></pre>\n\n<p>The code i tried :</p>\n\n<pre><code>modified_l2 = {}\nfor val in list(df_obj.columns): \n    modified_l2[val] = {k: i for i,k in enumerate(df_obj[val].unique(),0)}\n\nfor cols in modified_l2.keys():\n    df_obj[cols+'_']=df_obj[cols].map(modified_l2[cols],na_action='ignore')\n</code></pre>\n\n<p>Achieved Result :</p>\n\n<p><a href=\"https://i.stack.imgur.com/bYRDD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/bYRDD.png\" alt=\"enter image description here\"></a></p>\n\n<p>Expected Result :</p>\n\n<p><a href=\"https://i.stack.imgur.com/PVkRd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/PVkRd.png\" alt=\"enter image description here\"></a></p>\n",
        "formatted_input": {
            "qid": 57533654,
            "link": "https://stackoverflow.com/questions/57533654/ignore-nulls-in-pandas-map-dictionary",
            "question": {
                "title": "Ignore Nulls in pandas map dictionary",
                "ques_desc": "My Dataframe looks like this : I am trying to label encode with nulls as such. My result should look like: The code i tried : Achieved Result : Expected Result : "
            },
            "io": [
                "COL1    COL2    COL3\nA        M       X\nB        F       Y\nNaN      M       Y\nA        nan     Y\n",
                "COL1_    COL2_    COL3_\n0        0       0\n1        1       1\nNaN      0       1\n0        nan     1\n"
            ],
            "answer": {
                "ans_desc": "Try using the below code, I first use the function, than I drop the NaNs, then I convert it into a list then I use the method for each value in the new list, and gives the index of the first occurence of the value, after that convert it into the Series, and make the the index of the series without NaNs, I am doing that since after I drop the NaNs it will turn from index 0, 1, 2, 3 to 0, 2, 3 or something like that, whereas the missing index will be NaN again, after that I add a underscore to each column, and I it with the original dataframe: Output: ",
                "code": [
                    "print(df.join(df.apply(lambda x: pd.Series(map(x.dropna().tolist().index, x.dropna()), index=x.dropna().index)).add_suffix('_')))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 465,
            "user_id": 11831806,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-Ft17asFByR4/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfRMl72ErQTkvGaz7cJQxW7VJcduA/photo.jpg?sz=128",
            "display_name": "dfahsjdahfsudaf",
            "link": "https://stackoverflow.com/users/11831806/dfahsjdahfsudaf"
        },
        "is_answered": true,
        "view_count": 358,
        "accepted_answer_id": 57516070,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1565902902,
        "creation_date": 1565901651,
        "question_id": 57515994,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57515994/changing-the-increment-value-in-data-frame-at-certain-row",
        "title": "Changing the Increment Value in Data Frame at Certain Row",
        "body": "<p>I have this data frame:</p>\n\n<pre><code>   Power\n    15\n    15\n    10\n    30 \n    15 \n    90\n    100\n    22\n    15\n</code></pre>\n\n<p>I wanted to create another column called 'Seconds' that increments by 10 every row so I wrote this code: </p>\n\n<pre><code>df.index += 1\ndf['Seconds'] = 10 * df.index.values\n</code></pre>\n\n<p>This produces the data frame:</p>\n\n<pre><code>Seconds   Power\n  10       15\n  20       15\n  30       10\n  40       30\n  50       15\n  60       90\n  70       100\n  80       22\n  90       15\n</code></pre>\n\n<p>I now want to make the Seconds column increment by 10 until the 5th row. At the 5th row, I want to change the increment to 0.1 until the 7th row. At the 7th row, I want to change the increment back to 10.</p>\n\n<p>So, I want the data frame to look like this:</p>\n\n<pre><code>Seconds   Power\n  10       15\n  20       15\n  30       10\n  40       30\n  50       15\n  50.1     90\n  50.2     100\n  60.2      22\n  70.2      15\n</code></pre>\n\n<p>How would I go about doing this? Should I change the index and multiply the index.values by a different value when I get to the row where the increment needs to change?</p>\n\n<p>Thanks in advance for your help.</p>\n",
        "answer_body": "<p>You can use <code>numpy.repeat</code> + <code>cumsum</code></p>\n\n<hr>\n\n<pre><code>np.repeat([10, 0.1, 10], [5, 2, 2]).cumsum()\n#                         ^  ^  ^\n#                        5th ^  ^\n#                           7th ^\n#                               Until end -&gt; df.shape - increments[:-1].sum() more generally\n</code></pre>\n\n<p></p>\n\n<pre><code>array([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])\n</code></pre>\n\n<hr>\n\n<p>In a more general sense, it is probably more intuitive to not have to calculate out the repeats by hand, and it would be easier to define them not by their difference with the previous value, but by absolute location in the array.</p>\n\n<p>Using some arithmetic, we can create a function that does all of the grunt-work for you.</p>\n\n<pre><code>def cumsum_custom_increment(increments, points, n):\n    points[1:] = np.diff(points)\n    d = np.r_[points, n - np.sum(points)]\n    return np.repeat(increments, d).cumsum()\n\n&gt;&gt;&gt; cumsum_custom_increment([10, 0.1, 10], [5, 7], df.shape[0])\narray([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])\n</code></pre>\n",
        "question_body": "<p>I have this data frame:</p>\n\n<pre><code>   Power\n    15\n    15\n    10\n    30 \n    15 \n    90\n    100\n    22\n    15\n</code></pre>\n\n<p>I wanted to create another column called 'Seconds' that increments by 10 every row so I wrote this code: </p>\n\n<pre><code>df.index += 1\ndf['Seconds'] = 10 * df.index.values\n</code></pre>\n\n<p>This produces the data frame:</p>\n\n<pre><code>Seconds   Power\n  10       15\n  20       15\n  30       10\n  40       30\n  50       15\n  60       90\n  70       100\n  80       22\n  90       15\n</code></pre>\n\n<p>I now want to make the Seconds column increment by 10 until the 5th row. At the 5th row, I want to change the increment to 0.1 until the 7th row. At the 7th row, I want to change the increment back to 10.</p>\n\n<p>So, I want the data frame to look like this:</p>\n\n<pre><code>Seconds   Power\n  10       15\n  20       15\n  30       10\n  40       30\n  50       15\n  50.1     90\n  50.2     100\n  60.2      22\n  70.2      15\n</code></pre>\n\n<p>How would I go about doing this? Should I change the index and multiply the index.values by a different value when I get to the row where the increment needs to change?</p>\n\n<p>Thanks in advance for your help.</p>\n",
        "formatted_input": {
            "qid": 57515994,
            "link": "https://stackoverflow.com/questions/57515994/changing-the-increment-value-in-data-frame-at-certain-row",
            "question": {
                "title": "Changing the Increment Value in Data Frame at Certain Row",
                "ques_desc": "I have this data frame: I wanted to create another column called 'Seconds' that increments by 10 every row so I wrote this code: This produces the data frame: I now want to make the Seconds column increment by 10 until the 5th row. At the 5th row, I want to change the increment to 0.1 until the 7th row. At the 7th row, I want to change the increment back to 10. So, I want the data frame to look like this: How would I go about doing this? Should I change the index and multiply the index.values by a different value when I get to the row where the increment needs to change? Thanks in advance for your help. "
            },
            "io": [
                "Seconds   Power\n  10       15\n  20       15\n  30       10\n  40       30\n  50       15\n  60       90\n  70       100\n  80       22\n  90       15\n",
                "Seconds   Power\n  10       15\n  20       15\n  30       10\n  40       30\n  50       15\n  50.1     90\n  50.2     100\n  60.2      22\n  70.2      15\n"
            ],
            "answer": {
                "ans_desc": "You can use + In a more general sense, it is probably more intuitive to not have to calculate out the repeats by hand, and it would be easier to define them not by their difference with the previous value, but by absolute location in the array. Using some arithmetic, we can create a function that does all of the grunt-work for you. ",
                "code": [
                    "np.repeat([10, 0.1, 10], [5, 2, 2]).cumsum()\n#                         ^  ^  ^\n#                        5th ^  ^\n#                           7th ^\n#                               Until end -> df.shape - increments[:-1].sum() more generally\n",
                    "array([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])\n",
                    "def cumsum_custom_increment(increments, points, n):\n    points[1:] = np.diff(points)\n    d = np.r_[points, n - np.sum(points)]\n    return np.repeat(increments, d).cumsum()\n\n>>> cumsum_custom_increment([10, 0.1, 10], [5, 7], df.shape[0])\narray([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 143,
            "user_id": 6239923,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-8jxfKD4nyHw/AAAAAAAAAAI/AAAAAAAAA_4/-4p64veEWL8/photo.jpg?sz=128",
            "display_name": "SDM1212",
            "link": "https://stackoverflow.com/users/6239923/sdm1212"
        },
        "is_answered": true,
        "view_count": 70,
        "accepted_answer_id": 57481770,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1565770711,
        "creation_date": 1565712846,
        "last_edit_date": 1565716254,
        "question_id": 57481676,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57481676/in-a-dataframe-how-can-i-count-a-specific-value-and-then-select-the-value-with-t",
        "title": "In a dataframe how can I count a specific value and then select the value with the highest count to create another dataframe?",
        "body": "<p>I am looking for a way to select specific rows of data from a dataframe.  Here is an example of the dataframe.</p>\n\n<pre><code>Id  \\  Value\n0    002D85EF   5\n1    002D85EF   1\n2    002D85EF   5\n3    00557D1B   1\n4    00557D1B   1\n5    00557D1B   5\n6    0063EAFB   5\n7    0063EAFB   5\n8    0063EAFB   5\n9    006DE4E3   1\n10   006DE4E3   5\n11   006DE4E3   1\n12   006DE4E3   5\n</code></pre>\n\n<p>I am looking for an output frame like this:</p>\n\n<pre><code>Id  \\  Value\n0    002D85EF   5\n1    00557D1B   1\n2    0063EAFB   5\n</code></pre>\n\n<p>Note,  ID 006DE4E3  is not in the output because there the counts of the value was equal.</p>\n\n<p>Thank You!</p>\n",
        "answer_body": "<p>(Since i can not comment, i directly try to give You a hint, probably not the answer.)</p>\n\n<p>try:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df.groupby('Value').max()\n</code></pre>\n\n<p>or </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df.groupby('Value').count().max()\n</code></pre>\n\n<p>btw. the given row ids in Your answer do not match the above frame row ids and values. Also i don't understand why You do not select the <code>10   006DE4E3   5</code> row</p>\n\n<p><strong>edit</strong>\nafter clarification, i think You want is:\nreturn the first of each occurrence of the Id (in a sorted frame). But only, if the all values of the ids group are not distributed equally.\nFor that my answer is:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\na = np.random.randint(5, high=10, size=(20, 1))\nb = np.random.choice(['a', 'b', 'c', 'd'], 20)[:, None]\nc = pd.DataFrame(np.hstack([b,a]), columns=['id', 'value'])\n\n\ndef first_or_none(grp, col_name):\n    cnts = grp.groupby(col_name).count()\n    if len(cnts) == len(cnts.nunique()):\n        return None\n    else:\n        return grp.iloc[0]\n\nc.groupby(['id']).apply(first_or_none, 'value').dropna()\n</code></pre>\n\n<p>In this example the frame <em>c</em> is unsorted...</p>\n",
        "question_body": "<p>I am looking for a way to select specific rows of data from a dataframe.  Here is an example of the dataframe.</p>\n\n<pre><code>Id  \\  Value\n0    002D85EF   5\n1    002D85EF   1\n2    002D85EF   5\n3    00557D1B   1\n4    00557D1B   1\n5    00557D1B   5\n6    0063EAFB   5\n7    0063EAFB   5\n8    0063EAFB   5\n9    006DE4E3   1\n10   006DE4E3   5\n11   006DE4E3   1\n12   006DE4E3   5\n</code></pre>\n\n<p>I am looking for an output frame like this:</p>\n\n<pre><code>Id  \\  Value\n0    002D85EF   5\n1    00557D1B   1\n2    0063EAFB   5\n</code></pre>\n\n<p>Note,  ID 006DE4E3  is not in the output because there the counts of the value was equal.</p>\n\n<p>Thank You!</p>\n",
        "formatted_input": {
            "qid": 57481676,
            "link": "https://stackoverflow.com/questions/57481676/in-a-dataframe-how-can-i-count-a-specific-value-and-then-select-the-value-with-t",
            "question": {
                "title": "In a dataframe how can I count a specific value and then select the value with the highest count to create another dataframe?",
                "ques_desc": "I am looking for a way to select specific rows of data from a dataframe. Here is an example of the dataframe. I am looking for an output frame like this: Note, ID 006DE4E3 is not in the output because there the counts of the value was equal. Thank You! "
            },
            "io": [
                "Id  \\  Value\n0    002D85EF   5\n1    002D85EF   1\n2    002D85EF   5\n3    00557D1B   1\n4    00557D1B   1\n5    00557D1B   5\n6    0063EAFB   5\n7    0063EAFB   5\n8    0063EAFB   5\n9    006DE4E3   1\n10   006DE4E3   5\n11   006DE4E3   1\n12   006DE4E3   5\n",
                "Id  \\  Value\n0    002D85EF   5\n1    00557D1B   1\n2    0063EAFB   5\n"
            ],
            "answer": {
                "ans_desc": "(Since i can not comment, i directly try to give You a hint, probably not the answer.) try: or btw. the given row ids in Your answer do not match the above frame row ids and values. Also i don't understand why You do not select the row edit after clarification, i think You want is: return the first of each occurrence of the Id (in a sorted frame). But only, if the all values of the ids group are not distributed equally. For that my answer is: In this example the frame c is unsorted... ",
                "code": [
                    "import numpy as np\nimport pandas as pd\na = np.random.randint(5, high=10, size=(20, 1))\nb = np.random.choice(['a', 'b', 'c', 'd'], 20)[:, None]\nc = pd.DataFrame(np.hstack([b,a]), columns=['id', 'value'])\n\n\ndef first_or_none(grp, col_name):\n    cnts = grp.groupby(col_name).count()\n    if len(cnts) == len(cnts.nunique()):\n        return None\n    else:\n        return grp.iloc[0]\n\nc.groupby(['id']).apply(first_or_none, 'value').dropna()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "multiple-columns"
        ],
        "owner": {
            "reputation": 103,
            "user_id": 11922644,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/29fc4bab9a6f0f1741708ec6e2d1b460?s=128&d=identicon&r=PG&f=1",
            "display_name": "ash90",
            "link": "https://stackoverflow.com/users/11922644/ash90"
        },
        "is_answered": true,
        "view_count": 650,
        "accepted_answer_id": 57479601,
        "answer_count": 4,
        "score": 3,
        "last_activity_date": 1565706212,
        "creation_date": 1565704730,
        "last_edit_date": 1565706212,
        "question_id": 57479385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57479385/how-to-rearrange-reorder-the-rows-and-columns-in-python-dataframe",
        "title": "How to rearrange/reorder the rows and columns in python dataframe?",
        "body": "<p><a href=\"https://i.stack.imgur.com/2TWVn.png\" rel=\"nofollow noreferrer\">SCREEN SHOT OF ACTUAL DATA FRAME</a><strong>Dataframe of 5000 rows and 192 columns</strong></p>\n\n<p>I want to change the size of my data frame of m rows and n columns (m= 5000 and n = 192) into a size of n/3 rows(64 rows) and m*5000 columns(15000 columns)??</p>\n\n<p><strong>existing data frame</strong></p>\n\n<pre><code>0 A1 A2 A3 A4 A5 A6 A7 A8 A9.....A192\n1 B1 B2 B3 B4 B5 B6 B7 B8 B9.....B192\n.\n.\n.\n5000 192 X1 X2 X3 X4 X5 X6 X7 X8 X9.....X192\n</code></pre>\n\n<p><strong>DESIRED data frame</strong></p>\n\n<pre><code>0 A1 A2 A3 B1 B2 B3.....X1 X2 X3\n1 A4 A5 A6 B4 B5 B6.....X4 X5 X6\n2 A7 A8 A9 B7 B8 B9.....X7 X8 X9\n.\n.\n64 A190 A191 A192 B190 B191 B192.....X190 X191 X192\n</code></pre>\n",
        "answer_body": "<p>IIUC, can use a loop</p>\n\n<pre><code>pd.DataFrame([df.iloc[:, e:e+3].values.flatten() for e in range(0, 192, 3)])\n</code></pre>\n",
        "question_body": "<p><a href=\"https://i.stack.imgur.com/2TWVn.png\" rel=\"nofollow noreferrer\">SCREEN SHOT OF ACTUAL DATA FRAME</a><strong>Dataframe of 5000 rows and 192 columns</strong></p>\n\n<p>I want to change the size of my data frame of m rows and n columns (m= 5000 and n = 192) into a size of n/3 rows(64 rows) and m*5000 columns(15000 columns)??</p>\n\n<p><strong>existing data frame</strong></p>\n\n<pre><code>0 A1 A2 A3 A4 A5 A6 A7 A8 A9.....A192\n1 B1 B2 B3 B4 B5 B6 B7 B8 B9.....B192\n.\n.\n.\n5000 192 X1 X2 X3 X4 X5 X6 X7 X8 X9.....X192\n</code></pre>\n\n<p><strong>DESIRED data frame</strong></p>\n\n<pre><code>0 A1 A2 A3 B1 B2 B3.....X1 X2 X3\n1 A4 A5 A6 B4 B5 B6.....X4 X5 X6\n2 A7 A8 A9 B7 B8 B9.....X7 X8 X9\n.\n.\n64 A190 A191 A192 B190 B191 B192.....X190 X191 X192\n</code></pre>\n",
        "formatted_input": {
            "qid": 57479385,
            "link": "https://stackoverflow.com/questions/57479385/how-to-rearrange-reorder-the-rows-and-columns-in-python-dataframe",
            "question": {
                "title": "How to rearrange/reorder the rows and columns in python dataframe?",
                "ques_desc": "SCREEN SHOT OF ACTUAL DATA FRAMEDataframe of 5000 rows and 192 columns I want to change the size of my data frame of m rows and n columns (m= 5000 and n = 192) into a size of n/3 rows(64 rows) and m*5000 columns(15000 columns)?? existing data frame DESIRED data frame "
            },
            "io": [
                "0 A1 A2 A3 A4 A5 A6 A7 A8 A9.....A192\n1 B1 B2 B3 B4 B5 B6 B7 B8 B9.....B192\n.\n.\n.\n5000 192 X1 X2 X3 X4 X5 X6 X7 X8 X9.....X192\n",
                "0 A1 A2 A3 B1 B2 B3.....X1 X2 X3\n1 A4 A5 A6 B4 B5 B6.....X4 X5 X6\n2 A7 A8 A9 B7 B8 B9.....X7 X8 X9\n.\n.\n64 A190 A191 A192 B190 B191 B192.....X190 X191 X192\n"
            ],
            "answer": {
                "ans_desc": "IIUC, can use a loop ",
                "code": [
                    "pd.DataFrame([df.iloc[:, e:e+3].values.flatten() for e in range(0, 192, 3)])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 11311413,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-FgHB0S5yQ-k/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcRXm91UEbGAI-_DVyr1-TcvTWi1A/mo/photo.jpg?sz=128",
            "display_name": "marcowitt",
            "link": "https://stackoverflow.com/users/11311413/marcowitt"
        },
        "is_answered": true,
        "view_count": 578,
        "accepted_answer_id": 57428686,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1565348895,
        "creation_date": 1565348249,
        "last_edit_date": 1565348895,
        "question_id": 57428633,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57428633/change-value-of-datetime-in-pandas-dataframe",
        "title": "Change value of datetime in pandas Dataframe",
        "body": "<p>i want to change the value of a pandas dataframe column with datetime format.\nBasically i want to add always 20 seconds to a row. Im new to python/pandas so i dont know any ways to solve that issue.</p>\n\n<p>Here is my code so far:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df_date = dataframe['date'] # get date column\ndf_date = datedf.to_frame() #convert series to df\n\n\nfor index, row in df_date.iterrows():\n    if (index % 3 == 0):\n        #minute +1 f. e. from 12:00:40 to 12:01:0\n\n    elif (index % 3 == 1) or (index % 3 == 2):     \n        # seconds + 20 \n        # from 12:01:00 to 12:01:20\n        # or from 12:01:20 to 12:01:40\n\n</code></pre>\n\n<p>Dataframe df_date:<br/><br/>\noriginal:</p>\n\n<pre><code>0  2017-03-10 01:00:00\n1  2017-03-10 01:00:00\n2  2017-03-10 01:00:00\n3  2017-03-10 01:00:00\n4  2017-03-10 01:00:00\n</code></pre>\n\n<p>...<br/>\n<br/>\nexpected:</p>\n\n<pre><code>0  2017-03-10 01:00:20\n1  2017-03-10 01:00:40\n2  2017-03-10 01:01:00\n3  2017-03-10 01:00:20\n4  2017-03-10 01:00:40\n</code></pre>\n\n<p>...<br/></p>\n\n<p>Any help on this would be appreciated!</p>\n",
        "answer_body": "<p>Add timedeltas created by <code>np.arange</code> multiple by <code>20</code> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_timedelta.html\" rel=\"nofollow noreferrer\"><code>to_timedelta</code></a>:</p>\n\n<pre><code>td = pd.to_timedelta(np.arange(1, len(dataframe) + 1) * 20, unit='s')\n</code></pre>\n\n<p>Or created by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.timedelta_range.html\" rel=\"nofollow noreferrer\"><code>timedelta_range</code></a>:</p>\n\n<pre><code>td = pd.timedelta_range(0, periods=len(dataframe), freq='20s') \n</code></pre>\n\n<hr>\n\n<pre><code>dataframe['date'] = pd.to_datetime(dataframe['date']) + td\nprint (dataframe)\n                 date\n0 2017-03-10 01:00:20\n1 2017-03-10 01:00:40\n2 2017-03-10 01:01:00\n3 2017-03-10 01:01:20\n4 2017-03-10 01:01:40\n</code></pre>\n",
        "question_body": "<p>i want to change the value of a pandas dataframe column with datetime format.\nBasically i want to add always 20 seconds to a row. Im new to python/pandas so i dont know any ways to solve that issue.</p>\n\n<p>Here is my code so far:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df_date = dataframe['date'] # get date column\ndf_date = datedf.to_frame() #convert series to df\n\n\nfor index, row in df_date.iterrows():\n    if (index % 3 == 0):\n        #minute +1 f. e. from 12:00:40 to 12:01:0\n\n    elif (index % 3 == 1) or (index % 3 == 2):     \n        # seconds + 20 \n        # from 12:01:00 to 12:01:20\n        # or from 12:01:20 to 12:01:40\n\n</code></pre>\n\n<p>Dataframe df_date:<br/><br/>\noriginal:</p>\n\n<pre><code>0  2017-03-10 01:00:00\n1  2017-03-10 01:00:00\n2  2017-03-10 01:00:00\n3  2017-03-10 01:00:00\n4  2017-03-10 01:00:00\n</code></pre>\n\n<p>...<br/>\n<br/>\nexpected:</p>\n\n<pre><code>0  2017-03-10 01:00:20\n1  2017-03-10 01:00:40\n2  2017-03-10 01:01:00\n3  2017-03-10 01:00:20\n4  2017-03-10 01:00:40\n</code></pre>\n\n<p>...<br/></p>\n\n<p>Any help on this would be appreciated!</p>\n",
        "formatted_input": {
            "qid": 57428633,
            "link": "https://stackoverflow.com/questions/57428633/change-value-of-datetime-in-pandas-dataframe",
            "question": {
                "title": "Change value of datetime in pandas Dataframe",
                "ques_desc": "i want to change the value of a pandas dataframe column with datetime format. Basically i want to add always 20 seconds to a row. Im new to python/pandas so i dont know any ways to solve that issue. Here is my code so far: Dataframe df_date: original: ... expected: ... Any help on this would be appreciated! "
            },
            "io": [
                "0  2017-03-10 01:00:00\n1  2017-03-10 01:00:00\n2  2017-03-10 01:00:00\n3  2017-03-10 01:00:00\n4  2017-03-10 01:00:00\n",
                "0  2017-03-10 01:00:20\n1  2017-03-10 01:00:40\n2  2017-03-10 01:01:00\n3  2017-03-10 01:00:20\n4  2017-03-10 01:00:40\n"
            ],
            "answer": {
                "ans_desc": "Add timedeltas created by multiple by with : Or created by : ",
                "code": [
                    "td = pd.to_timedelta(np.arange(1, len(dataframe) + 1) * 20, unit='s')\n",
                    "td = pd.timedelta_range(0, periods=len(dataframe), freq='20s') \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 4310,
            "user_id": 3608005,
            "user_type": "registered",
            "accept_rate": 73,
            "profile_image": "https://www.gravatar.com/avatar/235dcc1c473c4be1ddec787d32687650?s=128&d=identicon&r=PG",
            "display_name": "Moritz",
            "link": "https://stackoverflow.com/users/3608005/moritz"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 57412029,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1565266692,
        "creation_date": 1565264257,
        "last_edit_date": 1565265151,
        "question_id": 57411679,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57411679/pandas-concatenate-levels-in-multiindex",
        "title": "Pandas concatenate levels in multiindex",
        "body": "<p>I do have following excel file:</p>\n\n<p><a href=\"https://i.stack.imgur.com/028Yd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/028Yd.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>{0: {0: nan, 1: nan, 2: nan, 3: 'A', 4: 'A', 5: 'B', 6: 'B', 7: 'C', 8: 'C'},\n 1: {0: nan, 1: nan, 2: nan, 3: 1.0, 4: 2.0, 5: 1.0, 6: 2.0, 7: 1.0, 8: 2.0},\n 2: {0: 'AA1', 1: 'a', 2: 'ng/mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 3: {0: 'AA2', 1: 'a', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 4: {0: 'BB1', 1: 'b', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 5: {0: 'BB2', 1: 'b', 2: 'mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 6: {0: 'CC1', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 7: {0: 'CC2', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}}\n</code></pre>\n\n<p>I would like to create following dataframe:</p>\n\n<pre><code>level_0     AA1   AA2   CB1  BB2   CC1   CC2\nnew     a ng/mL a N/A b N/A b mL c N/A c N/A\n0 1                                         \nA 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nB 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nC 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\n</code></pre>\n\n<p>What I tried:</p>\n\n<pre><code># read the column index separately to avoid pandas inputting \"Unnamed: ...\"\n# for the nans\ndf = pd.read_excel(file_path, skiprows=3, index_col=None, header=None)\ndf.set_index([0, 1], inplace=True)\n\n# the column index\ncols = pd.read_excel(file_path, nrows=3, index_col=None, header=None).loc[:, 2:]\ncols = cols.fillna('N/A')\nidx = pd.MultiIndex.from_arrays(cols.values)\ndf.columns = idx\n</code></pre>\n\n<p>The new dataframe:</p>\n\n<pre><code>      AA1 AA2 CB1 BB2 CC1 CC2\n        a   a   b   b   c   c\n    ng/mL N/A N/A  mL N/A N/A\n0 1                          \nA 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\nB 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\nC 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\n</code></pre>\n\n<p>This approach works but is kind of tedious:</p>\n\n<pre><code>df1 = df.T.reset_index()\ndf1['new'] = df1.loc[:, 'level_1'] + ' ' + df1.loc[:, 'level_2']\ndf1.set_index(['level_0', 'new']).drop(['level_1', 'level_2'], axis=1).T\n</code></pre>\n\n<p>Which gives me:</p>\n\n<pre><code>level_0     AA1   AA2   CB1  BB2   CC1   CC2\nnew     a ng/mL a N/A b N/A b mL c N/A c N/A\n0 1                                         \nA 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nB 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nC 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\n</code></pre>\n\n<p>Is there a simpler solution available ?</p>\n",
        "answer_body": "<p>Use:</p>\n\n<pre><code>#file from sample data\n\nd = {0: {0:  np.nan, 1:  np.nan, 2:  np.nan, 3: 'A', 4: 'A', 5: 'B', 6: 'B', 7: 'C', 8: 'C'}, \n     1: {0:  np.nan, 1:  np.nan, 2:  np.nan, 3: 1.0, 4: 2.0, 5: 1.0, 6: 2.0, 7: 1.0, 8: 2.0}, \n     2: {0: 'AA1', 1: 'a', 2: 'ng/mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, \n     3: {0: 'AA2', 1: 'a', 2:  np.nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, \n     4: {0: 'BB1', 1: 'b', 2:  np.nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, \n     5: {0: 'BB2', 1: 'b', 2: 'mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, \n     6: {0: 'CC1', 1: 'c', 2:  np.nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, \n     7: {0: 'CC2', 1: 'c', 2:  np.nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}}\n\ndf = pd.DataFrame(d)\n\ndf.to_excel('file.xlsx', header=False, index=False)\n</code></pre>\n\n<p>First create <code>MultiIndex DataFrame</code> with <code>header=[0,1,2]</code>, then create <code>MultiIndex</code> by first 2 columns with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.set_index</code></a> and remove index names by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>DataFrame.reset_index</code></a>:</p>\n\n<pre><code>df = pd.read_excel('file.xlsx', header=[0,1,2])\n\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n</code></pre>\n\n<p>Then loop by each level in list comprehension and join second with third level if not <code>Unnamed</code>, last use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.from_tuples.html\" rel=\"nofollow noreferrer\"><code>MultiIndex.from_tuples</code></a>:</p>\n\n<pre><code>tuples = [(a, f'{b} N/A') if c.startswith('Unnamed') \n          else (a, f'{b} {c}') \n          for a, b, c in df.columns]\n\nprint (tuples)\n[('AA1', 'a ng/mL'), ('AA2', 'a N/A'), \n ('BB1', 'b N/A'), ('BB2', 'b mL'),\n ('CC1', 'c N/A'), ('CC2', 'c N/A')]\n\ndf.columns = pd.MultiIndex.from_tuples(tuples)\nprint (df)\n        AA1   AA2   BB1  BB2   CC1   CC2\n    a ng/mL a N/A b N/A b mL c N/A c N/A\nA 1       1     1     1    1     1     1\n  2       1     1     1    1     1     1\nB 1       1     1     1    1     1     1\n  2       1     1     1    1     1     1\nC 1       1     1     1    1     1     1\n  2       1     1     1    1     1     1\n</code></pre>\n\n<p>Another idea is use:</p>\n\n<pre><code>df = pd.read_excel('file.xlsx', header=[0,1,2])\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n\nlv1 = df.columns.get_level_values(0)\nlv2 = df.columns.get_level_values(1)\nlv3 = df.columns.get_level_values(2)\nlv3 = lv3.where(~lv3.str.startswith('Unnamed'),'N/A')\n\ndf.columns = [lv1, lv2.to_series() + ' ' + lv3]\n</code></pre>\n",
        "question_body": "<p>I do have following excel file:</p>\n\n<p><a href=\"https://i.stack.imgur.com/028Yd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/028Yd.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>{0: {0: nan, 1: nan, 2: nan, 3: 'A', 4: 'A', 5: 'B', 6: 'B', 7: 'C', 8: 'C'},\n 1: {0: nan, 1: nan, 2: nan, 3: 1.0, 4: 2.0, 5: 1.0, 6: 2.0, 7: 1.0, 8: 2.0},\n 2: {0: 'AA1', 1: 'a', 2: 'ng/mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 3: {0: 'AA2', 1: 'a', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 4: {0: 'BB1', 1: 'b', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 5: {0: 'BB2', 1: 'b', 2: 'mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 6: {0: 'CC1', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 7: {0: 'CC2', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}}\n</code></pre>\n\n<p>I would like to create following dataframe:</p>\n\n<pre><code>level_0     AA1   AA2   CB1  BB2   CC1   CC2\nnew     a ng/mL a N/A b N/A b mL c N/A c N/A\n0 1                                         \nA 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nB 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nC 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\n</code></pre>\n\n<p>What I tried:</p>\n\n<pre><code># read the column index separately to avoid pandas inputting \"Unnamed: ...\"\n# for the nans\ndf = pd.read_excel(file_path, skiprows=3, index_col=None, header=None)\ndf.set_index([0, 1], inplace=True)\n\n# the column index\ncols = pd.read_excel(file_path, nrows=3, index_col=None, header=None).loc[:, 2:]\ncols = cols.fillna('N/A')\nidx = pd.MultiIndex.from_arrays(cols.values)\ndf.columns = idx\n</code></pre>\n\n<p>The new dataframe:</p>\n\n<pre><code>      AA1 AA2 CB1 BB2 CC1 CC2\n        a   a   b   b   c   c\n    ng/mL N/A N/A  mL N/A N/A\n0 1                          \nA 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\nB 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\nC 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\n</code></pre>\n\n<p>This approach works but is kind of tedious:</p>\n\n<pre><code>df1 = df.T.reset_index()\ndf1['new'] = df1.loc[:, 'level_1'] + ' ' + df1.loc[:, 'level_2']\ndf1.set_index(['level_0', 'new']).drop(['level_1', 'level_2'], axis=1).T\n</code></pre>\n\n<p>Which gives me:</p>\n\n<pre><code>level_0     AA1   AA2   CB1  BB2   CC1   CC2\nnew     a ng/mL a N/A b N/A b mL c N/A c N/A\n0 1                                         \nA 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nB 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\nC 1           1     1     1    1     1     1\n  2           1     1     1    1     1     1\n</code></pre>\n\n<p>Is there a simpler solution available ?</p>\n",
        "formatted_input": {
            "qid": 57411679,
            "link": "https://stackoverflow.com/questions/57411679/pandas-concatenate-levels-in-multiindex",
            "question": {
                "title": "Pandas concatenate levels in multiindex",
                "ques_desc": "I do have following excel file: I would like to create following dataframe: What I tried: The new dataframe: This approach works but is kind of tedious: Which gives me: Is there a simpler solution available ? "
            },
            "io": [
                "{0: {0: nan, 1: nan, 2: nan, 3: 'A', 4: 'A', 5: 'B', 6: 'B', 7: 'C', 8: 'C'},\n 1: {0: nan, 1: nan, 2: nan, 3: 1.0, 4: 2.0, 5: 1.0, 6: 2.0, 7: 1.0, 8: 2.0},\n 2: {0: 'AA1', 1: 'a', 2: 'ng/mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 3: {0: 'AA2', 1: 'a', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 4: {0: 'BB1', 1: 'b', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 5: {0: 'BB2', 1: 'b', 2: 'mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 6: {0: 'CC1', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1},\n 7: {0: 'CC2', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}}\n",
                "      AA1 AA2 CB1 BB2 CC1 CC2\n        a   a   b   b   c   c\n    ng/mL N/A N/A  mL N/A N/A\n0 1                          \nA 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\nB 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\nC 1     1   1   1   1   1   1\n  2     1   1   1   1   1   1\n"
            ],
            "answer": {
                "ans_desc": "Use: First create with , then create by first 2 columns with and remove index names by : Then loop by each level in list comprehension and join second with third level if not , last use : Another idea is use: ",
                "code": [
                    "df = pd.read_excel('file.xlsx', header=[0,1,2])\n\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n",
                    "df = pd.read_excel('file.xlsx', header=[0,1,2])\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n\nlv1 = df.columns.get_level_values(0)\nlv2 = df.columns.get_level_values(1)\nlv3 = df.columns.get_level_values(2)\nlv3 = lv3.where(~lv3.str.startswith('Unnamed'),'N/A')\n\ndf.columns = [lv1, lv2.to_series() + ' ' + lv3]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "diff"
        ],
        "owner": {
            "reputation": 353,
            "user_id": 10409809,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-A9nuy_5shwM/AAAAAAAAAAI/AAAAAAAAASE/ydd_oiwK7Gc/photo.jpg?sz=128",
            "display_name": "jessirocha",
            "link": "https://stackoverflow.com/users/10409809/jessirocha"
        },
        "is_answered": true,
        "view_count": 54,
        "accepted_answer_id": 57376908,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1565189639,
        "creation_date": 1565095137,
        "last_edit_date": 1565189639,
        "question_id": 57376287,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57376287/identify-increasing-features-in-a-data-frame",
        "title": "Identify increasing features in a data frame",
        "body": "<p>I have a data frame that present some features with cumulative values. I need to identify those features in order to revert the cumulative values. \nThis is how my dataset looks (plus about 50 variables):</p>\n\n<pre><code>a      b     \n346    17    \n76     52    \n459    70    \n680    96    \n679    167   \n246    180   \n</code></pre>\n\n<p>What I wish to achieve is:</p>\n\n<pre><code>a      b     \n346    17    \n76     35    \n459    18    \n680    26    \n679    71   \n246    13   \n</code></pre>\n\n<p>I've seem this answer, but it first revert the values and then try to identify the columns. Can't I do the other way around? First identify the features and then revert the values?</p>\n\n<blockquote>\n  <p><a href=\"https://stackoverflow.com/questions/55195045/finding-cumulative-features-in-dataframe\">Finding cumulative features in dataframe?</a></p>\n</blockquote>\n\n<p>What I do at the moment is run the following code in order to give me the feature's names with cumulative values: </p>\n\n<pre><code> def accmulate_col(value):\n     count = 0\n     count_1 = False\n     name = []\n     for i in range(len(value)-1):\n         if value[i+1]-value[i] &gt;= 0:\n             count += 1\n         if value[i+1]-value[i] &gt; 0:\n             count_1 = True\n     name.append(1) if count == len(value)-1 and count_1 else name.append(0)\n     return name\n\n df.apply(accmulate_col)\n</code></pre>\n\n<p>Afterwards, I save these features names manually in a list called cum_features and revert the values, creating the desired dataset:</p>\n\n<pre><code>df_clean = df.copy()\ndf_clean[cum_cols] = df_clean[cum_features].apply(lambda col: np.diff(col, prepend=0))\n</code></pre>\n\n<p>Is there a better way to solve my problem? </p>\n",
        "answer_body": "<p>To identify which columns have increasing* values throughout the whole column, you will need to apply conditions on all the values. So in that sense, you <em>have</em> to use the values first to figure out what columns fit the conditions.</p>\n\n<p>With that out of the way, given a dataframe such as:</p>\n\n<pre><code>import pandas as pd\nd = {'a': [1,2,3,4],\n     'b': [4,3,2,1]\n     }\ndf = pd.DataFrame(d)\n#Output:\n   a  b\n0  1  4\n1  2  3\n2  3  2\n3  4  1\n</code></pre>\n\n<p>Figuring out which columns contain increasing values is just a question of using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\" rel=\"nofollow noreferrer\">diff</a> on all values in the dataframe, and checking which ones are increasing throughout the whole column.</p>\n\n<p>That can be written as:</p>\n\n<pre><code>out = (df.diff().dropna()&gt;0).all()\n#Output:\na     True\nb    False\ndtype: bool\n</code></pre>\n\n<p>Then, you can just use the column names to select only those with <code>True</code> in them</p>\n\n<pre><code>new_df = df[df.columns[out]]\n#Output:\n   a\n0  1\n1  2\n2  3\n3  4\n</code></pre>\n\n<hr>\n\n<p>*(the term cumulative doesn't really represent the conditions you used.Did you want it to be cumulative or just increasing? Cumulative implies that the value in a particular row/index was the sum of all previous values upto that index, while increasing is just that, the value in current row/index is greater than previous.)</p>\n",
        "question_body": "<p>I have a data frame that present some features with cumulative values. I need to identify those features in order to revert the cumulative values. \nThis is how my dataset looks (plus about 50 variables):</p>\n\n<pre><code>a      b     \n346    17    \n76     52    \n459    70    \n680    96    \n679    167   \n246    180   \n</code></pre>\n\n<p>What I wish to achieve is:</p>\n\n<pre><code>a      b     \n346    17    \n76     35    \n459    18    \n680    26    \n679    71   \n246    13   \n</code></pre>\n\n<p>I've seem this answer, but it first revert the values and then try to identify the columns. Can't I do the other way around? First identify the features and then revert the values?</p>\n\n<blockquote>\n  <p><a href=\"https://stackoverflow.com/questions/55195045/finding-cumulative-features-in-dataframe\">Finding cumulative features in dataframe?</a></p>\n</blockquote>\n\n<p>What I do at the moment is run the following code in order to give me the feature's names with cumulative values: </p>\n\n<pre><code> def accmulate_col(value):\n     count = 0\n     count_1 = False\n     name = []\n     for i in range(len(value)-1):\n         if value[i+1]-value[i] &gt;= 0:\n             count += 1\n         if value[i+1]-value[i] &gt; 0:\n             count_1 = True\n     name.append(1) if count == len(value)-1 and count_1 else name.append(0)\n     return name\n\n df.apply(accmulate_col)\n</code></pre>\n\n<p>Afterwards, I save these features names manually in a list called cum_features and revert the values, creating the desired dataset:</p>\n\n<pre><code>df_clean = df.copy()\ndf_clean[cum_cols] = df_clean[cum_features].apply(lambda col: np.diff(col, prepend=0))\n</code></pre>\n\n<p>Is there a better way to solve my problem? </p>\n",
        "formatted_input": {
            "qid": 57376287,
            "link": "https://stackoverflow.com/questions/57376287/identify-increasing-features-in-a-data-frame",
            "question": {
                "title": "Identify increasing features in a data frame",
                "ques_desc": "I have a data frame that present some features with cumulative values. I need to identify those features in order to revert the cumulative values. This is how my dataset looks (plus about 50 variables): What I wish to achieve is: I've seem this answer, but it first revert the values and then try to identify the columns. Can't I do the other way around? First identify the features and then revert the values? Finding cumulative features in dataframe? What I do at the moment is run the following code in order to give me the feature's names with cumulative values: Afterwards, I save these features names manually in a list called cum_features and revert the values, creating the desired dataset: Is there a better way to solve my problem? "
            },
            "io": [
                "a      b     \n346    17    \n76     52    \n459    70    \n680    96    \n679    167   \n246    180   \n",
                "a      b     \n346    17    \n76     35    \n459    18    \n680    26    \n679    71   \n246    13   \n"
            ],
            "answer": {
                "ans_desc": "To identify which columns have increasing* values throughout the whole column, you will need to apply conditions on all the values. So in that sense, you have to use the values first to figure out what columns fit the conditions. With that out of the way, given a dataframe such as: Figuring out which columns contain increasing values is just a question of using diff on all values in the dataframe, and checking which ones are increasing throughout the whole column. That can be written as: Then, you can just use the column names to select only those with in them *(the term cumulative doesn't really represent the conditions you used.Did you want it to be cumulative or just increasing? Cumulative implies that the value in a particular row/index was the sum of all previous values upto that index, while increasing is just that, the value in current row/index is greater than previous.) ",
                "code": [
                    "out = (df.diff().dropna()>0).all()\n#Output:\na     True\nb    False\ndtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1123,
            "user_id": 1735774,
            "user_type": "registered",
            "accept_rate": 70,
            "profile_image": "https://www.gravatar.com/avatar/7a8728db5177c7031e09301442144753?s=128&d=identicon&r=PG",
            "display_name": "Anonymous",
            "link": "https://stackoverflow.com/users/1735774/anonymous"
        },
        "is_answered": true,
        "view_count": 11290,
        "accepted_answer_id": 35682788,
        "answer_count": 2,
        "score": 8,
        "last_activity_date": 1565166065,
        "creation_date": 1456663656,
        "last_edit_date": 1456664119,
        "question_id": 35682719,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/35682719/drop-rows-with-a-question-mark-value-in-any-column-in-a-pandas-dataframe",
        "title": "Drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe",
        "body": "<p>I want to remove all rows (or take all rows without) a question mark symbol in any column. I also want to change the elements to <strong>float</strong> type.</p>\n\n<p><strong>Input:</strong></p>\n\n<pre><code>X Y Z\n0 1 ?\n1 2 3\n? ? 4\n4 4 4\n? 2 5\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>X Y Z\n1 2 3\n4 4 4\n</code></pre>\n\n<p>Preferably using pandas dataframe operations.</p>\n",
        "answer_body": "<p>You can try first find string <code>?</code> in columns, create boolean mask and last filter rows - use <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\" rel=\"noreferrer\">boolean indexing</a>. If you need convert columns to <code>float</code>, use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html\" rel=\"noreferrer\"><code>astype</code></a>:</p>\n\n<pre><code>print ~((df['X'] == '?' )  (df['Y'] == '?' ) | (df['Z'] == '?' ))\n0    False\n1     True\n2    False\n3     True\n4    False\ndtype: bool\n\n\ndf1 = df[~((df['X'] == '?' ) | (df['Y'] == '?' ) | (df['Z'] == '?' ))].astype(float)\nprint df1\n   X  Y  Z\n1  1  2  3\n3  4  4  4\n\nprint df1.dtypes\nX    float64\nY    float64\nZ    float64\ndtype: object\n</code></pre>\n\n<p>Or you can try:</p>\n\n<pre><code>df['X'] = pd.to_numeric(df['X'], errors='coerce')\ndf['Y'] = pd.to_numeric(df['Y'], errors='coerce')\ndf['Z'] = pd.to_numeric(df['Z'], errors='coerce')\nprint df\n    X   Y   Z\n0   0   1 NaN\n1   1   2   3\n2 NaN NaN   4\n3   4   4   4\n4 NaN   2   5\nprint ((df['X'].notnull() ) &amp; (df['Y'].notnull() ) &amp; (df['Z'].notnull() ))\n0    False\n1     True\n2    False\n3     True\n4    False\ndtype: bool\n\nprint df[ ((df['X'].notnull() ) &amp; (df['Y'].notnull() ) &amp; (df['Z'].notnull() )) ].astype(float)\n   X  Y  Z\n1  1  2  3\n3  4  4  4\n</code></pre>\n\n<p>Better is use:</p>\n\n<pre><code>df = df[(df != '?').all(axis=1)]\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df = df[~(df == '?').any(axis=1)]\n</code></pre>\n",
        "question_body": "<p>I want to remove all rows (or take all rows without) a question mark symbol in any column. I also want to change the elements to <strong>float</strong> type.</p>\n\n<p><strong>Input:</strong></p>\n\n<pre><code>X Y Z\n0 1 ?\n1 2 3\n? ? 4\n4 4 4\n? 2 5\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>X Y Z\n1 2 3\n4 4 4\n</code></pre>\n\n<p>Preferably using pandas dataframe operations.</p>\n",
        "formatted_input": {
            "qid": 35682719,
            "link": "https://stackoverflow.com/questions/35682719/drop-rows-with-a-question-mark-value-in-any-column-in-a-pandas-dataframe",
            "question": {
                "title": "Drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe",
                "ques_desc": "I want to remove all rows (or take all rows without) a question mark symbol in any column. I also want to change the elements to float type. Input: Output: Preferably using pandas dataframe operations. "
            },
            "io": [
                "X Y Z\n0 1 ?\n1 2 3\n? ? 4\n4 4 4\n? 2 5\n",
                "X Y Z\n1 2 3\n4 4 4\n"
            ],
            "answer": {
                "ans_desc": "You can try first find string in columns, create boolean mask and last filter rows - use boolean indexing. If you need convert columns to , use : Or you can try: Better is use: Or: ",
                "code": [
                    "df = df[(df != '?').all(axis=1)]\n",
                    "df = df[~(df == '?').any(axis=1)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 11580076,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/da335a8a70afdd822494891499fbea3e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Lee",
            "link": "https://stackoverflow.com/users/11580076/lee"
        },
        "is_answered": true,
        "view_count": 105,
        "accepted_answer_id": 57264807,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1564464333,
        "creation_date": 1564461578,
        "question_id": 57264388,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57264388/dataframe-to-dictionary-values-came-out-scrambled",
        "title": "Dataframe to dictionary, values came out scrambled",
        "body": "<p>I have a dataframe that contains two columns that I would like to convert into a dictionary to use as a map.<br>\nI have tried multiple ways of converting, but my dictionary values always comes up in the wrong order.\nMy python version is 3 and Pandas version is 0.24.2.</p>\n\n<p>This is what the first few rows of my dataframe looks like:</p>\n\n<pre><code>geozip.head()\nOut[30]: \n    Geoid    ZIP\n0  100100  36276\n1  100124  36310\n2  100460  35005\n3  100460  35062\n4  100460  35214\n</code></pre>\n\n<p>I would like my dictionary to look like this:</p>\n\n<pre><code>{100100: 36276,\n100124: 36310,\n100460: 35005,\n100460: 35062,\n100460: 35214,...}\n</code></pre>\n\n<p>But instead my outputs came up with the wrong order for the values.</p>\n\n<pre><code>{100100: 98520,\n 100124: 36310,\n 100460: 57520,\n 100484: 35540,\n 100676: 19018,\n 100820: 57311,\n 100988: 15483,\n 101132: 36861,...}\n</code></pre>\n\n<p>I tried this first but the dictionary came out unordered:</p>\n\n<pre><code>geozipmap = geozip.set_index('Geoid')['ZIP'].to_dict()\n</code></pre>\n\n<p>Then I tried coverting the two columns into list first then convert to dictionary, but same problem occurred:</p>\n\n<pre><code>geoid = geozip.Geoid.tolist()\nzipcode = geozip.ZIP.tolist()\ngeozipmap = dict(zip(geoid, zipcode))\n</code></pre>\n\n<p>I tried converting to OrderedDict and that didn't work either. \nThen I've tried:</p>\n\n<pre><code>geozipmap = {k: v for k, v in zip(geoid, zipcode)}\n</code></pre>\n\n<p>I've also tried:</p>\n\n<pre><code>geozipmap = {}\nfor index, g in enumerate(geoid):\n    geozipmap[geoid[index]] = zipcode[index]    \n</code></pre>\n\n<p>I've also tried the answers suggested:\n<a href=\"https://stackoverflow.com/questions/40983364/panda-dataframe-to-ordered-dictionary\">panda dataframe to ordered dictionary</a></p>\n\n<p>None of these work.  Really not sure what is going on?</p>\n",
        "answer_body": "<p>try this <code>default_dict</code> and if same <code>key</code> have multiple values you can provide those as list </p>\n\n<pre><code>from collections import defaultdict\n\ndf =pd.DataFrame(data={\"Geoid\":[100100,100124,100460,100460,100460],\n                   \"ZIP\":[36276,36310,35005,35062,35214]})\ndata_dict = defaultdict(list)\n\nfor k,v in zip(df['Geoid'],df['ZIP']):\n   data_dict[k].append(v)\n\nprint(data_dict)\n</code></pre>\n\n<pre><code>defaultdict(&lt;class 'list'&gt;, {100100: [36276], 100124: [36310], 100460: [35005, 35062, 35214]})\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that contains two columns that I would like to convert into a dictionary to use as a map.<br>\nI have tried multiple ways of converting, but my dictionary values always comes up in the wrong order.\nMy python version is 3 and Pandas version is 0.24.2.</p>\n\n<p>This is what the first few rows of my dataframe looks like:</p>\n\n<pre><code>geozip.head()\nOut[30]: \n    Geoid    ZIP\n0  100100  36276\n1  100124  36310\n2  100460  35005\n3  100460  35062\n4  100460  35214\n</code></pre>\n\n<p>I would like my dictionary to look like this:</p>\n\n<pre><code>{100100: 36276,\n100124: 36310,\n100460: 35005,\n100460: 35062,\n100460: 35214,...}\n</code></pre>\n\n<p>But instead my outputs came up with the wrong order for the values.</p>\n\n<pre><code>{100100: 98520,\n 100124: 36310,\n 100460: 57520,\n 100484: 35540,\n 100676: 19018,\n 100820: 57311,\n 100988: 15483,\n 101132: 36861,...}\n</code></pre>\n\n<p>I tried this first but the dictionary came out unordered:</p>\n\n<pre><code>geozipmap = geozip.set_index('Geoid')['ZIP'].to_dict()\n</code></pre>\n\n<p>Then I tried coverting the two columns into list first then convert to dictionary, but same problem occurred:</p>\n\n<pre><code>geoid = geozip.Geoid.tolist()\nzipcode = geozip.ZIP.tolist()\ngeozipmap = dict(zip(geoid, zipcode))\n</code></pre>\n\n<p>I tried converting to OrderedDict and that didn't work either. \nThen I've tried:</p>\n\n<pre><code>geozipmap = {k: v for k, v in zip(geoid, zipcode)}\n</code></pre>\n\n<p>I've also tried:</p>\n\n<pre><code>geozipmap = {}\nfor index, g in enumerate(geoid):\n    geozipmap[geoid[index]] = zipcode[index]    \n</code></pre>\n\n<p>I've also tried the answers suggested:\n<a href=\"https://stackoverflow.com/questions/40983364/panda-dataframe-to-ordered-dictionary\">panda dataframe to ordered dictionary</a></p>\n\n<p>None of these work.  Really not sure what is going on?</p>\n",
        "formatted_input": {
            "qid": 57264388,
            "link": "https://stackoverflow.com/questions/57264388/dataframe-to-dictionary-values-came-out-scrambled",
            "question": {
                "title": "Dataframe to dictionary, values came out scrambled",
                "ques_desc": "I have a dataframe that contains two columns that I would like to convert into a dictionary to use as a map. I have tried multiple ways of converting, but my dictionary values always comes up in the wrong order. My python version is 3 and Pandas version is 0.24.2. This is what the first few rows of my dataframe looks like: I would like my dictionary to look like this: But instead my outputs came up with the wrong order for the values. I tried this first but the dictionary came out unordered: Then I tried coverting the two columns into list first then convert to dictionary, but same problem occurred: I tried converting to OrderedDict and that didn't work either. Then I've tried: I've also tried: I've also tried the answers suggested: panda dataframe to ordered dictionary None of these work. Really not sure what is going on? "
            },
            "io": [
                "{100100: 36276,\n100124: 36310,\n100460: 35005,\n100460: 35062,\n100460: 35214,...}\n",
                "{100100: 98520,\n 100124: 36310,\n 100460: 57520,\n 100484: 35540,\n 100676: 19018,\n 100820: 57311,\n 100988: 15483,\n 101132: 36861,...}\n"
            ],
            "answer": {
                "ans_desc": "try this and if same have multiple values you can provide those as list ",
                "code": [
                    "defaultdict(<class 'list'>, {100100: [36276], 100124: [36310], 100460: [35005, 35062, 35214]})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "plot"
        ],
        "owner": {
            "reputation": 1524,
            "user_id": 2830451,
            "user_type": "registered",
            "accept_rate": 80,
            "profile_image": "https://www.gravatar.com/avatar/53b98d91a8c07539c66f7a3bd190276f?s=128&d=identicon&r=PG&f=1",
            "display_name": "user2830451",
            "link": "https://stackoverflow.com/users/2830451/user2830451"
        },
        "is_answered": true,
        "view_count": 585,
        "accepted_answer_id": 57263416,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1564458595,
        "creation_date": 1564445253,
        "last_edit_date": 1564447661,
        "question_id": 57262708,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57262708/plot-partial-dependency-with-model-built-from-pandas-data-frame",
        "title": "Plot partial dependency with model built from pandas data frame",
        "body": "<p>I have a model that is trained from a pandas dataframe. It can predict dataframe input without problem:</p>\n\n<pre><code>from xgboost import XGBClassifier\nclf = XGBClassifier()\nclf = clf.fit(X_train, y_train) # X_train is a pandas dataFrame with 5 columns: a,b,c,d,e.\nclf.predict_proba(X_train) \n</code></pre>\n\n<p>However, when I use the exact data and model to plot the partial dependency graph, I have the following error:</p>\n\n<pre><code>ValueError: feature_names mismatch: ['a', 'b', 'c', 'd', 'e'] ['f0', 'f1', 'f2', 'f3', 'f4']\nexpected b, a, d, c, e in input data\ntraining data did not have the following fields: f2, f3, f1, f0, f4\n</code></pre>\n\n<p>The code I used was:</p>\n\n<pre><code>plot_partial_dependence(estimator=clf, X=X_train, features=[0,1])\n</code></pre>\n\n<p>I understand that I can convert X_train to numpy.ndarray before training the model, and it solves the problem. However, as the actual classifier is very large and it took a long time to train already, I would like to re-use the classifier that was trained with pandas dataframe.</p>\n\n<p>Is there a way to do that? Thank you very much!</p>\n\n<p>Edit the OP to include some sample data:</p>\n\n<p>X_train.head(10):</p>\n\n<pre><code>    a        b        c    d           e\n0  34   226830  5249738  409  1186.78850\n1  36    38940  8210911   76  2326.72880\n2  36    38940  8210911   76  2326.72880\n3  34   761188  5074516  698   370.27365\n4  36  1097060  9072727  296   576.91693\n5  36  1097060  9072727  296   576.91693\n6  25    62240   881740  102   194.59651\n7  25    62240   881740  102   194.59651\n8  25    62240   881740  102   194.59651\n9  28    65484  1391620  105   259.25095\n</code></pre>\n\n<p>y_train.head(10):</p>\n\n<pre><code>0    1\n1    1\n2    1\n3    1\n4    1\n5    1\n6    1\n7    1\n8    1\n9    1\n</code></pre>\n",
        "answer_body": "<p>Congrats! You found a deficiency between <code>sklearn</code> and <code>xgboost</code>.</p>\n\n<p>Using the traceback to guide me, I stuck a <code>print(data.feature_names)</code> as the first line in <a href=\"https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/core.py#L1670\" rel=\"nofollow noreferrer\"><code>Booster._validate_features</code></a>. When I run your method (with dummy data I created), I get output like this:</p>\n\n<pre><code>['a', 'b', 'c', 'd', 'e']\n.\n.\n.\n['a', 'b', 'c', 'd', 'e']\n['f0', 'f1', 'f2', 'f3', 'f4']\n</code></pre>\n\n<p>The first several lines where the feature names are correct are from fitting the model. When fitting, apparently, it is possible to set the feature names. The last line is from calling the <code>plot_partial_dependence</code>. Seemingly, there is no way for sklearn to propagate the column names to xgboost using this method and so the latter defaults to 'f0', 'f1', etc.</p>\n\n<p><strong>WARNING: I am uncertain if disabling feature validation in the manner described below has adverse affects (namely, that feature names are confused). It is hard to tell when using dummy data as I have. Take the resulting partial dependence plots with a grain of salt. You may want to check the results from XGBClassifier against those from sklearn's GradientBoostingClassifier as a precaution. Or, rename the columns to ['f0', 'f1', 'f2', 'f3', 'f4'] prior to training.</strong></p>\n\n<p>On the plus side, you can get around this without having to change your column names. Ideally, the <code>plot_partial_dependence</code> function would allow us to specify a list of keyword arguments to pass to the <code>response_method</code> (i.e. <code>clf.predict_proba</code>) because we would ideally pass <code>validate_features=False</code>. In lieu of this interface, I propose the following hack:</p>\n\n<pre><code># store keyword argument default values\ntmpdefaults = XGBClassifier.predict_proba.__defaults__\n# change default value of validate_features to False\nXGBClassifier.predict_proba.__defaults__ = (None, False)\n\n# plot\nplot_partial_dependence(estimator=clf, X=X_train, features=[0, 1], feature_names=X_train.columns.tolist())\nplt.show()\n\n# reset default keyword argument values to original\nXGBClassifier.predict_proba.__defaults = tmpdefaults\n</code></pre>\n",
        "question_body": "<p>I have a model that is trained from a pandas dataframe. It can predict dataframe input without problem:</p>\n\n<pre><code>from xgboost import XGBClassifier\nclf = XGBClassifier()\nclf = clf.fit(X_train, y_train) # X_train is a pandas dataFrame with 5 columns: a,b,c,d,e.\nclf.predict_proba(X_train) \n</code></pre>\n\n<p>However, when I use the exact data and model to plot the partial dependency graph, I have the following error:</p>\n\n<pre><code>ValueError: feature_names mismatch: ['a', 'b', 'c', 'd', 'e'] ['f0', 'f1', 'f2', 'f3', 'f4']\nexpected b, a, d, c, e in input data\ntraining data did not have the following fields: f2, f3, f1, f0, f4\n</code></pre>\n\n<p>The code I used was:</p>\n\n<pre><code>plot_partial_dependence(estimator=clf, X=X_train, features=[0,1])\n</code></pre>\n\n<p>I understand that I can convert X_train to numpy.ndarray before training the model, and it solves the problem. However, as the actual classifier is very large and it took a long time to train already, I would like to re-use the classifier that was trained with pandas dataframe.</p>\n\n<p>Is there a way to do that? Thank you very much!</p>\n\n<p>Edit the OP to include some sample data:</p>\n\n<p>X_train.head(10):</p>\n\n<pre><code>    a        b        c    d           e\n0  34   226830  5249738  409  1186.78850\n1  36    38940  8210911   76  2326.72880\n2  36    38940  8210911   76  2326.72880\n3  34   761188  5074516  698   370.27365\n4  36  1097060  9072727  296   576.91693\n5  36  1097060  9072727  296   576.91693\n6  25    62240   881740  102   194.59651\n7  25    62240   881740  102   194.59651\n8  25    62240   881740  102   194.59651\n9  28    65484  1391620  105   259.25095\n</code></pre>\n\n<p>y_train.head(10):</p>\n\n<pre><code>0    1\n1    1\n2    1\n3    1\n4    1\n5    1\n6    1\n7    1\n8    1\n9    1\n</code></pre>\n",
        "formatted_input": {
            "qid": 57262708,
            "link": "https://stackoverflow.com/questions/57262708/plot-partial-dependency-with-model-built-from-pandas-data-frame",
            "question": {
                "title": "Plot partial dependency with model built from pandas data frame",
                "ques_desc": "I have a model that is trained from a pandas dataframe. It can predict dataframe input without problem: However, when I use the exact data and model to plot the partial dependency graph, I have the following error: The code I used was: I understand that I can convert X_train to numpy.ndarray before training the model, and it solves the problem. However, as the actual classifier is very large and it took a long time to train already, I would like to re-use the classifier that was trained with pandas dataframe. Is there a way to do that? Thank you very much! Edit the OP to include some sample data: X_train.head(10): y_train.head(10): "
            },
            "io": [
                "    a        b        c    d           e\n0  34   226830  5249738  409  1186.78850\n1  36    38940  8210911   76  2326.72880\n2  36    38940  8210911   76  2326.72880\n3  34   761188  5074516  698   370.27365\n4  36  1097060  9072727  296   576.91693\n5  36  1097060  9072727  296   576.91693\n6  25    62240   881740  102   194.59651\n7  25    62240   881740  102   194.59651\n8  25    62240   881740  102   194.59651\n9  28    65484  1391620  105   259.25095\n",
                "0    1\n1    1\n2    1\n3    1\n4    1\n5    1\n6    1\n7    1\n8    1\n9    1\n"
            ],
            "answer": {
                "ans_desc": "Congrats! You found a deficiency between and . Using the traceback to guide me, I stuck a as the first line in . When I run your method (with dummy data I created), I get output like this: The first several lines where the feature names are correct are from fitting the model. When fitting, apparently, it is possible to set the feature names. The last line is from calling the . Seemingly, there is no way for sklearn to propagate the column names to xgboost using this method and so the latter defaults to 'f0', 'f1', etc. WARNING: I am uncertain if disabling feature validation in the manner described below has adverse affects (namely, that feature names are confused). It is hard to tell when using dummy data as I have. Take the resulting partial dependence plots with a grain of salt. You may want to check the results from XGBClassifier against those from sklearn's GradientBoostingClassifier as a precaution. Or, rename the columns to ['f0', 'f1', 'f2', 'f3', 'f4'] prior to training. On the plus side, you can get around this without having to change your column names. Ideally, the function would allow us to specify a list of keyword arguments to pass to the (i.e. ) because we would ideally pass . In lieu of this interface, I propose the following hack: ",
                "code": [
                    "# store keyword argument default values\ntmpdefaults = XGBClassifier.predict_proba.__defaults__\n# change default value of validate_features to False\nXGBClassifier.predict_proba.__defaults__ = (None, False)\n\n# plot\nplot_partial_dependence(estimator=clf, X=X_train, features=[0, 1], feature_names=X_train.columns.tolist())\nplt.show()\n\n# reset default keyword argument values to original\nXGBClassifier.predict_proba.__defaults = tmpdefaults\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 11106040,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-JFfhYF8RDiU/AAAAAAAAAAI/AAAAAAAAAGY/-UNIo0YXsmo/photo.jpg?sz=128",
            "display_name": "Alexander M&#246;ller",
            "link": "https://stackoverflow.com/users/11106040/alexander-m%c3%b6ller"
        },
        "is_answered": true,
        "view_count": 88,
        "accepted_answer_id": 57234145,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1564272741,
        "creation_date": 1564245680,
        "last_edit_date": 1564272741,
        "question_id": 57234137,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57234137/group-rows-dates-and-summarize-serveral-columns-several-measured-values-for-e",
        "title": "group rows (dates) and summarize serveral columns (several measured values for eacht date) in Python Pandas",
        "body": "<p>I use Python Pandas and load a table like this from Postgres: </p>\n\n<pre><code>date                  v00  v01  v02  v03\n2001-01-01 00:00:00   30   40   50   100\n2001-01-01 00:00:00   80   120  20    60\n2001-02-01 00:00:00   20    70  50    20\n</code></pre>\n\n<p>I want to group the Date rows using Pandas and summarize the values. The result should look like this</p>\n\n<pre><code>2001-01-01 00:00:00   500\n2001-02-01 00:00:00   160\n</code></pre>\n\n<p>I can group the dates and summarize the values separately, but not in one view.</p>\n\n<p>My results are </p>\n\n<pre><code>1 220\n2 280\n3 160\n</code></pre>\n\n<p>And </p>\n\n<pre><code>date                  v00  v01  v02  v03\n2001-01-01 00:00:00   110  160  70   160\n2001-02-01 00:00:00   20    70  50    20\n</code></pre>\n\n<p>Thats the Code:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import psycopg2 as ps \nimport pandas as pd \nimport openpyxl\n\n\nconn = ps.connect(host=\"host\", user=\"user\", password=\"password\", dbname=\"Python_ueben\")\n\ncur = conn.cursor()\n\nprint('connect')\n\n\"\"\" schema = input(\"Geben Sie das Schema ein\")\ntable = input(\" Geben Sie die Tabele ein\") \"\"\"\n\n\ndef load_data(schema, table):\n\n    sql_command = \"SELECT * FROM {}.{};\".format(str(schema), str(table))\n    print (sql_command)\n\n    # Load the data\n    data = pd.read_sql(sql_command, conn)\n\n    groub = data.groupby(['date']) # group Date and save in variable\n    print(data.sum(axis=1, skipna=True)) #sum values v00 - v03\n    print(groub.sum(axis=1, skipna=True)) # group and sum, but not the right result\n    #print(data.groupby(['date']).sum(axis=0, skipna=False))\n\n\n\nload_data('public', 'zeitreihe')\n\n\n</code></pre>\n",
        "answer_body": "<p>If <code>Date</code> is column first aggregate sum and then <code>sum</code> per <code>axis=1</code>:</p>\n\n<pre><code>df1 = df.groupby('Date').sum().sum(axis=1).reset_index(name='sum')\nprint (df1)\n                  Date  sum\n0  2001-01-01 00:00:00  500\n1  2001-02-01 00:00:00  160\n</code></pre>\n\n<p>Or create <code>index</code> by <code>Date</code> column, then sum all rows and last sum per index (aggregate sum per index):</p>\n\n<pre><code>df1 = df.set_index('Date').sum(axis=1).sum(level=0).reset_index(name='sum')\n</code></pre>\n\n<p>If <code>Date</code> is index solution above is simplify:</p>\n\n<pre><code>df1 = df.sum(axis=1).sum(level=0).reset_index(name='sum')\n\ndf1 = df.sum(level=0).sum(axis=1).reset_index(name='sum')\n</code></pre>\n",
        "question_body": "<p>I use Python Pandas and load a table like this from Postgres: </p>\n\n<pre><code>date                  v00  v01  v02  v03\n2001-01-01 00:00:00   30   40   50   100\n2001-01-01 00:00:00   80   120  20    60\n2001-02-01 00:00:00   20    70  50    20\n</code></pre>\n\n<p>I want to group the Date rows using Pandas and summarize the values. The result should look like this</p>\n\n<pre><code>2001-01-01 00:00:00   500\n2001-02-01 00:00:00   160\n</code></pre>\n\n<p>I can group the dates and summarize the values separately, but not in one view.</p>\n\n<p>My results are </p>\n\n<pre><code>1 220\n2 280\n3 160\n</code></pre>\n\n<p>And </p>\n\n<pre><code>date                  v00  v01  v02  v03\n2001-01-01 00:00:00   110  160  70   160\n2001-02-01 00:00:00   20    70  50    20\n</code></pre>\n\n<p>Thats the Code:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import psycopg2 as ps \nimport pandas as pd \nimport openpyxl\n\n\nconn = ps.connect(host=\"host\", user=\"user\", password=\"password\", dbname=\"Python_ueben\")\n\ncur = conn.cursor()\n\nprint('connect')\n\n\"\"\" schema = input(\"Geben Sie das Schema ein\")\ntable = input(\" Geben Sie die Tabele ein\") \"\"\"\n\n\ndef load_data(schema, table):\n\n    sql_command = \"SELECT * FROM {}.{};\".format(str(schema), str(table))\n    print (sql_command)\n\n    # Load the data\n    data = pd.read_sql(sql_command, conn)\n\n    groub = data.groupby(['date']) # group Date and save in variable\n    print(data.sum(axis=1, skipna=True)) #sum values v00 - v03\n    print(groub.sum(axis=1, skipna=True)) # group and sum, but not the right result\n    #print(data.groupby(['date']).sum(axis=0, skipna=False))\n\n\n\nload_data('public', 'zeitreihe')\n\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 57234137,
            "link": "https://stackoverflow.com/questions/57234137/group-rows-dates-and-summarize-serveral-columns-several-measured-values-for-e",
            "question": {
                "title": "group rows (dates) and summarize serveral columns (several measured values for eacht date) in Python Pandas",
                "ques_desc": "I use Python Pandas and load a table like this from Postgres: I want to group the Date rows using Pandas and summarize the values. The result should look like this I can group the dates and summarize the values separately, but not in one view. My results are And Thats the Code: "
            },
            "io": [
                "2001-01-01 00:00:00   500\n2001-02-01 00:00:00   160\n",
                "1 220\n2 280\n3 160\n"
            ],
            "answer": {
                "ans_desc": "If is column first aggregate sum and then per : Or create by column, then sum all rows and last sum per index (aggregate sum per index): If is index solution above is simplify: ",
                "code": [
                    "df1 = df.sum(axis=1).sum(level=0).reset_index(name='sum')\n\ndf1 = df.sum(level=0).sum(axis=1).reset_index(name='sum')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "interpolation"
        ],
        "owner": {
            "reputation": 85,
            "user_id": 11044509,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-wBudXOy4x5I/AAAAAAAAAAI/AAAAAAAAAAA/ACevoQP7KoxkpqY2shW8ybx1LG8XF7pRsw/mo/photo.jpg?sz=128",
            "display_name": "NSK",
            "link": "https://stackoverflow.com/users/11044509/nsk"
        },
        "is_answered": true,
        "view_count": 125,
        "accepted_answer_id": 57143620,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1563865504,
        "creation_date": 1563787872,
        "last_edit_date": 1563789584,
        "question_id": 57143033,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57143033/using-pandas-interpolate",
        "title": "Using pandas.interpolate()",
        "body": "<p>Suppose </p>\n\n<pre><code>test = pd.DataFrame([1,2,3,np.nan,np.nan,np.nan,np.nan,np.nan,4,5,6,np.nan,np.nan,np.nan,np.nan,np.nan,3,4,np.nan])\n</code></pre>\n\n<p>I would like to apply following command:</p>\n\n<pre><code>test.interpolate(limit = 2, limit_direction  = 'both', limit_area = 'inside')\n</code></pre>\n\n<p>which returns</p>\n\n<pre><code>           0\n0   1.000000\n1   2.000000\n2   3.000000\n3   3.166667\n4   3.333333\n5        NaN\n6   3.666667\n7   3.833333\n8   4.000000\n9   5.000000\n10  6.000000\n11  5.500000\n12  5.000000\n13       NaN\n14  4.000000\n15  3.500000\n16  3.000000\n17  4.000000\n18       NaN\n</code></pre>\n\n<p>Question: How can i apply a restriction on the minimum number of valid numbers (i.e not NaN) before AND after a group of NaNs, so as to apply the interpolation</p>\n\n<p>In this example, i would like to fill first group of NaNs because there are minimum 3 valid numbers before AND after, but NOT interpolate the second group of NaNs, as there are only two valid numbers after the NaNs (and not 3 as i would prefer)</p>\n\n<p>Expected result:</p>\n\n<pre><code>           0\n0   1.000000\n1   2.000000\n2   3.000000\n3   3.166667\n4   3.333333\n5        NaN\n6   3.666667\n7   3.833333\n8   4.000000\n9   5.000000\n10  6.000000\n11       NaN\n12       NaN\n13       NaN\n14       NaN\n15       NaN\n16  3.000000\n17  4.000000\n18       NaN\n</code></pre>\n",
        "answer_body": "<p>EDIT 1: revised my first answer. One more go with some sort of mask approach based on <a href=\"https://stackoverflow.com/questions/41721674/find-consecutive-repeated-nan-in-a-numpy-array\">this Q&amp;A</a>.\nEDIT 2: added copy back to pd df using <code>deepcopy</code> to avoid the copy-by-reference issue.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    import numpy as np\n    import pandas as pd\n    from copy import deepcopy\n\n    a = np.array([1,2,3,np.nan,np.nan,np.nan,np.nan,np.nan,4,5,6,np.nan,np.nan,np.nan,np.nan,np.nan,3,4,np.nan,1])\n\n    df = pd.DataFrame(a)\n    # store values for later, to keep information from blocks that are below size limit:\n    temp = deepcopy(df[df[0].notnull()]) \n\n    mask = np.concatenate(([False],np.isfinite(a),[False]))\n    idx = np.nonzero(mask[1:] != mask[:-1])[0] # start and stop indices of your blocks of finite numbers\n    counts = (np.flatnonzero(mask[1:] &lt; mask[:-1]) - np.flatnonzero(mask[1:] &gt; mask[:-1])) # n finite numbers per block\n\n    sz_limit = 2 # set limit, exclusive in this case\n    for i, size in enumerate(counts):\n        if size &lt;= sz_limit:\n            a[idx[i*2]:idx[i*2+1]] = np.nan\n\n</code></pre>\n\n<p>now call the interpolation and write back values from 'too small' blocks:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>\n    a_inter = pd.DataFrame(a).interpolate(limit = 2, limit_direction = 'both', limit_area = 'inside') \n    a_inter.update(other = temp)  \n\n</code></pre>\n\n<p><code>a_inter</code> is then</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>           0\n0   1.000000\n1   2.000000\n2   3.000000\n3   3.166667\n4   3.333333\n5        NaN\n6   3.666667\n7   3.833333\n8   4.000000\n9   5.000000\n10  6.000000\n11       NaN\n12       NaN\n13       NaN\n14       NaN\n15       NaN\n16       NaN\n17       NaN\n18       NaN\n</code></pre>\n\n<p>to improve this hack, you could put the masking in a function and get rid of the for loop.</p>\n",
        "question_body": "<p>Suppose </p>\n\n<pre><code>test = pd.DataFrame([1,2,3,np.nan,np.nan,np.nan,np.nan,np.nan,4,5,6,np.nan,np.nan,np.nan,np.nan,np.nan,3,4,np.nan])\n</code></pre>\n\n<p>I would like to apply following command:</p>\n\n<pre><code>test.interpolate(limit = 2, limit_direction  = 'both', limit_area = 'inside')\n</code></pre>\n\n<p>which returns</p>\n\n<pre><code>           0\n0   1.000000\n1   2.000000\n2   3.000000\n3   3.166667\n4   3.333333\n5        NaN\n6   3.666667\n7   3.833333\n8   4.000000\n9   5.000000\n10  6.000000\n11  5.500000\n12  5.000000\n13       NaN\n14  4.000000\n15  3.500000\n16  3.000000\n17  4.000000\n18       NaN\n</code></pre>\n\n<p>Question: How can i apply a restriction on the minimum number of valid numbers (i.e not NaN) before AND after a group of NaNs, so as to apply the interpolation</p>\n\n<p>In this example, i would like to fill first group of NaNs because there are minimum 3 valid numbers before AND after, but NOT interpolate the second group of NaNs, as there are only two valid numbers after the NaNs (and not 3 as i would prefer)</p>\n\n<p>Expected result:</p>\n\n<pre><code>           0\n0   1.000000\n1   2.000000\n2   3.000000\n3   3.166667\n4   3.333333\n5        NaN\n6   3.666667\n7   3.833333\n8   4.000000\n9   5.000000\n10  6.000000\n11       NaN\n12       NaN\n13       NaN\n14       NaN\n15       NaN\n16  3.000000\n17  4.000000\n18       NaN\n</code></pre>\n",
        "formatted_input": {
            "qid": 57143033,
            "link": "https://stackoverflow.com/questions/57143033/using-pandas-interpolate",
            "question": {
                "title": "Using pandas.interpolate()",
                "ques_desc": "Suppose I would like to apply following command: which returns Question: How can i apply a restriction on the minimum number of valid numbers (i.e not NaN) before AND after a group of NaNs, so as to apply the interpolation In this example, i would like to fill first group of NaNs because there are minimum 3 valid numbers before AND after, but NOT interpolate the second group of NaNs, as there are only two valid numbers after the NaNs (and not 3 as i would prefer) Expected result: "
            },
            "io": [
                "           0\n0   1.000000\n1   2.000000\n2   3.000000\n3   3.166667\n4   3.333333\n5        NaN\n6   3.666667\n7   3.833333\n8   4.000000\n9   5.000000\n10  6.000000\n11  5.500000\n12  5.000000\n13       NaN\n14  4.000000\n15  3.500000\n16  3.000000\n17  4.000000\n18       NaN\n",
                "           0\n0   1.000000\n1   2.000000\n2   3.000000\n3   3.166667\n4   3.333333\n5        NaN\n6   3.666667\n7   3.833333\n8   4.000000\n9   5.000000\n10  6.000000\n11       NaN\n12       NaN\n13       NaN\n14       NaN\n15       NaN\n16  3.000000\n17  4.000000\n18       NaN\n"
            ],
            "answer": {
                "ans_desc": "EDIT 1: revised my first answer. One more go with some sort of mask approach based on this Q&A. EDIT 2: added copy back to pd df using to avoid the copy-by-reference issue. now call the interpolation and write back values from 'too small' blocks: is then to improve this hack, you could put the masking in a function and get rid of the for loop. ",
                "code": [
                    "    import numpy as np\n    import pandas as pd\n    from copy import deepcopy\n\n    a = np.array([1,2,3,np.nan,np.nan,np.nan,np.nan,np.nan,4,5,6,np.nan,np.nan,np.nan,np.nan,np.nan,3,4,np.nan,1])\n\n    df = pd.DataFrame(a)\n    # store values for later, to keep information from blocks that are below size limit:\n    temp = deepcopy(df[df[0].notnull()]) \n\n    mask = np.concatenate(([False],np.isfinite(a),[False]))\n    idx = np.nonzero(mask[1:] != mask[:-1])[0] # start and stop indices of your blocks of finite numbers\n    counts = (np.flatnonzero(mask[1:] < mask[:-1]) - np.flatnonzero(mask[1:] > mask[:-1])) # n finite numbers per block\n\n    sz_limit = 2 # set limit, exclusive in this case\n    for i, size in enumerate(counts):\n        if size <= sz_limit:\n            a[idx[i*2]:idx[i*2+1]] = np.nan\n\n",
                    "\n    a_inter = pd.DataFrame(a).interpolate(limit = 2, limit_direction = 'both', limit_area = 'inside') \n    a_inter.update(other = temp)  \n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 119,
            "user_id": 9764257,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/570f861aaa0062bd99c5e42a33115600?s=128&d=identicon&r=PG&f=1",
            "display_name": "John Dign",
            "link": "https://stackoverflow.com/users/9764257/john-dign"
        },
        "is_answered": true,
        "view_count": 97,
        "accepted_answer_id": 57140736,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1563779419,
        "creation_date": 1563778632,
        "last_edit_date": 1563779419,
        "question_id": 57140598,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57140598/add-columns-to-a-dataset-without-any-columns",
        "title": "Add columns to a dataset without any columns",
        "body": "<p>I want to load a dataset into a dataframe and then add columns to the dataset. Right now when I add columns, it removes the first line of data.</p>\n\n<p>To visualize for happens;</p>\n\n<p>Let's assume the following data from a csv is loaded to the dataframe</p>\n\n<p>21,5,14</p>\n\n<p>456,47,1</p>\n\n<p>47,89,66</p>\n\n<pre><code># Assume that the user uploaded a CSV file\ndf = pd.read_csv(\n            io.StringIO('csv_file_data', index_col=False, low_memory=False)\n</code></pre>\n\n<p>It will look like this</p>\n\n<pre><code>   21  5  14\n0  456 47 1\n1  47  89 66\n</code></pre>\n\n<p>So basically the first line of data is now shown as the columns, if your visualize the dataframe.</p>\n\n<p>When, I try to add columns\nWhere, file_structure, is a list with the columns</p>\n\n<pre><code>df.columns = file_structure\n</code></pre>\n\n<p>Does now look like this;</p>\n\n<pre><code>   x   y  z\n0  456 47 1\n1  47  89 66\n</code></pre>\n",
        "answer_body": "<pre><code>    df = pd.read_csv(\n            io.StringIO(decoded.decode('utf-8')), index_col=False, low_memory=False, header=None, names=file_structure\n)\n</code></pre>\n\n<p>names lets you set column names\nheader sets an index to use as colum names</p>\n\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv</a></p>\n",
        "question_body": "<p>I want to load a dataset into a dataframe and then add columns to the dataset. Right now when I add columns, it removes the first line of data.</p>\n\n<p>To visualize for happens;</p>\n\n<p>Let's assume the following data from a csv is loaded to the dataframe</p>\n\n<p>21,5,14</p>\n\n<p>456,47,1</p>\n\n<p>47,89,66</p>\n\n<pre><code># Assume that the user uploaded a CSV file\ndf = pd.read_csv(\n            io.StringIO('csv_file_data', index_col=False, low_memory=False)\n</code></pre>\n\n<p>It will look like this</p>\n\n<pre><code>   21  5  14\n0  456 47 1\n1  47  89 66\n</code></pre>\n\n<p>So basically the first line of data is now shown as the columns, if your visualize the dataframe.</p>\n\n<p>When, I try to add columns\nWhere, file_structure, is a list with the columns</p>\n\n<pre><code>df.columns = file_structure\n</code></pre>\n\n<p>Does now look like this;</p>\n\n<pre><code>   x   y  z\n0  456 47 1\n1  47  89 66\n</code></pre>\n",
        "formatted_input": {
            "qid": 57140598,
            "link": "https://stackoverflow.com/questions/57140598/add-columns-to-a-dataset-without-any-columns",
            "question": {
                "title": "Add columns to a dataset without any columns",
                "ques_desc": "I want to load a dataset into a dataframe and then add columns to the dataset. Right now when I add columns, it removes the first line of data. To visualize for happens; Let's assume the following data from a csv is loaded to the dataframe 21,5,14 456,47,1 47,89,66 It will look like this So basically the first line of data is now shown as the columns, if your visualize the dataframe. When, I try to add columns Where, file_structure, is a list with the columns Does now look like this; "
            },
            "io": [
                "   21  5  14\n0  456 47 1\n1  47  89 66\n",
                "   x   y  z\n0  456 47 1\n1  47  89 66\n"
            ],
            "answer": {
                "ans_desc": " names lets you set column names header sets an index to use as colum names https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv ",
                "code": [
                    "    df = pd.read_csv(\n            io.StringIO(decoded.decode('utf-8')), index_col=False, low_memory=False, header=None, names=file_structure\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 12012,
            "user_id": 687739,
            "user_type": "registered",
            "accept_rate": 77,
            "profile_image": "https://www.gravatar.com/avatar/0f8509ebd65dc536971efa17a619aa08?s=128&d=identicon&r=PG",
            "display_name": "Jason Strimpel",
            "link": "https://stackoverflow.com/users/687739/jason-strimpel"
        },
        "is_answered": true,
        "view_count": 240,
        "accepted_answer_id": 57050170,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1563256519,
        "creation_date": 1563251404,
        "question_id": 57050068,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57050068/pandas-list-of-tuples-to-multiindex",
        "title": "Pandas list of tuples to MultiIndex",
        "body": "<p>I have a <code>DataFrame</code> that looks like this:</p>\n\n<pre><code>    id    t_l\n0   100   [('a', 1), ('b', 2)]\n1   151   [('x', 4), ('y', 3)]\n</code></pre>\n\n<p>I need to return a <code>DataFrame</code> that looks like this:</p>\n\n<pre><code>    id    f    g\n0   100  'a'   1\n1        'b'   2\n2   151  'x'   4\n3        'y'   3\n</code></pre>\n\n<p>What is the best approach to this?</p>\n",
        "answer_body": "<p><strong>Edit</strong>: \n@ALollz makes a good point about speed of <code>np.concatenate</code> vs. <code>chain.from_iterable(df.t_l)</code>. I <code>%timeit</code> and it is true. Therefore, I added solution using <code>from_iterable(df.t_l)</code></p>\n\n<pre><code>from itertools import chain\npd.DataFrame(chain.from_iterable(df.t_l), index=np.repeat(df.id, df.t_l.str.len()), \\\n                                          columns=['f', 'g']).reset_index()\n</code></pre>\n\n<p><strong>Original</strong>:</p>\n\n<p>I would construct a new <code>df</code> using <code>np.concatenate</code> for data and <code>np.repeat</code> for index. Finally, <code>reset_index</code> to put <code>id</code> back to column</p>\n\n<pre><code>pd.DataFrame(np.concatenate(df.t_l), index=np.repeat(df.id, df.t_l.str.len()), \\\n                                              columns=['f', 'g']).reset_index()\n\nOut[596]:\n    id  f  g\n0  100  a  1\n1  100  b  2\n2  151  x  4\n3  151  y  3\n</code></pre>\n",
        "question_body": "<p>I have a <code>DataFrame</code> that looks like this:</p>\n\n<pre><code>    id    t_l\n0   100   [('a', 1), ('b', 2)]\n1   151   [('x', 4), ('y', 3)]\n</code></pre>\n\n<p>I need to return a <code>DataFrame</code> that looks like this:</p>\n\n<pre><code>    id    f    g\n0   100  'a'   1\n1        'b'   2\n2   151  'x'   4\n3        'y'   3\n</code></pre>\n\n<p>What is the best approach to this?</p>\n",
        "formatted_input": {
            "qid": 57050068,
            "link": "https://stackoverflow.com/questions/57050068/pandas-list-of-tuples-to-multiindex",
            "question": {
                "title": "Pandas list of tuples to MultiIndex",
                "ques_desc": "I have a that looks like this: I need to return a that looks like this: What is the best approach to this? "
            },
            "io": [
                "    id    t_l\n0   100   [('a', 1), ('b', 2)]\n1   151   [('x', 4), ('y', 3)]\n",
                "    id    f    g\n0   100  'a'   1\n1        'b'   2\n2   151  'x'   4\n3        'y'   3\n"
            ],
            "answer": {
                "ans_desc": "Edit: @ALollz makes a good point about speed of vs. . I and it is true. Therefore, I added solution using Original: I would construct a new using for data and for index. Finally, to put back to column ",
                "code": [
                    "from itertools import chain\npd.DataFrame(chain.from_iterable(df.t_l), index=np.repeat(df.id, df.t_l.str.len()), \\\n                                          columns=['f', 'g']).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 111,
            "user_id": 11696358,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a4b7e77985c50eecfc815080fbebb641?s=128&d=identicon&r=PG&f=1",
            "display_name": "user11696358",
            "link": "https://stackoverflow.com/users/11696358/user11696358"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 57043933,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1563208686,
        "creation_date": 1563207563,
        "question_id": 57043695,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57043695/pandas-downsample-a-more-frequent-dataframe-to-the-frequency-of-a-less-frequen",
        "title": "pandas - downsample a more frequent DataFrame to the frequency of a less frequent DataFrame",
        "body": "<p>I have two DataFrames that have different data measured at different frequencies, as in those csv examples:</p>\n\n<p>df1:</p>\n\n<pre><code>i,m1,m2,t\n0,0.556529,6.863255,43564.844\n1,0.5565576199999884,6.86327749999999,43564.863999999994\n2,0.5565559400000003,6.8632764,43564.884\n3,0.5565699799999941,6.863286799999996,43564.903999999995\n4,0.5565570200000007,6.863277200000001,43564.924\n5,0.5565316400000097,6.863257100000007,43564.944\n...\n</code></pre>\n\n<p>df2:</p>\n\n<pre><code>i,m3,m4,t\n0,306.81162500000596,-1.2126870045404683,43564.878125\n1,306.86175000000725,-1.1705838272666433,43564.928250000004\n2,306.77552454544787,-1.1240195386446195,43564.97837499999\n3,306.85900545454086,-1.0210345363692084,43565.0285\n4,306.8354250000052,-1.0052431772666657,43565.078625\n5,306.88397499999286,-0.9468344809917896,43565.12875\n...\n</code></pre>\n\n<p>I would like to obtain a single df that have all the measures of both dfs at the times of the first one (which get data less frequently).</p>\n\n<p>I tried to do that with a for loop averaging over the df2 measures between two timestamps of df1 but it was <strong>extremely slow</strong>.</p>\n",
        "answer_body": "<p>IIUC,  <code>i</code> is index column and you want to put <code>df2['t']</code> in bins and averaging the other columns. So you can use <code>pd.cut</code>:</p>\n\n<pre><code>groups =pd.cut(df2.t, bins= list(df1.t) + [np.inf],\n               right=False,\n               labels=df1['t'])\n\n# cols to copy\ncols = [col for col in df2.columns if col != 't']\n\n# groupby and get the average\nnew_df = (df2[cols].groupby(groups)\n                   .mean()\n                   .reset_index()\n         )\n</code></pre>\n\n<p>Then <code>new_df</code> is:</p>\n\n<pre><code>           t          m3        m4\n0  43564.844         NaN       NaN\n1  43564.864  306.811625 -1.212687\n2  43564.884         NaN       NaN\n3  43564.904         NaN       NaN\n4  43564.924  306.861750 -1.170584\n5  43564.944  306.838482 -1.024283\n</code></pre>\n\n<p>which you can merge with <code>df1</code> on <code>t</code>:</p>\n\n<pre><code>df1.merge(new_df, on='t', how='left')\n</code></pre>\n\n<p>and get:</p>\n\n<pre><code>         m1        m2        t          m3        m4\n0  0.556529  6.863255  43564.8         NaN       NaN\n1  0.556558  6.863277  43564.9  306.811625 -1.212687\n2  0.556556  6.863276  43564.9         NaN       NaN\n3  0.556570  6.863287  43564.9         NaN       NaN\n4  0.556557  6.863277  43564.9  306.861750 -1.170584\n5  0.556532  6.863257  43564.9  306.838482 -1.024283\n</code></pre>\n",
        "question_body": "<p>I have two DataFrames that have different data measured at different frequencies, as in those csv examples:</p>\n\n<p>df1:</p>\n\n<pre><code>i,m1,m2,t\n0,0.556529,6.863255,43564.844\n1,0.5565576199999884,6.86327749999999,43564.863999999994\n2,0.5565559400000003,6.8632764,43564.884\n3,0.5565699799999941,6.863286799999996,43564.903999999995\n4,0.5565570200000007,6.863277200000001,43564.924\n5,0.5565316400000097,6.863257100000007,43564.944\n...\n</code></pre>\n\n<p>df2:</p>\n\n<pre><code>i,m3,m4,t\n0,306.81162500000596,-1.2126870045404683,43564.878125\n1,306.86175000000725,-1.1705838272666433,43564.928250000004\n2,306.77552454544787,-1.1240195386446195,43564.97837499999\n3,306.85900545454086,-1.0210345363692084,43565.0285\n4,306.8354250000052,-1.0052431772666657,43565.078625\n5,306.88397499999286,-0.9468344809917896,43565.12875\n...\n</code></pre>\n\n<p>I would like to obtain a single df that have all the measures of both dfs at the times of the first one (which get data less frequently).</p>\n\n<p>I tried to do that with a for loop averaging over the df2 measures between two timestamps of df1 but it was <strong>extremely slow</strong>.</p>\n",
        "formatted_input": {
            "qid": 57043695,
            "link": "https://stackoverflow.com/questions/57043695/pandas-downsample-a-more-frequent-dataframe-to-the-frequency-of-a-less-frequen",
            "question": {
                "title": "pandas - downsample a more frequent DataFrame to the frequency of a less frequent DataFrame",
                "ques_desc": "I have two DataFrames that have different data measured at different frequencies, as in those csv examples: df1: df2: I would like to obtain a single df that have all the measures of both dfs at the times of the first one (which get data less frequently). I tried to do that with a for loop averaging over the df2 measures between two timestamps of df1 but it was extremely slow. "
            },
            "io": [
                "i,m1,m2,t\n0,0.556529,6.863255,43564.844\n1,0.5565576199999884,6.86327749999999,43564.863999999994\n2,0.5565559400000003,6.8632764,43564.884\n3,0.5565699799999941,6.863286799999996,43564.903999999995\n4,0.5565570200000007,6.863277200000001,43564.924\n5,0.5565316400000097,6.863257100000007,43564.944\n...\n",
                "i,m3,m4,t\n0,306.81162500000596,-1.2126870045404683,43564.878125\n1,306.86175000000725,-1.1705838272666433,43564.928250000004\n2,306.77552454544787,-1.1240195386446195,43564.97837499999\n3,306.85900545454086,-1.0210345363692084,43565.0285\n4,306.8354250000052,-1.0052431772666657,43565.078625\n5,306.88397499999286,-0.9468344809917896,43565.12875\n...\n"
            ],
            "answer": {
                "ans_desc": "IIUC, is index column and you want to put in bins and averaging the other columns. So you can use : Then is: which you can merge with on : and get: ",
                "code": [
                    "groups =pd.cut(df2.t, bins= list(df1.t) + [np.inf],\n               right=False,\n               labels=df1['t'])\n\n# cols to copy\ncols = [col for col in df2.columns if col != 't']\n\n# groupby and get the average\nnew_df = (df2[cols].groupby(groups)\n                   .mean()\n                   .reset_index()\n         )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 113,
            "user_id": 11715869,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/85372f5ce1c0a8b88f35c268474fcfdc?s=128&d=identicon&r=PG&f=1",
            "display_name": "blankslatecoder",
            "link": "https://stackoverflow.com/users/11715869/blankslatecoder"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 56976431,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1562782384,
        "creation_date": 1562781131,
        "last_edit_date": 1562781951,
        "question_id": 56976142,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56976142/error-when-trying-to-insert-into-dataframe",
        "title": "Error when trying to .insert() into dataframe",
        "body": "<p>I have some code that takes a csv file, that finds the min/max each day, then tells me what time that happens. I also have 2 variables to find the percentage for both max/min.</p>\n\n<p>This is currently the output for the dataframe</p>\n\n<pre><code>&gt;Out\n       High   Low\n10:00   6.0  10.0\n10:05  10.0   3.0\n10:10   1.0   7.0\n10:15   1.0   NaN\n10:20   4.0   4.0\n10:25   4.0   4.0\n10:30   5.0   1.0\n10:35   5.0   6.0\n10:40   3.0   2.0\n10:45   4.0   5.0\n10:50   4.0   1.0\n10:55   3.0   4.0\n11:00   4.0   5.0\n&gt;\n</code></pre>\n\n<p>Then I have 2 varables for the % of High/Lows.. (Just ph shown)</p>\n\n<pre><code>&gt;[84 rows x 2 columns]\nTime\n10:00    0.015306\n10:05    0.025510\n10:10    0.002551\n10:15    0.002551\n10:20    0.010204\n10:25    0.010204\n&gt;\n</code></pre>\n\n<p>I've tried to do an .insert(), but recieve this error.</p>\n\n<p>TypeError: insert() takes from 4 to 5 positional arguments but 6 were given</p>\n\n<p>This was my code</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#adding % to end of dataframe\nresult.insert(3,\"High %\", ph, \"Low %\", pl)\n</code></pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv(\"C:\\\\Users\\\\me\\\\Downloads\\\\file.csv\", encoding = \"ISO-8859-1\")\n\n\n#High grouped by Date\ndf2 = df.loc[df.groupby('Date')['High'].idxmax()]\n\n\n#dropping columns of no use\ndf2.drop(['Ticker','Open','Low','Close'], axis=1, inplace=True)\n\n#creating a variable to bucket the time\nTH = df2.groupby('Time').size()\n\n#Low grouped by Date\ndf3 = df.loc[df.groupby('Date')['Low'].idxmin()]\n\n#dropping columns of no use\ndf3.drop(['Ticker','Open','Low','Close'], axis=1, inplace=True)\n\n#creating a variable to bucket the time\nTL = df3.groupby('Time').size()\n\n#Merging Both Dataframes\nframes = [TH, TL]\nresult = pd.concat((frames), axis = 1)\nresult.columns = ['High','Low']\n\n#Percentage\nph = TH/TH.sum()\npl = TL/TL.sum()\n\n</code></pre>\n\n<p>I would like the output to show the % in columns 3 &amp;4</p>\n\n<pre><code>&gt;Out\n       High   Low    % High    %Low\n10:00   6.0  10.0    .015306   \n10:05  10.0   3.0    .025510\n10:10   1.0   7.0    .002551\n10:15   1.0   NaN    .002551\n10:20   4.0   4.0    .010204\n10:25   4.0   4.0    .010204\n\n&gt;\n</code></pre>\n",
        "answer_body": "<p>You can only add one column at a time with insert. And as you intend to add the new columns at the end of the dataframe you do not even need insert:</p>\n\n<pre><code>#adding % to end of dataframe\nresult[\"High %\"] = ph\nresult[\"Low %\"] = pl\n</code></pre>\n\n<hr>\n\n<p>If you insist on using insert the correct syntax would be:</p>\n\n<pre><code>result.insert(2, \"High %\", ph)\nresult.insert(3, \"Low %\", pl)\n</code></pre>\n",
        "question_body": "<p>I have some code that takes a csv file, that finds the min/max each day, then tells me what time that happens. I also have 2 variables to find the percentage for both max/min.</p>\n\n<p>This is currently the output for the dataframe</p>\n\n<pre><code>&gt;Out\n       High   Low\n10:00   6.0  10.0\n10:05  10.0   3.0\n10:10   1.0   7.0\n10:15   1.0   NaN\n10:20   4.0   4.0\n10:25   4.0   4.0\n10:30   5.0   1.0\n10:35   5.0   6.0\n10:40   3.0   2.0\n10:45   4.0   5.0\n10:50   4.0   1.0\n10:55   3.0   4.0\n11:00   4.0   5.0\n&gt;\n</code></pre>\n\n<p>Then I have 2 varables for the % of High/Lows.. (Just ph shown)</p>\n\n<pre><code>&gt;[84 rows x 2 columns]\nTime\n10:00    0.015306\n10:05    0.025510\n10:10    0.002551\n10:15    0.002551\n10:20    0.010204\n10:25    0.010204\n&gt;\n</code></pre>\n\n<p>I've tried to do an .insert(), but recieve this error.</p>\n\n<p>TypeError: insert() takes from 4 to 5 positional arguments but 6 were given</p>\n\n<p>This was my code</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#adding % to end of dataframe\nresult.insert(3,\"High %\", ph, \"Low %\", pl)\n</code></pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv(\"C:\\\\Users\\\\me\\\\Downloads\\\\file.csv\", encoding = \"ISO-8859-1\")\n\n\n#High grouped by Date\ndf2 = df.loc[df.groupby('Date')['High'].idxmax()]\n\n\n#dropping columns of no use\ndf2.drop(['Ticker','Open','Low','Close'], axis=1, inplace=True)\n\n#creating a variable to bucket the time\nTH = df2.groupby('Time').size()\n\n#Low grouped by Date\ndf3 = df.loc[df.groupby('Date')['Low'].idxmin()]\n\n#dropping columns of no use\ndf3.drop(['Ticker','Open','Low','Close'], axis=1, inplace=True)\n\n#creating a variable to bucket the time\nTL = df3.groupby('Time').size()\n\n#Merging Both Dataframes\nframes = [TH, TL]\nresult = pd.concat((frames), axis = 1)\nresult.columns = ['High','Low']\n\n#Percentage\nph = TH/TH.sum()\npl = TL/TL.sum()\n\n</code></pre>\n\n<p>I would like the output to show the % in columns 3 &amp;4</p>\n\n<pre><code>&gt;Out\n       High   Low    % High    %Low\n10:00   6.0  10.0    .015306   \n10:05  10.0   3.0    .025510\n10:10   1.0   7.0    .002551\n10:15   1.0   NaN    .002551\n10:20   4.0   4.0    .010204\n10:25   4.0   4.0    .010204\n\n&gt;\n</code></pre>\n",
        "formatted_input": {
            "qid": 56976142,
            "link": "https://stackoverflow.com/questions/56976142/error-when-trying-to-insert-into-dataframe",
            "question": {
                "title": "Error when trying to .insert() into dataframe",
                "ques_desc": "I have some code that takes a csv file, that finds the min/max each day, then tells me what time that happens. I also have 2 variables to find the percentage for both max/min. This is currently the output for the dataframe Then I have 2 varables for the % of High/Lows.. (Just ph shown) I've tried to do an .insert(), but recieve this error. TypeError: insert() takes from 4 to 5 positional arguments but 6 were given This was my code I would like the output to show the % in columns 3 &4 "
            },
            "io": [
                ">Out\n       High   Low\n10:00   6.0  10.0\n10:05  10.0   3.0\n10:10   1.0   7.0\n10:15   1.0   NaN\n10:20   4.0   4.0\n10:25   4.0   4.0\n10:30   5.0   1.0\n10:35   5.0   6.0\n10:40   3.0   2.0\n10:45   4.0   5.0\n10:50   4.0   1.0\n10:55   3.0   4.0\n11:00   4.0   5.0\n>\n",
                ">Out\n       High   Low    % High    %Low\n10:00   6.0  10.0    .015306   \n10:05  10.0   3.0    .025510\n10:10   1.0   7.0    .002551\n10:15   1.0   NaN    .002551\n10:20   4.0   4.0    .010204\n10:25   4.0   4.0    .010204\n\n>\n"
            ],
            "answer": {
                "ans_desc": "You can only add one column at a time with insert. And as you intend to add the new columns at the end of the dataframe you do not even need insert: If you insist on using insert the correct syntax would be: ",
                "code": [
                    "#adding % to end of dataframe\nresult[\"High %\"] = ph\nresult[\"Low %\"] = pl\n",
                    "result.insert(2, \"High %\", ph)\nresult.insert(3, \"Low %\", pl)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 281,
            "user_id": 10705248,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/31d5aaf7570fc3f11a673167b1c3d53c?s=128&d=identicon&r=PG&f=1",
            "display_name": "lsr729",
            "link": "https://stackoverflow.com/users/10705248/lsr729"
        },
        "is_answered": true,
        "view_count": 41,
        "closed_date": 1562617053,
        "accepted_answer_id": 56929294,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1562569082,
        "creation_date": 1562566626,
        "question_id": 56929277,
        "link": "https://stackoverflow.com/questions/56929277/can-not-make-desired-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "Can not make desired pandas dataframe",
        "body": "<p>I am trying to make a pandas dataframe using 2 paramters as columns. But it makes a dataframe transpose of what I need. \nI have <code>a</code> and <code>b</code> as column parameters as follows:</p>\n\n<pre><code>a=[1,2,3,4,5]\nb=[11,22,33,44,55]\n\npd.DataFrame([a,b])\n</code></pre>\n\n<p>This gives the following dataframe:</p>\n\n<pre><code>\n    0   1   2   3   4 \n0   1   2   3   4   5\n1   11  22  33  44  55\n</code></pre>\n\n<p>However, I want the dataframe as:</p>\n\n<pre><code>    0   1 \n0   1   11\n1   2   22\n2   3   33\n3   4   44\n4   5   55\n</code></pre>\n",
        "answer_body": "<p>One solution is transpose by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.T.html\" rel=\"nofollow noreferrer\"><code>DataFrame.T</code></a>:</p>\n\n<pre><code>df = pd.DataFrame([a,b]).T\n</code></pre>\n\n<p>Or use <code>zip</code>:</p>\n\n<pre><code>df = pd.DataFrame(list(zip(a, b)))\n#pandas 0.24+\n#df = pd.DataFrame(zip(a, b))\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n   0   1\n0  1  11\n1  2  22\n2  3  33\n3  4  44\n4  5  55\n</code></pre>\n\n<p>Also is possible specify column names by dictionary:</p>\n\n<pre><code>df = pd.DataFrame({'a':a, 'b':b})\nprint (df)\n   a   b\n0  1  11\n1  2  22\n2  3  33\n3  4  44\n4  5  55\n</code></pre>\n",
        "question_body": "<p>I am trying to make a pandas dataframe using 2 paramters as columns. But it makes a dataframe transpose of what I need. \nI have <code>a</code> and <code>b</code> as column parameters as follows:</p>\n\n<pre><code>a=[1,2,3,4,5]\nb=[11,22,33,44,55]\n\npd.DataFrame([a,b])\n</code></pre>\n\n<p>This gives the following dataframe:</p>\n\n<pre><code>\n    0   1   2   3   4 \n0   1   2   3   4   5\n1   11  22  33  44  55\n</code></pre>\n\n<p>However, I want the dataframe as:</p>\n\n<pre><code>    0   1 \n0   1   11\n1   2   22\n2   3   33\n3   4   44\n4   5   55\n</code></pre>\n",
        "formatted_input": {
            "qid": 56929277,
            "link": "https://stackoverflow.com/questions/56929277/can-not-make-desired-pandas-dataframe",
            "question": {
                "title": "Can not make desired pandas dataframe",
                "ques_desc": "I am trying to make a pandas dataframe using 2 paramters as columns. But it makes a dataframe transpose of what I need. I have and as column parameters as follows: This gives the following dataframe: However, I want the dataframe as: "
            },
            "io": [
                "\n    0   1   2   3   4 \n0   1   2   3   4   5\n1   11  22  33  44  55\n",
                "    0   1 \n0   1   11\n1   2   22\n2   3   33\n3   4   44\n4   5   55\n"
            ],
            "answer": {
                "ans_desc": "One solution is transpose by : Or use : Also is possible specify column names by dictionary: ",
                "code": [
                    "df = pd.DataFrame(list(zip(a, b)))\n#pandas 0.24+\n#df = pd.DataFrame(zip(a, b))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 113,
            "user_id": 8677466,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://www.gravatar.com/avatar/f95c71bde0d7867be0d6b3966e62448c?s=128&d=identicon&r=PG&f=1",
            "display_name": "WLC",
            "link": "https://stackoverflow.com/users/8677466/wlc"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 56915895,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1562431986,
        "creation_date": 1562428054,
        "question_id": 56915574,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56915574/python-populating-tuples-in-tuples-over-dataframe-range",
        "title": "python: populating tuples in tuples over dataframe range",
        "body": "<p>I have 4 portfolios a,b,c,d which can take on values either \"no\" or \"own\" over a period of time. (code included below to facilitate replication)</p>\n\n<pre><code>ano=('a','no',datetime(2018,1,1), datetime(2018,1,2))\naown=('a','own',datetime(2018,1,3), datetime(2018,1,4))\nbno=('b','no',datetime(2018,1,1), datetime(2018,1,5))\nbown=('b','own',datetime(2018,1,6), datetime(2018,1,7))\ncown=('c','own',datetime(2018,1,9), datetime(2018,1,10))\ndown=('d','own',datetime(2018,1,9), datetime(2018,1,9))\n\nsch=pd.DataFrame([ano,aown,bno,bown,cown,down],columns=['portf','base','st','end'])\n</code></pre>\n\n<p>Summary of schedule:</p>\n\n<pre><code>    portf   base    st          end\n0   a       no      2018-01-01  2018-01-02\n1   a       own     2018-01-03  2018-01-04\n2   b       no      2018-01-01  2018-01-05\n3   b       own     2018-01-06  2018-01-07\n4   c       own     2018-01-09  2018-01-10\n5   d       own     2018-01-09  2018-01-09  \n</code></pre>\n\n<p>What I have tried: create a holding dataframe and filling in values based on the schedule. Unfortunately the first portfolio 'a' gets overridden </p>\n\n<pre><code>df=pd.DataFrame(index=pd.date_range(min(sch.st),max(sch.end)),columns=['portf','base'])\nfor row in range(len(sch)):\n        df.loc[sch['st'][row]:sch['end'][row],['portf','base']]= sch.loc[row,['portf','base']].values\n\n            portf   base\n2018-01-01  b       no\n2018-01-02  b       no\n2018-01-03  b       no\n2018-01-04  b       no\n2018-01-05  b       no\n2018-01-06  b       own\n2018-01-07  b       own\n2018-01-08  NaN     NaN\n2018-01-09  d       own\n2018-01-10  c       own\n</code></pre>\n\n<p>desired output:</p>\n\n<pre><code>2018-01-01  (('a','no'), ('b','no'))\n2018-01-02  (('a','no'), ('b','no'))\n2018-01-03  (('a','own'), ('b','no'))\n2018-01-04  (('a','own'), ('b','no'))\n2018-01-05  ('b','no')\n...\n</code></pre>\n\n<p>I am sure there's an easier way of achieving this but probably this is an example I haven't encountered before. Many thanks in advance!</p>\n",
        "answer_body": "<p>I would organize the data differently, index is date, columns for portf and the values are base. </p>\n\n<p>First we need to reshape the data and resample to daily fields. Then it's a simple pivot. </p>\n\n<pre><code>cols = ['portf', 'base']\ns = (df.reset_index()\n       .melt(cols+['index'], value_name='date')\n       .set_index('date')\n       .groupby(cols+['index'], group_keys=False)\n       .resample('D').ffill()\n       .drop(columns=['variable', 'index'])\n       .reset_index())\n\nres = s.pivot(index='date', columns='portf')\nres = res.resample('D').first()  # Recover missing dates between\n</code></pre>\n\n<h3>Output <code>res</code></h3>\n\n<pre><code>           base               \nportf         a    b    c    d\n2018-01-01   no   no  NaN  NaN\n2018-01-02   no   no  NaN  NaN\n2018-01-03  own   no  NaN  NaN\n2018-01-04  own   no  NaN  NaN\n2018-01-05  NaN   no  NaN  NaN\n2018-01-06  NaN  own  NaN  NaN\n2018-01-07  NaN  own  NaN  NaN\n2018-01-08  NaN  NaN  NaN  NaN\n2018-01-09  NaN  NaN  own  own\n2018-01-10  NaN  NaN  own  NaN\n</code></pre>\n\n<hr>\n\n<p>If you need your other output, we can get there with some less than ideal <code>Series.apply</code> calls. This will be very bad for a large DataFrame; I would seriously consider keeping the above. </p>\n\n<pre><code>s.set_index('date').apply(tuple, axis=1).groupby('date').apply(tuple)\n\ndate\n2018-01-01      ((a, no), (b, no))\n2018-01-02      ((a, no), (b, no))\n2018-01-03     ((a, own), (b, no))\n2018-01-04     ((a, own), (b, no))\n2018-01-05              ((b, no),)\n2018-01-06             ((b, own),)\n2018-01-07             ((b, own),)\n2018-01-09    ((c, own), (d, own))\n2018-01-10             ((c, own),)\ndtype: object\n</code></pre>\n",
        "question_body": "<p>I have 4 portfolios a,b,c,d which can take on values either \"no\" or \"own\" over a period of time. (code included below to facilitate replication)</p>\n\n<pre><code>ano=('a','no',datetime(2018,1,1), datetime(2018,1,2))\naown=('a','own',datetime(2018,1,3), datetime(2018,1,4))\nbno=('b','no',datetime(2018,1,1), datetime(2018,1,5))\nbown=('b','own',datetime(2018,1,6), datetime(2018,1,7))\ncown=('c','own',datetime(2018,1,9), datetime(2018,1,10))\ndown=('d','own',datetime(2018,1,9), datetime(2018,1,9))\n\nsch=pd.DataFrame([ano,aown,bno,bown,cown,down],columns=['portf','base','st','end'])\n</code></pre>\n\n<p>Summary of schedule:</p>\n\n<pre><code>    portf   base    st          end\n0   a       no      2018-01-01  2018-01-02\n1   a       own     2018-01-03  2018-01-04\n2   b       no      2018-01-01  2018-01-05\n3   b       own     2018-01-06  2018-01-07\n4   c       own     2018-01-09  2018-01-10\n5   d       own     2018-01-09  2018-01-09  \n</code></pre>\n\n<p>What I have tried: create a holding dataframe and filling in values based on the schedule. Unfortunately the first portfolio 'a' gets overridden </p>\n\n<pre><code>df=pd.DataFrame(index=pd.date_range(min(sch.st),max(sch.end)),columns=['portf','base'])\nfor row in range(len(sch)):\n        df.loc[sch['st'][row]:sch['end'][row],['portf','base']]= sch.loc[row,['portf','base']].values\n\n            portf   base\n2018-01-01  b       no\n2018-01-02  b       no\n2018-01-03  b       no\n2018-01-04  b       no\n2018-01-05  b       no\n2018-01-06  b       own\n2018-01-07  b       own\n2018-01-08  NaN     NaN\n2018-01-09  d       own\n2018-01-10  c       own\n</code></pre>\n\n<p>desired output:</p>\n\n<pre><code>2018-01-01  (('a','no'), ('b','no'))\n2018-01-02  (('a','no'), ('b','no'))\n2018-01-03  (('a','own'), ('b','no'))\n2018-01-04  (('a','own'), ('b','no'))\n2018-01-05  ('b','no')\n...\n</code></pre>\n\n<p>I am sure there's an easier way of achieving this but probably this is an example I haven't encountered before. Many thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 56915574,
            "link": "https://stackoverflow.com/questions/56915574/python-populating-tuples-in-tuples-over-dataframe-range",
            "question": {
                "title": "python: populating tuples in tuples over dataframe range",
                "ques_desc": "I have 4 portfolios a,b,c,d which can take on values either \"no\" or \"own\" over a period of time. (code included below to facilitate replication) Summary of schedule: What I have tried: create a holding dataframe and filling in values based on the schedule. Unfortunately the first portfolio 'a' gets overridden desired output: I am sure there's an easier way of achieving this but probably this is an example I haven't encountered before. Many thanks in advance! "
            },
            "io": [
                "    portf   base    st          end\n0   a       no      2018-01-01  2018-01-02\n1   a       own     2018-01-03  2018-01-04\n2   b       no      2018-01-01  2018-01-05\n3   b       own     2018-01-06  2018-01-07\n4   c       own     2018-01-09  2018-01-10\n5   d       own     2018-01-09  2018-01-09  \n",
                "2018-01-01  (('a','no'), ('b','no'))\n2018-01-02  (('a','no'), ('b','no'))\n2018-01-03  (('a','own'), ('b','no'))\n2018-01-04  (('a','own'), ('b','no'))\n2018-01-05  ('b','no')\n...\n"
            ],
            "answer": {
                "ans_desc": "I would organize the data differently, index is date, columns for portf and the values are base. First we need to reshape the data and resample to daily fields. Then it's a simple pivot. Output If you need your other output, we can get there with some less than ideal calls. This will be very bad for a large DataFrame; I would seriously consider keeping the above. ",
                "code": [
                    "cols = ['portf', 'base']\ns = (df.reset_index()\n       .melt(cols+['index'], value_name='date')\n       .set_index('date')\n       .groupby(cols+['index'], group_keys=False)\n       .resample('D').ffill()\n       .drop(columns=['variable', 'index'])\n       .reset_index())\n\nres = s.pivot(index='date', columns='portf')\nres = res.resample('D').first()  # Recover missing dates between\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "apply"
        ],
        "owner": {
            "reputation": 20638,
            "user_id": 1534017,
            "user_type": "registered",
            "accept_rate": 92,
            "profile_image": "https://www.gravatar.com/avatar/aa667d0e455aa30f3410756d719de795?s=128&d=identicon&r=PG",
            "display_name": "Cleb",
            "link": "https://stackoverflow.com/users/1534017/cleb"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 56891445,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1562257606,
        "creation_date": 1562255976,
        "last_edit_date": 1562257606,
        "question_id": 56891370,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56891370/how-to-manipulate-column-entries-using-only-one-specific-output-of-a-function-th",
        "title": "How to manipulate column entries using only one specific output of a function that returns several values?",
        "body": "<p>I have a dataframe like this:</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'a': range(4), 'b': range(2, 6)})\n\n   a  b\n0  0  2\n1  1  3\n2  2  4\n3  3  5\n</code></pre>\n\n<p>and I have a function that returns several values. Here I just use a <strong>dummy function</strong> that returns the minimum and maximum for a certain input iterable:</p>\n\n<pre><code>def return_min_max(x):\n    return (np.min(x), np.max(x))\n</code></pre>\n\n<p>Now I want to e.g. add the maximum of each column to each value in the respective column.</p>\n\n<p>So </p>\n\n<pre><code>df.apply(return_min_max)\n</code></pre>\n\n<p>gives</p>\n\n<pre><code>a    (0, 3)\nb    (2, 5)\n</code></pre>\n\n<p>and then</p>\n\n<pre><code>df.add(df.apply(return_min_max).apply(lambda x: x[1]))\n</code></pre>\n\n<p>yields the desired outcome</p>\n\n<pre><code>   a   b\n0  3   7\n1  4   8\n2  5   9\n3  6  10\n</code></pre>\n\n<p>I am wondering whether there is a more straightforward way that avoids the two chained <code>apply</code>'s.</p>\n\n<p>Just to make sure:</p>\n\n<p>I am NOT interested in a</p>\n\n<pre><code>df.add(df.max())\n</code></pre>\n\n<p>type solution. I highlighted the <code>dummy_function</code> to illustrate that this not my actual function but just serves as a minimal example function that has several outputs.</p>\n",
        "answer_body": "<p><code>DataFrame.max</code> will returns a Series of the column-wise maximum values. <code>DataFrame.add()</code> will then add this <code>Series</code>, aligning on columns. </p>\n\n<pre><code>df.add(df.max())\n\n#   a   b\n#0  3   7\n#1  4   8\n#2  5   9\n#3  6  10\n</code></pre>\n\n<hr>\n\n<p>If you're real function is much more complicated, there are a few alternatives. </p>\n\n<p>Keep it as is, use <code>.str</code> to access the max element.</p>\n\n<pre><code>def return_min_max(x):\n    return (np.min(x), np.max(x))\n\ndf.add(df.apply(return_min_max).str[1])\n</code></pre>\n\n<p>Consider returning a Series with the index being descriptive about what is returned:</p>\n\n<pre><code>def return_min_max(x):\n    return pd.Series([np.min(x), np.max(x)], index=['min', 'max'])\n\ndf.add(df.apply(return_min_max).loc['max'])\n</code></pre>\n\n<p>Or if the returns can be separated (in this case <code>max</code> and <code>min</code> really don't need to be done in the same function), it's simpler to have them separated:</p>\n\n<pre><code>def return_max(x):\n    return np.max(x)\n\ndf.add(df.apply(return_max))\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'a': range(4), 'b': range(2, 6)})\n\n   a  b\n0  0  2\n1  1  3\n2  2  4\n3  3  5\n</code></pre>\n\n<p>and I have a function that returns several values. Here I just use a <strong>dummy function</strong> that returns the minimum and maximum for a certain input iterable:</p>\n\n<pre><code>def return_min_max(x):\n    return (np.min(x), np.max(x))\n</code></pre>\n\n<p>Now I want to e.g. add the maximum of each column to each value in the respective column.</p>\n\n<p>So </p>\n\n<pre><code>df.apply(return_min_max)\n</code></pre>\n\n<p>gives</p>\n\n<pre><code>a    (0, 3)\nb    (2, 5)\n</code></pre>\n\n<p>and then</p>\n\n<pre><code>df.add(df.apply(return_min_max).apply(lambda x: x[1]))\n</code></pre>\n\n<p>yields the desired outcome</p>\n\n<pre><code>   a   b\n0  3   7\n1  4   8\n2  5   9\n3  6  10\n</code></pre>\n\n<p>I am wondering whether there is a more straightforward way that avoids the two chained <code>apply</code>'s.</p>\n\n<p>Just to make sure:</p>\n\n<p>I am NOT interested in a</p>\n\n<pre><code>df.add(df.max())\n</code></pre>\n\n<p>type solution. I highlighted the <code>dummy_function</code> to illustrate that this not my actual function but just serves as a minimal example function that has several outputs.</p>\n",
        "formatted_input": {
            "qid": 56891370,
            "link": "https://stackoverflow.com/questions/56891370/how-to-manipulate-column-entries-using-only-one-specific-output-of-a-function-th",
            "question": {
                "title": "How to manipulate column entries using only one specific output of a function that returns several values?",
                "ques_desc": "I have a dataframe like this: and I have a function that returns several values. Here I just use a dummy function that returns the minimum and maximum for a certain input iterable: Now I want to e.g. add the maximum of each column to each value in the respective column. So gives and then yields the desired outcome I am wondering whether there is a more straightforward way that avoids the two chained 's. Just to make sure: I am NOT interested in a type solution. I highlighted the to illustrate that this not my actual function but just serves as a minimal example function that has several outputs. "
            },
            "io": [
                "a    (0, 3)\nb    (2, 5)\n",
                "   a   b\n0  3   7\n1  4   8\n2  5   9\n3  6  10\n"
            ],
            "answer": {
                "ans_desc": " will returns a Series of the column-wise maximum values. will then add this , aligning on columns. If you're real function is much more complicated, there are a few alternatives. Keep it as is, use to access the max element. Consider returning a Series with the index being descriptive about what is returned: Or if the returns can be separated (in this case and really don't need to be done in the same function), it's simpler to have them separated: ",
                "code": [
                    "def return_min_max(x):\n    return (np.min(x), np.max(x))\n\ndf.add(df.apply(return_min_max).str[1])\n",
                    "def return_min_max(x):\n    return pd.Series([np.min(x), np.max(x)], index=['min', 'max'])\n\ndf.add(df.apply(return_min_max).loc['max'])\n",
                    "def return_max(x):\n    return np.max(x)\n\ndf.add(df.apply(return_max))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 11734171,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5ec526b1e5d878722add6bf9733d09ed?s=128&d=identicon&r=PG&f=1",
            "display_name": "dfust",
            "link": "https://stackoverflow.com/users/11734171/dfust"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 56870082,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1562161769,
        "creation_date": 1562155081,
        "last_edit_date": 1562155433,
        "question_id": 56869643,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56869643/pandas-remove-index-entry-and-all-its-rows-from-multilevel-index-when-all-da",
        "title": "Pandas: Remove index entry (and all it&#39;s rows) from multilevel index when all data in a column is NaN",
        "body": "<p>I'd like to clean up some data I have in a dataframe with a multilevel index.</p>\n\n<pre><code>                | A   | B   | \n----------------+-----+-----+\nfoo  2019-01-01 | x   | NaN |\n     2019-01-02 | x   | NaN |\n     2019-01-03 | NaN | NaN |\n................+.....+.....+\nbar  2019-01-01 | NaN | x   |\n     2019-01-02 | NaN | y   |\n     2019-01-03 | NaN | z   |\n................+.....+.....+\nbaz  2019-01-01 | x   | x   |\n     2019-01-02 | x   | x   |\n     2019-01-03 | x   | x   |\n\n</code></pre>\n\n<p>I'd like to loose the complete group indexed by <strong>bar</strong>, because all of the data in column <strong>A</strong> is <em>NaN</em>. I'd like to keep <strong>foo</strong>, because only some of the data in column <strong>A</strong> is <em>NaN</em> (column <strong>B</strong> is not important here, even if it's all <em>NaN</em>). I'd like to keep <strong>baz</strong>, because not all of column <strong>A</strong>is <em>NaN</em>.\nSo my result should look like this:</p>\n\n<pre><code>                | A   | B   | \n----------------+-----+-----+\nfoo  2019-01-01 | x   | NaN |\n     2019-01-02 | x   | NaN |\n     2019-01-03 | NaN | NaN |\n................+.....+.....+\nbaz  2019-01-01 | x   | x   |\n     2019-01-02 | x   | x   |\n     2019-01-03 | x   | x   |\n\n</code></pre>\n\n<p>What's the best way to do this with pandas and python? I suppose there is a better way than looping through the data...</p>\n",
        "answer_body": "<h3><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html\" rel=\"nofollow noreferrer\"><code>groupby.transform</code></a>, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.notna.html\" rel=\"nofollow noreferrer\"><code>notna()</code></a> &amp; <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.any.html\" rel=\"nofollow noreferrer\"><code>any()</code></a></h3>\n\n<p>We can <code>groupby</code> on your first level index and then check if <em>any</em> of the values in column A are not <code>NaN</code>.</p>\n\n<p>We use <code>transform</code> to get the same shaped boolean array back so we can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a> to filter out the correct rows.</p>\n\n<pre><code>m = df['A'].notna().groupby(level=0).transform('any')\ndf[m]\n</code></pre>\n\n<pre><code>                  A    B\nidx idx2                \nfoo 2019-01-01    x  NaN\n    2019-01-02    x  NaN\n    2019-01-03  NaN  NaN\nbaz 2019-01-01    x    x\n    2019-01-02    x    x\n    2019-01-03    x    x\n</code></pre>\n\n<hr>\n\n<p>What does <code>m</code> return?</p>\n\n<pre><code>m = df['A'].notna().groupby(level=0).transform('any')\nprint(m)\n\nidx  idx2      \nfoo  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nbar  2019-01-01    False\n     2019-01-02    False\n     2019-01-03    False\nbaz  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nName: A, dtype: bool\n</code></pre>\n",
        "question_body": "<p>I'd like to clean up some data I have in a dataframe with a multilevel index.</p>\n\n<pre><code>                | A   | B   | \n----------------+-----+-----+\nfoo  2019-01-01 | x   | NaN |\n     2019-01-02 | x   | NaN |\n     2019-01-03 | NaN | NaN |\n................+.....+.....+\nbar  2019-01-01 | NaN | x   |\n     2019-01-02 | NaN | y   |\n     2019-01-03 | NaN | z   |\n................+.....+.....+\nbaz  2019-01-01 | x   | x   |\n     2019-01-02 | x   | x   |\n     2019-01-03 | x   | x   |\n\n</code></pre>\n\n<p>I'd like to loose the complete group indexed by <strong>bar</strong>, because all of the data in column <strong>A</strong> is <em>NaN</em>. I'd like to keep <strong>foo</strong>, because only some of the data in column <strong>A</strong> is <em>NaN</em> (column <strong>B</strong> is not important here, even if it's all <em>NaN</em>). I'd like to keep <strong>baz</strong>, because not all of column <strong>A</strong>is <em>NaN</em>.\nSo my result should look like this:</p>\n\n<pre><code>                | A   | B   | \n----------------+-----+-----+\nfoo  2019-01-01 | x   | NaN |\n     2019-01-02 | x   | NaN |\n     2019-01-03 | NaN | NaN |\n................+.....+.....+\nbaz  2019-01-01 | x   | x   |\n     2019-01-02 | x   | x   |\n     2019-01-03 | x   | x   |\n\n</code></pre>\n\n<p>What's the best way to do this with pandas and python? I suppose there is a better way than looping through the data...</p>\n",
        "formatted_input": {
            "qid": 56869643,
            "link": "https://stackoverflow.com/questions/56869643/pandas-remove-index-entry-and-all-its-rows-from-multilevel-index-when-all-da",
            "question": {
                "title": "Pandas: Remove index entry (and all it&#39;s rows) from multilevel index when all data in a column is NaN",
                "ques_desc": "I'd like to clean up some data I have in a dataframe with a multilevel index. I'd like to loose the complete group indexed by bar, because all of the data in column A is NaN. I'd like to keep foo, because only some of the data in column A is NaN (column B is not important here, even if it's all NaN). I'd like to keep baz, because not all of column Ais NaN. So my result should look like this: What's the best way to do this with pandas and python? I suppose there is a better way than looping through the data... "
            },
            "io": [
                "                | A   | B   | \n----------------+-----+-----+\nfoo  2019-01-01 | x   | NaN |\n     2019-01-02 | x   | NaN |\n     2019-01-03 | NaN | NaN |\n................+.....+.....+\nbar  2019-01-01 | NaN | x   |\n     2019-01-02 | NaN | y   |\n     2019-01-03 | NaN | z   |\n................+.....+.....+\nbaz  2019-01-01 | x   | x   |\n     2019-01-02 | x   | x   |\n     2019-01-03 | x   | x   |\n\n",
                "                | A   | B   | \n----------------+-----+-----+\nfoo  2019-01-01 | x   | NaN |\n     2019-01-02 | x   | NaN |\n     2019-01-03 | NaN | NaN |\n................+.....+.....+\nbaz  2019-01-01 | x   | x   |\n     2019-01-02 | x   | x   |\n     2019-01-03 | x   | x   |\n\n"
            ],
            "answer": {
                "ans_desc": ", & We can on your first level index and then check if any of the values in column A are not . We use to get the same shaped boolean array back so we can use to filter out the correct rows. What does return? ",
                "code": [
                    "m = df['A'].notna().groupby(level=0).transform('any')\nprint(m)\n\nidx  idx2      \nfoo  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nbar  2019-01-01    False\n     2019-01-02    False\n     2019-01-03    False\nbaz  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nName: A, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 599,
            "user_id": 4624106,
            "user_type": "registered",
            "accept_rate": 67,
            "profile_image": "https://www.gravatar.com/avatar/ec8065d1fddb547b919b84fef5c43f2f?s=128&d=identicon&r=PG&f=1",
            "display_name": "giupardeb",
            "link": "https://stackoverflow.com/users/4624106/giupardeb"
        },
        "is_answered": true,
        "view_count": 33291,
        "accepted_answer_id": 48864996,
        "answer_count": 2,
        "score": 25,
        "last_activity_date": 1562048014,
        "creation_date": 1519038829,
        "last_edit_date": 1550524134,
        "question_id": 48864923,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/48864923/pythonpandas-select-certain-rows-by-index-of-another-dataframe",
        "title": "Python[pandas]: Select certain rows by index of another dataframe",
        "body": "<p>I have a dataframe and I would select only rows that contain index value into df1.index.</p>\n\n<p>for Example:</p>\n\n<pre><code>In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n2  4  5  0  2\n3  5  5  1  0\n22 1  3  9  6\n</code></pre>\n\n<p>and these indexes</p>\n\n<pre><code>In[96]:df1.index\nOut[96]:\nInt64Index([  1,   3,   4,   5,   6,   7,  22,  28,  29,  32,], dtype='int64', length=253)\n</code></pre>\n\n<p>I would like this output:</p>\n\n<pre><code>In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n3  5  5  1  0\n22 1  3  9  6\n</code></pre>\n\n<p>thanks</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html\" rel=\"noreferrer\"><code>isin</code></a>:</p>\n\n<pre><code>df = df[df.index.isin(df1.index)]\n</code></pre>\n\n<p>Or get all intersectioned indices and select by <code>loc</code>:</p>\n\n<pre><code>df = df.loc[df.index &amp; df1.index]\ndf = df.loc[np.intersect1d(df.index, df1.index)]\ndf = df.loc[df.index.intersection(df1.index)]\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n    A  B  C  D\n1   1  4  9  1\n3   5  5  1  0\n22  1  3  9  6\n</code></pre>\n\n<p>EDIT:</p>\n\n<blockquote>\n  <p>I tried solution: df = df.loc[df1.index]. Do you think that this solution is correct?</p>\n</blockquote>\n\n<p>Solution is incorrect:</p>\n\n<pre><code>df = df.loc[df1.index]\nprint (df)\n\n      A    B    C    D\n1   1.0  4.0  9.0  1.0\n3   5.0  5.0  1.0  0.0\n4   NaN  NaN  NaN  NaN\n5   NaN  NaN  NaN  NaN\n6   NaN  NaN  NaN  NaN\n7   NaN  NaN  NaN  NaN\n22  1.0  3.0  9.0  6.0\n28  NaN  NaN  NaN  NaN\n29  NaN  NaN  NaN  NaN\n32  NaN  NaN  NaN  NaN\nC:/Dropbox/work-joy/so/_t/t.py:23: FutureWarning: \nPassing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n  print (df)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe and I would select only rows that contain index value into df1.index.</p>\n\n<p>for Example:</p>\n\n<pre><code>In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n2  4  5  0  2\n3  5  5  1  0\n22 1  3  9  6\n</code></pre>\n\n<p>and these indexes</p>\n\n<pre><code>In[96]:df1.index\nOut[96]:\nInt64Index([  1,   3,   4,   5,   6,   7,  22,  28,  29,  32,], dtype='int64', length=253)\n</code></pre>\n\n<p>I would like this output:</p>\n\n<pre><code>In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n3  5  5  1  0\n22 1  3  9  6\n</code></pre>\n\n<p>thanks</p>\n",
        "formatted_input": {
            "qid": 48864923,
            "link": "https://stackoverflow.com/questions/48864923/pythonpandas-select-certain-rows-by-index-of-another-dataframe",
            "question": {
                "title": "Python[pandas]: Select certain rows by index of another dataframe",
                "ques_desc": "I have a dataframe and I would select only rows that contain index value into df1.index. for Example: and these indexes I would like this output: thanks "
            },
            "io": [
                "In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n2  4  5  0  2\n3  5  5  1  0\n22 1  3  9  6\n",
                "In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n3  5  5  1  0\n22 1  3  9  6\n"
            ],
            "answer": {
                "ans_desc": "Use : Or get all intersectioned indices and select by : EDIT: I tried solution: df = df.loc[df1.index]. Do you think that this solution is correct? Solution is incorrect: ",
                "code": [
                    "df = df.loc[df.index & df1.index]\ndf = df.loc[np.intersect1d(df.index, df1.index)]\ndf = df.loc[df.index.intersection(df1.index)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "count"
        ],
        "owner": {
            "reputation": 4095,
            "user_id": 8306666,
            "user_type": "registered",
            "accept_rate": 38,
            "profile_image": "https://www.gravatar.com/avatar/2f20237f2695cc2f6951fb562493cd7f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nazim Kerimbekov",
            "link": "https://stackoverflow.com/users/8306666/nazim-kerimbekov"
        },
        "is_answered": true,
        "view_count": 57,
        "closed_date": 1561831582,
        "accepted_answer_id": 56813730,
        "answer_count": 2,
        "score": -4,
        "last_activity_date": 1561782066,
        "creation_date": 1561760483,
        "question_id": 56813691,
        "link": "https://stackoverflow.com/questions/56813691/how-to-append-value-counts-output-to-the-original-df",
        "closed_reason": "Needs more focus",
        "title": "How to append value_counts() output to the original df?",
        "body": "<p>So I have the following <code>df</code>:</p>\n\n<pre><code>       Open      High       Low     Close  \n0    0.001268  0.001277  0.001266  0.001271   \n1    0.001268  0.001269  0.001265  0.001266   \n2    0.001265  0.001265  0.001242  0.001254   \n3    0.001253  0.001271  0.001244  0.001251   \n4    0.001253  0.001259  0.001249  0.001257   \n5    0.001257  0.001260  0.001241  0.001248\n</code></pre>\n\n<p>when I running this line:</p>\n\n<pre><code>df[\"Open\"].value_counts()\n</code></pre>\n\n<p>I get the following output:</p>\n\n<pre><code>0.001253    2\n0.001268    2\n0.001265    1\n0.001257    1\n</code></pre>\n\n<p>I was wonder how I could append this output to the original <code>df</code> to make it look like this:</p>\n\n<pre><code>       Open      High       Low     Close     Open_count  \n0    0.001268  0.001277  0.001266  0.001271       2\n1    0.001268  0.001269  0.001265  0.001266       2\n2    0.001265  0.001265  0.001242  0.001254       1\n3    0.001253  0.001271  0.001244  0.001251       2\n4    0.001253  0.001259  0.001249  0.001257       2\n5    0.001257  0.001260  0.001241  0.001248       1\n</code></pre>\n\n<p>Thank you very much for your help in advance.</p>\n",
        "answer_body": "<p><code>merge</code> is your friend:</p>\n\n<pre><code>s = df[\"Open\"].value_counts()\n\ndf.merge(s, left_on='Open', right_index=True,\n         suffixes=['','_count'])\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>       Open      High       Low     Close  Open_count\n0  0.001268  0.001277  0.001266  0.001271           2\n1  0.001268  0.001269  0.001265  0.001266           2\n2  0.001265  0.001265  0.001242  0.001254           1\n3  0.001253  0.001271  0.001244  0.001251           2\n4  0.001253  0.001259  0.001249  0.001257           2\n5  0.001257  0.001260  0.001241  0.001248           1\n</code></pre>\n",
        "question_body": "<p>So I have the following <code>df</code>:</p>\n\n<pre><code>       Open      High       Low     Close  \n0    0.001268  0.001277  0.001266  0.001271   \n1    0.001268  0.001269  0.001265  0.001266   \n2    0.001265  0.001265  0.001242  0.001254   \n3    0.001253  0.001271  0.001244  0.001251   \n4    0.001253  0.001259  0.001249  0.001257   \n5    0.001257  0.001260  0.001241  0.001248\n</code></pre>\n\n<p>when I running this line:</p>\n\n<pre><code>df[\"Open\"].value_counts()\n</code></pre>\n\n<p>I get the following output:</p>\n\n<pre><code>0.001253    2\n0.001268    2\n0.001265    1\n0.001257    1\n</code></pre>\n\n<p>I was wonder how I could append this output to the original <code>df</code> to make it look like this:</p>\n\n<pre><code>       Open      High       Low     Close     Open_count  \n0    0.001268  0.001277  0.001266  0.001271       2\n1    0.001268  0.001269  0.001265  0.001266       2\n2    0.001265  0.001265  0.001242  0.001254       1\n3    0.001253  0.001271  0.001244  0.001251       2\n4    0.001253  0.001259  0.001249  0.001257       2\n5    0.001257  0.001260  0.001241  0.001248       1\n</code></pre>\n\n<p>Thank you very much for your help in advance.</p>\n",
        "formatted_input": {
            "qid": 56813691,
            "link": "https://stackoverflow.com/questions/56813691/how-to-append-value-counts-output-to-the-original-df",
            "question": {
                "title": "How to append value_counts() output to the original df?",
                "ques_desc": "So I have the following : when I running this line: I get the following output: I was wonder how I could append this output to the original to make it look like this: Thank you very much for your help in advance. "
            },
            "io": [
                "       Open      High       Low     Close  \n0    0.001268  0.001277  0.001266  0.001271   \n1    0.001268  0.001269  0.001265  0.001266   \n2    0.001265  0.001265  0.001242  0.001254   \n3    0.001253  0.001271  0.001244  0.001251   \n4    0.001253  0.001259  0.001249  0.001257   \n5    0.001257  0.001260  0.001241  0.001248\n",
                "0.001253    2\n0.001268    2\n0.001265    1\n0.001257    1\n"
            ],
            "answer": {
                "ans_desc": " is your friend: Output: ",
                "code": [
                    "s = df[\"Open\"].value_counts()\n\ndf.merge(s, left_on='Open', right_index=True,\n         suffixes=['','_count'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 75,
            "user_id": 5285822,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-44juKSkxKAk/AAAAAAAAAAI/AAAAAAAAAak/Vm9PkQWGY-E/photo.jpg?sz=128",
            "display_name": "Peter Tran",
            "link": "https://stackoverflow.com/users/5285822/peter-tran"
        },
        "is_answered": true,
        "view_count": 101,
        "accepted_answer_id": 56810492,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1561742592,
        "creation_date": 1561739430,
        "question_id": 56810353,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56810353/pandas-keep-values-in-a-set-of-columns-if-they-exist-in-another-set-of-columns",
        "title": "Pandas: Keep values in a set of columns if they exist in another set of columns in the same row, otherwise set it to NaN",
        "body": "<p>I have a couple of columns in my dataframe that have values in them. I want to only keep those values in those columns if they exist in another set of columns in the same row. Otherwise, I want to set the value to <code>NaN</code>.</p>\n\n<p>Here's an example dataframe:</p>\n\n<pre><code>    A   B   C   D\n0   1  30   1  29\n1   5  42  99   5\n2  64  67  12  22\n3   2  22  22   0\n4  43   6   9  43\n</code></pre>\n\n<p>In this case, I want <code>C</code> and <code>D</code> to be changed based on <code>A</code> and <code>B</code>:</p>\n\n<pre><code>    A   B     C     D\n0   1  30   1.0   NaN\n1   5  42   NaN   5.0\n2  64  67   NaN   NaN\n3   2  22  22.0   NaN\n4  43   6   NaN  43.0\n</code></pre>\n\n<p>It's been difficult to form a query to google this, and the closest I've gotten is to use <code>pandas.DataFrame.isin</code> like this:</p>\n\n<pre><code>from operator import concat\nfirst = df.head(1)\nfirst[['C', 'D']].isin(reduce(concat, first[['A', 'B']].values.tolist()))\n</code></pre>\n\n<p>Which gives me this:</p>\n\n<pre><code>      C      D\n0  True  False\n</code></pre>\n\n<p>Which appears to be somewhat useful, but I'm not sure if this is the right path or what to do with it from here.</p>\n",
        "answer_body": "<h3>Numpy broadcasting and <code>pd.DataFrame.where</code></h3>\n\n<pre><code>cd = df[['C', 'D']].to_numpy()\nab = df[['A', 'B']].to_numpy()\n\ndf[['C', 'D']] = df[['C', 'D']].where((cd[..., None] == ab[:, None]).any(axis=2))\n\ndf\n\n    A   B     C     D\n0   1  30   1.0   NaN\n1   5  42   NaN   5.0\n2  64  67   NaN   NaN\n3   2  22  22.0   NaN\n4  43   6   NaN  43.0\n</code></pre>\n\n<hr>\n\n<h3>Less Numpy</h3>\n\n<pre><code>df[['C', 'D']] = [\n    (c if c in ab else np.nan, d if d in ab else np.nan)\n    for *ab, c, d in zip(*map(df.get, df))\n]\n\ndf\n\n    A   B     C     D\n0   1  30   1.0   NaN\n1   5  42   NaN   5.0\n2  64  67   NaN   NaN\n3   2  22  22.0   NaN\n4  43   6   NaN  43.0\n</code></pre>\n\n<p>Same thing but more specific with the columns</p>\n\n<pre><code>df[['C', 'D']] = [\n    (c if c in ab else np.nan, d if d in ab else np.nan)\n    for *ab, c, d in zip(*map(df.get, ['A', 'B', 'C', 'D']))\n]\n</code></pre>\n",
        "question_body": "<p>I have a couple of columns in my dataframe that have values in them. I want to only keep those values in those columns if they exist in another set of columns in the same row. Otherwise, I want to set the value to <code>NaN</code>.</p>\n\n<p>Here's an example dataframe:</p>\n\n<pre><code>    A   B   C   D\n0   1  30   1  29\n1   5  42  99   5\n2  64  67  12  22\n3   2  22  22   0\n4  43   6   9  43\n</code></pre>\n\n<p>In this case, I want <code>C</code> and <code>D</code> to be changed based on <code>A</code> and <code>B</code>:</p>\n\n<pre><code>    A   B     C     D\n0   1  30   1.0   NaN\n1   5  42   NaN   5.0\n2  64  67   NaN   NaN\n3   2  22  22.0   NaN\n4  43   6   NaN  43.0\n</code></pre>\n\n<p>It's been difficult to form a query to google this, and the closest I've gotten is to use <code>pandas.DataFrame.isin</code> like this:</p>\n\n<pre><code>from operator import concat\nfirst = df.head(1)\nfirst[['C', 'D']].isin(reduce(concat, first[['A', 'B']].values.tolist()))\n</code></pre>\n\n<p>Which gives me this:</p>\n\n<pre><code>      C      D\n0  True  False\n</code></pre>\n\n<p>Which appears to be somewhat useful, but I'm not sure if this is the right path or what to do with it from here.</p>\n",
        "formatted_input": {
            "qid": 56810353,
            "link": "https://stackoverflow.com/questions/56810353/pandas-keep-values-in-a-set-of-columns-if-they-exist-in-another-set-of-columns",
            "question": {
                "title": "Pandas: Keep values in a set of columns if they exist in another set of columns in the same row, otherwise set it to NaN",
                "ques_desc": "I have a couple of columns in my dataframe that have values in them. I want to only keep those values in those columns if they exist in another set of columns in the same row. Otherwise, I want to set the value to . Here's an example dataframe: In this case, I want and to be changed based on and : It's been difficult to form a query to google this, and the closest I've gotten is to use like this: Which gives me this: Which appears to be somewhat useful, but I'm not sure if this is the right path or what to do with it from here. "
            },
            "io": [
                "    A   B   C   D\n0   1  30   1  29\n1   5  42  99   5\n2  64  67  12  22\n3   2  22  22   0\n4  43   6   9  43\n",
                "    A   B     C     D\n0   1  30   1.0   NaN\n1   5  42   NaN   5.0\n2  64  67   NaN   NaN\n3   2  22  22.0   NaN\n4  43   6   NaN  43.0\n"
            ],
            "answer": {
                "ans_desc": "Numpy broadcasting and Less Numpy Same thing but more specific with the columns ",
                "code": [
                    "df[['C', 'D']] = [\n    (c if c in ab else np.nan, d if d in ab else np.nan)\n    for *ab, c, d in zip(*map(df.get, ['A', 'B', 'C', 'D']))\n]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "melt"
        ],
        "owner": {
            "reputation": 81,
            "user_id": 10101144,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-i4vesJ2F8AM/AAAAAAAAAAI/AAAAAAAAAAA/AAnnY7rn_9tSywFlM0heY2mWsAdDWPQJpg/mo/photo.jpg?sz=128",
            "display_name": "Richard Kapustynskyj",
            "link": "https://stackoverflow.com/users/10101144/richard-kapustynskyj"
        },
        "is_answered": true,
        "view_count": 985,
        "accepted_answer_id": 56776760,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1561576218,
        "creation_date": 1561563666,
        "last_edit_date": 1561576218,
        "question_id": 56776539,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56776539/how-to-i-convert-a-pandas-dataframe-row-into-multiple-rows",
        "title": "How to I convert a pandas dataframe row into multiple rows",
        "body": "<p>I have a pandas dataframe that has one row per object. Within that object, there are subobjects. I want to create a dataframe which contains one row per subobject.</p>\n\n<p>I've read stuff on melt but can't begin to work out how to use it for what I want to do.</p>\n\n<p>I want to go from</p>\n\n<pre><code>ObjectID    Sub1_ID Sub1_Var1   Sub1_Var2   Sub1_Var3   Sub2_ID Sub2_Var1   Sub2_Var2   Sub2_Var3\n1           98398   3           10          9           19231           6           7           5\n2           87868   8           5           4               \n3           4579    5           6           6           24833           6           2           2\n4           2514    1           6           9   \n</code></pre>\n\n<p>to</p>\n\n<pre><code>ObjectID    Sub_ID  Var1    Var2    Var3\n1           98398   3       10      9\n1           19231   6       7       5\n2           87868   8       5       4\n3           4579    5       6       6\n3           24833   6       2       2\n4           2514    1       6       9\n</code></pre>\n",
        "answer_body": "<p>One way you can do this is using MultiIndex with <code>from_arrays</code> and then use <code>stack</code> to reshape the dataframe:</p>\n\n<pre><code>df1 = df.set_index('ObjectID')\n\ndf1.columns = pd.MultiIndex.from_arrays(zip(*df1.columns.str.split('_')))\n\ndf1.stack(0).reset_index().drop('level_1', axis=1)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   ObjectID       ID  Var1  Var2  Var3\n0         1  98398.0   3.0  10.0   9.0\n1         1  19231.0   6.0   7.0   5.0\n2         2  87868.0   8.0   5.0   4.0\n3         3   4579.0   5.0   6.0   6.0\n4         3  24833.0   6.0   2.0   2.0\n5         4   2514.0   1.0   6.0   9.0\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe that has one row per object. Within that object, there are subobjects. I want to create a dataframe which contains one row per subobject.</p>\n\n<p>I've read stuff on melt but can't begin to work out how to use it for what I want to do.</p>\n\n<p>I want to go from</p>\n\n<pre><code>ObjectID    Sub1_ID Sub1_Var1   Sub1_Var2   Sub1_Var3   Sub2_ID Sub2_Var1   Sub2_Var2   Sub2_Var3\n1           98398   3           10          9           19231           6           7           5\n2           87868   8           5           4               \n3           4579    5           6           6           24833           6           2           2\n4           2514    1           6           9   \n</code></pre>\n\n<p>to</p>\n\n<pre><code>ObjectID    Sub_ID  Var1    Var2    Var3\n1           98398   3       10      9\n1           19231   6       7       5\n2           87868   8       5       4\n3           4579    5       6       6\n3           24833   6       2       2\n4           2514    1       6       9\n</code></pre>\n",
        "formatted_input": {
            "qid": 56776539,
            "link": "https://stackoverflow.com/questions/56776539/how-to-i-convert-a-pandas-dataframe-row-into-multiple-rows",
            "question": {
                "title": "How to I convert a pandas dataframe row into multiple rows",
                "ques_desc": "I have a pandas dataframe that has one row per object. Within that object, there are subobjects. I want to create a dataframe which contains one row per subobject. I've read stuff on melt but can't begin to work out how to use it for what I want to do. I want to go from to "
            },
            "io": [
                "ObjectID    Sub1_ID Sub1_Var1   Sub1_Var2   Sub1_Var3   Sub2_ID Sub2_Var1   Sub2_Var2   Sub2_Var3\n1           98398   3           10          9           19231           6           7           5\n2           87868   8           5           4               \n3           4579    5           6           6           24833           6           2           2\n4           2514    1           6           9   \n",
                "ObjectID    Sub_ID  Var1    Var2    Var3\n1           98398   3       10      9\n1           19231   6       7       5\n2           87868   8       5       4\n3           4579    5       6       6\n3           24833   6       2       2\n4           2514    1       6       9\n"
            ],
            "answer": {
                "ans_desc": "One way you can do this is using MultiIndex with and then use to reshape the dataframe: Output: ",
                "code": [
                    "df1 = df.set_index('ObjectID')\n\ndf1.columns = pd.MultiIndex.from_arrays(zip(*df1.columns.str.split('_')))\n\ndf1.stack(0).reset_index().drop('level_1', axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 233,
            "user_id": 10748447,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6bb9d4530f848b9f2be323220771ef88?s=128&d=identicon&r=PG&f=1",
            "display_name": "MHanu",
            "link": "https://stackoverflow.com/users/10748447/mhanu"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 56757919,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1561547684,
        "creation_date": 1561477142,
        "last_edit_date": 1561542650,
        "question_id": 56757624,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56757624/increase-specific-rows-by-multiplication-until-sum-of-columns-fulfils-criteria",
        "title": "Increase specific rows by multiplication until sum of columns fulfils criteria",
        "body": "<p>I have a dataframe with 4 columns and I want to do the following steps (ideally in one code):\n- Filter rows where the sum of the 4 columns is lower than 0.9\n- Multiply each cell in each row so that the sum of the row is 0.9\n- In case there is a 0 in any cell, this cell stays unchanged (as multiplying 0 with anything remains 0)\n- At the end display all rows, also the ones that were not changed</p>\n\n<p>Here is an example dataframe:</p>\n\n<pre><code>df = pd.DataFrame({'A':[0.03, 0.0, 0.7],\n           'B': [0.1234, 0.4, 0.333],\n           'C': [0.5, 0.4, 0.0333]})\n\n\nprint (df)\n  Name    A    B    C   \n0 Bread  0.03 0.1234 0.5000 \n1 Butter 0.00 0.4000 0.4000\n2 Cheese 0.70 0.3330 0.0333 \n\nSum = df[\"A\"]+df[\"B\"]+df[\"C\"]\nprint (Sum)\n\n0    0.6534\n1    0.8000\n2    1.0663\n</code></pre>\n\n<p>Now only rows 0 and 1 should be affected by the algorithm</p>\n\n<p>I had used this one which worked partly here: </p>\n\n<pre><code>df = df4.mul(0.9/df4.sum(axis=1),axis=0)\n</code></pre>\n\n<p>But I do now know how to work only with the columns A to C and how I can first filter by the rows where the sum is below 0.9 and then how to show all rows again.</p>\n\n<p>So my desired outcome is something like this:</p>\n\n<pre><code>print (df)\n   Name     A         B         C\n0  Bread    0.0414  0.170292  0.690000\n1  Butter   0.0000  0.452000  0.452000\n2  Cheese   0.70    0.3330   0.0333\n</code></pre>\n\n<p>Important, all columns (including product column) and rows should still be there and the format be a dataframe with all of the rows. I only added the sum function below to see that they add up to 0.9 or more. </p>\n\n<pre><code>Sum = df[\"A\"]+df[\"B\"]+df[\"C\"]\n    print (Sum)\n\n0    0.9\n1    0.9\n2    1.0663\n</code></pre>\n",
        "answer_body": "<p>To save the intermediate values in a new dataframe <code>df2</code>:</p>\n\n<pre><code>df2 = df.apply(lambda x : x if x.sum() &gt; 0.9 else x.mul(0.9/x.sum()), axis=1)\n</code></pre>\n\n<p><code>df2</code> is:</p>\n\n<pre><code>df2\n          A         B         C\n0  0.041322  0.169972  0.688705\n1  0.000000  0.450000  0.450000\n2  0.700000  0.333000  0.033300\n</code></pre>\n\n<p>And if you do:</p>\n\n<pre><code>df2.sum(axis=1)\n</code></pre>\n\n<p>you get:</p>\n\n<pre><code>0    0.9000\n1    0.9000\n2    1.0663\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with 4 columns and I want to do the following steps (ideally in one code):\n- Filter rows where the sum of the 4 columns is lower than 0.9\n- Multiply each cell in each row so that the sum of the row is 0.9\n- In case there is a 0 in any cell, this cell stays unchanged (as multiplying 0 with anything remains 0)\n- At the end display all rows, also the ones that were not changed</p>\n\n<p>Here is an example dataframe:</p>\n\n<pre><code>df = pd.DataFrame({'A':[0.03, 0.0, 0.7],\n           'B': [0.1234, 0.4, 0.333],\n           'C': [0.5, 0.4, 0.0333]})\n\n\nprint (df)\n  Name    A    B    C   \n0 Bread  0.03 0.1234 0.5000 \n1 Butter 0.00 0.4000 0.4000\n2 Cheese 0.70 0.3330 0.0333 \n\nSum = df[\"A\"]+df[\"B\"]+df[\"C\"]\nprint (Sum)\n\n0    0.6534\n1    0.8000\n2    1.0663\n</code></pre>\n\n<p>Now only rows 0 and 1 should be affected by the algorithm</p>\n\n<p>I had used this one which worked partly here: </p>\n\n<pre><code>df = df4.mul(0.9/df4.sum(axis=1),axis=0)\n</code></pre>\n\n<p>But I do now know how to work only with the columns A to C and how I can first filter by the rows where the sum is below 0.9 and then how to show all rows again.</p>\n\n<p>So my desired outcome is something like this:</p>\n\n<pre><code>print (df)\n   Name     A         B         C\n0  Bread    0.0414  0.170292  0.690000\n1  Butter   0.0000  0.452000  0.452000\n2  Cheese   0.70    0.3330   0.0333\n</code></pre>\n\n<p>Important, all columns (including product column) and rows should still be there and the format be a dataframe with all of the rows. I only added the sum function below to see that they add up to 0.9 or more. </p>\n\n<pre><code>Sum = df[\"A\"]+df[\"B\"]+df[\"C\"]\n    print (Sum)\n\n0    0.9\n1    0.9\n2    1.0663\n</code></pre>\n",
        "formatted_input": {
            "qid": 56757624,
            "link": "https://stackoverflow.com/questions/56757624/increase-specific-rows-by-multiplication-until-sum-of-columns-fulfils-criteria",
            "question": {
                "title": "Increase specific rows by multiplication until sum of columns fulfils criteria",
                "ques_desc": "I have a dataframe with 4 columns and I want to do the following steps (ideally in one code): - Filter rows where the sum of the 4 columns is lower than 0.9 - Multiply each cell in each row so that the sum of the row is 0.9 - In case there is a 0 in any cell, this cell stays unchanged (as multiplying 0 with anything remains 0) - At the end display all rows, also the ones that were not changed Here is an example dataframe: Now only rows 0 and 1 should be affected by the algorithm I had used this one which worked partly here: But I do now know how to work only with the columns A to C and how I can first filter by the rows where the sum is below 0.9 and then how to show all rows again. So my desired outcome is something like this: Important, all columns (including product column) and rows should still be there and the format be a dataframe with all of the rows. I only added the sum function below to see that they add up to 0.9 or more. "
            },
            "io": [
                "print (df)\n   Name     A         B         C\n0  Bread    0.0414  0.170292  0.690000\n1  Butter   0.0000  0.452000  0.452000\n2  Cheese   0.70    0.3330   0.0333\n",
                "Sum = df[\"A\"]+df[\"B\"]+df[\"C\"]\n    print (Sum)\n\n0    0.9\n1    0.9\n2    1.0663\n"
            ],
            "answer": {
                "ans_desc": "To save the intermediate values in a new dataframe : is: And if you do: you get: ",
                "code": [
                    "df2 = df.apply(lambda x : x if x.sum() > 0.9 else x.mul(0.9/x.sum()), axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 1136,
            "user_id": 4564238,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://graph.facebook.com/10206219102981377/picture?type=large",
            "display_name": "Antonio L&#243;pez Ruiz",
            "link": "https://stackoverflow.com/users/4564238/antonio-l%c3%b3pez-ruiz"
        },
        "is_answered": true,
        "view_count": 689,
        "accepted_answer_id": 56762229,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1561498035,
        "creation_date": 1561497674,
        "question_id": 56762170,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56762170/converting-dataframe-to-dictionary-with-header-as-key-and-column-as-array-with-v",
        "title": "Converting DataFrame to dictionary with header as key and column as array with values",
        "body": "<p>I have a dataframe as follows:</p>\n\n<pre><code>A B C \n1 6 1 \n2 5 7 \n3 4 9\n4 2 2\n</code></pre>\n\n<p>I want a dictionary like this:</p>\n\n<pre><code>{A: [1,2,3,4], B:[6,5,4,2], C:[1,7,9,2]}\n</code></pre>\n\n<p>I have tried using the normal <code>df.to_dict()</code> and it is no where close. If  I use the transposed dataframe, hence <code>df.T.to_dict()</code> it gets close, but I have something like this:</p>\n\n<pre><code>{0: {A: 1, B: 6, C:1} , ... , 4:{A: 4, B: 2, C: 2 } }\n</code></pre>\n\n<p>The questions in stack overflow are limited to the dictionary having one value per key, not an array. </p>\n\n<p>It would be very valuable for me to use <code>to_dict()</code> and avoid any for loop, since the database I am using is quite big and I want the computational complexity to be as low as possible. </p>\n",
        "answer_body": "<h3><code>orient='list'</code></h3>\n\n<pre><code>df.to_dict(orient='list')\n</code></pre>\n\n<p>Or: the method actually just checks the first character</p>\n\n<pre><code>df.to_dict('l')\n</code></pre>\n\n<p>If you want to preserve the numpy array</p>\n\n<pre><code>{k: v.to_numpy() for k, v in df.items()}\n</code></pre>\n",
        "question_body": "<p>I have a dataframe as follows:</p>\n\n<pre><code>A B C \n1 6 1 \n2 5 7 \n3 4 9\n4 2 2\n</code></pre>\n\n<p>I want a dictionary like this:</p>\n\n<pre><code>{A: [1,2,3,4], B:[6,5,4,2], C:[1,7,9,2]}\n</code></pre>\n\n<p>I have tried using the normal <code>df.to_dict()</code> and it is no where close. If  I use the transposed dataframe, hence <code>df.T.to_dict()</code> it gets close, but I have something like this:</p>\n\n<pre><code>{0: {A: 1, B: 6, C:1} , ... , 4:{A: 4, B: 2, C: 2 } }\n</code></pre>\n\n<p>The questions in stack overflow are limited to the dictionary having one value per key, not an array. </p>\n\n<p>It would be very valuable for me to use <code>to_dict()</code> and avoid any for loop, since the database I am using is quite big and I want the computational complexity to be as low as possible. </p>\n",
        "formatted_input": {
            "qid": 56762170,
            "link": "https://stackoverflow.com/questions/56762170/converting-dataframe-to-dictionary-with-header-as-key-and-column-as-array-with-v",
            "question": {
                "title": "Converting DataFrame to dictionary with header as key and column as array with values",
                "ques_desc": "I have a dataframe as follows: I want a dictionary like this: I have tried using the normal and it is no where close. If I use the transposed dataframe, hence it gets close, but I have something like this: The questions in stack overflow are limited to the dictionary having one value per key, not an array. It would be very valuable for me to use and avoid any for loop, since the database I am using is quite big and I want the computational complexity to be as low as possible. "
            },
            "io": [
                "A B C \n1 6 1 \n2 5 7 \n3 4 9\n4 2 2\n",
                "{0: {A: 1, B: 6, C:1} , ... , 4:{A: 4, B: 2, C: 2 } }\n"
            ],
            "answer": {
                "ans_desc": " Or: the method actually just checks the first character If you want to preserve the numpy array ",
                "code": [
                    "{k: v.to_numpy() for k, v in df.items()}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "for-loop",
            "apply"
        ],
        "owner": {
            "reputation": 315,
            "user_id": 7557277,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/TJi8f.jpg?s=128&g=1",
            "display_name": "Jo&#227;o Machado",
            "link": "https://stackoverflow.com/users/7557277/jo%c3%a3o-machado"
        },
        "is_answered": true,
        "view_count": 253,
        "accepted_answer_id": 56746141,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1561430396,
        "creation_date": 1561386413,
        "last_edit_date": 1561428890,
        "question_id": 56738691,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56738691/is-there-a-way-to-use-previous-row-value-in-pandas-apply-function-when-previous",
        "title": "Is there a way to use previous row value in pandas&#39; apply function when previous value is iteratively summed ?or an efficient way?",
        "body": "<p>I have a dataframe with some columns and I would like to apply the following transformation in an efficient manner. </p>\n\n<p>Given  the Dataframe below:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   C    D\n ===========\n  Nan  10\n  0  22 \n  2  280\n  4  250\n  6  270\n</code></pre>\n\n<p>It should be transformed in such a way I can get the following output:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   C    D\n ===========\n  Nan  10\n  0  22 \n  2  280\n  6  252\n  12 276 \n</code></pre>\n\n<p>Note that:</p>\n\n<p><strong>C[i]</strong> = <strong>C[i]</strong> + <strong>C[i - 1]</strong> + ... + <strong>C[0]</strong> </p>\n\n<p>and</p>\n\n<p><strong>D[i]</strong> = <strong>D[i]</strong> + <strong>C[i - 1]</strong></p>\n\n<p>NaN values should be filtered.</p>\n\n<p>Thx!</p>\n",
        "answer_body": "<pre><code>df['C'] = df['C'].cumsum()\ndf['D'] = df['D'].add(df['C'].shift(1).fillna(0))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>      C      D\n0   NaN      10.0\n1   0.0      22.0\n2   2.0     280.0\n3   6.0     252.0\n4   12.0    276.0\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with some columns and I would like to apply the following transformation in an efficient manner. </p>\n\n<p>Given  the Dataframe below:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   C    D\n ===========\n  Nan  10\n  0  22 \n  2  280\n  4  250\n  6  270\n</code></pre>\n\n<p>It should be transformed in such a way I can get the following output:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>   C    D\n ===========\n  Nan  10\n  0  22 \n  2  280\n  6  252\n  12 276 \n</code></pre>\n\n<p>Note that:</p>\n\n<p><strong>C[i]</strong> = <strong>C[i]</strong> + <strong>C[i - 1]</strong> + ... + <strong>C[0]</strong> </p>\n\n<p>and</p>\n\n<p><strong>D[i]</strong> = <strong>D[i]</strong> + <strong>C[i - 1]</strong></p>\n\n<p>NaN values should be filtered.</p>\n\n<p>Thx!</p>\n",
        "formatted_input": {
            "qid": 56738691,
            "link": "https://stackoverflow.com/questions/56738691/is-there-a-way-to-use-previous-row-value-in-pandas-apply-function-when-previous",
            "question": {
                "title": "Is there a way to use previous row value in pandas&#39; apply function when previous value is iteratively summed ?or an efficient way?",
                "ques_desc": "I have a dataframe with some columns and I would like to apply the following transformation in an efficient manner. Given the Dataframe below: It should be transformed in such a way I can get the following output: Note that: C[i] = C[i] + C[i - 1] + ... + C[0] and D[i] = D[i] + C[i - 1] NaN values should be filtered. Thx! "
            },
            "io": [
                "   C    D\n ===========\n  Nan  10\n  0  22 \n  2  280\n  4  250\n  6  270\n",
                "   C    D\n ===========\n  Nan  10\n  0  22 \n  2  280\n  6  252\n  12 276 \n"
            ],
            "answer": {
                "ans_desc": " Output: ",
                "code": [
                    "df['C'] = df['C'].cumsum()\ndf['D'] = df['D'].add(df['C'].shift(1).fillna(0))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 1340,
            "user_id": 2902740,
            "user_type": "registered",
            "accept_rate": 85,
            "profile_image": "https://www.gravatar.com/avatar/70bd9c4e41cf2d86926baf911aea992b?s=128&d=identicon&r=PG",
            "display_name": "DaveRGP",
            "link": "https://stackoverflow.com/users/2902740/davergp"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 56682879,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1561390011,
        "creation_date": 1561018222,
        "last_edit_date": 1561022514,
        "question_id": 56681714,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56681714/how-should-i-construct-this-json-return-from-a-pandas-dataframe",
        "title": "How should I construct this json return from a pandas dataframe",
        "body": "<p>I have some data, organised by date, as a <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html\" rel=\"nofollow noreferrer\">datetime index</a>. I then subset it so it is effectively irregular:</p>\n\n<pre><code>date_rng = pd.date_range(start='1/1/2018', end='1/8/2018', freq='H')\ndf = pd.DataFrame(date_rng, columns=['date'])\ndf['data'] = np.random.randint(0,100,size=(len(date_rng)))\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date', drop=True)\ndf = df[df['data'] &gt; 75]\n</code></pre>\n\n<p>In my <em>service</em> (this is not for interactive use) I am given a string which I pass to <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\" rel=\"nofollow noreferrer\"><code>pandas.resample</code></a> to aggregate my time data to any level. The string is supplied directly to the <code>rule</code> argument, and can be values like <code>'H'</code>, <code>'3T'</code>, <code>'Q'</code>, <code>'M'</code></p>\n\n<p>I would like to use the same string to create an json which is similar to the following structure:</p>\n\n<pre><code>{'my_aggregate_level_1': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'},\n'my_aggregate_level_2':{'2018-01-08 03:00:00'}}\n</code></pre>\n\n<p>The array will be 'ragged' i.e. not all records will be present, and not all keys in the json will have the same length of array.</p>\n\n<p>The goals for a good solution are:</p>\n\n<ol>\n<li><code>my_aggregate_level</code> is the level set by the same string parameter as given to <code>resample</code></li>\n<li>the array in the aggregate level will always be the raw, datetime hourly values</li>\n<li><em>Ideally</em> the string parameter is not associated with a bunch of 'translatation rules' such as \"If <code>'D'</code> then use <code>.strftime</code> like this, but if <code>'H'</code> use this and if <code>'M'</code> use this</li>\n<li><code>'H'</code> would just return a single array, of the raw values</li>\n</ol>\n\n<p>So in practice if a <code>'D'</code> is supplied:</p>\n\n<pre><code>{'2018-01-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'},\n'2018-01-08':{'2018-01-08 03:00:00'}}\n</code></pre>\n\n<p>Note that there are two keys at daily level, with the values in the array split to the correct day.</p>\n\n<p>If <code>'M'</code> is supplied:</p>\n\n<pre><code>{'2018-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00', '2018-01-08 03:00:00'}}\n</code></pre>\n\n<p>Note that this means the contents of the value array will be 3 in this example, as the 3 datetimes are all in the same month </p>\n\n<h2>Things I've tried/looked at that I haven't made work well:</h2>\n\n<ul>\n<li><code>Groupers</code>, they look like they aggregate only, based on some rules. I specifically need to return the actual records</li>\n<li>Parseing a new column based on the <code>rule</code> argument would technically work, but it seems wrong as I would have to start converting each <code>rule</code> to a <code>strftime</code> or similar. I have not yet found a function that accepts the same character string and does not also perform aggregations </li>\n<li>Is setting a multi-index a solution to this? It might be but I'm not certain how to populate it in regards to the point above about the <code>'D'</code>, <code>'M'</code>, etc.</li>\n<li>custom resampler:</li>\n</ul>\n\n<pre><code>def custom_resampler(array_like):\n    return array_like\n\ndf.resample('W').apply(custom_resampler)\n</code></pre>\n\n<p>Which is not working.</p>\n\n<p>I understand, it may be that this is not solvable with the rules above, but I am probably not good enough at <code>pandas</code> yet to realise it.</p>\n",
        "answer_body": "<h2>UPDATE:</h2>\n\n<p>The return structure is infact a series, and as such can be filtered for empty list index points using this method:</p>\n\n<pre><code>df.resample('D').apply(custom_resampler).apply(len) &gt; 0\n</code></pre>\n\n<h2>ORIGINAL:</h2>\n\n<p>I believe I have worked out a solution.</p>\n\n<p>Specifically, I was using custom resamplers wrong, and in fact they can do the json conversion itself. I need to process a little to get to the result I need, but for now this is the answer:</p>\n\n<pre><code>def custom_resampler(array_like):\n    return array_like.to_json()\n\ndf.resample('D').apply(custom_resampler)\n</code></pre>\n\n<p>Very happy to be corrected/refined/improved on this.</p>\n",
        "question_body": "<p>I have some data, organised by date, as a <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html\" rel=\"nofollow noreferrer\">datetime index</a>. I then subset it so it is effectively irregular:</p>\n\n<pre><code>date_rng = pd.date_range(start='1/1/2018', end='1/8/2018', freq='H')\ndf = pd.DataFrame(date_rng, columns=['date'])\ndf['data'] = np.random.randint(0,100,size=(len(date_rng)))\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date', drop=True)\ndf = df[df['data'] &gt; 75]\n</code></pre>\n\n<p>In my <em>service</em> (this is not for interactive use) I am given a string which I pass to <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\" rel=\"nofollow noreferrer\"><code>pandas.resample</code></a> to aggregate my time data to any level. The string is supplied directly to the <code>rule</code> argument, and can be values like <code>'H'</code>, <code>'3T'</code>, <code>'Q'</code>, <code>'M'</code></p>\n\n<p>I would like to use the same string to create an json which is similar to the following structure:</p>\n\n<pre><code>{'my_aggregate_level_1': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'},\n'my_aggregate_level_2':{'2018-01-08 03:00:00'}}\n</code></pre>\n\n<p>The array will be 'ragged' i.e. not all records will be present, and not all keys in the json will have the same length of array.</p>\n\n<p>The goals for a good solution are:</p>\n\n<ol>\n<li><code>my_aggregate_level</code> is the level set by the same string parameter as given to <code>resample</code></li>\n<li>the array in the aggregate level will always be the raw, datetime hourly values</li>\n<li><em>Ideally</em> the string parameter is not associated with a bunch of 'translatation rules' such as \"If <code>'D'</code> then use <code>.strftime</code> like this, but if <code>'H'</code> use this and if <code>'M'</code> use this</li>\n<li><code>'H'</code> would just return a single array, of the raw values</li>\n</ol>\n\n<p>So in practice if a <code>'D'</code> is supplied:</p>\n\n<pre><code>{'2018-01-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'},\n'2018-01-08':{'2018-01-08 03:00:00'}}\n</code></pre>\n\n<p>Note that there are two keys at daily level, with the values in the array split to the correct day.</p>\n\n<p>If <code>'M'</code> is supplied:</p>\n\n<pre><code>{'2018-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00', '2018-01-08 03:00:00'}}\n</code></pre>\n\n<p>Note that this means the contents of the value array will be 3 in this example, as the 3 datetimes are all in the same month </p>\n\n<h2>Things I've tried/looked at that I haven't made work well:</h2>\n\n<ul>\n<li><code>Groupers</code>, they look like they aggregate only, based on some rules. I specifically need to return the actual records</li>\n<li>Parseing a new column based on the <code>rule</code> argument would technically work, but it seems wrong as I would have to start converting each <code>rule</code> to a <code>strftime</code> or similar. I have not yet found a function that accepts the same character string and does not also perform aggregations </li>\n<li>Is setting a multi-index a solution to this? It might be but I'm not certain how to populate it in regards to the point above about the <code>'D'</code>, <code>'M'</code>, etc.</li>\n<li>custom resampler:</li>\n</ul>\n\n<pre><code>def custom_resampler(array_like):\n    return array_like\n\ndf.resample('W').apply(custom_resampler)\n</code></pre>\n\n<p>Which is not working.</p>\n\n<p>I understand, it may be that this is not solvable with the rules above, but I am probably not good enough at <code>pandas</code> yet to realise it.</p>\n",
        "formatted_input": {
            "qid": 56681714,
            "link": "https://stackoverflow.com/questions/56681714/how-should-i-construct-this-json-return-from-a-pandas-dataframe",
            "question": {
                "title": "How should I construct this json return from a pandas dataframe",
                "ques_desc": "I have some data, organised by date, as a datetime index. I then subset it so it is effectively irregular: In my service (this is not for interactive use) I am given a string which I pass to to aggregate my time data to any level. The string is supplied directly to the argument, and can be values like , , , I would like to use the same string to create an json which is similar to the following structure: The array will be 'ragged' i.e. not all records will be present, and not all keys in the json will have the same length of array. The goals for a good solution are: is the level set by the same string parameter as given to the array in the aggregate level will always be the raw, datetime hourly values Ideally the string parameter is not associated with a bunch of 'translatation rules' such as \"If then use like this, but if use this and if use this would just return a single array, of the raw values So in practice if a is supplied: Note that there are two keys at daily level, with the values in the array split to the correct day. If is supplied: Note that this means the contents of the value array will be 3 in this example, as the 3 datetimes are all in the same month Things I've tried/looked at that I haven't made work well: , they look like they aggregate only, based on some rules. I specifically need to return the actual records Parseing a new column based on the argument would technically work, but it seems wrong as I would have to start converting each to a or similar. I have not yet found a function that accepts the same character string and does not also perform aggregations Is setting a multi-index a solution to this? It might be but I'm not certain how to populate it in regards to the point above about the , , etc. custom resampler: Which is not working. I understand, it may be that this is not solvable with the rules above, but I am probably not good enough at yet to realise it. "
            },
            "io": [
                "{'2018-01-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'},\n'2018-01-08':{'2018-01-08 03:00:00'}}\n",
                "{'2018-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00', '2018-01-08 03:00:00'}}\n"
            ],
            "answer": {
                "ans_desc": "UPDATE: The return structure is infact a series, and as such can be filtered for empty list index points using this method: ORIGINAL: I believe I have worked out a solution. Specifically, I was using custom resamplers wrong, and in fact they can do the json conversion itself. I need to process a little to get to the result I need, but for now this is the answer: Very happy to be corrected/refined/improved on this. ",
                "code": [
                    "def custom_resampler(array_like):\n    return array_like.to_json()\n\ndf.resample('D').apply(custom_resampler)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 889,
            "user_id": 1922184,
            "user_type": "registered",
            "accept_rate": 64,
            "profile_image": "https://www.gravatar.com/avatar/6ebcd95739a0a86080a7024486e910e3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Relative0",
            "link": "https://stackoverflow.com/users/1922184/relative0"
        },
        "is_answered": true,
        "view_count": 218,
        "accepted_answer_id": 56723259,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1561289757,
        "creation_date": 1561287617,
        "last_edit_date": 1561289733,
        "question_id": 56723207,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56723207/negating-column-values-and-adding-particular-values-in-only-some-columns-in-a-pa",
        "title": "Negating column values and adding particular values in only some columns in a Pandas Dataframe",
        "body": "<p>Taking a Pandas dataframe df I would like to be able to both take away the value in the particular column for all rows/entries and also add another value. This value to be added is a fixed additive for each of the columns.</p>\n\n<p>I believe I could reproduce df, say dfcopy=df, set all cell values in dfcopy to the particular numbers and then subtract df from dfcopy but am hoping for a simpler way.</p>\n\n<p>I am thinking that I need to somehow modify</p>\n\n<pre><code>df.iloc[:, [0,3,4]]\n</code></pre>\n\n<p>So for example of how this should look:</p>\n\n<pre><code>    A   B   C   D   E\n0   1.0 3.0 1.0 2.0 7.0\n1   2.0 1.0 8.0 5.0 3.0\n2   1.0 1.0 1.0 1.0 6.0\n</code></pre>\n\n<p>Then negating only those values in columns (0,3,4) and then adding 10 (for example) we would have:</p>\n\n<pre><code>    A   B   C   D   E\n0   9.0 3.0 1.0 8.0 3.0\n1   8.0 1.0 8.0 5.0 7.0\n2   9.0 1.0 1.0 9.0 4.0\n</code></pre>\n\n<p>Thanks.</p>\n",
        "answer_body": "<p>pandas is very intuitive in letting you perform these operations,</p>\n\n<p>negate:</p>\n\n<pre><code>df.iloc[:, [0,2,7,10,11] = -df.iloc[:, [0,2,7,10,11]\n</code></pre>\n\n<p>add a constant c:</p>\n\n<pre><code>df.iloc[:, [0,2,7,10,11] = df.iloc[:, [0,2,7,10,11]+c\n</code></pre>\n\n<p>or change to constant value c:</p>\n\n<pre><code>df.iloc[:, [0,2,7,10,11] = c\n</code></pre>\n\n<p>and any other arithmetics you can think of</p>\n",
        "question_body": "<p>Taking a Pandas dataframe df I would like to be able to both take away the value in the particular column for all rows/entries and also add another value. This value to be added is a fixed additive for each of the columns.</p>\n\n<p>I believe I could reproduce df, say dfcopy=df, set all cell values in dfcopy to the particular numbers and then subtract df from dfcopy but am hoping for a simpler way.</p>\n\n<p>I am thinking that I need to somehow modify</p>\n\n<pre><code>df.iloc[:, [0,3,4]]\n</code></pre>\n\n<p>So for example of how this should look:</p>\n\n<pre><code>    A   B   C   D   E\n0   1.0 3.0 1.0 2.0 7.0\n1   2.0 1.0 8.0 5.0 3.0\n2   1.0 1.0 1.0 1.0 6.0\n</code></pre>\n\n<p>Then negating only those values in columns (0,3,4) and then adding 10 (for example) we would have:</p>\n\n<pre><code>    A   B   C   D   E\n0   9.0 3.0 1.0 8.0 3.0\n1   8.0 1.0 8.0 5.0 7.0\n2   9.0 1.0 1.0 9.0 4.0\n</code></pre>\n\n<p>Thanks.</p>\n",
        "formatted_input": {
            "qid": 56723207,
            "link": "https://stackoverflow.com/questions/56723207/negating-column-values-and-adding-particular-values-in-only-some-columns-in-a-pa",
            "question": {
                "title": "Negating column values and adding particular values in only some columns in a Pandas Dataframe",
                "ques_desc": "Taking a Pandas dataframe df I would like to be able to both take away the value in the particular column for all rows/entries and also add another value. This value to be added is a fixed additive for each of the columns. I believe I could reproduce df, say dfcopy=df, set all cell values in dfcopy to the particular numbers and then subtract df from dfcopy but am hoping for a simpler way. I am thinking that I need to somehow modify So for example of how this should look: Then negating only those values in columns (0,3,4) and then adding 10 (for example) we would have: Thanks. "
            },
            "io": [
                "    A   B   C   D   E\n0   1.0 3.0 1.0 2.0 7.0\n1   2.0 1.0 8.0 5.0 3.0\n2   1.0 1.0 1.0 1.0 6.0\n",
                "    A   B   C   D   E\n0   9.0 3.0 1.0 8.0 3.0\n1   8.0 1.0 8.0 5.0 7.0\n2   9.0 1.0 1.0 9.0 4.0\n"
            ],
            "answer": {
                "ans_desc": "pandas is very intuitive in letting you perform these operations, negate: add a constant c: or change to constant value c: and any other arithmetics you can think of ",
                "code": [
                    "df.iloc[:, [0,2,7,10,11] = -df.iloc[:, [0,2,7,10,11]\n",
                    "df.iloc[:, [0,2,7,10,11] = df.iloc[:, [0,2,7,10,11]+c\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 151,
            "user_id": 4137346,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/104414f78cc6c907444cd2663d84bb3a?s=128&d=identicon&r=PG&f=1",
            "display_name": "deeraf ",
            "link": "https://stackoverflow.com/users/4137346/deeraf"
        },
        "is_answered": true,
        "view_count": 2459,
        "accepted_answer_id": 56666359,
        "answer_count": 5,
        "score": 11,
        "last_activity_date": 1561076319,
        "creation_date": 1560942490,
        "last_edit_date": 1560943272,
        "question_id": 56666271,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56666271/how-to-find-the-last-non-zero-element-in-every-column-throughout-dataframe",
        "title": "How to find the last non zero element in every column throughout dataframe?",
        "body": "<p>How can one go about finding the last occurring non zero element in every column of a dataframe? </p>\n\n<p><strong>Input</strong></p>\n\n<pre><code>    A  B\n0   0  1\n1   0  2\n2   9  0\n3  10  0\n4   0  0\n5   0  0\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>    A  B\n0  10  2\n</code></pre>\n",
        "answer_body": "<p>Here's one approach using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.argmax.html\" rel=\"noreferrer\"><code>ndarray.argmax</code></a> and advanced indexing:</p>\n\n<pre><code>first_max = df.values[df.ne(0).values.argmax(0), range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n</code></pre>\n\n<hr>\n\n<pre><code>df = pd.DataFrame({'A': [0,0,0,10,0,0] , 'B': [0,2,0,0,0,0]})\n\nfirst_max = df.values[df.ne(0).values.argmax(0), range(df.shape[1])]\n# array([10,  2])\npd.DataFrame([first_max], columns=df.columns)\n\n    A  B\n0  10  2\n</code></pre>\n\n<hr>\n\n<p><b> Update </b></p>\n\n<p>In order to find the <em>last</em> nonzero:</p>\n\n<pre><code>row_ix = df.shape[0]-df.ne(0).values[::-1].argmax(0)-1\nfirst_max = df.values[row_ix, range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n</code></pre>\n",
        "question_body": "<p>How can one go about finding the last occurring non zero element in every column of a dataframe? </p>\n\n<p><strong>Input</strong></p>\n\n<pre><code>    A  B\n0   0  1\n1   0  2\n2   9  0\n3  10  0\n4   0  0\n5   0  0\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>    A  B\n0  10  2\n</code></pre>\n",
        "formatted_input": {
            "qid": 56666271,
            "link": "https://stackoverflow.com/questions/56666271/how-to-find-the-last-non-zero-element-in-every-column-throughout-dataframe",
            "question": {
                "title": "How to find the last non zero element in every column throughout dataframe?",
                "ques_desc": "How can one go about finding the last occurring non zero element in every column of a dataframe? Input Output "
            },
            "io": [
                "    A  B\n0   0  1\n1   0  2\n2   9  0\n3  10  0\n4   0  0\n5   0  0\n",
                "    A  B\n0  10  2\n"
            ],
            "answer": {
                "ans_desc": "Here's one approach using and advanced indexing: Update In order to find the last nonzero: ",
                "code": [
                    "first_max = df.values[df.ne(0).values.argmax(0), range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n",
                    "row_ix = df.shape[0]-df.ne(0).values[::-1].argmax(0)-1\nfirst_max = df.values[row_ix, range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "python-2.7",
            "dataframe"
        ],
        "owner": {
            "reputation": 21,
            "user_id": 11334265,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/qz4gw.png?s=128&g=1",
            "display_name": "fwench_toast",
            "link": "https://stackoverflow.com/users/11334265/fwench-toast"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 56694988,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1561072729,
        "creation_date": 1561061585,
        "question_id": 56693262,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56693262/re-arranging-a-single-column-of-strings-based-on-text-containing-different-dates",
        "title": "Re-arranging a single column of strings based on text containing different dates, by date",
        "body": "<p>I am looking to arrange a dataframe by dates, however, the dates are a part of a string within each row. The rows must be rearranged in order by day. </p>\n\n<p>Other solutions from stack overflow show how to sort based on a column of dates alone, this example is different because other information is a part of each string and is mixed with the dates. </p>\n\n<p>The dataframe is one column with an index, but the rows are not arranged in  order from the dates contained on the far right side of each string.\nThe score numbers are random and do not require any attention. </p>\n\n<pre><code>                        0\n__________________________\n0     score17 6-20-19.xlsx\n1     score23 6-7-19.xlsx\n2     score4  6-17-19.xlsx      \n3     score34 6-8-19.xlsx\n4     score10 6-7-19.xlsx\n</code></pre>\n\n<p>The expected dataframe should look like this (repeated dates have no preference for order between each other and index doesn't matter). The respective scores must stay with their associated dates.</p>\n\n<pre><code>                         0\n__________________________\n1     score23 6-7-19.xlsx\n4     score10 6-7-19.xlsx\n3     score34 6-8-19.xlsx\n2     score4  6-17-19.xlsx\n0     score17 6-20-19.xlsx\n</code></pre>\n\n<p>What is a way to do this?</p>\n",
        "answer_body": "<p>A somewhat crude way using common string expressions to create a few columns, then sort accordingly.</p>\n\n<p>First, I'd suggest \"stripping\" your column to ensure leading/lagging whitespace is not an issue, your example hand non standard spacing.</p>\n\n<pre><code>df['column_name'] = df['column_name'].str.strip()\n</code></pre>\n\n<p>You can then split your column at the first \"space\" (' ') like so, note this creates two columns:</p>\n\n<pre><code>df[['score', 'date']] = df['column_name'].str.split(' ', n=1, expand=True)\n</code></pre>\n\n<p>You can then split your \"date\" column at the period ('.') to get rid of the extension:</p>\n\n<pre><code>df['date'] = df['date'].str.split('.', expand = True)\n</code></pre>\n\n<p>Then cast that \"date\" column as datetime:</p>\n\n<pre><code>df['date'] = df['date'].astype('datetime64[ns]')\n</code></pre>\n\n<p>Now you can sort your dataframe based on this \"date\" column, setting ascending = True/False based on your desired format.</p>\n\n<pre><code>df.sort_values(by='date', ascending = False)\n</code></pre>\n",
        "question_body": "<p>I am looking to arrange a dataframe by dates, however, the dates are a part of a string within each row. The rows must be rearranged in order by day. </p>\n\n<p>Other solutions from stack overflow show how to sort based on a column of dates alone, this example is different because other information is a part of each string and is mixed with the dates. </p>\n\n<p>The dataframe is one column with an index, but the rows are not arranged in  order from the dates contained on the far right side of each string.\nThe score numbers are random and do not require any attention. </p>\n\n<pre><code>                        0\n__________________________\n0     score17 6-20-19.xlsx\n1     score23 6-7-19.xlsx\n2     score4  6-17-19.xlsx      \n3     score34 6-8-19.xlsx\n4     score10 6-7-19.xlsx\n</code></pre>\n\n<p>The expected dataframe should look like this (repeated dates have no preference for order between each other and index doesn't matter). The respective scores must stay with their associated dates.</p>\n\n<pre><code>                         0\n__________________________\n1     score23 6-7-19.xlsx\n4     score10 6-7-19.xlsx\n3     score34 6-8-19.xlsx\n2     score4  6-17-19.xlsx\n0     score17 6-20-19.xlsx\n</code></pre>\n\n<p>What is a way to do this?</p>\n",
        "formatted_input": {
            "qid": 56693262,
            "link": "https://stackoverflow.com/questions/56693262/re-arranging-a-single-column-of-strings-based-on-text-containing-different-dates",
            "question": {
                "title": "Re-arranging a single column of strings based on text containing different dates, by date",
                "ques_desc": "I am looking to arrange a dataframe by dates, however, the dates are a part of a string within each row. The rows must be rearranged in order by day. Other solutions from stack overflow show how to sort based on a column of dates alone, this example is different because other information is a part of each string and is mixed with the dates. The dataframe is one column with an index, but the rows are not arranged in order from the dates contained on the far right side of each string. The score numbers are random and do not require any attention. The expected dataframe should look like this (repeated dates have no preference for order between each other and index doesn't matter). The respective scores must stay with their associated dates. What is a way to do this? "
            },
            "io": [
                "                        0\n__________________________\n0     score17 6-20-19.xlsx\n1     score23 6-7-19.xlsx\n2     score4  6-17-19.xlsx      \n3     score34 6-8-19.xlsx\n4     score10 6-7-19.xlsx\n",
                "                         0\n__________________________\n1     score23 6-7-19.xlsx\n4     score10 6-7-19.xlsx\n3     score34 6-8-19.xlsx\n2     score4  6-17-19.xlsx\n0     score17 6-20-19.xlsx\n"
            ],
            "answer": {
                "ans_desc": "A somewhat crude way using common string expressions to create a few columns, then sort accordingly. First, I'd suggest \"stripping\" your column to ensure leading/lagging whitespace is not an issue, your example hand non standard spacing. You can then split your column at the first \"space\" (' ') like so, note this creates two columns: You can then split your \"date\" column at the period ('.') to get rid of the extension: Then cast that \"date\" column as datetime: Now you can sort your dataframe based on this \"date\" column, setting ascending = True/False based on your desired format. ",
                "code": [
                    "df[['score', 'date']] = df['column_name'].str.split(' ', n=1, expand=True)\n",
                    "df['date'] = df['date'].str.split('.', expand = True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "arrays",
            "pandas",
            "dataframe",
            "concatenation"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 11541449,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/2259817940774268/picture?type=large",
            "display_name": "Kaio Giovanni",
            "link": "https://stackoverflow.com/users/11541449/kaio-giovanni"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 56677505,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1560991554,
        "creation_date": 1560988550,
        "last_edit_date": 1560989970,
        "question_id": 56677321,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56677321/pandas-concatenation-not-working-properly",
        "title": "Pandas Concatenation not working properly",
        "body": "<p>So I've been setting up a label archive on my deep learning classifier and I wanted to concatenate the labels of an already existing 2D archive into one I just made.</p>\n\n<p>The one that exists is 'y_trainvalid' (39209, 43), which stands for 39209 images in 43 classes. The new label archive I'm trying to add is 'new_file_label' (23, 43). On these archives, the number set to 1 if it matches the class and 0 if it doesn't.\nHere's a sample of both of them:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>print(y_trainvalid)\nprint(new_file_label)\n\n       0    1    2    3    4    5    6   ...   36   37   38   39   40   41   42\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n5     0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n6     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n7     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n8     0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n10    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n11    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n12    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n13    0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n14    0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n15    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n16    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n17    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n18    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n19    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n20    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n21    0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n22    0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n23    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n24    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n25    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n26    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n27    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n28    0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n29    0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n4380  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4381  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4382  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4383  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4384  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4385  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4386  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4387  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4388  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4389  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4390  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4391  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4392  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4393  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4394  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4395  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4396  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4397  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4398  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4399  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4400  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4401  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4402  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4403  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4404  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4405  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4406  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4407  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4408  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4409  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n[39209 rows x 43 columns]\n      0    1    2    3    4    5    6  ...   36   37   38   39   40   41   42\n0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n16  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n19  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n20  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n22  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n[23 rows x 43 columns]\n\n</code></pre>\n\n<p>When I tried to concatenate using this command:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>y_trainvalid2 = pd.concat([y_trainvalid, new_file_label], ignore_index=True)\n</code></pre>\n\n<p>Something like this appeared:</p>\n\n<pre><code> 0    1    2    3    4    5    6  ...   41   42    5    6    7    8    9\n39204  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39205  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39206  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39207  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39208  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39209  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39210  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39211  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39212  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39213  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39214  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39215  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39216  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39217  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39218  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39219  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39220  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39221  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39222  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39223  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39224  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39225  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39226  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39227  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39228  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39229  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39230  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39231  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n</code></pre>\n\n<p>As if it doubled the amount of columns to fit the data instead of putting the new data just below it. I'm not sure why this is happening cause I'm pretty sure both label archives have the same number of columns.</p>\n\n<p>When I print use the 'y_trainvalid2.head().to_dict()' command, this appears:</p>\n\n<pre><code>{0: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '0': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 1: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '1': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 10: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '10': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '11': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 12: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '12': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 13: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '13': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 14: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '14': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '15': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 16: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '16': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 17: {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '17': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 18: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '18': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 19: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '19': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 2: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '2': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 20: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '20': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 21: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '21': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 22: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '22': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 23: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '23': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 24: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '24': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 25: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '25': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 26: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '26': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 27: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '27': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 28: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '28': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 29: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '29': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '3': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 30: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '30': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 31: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '31': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 32: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0},\n '32': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 33: {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0},\n '33': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 34: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '34': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 35: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '35': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 36: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '36': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 37: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '37': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 38: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0},\n '38': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 39: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '39': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 4: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '4': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 40: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '40': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 41: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '41': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 42: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '42': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '5': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 6: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '6': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '7': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 8: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '8': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 9: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '9': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}}\n\n</code></pre>\n\n<p>How do I solve this problem?</p>\n",
        "answer_body": "<pre><code>y_trainvalid.columns = [str(x) for x in y_trainvalid.columns]\nnew_file_label.columns = [str(x) for x in new_file_label.columns]\ny_trainvalid2 = pd.concat([y_trainvalid, new_file_label])\n</code></pre>\n",
        "question_body": "<p>So I've been setting up a label archive on my deep learning classifier and I wanted to concatenate the labels of an already existing 2D archive into one I just made.</p>\n\n<p>The one that exists is 'y_trainvalid' (39209, 43), which stands for 39209 images in 43 classes. The new label archive I'm trying to add is 'new_file_label' (23, 43). On these archives, the number set to 1 if it matches the class and 0 if it doesn't.\nHere's a sample of both of them:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>print(y_trainvalid)\nprint(new_file_label)\n\n       0    1    2    3    4    5    6   ...   36   37   38   39   40   41   42\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n5     0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n6     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n7     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n8     0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n10    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n11    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n12    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n13    0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n14    0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n15    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n16    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n17    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n18    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n19    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n20    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n21    0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n22    0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n23    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n24    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n25    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n26    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n27    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n28    0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n29    0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n4380  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4381  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4382  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4383  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4384  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4385  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4386  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4387  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4388  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4389  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4390  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4391  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4392  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4393  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4394  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4395  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4396  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4397  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4398  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4399  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4400  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4401  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4402  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4403  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4404  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4405  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4406  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4407  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4408  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4409  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n[39209 rows x 43 columns]\n      0    1    2    3    4    5    6  ...   36   37   38   39   40   41   42\n0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n16  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n19  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n20  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n22  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n[23 rows x 43 columns]\n\n</code></pre>\n\n<p>When I tried to concatenate using this command:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>y_trainvalid2 = pd.concat([y_trainvalid, new_file_label], ignore_index=True)\n</code></pre>\n\n<p>Something like this appeared:</p>\n\n<pre><code> 0    1    2    3    4    5    6  ...   41   42    5    6    7    8    9\n39204  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39205  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39206  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39207  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39208  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39209  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39210  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39211  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39212  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39213  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39214  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39215  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39216  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39217  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39218  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39219  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39220  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39221  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39222  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39223  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39224  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39225  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39226  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39227  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39228  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39229  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39230  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39231  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n</code></pre>\n\n<p>As if it doubled the amount of columns to fit the data instead of putting the new data just below it. I'm not sure why this is happening cause I'm pretty sure both label archives have the same number of columns.</p>\n\n<p>When I print use the 'y_trainvalid2.head().to_dict()' command, this appears:</p>\n\n<pre><code>{0: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '0': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 1: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '1': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 10: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '10': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '11': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 12: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '12': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 13: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '13': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 14: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '14': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '15': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 16: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '16': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 17: {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '17': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 18: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '18': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 19: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '19': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 2: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '2': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 20: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '20': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 21: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '21': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 22: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '22': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 23: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '23': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 24: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '24': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 25: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '25': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 26: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '26': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 27: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '27': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 28: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '28': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 29: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '29': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '3': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 30: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '30': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 31: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '31': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 32: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0},\n '32': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 33: {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0},\n '33': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 34: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '34': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 35: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '35': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 36: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '36': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 37: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '37': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 38: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0},\n '38': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 39: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '39': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 4: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '4': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 40: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '40': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 41: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '41': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 42: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '42': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '5': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 6: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '6': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '7': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 8: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '8': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 9: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '9': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}}\n\n</code></pre>\n\n<p>How do I solve this problem?</p>\n",
        "formatted_input": {
            "qid": 56677321,
            "link": "https://stackoverflow.com/questions/56677321/pandas-concatenation-not-working-properly",
            "question": {
                "title": "Pandas Concatenation not working properly",
                "ques_desc": "So I've been setting up a label archive on my deep learning classifier and I wanted to concatenate the labels of an already existing 2D archive into one I just made. The one that exists is 'y_trainvalid' (39209, 43), which stands for 39209 images in 43 classes. The new label archive I'm trying to add is 'new_file_label' (23, 43). On these archives, the number set to 1 if it matches the class and 0 if it doesn't. Here's a sample of both of them: When I tried to concatenate using this command: Something like this appeared: As if it doubled the amount of columns to fit the data instead of putting the new data just below it. I'm not sure why this is happening cause I'm pretty sure both label archives have the same number of columns. When I print use the 'y_trainvalid2.head().to_dict()' command, this appears: How do I solve this problem? "
            },
            "io": [
                " 0    1    2    3    4    5    6  ...   41   42    5    6    7    8    9\n39204  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39205  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39206  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39207  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39208  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n39209  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39210  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39211  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39212  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39213  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39214  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39215  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39216  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39217  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39218  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39219  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39220  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39221  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39222  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39223  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39224  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39225  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39226  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39227  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39228  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39229  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39230  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n39231  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
                "{0: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '0': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 1: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '1': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 10: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '10': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '11': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 12: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '12': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 13: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '13': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 14: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '14': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '15': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 16: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '16': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 17: {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '17': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 18: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '18': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 19: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '19': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 2: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '2': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 20: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '20': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 21: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '21': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 22: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '22': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 23: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '23': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 24: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '24': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 25: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '25': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 26: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '26': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 27: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '27': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 28: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '28': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 29: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '29': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '3': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 30: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '30': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 31: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '31': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 32: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0},\n '32': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 33: {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0},\n '33': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 34: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '34': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 35: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '35': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 36: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '36': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 37: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '37': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 38: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0},\n '38': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 39: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '39': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 4: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '4': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 40: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '40': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 41: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '41': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 42: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '42': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '5': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 6: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '6': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '7': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 8: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '8': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},\n 9: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0},\n '9': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}}\n\n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "y_trainvalid.columns = [str(x) for x in y_trainvalid.columns]\nnew_file_label.columns = [str(x) for x in new_file_label.columns]\ny_trainvalid2 = pd.concat([y_trainvalid, new_file_label])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 110,
        "accepted_answer_id": 56662762,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1560932685,
        "creation_date": 1560930931,
        "question_id": 56662690,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56662690/how-to-find-the-last-value-of-consecutive-values-in-pandas-dataframe",
        "title": "how to find the last value of consecutive values in pandas dataframe?",
        "body": "<p>I have a data frame like this </p>\n\n<pre><code>df:\ncol1     col2\n 1        10\n 1        20\n 2        11\n 3        33\n 1        20\n 1        10\n 2        24\n 3        21\n 3        28\n</code></pre>\n\n<p>I want to group by this data frame on col1 where there is consecutive values, and take the last value for each consecutive groups, </p>\n\n<p>The final data frame should look like:</p>\n\n<pre><code>df\ncol1    col2\n 1       20\n 2       11\n 3       33\n 1       10\n 2       24\n 3       28\n</code></pre>\n\n<p>I have tried something like: </p>\n\n<pre><code> df['b_new'] = df.groupby('col1')['col2'].transform('last')\n</code></pre>\n\n<p>But its missing the consecutive condition.</p>\n\n<p>How to implement it in most effective way using pandas/ python</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a> with filtering by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ne.html\" rel=\"nofollow noreferrer\"><code>Series.ne</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html\" rel=\"nofollow noreferrer\"><code>Series.shift</code></a>ed Series with <code>-1</code> for last dupe consecutive rows:</p>\n\n<pre><code>df1 = df[df['col1'].ne(df['col1'].shift(-1))]\nprint (df1)\n   col1  col2\n1     1    20\n2     2    11\n3     3    33\n5     1    10\n6     2    24\n8     3    28\n</code></pre>\n\n<p><strong>Detail</strong>:</p>\n\n<pre><code>print (df['col1'].ne(df['col1'].shift(-1)))\n0    False\n1     True\n2     True\n3     True\n4    False\n5     True\n6     True\n7    False\n8     True\nName: col1, dtype: bool\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this </p>\n\n<pre><code>df:\ncol1     col2\n 1        10\n 1        20\n 2        11\n 3        33\n 1        20\n 1        10\n 2        24\n 3        21\n 3        28\n</code></pre>\n\n<p>I want to group by this data frame on col1 where there is consecutive values, and take the last value for each consecutive groups, </p>\n\n<p>The final data frame should look like:</p>\n\n<pre><code>df\ncol1    col2\n 1       20\n 2       11\n 3       33\n 1       10\n 2       24\n 3       28\n</code></pre>\n\n<p>I have tried something like: </p>\n\n<pre><code> df['b_new'] = df.groupby('col1')['col2'].transform('last')\n</code></pre>\n\n<p>But its missing the consecutive condition.</p>\n\n<p>How to implement it in most effective way using pandas/ python</p>\n",
        "formatted_input": {
            "qid": 56662690,
            "link": "https://stackoverflow.com/questions/56662690/how-to-find-the-last-value-of-consecutive-values-in-pandas-dataframe",
            "question": {
                "title": "how to find the last value of consecutive values in pandas dataframe?",
                "ques_desc": "I have a data frame like this I want to group by this data frame on col1 where there is consecutive values, and take the last value for each consecutive groups, The final data frame should look like: I have tried something like: But its missing the consecutive condition. How to implement it in most effective way using pandas/ python "
            },
            "io": [
                "df:\ncol1     col2\n 1        10\n 1        20\n 2        11\n 3        33\n 1        20\n 1        10\n 2        24\n 3        21\n 3        28\n",
                "df\ncol1    col2\n 1       20\n 2       11\n 3       33\n 1       10\n 2       24\n 3       28\n"
            ],
            "answer": {
                "ans_desc": "Use with filtering by with ed Series with for last dupe consecutive rows: Detail: ",
                "code": [
                    "print (df['col1'].ne(df['col1'].shift(-1)))\n0    False\n1     True\n2     True\n3     True\n4    False\n5     True\n6     True\n7    False\n8     True\nName: col1, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-analysis"
        ],
        "owner": {
            "reputation": 4482,
            "user_id": 3922295,
            "user_type": "registered",
            "accept_rate": 79,
            "profile_image": "https://www.gravatar.com/avatar/d9d14b9e3e69ff6fcc3718f491516f22?s=128&d=identicon&r=PG&f=1",
            "display_name": "alacy",
            "link": "https://stackoverflow.com/users/3922295/alacy"
        },
        "is_answered": true,
        "view_count": 8307,
        "accepted_answer_id": 27514161,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1560780326,
        "creation_date": 1418763930,
        "last_edit_date": 1418764713,
        "question_id": 27513890,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/27513890/how-do-you-stack-two-pandas-dataframe-columns-on-top-of-each-other",
        "title": "How do you stack two Pandas Dataframe columns on top of each other?",
        "body": "<p>Is there a library function or correct way of stacking two Pandas data frame columns on top of each other?</p>\n\n<p>For example make 4 columns into 2:</p>\n\n<pre><code>a1  b1  a2  b2\n 1   2   3   4\n 5   6   7   8\n</code></pre>\n\n<p>to </p>\n\n<pre><code>c   d\n1   2\n5   6\n3   4\n7   8\n</code></pre>\n\n<p>The documentation for Pandas Data Frames that I read for the most part only deal with concatenating rows and doing row manipulation, but I'm sure there has to be a way to do what I described and I am sure it's very simple. </p>\n\n<p>Any help would be great. </p>\n",
        "answer_body": "<p>You can select the first two and second two columns using <code>pandas.DataFrame.iloc</code>. Then, change the column name of both parts to <code>c</code> and <code>d</code>. Afterwards, you can just join them using <code>pandas.concat</code>.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.arange(1, 9).reshape((2, 4)),\n        columns=[\"a1\", \"b1\", \"a2\", \"b2\"])\n\npart1 = df.iloc[:,0:2]\npart2 = df.iloc[:,2:4]\n\nnew_columns = [\"c\", \"d\"]\npart1.columns = new_columns\npart2.columns = new_columns\n\nprint pd.concat([part1, part2], ignore_index=True)\n</code></pre>\n\n<p>This gives you:</p>\n\n<pre><code>   c  d\n0  1  2\n1  5  6\n2  3  4\n3  7  8\n</code></pre>\n",
        "question_body": "<p>Is there a library function or correct way of stacking two Pandas data frame columns on top of each other?</p>\n\n<p>For example make 4 columns into 2:</p>\n\n<pre><code>a1  b1  a2  b2\n 1   2   3   4\n 5   6   7   8\n</code></pre>\n\n<p>to </p>\n\n<pre><code>c   d\n1   2\n5   6\n3   4\n7   8\n</code></pre>\n\n<p>The documentation for Pandas Data Frames that I read for the most part only deal with concatenating rows and doing row manipulation, but I'm sure there has to be a way to do what I described and I am sure it's very simple. </p>\n\n<p>Any help would be great. </p>\n",
        "formatted_input": {
            "qid": 27513890,
            "link": "https://stackoverflow.com/questions/27513890/how-do-you-stack-two-pandas-dataframe-columns-on-top-of-each-other",
            "question": {
                "title": "How do you stack two Pandas Dataframe columns on top of each other?",
                "ques_desc": "Is there a library function or correct way of stacking two Pandas data frame columns on top of each other? For example make 4 columns into 2: to The documentation for Pandas Data Frames that I read for the most part only deal with concatenating rows and doing row manipulation, but I'm sure there has to be a way to do what I described and I am sure it's very simple. Any help would be great. "
            },
            "io": [
                "a1  b1  a2  b2\n 1   2   3   4\n 5   6   7   8\n",
                "c   d\n1   2\n5   6\n3   4\n7   8\n"
            ],
            "answer": {
                "ans_desc": "You can select the first two and second two columns using . Then, change the column name of both parts to and . Afterwards, you can just join them using . This gives you: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.arange(1, 9).reshape((2, 4)),\n        columns=[\"a1\", \"b1\", \"a2\", \"b2\"])\n\npart1 = df.iloc[:,0:2]\npart2 = df.iloc[:,2:4]\n\nnew_columns = [\"c\", \"d\"]\npart1.columns = new_columns\npart2.columns = new_columns\n\nprint pd.concat([part1, part2], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 161,
        "accepted_answer_id": 56593622,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1560499011,
        "creation_date": 1560493613,
        "last_edit_date": 1560495434,
        "question_id": 56592574,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56592574/how-to-create-a-json-from-pandas-data-frame-where-columns-are-the-key",
        "title": "How to create a json from pandas data frame where columns are the key",
        "body": "<p>I have a data frame df</p>\n\n<pre><code>df:\ncol1    col2  col3\n 1        2     3\n 4        5     6\n 7        8     9\n</code></pre>\n\n<p>The json I am looking for is:</p>\n\n<pre><code> {\n            \"col1\": 1,\n            \"col1\": 4,\n            \"col1\": 7,\n        },\n        {\n            \"col2\": 2,\n            \"col2\": 5,\n            \"col2\": 8\n        },\n        {\n            \"col3\": 3,\n            \"col3\": 6,\n            \"col3\": 9,\n        }\n</code></pre>\n\n<p>I have tries df.to_json but its not working </p>\n\n<pre><code>df.to_json(orients=records)\nit gives this output\n'[{\"col1\":1,\"col2\":2,\"col3\":3},{\"col1\":4,\"col2\":5,\"col3\":6}, \n {\"col1\":7,\"col2\":8,\"col3\":9}]\n</code></pre>\n\n<p>This is not the output i was looking for</p>\n\n<p>How to do it in most effective way using pandas/python ?</p>\n",
        "answer_body": "<p>JSON files are treated as dicts in python, the JSON file you specified has duplicate keys and could only be parsed as a string (and not using the python json library).\nThe following code:</p>\n\n<pre><code>import json\nfrom io import StringIO\n\ndf = pd.DataFrame(np.arange(1,10).reshape((3,3)), columns=['col1','col2','col3'])\nio = StringIO()\ndf.to_json(io, orient='columns')\nparsed = json.loads(io.getvalue())\nwith open(\"pretty.json\", '+w') as of:\n    json.dump(parsed, of, indent=4)\n</code></pre>\n\n<p>will produce the following JSON:</p>\n\n<pre><code>{\n    \"col1\": {\n        \"0\": 1,\n        \"1\": 4,\n        \"2\": 7\n    },\n    \"col2\": {\n        \"0\": 2,\n        \"1\": 5,\n        \"2\": 8\n    },\n    \"col3\": {\n        \"0\": 3,\n        \"1\": 6,\n        \"2\": 9\n    }\n}\n</code></pre>\n\n<p>which you could later load to python. alternatively, this script will produce exatcly the string you want:</p>\n\n<pre><code>with open(\"exact.json\", \"w+\") as of:\n    of.write('[\\n\\t{\\n' + '\\t},\\n\\t{\\n'.join([\"\".join([\"\\t\\t\\\"%s\\\": %s,\\n\"%(c, df[c][i]) for i in df.index]) for c in df.columns])+'\\t}\\n]')\n</code></pre>\n\n<p>and the output would be:</p>\n\n<pre><code>[\n    {\n        \"col1\": 1,\n        \"col1\": 4,\n        \"col1\": 7,\n    },\n    {\n        \"col2\": 2,\n        \"col2\": 5,\n        \"col2\": 8,\n    },\n    {\n        \"col3\": 3,\n        \"col3\": 6,\n        \"col3\": 9,\n    }\n]\n</code></pre>\n\n<p>edit: fixed brackets</p>\n",
        "question_body": "<p>I have a data frame df</p>\n\n<pre><code>df:\ncol1    col2  col3\n 1        2     3\n 4        5     6\n 7        8     9\n</code></pre>\n\n<p>The json I am looking for is:</p>\n\n<pre><code> {\n            \"col1\": 1,\n            \"col1\": 4,\n            \"col1\": 7,\n        },\n        {\n            \"col2\": 2,\n            \"col2\": 5,\n            \"col2\": 8\n        },\n        {\n            \"col3\": 3,\n            \"col3\": 6,\n            \"col3\": 9,\n        }\n</code></pre>\n\n<p>I have tries df.to_json but its not working </p>\n\n<pre><code>df.to_json(orients=records)\nit gives this output\n'[{\"col1\":1,\"col2\":2,\"col3\":3},{\"col1\":4,\"col2\":5,\"col3\":6}, \n {\"col1\":7,\"col2\":8,\"col3\":9}]\n</code></pre>\n\n<p>This is not the output i was looking for</p>\n\n<p>How to do it in most effective way using pandas/python ?</p>\n",
        "formatted_input": {
            "qid": 56592574,
            "link": "https://stackoverflow.com/questions/56592574/how-to-create-a-json-from-pandas-data-frame-where-columns-are-the-key",
            "question": {
                "title": "How to create a json from pandas data frame where columns are the key",
                "ques_desc": "I have a data frame df The json I am looking for is: I have tries df.to_json but its not working This is not the output i was looking for How to do it in most effective way using pandas/python ? "
            },
            "io": [
                "df:\ncol1    col2  col3\n 1        2     3\n 4        5     6\n 7        8     9\n",
                " {\n            \"col1\": 1,\n            \"col1\": 4,\n            \"col1\": 7,\n        },\n        {\n            \"col2\": 2,\n            \"col2\": 5,\n            \"col2\": 8\n        },\n        {\n            \"col3\": 3,\n            \"col3\": 6,\n            \"col3\": 9,\n        }\n"
            ],
            "answer": {
                "ans_desc": "JSON files are treated as dicts in python, the JSON file you specified has duplicate keys and could only be parsed as a string (and not using the python json library). The following code: will produce the following JSON: which you could later load to python. alternatively, this script will produce exatcly the string you want: and the output would be: edit: fixed brackets ",
                "code": [
                    "import json\nfrom io import StringIO\n\ndf = pd.DataFrame(np.arange(1,10).reshape((3,3)), columns=['col1','col2','col3'])\nio = StringIO()\ndf.to_json(io, orient='columns')\nparsed = json.loads(io.getvalue())\nwith open(\"pretty.json\", '+w') as of:\n    json.dump(parsed, of, indent=4)\n",
                    "with open(\"exact.json\", \"w+\") as of:\n    of.write('[\\n\\t{\\n' + '\\t},\\n\\t{\\n'.join([\"\".join([\"\\t\\t\\\"%s\\\": %s,\\n\"%(c, df[c][i]) for i in df.index]) for c in df.columns])+'\\t}\\n]')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 851,
            "user_id": 3365886,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/5a7066bc8f68179d5f68386e2f6f3e8c?s=128&d=identicon&r=PG&f=1",
            "display_name": "Garet Jax",
            "link": "https://stackoverflow.com/users/3365886/garet-jax"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 56572563,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1560392897,
        "creation_date": 1560392375,
        "question_id": 56572543,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56572543/combine-pandas-dataframes-eliminating-common-columns-with-python",
        "title": "Combine pandas dataframes eliminating common columns with python",
        "body": "<p>I have 3 dataframes:</p>\n\n<pre><code>df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\\\n                    'B': ['B0', 'B1', 'B2', 'B3'],\\\n                    'C': ['C0', 'C1', 'C2', 'C3'],\\\n                    'D': ['D0', 'D1', 'D2', 'D3']},\\\n                    index=[0,1,2,3])\n\ndf2 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\\\n                    'E': ['E0', 'E1', 'E2', 'E3']},\\\n                    index=[0,1,2,3])\n\ndf3 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\\\n                    'F': ['F0', 'F1', 'F2', 'F3']},\\\n                    index=[0,1,2,3])\n</code></pre>\n\n<p>I want to combine them together to get the following results:</p>\n\n<pre><code>    A   B   C   D   E   F\n0  A0  B0  C0  D0  E0  F0\n1  A1  B1  C1  D1  E1  F1\n2  A2  B2  C2  D2  E2  F2\n3  A3  B3  C3  D3  E3  F3\n</code></pre>\n\n<p>When I try to combine them, I keep getting:</p>\n\n<pre><code>    A   B   C   D   A   E   A   F\n0  A0  B0  C0  D0  A0  E0  A0  F0\n1  A1  B1  C1  D1  A1  E1  A1  F1\n2  A2  B2  C2  D2  A2  E2  A2  F2\n3  A3  B3  C3  D3  A3  E3  A3  F3\n</code></pre>\n\n<p>The common column (A) is duplicated once for each dataframe used in the concat call.  I have tried various combinations on:</p>\n\n<pre><code>df4 = pd.concat([df1, df2, df3], axis=1, sort=False)\n</code></pre>\n\n<p>Some variations have been disastrous while some keep giving the undesired result.  Any suggestions would be much appreciated.  Thanks.</p>\n",
        "answer_body": "<p>Try</p>\n\n<pre><code>df4 = (pd.concat((df.set_index('A') for df in (df1,df2,df3)), axis=1)\n         .reset_index()\n      )\n</code></pre>\n\n<p>Output: </p>\n\n<pre><code>    A   B   C   D   E   F\n0  A0  B0  C0  D0  E0  F0\n1  A1  B1  C1  D1  E1  F1\n2  A2  B2  C2  D2  E2  F2\n3  A3  B3  C3  D3  E3  F3\n</code></pre>\n",
        "question_body": "<p>I have 3 dataframes:</p>\n\n<pre><code>df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\\\n                    'B': ['B0', 'B1', 'B2', 'B3'],\\\n                    'C': ['C0', 'C1', 'C2', 'C3'],\\\n                    'D': ['D0', 'D1', 'D2', 'D3']},\\\n                    index=[0,1,2,3])\n\ndf2 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\\\n                    'E': ['E0', 'E1', 'E2', 'E3']},\\\n                    index=[0,1,2,3])\n\ndf3 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\\\n                    'F': ['F0', 'F1', 'F2', 'F3']},\\\n                    index=[0,1,2,3])\n</code></pre>\n\n<p>I want to combine them together to get the following results:</p>\n\n<pre><code>    A   B   C   D   E   F\n0  A0  B0  C0  D0  E0  F0\n1  A1  B1  C1  D1  E1  F1\n2  A2  B2  C2  D2  E2  F2\n3  A3  B3  C3  D3  E3  F3\n</code></pre>\n\n<p>When I try to combine them, I keep getting:</p>\n\n<pre><code>    A   B   C   D   A   E   A   F\n0  A0  B0  C0  D0  A0  E0  A0  F0\n1  A1  B1  C1  D1  A1  E1  A1  F1\n2  A2  B2  C2  D2  A2  E2  A2  F2\n3  A3  B3  C3  D3  A3  E3  A3  F3\n</code></pre>\n\n<p>The common column (A) is duplicated once for each dataframe used in the concat call.  I have tried various combinations on:</p>\n\n<pre><code>df4 = pd.concat([df1, df2, df3], axis=1, sort=False)\n</code></pre>\n\n<p>Some variations have been disastrous while some keep giving the undesired result.  Any suggestions would be much appreciated.  Thanks.</p>\n",
        "formatted_input": {
            "qid": 56572543,
            "link": "https://stackoverflow.com/questions/56572543/combine-pandas-dataframes-eliminating-common-columns-with-python",
            "question": {
                "title": "Combine pandas dataframes eliminating common columns with python",
                "ques_desc": "I have 3 dataframes: I want to combine them together to get the following results: When I try to combine them, I keep getting: The common column (A) is duplicated once for each dataframe used in the concat call. I have tried various combinations on: Some variations have been disastrous while some keep giving the undesired result. Any suggestions would be much appreciated. Thanks. "
            },
            "io": [
                "    A   B   C   D   E   F\n0  A0  B0  C0  D0  E0  F0\n1  A1  B1  C1  D1  E1  F1\n2  A2  B2  C2  D2  E2  F2\n3  A3  B3  C3  D3  E3  F3\n",
                "    A   B   C   D   A   E   A   F\n0  A0  B0  C0  D0  A0  E0  A0  F0\n1  A1  B1  C1  D1  A1  E1  A1  F1\n2  A2  B2  C2  D2  A2  E2  A2  F2\n3  A3  B3  C3  D3  A3  E3  A3  F3\n"
            ],
            "answer": {
                "ans_desc": "Try Output: ",
                "code": [
                    "df4 = (pd.concat((df.set_index('A') for df in (df1,df2,df3)), axis=1)\n         .reset_index()\n      )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 71,
            "user_id": 9608063,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8dd7082fefe3ed90bc2a75734bb91cc6?s=128&d=identicon&r=PG&f=1",
            "display_name": "Funky",
            "link": "https://stackoverflow.com/users/9608063/funky"
        },
        "is_answered": true,
        "view_count": 223,
        "accepted_answer_id": 56488128,
        "answer_count": 3,
        "score": -1,
        "last_activity_date": 1559884459,
        "creation_date": 1559882621,
        "question_id": 56488071,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56488071/how-to-compress-dataframe-by-removing-columns-that-contains-nan-value-in-betwe",
        "title": "How to compress dataframe by removing columns that contains &#39;NaN&#39; value in between columns that has a value?",
        "body": "<p>I am currently following the answer <a href=\"https://stackoverflow.com/questions/56472792/how-to-split-lists-over-a-range-of-column/56473239#56473239\">here</a>. It mostly worked but when I viewed the whole dataframe, I saw that there are columns that contains 'NaN' values in between columns that do contain a value. </p>\n\n<p>For example I keep getting a result of something like this:</p>\n\n<pre><code>     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9\n300 1001|1001|1002|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN   \n301 1010|1010|NaN |  NaN | 1000 | 2000|1234| NaN|  NaN | 1213 | 1415\n302 1100|1234|5678| 9101 | 1121 | 3141|2345|6789| 1011 | 1617 | 1819\n303 1000|2001|9876|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN  \n</code></pre>\n\n<p>Is there a way to remove those cells that contains NaN such that the output would be like this:</p>\n\n<pre><code>     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9\n300 1001|1001|1002|  1001| 1002 | NaN |NaN | NaN|  NaN | NaN  | NaN   \n301 1010|1010|1000|  2000| 1234 | 1213|1415| NaN|  NaN | NaN  | NaN\n302 1100|1234|5678|  9101| 1121 | 3141|2345|6789| 1011 | 1617 | 1819\n303 1000|2001|9876|  1001| 1002 | NaN |NaN |NaN |  NaN | NaN  | NaN \n</code></pre>\n",
        "answer_body": "<p>Using <code>pd.DataFrame.iterrows</code> with <code>pd.concat</code>:</p>\n\n<pre><code>import pandas as pd\n\ndf[df.columns] = pd.concat([s.dropna().reset_index(drop=True) for i,s in df.iterrows()], 1).T\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>         ID     0     1     2     3     4     5     6     7     8     9\n0  300 1001  1001  1002  1001  1002   NaN   NaN   NaN   NaN   NaN   NaN\n1  301 1010  1010  1000  2000  1234  1213  1415   NaN   NaN   NaN   NaN\n2  302 1100  1234  5678  9101  1121  3141  2345  6789  1011  1617  1819\n3  303 1000  2001  9876  1001  1002   NaN   NaN   NaN   NaN   NaN   NaN\n</code></pre>\n",
        "question_body": "<p>I am currently following the answer <a href=\"https://stackoverflow.com/questions/56472792/how-to-split-lists-over-a-range-of-column/56473239#56473239\">here</a>. It mostly worked but when I viewed the whole dataframe, I saw that there are columns that contains 'NaN' values in between columns that do contain a value. </p>\n\n<p>For example I keep getting a result of something like this:</p>\n\n<pre><code>     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9\n300 1001|1001|1002|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN   \n301 1010|1010|NaN |  NaN | 1000 | 2000|1234| NaN|  NaN | 1213 | 1415\n302 1100|1234|5678| 9101 | 1121 | 3141|2345|6789| 1011 | 1617 | 1819\n303 1000|2001|9876|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN  \n</code></pre>\n\n<p>Is there a way to remove those cells that contains NaN such that the output would be like this:</p>\n\n<pre><code>     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9\n300 1001|1001|1002|  1001| 1002 | NaN |NaN | NaN|  NaN | NaN  | NaN   \n301 1010|1010|1000|  2000| 1234 | 1213|1415| NaN|  NaN | NaN  | NaN\n302 1100|1234|5678|  9101| 1121 | 3141|2345|6789| 1011 | 1617 | 1819\n303 1000|2001|9876|  1001| 1002 | NaN |NaN |NaN |  NaN | NaN  | NaN \n</code></pre>\n",
        "formatted_input": {
            "qid": 56488071,
            "link": "https://stackoverflow.com/questions/56488071/how-to-compress-dataframe-by-removing-columns-that-contains-nan-value-in-betwe",
            "question": {
                "title": "How to compress dataframe by removing columns that contains &#39;NaN&#39; value in between columns that has a value?",
                "ques_desc": "I am currently following the answer here. It mostly worked but when I viewed the whole dataframe, I saw that there are columns that contains 'NaN' values in between columns that do contain a value. For example I keep getting a result of something like this: Is there a way to remove those cells that contains NaN such that the output would be like this: "
            },
            "io": [
                "     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9\n300 1001|1001|1002|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN   \n301 1010|1010|NaN |  NaN | 1000 | 2000|1234| NaN|  NaN | 1213 | 1415\n302 1100|1234|5678| 9101 | 1121 | 3141|2345|6789| 1011 | 1617 | 1819\n303 1000|2001|9876|  NaN | NaN  | NaN |1001|1002|  NaN | NaN  | NaN  \n",
                "     ID | 0  | 1  |   2  |  3   | 4   | 5  | 6  |  7   |  8   | 9\n300 1001|1001|1002|  1001| 1002 | NaN |NaN | NaN|  NaN | NaN  | NaN   \n301 1010|1010|1000|  2000| 1234 | 1213|1415| NaN|  NaN | NaN  | NaN\n302 1100|1234|5678|  9101| 1121 | 3141|2345|6789| 1011 | 1617 | 1819\n303 1000|2001|9876|  1001| 1002 | NaN |NaN |NaN |  NaN | NaN  | NaN \n"
            ],
            "answer": {
                "ans_desc": "Using with : Output: ",
                "code": [
                    "import pandas as pd\n\ndf[df.columns] = pd.concat([s.dropna().reset_index(drop=True) for i,s in df.iterrows()], 1).T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2141,
            "user_id": 5040775,
            "user_type": "registered",
            "accept_rate": 68,
            "profile_image": "https://graph.facebook.com/10207130495004756/picture?type=large",
            "display_name": "Jun Jang",
            "link": "https://stackoverflow.com/users/5040775/jun-jang"
        },
        "is_answered": true,
        "view_count": 1333,
        "accepted_answer_id": 45239497,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1559279241,
        "creation_date": 1500644819,
        "last_edit_date": 1559279241,
        "question_id": 45239357,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/45239357/leading-zero-issues-with-pandas-read-csv-function",
        "title": "Leading zero issues with pandas read_csv function",
        "body": "<p>I have a column of values such as this:</p>\n\n<pre><code>123, 234, 345, 456, 567\n</code></pre>\n\n<p>When I do</p>\n\n<pre><code>pd.read_csv(dtype = {'column': str})\n</code></pre>\n\n<p>or</p>\n\n<pre><code>pd.read_csv(dtype = 'column': object})\n</code></pre>\n\n<p>they both produce values like</p>\n\n<pre><code>00123, 00234, 00345, 00456, 00567.\n</code></pre>\n\n<p>I was searching through stackexchange, and people say that you should use <code>dtype: object</code>, but it doesn't work for me..</p>\n",
        "answer_body": "<p>If you want to read in your data as integers, drop the <code>dtype</code>:</p>\n\n<pre><code>df = pd.read_csv('data.csv')\n</code></pre>\n\n<p>If you want to convert the fields to strings, you can apply a <code>str</code> transformation with <code>df.astype</code>:</p>\n\n<pre><code>df = pd.read_csv('data.csv').astype(str)\n</code></pre>\n\n<p>Another option would be to use a converter:</p>\n\n<pre><code>df = pd.read_csv('data.csv', converters={'ColName': str})\n</code></pre>\n",
        "question_body": "<p>I have a column of values such as this:</p>\n\n<pre><code>123, 234, 345, 456, 567\n</code></pre>\n\n<p>When I do</p>\n\n<pre><code>pd.read_csv(dtype = {'column': str})\n</code></pre>\n\n<p>or</p>\n\n<pre><code>pd.read_csv(dtype = 'column': object})\n</code></pre>\n\n<p>they both produce values like</p>\n\n<pre><code>00123, 00234, 00345, 00456, 00567.\n</code></pre>\n\n<p>I was searching through stackexchange, and people say that you should use <code>dtype: object</code>, but it doesn't work for me..</p>\n",
        "formatted_input": {
            "qid": 45239357,
            "link": "https://stackoverflow.com/questions/45239357/leading-zero-issues-with-pandas-read-csv-function",
            "question": {
                "title": "Leading zero issues with pandas read_csv function",
                "ques_desc": "I have a column of values such as this: When I do or they both produce values like I was searching through stackexchange, and people say that you should use , but it doesn't work for me.. "
            },
            "io": [
                "123, 234, 345, 456, 567\n",
                "00123, 00234, 00345, 00456, 00567.\n"
            ],
            "answer": {
                "ans_desc": "If you want to read in your data as integers, drop the : If you want to convert the fields to strings, you can apply a transformation with : Another option would be to use a converter: ",
                "code": [
                    "df = pd.read_csv('data.csv', converters={'ColName': str})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 121,
            "user_id": 11561386,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b79cc2b86d7c7733b66e6283267d12be?s=128&d=identicon&r=PG&f=1",
            "display_name": "zaydar",
            "link": "https://stackoverflow.com/users/11561386/zaydar"
        },
        "is_answered": true,
        "view_count": 365,
        "closed_date": 1558972500,
        "accepted_answer_id": 56328605,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1558972863,
        "creation_date": 1558969021,
        "last_edit_date": 1558969078,
        "question_id": 56328404,
        "link": "https://stackoverflow.com/questions/56328404/how-to-apply-a-method-to-a-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "How to apply a method to a Pandas Dataframe",
        "body": "<p>I have this dataframe</p>\n\n<pre><code>   Col1              Col2\n\n0  A (1000 EUR)  C ( 3000 USD)\n\n1  B (2000 CHF)  D ( 4000 GBP)\n</code></pre>\n\n<p>I would like to convert it to </p>\n\n<pre><code>   Col1  Col2\n\n0  1000  3000\n\n1  2000  4000\n</code></pre>\n\n<p>I know how to create a dataframe (with indexes) for 1 column, but not for multiple columns</p>\n\n<p>This code produces this result</p>\n\n<pre><code>   Col1\n\n0  1000\n\n1  2000 \n\na = z['Col1'].str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce')\n</code></pre>\n\n<p>how can I amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally I wouln't want to have to enter the same code for every column)</p>\n",
        "answer_body": "<p>You can use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\" rel=\"nofollow noreferrer\">apply</a> function to apply your operation to all elements in both rows.</p>\n\n<pre><code># creates your dataframe\ndf = pd.DataFrame({'Col1':['A (1000 EUR)','B (2000 CHF)'], 'Col2':['C (3000 USD)', 'D (4000 GBP)']})\n\n# use the apply function to  apply your code to all elements of both columns\ndf = df.apply(lambda x: x.str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce'))\n</code></pre>\n\n<p>Does the trick for me</p>\n",
        "question_body": "<p>I have this dataframe</p>\n\n<pre><code>   Col1              Col2\n\n0  A (1000 EUR)  C ( 3000 USD)\n\n1  B (2000 CHF)  D ( 4000 GBP)\n</code></pre>\n\n<p>I would like to convert it to </p>\n\n<pre><code>   Col1  Col2\n\n0  1000  3000\n\n1  2000  4000\n</code></pre>\n\n<p>I know how to create a dataframe (with indexes) for 1 column, but not for multiple columns</p>\n\n<p>This code produces this result</p>\n\n<pre><code>   Col1\n\n0  1000\n\n1  2000 \n\na = z['Col1'].str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce')\n</code></pre>\n\n<p>how can I amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally I wouln't want to have to enter the same code for every column)</p>\n",
        "formatted_input": {
            "qid": 56328404,
            "link": "https://stackoverflow.com/questions/56328404/how-to-apply-a-method-to-a-pandas-dataframe",
            "question": {
                "title": "How to apply a method to a Pandas Dataframe",
                "ques_desc": "I have this dataframe I would like to convert it to I know how to create a dataframe (with indexes) for 1 column, but not for multiple columns This code produces this result how can I amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally I wouln't want to have to enter the same code for every column) "
            },
            "io": [
                "   Col1              Col2\n\n0  A (1000 EUR)  C ( 3000 USD)\n\n1  B (2000 CHF)  D ( 4000 GBP)\n",
                "   Col1  Col2\n\n0  1000  3000\n\n1  2000  4000\n"
            ],
            "answer": {
                "ans_desc": "You can use the apply function to apply your operation to all elements in both rows. Does the trick for me ",
                "code": [
                    "# creates your dataframe\ndf = pd.DataFrame({'Col1':['A (1000 EUR)','B (2000 CHF)'], 'Col2':['C (3000 USD)', 'D (4000 GBP)']})\n\n# use the apply function to  apply your code to all elements of both columns\ndf = df.apply(lambda x: x.str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 11549214,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f888be4436d14d66bdde0aff67d726a3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Portis",
            "link": "https://stackoverflow.com/users/11549214/portis"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 56290059,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1558697797,
        "creation_date": 1558690349,
        "last_edit_date": 1558697797,
        "question_id": 56289734,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56289734/iterate-over-columns-in-python-dataframe-to-do-calculations-and-insert-new-colum",
        "title": "Iterate over columns in python dataframe to do calculations and insert new columns between existing columns",
        "body": "<p>I'm new to python and programming in general and can't seem to find a solution to my problem. I have a dataframe imported from an excel sheet with 15 rows of species and their number and 3 columns which are locations where they are found. That is a species by station matrix: </p>\n\n<pre><code>              A1    A2    A3\nSpecies 1   1259   600   151\nSpecies 2    912  1820   899\nSpecies 3   1288  1491   631\nSpecies 4     36   609  1946\nSpecies 5   1639   819  1864\nSpecies 6   1989   748   843\nSpecies 7    688   271  1206\nSpecies 8   1031   341   756\nSpecies 9   1517  1164   138\nSpecies 10  1290   669   811\nSpecies 11    16   409  1686\nSpecies 12   329   521   954\nSpecies 13  1782   958  1727\nSpecies 14   464  1804  1105\nSpecies 15  1002  1483   109\n</code></pre>\n\n<p>I want to calculate for each column the top-10 species (index), their value, percentage of total in column, cumulative percentage and insert the new columns after each exististing column and return in one dataframe.</p>\n\n<p>This is the result I'm looking for (example with two first columns):</p>\n\n<pre><code>     Species    A1  pct  cum_pct     Species    A2  pct  cum_pct   \n0   Species 6  1989   13       13   Species 2  1820   13       13  \n1  Species 13  1782   11       24  Species 14  1804   13       26   \n2   Species 5  1639   10       35   Species 3  1491   10       37   \n3   Species 9  1517    9       45  Species 15  1483   10       48  \n4  Species 10  1290    8       53   Species 9  1164    8       56   \n5   Species 3  1288    8       62  Species 13   958    6       63    \n6   Species 1  1259    8       70   Species 5   819    5       69  \n7   Species 8  1031    6       77   Species 6   748    5       75    \n8  Species 15  1002    6       83  Species 10   669    4       79   \n9   Species 2   912    5       89   Species 4   609    4       84    \n</code></pre>\n\n<p>I have managed to do this by calculating each column and make new data frames and using concat to merge the data frames together in the end using the following code:</p>\n\n<pre><code>df = pd.read_excel(r\"\") #local excel file\n\n#extract first column and remove others\ndf = df.drop(df.columns[1:], axis=1) \n\n# create column which has percentage for each element: divide value by total sum\ndf[\"pct\"] = 100*(df.iloc[:, 0] /df.iloc[:, 0].sum())\n\n#sort by value in Column 1 (0) return only top n (10) values\ndf = df.sort_values(by=df.columns[0], ascending=False).head(10)\n\n# Create column with cumulative sum\ndf[\"cum_pct\"] = df.pct.cumsum()\n\n#make index as column and change name to Species\ndf = df.reset_index()\n\ndf = df.rename(index=str, columns={\"index\": \"Species\"})\n\n\n# For column 2\ndf1 = pd.read_excel(r\"\") #local excel file\n\ndf1 = df1.drop(df1.columns[2:], axis=1) \ndf1 = df1.drop(df1.columns[0], axis=1) \n\n# create column which has percentage for each element: divide value by total sum\ndf1[\"pct\"] = 100*(df1.iloc[:, 0] /df1.iloc[:, 0].sum())\n\n\n#sort by value in Column 1 (0) return only top n (10) values\ndf1 = df1.sort_values(by=df1.columns[0], ascending=False).head(10)\n\n# Create column with cumulative sum\ndf1[\"cum_pct\"] = df1.pct.cumsum()\n\n# set index as first column\ndf1 = df1.reset_index()\n\ndf1 = df1.rename(index=str, columns={\"index\": \"Species\"})\n\n\n# concatenate all dataframes\nresult = pd.concat([df, df1,], axis=1, join_axes=[df.index])\n\n#convert numbers to int, exception = ignore\nresult = result.astype(int, errors=\"ignore\")\n\nprint(result)\n</code></pre>\n\n<p>This code works, but my datasets are much larger with often more then 50 columns so I'm wondering if it possible to an iteration for each column that results in the same dataframe as shown above. Sorry for the long read. </p>\n",
        "answer_body": "<p>Using a <code>for</code> loop, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.nlargest.html\" rel=\"nofollow noreferrer\"><code>Series.nlargest</code></a>, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\" rel=\"nofollow noreferrer\"><code>DataFrame.assign</code></a> with <code>lambda</code> functions to calculate <code>pct</code> and <code>cum_pct</code> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>pandas.concat</code></a> to combine for final output frame:</p>\n\n<pre><code>frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n</code></pre>\n\n<p>[out]</p>\n\n<pre><code>      Species    A1       pct   cum_pct     Species    A2       pct   cum_pct  \\\n0   Species 6  1989  0.130495  0.130495   Species 2  1820  0.132779  0.132779   \n1  Species 13  1782  0.116914  0.247408  Species 14  1804  0.131612  0.264390   \n2   Species 5  1639  0.107532  0.354940   Species 3  1491  0.108777  0.373167   \n3   Species 9  1517  0.099528  0.454468  Species 15  1483  0.108193  0.481360   \n4  Species 10  1290  0.084635  0.539102   Species 9  1164  0.084920  0.566280   \n5   Species 3  1288  0.084503  0.623606  Species 13   958  0.069891  0.636171   \n6   Species 1  1259  0.082601  0.706207   Species 5   819  0.059750  0.695922   \n7   Species 8  1031  0.067642  0.773849   Species 6   748  0.054571  0.750492   \n8  Species 15  1002  0.065739  0.839588  Species 10   669  0.048807  0.799300   \n9   Species 2   912  0.059835  0.899423   Species 4   609  0.044430  0.843729   \n\n      Species    A3       pct   cum_pct  \n0   Species 4  1946  0.131256  0.131256  \n1   Species 5  1864  0.125725  0.256981  \n2  Species 13  1727  0.116485  0.373466  \n3  Species 11  1686  0.113719  0.487185  \n4   Species 7  1206  0.081344  0.568528  \n5  Species 14  1105  0.074531  0.643059  \n6  Species 12   954  0.064346  0.707406  \n7   Species 2   899  0.060637  0.768043  \n8   Species 6   843  0.056860  0.824902  \n9  Species 10   811  0.054701  0.879603\n</code></pre>\n\n<hr>\n\n<p>If it's necessary to format the calculated fields <code>pct</code> and <code>cum_pct</code> as <code>int</code>, then instead use:</p>\n\n<pre><code>frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .assign(pct=lambda x: x['pct'].mul(100).astype(int),\n                          cum_pct=lambda x: x['cum_pct'].mul(100).astype(int))\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n</code></pre>\n\n<p>[out]</p>\n\n<pre><code>     Species    A1  pct  cum_pct     Species    A2  pct  cum_pct     Species  \\\n0   Species 6  1989   13       13   Species 2  1820   13       13   Species 4   \n1  Species 13  1782   11       24  Species 14  1804   13       26   Species 5   \n2   Species 5  1639   10       35   Species 3  1491   10       37  Species 13   \n3   Species 9  1517    9       45  Species 15  1483   10       48  Species 11   \n4  Species 10  1290    8       53   Species 9  1164    8       56   Species 7   \n5   Species 3  1288    8       62  Species 13   958    6       63  Species 14   \n6   Species 1  1259    8       70   Species 5   819    5       69  Species 12   \n7   Species 8  1031    6       77   Species 6   748    5       75   Species 2   \n8  Species 15  1002    6       83  Species 10   669    4       79   Species 6   \n9   Species 2   912    5       89   Species 4   609    4       84  Species 10   \n\n     A3  pct  cum_pct  \n0  1946   13       13  \n1  1864   12       25  \n2  1727   11       37  \n3  1686   11       48  \n4  1206    8       56  \n5  1105    7       64  \n6   954    6       70  \n7   899    6       76  \n8   843    5       82  \n9   811    5       87\n</code></pre>\n",
        "question_body": "<p>I'm new to python and programming in general and can't seem to find a solution to my problem. I have a dataframe imported from an excel sheet with 15 rows of species and their number and 3 columns which are locations where they are found. That is a species by station matrix: </p>\n\n<pre><code>              A1    A2    A3\nSpecies 1   1259   600   151\nSpecies 2    912  1820   899\nSpecies 3   1288  1491   631\nSpecies 4     36   609  1946\nSpecies 5   1639   819  1864\nSpecies 6   1989   748   843\nSpecies 7    688   271  1206\nSpecies 8   1031   341   756\nSpecies 9   1517  1164   138\nSpecies 10  1290   669   811\nSpecies 11    16   409  1686\nSpecies 12   329   521   954\nSpecies 13  1782   958  1727\nSpecies 14   464  1804  1105\nSpecies 15  1002  1483   109\n</code></pre>\n\n<p>I want to calculate for each column the top-10 species (index), their value, percentage of total in column, cumulative percentage and insert the new columns after each exististing column and return in one dataframe.</p>\n\n<p>This is the result I'm looking for (example with two first columns):</p>\n\n<pre><code>     Species    A1  pct  cum_pct     Species    A2  pct  cum_pct   \n0   Species 6  1989   13       13   Species 2  1820   13       13  \n1  Species 13  1782   11       24  Species 14  1804   13       26   \n2   Species 5  1639   10       35   Species 3  1491   10       37   \n3   Species 9  1517    9       45  Species 15  1483   10       48  \n4  Species 10  1290    8       53   Species 9  1164    8       56   \n5   Species 3  1288    8       62  Species 13   958    6       63    \n6   Species 1  1259    8       70   Species 5   819    5       69  \n7   Species 8  1031    6       77   Species 6   748    5       75    \n8  Species 15  1002    6       83  Species 10   669    4       79   \n9   Species 2   912    5       89   Species 4   609    4       84    \n</code></pre>\n\n<p>I have managed to do this by calculating each column and make new data frames and using concat to merge the data frames together in the end using the following code:</p>\n\n<pre><code>df = pd.read_excel(r\"\") #local excel file\n\n#extract first column and remove others\ndf = df.drop(df.columns[1:], axis=1) \n\n# create column which has percentage for each element: divide value by total sum\ndf[\"pct\"] = 100*(df.iloc[:, 0] /df.iloc[:, 0].sum())\n\n#sort by value in Column 1 (0) return only top n (10) values\ndf = df.sort_values(by=df.columns[0], ascending=False).head(10)\n\n# Create column with cumulative sum\ndf[\"cum_pct\"] = df.pct.cumsum()\n\n#make index as column and change name to Species\ndf = df.reset_index()\n\ndf = df.rename(index=str, columns={\"index\": \"Species\"})\n\n\n# For column 2\ndf1 = pd.read_excel(r\"\") #local excel file\n\ndf1 = df1.drop(df1.columns[2:], axis=1) \ndf1 = df1.drop(df1.columns[0], axis=1) \n\n# create column which has percentage for each element: divide value by total sum\ndf1[\"pct\"] = 100*(df1.iloc[:, 0] /df1.iloc[:, 0].sum())\n\n\n#sort by value in Column 1 (0) return only top n (10) values\ndf1 = df1.sort_values(by=df1.columns[0], ascending=False).head(10)\n\n# Create column with cumulative sum\ndf1[\"cum_pct\"] = df1.pct.cumsum()\n\n# set index as first column\ndf1 = df1.reset_index()\n\ndf1 = df1.rename(index=str, columns={\"index\": \"Species\"})\n\n\n# concatenate all dataframes\nresult = pd.concat([df, df1,], axis=1, join_axes=[df.index])\n\n#convert numbers to int, exception = ignore\nresult = result.astype(int, errors=\"ignore\")\n\nprint(result)\n</code></pre>\n\n<p>This code works, but my datasets are much larger with often more then 50 columns so I'm wondering if it possible to an iteration for each column that results in the same dataframe as shown above. Sorry for the long read. </p>\n",
        "formatted_input": {
            "qid": 56289734,
            "link": "https://stackoverflow.com/questions/56289734/iterate-over-columns-in-python-dataframe-to-do-calculations-and-insert-new-colum",
            "question": {
                "title": "Iterate over columns in python dataframe to do calculations and insert new columns between existing columns",
                "ques_desc": "I'm new to python and programming in general and can't seem to find a solution to my problem. I have a dataframe imported from an excel sheet with 15 rows of species and their number and 3 columns which are locations where they are found. That is a species by station matrix: I want to calculate for each column the top-10 species (index), their value, percentage of total in column, cumulative percentage and insert the new columns after each exististing column and return in one dataframe. This is the result I'm looking for (example with two first columns): I have managed to do this by calculating each column and make new data frames and using concat to merge the data frames together in the end using the following code: This code works, but my datasets are much larger with often more then 50 columns so I'm wondering if it possible to an iteration for each column that results in the same dataframe as shown above. Sorry for the long read. "
            },
            "io": [
                "              A1    A2    A3\nSpecies 1   1259   600   151\nSpecies 2    912  1820   899\nSpecies 3   1288  1491   631\nSpecies 4     36   609  1946\nSpecies 5   1639   819  1864\nSpecies 6   1989   748   843\nSpecies 7    688   271  1206\nSpecies 8   1031   341   756\nSpecies 9   1517  1164   138\nSpecies 10  1290   669   811\nSpecies 11    16   409  1686\nSpecies 12   329   521   954\nSpecies 13  1782   958  1727\nSpecies 14   464  1804  1105\nSpecies 15  1002  1483   109\n",
                "     Species    A1  pct  cum_pct     Species    A2  pct  cum_pct   \n0   Species 6  1989   13       13   Species 2  1820   13       13  \n1  Species 13  1782   11       24  Species 14  1804   13       26   \n2   Species 5  1639   10       35   Species 3  1491   10       37   \n3   Species 9  1517    9       45  Species 15  1483   10       48  \n4  Species 10  1290    8       53   Species 9  1164    8       56   \n5   Species 3  1288    8       62  Species 13   958    6       63    \n6   Species 1  1259    8       70   Species 5   819    5       69  \n7   Species 8  1031    6       77   Species 6   748    5       75    \n8  Species 15  1002    6       83  Species 10   669    4       79   \n9   Species 2   912    5       89   Species 4   609    4       84    \n"
            ],
            "answer": {
                "ans_desc": "Using a loop, , with functions to calculate and and to combine for final output frame: [out] If it's necessary to format the calculated fields and as , then instead use: [out] ",
                "code": [
                    "frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n",
                    "frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .assign(pct=lambda x: x['pct'].mul(100).astype(int),\n                          cum_pct=lambda x: x['cum_pct'].mul(100).astype(int))\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1400,
            "user_id": 9758660,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "Jim Eisenberg",
            "link": "https://stackoverflow.com/users/9758660/jim-eisenberg"
        },
        "is_answered": true,
        "view_count": 1215,
        "accepted_answer_id": 56271776,
        "answer_count": 3,
        "score": 4,
        "last_activity_date": 1558603003,
        "creation_date": 1558601789,
        "question_id": 56271520,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56271520/pandas-create-multiple-dataframes-based-on-duplicate-index-dataframe",
        "title": "pandas create multiple dataframes based on duplicate index dataframe",
        "body": "<p>If I have a dataframe with duplicates in the index, how would I create a set of dataframes with no duplicates in the index?</p>\n\n<p>More precisely, given the dataframe:</p>\n\n<pre><code>   a  b\n1  1  6\n1  2  7\n2  3  8\n2  4  9\n2  5  0\n</code></pre>\n\n<p>I would want as output, a list of dataframes:</p>\n\n<pre><code>   a  b\n1  1  6\n2  3  8\n\n\n   a  b\n1  2  7\n2  4  9\n\n\n   a  b\n2  5  0\n</code></pre>\n\n<p>This needs to be scalable to as many dataframes as needed based on the number of duplicates.</p>\n",
        "answer_body": "<p>Another approach is to use <code>pd.DataFrame.groupby.nth</code>:</p>\n\n<pre><code>import numpy as np\n\ng = df.groupby(df.index)\ncnt = np.bincount(df.index).max()\ndfs = [g.nth(i) for i in range(cnt)]\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[  a  b\n1  1  6\n2  3  8,    \n   a  b\n1  2  7\n2  4  9,\n   a  b\n2  5  0]\n</code></pre>\n",
        "question_body": "<p>If I have a dataframe with duplicates in the index, how would I create a set of dataframes with no duplicates in the index?</p>\n\n<p>More precisely, given the dataframe:</p>\n\n<pre><code>   a  b\n1  1  6\n1  2  7\n2  3  8\n2  4  9\n2  5  0\n</code></pre>\n\n<p>I would want as output, a list of dataframes:</p>\n\n<pre><code>   a  b\n1  1  6\n2  3  8\n\n\n   a  b\n1  2  7\n2  4  9\n\n\n   a  b\n2  5  0\n</code></pre>\n\n<p>This needs to be scalable to as many dataframes as needed based on the number of duplicates.</p>\n",
        "formatted_input": {
            "qid": 56271520,
            "link": "https://stackoverflow.com/questions/56271520/pandas-create-multiple-dataframes-based-on-duplicate-index-dataframe",
            "question": {
                "title": "pandas create multiple dataframes based on duplicate index dataframe",
                "ques_desc": "If I have a dataframe with duplicates in the index, how would I create a set of dataframes with no duplicates in the index? More precisely, given the dataframe: I would want as output, a list of dataframes: This needs to be scalable to as many dataframes as needed based on the number of duplicates. "
            },
            "io": [
                "   a  b\n1  1  6\n1  2  7\n2  3  8\n2  4  9\n2  5  0\n",
                "   a  b\n1  1  6\n2  3  8\n\n\n   a  b\n1  2  7\n2  4  9\n\n\n   a  b\n2  5  0\n"
            ],
            "answer": {
                "ans_desc": "Another approach is to use : Output: ",
                "code": [
                    "import numpy as np\n\ng = df.groupby(df.index)\ncnt = np.bincount(df.index).max()\ndfs = [g.nth(i) for i in range(cnt)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 415,
            "user_id": 6589617,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/Rpqq9.jpg?s=128&g=1",
            "display_name": "geher",
            "link": "https://stackoverflow.com/users/6589617/geher"
        },
        "is_answered": true,
        "view_count": 1516,
        "accepted_answer_id": 50529810,
        "answer_count": 6,
        "score": 5,
        "last_activity_date": 1558511154,
        "creation_date": 1527251957,
        "last_edit_date": 1527255022,
        "question_id": 50529459,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50529459/append-rows-to-groups-in-pandas",
        "title": "Append rows to groups in pandas",
        "body": "<p>I'm trying to append a number of NaN rows to each group in a pandas dataframe. Essentially I want to pad each group to be 5 rows long. Ordering is important. I have:</p>\n\n<pre><code>    Rank id\n0   1  a\n1   2  a\n2   3  a\n3   4  a\n4   5  a\n5   1  c\n6   2  c\n7   1  e\n8   2  e\n9   3  e\n</code></pre>\n\n<p>I want:</p>\n\n<pre><code>    Rank id\n0   1    a\n1   2    a\n2   3    a\n3   4    a\n4   5    a\n5   1    c\n6   2    c\n7   NaN  c\n8   NaN  c\n9   NaN  c\n10  1    e\n11  2    e\n12  3    e\n13  NaN  e\n14  NaN  e\n</code></pre>\n",
        "answer_body": "<p>Using <code>pd.crosstab</code>:</p>\n\n<pre><code>df = pd.crosstab(df.Rank, df.ID).iloc[:5].unstack().reset_index()\ndf.loc[(df[0]==0),'Rank'] = np.nan\ndel df[0]\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   ID  Rank\n0   a   1.0\n1   a   2.0\n2   a   3.0\n3   a   4.0\n4   a   5.0\n5   c   1.0\n6   c   2.0\n7   c   NaN\n8   c   NaN\n9   c   NaN\n10  e   1.0\n11  e   2.0\n12  e   3.0\n13  e   NaN\n14  e   NaN\n</code></pre>\n\n<p>Another approach, assuming the maximum group size in <code>df</code> is exactly 5.</p>\n\n<pre><code>In [251]: df.groupby('ID').Rank.apply(np.array).apply(pd.Series).stack(dropna=False)\nOut[251]: \nID\na   0    1.0\n    1    2.0\n    2    3.0\n    3    4.0\n    4    5.0\nc   0    1.0\n    1    2.0\n    2    NaN\n    3    NaN\n    4    NaN\ne   0    1.0\n    1    2.0\n    2    3.0\n    3    NaN\n    4    NaN\ndtype: float64\n</code></pre>\n\n<p>Full explanation:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(pd.compat.StringIO(\"\"\"Rank ID\n0   1  a\n1   2  a\n2   3  a\n3   4  a\n4   5  a\n6   1  c\n7   2  c\n8   1  e\n9   2  e\n10  3  e\"\"\"), sep=r' +')\n\ndf = pd.crosstab(df.Rank, df.ID).iloc[:5].T.stack().reset_index()\ndf.loc[(df[0]==0),'Rank'] = np.nan\ndel df[0]\n\n# pd.crosstab(df.Rank, df.ID) produces:\n\n# ID    a  c  e\n# Rank\n# 1.0   1  1  1\n# 2.0   1  1  1\n# 3.0   1  0  1\n# 4.0   1  0  0\n# 5.0   1  0  0\n\n# applying .T.stack().reset_index() yields:\n\n   # ID  Rank  0\n# 0   a   1.0  1\n# 1   a   2.0  1\n# 2   a   3.0  1\n# 3   a   4.0  1\n# 4   a   5.0  1\n# 5   c   1.0  1\n# 6   c   2.0  1\n# 7   c   3.0  0\n# 8   c   4.0  0\n# 9   c   5.0  0\n# 10  e   1.0  1\n# 11  e   2.0  1\n# 12  e   3.0  1\n# 13  e   4.0  0\n# 14  e   5.0  0\n\n# finally, use df[0] to filter df['Rank']\n</code></pre>\n",
        "question_body": "<p>I'm trying to append a number of NaN rows to each group in a pandas dataframe. Essentially I want to pad each group to be 5 rows long. Ordering is important. I have:</p>\n\n<pre><code>    Rank id\n0   1  a\n1   2  a\n2   3  a\n3   4  a\n4   5  a\n5   1  c\n6   2  c\n7   1  e\n8   2  e\n9   3  e\n</code></pre>\n\n<p>I want:</p>\n\n<pre><code>    Rank id\n0   1    a\n1   2    a\n2   3    a\n3   4    a\n4   5    a\n5   1    c\n6   2    c\n7   NaN  c\n8   NaN  c\n9   NaN  c\n10  1    e\n11  2    e\n12  3    e\n13  NaN  e\n14  NaN  e\n</code></pre>\n",
        "formatted_input": {
            "qid": 50529459,
            "link": "https://stackoverflow.com/questions/50529459/append-rows-to-groups-in-pandas",
            "question": {
                "title": "Append rows to groups in pandas",
                "ques_desc": "I'm trying to append a number of NaN rows to each group in a pandas dataframe. Essentially I want to pad each group to be 5 rows long. Ordering is important. I have: I want: "
            },
            "io": [
                "    Rank id\n0   1  a\n1   2  a\n2   3  a\n3   4  a\n4   5  a\n5   1  c\n6   2  c\n7   1  e\n8   2  e\n9   3  e\n",
                "    Rank id\n0   1    a\n1   2    a\n2   3    a\n3   4    a\n4   5    a\n5   1    c\n6   2    c\n7   NaN  c\n8   NaN  c\n9   NaN  c\n10  1    e\n11  2    e\n12  3    e\n13  NaN  e\n14  NaN  e\n"
            ],
            "answer": {
                "ans_desc": "Using : Output: Another approach, assuming the maximum group size in is exactly 5. Full explanation: ",
                "code": [
                    "df = pd.crosstab(df.Rank, df.ID).iloc[:5].unstack().reset_index()\ndf.loc[(df[0]==0),'Rank'] = np.nan\ndel df[0]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 97,
            "user_id": 9933134,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/58b1d42e329e5552ec89c67819e729ca?s=128&d=identicon&r=PG&f=1",
            "display_name": "Fighting",
            "link": "https://stackoverflow.com/users/9933134/fighting"
        },
        "is_answered": true,
        "view_count": 121,
        "accepted_answer_id": 56122364,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1557818480,
        "creation_date": 1557800885,
        "last_edit_date": 1557802192,
        "question_id": 56122209,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56122209/how-to-slice-a-dataframe-and-reassemble-it-into-a-new-dataframe",
        "title": "how to slice a dataframe and reassemble it into a new dataframe",
        "body": "<p>I get a dataframe like this:</p>\n\n<pre><code>A   YEAR2000    B   YEAR2001    C   YEAR2002\na      1        b     3         a      7\nb      3        c     5         e      6\nc      6        d     2         f      3\n                e     1         g      0\n</code></pre>\n\n<p>Slice every two columns and then reorganize to form a new dataframe, as follows:</p>\n\n<pre><code>type    YEAR2000    YEAR2001    YEAR2002\na         1                         7\nb         3            3    \nc         6            5    \nd                      2    \ne                      1            6\nf                                   3\ng                                   0\n</code></pre>\n\n<p>I have tried <code>pd.concat()</code> but somethings wrong happened! Thank you.</p>\n",
        "answer_body": "<p>Using merge twice will achieve it.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df1 = pd.DataFrame([['a', 1], ['b', 3], ['c', 6]],columns=['letter', 'number'])\ndf2 = pd.DataFrame([['b', 3], ['c', 5], ['d', 2], ['e', 1]],columns=['letter', 'number'])\ndf3 = pd.DataFrame([['a', 7], ['e', 6], ['f', 3], ['g', 0]],columns=['letter', 'number'])\npd.merge(pd.merge(df1, df2, how='outer', on='letter'), df3, how='outer', on='letter')\n</code></pre>\n\n<p>for cleaner look:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>df1.merge(df2, how='outer', on='letter').merge(df3, how='outer', on='letter')\n</code></pre>\n\n<p><br>\nif you have mutiple dataframe, put them into a list and use comprehension with reduce.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from functools import reduce\ndfs = [df1, df2, df3]\nreduce(lambda left, right: left.merge(right, how='outer', on='letter'), dfs)\n</code></pre>\n",
        "question_body": "<p>I get a dataframe like this:</p>\n\n<pre><code>A   YEAR2000    B   YEAR2001    C   YEAR2002\na      1        b     3         a      7\nb      3        c     5         e      6\nc      6        d     2         f      3\n                e     1         g      0\n</code></pre>\n\n<p>Slice every two columns and then reorganize to form a new dataframe, as follows:</p>\n\n<pre><code>type    YEAR2000    YEAR2001    YEAR2002\na         1                         7\nb         3            3    \nc         6            5    \nd                      2    \ne                      1            6\nf                                   3\ng                                   0\n</code></pre>\n\n<p>I have tried <code>pd.concat()</code> but somethings wrong happened! Thank you.</p>\n",
        "formatted_input": {
            "qid": 56122209,
            "link": "https://stackoverflow.com/questions/56122209/how-to-slice-a-dataframe-and-reassemble-it-into-a-new-dataframe",
            "question": {
                "title": "how to slice a dataframe and reassemble it into a new dataframe",
                "ques_desc": "I get a dataframe like this: Slice every two columns and then reorganize to form a new dataframe, as follows: I have tried but somethings wrong happened! Thank you. "
            },
            "io": [
                "A   YEAR2000    B   YEAR2001    C   YEAR2002\na      1        b     3         a      7\nb      3        c     5         e      6\nc      6        d     2         f      3\n                e     1         g      0\n",
                "type    YEAR2000    YEAR2001    YEAR2002\na         1                         7\nb         3            3    \nc         6            5    \nd                      2    \ne                      1            6\nf                                   3\ng                                   0\n"
            ],
            "answer": {
                "ans_desc": "Using merge twice will achieve it. for cleaner look: if you have mutiple dataframe, put them into a list and use comprehension with reduce. ",
                "code": [
                    "df1 = pd.DataFrame([['a', 1], ['b', 3], ['c', 6]],columns=['letter', 'number'])\ndf2 = pd.DataFrame([['b', 3], ['c', 5], ['d', 2], ['e', 1]],columns=['letter', 'number'])\ndf3 = pd.DataFrame([['a', 7], ['e', 6], ['f', 3], ['g', 0]],columns=['letter', 'number'])\npd.merge(pd.merge(df1, df2, how='outer', on='letter'), df3, how='outer', on='letter')\n",
                    "df1.merge(df2, how='outer', on='letter').merge(df3, how='outer', on='letter')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 97,
            "user_id": 9933134,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/58b1d42e329e5552ec89c67819e729ca?s=128&d=identicon&r=PG&f=1",
            "display_name": "Fighting",
            "link": "https://stackoverflow.com/users/9933134/fighting"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 56086682,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1557543719,
        "creation_date": 1557541301,
        "question_id": 56086666,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56086666/how-does-pandas-convert-one-column-of-data-into-another",
        "title": "How does pandas convert one column of data into another?",
        "body": "<p>I have a dataframe generated by pandas, as follows\uff1a</p>\n\n<pre><code>NO  CODE\n1   a\n2   a\n3   a\n4   a\n5   a\n6   a\n7   b\n8   b\n9   a\n10  a\n11  a\n12  a\n13  b\n14  a\n15  a\n16  a\n</code></pre>\n\n<p>I want to convert the CODE column data to get the NUM column. The encoding rules are as follows:</p>\n\n<pre><code>NO CODE NUM\n1   a   1\n2   a   2\n3   a   3\n4   a   4\n5   a   5\n6   a   6\n7   b   b\n8   b   b\n9   a   1\n10  a   2\n11  a   3\n12  a   4\n13  b   b\n14  a   1\n15  a   2\n16  a   3\n\n</code></pre>\n\n<p>thank you\uff01</p>\n",
        "answer_body": "<p>Try:</p>\n\n<pre><code>a_group = df.CODE.eq('a')\n\ndf['NUM'] = np.where(a_group, \n                     df.groupby(a_group.ne(a_group.shift()).cumsum())\n                       .CODE.cumcount()+1, \n                     df.CODE)\n</code></pre>\n\n<p>on </p>\n\n<pre><code>df = pd.DataFrame({'CODE':list('baaaaaabbaaaabbaa')})\n</code></pre>\n\n<p>yields</p>\n\n<pre><code>    CODE    NUM\n--  ------  -----\n 0  b       b\n 1  a       1\n 2  a       2\n 3  a       3\n 4  a       4\n 5  a       5\n 6  a       6\n 7  b       b\n 8  b       b\n 9  a       1\n10  a       2\n11  a       3\n12  a       4\n13  b       b\n14  b       b\n15  a       1\n16  a       2\n</code></pre>\n",
        "question_body": "<p>I have a dataframe generated by pandas, as follows\uff1a</p>\n\n<pre><code>NO  CODE\n1   a\n2   a\n3   a\n4   a\n5   a\n6   a\n7   b\n8   b\n9   a\n10  a\n11  a\n12  a\n13  b\n14  a\n15  a\n16  a\n</code></pre>\n\n<p>I want to convert the CODE column data to get the NUM column. The encoding rules are as follows:</p>\n\n<pre><code>NO CODE NUM\n1   a   1\n2   a   2\n3   a   3\n4   a   4\n5   a   5\n6   a   6\n7   b   b\n8   b   b\n9   a   1\n10  a   2\n11  a   3\n12  a   4\n13  b   b\n14  a   1\n15  a   2\n16  a   3\n\n</code></pre>\n\n<p>thank you\uff01</p>\n",
        "formatted_input": {
            "qid": 56086666,
            "link": "https://stackoverflow.com/questions/56086666/how-does-pandas-convert-one-column-of-data-into-another",
            "question": {
                "title": "How does pandas convert one column of data into another?",
                "ques_desc": "I have a dataframe generated by pandas, as follows\uff1a I want to convert the CODE column data to get the NUM column. The encoding rules are as follows: thank you\uff01 "
            },
            "io": [
                "NO  CODE\n1   a\n2   a\n3   a\n4   a\n5   a\n6   a\n7   b\n8   b\n9   a\n10  a\n11  a\n12  a\n13  b\n14  a\n15  a\n16  a\n",
                "NO CODE NUM\n1   a   1\n2   a   2\n3   a   3\n4   a   4\n5   a   5\n6   a   6\n7   b   b\n8   b   b\n9   a   1\n10  a   2\n11  a   3\n12  a   4\n13  b   b\n14  a   1\n15  a   2\n16  a   3\n\n"
            ],
            "answer": {
                "ans_desc": "Try: on yields ",
                "code": [
                    "a_group = df.CODE.eq('a')\n\ndf['NUM'] = np.where(a_group, \n                     df.groupby(a_group.ne(a_group.shift()).cumsum())\n                       .CODE.cumcount()+1, \n                     df.CODE)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "dictionary",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3693,
            "user_id": 1083734,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/c2a5df1087b974441106b604a4cf1c27?s=128&d=identicon&r=PG",
            "display_name": "user1083734",
            "link": "https://stackoverflow.com/users/1083734/user1083734"
        },
        "is_answered": true,
        "view_count": 169355,
        "accepted_answer_id": 17426500,
        "answer_count": 4,
        "score": 187,
        "last_activity_date": 1557419387,
        "creation_date": 1372769925,
        "question_id": 17426292,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/17426292/what-is-the-most-efficient-way-to-create-a-dictionary-of-two-pandas-dataframe-co",
        "title": "What is the most efficient way to create a dictionary of two pandas Dataframe columns?",
        "body": "<p>What is the most efficient way to organise the following pandas Dataframe:</p>\n\n<p>data =</p>\n\n<pre><code>Position    Letter\n1           a\n2           b\n3           c\n4           d\n5           e\n</code></pre>\n\n<p>into a dictionary like <code>alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']</code>?</p>\n",
        "answer_body": "<pre><code>In [9]: pd.Series(df.Letter.values,index=df.Position).to_dict()\nOut[9]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}\n</code></pre>\n\n<p>Speed comparion (using Wouter's method)</p>\n\n<pre><code>In [6]: df = pd.DataFrame(randint(0,10,10000).reshape(5000,2),columns=list('AB'))\n\nIn [7]: %timeit dict(zip(df.A,df.B))\n1000 loops, best of 3: 1.27 ms per loop\n\nIn [8]: %timeit pd.Series(df.A.values,index=df.B).to_dict()\n1000 loops, best of 3: 987 us per loop\n</code></pre>\n",
        "question_body": "<p>What is the most efficient way to organise the following pandas Dataframe:</p>\n\n<p>data =</p>\n\n<pre><code>Position    Letter\n1           a\n2           b\n3           c\n4           d\n5           e\n</code></pre>\n\n<p>into a dictionary like <code>alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']</code>?</p>\n",
        "formatted_input": {
            "qid": 17426292,
            "link": "https://stackoverflow.com/questions/17426292/what-is-the-most-efficient-way-to-create-a-dictionary-of-two-pandas-dataframe-co",
            "question": {
                "title": "What is the most efficient way to create a dictionary of two pandas Dataframe columns?",
                "ques_desc": "What is the most efficient way to organise the following pandas Dataframe: data = into a dictionary like ? "
            },
            "io": [
                "Position    Letter\n1           a\n2           b\n3           c\n4           d\n5           e\n",
                "alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']"
            ],
            "answer": {
                "ans_desc": " Speed comparion (using Wouter's method) ",
                "code": [
                    "In [9]: pd.Series(df.Letter.values,index=df.Position).to_dict()\nOut[9]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "lambda"
        ],
        "owner": {
            "reputation": 1635,
            "user_id": 8378885,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://graph.facebook.com/10159144384065441/picture?type=large",
            "display_name": "HelloToEarth",
            "link": "https://stackoverflow.com/users/8378885/hellotoearth"
        },
        "is_answered": true,
        "view_count": 215,
        "accepted_answer_id": 56044508,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1557331517,
        "creation_date": 1557329904,
        "question_id": 56044354,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56044354/function-on-dataframe-rows-to-reduce-duplicate-pairs-python",
        "title": "Function on dataframe rows to reduce duplicate pairs Python",
        "body": "<p>I've got a dataframe that looks like:</p>\n\n<pre><code>0     1      2      3       4       5       6       7      8     9     10     11\n12    13     13     13.4    13.4    12.4    12.4    16     0     0     0      0\n14    12.2   12.2   13.4    13.4    12.6    12.6    19     5     5     6.7    6.7\n.\n.\n.\n</code></pre>\n\n<p>Each 'layer'/row has pairs that are duplicates that I want to reduce.</p>\n\n<p>The one problem is that there are repeating 0s as well so I cannot just simply remove duplicates per row or it will leave an uneven number of rows.</p>\n\n<p>My desired output would be a <strong>lambda function</strong> that I could apply to all rows of this dataframe to get this:</p>\n\n<pre><code>0     1      2      3       4       5      6 \n12    13     13.4   12.4    16      0      0\n14    12.2   13.4   12.6    19      5      6.7\n.\n.\n.\n</code></pre>\n\n<p>Is there a simple function I could write to do this?</p>\n",
        "answer_body": "<h1>Method 1 using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html\" rel=\"nofollow noreferrer\"><code>transpose</code></a></h1>\n\n<p>As mentioned by Yuca in the comments:</p>\n\n<pre><code>df = df.T.drop_duplicates().T\ndf.columns = range(len(df.columns))\n\nprint(df)\n      0     1     2     3     4    5    6\n0  12.0  13.0  13.4  12.4  16.0  0.0  0.0\n1  14.0  12.2  13.4  12.6  19.0  5.0  6.7\n</code></pre>\n\n<h1>Method 2 using <code>list comprehension</code> with even numbers</h1>\n\n<p>We can make a list of <em>even</em> numbers and then select those columns based on their index:</p>\n\n<pre><code>idxcols = [x-1 for x in range(len(df.columns)) if x % 2]\n\ndf = df.iloc[:, idxcols]\n\ndf.columns = range(len(df.columns))\n</code></pre>\n\n<hr>\n\n<pre><code>print(df)\n    0     1     2     3  4    5\n0  12  13.0  13.4  12.4  0  0.0\n1  14  12.2  13.4  12.6  5  6.7\n</code></pre>\n",
        "question_body": "<p>I've got a dataframe that looks like:</p>\n\n<pre><code>0     1      2      3       4       5       6       7      8     9     10     11\n12    13     13     13.4    13.4    12.4    12.4    16     0     0     0      0\n14    12.2   12.2   13.4    13.4    12.6    12.6    19     5     5     6.7    6.7\n.\n.\n.\n</code></pre>\n\n<p>Each 'layer'/row has pairs that are duplicates that I want to reduce.</p>\n\n<p>The one problem is that there are repeating 0s as well so I cannot just simply remove duplicates per row or it will leave an uneven number of rows.</p>\n\n<p>My desired output would be a <strong>lambda function</strong> that I could apply to all rows of this dataframe to get this:</p>\n\n<pre><code>0     1      2      3       4       5      6 \n12    13     13.4   12.4    16      0      0\n14    12.2   13.4   12.6    19      5      6.7\n.\n.\n.\n</code></pre>\n\n<p>Is there a simple function I could write to do this?</p>\n",
        "formatted_input": {
            "qid": 56044354,
            "link": "https://stackoverflow.com/questions/56044354/function-on-dataframe-rows-to-reduce-duplicate-pairs-python",
            "question": {
                "title": "Function on dataframe rows to reduce duplicate pairs Python",
                "ques_desc": "I've got a dataframe that looks like: Each 'layer'/row has pairs that are duplicates that I want to reduce. The one problem is that there are repeating 0s as well so I cannot just simply remove duplicates per row or it will leave an uneven number of rows. My desired output would be a lambda function that I could apply to all rows of this dataframe to get this: Is there a simple function I could write to do this? "
            },
            "io": [
                "0     1      2      3       4       5       6       7      8     9     10     11\n12    13     13     13.4    13.4    12.4    12.4    16     0     0     0      0\n14    12.2   12.2   13.4    13.4    12.6    12.6    19     5     5     6.7    6.7\n.\n.\n.\n",
                "0     1      2      3       4       5      6 \n12    13     13.4   12.4    16      0      0\n14    12.2   13.4   12.6    19      5      6.7\n.\n.\n.\n"
            ],
            "answer": {
                "ans_desc": "Method 1 using As mentioned by Yuca in the comments: Method 2 using with even numbers We can make a list of even numbers and then select those columns based on their index: ",
                "code": [
                    "idxcols = [x-1 for x in range(len(df.columns)) if x % 2]\n\ndf = df.iloc[:, idxcols]\n\ndf.columns = range(len(df.columns))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "sorting",
            "dataframe"
        ],
        "owner": {
            "reputation": 433,
            "user_id": 10614343,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/adfafbdf20f5d444b6de142e448ab5b1?s=128&d=identicon&r=PG&f=1",
            "display_name": "john doe",
            "link": "https://stackoverflow.com/users/10614343/john-doe"
        },
        "is_answered": true,
        "view_count": 292,
        "accepted_answer_id": 56036243,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1557304039,
        "creation_date": 1557302761,
        "last_edit_date": 1557304039,
        "question_id": 56036178,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56036178/how-can-i-sort-numbers-in-a-string-in-pandas",
        "title": "How can I sort numbers in a string in pandas?",
        "body": "<p>I have a dataframe like this:</p>\n\n<pre><code>id   String\n1    345 -456 -13  879\n2    158 -926 -81  249 35 -4 -53  9\n3    945 -506 -103\n</code></pre>\n\n<p>I want to sort it in desending order like this</p>\n\n<pre><code>id   String\n1    879 345 -13 -457\n2    249 158 35 9 -4 -53 -81 -926\n3    945 -103 -506\n</code></pre>\n\n<p>I tried this:</p>\n\n<p><code>df['string'] = df['string'].str.split(' ').map(lambda x: ' '.join(sorted(x)))</code></p>\n\n<p>The above function do some kind of sorting but not the way I want it. </p>\n",
        "answer_body": "<p>First is necessary convert values to integers, sorting and then convert to strings back:</p>\n\n<pre><code>f = lambda x: ' '.join(map(str, sorted(map(int, x), reverse=True)))\n#another solution\n#f = lambda x: ' '.join(str(z) for z in sorted((int(y) for y in x), reverse=True))\ndf['string'] = df['string'].str.split().map(f)\nprint (df)\n   id                        string\n0   1              879 345 -13 -456\n1   2  249 158 35 9 -4 -53 -81 -926\n2   3                 945 -103 -506\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>f = lambda x: ' '.join(map(str, sorted(map(int, x.split()), reverse=True)))\ndf['string'] = df['string'].map(f)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n\n<pre><code>id   String\n1    345 -456 -13  879\n2    158 -926 -81  249 35 -4 -53  9\n3    945 -506 -103\n</code></pre>\n\n<p>I want to sort it in desending order like this</p>\n\n<pre><code>id   String\n1    879 345 -13 -457\n2    249 158 35 9 -4 -53 -81 -926\n3    945 -103 -506\n</code></pre>\n\n<p>I tried this:</p>\n\n<p><code>df['string'] = df['string'].str.split(' ').map(lambda x: ' '.join(sorted(x)))</code></p>\n\n<p>The above function do some kind of sorting but not the way I want it. </p>\n",
        "formatted_input": {
            "qid": 56036178,
            "link": "https://stackoverflow.com/questions/56036178/how-can-i-sort-numbers-in-a-string-in-pandas",
            "question": {
                "title": "How can I sort numbers in a string in pandas?",
                "ques_desc": "I have a dataframe like this: I want to sort it in desending order like this I tried this: The above function do some kind of sorting but not the way I want it. "
            },
            "io": [
                "id   String\n1    345 -456 -13  879\n2    158 -926 -81  249 35 -4 -53  9\n3    945 -506 -103\n",
                "id   String\n1    879 345 -13 -457\n2    249 158 35 9 -4 -53 -81 -926\n3    945 -103 -506\n"
            ],
            "answer": {
                "ans_desc": "First is necessary convert values to integers, sorting and then convert to strings back: Or: ",
                "code": [
                    "f = lambda x: ' '.join(map(str, sorted(map(int, x.split()), reverse=True)))\ndf['string'] = df['string'].map(f)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "combinations"
        ],
        "owner": {
            "reputation": 65,
            "user_id": 3698934,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5472a51cb09556f1fef66db331855b49?s=128&d=identicon&r=PG&f=1",
            "display_name": "CheyRav90",
            "link": "https://stackoverflow.com/users/3698934/cheyrav90"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 56020448,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1557224926,
        "creation_date": 1557223821,
        "last_edit_date": 1557224926,
        "question_id": 56020272,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56020272/convert-rows-per-unique-id-into-all-comma-separated-possibilities",
        "title": "Convert Rows per Unique Id into all comma separated possibilities",
        "body": "<p>i have some data in the following format, at the moment this is in a Pandas Dataframe. </p>\n\n<pre><code>Row   Uid    Lender\n1     1      HSBC\n2     1      Lloyds\n3     1      Barclays\n4     2      Lloyds\n5     2      Barclays\n6     2      Santander\n7     2      RBS\n8     2      HSBC\n</code></pre>\n\n<p>What i require is all of the possible combinations of the Lenders columns for each Uid so the output would be something like this </p>\n\n<pre><code>Row   Uid   LenderCombo\n1     1     Barclays\n2     1     Lloyds\n3     1     HSBC\n4     1     Barclays, HSBC\n5     1     Barclays, Lloyds\n6     1     HSBC, Lloyds \n7     1     Barclays, HSBC, Lloyds\n</code></pre>\n\n<p>And the same for Uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. </p>\n\n<p>Thanks,</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.apply.html\" rel=\"nofollow noreferrer\"><code>GroupBy.apply</code></a> with custom function and join tuples by <code>join</code>:</p>\n\n<pre><code>from itertools import chain, combinations\n\n#https://stackoverflow.com/a/5898031\ndef all_subsets(ss):\n    return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))\n\ndf = (df.groupby('Uid')['Lender']\n       .apply(lambda x: pd.Series([', '.join(y) for y in all_subsets(x)]))\n       .reset_index()\n       .rename(columns={'level_1':'Row'}))\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n    Uid  Row                                  Lender\n0     1    0                                    HSBC\n1     1    1                                  Lloyds\n2     1    2                                Barclays\n3     1    3                            HSBC, Lloyds\n4     1    4                          HSBC, Barclays\n5     1    5                        Lloyds, Barclays\n6     1    6                  HSBC, Lloyds, Barclays\n7     2    0                                  Lloyds\n8     2    1                                Barclays\n9     2    2                               Santander\n10    2    3                                     RBS\n11    2    4                                    HSBC\n12    2    5                        Lloyds, Barclays\n13    2    6                       Lloyds, Santander\n14    2    7                             Lloyds, RBS\n15    2    8                            Lloyds, HSBC\n16    2    9                     Barclays, Santander\n17    2   10                           Barclays, RBS\n18    2   11                          Barclays, HSBC\n19    2   12                          Santander, RBS\n20    2   13                         Santander, HSBC\n21    2   14                               RBS, HSBC\n22    2   15             Lloyds, Barclays, Santander\n23    2   16                   Lloyds, Barclays, RBS\n24    2   17                  Lloyds, Barclays, HSBC\n25    2   18                  Lloyds, Santander, RBS\n26    2   19                 Lloyds, Santander, HSBC\n27    2   20                       Lloyds, RBS, HSBC\n28    2   21                Barclays, Santander, RBS\n29    2   22               Barclays, Santander, HSBC\n30    2   23                     Barclays, RBS, HSBC\n31    2   24                    Santander, RBS, HSBC\n32    2   25        Lloyds, Barclays, Santander, RBS\n33    2   26       Lloyds, Barclays, Santander, HSBC\n34    2   27             Lloyds, Barclays, RBS, HSBC\n35    2   28            Lloyds, Santander, RBS, HSBC\n36    2   29          Barclays, Santander, RBS, HSBC\n37    2   30  Lloyds, Barclays, Santander, RBS, HSBC\n</code></pre>\n",
        "question_body": "<p>i have some data in the following format, at the moment this is in a Pandas Dataframe. </p>\n\n<pre><code>Row   Uid    Lender\n1     1      HSBC\n2     1      Lloyds\n3     1      Barclays\n4     2      Lloyds\n5     2      Barclays\n6     2      Santander\n7     2      RBS\n8     2      HSBC\n</code></pre>\n\n<p>What i require is all of the possible combinations of the Lenders columns for each Uid so the output would be something like this </p>\n\n<pre><code>Row   Uid   LenderCombo\n1     1     Barclays\n2     1     Lloyds\n3     1     HSBC\n4     1     Barclays, HSBC\n5     1     Barclays, Lloyds\n6     1     HSBC, Lloyds \n7     1     Barclays, HSBC, Lloyds\n</code></pre>\n\n<p>And the same for Uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. </p>\n\n<p>Thanks,</p>\n",
        "formatted_input": {
            "qid": 56020272,
            "link": "https://stackoverflow.com/questions/56020272/convert-rows-per-unique-id-into-all-comma-separated-possibilities",
            "question": {
                "title": "Convert Rows per Unique Id into all comma separated possibilities",
                "ques_desc": "i have some data in the following format, at the moment this is in a Pandas Dataframe. What i require is all of the possible combinations of the Lenders columns for each Uid so the output would be something like this And the same for Uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. Thanks, "
            },
            "io": [
                "Row   Uid    Lender\n1     1      HSBC\n2     1      Lloyds\n3     1      Barclays\n4     2      Lloyds\n5     2      Barclays\n6     2      Santander\n7     2      RBS\n8     2      HSBC\n",
                "Row   Uid   LenderCombo\n1     1     Barclays\n2     1     Lloyds\n3     1     HSBC\n4     1     Barclays, HSBC\n5     1     Barclays, Lloyds\n6     1     HSBC, Lloyds \n7     1     Barclays, HSBC, Lloyds\n"
            ],
            "answer": {
                "ans_desc": "Use with custom function and join tuples by : ",
                "code": [
                    "from itertools import chain, combinations\n\n#https://stackoverflow.com/a/5898031\ndef all_subsets(ss):\n    return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))\n\ndf = (df.groupby('Uid')['Lender']\n       .apply(lambda x: pd.Series([', '.join(y) for y in all_subsets(x)]))\n       .reset_index()\n       .rename(columns={'level_1':'Row'}))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 11453238,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-XjCGhc_I2fk/AAAAAAAAAAI/AAAAAAAAbk0/Z8lV7hqbfB0/photo.jpg?sz=128",
            "display_name": "Nandy",
            "link": "https://stackoverflow.com/users/11453238/nandy"
        },
        "is_answered": true,
        "view_count": 360,
        "accepted_answer_id": 55989239,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1557036084,
        "creation_date": 1557002695,
        "last_edit_date": 1557036084,
        "question_id": 55986653,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55986653/is-there-a-way-to-get-the-mean-value-of-previous-two-columns-in-pandas",
        "title": "Is there a way to get the mean value of previous two columns in pandas?",
        "body": "<p>I want to calculate the mean value of previous two rows and fill the NAN's in my dataframe. There are only few rows with missing values in the <code>2010-19</code> column.</p>\n\n<p>I tried using <code>bfill</code> and <code>ffill</code> but it only captures the previous or next row/column value and fill NAN. </p>\n\n<p>My example data set has 7 columns as below:</p>\n\n<pre><code>X       1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19\nHyderabad    10          20       NAN         1         3           NAN\n</code></pre>\n\n<p>The output I want:</p>\n\n<pre><code>X          1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19\nHyderabad    10          20          15         1           3         2\n</code></pre>\n",
        "answer_body": "<p>To use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\" rel=\"nofollow noreferrer\"><code>fillna</code></a> row-wise in this way, an easy solution is to provide an pandas series as argument to <code>fillna</code>. This will replace <code>NaN</code> values depending on the index.</p>\n\n<p>Since the column names have duplicates the below code uses the column indices. Assuming a dataframe called <code>df</code>:</p>\n\n<pre><code>col_indices = [3, 6]\n\nfor i in col_indices:\n    means = df.iloc[:, [i-1, i-2]].mean(axis=1)\n    df.iloc[:, i].fillna(means, inplace=True)\n</code></pre>\n\n<p>This will fill the <code>NaN</code> values with the mean of the two columns to the left of each column in <code>col_indices</code>.</p>\n",
        "question_body": "<p>I want to calculate the mean value of previous two rows and fill the NAN's in my dataframe. There are only few rows with missing values in the <code>2010-19</code> column.</p>\n\n<p>I tried using <code>bfill</code> and <code>ffill</code> but it only captures the previous or next row/column value and fill NAN. </p>\n\n<p>My example data set has 7 columns as below:</p>\n\n<pre><code>X       1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19\nHyderabad    10          20       NAN         1         3           NAN\n</code></pre>\n\n<p>The output I want:</p>\n\n<pre><code>X          1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19\nHyderabad    10          20          15         1           3         2\n</code></pre>\n",
        "formatted_input": {
            "qid": 55986653,
            "link": "https://stackoverflow.com/questions/55986653/is-there-a-way-to-get-the-mean-value-of-previous-two-columns-in-pandas",
            "question": {
                "title": "Is there a way to get the mean value of previous two columns in pandas?",
                "ques_desc": "I want to calculate the mean value of previous two rows and fill the NAN's in my dataframe. There are only few rows with missing values in the column. I tried using and but it only captures the previous or next row/column value and fill NAN. My example data set has 7 columns as below: The output I want: "
            },
            "io": [
                "X       1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19\nHyderabad    10          20       NAN         1         3           NAN\n",
                "X          1990-2000   2000-2010   2010-19   1990-2000  2000-2010   2010-19\nHyderabad    10          20          15         1           3         2\n"
            ],
            "answer": {
                "ans_desc": "To use row-wise in this way, an easy solution is to provide an pandas series as argument to . This will replace values depending on the index. Since the column names have duplicates the below code uses the column indices. Assuming a dataframe called : This will fill the values with the mean of the two columns to the left of each column in . ",
                "code": [
                    "col_indices = [3, 6]\n\nfor i in col_indices:\n    means = df.iloc[:, [i-1, i-2]].mean(axis=1)\n    df.iloc[:, i].fillna(means, inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 367,
            "user_id": 6317524,
            "user_type": "registered",
            "accept_rate": 68,
            "profile_image": "https://www.gravatar.com/avatar/60f5811af3a8639fe0a1f7fd4475d95d?s=128&d=identicon&r=PG&f=1",
            "display_name": "trob",
            "link": "https://stackoverflow.com/users/6317524/trob"
        },
        "is_answered": true,
        "view_count": 387,
        "accepted_answer_id": 43098659,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1556831924,
        "creation_date": 1490798663,
        "last_edit_date": 1490803840,
        "question_id": 43096821,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/43096821/operation-on-pandas-dataframe-columns-using-its-index",
        "title": "Operation on Pandas Dataframe columns using its Index",
        "body": "<p>This should be relatively easy.  I have a pandas dataframe (Dates): </p>\n\n<pre><code>    A   B   C\n1/8/2017    1/11/2017   1/20/2017   1/25/2017\n1/9/2017    1/11/2017   1/20/2017   1/25/2017\n1/10/2017   1/11/2017   1/20/2017   1/25/2017\n1/11/2017   1/20/2017   1/25/2017   1/31/2017\n1/12/2017   1/20/2017   1/25/2017   1/31/2017\n1/13/2017   1/20/2017   1/25/2017   1/31/2017\n</code></pre>\n\n<p>I would like to take the difference between Dates.index and Dates.  The output would be like so:</p>\n\n<pre><code>    A   B   C\n1/8/2017     3   12      17 \n1/9/2017     2   11      16 \n1/10/2017    1   10      15 \n1/11/2017    9   14      20 \n1/12/2017    8   13      19 \n1/13/2017    7   12      18 \n</code></pre>\n\n<p>Naturally, I tried this:</p>\n\n<pre><code>Dates - Dates.index\n</code></pre>\n\n<p>But I receive this lovely TypeError:</p>\n\n<pre><code>TypeError: Could not operate DatetimeIndex...with block values ufunc subtract cannot use operands with types dtype('&lt;M8[ns]') and dtype('O')\n</code></pre>\n\n<p>Instead, I've written a loop to go column by column, but that just seems silly. Can anyone suggest a pythonic way to do this?</p>\n\n<p>EDIT</p>\n\n<pre><code>In [1]: import pandas as pd\nimport numpy as np\nimport datetime\ndates = pd.date_range('20170108',periods=6)\ndf = pd.DataFrame(np.empty([len(dates),3]),index=dates,columns=list('ABC'))\ndf['A'].loc[0:3] = datetime.date(2017, 1, 11)\ndf['B'].loc[0:3] = datetime.date(2017, 1, 20)\ndf['C'].loc[0:3] = datetime.date(2017, 1, 25)\ndf['A'].loc[3:6] = datetime.date(2017, 1, 20)\ndf['B'].loc[3:6] = datetime.date(2017, 1, 25)\ndf['C'].loc[3:6] = datetime.date(2017, 1, 31)\n\nIn [2]: print(df)\n                     A           B           C\n2017-01-08  2017-01-11  2017-01-20  2017-01-25\n2017-01-09  2017-01-11  2017-01-20  2017-01-25\n2017-01-10  2017-01-11  2017-01-20  2017-01-25\n2017-01-11  2017-01-20  2017-01-25  2017-01-31\n2017-01-12  2017-01-20  2017-01-25  2017-01-31\n2017-01-13  2017-01-20  2017-01-25  2017-01-31\n\nIn [3]: df = df.sub(df.index.to_series(),axis=0)\n\nValueError: operands could not be broadcast together with shapes (18,) (6,) \n</code></pre>\n",
        "answer_body": "<p>I think a more explicit and elegant way to do this is to simply use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a>.</p>\n\n<p><code>df = df.apply(pd.to_datetime, axis=\"columns\")  # just to make sure values are datetime\ndf.apply(lambda x: x - df.index.to_series(), axis=\"rows)</code></p>\n",
        "question_body": "<p>This should be relatively easy.  I have a pandas dataframe (Dates): </p>\n\n<pre><code>    A   B   C\n1/8/2017    1/11/2017   1/20/2017   1/25/2017\n1/9/2017    1/11/2017   1/20/2017   1/25/2017\n1/10/2017   1/11/2017   1/20/2017   1/25/2017\n1/11/2017   1/20/2017   1/25/2017   1/31/2017\n1/12/2017   1/20/2017   1/25/2017   1/31/2017\n1/13/2017   1/20/2017   1/25/2017   1/31/2017\n</code></pre>\n\n<p>I would like to take the difference between Dates.index and Dates.  The output would be like so:</p>\n\n<pre><code>    A   B   C\n1/8/2017     3   12      17 \n1/9/2017     2   11      16 \n1/10/2017    1   10      15 \n1/11/2017    9   14      20 \n1/12/2017    8   13      19 \n1/13/2017    7   12      18 \n</code></pre>\n\n<p>Naturally, I tried this:</p>\n\n<pre><code>Dates - Dates.index\n</code></pre>\n\n<p>But I receive this lovely TypeError:</p>\n\n<pre><code>TypeError: Could not operate DatetimeIndex...with block values ufunc subtract cannot use operands with types dtype('&lt;M8[ns]') and dtype('O')\n</code></pre>\n\n<p>Instead, I've written a loop to go column by column, but that just seems silly. Can anyone suggest a pythonic way to do this?</p>\n\n<p>EDIT</p>\n\n<pre><code>In [1]: import pandas as pd\nimport numpy as np\nimport datetime\ndates = pd.date_range('20170108',periods=6)\ndf = pd.DataFrame(np.empty([len(dates),3]),index=dates,columns=list('ABC'))\ndf['A'].loc[0:3] = datetime.date(2017, 1, 11)\ndf['B'].loc[0:3] = datetime.date(2017, 1, 20)\ndf['C'].loc[0:3] = datetime.date(2017, 1, 25)\ndf['A'].loc[3:6] = datetime.date(2017, 1, 20)\ndf['B'].loc[3:6] = datetime.date(2017, 1, 25)\ndf['C'].loc[3:6] = datetime.date(2017, 1, 31)\n\nIn [2]: print(df)\n                     A           B           C\n2017-01-08  2017-01-11  2017-01-20  2017-01-25\n2017-01-09  2017-01-11  2017-01-20  2017-01-25\n2017-01-10  2017-01-11  2017-01-20  2017-01-25\n2017-01-11  2017-01-20  2017-01-25  2017-01-31\n2017-01-12  2017-01-20  2017-01-25  2017-01-31\n2017-01-13  2017-01-20  2017-01-25  2017-01-31\n\nIn [3]: df = df.sub(df.index.to_series(),axis=0)\n\nValueError: operands could not be broadcast together with shapes (18,) (6,) \n</code></pre>\n",
        "formatted_input": {
            "qid": 43096821,
            "link": "https://stackoverflow.com/questions/43096821/operation-on-pandas-dataframe-columns-using-its-index",
            "question": {
                "title": "Operation on Pandas Dataframe columns using its Index",
                "ques_desc": "This should be relatively easy. I have a pandas dataframe (Dates): I would like to take the difference between Dates.index and Dates. The output would be like so: Naturally, I tried this: But I receive this lovely TypeError: Instead, I've written a loop to go column by column, but that just seems silly. Can anyone suggest a pythonic way to do this? EDIT "
            },
            "io": [
                "    A   B   C\n1/8/2017    1/11/2017   1/20/2017   1/25/2017\n1/9/2017    1/11/2017   1/20/2017   1/25/2017\n1/10/2017   1/11/2017   1/20/2017   1/25/2017\n1/11/2017   1/20/2017   1/25/2017   1/31/2017\n1/12/2017   1/20/2017   1/25/2017   1/31/2017\n1/13/2017   1/20/2017   1/25/2017   1/31/2017\n",
                "    A   B   C\n1/8/2017     3   12      17 \n1/9/2017     2   11      16 \n1/10/2017    1   10      15 \n1/11/2017    9   14      20 \n1/12/2017    8   13      19 \n1/13/2017    7   12      18 \n"
            ],
            "answer": {
                "ans_desc": "I think a more explicit and elegant way to do this is to simply use . ",
                "code": [
                    "df = df.apply(pd.to_datetime, axis=\"columns\")  # just to make sure values are datetime\ndf.apply(lambda x: x - df.index.to_series(), axis=\"rows)"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 328,
            "user_id": 6414747,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/46396451313c837c27586bbe2a145072?s=128&d=identicon&r=PG&f=1",
            "display_name": "cekar",
            "link": "https://stackoverflow.com/users/6414747/cekar"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 55920522,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1556625999,
        "creation_date": 1556625368,
        "question_id": 55920437,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55920437/how-to-set-pandas-to-extract-certain-rows-of-certain-columns-and-stack-them-on-t",
        "title": "how to set Pandas to extract certain rows of certain columns and stack them on top of each other?",
        "body": "<p>How can I extract certain columns and rows to stack them together?</p>\n\n<p>I created a simple exemplary dataframe with this data:</p>\n\n<pre><code>data = {'d1':[101,201,301,401],\n        'd2':[102,202,302,402],\n        'd3':[103,203,303,403],\n        'd4':[104,204,304,404]\n        }\n\ndfa = pd.DataFrame(data, index=['t1','t2','t3','t4'])\ndfa\n     d1   d2   d3   d4\nt1  101  102  103  104\nt2  201  202  203  204\nt3  301  302  303  304\nt4  401  402  403  404\n</code></pre>\n\n<p>This is what I would like to get:</p>\n\n<pre><code>   d1_d2_d3-t1-t2\n1             101\n2             201\n3             102\n4             202\n5             103\n6             203\n</code></pre>\n\n<p>Another Format I would like to get is in two columns:</p>\n\n<pre><code>   d1_d2_d3-t1-t2  d1_d2_d3-t3-t4\n1             101             301\n2             201             401\n3             102             302\n4             202             402\n5             103             303\n6             203             403\n</code></pre>\n\n<p>The headers in the desired results are just for explanation</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>DataFrame.loc</code></a> for filtering with <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html\" rel=\"nofollow noreferrer\"><code>DataFrame.melt</code></a> for reshape:</p>\n\n<pre><code>idx = ['t1','t2']\ncols = ['d1','d2', 'd3']\ndf = dfa.loc[idx, cols].melt(value_name='data')[['data']]\n</code></pre>\n\n<p>Another solution is convert values to numpy array, flatten by <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html\" rel=\"nofollow noreferrer\"><code>numpy.ravel</code></a> and create new DataFrame by constructor:</p>\n\n<pre><code>df = pd.DataFrame({'data': dfa.loc[idx, cols].values.ravel()})\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n   data\n0   101\n1   201\n2   102\n3   202\n4   103\n5   203\n\nidx1 = ['t1','t2']\nidx2 = ['t3','t4']\ncols = ['d1','d2', 'd3']\ndf = pd.concat([dfa.loc[idx1, cols].melt(value_name='data1')[['data1']],\n                dfa.loc[idx2, cols].melt(value_name='data2')[['data2']]], axis=1)\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df = pd.DataFrame({'data1': dfa.loc[idx1, cols].values.ravel(),\n                   'data2': dfa.loc[idx2, cols].values.ravel()})\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n   data1  data2\n0    101    301\n1    201    401\n2    102    302\n3    202    402\n4    103    303\n5    203    403\n</code></pre>\n",
        "question_body": "<p>How can I extract certain columns and rows to stack them together?</p>\n\n<p>I created a simple exemplary dataframe with this data:</p>\n\n<pre><code>data = {'d1':[101,201,301,401],\n        'd2':[102,202,302,402],\n        'd3':[103,203,303,403],\n        'd4':[104,204,304,404]\n        }\n\ndfa = pd.DataFrame(data, index=['t1','t2','t3','t4'])\ndfa\n     d1   d2   d3   d4\nt1  101  102  103  104\nt2  201  202  203  204\nt3  301  302  303  304\nt4  401  402  403  404\n</code></pre>\n\n<p>This is what I would like to get:</p>\n\n<pre><code>   d1_d2_d3-t1-t2\n1             101\n2             201\n3             102\n4             202\n5             103\n6             203\n</code></pre>\n\n<p>Another Format I would like to get is in two columns:</p>\n\n<pre><code>   d1_d2_d3-t1-t2  d1_d2_d3-t3-t4\n1             101             301\n2             201             401\n3             102             302\n4             202             402\n5             103             303\n6             203             403\n</code></pre>\n\n<p>The headers in the desired results are just for explanation</p>\n",
        "formatted_input": {
            "qid": 55920437,
            "link": "https://stackoverflow.com/questions/55920437/how-to-set-pandas-to-extract-certain-rows-of-certain-columns-and-stack-them-on-t",
            "question": {
                "title": "how to set Pandas to extract certain rows of certain columns and stack them on top of each other?",
                "ques_desc": "How can I extract certain columns and rows to stack them together? I created a simple exemplary dataframe with this data: This is what I would like to get: Another Format I would like to get is in two columns: The headers in the desired results are just for explanation "
            },
            "io": [
                "   d1_d2_d3-t1-t2\n1             101\n2             201\n3             102\n4             202\n5             103\n6             203\n",
                "   d1_d2_d3-t1-t2  d1_d2_d3-t3-t4\n1             101             301\n2             201             401\n3             102             302\n4             202             402\n5             103             303\n6             203             403\n"
            ],
            "answer": {
                "ans_desc": "Use for filtering with for reshape: Another solution is convert values to numpy array, flatten by and create new DataFrame by constructor: Or: ",
                "code": [
                    "idx = ['t1','t2']\ncols = ['d1','d2', 'd3']\ndf = dfa.loc[idx, cols].melt(value_name='data')[['data']]\n",
                    "df = pd.DataFrame({'data': dfa.loc[idx, cols].values.ravel()})\n",
                    "df = pd.DataFrame({'data1': dfa.loc[idx1, cols].values.ravel(),\n                   'data2': dfa.loc[idx2, cols].values.ravel()})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "machine-learning",
            "data-science"
        ],
        "owner": {
            "reputation": 95,
            "user_id": 5843297,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-FgdBkAahawo/AAAAAAAAAAI/AAAAAAAAASg/b5e0BJop5og/photo.jpg?sz=128",
            "display_name": "gonzadevelop",
            "link": "https://stackoverflow.com/users/5843297/gonzadevelop"
        },
        "is_answered": true,
        "view_count": 6882,
        "accepted_answer_id": 39359665,
        "answer_count": 2,
        "score": 5,
        "last_activity_date": 1556617168,
        "creation_date": 1473205110,
        "question_id": 39359272,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/39359272/add-new-columns-to-pandas-dataframe-based-on-other-dataframe",
        "title": "Add new columns to pandas dataframe based on other dataframe",
        "body": "<p>I'm trying to set a new column (two columns in fact) in a pandas dataframe, with the data comes from other dataframe.</p>\n\n<p>I have the following two dataframes (they are example for this purpose, the original dataframes are so much bigger):</p>\n\n<pre><code>In [116]: df0\nOut[116]:     \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n\n\nIn [118]: df1\nOut[118]: \n   A  D  E\n0  2  7  2\n1  6  5  5\n2  4  3  2\n3  0  1  0\n4  5  4  6\n5  0  1  0\n</code></pre>\n\n<p>And I want to have a new dataframe (or added to df0, whatever), as:</p>\n\n<pre><code>df2: \n   A  B  C  D  E\n0  0  1  0  1  0\n1  2  3  2  7  2\n2  4  5  4  3  2\n3  5  5  5  4  6\n</code></pre>\n\n<p>As you can see, in the resulting dataframe isn't present the row with A=6 which is present in df1 but not in df0. Also the row with A=0 is duplicated in df1, but not in the result df2.</p>\n\n<p>Actually, I'm having trouble with the selection method. I can do this:</p>\n\n<pre><code>df1.loc[df1['A'].isin(df0['A'])]\n</code></pre>\n\n<p>But I'm not sure how to apply the part of keep with unique data (remember that df1 can contain duplicated data) and add the two columns to the df2 dataset (or add them to df0).\nI've search here and I don't know see how to apply something like groupby, or even map.</p>\n\n<p>Any idea?</p>\n\n<p>Thanks!</p>\n",
        "answer_body": "<p>This is a basic application of <code>merge</code> (<a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\" rel=\"nofollow\">docs</a>):</p>\n\n<pre><code>import pandas as pd\ndf2 = pd.merge(df0,df1, left_index=True, right_index=True)\n</code></pre>\n",
        "question_body": "<p>I'm trying to set a new column (two columns in fact) in a pandas dataframe, with the data comes from other dataframe.</p>\n\n<p>I have the following two dataframes (they are example for this purpose, the original dataframes are so much bigger):</p>\n\n<pre><code>In [116]: df0\nOut[116]:     \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n\n\nIn [118]: df1\nOut[118]: \n   A  D  E\n0  2  7  2\n1  6  5  5\n2  4  3  2\n3  0  1  0\n4  5  4  6\n5  0  1  0\n</code></pre>\n\n<p>And I want to have a new dataframe (or added to df0, whatever), as:</p>\n\n<pre><code>df2: \n   A  B  C  D  E\n0  0  1  0  1  0\n1  2  3  2  7  2\n2  4  5  4  3  2\n3  5  5  5  4  6\n</code></pre>\n\n<p>As you can see, in the resulting dataframe isn't present the row with A=6 which is present in df1 but not in df0. Also the row with A=0 is duplicated in df1, but not in the result df2.</p>\n\n<p>Actually, I'm having trouble with the selection method. I can do this:</p>\n\n<pre><code>df1.loc[df1['A'].isin(df0['A'])]\n</code></pre>\n\n<p>But I'm not sure how to apply the part of keep with unique data (remember that df1 can contain duplicated data) and add the two columns to the df2 dataset (or add them to df0).\nI've search here and I don't know see how to apply something like groupby, or even map.</p>\n\n<p>Any idea?</p>\n\n<p>Thanks!</p>\n",
        "formatted_input": {
            "qid": 39359272,
            "link": "https://stackoverflow.com/questions/39359272/add-new-columns-to-pandas-dataframe-based-on-other-dataframe",
            "question": {
                "title": "Add new columns to pandas dataframe based on other dataframe",
                "ques_desc": "I'm trying to set a new column (two columns in fact) in a pandas dataframe, with the data comes from other dataframe. I have the following two dataframes (they are example for this purpose, the original dataframes are so much bigger): And I want to have a new dataframe (or added to df0, whatever), as: As you can see, in the resulting dataframe isn't present the row with A=6 which is present in df1 but not in df0. Also the row with A=0 is duplicated in df1, but not in the result df2. Actually, I'm having trouble with the selection method. I can do this: But I'm not sure how to apply the part of keep with unique data (remember that df1 can contain duplicated data) and add the two columns to the df2 dataset (or add them to df0). I've search here and I don't know see how to apply something like groupby, or even map. Any idea? Thanks! "
            },
            "io": [
                "In [116]: df0\nOut[116]:     \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n\n\nIn [118]: df1\nOut[118]: \n   A  D  E\n0  2  7  2\n1  6  5  5\n2  4  3  2\n3  0  1  0\n4  5  4  6\n5  0  1  0\n",
                "df2: \n   A  B  C  D  E\n0  0  1  0  1  0\n1  2  3  2  7  2\n2  4  5  4  3  2\n3  5  5  5  4  6\n"
            ],
            "answer": {
                "ans_desc": "This is a basic application of (docs): ",
                "code": [
                    "import pandas as pd\ndf2 = pd.merge(df0,df1, left_index=True, right_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "timestamp",
            "string-to-datetime"
        ],
        "owner": {
            "reputation": 1830,
            "user_id": 5976033,
            "user_type": "registered",
            "accept_rate": 29,
            "profile_image": "https://www.gravatar.com/avatar/22324cfa2276b9d4579d35c6987df251?s=128&d=identicon&r=PG&f=1",
            "display_name": "SeaDude",
            "link": "https://stackoverflow.com/users/5976033/seadude"
        },
        "is_answered": true,
        "view_count": 778,
        "accepted_answer_id": 55860017,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1556601120,
        "creation_date": 1556245041,
        "last_edit_date": 1556601120,
        "question_id": 55859947,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55859947/how-to-concatenate-pandas-dataframe-date-and-different-time-formats-to-single-ti",
        "title": "How to concatenate pandas dataframe date and different time formats to single timestamp?",
        "body": "<p>I have two columns in a <code>pandas</code> data frame as outlined below. Notice how some of the <code>EVENT_TIME</code> is in <code>hh.mm.ss</code>, some is in <code>hh:mm:ss AM/PM</code> format. </p>\n\n<p><a href=\"https://i.stack.imgur.com/sNIc3.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/sNIc3.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>When running...</strong></p>\n\n<pre><code>import pandas\n\ndf['EVENT_DATE'] = pd.to_datetime(df['EVENT_DATE'], format='%Y%m%d')\n\nprint(df['EVENT_DATE'])\n</code></pre>\n\n<p>...I can get <code>EVENT_DATE</code> in a consumable (for my purposes) format (e.g. <code>1999-07-28</code>). </p>\n\n<p><strong>But when running...</strong></p>\n\n<pre><code>df['EVENT_TIME'] = pd.to_datetime(df['EVENT_TIME'], format='%H.%M.%S', errors='coerce')\ndf['EVENT_TIME'] = pd.to_datetime(df['EVENT_TIME'], format='%I:%M:%S %p', errors='coerce')\n\nprint(df['EVENT_TIME'])\n</code></pre>\n\n<p><strong>...<code>1900-01-01</code> is added to the times and is not being applied to all rows.</strong></p>\n\n<pre><code>1900-01-01 16:40:00\n1900-01-01 15:55:00\n1900-01-01 14:30:00\n1900-01-01 13:26:00\nNaT\nNaT\nNaT\nNaT\n</code></pre>\n\n<p>How do I concatenate the date and times (which include multiple time formats) in a single timestamp?</p>\n\n<p><strong>Edit1:</strong></p>\n\n<p><strong>@Wen-Ben 's solution got me here:</strong></p>\n\n<pre><code>1      19:53:00\n11     14:30:00\n15     16:30:00\n</code></pre>\n\n<p><strong>Then to concatenate EVENT_DATE and EVENT_TIME, I found this (which works):</strong></p>\n\n<pre><code>df['TIMESTAMP'] = df.apply(lambda r : pd.datetime.combine(r['EVENT_DATE'], r['EVENT_TIME']),1)\n</code></pre>\n\n<p><strong>...results in:</strong></p>\n\n<pre><code>1     1999-07-28 19:53:00\n11    2001-07-28 14:30:00\n15    2002-06-07 16:30:00\n</code></pre>\n\n<p><strong>Next I want to get this into ISO8601 format. So I found this (which works):</strong></p>\n\n<pre><code>pd.to_datetime(df['TIMESTAMP']).apply(lambda x: x.strftime('%Y%m%dT%H:%M%SZ'))\n</code></pre>\n\n<p><strong>...results in:</strong></p>\n\n<pre><code>1      19990728T19:5300Z\n11     20010728T14:3000Z\n15     20020607T16:3000Z\n</code></pre>\n\n<p><strong>HERES MY NEW PROBLEM:</strong></p>\n\n<p>Running <code>print(TIMESTAMP)</code> still shows the concatenated versions (e.g. <code>1999-07-28 19:53:00</code>) instead of the ISO version (e.g.<code>19990728T19:5300Z</code>)</p>\n\n<p><strong>How do I get the ISO8601 column \"added\" to the dataframe?</strong> </p>\n\n<p>Ideally, I want it to take the place of <code>TIMESTAMP</code>. I want it as a transformation of the data, not tacked on as a new column.</p>\n",
        "answer_body": "<p>Using <code>fillna</code></p>\n\n<pre><code>s1=pd.to_datetime(df['EVENT_TIME'], format='%H.%M.%S', errors='coerce')\ns2=pd.to_datetime(df['EVENT_TIME'], format='%I:%M:%S %p', errors='coerce')\ndf['EVENT_TIME']=s1.fillna(s2)\n</code></pre>\n",
        "question_body": "<p>I have two columns in a <code>pandas</code> data frame as outlined below. Notice how some of the <code>EVENT_TIME</code> is in <code>hh.mm.ss</code>, some is in <code>hh:mm:ss AM/PM</code> format. </p>\n\n<p><a href=\"https://i.stack.imgur.com/sNIc3.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/sNIc3.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>When running...</strong></p>\n\n<pre><code>import pandas\n\ndf['EVENT_DATE'] = pd.to_datetime(df['EVENT_DATE'], format='%Y%m%d')\n\nprint(df['EVENT_DATE'])\n</code></pre>\n\n<p>...I can get <code>EVENT_DATE</code> in a consumable (for my purposes) format (e.g. <code>1999-07-28</code>). </p>\n\n<p><strong>But when running...</strong></p>\n\n<pre><code>df['EVENT_TIME'] = pd.to_datetime(df['EVENT_TIME'], format='%H.%M.%S', errors='coerce')\ndf['EVENT_TIME'] = pd.to_datetime(df['EVENT_TIME'], format='%I:%M:%S %p', errors='coerce')\n\nprint(df['EVENT_TIME'])\n</code></pre>\n\n<p><strong>...<code>1900-01-01</code> is added to the times and is not being applied to all rows.</strong></p>\n\n<pre><code>1900-01-01 16:40:00\n1900-01-01 15:55:00\n1900-01-01 14:30:00\n1900-01-01 13:26:00\nNaT\nNaT\nNaT\nNaT\n</code></pre>\n\n<p>How do I concatenate the date and times (which include multiple time formats) in a single timestamp?</p>\n\n<p><strong>Edit1:</strong></p>\n\n<p><strong>@Wen-Ben 's solution got me here:</strong></p>\n\n<pre><code>1      19:53:00\n11     14:30:00\n15     16:30:00\n</code></pre>\n\n<p><strong>Then to concatenate EVENT_DATE and EVENT_TIME, I found this (which works):</strong></p>\n\n<pre><code>df['TIMESTAMP'] = df.apply(lambda r : pd.datetime.combine(r['EVENT_DATE'], r['EVENT_TIME']),1)\n</code></pre>\n\n<p><strong>...results in:</strong></p>\n\n<pre><code>1     1999-07-28 19:53:00\n11    2001-07-28 14:30:00\n15    2002-06-07 16:30:00\n</code></pre>\n\n<p><strong>Next I want to get this into ISO8601 format. So I found this (which works):</strong></p>\n\n<pre><code>pd.to_datetime(df['TIMESTAMP']).apply(lambda x: x.strftime('%Y%m%dT%H:%M%SZ'))\n</code></pre>\n\n<p><strong>...results in:</strong></p>\n\n<pre><code>1      19990728T19:5300Z\n11     20010728T14:3000Z\n15     20020607T16:3000Z\n</code></pre>\n\n<p><strong>HERES MY NEW PROBLEM:</strong></p>\n\n<p>Running <code>print(TIMESTAMP)</code> still shows the concatenated versions (e.g. <code>1999-07-28 19:53:00</code>) instead of the ISO version (e.g.<code>19990728T19:5300Z</code>)</p>\n\n<p><strong>How do I get the ISO8601 column \"added\" to the dataframe?</strong> </p>\n\n<p>Ideally, I want it to take the place of <code>TIMESTAMP</code>. I want it as a transformation of the data, not tacked on as a new column.</p>\n",
        "formatted_input": {
            "qid": 55859947,
            "link": "https://stackoverflow.com/questions/55859947/how-to-concatenate-pandas-dataframe-date-and-different-time-formats-to-single-ti",
            "question": {
                "title": "How to concatenate pandas dataframe date and different time formats to single timestamp?",
                "ques_desc": "I have two columns in a data frame as outlined below. Notice how some of the is in , some is in format. When running... ...I can get in a consumable (for my purposes) format (e.g. ). But when running... ... is added to the times and is not being applied to all rows. How do I concatenate the date and times (which include multiple time formats) in a single timestamp? Edit1: @Wen-Ben 's solution got me here: Then to concatenate EVENT_DATE and EVENT_TIME, I found this (which works): ...results in: Next I want to get this into ISO8601 format. So I found this (which works): ...results in: HERES MY NEW PROBLEM: Running still shows the concatenated versions (e.g. ) instead of the ISO version (e.g.) How do I get the ISO8601 column \"added\" to the dataframe? Ideally, I want it to take the place of . I want it as a transformation of the data, not tacked on as a new column. "
            },
            "io": [
                "1      19:53:00\n11     14:30:00\n15     16:30:00\n",
                "1     1999-07-28 19:53:00\n11    2001-07-28 14:30:00\n15    2002-06-07 16:30:00\n"
            ],
            "answer": {
                "ans_desc": "Using ",
                "code": [
                    "s1=pd.to_datetime(df['EVENT_TIME'], format='%H.%M.%S', errors='coerce')\ns2=pd.to_datetime(df['EVENT_TIME'], format='%I:%M:%S %p', errors='coerce')\ndf['EVENT_TIME']=s1.fillna(s2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "recommendation-engine"
        ],
        "owner": {
            "reputation": 475,
            "user_id": 9296990,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/dc468e3e5ffb875693abe69e5f379f97?s=128&d=identicon&r=PG&f=1",
            "display_name": "mrsquid",
            "link": "https://stackoverflow.com/users/9296990/mrsquid"
        },
        "is_answered": true,
        "view_count": 1062,
        "accepted_answer_id": 48825674,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1556508832,
        "creation_date": 1518761792,
        "last_edit_date": 1518764457,
        "question_id": 48821092,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/48821092/predicting-values-in-movie-recommendations",
        "title": "Predicting Values in Movie Recommendations",
        "body": "<p>I've been trying to create a recommendation system using the movielens dataset in python. My goal is to determine the similarity between users and then output the top five recommended movies for each user in this format:</p>\n\n<pre><code>User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\nUser-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\n</code></pre>\n\n<p>The data I am using for now is this <a href=\"https://drive.google.com/open?id=1IxUlfxfNOcfAbGc3l6SVbPKw8XDWWist\" rel=\"nofollow noreferrer\">ratings</a> dataset. </p>\n\n<p>Here is the code so far:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn import cross_validation as cv\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import svds\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('ratings.csv')\n\n\ndf.drop('timestamp', axis=1, inplace=True)\nn_users = df.userId.unique().shape[0]\nn_items = df.movieId.unique().shape[0]\n\n#Pivot table so users are rows and movies are columns, ratings are then values\ndf = df.pivot(index='userId', columns='movieId', values='rating')\n\n#subtract row mean from each rating to center data\ndf = df.sub(df.mean(axis=1), axis=0)\n\n#copy to fill in predictions\nc1 = df.copy()\nc1 = c1.fillna('a')\n\n#second copy to find which values were filled in and return the highest rated values\nc2 = c1.copy()\n\n#fill NAN with 0\ndf = df.fillna(0)\n\n\n#Get cosine similarity between rows\nsimilarity = pd.DataFrame(cosine_similarity(df))\n\n#get top 5 similar profiles\ntmp = similarity.apply(lambda row: sorted(zip(similarity.columns, row), key=lambda c: -c[1]), axis=1)\ntmp = tmp.ix[:,1:6]\nl = np.array(tmp)\n\n##Prediction function - does not work needs improvement\ndef predict(df, c1, l):\nfor i in range(c1.shape[0]):\n    for j in range(i+1, c1.shape[1]):\n        try:\n            if c1.iloc[i][j] == 'a':\n                num = df[l[i][0][0]]*l[i][0][1] + df[l[i][1][0]]*l[i][1][1] + df[l[i][2][0]]*l[i][2][1] + df[l[i][3][0]]*l[i][3][1] + df[l[i][4][0]]*l[i][4][1]\n                den = l[i][0][1] + l[i][1][0] + l[i][2][0] + l[i][3][0] + l[i][4][0]\n                c1[i][j] = num/den\n        except:\n            pass\nreturn c1\n\nres = predict(df, c1, l)\nprint(res)\n\nres = predict(df, c1, l)\nprint(res)\n</code></pre>\n\n<p>I am trying to implement the prediction function. I want to predict the missing values and add them to c1. I am trying to implement <a href=\"https://drive.google.com/open?id=13aHs_uFEuKg9-m11KtVybAgnnZw1ad21\" rel=\"nofollow noreferrer\">this</a>. The formula as well as an example of how it should be used is in the picture. As you can see it uses the similarity scores of the most similar users.</p>\n\n<p>The output of similarity looks like this: For example here is user1's similarity:</p>\n\n<pre><code>[(34, 0.19269904365720053) (196, 0.19187531680008307)\n (538, 0.14932027335788825) (67, 0.14093020024386654)\n (419, 0.11034407313683092) (319, 0.10055810007385564)]\n</code></pre>\n\n<p>I need help using these similarities in the prediction function to predict missing movie ratings. If that is solved I will then have to find the top 5 recommended movies for each user and output them in the format above.</p>\n\n<p>I currently need help with the prediction function. Any advice helps. Please let me know if you need any more information or clarification.</p>\n\n<p>Thank you for reading</p>\n",
        "answer_body": "<p>First of all, vectorisation makes complex problems much easier. here are a few suggestion to improve what you already have</p>\n\n<ol>\n<li>use the userID as as columns in the pivot table, this makes the example for the prediction easier to see</li>\n<li>NaN stands for missing value, which is conceptually not the same as 0, in this particular case a high negative number will do and will only bee needed when using the cosine similarity function</li>\n<li>take advantage of pandas advanced features, e.g. to keep the original values but add the predictions, fillna can be used</li>\n<li>when constructing the <code>similarity</code> dataframe be sure to keep track of the useIds, you can do so by setting the index and columns to <code>df.columns</code></li>\n</ol>\n\n<p>Here is my edited version of the code including predict implementation:</p>\n\n<p>```</p>\n\n<pre><code>import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import scale\n\n\ndef predict(l):\n    # finds the userIds corresponding to the top 5 similarities\n    # calculate the prediction according to the formula\n    return (df[l.index] * l).sum(axis=1) / l.sum()\n\n\n# use userID as columns for convinience when interpretering the forumla\ndf = pd.read_csv('ratings.csv').pivot(columns='userId',\n                                                index='movieId',\n                                                values='rating')\n\nsimilarity = pd.DataFrame(cosine_similarity(\n    scale(df.T.fillna(-1000))),\n    index=df.columns,\n    columns=df.columns)\n# iterate each column (userID),\n# for each userID find the highest five similarities\n# and use to calculate the prediction for that user,\n# use fillna so that original ratings dont change\n\nres = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in col.fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\nprint(res)\n</code></pre>\n\n<p>```</p>\n\n<p>here is a sample of the output</p>\n\n<pre><code>userId\n1    1172 1953 2105 1339 1029\n2           17 39 150 222 265\n3      318 356 1197 2959 3949\n4          34 112 141 260 296\n5    597 1035 1380 2081 33166\ndtype: object\n</code></pre>\n\n<h1>Edit</h1>\n\n<p>The code above will recommends the top 5 regardless of whether the user already watched/rated them. to fix this we can reset the values of the original ratings to 0 when choosing the recommendations as shown below\\</p>\n\n<pre><code>res = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in (0 * col).fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\n</code></pre>\n\n<p>The output</p>\n\n<pre><code>userId\n1           2278 4085 3072 585 256\n2               595 597 32 344 316\n3              590 457 150 380 253\n4         1375 2571 2011 1287 2455\n5              480 590 457 296 165\n6          1196 7064 26151 260 480\n....\n</code></pre>\n",
        "question_body": "<p>I've been trying to create a recommendation system using the movielens dataset in python. My goal is to determine the similarity between users and then output the top five recommended movies for each user in this format:</p>\n\n<pre><code>User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\nUser-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\n</code></pre>\n\n<p>The data I am using for now is this <a href=\"https://drive.google.com/open?id=1IxUlfxfNOcfAbGc3l6SVbPKw8XDWWist\" rel=\"nofollow noreferrer\">ratings</a> dataset. </p>\n\n<p>Here is the code so far:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn import cross_validation as cv\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import svds\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('ratings.csv')\n\n\ndf.drop('timestamp', axis=1, inplace=True)\nn_users = df.userId.unique().shape[0]\nn_items = df.movieId.unique().shape[0]\n\n#Pivot table so users are rows and movies are columns, ratings are then values\ndf = df.pivot(index='userId', columns='movieId', values='rating')\n\n#subtract row mean from each rating to center data\ndf = df.sub(df.mean(axis=1), axis=0)\n\n#copy to fill in predictions\nc1 = df.copy()\nc1 = c1.fillna('a')\n\n#second copy to find which values were filled in and return the highest rated values\nc2 = c1.copy()\n\n#fill NAN with 0\ndf = df.fillna(0)\n\n\n#Get cosine similarity between rows\nsimilarity = pd.DataFrame(cosine_similarity(df))\n\n#get top 5 similar profiles\ntmp = similarity.apply(lambda row: sorted(zip(similarity.columns, row), key=lambda c: -c[1]), axis=1)\ntmp = tmp.ix[:,1:6]\nl = np.array(tmp)\n\n##Prediction function - does not work needs improvement\ndef predict(df, c1, l):\nfor i in range(c1.shape[0]):\n    for j in range(i+1, c1.shape[1]):\n        try:\n            if c1.iloc[i][j] == 'a':\n                num = df[l[i][0][0]]*l[i][0][1] + df[l[i][1][0]]*l[i][1][1] + df[l[i][2][0]]*l[i][2][1] + df[l[i][3][0]]*l[i][3][1] + df[l[i][4][0]]*l[i][4][1]\n                den = l[i][0][1] + l[i][1][0] + l[i][2][0] + l[i][3][0] + l[i][4][0]\n                c1[i][j] = num/den\n        except:\n            pass\nreturn c1\n\nres = predict(df, c1, l)\nprint(res)\n\nres = predict(df, c1, l)\nprint(res)\n</code></pre>\n\n<p>I am trying to implement the prediction function. I want to predict the missing values and add them to c1. I am trying to implement <a href=\"https://drive.google.com/open?id=13aHs_uFEuKg9-m11KtVybAgnnZw1ad21\" rel=\"nofollow noreferrer\">this</a>. The formula as well as an example of how it should be used is in the picture. As you can see it uses the similarity scores of the most similar users.</p>\n\n<p>The output of similarity looks like this: For example here is user1's similarity:</p>\n\n<pre><code>[(34, 0.19269904365720053) (196, 0.19187531680008307)\n (538, 0.14932027335788825) (67, 0.14093020024386654)\n (419, 0.11034407313683092) (319, 0.10055810007385564)]\n</code></pre>\n\n<p>I need help using these similarities in the prediction function to predict missing movie ratings. If that is solved I will then have to find the top 5 recommended movies for each user and output them in the format above.</p>\n\n<p>I currently need help with the prediction function. Any advice helps. Please let me know if you need any more information or clarification.</p>\n\n<p>Thank you for reading</p>\n",
        "formatted_input": {
            "qid": 48821092,
            "link": "https://stackoverflow.com/questions/48821092/predicting-values-in-movie-recommendations",
            "question": {
                "title": "Predicting Values in Movie Recommendations",
                "ques_desc": "I've been trying to create a recommendation system using the movielens dataset in python. My goal is to determine the similarity between users and then output the top five recommended movies for each user in this format: The data I am using for now is this ratings dataset. Here is the code so far: I am trying to implement the prediction function. I want to predict the missing values and add them to c1. I am trying to implement this. The formula as well as an example of how it should be used is in the picture. As you can see it uses the similarity scores of the most similar users. The output of similarity looks like this: For example here is user1's similarity: I need help using these similarities in the prediction function to predict missing movie ratings. If that is solved I will then have to find the top 5 recommended movies for each user and output them in the format above. I currently need help with the prediction function. Any advice helps. Please let me know if you need any more information or clarification. Thank you for reading "
            },
            "io": [
                "User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\nUser-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\n",
                "[(34, 0.19269904365720053) (196, 0.19187531680008307)\n (538, 0.14932027335788825) (67, 0.14093020024386654)\n (419, 0.11034407313683092) (319, 0.10055810007385564)]\n"
            ],
            "answer": {
                "ans_desc": "First of all, vectorisation makes complex problems much easier. here are a few suggestion to improve what you already have use the userID as as columns in the pivot table, this makes the example for the prediction easier to see NaN stands for missing value, which is conceptually not the same as 0, in this particular case a high negative number will do and will only bee needed when using the cosine similarity function take advantage of pandas advanced features, e.g. to keep the original values but add the predictions, fillna can be used when constructing the dataframe be sure to keep track of the useIds, you can do so by setting the index and columns to Here is my edited version of the code including predict implementation: ``` ``` here is a sample of the output Edit The code above will recommends the top 5 regardless of whether the user already watched/rated them. to fix this we can reset the values of the original ratings to 0 when choosing the recommendations as shown below\\ The output ",
                "code": [
                    "import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import scale\n\n\ndef predict(l):\n    # finds the userIds corresponding to the top 5 similarities\n    # calculate the prediction according to the formula\n    return (df[l.index] * l).sum(axis=1) / l.sum()\n\n\n# use userID as columns for convinience when interpretering the forumla\ndf = pd.read_csv('ratings.csv').pivot(columns='userId',\n                                                index='movieId',\n                                                values='rating')\n\nsimilarity = pd.DataFrame(cosine_similarity(\n    scale(df.T.fillna(-1000))),\n    index=df.columns,\n    columns=df.columns)\n# iterate each column (userID),\n# for each userID find the highest five similarities\n# and use to calculate the prediction for that user,\n# use fillna so that original ratings dont change\n\nres = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in col.fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\nprint(res)\n",
                    "res = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in (0 * col).fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3653,
            "user_id": 1506850,
            "user_type": "registered",
            "accept_rate": 94,
            "profile_image": "https://www.gravatar.com/avatar/529adde7e6c422b0ab9b990d9b09e8da?s=128&d=identicon&r=PG&f=1",
            "display_name": "00__00__00",
            "link": "https://stackoverflow.com/users/1506850/00-00-00"
        },
        "is_answered": true,
        "view_count": 391,
        "accepted_answer_id": 55829749,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1556109044,
        "creation_date": 1556107323,
        "question_id": 55829605,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55829605/split-list-element-in-dataframe-over-multiple-rows",
        "title": "Split list element in dataframe over multiple rows",
        "body": "<p>I have a df.</p>\n\n<pre><code>df=pd.DataFrame(data=[[301,301,302,303],[['a'],['b','c'],['e','f',33,'Z'],42],index=['id','foo']).T\n</code></pre>\n\n<p>I would like to get to a second dataframe, with only scalar values in <code>foo</code>.\nIf and only if the original values was a list, I would like to spread it over multiple new rows, with the other values duplicated.</p>\n\n<p>e.g from:</p>\n\n<pre><code> id     foo\n0  301     [a]\n1  301  [b, c]\n2  302  [e, f,33,'Z']\n3  303   42\n</code></pre>\n\n<p>to:</p>\n\n<pre><code> id     foo\n0  301     a\n1  301     b\n1  301     c \n2  302     e\n2  302     f\n2  302     33\n2  302     Z\n3  303     42\n</code></pre>\n\n<p>In <a href=\"https://stackoverflow.com/questions/42668399/split-set-values-from-pandas-dataframe-cell-over-multiple-rows\">Split set values from Pandas dataframe cell over multiple rows</a>,\nI have learned how to do this for one columns, but how to handle the case where df has multiple columns which need to be duplicated as <code>id</code>?</p>\n",
        "answer_body": "<p>If you want avoid use <code>apply(pd.Series)</code> because <a href=\"https://stackoverflow.com/a/35491399/2901002\">slow</a>, here is another solution - convert non lists values to one element list first and then apply solution:</p>\n\n<pre><code>df['foo']  = [x if isinstance(x, list) else [x] for x in df['foo']]\n\nfrom itertools import chain\n\ndf = pd.DataFrame({\n    'id' : df['id'].values.repeat(df['foo'].str.len()),\n    'foo' : list(chain.from_iterable(df['foo'].tolist()))\n\n})\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>L  = [x if isinstance(x, list) else [x] for x in df['foo']]\n\nfrom itertools import chain\n\ndf = pd.DataFrame({\n    'id' : df['id'].values.repeat([len(x) for x in L]),\n    'foo' : list(chain.from_iterable(L))\n\n})\nprint (df)\n    id foo\n0  301   a\n1  301   b\n2  301   c\n3  302   e\n4  302   f\n5  302  33\n6  302   Z\n7  303  42\n</code></pre>\n\n<p>If small data or performance is not important - solution with <code>pop</code> for extract column <code>foo</code>:</p>\n\n<pre><code>s = df.pop('foo').apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.join(s).reset_index(drop=True)\n</code></pre>\n\n<p>Or solution with <code>drop</code>:</p>\n\n<pre><code>s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.drop('foo', axis=1).join(s).reset_index(drop=True)\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n\n    id foo\n0  301   a\n1  301   b\n2  301   c\n3  302   e\n4  302   f\n5  302  33\n6  302   Z\n7  303  42\n</code></pre>\n\n<hr>\n\n<pre><code>df=pd.DataFrame(data=[[301,301,302,303],[['a'],['b','c'],['e','f',33,'Z'],42]],index=['id','foo']).T\n\ndf = pd.concat([df] * 1000, ignore_index=True)\n\ndef f(df):\n    s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\n    return df.drop('foo', axis=1).join(s).reset_index(drop=True)\n\n\nIn [241]: %timeit (f(df))\n814 ms \u00b1 11.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [242]: %%timeit\n     ...: L  = [x if isinstance(x, list) else [x] for x in df['foo']]\n     ...: \n     ...: from itertools import chain\n     ...: \n     ...: pd.DataFrame({\n     ...:     'id' : df['id'].values.repeat([len(x) for x in L]),\n     ...:     'foo' : list(chain.from_iterable(L))\n     ...: \n     ...: })\n     ...: \n2.6 ms \u00b1 15.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n",
        "question_body": "<p>I have a df.</p>\n\n<pre><code>df=pd.DataFrame(data=[[301,301,302,303],[['a'],['b','c'],['e','f',33,'Z'],42],index=['id','foo']).T\n</code></pre>\n\n<p>I would like to get to a second dataframe, with only scalar values in <code>foo</code>.\nIf and only if the original values was a list, I would like to spread it over multiple new rows, with the other values duplicated.</p>\n\n<p>e.g from:</p>\n\n<pre><code> id     foo\n0  301     [a]\n1  301  [b, c]\n2  302  [e, f,33,'Z']\n3  303   42\n</code></pre>\n\n<p>to:</p>\n\n<pre><code> id     foo\n0  301     a\n1  301     b\n1  301     c \n2  302     e\n2  302     f\n2  302     33\n2  302     Z\n3  303     42\n</code></pre>\n\n<p>In <a href=\"https://stackoverflow.com/questions/42668399/split-set-values-from-pandas-dataframe-cell-over-multiple-rows\">Split set values from Pandas dataframe cell over multiple rows</a>,\nI have learned how to do this for one columns, but how to handle the case where df has multiple columns which need to be duplicated as <code>id</code>?</p>\n",
        "formatted_input": {
            "qid": 55829605,
            "link": "https://stackoverflow.com/questions/55829605/split-list-element-in-dataframe-over-multiple-rows",
            "question": {
                "title": "Split list element in dataframe over multiple rows",
                "ques_desc": "I have a df. I would like to get to a second dataframe, with only scalar values in . If and only if the original values was a list, I would like to spread it over multiple new rows, with the other values duplicated. e.g from: to: In Split set values from Pandas dataframe cell over multiple rows, I have learned how to do this for one columns, but how to handle the case where df has multiple columns which need to be duplicated as ? "
            },
            "io": [
                " id     foo\n0  301     [a]\n1  301  [b, c]\n2  302  [e, f,33,'Z']\n3  303   42\n",
                " id     foo\n0  301     a\n1  301     b\n1  301     c \n2  302     e\n2  302     f\n2  302     33\n2  302     Z\n3  303     42\n"
            ],
            "answer": {
                "ans_desc": "If you want avoid use because slow, here is another solution - convert non lists values to one element list first and then apply solution: Or: If small data or performance is not important - solution with for extract column : Or solution with : ",
                "code": [
                    "df['foo']  = [x if isinstance(x, list) else [x] for x in df['foo']]\n\nfrom itertools import chain\n\ndf = pd.DataFrame({\n    'id' : df['id'].values.repeat(df['foo'].str.len()),\n    'foo' : list(chain.from_iterable(df['foo'].tolist()))\n\n})\n",
                    "s = df.pop('foo').apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.join(s).reset_index(drop=True)\n",
                    "s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.drop('foo', axis=1).join(s).reset_index(drop=True)\n",
                    "df=pd.DataFrame(data=[[301,301,302,303],[['a'],['b','c'],['e','f',33,'Z'],42]],index=['id','foo']).T\n\ndf = pd.concat([df] * 1000, ignore_index=True)\n\ndef f(df):\n    s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\n    return df.drop('foo', axis=1).join(s).reset_index(drop=True)\n\n\nIn [241]: %timeit (f(df))\n814 ms \u00b1 11.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [242]: %%timeit\n     ...: L  = [x if isinstance(x, list) else [x] for x in df['foo']]\n     ...: \n     ...: from itertools import chain\n     ...: \n     ...: pd.DataFrame({\n     ...:     'id' : df['id'].values.repeat([len(x) for x in L]),\n     ...:     'foo' : list(chain.from_iterable(L))\n     ...: \n     ...: })\n     ...: \n2.6 ms \u00b1 15.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user11040244"
        },
        "is_answered": true,
        "view_count": 404,
        "accepted_answer_id": 55803401,
        "answer_count": 1,
        "score": 5,
        "last_activity_date": 1555991576,
        "creation_date": 1555985364,
        "last_edit_date": 1555986066,
        "question_id": 55803354,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55803354/add-values-to-existing-rows-dataframe",
        "title": "Add values to existing rows -DataFrame",
        "body": "<p>I'm appending some weather data (from json- dict) - in Japanese to DataFrame.</p>\n\n<p>I would like to have something like this </p>\n\n<pre><code>        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds    \u98a8\u901f: 2.1m\n 1     NaN          \u5411\u304d: 230\n</code></pre>\n\n<p>But I have this</p>\n\n<pre><code>        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds        NaN\n 1      NaN          \u98a8\u901f: 2.1m\n 2      NaN           \u5411\u304d: 230\n</code></pre>\n\n<p>How Could I change the Codes to make it like that?\nHere is the code</p>\n\n<pre><code>df = pd.DataFrame(columns=['\u5929\u6c17','\u98a8'])\ndf = df.append({'\u5929\u6c17': weather_status}, ignore_index=True) # \u72b6\u614b: Clouds - value\ndf = df.append({'\u98a8': wind_speed}, ignore_index=True) # \u98a8\u901f: 2.1m -value\ndf = df.append({'\u98a8': wind_deg}, ignore_index=True) # \u5411\u304d: 230 -value\nprint(df)\n</code></pre>\n",
        "answer_body": "<p>One way could be:</p>\n\n<pre><code>df = pd.DataFrame(columns=['\u5929\u6c17','\u98a8'])\ndf = df.append({'\u5929\u6c17': weather_status, '\u98a8': wind_speed}, ignore_index=True)\ndf = df.append({'\u98a8': wind_deg}, ignore_index=True)\nprint(df)\n</code></pre>\n\n<p>And print without index</p>\n\n<pre><code>print(df.to_string(index=False))\n</code></pre>\n",
        "question_body": "<p>I'm appending some weather data (from json- dict) - in Japanese to DataFrame.</p>\n\n<p>I would like to have something like this </p>\n\n<pre><code>        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds    \u98a8\u901f: 2.1m\n 1     NaN          \u5411\u304d: 230\n</code></pre>\n\n<p>But I have this</p>\n\n<pre><code>        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds        NaN\n 1      NaN          \u98a8\u901f: 2.1m\n 2      NaN           \u5411\u304d: 230\n</code></pre>\n\n<p>How Could I change the Codes to make it like that?\nHere is the code</p>\n\n<pre><code>df = pd.DataFrame(columns=['\u5929\u6c17','\u98a8'])\ndf = df.append({'\u5929\u6c17': weather_status}, ignore_index=True) # \u72b6\u614b: Clouds - value\ndf = df.append({'\u98a8': wind_speed}, ignore_index=True) # \u98a8\u901f: 2.1m -value\ndf = df.append({'\u98a8': wind_deg}, ignore_index=True) # \u5411\u304d: 230 -value\nprint(df)\n</code></pre>\n",
        "formatted_input": {
            "qid": 55803354,
            "link": "https://stackoverflow.com/questions/55803354/add-values-to-existing-rows-dataframe",
            "question": {
                "title": "Add values to existing rows -DataFrame",
                "ques_desc": "I'm appending some weather data (from json- dict) - in Japanese to DataFrame. I would like to have something like this But I have this How Could I change the Codes to make it like that? Here is the code "
            },
            "io": [
                "        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds    \u98a8\u901f: 2.1m\n 1     NaN          \u5411\u304d: 230\n",
                "        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds        NaN\n 1      NaN          \u98a8\u901f: 2.1m\n 2      NaN           \u5411\u304d: 230\n"
            ],
            "answer": {
                "ans_desc": "One way could be: And print without index ",
                "code": [
                    "df = pd.DataFrame(columns=['\u5929\u6c17','\u98a8'])\ndf = df.append({'\u5929\u6c17': weather_status, '\u98a8': wind_speed}, ignore_index=True)\ndf = df.append({'\u98a8': wind_deg}, ignore_index=True)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "series"
        ],
        "owner": {
            "reputation": 53,
            "user_id": 4524799,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/anK00.png?s=128&g=1",
            "display_name": "Necas",
            "link": "https://stackoverflow.com/users/4524799/necas"
        },
        "is_answered": true,
        "view_count": 1612,
        "accepted_answer_id": 55777822,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1555795334,
        "creation_date": 1555790802,
        "last_edit_date": 1555792332,
        "question_id": 55777300,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55777300/how-to-print-rows-if-a-list-of-values-appear-in-any-column-of-pandas-dataframe",
        "title": "How to print rows if a list of values appear in any column of pandas dataframe",
        "body": "<p>How to print rows if values appear in any column of pandas dataframe</p>\n\n<p>I would like to print all rows of a dataframe where I find some values from a list of values in any of the columns. The dataframe follows this structure:</p>\n\n<pre><code>1476 13/03/2013  4 10 26 37 47 57\n1475 09/03/2013 12 13 37 44 48 51\n1474 06/03/2013  1  2  3 11 28 43\n1473 02/03/2013  2 12 33 57 58 60\n1472 27/02/2013 12 18 23 25 45 50\n1471 23/02/2013 10 25 33 36 40 58\n1470 20/02/2013  2 34 36 38 51 55\n1469 16/02/2013  4 13 35 54 56 58\n1468 13/02/2013  1  2 10 19 20 37\n1467 09/02/2013 23 24 26 41 52 53\n1466 06/02/2013  4  6 13 34 37 51\n1465 02/02/2013  6 11 16 26 44 53\n1464 30/01/2013  2 24 32 50 54 59\n1463 26/01/2013 13 22 28 29 40 48\n1462 23/01/2013  5  9 25 27 38 40\n1461 19/01/2013 31 36 44 47 49 54\n1460 16/01/2013  4 14 27 38 50 52\n1459 12/01/2013  2  6 30 34 35 52\n1458 09/01/2013  2  4 16 33 44 51\n1457 05/01/2013 15 16 34 42 46 59\n1456 02/01/2013  6  8 14 26 36 40\n1455 31/12/2012 14 32 33 36 41 52\n1454 22/12/2012  4 27 29 41 48 52\n1453 20/12/2012  6 13 25 32 47 57\n</code></pre>\n\n<p>First: I have a Series of values with size 3 that I get from a combinatory of 6 different values.</p>\n\n<p>Second: I have a dataframe with 2143 rows. I want to check if in any of these rows, I have those three values in any sort of order in the columns.</p>\n\n<pre><code>from itertools import combinations, groupby\nfrom pandas import Series\nfrom operator import itemgetter\n\ninputlist = [2,12,35,51,57,58]\ncombined = combinations(inputlist, 3)\n\nseries = Series(list(g) for k, g in groupby(combined, key=itemgetter(0)))\n</code></pre>\n\n<p>Gave me this:</p>\n\n<pre><code>0    [(2, 12, 35), (2, 12, 51), (2, 12, 57), (2, 12...\n1    [(12, 35, 51), (12, 35, 57), (12, 35, 58), (12...\n2           [(35, 51, 57), (35, 51, 58), (35, 57, 58)]\n3                                       [(51, 57, 58)]\n\n</code></pre>\n\n<p>I just tried the query command and this is what I've got:</p>\n\n<p>df_ordered.query('_1 == 2 &amp; _2 == 12')</p>\n\n<pre><code>ID      DATE        _1  _2  _3  _4  _5  _6\n\n405     2002-10-19  2   12  32  38  47  48\n615     2004-11-17  2   12  16  24  26  54\n732     2006-01-28  2   12  26  31  43  46\n1361    2012-02-11  2   12  19  22  36  58\n1472    2013-03-02  2   12  33  57  58  60\n1523    2013-08-24  2   12  40  46  52  53\n1711    2015-06-10  2   12  19  29  50  59\n2142    2019-04-17  2   12  35  51  57  58 \n\n</code></pre>\n\n<p>Now, I want to expand the same thing, but I want to look at all those columns and find any of those values. </p>\n\n<p>I also didn't know how to plug those series into a loop to find the values into the query statement.</p>\n\n<p>EDIT: I tried the <code>isin</code> command, but I have no ideia how to expand it to the 6 columns I have.</p>\n\n<pre><code>df[df._1.isin(combined)]\n</code></pre>\n",
        "answer_body": "<p>IIUC, you could try creating a <code>boolean mask</code> with a list comprehension using <a href=\"https://python-reference.readthedocs.io/en/latest/docs/sets/op_issuperset.html\" rel=\"nofollow noreferrer\"><code>set.issuperset</code></a>, <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\" rel=\"nofollow noreferrer\"><code>numpy.reshape</code></a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.any.html\" rel=\"nofollow noreferrer\"><code>numpy.any</code></a>:</p>\n\n<pre><code>import numpy as np\nfrom itertools import combinations\n\ninputlist = [2,12,35,51,57,58]\ncombined = np.array(list(combinations(inputlist, 3)))\n\nmask = (np.array([set(row).issuperset(c) for row in df.values for c in combined])\n        .reshape(len(df), -1).any(1))\n\nprint(df[mask])\n</code></pre>\n\n<p>[out]</p>\n\n<pre><code>     ID        DATE  _1  _2  _3  _4  _5  _6\n3  1473  02/03/2013   2  12  33  57  58  60\n</code></pre>\n",
        "question_body": "<p>How to print rows if values appear in any column of pandas dataframe</p>\n\n<p>I would like to print all rows of a dataframe where I find some values from a list of values in any of the columns. The dataframe follows this structure:</p>\n\n<pre><code>1476 13/03/2013  4 10 26 37 47 57\n1475 09/03/2013 12 13 37 44 48 51\n1474 06/03/2013  1  2  3 11 28 43\n1473 02/03/2013  2 12 33 57 58 60\n1472 27/02/2013 12 18 23 25 45 50\n1471 23/02/2013 10 25 33 36 40 58\n1470 20/02/2013  2 34 36 38 51 55\n1469 16/02/2013  4 13 35 54 56 58\n1468 13/02/2013  1  2 10 19 20 37\n1467 09/02/2013 23 24 26 41 52 53\n1466 06/02/2013  4  6 13 34 37 51\n1465 02/02/2013  6 11 16 26 44 53\n1464 30/01/2013  2 24 32 50 54 59\n1463 26/01/2013 13 22 28 29 40 48\n1462 23/01/2013  5  9 25 27 38 40\n1461 19/01/2013 31 36 44 47 49 54\n1460 16/01/2013  4 14 27 38 50 52\n1459 12/01/2013  2  6 30 34 35 52\n1458 09/01/2013  2  4 16 33 44 51\n1457 05/01/2013 15 16 34 42 46 59\n1456 02/01/2013  6  8 14 26 36 40\n1455 31/12/2012 14 32 33 36 41 52\n1454 22/12/2012  4 27 29 41 48 52\n1453 20/12/2012  6 13 25 32 47 57\n</code></pre>\n\n<p>First: I have a Series of values with size 3 that I get from a combinatory of 6 different values.</p>\n\n<p>Second: I have a dataframe with 2143 rows. I want to check if in any of these rows, I have those three values in any sort of order in the columns.</p>\n\n<pre><code>from itertools import combinations, groupby\nfrom pandas import Series\nfrom operator import itemgetter\n\ninputlist = [2,12,35,51,57,58]\ncombined = combinations(inputlist, 3)\n\nseries = Series(list(g) for k, g in groupby(combined, key=itemgetter(0)))\n</code></pre>\n\n<p>Gave me this:</p>\n\n<pre><code>0    [(2, 12, 35), (2, 12, 51), (2, 12, 57), (2, 12...\n1    [(12, 35, 51), (12, 35, 57), (12, 35, 58), (12...\n2           [(35, 51, 57), (35, 51, 58), (35, 57, 58)]\n3                                       [(51, 57, 58)]\n\n</code></pre>\n\n<p>I just tried the query command and this is what I've got:</p>\n\n<p>df_ordered.query('_1 == 2 &amp; _2 == 12')</p>\n\n<pre><code>ID      DATE        _1  _2  _3  _4  _5  _6\n\n405     2002-10-19  2   12  32  38  47  48\n615     2004-11-17  2   12  16  24  26  54\n732     2006-01-28  2   12  26  31  43  46\n1361    2012-02-11  2   12  19  22  36  58\n1472    2013-03-02  2   12  33  57  58  60\n1523    2013-08-24  2   12  40  46  52  53\n1711    2015-06-10  2   12  19  29  50  59\n2142    2019-04-17  2   12  35  51  57  58 \n\n</code></pre>\n\n<p>Now, I want to expand the same thing, but I want to look at all those columns and find any of those values. </p>\n\n<p>I also didn't know how to plug those series into a loop to find the values into the query statement.</p>\n\n<p>EDIT: I tried the <code>isin</code> command, but I have no ideia how to expand it to the 6 columns I have.</p>\n\n<pre><code>df[df._1.isin(combined)]\n</code></pre>\n",
        "formatted_input": {
            "qid": 55777300,
            "link": "https://stackoverflow.com/questions/55777300/how-to-print-rows-if-a-list-of-values-appear-in-any-column-of-pandas-dataframe",
            "question": {
                "title": "How to print rows if a list of values appear in any column of pandas dataframe",
                "ques_desc": "How to print rows if values appear in any column of pandas dataframe I would like to print all rows of a dataframe where I find some values from a list of values in any of the columns. The dataframe follows this structure: First: I have a Series of values with size 3 that I get from a combinatory of 6 different values. Second: I have a dataframe with 2143 rows. I want to check if in any of these rows, I have those three values in any sort of order in the columns. Gave me this: I just tried the query command and this is what I've got: df_ordered.query('_1 == 2 & _2 == 12') Now, I want to expand the same thing, but I want to look at all those columns and find any of those values. I also didn't know how to plug those series into a loop to find the values into the query statement. EDIT: I tried the command, but I have no ideia how to expand it to the 6 columns I have. "
            },
            "io": [
                "1476 13/03/2013  4 10 26 37 47 57\n1475 09/03/2013 12 13 37 44 48 51\n1474 06/03/2013  1  2  3 11 28 43\n1473 02/03/2013  2 12 33 57 58 60\n1472 27/02/2013 12 18 23 25 45 50\n1471 23/02/2013 10 25 33 36 40 58\n1470 20/02/2013  2 34 36 38 51 55\n1469 16/02/2013  4 13 35 54 56 58\n1468 13/02/2013  1  2 10 19 20 37\n1467 09/02/2013 23 24 26 41 52 53\n1466 06/02/2013  4  6 13 34 37 51\n1465 02/02/2013  6 11 16 26 44 53\n1464 30/01/2013  2 24 32 50 54 59\n1463 26/01/2013 13 22 28 29 40 48\n1462 23/01/2013  5  9 25 27 38 40\n1461 19/01/2013 31 36 44 47 49 54\n1460 16/01/2013  4 14 27 38 50 52\n1459 12/01/2013  2  6 30 34 35 52\n1458 09/01/2013  2  4 16 33 44 51\n1457 05/01/2013 15 16 34 42 46 59\n1456 02/01/2013  6  8 14 26 36 40\n1455 31/12/2012 14 32 33 36 41 52\n1454 22/12/2012  4 27 29 41 48 52\n1453 20/12/2012  6 13 25 32 47 57\n",
                "0    [(2, 12, 35), (2, 12, 51), (2, 12, 57), (2, 12...\n1    [(12, 35, 51), (12, 35, 57), (12, 35, 58), (12...\n2           [(35, 51, 57), (35, 51, 58), (35, 57, 58)]\n3                                       [(51, 57, 58)]\n\n"
            ],
            "answer": {
                "ans_desc": "IIUC, you could try creating a with a list comprehension using , and : [out] ",
                "code": [
                    "import numpy as np\nfrom itertools import combinations\n\ninputlist = [2,12,35,51,57,58]\ncombined = np.array(list(combinations(inputlist, 3)))\n\nmask = (np.array([set(row).issuperset(c) for row in df.values for c in combined])\n        .reshape(len(df), -1).any(1))\n\nprint(df[mask])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 141,
            "user_id": 11380913,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-I0pQiyFiwdY/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcbaguY2kQ5Yyxay6_ultqvQVLFVg/mo/photo.jpg?sz=128",
            "display_name": "Roy Han",
            "link": "https://stackoverflow.com/users/11380913/roy-han"
        },
        "is_answered": true,
        "view_count": 88,
        "accepted_answer_id": 55756206,
        "answer_count": 3,
        "score": 4,
        "last_activity_date": 1555647457,
        "creation_date": 1555643463,
        "question_id": 55756126,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55756126/choose-a-value-from-a-set-of-columns-based-on-value-and-create-new-column-with-t",
        "title": "Choose a value from a set of columns based on value and create new column with the value?",
        "body": "<p>so if I have a pandas Dataframe like:</p>\n\n<pre><code>   A  B  C  D\n0  1  2  3  a \n1  2  4  6  a\n2  4  8  8  b\n3  2  3  5  c\n</code></pre>\n\n<p>and want to insert row 'E' by choosing from columns 'A', 'B', or 'C' based on conditions in column 'D', how would I go about doing this? For example: if D == a, choose 'A', else choose 'B', outputting:</p>\n\n<pre><code>   A  B  C  D  E\n0  1  2  3  a  1\n1  2  4  6  a  2\n2  4  8  8  b  8\n3  2  3  5  c  3\n</code></pre>\n\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>This is <code>lookup</code> </p>\n\n<pre><code>df.lookup(df.index,df.D.str.upper())\nOut[749]: array([1, 2, 8, 5], dtype=int64)\n\ndf['E']=df.lookup(df.index,df.D.str.upper())\n</code></pre>\n",
        "question_body": "<p>so if I have a pandas Dataframe like:</p>\n\n<pre><code>   A  B  C  D\n0  1  2  3  a \n1  2  4  6  a\n2  4  8  8  b\n3  2  3  5  c\n</code></pre>\n\n<p>and want to insert row 'E' by choosing from columns 'A', 'B', or 'C' based on conditions in column 'D', how would I go about doing this? For example: if D == a, choose 'A', else choose 'B', outputting:</p>\n\n<pre><code>   A  B  C  D  E\n0  1  2  3  a  1\n1  2  4  6  a  2\n2  4  8  8  b  8\n3  2  3  5  c  3\n</code></pre>\n\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 55756126,
            "link": "https://stackoverflow.com/questions/55756126/choose-a-value-from-a-set-of-columns-based-on-value-and-create-new-column-with-t",
            "question": {
                "title": "Choose a value from a set of columns based on value and create new column with the value?",
                "ques_desc": "so if I have a pandas Dataframe like: and want to insert row 'E' by choosing from columns 'A', 'B', or 'C' based on conditions in column 'D', how would I go about doing this? For example: if D == a, choose 'A', else choose 'B', outputting: Thanks in advance! "
            },
            "io": [
                "   A  B  C  D\n0  1  2  3  a \n1  2  4  6  a\n2  4  8  8  b\n3  2  3  5  c\n",
                "   A  B  C  D  E\n0  1  2  3  a  1\n1  2  4  6  a  2\n2  4  8  8  b  8\n3  2  3  5  c  3\n"
            ],
            "answer": {
                "ans_desc": "This is ",
                "code": [
                    "df.lookup(df.index,df.D.str.upper())\nOut[749]: array([1, 2, 8, 5], dtype=int64)\n\ndf['E']=df.lookup(df.index,df.D.str.upper())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "calculated-columns"
        ],
        "owner": {
            "reputation": 379,
            "user_id": 9283258,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/2c95b5aca3c1a96ed210a03982d2633a?s=128&d=identicon&r=PG&f=1",
            "display_name": "MaMo",
            "link": "https://stackoverflow.com/users/9283258/mamo"
        },
        "is_answered": true,
        "view_count": 1334,
        "accepted_answer_id": 49266583,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1555524216,
        "creation_date": 1520977707,
        "last_edit_date": 1521001243,
        "question_id": 49266390,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49266390/python-count-combinations-of-values-within-two-columns-and-find-max-frequency-o",
        "title": "Python: Count combinations of values within two columns and find max frequency of each combination",
        "body": "<p>My pandas dataframe looks like this:</p>\n\n<pre><code>+-----+---------+-------+\n| No. | Section | Group |\n+-----+---------+-------+\n| 123 |     222 |     1 |\n| 234 |     222 |     1 |\n| 345 |     222 |     1 |\n| 456 |     222 |     3 |\n| 567 |     241 |     1 |\n| 678 |     241 |     2 |\n| 789 |     241 |     2 |\n| 890 |     241 |     3 |\n+-----+---------+-------+\n</code></pre>\n\n<p>First, I need to add another column containing the frequency of each combination of <em>Section</em> and <em>Group</em>. It is important to keep all rows.</p>\n\n<p>Desired output:</p>\n\n<pre><code>+-----+---------+-------+-------+\n| No. | Section | Group | Count |\n+-----+---------+-------+-------+\n| 123 |     222 |     1 |     3 |\n| 234 |     222 |     1 |     3 |\n| 345 |     222 |     1 |     3 |\n| 456 |     222 |     3 |     1 |\n| 567 |     241 |     1 |     1 |\n| 678 |     241 |     2 |     2 |\n| 789 |     241 |     2 |     2 |\n| 890 |     241 |     3 |     1 |\n+-----+---------+-------+-------+\n</code></pre>\n\n<p>The second step would be marking the highest value within <em>Count</em> for each <em>Section</em>. For example, with a <code>True/False</code> column like this:</p>\n\n<pre><code>+-----+---------+-------+-------+-------+\n| No. | Section | Group | Count |  Max  |\n+-----+---------+-------+-------+-------+\n| 123 |     222 |     1 |     3 | True  |\n| 234 |     222 |     1 |     3 | True  |\n| 345 |     222 |     1 |     3 | True  |\n| 456 |     222 |     3 |     1 | False |\n| 567 |     241 |     1 |     1 | False |\n| 678 |     241 |     2 |     2 | True  |\n| 789 |     241 |     2 |     2 | True  |\n| 890 |     241 |     3 |     1 | False |\n+-----+---------+-------+-------+-------+\n</code></pre>\n\n<p>The original data frame has lots of rows. That is why I'm asking for an efficient way because I cannot think of one.</p>\n\n<p>Thank you very much!</p>\n",
        "answer_body": "<p>Look at <code>transform</code> </p>\n\n<pre><code>df['Count']=df.groupby(['Section','Group'])['Group'].transform('size')\ndf['Max']=df.groupby(['Section'])['Count'].transform('max')==df['Count']\ndf\nOut[508]: \n    No  Section  Group  Count    Max\n0  123      222      1      3   True\n1  234      222      1      3   True\n2  345      222      1      3   True\n3  456      222      3      1  False\n4  567      241      1      1  False\n5  678      241      2      2   True\n6  789      241      2      2   True\n7  890      241      3      1  False\n</code></pre>\n",
        "question_body": "<p>My pandas dataframe looks like this:</p>\n\n<pre><code>+-----+---------+-------+\n| No. | Section | Group |\n+-----+---------+-------+\n| 123 |     222 |     1 |\n| 234 |     222 |     1 |\n| 345 |     222 |     1 |\n| 456 |     222 |     3 |\n| 567 |     241 |     1 |\n| 678 |     241 |     2 |\n| 789 |     241 |     2 |\n| 890 |     241 |     3 |\n+-----+---------+-------+\n</code></pre>\n\n<p>First, I need to add another column containing the frequency of each combination of <em>Section</em> and <em>Group</em>. It is important to keep all rows.</p>\n\n<p>Desired output:</p>\n\n<pre><code>+-----+---------+-------+-------+\n| No. | Section | Group | Count |\n+-----+---------+-------+-------+\n| 123 |     222 |     1 |     3 |\n| 234 |     222 |     1 |     3 |\n| 345 |     222 |     1 |     3 |\n| 456 |     222 |     3 |     1 |\n| 567 |     241 |     1 |     1 |\n| 678 |     241 |     2 |     2 |\n| 789 |     241 |     2 |     2 |\n| 890 |     241 |     3 |     1 |\n+-----+---------+-------+-------+\n</code></pre>\n\n<p>The second step would be marking the highest value within <em>Count</em> for each <em>Section</em>. For example, with a <code>True/False</code> column like this:</p>\n\n<pre><code>+-----+---------+-------+-------+-------+\n| No. | Section | Group | Count |  Max  |\n+-----+---------+-------+-------+-------+\n| 123 |     222 |     1 |     3 | True  |\n| 234 |     222 |     1 |     3 | True  |\n| 345 |     222 |     1 |     3 | True  |\n| 456 |     222 |     3 |     1 | False |\n| 567 |     241 |     1 |     1 | False |\n| 678 |     241 |     2 |     2 | True  |\n| 789 |     241 |     2 |     2 | True  |\n| 890 |     241 |     3 |     1 | False |\n+-----+---------+-------+-------+-------+\n</code></pre>\n\n<p>The original data frame has lots of rows. That is why I'm asking for an efficient way because I cannot think of one.</p>\n\n<p>Thank you very much!</p>\n",
        "formatted_input": {
            "qid": 49266390,
            "link": "https://stackoverflow.com/questions/49266390/python-count-combinations-of-values-within-two-columns-and-find-max-frequency-o",
            "question": {
                "title": "Python: Count combinations of values within two columns and find max frequency of each combination",
                "ques_desc": "My pandas dataframe looks like this: First, I need to add another column containing the frequency of each combination of Section and Group. It is important to keep all rows. Desired output: The second step would be marking the highest value within Count for each Section. For example, with a column like this: The original data frame has lots of rows. That is why I'm asking for an efficient way because I cannot think of one. Thank you very much! "
            },
            "io": [
                "+-----+---------+-------+\n| No. | Section | Group |\n+-----+---------+-------+\n| 123 |     222 |     1 |\n| 234 |     222 |     1 |\n| 345 |     222 |     1 |\n| 456 |     222 |     3 |\n| 567 |     241 |     1 |\n| 678 |     241 |     2 |\n| 789 |     241 |     2 |\n| 890 |     241 |     3 |\n+-----+---------+-------+\n",
                "+-----+---------+-------+-------+\n| No. | Section | Group | Count |\n+-----+---------+-------+-------+\n| 123 |     222 |     1 |     3 |\n| 234 |     222 |     1 |     3 |\n| 345 |     222 |     1 |     3 |\n| 456 |     222 |     3 |     1 |\n| 567 |     241 |     1 |     1 |\n| 678 |     241 |     2 |     2 |\n| 789 |     241 |     2 |     2 |\n| 890 |     241 |     3 |     1 |\n+-----+---------+-------+-------+\n"
            ],
            "answer": {
                "ans_desc": "Look at ",
                "code": [
                    "df['Count']=df.groupby(['Section','Group'])['Group'].transform('size')\ndf['Max']=df.groupby(['Section'])['Count'].transform('max')==df['Count']\ndf\nOut[508]: \n    No  Section  Group  Count    Max\n0  123      222      1      3   True\n1  234      222      1      3   True\n2  345      222      1      3   True\n3  456      222      3      1  False\n4  567      241      1      1  False\n5  678      241      2      2   True\n6  789      241      2      2   True\n7  890      241      3      1  False\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "sorting",
            "dataframe"
        ],
        "owner": {
            "reputation": 7162,
            "user_id": 861204,
            "user_type": "registered",
            "accept_rate": 32,
            "profile_image": "https://www.gravatar.com/avatar/563699fb1d321a7f86627ddd55168dec?s=128&d=identicon&r=PG",
            "display_name": "nilkash",
            "link": "https://stackoverflow.com/users/861204/nilkash"
        },
        "is_answered": true,
        "view_count": 356,
        "accepted_answer_id": 55672277,
        "answer_count": 4,
        "score": 3,
        "last_activity_date": 1555227336,
        "creation_date": 1555219970,
        "last_edit_date": 1555220276,
        "question_id": 55672247,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55672247/sort-dataframe-column-with-given-input-list",
        "title": "Sort DataFrame column with given input list",
        "body": "<p>Hi I want to sort DataFrame column with given input list values.\nMy list looks like : </p>\n\n<pre><code>inputlist\n[309.0, 585.0, 221.0, 789.0, 195.0, 354.0, 307.0, 698.0, 426.0]\n</code></pre>\n\n<p>And DataFrame is :</p>\n\n<pre><code>  val    kaywords\n\n195    keyword3\n221    keyword5\n307    keyword8\n309    keyword9\n354    keyword0\n426    keyword1\n585    keyword2\n698    keyword4\n789    keyword33\n</code></pre>\n\n<p>Here I want to sort DataFrame column 'val' on basis of given 'inputlist'.</p>\n\n<p>I am expecting following output :</p>\n\n<pre><code>val    kaywords\n\n309    keyword9\n585    keyword2\n221    keyword5\n789    keyword33\n195    keyword3\n354    keyword0\n307    keyword8\n698    keyword4\n426    keyword1\n</code></pre>\n",
        "answer_body": "<p>Use ordered <code>categorical</code>, but first convert values of list to integers:</p>\n\n<pre><code>inputlist = [309.0, 585.0, 221.0, 789.0, 195.0, 354.0, 307.0, 698.0, 426.0]\n\ndf['val'] = pd.Categorical(df['val'], ordered=True, categories=[int(x) for x in inputlist])\ndf = df.sort_values('val')\nprint (df)\n   val   kaywords\n3  309   keyword9\n6  585   keyword2\n1  221   keyword5\n8  789  keyword33\n0  195   keyword3\n4  354   keyword0\n2  307   keyword8\n7  698   keyword4\n5  426   keyword1\n</code></pre>\n\n<p>Another idea if all values from <code>val</code> exist in <code>inputlist</code>:</p>\n\n<pre><code>inputlist = [int(x) for x in inputlist]\ndf = df.set_index('val').reindex(inputlist).reset_index()\n</code></pre>\n",
        "question_body": "<p>Hi I want to sort DataFrame column with given input list values.\nMy list looks like : </p>\n\n<pre><code>inputlist\n[309.0, 585.0, 221.0, 789.0, 195.0, 354.0, 307.0, 698.0, 426.0]\n</code></pre>\n\n<p>And DataFrame is :</p>\n\n<pre><code>  val    kaywords\n\n195    keyword3\n221    keyword5\n307    keyword8\n309    keyword9\n354    keyword0\n426    keyword1\n585    keyword2\n698    keyword4\n789    keyword33\n</code></pre>\n\n<p>Here I want to sort DataFrame column 'val' on basis of given 'inputlist'.</p>\n\n<p>I am expecting following output :</p>\n\n<pre><code>val    kaywords\n\n309    keyword9\n585    keyword2\n221    keyword5\n789    keyword33\n195    keyword3\n354    keyword0\n307    keyword8\n698    keyword4\n426    keyword1\n</code></pre>\n",
        "formatted_input": {
            "qid": 55672247,
            "link": "https://stackoverflow.com/questions/55672247/sort-dataframe-column-with-given-input-list",
            "question": {
                "title": "Sort DataFrame column with given input list",
                "ques_desc": "Hi I want to sort DataFrame column with given input list values. My list looks like : And DataFrame is : Here I want to sort DataFrame column 'val' on basis of given 'inputlist'. I am expecting following output : "
            },
            "io": [
                "  val    kaywords\n\n195    keyword3\n221    keyword5\n307    keyword8\n309    keyword9\n354    keyword0\n426    keyword1\n585    keyword2\n698    keyword4\n789    keyword33\n",
                "val    kaywords\n\n309    keyword9\n585    keyword2\n221    keyword5\n789    keyword33\n195    keyword3\n354    keyword0\n307    keyword8\n698    keyword4\n426    keyword1\n"
            ],
            "answer": {
                "ans_desc": "Use ordered , but first convert values of list to integers: Another idea if all values from exist in : ",
                "code": [
                    "inputlist = [int(x) for x in inputlist]\ndf = df.set_index('val').reindex(inputlist).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 1380,
            "user_id": 5865579,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/185a0b751c11ba968e2cf51ec30c37d0?s=128&d=identicon&r=PG&f=1",
            "display_name": "jason",
            "link": "https://stackoverflow.com/users/5865579/jason"
        },
        "is_answered": true,
        "view_count": 3772,
        "accepted_answer_id": 55661349,
        "answer_count": 1,
        "score": 7,
        "last_activity_date": 1555144931,
        "creation_date": 1555120551,
        "last_edit_date": 1555144931,
        "question_id": 55661343,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55661343/how-to-convert-one-column-in-dataframe-into-a-2d-array-in-python",
        "title": "how to convert one column in dataframe into a 2D array in python",
        "body": "<p>I have an dataframe which contain the observed data as:</p>\n\n<pre><code>import pandas as pd\nd = {'ID': [0,1,2], 'Value': \n[[1,2,1],[5,4,6],[7,20,9]]}\ndf = pd.DataFrame(data=d)\n</code></pre>\n\n<p>how can I get an array from the value to form a 2D <code>numpy.ndarray</code></p>\n\n<pre><code> [[1, 2, 1],\n [5, 4, 6],\n [7, 20, 9]]\n</code></pre>\n\n<p>with shape:(3,3)</p>\n\n<p>I try </p>\n\n<pre><code>print(df['Value'].values)\n</code></pre>\n\n<p>but it gives me</p>\n\n<pre><code>[list([1, 2, 1]) list([5, 4, 6]) list([7, 20, 9])]\n</code></pre>\n\n<p>which is not what I want</p>\n",
        "answer_body": "<p>You can extract the column lists, then array-ify using a couple of methods below.</p>\n\n<pre><code>np.array(df['Value'].tolist())\n\narray([[ 1,  2,  1],\n       [ 5,  4,  6],\n       [ 7, 20,  9]])\n</code></pre>\n\n<hr>\n\n<pre><code># np.vstack(df['Value'])\nnp.stack(df['Value'])\n\narray([[ 1,  2,  1],\n       [ 5,  4,  6],\n       [ 7, 20,  9]])\n</code></pre>\n\n<hr>\n\n<p>If the lists are un-evenly sized, this will return a regular 2D array with nans in missing positions.</p>\n\n<pre><code>df['Value'] = [[1, 2], [3], [4, 5, 6]]\ndf\n\n   ID      Value\n0   0     [1, 2]\n1   1        [3]\n2   2  [4, 5, 6]\n</code></pre>\n\n<p></p>\n\n<pre><code># pd.DataFrame(df['Value'].tolist()).values   #  &lt; v0.24\npd.DataFrame(df['Value'].tolist()).to_numpy() #  v0.24+\n\narray([[ 1.,  2., nan],\n       [ 3., nan, nan],\n       [ 4.,  5.,  6.]])\n</code></pre>\n",
        "question_body": "<p>I have an dataframe which contain the observed data as:</p>\n\n<pre><code>import pandas as pd\nd = {'ID': [0,1,2], 'Value': \n[[1,2,1],[5,4,6],[7,20,9]]}\ndf = pd.DataFrame(data=d)\n</code></pre>\n\n<p>how can I get an array from the value to form a 2D <code>numpy.ndarray</code></p>\n\n<pre><code> [[1, 2, 1],\n [5, 4, 6],\n [7, 20, 9]]\n</code></pre>\n\n<p>with shape:(3,3)</p>\n\n<p>I try </p>\n\n<pre><code>print(df['Value'].values)\n</code></pre>\n\n<p>but it gives me</p>\n\n<pre><code>[list([1, 2, 1]) list([5, 4, 6]) list([7, 20, 9])]\n</code></pre>\n\n<p>which is not what I want</p>\n",
        "formatted_input": {
            "qid": 55661343,
            "link": "https://stackoverflow.com/questions/55661343/how-to-convert-one-column-in-dataframe-into-a-2d-array-in-python",
            "question": {
                "title": "how to convert one column in dataframe into a 2D array in python",
                "ques_desc": "I have an dataframe which contain the observed data as: how can I get an array from the value to form a 2D with shape:(3,3) I try but it gives me which is not what I want "
            },
            "io": [
                " [[1, 2, 1],\n [5, 4, 6],\n [7, 20, 9]]\n",
                "[list([1, 2, 1]) list([5, 4, 6]) list([7, 20, 9])]\n"
            ],
            "answer": {
                "ans_desc": "You can extract the column lists, then array-ify using a couple of methods below. If the lists are un-evenly sized, this will return a regular 2D array with nans in missing positions. ",
                "code": [
                    "# pd.DataFrame(df['Value'].tolist()).values   #  < v0.24\npd.DataFrame(df['Value'].tolist()).to_numpy() #  v0.24+\n\narray([[ 1.,  2., nan],\n       [ 3., nan, nan],\n       [ 4.,  5.,  6.]])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "min"
        ],
        "owner": {
            "reputation": 725,
            "user_id": 10429891,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-M473SUk4uyk/AAAAAAAAAAI/AAAAAAAAABY/uuxCZRmisp4/photo.jpg?sz=128",
            "display_name": "Adrian",
            "link": "https://stackoverflow.com/users/10429891/adrian"
        },
        "is_answered": true,
        "view_count": 2663,
        "accepted_answer_id": 55654134,
        "answer_count": 1,
        "score": 9,
        "last_activity_date": 1555080845,
        "creation_date": 1555079985,
        "question_id": 55654105,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55654105/pandas-get-the-min-value-between-2-dataframe-columns",
        "title": "Pandas: get the min value between 2 dataframe columns",
        "body": "<p>I have 2 columns and I want a 3rd column to be the minimum value between them.\nMy data looks like this:</p>\n\n<pre><code>   A  B\n0  2  1\n1  2  1\n2  2  4\n3  2  4\n4  3  5\n5  3  5\n6  3  6\n7  3  6\n</code></pre>\n\n<p>And I want to get a column C in the following way:</p>\n\n<pre><code>   A  B   C\n0  2  1   1\n1  2  1   1\n2  2  4   2\n3  2  4   2\n4  3  5   3\n5  3  5   3\n6  3  6   3\n7  3  6   3\n</code></pre>\n\n<p>Some helping code:</p>\n\n<pre><code>df = pd.DataFrame({'A': [2, 2, 2, 2, 3, 3, 3, 3],\n                   'B': [1, 1, 4, 4, 5, 5, 6, 6]})\n</code></pre>\n\n<p>Thanks!</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html#pandas.DataFrame.min\" rel=\"noreferrer\"><code>df.min(axis=1)</code></a></p>\n\n<pre><code>df['c'] = df.min(axis=1)\ndf\nOut[41]: \n   A  B  c\n0  2  1  1\n1  2  1  1\n2  2  4  2\n3  2  4  2\n4  3  5  3\n5  3  5  3\n6  3  6  3\n7  3  6  3\n</code></pre>\n\n<p>This returns the min row-wise (when passing <code>axis=1</code>)</p>\n\n<p>For non-heterogenous dtypes and large dfs you can use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.min.html\" rel=\"noreferrer\"><code>numpy.min</code></a> which will be quicker:</p>\n\n<pre><code>In[42]:\ndf['c'] = np.min(df.values,axis=1)\ndf\n\nOut[42]: \n   A  B  c\n0  2  1  1\n1  2  1  1\n2  2  4  2\n3  2  4  2\n4  3  5  3\n5  3  5  3\n6  3  6  3\n7  3  6  3\n</code></pre>\n\n<p><strong>timings</strong>:</p>\n\n<pre><code>In[45]:\ndf = pd.DataFrame({'A': [2, 2, 2, 2, 3, 3, 3, 3],\n                   'B': [1, 1, 4, 4, 5, 5, 6, 6]})\ndf = pd.concat([df]*1000, ignore_index=True)\ndf.shape\n\nOut[45]: (8000, 2)\n</code></pre>\n\n<p>So for a 8K row df:</p>\n\n<pre><code>%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n314 \u00b5s \u00b1 3.63 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n34.4 \u00b5s \u00b1 161 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre>\n\n<p>You can see that the numpy version is nearly 10x quicker (note I pass <code>df.values</code> so we pass a numpy array), this will become more of a factor when we get to even larger dfs</p>\n\n<p><strong>Note</strong></p>\n\n<p>for versions <code>0.24.0</code> or greater, use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html?highlight=to_numpy\" rel=\"noreferrer\"><code>to_numpy()</code></a></p>\n\n<p>so the above becomes:</p>\n\n<pre><code>df['c'] = np.min(df.to_numpy(),axis=1)\n</code></pre>\n\n<p><strong>Timings</strong>:</p>\n\n<pre><code>%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n%timeit np.min(df.to_numpy(),axis=1)\n314 \u00b5s \u00b1 3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n35.2 \u00b5s \u00b1 680 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n35.5 \u00b5s \u00b1 262 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre>\n\n<p>There is a minor discrepancy between <code>.values</code> and <code>to_numpy()</code>, it depends on whether you know upfront that the dtype is not mixed, and that the likely dtype is a factor e.g. <code>float 16</code> vs <code>float 32</code> see that link for further explanation. Pandas is doing a little more checking when calling <code>to_numpy</code></p>\n",
        "question_body": "<p>I have 2 columns and I want a 3rd column to be the minimum value between them.\nMy data looks like this:</p>\n\n<pre><code>   A  B\n0  2  1\n1  2  1\n2  2  4\n3  2  4\n4  3  5\n5  3  5\n6  3  6\n7  3  6\n</code></pre>\n\n<p>And I want to get a column C in the following way:</p>\n\n<pre><code>   A  B   C\n0  2  1   1\n1  2  1   1\n2  2  4   2\n3  2  4   2\n4  3  5   3\n5  3  5   3\n6  3  6   3\n7  3  6   3\n</code></pre>\n\n<p>Some helping code:</p>\n\n<pre><code>df = pd.DataFrame({'A': [2, 2, 2, 2, 3, 3, 3, 3],\n                   'B': [1, 1, 4, 4, 5, 5, 6, 6]})\n</code></pre>\n\n<p>Thanks!</p>\n",
        "formatted_input": {
            "qid": 55654105,
            "link": "https://stackoverflow.com/questions/55654105/pandas-get-the-min-value-between-2-dataframe-columns",
            "question": {
                "title": "Pandas: get the min value between 2 dataframe columns",
                "ques_desc": "I have 2 columns and I want a 3rd column to be the minimum value between them. My data looks like this: And I want to get a column C in the following way: Some helping code: Thanks! "
            },
            "io": [
                "   A  B\n0  2  1\n1  2  1\n2  2  4\n3  2  4\n4  3  5\n5  3  5\n6  3  6\n7  3  6\n",
                "   A  B   C\n0  2  1   1\n1  2  1   1\n2  2  4   2\n3  2  4   2\n4  3  5   3\n5  3  5   3\n6  3  6   3\n7  3  6   3\n"
            ],
            "answer": {
                "ans_desc": "Use This returns the min row-wise (when passing ) For non-heterogenous dtypes and large dfs you can use which will be quicker: timings: So for a 8K row df: You can see that the numpy version is nearly 10x quicker (note I pass so we pass a numpy array), this will become more of a factor when we get to even larger dfs Note for versions or greater, use so the above becomes: Timings: There is a minor discrepancy between and , it depends on whether you know upfront that the dtype is not mixed, and that the likely dtype is a factor e.g. vs see that link for further explanation. Pandas is doing a little more checking when calling ",
                "code": [
                    "%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n314 \u00b5s \u00b1 3.63 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n34.4 \u00b5s \u00b1 161 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n",
                    "%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n%timeit np.min(df.to_numpy(),axis=1)\n314 \u00b5s \u00b1 3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n35.2 \u00b5s \u00b1 680 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n35.5 \u00b5s \u00b1 262 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 5796352,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/094c9b4b0ed9cc29736fb31968cbec49?s=128&d=identicon&r=PG&f=1",
            "display_name": "A Potdar",
            "link": "https://stackoverflow.com/users/5796352/a-potdar"
        },
        "is_answered": true,
        "view_count": 551,
        "accepted_answer_id": 55421281,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1553876193,
        "creation_date": 1553871642,
        "last_edit_date": 1553872736,
        "question_id": 55420231,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55420231/how-to-shift-pandas-column-elements-for-given-index-based-on-condition",
        "title": "How to shift pandas column elements for given index based on condition?",
        "body": "<p>I have recently started using python and pandas, please bear with me on this. \nI have two columns (A, B) of data (dataframe) that should be arranged in particular sequence based on certain relation between two columns (let's say elements of column A should be smaller than elements column B for a given index), if relation is not satisfied data should shifted (only for A) by a row starting from the index where condition is not satisfied throughout the length of a column. And it should be replaced by NaN where is condition is not met. </p>\n\n<p>I have tried shift(1) function. This works only if the first element doesn't meet the condition but if there is any other element or multiple elements don't meet criteria it creates multiple NaNs at the start of column A instead of at the place where criteria is not met.</p>\n\n<pre><code>mdata1 = [[3,2],[5,4],[8,6],[10,7],[float('NaN'),9],[float('NaN'),11]]\nmdf1 = pd.DataFrame(mdata1,columns=['A','B'])\n\nfor xt in range (0,len(mdf1)):\n    if mdf1.A[xt]&gt;mdf1.B[xt]:\n        mdf1['A'] = mdf1['A'].shift(1)\n</code></pre>\n\n<p>Actual result </p>\n\n<pre><code>A   B\nNaN 2\nNaN 4\n3.0 6\n5.0 7\n8.0 9\n10.0    11\n</code></pre>\n\n<p>Expected result</p>\n\n<pre><code>A   B\nNaN 2\n3.0 4\n5.0 6\nNaN 7\n8.0 9\n10.0    11\n</code></pre>\n",
        "answer_body": "<p>I don't understand what you want to do exactly. but by simply altering your code i get the expected results:</p>\n\n<pre><code>for xt in range (0,len(mdf1)):\nif mdf1.A[xt]&gt;mdf1.B[xt]:\n    mdf1.loc[xt:,'A'] = mdf1[xt:]['A'].shift(1)\n</code></pre>\n\n<p>shift(1) shifts the whole column/dataframe by one row, thus you need to start shifting from the index you're at to get what you want.</p>\n",
        "question_body": "<p>I have recently started using python and pandas, please bear with me on this. \nI have two columns (A, B) of data (dataframe) that should be arranged in particular sequence based on certain relation between two columns (let's say elements of column A should be smaller than elements column B for a given index), if relation is not satisfied data should shifted (only for A) by a row starting from the index where condition is not satisfied throughout the length of a column. And it should be replaced by NaN where is condition is not met. </p>\n\n<p>I have tried shift(1) function. This works only if the first element doesn't meet the condition but if there is any other element or multiple elements don't meet criteria it creates multiple NaNs at the start of column A instead of at the place where criteria is not met.</p>\n\n<pre><code>mdata1 = [[3,2],[5,4],[8,6],[10,7],[float('NaN'),9],[float('NaN'),11]]\nmdf1 = pd.DataFrame(mdata1,columns=['A','B'])\n\nfor xt in range (0,len(mdf1)):\n    if mdf1.A[xt]&gt;mdf1.B[xt]:\n        mdf1['A'] = mdf1['A'].shift(1)\n</code></pre>\n\n<p>Actual result </p>\n\n<pre><code>A   B\nNaN 2\nNaN 4\n3.0 6\n5.0 7\n8.0 9\n10.0    11\n</code></pre>\n\n<p>Expected result</p>\n\n<pre><code>A   B\nNaN 2\n3.0 4\n5.0 6\nNaN 7\n8.0 9\n10.0    11\n</code></pre>\n",
        "formatted_input": {
            "qid": 55420231,
            "link": "https://stackoverflow.com/questions/55420231/how-to-shift-pandas-column-elements-for-given-index-based-on-condition",
            "question": {
                "title": "How to shift pandas column elements for given index based on condition?",
                "ques_desc": "I have recently started using python and pandas, please bear with me on this. I have two columns (A, B) of data (dataframe) that should be arranged in particular sequence based on certain relation between two columns (let's say elements of column A should be smaller than elements column B for a given index), if relation is not satisfied data should shifted (only for A) by a row starting from the index where condition is not satisfied throughout the length of a column. And it should be replaced by NaN where is condition is not met. I have tried shift(1) function. This works only if the first element doesn't meet the condition but if there is any other element or multiple elements don't meet criteria it creates multiple NaNs at the start of column A instead of at the place where criteria is not met. Actual result Expected result "
            },
            "io": [
                "A   B\nNaN 2\nNaN 4\n3.0 6\n5.0 7\n8.0 9\n10.0    11\n",
                "A   B\nNaN 2\n3.0 4\n5.0 6\nNaN 7\n8.0 9\n10.0    11\n"
            ],
            "answer": {
                "ans_desc": "I don't understand what you want to do exactly. but by simply altering your code i get the expected results: shift(1) shifts the whole column/dataframe by one row, thus you need to start shifting from the index you're at to get what you want. ",
                "code": [
                    "for xt in range (0,len(mdf1)):\nif mdf1.A[xt]>mdf1.B[xt]:\n    mdf1.loc[xt:,'A'] = mdf1[xt:]['A'].shift(1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 55,
            "user_id": 11258041,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-NKiTUtPows0/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdAvuLDwUMPZjtw6JMYckrLJSsaTQ/mo/photo.jpg?sz=128",
            "display_name": "cck1110",
            "link": "https://stackoverflow.com/users/11258041/cck1110"
        },
        "is_answered": true,
        "view_count": 648,
        "accepted_answer_id": 55388736,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1553736860,
        "creation_date": 1553735275,
        "last_edit_date": 1553736413,
        "question_id": 55388727,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55388727/pandas-filter-rows-based-on-condition-but-always-retain-the-first-row",
        "title": "Pandas filter rows based on condition, but always retain the first row",
        "body": "<p>I would like to drop some rows that meets certain conditions but I do not want to drop the first row even if the first row meets that criteria.</p>\n\n<p>I tried dropping rows by using the df.drop function but it will erase the first row if the first row meets that condition. I do not want that.</p>\n\n<p>Data looks something like this:</p>\n\n<pre><code>Column1 Column2 Column3\n  1        3      A\n  2        1      B\n  3        3      C\n  4        1      D\n  5        1      E\n  6        3      F\n</code></pre>\n\n<p>I want to do it in a way that if a row has a value of 3 in column2 then drop it. </p>\n\n<p>And I want the new data to be like this (after dropping but keeping the first one even though the first row had a value of 3 in column 2):</p>\n\n<pre><code>Column1 Column2 Column3\n  1        3      A\n  2        1      B\n  4        1      D\n  5        1      E\n</code></pre>\n",
        "answer_body": "<p>You can make \"retain first row\" a part of your condition for dropping/keeping rows.</p>\n\n<p>The condition for keeping the rows would be <code>&lt;ORIGINAL CONDITION&gt; or &lt;CONDITION TO KEEP FIRST ROW&gt;</code>. In code, this is</p>\n\n<pre><code># (condition to drop 3) | (condition to keep 0th row)\ndf[(df['Column2'] != 3) | (df.index == 0)]\n\n   Column1  Column2 Column3\n0        1        3       A\n1        2        1       B\n3        4        1       D\n4        5        1       E\n</code></pre>\n\n<p>Inversely, using DeMorgan's Laws, the condition for dropping rows would be <code>(df['Column2'] == 3) &amp; (df.index != 0)</code>. We then invert the condition to get our expected output,</p>\n\n<pre><code>df[~((df['Column2'] == 3) &amp; (df.index != 0))]\n\n   Column1  Column2 Column3\n0        1        3       A\n1        2        1       B\n3        4        1       D\n4        5        1       E\n</code></pre>\n\n<p>These work assuming your index is a <code>RangeIndex</code>. If not, use <code>pd.RangeIndex(len(df)) == 0</code> as the second condition instead.</p>\n",
        "question_body": "<p>I would like to drop some rows that meets certain conditions but I do not want to drop the first row even if the first row meets that criteria.</p>\n\n<p>I tried dropping rows by using the df.drop function but it will erase the first row if the first row meets that condition. I do not want that.</p>\n\n<p>Data looks something like this:</p>\n\n<pre><code>Column1 Column2 Column3\n  1        3      A\n  2        1      B\n  3        3      C\n  4        1      D\n  5        1      E\n  6        3      F\n</code></pre>\n\n<p>I want to do it in a way that if a row has a value of 3 in column2 then drop it. </p>\n\n<p>And I want the new data to be like this (after dropping but keeping the first one even though the first row had a value of 3 in column 2):</p>\n\n<pre><code>Column1 Column2 Column3\n  1        3      A\n  2        1      B\n  4        1      D\n  5        1      E\n</code></pre>\n",
        "formatted_input": {
            "qid": 55388727,
            "link": "https://stackoverflow.com/questions/55388727/pandas-filter-rows-based-on-condition-but-always-retain-the-first-row",
            "question": {
                "title": "Pandas filter rows based on condition, but always retain the first row",
                "ques_desc": "I would like to drop some rows that meets certain conditions but I do not want to drop the first row even if the first row meets that criteria. I tried dropping rows by using the df.drop function but it will erase the first row if the first row meets that condition. I do not want that. Data looks something like this: I want to do it in a way that if a row has a value of 3 in column2 then drop it. And I want the new data to be like this (after dropping but keeping the first one even though the first row had a value of 3 in column 2): "
            },
            "io": [
                "Column1 Column2 Column3\n  1        3      A\n  2        1      B\n  3        3      C\n  4        1      D\n  5        1      E\n  6        3      F\n",
                "Column1 Column2 Column3\n  1        3      A\n  2        1      B\n  4        1      D\n  5        1      E\n"
            ],
            "answer": {
                "ans_desc": "You can make \"retain first row\" a part of your condition for dropping/keeping rows. The condition for keeping the rows would be . In code, this is Inversely, using DeMorgan's Laws, the condition for dropping rows would be . We then invert the condition to get our expected output, These work assuming your index is a . If not, use as the second condition instead. ",
                "code": [
                    "<ORIGINAL CONDITION> or <CONDITION TO KEEP FIRST ROW>",
                    "(df['Column2'] == 3) & (df.index != 0)"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 60,
        "accepted_answer_id": 55357045,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1553607909,
        "creation_date": 1553602723,
        "last_edit_date": 1553607530,
        "question_id": 55356991,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55356991/add-row-values-of-all-columns-when-a-particular-column-value-is-null-until-it-ge",
        "title": "Add row values of all columns when a particular column value is null until it gets the not null values?",
        "body": "<p>I have a data frame like this:</p>\n\n<pre><code>df\ncol1      col2      col3      col4\n A         12        34        XX\n B         20        25        PP\n B         nan       nan       nan\n nan       P         54        nan\n nan       R         nan       nan\n nan       nan       nan       PQ\n  C         D         32       SS\n  R         S         32       RS\n</code></pre>\n\n<p>If col1 value is null I want to add all the values of other columns untill it finds the notnull element in col1</p>\n\n<p>The data frame i am looking for should look like:</p>\n\n<pre><code>col1     col2     col3     col4\n A         12       34       XX\n B         20       25       PP\n B         PR       54       PQ\n C          D       32       SS\n R          S       32       RS        \n</code></pre>\n\n<p>How to do in in most efficient way using python/pandas</p>\n",
        "answer_body": "<p>If want processes all columns like strings first forward filling missing values in <code>col1</code>, replace <code>NaN</code>s to empty strings, convert all values to <code>strings</code> and use <code>sum</code>:</p>\n\n<pre><code>df['col1'] = df['col1'].ffill()\ndf = df.set_index('col1').fillna('').astype(str).sum(level=0).reset_index()\nprint (df)\n  col1 col2  col3 col4\n0    A   12  34.0   XX\n1    B   PR  54.0   PQ\n2    C    D  32.0   SS\n\nprint (df.dtypes)\ncol1     object\ncol2     object\ncol3     object\ncol4     object\ndtype: object\n</code></pre>\n\n<p>If need processes only numeric columns with aggregate method, e.g. <code>mean</code> use lambda function with <code>if-else</code>:</p>\n\n<pre><code>df['col1'] = df['col1'].ffill()\nc = df.select_dtypes(object).columns\ndf[c] = df[c].fillna('')\n\nf = lambda x: x.mean() if np.issubdtype(x.dtype, np.number) else ''.join(x)\ndf = df.groupby('col1').agg(f).reset_index()\nprint (df)\n  col1 col2  col3 col4\n0    A   12  34.0   XX\n1    B   PR  54.0   PQ\n2    C    D  32.0   SS\n\nprint (df.dtypes)\ncol1     object\ncol2     object\ncol3    float64\ncol4     object\ndtype: object\n</code></pre>\n\n<p>EDIT: New helper column is used:</p>\n\n<pre><code>df['new'] = df['col1'].notna().cumsum()\ndf['col1'] = df['col1'].ffill()\nc = df.select_dtypes(object).columns\ndf[c] = df[c].fillna('')\n\nf = lambda x: x.mean() if np.issubdtype(x.dtype, np.number) else ''.join(x)\ndf = df.groupby(['col1', 'new']).agg(f).reset_index(level=1, drop=True).reset_index()\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this:</p>\n\n<pre><code>df\ncol1      col2      col3      col4\n A         12        34        XX\n B         20        25        PP\n B         nan       nan       nan\n nan       P         54        nan\n nan       R         nan       nan\n nan       nan       nan       PQ\n  C         D         32       SS\n  R         S         32       RS\n</code></pre>\n\n<p>If col1 value is null I want to add all the values of other columns untill it finds the notnull element in col1</p>\n\n<p>The data frame i am looking for should look like:</p>\n\n<pre><code>col1     col2     col3     col4\n A         12       34       XX\n B         20       25       PP\n B         PR       54       PQ\n C          D       32       SS\n R          S       32       RS        \n</code></pre>\n\n<p>How to do in in most efficient way using python/pandas</p>\n",
        "formatted_input": {
            "qid": 55356991,
            "link": "https://stackoverflow.com/questions/55356991/add-row-values-of-all-columns-when-a-particular-column-value-is-null-until-it-ge",
            "question": {
                "title": "Add row values of all columns when a particular column value is null until it gets the not null values?",
                "ques_desc": "I have a data frame like this: If col1 value is null I want to add all the values of other columns untill it finds the notnull element in col1 The data frame i am looking for should look like: How to do in in most efficient way using python/pandas "
            },
            "io": [
                "df\ncol1      col2      col3      col4\n A         12        34        XX\n B         20        25        PP\n B         nan       nan       nan\n nan       P         54        nan\n nan       R         nan       nan\n nan       nan       nan       PQ\n  C         D         32       SS\n  R         S         32       RS\n",
                "col1     col2     col3     col4\n A         12       34       XX\n B         20       25       PP\n B         PR       54       PQ\n C          D       32       SS\n R          S       32       RS        \n"
            ],
            "answer": {
                "ans_desc": "If want processes all columns like strings first forward filling missing values in , replace s to empty strings, convert all values to and use : If need processes only numeric columns with aggregate method, e.g. use lambda function with : EDIT: New helper column is used: ",
                "code": [
                    "df['new'] = df['col1'].notna().cumsum()\ndf['col1'] = df['col1'].ffill()\nc = df.select_dtypes(object).columns\ndf[c] = df[c].fillna('')\n\nf = lambda x: x.mean() if np.issubdtype(x.dtype, np.number) else ''.join(x)\ndf = df.groupby(['col1', 'new']).agg(f).reset_index(level=1, drop=True).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 855,
            "user_id": 2552610,
            "user_type": "registered",
            "accept_rate": 56,
            "profile_image": "https://i.stack.imgur.com/jD7Hz.png?s=128&g=1",
            "display_name": "harry04",
            "link": "https://stackoverflow.com/users/2552610/harry04"
        },
        "is_answered": true,
        "view_count": 651,
        "accepted_answer_id": 55214495,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1552917368,
        "creation_date": 1552880337,
        "last_edit_date": 1552880741,
        "question_id": 55214456,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55214456/pandas-ignoring-empty-values-when-concatenating-grouped-rows",
        "title": "Pandas - Ignoring empty values when concatenating grouped rows",
        "body": "<p>I'm trying to group a dataframe based on a column value, and I want to concatenate (join) the values in the other columns.</p>\n\n<p>I'm doing something like - </p>\n\n<pre><code>df_combined = df_combined.groupby('UC').agg({'LO Number': ', '.join,\n                                             'K Code': ', '.join})\n</code></pre>\n\n<p>But, this gives me some <code>nan</code> values where the <code>K Code</code> columns has no values. So the result looks like</p>\n\n<pre><code>K Code\n\nK0016, K0068, nan, nan, A0046\n\nnan, nan, nan\n</code></pre>\n\n<p>How can I get rid of these nan values in the <code>K Code</code> column? Also, is there a way to get a third column that has the number of values present in <code>K Code</code> column. For eg. for the above, </p>\n\n<pre><code>Count\n\n3   \n\n0\n</code></pre>\n\n<p>Edit: Sample Data - </p>\n\n<pre><code>UC      LO Number      K Code\nC001     C001.1        K0068\nC001     C001.2        K0372\nC002     C002.1        \nC002     C002.3        K0032\nC002     C002.5          \n</code></pre>\n\n<p>Thanks! :)</p>\n",
        "answer_body": "<p>You can try using <code>lambda</code> with <code>agg</code>, however this will create the multiple index </p>\n\n<p>Since you nan is <code>nan</code> do replace before run below </p>\n\n<pre><code>df=df.replace({'nan':np.nan})\n\n\ndf_combined.groupby('UC').agg({'LO Number': ', '.join,\n                                             'K Code': [lambda x : ', '.join(y for y in x if y==y),'count']})\n</code></pre>\n\n<p>If you do not want the multiple index</p>\n\n<pre><code>df_combined.assign(count=df_combined['K Code']).\n         groupby('UC').agg({'LO Number': ', '.join,\n                           'K Code': lambda x : ', '.join(y for y in x if y==y),\n                            'count':'count'})\n</code></pre>\n",
        "question_body": "<p>I'm trying to group a dataframe based on a column value, and I want to concatenate (join) the values in the other columns.</p>\n\n<p>I'm doing something like - </p>\n\n<pre><code>df_combined = df_combined.groupby('UC').agg({'LO Number': ', '.join,\n                                             'K Code': ', '.join})\n</code></pre>\n\n<p>But, this gives me some <code>nan</code> values where the <code>K Code</code> columns has no values. So the result looks like</p>\n\n<pre><code>K Code\n\nK0016, K0068, nan, nan, A0046\n\nnan, nan, nan\n</code></pre>\n\n<p>How can I get rid of these nan values in the <code>K Code</code> column? Also, is there a way to get a third column that has the number of values present in <code>K Code</code> column. For eg. for the above, </p>\n\n<pre><code>Count\n\n3   \n\n0\n</code></pre>\n\n<p>Edit: Sample Data - </p>\n\n<pre><code>UC      LO Number      K Code\nC001     C001.1        K0068\nC001     C001.2        K0372\nC002     C002.1        \nC002     C002.3        K0032\nC002     C002.5          \n</code></pre>\n\n<p>Thanks! :)</p>\n",
        "formatted_input": {
            "qid": 55214456,
            "link": "https://stackoverflow.com/questions/55214456/pandas-ignoring-empty-values-when-concatenating-grouped-rows",
            "question": {
                "title": "Pandas - Ignoring empty values when concatenating grouped rows",
                "ques_desc": "I'm trying to group a dataframe based on a column value, and I want to concatenate (join) the values in the other columns. I'm doing something like - But, this gives me some values where the columns has no values. So the result looks like How can I get rid of these nan values in the column? Also, is there a way to get a third column that has the number of values present in column. For eg. for the above, Edit: Sample Data - Thanks! :) "
            },
            "io": [
                "K Code\n\nK0016, K0068, nan, nan, A0046\n\nnan, nan, nan\n",
                "UC      LO Number      K Code\nC001     C001.1        K0068\nC001     C001.2        K0372\nC002     C002.1        \nC002     C002.3        K0032\nC002     C002.5          \n"
            ],
            "answer": {
                "ans_desc": "You can try using with , however this will create the multiple index Since you nan is do replace before run below If you do not want the multiple index ",
                "code": [
                    "df=df.replace({'nan':np.nan})\n\n\ndf_combined.groupby('UC').agg({'LO Number': ', '.join,\n                                             'K Code': [lambda x : ', '.join(y for y in x if y==y),'count']})\n",
                    "df_combined.assign(count=df_combined['K Code']).\n         groupby('UC').agg({'LO Number': ', '.join,\n                           'K Code': lambda x : ', '.join(y for y in x if y==y),\n                            'count':'count'})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "tokenize"
        ],
        "migrated_from": {
            "other_site": {
                "styling": {
                    "tag_background_color": "#FFF",
                    "tag_foreground_color": "#000",
                    "link_color": "#0077CC"
                },
                "related_sites": [
                    {
                        "relation": "meta",
                        "api_site_parameter": "datascience.meta",
                        "site_url": "https://datascience.meta.stackexchange.com",
                        "name": "Data Science Meta Stack Exchange"
                    },
                    {
                        "relation": "chat",
                        "site_url": "https://chat.stackexchange.com?tab=site&host=datascience.stackexchange.com",
                        "name": "Chat Stack Exchange"
                    }
                ],
                "markdown_extensions": [
                    "MathJax",
                    "Prettify"
                ],
                "launch_date": 1499704419,
                "open_beta_date": 1402346566,
                "closed_beta_date": 1400007600,
                "site_state": "normal",
                "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/datascience/Img/apple-touch-icon@2.png",
                "favicon_url": "https://cdn.sstatic.net/Sites/datascience/Img/favicon.ico",
                "icon_url": "https://cdn.sstatic.net/Sites/datascience/Img/apple-touch-icon.png",
                "audience": "Data science professionals, Machine Learning specialists, and those interested in learning more about the field",
                "site_url": "https://datascience.stackexchange.com",
                "api_site_parameter": "datascience",
                "logo_url": "https://cdn.sstatic.net/Sites/datascience/Img/logo.png",
                "name": "Data Science",
                "site_type": "main_site"
            },
            "on_date": 1552437954,
            "question_id": 47190
        },
        "owner": {
            "reputation": 21,
            "user_id": 11195712,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/69157f21ecb138aa8db3cc28c98272a3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Krukt",
            "link": "https://stackoverflow.com/users/11195712/krukt"
        },
        "is_answered": true,
        "view_count": 730,
        "accepted_answer_id": 55132746,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1552662975,
        "creation_date": 1552430852,
        "question_id": 55132744,
        "link": "https://stackoverflow.com/questions/55132744/aggregate-values-of-same-name-pandas-dataframe-columns-to-single-column",
        "title": "Aggregate values of same name pandas dataframe columns to single column",
        "body": "<p>I have multiple csv files that were produced by tokenizing code. These files contain keywords in uppercase and lowercase. I would like to merge all those files in one single dataframe which contains all the unique values (summed) in lowercase. What would you suggest to get the result below?</p>\n\n<p>Initial DF:</p>\n\n<pre><code>+---+---+----+-----+\n| a | b |  A |  B  |\n+---+---+----+-----+\n| 1 | 2 |  3 |   1 |\n| 2 | 1 |  3 |   1 |\n+---+---+----+-----+\n</code></pre>\n\n<p>Result</p>\n\n<pre><code>+---+---+\n| a | b |\n+---+---+\n| 4 | 3 |\n| 5 | 2 |\n+---+---+\n</code></pre>\n\n<p>I don't have access to the raw data from which the csv files where created so I cannot correct this at an earlier step. At the moment I have tried mapping .lower() to the dataframe headers that I create, but it returns seperate columns with the same name like so:</p>\n\n<p><a href=\"https://i.stack.imgur.com/vJWAE.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vJWAE.png\" alt=\" .lower() after merging\"></a></p>\n\n<p>Using pandas is not essential. I have thought of converting the csv files to dictionaries and then trying the above procedure (turns out it is much more complicated than I thought), or using lists. Also, group by does not do the job as it will remove non duplicate column names. Any approach is welcome.</p>\n",
        "answer_body": "<h2>Code:</h2>\n\n<p>You could iterate through the columns summing those that have the same lowercase representation:</p>\n\n<pre><code>def sumDupeColumns(df):\n    \"\"\"Return dataframe with columns with the same lowercase spelling summed.\"\"\"\n\n    # Get list of unique lowercase column headers\n    columns = set(map(str.lower, df.columns))\n    # Create new (zero-initialised) dataframe for output\n    df1 = pd.DataFrame(data=np.zeros((len(df), len(columns))), columns=columns)\n\n    # Sum matching columns\n    for col in df.columns:\n        df1[col.lower()] += df[col]\n\n    return df1\n</code></pre>\n\n<hr>\n\n<h2>Example:</h2>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\nnp.random.seed(seed=42)\n\n# Generate DataFrame with random int input and 'duplicate' columns to sum\ndf = pd.DataFrame(columns = ['a','A','b','B','Cc','cC','d','eEe','eeE','Eee'], \n                  data = np.random.randint(9, size=(5,10))\n\ndf = sumDupeColumns(df)\n</code></pre>\n\n<p></p>\n\n<pre><code>&gt;&gt;&gt; print(df)\n\n     d   eee   cc     a     b\n0  6.0  14.0  8.0   9.0  11.0\n1  7.0  10.0  5.0  14.0   7.0\n2  3.0  14.0  8.0   5.0   8.0\n3  3.0  17.0  7.0   8.0  12.0\n4  0.0  11.0  9.0   5.0   9.0\n</code></pre>\n",
        "question_body": "<p>I have multiple csv files that were produced by tokenizing code. These files contain keywords in uppercase and lowercase. I would like to merge all those files in one single dataframe which contains all the unique values (summed) in lowercase. What would you suggest to get the result below?</p>\n\n<p>Initial DF:</p>\n\n<pre><code>+---+---+----+-----+\n| a | b |  A |  B  |\n+---+---+----+-----+\n| 1 | 2 |  3 |   1 |\n| 2 | 1 |  3 |   1 |\n+---+---+----+-----+\n</code></pre>\n\n<p>Result</p>\n\n<pre><code>+---+---+\n| a | b |\n+---+---+\n| 4 | 3 |\n| 5 | 2 |\n+---+---+\n</code></pre>\n\n<p>I don't have access to the raw data from which the csv files where created so I cannot correct this at an earlier step. At the moment I have tried mapping .lower() to the dataframe headers that I create, but it returns seperate columns with the same name like so:</p>\n\n<p><a href=\"https://i.stack.imgur.com/vJWAE.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vJWAE.png\" alt=\" .lower() after merging\"></a></p>\n\n<p>Using pandas is not essential. I have thought of converting the csv files to dictionaries and then trying the above procedure (turns out it is much more complicated than I thought), or using lists. Also, group by does not do the job as it will remove non duplicate column names. Any approach is welcome.</p>\n",
        "formatted_input": {
            "qid": 55132744,
            "link": "https://stackoverflow.com/questions/55132744/aggregate-values-of-same-name-pandas-dataframe-columns-to-single-column",
            "question": {
                "title": "Aggregate values of same name pandas dataframe columns to single column",
                "ques_desc": "I have multiple csv files that were produced by tokenizing code. These files contain keywords in uppercase and lowercase. I would like to merge all those files in one single dataframe which contains all the unique values (summed) in lowercase. What would you suggest to get the result below? Initial DF: Result I don't have access to the raw data from which the csv files where created so I cannot correct this at an earlier step. At the moment I have tried mapping .lower() to the dataframe headers that I create, but it returns seperate columns with the same name like so: Using pandas is not essential. I have thought of converting the csv files to dictionaries and then trying the above procedure (turns out it is much more complicated than I thought), or using lists. Also, group by does not do the job as it will remove non duplicate column names. Any approach is welcome. "
            },
            "io": [
                "+---+---+----+-----+\n| a | b |  A |  B  |\n+---+---+----+-----+\n| 1 | 2 |  3 |   1 |\n| 2 | 1 |  3 |   1 |\n+---+---+----+-----+\n",
                "+---+---+\n| a | b |\n+---+---+\n| 4 | 3 |\n| 5 | 2 |\n+---+---+\n"
            ],
            "answer": {
                "ans_desc": "Code: You could iterate through the columns summing those that have the same lowercase representation: Example: ",
                "code": [
                    "def sumDupeColumns(df):\n    \"\"\"Return dataframe with columns with the same lowercase spelling summed.\"\"\"\n\n    # Get list of unique lowercase column headers\n    columns = set(map(str.lower, df.columns))\n    # Create new (zero-initialised) dataframe for output\n    df1 = pd.DataFrame(data=np.zeros((len(df), len(columns))), columns=columns)\n\n    # Sum matching columns\n    for col in df.columns:\n        df1[col.lower()] += df[col]\n\n    return df1\n",
                    "import pandas as pd\nimport numpy as np\n\nnp.random.seed(seed=42)\n\n# Generate DataFrame with random int input and 'duplicate' columns to sum\ndf = pd.DataFrame(columns = ['a','A','b','B','Cc','cC','d','eEe','eeE','Eee'], \n                  data = np.random.randint(9, size=(5,10))\n\ndf = sumDupeColumns(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 411,
            "user_id": 10268559,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a359ef979ce1fe9c1ac6880ab7bd8a71?s=128&d=identicon&r=PG&f=1",
            "display_name": "MRHarv",
            "link": "https://stackoverflow.com/users/10268559/mrharv"
        },
        "is_answered": true,
        "view_count": 53,
        "accepted_answer_id": 54945139,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1551507229,
        "creation_date": 1551442619,
        "last_edit_date": 1551445537,
        "question_id": 54944513,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54944513/transforming-pandas-dataframe-where-column-entries-are-column-headers",
        "title": "Transforming pandas dataframe, where column entries are column headers",
        "body": "<p>My dataset has 12 columns, X1-X6 and Y1-Y6. The variables X and Y match to each other - the first record means: 80 parts of A, 10 parts of C, 2 parts of J and 8 parts of K (each row has 100 total).</p>\n\n<p>I would like to be able to transform my dataset into a dataset in which the entries in columns X1-X6 are now the headers. See before and after datasets below.</p>\n\n<p>My dataset (before):</p>\n\n<pre><code>   X1 X2 X3   X4   X5   X6    Y1    Y2    Y3    Y4    Y5   Y6\n0   A  C  J    K  NaN  NaN  80.0  10.0   2.0   8.0   NaN  NaN\n1   F  N  O  NaN  NaN  NaN   2.0  25.0  73.0   NaN   NaN  NaN\n2   A  H  J    M  NaN  NaN  70.0   6.0  15.0   9.0   NaN  NaN\n3   B  I  K    P  NaN  NaN   0.5   1.5   2.0  96.0   NaN  NaN\n4   A  B  F    H    O    P  83.0   4.0   9.0   2.0   1.0  1.0\n5   A  B  F    G  NaN  NaN   1.0  16.0   9.0  74.0   NaN  NaN\n6   A  B  D    F    L  NaN  95.0   2.0   1.0   1.0   1.0  NaN\n7   B  F  H    P  NaN  NaN   0.2   0.4   0.4  99.0   NaN  NaN\n8   A  D  F    L  NaN  NaN  35.0  12.0  30.0  23.0   NaN  NaN\n9   A  B  F    I    O  NaN  95.0   0.3   0.1   1.6   3.0  NaN\n10  B  E  G  NaN  NaN  NaN  10.0  31.0  59.0   NaN   NaN  NaN\n11  A  F  G    L  NaN  NaN  24.0   6.0  67.0   3.0   NaN  NaN\n12  A  C  I  NaN  NaN  NaN  65.0  30.0   5.0   NaN   NaN  NaN\n13  A  F  G    L  NaN  NaN  55.0   6.0   4.0  35.0   NaN  NaN\n14  A  F  J    K    L  NaN  22.0   3.0  12.0   0.8  62.2  NaN\n15  B  F  I    P  NaN  NaN   0.6   1.2   0.2  98.0   NaN  NaN\n16  A  B  F    H    O  NaN  27.0   6.0  46.0  13.0   8.0  NaN\n</code></pre>\n\n<p>The dataset I'd like to transform to:</p>\n\n<pre><code>   A     B     C     D     E     F     G     H    I     J    K     L    M  \\\n0 80.0  NaN  10.0   NaN   NaN   NaN   NaN   NaN  NaN   2.0  8.0   NaN  NaN   \n1 NaN   NaN   NaN   NaN   NaN   2.0   NaN   NaN  NaN   NaN  NaN   NaN  NaN   \n2 70.0  NaN   NaN   NaN   NaN   NaN   NaN   6.0  NaN  15.0  NaN   NaN  9.0   \n3 NaN   0.5   NaN   NaN   NaN   NaN   NaN   NaN  1.5   NaN  2.0   NaN  NaN   \n4 83.0  4.0   NaN   NaN   NaN   9.0   NaN   2.0  NaN   NaN  NaN   NaN  NaN   \n5 1.0   16.0   NaN   NaN   NaN   9.0  74.0   NaN  NaN   NaN  NaN   NaN  NaN   \n6 95.0   2.0   NaN   1.0   NaN   1.0   NaN   NaN  NaN   NaN  NaN   1.0  NaN   \n7 NaN   0.2   NaN   NaN   NaN   0.4   NaN   0.4  NaN   NaN  NaN   NaN  NaN   \n8 35.0   NaN   NaN  12.0   NaN  30.0   NaN   NaN  NaN   NaN  NaN  23.0  NaN   \n9 95.0   0.3   NaN   NaN   NaN   0.1   NaN   NaN  1.6   NaN  NaN   NaN  NaN   \n10 NaN  10.0   NaN   NaN  31.0  NaN   59.0   NaN  NaN   NaN  NaN   NaN  NaN   \n11 24.0  NaN   NaN   NaN   NaN   6.0  67.0   NaN  NaN   NaN  NaN   3.0  NaN   \n12 65.0  NaN  30.0   NaN   NaN   NaN   NaN   NaN  5.0   NaN  NaN   NaN  NaN   \n13 55.0  NaN   NaN   NaN   NaN   6.0   4.0   NaN  NaN   NaN  NaN  35.0  NaN   \n14 22.0  NaN   NaN   NaN   NaN   3.0   NaN   NaN  NaN  12.0  0.8  62.2  NaN   \n15 NaN   0.6   NaN   NaN   NaN   1.2   NaN   NaN  0.2   NaN  NaN   NaN  NaN   \n16 27.0  6.0   NaN   NaN   NaN  46.0   NaN  13.0  NaN   NaN  NaN   NaN  NaN   \n\n      N     O     P  \n0    NaN   NaN   NaN  \n1   25.0  73.0   NaN  \n2    NaN   NaN   NaN  \n3    NaN   NaN  96.0  \n4    NaN   1.0   1.0  \n5    NaN   NaN   NaN  \n6    NaN   NaN   NaN  \n7    NaN   NaN  99.0  \n8    NaN   NaN   NaN  \n9    NaN   3.0   NaN  \n10   NaN   NaN   NaN  \n11   NaN   NaN   NaN  \n12   NaN   NaN   NaN  \n13   NaN   NaN   NaN  \n14   NaN   NaN   NaN  \n15   NaN   NaN  98.0  \n16   NaN   8.0   NaN \n</code></pre>\n",
        "answer_body": "<p>As you know that you want the Xi part to contain the column names for the new dataframe, while the Yi part would be the value, it is enough to change every line in a dict where Xi is the key and Yi the value. Then you use the list of that dictionnaries to feed the new dataframe:</p>\n\n<pre><code>data = list(df.apply(lambda x: {x['X'+ str(i)]: x['Y'+str(i)] for i in range(1,7)\n                                if x['X'+str(i)]!= 'NaN'}, axis=1))\n\nresul = pd.DataFrame(data)\nprint(resul)\n</code></pre>\n\n<p>gives:</p>\n\n<pre><code>       A     B     C     D     E     F  ...     K     L    M     N     O     P\n0   80.0   NaN  10.0   NaN   NaN   NaN  ...   8.0   NaN  NaN   NaN   NaN   NaN\n1    NaN   NaN   NaN   NaN   NaN   2.0  ...   NaN   NaN  NaN  25.0  73.0   NaN\n2   70.0   NaN   NaN   NaN   NaN   NaN  ...   NaN   NaN  9.0   NaN   NaN   NaN\n3    NaN   0.5   NaN   NaN   NaN   NaN  ...   2.0   NaN  NaN   NaN   NaN  96.0\n4   83.0   4.0   NaN   NaN   NaN   9.0  ...   NaN   NaN  NaN   NaN   1.0   1.0\n5    1.0  16.0   NaN   NaN   NaN   9.0  ...   NaN   NaN  NaN   NaN   NaN   NaN\n6   95.0   2.0   NaN   1.0   NaN   1.0  ...   NaN   1.0  NaN   NaN   NaN   NaN\n7    NaN   0.2   NaN   NaN   NaN   0.4  ...   NaN   NaN  NaN   NaN   NaN  99.0\n8   35.0   NaN   NaN  12.0   NaN  30.0  ...   NaN  23.0  NaN   NaN   NaN   NaN\n9   95.0   0.3   NaN   NaN   NaN   0.1  ...   NaN   NaN  NaN   NaN   3.0   NaN\n10   NaN  10.0   NaN   NaN  31.0   NaN  ...   NaN   NaN  NaN   NaN   NaN   NaN\n11  24.0   NaN   NaN   NaN   NaN   6.0  ...   NaN   3.0  NaN   NaN   NaN   NaN\n12  65.0   NaN  30.0   NaN   NaN   NaN  ...   NaN   NaN  NaN   NaN   NaN   NaN\n13  55.0   NaN   NaN   NaN   NaN   6.0  ...   NaN  35.0  NaN   NaN   NaN   NaN\n14  22.0   NaN   NaN   NaN   NaN   3.0  ...   0.8  62.2  NaN   NaN   NaN   NaN\n15   NaN   0.6   NaN   NaN   NaN   1.2  ...   NaN   NaN  NaN   NaN   NaN  98.0\n16  27.0   6.0   NaN   NaN   NaN  46.0  ...   NaN   NaN  NaN   NaN   8.0   NaN\n\n[17 rows x 16 columns]\n</code></pre>\n",
        "question_body": "<p>My dataset has 12 columns, X1-X6 and Y1-Y6. The variables X and Y match to each other - the first record means: 80 parts of A, 10 parts of C, 2 parts of J and 8 parts of K (each row has 100 total).</p>\n\n<p>I would like to be able to transform my dataset into a dataset in which the entries in columns X1-X6 are now the headers. See before and after datasets below.</p>\n\n<p>My dataset (before):</p>\n\n<pre><code>   X1 X2 X3   X4   X5   X6    Y1    Y2    Y3    Y4    Y5   Y6\n0   A  C  J    K  NaN  NaN  80.0  10.0   2.0   8.0   NaN  NaN\n1   F  N  O  NaN  NaN  NaN   2.0  25.0  73.0   NaN   NaN  NaN\n2   A  H  J    M  NaN  NaN  70.0   6.0  15.0   9.0   NaN  NaN\n3   B  I  K    P  NaN  NaN   0.5   1.5   2.0  96.0   NaN  NaN\n4   A  B  F    H    O    P  83.0   4.0   9.0   2.0   1.0  1.0\n5   A  B  F    G  NaN  NaN   1.0  16.0   9.0  74.0   NaN  NaN\n6   A  B  D    F    L  NaN  95.0   2.0   1.0   1.0   1.0  NaN\n7   B  F  H    P  NaN  NaN   0.2   0.4   0.4  99.0   NaN  NaN\n8   A  D  F    L  NaN  NaN  35.0  12.0  30.0  23.0   NaN  NaN\n9   A  B  F    I    O  NaN  95.0   0.3   0.1   1.6   3.0  NaN\n10  B  E  G  NaN  NaN  NaN  10.0  31.0  59.0   NaN   NaN  NaN\n11  A  F  G    L  NaN  NaN  24.0   6.0  67.0   3.0   NaN  NaN\n12  A  C  I  NaN  NaN  NaN  65.0  30.0   5.0   NaN   NaN  NaN\n13  A  F  G    L  NaN  NaN  55.0   6.0   4.0  35.0   NaN  NaN\n14  A  F  J    K    L  NaN  22.0   3.0  12.0   0.8  62.2  NaN\n15  B  F  I    P  NaN  NaN   0.6   1.2   0.2  98.0   NaN  NaN\n16  A  B  F    H    O  NaN  27.0   6.0  46.0  13.0   8.0  NaN\n</code></pre>\n\n<p>The dataset I'd like to transform to:</p>\n\n<pre><code>   A     B     C     D     E     F     G     H    I     J    K     L    M  \\\n0 80.0  NaN  10.0   NaN   NaN   NaN   NaN   NaN  NaN   2.0  8.0   NaN  NaN   \n1 NaN   NaN   NaN   NaN   NaN   2.0   NaN   NaN  NaN   NaN  NaN   NaN  NaN   \n2 70.0  NaN   NaN   NaN   NaN   NaN   NaN   6.0  NaN  15.0  NaN   NaN  9.0   \n3 NaN   0.5   NaN   NaN   NaN   NaN   NaN   NaN  1.5   NaN  2.0   NaN  NaN   \n4 83.0  4.0   NaN   NaN   NaN   9.0   NaN   2.0  NaN   NaN  NaN   NaN  NaN   \n5 1.0   16.0   NaN   NaN   NaN   9.0  74.0   NaN  NaN   NaN  NaN   NaN  NaN   \n6 95.0   2.0   NaN   1.0   NaN   1.0   NaN   NaN  NaN   NaN  NaN   1.0  NaN   \n7 NaN   0.2   NaN   NaN   NaN   0.4   NaN   0.4  NaN   NaN  NaN   NaN  NaN   \n8 35.0   NaN   NaN  12.0   NaN  30.0   NaN   NaN  NaN   NaN  NaN  23.0  NaN   \n9 95.0   0.3   NaN   NaN   NaN   0.1   NaN   NaN  1.6   NaN  NaN   NaN  NaN   \n10 NaN  10.0   NaN   NaN  31.0  NaN   59.0   NaN  NaN   NaN  NaN   NaN  NaN   \n11 24.0  NaN   NaN   NaN   NaN   6.0  67.0   NaN  NaN   NaN  NaN   3.0  NaN   \n12 65.0  NaN  30.0   NaN   NaN   NaN   NaN   NaN  5.0   NaN  NaN   NaN  NaN   \n13 55.0  NaN   NaN   NaN   NaN   6.0   4.0   NaN  NaN   NaN  NaN  35.0  NaN   \n14 22.0  NaN   NaN   NaN   NaN   3.0   NaN   NaN  NaN  12.0  0.8  62.2  NaN   \n15 NaN   0.6   NaN   NaN   NaN   1.2   NaN   NaN  0.2   NaN  NaN   NaN  NaN   \n16 27.0  6.0   NaN   NaN   NaN  46.0   NaN  13.0  NaN   NaN  NaN   NaN  NaN   \n\n      N     O     P  \n0    NaN   NaN   NaN  \n1   25.0  73.0   NaN  \n2    NaN   NaN   NaN  \n3    NaN   NaN  96.0  \n4    NaN   1.0   1.0  \n5    NaN   NaN   NaN  \n6    NaN   NaN   NaN  \n7    NaN   NaN  99.0  \n8    NaN   NaN   NaN  \n9    NaN   3.0   NaN  \n10   NaN   NaN   NaN  \n11   NaN   NaN   NaN  \n12   NaN   NaN   NaN  \n13   NaN   NaN   NaN  \n14   NaN   NaN   NaN  \n15   NaN   NaN  98.0  \n16   NaN   8.0   NaN \n</code></pre>\n",
        "formatted_input": {
            "qid": 54944513,
            "link": "https://stackoverflow.com/questions/54944513/transforming-pandas-dataframe-where-column-entries-are-column-headers",
            "question": {
                "title": "Transforming pandas dataframe, where column entries are column headers",
                "ques_desc": "My dataset has 12 columns, X1-X6 and Y1-Y6. The variables X and Y match to each other - the first record means: 80 parts of A, 10 parts of C, 2 parts of J and 8 parts of K (each row has 100 total). I would like to be able to transform my dataset into a dataset in which the entries in columns X1-X6 are now the headers. See before and after datasets below. My dataset (before): The dataset I'd like to transform to: "
            },
            "io": [
                "   X1 X2 X3   X4   X5   X6    Y1    Y2    Y3    Y4    Y5   Y6\n0   A  C  J    K  NaN  NaN  80.0  10.0   2.0   8.0   NaN  NaN\n1   F  N  O  NaN  NaN  NaN   2.0  25.0  73.0   NaN   NaN  NaN\n2   A  H  J    M  NaN  NaN  70.0   6.0  15.0   9.0   NaN  NaN\n3   B  I  K    P  NaN  NaN   0.5   1.5   2.0  96.0   NaN  NaN\n4   A  B  F    H    O    P  83.0   4.0   9.0   2.0   1.0  1.0\n5   A  B  F    G  NaN  NaN   1.0  16.0   9.0  74.0   NaN  NaN\n6   A  B  D    F    L  NaN  95.0   2.0   1.0   1.0   1.0  NaN\n7   B  F  H    P  NaN  NaN   0.2   0.4   0.4  99.0   NaN  NaN\n8   A  D  F    L  NaN  NaN  35.0  12.0  30.0  23.0   NaN  NaN\n9   A  B  F    I    O  NaN  95.0   0.3   0.1   1.6   3.0  NaN\n10  B  E  G  NaN  NaN  NaN  10.0  31.0  59.0   NaN   NaN  NaN\n11  A  F  G    L  NaN  NaN  24.0   6.0  67.0   3.0   NaN  NaN\n12  A  C  I  NaN  NaN  NaN  65.0  30.0   5.0   NaN   NaN  NaN\n13  A  F  G    L  NaN  NaN  55.0   6.0   4.0  35.0   NaN  NaN\n14  A  F  J    K    L  NaN  22.0   3.0  12.0   0.8  62.2  NaN\n15  B  F  I    P  NaN  NaN   0.6   1.2   0.2  98.0   NaN  NaN\n16  A  B  F    H    O  NaN  27.0   6.0  46.0  13.0   8.0  NaN\n",
                "   A     B     C     D     E     F     G     H    I     J    K     L    M  \\\n0 80.0  NaN  10.0   NaN   NaN   NaN   NaN   NaN  NaN   2.0  8.0   NaN  NaN   \n1 NaN   NaN   NaN   NaN   NaN   2.0   NaN   NaN  NaN   NaN  NaN   NaN  NaN   \n2 70.0  NaN   NaN   NaN   NaN   NaN   NaN   6.0  NaN  15.0  NaN   NaN  9.0   \n3 NaN   0.5   NaN   NaN   NaN   NaN   NaN   NaN  1.5   NaN  2.0   NaN  NaN   \n4 83.0  4.0   NaN   NaN   NaN   9.0   NaN   2.0  NaN   NaN  NaN   NaN  NaN   \n5 1.0   16.0   NaN   NaN   NaN   9.0  74.0   NaN  NaN   NaN  NaN   NaN  NaN   \n6 95.0   2.0   NaN   1.0   NaN   1.0   NaN   NaN  NaN   NaN  NaN   1.0  NaN   \n7 NaN   0.2   NaN   NaN   NaN   0.4   NaN   0.4  NaN   NaN  NaN   NaN  NaN   \n8 35.0   NaN   NaN  12.0   NaN  30.0   NaN   NaN  NaN   NaN  NaN  23.0  NaN   \n9 95.0   0.3   NaN   NaN   NaN   0.1   NaN   NaN  1.6   NaN  NaN   NaN  NaN   \n10 NaN  10.0   NaN   NaN  31.0  NaN   59.0   NaN  NaN   NaN  NaN   NaN  NaN   \n11 24.0  NaN   NaN   NaN   NaN   6.0  67.0   NaN  NaN   NaN  NaN   3.0  NaN   \n12 65.0  NaN  30.0   NaN   NaN   NaN   NaN   NaN  5.0   NaN  NaN   NaN  NaN   \n13 55.0  NaN   NaN   NaN   NaN   6.0   4.0   NaN  NaN   NaN  NaN  35.0  NaN   \n14 22.0  NaN   NaN   NaN   NaN   3.0   NaN   NaN  NaN  12.0  0.8  62.2  NaN   \n15 NaN   0.6   NaN   NaN   NaN   1.2   NaN   NaN  0.2   NaN  NaN   NaN  NaN   \n16 27.0  6.0   NaN   NaN   NaN  46.0   NaN  13.0  NaN   NaN  NaN   NaN  NaN   \n\n      N     O     P  \n0    NaN   NaN   NaN  \n1   25.0  73.0   NaN  \n2    NaN   NaN   NaN  \n3    NaN   NaN  96.0  \n4    NaN   1.0   1.0  \n5    NaN   NaN   NaN  \n6    NaN   NaN   NaN  \n7    NaN   NaN  99.0  \n8    NaN   NaN   NaN  \n9    NaN   3.0   NaN  \n10   NaN   NaN   NaN  \n11   NaN   NaN   NaN  \n12   NaN   NaN   NaN  \n13   NaN   NaN   NaN  \n14   NaN   NaN   NaN  \n15   NaN   NaN  98.0  \n16   NaN   8.0   NaN \n"
            ],
            "answer": {
                "ans_desc": "As you know that you want the Xi part to contain the column names for the new dataframe, while the Yi part would be the value, it is enough to change every line in a dict where Xi is the key and Yi the value. Then you use the list of that dictionnaries to feed the new dataframe: gives: ",
                "code": [
                    "data = list(df.apply(lambda x: {x['X'+ str(i)]: x['Y'+str(i)] for i in range(1,7)\n                                if x['X'+str(i)]!= 'NaN'}, axis=1))\n\nresul = pd.DataFrame(data)\nprint(resul)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 10547397,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7695c0795c4ca07224bfd772864b4edd?s=128&d=identicon&r=PG&f=1",
            "display_name": "quicklegit",
            "link": "https://stackoverflow.com/users/10547397/quicklegit"
        },
        "is_answered": true,
        "view_count": 4129,
        "accepted_answer_id": 54923471,
        "answer_count": 1,
        "score": 9,
        "last_activity_date": 1551350114,
        "creation_date": 1551349354,
        "question_id": 54923349,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54923349/top-3-values-per-row-in-pandas",
        "title": "Top 3 Values Per Row in Pandas",
        "body": "<p>I have a large Pandas dataframe that is in the vein of:</p>\n\n<pre><code>| ID | Var1 | Var2 | Var3 | Var4 | Var5 |\n|----|------|------|------|------|------|\n| 1  | 1    | 2    | 3    | 4    | 5    |\n| 2  | 10   | 9    | 8    | 7    | 6    |\n| 3  | 25   | 37   | 41   | 24   | 21   |\n| 4  | 102  | 11   | 72   | 56   | 151  |\n...\n</code></pre>\n\n<p>and I would like to generate output that looks like this, taking the column names of the 3 highest values for each row:</p>\n\n<pre><code>| ID | 1st Max | 2nd Max | 3rd Max |\n|----|---------|---------|---------|\n| 1  | Var5    | Var4    | Var3    |\n| 2  | Var1    | Var2    | Var3    |\n| 3  | Var3    | Var2    | Var1    |\n| 4  | Var5    | Var1    | Var3    |\n...\n</code></pre>\n\n<p>I have tried using df.idmax(axis=1) which returns the 1st maximum column name but am unsure how to compute the other two? </p>\n\n<p>Any help on this would be truly appreciated, thanks! </p>\n",
        "answer_body": "<p>Use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\" rel=\"noreferrer\"><code>numpy.argsort</code></a> for positions of sorted values with select <code>top3</code> by indexing, last pass it to <code>DataFrame</code> constructor:</p>\n\n<pre><code>df = df.set_index('ID')\ndf = pd.DataFrame(df.columns.values[np.argsort(-df.values, axis=1)[:, :3]], \n                  index=df.index,\n                  columns = ['1st Max','2nd Max','3rd Max']).reset_index()\nprint (df)\n   ID 1st Max 2nd Max 3rd Max\n0   1    Var5    Var4    Var3\n1   2    Var1    Var2    Var3\n2   3    Var3    Var2    Var1\n3   4    Var5    Var1    Var3\n</code></pre>\n\n<p>Or if performance is not important use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nlargest.html\" rel=\"noreferrer\"><code>nlargest</code></a> with <code>apply</code> per each row:</p>\n\n<pre><code>c = ['1st Max','2nd Max','3rd Max']\ndf = (df.set_index('ID')\n        .apply(lambda x: pd.Series(x.nlargest(3).index, index=c), axis=1)\n        .reset_index())\n</code></pre>\n",
        "question_body": "<p>I have a large Pandas dataframe that is in the vein of:</p>\n\n<pre><code>| ID | Var1 | Var2 | Var3 | Var4 | Var5 |\n|----|------|------|------|------|------|\n| 1  | 1    | 2    | 3    | 4    | 5    |\n| 2  | 10   | 9    | 8    | 7    | 6    |\n| 3  | 25   | 37   | 41   | 24   | 21   |\n| 4  | 102  | 11   | 72   | 56   | 151  |\n...\n</code></pre>\n\n<p>and I would like to generate output that looks like this, taking the column names of the 3 highest values for each row:</p>\n\n<pre><code>| ID | 1st Max | 2nd Max | 3rd Max |\n|----|---------|---------|---------|\n| 1  | Var5    | Var4    | Var3    |\n| 2  | Var1    | Var2    | Var3    |\n| 3  | Var3    | Var2    | Var1    |\n| 4  | Var5    | Var1    | Var3    |\n...\n</code></pre>\n\n<p>I have tried using df.idmax(axis=1) which returns the 1st maximum column name but am unsure how to compute the other two? </p>\n\n<p>Any help on this would be truly appreciated, thanks! </p>\n",
        "formatted_input": {
            "qid": 54923349,
            "link": "https://stackoverflow.com/questions/54923349/top-3-values-per-row-in-pandas",
            "question": {
                "title": "Top 3 Values Per Row in Pandas",
                "ques_desc": "I have a large Pandas dataframe that is in the vein of: and I would like to generate output that looks like this, taking the column names of the 3 highest values for each row: I have tried using df.idmax(axis=1) which returns the 1st maximum column name but am unsure how to compute the other two? Any help on this would be truly appreciated, thanks! "
            },
            "io": [
                "| ID | Var1 | Var2 | Var3 | Var4 | Var5 |\n|----|------|------|------|------|------|\n| 1  | 1    | 2    | 3    | 4    | 5    |\n| 2  | 10   | 9    | 8    | 7    | 6    |\n| 3  | 25   | 37   | 41   | 24   | 21   |\n| 4  | 102  | 11   | 72   | 56   | 151  |\n...\n",
                "| ID | 1st Max | 2nd Max | 3rd Max |\n|----|---------|---------|---------|\n| 1  | Var5    | Var4    | Var3    |\n| 2  | Var1    | Var2    | Var3    |\n| 3  | Var3    | Var2    | Var1    |\n| 4  | Var5    | Var1    | Var3    |\n...\n"
            ],
            "answer": {
                "ans_desc": "Use for positions of sorted values with select by indexing, last pass it to constructor: Or if performance is not important use with per each row: ",
                "code": [
                    "c = ['1st Max','2nd Max','3rd Max']\ndf = (df.set_index('ID')\n        .apply(lambda x: pd.Series(x.nlargest(3).index, index=c), axis=1)\n        .reset_index())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "label"
        ],
        "owner": {
            "reputation": 787,
            "user_id": 7062012,
            "user_type": "registered",
            "accept_rate": 79,
            "profile_image": "https://i.stack.imgur.com/CQZ0w.jpg?s=128&g=1",
            "display_name": "August Williams",
            "link": "https://stackoverflow.com/users/7062012/august-williams"
        },
        "is_answered": true,
        "view_count": 9251,
        "accepted_answer_id": 42142888,
        "answer_count": 2,
        "score": 13,
        "last_activity_date": 1551305077,
        "creation_date": 1486660086,
        "last_edit_date": 1486667478,
        "question_id": 42142756,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/42142756/how-can-i-change-a-specific-row-label-in-a-pandas-dataframe",
        "title": "How can I change a specific row label in a Pandas dataframe?",
        "body": "<p>I have a dataframe such as:</p>\n\n<pre><code>      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\n5  11.4   5.6  3.2  1.6  0.8  1.0\n</code></pre>\n\n<p>Where the final row contains averages. I would like to rename the final row label to <code>\"A\"</code> so that the dataframe will look like this:</p>\n\n<pre><code>      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\nA  11.4   5.6  3.2  1.6  0.8  1.0\n</code></pre>\n\n<p>I understand columns can be done with <code>df.columns = . . .</code>. But how can I do this with a specific row label?</p>\n",
        "answer_body": "<p>You can get the last index using negative indexing similar to that in Python </p>\n\n<pre><code>last = df.index[-1]\n</code></pre>\n\n<p>Then</p>\n\n<pre><code>df = df.rename(index={last: 'a'})\n</code></pre>\n\n<p>Edit: If you are looking for a one-liner,</p>\n\n<pre><code>df.index = df.index[:-1].tolist() + ['a']\n</code></pre>\n",
        "question_body": "<p>I have a dataframe such as:</p>\n\n<pre><code>      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\n5  11.4   5.6  3.2  1.6  0.8  1.0\n</code></pre>\n\n<p>Where the final row contains averages. I would like to rename the final row label to <code>\"A\"</code> so that the dataframe will look like this:</p>\n\n<pre><code>      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\nA  11.4   5.6  3.2  1.6  0.8  1.0\n</code></pre>\n\n<p>I understand columns can be done with <code>df.columns = . . .</code>. But how can I do this with a specific row label?</p>\n",
        "formatted_input": {
            "qid": 42142756,
            "link": "https://stackoverflow.com/questions/42142756/how-can-i-change-a-specific-row-label-in-a-pandas-dataframe",
            "question": {
                "title": "How can I change a specific row label in a Pandas dataframe?",
                "ques_desc": "I have a dataframe such as: Where the final row contains averages. I would like to rename the final row label to so that the dataframe will look like this: I understand columns can be done with . But how can I do this with a specific row label? "
            },
            "io": [
                "      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\n5  11.4   5.6  3.2  1.6  0.8  1.0\n",
                "      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\nA  11.4   5.6  3.2  1.6  0.8  1.0\n"
            ],
            "answer": {
                "ans_desc": "You can get the last index using negative indexing similar to that in Python Then Edit: If you are looking for a one-liner, ",
                "code": [
                    "df.index = df.index[:-1].tolist() + ['a']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "datetime",
            "dataframe"
        ],
        "owner": {
            "reputation": 519,
            "user_id": 6014171,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/df7efae0e1b0437649d43ad6cbac0ac5?s=128&d=identicon&r=PG&f=1",
            "display_name": "cholo14",
            "link": "https://stackoverflow.com/users/6014171/cholo14"
        },
        "is_answered": true,
        "view_count": 96,
        "accepted_answer_id": 54901541,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1551257981,
        "creation_date": 1551203392,
        "last_edit_date": 1551220332,
        "question_id": 54891381,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54891381/change-day-to-specific-entries-in-pandas-dataframe",
        "title": "Change day to specific entries in pandas dataframe",
        "body": "<p>I have a dataframe in pandas which has an error in the index: each entry between 23:00:00 and 23:59:59 has a wrong date. I would need to subtract one  day (i.e. 24 hours) to each entry between those two times. </p>\n\n<p>I know that I can obtain the entries between those two times as <code>df[df.hour == 23]</code>, where <code>df</code> is my dataframe. However, can I modify the day only for those specific entries of the dataframe index? </p>\n\n<p>Resetting would take me more time, since my dataframe index is not evenly spaced as you can see from the figure below (the step between two consecutive entries is once 15 minutes and once 30 minutes). Note also from the figure the wrong date in the last three entries: it should be 2018-02-05 and not 2018-02-06.</p>\n\n<p>I tried to do this</p>\n\n<pre><code>df[df.index.hour == 23].index.day = df[df.index.hour == 23].index.day - 1\n</code></pre>\n\n<p>but I get <code>AttributeError: can't set attribute</code></p>\n\n<p>Sample data:</p>\n\n<pre><code>2018-02-05 22:00:00    271.8000\n2018-02-05 22:30:00    271.5600\n2018-02-05 22:45:00    271.4400\n2018-02-06 23:15:00    271.3750\n2018-02-06 23:30:00    271.3425\n2018-02-06 00:00:00    271.2700\n2018-02-06 00:15:00    271.2300\n2018-02-06 00:45:00    271.1500\n2018-02-06 01:00:00    271.1475\n2018-02-06 01:30:00    271.1425\n2018-02-06 01:45:00    271.1400\n</code></pre>\n\n<p>Expected output:</p>\n\n<pre><code>2018-02-05 22:00:00    271.8000\n2018-02-05 22:30:00    271.5600\n2018-02-05 22:45:00    271.4400\n2018-02-05 23:15:00    271.3750\n2018-02-05 23:30:00    271.3425\n2018-02-06 00:00:00    271.2700\n2018-02-06 00:15:00    271.2300\n2018-02-06 00:45:00    271.1500\n2018-02-06 01:00:00    271.1475\n2018-02-06 01:30:00    271.1425\n2018-02-06 01:45:00    271.1400\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/8Y4aM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8Y4aM.png\" alt=\"enter image description here\"></a></p>\n",
        "answer_body": "<p>I solved the issue myself by using <a href=\"https://stackoverflow.com/a/40428133/6014171\">this answer</a>. This is my code:</p>\n\n<pre><code>as_list = df.index.tolist()\nnew_index = []\nfor idx,entry in enumerate(as_list):\n    if entry.hour == 23:\n        if entry.day != 1:            \n            new_index.append(as_list[idx].replace(day = as_list[idx].day - 1))\n        else:\n            new_day = calendar.monthrange(as_list[idx].year, as_list[idx].month -1)[1]\n            new_index.append(as_list[idx].replace(day = new_day, month = entry.month -1))\n    else:\n        new_index.append(entry)\ndf.index = new_index\n</code></pre>\n",
        "question_body": "<p>I have a dataframe in pandas which has an error in the index: each entry between 23:00:00 and 23:59:59 has a wrong date. I would need to subtract one  day (i.e. 24 hours) to each entry between those two times. </p>\n\n<p>I know that I can obtain the entries between those two times as <code>df[df.hour == 23]</code>, where <code>df</code> is my dataframe. However, can I modify the day only for those specific entries of the dataframe index? </p>\n\n<p>Resetting would take me more time, since my dataframe index is not evenly spaced as you can see from the figure below (the step between two consecutive entries is once 15 minutes and once 30 minutes). Note also from the figure the wrong date in the last three entries: it should be 2018-02-05 and not 2018-02-06.</p>\n\n<p>I tried to do this</p>\n\n<pre><code>df[df.index.hour == 23].index.day = df[df.index.hour == 23].index.day - 1\n</code></pre>\n\n<p>but I get <code>AttributeError: can't set attribute</code></p>\n\n<p>Sample data:</p>\n\n<pre><code>2018-02-05 22:00:00    271.8000\n2018-02-05 22:30:00    271.5600\n2018-02-05 22:45:00    271.4400\n2018-02-06 23:15:00    271.3750\n2018-02-06 23:30:00    271.3425\n2018-02-06 00:00:00    271.2700\n2018-02-06 00:15:00    271.2300\n2018-02-06 00:45:00    271.1500\n2018-02-06 01:00:00    271.1475\n2018-02-06 01:30:00    271.1425\n2018-02-06 01:45:00    271.1400\n</code></pre>\n\n<p>Expected output:</p>\n\n<pre><code>2018-02-05 22:00:00    271.8000\n2018-02-05 22:30:00    271.5600\n2018-02-05 22:45:00    271.4400\n2018-02-05 23:15:00    271.3750\n2018-02-05 23:30:00    271.3425\n2018-02-06 00:00:00    271.2700\n2018-02-06 00:15:00    271.2300\n2018-02-06 00:45:00    271.1500\n2018-02-06 01:00:00    271.1475\n2018-02-06 01:30:00    271.1425\n2018-02-06 01:45:00    271.1400\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/8Y4aM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8Y4aM.png\" alt=\"enter image description here\"></a></p>\n",
        "formatted_input": {
            "qid": 54891381,
            "link": "https://stackoverflow.com/questions/54891381/change-day-to-specific-entries-in-pandas-dataframe",
            "question": {
                "title": "Change day to specific entries in pandas dataframe",
                "ques_desc": "I have a dataframe in pandas which has an error in the index: each entry between 23:00:00 and 23:59:59 has a wrong date. I would need to subtract one day (i.e. 24 hours) to each entry between those two times. I know that I can obtain the entries between those two times as , where is my dataframe. However, can I modify the day only for those specific entries of the dataframe index? Resetting would take me more time, since my dataframe index is not evenly spaced as you can see from the figure below (the step between two consecutive entries is once 15 minutes and once 30 minutes). Note also from the figure the wrong date in the last three entries: it should be 2018-02-05 and not 2018-02-06. I tried to do this but I get Sample data: Expected output: "
            },
            "io": [
                "2018-02-05 22:00:00    271.8000\n2018-02-05 22:30:00    271.5600\n2018-02-05 22:45:00    271.4400\n2018-02-06 23:15:00    271.3750\n2018-02-06 23:30:00    271.3425\n2018-02-06 00:00:00    271.2700\n2018-02-06 00:15:00    271.2300\n2018-02-06 00:45:00    271.1500\n2018-02-06 01:00:00    271.1475\n2018-02-06 01:30:00    271.1425\n2018-02-06 01:45:00    271.1400\n",
                "2018-02-05 22:00:00    271.8000\n2018-02-05 22:30:00    271.5600\n2018-02-05 22:45:00    271.4400\n2018-02-05 23:15:00    271.3750\n2018-02-05 23:30:00    271.3425\n2018-02-06 00:00:00    271.2700\n2018-02-06 00:15:00    271.2300\n2018-02-06 00:45:00    271.1500\n2018-02-06 01:00:00    271.1475\n2018-02-06 01:30:00    271.1425\n2018-02-06 01:45:00    271.1400\n"
            ],
            "answer": {
                "ans_desc": "I solved the issue myself by using this answer. This is my code: ",
                "code": [
                    "as_list = df.index.tolist()\nnew_index = []\nfor idx,entry in enumerate(as_list):\n    if entry.hour == 23:\n        if entry.day != 1:            \n            new_index.append(as_list[idx].replace(day = as_list[idx].day - 1))\n        else:\n            new_day = calendar.monthrange(as_list[idx].year, as_list[idx].month -1)[1]\n            new_index.append(as_list[idx].replace(day = new_day, month = entry.month -1))\n    else:\n        new_index.append(entry)\ndf.index = new_index\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 968,
        "accepted_answer_id": 54884193,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1551179908,
        "creation_date": 1551179299,
        "question_id": 54884153,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54884153/remove-rows-when-the-occurrence-of-a-column-value-in-the-data-frame-is-less-than",
        "title": "Remove rows when the occurrence of a column value in the data frame is less than a certain number using pandas/python?",
        "body": "<p>I have a data frame like this:</p>\n\n<pre><code>df\ncol1    col2\nA         1\nB         1\nC         2\nD         3\nD         2\nB         1\nD         5\n</code></pre>\n\n<p>I have seen that col1 values with B and D occurs more than one times in the data frame.</p>\n\n<p>I want to keep those values with occurrence more than one, the final data frame will look like:</p>\n\n<pre><code>col1     col2\n B         1\n D         3\n D         2\n B         1\n D         5\n</code></pre>\n\n<p>How to do this in most efficient way using pandas/python ? </p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html\" rel=\"nofollow noreferrer\"><code>DataFrame.duplicated</code></a> with specify column <code>col1</code> for search dupes with <code>keep=False</code> for return <code>True</code>s for all dupe rows, last filter by <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a>:</p>\n\n<pre><code>df = df[df.duplicated('col1', keep=False)]\nprint (df)\n  col1  col2\n1    B     1\n3    D     3\n4    D     2\n5    B     1\n6    D     5\n</code></pre>\n\n<p>If need specify threshold use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>transform</code></a> with <code>size</code> and filter same way like first solution:</p>\n\n<pre><code>df = df[df.groupby('col1')['col1'].transform('size') &gt; 1]\nprint (df)\n  col1  col2\n1    B     1\n3    D     3\n4    D     2\n5    B     1\n6    D     5\n</code></pre>\n\n<p>Alternative solution with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\" rel=\"nofollow noreferrer\"><code>value_counts</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\" rel=\"nofollow noreferrer\"><code>map</code></a>:</p>\n\n<pre><code>df = df[df['col1'].map(df['col1'].value_counts()) &gt; 1]\n</code></pre>\n\n<p>If performance is not important use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.filter.html\" rel=\"nofollow noreferrer\"><code>DataFrameGroupBy.filter</code></a>:</p>\n\n<pre><code>df = df.groupby('col1').filter(lambda x: len(x) &gt; 1)\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this:</p>\n\n<pre><code>df\ncol1    col2\nA         1\nB         1\nC         2\nD         3\nD         2\nB         1\nD         5\n</code></pre>\n\n<p>I have seen that col1 values with B and D occurs more than one times in the data frame.</p>\n\n<p>I want to keep those values with occurrence more than one, the final data frame will look like:</p>\n\n<pre><code>col1     col2\n B         1\n D         3\n D         2\n B         1\n D         5\n</code></pre>\n\n<p>How to do this in most efficient way using pandas/python ? </p>\n",
        "formatted_input": {
            "qid": 54884153,
            "link": "https://stackoverflow.com/questions/54884153/remove-rows-when-the-occurrence-of-a-column-value-in-the-data-frame-is-less-than",
            "question": {
                "title": "Remove rows when the occurrence of a column value in the data frame is less than a certain number using pandas/python?",
                "ques_desc": "I have a data frame like this: I have seen that col1 values with B and D occurs more than one times in the data frame. I want to keep those values with occurrence more than one, the final data frame will look like: How to do this in most efficient way using pandas/python ? "
            },
            "io": [
                "df\ncol1    col2\nA         1\nB         1\nC         2\nD         3\nD         2\nB         1\nD         5\n",
                "col1     col2\n B         1\n D         3\n D         2\n B         1\n D         5\n"
            ],
            "answer": {
                "ans_desc": "Use with specify column for search dupes with for return s for all dupe rows, last filter by : If need specify threshold use with and filter same way like first solution: Alternative solution with and : If performance is not important use : ",
                "code": [
                    "df = df[df['col1'].map(df['col1'].value_counts()) > 1]\n",
                    "df = df.groupby('col1').filter(lambda x: len(x) > 1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 581,
            "user_id": 5617580,
            "user_type": "registered",
            "accept_rate": 82,
            "profile_image": "https://www.gravatar.com/avatar/3d231da78780209632350c3475e6587a?s=128&d=identicon&r=PG&f=1",
            "display_name": "Bing",
            "link": "https://stackoverflow.com/users/5617580/bing"
        },
        "is_answered": true,
        "view_count": 266,
        "accepted_answer_id": 54806718,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1550751070,
        "creation_date": 1550750747,
        "last_edit_date": 1550751070,
        "question_id": 54806672,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54806672/pandascalculate-mean-of-a-group-of-n-values-of-each-columns-of-a-dataframe",
        "title": "Pandas:Calculate mean of a group of n values of each columns of a dataframe",
        "body": "<p>I have a dataframe of the following type:</p>\n\n<pre><code>     A      B    \n0    1      2    \n1    4      5    \n2    7      8    \n3    10    11   \n4    13    14   \n5    16    17   \n</code></pre>\n\n<p>I want to calculate the mean of the first 3 element of each column and then next 3 elements and so on and then store in a dataframe.</p>\n\n<p>Desired Output-</p>\n\n<pre><code>      A      B    \n0     4      5\n1     12    14\n</code></pre>\n\n<p>Using Group By was one of the approach I thought of but I am unable to figure out how to use Group by in this case.</p>\n",
        "answer_body": "<p>If default <code>RangeIndex</code> then use integer division and pass to <code>groupby</code>:</p>\n\n<pre><code>df = df.groupby(df.index // 3).mean()\nprint (df)\n    A   B\n0   4   5\n1  13  14\n</code></pre>\n\n<p><strong>Detail</strong>:</p>\n\n<pre><code>print (df.index // 3)\nInt64Index([0, 0, 0, 1, 1, 1], dtype='int64')\n</code></pre>\n\n<p>General solution with array created by length of DataFrame - working with all index values:</p>\n\n<pre><code>df = df.groupby(np.arange(len(df)) // 3).mean()\n</code></pre>\n\n<p><strong>Detail</strong>:</p>\n\n<pre><code>print (np.arange(len(df)) // 3)\n[0 0 0 1 1 1]\n</code></pre>\n",
        "question_body": "<p>I have a dataframe of the following type:</p>\n\n<pre><code>     A      B    \n0    1      2    \n1    4      5    \n2    7      8    \n3    10    11   \n4    13    14   \n5    16    17   \n</code></pre>\n\n<p>I want to calculate the mean of the first 3 element of each column and then next 3 elements and so on and then store in a dataframe.</p>\n\n<p>Desired Output-</p>\n\n<pre><code>      A      B    \n0     4      5\n1     12    14\n</code></pre>\n\n<p>Using Group By was one of the approach I thought of but I am unable to figure out how to use Group by in this case.</p>\n",
        "formatted_input": {
            "qid": 54806672,
            "link": "https://stackoverflow.com/questions/54806672/pandascalculate-mean-of-a-group-of-n-values-of-each-columns-of-a-dataframe",
            "question": {
                "title": "Pandas:Calculate mean of a group of n values of each columns of a dataframe",
                "ques_desc": "I have a dataframe of the following type: I want to calculate the mean of the first 3 element of each column and then next 3 elements and so on and then store in a dataframe. Desired Output- Using Group By was one of the approach I thought of but I am unable to figure out how to use Group by in this case. "
            },
            "io": [
                "     A      B    \n0    1      2    \n1    4      5    \n2    7      8    \n3    10    11   \n4    13    14   \n5    16    17   \n",
                "      A      B    \n0     4      5\n1     12    14\n"
            ],
            "answer": {
                "ans_desc": "If default then use integer division and pass to : Detail: General solution with array created by length of DataFrame - working with all index values: Detail: ",
                "code": [
                    "print (df.index // 3)\nInt64Index([0, 0, 0, 1, 1, 1], dtype='int64')\n",
                    "df = df.groupby(np.arange(len(df)) // 3).mean()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 111,
            "user_id": 9639867,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/804b2387e982a6360effed9115ae5aa3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dustrokes",
            "link": "https://stackoverflow.com/users/9639867/dustrokes"
        },
        "is_answered": true,
        "view_count": 155,
        "accepted_answer_id": 54777838,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1550661966,
        "creation_date": 1550628141,
        "last_edit_date": 1550661966,
        "question_id": 54777760,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54777760/pd-dataframe-prints-output-in-a-single-column",
        "title": "pd.DataFrame prints output in a single column",
        "body": "<p>I have a file which I'm trying to convert into data frame using pandas. It is inside a loop and returns me the output as shown below. Here is the code I'm using: </p>\n\n<pre><code>import pandas as pd\nimport csv\nwith open('File.tbl', 'r') as  f:\n    P=list(f)\n    del P[0]\nfor o in P:\n    M=o.split()\n    B= M[:4]             #selecting specific columns only\n    E= pd.DataFrame(B)   #converting into DataFrame\n    print(E)\n    G.to_csv('para.csv', sep=',')\n</code></pre>\n\n<p>Here the tbl file is not tab seperated and to create a tab separation, I've to convert it into list. Here is the reult I'm getting: </p>\n\n<pre><code>0    B\n1  244\n2    S\n3    0\n     0\n0    B\n1  245\n2    A\n3    0\n</code></pre>\n\n<p>The expected output is like this: </p>\n\n<pre><code>0    B   244  S  0\n\n0    B   245  A  0\n</code></pre>\n\n<p>Any help will be highly appreciated. </p>\n",
        "answer_body": "<p>Try this:</p>\n\n<pre><code>import pandas as pd\nimport csv\ndf=pd.DataFrame()\nwith open('File.tbl', 'r') as  f:\n    P=list(f)\n    del P[0]\nfor o in P:\n    M=o.split()\n    B= M[:4]             #selecting specific columns only\n    df = pd.concat([df,pd.DataFrame(B).T])    #converting into DataFrame\ndf.to_csv('para.csv', sep=',')\n</code></pre>\n",
        "question_body": "<p>I have a file which I'm trying to convert into data frame using pandas. It is inside a loop and returns me the output as shown below. Here is the code I'm using: </p>\n\n<pre><code>import pandas as pd\nimport csv\nwith open('File.tbl', 'r') as  f:\n    P=list(f)\n    del P[0]\nfor o in P:\n    M=o.split()\n    B= M[:4]             #selecting specific columns only\n    E= pd.DataFrame(B)   #converting into DataFrame\n    print(E)\n    G.to_csv('para.csv', sep=',')\n</code></pre>\n\n<p>Here the tbl file is not tab seperated and to create a tab separation, I've to convert it into list. Here is the reult I'm getting: </p>\n\n<pre><code>0    B\n1  244\n2    S\n3    0\n     0\n0    B\n1  245\n2    A\n3    0\n</code></pre>\n\n<p>The expected output is like this: </p>\n\n<pre><code>0    B   244  S  0\n\n0    B   245  A  0\n</code></pre>\n\n<p>Any help will be highly appreciated. </p>\n",
        "formatted_input": {
            "qid": 54777760,
            "link": "https://stackoverflow.com/questions/54777760/pd-dataframe-prints-output-in-a-single-column",
            "question": {
                "title": "pd.DataFrame prints output in a single column",
                "ques_desc": "I have a file which I'm trying to convert into data frame using pandas. It is inside a loop and returns me the output as shown below. Here is the code I'm using: Here the tbl file is not tab seperated and to create a tab separation, I've to convert it into list. Here is the reult I'm getting: The expected output is like this: Any help will be highly appreciated. "
            },
            "io": [
                "0    B\n1  244\n2    S\n3    0\n     0\n0    B\n1  245\n2    A\n3    0\n",
                "0    B   244  S  0\n\n0    B   245  A  0\n"
            ],
            "answer": {
                "ans_desc": "Try this: ",
                "code": [
                    "import pandas as pd\nimport csv\ndf=pd.DataFrame()\nwith open('File.tbl', 'r') as  f:\n    P=list(f)\n    del P[0]\nfor o in P:\n    M=o.split()\n    B= M[:4]             #selecting specific columns only\n    df = pd.concat([df,pd.DataFrame(B).T])    #converting into DataFrame\ndf.to_csv('para.csv', sep=',')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 525,
            "user_id": 9620853,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dcf698d3257060472995cdd2f46e38f8?s=128&d=identicon&r=PG&f=1",
            "display_name": "Cesar",
            "link": "https://stackoverflow.com/users/9620853/cesar"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 54783975,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1550659080,
        "creation_date": 1550657359,
        "last_edit_date": 1550658604,
        "question_id": 54783721,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54783721/add-a-fix-value-to-a-dataframe-accumulating-to-future-ones",
        "title": "Add a fix value to a dataframe (accumulating to future ones)",
        "body": "<p>I am trying to simulate inventory level during the next 6 months:</p>\n\n<p>1- I have the expected accumulated demand for each day of next 6 months.\n<a href=\"https://i.stack.imgur.com/hEYd6.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/hEYd6.png\" alt=\"enter image description here\"></a>\n So, with no reorder, my balance would be more negative everyday.</p>\n\n<p>2- My idea is:\n Everytime the inventory level is lower than 3000, I would send an order to buy 10000, and after 3 days, my level would increase again:\n<a href=\"https://i.stack.imgur.com/TyWMs.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/TyWMs.png\" alt=\"enter image description here\"></a></p>\n\n<p>How is the best way to add this value into all the future values ?</p>\n\n<pre><code>       ds         saldo\n0 2019-01-01  10200.839819\n1 2019-01-02   5219.412952\n2 2019-01-03      3.161876\n3 2019-01-04  -5507.506201\n4 2019-01-05 -10730.291221\n5 2019-01-06 -14406.833593\n6 2019-01-07 -17781.500396\n7 2019-01-08 -21545.503098\n8 2019-01-09 -25394.427708\n</code></pre>\n\n<p>I started doing like this :</p>\n\n<pre><code>c = 0\nfor index, row in forecast_data.iterrows():\n  if row['saldo'] &lt; 3000:\n    c += 1\n    if c == 3:\n      row['saldo'] + 10000\n      c = 0\n</code></pre>\n\n<p>But it just adds to the actual row, not for the accumulated future ones.</p>\n\n<pre><code>print(row['ds'], row['saldo'])\n9 2019-01-10 -29277.647817\n</code></pre>\n",
        "answer_body": "<p>The problem is that you are only adding 1000 to the <code>row</code>, not to every entry that follows. Try replacing</p>\n\n<pre><code>row['saldo'] + 10000\n</code></pre>\n\n<p>with </p>\n\n<pre><code>forecast_data['saldo'][index:] += forecast_data['saldo'][index:] + 1000\n</code></pre>\n\n<p>This will add 1000 to every following entry.</p>\n",
        "question_body": "<p>I am trying to simulate inventory level during the next 6 months:</p>\n\n<p>1- I have the expected accumulated demand for each day of next 6 months.\n<a href=\"https://i.stack.imgur.com/hEYd6.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/hEYd6.png\" alt=\"enter image description here\"></a>\n So, with no reorder, my balance would be more negative everyday.</p>\n\n<p>2- My idea is:\n Everytime the inventory level is lower than 3000, I would send an order to buy 10000, and after 3 days, my level would increase again:\n<a href=\"https://i.stack.imgur.com/TyWMs.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/TyWMs.png\" alt=\"enter image description here\"></a></p>\n\n<p>How is the best way to add this value into all the future values ?</p>\n\n<pre><code>       ds         saldo\n0 2019-01-01  10200.839819\n1 2019-01-02   5219.412952\n2 2019-01-03      3.161876\n3 2019-01-04  -5507.506201\n4 2019-01-05 -10730.291221\n5 2019-01-06 -14406.833593\n6 2019-01-07 -17781.500396\n7 2019-01-08 -21545.503098\n8 2019-01-09 -25394.427708\n</code></pre>\n\n<p>I started doing like this :</p>\n\n<pre><code>c = 0\nfor index, row in forecast_data.iterrows():\n  if row['saldo'] &lt; 3000:\n    c += 1\n    if c == 3:\n      row['saldo'] + 10000\n      c = 0\n</code></pre>\n\n<p>But it just adds to the actual row, not for the accumulated future ones.</p>\n\n<pre><code>print(row['ds'], row['saldo'])\n9 2019-01-10 -29277.647817\n</code></pre>\n",
        "formatted_input": {
            "qid": 54783721,
            "link": "https://stackoverflow.com/questions/54783721/add-a-fix-value-to-a-dataframe-accumulating-to-future-ones",
            "question": {
                "title": "Add a fix value to a dataframe (accumulating to future ones)",
                "ques_desc": "I am trying to simulate inventory level during the next 6 months: 1- I have the expected accumulated demand for each day of next 6 months. So, with no reorder, my balance would be more negative everyday. 2- My idea is: Everytime the inventory level is lower than 3000, I would send an order to buy 10000, and after 3 days, my level would increase again: How is the best way to add this value into all the future values ? I started doing like this : But it just adds to the actual row, not for the accumulated future ones. "
            },
            "io": [
                "       ds         saldo\n0 2019-01-01  10200.839819\n1 2019-01-02   5219.412952\n2 2019-01-03      3.161876\n3 2019-01-04  -5507.506201\n4 2019-01-05 -10730.291221\n5 2019-01-06 -14406.833593\n6 2019-01-07 -17781.500396\n7 2019-01-08 -21545.503098\n8 2019-01-09 -25394.427708\n",
                "print(row['ds'], row['saldo'])\n9 2019-01-10 -29277.647817\n"
            ],
            "answer": {
                "ans_desc": "The problem is that you are only adding 1000 to the , not to every entry that follows. Try replacing with This will add 1000 to every following entry. ",
                "code": [
                    "forecast_data['saldo'][index:] += forecast_data['saldo'][index:] + 1000\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "sorting",
            "date",
            "dataframe"
        ],
        "owner": {
            "reputation": 943,
            "user_id": 5036928,
            "user_type": "registered",
            "accept_rate": 52,
            "profile_image": "https://lh6.googleusercontent.com/-beIS1MFhU_I/AAAAAAAAAAI/AAAAAAAAAHU/tTFHhB_-kPs/photo.jpg?sz=128",
            "display_name": "Sterling Butters",
            "link": "https://stackoverflow.com/users/5036928/sterling-butters"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 54753757,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1550523762,
        "creation_date": 1550510064,
        "last_edit_date": 1550523762,
        "question_id": 54752268,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54752268/pandas-complex-merge-of-dataframes-with-date-basis",
        "title": "Pandas: Complex merge of DataFrames with date basis",
        "body": "<p>I have 2 pandas <code>DataFrame</code>'s that I need to merge in a bit of a complex manner so I am in need of some help.</p>\n\n<h3>DataFrame to be inserted:</h3>\n\n<pre><code>            AAPL shares  GOOG shares  MSFT shares\ndate                                             \n2019-01-01          0.0         10.0          0.0\n2019-01-05          0.0          0.0         15.0\n2019-01-12          0.0          0.0          7.0\n2019-01-13          3.0          0.0          0.0\n2019-01-14          0.0         -5.0          0.0\n</code></pre>\n\n<h3>DataFrame to receive insertion</h3>\n\n<pre><code>               0      1           2        3           4       5\n0     1998-01-02  16.25  2014-03-27   558.46  1998-01-02  131.13\n1     1998-01-05  15.88  2014-03-28   559.99  1998-01-05  130.38\n2     1998-01-06  18.94  2014-03-31   556.97  1998-01-06  131.13\n3     1998-01-07  17.50  2014-04-01   567.16  1998-01-07  129.56\n4     1998-01-08  18.19  2014-04-02   567.00  1998-01-08  130.50\n5     1998-01-09  18.19  2014-04-03   569.74  1998-01-09  127.00\n6     1998-01-12  18.25  2014-04-04   543.14  1998-01-12  129.50\n7     1998-01-13  19.50  2014-04-07   538.15  1998-01-13  132.13\n8     1998-01-14  19.75  2014-04-08   554.90  1998-01-14  131.13\n9     1998-01-15  19.19  2014-04-09   564.14  1998-01-15  132.31\n10    1998-01-16  18.81  2014-04-10   540.95  1998-01-16  135.25\n11    1998-01-20  19.06  2014-04-11   530.60  1998-01-20  137.81\n12    1998-01-21  18.91  2014-04-14   532.52  1998-01-21  137.00\n13    1998-01-22  19.25  2014-04-15   536.44  1998-01-22  138.63\n14    1998-01-23  19.50  2014-04-16   556.54  1998-01-23  138.25\n15    1998-01-26  19.44  2014-04-17   536.10  1998-01-26  141.75\n</code></pre>\n\n<p>1) The <code>receiving_df</code> needs to establish a common basis for <code>date</code> (notice column <code>2</code> is different), thus the <code>DataFrame</code> needs to be organized into <code>date</code>, <code>1</code>, <code>3</code>, <code>5</code> where the dates of <code>0</code>, <code>2</code>, and <code>4</code> are used for assembling <code>date</code> to correctly reflect the values in <code>1</code>, <code>3</code>, <code>5</code> at a certain date.</p>\n\n<p>Sample output from step 1:</p>\n\n<pre><code>               0      1       3       5\n0     1998-01-02  16.25      NA  131.13\n1     1998-01-05  15.88      NA  130.38\n2     1998-01-06  18.94      NA  131.13\n3     1998-01-07  17.50      NA  129.56\n4     1998-01-08  18.19      NA  130.50\n5     1998-01-09  18.19      NA  127.00\n6     1998-01-12  18.25      NA  129.50\n7     1998-01-13  19.50      NA  132.13\n8     1998-01-14  19.75      NA  131.13\n...\n10    2014-04-10  18.81  558.46  135.25\n11    2014-04-11  19.06  559.99  137.81\n12    2014-04-14  18.91  556.97  137.00\n13    2014-04-15  19.25  567.16  138.63\n14    2014-04-16  19.50  567.00  138.25\n15    2014-04-17  19.44  569.74  141.75\n...\n</code></pre>\n\n<p>2) <code>inserting_df</code> will need to be sorted by date according to <code>receiving_df['date']</code> and the columns <code>AAPL shares</code>, <code>GOOG shares</code>, <code>MSFT shares</code> will be added as columns in the <code>receiving_df</code>. I imagine this will follow similar methods as in 1).</p>\n\n<p>Sample output from step 2:</p>\n\n<pre><code>               0   AAPL shares   1      GOOG shares       3   MSFT shares        5\n0     1998-01-02           0.0   16.25          0.0       NA          0.0   131.13\n1     1998-01-05           0.0   15.88          0.0       NA          0.0   130.38\n2     1998-01-06           0.0   18.94          0.0       NA          0.0   131.13\n3     1998-01-07           0.0   17.50          0.0       NA          0.0   129.56\n4     1998-01-08           0.0   18.19          0.0       NA          0.0   130.50\n5     1998-01-09           0.0   18.19          0.0       NA          0.0   127.00\n6     1998-01-12           0.0   18.25          0.0       NA          0.0   129.50\n7     1998-01-13           0.0   19.50          0.0       NA          0.0   132.13\n8     1998-01-14           0.0   19.75          0.0       NA          0.0   131.13\n...                                     \n10    2014-04-10           0.0   18.81          0.0   558.46          0.0   135.25\n11    2014-04-11           0.0   19.06          0.0   559.99          0.0   137.81\n12    2014-04-14           0.0   18.91          0.0   556.97          0.0   137.00\n13    2014-04-15           0.0   19.25          0.0   567.16          0.0   138.63\n14    2014-04-16           0.0   19.50          0.0   567.00          0.0   138.25\n15    2014-04-17           0.0   19.44          0.0   569.74          0.0   141.75\n...            \n&lt;#&gt;   2019-01-01           0.0   &lt;value&gt;       10.0   &lt;value&gt;         0.0   &lt;value&gt;  \n&lt;#&gt;   2019-01-02           0.0   &lt;value&gt;        0.0   &lt;value&gt;        15.0   &lt;value&gt;\n&lt;#&gt;   2019-01-03           0.0   &lt;value&gt;        0.0   &lt;value&gt;         7.0   &lt;value&gt;\n&lt;#&gt;   2019-01-04           3.0   &lt;value&gt;        0.0   &lt;value&gt;         0.0   &lt;value&gt;\n&lt;#&gt;   2019-01-05           0.0   &lt;value&gt;       -5.0   &lt;value&gt;         0.0   &lt;value&gt;\n</code></pre>\n\n<p>3) The new columns <code>AAPL shares</code>, <code>GOOG shares</code>, <code>MSFT shares</code> will need to be filled forward with cumsum but I think I got that down:\n~ <code>df.set_index('date').sort_index().fillna(value=0).cumsum())</code></p>\n\n<p>Sample output from step 3:</p>\n\n<pre><code>               0   AAPL shares   1      GOOG shares       3   MSFT shares        5\n0     1998-01-02           0.0   16.25          0.0       NA          0.0   131.13\n1     1998-01-05           0.0   15.88          0.0       NA          0.0   130.38\n2     1998-01-06           0.0   18.94          0.0       NA          0.0   131.13\n3     1998-01-07           0.0   17.50          0.0       NA          0.0   129.56\n4     1998-01-08           0.0   18.19          0.0       NA          0.0   130.50\n5     1998-01-09           0.0   18.19          0.0       NA          0.0   127.00\n6     1998-01-12           0.0   18.25          0.0       NA          0.0   129.50\n7     1998-01-13           0.0   19.50          0.0       NA          0.0   132.13\n8     1998-01-14           0.0   19.75          0.0       NA          0.0   131.13\n...\n10    2014-04-10           0.0   18.81          0.0   558.46          0.0   135.25\n11    2014-04-11           0.0   19.06          0.0   559.99          0.0   137.81\n12    2014-04-14           0.0   18.91          0.0   556.97          0.0   137.00\n13    2014-04-15           0.0   19.25          0.0   567.16          0.0   138.63\n14    2014-04-16           0.0   19.50          0.0   567.00          0.0   138.25\n15    2014-04-17           0.0   19.44          0.0   569.74          0.0   141.75\n...\n&lt;#&gt;   2019-01-01           0.0   &lt;value&gt;       10.0   &lt;value&gt;         0.0   &lt;value&gt;\n&lt;#&gt;   2019-01-02           0.0   &lt;value&gt;       10.0   &lt;value&gt;        15.0   &lt;value&gt;\n&lt;#&gt;   2019-01-03           0.0   &lt;value&gt;       10.0   &lt;value&gt;        22.0   &lt;value&gt;\n&lt;#&gt;   2019-01-04           3.0   &lt;value&gt;       10.0   &lt;value&gt;        22.0   &lt;value&gt;\n&lt;#&gt;   2019-01-05           3.0   &lt;value&gt;        5.0   &lt;value&gt;        22.0   &lt;value&gt;\n</code></pre>\n\n<p>So the end goal would result in the values and shares held according to the <code>date</code> index. For <code>date</code>'s that will not have a column <code>2</code> value (since column <code>2</code> is \"missing\" some dates) in the resulting <code>receiving_df</code>, it would be best to make that value <code>N/A</code> but 0 would suffice. </p>\n\n<p>Happy to clarify anything, I appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! </p>\n\n<p>EDIT:\nTrying to merge in a loop now since number of <code>date</code>-<code>value</code> pairs may vary. I now have a list of separate <code>DataFrames</code> for the <code>date</code>-<code>value</code> pairs: <code>dfs_list</code>. Since number of pairs may vary, it seems best not to <code>set_index</code> based on column labels hence <code>set_index(rec_df.columns[0])</code>. </p>\n\n<pre><code>rec_df = dfs_list[0].set_index(dfs_list[0].columns[0])\nfor dataframe in range(len(dfs_list)-1):\n            rec_df = pd.merge(left=rec_df, right=dfs_list[dataframe+1].set_index(dfs_list[dataframe+1].columns[0]),\n                              left_index=True, right_index=True,\n                              how='outer')\n</code></pre>\n",
        "answer_body": "<p>Based on your clarifications, here's what I understand a solution to be.</p>\n\n<p>Given the dataframes given, I added column names to make things more clear, and renamed <code>receive_df</code> to just <code>df</code> to save on typing:</p>\n\n<pre><code>df.columns=['d1','p1','d2','p2','d3','p3']\n\ndf1=pd.merge(left=df[['d1','p1']].set_index('d1'),right=df[['d3','p3']].set_index('d3'), left_index=True, right_index=True, how='outer')\n\ndf1.head()\n\n            p1      p3\nd1      \n1998-01-02  16.25   131.13\n1998-01-05  15.88   130.38\n1998-01-06  18.94   131.13\n1998-01-07  17.50   129.56\n1998-01-08  18.19   130.50\n\nrec_df=pd.merge(left=df1,right=df[['d2','p2']].set_index('d2'),left_index=True, right_index=True, how='outer')\n\nrec_df\n\n            p1      p3      p2\n1998-01-02  16.25   131.13  NaN\n1998-01-05  15.88   130.38  NaN\n1998-01-06  18.94   131.13  NaN\n1998-01-07  17.50   129.56  NaN\n1998-01-08  18.19   130.50  NaN\n1998-01-09  18.19   127.00  NaN\n1998-01-12  18.25   129.50  NaN\n1998-01-13  19.50   132.13  NaN\n1998-01-14  19.75   131.13  NaN\n1998-01-15  19.19   132.31  NaN\n1998-01-16  18.81   135.25  NaN\n1998-01-20  19.06   137.81  NaN\n1998-01-21  18.91   137.00  NaN\n1998-01-22  19.25   138.63  NaN\n1998-01-23  19.50   138.25  NaN\n1998-01-26  19.44   141.75  NaN\n2014-03-27  NaN NaN 558.46\n2014-03-28  NaN NaN 559.99\n2014-03-31  NaN NaN 556.97\n2014-04-01  NaN NaN 567.16\n2014-04-02  NaN NaN 567.00\n2014-04-03  NaN NaN 569.74\n2014-04-04  NaN NaN 543.14\n2014-04-07  NaN NaN 538.15\n2014-04-08  NaN NaN 554.90\n2014-04-09  NaN NaN 564.14\n2014-04-10  NaN NaN 540.95\n2014-04-11  NaN NaN 530.60\n2014-04-14  NaN NaN 532.52\n2014-04-15  NaN NaN 536.44\n2014-04-16  NaN NaN 556.54\n2014-04-17  NaN NaN 536.10\n</code></pre>\n\n<p>Now, here's where I couldn't test, since your insert df has no matching date indices from your receive df, but that <em>should</em> look like</p>\n\n<pre><code>final_df=pd.merge(left=rec_df, right=insert_df, left_index=True, right_index=True, dropna=False)\n</code></pre>\n\n<p>Of course, this assumes that the <code>date</code> column is set as the index already as in your sample. If you want a numerical index at the end, you can <code>reset_index(in_place=True)</code>, or you can leave the date as the index.</p>\n\n<p>It looks like you already have a handle on the last part of the task, and I can't test it anyway with the given data.</p>\n\n<p>And note also that you can reorder the columns however you want depending how you want your output to look (<code>df=df[[list of columns in order]]</code>)</p>\n",
        "question_body": "<p>I have 2 pandas <code>DataFrame</code>'s that I need to merge in a bit of a complex manner so I am in need of some help.</p>\n\n<h3>DataFrame to be inserted:</h3>\n\n<pre><code>            AAPL shares  GOOG shares  MSFT shares\ndate                                             \n2019-01-01          0.0         10.0          0.0\n2019-01-05          0.0          0.0         15.0\n2019-01-12          0.0          0.0          7.0\n2019-01-13          3.0          0.0          0.0\n2019-01-14          0.0         -5.0          0.0\n</code></pre>\n\n<h3>DataFrame to receive insertion</h3>\n\n<pre><code>               0      1           2        3           4       5\n0     1998-01-02  16.25  2014-03-27   558.46  1998-01-02  131.13\n1     1998-01-05  15.88  2014-03-28   559.99  1998-01-05  130.38\n2     1998-01-06  18.94  2014-03-31   556.97  1998-01-06  131.13\n3     1998-01-07  17.50  2014-04-01   567.16  1998-01-07  129.56\n4     1998-01-08  18.19  2014-04-02   567.00  1998-01-08  130.50\n5     1998-01-09  18.19  2014-04-03   569.74  1998-01-09  127.00\n6     1998-01-12  18.25  2014-04-04   543.14  1998-01-12  129.50\n7     1998-01-13  19.50  2014-04-07   538.15  1998-01-13  132.13\n8     1998-01-14  19.75  2014-04-08   554.90  1998-01-14  131.13\n9     1998-01-15  19.19  2014-04-09   564.14  1998-01-15  132.31\n10    1998-01-16  18.81  2014-04-10   540.95  1998-01-16  135.25\n11    1998-01-20  19.06  2014-04-11   530.60  1998-01-20  137.81\n12    1998-01-21  18.91  2014-04-14   532.52  1998-01-21  137.00\n13    1998-01-22  19.25  2014-04-15   536.44  1998-01-22  138.63\n14    1998-01-23  19.50  2014-04-16   556.54  1998-01-23  138.25\n15    1998-01-26  19.44  2014-04-17   536.10  1998-01-26  141.75\n</code></pre>\n\n<p>1) The <code>receiving_df</code> needs to establish a common basis for <code>date</code> (notice column <code>2</code> is different), thus the <code>DataFrame</code> needs to be organized into <code>date</code>, <code>1</code>, <code>3</code>, <code>5</code> where the dates of <code>0</code>, <code>2</code>, and <code>4</code> are used for assembling <code>date</code> to correctly reflect the values in <code>1</code>, <code>3</code>, <code>5</code> at a certain date.</p>\n\n<p>Sample output from step 1:</p>\n\n<pre><code>               0      1       3       5\n0     1998-01-02  16.25      NA  131.13\n1     1998-01-05  15.88      NA  130.38\n2     1998-01-06  18.94      NA  131.13\n3     1998-01-07  17.50      NA  129.56\n4     1998-01-08  18.19      NA  130.50\n5     1998-01-09  18.19      NA  127.00\n6     1998-01-12  18.25      NA  129.50\n7     1998-01-13  19.50      NA  132.13\n8     1998-01-14  19.75      NA  131.13\n...\n10    2014-04-10  18.81  558.46  135.25\n11    2014-04-11  19.06  559.99  137.81\n12    2014-04-14  18.91  556.97  137.00\n13    2014-04-15  19.25  567.16  138.63\n14    2014-04-16  19.50  567.00  138.25\n15    2014-04-17  19.44  569.74  141.75\n...\n</code></pre>\n\n<p>2) <code>inserting_df</code> will need to be sorted by date according to <code>receiving_df['date']</code> and the columns <code>AAPL shares</code>, <code>GOOG shares</code>, <code>MSFT shares</code> will be added as columns in the <code>receiving_df</code>. I imagine this will follow similar methods as in 1).</p>\n\n<p>Sample output from step 2:</p>\n\n<pre><code>               0   AAPL shares   1      GOOG shares       3   MSFT shares        5\n0     1998-01-02           0.0   16.25          0.0       NA          0.0   131.13\n1     1998-01-05           0.0   15.88          0.0       NA          0.0   130.38\n2     1998-01-06           0.0   18.94          0.0       NA          0.0   131.13\n3     1998-01-07           0.0   17.50          0.0       NA          0.0   129.56\n4     1998-01-08           0.0   18.19          0.0       NA          0.0   130.50\n5     1998-01-09           0.0   18.19          0.0       NA          0.0   127.00\n6     1998-01-12           0.0   18.25          0.0       NA          0.0   129.50\n7     1998-01-13           0.0   19.50          0.0       NA          0.0   132.13\n8     1998-01-14           0.0   19.75          0.0       NA          0.0   131.13\n...                                     \n10    2014-04-10           0.0   18.81          0.0   558.46          0.0   135.25\n11    2014-04-11           0.0   19.06          0.0   559.99          0.0   137.81\n12    2014-04-14           0.0   18.91          0.0   556.97          0.0   137.00\n13    2014-04-15           0.0   19.25          0.0   567.16          0.0   138.63\n14    2014-04-16           0.0   19.50          0.0   567.00          0.0   138.25\n15    2014-04-17           0.0   19.44          0.0   569.74          0.0   141.75\n...            \n&lt;#&gt;   2019-01-01           0.0   &lt;value&gt;       10.0   &lt;value&gt;         0.0   &lt;value&gt;  \n&lt;#&gt;   2019-01-02           0.0   &lt;value&gt;        0.0   &lt;value&gt;        15.0   &lt;value&gt;\n&lt;#&gt;   2019-01-03           0.0   &lt;value&gt;        0.0   &lt;value&gt;         7.0   &lt;value&gt;\n&lt;#&gt;   2019-01-04           3.0   &lt;value&gt;        0.0   &lt;value&gt;         0.0   &lt;value&gt;\n&lt;#&gt;   2019-01-05           0.0   &lt;value&gt;       -5.0   &lt;value&gt;         0.0   &lt;value&gt;\n</code></pre>\n\n<p>3) The new columns <code>AAPL shares</code>, <code>GOOG shares</code>, <code>MSFT shares</code> will need to be filled forward with cumsum but I think I got that down:\n~ <code>df.set_index('date').sort_index().fillna(value=0).cumsum())</code></p>\n\n<p>Sample output from step 3:</p>\n\n<pre><code>               0   AAPL shares   1      GOOG shares       3   MSFT shares        5\n0     1998-01-02           0.0   16.25          0.0       NA          0.0   131.13\n1     1998-01-05           0.0   15.88          0.0       NA          0.0   130.38\n2     1998-01-06           0.0   18.94          0.0       NA          0.0   131.13\n3     1998-01-07           0.0   17.50          0.0       NA          0.0   129.56\n4     1998-01-08           0.0   18.19          0.0       NA          0.0   130.50\n5     1998-01-09           0.0   18.19          0.0       NA          0.0   127.00\n6     1998-01-12           0.0   18.25          0.0       NA          0.0   129.50\n7     1998-01-13           0.0   19.50          0.0       NA          0.0   132.13\n8     1998-01-14           0.0   19.75          0.0       NA          0.0   131.13\n...\n10    2014-04-10           0.0   18.81          0.0   558.46          0.0   135.25\n11    2014-04-11           0.0   19.06          0.0   559.99          0.0   137.81\n12    2014-04-14           0.0   18.91          0.0   556.97          0.0   137.00\n13    2014-04-15           0.0   19.25          0.0   567.16          0.0   138.63\n14    2014-04-16           0.0   19.50          0.0   567.00          0.0   138.25\n15    2014-04-17           0.0   19.44          0.0   569.74          0.0   141.75\n...\n&lt;#&gt;   2019-01-01           0.0   &lt;value&gt;       10.0   &lt;value&gt;         0.0   &lt;value&gt;\n&lt;#&gt;   2019-01-02           0.0   &lt;value&gt;       10.0   &lt;value&gt;        15.0   &lt;value&gt;\n&lt;#&gt;   2019-01-03           0.0   &lt;value&gt;       10.0   &lt;value&gt;        22.0   &lt;value&gt;\n&lt;#&gt;   2019-01-04           3.0   &lt;value&gt;       10.0   &lt;value&gt;        22.0   &lt;value&gt;\n&lt;#&gt;   2019-01-05           3.0   &lt;value&gt;        5.0   &lt;value&gt;        22.0   &lt;value&gt;\n</code></pre>\n\n<p>So the end goal would result in the values and shares held according to the <code>date</code> index. For <code>date</code>'s that will not have a column <code>2</code> value (since column <code>2</code> is \"missing\" some dates) in the resulting <code>receiving_df</code>, it would be best to make that value <code>N/A</code> but 0 would suffice. </p>\n\n<p>Happy to clarify anything, I appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! </p>\n\n<p>EDIT:\nTrying to merge in a loop now since number of <code>date</code>-<code>value</code> pairs may vary. I now have a list of separate <code>DataFrames</code> for the <code>date</code>-<code>value</code> pairs: <code>dfs_list</code>. Since number of pairs may vary, it seems best not to <code>set_index</code> based on column labels hence <code>set_index(rec_df.columns[0])</code>. </p>\n\n<pre><code>rec_df = dfs_list[0].set_index(dfs_list[0].columns[0])\nfor dataframe in range(len(dfs_list)-1):\n            rec_df = pd.merge(left=rec_df, right=dfs_list[dataframe+1].set_index(dfs_list[dataframe+1].columns[0]),\n                              left_index=True, right_index=True,\n                              how='outer')\n</code></pre>\n",
        "formatted_input": {
            "qid": 54752268,
            "link": "https://stackoverflow.com/questions/54752268/pandas-complex-merge-of-dataframes-with-date-basis",
            "question": {
                "title": "Pandas: Complex merge of DataFrames with date basis",
                "ques_desc": "I have 2 pandas 's that I need to merge in a bit of a complex manner so I am in need of some help. DataFrame to be inserted: DataFrame to receive insertion 1) The needs to establish a common basis for (notice column is different), thus the needs to be organized into , , , where the dates of , , and are used for assembling to correctly reflect the values in , , at a certain date. Sample output from step 1: 2) will need to be sorted by date according to and the columns , , will be added as columns in the . I imagine this will follow similar methods as in 1). Sample output from step 2: 3) The new columns , , will need to be filled forward with cumsum but I think I got that down: ~ Sample output from step 3: So the end goal would result in the values and shares held according to the index. For 's that will not have a column value (since column is \"missing\" some dates) in the resulting , it would be best to make that value but 0 would suffice. Happy to clarify anything, I appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! EDIT: Trying to merge in a loop now since number of - pairs may vary. I now have a list of separate for the - pairs: . Since number of pairs may vary, it seems best not to based on column labels hence . "
            },
            "io": [
                "               0      1           2        3           4       5\n0     1998-01-02  16.25  2014-03-27   558.46  1998-01-02  131.13\n1     1998-01-05  15.88  2014-03-28   559.99  1998-01-05  130.38\n2     1998-01-06  18.94  2014-03-31   556.97  1998-01-06  131.13\n3     1998-01-07  17.50  2014-04-01   567.16  1998-01-07  129.56\n4     1998-01-08  18.19  2014-04-02   567.00  1998-01-08  130.50\n5     1998-01-09  18.19  2014-04-03   569.74  1998-01-09  127.00\n6     1998-01-12  18.25  2014-04-04   543.14  1998-01-12  129.50\n7     1998-01-13  19.50  2014-04-07   538.15  1998-01-13  132.13\n8     1998-01-14  19.75  2014-04-08   554.90  1998-01-14  131.13\n9     1998-01-15  19.19  2014-04-09   564.14  1998-01-15  132.31\n10    1998-01-16  18.81  2014-04-10   540.95  1998-01-16  135.25\n11    1998-01-20  19.06  2014-04-11   530.60  1998-01-20  137.81\n12    1998-01-21  18.91  2014-04-14   532.52  1998-01-21  137.00\n13    1998-01-22  19.25  2014-04-15   536.44  1998-01-22  138.63\n14    1998-01-23  19.50  2014-04-16   556.54  1998-01-23  138.25\n15    1998-01-26  19.44  2014-04-17   536.10  1998-01-26  141.75\n",
                "               0      1       3       5\n0     1998-01-02  16.25      NA  131.13\n1     1998-01-05  15.88      NA  130.38\n2     1998-01-06  18.94      NA  131.13\n3     1998-01-07  17.50      NA  129.56\n4     1998-01-08  18.19      NA  130.50\n5     1998-01-09  18.19      NA  127.00\n6     1998-01-12  18.25      NA  129.50\n7     1998-01-13  19.50      NA  132.13\n8     1998-01-14  19.75      NA  131.13\n...\n10    2014-04-10  18.81  558.46  135.25\n11    2014-04-11  19.06  559.99  137.81\n12    2014-04-14  18.91  556.97  137.00\n13    2014-04-15  19.25  567.16  138.63\n14    2014-04-16  19.50  567.00  138.25\n15    2014-04-17  19.44  569.74  141.75\n...\n"
            ],
            "answer": {
                "ans_desc": "Based on your clarifications, here's what I understand a solution to be. Given the dataframes given, I added column names to make things more clear, and renamed to just to save on typing: Now, here's where I couldn't test, since your insert df has no matching date indices from your receive df, but that should look like Of course, this assumes that the column is set as the index already as in your sample. If you want a numerical index at the end, you can , or you can leave the date as the index. It looks like you already have a handle on the last part of the task, and I can't test it anyway with the given data. And note also that you can reorder the columns however you want depending how you want your output to look () ",
                "code": [
                    "final_df=pd.merge(left=rec_df, right=insert_df, left_index=True, right_index=True, dropna=False)\n",
                    "df=df[[list of columns in order]]"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-2.7",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user8176353"
        },
        "is_answered": true,
        "view_count": 275,
        "accepted_answer_id": 44606480,
        "answer_count": 5,
        "score": 3,
        "last_activity_date": 1550460903,
        "creation_date": 1497714303,
        "last_edit_date": 1497714803,
        "question_id": 44606413,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/44606413/grouping-columns-by-unique-values-in-python",
        "title": "Grouping columns by unique values in Python",
        "body": "<p>I have a data set with two columns and I need to change it from this format:</p>\n\n<pre><code>10  1 \n10  5\n10  3\n11  5\n11  4\n12  6\n12  2\n</code></pre>\n\n<p>to this</p>\n\n<pre><code>10  1  5  3\n11  5  4\n12  6  2\n</code></pre>\n\n<p>I need every unique value in the first column to be on its own row.  </p>\n\n<p>I am a beginner with Python and beyond reading in my text file, I'm at a loss for how to proceed. </p>\n",
        "answer_body": "<p>You can use Pandas dataframes.</p>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'A':[10,10,10,11,11,12,12],'B':[1,5,3,5,4,6,2]})\nprint(df)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>    A  B\n0  10  1\n1  10  5\n2  10  3\n3  11  5\n4  11  4\n5  12  6\n6  12  2\n</code></pre>\n\n<p>Let's use <code>groupby</code> and <code>join</code>:</p>\n\n<pre><code>df.groupby('A')['B'].apply(lambda x:' '.join(x.astype(str)))\n</code></pre>\n\n<p>Output: </p>\n\n<pre><code>A\n10    1 5 3\n11      5 4\n12      6 2\nName: B, dtype: object\n</code></pre>\n",
        "question_body": "<p>I have a data set with two columns and I need to change it from this format:</p>\n\n<pre><code>10  1 \n10  5\n10  3\n11  5\n11  4\n12  6\n12  2\n</code></pre>\n\n<p>to this</p>\n\n<pre><code>10  1  5  3\n11  5  4\n12  6  2\n</code></pre>\n\n<p>I need every unique value in the first column to be on its own row.  </p>\n\n<p>I am a beginner with Python and beyond reading in my text file, I'm at a loss for how to proceed. </p>\n",
        "formatted_input": {
            "qid": 44606413,
            "link": "https://stackoverflow.com/questions/44606413/grouping-columns-by-unique-values-in-python",
            "question": {
                "title": "Grouping columns by unique values in Python",
                "ques_desc": "I have a data set with two columns and I need to change it from this format: to this I need every unique value in the first column to be on its own row. I am a beginner with Python and beyond reading in my text file, I'm at a loss for how to proceed. "
            },
            "io": [
                "10  1 \n10  5\n10  3\n11  5\n11  4\n12  6\n12  2\n",
                "10  1  5  3\n11  5  4\n12  6  2\n"
            ],
            "answer": {
                "ans_desc": "You can use Pandas dataframes. Output: Let's use and : Output: ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'A':[10,10,10,11,11,12,12],'B':[1,5,3,5,4,6,2]})\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "rolling-computation",
            "rolling-sum"
        ],
        "owner": {
            "reputation": 199,
            "user_id": 5539351,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0abb88290f5d78a18d90f028a32464dd?s=128&d=identicon&r=PG&f=1",
            "display_name": "bolla",
            "link": "https://stackoverflow.com/users/5539351/bolla"
        },
        "is_answered": true,
        "view_count": 5970,
        "accepted_answer_id": 46724452,
        "answer_count": 1,
        "score": 7,
        "last_activity_date": 1550444663,
        "creation_date": 1507873865,
        "last_edit_date": 1550444663,
        "question_id": 46723310,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46723310/conditional-mean-and-sum-of-previous-n-rows-in-pandas-dataframe",
        "title": "Conditional mean and sum of previous N rows in pandas dataframe",
        "body": "<p>Concerned is this exemplary pandas dataframe:</p>\n\n<pre><code>      Measurement  Trigger  Valid\n   0          2.0    False   True\n   1          4.0    False   True\n   2          3.0    False   True\n   3          0.0     True  False\n   4        100.0    False   True\n   5          3.0    False   True\n   6          2.0    False   True\n   7          1.0     True   True\n</code></pre>\n\n<p>Whenever <code>Trigger</code> is <code>True</code>, I wish to calculate sum and mean of the last 3 (starting from current) valid measurements. Measurements are considered valid, if the column <code>Valid</code> is <code>True</code>. So let's clarify using the two examples in the above dataframe:</p>\n\n<ol>\n<li><code>Index 3</code>: Indices <code>2,1,0</code> should be used. Expected <code>Sum = 9.0, Mean = 3.0</code></li>\n<li><code>Index 7</code>: Indices <code>7,6,5</code> should be used. Expected <code>Sum = 6.0, Mean = 2.0</code></li>\n</ol>\n\n<p>I have tried <code>pandas.rolling</code> and creating new, shifted columns, but was not successful. See the following excerpt from my tests (which should directly run):</p>\n\n<pre><code>import unittest\nimport pandas as pd\nimport numpy as np\nfrom pandas.util.testing import assert_series_equal\n\ndef create_sample_dataframe_2():\n    df = pd.DataFrame(\n        {\"Measurement\" : [2.0,   4.0,   3.0,   0.0,   100.0, 3.0,   2.0,   1.0 ],\n         \"Valid\"       : [True,  True,  True,  False, True,  True,  True,  True],\n         \"Trigger\"     : [False, False, False, True,  False, False, False, True],\n         })\n    return df\n\ndef expected_result():\n    return pd.DataFrame({\"Sum\" : [np.nan, np.nan, np.nan, 9.0, np.nan, np.nan, np.nan, 6.0],\n                         \"Mean\" :[np.nan, np.nan, np.nan, 3.0, np.nan, np.nan, np.nan, 2.0]})\n\nclass Data_Preparation_Functions(unittest.TestCase):\n\n    def test_backsummation(self):\n        N_SUMMANDS = 3\n        temp_vars = []\n\n        df = create_sample_dataframe_2()\n        for i in range(0,N_SUMMANDS):\n            temp_var = \"M_{0}\".format(i)\n            df[temp_var] = df[\"Measurement\"].shift(i)\n            temp_vars.append(temp_var)\n\n        df[\"Sum\"]  = df[temp_vars].sum(axis=1)\n        df[\"Mean\"] = df[temp_vars].mean(axis=1)\n        df.loc[(df[\"Trigger\"]==False), \"Sum\"] = np.nan\n        df.loc[(df[\"Trigger\"]==False), \"Mean\"] = np.nan\n\n        assert_series_equal(expected_result()[\"Sum\"],df[\"Sum\"])\n        assert_series_equal(expected_result()[\"Mean\"],df[\"Mean\"])\n\n    def test_rolling(self):\n        df = create_sample_dataframe_2()\n        df[\"Sum\"]  = df[(df[\"Valid\"] == True)][\"Measurement\"].rolling(window=3).sum()\n        df[\"Mean\"] = df[(df[\"Valid\"] == True)][\"Measurement\"].rolling(window=3).mean()\n\n        df.loc[(df[\"Trigger\"]==False), \"Sum\"] = np.nan\n        df.loc[(df[\"Trigger\"]==False), \"Mean\"] = np.nan\n        assert_series_equal(expected_result()[\"Sum\"],df[\"Sum\"])\n        assert_series_equal(expected_result()[\"Mean\"],df[\"Mean\"])\n\n\nif __name__ == '__main__':\n    suite = unittest.TestLoader().loadTestsFromTestCase(Data_Preparation_Functions)\n    unittest.TextTestRunner(verbosity=2).run(suite)\n</code></pre>\n\n<p>Any help or solution is greatly appreciated. Thanks and Cheers!</p>\n\n<p>EDIT: Clarification: This is the resulting dataframe I expect:</p>\n\n<pre><code>      Measurement  Trigger  Valid   Sum   Mean\n   0          2.0    False   True   NaN    NaN\n   1          4.0    False   True   NaN    NaN\n   2          3.0    False   True   NaN    NaN\n   3          0.0     True  False   9.0    3.0\n   4        100.0    False   True   NaN    NaN\n   5          3.0    False   True   NaN    NaN\n   6          2.0    False   True   NaN    NaN\n   7          1.0     True   True   6.0    2.0\n</code></pre>\n\n<p>EDIT2: Another clarification:</p>\n\n<p>I did indeed not miscalculate, but rather I did not make my intentions as clear as I could have. Here's another try using the same dataframe:</p>\n\n<p><a href=\"https://i.stack.imgur.com/6o74c.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/6o74c.png\" alt=\"Desired dataframe, relevant fields highlighted\"></a></p>\n\n<p>Let's first look at the <code>Trigger</code> column: We find the first <code>True</code> in index 3 (green rectangle). So index 3 is the point, where we start looking. There is no valid measurement at index 3 (Column <code>Valid</code> is <code>False</code>; red rectangle). So, we start to go further back in time, until we have accumulated three lines, where <code>Valid</code> is <code>True</code>. This happens for indices 2,1 and 0. For these three indices, we calculate the sum and mean of the column <code>Measurement</code> (blue rectangle): </p>\n\n<ul>\n<li>SUM: 2.0 + 4.0 + 3.0 = 9.0 </li>\n<li>MEAN: (2.0 + 4.0 + 3.0) / 3 = 3.0</li>\n</ul>\n\n<p>Now we start the next iteration of this little algorithm: Look again for the next <code>True</code> in the <code>Trigger</code> column. We find it at index 7 (green rectangle). There is also a valid measuremnt at index 7, so we include it this time. For our calculation, we use indices 7,6 and 5 (green rectangle), and thus get:</p>\n\n<ul>\n<li>SUM: 1.0 + 2.0 + 3.0 = 6.0 </li>\n<li>MEAN: (1.0 + 2.0 + 3.0) / 3 = 2.0</li>\n</ul>\n\n<p>I hope, this sheds more light on this little problem.</p>\n",
        "answer_body": "<p>Heres an option, take the 3 period rolling mean and sum</p>\n\n<pre><code>df['RollM'] = df.Measurement.rolling(window=3,min_periods=0).mean()\n\ndf['RollS'] = df.Measurement.rolling(window=3,min_periods=0).sum()\n</code></pre>\n\n<p>Now set False Triggers equals to <code>NaN</code></p>\n\n<pre><code>df.loc[df.Trigger == False,['RollS','RollM']] = np.nan\n</code></pre>\n\n<p>yields</p>\n\n<pre><code>   Measurement  Trigger  Valid     RollM  RollS\n0          2.0    False   True       NaN    NaN\n1          4.0    False   True       NaN    NaN\n2          3.0    False   True       NaN    NaN\n3          0.0     True  False  2.333333    7.0\n4        100.0    False   True       NaN    NaN\n5          3.0    False   True       NaN    NaN\n6          2.0    False   True       NaN    NaN\n7          1.0     True   True  2.000000    6.0\n</code></pre>\n\n<hr>\n\n<p>Edit, updated to reflect valid argument</p>\n\n<pre><code>df['mean'],df['sum'] = np.nan,np.nan\n\nroller = df.Measurement.rolling(window=3,min_periods=0).agg(['mean','sum'])\n\ndf.loc[(df.Trigger == True) &amp; (df.Valid == True),['mean','sum']] = roller\n\ndf.loc[(df.Trigger == True) &amp; (df.Valid == False),['mean','sum']] = roller.shift(1)\n</code></pre>\n\n<p>Yields</p>\n\n<pre><code>  Measurement  Trigger  Valid  mean  sum\n0          2.0    False   True   NaN  NaN\n1          4.0    False   True   NaN  NaN\n2          3.0    False   True   NaN  NaN\n3          0.0     True  False   3.0  9.0\n4        100.0    False   True   NaN  NaN\n5          3.0    False   True   NaN  NaN\n6          2.0    False   True   NaN  NaN\n7          1.0     True   True   2.0  6.0\n</code></pre>\n",
        "question_body": "<p>Concerned is this exemplary pandas dataframe:</p>\n\n<pre><code>      Measurement  Trigger  Valid\n   0          2.0    False   True\n   1          4.0    False   True\n   2          3.0    False   True\n   3          0.0     True  False\n   4        100.0    False   True\n   5          3.0    False   True\n   6          2.0    False   True\n   7          1.0     True   True\n</code></pre>\n\n<p>Whenever <code>Trigger</code> is <code>True</code>, I wish to calculate sum and mean of the last 3 (starting from current) valid measurements. Measurements are considered valid, if the column <code>Valid</code> is <code>True</code>. So let's clarify using the two examples in the above dataframe:</p>\n\n<ol>\n<li><code>Index 3</code>: Indices <code>2,1,0</code> should be used. Expected <code>Sum = 9.0, Mean = 3.0</code></li>\n<li><code>Index 7</code>: Indices <code>7,6,5</code> should be used. Expected <code>Sum = 6.0, Mean = 2.0</code></li>\n</ol>\n\n<p>I have tried <code>pandas.rolling</code> and creating new, shifted columns, but was not successful. See the following excerpt from my tests (which should directly run):</p>\n\n<pre><code>import unittest\nimport pandas as pd\nimport numpy as np\nfrom pandas.util.testing import assert_series_equal\n\ndef create_sample_dataframe_2():\n    df = pd.DataFrame(\n        {\"Measurement\" : [2.0,   4.0,   3.0,   0.0,   100.0, 3.0,   2.0,   1.0 ],\n         \"Valid\"       : [True,  True,  True,  False, True,  True,  True,  True],\n         \"Trigger\"     : [False, False, False, True,  False, False, False, True],\n         })\n    return df\n\ndef expected_result():\n    return pd.DataFrame({\"Sum\" : [np.nan, np.nan, np.nan, 9.0, np.nan, np.nan, np.nan, 6.0],\n                         \"Mean\" :[np.nan, np.nan, np.nan, 3.0, np.nan, np.nan, np.nan, 2.0]})\n\nclass Data_Preparation_Functions(unittest.TestCase):\n\n    def test_backsummation(self):\n        N_SUMMANDS = 3\n        temp_vars = []\n\n        df = create_sample_dataframe_2()\n        for i in range(0,N_SUMMANDS):\n            temp_var = \"M_{0}\".format(i)\n            df[temp_var] = df[\"Measurement\"].shift(i)\n            temp_vars.append(temp_var)\n\n        df[\"Sum\"]  = df[temp_vars].sum(axis=1)\n        df[\"Mean\"] = df[temp_vars].mean(axis=1)\n        df.loc[(df[\"Trigger\"]==False), \"Sum\"] = np.nan\n        df.loc[(df[\"Trigger\"]==False), \"Mean\"] = np.nan\n\n        assert_series_equal(expected_result()[\"Sum\"],df[\"Sum\"])\n        assert_series_equal(expected_result()[\"Mean\"],df[\"Mean\"])\n\n    def test_rolling(self):\n        df = create_sample_dataframe_2()\n        df[\"Sum\"]  = df[(df[\"Valid\"] == True)][\"Measurement\"].rolling(window=3).sum()\n        df[\"Mean\"] = df[(df[\"Valid\"] == True)][\"Measurement\"].rolling(window=3).mean()\n\n        df.loc[(df[\"Trigger\"]==False), \"Sum\"] = np.nan\n        df.loc[(df[\"Trigger\"]==False), \"Mean\"] = np.nan\n        assert_series_equal(expected_result()[\"Sum\"],df[\"Sum\"])\n        assert_series_equal(expected_result()[\"Mean\"],df[\"Mean\"])\n\n\nif __name__ == '__main__':\n    suite = unittest.TestLoader().loadTestsFromTestCase(Data_Preparation_Functions)\n    unittest.TextTestRunner(verbosity=2).run(suite)\n</code></pre>\n\n<p>Any help or solution is greatly appreciated. Thanks and Cheers!</p>\n\n<p>EDIT: Clarification: This is the resulting dataframe I expect:</p>\n\n<pre><code>      Measurement  Trigger  Valid   Sum   Mean\n   0          2.0    False   True   NaN    NaN\n   1          4.0    False   True   NaN    NaN\n   2          3.0    False   True   NaN    NaN\n   3          0.0     True  False   9.0    3.0\n   4        100.0    False   True   NaN    NaN\n   5          3.0    False   True   NaN    NaN\n   6          2.0    False   True   NaN    NaN\n   7          1.0     True   True   6.0    2.0\n</code></pre>\n\n<p>EDIT2: Another clarification:</p>\n\n<p>I did indeed not miscalculate, but rather I did not make my intentions as clear as I could have. Here's another try using the same dataframe:</p>\n\n<p><a href=\"https://i.stack.imgur.com/6o74c.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/6o74c.png\" alt=\"Desired dataframe, relevant fields highlighted\"></a></p>\n\n<p>Let's first look at the <code>Trigger</code> column: We find the first <code>True</code> in index 3 (green rectangle). So index 3 is the point, where we start looking. There is no valid measurement at index 3 (Column <code>Valid</code> is <code>False</code>; red rectangle). So, we start to go further back in time, until we have accumulated three lines, where <code>Valid</code> is <code>True</code>. This happens for indices 2,1 and 0. For these three indices, we calculate the sum and mean of the column <code>Measurement</code> (blue rectangle): </p>\n\n<ul>\n<li>SUM: 2.0 + 4.0 + 3.0 = 9.0 </li>\n<li>MEAN: (2.0 + 4.0 + 3.0) / 3 = 3.0</li>\n</ul>\n\n<p>Now we start the next iteration of this little algorithm: Look again for the next <code>True</code> in the <code>Trigger</code> column. We find it at index 7 (green rectangle). There is also a valid measuremnt at index 7, so we include it this time. For our calculation, we use indices 7,6 and 5 (green rectangle), and thus get:</p>\n\n<ul>\n<li>SUM: 1.0 + 2.0 + 3.0 = 6.0 </li>\n<li>MEAN: (1.0 + 2.0 + 3.0) / 3 = 2.0</li>\n</ul>\n\n<p>I hope, this sheds more light on this little problem.</p>\n",
        "formatted_input": {
            "qid": 46723310,
            "link": "https://stackoverflow.com/questions/46723310/conditional-mean-and-sum-of-previous-n-rows-in-pandas-dataframe",
            "question": {
                "title": "Conditional mean and sum of previous N rows in pandas dataframe",
                "ques_desc": "Concerned is this exemplary pandas dataframe: Whenever is , I wish to calculate sum and mean of the last 3 (starting from current) valid measurements. Measurements are considered valid, if the column is . So let's clarify using the two examples in the above dataframe: : Indices should be used. Expected : Indices should be used. Expected I have tried and creating new, shifted columns, but was not successful. See the following excerpt from my tests (which should directly run): Any help or solution is greatly appreciated. Thanks and Cheers! EDIT: Clarification: This is the resulting dataframe I expect: EDIT2: Another clarification: I did indeed not miscalculate, but rather I did not make my intentions as clear as I could have. Here's another try using the same dataframe: Let's first look at the column: We find the first in index 3 (green rectangle). So index 3 is the point, where we start looking. There is no valid measurement at index 3 (Column is ; red rectangle). So, we start to go further back in time, until we have accumulated three lines, where is . This happens for indices 2,1 and 0. For these three indices, we calculate the sum and mean of the column (blue rectangle): SUM: 2.0 + 4.0 + 3.0 = 9.0 MEAN: (2.0 + 4.0 + 3.0) / 3 = 3.0 Now we start the next iteration of this little algorithm: Look again for the next in the column. We find it at index 7 (green rectangle). There is also a valid measuremnt at index 7, so we include it this time. For our calculation, we use indices 7,6 and 5 (green rectangle), and thus get: SUM: 1.0 + 2.0 + 3.0 = 6.0 MEAN: (1.0 + 2.0 + 3.0) / 3 = 2.0 I hope, this sheds more light on this little problem. "
            },
            "io": [
                "Sum = 9.0, Mean = 3.0",
                "Sum = 6.0, Mean = 2.0"
            ],
            "answer": {
                "ans_desc": "Heres an option, take the 3 period rolling mean and sum Now set False Triggers equals to yields Edit, updated to reflect valid argument Yields ",
                "code": [
                    "df['RollM'] = df.Measurement.rolling(window=3,min_periods=0).mean()\n\ndf['RollS'] = df.Measurement.rolling(window=3,min_periods=0).sum()\n",
                    "df.loc[df.Trigger == False,['RollS','RollM']] = np.nan\n",
                    "   Measurement  Trigger  Valid     RollM  RollS\n0          2.0    False   True       NaN    NaN\n1          4.0    False   True       NaN    NaN\n2          3.0    False   True       NaN    NaN\n3          0.0     True  False  2.333333    7.0\n4        100.0    False   True       NaN    NaN\n5          3.0    False   True       NaN    NaN\n6          2.0    False   True       NaN    NaN\n7          1.0     True   True  2.000000    6.0\n",
                    "df['mean'],df['sum'] = np.nan,np.nan\n\nroller = df.Measurement.rolling(window=3,min_periods=0).agg(['mean','sum'])\n\ndf.loc[(df.Trigger == True) & (df.Valid == True),['mean','sum']] = roller\n\ndf.loc[(df.Trigger == True) & (df.Valid == False),['mean','sum']] = roller.shift(1)\n",
                    "  Measurement  Trigger  Valid  mean  sum\n0          2.0    False   True   NaN  NaN\n1          4.0    False   True   NaN  NaN\n2          3.0    False   True   NaN  NaN\n3          0.0     True  False   3.0  9.0\n4        100.0    False   True   NaN  NaN\n5          3.0    False   True   NaN  NaN\n6          2.0    False   True   NaN  NaN\n7          1.0     True   True   2.0  6.0\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 812,
            "user_id": 6150310,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cb499bc6f5987c406bdb42908a73b666?s=128&d=identicon&r=PG&f=1",
            "display_name": "mlenthusiast",
            "link": "https://stackoverflow.com/users/6150310/mlenthusiast"
        },
        "is_answered": true,
        "view_count": 62,
        "accepted_answer_id": 54737959,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1550442570,
        "creation_date": 1550439174,
        "last_edit_date": 1550442570,
        "question_id": 54737891,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54737891/convert-the-last-non-zero-value-to-0-for-each-row-in-a-pandas-dataframe",
        "title": "Convert the last non-zero value to 0 for each row in a pandas DataFrame",
        "body": "<p>I'm trying to modify my data frame in a way that the last variable of a label encoded feature is converted to 0. For example, I have this data frame, top row being the labels and the first column as the index:</p>\n\n<pre><code>df\n   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   1\n1  0  0  0  1  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  1   0\n</code></pre>\n\n<p>Columns 1-10 are the ones that have been encoded. What I want to convert this data frame to, without changing anything else is:</p>\n\n<pre><code>   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   0\n1  0  0  0  0  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  0   0\n</code></pre>\n\n<p>So the last values occurring in each row should be converted to 0. I was thinking of using the last_valid_index method, but that would take in the other remaining columns and change that as well, which I don't want. Any help is appreciated</p>\n",
        "answer_body": "<p>You can use <code>cumsum</code> to build a boolean mask, and set to zero. </p>\n\n<pre><code>v = df.cumsum(axis=1)\ndf[v.lt(v.max(axis=1), axis=0)].fillna(0, downcast='infer')\n\n   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   0\n1  0  0  0  0  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  0   0\n</code></pre>\n\n<hr>\n\n<p>Another similar option is reversing before calling <code>cumsum</code>, you can now do this in a single line.</p>\n\n<pre><code>df[~df.iloc[:, ::-1].cumsum(1).le(1)].fillna(0, downcast='infer')\n\n   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   0\n1  0  0  0  0  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  0   0\n</code></pre>\n\n<p>If you have more columns, just apply these operations on the slice. Later, assign back.</p>\n\n<pre><code>u = df.iloc[:, :10]\ndf[u.columns] = u[~u.iloc[:, ::-1].cumsum(1).le(1)].fillna(0, downcast='infer')\n</code></pre>\n",
        "question_body": "<p>I'm trying to modify my data frame in a way that the last variable of a label encoded feature is converted to 0. For example, I have this data frame, top row being the labels and the first column as the index:</p>\n\n<pre><code>df\n   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   1\n1  0  0  0  1  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  1   0\n</code></pre>\n\n<p>Columns 1-10 are the ones that have been encoded. What I want to convert this data frame to, without changing anything else is:</p>\n\n<pre><code>   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   0\n1  0  0  0  0  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  0   0\n</code></pre>\n\n<p>So the last values occurring in each row should be converted to 0. I was thinking of using the last_valid_index method, but that would take in the other remaining columns and change that as well, which I don't want. Any help is appreciated</p>\n",
        "formatted_input": {
            "qid": 54737891,
            "link": "https://stackoverflow.com/questions/54737891/convert-the-last-non-zero-value-to-0-for-each-row-in-a-pandas-dataframe",
            "question": {
                "title": "Convert the last non-zero value to 0 for each row in a pandas DataFrame",
                "ques_desc": "I'm trying to modify my data frame in a way that the last variable of a label encoded feature is converted to 0. For example, I have this data frame, top row being the labels and the first column as the index: Columns 1-10 are the ones that have been encoded. What I want to convert this data frame to, without changing anything else is: So the last values occurring in each row should be converted to 0. I was thinking of using the last_valid_index method, but that would take in the other remaining columns and change that as well, which I don't want. Any help is appreciated "
            },
            "io": [
                "df\n   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   1\n1  0  0  0  1  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  1   0\n",
                "   1  2  3  4  5  6  7  8  9  10\n0  0  1  0  0  0  0  0  0  1   0\n1  0  0  0  0  0  0  0  0  0   0\n2  0  0  0  0  0  0  0  0  0   0\n"
            ],
            "answer": {
                "ans_desc": "You can use to build a boolean mask, and set to zero. Another similar option is reversing before calling , you can now do this in a single line. If you have more columns, just apply these operations on the slice. Later, assign back. ",
                "code": [
                    "u = df.iloc[:, :10]\ndf[u.columns] = u[~u.iloc[:, ::-1].cumsum(1).le(1)].fillna(0, downcast='infer')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 365,
            "user_id": 6540155,
            "user_type": "registered",
            "accept_rate": 92,
            "profile_image": "https://www.gravatar.com/avatar/d060b9594c65947b9ecbcf8484cee3c0?s=128&d=identicon&r=PG&f=1",
            "display_name": "user_01",
            "link": "https://stackoverflow.com/users/6540155/user-01"
        },
        "is_answered": true,
        "view_count": 2773,
        "closed_date": 1601613524,
        "accepted_answer_id": 50420112,
        "answer_count": 4,
        "score": 3,
        "last_activity_date": 1550428884,
        "creation_date": 1526672253,
        "question_id": 50418202,
        "link": "https://stackoverflow.com/questions/50418202/pandas-remove-all-nan-values-in-all-columns",
        "closed_reason": "Duplicate",
        "title": "Pandas: Remove all NaN values in all columns",
        "body": "<p>I have a data frame with many null records:</p>\n\n<pre><code>Col_1    Col_2      Col_3\n10         5          2\n22         7          7\n3         9          5       \n4         NaN       NaN\n5         NaN       NaN\n6         4         NaN\n7         6          7\n8         10        NaN\n12        NaN        1\n</code></pre>\n\n<p>I want to remove all NaN values in all rows of columns . As you could see, each column has different number of rows. So, I want to get something like this:</p>\n\n<pre><code>Col_1    Col_2      Col_3\n10         5          2\n22         7          7\n3          9          5       \n4          4          7\n6          6          1\n7         10          \n8                 \n12    \n</code></pre>\n\n<p>I tried</p>\n\n<pre><code>filtered_df = df.dropna(how='any')\n</code></pre>\n\n<p>But it removes all records in the dataframe. How may I do that ?           </p>\n",
        "answer_body": "<blockquote>\n  <p>As you could see, each column has different number of rows.</p>\n</blockquote>\n\n<p>A DataFrame is a tabular data structure: you can look up an index and a column, and find the value. If the number of rows is different per columns, then the index is meaningless and misleading. A <code>dict</code> might be a better alternative:</p>\n\n<pre><code>{c: df[c].dropna().values for c in df.columns}\n</code></pre>\n\n<p>or</p>\n\n<pre><code>{c: list(df[c]) for c in df.columns}\n</code></pre>\n",
        "question_body": "<p>I have a data frame with many null records:</p>\n\n<pre><code>Col_1    Col_2      Col_3\n10         5          2\n22         7          7\n3         9          5       \n4         NaN       NaN\n5         NaN       NaN\n6         4         NaN\n7         6          7\n8         10        NaN\n12        NaN        1\n</code></pre>\n\n<p>I want to remove all NaN values in all rows of columns . As you could see, each column has different number of rows. So, I want to get something like this:</p>\n\n<pre><code>Col_1    Col_2      Col_3\n10         5          2\n22         7          7\n3          9          5       \n4          4          7\n6          6          1\n7         10          \n8                 \n12    \n</code></pre>\n\n<p>I tried</p>\n\n<pre><code>filtered_df = df.dropna(how='any')\n</code></pre>\n\n<p>But it removes all records in the dataframe. How may I do that ?           </p>\n",
        "formatted_input": {
            "qid": 50418202,
            "link": "https://stackoverflow.com/questions/50418202/pandas-remove-all-nan-values-in-all-columns",
            "question": {
                "title": "Pandas: Remove all NaN values in all columns",
                "ques_desc": "I have a data frame with many null records: I want to remove all NaN values in all rows of columns . As you could see, each column has different number of rows. So, I want to get something like this: I tried But it removes all records in the dataframe. How may I do that ? "
            },
            "io": [
                "Col_1    Col_2      Col_3\n10         5          2\n22         7          7\n3         9          5       \n4         NaN       NaN\n5         NaN       NaN\n6         4         NaN\n7         6          7\n8         10        NaN\n12        NaN        1\n",
                "Col_1    Col_2      Col_3\n10         5          2\n22         7          7\n3          9          5       \n4          4          7\n6          6          1\n7         10          \n8                 \n12    \n"
            ],
            "answer": {
                "ans_desc": " As you could see, each column has different number of rows. A DataFrame is a tabular data structure: you can look up an index and a column, and find the value. If the number of rows is different per columns, then the index is meaningless and misleading. A might be a better alternative: or ",
                "code": [
                    "{c: df[c].dropna().values for c in df.columns}\n",
                    "{c: list(df[c]) for c in df.columns}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 365,
            "user_id": 11006089,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8e0d68ed2aee13a275b2680c7d63bd5a?s=128&d=identicon&r=PG&f=1",
            "display_name": "Rohit",
            "link": "https://stackoverflow.com/users/11006089/rohit"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 54734285,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1550414153,
        "creation_date": 1550411047,
        "last_edit_date": 1550411995,
        "question_id": 54733869,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54733869/how-to-get-the-rank-of-current-row-compared-to-previous-rows",
        "title": "How to get the Rank of current row compared to previous rows",
        "body": "<p>How to get the Rank of current row compared to previous rows </p>\n\n<p>I have a dataframe like:</p>\n\n<pre><code>Instru Price Volume\nABCD   1000  100258\nABCD   1000  100252\nABCD   1000  100168\nABCD   1000  100390\nABCD   1000  100470\nABCD   1000  100420\n</code></pre>\n\n<p>I want to get the rank of current row compared to all previous rows for Volume Column. </p>\n\n<p>Desired Dataframe Data:</p>\n\n<pre><code>Instru Price Volume  Rank\nABCD   1000  100258  1     =&gt; 1st Row so Rank 1\nABCD   1000  100252  2     =&gt; Rank 2 (Compare 100258,100252)\nABCD   1000  100168  3     =&gt; Rank 3 (Compare 100258,100252,100168)\nABCD   1000  100390  1     =&gt; Rank 1 (Compare 100390,100258,100252,100168)\nABCD   1000  100470  1     =&gt; Rank 1 (Compare 100470,100390,100258,100252,100168)\nABCD   1000  100420  2     =&gt; Rank 2 (Compare 100470,100420,100390,100258,100252,100168)\n</code></pre>\n\n<p>pandas.DataFrame.rank Function doesnot serve my purpose.</p>\n",
        "answer_body": "<p>Use <a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.searchsorted.html\" rel=\"noreferrer\">np.searchsorted</a> after a <em>cumulative sort</em>:</p>\n\n<pre><code>df['Rank'] = np.array([i - np.searchsorted(sorted(df.Volume[:i]), v) for i, v in enumerate(df.Volume)]) + 1\nprint(df)\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>  Instru  Price  Volume  Rank\n0   ABCD   1000  100258     1\n1   ABCD   1000  100252     2\n2   ABCD   1000  100168     3\n3   ABCD   1000  100390     1\n4   ABCD   1000  100470     1\n5   ABCD   1000  100420     2\n</code></pre>\n",
        "question_body": "<p>How to get the Rank of current row compared to previous rows </p>\n\n<p>I have a dataframe like:</p>\n\n<pre><code>Instru Price Volume\nABCD   1000  100258\nABCD   1000  100252\nABCD   1000  100168\nABCD   1000  100390\nABCD   1000  100470\nABCD   1000  100420\n</code></pre>\n\n<p>I want to get the rank of current row compared to all previous rows for Volume Column. </p>\n\n<p>Desired Dataframe Data:</p>\n\n<pre><code>Instru Price Volume  Rank\nABCD   1000  100258  1     =&gt; 1st Row so Rank 1\nABCD   1000  100252  2     =&gt; Rank 2 (Compare 100258,100252)\nABCD   1000  100168  3     =&gt; Rank 3 (Compare 100258,100252,100168)\nABCD   1000  100390  1     =&gt; Rank 1 (Compare 100390,100258,100252,100168)\nABCD   1000  100470  1     =&gt; Rank 1 (Compare 100470,100390,100258,100252,100168)\nABCD   1000  100420  2     =&gt; Rank 2 (Compare 100470,100420,100390,100258,100252,100168)\n</code></pre>\n\n<p>pandas.DataFrame.rank Function doesnot serve my purpose.</p>\n",
        "formatted_input": {
            "qid": 54733869,
            "link": "https://stackoverflow.com/questions/54733869/how-to-get-the-rank-of-current-row-compared-to-previous-rows",
            "question": {
                "title": "How to get the Rank of current row compared to previous rows",
                "ques_desc": "How to get the Rank of current row compared to previous rows I have a dataframe like: I want to get the rank of current row compared to all previous rows for Volume Column. Desired Dataframe Data: pandas.DataFrame.rank Function doesnot serve my purpose. "
            },
            "io": [
                "Instru Price Volume\nABCD   1000  100258\nABCD   1000  100252\nABCD   1000  100168\nABCD   1000  100390\nABCD   1000  100470\nABCD   1000  100420\n",
                "Instru Price Volume  Rank\nABCD   1000  100258  1     => 1st Row so Rank 1\nABCD   1000  100252  2     => Rank 2 (Compare 100258,100252)\nABCD   1000  100168  3     => Rank 3 (Compare 100258,100252,100168)\nABCD   1000  100390  1     => Rank 1 (Compare 100390,100258,100252,100168)\nABCD   1000  100470  1     => Rank 1 (Compare 100470,100390,100258,100252,100168)\nABCD   1000  100420  2     => Rank 2 (Compare 100470,100420,100390,100258,100252,100168)\n"
            ],
            "answer": {
                "ans_desc": "Use np.searchsorted after a cumulative sort: Output ",
                "code": [
                    "df['Rank'] = np.array([i - np.searchsorted(sorted(df.Volume[:i]), v) for i, v in enumerate(df.Volume)]) + 1\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2320,
            "user_id": 2835640,
            "user_type": "registered",
            "accept_rate": 95,
            "profile_image": "https://www.gravatar.com/avatar/1306087f30764a6ac039b5b9ee7f1e2c?s=128&d=identicon&r=PG&f=1",
            "display_name": "Hans Roggeman",
            "link": "https://stackoverflow.com/users/2835640/hans-roggeman"
        },
        "is_answered": true,
        "view_count": 371,
        "accepted_answer_id": 54692335,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1550156864,
        "creation_date": 1549833850,
        "last_edit_date": 1550024554,
        "question_id": 54621203,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54621203/efficient-python-pandas-equivalent-implementation-of-r-sweep-with-multiple-argum",
        "title": "Efficient python pandas equivalent/implementation of R sweep with multiple arguments",
        "body": "<p>Other questions attempting to provide the <code>python</code> equivalent to <code>R</code>'s <code>sweep</code>function (like <a href=\"https://stackoverflow.com/questions/23117756/python-numpy-or-pandas-equivalent-of-the-r-function-swee\">here</a>) do not really address the case of multiple arguments where it is most useful. </p>\n\n<p>Say I wish to apply a 2 argument function to each row of a Dataframe with the matching element from a column of another DataFrame:</p>\n\n<pre><code>df = data.frame(\"A\" = 1:3,\"B\" = 11:13)\ndf2= data.frame(\"X\" = 10:12,\"Y\" = 10000:10002)\nsweep(df,1, FUN=\"*\",df2$X)\n</code></pre>\n\n<p>In <code>python</code> I got the equivalent using <code>apply</code> on what is basically a loop through the row counts. </p>\n\n<pre><code>df = pd.DataFrame( { \"A\" : range(1,4),\"B\" : range(11,14) } )\ndf2 = pd.DataFrame( { \"X\" : range(10,13),\"Y\" : range(10000,10003) } )\npd.Series(range(df.shape[0])).apply(lambda row_count: np.multiply(df.iloc[row_count,:],df2.iloc[row_count,df2.columns.get_loc('X')]))\n</code></pre>\n\n<p>I highly doubt this is efficient in <code>pandas</code>, what is a better way of doing this?  </p>\n\n<p>Both bits of code should result in a Dataframe/matrix of 6 numbers when applying <code>*</code>:</p>\n\n<pre><code>   A   B\n1 10 110\n2 22 132\n3 36 156\n</code></pre>\n\n<p>I should state clearly that the aim is to insert one's own function into this <code>sweep</code> like behavior say:</p>\n\n<pre><code>df = data.frame(\"A\" = 1:3,\"B\" = 11:13)\ndf2= data.frame(\"X\" = 10:12,\"Y\" = 10000:10002)\nmyFunc = function(a,b) { floor((a + b)^min(a/2,b/3))  }\nsweep(df,1, FUN=myFunc,df2$X)\n</code></pre>\n\n<p>resulting in:</p>\n\n<pre><code> A B\n[1,] 3 4\n[2,] 3 4\n[3,] 3 5\n</code></pre>\n\n<p>What is a good way of doing that in python pandas? </p>\n",
        "answer_body": "<p>If I understand this correctly, you are looking to apply a binary function f(x,y) to a dataframe (for the x) row-wise with arguments from a series for y. One way to do this is to borrow the implementation from pandas internals itself. If you want to extend this function (e.g. apply along columns, it can be done in a similar manner, as long as f is binary. If you need more arguments, you can simply do a <code>partial</code> on f to make it binary</p>\n\n<pre><code>import pandas as pd\nfrom pandas.core.dtypes.generic import ABCSeries\n\ndef sweep(df, series, FUN):\n    assert isinstance(series, ABCSeries)\n\n    # row-wise application\n    assert len(df) == len(series)\n    return df._combine_match_index(series, FUN)\n\n\n# define your binary operator\ndef f(x, y):\n    return x*y    \n\n# the input data frames\ndf = pd.DataFrame( { \"A\" : range(1,4),\"B\" : range(11,14) } )\ndf2 = pd.DataFrame( { \"X\" : range(10,13),\"Y\" : range(10000,10003) } )\n\n# apply\ntest1 = sweep(df, df2.X, f)\n\n# performance\n# %timeit sweep(df, df2.X, f)\n# 155 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)#\n\n# another method\nimport numpy as np\ntest2 = pd.Series(range(df.shape[0])).apply(lambda row_count: np.multiply(df.iloc[row_count,:],df2.iloc[row_count,df2.columns.get_loc('X')]))\n\n# %timeit performance\n# 1.54 ms \u00b1 56.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nassert all(test1 == test2)\n</code></pre>\n\n<p>Hope this helps.</p>\n",
        "question_body": "<p>Other questions attempting to provide the <code>python</code> equivalent to <code>R</code>'s <code>sweep</code>function (like <a href=\"https://stackoverflow.com/questions/23117756/python-numpy-or-pandas-equivalent-of-the-r-function-swee\">here</a>) do not really address the case of multiple arguments where it is most useful. </p>\n\n<p>Say I wish to apply a 2 argument function to each row of a Dataframe with the matching element from a column of another DataFrame:</p>\n\n<pre><code>df = data.frame(\"A\" = 1:3,\"B\" = 11:13)\ndf2= data.frame(\"X\" = 10:12,\"Y\" = 10000:10002)\nsweep(df,1, FUN=\"*\",df2$X)\n</code></pre>\n\n<p>In <code>python</code> I got the equivalent using <code>apply</code> on what is basically a loop through the row counts. </p>\n\n<pre><code>df = pd.DataFrame( { \"A\" : range(1,4),\"B\" : range(11,14) } )\ndf2 = pd.DataFrame( { \"X\" : range(10,13),\"Y\" : range(10000,10003) } )\npd.Series(range(df.shape[0])).apply(lambda row_count: np.multiply(df.iloc[row_count,:],df2.iloc[row_count,df2.columns.get_loc('X')]))\n</code></pre>\n\n<p>I highly doubt this is efficient in <code>pandas</code>, what is a better way of doing this?  </p>\n\n<p>Both bits of code should result in a Dataframe/matrix of 6 numbers when applying <code>*</code>:</p>\n\n<pre><code>   A   B\n1 10 110\n2 22 132\n3 36 156\n</code></pre>\n\n<p>I should state clearly that the aim is to insert one's own function into this <code>sweep</code> like behavior say:</p>\n\n<pre><code>df = data.frame(\"A\" = 1:3,\"B\" = 11:13)\ndf2= data.frame(\"X\" = 10:12,\"Y\" = 10000:10002)\nmyFunc = function(a,b) { floor((a + b)^min(a/2,b/3))  }\nsweep(df,1, FUN=myFunc,df2$X)\n</code></pre>\n\n<p>resulting in:</p>\n\n<pre><code> A B\n[1,] 3 4\n[2,] 3 4\n[3,] 3 5\n</code></pre>\n\n<p>What is a good way of doing that in python pandas? </p>\n",
        "formatted_input": {
            "qid": 54621203,
            "link": "https://stackoverflow.com/questions/54621203/efficient-python-pandas-equivalent-implementation-of-r-sweep-with-multiple-argum",
            "question": {
                "title": "Efficient python pandas equivalent/implementation of R sweep with multiple arguments",
                "ques_desc": "Other questions attempting to provide the equivalent to 's function (like here) do not really address the case of multiple arguments where it is most useful. Say I wish to apply a 2 argument function to each row of a Dataframe with the matching element from a column of another DataFrame: In I got the equivalent using on what is basically a loop through the row counts. I highly doubt this is efficient in , what is a better way of doing this? Both bits of code should result in a Dataframe/matrix of 6 numbers when applying : I should state clearly that the aim is to insert one's own function into this like behavior say: resulting in: What is a good way of doing that in python pandas? "
            },
            "io": [
                "   A   B\n1 10 110\n2 22 132\n3 36 156\n",
                " A B\n[1,] 3 4\n[2,] 3 4\n[3,] 3 5\n"
            ],
            "answer": {
                "ans_desc": "If I understand this correctly, you are looking to apply a binary function f(x,y) to a dataframe (for the x) row-wise with arguments from a series for y. One way to do this is to borrow the implementation from pandas internals itself. If you want to extend this function (e.g. apply along columns, it can be done in a similar manner, as long as f is binary. If you need more arguments, you can simply do a on f to make it binary Hope this helps. ",
                "code": [
                    "import pandas as pd\nfrom pandas.core.dtypes.generic import ABCSeries\n\ndef sweep(df, series, FUN):\n    assert isinstance(series, ABCSeries)\n\n    # row-wise application\n    assert len(df) == len(series)\n    return df._combine_match_index(series, FUN)\n\n\n# define your binary operator\ndef f(x, y):\n    return x*y    \n\n# the input data frames\ndf = pd.DataFrame( { \"A\" : range(1,4),\"B\" : range(11,14) } )\ndf2 = pd.DataFrame( { \"X\" : range(10,13),\"Y\" : range(10000,10003) } )\n\n# apply\ntest1 = sweep(df, df2.X, f)\n\n# performance\n# %timeit sweep(df, df2.X, f)\n# 155 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)#\n\n# another method\nimport numpy as np\ntest2 = pd.Series(range(df.shape[0])).apply(lambda row_count: np.multiply(df.iloc[row_count,:],df2.iloc[row_count,df2.columns.get_loc('X')]))\n\n# %timeit performance\n# 1.54 ms \u00b1 56.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nassert all(test1 == test2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary",
            "data-analysis"
        ],
        "owner": {
            "reputation": 43,
            "user_id": 11057539,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/75de8a911221be4657e7abd386dae0d4?s=128&d=identicon&r=PG&f=1",
            "display_name": "IJ.K",
            "link": "https://stackoverflow.com/users/11057539/ij-k"
        },
        "is_answered": true,
        "view_count": 1569,
        "accepted_answer_id": 54674990,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1550078264,
        "creation_date": 1550075083,
        "last_edit_date": 1550075942,
        "question_id": 54674956,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54674956/transfer-pandas-dataframe-column-names-to-dictionary",
        "title": "Transfer pandas dataframe column names to dictionary",
        "body": "<p>I'm trying to convert a pandas dataframe column names into a dictionary. Not so worried about the actual data in the dataframe.</p>\n\n<p>Say I have an example dataframe like this and I'm not too worried about index just now:</p>\n\n<pre><code>Col1 Col2 Col3 Col4\n--------------------\n a    b    c    a\n b    d    e    c\n</code></pre>\n\n<p>I'd like to get an output of a dictionary like:</p>\n\n<pre><code>{'Col1': 0, 'Col2': 1, 'Col3': 2, 'Col4': 3}\n</code></pre>\n\n<p>Not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order.</p>\n",
        "answer_body": "<p>That is straight forward with a comprehension as:</p>\n\n<h3>Code:</h3>\n\n<pre><code>{c: i for i, c in enumerate(df.columns)}\n</code></pre>\n\n<h3>Test Code:</h3>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'date': ['2015-01-01', '2015-01-02', '2015-01-03'],\n                   'value': ['a', 'b', 'c'],\n                   'num': [1, 2, 3]\n                   })\n\nprint(df)\nprint({c: i for i, c in enumerate(df.columns)})\n</code></pre>\n\n<h3>Results:</h3>\n\n<pre><code>         date  num value\n0  2015-01-01    1     a\n1  2015-01-02    2     b\n2  2015-01-03    3     c\n\n{'date': 0, 'num': 1, 'value': 2}\n</code></pre>\n",
        "question_body": "<p>I'm trying to convert a pandas dataframe column names into a dictionary. Not so worried about the actual data in the dataframe.</p>\n\n<p>Say I have an example dataframe like this and I'm not too worried about index just now:</p>\n\n<pre><code>Col1 Col2 Col3 Col4\n--------------------\n a    b    c    a\n b    d    e    c\n</code></pre>\n\n<p>I'd like to get an output of a dictionary like:</p>\n\n<pre><code>{'Col1': 0, 'Col2': 1, 'Col3': 2, 'Col4': 3}\n</code></pre>\n\n<p>Not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order.</p>\n",
        "formatted_input": {
            "qid": 54674956,
            "link": "https://stackoverflow.com/questions/54674956/transfer-pandas-dataframe-column-names-to-dictionary",
            "question": {
                "title": "Transfer pandas dataframe column names to dictionary",
                "ques_desc": "I'm trying to convert a pandas dataframe column names into a dictionary. Not so worried about the actual data in the dataframe. Say I have an example dataframe like this and I'm not too worried about index just now: I'd like to get an output of a dictionary like: Not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order. "
            },
            "io": [
                "Col1 Col2 Col3 Col4\n--------------------\n a    b    c    a\n b    d    e    c\n",
                "{'Col1': 0, 'Col2': 1, 'Col3': 2, 'Col4': 3}\n"
            ],
            "answer": {
                "ans_desc": "That is straight forward with a comprehension as: Code: Test Code: Results: ",
                "code": [
                    "{c: i for i, c in enumerate(df.columns)}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 9577059,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/EiiAa.jpg?s=128&g=1",
            "display_name": "szybia",
            "link": "https://stackoverflow.com/users/9577059/szybia"
        },
        "is_answered": true,
        "view_count": 690,
        "accepted_answer_id": 54630098,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1549962124,
        "creation_date": 1549884762,
        "last_edit_date": 1549962124,
        "question_id": 54629663,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54629663/how-to-drop-a-percentage-of-rows-where-the-column-value-is-nan",
        "title": "How to drop a percentage of rows where the column value is NaN",
        "body": "<p>Let's say I have a <code>df</code> where a certain column has 50% missing values.</p>\n\n<p>How can I drop let's say 10% of the rows which are missing values in respect to the column?</p>\n\n<p>Basically how can I reduce the percentage of missing values of a column from 50% to 40%?</p>\n\n<p>Input (50% of values are missing (6/12)):</p>\n\n<pre><code>         0\n    0  1.0\n    1  1.0\n    2  NaN\n    3  NaN\n    4  NaN\n    5  1.0\n    6  NaN\n    7  1.0\n    8  NaN\n    9  1.0\n   10  NaN\n   11  1.0\n</code></pre>\n\n<p>Output (40% of values are missing (4/10)):\nWe dropped the last 2 NaN rows with ID's of 8 and 10</p>\n\n<pre><code>         0\n    0  1.0\n    1  1.0\n    2  NaN\n    3  NaN\n    4  NaN\n    5  1.0\n    6  NaN\n    7  1.0\n    9  1.0\n   11  1.0\n</code></pre>\n",
        "answer_body": "<p>To get the array with the indices with nan values in your column, use:</p>\n\n<pre><code>nan_indices = df.index[df['your_column'].isna()]\n</code></pre>\n\n<p>To drop, say, the first 20%, use:</p>\n\n<pre><code>df.drop(nan_indices[:int(len(nan_indices) * 0.2)])   #to create a new DataFrame, if you want to modify the original one, put inplace=True\n</code></pre>\n",
        "question_body": "<p>Let's say I have a <code>df</code> where a certain column has 50% missing values.</p>\n\n<p>How can I drop let's say 10% of the rows which are missing values in respect to the column?</p>\n\n<p>Basically how can I reduce the percentage of missing values of a column from 50% to 40%?</p>\n\n<p>Input (50% of values are missing (6/12)):</p>\n\n<pre><code>         0\n    0  1.0\n    1  1.0\n    2  NaN\n    3  NaN\n    4  NaN\n    5  1.0\n    6  NaN\n    7  1.0\n    8  NaN\n    9  1.0\n   10  NaN\n   11  1.0\n</code></pre>\n\n<p>Output (40% of values are missing (4/10)):\nWe dropped the last 2 NaN rows with ID's of 8 and 10</p>\n\n<pre><code>         0\n    0  1.0\n    1  1.0\n    2  NaN\n    3  NaN\n    4  NaN\n    5  1.0\n    6  NaN\n    7  1.0\n    9  1.0\n   11  1.0\n</code></pre>\n",
        "formatted_input": {
            "qid": 54629663,
            "link": "https://stackoverflow.com/questions/54629663/how-to-drop-a-percentage-of-rows-where-the-column-value-is-nan",
            "question": {
                "title": "How to drop a percentage of rows where the column value is NaN",
                "ques_desc": "Let's say I have a where a certain column has 50% missing values. How can I drop let's say 10% of the rows which are missing values in respect to the column? Basically how can I reduce the percentage of missing values of a column from 50% to 40%? Input (50% of values are missing (6/12)): Output (40% of values are missing (4/10)): We dropped the last 2 NaN rows with ID's of 8 and 10 "
            },
            "io": [
                "         0\n    0  1.0\n    1  1.0\n    2  NaN\n    3  NaN\n    4  NaN\n    5  1.0\n    6  NaN\n    7  1.0\n    8  NaN\n    9  1.0\n   10  NaN\n   11  1.0\n",
                "         0\n    0  1.0\n    1  1.0\n    2  NaN\n    3  NaN\n    4  NaN\n    5  1.0\n    6  NaN\n    7  1.0\n    9  1.0\n   11  1.0\n"
            ],
            "answer": {
                "ans_desc": "To get the array with the indices with nan values in your column, use: To drop, say, the first 20%, use: ",
                "code": [
                    "df.drop(nan_indices[:int(len(nan_indices) * 0.2)])   #to create a new DataFrame, if you want to modify the original one, put inplace=True\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 5677487,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/fdc9cafe993fa57aef6f267a0b4ea269?s=128&d=identicon&r=PG&f=1",
            "display_name": "araina",
            "link": "https://stackoverflow.com/users/5677487/araina"
        },
        "is_answered": true,
        "view_count": 978,
        "accepted_answer_id": 54640956,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1549930921,
        "creation_date": 1549928295,
        "last_edit_date": 1549929201,
        "question_id": 54640734,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54640734/faster-way-to-update-a-column-in-a-pandas-data-frame-based-on-the-value-of-anoth",
        "title": "Faster way to update a column in a pandas data frame based on the value of another column",
        "body": "<p>I have a pandas data frame with columns = [A, B, C, D, ...I, Z]. There are around ~80000 rows in the dataframe, and columns A, B, C, D, ..., I have value 0 for all these rows. Z has a value between [0, 9]. What I am trying to do is update the value of the x'th column for all rows in the data frame, where x is the current value of Z. If value of x is 0, then ignore. The data frame looks like - </p>\n\n<pre><code>    A    B    C    D  ...  Z\n0   0    0    0    0  ...  9\n1   0    0    0    0  ...  1\n2   0    0    0    0  ...  2\n3   0    0    0    0  ...  3    \n</code></pre>\n\n<p>This is what I have so far.</p>\n\n<pre><code>cols = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']  \nfor index, row in df.iterrows():\n            if row['Z'] != 9:\n                df.loc[index, cols[int(row['Z'])]] = 1\n</code></pre>\n\n<p>This is way too slow, and causes the script to stop executing midway. Is there a faster or better way to do it? I tried looking at np.where and np.apply, but I am not able to figure out the syntax. This is what I tried using np.apply -</p>\n\n<pre><code>df.iloc[what goes here?] = df['Z'].apply(lambda x: 1 if x != 9)\n</code></pre>\n\n<p>The desired output for the above sample is -</p>\n\n<pre><code>    A    B    C    D  ...  Z\n0   0    0    0    0  ...  9\n1   0    1    0    0  ...  1\n2   0    0    1    0  ...  2\n3   0    0    0    1  ...  3 \n</code></pre>\n",
        "answer_body": "<pre><code>import numpy as np\nimport pandas as pd\ncols = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'temp'])\ndf = pd.DataFrame(columns=cols[:-1])\ndf['Z'] = [9,1,2,3,1,5,4]\ndf = df.fillna(0)\ndf.update(pd.get_dummies(cols[df['Z']]))\nprint(df)\n</code></pre>\n\n<p>yields</p>\n\n<pre><code>   A  B  C  D  E  F  G  H  I  Z\n0  0  0  0  0  0  0  0  0  0  9\n1  0  1  0  0  0  0  0  0  0  1\n2  0  0  1  0  0  0  0  0  0  2\n3  0  0  0  1  0  0  0  0  0  3\n4  0  1  0  0  0  0  0  0  0  1\n5  0  0  0  0  0  1  0  0  0  5\n6  0  0  0  0  1  0  0  0  0  4\n</code></pre>\n\n<hr>\n\n<p>Pandas has a function, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\" rel=\"nofollow noreferrer\">pd.get_dummies</a>, which does exactly what you want:</p>\n\n<pre><code>In [274]: pd.get_dummies(['A','C','B','D'])\nOut[274]: \n   A  B  C  D\n0  1  0  0  0\n1  0  0  1  0\n2  0  1  0  0\n3  0  0  0  1\n</code></pre>\n\n<p>By making <code>cols</code> a NumPy array, you can use <a href=\"https://docs.scipy.org/doc/numpy-1.15.1/reference/arrays.indexing.html#integer-array-indexing\" rel=\"nofollow noreferrer\">NumPy integer array indexing</a> to generate\nthe desired column labels. (The purpose of the <code>'temp'</code> column is explained below):</p>\n\n<pre><code>In [276]: cols[df['Z']]\nOut[276]: array(['temp', 'B', 'C', 'D', 'B', 'F', 'E'], dtype='&lt;U3')\n</code></pre>\n\n<p>So that <code>get_dummies</code> generates this DataFrame:</p>\n\n<pre><code>In [277]: pd.get_dummies(cols[df['Z']])\nOut[277]: \n   B  C  D  E  F  temp\n0  0  0  0  0  0     1\n1  1  0  0  0  0     0\n2  0  1  0  0  0     0\n3  0  0  1  0  0     0\n4  1  0  0  0  0     0\n5  0  0  0  0  1     0\n6  0  0  0  1  0     0\n</code></pre>\n\n<p><code>df.update(other)</code> copies non-NaN values from the <code>other</code> DataFrame into <code>df</code>. Since <code>df</code> does not have a column labeled <code>temp</code>, the values in that column are ignored.</p>\n\n<hr>\n\n<p>Alternatively, construct <code>df</code> <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\">by concatenating</a> <code>df['Z']</code> with <code>pd.get_dummies(cols[df['Z']])</code>:</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\ncols = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'temp'])\ndf = pd.DataFrame({'Z':[9,1,2,3,1,5,4]})\n\ndf = pd.concat([pd.get_dummies(cols[df['Z']]), df['Z']], axis=1)\ndf = df.drop('temp', axis=1)\nprint(df)\n</code></pre>\n\n<p>yields</p>\n\n<pre><code>   B  C  D  E  F  Z\n0  0  0  0  0  0  9\n1  1  0  0  0  0  1\n2  0  1  0  0  0  2\n3  0  0  1  0  0  3\n4  1  0  0  0  0  1\n5  0  0  0  0  1  5\n6  0  0  0  1  0  4\n</code></pre>\n\n<p>Notice that some columns may be missing if there is no value in the <code>Z</code> column which corresponds to it.</p>\n",
        "question_body": "<p>I have a pandas data frame with columns = [A, B, C, D, ...I, Z]. There are around ~80000 rows in the dataframe, and columns A, B, C, D, ..., I have value 0 for all these rows. Z has a value between [0, 9]. What I am trying to do is update the value of the x'th column for all rows in the data frame, where x is the current value of Z. If value of x is 0, then ignore. The data frame looks like - </p>\n\n<pre><code>    A    B    C    D  ...  Z\n0   0    0    0    0  ...  9\n1   0    0    0    0  ...  1\n2   0    0    0    0  ...  2\n3   0    0    0    0  ...  3    \n</code></pre>\n\n<p>This is what I have so far.</p>\n\n<pre><code>cols = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']  \nfor index, row in df.iterrows():\n            if row['Z'] != 9:\n                df.loc[index, cols[int(row['Z'])]] = 1\n</code></pre>\n\n<p>This is way too slow, and causes the script to stop executing midway. Is there a faster or better way to do it? I tried looking at np.where and np.apply, but I am not able to figure out the syntax. This is what I tried using np.apply -</p>\n\n<pre><code>df.iloc[what goes here?] = df['Z'].apply(lambda x: 1 if x != 9)\n</code></pre>\n\n<p>The desired output for the above sample is -</p>\n\n<pre><code>    A    B    C    D  ...  Z\n0   0    0    0    0  ...  9\n1   0    1    0    0  ...  1\n2   0    0    1    0  ...  2\n3   0    0    0    1  ...  3 \n</code></pre>\n",
        "formatted_input": {
            "qid": 54640734,
            "link": "https://stackoverflow.com/questions/54640734/faster-way-to-update-a-column-in-a-pandas-data-frame-based-on-the-value-of-anoth",
            "question": {
                "title": "Faster way to update a column in a pandas data frame based on the value of another column",
                "ques_desc": "I have a pandas data frame with columns = [A, B, C, D, ...I, Z]. There are around ~80000 rows in the dataframe, and columns A, B, C, D, ..., I have value 0 for all these rows. Z has a value between [0, 9]. What I am trying to do is update the value of the x'th column for all rows in the data frame, where x is the current value of Z. If value of x is 0, then ignore. The data frame looks like - This is what I have so far. This is way too slow, and causes the script to stop executing midway. Is there a faster or better way to do it? I tried looking at np.where and np.apply, but I am not able to figure out the syntax. This is what I tried using np.apply - The desired output for the above sample is - "
            },
            "io": [
                "    A    B    C    D  ...  Z\n0   0    0    0    0  ...  9\n1   0    0    0    0  ...  1\n2   0    0    0    0  ...  2\n3   0    0    0    0  ...  3    \n",
                "    A    B    C    D  ...  Z\n0   0    0    0    0  ...  9\n1   0    1    0    0  ...  1\n2   0    0    1    0  ...  2\n3   0    0    0    1  ...  3 \n"
            ],
            "answer": {
                "ans_desc": " yields Pandas has a function, pd.get_dummies, which does exactly what you want: By making a NumPy array, you can use NumPy integer array indexing to generate the desired column labels. (The purpose of the column is explained below): So that generates this DataFrame: copies non-NaN values from the DataFrame into . Since does not have a column labeled , the values in that column are ignored. Alternatively, construct by concatenating with : yields Notice that some columns may be missing if there is no value in the column which corresponds to it. ",
                "code": [
                    "In [276]: cols[df['Z']]\nOut[276]: array(['temp', 'B', 'C', 'D', 'B', 'F', 'E'], dtype='<U3')\n",
                    "import numpy as np\nimport pandas as pd\ncols = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'temp'])\ndf = pd.DataFrame({'Z':[9,1,2,3,1,5,4]})\n\ndf = pd.concat([pd.get_dummies(cols[df['Z']]), df['Z']], axis=1)\ndf = df.drop('temp', axis=1)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "join",
            "mapping"
        ],
        "owner": {
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 60,
        "accepted_answer_id": 54607134,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1549724421,
        "creation_date": 1549722030,
        "last_edit_date": 1549722850,
        "question_id": 54607098,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54607098/how-to-create-a-join-in-dataframe-based-on-the-other-dataframe",
        "title": "How to create a join in Dataframe based on the other dataframe?",
        "body": "<p>I have 2 dataframes. One containing student batch details and another one with points. I want to join 2 dataframes.</p>\n\n<p>Dataframe1 contains</p>\n\n<pre><code>+-------+-------+-------+--+\n|  s1   |  s2   |  s3   |  |\n+-------+-------+-------+--+\n| Stud1 | Stud2 | Stud3 |  |\n| Stud2 | Stud4 | Stud1 |  |\n| Stud1 | Stud3 | Stud4 |  |\n+-------+-------+-------+--+\n</code></pre>\n\n<p>Dataframe2 contains</p>\n\n<pre><code>+-------+-------+----------+--+\n| Name  | Point | Category |  |\n+-------+-------+----------+--+\n| Stud1 |    90 | Good     |  |\n| Stud2 |    80 | Average  |  |\n| Stud3 |    95 | Good     |  |\n| Stud4 |    55 | Poor     |  |\n+-------+-------+----------+\n</code></pre>\n\n<p>I am trying to map the mark in the same dataset for each student.</p>\n\n<pre><code>+-------+-------+-------+----+----+----+\n| Stud1 | Stud2 | Stud3 | 90 | 80 | 95 |\n| Stud2 | Stud4 | Stud1 | 80 | 55 | 90 |\n| Stud1 | Stud3 | Stud4 | 90 | 95 | 55 |\n+-------+-------+-------+----+----+----+\n</code></pre>\n\n<p>I tried below code but it is replacing the values one by one.</p>\n\n<pre><code>s = df3['p1'].map(dfnamepoints.set_index('name')['points'])\ndf4 = df3.drop('p1', 1).assign(points = s)\n</code></pre>\n",
        "answer_body": "<p>Solution working same if all values from <code>df3</code> exist in column <code>Name</code>:</p>\n\n<pre><code>s = dfnamepoints.set_index('Name')['Point']\ndf = df3.join(df3.replace(s).add_prefix('new_'))\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df = df3.join(df3.apply(lambda x: x.map(s)).add_prefix('new_'))\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df = df3.join(df3.applymap(s.get).add_prefix('new_'))\n\nprint (df)\n      s1     s2     s3  new_s1  new_s2  new_s3\n0  Stud1  Stud2  Stud3      90      80      95\n1  Stud2  Stud4  Stud1      80      55      90\n2  Stud1  Stud3  Stud4      90      95      55\n</code></pre>\n\n<p>If not, output is different - for not exist values (<code>Stud1</code>) get <code>NaN</code>s:</p>\n\n<pre><code>print (dfnamepoints)\n    Name  Point Category\n0  Stud2     80  Average\n1  Stud3     95     Good\n2  Stud4     55     Poor\n\ndf = df3.join(df3.applymap(s.get).add_prefix('new_'))\n#or \ndf = df3.join(df3.applymap(s.get).add_prefix('new_'))\n\nprint (df)\n      s1     s2     s3  new_s1  new_s2  new_s3\n0  Stud1  Stud2  Stud3     NaN      80    95.0\n1  Stud2  Stud4  Stud1    80.0      55     NaN\n2  Stud1  Stud3  Stud4     NaN      95    55.0\n</code></pre>\n\n<p>And for <code>replace</code> get original value:</p>\n\n<pre><code>df = df3.join(df3.replace(s).add_prefix('new_'))\nprint (df)\n      s1     s2     s3 new_s1  new_s2 new_s3\n0  Stud1  Stud2  Stud3  Stud1      80     95\n1  Stud2  Stud4  Stud1     80      55  Stud1\n2  Stud1  Stud3  Stud4  Stud1      95     55\n</code></pre>\n",
        "question_body": "<p>I have 2 dataframes. One containing student batch details and another one with points. I want to join 2 dataframes.</p>\n\n<p>Dataframe1 contains</p>\n\n<pre><code>+-------+-------+-------+--+\n|  s1   |  s2   |  s3   |  |\n+-------+-------+-------+--+\n| Stud1 | Stud2 | Stud3 |  |\n| Stud2 | Stud4 | Stud1 |  |\n| Stud1 | Stud3 | Stud4 |  |\n+-------+-------+-------+--+\n</code></pre>\n\n<p>Dataframe2 contains</p>\n\n<pre><code>+-------+-------+----------+--+\n| Name  | Point | Category |  |\n+-------+-------+----------+--+\n| Stud1 |    90 | Good     |  |\n| Stud2 |    80 | Average  |  |\n| Stud3 |    95 | Good     |  |\n| Stud4 |    55 | Poor     |  |\n+-------+-------+----------+\n</code></pre>\n\n<p>I am trying to map the mark in the same dataset for each student.</p>\n\n<pre><code>+-------+-------+-------+----+----+----+\n| Stud1 | Stud2 | Stud3 | 90 | 80 | 95 |\n| Stud2 | Stud4 | Stud1 | 80 | 55 | 90 |\n| Stud1 | Stud3 | Stud4 | 90 | 95 | 55 |\n+-------+-------+-------+----+----+----+\n</code></pre>\n\n<p>I tried below code but it is replacing the values one by one.</p>\n\n<pre><code>s = df3['p1'].map(dfnamepoints.set_index('name')['points'])\ndf4 = df3.drop('p1', 1).assign(points = s)\n</code></pre>\n",
        "formatted_input": {
            "qid": 54607098,
            "link": "https://stackoverflow.com/questions/54607098/how-to-create-a-join-in-dataframe-based-on-the-other-dataframe",
            "question": {
                "title": "How to create a join in Dataframe based on the other dataframe?",
                "ques_desc": "I have 2 dataframes. One containing student batch details and another one with points. I want to join 2 dataframes. Dataframe1 contains Dataframe2 contains I am trying to map the mark in the same dataset for each student. I tried below code but it is replacing the values one by one. "
            },
            "io": [
                "+-------+-------+-------+--+\n|  s1   |  s2   |  s3   |  |\n+-------+-------+-------+--+\n| Stud1 | Stud2 | Stud3 |  |\n| Stud2 | Stud4 | Stud1 |  |\n| Stud1 | Stud3 | Stud4 |  |\n+-------+-------+-------+--+\n",
                "+-------+-------+-------+----+----+----+\n| Stud1 | Stud2 | Stud3 | 90 | 80 | 95 |\n| Stud2 | Stud4 | Stud1 | 80 | 55 | 90 |\n| Stud1 | Stud3 | Stud4 | 90 | 95 | 55 |\n+-------+-------+-------+----+----+----+\n"
            ],
            "answer": {
                "ans_desc": "Solution working same if all values from exist in column : Or: Or: If not, output is different - for not exist values () get s: And for get original value: ",
                "code": [
                    "s = dfnamepoints.set_index('Name')['Point']\ndf = df3.join(df3.replace(s).add_prefix('new_'))\n",
                    "df = df3.join(df3.apply(lambda x: x.map(s)).add_prefix('new_'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 7,
            "user_id": 10994840,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1cf2dde0013f7eaf0cb74173e71eeaed?s=128&d=identicon&r=PG&f=1",
            "display_name": "Praveen Kumar",
            "link": "https://stackoverflow.com/users/10994840/praveen-kumar"
        },
        "is_answered": true,
        "view_count": 56,
        "accepted_answer_id": 54578880,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1549561713,
        "creation_date": 1549556412,
        "last_edit_date": 1549560389,
        "question_id": 54577752,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54577752/pandas-dataframe-from-dictionary",
        "title": "Pandas DataFrame from Dictionary",
        "body": "<p>Let's assume that I have a JSON file like below, and I want to convert this file into a data frame with 2 columns:</p>\n\n<pre><code>{\"1087\": [4,5,6,7,8,9,10,12,13,21,22,23,24,25,26,27,28,34,35 ,37,39,40,42,44,45,46,47,48,51,52,54,55,56,59,60,61,63,64,65,66,67,68,72,73,74,75,78,80,81,82,83,84,85,87,88,92,94,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,149,180,181,196,198,200,202,206,222,223,226,227,230,231,232,233,234,235,242,255,257,258,259,261,263,264,265,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,302,303,304,305,306,307,308,309,310,311,313,314,316,318,319,320,323,325,326,327,328,330,334,336,337,339,340,342,343,350,351,354,355,362,363,365,366,367,368,369,370,371,372,374,375,376,377,378,379,380,383,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,427,428,429,430,431,432,433,434,435,437,438,444,446,449,451,455,457,461,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,494,496,498,499,500,502,504,506,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,559,560,561,565,567,569,571,573,574,576,579,580,581,583,586,587,588,590,593,594,597,598,600,601,602,604,605,606,607,608,609,611,612,613,614,615,616,617,620,621,622,624,625,626,629,631,633,634,636,638,639,640,641,643,644,647,649,650,651,652,653,654,657,658,664,665,666,667,669,671,674,675,676,677,678,682,683,685,686,687,688,692,694,695,702,703,705,708,712,713,714,715,716,717,718,720,728,732,734,735,739,740,742,743,745,746,751,752,759,769,770,772,778,779,780,783,784,786,792,805,815,823,831,832,834,835,836,837,838,839,852,854,855,856,867,875,877,879,888,890,891,896,900,908,909,910,911,912,913,914,915,916,917,918,919,934,935,936,937,938,939,944,945,946,950,951,952,953,957,958,959,960,964,965,966,967,971,975,977,978,980,981,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1027,1028,1033,1034,1035,1036,1037,1038,1039,1049,1061,1063,1065,1067,1069,1070,1071,1072,1073,1074,1076,1077,1078,1080,1081,1084,1088,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1127,1128,1129,1130,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1151,1155,1156,1201,1202,1203,1204,1207,1208,1209,1213,1214,1215,1216,1217,1220,1221,1222,1223,1224,1232,1233,1235,1237,1238,1241,1243,1244,1245,1248,1249,1251,1254,1269,1271,1273,1274,1275,1284,1289,1298,1301,1302,1303,1470,1495,1500,1501,1508,1509,1517,1518,1572,1573,1574,1575,1614,1619,1620,1625,1633,1639,1661,1669,1670,1671,1692,1693,1694,1695,1696,1698,1699,1700,1701,1706,1707,1708,1709,1711,1712,1713,1715,1720,1726,1728,1729,1730,1731,1732,1734,1755,1771,1780,1781,1785,1788,1794,1795,1797,1801,1802,1803,1805,1827,1829,1830,1836,1838,1843,1845,1847,1849,1851,1852,1853,1854,1855,1897,1899,1901,1920,1922,1923,1974,1987,1988,1989,1990,1991,1993,1994,2013,2014,2038,2039,2040,2044,2057,2086,2108,2144,2150,2215,2216,2218,2219,2220,2227,2228,2229,2230,2250,2258,2271,2279,2283,2285,2286,2287,2295,2302,2327,2390,2397,2406,2407,2411,2413,2414,2415,2419,2421,2429,2441,2471,2472,2490,2493,2507,2514,2519,2524,2525,2531,2532,2535,2538,2541,2551,2552,2553,2555,2560,2561,2562,2564,2570,2577,2578,2579,2580,2581,2586,2587,2588,2589,2591,2592,2594,2595,2596,2597,2599,2600,2601,2602,2603,2604,2605,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2625,2626,2627,2628,2629,2630,2631,2634,2635,2665,2668,2669,2671,2673,2681,2682,2683,2684,2705,2706,2707,2708,2709,2710,2711,2712,2713,2750,2766,2768,2769,2770,2798,2804,2817,2821,2822,2823,2824,2825,2826,2844,2847,2853,2855,2858,2860,2861,2862,2863,2864,2865,2880,2900,2901,2902,2903,2906,2911,2912,2913,2916,2918,2922,2925,2926,2932,2935,2941,2943,2945,2947,2948,2949,2950,2958,2959,2966,2967,2972,2976,2977,2978,2979,2980,2981,2982,2987,2988,2991,2992,2993,2994,2995,2996,2999,3001,3003,3007,3008,3011,3012,3015,3016,3017,3018,3024,3030,3031,3033,3034,3045,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3093,3099,3105,3112,3113,3114,3115,3116,3117,3127,3128,3154,3155,3156,3157,3297,3299,3300,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3336,3339,3416,3417,3420,3424,3550,3587,3588,3589,3590,3591,3592,3593,3598,3599,3600,3602,3603,3604,3605,3606,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3625,3655,3656,3657,3718,3721,3724,3725,3726,3730,3732,3733,3736,3738,3741,3743,3744,3747,3748,3750,3752,3754,3756,3762,3763,3764,3765,3766,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3795,3797,3798,3800,3806,3811,3864,3866,3867,3871,3881,3883,3884,3885,3886,3925,3926,3929,3930,3935,3936,3940,4018,4030,4045,4049,4050,4051,4054,4058,4059,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4104,4109,4110,4113,4116,4117,4118,4119,4161,4267,4285,4310,4317,4335,4358,4359,4365,4366,4467,4471,4475,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4638,4639,4764,4765,4766,4782,4803,4824,4827,4828,4830,4888,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4926,4990,6998,7026,7027,7028],\"1096\": [25,26,27,28,45,46,63,64,65,66,67,80,81,82,83,84,85,128,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,330,366,367,376,385,386,387,388,391,417,418,419,420,437,449,451,553,555,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,641,643,644,779,780,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1273,1274,1275,1278,1280,1289,1292,1670,1671,1713,1730,1731,1847,1849,1993,2086,2218,2219,2220,2258,2421,2586,2587,2608,2610,2611,2629,2631,2673,2708,2709,2710,2711,2712,2713,2821,2822,2823,2825,2844,2847,2858,2860,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2959,2976,2977,2978,2979,2980,2981,2982,2991,2992,2993,2994,2995,3001,3003,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3112,3113,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3416,3417,3589,3590,3591,3592,3593,3598,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3656,3657,3732,3738,3743,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3797,3798,3871,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4267,4358,4359,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4764,4765,4766,7026,7027,7028],\"1144\": [25,26,27,28,144,372,374,422,768,1005,1051,1052,1053,1054,1057,1058,1060,1062,1064,1066,1068,1098,1101,1146,1703,1704,1705,1713,1716,1994,2086,2382,3095,3096,3097,3114,3115,3116,3339,3619,3620,3621,3732,3738,3743,3881,3883,3884,3885,3886,4113,4116,4117,4118,4119,4267,4285,4365,4370,4371,4372,4373,4374,4375,4471,4764,4765,4766,4803,4824,4828,4830,4990],\"-1\": [40,63,64,65,66,67,68,80,81,82,83,84,85,87,130,131,132,133,134,135,136,137,138,139,140,141,234,261,263,264,267,268,269,271,272,293,308,314,318,319,337,366,367,375,376,385,386,387,388,391,407,416,417,418,419,420,435,461,489,559,561,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,623,624,625,626,632,634,636,644,666,682,683,685,686,687,688,694,695,696,720,737,854,855,870,882,883,888,896,916,917,918,919,930,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,971,978,982,986,987,988,993,994,995,996,1000,1001,1002,1036,1037,1038,1039,1081,1132,1133,1134,1135,1136,1210,1317,1321,1341,1347,1377,1378,1380,1383,1384,1386,1396,1398,1408,1410,1432,1456,1458,1473,1500,1501,1525,1614,1670,1730,1808,1838,1982,1983,1984,1985,1993,2033,2034,2038,2039,2040,2069,2150,2151,2258,2355,2356,2571,2596,2692,2729,2737,2821,2822,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2976,2977,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3150,3151,3316,3416,3417,3594,3744,3769,3770,3773,3776,3785,3786,3787,3871,4064,4066,4068,4070,4072,4074,4076,4078,4080,4082,4084,4086,4088,4092,4094,4096,4098,4109,4288,4321,4330,4466,4991,5567,6913],\"2060\": [47,65,67,80,81,148,155,156,166,167,168,226,227,267,268,269,580,594,597,600,601,602,604,607,608,609,611,614,634,636,682,683,685,686,687,688,694,695,696,728,733,738,744,944,945,946,993,994,995,1317,1321,1341,1347,1377,1378,1380,1383,1384,1385,1386,1387,1396,1398,1408,1410,1432,1456,1458,1473,1525,1696,1736,1737,1738,1739,1754,1808,1859,1865,1873,1879,1885,1892,1922,1982,1983,1984,1985,1993,1994,2038,2039,2040,2150,2151,2254,2300,2355,2356,2377,2391,2448,2478,2530,2564,2723,2742,2745,2746,2747,2882,2922,2925,2947,2948,2949,2950,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3341,3383,3482,3493,3494,3506,3530,3672,3675,3821,4332,4439,4440,4459,4908,4913,4914,4915,4916,4917,4919,4920,4921,4922,4923,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5177,5178,5179,5180,5181,5182,5183,5184,5188,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5217,5218,5219,5220,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5486,5487,5488,5489,5490,5491,5492,5493,5494,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5559,5566,5567,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,7037],\"1742\": [47,60,61,63,64,65,66,67,80,81,82,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,226,227,232,233,242,267,268,269,271,314,319,323,366,367,376,385,388,417,418,419,420,554,559,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,633,634,636,643,700,701,702,703,705,717,728,745,746,834,835,836,837,838,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1088,1119,1120,1121,1122,1123,1207,1208,1209,1213,1214,1215,1216,1269,1386,1508,1509,1692,1693,1694,1695,1698,1699,1700,1701,1720,1726,1727,1729,1780,1781,1830,1851,1920,1993,2127,2216,2258,2295,2390,2564,2621,2821,2823,2844,2847,2862,2863,2864,2865,2911,2912,2913,2916,2922,2925,2935,2943,2945,2947,2948,2949,2950,2978,2979,2980,2981,2982,2987,2988,2996,3007,3008,3011,3012,3015,3016,3017,3018,3072,3099,3112,3113,3154,3155,3156,3157,3311,3312,3315,3318,3353,3355,3420,3422,3423,3590,3591,3592,3625,3732,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3794,3801,3871,3940,4049,4050,4058,4063,4169,4170,4476,4477,4482,4616,4906,6864,6865],\"1125\": [47,53,65,67,80,81,172,174,187,190,196,198,200,202,246,267,268,269,309,313,316,319,320,323,324,325,326,370,372,374,421,448,594,597,600,634,636,657,658,673,679,692,708,735,860,944,945,946,993,994,995,1061,1063,1065,1067,1220,1222,1223,1277,1502,1517,1518,1572,1573,1574,1575,1621,1622,1623,1632,1635,1637,1661,1696,1849,1897,1899,1901,1968,1993,2032,2033,2034,2069,2283,2421,2422,2423,2445,2471,2472,2490,2493,2527,2529,2609,2623,2627,2669,2671,2729,2738,2739,2804,2825,2826,2853,2854,2855,2856,2894,2895,2901,2902,2903,2904,2918,2926,2932,3024,3107,3114,3115,3116,3299,3353,3354,3355,3356,3364,3365,3373,3390,3391,3392,3393,3415,3422,3423,3425,3541,3550,3715,3719,3720,3721,3792,3793,3794,3795,3800,3835,3836,3844,3845,3846,3847,3864,3866,3867,3920,3925,3926,3929,3930,3935,3936,4030,4059,4090,4111,4112,4138,4143,4145,4161,4162,4165,4306,4311,4351,4361,4368,4397,4457,4467,4471,4480,4638,4639,4754,4764,4765,4766,4770,4803,4806,4807,4808,4824,4830,4888,4904,4905,4911,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,6892,6893],\"1095\": [64,65,66,67,80,81,187,190,196,198,200,202,219,267,268,269,319,320,324,385,388,559,573,576,580,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,644,679,689,690,691,751,756,842,843,844,845,846,847,848,937,938,939,944,945,946,951,952,953,958,959,960,964,965,966,967,986,987,988,993,994,995,1000,1001,1002,1993,2098,2250,2258,2354,2421,2495,2496,2821,2823,2839,2844,2847,2854,2856,2862,2863,2864,2865,2922,2925,2947,2948,2949,2950,2978,2980,3007,3011,3015,3016,3017,3018,3113,3299,3306,3310,3315,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3368,3422,3423,3590,3591,3592,3748,3750,3752,3754,3756,3762,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3792,3793,3794,3795,3801,3871],\"1145\": [64,65,66,67,80,81,82,84,85,125,129,130,131,132,133,134,135,140,141,263,264,267,268,269,327,334,351,367,388,446,553,594,597,600,601,602,604,607,608,609,611,614,620,622,629,631,634,643,692,735,786,867,896,937,938,939,944,945,946,950,951,952,953,958,959,960,975,977,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1077,1088,1119,1120,1121,1122,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1204,1224,1232,1233,1244,1245,1275,1517,1518,1572,1573,1574,1575,1661,1669,1670,1671,1709,1711,1729,1730,1731,1843,1845,1897,1899,1901,1993,1994,2013,2057,2218,2219,2220,2227,2228,2229,2230,2250,2258,2283,2327,2471,2472,2490,2493,2560,2562,2587,2596,2608,2609,2610,2611,2630,2631,2673,2682,2683,2684,2705,2706,2708,2709,2710,2711,2712,2713,2817,2821,2825,2844,2862,2863,2880,2900,2901,2902,2916,2918,2922,2925,2926,2941,2947,2948,2949,2950,2977,2978,2982,2991,2992,2993,3007,3011,3015,3016,3017,3018,3033,3034,3072,3297,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3417,3588,3721,3744,3864,3866,3867,4030,4034,4059,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4161,4358,4359,4366,4638,4639],\"1966\": [64,65,66,67,80,81,82,83,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,366,367,376,385,386,387,388,391,417,418,419,420,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,616,624,625,626,634,636,639,640,641,644,778,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1075,1114,1115,1116,1117,1118,1728,1993,2258,2596,2673,2821,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3112,3113,3590,3591,3592,3744,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3800,3871,4062,4764,4765,4766],\"2148\": [159,220,696,1050,1277,1278,1280,1696,1994,2032,2033,2034,2151,2319,2429,2432,2433,2434,2435,2436,2437,2441,2445,3299,3310,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3622,4913,6945,6946,6947,6948,6949,6950,6951,6952,6953,6990,6991],\"2387\": [159,1050,1994,2708,2709,2710,2711,2712,2713,2976,2977,3416,3417,3622,4358,4359],\"865\": [216,217,851,860,2053,2054,2055,2056,2131,2132,2422,2423],\"2442\": [220,1277,1621,1622,1623,1632,1635,1637,1859,1865,1873,1879,1885,1892,1994,2032,2033,2034,2432,2433,2434,2435,2436,2437,2445,2780,2789,3299],\"2370\": [321,692,1245,1517,1518,1572,1573,1574,1575,1661,1897,1899,1901,2068,2094,2095,2096,2106,2109,2263,2264,2270,2283,2284,2303,2327,2366,2367,2390,2428,2490,2493,2719,2722,2726,2735,2736,2738,2739,2740,2741,2765,2827,2894,2895,2901,2902,2903,2926,3019,3024,3031,3077,3078,3079,3081,3083,3084,3085,3349,3590,3591,3592,3605,3606,3715,3716,3853,3854,3855,3856,3857,3861,4112,4120,4284,4306,4398,4620,4621,6902,6903],\"1950\": [684,816,1285,1286,1656,1657,2405,2512,2527,3651],\"3852\": [779,780,1213,1214,1289,1847,1849,3339,3732,3738,3743,3790,3797,3800,4054,4113,4765,4766],\"2381\": [781,782,810,2023,2024,2632,2633,4365],\"1108\": [920,921],\"1105\": [1276,1502,1994,2032,2269,2319,2342,2343,2344,2348,2349,2350,2420,2421,2429,2441,3310,3321,3322,3323,3326,3327,3328],\"2725\": [2723,4161],\"2727\": [2729],\"2728\": [2730],\"2820\": [2858,2860]}\n</code></pre>\n\n<p>This is what I already tried, but I am unable to create DF and probably there is a more efficient way of doing this task:</p>\n\n<pre><code>import pandas as pd\nimport json\nimport os\n\npath=\"C:\\\\Users\\Z003Z9CF\\Documents\"\nos.chdir(path)\nwith open('newdependency.json') as data_file:    \n    data = json.load(data_file)\ns1 = pd.Series(data,name='Child')\ndf = pd.concat([s1], axis=1)\ndf.index.name = 'Parent'\ndf\n</code></pre>\n\n<p>Expecting output like this for all elements in a Json file</p>\n\n<pre><code>Parent   Child\n1087       4\n1087       5\n1087       6\n.....           ......\n1096       25\n1096       26\n1096       27\n1096       28\n......        ......\n1144      25\n1144      26\n1144      27\n.....         .....\n</code></pre>\n\n<p>Also I want to create a directed graph for each parent node with child nodes as clusters because parent nodes has the same child nodes which are also present in other parent nodes.\nThanks in Advance</p>\n",
        "answer_body": "<p>Pandas will not always have a way to magically parse a data structure directly (whether you are using just the <code>DataFrame</code> constructor or one of its classmethods such as <code>from_dict()</code>).</p>\n\n<p>Here, you can just pass a modified version of the native Python structure directly:</p>\n\n<pre><code>In [10]: data = { \n   ...:     '1108': [920, 921], \n   ...:     '2381': [781, 782, 810, 2023, 2024, 2632, 2633, 4365], \n   ...:     \"2728\": [2730] \n   ...: } \n\nIn [11]: df = pd.DataFrame([[k, i] for k, v in data.items() for i in v], \n    ...:                   columns=['parent', 'child'])                                                                                                                                                                                                               \n\nIn [12]: df['parent'] = df['parent'].astype(int)                                                                                                                                                                                                                       \n\nIn [13]: df                                                                                                                                                                                                                                                  \nOut[13]: \n   parent  child\n0    1108    920\n1    1108    921\n2    2381    781\n3    2381    782\n4    2381    810\n5    2381   2023\n6    2381   2024\n7    2381   2632\n8    2381   2633\n9    2381   4365\n10   2728   2730\n</code></pre>\n\n<p>The expression <code>[[k, i] for k, v in data.items() for i in v]</code> combines a dict comprehension and a list comprehension.  It looks like:</p>\n\n<pre><code>In [14]: [[k, i] for k, v in data.items() for i in v]                                                                                                                                                                                                        \nOut[14]:\n[['1108', 920],\n ['1108', 921],\n ['2381', 781],\n ['2381', 782],\n ['2381', 810],\n ['2381', 2023],\n ['2381', 2024],\n ['2381', 2632],\n ['2381', 2633],\n ['2381', 4365],\n ['2728', 2730]]\n</code></pre>\n",
        "question_body": "<p>Let's assume that I have a JSON file like below, and I want to convert this file into a data frame with 2 columns:</p>\n\n<pre><code>{\"1087\": [4,5,6,7,8,9,10,12,13,21,22,23,24,25,26,27,28,34,35 ,37,39,40,42,44,45,46,47,48,51,52,54,55,56,59,60,61,63,64,65,66,67,68,72,73,74,75,78,80,81,82,83,84,85,87,88,92,94,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,149,180,181,196,198,200,202,206,222,223,226,227,230,231,232,233,234,235,242,255,257,258,259,261,263,264,265,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,302,303,304,305,306,307,308,309,310,311,313,314,316,318,319,320,323,325,326,327,328,330,334,336,337,339,340,342,343,350,351,354,355,362,363,365,366,367,368,369,370,371,372,374,375,376,377,378,379,380,383,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,427,428,429,430,431,432,433,434,435,437,438,444,446,449,451,455,457,461,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,494,496,498,499,500,502,504,506,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,559,560,561,565,567,569,571,573,574,576,579,580,581,583,586,587,588,590,593,594,597,598,600,601,602,604,605,606,607,608,609,611,612,613,614,615,616,617,620,621,622,624,625,626,629,631,633,634,636,638,639,640,641,643,644,647,649,650,651,652,653,654,657,658,664,665,666,667,669,671,674,675,676,677,678,682,683,685,686,687,688,692,694,695,702,703,705,708,712,713,714,715,716,717,718,720,728,732,734,735,739,740,742,743,745,746,751,752,759,769,770,772,778,779,780,783,784,786,792,805,815,823,831,832,834,835,836,837,838,839,852,854,855,856,867,875,877,879,888,890,891,896,900,908,909,910,911,912,913,914,915,916,917,918,919,934,935,936,937,938,939,944,945,946,950,951,952,953,957,958,959,960,964,965,966,967,971,975,977,978,980,981,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1027,1028,1033,1034,1035,1036,1037,1038,1039,1049,1061,1063,1065,1067,1069,1070,1071,1072,1073,1074,1076,1077,1078,1080,1081,1084,1088,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1127,1128,1129,1130,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1151,1155,1156,1201,1202,1203,1204,1207,1208,1209,1213,1214,1215,1216,1217,1220,1221,1222,1223,1224,1232,1233,1235,1237,1238,1241,1243,1244,1245,1248,1249,1251,1254,1269,1271,1273,1274,1275,1284,1289,1298,1301,1302,1303,1470,1495,1500,1501,1508,1509,1517,1518,1572,1573,1574,1575,1614,1619,1620,1625,1633,1639,1661,1669,1670,1671,1692,1693,1694,1695,1696,1698,1699,1700,1701,1706,1707,1708,1709,1711,1712,1713,1715,1720,1726,1728,1729,1730,1731,1732,1734,1755,1771,1780,1781,1785,1788,1794,1795,1797,1801,1802,1803,1805,1827,1829,1830,1836,1838,1843,1845,1847,1849,1851,1852,1853,1854,1855,1897,1899,1901,1920,1922,1923,1974,1987,1988,1989,1990,1991,1993,1994,2013,2014,2038,2039,2040,2044,2057,2086,2108,2144,2150,2215,2216,2218,2219,2220,2227,2228,2229,2230,2250,2258,2271,2279,2283,2285,2286,2287,2295,2302,2327,2390,2397,2406,2407,2411,2413,2414,2415,2419,2421,2429,2441,2471,2472,2490,2493,2507,2514,2519,2524,2525,2531,2532,2535,2538,2541,2551,2552,2553,2555,2560,2561,2562,2564,2570,2577,2578,2579,2580,2581,2586,2587,2588,2589,2591,2592,2594,2595,2596,2597,2599,2600,2601,2602,2603,2604,2605,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2625,2626,2627,2628,2629,2630,2631,2634,2635,2665,2668,2669,2671,2673,2681,2682,2683,2684,2705,2706,2707,2708,2709,2710,2711,2712,2713,2750,2766,2768,2769,2770,2798,2804,2817,2821,2822,2823,2824,2825,2826,2844,2847,2853,2855,2858,2860,2861,2862,2863,2864,2865,2880,2900,2901,2902,2903,2906,2911,2912,2913,2916,2918,2922,2925,2926,2932,2935,2941,2943,2945,2947,2948,2949,2950,2958,2959,2966,2967,2972,2976,2977,2978,2979,2980,2981,2982,2987,2988,2991,2992,2993,2994,2995,2996,2999,3001,3003,3007,3008,3011,3012,3015,3016,3017,3018,3024,3030,3031,3033,3034,3045,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3093,3099,3105,3112,3113,3114,3115,3116,3117,3127,3128,3154,3155,3156,3157,3297,3299,3300,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3336,3339,3416,3417,3420,3424,3550,3587,3588,3589,3590,3591,3592,3593,3598,3599,3600,3602,3603,3604,3605,3606,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3625,3655,3656,3657,3718,3721,3724,3725,3726,3730,3732,3733,3736,3738,3741,3743,3744,3747,3748,3750,3752,3754,3756,3762,3763,3764,3765,3766,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3795,3797,3798,3800,3806,3811,3864,3866,3867,3871,3881,3883,3884,3885,3886,3925,3926,3929,3930,3935,3936,3940,4018,4030,4045,4049,4050,4051,4054,4058,4059,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4104,4109,4110,4113,4116,4117,4118,4119,4161,4267,4285,4310,4317,4335,4358,4359,4365,4366,4467,4471,4475,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4638,4639,4764,4765,4766,4782,4803,4824,4827,4828,4830,4888,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4926,4990,6998,7026,7027,7028],\"1096\": [25,26,27,28,45,46,63,64,65,66,67,80,81,82,83,84,85,128,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,330,366,367,376,385,386,387,388,391,417,418,419,420,437,449,451,553,555,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,641,643,644,779,780,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1273,1274,1275,1278,1280,1289,1292,1670,1671,1713,1730,1731,1847,1849,1993,2086,2218,2219,2220,2258,2421,2586,2587,2608,2610,2611,2629,2631,2673,2708,2709,2710,2711,2712,2713,2821,2822,2823,2825,2844,2847,2858,2860,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2959,2976,2977,2978,2979,2980,2981,2982,2991,2992,2993,2994,2995,3001,3003,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3112,3113,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3416,3417,3589,3590,3591,3592,3593,3598,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3656,3657,3732,3738,3743,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3797,3798,3871,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4267,4358,4359,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4764,4765,4766,7026,7027,7028],\"1144\": [25,26,27,28,144,372,374,422,768,1005,1051,1052,1053,1054,1057,1058,1060,1062,1064,1066,1068,1098,1101,1146,1703,1704,1705,1713,1716,1994,2086,2382,3095,3096,3097,3114,3115,3116,3339,3619,3620,3621,3732,3738,3743,3881,3883,3884,3885,3886,4113,4116,4117,4118,4119,4267,4285,4365,4370,4371,4372,4373,4374,4375,4471,4764,4765,4766,4803,4824,4828,4830,4990],\"-1\": [40,63,64,65,66,67,68,80,81,82,83,84,85,87,130,131,132,133,134,135,136,137,138,139,140,141,234,261,263,264,267,268,269,271,272,293,308,314,318,319,337,366,367,375,376,385,386,387,388,391,407,416,417,418,419,420,435,461,489,559,561,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,623,624,625,626,632,634,636,644,666,682,683,685,686,687,688,694,695,696,720,737,854,855,870,882,883,888,896,916,917,918,919,930,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,971,978,982,986,987,988,993,994,995,996,1000,1001,1002,1036,1037,1038,1039,1081,1132,1133,1134,1135,1136,1210,1317,1321,1341,1347,1377,1378,1380,1383,1384,1386,1396,1398,1408,1410,1432,1456,1458,1473,1500,1501,1525,1614,1670,1730,1808,1838,1982,1983,1984,1985,1993,2033,2034,2038,2039,2040,2069,2150,2151,2258,2355,2356,2571,2596,2692,2729,2737,2821,2822,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2976,2977,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3150,3151,3316,3416,3417,3594,3744,3769,3770,3773,3776,3785,3786,3787,3871,4064,4066,4068,4070,4072,4074,4076,4078,4080,4082,4084,4086,4088,4092,4094,4096,4098,4109,4288,4321,4330,4466,4991,5567,6913],\"2060\": [47,65,67,80,81,148,155,156,166,167,168,226,227,267,268,269,580,594,597,600,601,602,604,607,608,609,611,614,634,636,682,683,685,686,687,688,694,695,696,728,733,738,744,944,945,946,993,994,995,1317,1321,1341,1347,1377,1378,1380,1383,1384,1385,1386,1387,1396,1398,1408,1410,1432,1456,1458,1473,1525,1696,1736,1737,1738,1739,1754,1808,1859,1865,1873,1879,1885,1892,1922,1982,1983,1984,1985,1993,1994,2038,2039,2040,2150,2151,2254,2300,2355,2356,2377,2391,2448,2478,2530,2564,2723,2742,2745,2746,2747,2882,2922,2925,2947,2948,2949,2950,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3341,3383,3482,3493,3494,3506,3530,3672,3675,3821,4332,4439,4440,4459,4908,4913,4914,4915,4916,4917,4919,4920,4921,4922,4923,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5177,5178,5179,5180,5181,5182,5183,5184,5188,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5217,5218,5219,5220,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5486,5487,5488,5489,5490,5491,5492,5493,5494,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5559,5566,5567,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,7037],\"1742\": [47,60,61,63,64,65,66,67,80,81,82,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,226,227,232,233,242,267,268,269,271,314,319,323,366,367,376,385,388,417,418,419,420,554,559,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,633,634,636,643,700,701,702,703,705,717,728,745,746,834,835,836,837,838,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1088,1119,1120,1121,1122,1123,1207,1208,1209,1213,1214,1215,1216,1269,1386,1508,1509,1692,1693,1694,1695,1698,1699,1700,1701,1720,1726,1727,1729,1780,1781,1830,1851,1920,1993,2127,2216,2258,2295,2390,2564,2621,2821,2823,2844,2847,2862,2863,2864,2865,2911,2912,2913,2916,2922,2925,2935,2943,2945,2947,2948,2949,2950,2978,2979,2980,2981,2982,2987,2988,2996,3007,3008,3011,3012,3015,3016,3017,3018,3072,3099,3112,3113,3154,3155,3156,3157,3311,3312,3315,3318,3353,3355,3420,3422,3423,3590,3591,3592,3625,3732,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3794,3801,3871,3940,4049,4050,4058,4063,4169,4170,4476,4477,4482,4616,4906,6864,6865],\"1125\": [47,53,65,67,80,81,172,174,187,190,196,198,200,202,246,267,268,269,309,313,316,319,320,323,324,325,326,370,372,374,421,448,594,597,600,634,636,657,658,673,679,692,708,735,860,944,945,946,993,994,995,1061,1063,1065,1067,1220,1222,1223,1277,1502,1517,1518,1572,1573,1574,1575,1621,1622,1623,1632,1635,1637,1661,1696,1849,1897,1899,1901,1968,1993,2032,2033,2034,2069,2283,2421,2422,2423,2445,2471,2472,2490,2493,2527,2529,2609,2623,2627,2669,2671,2729,2738,2739,2804,2825,2826,2853,2854,2855,2856,2894,2895,2901,2902,2903,2904,2918,2926,2932,3024,3107,3114,3115,3116,3299,3353,3354,3355,3356,3364,3365,3373,3390,3391,3392,3393,3415,3422,3423,3425,3541,3550,3715,3719,3720,3721,3792,3793,3794,3795,3800,3835,3836,3844,3845,3846,3847,3864,3866,3867,3920,3925,3926,3929,3930,3935,3936,4030,4059,4090,4111,4112,4138,4143,4145,4161,4162,4165,4306,4311,4351,4361,4368,4397,4457,4467,4471,4480,4638,4639,4754,4764,4765,4766,4770,4803,4806,4807,4808,4824,4830,4888,4904,4905,4911,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,6892,6893],\"1095\": [64,65,66,67,80,81,187,190,196,198,200,202,219,267,268,269,319,320,324,385,388,559,573,576,580,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,644,679,689,690,691,751,756,842,843,844,845,846,847,848,937,938,939,944,945,946,951,952,953,958,959,960,964,965,966,967,986,987,988,993,994,995,1000,1001,1002,1993,2098,2250,2258,2354,2421,2495,2496,2821,2823,2839,2844,2847,2854,2856,2862,2863,2864,2865,2922,2925,2947,2948,2949,2950,2978,2980,3007,3011,3015,3016,3017,3018,3113,3299,3306,3310,3315,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3368,3422,3423,3590,3591,3592,3748,3750,3752,3754,3756,3762,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3792,3793,3794,3795,3801,3871],\"1145\": [64,65,66,67,80,81,82,84,85,125,129,130,131,132,133,134,135,140,141,263,264,267,268,269,327,334,351,367,388,446,553,594,597,600,601,602,604,607,608,609,611,614,620,622,629,631,634,643,692,735,786,867,896,937,938,939,944,945,946,950,951,952,953,958,959,960,975,977,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1077,1088,1119,1120,1121,1122,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1204,1224,1232,1233,1244,1245,1275,1517,1518,1572,1573,1574,1575,1661,1669,1670,1671,1709,1711,1729,1730,1731,1843,1845,1897,1899,1901,1993,1994,2013,2057,2218,2219,2220,2227,2228,2229,2230,2250,2258,2283,2327,2471,2472,2490,2493,2560,2562,2587,2596,2608,2609,2610,2611,2630,2631,2673,2682,2683,2684,2705,2706,2708,2709,2710,2711,2712,2713,2817,2821,2825,2844,2862,2863,2880,2900,2901,2902,2916,2918,2922,2925,2926,2941,2947,2948,2949,2950,2977,2978,2982,2991,2992,2993,3007,3011,3015,3016,3017,3018,3033,3034,3072,3297,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3417,3588,3721,3744,3864,3866,3867,4030,4034,4059,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4161,4358,4359,4366,4638,4639],\"1966\": [64,65,66,67,80,81,82,83,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,366,367,376,385,386,387,388,391,417,418,419,420,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,616,624,625,626,634,636,639,640,641,644,778,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1075,1114,1115,1116,1117,1118,1728,1993,2258,2596,2673,2821,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3112,3113,3590,3591,3592,3744,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3800,3871,4062,4764,4765,4766],\"2148\": [159,220,696,1050,1277,1278,1280,1696,1994,2032,2033,2034,2151,2319,2429,2432,2433,2434,2435,2436,2437,2441,2445,3299,3310,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3622,4913,6945,6946,6947,6948,6949,6950,6951,6952,6953,6990,6991],\"2387\": [159,1050,1994,2708,2709,2710,2711,2712,2713,2976,2977,3416,3417,3622,4358,4359],\"865\": [216,217,851,860,2053,2054,2055,2056,2131,2132,2422,2423],\"2442\": [220,1277,1621,1622,1623,1632,1635,1637,1859,1865,1873,1879,1885,1892,1994,2032,2033,2034,2432,2433,2434,2435,2436,2437,2445,2780,2789,3299],\"2370\": [321,692,1245,1517,1518,1572,1573,1574,1575,1661,1897,1899,1901,2068,2094,2095,2096,2106,2109,2263,2264,2270,2283,2284,2303,2327,2366,2367,2390,2428,2490,2493,2719,2722,2726,2735,2736,2738,2739,2740,2741,2765,2827,2894,2895,2901,2902,2903,2926,3019,3024,3031,3077,3078,3079,3081,3083,3084,3085,3349,3590,3591,3592,3605,3606,3715,3716,3853,3854,3855,3856,3857,3861,4112,4120,4284,4306,4398,4620,4621,6902,6903],\"1950\": [684,816,1285,1286,1656,1657,2405,2512,2527,3651],\"3852\": [779,780,1213,1214,1289,1847,1849,3339,3732,3738,3743,3790,3797,3800,4054,4113,4765,4766],\"2381\": [781,782,810,2023,2024,2632,2633,4365],\"1108\": [920,921],\"1105\": [1276,1502,1994,2032,2269,2319,2342,2343,2344,2348,2349,2350,2420,2421,2429,2441,3310,3321,3322,3323,3326,3327,3328],\"2725\": [2723,4161],\"2727\": [2729],\"2728\": [2730],\"2820\": [2858,2860]}\n</code></pre>\n\n<p>This is what I already tried, but I am unable to create DF and probably there is a more efficient way of doing this task:</p>\n\n<pre><code>import pandas as pd\nimport json\nimport os\n\npath=\"C:\\\\Users\\Z003Z9CF\\Documents\"\nos.chdir(path)\nwith open('newdependency.json') as data_file:    \n    data = json.load(data_file)\ns1 = pd.Series(data,name='Child')\ndf = pd.concat([s1], axis=1)\ndf.index.name = 'Parent'\ndf\n</code></pre>\n\n<p>Expecting output like this for all elements in a Json file</p>\n\n<pre><code>Parent   Child\n1087       4\n1087       5\n1087       6\n.....           ......\n1096       25\n1096       26\n1096       27\n1096       28\n......        ......\n1144      25\n1144      26\n1144      27\n.....         .....\n</code></pre>\n\n<p>Also I want to create a directed graph for each parent node with child nodes as clusters because parent nodes has the same child nodes which are also present in other parent nodes.\nThanks in Advance</p>\n",
        "formatted_input": {
            "qid": 54577752,
            "link": "https://stackoverflow.com/questions/54577752/pandas-dataframe-from-dictionary",
            "question": {
                "title": "Pandas DataFrame from Dictionary",
                "ques_desc": "Let's assume that I have a JSON file like below, and I want to convert this file into a data frame with 2 columns: This is what I already tried, but I am unable to create DF and probably there is a more efficient way of doing this task: Expecting output like this for all elements in a Json file Also I want to create a directed graph for each parent node with child nodes as clusters because parent nodes has the same child nodes which are also present in other parent nodes. Thanks in Advance "
            },
            "io": [
                "{\"1087\": [4,5,6,7,8,9,10,12,13,21,22,23,24,25,26,27,28,34,35 ,37,39,40,42,44,45,46,47,48,51,52,54,55,56,59,60,61,63,64,65,66,67,68,72,73,74,75,78,80,81,82,83,84,85,87,88,92,94,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,149,180,181,196,198,200,202,206,222,223,226,227,230,231,232,233,234,235,242,255,257,258,259,261,263,264,265,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,302,303,304,305,306,307,308,309,310,311,313,314,316,318,319,320,323,325,326,327,328,330,334,336,337,339,340,342,343,350,351,354,355,362,363,365,366,367,368,369,370,371,372,374,375,376,377,378,379,380,383,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,427,428,429,430,431,432,433,434,435,437,438,444,446,449,451,455,457,461,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,494,496,498,499,500,502,504,506,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,559,560,561,565,567,569,571,573,574,576,579,580,581,583,586,587,588,590,593,594,597,598,600,601,602,604,605,606,607,608,609,611,612,613,614,615,616,617,620,621,622,624,625,626,629,631,633,634,636,638,639,640,641,643,644,647,649,650,651,652,653,654,657,658,664,665,666,667,669,671,674,675,676,677,678,682,683,685,686,687,688,692,694,695,702,703,705,708,712,713,714,715,716,717,718,720,728,732,734,735,739,740,742,743,745,746,751,752,759,769,770,772,778,779,780,783,784,786,792,805,815,823,831,832,834,835,836,837,838,839,852,854,855,856,867,875,877,879,888,890,891,896,900,908,909,910,911,912,913,914,915,916,917,918,919,934,935,936,937,938,939,944,945,946,950,951,952,953,957,958,959,960,964,965,966,967,971,975,977,978,980,981,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1027,1028,1033,1034,1035,1036,1037,1038,1039,1049,1061,1063,1065,1067,1069,1070,1071,1072,1073,1074,1076,1077,1078,1080,1081,1084,1088,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1127,1128,1129,1130,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1151,1155,1156,1201,1202,1203,1204,1207,1208,1209,1213,1214,1215,1216,1217,1220,1221,1222,1223,1224,1232,1233,1235,1237,1238,1241,1243,1244,1245,1248,1249,1251,1254,1269,1271,1273,1274,1275,1284,1289,1298,1301,1302,1303,1470,1495,1500,1501,1508,1509,1517,1518,1572,1573,1574,1575,1614,1619,1620,1625,1633,1639,1661,1669,1670,1671,1692,1693,1694,1695,1696,1698,1699,1700,1701,1706,1707,1708,1709,1711,1712,1713,1715,1720,1726,1728,1729,1730,1731,1732,1734,1755,1771,1780,1781,1785,1788,1794,1795,1797,1801,1802,1803,1805,1827,1829,1830,1836,1838,1843,1845,1847,1849,1851,1852,1853,1854,1855,1897,1899,1901,1920,1922,1923,1974,1987,1988,1989,1990,1991,1993,1994,2013,2014,2038,2039,2040,2044,2057,2086,2108,2144,2150,2215,2216,2218,2219,2220,2227,2228,2229,2230,2250,2258,2271,2279,2283,2285,2286,2287,2295,2302,2327,2390,2397,2406,2407,2411,2413,2414,2415,2419,2421,2429,2441,2471,2472,2490,2493,2507,2514,2519,2524,2525,2531,2532,2535,2538,2541,2551,2552,2553,2555,2560,2561,2562,2564,2570,2577,2578,2579,2580,2581,2586,2587,2588,2589,2591,2592,2594,2595,2596,2597,2599,2600,2601,2602,2603,2604,2605,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2625,2626,2627,2628,2629,2630,2631,2634,2635,2665,2668,2669,2671,2673,2681,2682,2683,2684,2705,2706,2707,2708,2709,2710,2711,2712,2713,2750,2766,2768,2769,2770,2798,2804,2817,2821,2822,2823,2824,2825,2826,2844,2847,2853,2855,2858,2860,2861,2862,2863,2864,2865,2880,2900,2901,2902,2903,2906,2911,2912,2913,2916,2918,2922,2925,2926,2932,2935,2941,2943,2945,2947,2948,2949,2950,2958,2959,2966,2967,2972,2976,2977,2978,2979,2980,2981,2982,2987,2988,2991,2992,2993,2994,2995,2996,2999,3001,3003,3007,3008,3011,3012,3015,3016,3017,3018,3024,3030,3031,3033,3034,3045,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3093,3099,3105,3112,3113,3114,3115,3116,3117,3127,3128,3154,3155,3156,3157,3297,3299,3300,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3336,3339,3416,3417,3420,3424,3550,3587,3588,3589,3590,3591,3592,3593,3598,3599,3600,3602,3603,3604,3605,3606,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3625,3655,3656,3657,3718,3721,3724,3725,3726,3730,3732,3733,3736,3738,3741,3743,3744,3747,3748,3750,3752,3754,3756,3762,3763,3764,3765,3766,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3795,3797,3798,3800,3806,3811,3864,3866,3867,3871,3881,3883,3884,3885,3886,3925,3926,3929,3930,3935,3936,3940,4018,4030,4045,4049,4050,4051,4054,4058,4059,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4104,4109,4110,4113,4116,4117,4118,4119,4161,4267,4285,4310,4317,4335,4358,4359,4365,4366,4467,4471,4475,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4638,4639,4764,4765,4766,4782,4803,4824,4827,4828,4830,4888,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4926,4990,6998,7026,7027,7028],\"1096\": [25,26,27,28,45,46,63,64,65,66,67,80,81,82,83,84,85,128,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,330,366,367,376,385,386,387,388,391,417,418,419,420,437,449,451,553,555,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,641,643,644,779,780,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1273,1274,1275,1278,1280,1289,1292,1670,1671,1713,1730,1731,1847,1849,1993,2086,2218,2219,2220,2258,2421,2586,2587,2608,2610,2611,2629,2631,2673,2708,2709,2710,2711,2712,2713,2821,2822,2823,2825,2844,2847,2858,2860,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2959,2976,2977,2978,2979,2980,2981,2982,2991,2992,2993,2994,2995,3001,3003,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3112,3113,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3416,3417,3589,3590,3591,3592,3593,3598,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3656,3657,3732,3738,3743,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3797,3798,3871,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4267,4358,4359,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4764,4765,4766,7026,7027,7028],\"1144\": [25,26,27,28,144,372,374,422,768,1005,1051,1052,1053,1054,1057,1058,1060,1062,1064,1066,1068,1098,1101,1146,1703,1704,1705,1713,1716,1994,2086,2382,3095,3096,3097,3114,3115,3116,3339,3619,3620,3621,3732,3738,3743,3881,3883,3884,3885,3886,4113,4116,4117,4118,4119,4267,4285,4365,4370,4371,4372,4373,4374,4375,4471,4764,4765,4766,4803,4824,4828,4830,4990],\"-1\": [40,63,64,65,66,67,68,80,81,82,83,84,85,87,130,131,132,133,134,135,136,137,138,139,140,141,234,261,263,264,267,268,269,271,272,293,308,314,318,319,337,366,367,375,376,385,386,387,388,391,407,416,417,418,419,420,435,461,489,559,561,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,623,624,625,626,632,634,636,644,666,682,683,685,686,687,688,694,695,696,720,737,854,855,870,882,883,888,896,916,917,918,919,930,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,971,978,982,986,987,988,993,994,995,996,1000,1001,1002,1036,1037,1038,1039,1081,1132,1133,1134,1135,1136,1210,1317,1321,1341,1347,1377,1378,1380,1383,1384,1386,1396,1398,1408,1410,1432,1456,1458,1473,1500,1501,1525,1614,1670,1730,1808,1838,1982,1983,1984,1985,1993,2033,2034,2038,2039,2040,2069,2150,2151,2258,2355,2356,2571,2596,2692,2729,2737,2821,2822,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2976,2977,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3150,3151,3316,3416,3417,3594,3744,3769,3770,3773,3776,3785,3786,3787,3871,4064,4066,4068,4070,4072,4074,4076,4078,4080,4082,4084,4086,4088,4092,4094,4096,4098,4109,4288,4321,4330,4466,4991,5567,6913],\"2060\": [47,65,67,80,81,148,155,156,166,167,168,226,227,267,268,269,580,594,597,600,601,602,604,607,608,609,611,614,634,636,682,683,685,686,687,688,694,695,696,728,733,738,744,944,945,946,993,994,995,1317,1321,1341,1347,1377,1378,1380,1383,1384,1385,1386,1387,1396,1398,1408,1410,1432,1456,1458,1473,1525,1696,1736,1737,1738,1739,1754,1808,1859,1865,1873,1879,1885,1892,1922,1982,1983,1984,1985,1993,1994,2038,2039,2040,2150,2151,2254,2300,2355,2356,2377,2391,2448,2478,2530,2564,2723,2742,2745,2746,2747,2882,2922,2925,2947,2948,2949,2950,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3341,3383,3482,3493,3494,3506,3530,3672,3675,3821,4332,4439,4440,4459,4908,4913,4914,4915,4916,4917,4919,4920,4921,4922,4923,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5177,5178,5179,5180,5181,5182,5183,5184,5188,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5217,5218,5219,5220,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5486,5487,5488,5489,5490,5491,5492,5493,5494,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5559,5566,5567,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,7037],\"1742\": [47,60,61,63,64,65,66,67,80,81,82,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,226,227,232,233,242,267,268,269,271,314,319,323,366,367,376,385,388,417,418,419,420,554,559,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,633,634,636,643,700,701,702,703,705,717,728,745,746,834,835,836,837,838,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1088,1119,1120,1121,1122,1123,1207,1208,1209,1213,1214,1215,1216,1269,1386,1508,1509,1692,1693,1694,1695,1698,1699,1700,1701,1720,1726,1727,1729,1780,1781,1830,1851,1920,1993,2127,2216,2258,2295,2390,2564,2621,2821,2823,2844,2847,2862,2863,2864,2865,2911,2912,2913,2916,2922,2925,2935,2943,2945,2947,2948,2949,2950,2978,2979,2980,2981,2982,2987,2988,2996,3007,3008,3011,3012,3015,3016,3017,3018,3072,3099,3112,3113,3154,3155,3156,3157,3311,3312,3315,3318,3353,3355,3420,3422,3423,3590,3591,3592,3625,3732,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3794,3801,3871,3940,4049,4050,4058,4063,4169,4170,4476,4477,4482,4616,4906,6864,6865],\"1125\": [47,53,65,67,80,81,172,174,187,190,196,198,200,202,246,267,268,269,309,313,316,319,320,323,324,325,326,370,372,374,421,448,594,597,600,634,636,657,658,673,679,692,708,735,860,944,945,946,993,994,995,1061,1063,1065,1067,1220,1222,1223,1277,1502,1517,1518,1572,1573,1574,1575,1621,1622,1623,1632,1635,1637,1661,1696,1849,1897,1899,1901,1968,1993,2032,2033,2034,2069,2283,2421,2422,2423,2445,2471,2472,2490,2493,2527,2529,2609,2623,2627,2669,2671,2729,2738,2739,2804,2825,2826,2853,2854,2855,2856,2894,2895,2901,2902,2903,2904,2918,2926,2932,3024,3107,3114,3115,3116,3299,3353,3354,3355,3356,3364,3365,3373,3390,3391,3392,3393,3415,3422,3423,3425,3541,3550,3715,3719,3720,3721,3792,3793,3794,3795,3800,3835,3836,3844,3845,3846,3847,3864,3866,3867,3920,3925,3926,3929,3930,3935,3936,4030,4059,4090,4111,4112,4138,4143,4145,4161,4162,4165,4306,4311,4351,4361,4368,4397,4457,4467,4471,4480,4638,4639,4754,4764,4765,4766,4770,4803,4806,4807,4808,4824,4830,4888,4904,4905,4911,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,6892,6893],\"1095\": [64,65,66,67,80,81,187,190,196,198,200,202,219,267,268,269,319,320,324,385,388,559,573,576,580,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,644,679,689,690,691,751,756,842,843,844,845,846,847,848,937,938,939,944,945,946,951,952,953,958,959,960,964,965,966,967,986,987,988,993,994,995,1000,1001,1002,1993,2098,2250,2258,2354,2421,2495,2496,2821,2823,2839,2844,2847,2854,2856,2862,2863,2864,2865,2922,2925,2947,2948,2949,2950,2978,2980,3007,3011,3015,3016,3017,3018,3113,3299,3306,3310,3315,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3368,3422,3423,3590,3591,3592,3748,3750,3752,3754,3756,3762,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3792,3793,3794,3795,3801,3871],\"1145\": [64,65,66,67,80,81,82,84,85,125,129,130,131,132,133,134,135,140,141,263,264,267,268,269,327,334,351,367,388,446,553,594,597,600,601,602,604,607,608,609,611,614,620,622,629,631,634,643,692,735,786,867,896,937,938,939,944,945,946,950,951,952,953,958,959,960,975,977,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1077,1088,1119,1120,1121,1122,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1204,1224,1232,1233,1244,1245,1275,1517,1518,1572,1573,1574,1575,1661,1669,1670,1671,1709,1711,1729,1730,1731,1843,1845,1897,1899,1901,1993,1994,2013,2057,2218,2219,2220,2227,2228,2229,2230,2250,2258,2283,2327,2471,2472,2490,2493,2560,2562,2587,2596,2608,2609,2610,2611,2630,2631,2673,2682,2683,2684,2705,2706,2708,2709,2710,2711,2712,2713,2817,2821,2825,2844,2862,2863,2880,2900,2901,2902,2916,2918,2922,2925,2926,2941,2947,2948,2949,2950,2977,2978,2982,2991,2992,2993,3007,3011,3015,3016,3017,3018,3033,3034,3072,3297,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3417,3588,3721,3744,3864,3866,3867,4030,4034,4059,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4161,4358,4359,4366,4638,4639],\"1966\": [64,65,66,67,80,81,82,83,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,366,367,376,385,386,387,388,391,417,418,419,420,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,616,624,625,626,634,636,639,640,641,644,778,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1075,1114,1115,1116,1117,1118,1728,1993,2258,2596,2673,2821,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3112,3113,3590,3591,3592,3744,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3800,3871,4062,4764,4765,4766],\"2148\": [159,220,696,1050,1277,1278,1280,1696,1994,2032,2033,2034,2151,2319,2429,2432,2433,2434,2435,2436,2437,2441,2445,3299,3310,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3622,4913,6945,6946,6947,6948,6949,6950,6951,6952,6953,6990,6991],\"2387\": [159,1050,1994,2708,2709,2710,2711,2712,2713,2976,2977,3416,3417,3622,4358,4359],\"865\": [216,217,851,860,2053,2054,2055,2056,2131,2132,2422,2423],\"2442\": [220,1277,1621,1622,1623,1632,1635,1637,1859,1865,1873,1879,1885,1892,1994,2032,2033,2034,2432,2433,2434,2435,2436,2437,2445,2780,2789,3299],\"2370\": [321,692,1245,1517,1518,1572,1573,1574,1575,1661,1897,1899,1901,2068,2094,2095,2096,2106,2109,2263,2264,2270,2283,2284,2303,2327,2366,2367,2390,2428,2490,2493,2719,2722,2726,2735,2736,2738,2739,2740,2741,2765,2827,2894,2895,2901,2902,2903,2926,3019,3024,3031,3077,3078,3079,3081,3083,3084,3085,3349,3590,3591,3592,3605,3606,3715,3716,3853,3854,3855,3856,3857,3861,4112,4120,4284,4306,4398,4620,4621,6902,6903],\"1950\": [684,816,1285,1286,1656,1657,2405,2512,2527,3651],\"3852\": [779,780,1213,1214,1289,1847,1849,3339,3732,3738,3743,3790,3797,3800,4054,4113,4765,4766],\"2381\": [781,782,810,2023,2024,2632,2633,4365],\"1108\": [920,921],\"1105\": [1276,1502,1994,2032,2269,2319,2342,2343,2344,2348,2349,2350,2420,2421,2429,2441,3310,3321,3322,3323,3326,3327,3328],\"2725\": [2723,4161],\"2727\": [2729],\"2728\": [2730],\"2820\": [2858,2860]}\n",
                "Parent   Child\n1087       4\n1087       5\n1087       6\n.....           ......\n1096       25\n1096       26\n1096       27\n1096       28\n......        ......\n1144      25\n1144      26\n1144      27\n.....         .....\n"
            ],
            "answer": {
                "ans_desc": "Pandas will not always have a way to magically parse a data structure directly (whether you are using just the constructor or one of its classmethods such as ). Here, you can just pass a modified version of the native Python structure directly: The expression combines a dict comprehension and a list comprehension. It looks like: ",
                "code": [
                    "[[k, i] for k, v in data.items() for i in v]",
                    "In [14]: [[k, i] for k, v in data.items() for i in v]                                                                                                                                                                                                        \nOut[14]:\n[['1108', 920],\n ['1108', 921],\n ['2381', 781],\n ['2381', 782],\n ['2381', 810],\n ['2381', 2023],\n ['2381', 2024],\n ['2381', 2632],\n ['2381', 2633],\n ['2381', 4365],\n ['2728', 2730]]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "grouping"
        ],
        "owner": {
            "reputation": 452,
            "user_id": 3938025,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://i.stack.imgur.com/gKV2z.png?s=128&g=1",
            "display_name": "Cody Glickman",
            "link": "https://stackoverflow.com/users/3938025/cody-glickman"
        },
        "is_answered": true,
        "view_count": 4518,
        "accepted_answer_id": 54518598,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1549291962,
        "creation_date": 1549291217,
        "question_id": 54518504,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54518504/check-if-group-contains-same-value-in-pandas",
        "title": "Check if group contains same value in Pandas",
        "body": "<p>I am curious if there is a pre-built function in Pandas to check if all members of a group (factors in a column) contain the same value in another column. </p>\n\n<p>i.e. if my dataframe was similar to below it would return an empty list.</p>\n\n<pre><code>Col1    Col2\n2        A\n2        A\n0        B\n0        B\n</code></pre>\n\n<p>However, if my dataframe appeared as such (<strong>notice the 1 in Col1</strong>):</p>\n\n<pre><code>Col1    Col2\n2        A\n2        A\n0        B\n1        B \n</code></pre>\n\n<p>Then the output would be a list containing the object \"B\" since the group B has different values in Col1. </p>\n",
        "answer_body": "<p>Use groupby nunique and index of unique values > 1</p>\n\n<pre><code>a = df.groupby('Col2').Col1.nunique() &gt; 1\na[a].index.tolist()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>['B']\n</code></pre>\n",
        "question_body": "<p>I am curious if there is a pre-built function in Pandas to check if all members of a group (factors in a column) contain the same value in another column. </p>\n\n<p>i.e. if my dataframe was similar to below it would return an empty list.</p>\n\n<pre><code>Col1    Col2\n2        A\n2        A\n0        B\n0        B\n</code></pre>\n\n<p>However, if my dataframe appeared as such (<strong>notice the 1 in Col1</strong>):</p>\n\n<pre><code>Col1    Col2\n2        A\n2        A\n0        B\n1        B \n</code></pre>\n\n<p>Then the output would be a list containing the object \"B\" since the group B has different values in Col1. </p>\n",
        "formatted_input": {
            "qid": 54518504,
            "link": "https://stackoverflow.com/questions/54518504/check-if-group-contains-same-value-in-pandas",
            "question": {
                "title": "Check if group contains same value in Pandas",
                "ques_desc": "I am curious if there is a pre-built function in Pandas to check if all members of a group (factors in a column) contain the same value in another column. i.e. if my dataframe was similar to below it would return an empty list. However, if my dataframe appeared as such (notice the 1 in Col1): Then the output would be a list containing the object \"B\" since the group B has different values in Col1. "
            },
            "io": [
                "Col1    Col2\n2        A\n2        A\n0        B\n0        B\n",
                "Col1    Col2\n2        A\n2        A\n0        B\n1        B \n"
            ],
            "answer": {
                "ans_desc": "Use groupby nunique and index of unique values > 1 Output: ",
                "code": [
                    "a = df.groupby('Col2').Col1.nunique() > 1\na[a].index.tolist()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dictionary",
            "dataframe",
            "indexing"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 10990619,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/09c55d4db8a10bcacdafeed4482ee840?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ola.Rad",
            "link": "https://stackoverflow.com/users/10990619/ola-rad"
        },
        "is_answered": true,
        "view_count": 238,
        "accepted_answer_id": 54460152,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1548940400,
        "creation_date": 1548935931,
        "last_edit_date": 1548940400,
        "question_id": 54460092,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54460092/matching-dictionaries-with-columns-and-indices-in-dataframe-python",
        "title": "Matching dictionaries with columns and indices in DataFrame | python",
        "body": "<p>I have a DataFrame with column names as on  example and the indices from 0 to 1000. The dataframe is filled with zeros.</p>\n\n<pre><code>House 1 | House 2 | House 5 | House 8 | ...\n0\n1\n2\n3\n4...\n</code></pre>\n\n<p>Then, I have dictionary, e.g.:</p>\n\n<pre><code>dict_of_houses = {'House 1':[100,201,306,387,500,900],'House 2':[31,87,254,675,987],'House 5':[23,45,67,123,345,654,789,808,864,987,999],'House 8':[23,675,786,858,868,912,934]}\n</code></pre>\n\n<p><em>Dictionary name edited in order not to confuse anyone later.</em></p>\n\n<p>My goal is to:</p>\n\n<ul>\n<li>for every dict key match it with the column</li>\n<li>for every number in the list as dictionary value to match with the index</li>\n<li>if there is a match of index and column, then change the cell to 1</li>\n<li>else: leave zero</li>\n</ul>\n\n<p>How would you do that?</p>\n",
        "answer_body": "<p>You can use a <code>for</code> loop:</p>\n\n<pre><code>for house, indices in dict_.items():\n    df.loc[indices, house] = 1\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame with column names as on  example and the indices from 0 to 1000. The dataframe is filled with zeros.</p>\n\n<pre><code>House 1 | House 2 | House 5 | House 8 | ...\n0\n1\n2\n3\n4...\n</code></pre>\n\n<p>Then, I have dictionary, e.g.:</p>\n\n<pre><code>dict_of_houses = {'House 1':[100,201,306,387,500,900],'House 2':[31,87,254,675,987],'House 5':[23,45,67,123,345,654,789,808,864,987,999],'House 8':[23,675,786,858,868,912,934]}\n</code></pre>\n\n<p><em>Dictionary name edited in order not to confuse anyone later.</em></p>\n\n<p>My goal is to:</p>\n\n<ul>\n<li>for every dict key match it with the column</li>\n<li>for every number in the list as dictionary value to match with the index</li>\n<li>if there is a match of index and column, then change the cell to 1</li>\n<li>else: leave zero</li>\n</ul>\n\n<p>How would you do that?</p>\n",
        "formatted_input": {
            "qid": 54460092,
            "link": "https://stackoverflow.com/questions/54460092/matching-dictionaries-with-columns-and-indices-in-dataframe-python",
            "question": {
                "title": "Matching dictionaries with columns and indices in DataFrame | python",
                "ques_desc": "I have a DataFrame with column names as on example and the indices from 0 to 1000. The dataframe is filled with zeros. Then, I have dictionary, e.g.: Dictionary name edited in order not to confuse anyone later. My goal is to: for every dict key match it with the column for every number in the list as dictionary value to match with the index if there is a match of index and column, then change the cell to 1 else: leave zero How would you do that? "
            },
            "io": [
                "House 1 | House 2 | House 5 | House 8 | ...\n0\n1\n2\n3\n4...\n",
                "dict_of_houses = {'House 1':[100,201,306,387,500,900],'House 2':[31,87,254,675,987],'House 5':[23,45,67,123,345,654,789,808,864,987,999],'House 8':[23,675,786,858,868,912,934]}\n"
            ],
            "answer": {
                "ans_desc": "You can use a loop: ",
                "code": [
                    "for house, indices in dict_.items():\n    df.loc[indices, house] = 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 419,
            "user_id": 9157212,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/TDPVE.jpg?s=128&g=1",
            "display_name": "jkortner",
            "link": "https://stackoverflow.com/users/9157212/jkortner"
        },
        "is_answered": true,
        "view_count": 867,
        "accepted_answer_id": 48749230,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1548922989,
        "creation_date": 1518447222,
        "last_edit_date": 1548922989,
        "question_id": 48749201,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/48749201/how-to-sum-n-columns-in-python",
        "title": "How to sum N columns in python?",
        "body": "<p>I've a pandas df and I'd like to sum N of the columns. The df might look like this:</p>\n\n<pre><code>A B C D ... X\n\n1 4 2 6     3\n2 3 1 2     2 \n3 1 1 2     4\n4 2 3 5 ... 1\n</code></pre>\n\n<p>I'd like to get a df like this:</p>\n\n<pre><code>A Z\n\n1 15\n2 8\n3 8\n4 11\n</code></pre>\n\n<p>The A variable is not an index, but a variable. </p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html\" rel=\"nofollow noreferrer\"><code>join</code></a> for new <code>Series</code> created by <code>sum</code> all columns without <code>A</code>:</p>\n\n<pre><code>df = df[['A']].join(df.drop('A', 1).sum(axis=1).rename('Z'))\n</code></pre>\n\n<p>Or extract column <code>A</code> first by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pop.html\" rel=\"nofollow noreferrer\"><code>pop</code></a>:</p>\n\n<pre><code>df = df.pop('A').to_frame().join(df.sum(axis=1).rename('Z'))\n</code></pre>\n\n<p>If want select columns by positions use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html\" rel=\"nofollow noreferrer\"><code>iloc</code></a>:</p>\n\n<pre><code>df = df.iloc[:, [0]].join(df.iloc[:, 1:].sum(axis=1).rename('Z'))\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n   A   Z\n0  1  15\n1  2   8\n2  3   8\n3  4  11\n</code></pre>\n",
        "question_body": "<p>I've a pandas df and I'd like to sum N of the columns. The df might look like this:</p>\n\n<pre><code>A B C D ... X\n\n1 4 2 6     3\n2 3 1 2     2 \n3 1 1 2     4\n4 2 3 5 ... 1\n</code></pre>\n\n<p>I'd like to get a df like this:</p>\n\n<pre><code>A Z\n\n1 15\n2 8\n3 8\n4 11\n</code></pre>\n\n<p>The A variable is not an index, but a variable. </p>\n",
        "formatted_input": {
            "qid": 48749201,
            "link": "https://stackoverflow.com/questions/48749201/how-to-sum-n-columns-in-python",
            "question": {
                "title": "How to sum N columns in python?",
                "ques_desc": "I've a pandas df and I'd like to sum N of the columns. The df might look like this: I'd like to get a df like this: The A variable is not an index, but a variable. "
            },
            "io": [
                "A B C D ... X\n\n1 4 2 6     3\n2 3 1 2     2 \n3 1 1 2     4\n4 2 3 5 ... 1\n",
                "A Z\n\n1 15\n2 8\n3 8\n4 11\n"
            ],
            "answer": {
                "ans_desc": "Use for new created by all columns without : Or extract column first by : If want select columns by positions use : ",
                "code": [
                    "df = df.iloc[:, [0]].join(df.iloc[:, 1:].sum(axis=1).rename('Z'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 329,
            "user_id": 4003761,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/f937c9aded7d1f5611885959f1a9092f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Demosthene",
            "link": "https://stackoverflow.com/users/4003761/demosthene"
        },
        "is_answered": true,
        "view_count": 2217,
        "accepted_answer_id": 54408909,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1548703452,
        "creation_date": 1548696388,
        "last_edit_date": 1548702263,
        "question_id": 54407275,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54407275/add-column-to-dataframe-in-a-loop",
        "title": "Add column to DataFrame in a loop",
        "body": "<p>Let's say I have a very simple pandas dataframe, containing a single indexed column with \"initial values\". I want to read in a loop N other dataframes to fill a single \"comparison\" column, with matching indices.</p>\n\n<p>For instance, with my inital dataframe as</p>\n\n<pre><code>   Initial\n0        a\n1        b\n2        c\n3        d\n</code></pre>\n\n<p>and the following two dataframes to read in a loop</p>\n\n<pre><code>   Comparison\n0           e\n1           f\n   Comparison\n2           g\n3           h\n4           i  &lt;= note that this index doesn't exist in Initial so won't be matched\n</code></pre>\n\n<p>I would like to produce the following result</p>\n\n<pre><code>    Initial Comparison\n0        a           e\n1        b           f\n2        c           g\n3        d           h\n</code></pre>\n\n<p>Using <code>merge</code>, <code>concat</code> or <code>join</code>, I only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with <code>NaN</code>.</p>\n\n<p>What's the most pandas-pythonic way of achieving this?</p>\n\n<hr>\n\n<p>Below an example from the proposed duplicate solution:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\ndf1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])\nprint df1\ndf2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])\nprint df2\ndf3 = pd.DataFrame(np.array([[2,'g'],[3,'h'],[4,'i']]), columns=['','Compare'])\ndf3 = df3.set_index('')\nprint df3\nprint df1.merge(df2,left_index=True,right_index=True).merge(df3,left_index=True,right_index=True)\n&gt;&gt;\n      Initial\n0       a\n1       b\n2       c\n3       d\n  Compare\n0       e\n1       f\n  Compare\n\n2       g\n3       h\n4       i\nEmpty DataFrame\nColumns: [Initial, Compare_x, Compare_y]\nIndex: []\n</code></pre>\n\n<hr>\n\n<p><strong>Second edit:</strong> @W-B, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. It also requires turning off warnings, which might be dangerous...</p>\n\n<pre><code>pd.options.mode.chained_assignment = None\ndf1[\"Compare\"]=pd.Series()\nfor ind in df1.index.values:\n    if ind in df2.index.values:\n        df1[\"Compare\"][ind]=df2.T[ind][\"Compare\"]\n    if ind in df3.index.values:\n        df1[\"Compare\"][ind]=df3.T[ind][\"Compare\"]\nprint df1\n&gt;&gt;\n      Initial Compare\n0           a       e\n1           b       f\n2           c       g\n3           d       h\n</code></pre>\n",
        "answer_body": "<p>Ok , since Op need more info </p>\n\n<hr>\n\n<p>Data input</p>\n\n<pre><code>import functools\ndf1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])\ndf1['Compare']=np.nan\ndf2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])\ndf3 = pd.DataFrame(np.array(['g','h','i']), columns=['Compare'],index=[2,3,4])\n</code></pre>\n\n<hr>\n\n<p>Solution </p>\n\n<pre><code>newdf=functools.reduce(lambda x,y: x.fillna(y),[df1,df2,df3])\nnewdf\nOut[639]: \n  Initial Compare\n0       a       e\n1       b       f\n2       c       g\n3       d       h\n</code></pre>\n",
        "question_body": "<p>Let's say I have a very simple pandas dataframe, containing a single indexed column with \"initial values\". I want to read in a loop N other dataframes to fill a single \"comparison\" column, with matching indices.</p>\n\n<p>For instance, with my inital dataframe as</p>\n\n<pre><code>   Initial\n0        a\n1        b\n2        c\n3        d\n</code></pre>\n\n<p>and the following two dataframes to read in a loop</p>\n\n<pre><code>   Comparison\n0           e\n1           f\n   Comparison\n2           g\n3           h\n4           i  &lt;= note that this index doesn't exist in Initial so won't be matched\n</code></pre>\n\n<p>I would like to produce the following result</p>\n\n<pre><code>    Initial Comparison\n0        a           e\n1        b           f\n2        c           g\n3        d           h\n</code></pre>\n\n<p>Using <code>merge</code>, <code>concat</code> or <code>join</code>, I only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with <code>NaN</code>.</p>\n\n<p>What's the most pandas-pythonic way of achieving this?</p>\n\n<hr>\n\n<p>Below an example from the proposed duplicate solution:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\ndf1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])\nprint df1\ndf2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])\nprint df2\ndf3 = pd.DataFrame(np.array([[2,'g'],[3,'h'],[4,'i']]), columns=['','Compare'])\ndf3 = df3.set_index('')\nprint df3\nprint df1.merge(df2,left_index=True,right_index=True).merge(df3,left_index=True,right_index=True)\n&gt;&gt;\n      Initial\n0       a\n1       b\n2       c\n3       d\n  Compare\n0       e\n1       f\n  Compare\n\n2       g\n3       h\n4       i\nEmpty DataFrame\nColumns: [Initial, Compare_x, Compare_y]\nIndex: []\n</code></pre>\n\n<hr>\n\n<p><strong>Second edit:</strong> @W-B, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. It also requires turning off warnings, which might be dangerous...</p>\n\n<pre><code>pd.options.mode.chained_assignment = None\ndf1[\"Compare\"]=pd.Series()\nfor ind in df1.index.values:\n    if ind in df2.index.values:\n        df1[\"Compare\"][ind]=df2.T[ind][\"Compare\"]\n    if ind in df3.index.values:\n        df1[\"Compare\"][ind]=df3.T[ind][\"Compare\"]\nprint df1\n&gt;&gt;\n      Initial Compare\n0           a       e\n1           b       f\n2           c       g\n3           d       h\n</code></pre>\n",
        "formatted_input": {
            "qid": 54407275,
            "link": "https://stackoverflow.com/questions/54407275/add-column-to-dataframe-in-a-loop",
            "question": {
                "title": "Add column to DataFrame in a loop",
                "ques_desc": "Let's say I have a very simple pandas dataframe, containing a single indexed column with \"initial values\". I want to read in a loop N other dataframes to fill a single \"comparison\" column, with matching indices. For instance, with my inital dataframe as and the following two dataframes to read in a loop I would like to produce the following result Using , or , I only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with . What's the most pandas-pythonic way of achieving this? Below an example from the proposed duplicate solution: Second edit: @W-B, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. It also requires turning off warnings, which might be dangerous... "
            },
            "io": [
                "   Initial\n0        a\n1        b\n2        c\n3        d\n",
                "    Initial Comparison\n0        a           e\n1        b           f\n2        c           g\n3        d           h\n"
            ],
            "answer": {
                "ans_desc": "Ok , since Op need more info Data input Solution ",
                "code": [
                    "import functools\ndf1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])\ndf1['Compare']=np.nan\ndf2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])\ndf3 = pd.DataFrame(np.array(['g','h','i']), columns=['Compare'],index=[2,3,4])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 450,
            "user_id": 5530626,
            "user_type": "registered",
            "accept_rate": 88,
            "profile_image": "https://www.gravatar.com/avatar/94e8f8cf3148b10f80793caf11fa568b?s=128&d=identicon&r=PG&f=1",
            "display_name": "jvalenti",
            "link": "https://stackoverflow.com/users/5530626/jvalenti"
        },
        "is_answered": true,
        "view_count": 1106,
        "accepted_answer_id": 54372960,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1548451427,
        "creation_date": 1548448198,
        "last_edit_date": 1548448870,
        "question_id": 54372352,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54372352/python-pandas-calculate-percentage-change-using-last-non-na-value",
        "title": "python pandas - calculate percentage change using last non-na value",
        "body": "<p>I am pretty new to python (mostly I use R) and I would like to perform a simple calculation but keep getting errors and incorrect results. I would like to calculate the percentage change for a column in a pandas df using the latest non-na value.  A toy example is below.</p>\n\n<pre><code>price = ['Nan', 10, 13, 'NaN', 'NaN', 9]\ndf = pd.DataFrame(price, columns = ['price'])\ndf['price_chg'] = df.price.pct_change(periods = -1)\n</code></pre>\n\n<p>I keep getting a weird result:</p>\n\n<pre><code>price_chg = [Nan, -0.2307, 0, 0, 0.4444, NaN] \n</code></pre>\n\n<p>I guess this has to do with the Nan values.  How do I tell python to use the latest non-na value.  The desired result is as follows:</p>\n\n<pre><code>price_chg = [Nan, -0.2307, 0.4444, 0, 0, NaN]\n</code></pre>\n\n<p>Since I don't know very much python at all, any suggestions would be welcome, even more convoluted ones.</p>\n",
        "answer_body": "<p>I believe what you're looking for is to employ backfill when calling the <code>pct_change</code> function.</p>\n\n<p><code>df['price_chg'] = df.price.pct_change(periods = -1, fill_method='backfill')</code></p>\n\n<p>This results in:</p>\n\n<pre><code>1   -0.230769\n2    0.444444\n3    0.000000\n4    0.000000\n5         NaN\n</code></pre>\n\n<p><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pct_change.html#pandas.DataFrame.pct_change\" rel=\"nofollow noreferrer\">This page</a> describes the options you have when calling <code>pct_change</code>, including the <code>fill_method</code>.\nYou can learn more about the fill methods available in pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#filling-missing-values-fillna\" rel=\"nofollow noreferrer\">here</a></p>\n",
        "question_body": "<p>I am pretty new to python (mostly I use R) and I would like to perform a simple calculation but keep getting errors and incorrect results. I would like to calculate the percentage change for a column in a pandas df using the latest non-na value.  A toy example is below.</p>\n\n<pre><code>price = ['Nan', 10, 13, 'NaN', 'NaN', 9]\ndf = pd.DataFrame(price, columns = ['price'])\ndf['price_chg'] = df.price.pct_change(periods = -1)\n</code></pre>\n\n<p>I keep getting a weird result:</p>\n\n<pre><code>price_chg = [Nan, -0.2307, 0, 0, 0.4444, NaN] \n</code></pre>\n\n<p>I guess this has to do with the Nan values.  How do I tell python to use the latest non-na value.  The desired result is as follows:</p>\n\n<pre><code>price_chg = [Nan, -0.2307, 0.4444, 0, 0, NaN]\n</code></pre>\n\n<p>Since I don't know very much python at all, any suggestions would be welcome, even more convoluted ones.</p>\n",
        "formatted_input": {
            "qid": 54372352,
            "link": "https://stackoverflow.com/questions/54372352/python-pandas-calculate-percentage-change-using-last-non-na-value",
            "question": {
                "title": "python pandas - calculate percentage change using last non-na value",
                "ques_desc": "I am pretty new to python (mostly I use R) and I would like to perform a simple calculation but keep getting errors and incorrect results. I would like to calculate the percentage change for a column in a pandas df using the latest non-na value. A toy example is below. I keep getting a weird result: I guess this has to do with the Nan values. How do I tell python to use the latest non-na value. The desired result is as follows: Since I don't know very much python at all, any suggestions would be welcome, even more convoluted ones. "
            },
            "io": [
                "price_chg = [Nan, -0.2307, 0, 0, 0.4444, NaN] \n",
                "price_chg = [Nan, -0.2307, 0.4444, 0, 0, NaN]\n"
            ],
            "answer": {
                "ans_desc": "I believe what you're looking for is to employ backfill when calling the function. This results in: This page describes the options you have when calling , including the . You can learn more about the fill methods available in pandas here ",
                "code": [
                    "df['price_chg'] = df.price.pct_change(periods = -1, fill_method='backfill')"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 725,
            "user_id": 10429891,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-M473SUk4uyk/AAAAAAAAAAI/AAAAAAAAABY/uuxCZRmisp4/photo.jpg?sz=128",
            "display_name": "Adrian",
            "link": "https://stackoverflow.com/users/10429891/adrian"
        },
        "is_answered": true,
        "view_count": 820,
        "accepted_answer_id": 54333779,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1548269892,
        "creation_date": 1548268573,
        "last_edit_date": 1548269612,
        "question_id": 54333620,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54333620/select-columns-in-pandas-dataframe-that-have-an-integer-header",
        "title": "Select columns in panda&#39;s dataframe that have an integer header",
        "body": "<p>I have a dataframe in pandas that looks like this:</p>\n\n<pre><code>   100  200  300  400\n0    1    1    0    1\n1    1    1    1    0\n</code></pre>\n\n<p>What I want to do is select specific columns from this data frame. But when I try the following code (the df_matrix being the dataframe displayed at the top) :</p>\n\n<pre><code>intermediary_df = df_matrix[\"100\"]\n</code></pre>\n\n<p>It does not work and from what I can tell is because it is an integer. I tried to force it with str(100) but gave the same error as before:</p>\n\n<pre><code>File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 958, in pandas._libs.hashtable.Int64HashTable.get_item\nTypeError: an integer is required\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3078, in get_loc\n    return self._engine.get_loc(key)\n  File \"pandas\\_libs\\index.pyx\", line 140, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\_libs\\index.pyx\", line 164, in pandas._libs.index.IndexEngine.get_loc\nKeyError: '100'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"pandas\\_libs\\index.pyx\", line 162, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 958, in pandas._libs.hashtable.Int64HashTable.get_item\nTypeError: an integer is required\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"A:/python project/testing/testing4.py\", line 42, in &lt;module&gt;\n    intermediary_df = df_matrix[\"100\"]\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\frame.py\", line 2688, in __getitem__\n    return self._getitem_column(key)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\frame.py\", line 2695, in _getitem_column\n    return self._get_item_cache(key)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 2489, in _get_item_cache\n    values = self._data.get(item)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\internals.py\", line 4115, in get\n    loc = self.items.get_loc(item)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3080, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n  File \"pandas\\_libs\\index.pyx\", line 140, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\_libs\\index.pyx\", line 164, in pandas._libs.index.IndexEngine.get_loc\nKeyError: '100'\n</code></pre>\n\n<p>Does anyone know how to get around this? Thanks!</p>\n\n<p><strong>EDIT 1:</strong></p>\n\n<p>After trying to use <code>intermediary_df = df_matrix[100]</code> it worked as expecte. Btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use:</p>\n\n<pre><code>intermediary_df = df_matrix[[100, 300]]\n</code></pre>\n\n<p>and the output will be:</p>\n\n<pre><code>   100  300\n0    1    0\n1    1    1\n</code></pre>\n",
        "answer_body": "<p>Simply use below as in this case as your columns are <code>int</code> . </p>\n\n<pre><code>intermediary_df = df_matrix[100]`\n</code></pre>\n\n<p>If you want your columns to be accessed as <code>str</code>, Use:</p>\n\n<p><code>df.columns = [str(x) for x in df.columns]</code></p>\n\n<p>and then </p>\n\n<p><code>df['100']</code></p>\n\n<p>Output</p>\n\n<pre><code>0    1\n1    1\nName: 100, dtype: int64\n</code></pre>\n",
        "question_body": "<p>I have a dataframe in pandas that looks like this:</p>\n\n<pre><code>   100  200  300  400\n0    1    1    0    1\n1    1    1    1    0\n</code></pre>\n\n<p>What I want to do is select specific columns from this data frame. But when I try the following code (the df_matrix being the dataframe displayed at the top) :</p>\n\n<pre><code>intermediary_df = df_matrix[\"100\"]\n</code></pre>\n\n<p>It does not work and from what I can tell is because it is an integer. I tried to force it with str(100) but gave the same error as before:</p>\n\n<pre><code>File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 958, in pandas._libs.hashtable.Int64HashTable.get_item\nTypeError: an integer is required\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3078, in get_loc\n    return self._engine.get_loc(key)\n  File \"pandas\\_libs\\index.pyx\", line 140, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\_libs\\index.pyx\", line 164, in pandas._libs.index.IndexEngine.get_loc\nKeyError: '100'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"pandas\\_libs\\index.pyx\", line 162, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 958, in pandas._libs.hashtable.Int64HashTable.get_item\nTypeError: an integer is required\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"A:/python project/testing/testing4.py\", line 42, in &lt;module&gt;\n    intermediary_df = df_matrix[\"100\"]\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\frame.py\", line 2688, in __getitem__\n    return self._getitem_column(key)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\frame.py\", line 2695, in _getitem_column\n    return self._get_item_cache(key)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 2489, in _get_item_cache\n    values = self._data.get(item)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\internals.py\", line 4115, in get\n    loc = self.items.get_loc(item)\n  File \"A:\\python project\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3080, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n  File \"pandas\\_libs\\index.pyx\", line 140, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\_libs\\index.pyx\", line 164, in pandas._libs.index.IndexEngine.get_loc\nKeyError: '100'\n</code></pre>\n\n<p>Does anyone know how to get around this? Thanks!</p>\n\n<p><strong>EDIT 1:</strong></p>\n\n<p>After trying to use <code>intermediary_df = df_matrix[100]</code> it worked as expecte. Btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use:</p>\n\n<pre><code>intermediary_df = df_matrix[[100, 300]]\n</code></pre>\n\n<p>and the output will be:</p>\n\n<pre><code>   100  300\n0    1    0\n1    1    1\n</code></pre>\n",
        "formatted_input": {
            "qid": 54333620,
            "link": "https://stackoverflow.com/questions/54333620/select-columns-in-pandas-dataframe-that-have-an-integer-header",
            "question": {
                "title": "Select columns in panda&#39;s dataframe that have an integer header",
                "ques_desc": "I have a dataframe in pandas that looks like this: What I want to do is select specific columns from this data frame. But when I try the following code (the df_matrix being the dataframe displayed at the top) : It does not work and from what I can tell is because it is an integer. I tried to force it with str(100) but gave the same error as before: Does anyone know how to get around this? Thanks! EDIT 1: After trying to use it worked as expecte. Btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use: and the output will be: "
            },
            "io": [
                "   100  200  300  400\n0    1    1    0    1\n1    1    1    1    0\n",
                "   100  300\n0    1    0\n1    1    1\n"
            ],
            "answer": {
                "ans_desc": "Simply use below as in this case as your columns are . If you want your columns to be accessed as , Use: and then Output ",
                "code": [
                    "df.columns = [str(x) for x in df.columns]"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 10939523,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/8545430a534510ad9c4007ab18ce90f4?s=128&d=identicon&r=PG&f=1",
            "display_name": "K. Minseok",
            "link": "https://stackoverflow.com/users/10939523/k-minseok"
        },
        "is_answered": true,
        "view_count": 411,
        "accepted_answer_id": 54285751,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1548058414,
        "creation_date": 1547960047,
        "last_edit_date": 1548055497,
        "question_id": 54273695,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54273695/how-can-i-remove-the-nan-not-removing-the-data",
        "title": "How can I remove the &#39;NaN&#39; not removing the data?",
        "body": "<p>I'm trying to remove the 'NaN'.</p>\n\n<p>In detail, there is data on one line and 'NaN'.</p>\n\n<p>My data looks like the one below.</p>\n\n<pre><code>     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     NaN  NaN   \n1   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...   0.936  0.0   \n2   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n3   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n4   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n5   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n6   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n7   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n8   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n9   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n10  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n11  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n12  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n13  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n14  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n15  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n16  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n17  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n18  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n19  0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     NaN  NaN   \n20  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...   0.936  0.0   \n21  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n22  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n23  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n24  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n25  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n26  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n27  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n28  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n29  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n\n          14         15      16   17   18        19   20   21  \n0        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n2        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n3        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n4        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n5        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n6        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n7        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n8        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n9        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n10       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n11       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n12       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n13       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n14       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n15       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n16       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n17       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n18       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n19       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n20  8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n21       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n22       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n23       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n24       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n25       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n26       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n27       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n28       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n29       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n\n[30 rows x 21 columns]\n</code></pre>\n\n<p>I want to eliminate the NAN between the data and make one data for every 18 lines.</p>\n\n<pre><code>     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 \n\n          14         15      16   17   18        19   20   21   \n0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n\n</code></pre>\n\n<p>I tried option 'dropna()' (using 'how = 'all'' or 'thread = '10'').</p>\n\n<p>But these are not what I want.</p>\n\n<p>How can I remove NaN and merge data?</p>\n\n<hr>\n\n<p>Add</p>\n\n<p>This is the code that I using(python2).</p>\n\n<pre><code>df_concat = []\nfor j in range(len(data_file)):\n    print(\"%s data_file_concat  %s %s of %s finished\" % (Driver, data_file[j], j, len(data_file)))\n    x = pd.read_csv(data_file[j])\n    if len(df_concat) != 0:\n        df_concat = [df_concat, x]\n        df_concat = pd.concat(df_concat, sort=False)\n    else:\n        df_concat = x\n    print(\"%s df_concat %s of %s finished\" %(Driver,j,len(df_concat)))\n\n\n</code></pre>\n\n<p>The <code>df_concat</code> is the data that have NaN.</p>\n\n<p>If you look at the data, there are data in the 0th line from 1 to 10, and data in the 1st line from 11th to 21st.</p>\n\n<p>That is, there are two lines of data. </p>\n\n<p>I want to wrap this in a single line without NaN.</p>\n\n<pre><code>     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 \n\n          14         15      16   17   18        19   20   21   \n0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n\n</code></pre>\n\n<p>Like this result.</p>\n\n<p>I tried to re-index the row to time to using resampling.</p>\n\n<pre><code>df_concat.index = pd.to_datetime(df_concat.index, unit='s')\ndf_concat_colums=df_concat.columns\nstart = None\nend = None\n\nfor i in range(len(df_concat[df_concat_colums[0]])):\n    if ~pd.isnull(df_concat[df_concat_colums[0]][i]):\n        if start == None:\n            start = i\n        elif end == None:\n            end = i-1\n            break\n</code></pre>\n\n<p>And I save the start and end index.</p>\n\n<pre><code>index_time = df_concat['01'].index[end] - df_concat['01'].index[start]\n</code></pre>\n\n<p>And I save the index_time to use resampling time.</p>\n\n<pre><code>df_time_merge = df_concat.resample(index_time).mean()\n</code></pre>\n\n<p>The result of 'df_time_merge' is like this.\n<a href=\"https://i.stack.imgur.com/brNCg.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n\n<p>It's working!!</p>\n\n<p>But if I have data(starting with Nan) like this, the code didn't working.</p>\n\n<p><a href=\"https://i.stack.imgur.com/IEk4v.jpg\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n\n<p>If I run same code, the  <code>start = 0</code> and  <code>end = 0</code>. </p>\n\n<p>Where did I miss?</p>\n",
        "answer_body": "<p>Does this do what you want?</p>\n\n<pre><code>def make_sample():\n    test=np.full((8,12), np.nan)\n    test[0,:6]=np.arange(6)\n    test[1,6:]=np.arange(6,18,2)\n    test[4:6,:]=2*test[:2,:]\n    return test\n\ntest=make_sample()\n\nIn [74]: test\nOut[74]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n</code></pre>\n\n<p>Create an array to identify which rows are NOT all nans</p>\n\n<pre><code>filt=1^np.isnan(test).all(axis=1)\n\nIn [78]: filt\nOut[78]: array([1, 1, 0, 0, 1, 1, 0, 0])\n</code></pre>\n\n<p>Use tat array to compress test to rows that are not all nans</p>\n\n<pre><code>compress=np.compress(filt, test, axis=0)\n\nIn [80]: compress\nOut[80]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.]])\n</code></pre>\n\n<p>Set nans to zero</p>\n\n<pre><code>compress[np.isnan(compress)]=0\n\nIn [83]: compress\nOut[83]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0., 12., 16., 20., 24., 28., 32.]])\n</code></pre>\n\n<p>Add odd to even rows</p>\n\n<pre><code>In [84]: compress[::2,:]+compress[1::2,:]\nOut[84]:  \narray([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10., 12., 16., 20., 24., 28., 32.]])\n</code></pre>\n",
        "question_body": "<p>I'm trying to remove the 'NaN'.</p>\n\n<p>In detail, there is data on one line and 'NaN'.</p>\n\n<p>My data looks like the one below.</p>\n\n<pre><code>     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     NaN  NaN   \n1   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...   0.936  0.0   \n2   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n3   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n4   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n5   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n6   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n7   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n8   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n9   NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n10  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n11  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n12  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n13  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n14  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n15  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n16  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n17  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n18  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n19  0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     NaN  NaN   \n20  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...   0.936  0.0   \n21  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n22  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n23  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n24  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n25  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n26  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n27  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n28  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n29  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN   NaN ...     NaN  NaN   \n\n          14         15      16   17   18        19   20   21  \n0        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n2        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n3        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n4        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n5        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n6        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n7        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n8        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n9        NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n10       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n11       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n12       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n13       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n14       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n15       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n16       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n17       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n18       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n19       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n20  8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n21       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n22       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n23       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n24       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n25       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n26       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n27       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n28       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n29       NaN        NaN     NaN  NaN  NaN       NaN  NaN  NaN  \n\n[30 rows x 21 columns]\n</code></pre>\n\n<p>I want to eliminate the NAN between the data and make one data for every 18 lines.</p>\n\n<pre><code>     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 \n\n          14         15      16   17   18        19   20   21   \n0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n\n</code></pre>\n\n<p>I tried option 'dropna()' (using 'how = 'all'' or 'thread = '10'').</p>\n\n<p>But these are not what I want.</p>\n\n<p>How can I remove NaN and merge data?</p>\n\n<hr>\n\n<p>Add</p>\n\n<p>This is the code that I using(python2).</p>\n\n<pre><code>df_concat = []\nfor j in range(len(data_file)):\n    print(\"%s data_file_concat  %s %s of %s finished\" % (Driver, data_file[j], j, len(data_file)))\n    x = pd.read_csv(data_file[j])\n    if len(df_concat) != 0:\n        df_concat = [df_concat, x]\n        df_concat = pd.concat(df_concat, sort=False)\n    else:\n        df_concat = x\n    print(\"%s df_concat %s of %s finished\" %(Driver,j,len(df_concat)))\n\n\n</code></pre>\n\n<p>The <code>df_concat</code> is the data that have NaN.</p>\n\n<p>If you look at the data, there are data in the 0th line from 1 to 10, and data in the 1st line from 11th to 21st.</p>\n\n<p>That is, there are two lines of data. </p>\n\n<p>I want to wrap this in a single line without NaN.</p>\n\n<pre><code>     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 \n\n          14         15      16   17   18        19   20   21   \n0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n\n</code></pre>\n\n<p>Like this result.</p>\n\n<p>I tried to re-index the row to time to using resampling.</p>\n\n<pre><code>df_concat.index = pd.to_datetime(df_concat.index, unit='s')\ndf_concat_colums=df_concat.columns\nstart = None\nend = None\n\nfor i in range(len(df_concat[df_concat_colums[0]])):\n    if ~pd.isnull(df_concat[df_concat_colums[0]][i]):\n        if start == None:\n            start = i\n        elif end == None:\n            end = i-1\n            break\n</code></pre>\n\n<p>And I save the start and end index.</p>\n\n<pre><code>index_time = df_concat['01'].index[end] - df_concat['01'].index[start]\n</code></pre>\n\n<p>And I save the index_time to use resampling time.</p>\n\n<pre><code>df_time_merge = df_concat.resample(index_time).mean()\n</code></pre>\n\n<p>The result of 'df_time_merge' is like this.\n<a href=\"https://i.stack.imgur.com/brNCg.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n\n<p>It's working!!</p>\n\n<p>But if I have data(starting with Nan) like this, the code didn't working.</p>\n\n<p><a href=\"https://i.stack.imgur.com/IEk4v.jpg\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n\n<p>If I run same code, the  <code>start = 0</code> and  <code>end = 0</code>. </p>\n\n<p>Where did I miss?</p>\n",
        "formatted_input": {
            "qid": 54273695,
            "link": "https://stackoverflow.com/questions/54273695/how-can-i-remove-the-nan-not-removing-the-data",
            "question": {
                "title": "How can I remove the &#39;NaN&#39; not removing the data?",
                "ques_desc": "I'm trying to remove the 'NaN'. In detail, there is data on one line and 'NaN'. My data looks like the one below. I want to eliminate the NAN between the data and make one data for every 18 lines. I tried option 'dropna()' (using 'how = 'all'' or 'thread = '10''). But these are not what I want. How can I remove NaN and merge data? Add This is the code that I using(python2). The is the data that have NaN. If you look at the data, there are data in the 0th line from 1 to 10, and data in the 1st line from 11th to 21st. That is, there are two lines of data. I want to wrap this in a single line without NaN. Like this result. I tried to re-index the row to time to using resampling. And I save the start and end index. And I save the index_time to use resampling time. The result of 'df_time_merge' is like this. enter image description here It's working!! But if I have data(starting with Nan) like this, the code didn't working. enter image description here If I run same code, the and . Where did I miss? "
            },
            "io": [
                "     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 \n\n          14         15      16   17   18        19   20   21   \n0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n\n",
                "     01   02   03   04   05   06     07     08   09    10 ...      12   13  \\\n0   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0\n1   0.0  0.0  0.0  0.0  0.0  0.0  132.0  321.0  0.0  31.0 ...     0.936  0.0 \n\n          14         15      16   17   18        19   20   21   \n0   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n1   8.984375  15.234375  646.25  0.0  0.0  9.765625  0.0  0.0  \n\n"
            ],
            "answer": {
                "ans_desc": "Does this do what you want? Create an array to identify which rows are NOT all nans Use tat array to compress test to rows that are not all nans Set nans to zero Add odd to even rows ",
                "code": [
                    "def make_sample():\n    test=np.full((8,12), np.nan)\n    test[0,:6]=np.arange(6)\n    test[1,6:]=np.arange(6,18,2)\n    test[4:6,:]=2*test[:2,:]\n    return test\n\ntest=make_sample()\n\nIn [74]: test\nOut[74]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
                    "filt=1^np.isnan(test).all(axis=1)\n\nIn [78]: filt\nOut[78]: array([1, 1, 0, 0, 1, 1, 0, 0])\n",
                    "compress=np.compress(filt, test, axis=0)\n\nIn [80]: compress\nOut[80]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.]])\n",
                    "compress[np.isnan(compress)]=0\n\nIn [83]: compress\nOut[83]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0., 12., 16., 20., 24., 28., 32.]])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "string",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 180,
        "closed_date": 1546513484,
        "accepted_answer_id": 54020908,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1547976293,
        "creation_date": 1546512712,
        "question_id": 54020869,
        "link": "https://stackoverflow.com/questions/54020869/how-to-delete-continuous-four-digits-from-a-column-value-in-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "How to delete continuous four digits from a column value in pandas dataframe",
        "body": "<p>I have a data frame like this:</p>\n\n<pre><code>col1         col2                col3\n A        12134 tea2014           2\n B        2013 coffee 1           1\n C        green 2015 tea          4\n</code></pre>\n\n<p>I want to remove where the digits occurring for exact four times</p>\n\n<p>The result will look like:</p>\n\n<pre><code> col1         col2                col3\n A        12134 tea                 2\n B         coffee 1                 1\n C        green tea                 4\n</code></pre>\n\n<p>What is the best way to do it using python </p>\n",
        "answer_body": "<p>You will need <code>str.replace</code> with a carefully applied regex pattern:</p>\n\n<pre><code># Thanks to @WiktorStribi\u017cew for the improvement!\ndf['col2'] = df['col2'].str.replace(r'(?&lt;!\\d)\\d{4}(?!\\d)', '')\ndf\n\n  col1        col2  col3\n0    A   12134 tea     2\n1    B    coffee 1     1\n2    C  green  tea     4\n</code></pre>\n\n<p><strong>Regex Breakdown</strong><br>\nThe pattern <code>(?&lt;!\\d)\\d{4}(?!\\d)</code> will look for exactly 4 digits that are not preceeded by digits before or after (so strings of less/more than 4 digits are left alone).</p>\n\n<pre><code>(\n    ?&lt;!   # negative lookbehind \n    \\d    # any single digit\n)\n\\d{4}     # match exactly 4 digits\n(\n    ?!    # negative lookahead\n    \\d\n)\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this:</p>\n\n<pre><code>col1         col2                col3\n A        12134 tea2014           2\n B        2013 coffee 1           1\n C        green 2015 tea          4\n</code></pre>\n\n<p>I want to remove where the digits occurring for exact four times</p>\n\n<p>The result will look like:</p>\n\n<pre><code> col1         col2                col3\n A        12134 tea                 2\n B         coffee 1                 1\n C        green tea                 4\n</code></pre>\n\n<p>What is the best way to do it using python </p>\n",
        "formatted_input": {
            "qid": 54020869,
            "link": "https://stackoverflow.com/questions/54020869/how-to-delete-continuous-four-digits-from-a-column-value-in-pandas-dataframe",
            "question": {
                "title": "How to delete continuous four digits from a column value in pandas dataframe",
                "ques_desc": "I have a data frame like this: I want to remove where the digits occurring for exact four times The result will look like: What is the best way to do it using python "
            },
            "io": [
                "col1         col2                col3\n A        12134 tea2014           2\n B        2013 coffee 1           1\n C        green 2015 tea          4\n",
                " col1         col2                col3\n A        12134 tea                 2\n B         coffee 1                 1\n C        green tea                 4\n"
            ],
            "answer": {
                "ans_desc": "You will need with a carefully applied regex pattern: Regex Breakdown The pattern will look for exactly 4 digits that are not preceeded by digits before or after (so strings of less/more than 4 digits are left alone). ",
                "code": [
                    "(\n    ?<!   # negative lookbehind \n    \\d    # any single digit\n)\n\\d{4}     # match exactly 4 digits\n(\n    ?!    # negative lookahead\n    \\d\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 1635,
            "user_id": 8378885,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://graph.facebook.com/10159144384065441/picture?type=large",
            "display_name": "HelloToEarth",
            "link": "https://stackoverflow.com/users/8378885/hellotoearth"
        },
        "is_answered": true,
        "view_count": 1251,
        "accepted_answer_id": 54219138,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1547655130,
        "creation_date": 1547648467,
        "question_id": 54219106,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54219106/dropping-duplicate-rows-but-keeping-certain-values-pandas",
        "title": "Dropping duplicate rows but keeping certain values Pandas",
        "body": "<p>I have 2 similar dataframes that I concatenated that have a lot of repeated values because they are basically the same data set but for different years.</p>\n\n<p>The problem is that one of the sets has some values missing whereas the other sometimes has these values.</p>\n\n<p>For example:</p>\n\n<pre><code>Name        Unit       Year      Level\nNik         1          2000      12\nNik         1                    12\nJohn        2          2001      11\nJohn        2          2001      11\nStacy       1                    8\nStacy       1          1999      8\n.\n.\n</code></pre>\n\n<p>I want to drop duplicates on the <code>subset = ['Name', 'Unit', 'Level']</code> since some repetitions don't have years.</p>\n\n<p>However, I'm left with the data that has no <code>Year</code> and I'd like to keep the data with these values:</p>\n\n<pre><code>Name        Unit       Year      Level\nNik         1          2000      12\nJohn        2          2001      11\nStacy       1          1999      8\n.\n.\n</code></pre>\n\n<p>How do I keep these values rather than the blanks?</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> with default parameter <code>na_position='last'</code>, so should be omit, and then  <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html\" rel=\"nofollow noreferrer\"><code>drop_duplicates</code></a>:</p>\n\n<pre><code>print (df)\n    Name  Unit    Year  Level\n0    Nik     1     NaN     12\n1    Nik     1  2000.0     12\n2   John     2  2001.0     11\n3   John     2  2001.0     11\n4  Stacy     1     NaN      8\n5  Stacy     1  1999.0      8\n\nsubset = ['Name', 'Unit', 'Level']\ndf = df.sort_values('Year').drop_duplicates(subset)\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df = df.sort_values(subset + ['Year']).drop_duplicates(subset)\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n    Name  Unit    Year  Level\n5  Stacy     1  1999.0      8\n1    Nik     1  2000.0     12\n2   John     2  2001.0     11\n</code></pre>\n\n<p>Another solution with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.first.html\" rel=\"nofollow noreferrer\"><code>GroupBy.first</code></a> for return first non missing value of <code>Year</code> per groups:</p>\n\n<pre><code>df = df.groupby(subset, as_index=False, sort=False)['Year'].first()\nprint (df)\n    Name  Unit  Level    Year\n0    Nik     1     12  2000.0\n1   John     2     11  2001.0\n2  Stacy     1      8  1999.0\n</code></pre>\n",
        "question_body": "<p>I have 2 similar dataframes that I concatenated that have a lot of repeated values because they are basically the same data set but for different years.</p>\n\n<p>The problem is that one of the sets has some values missing whereas the other sometimes has these values.</p>\n\n<p>For example:</p>\n\n<pre><code>Name        Unit       Year      Level\nNik         1          2000      12\nNik         1                    12\nJohn        2          2001      11\nJohn        2          2001      11\nStacy       1                    8\nStacy       1          1999      8\n.\n.\n</code></pre>\n\n<p>I want to drop duplicates on the <code>subset = ['Name', 'Unit', 'Level']</code> since some repetitions don't have years.</p>\n\n<p>However, I'm left with the data that has no <code>Year</code> and I'd like to keep the data with these values:</p>\n\n<pre><code>Name        Unit       Year      Level\nNik         1          2000      12\nJohn        2          2001      11\nStacy       1          1999      8\n.\n.\n</code></pre>\n\n<p>How do I keep these values rather than the blanks?</p>\n",
        "formatted_input": {
            "qid": 54219106,
            "link": "https://stackoverflow.com/questions/54219106/dropping-duplicate-rows-but-keeping-certain-values-pandas",
            "question": {
                "title": "Dropping duplicate rows but keeping certain values Pandas",
                "ques_desc": "I have 2 similar dataframes that I concatenated that have a lot of repeated values because they are basically the same data set but for different years. The problem is that one of the sets has some values missing whereas the other sometimes has these values. For example: I want to drop duplicates on the since some repetitions don't have years. However, I'm left with the data that has no and I'd like to keep the data with these values: How do I keep these values rather than the blanks? "
            },
            "io": [
                "Name        Unit       Year      Level\nNik         1          2000      12\nNik         1                    12\nJohn        2          2001      11\nJohn        2          2001      11\nStacy       1                    8\nStacy       1          1999      8\n.\n.\n",
                "Name        Unit       Year      Level\nNik         1          2000      12\nJohn        2          2001      11\nStacy       1          1999      8\n.\n.\n"
            ],
            "answer": {
                "ans_desc": "Use with default parameter , so should be omit, and then : Or: Another solution with for return first non missing value of per groups: ",
                "code": [
                    "df = df.sort_values(subset + ['Year']).drop_duplicates(subset)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1781,
            "user_id": 11629296,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-V4c9GiE5HOQ/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-h5CRxB_JuauSyR0IiZyNUA9jo0TQ/mo/photo.jpg?sz=128",
            "display_name": "Kallol",
            "link": "https://stackoverflow.com/users/11629296/kallol"
        },
        "is_answered": true,
        "view_count": 2800,
        "accepted_answer_id": 54105485,
        "answer_count": 1,
        "score": 6,
        "last_activity_date": 1547021115,
        "creation_date": 1547020335,
        "question_id": 54105419,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54105419/add-numbers-with-duplicate-values-for-columns-in-pandas",
        "title": "Add numbers with duplicate values for columns in pandas",
        "body": "<p>I have a data frame like this:</p>\n\n<pre><code>df:\ncol1     col2\n 1        pqr\n 3        abc\n 2        pqr\n 4        xyz\n 1        pqr\n</code></pre>\n\n<p>I found that there is duplicate value and its pqr. I want to add 1,2,3 where pqr occurs. The final data frame I want to achieve is:</p>\n\n<pre><code>df1\ncol1      col2\n 1        pqr1\n 3        abc\n 2        pqr2\n 4        xyz\n 1        pqr3\n</code></pre>\n\n<p>How to do it in efficient way</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.duplicated.html\" rel=\"noreferrer\"><code>duplicated</code></a> with <code>keep=False</code> for all dupe rows and add counter created by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.cumcount.html\" rel=\"noreferrer\"><code>cumcount</code></a>:</p>\n\n<pre><code>mask = df['col2'].duplicated(keep=False)\ndf.loc[mask, 'col2'] += df.groupby('col2').cumcount().add(1).astype(str)\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df['col2'] = np.where(df['col2'].duplicated(keep=False), \n                      df['col2'] + df.groupby('col2').cumcount().add(1).astype(str),\n                      df['col2'])\nprint (df)\n   col1  col2\n0     1  pqr1\n1     3   abc\n2     2  pqr2\n3     4   xyz\n4     1  pqr3\n</code></pre>\n\n<p>If need same only for <code>pqr</code> values:</p>\n\n<pre><code>mask = df['col2'] == 'pqr'\ndf.loc[mask, 'col2'] += pd.Series(np.arange(1, mask.sum() + 1),\n                                  index=df.index[mask]).astype(str)\nprint (df)\n   col1  col2\n0     1  pqr1\n1     3   abc\n2     2  pqr2\n3     4   xyz\n4     1  pqr3\n</code></pre>\n",
        "question_body": "<p>I have a data frame like this:</p>\n\n<pre><code>df:\ncol1     col2\n 1        pqr\n 3        abc\n 2        pqr\n 4        xyz\n 1        pqr\n</code></pre>\n\n<p>I found that there is duplicate value and its pqr. I want to add 1,2,3 where pqr occurs. The final data frame I want to achieve is:</p>\n\n<pre><code>df1\ncol1      col2\n 1        pqr1\n 3        abc\n 2        pqr2\n 4        xyz\n 1        pqr3\n</code></pre>\n\n<p>How to do it in efficient way</p>\n",
        "formatted_input": {
            "qid": 54105419,
            "link": "https://stackoverflow.com/questions/54105419/add-numbers-with-duplicate-values-for-columns-in-pandas",
            "question": {
                "title": "Add numbers with duplicate values for columns in pandas",
                "ques_desc": "I have a data frame like this: I found that there is duplicate value and its pqr. I want to add 1,2,3 where pqr occurs. The final data frame I want to achieve is: How to do it in efficient way "
            },
            "io": [
                "df:\ncol1     col2\n 1        pqr\n 3        abc\n 2        pqr\n 4        xyz\n 1        pqr\n",
                "df1\ncol1      col2\n 1        pqr1\n 3        abc\n 2        pqr2\n 4        xyz\n 1        pqr3\n"
            ],
            "answer": {
                "ans_desc": "Use with for all dupe rows and add counter created by : Or: If need same only for values: ",
                "code": [
                    "mask = df['col2'].duplicated(keep=False)\ndf.loc[mask, 'col2'] += df.groupby('col2').cumcount().add(1).astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "mask"
        ],
        "owner": {
            "reputation": 3160,
            "user_id": 3939696,
            "user_type": "registered",
            "accept_rate": 66,
            "profile_image": "https://www.gravatar.com/avatar/a11d4a263e62bc8659a6d0bbe9d26afb?s=128&d=identicon&r=PG&f=1",
            "display_name": "YJZ",
            "link": "https://stackoverflow.com/users/3939696/yjz"
        },
        "is_answered": true,
        "view_count": 84,
        "accepted_answer_id": 54059477,
        "answer_count": 5,
        "score": 2,
        "last_activity_date": 1546766329,
        "creation_date": 1546759289,
        "last_edit_date": 1546766329,
        "question_id": 54059466,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54059466/report-difference-change-in-values-between-two-dataframes-of-identical-shape",
        "title": "Report difference/change in values between two dataFrames of identical shape",
        "body": "<p>The context is I want to compare two df's and find the difference.</p>\n\n<p>Here's df and df2 with a small difference:</p>\n\n<pre><code>df = pd.DataFrame({'a': range(3),\n                   'b': range(3)})\n\ndf2 = df.copy()\ndf2.iloc[1,1] = 100\n</code></pre>\n\n<p>Comparing them yields a 2D boolean df of the same shape:</p>\n\n<pre><code>df != df2\nOut[28]: \n       a      b\n0  False  False\n1  False   True\n2  False  False\n</code></pre>\n\n<p>I tried to extract the elements corresponding to the True's, but other elements (that I don't want) still occurs as NaN </p>\n\n<pre><code>df[df != df2]\nOut[29]: \n    a    b\n0 NaN  NaN\n1 NaN  1.0\n2 NaN  NaN\n</code></pre>\n\n<p>How to extract only the elements corresponding to the True's and the indices (so I know where in the df):</p>\n\n<pre><code>df[df != df2] # somehow?\nOut[30]: \n    b\n1 1.0\n</code></pre>\n\n<hr>\n\n<p>update: the above example has only one True. In a general situation with multiple True's, I think are two cases:</p>\n\n<ol>\n<li><p>df is small and one may want to see:</p>\n\n<pre><code>df = pd.DataFrame({'a': range(3),\n                   'b': range(3)})\n\ndf2 = df.copy()\ndf2.iloc[0,0] = 100\ndf2.iloc[1,1] = 100\n\ndf[df!=df2].dropna(how='all',axis=(0,1)) # U9-Forward's answer\nOut[39]: \n     a    b\n0  0.0  NaN\n1  NaN  1.0\n</code></pre></li>\n<li><p>df is large and one may want to see:</p>\n\n<pre><code>index    column   df_value     df2_value\n    0         a        0.0           100\n    1         b        1.0           100\n</code></pre></li>\n</ol>\n\n<p>@U9-Forward's answer works nicely for case 1, and when there's only one True.</p>\n\n<p>@coldspeed provided a comprehensive solution. Thanks!</p>\n",
        "answer_body": "<p>Mask on the values:</p>\n\n<pre><code>df.values[df != df2]\n# array([1])\n</code></pre>\n\n<hr>\n\n<p>How should this case be handled?</p>\n\n<pre><code>df2.at[0, 'a'] = 100\n\ndf\n   a  b\n0  0  0\n1  1  1\n2  2  2\n\ndf2\n     a    b\n0  100    0\n1    1  100\n2    2    2\n\ndf != df2 \n       a      b\n0   True  False\n1  False   True\n2  False  False\n\ndf.values[df != df2]\n# array([0, 1])\n\n# in the other answer\ndf[df!=df2].dropna(how='all',axis=(0,1))\n     a    b\n0  0.0  NaN\n1  NaN  1.0\n</code></pre>\n\n<p>Which is the required output?</p>\n\n<hr>\n\n<p>If you just the values in each column of <code>df</code> that differ, something simple like <code>agg</code> and <code>dropna</code> will do.</p>\n\n<pre><code>df[df != df2].agg(lambda x: x.dropna().tolist())\n\na    [0.0]\nb    [1.0]\ndtype: object\n</code></pre>\n\n<hr>\n\n<p>If you want the indices and columns, use <code>melt</code>:</p>\n\n<pre><code>u = df2.reset_index().melt('index')\nv = df.reset_index().melt('index')\n\nu[u['value'] != v['value']]\n   index variable  value\n0      0        a    100\n4      1        b    100\n</code></pre>\n\n<hr>\n\n<p>Or, use <code>np.nonzero</code>, to do this with numpy - True values are nonzero, the indices of these are returned.</p>\n\n<pre><code>m = (df != df2).values\nidx, cols = np.nonzero(m)\n\npd.DataFrame({\n    'index': df.index.values[idx],\n    'column': df.columns.values[cols],\n    'value_1': df.values[m],\n    'value_2': df2.values[m]\n})\n\n   index column  value_1  value_2\n0      0      a        0      100\n1      1      b        1      100\n</code></pre>\n",
        "question_body": "<p>The context is I want to compare two df's and find the difference.</p>\n\n<p>Here's df and df2 with a small difference:</p>\n\n<pre><code>df = pd.DataFrame({'a': range(3),\n                   'b': range(3)})\n\ndf2 = df.copy()\ndf2.iloc[1,1] = 100\n</code></pre>\n\n<p>Comparing them yields a 2D boolean df of the same shape:</p>\n\n<pre><code>df != df2\nOut[28]: \n       a      b\n0  False  False\n1  False   True\n2  False  False\n</code></pre>\n\n<p>I tried to extract the elements corresponding to the True's, but other elements (that I don't want) still occurs as NaN </p>\n\n<pre><code>df[df != df2]\nOut[29]: \n    a    b\n0 NaN  NaN\n1 NaN  1.0\n2 NaN  NaN\n</code></pre>\n\n<p>How to extract only the elements corresponding to the True's and the indices (so I know where in the df):</p>\n\n<pre><code>df[df != df2] # somehow?\nOut[30]: \n    b\n1 1.0\n</code></pre>\n\n<hr>\n\n<p>update: the above example has only one True. In a general situation with multiple True's, I think are two cases:</p>\n\n<ol>\n<li><p>df is small and one may want to see:</p>\n\n<pre><code>df = pd.DataFrame({'a': range(3),\n                   'b': range(3)})\n\ndf2 = df.copy()\ndf2.iloc[0,0] = 100\ndf2.iloc[1,1] = 100\n\ndf[df!=df2].dropna(how='all',axis=(0,1)) # U9-Forward's answer\nOut[39]: \n     a    b\n0  0.0  NaN\n1  NaN  1.0\n</code></pre></li>\n<li><p>df is large and one may want to see:</p>\n\n<pre><code>index    column   df_value     df2_value\n    0         a        0.0           100\n    1         b        1.0           100\n</code></pre></li>\n</ol>\n\n<p>@U9-Forward's answer works nicely for case 1, and when there's only one True.</p>\n\n<p>@coldspeed provided a comprehensive solution. Thanks!</p>\n",
        "formatted_input": {
            "qid": 54059466,
            "link": "https://stackoverflow.com/questions/54059466/report-difference-change-in-values-between-two-dataframes-of-identical-shape",
            "question": {
                "title": "Report difference/change in values between two dataFrames of identical shape",
                "ques_desc": "The context is I want to compare two df's and find the difference. Here's df and df2 with a small difference: Comparing them yields a 2D boolean df of the same shape: I tried to extract the elements corresponding to the True's, but other elements (that I don't want) still occurs as NaN How to extract only the elements corresponding to the True's and the indices (so I know where in the df): update: the above example has only one True. In a general situation with multiple True's, I think are two cases: df is small and one may want to see: df is large and one may want to see: @U9-Forward's answer works nicely for case 1, and when there's only one True. @coldspeed provided a comprehensive solution. Thanks! "
            },
            "io": [
                "df[df != df2]\nOut[29]: \n    a    b\n0 NaN  NaN\n1 NaN  1.0\n2 NaN  NaN\n",
                "df[df != df2] # somehow?\nOut[30]: \n    b\n1 1.0\n"
            ],
            "answer": {
                "ans_desc": "Mask on the values: How should this case be handled? Which is the required output? If you just the values in each column of that differ, something simple like and will do. If you want the indices and columns, use : Or, use , to do this with numpy - True values are nonzero, the indices of these are returned. ",
                "code": [
                    "df.values[df != df2]\n# array([1])\n",
                    "df[df != df2].agg(lambda x: x.dropna().tolist())\n\na    [0.0]\nb    [1.0]\ndtype: object\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "scikit-learn"
        ],
        "owner": {
            "reputation": 633,
            "user_id": 1271079,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://www.gravatar.com/avatar/a15e02b49f38872d35ae7d1b0aac9d9d?s=128&d=identicon&r=PG",
            "display_name": "taktak004",
            "link": "https://stackoverflow.com/users/1271079/taktak004"
        },
        "is_answered": true,
        "view_count": 100,
        "accepted_answer_id": 54039726,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1546608112,
        "creation_date": 1544201894,
        "last_edit_date": 1546608112,
        "question_id": 53673872,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53673872/pandas-dataframe-to-sparse-representation",
        "title": "Pandas dataframe to sparse representation",
        "body": "<p>I have a dense pandas dataframe.\nI would like to get a sparse dataframe out of it where each value of the original dataframe would be the column of a 1 in the resulting sparse dataframe.</p>\n\n<p>Example:</p>\n\n<p>Original df:</p>\n\n<pre><code>    a b\n0   5 3\n1   2 6\n</code></pre>\n\n<p>Sparse df:</p>\n\n<pre><code>(0,3): 1\n(0,5): 1\n(1,2): 1\n(1,6): 1\n</code></pre>\n\n<p>I do not care if in case of collision it is a 1 or the number of collision</p>\n\n<p>I will then pass this df to sklearn.linear_model.LogisticRegression fit function (I am not sure which kind of sparse matrix would be accepted here)</p>\n\n<p>What would be the appropriate approach ?</p>\n\n<p>I can create it by hand (iterating over the row) but the dataframe is quite big so I am trying to find an efficient way of doing it.</p>\n\n<p>Thanks</p>\n",
        "answer_body": "<p>A much faster solution than the one proposed by @Dark is to use the csr_matrix constructor but the ones will sum up in case of redundancy which is ok for my case:</p>\n\n<pre><code>nrow = len(df.index)\nncol = len(df.columns)\nindices = df.values.flatten()\ndata = np.full_like(indices, 1)\nnelement = len(indices)\nindptr = range(0, nelement+ncol, ncol)\nresult = csr_matrix((data, indices, indptr))\n</code></pre>\n",
        "question_body": "<p>I have a dense pandas dataframe.\nI would like to get a sparse dataframe out of it where each value of the original dataframe would be the column of a 1 in the resulting sparse dataframe.</p>\n\n<p>Example:</p>\n\n<p>Original df:</p>\n\n<pre><code>    a b\n0   5 3\n1   2 6\n</code></pre>\n\n<p>Sparse df:</p>\n\n<pre><code>(0,3): 1\n(0,5): 1\n(1,2): 1\n(1,6): 1\n</code></pre>\n\n<p>I do not care if in case of collision it is a 1 or the number of collision</p>\n\n<p>I will then pass this df to sklearn.linear_model.LogisticRegression fit function (I am not sure which kind of sparse matrix would be accepted here)</p>\n\n<p>What would be the appropriate approach ?</p>\n\n<p>I can create it by hand (iterating over the row) but the dataframe is quite big so I am trying to find an efficient way of doing it.</p>\n\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 53673872,
            "link": "https://stackoverflow.com/questions/53673872/pandas-dataframe-to-sparse-representation",
            "question": {
                "title": "Pandas dataframe to sparse representation",
                "ques_desc": "I have a dense pandas dataframe. I would like to get a sparse dataframe out of it where each value of the original dataframe would be the column of a 1 in the resulting sparse dataframe. Example: Original df: Sparse df: I do not care if in case of collision it is a 1 or the number of collision I will then pass this df to sklearn.linear_model.LogisticRegression fit function (I am not sure which kind of sparse matrix would be accepted here) What would be the appropriate approach ? I can create it by hand (iterating over the row) but the dataframe is quite big so I am trying to find an efficient way of doing it. Thanks "
            },
            "io": [
                "    a b\n0   5 3\n1   2 6\n",
                "(0,3): 1\n(0,5): 1\n(1,2): 1\n(1,6): 1\n"
            ],
            "answer": {
                "ans_desc": "A much faster solution than the one proposed by @Dark is to use the csr_matrix constructor but the ones will sum up in case of redundancy which is ok for my case: ",
                "code": [
                    "nrow = len(df.index)\nncol = len(df.columns)\nindices = df.values.flatten()\ndata = np.full_like(indices, 1)\nnelement = len(indices)\nindptr = range(0, nelement+ncol, ncol)\nresult = csr_matrix((data, indices, indptr))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "expand"
        ],
        "owner": {
            "reputation": 111,
            "user_id": 5835745,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2eaf56d3165072b80ce31ed2cc15c363?s=128&d=identicon&r=PG&f=1",
            "display_name": "wydy",
            "link": "https://stackoverflow.com/users/5835745/wydy"
        },
        "is_answered": true,
        "view_count": 53,
        "closed_date": 1546602272,
        "accepted_answer_id": 54037988,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1546602166,
        "creation_date": 1546600556,
        "question_id": 54037912,
        "link": "https://stackoverflow.com/questions/54037912/merge-order-with-items-in-columns",
        "closed_reason": "Duplicate",
        "title": "Merge order with items in columns",
        "body": "<p>I have a dataset with all the order, customer and orderitem information. I wandt to expand my orderitems in new columns, but without losing the information about the customer</p>\n\n<pre><code>CustomerId    OrderId    Item\n1    1    CD\n1    1    DVD\n2    2    CD\n</code></pre>\n\n<p>And the result should be somehow:</p>\n\n<pre><code>CustomerId    OrderId    CD    DVD\n1    1    1    1\n2    2    1    0\n</code></pre>\n\n<p>I tried </p>\n\n<pre><code>df2 = pd.concat([df, pd.get_dummies(df.Item)], axis='columns')\ndf2 = df2.groupby('CustomerId')\n</code></pre>\n",
        "answer_body": "<p>Simpler is <a href=\"https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.crosstab.html\" rel=\"nofollow noreferrer\"><code>crosstab</code></a>;</p>\n\n<pre><code>pd.crosstab([df.CustomerId, df.OrderId], df.Item).reset_index()\n\n   CustomerId  OrderId  CD  DVD\n0           1        1   1    1\n1           2        2   1    0\n</code></pre>\n\n<hr>\n\n<p>Or, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html\" rel=\"nofollow noreferrer\"><code>pivot_table</code></a> <strong>if performance is important</strong>.</p>\n\n<pre><code>df.pivot_table(index=['CustomerId', 'OrderId'], \n               columns=['Item'], \n               aggfunc='size', \n               fill_value=0)\n\nItem                CD  DVD\nCustomerId OrderId         \n1          1         1    1\n2          2         1    0\n</code></pre>\n\n<hr>\n\n<p>If you want to use dummies, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.get_dummies.html\" rel=\"nofollow noreferrer\"><code>str.get_dummies</code></a> is another option:</p>\n\n<pre><code># Solution similar to @jezrael but with str.get_dummies\n(df.set_index(['CustomerId', 'OrderId'])\n   .Item.str.get_dummies()\n   .sum(level=[0, 1])\n   .reset_index())\n\n   CustomerId  OrderId  CD  DVD\n0           1        1   1    1\n1           2        2   1    0\n</code></pre>\n\n<p>If you need the indicator,</p>\n\n<pre><code>(df.set_index(['CustomerId', 'OrderId'])\n   .Item.str.get_dummies()\n   .max(level=[0, 1])\n   .reset_index())\n</code></pre>\n",
        "question_body": "<p>I have a dataset with all the order, customer and orderitem information. I wandt to expand my orderitems in new columns, but without losing the information about the customer</p>\n\n<pre><code>CustomerId    OrderId    Item\n1    1    CD\n1    1    DVD\n2    2    CD\n</code></pre>\n\n<p>And the result should be somehow:</p>\n\n<pre><code>CustomerId    OrderId    CD    DVD\n1    1    1    1\n2    2    1    0\n</code></pre>\n\n<p>I tried </p>\n\n<pre><code>df2 = pd.concat([df, pd.get_dummies(df.Item)], axis='columns')\ndf2 = df2.groupby('CustomerId')\n</code></pre>\n",
        "formatted_input": {
            "qid": 54037912,
            "link": "https://stackoverflow.com/questions/54037912/merge-order-with-items-in-columns",
            "question": {
                "title": "Merge order with items in columns",
                "ques_desc": "I have a dataset with all the order, customer and orderitem information. I wandt to expand my orderitems in new columns, but without losing the information about the customer And the result should be somehow: I tried "
            },
            "io": [
                "CustomerId    OrderId    Item\n1    1    CD\n1    1    DVD\n2    2    CD\n",
                "CustomerId    OrderId    CD    DVD\n1    1    1    1\n2    2    1    0\n"
            ],
            "answer": {
                "ans_desc": "Simpler is ; Or, if performance is important. If you want to use dummies, is another option: If you need the indicator, ",
                "code": [
                    "(df.set_index(['CustomerId', 'OrderId'])\n   .Item.str.get_dummies()\n   .max(level=[0, 1])\n   .reset_index())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 79,
            "user_id": 10776762,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1aebbad3a93cba8cdb7668e4f9975541?s=128&d=identicon&r=PG&f=1",
            "display_name": "Denis",
            "link": "https://stackoverflow.com/users/10776762/denis"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 54006087,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1546432140,
        "creation_date": 1546430402,
        "last_edit_date": 1546432140,
        "question_id": 54005974,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/54005974/change-values-in-columns-of-dataframe-depending-on-values-of-other-columns-valu",
        "title": "Change values in columns of dataframe depending on values of other columns (values come from lists)",
        "body": "<p>I have a Data Frame in python, for example this one:</p>\n\n<pre><code>  col1 col2 col3 col4\n0    A    C    B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A    D    D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n</code></pre>\n\n<p>The code for the creation of the dataframe: </p>\n\n<pre><code>d = {'col1':['A','C','E','A','B','D','F','E','B'], 'col2':['C','E','A','D','B','D','F','E','B'],\n              'col3':['B','E','E','D','B','D','A','E','B'], 'col4':['D','A','A','D','B','D','F','E','B']}\ndf = pd.DataFrame(data=d)\n</code></pre>\n\n<p>Let the list1 be ['A','C','E'] and list2 be ['B','D','F'].\nWhat I want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). </p>\n\n<p>I have tried <code>df['col2'].loc[(df['col1'] in list1) &amp; (df[['col2'] in list2)]=''</code> which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. Could someone help please?</p>\n\n<p>This is my expected output:</p>\n\n<pre><code>  col1 col2 col3 col4\n0    A         B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A         D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n</code></pre>\n",
        "answer_body": "<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>pd.DataFrame.loc</code></a> is a method of <code>pd.DataFrame</code>, so use it with your dataframe, not with a series. In addition, you can test criteria on multiple series via <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.any.html\" rel=\"nofollow noreferrer\"><code>pd.DataFrame.any</code></a>:</p>\n\n<pre><code>m1 = df['col1'].isin(list1)\nm2 = df[['col2', 'col3', 'col4']].isin(list2).any(1)\n\ndf.loc[m1 &amp; m2, 'col2'] = ''\n</code></pre>\n\n<p>Result:</p>\n\n<pre><code>print(df)\n\n  col1 col2 col3 col4\n0    A         B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A         D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n</code></pre>\n",
        "question_body": "<p>I have a Data Frame in python, for example this one:</p>\n\n<pre><code>  col1 col2 col3 col4\n0    A    C    B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A    D    D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n</code></pre>\n\n<p>The code for the creation of the dataframe: </p>\n\n<pre><code>d = {'col1':['A','C','E','A','B','D','F','E','B'], 'col2':['C','E','A','D','B','D','F','E','B'],\n              'col3':['B','E','E','D','B','D','A','E','B'], 'col4':['D','A','A','D','B','D','F','E','B']}\ndf = pd.DataFrame(data=d)\n</code></pre>\n\n<p>Let the list1 be ['A','C','E'] and list2 be ['B','D','F'].\nWhat I want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). </p>\n\n<p>I have tried <code>df['col2'].loc[(df['col1'] in list1) &amp; (df[['col2'] in list2)]=''</code> which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. Could someone help please?</p>\n\n<p>This is my expected output:</p>\n\n<pre><code>  col1 col2 col3 col4\n0    A         B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A         D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n</code></pre>\n",
        "formatted_input": {
            "qid": 54005974,
            "link": "https://stackoverflow.com/questions/54005974/change-values-in-columns-of-dataframe-depending-on-values-of-other-columns-valu",
            "question": {
                "title": "Change values in columns of dataframe depending on values of other columns (values come from lists)",
                "ques_desc": "I have a Data Frame in python, for example this one: The code for the creation of the dataframe: Let the list1 be ['A','C','E'] and list2 be ['B','D','F']. What I want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). I have tried which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. Could someone help please? This is my expected output: "
            },
            "io": [
                "  col1 col2 col3 col4\n0    A    C    B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A    D    D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n",
                "  col1 col2 col3 col4\n0    A         B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A         D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n"
            ],
            "answer": {
                "ans_desc": " is a method of , so use it with your dataframe, not with a series. In addition, you can test criteria on multiple series via : Result: ",
                "code": [
                    "m1 = df['col1'].isin(list1)\nm2 = df[['col2', 'col3', 'col4']].isin(list2).any(1)\n\ndf.loc[m1 & m2, 'col2'] = ''\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "datetime",
            "dataframe"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 4079291,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a803e024af875364b2a20607241cb9fb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Shahad",
            "link": "https://stackoverflow.com/users/4079291/shahad"
        },
        "is_answered": true,
        "view_count": 212,
        "accepted_answer_id": 53977443,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1546172391,
        "creation_date": 1546166113,
        "question_id": 53976903,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53976903/find-a-shared-time-overlap-in-time-columns-in-python",
        "title": "Find a shared time (overlap) in time columns in python",
        "body": "<p>I'm trying to find a shared time where two datetime columns representing a start and an end time overlap with other records.</p>\n\n<p>for example if we have these two columns:</p>\n\n<pre><code>Start                    End \n2016-08-22 20:20:00      2016-08-22 20:30:00   \n2016-08-22 20:55:00      2016-08-22 21:53:00   \n2016-08-22 21:38:00      2016-08-22 21:58:00\n</code></pre>\n\n<p>I want to check the overlap between them, the output would be:</p>\n\n<pre><code> Start                    End                   Overlap\n2016-08-22 20:20:00      2016-08-22 20:30:00    NaN\n2016-08-22 20:55:00      2016-08-22 21:53:00   2016-08-22 21:38:00\n2016-08-22 21:38:00      2016-08-22 21:58:00   2016-08-22 21:38:00\n</code></pre>\n\n<p>is there an efficient way to achieve it? </p>\n",
        "answer_body": "<p>Here's a possible approach. You could define the following function:</p>\n\n<pre><code>def common_row(x):\n    rows = df.loc[df.index != x.name,:]\n    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() &gt; 0 for \n             y in rows.itertuples()]\n    shared = rows.index[s].values\n    if shared.size &gt; 0:\n        return df.loc[shared[0], 'Start']\n</code></pre>\n\n<p>What it does is look for other rows with time overlap and assigns the time in <code>Start</code> from the overlapping row (which will not be the current row, as your sample output suggests). </p>\n\n<p>If you apply this along <code>axis 1</code> you get:</p>\n\n<pre><code>df['Overlap'] = df.apply(lambda x: common_row(x), axis=1)\n\n         Start                 End             Overlap\n0 2016-08-22 20:20:00 2016-08-22 20:30:00                 NaT\n1 2016-08-22 20:55:00 2016-08-22 21:53:00 2016-08-22 21:38:00\n2 2016-08-22 21:38:00 2016-08-22 21:58:00 2016-08-22 20:55:00\n</code></pre>\n\n<hr>\n\n<p>If what you want is to have the index of the row with time overlapping you can instead use:</p>\n\n<pre><code>def common_row(x):\n    rows = df.loc[df.index != x.name,:]\n    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() &gt; 0 for \n             y in rows.itertuples()]\n    shared = rows.index[s].values\n    if shared.size &gt; 0:\n        return int(shared[0])\n</code></pre>\n\n<p>Which will in this case give:</p>\n\n<pre><code>df['Overlap'] = df.apply(lambda x: common_row(x), axis=1)\n          Start                 End          Overlap\n0 2016-08-22 20:20:00 2016-08-22 20:30:00      NaN\n1 2016-08-22 20:55:00 2016-08-22 21:53:00      2.0\n2 2016-08-22 21:38:00 2016-08-22 21:58:00      1.0\n</code></pre>\n",
        "question_body": "<p>I'm trying to find a shared time where two datetime columns representing a start and an end time overlap with other records.</p>\n\n<p>for example if we have these two columns:</p>\n\n<pre><code>Start                    End \n2016-08-22 20:20:00      2016-08-22 20:30:00   \n2016-08-22 20:55:00      2016-08-22 21:53:00   \n2016-08-22 21:38:00      2016-08-22 21:58:00\n</code></pre>\n\n<p>I want to check the overlap between them, the output would be:</p>\n\n<pre><code> Start                    End                   Overlap\n2016-08-22 20:20:00      2016-08-22 20:30:00    NaN\n2016-08-22 20:55:00      2016-08-22 21:53:00   2016-08-22 21:38:00\n2016-08-22 21:38:00      2016-08-22 21:58:00   2016-08-22 21:38:00\n</code></pre>\n\n<p>is there an efficient way to achieve it? </p>\n",
        "formatted_input": {
            "qid": 53976903,
            "link": "https://stackoverflow.com/questions/53976903/find-a-shared-time-overlap-in-time-columns-in-python",
            "question": {
                "title": "Find a shared time (overlap) in time columns in python",
                "ques_desc": "I'm trying to find a shared time where two datetime columns representing a start and an end time overlap with other records. for example if we have these two columns: I want to check the overlap between them, the output would be: is there an efficient way to achieve it? "
            },
            "io": [
                "Start                    End \n2016-08-22 20:20:00      2016-08-22 20:30:00   \n2016-08-22 20:55:00      2016-08-22 21:53:00   \n2016-08-22 21:38:00      2016-08-22 21:58:00\n",
                " Start                    End                   Overlap\n2016-08-22 20:20:00      2016-08-22 20:30:00    NaN\n2016-08-22 20:55:00      2016-08-22 21:53:00   2016-08-22 21:38:00\n2016-08-22 21:38:00      2016-08-22 21:58:00   2016-08-22 21:38:00\n"
            ],
            "answer": {
                "ans_desc": "Here's a possible approach. You could define the following function: What it does is look for other rows with time overlap and assigns the time in from the overlapping row (which will not be the current row, as your sample output suggests). If you apply this along you get: If what you want is to have the index of the row with time overlapping you can instead use: Which will in this case give: ",
                "code": [
                    "def common_row(x):\n    rows = df.loc[df.index != x.name,:]\n    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() > 0 for \n             y in rows.itertuples()]\n    shared = rows.index[s].values\n    if shared.size > 0:\n        return df.loc[shared[0], 'Start']\n",
                    "def common_row(x):\n    rows = df.loc[df.index != x.name,:]\n    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() > 0 for \n             y in rows.itertuples()]\n    shared = rows.index[s].values\n    if shared.size > 0:\n        return int(shared[0])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "data-structures"
        ],
        "owner": {
            "reputation": 610,
            "user_id": 10833061,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/23ba9c03b031e072e559e715589cb2c0?s=128&d=identicon&r=PG&f=1",
            "display_name": "MisterButter",
            "link": "https://stackoverflow.com/users/10833061/misterbutter"
        },
        "is_answered": true,
        "view_count": 3613,
        "accepted_answer_id": 53973967,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1546125823,
        "creation_date": 1546121260,
        "last_edit_date": 1546125088,
        "question_id": 53973711,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53973711/how-to-merge-rows-in-a-dataframe-based-on-a-column-value",
        "title": "How to merge rows in a dataframe based on a column value?",
        "body": "<p>I have a data-set that is in the shape of this, where each row represents a in a specific match that is specified by the <code>gameID</code>.</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>  gameID          Won/Lost   Home   Away  metric2 metric3 metric4   team1 team2 team3 team4\n2017020001         1          1      0      10      10      10      1     0     0      0\n2017020001         0          0      1      10      10      10      0     1     0      0\n</code></pre>\n\n<p>The thing I want to do is create a function that takes the rows with the same <code>gameID</code> and joins them. As you can see in data example below, the two rows represents one game that is split up into a home team (row_1 ) and an away team (row_2). I want these two rows to sit on one row only.</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>Won/Lost  h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4\n1            10       10         10        10         10        10      1       0        0      0         0      1        0      0\n</code></pre>\n\n<p>How do I get this result?</p>\n\n<p>EDIT: I created too much confusion, posting my code so you can get a better grasp of the problem I want to solve.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\nimport requests\nimport json\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\n\nresults = []\nfor game_id in range(2017020001, 2017020010, 1):\n    url = 'https://statsapi.web.nhl.com/api/v1/game/{}/boxscore'.format(game_id)\nr = requests.get(url)\ngame_data = r.json()\n\nfor homeaway in ['home','away']:\n\n    game_dict = game_data.get('teams').get(homeaway).get('teamStats').get('teamSkaterStats')\n    game_dict['team'] = game_data.get('teams').get(homeaway).get('team').get('name')\n    game_dict['homeaway'] = homeaway\n    game_dict['game_id'] = game_id\n    results.append(game_dict)\n\ndf = pd.DataFrame(results)\n\ndf['Won/Lost'] = df.groupby('game_id')['goals'].apply(lambda g: (g == g.max()).map({True: 1, False: 0}))\n\ndf[\"faceOffWinPercentage\"] = df[\"faceOffWinPercentage\"].astype('float')\ndf[\"powerPlayPercentage\"] = df[\"powerPlayPercentage\"].astype('float')\ndf[\"team\"] = df[\"team\"].astype('category')\ndf = pd.get_dummies(df, columns=['homeaway'])\ndf = pd.get_dummies(df, columns=['team'])\n</code></pre>\n",
        "answer_body": "<p>This is under the assumption that you have exactly two rows per <code>gameID</code> and that you want to group by that ID. (It also assumes that I understand the question.)</p>\n\n<p><strong>Improved solution</strong></p>\n\n<p>Given a dataframe <code>df</code> such as</p>\n\n<pre><code>       gameID  Won/Lost  Home  Away  metric2  metric3  metric4  team1  team2  team3  team4\n0  2017020001         1     1     0       10       10       10      1      0      0      0\n1  2017020001         0     0     1       10       10       10      0      1      0      0\n2  2017020002         1     1     0       10       10       10      1      0      0      0\n3  2017020002         0     0     1       10       10       10      0      1      0      0\n</code></pre>\n\n<p>you can use <code>pd.merge</code> (and some data munging) like this:</p>\n\n<pre><code>&gt;&gt;&gt; is_home = df['Home'] == 1                                                                                                                                                                                                                   \n&gt;&gt;&gt; home = df[is_home].drop(['Home', 'Away'], axis=1).add_prefix('h_').rename(columns={'h_gameID':'gameID'})                                                                                                                                    \n&gt;&gt;&gt; away = df[~is_home].drop(['Won/Lost', 'Home', 'Away'], axis=1).add_prefix('a_').rename(columns={'a_gameID':'gameID'})                                                                                                                       \n&gt;&gt;&gt; pd.merge(home, away, on='gameID')                                                                                                                                                                                                           \n       gameID  h_Won/Lost  h_metric2  h_metric3  h_metric4  h_team1  h_team2  h_team3  h_team4  a_metric2  a_metric3  a_metric4  a_team1  a_team2  a_team3  a_team4\n0  2017020001           1         10         10         10        1        0        0        0         10         10         10        0        1        0        0\n1  2017020002           1         10         10         10        1        0        0        0         10         10         10        0        1        0        0\n</code></pre>\n\n<p>(I kept the prefix for <code>Won/Lost</code> because it indicates that it's the statistic for the home team. Also, if anybody knows how to add the prefixes more elegantly without having to re-rename the <code>gameID</code> please leave a comment.)</p>\n\n<hr>\n\n<p><strong>Original Attempt</strong></p>\n\n<p>You can apply the following function after grouping</p>\n\n<pre><code>def munge(group): \n     is_home = group.Home == 1 \n     wonlost = group.loc[is_home, 'Won/Lost'].reset_index(drop=True) \n     group = group.loc[:, 'metric2':] \n     home = group[is_home].add_prefix('h_').reset_index(drop=True) \n     away = group[~is_home].add_prefix('a_').reset_index(drop=True) \n     return pd.concat([wonlost, home, away], axis=1) \n</code></pre>\n\n<p>... like this:</p>\n\n<pre><code>&gt;&gt;&gt; df.groupby('gameID').apply(munge).reset_index(level=1, drop=True)                                                                                                                                                                           \n            Won/Lost  h_metric2  h_metric3  h_metric4  h_team1  h_team2  h_team3  h_team4  a_metric2  a_metric3  a_metric4  a_team1  a_team2  a_team3  a_team4\ngameID                                                                                                                                                        \n2017020001         1         10         10         10        1        0        0        0         10         10         10        0        1        0        0\n2017020002         1         10         10         10        1        0        0        0         10         10         10        0        1        0        0\n</code></pre>\n",
        "question_body": "<p>I have a data-set that is in the shape of this, where each row represents a in a specific match that is specified by the <code>gameID</code>.</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>  gameID          Won/Lost   Home   Away  metric2 metric3 metric4   team1 team2 team3 team4\n2017020001         1          1      0      10      10      10      1     0     0      0\n2017020001         0          0      1      10      10      10      0     1     0      0\n</code></pre>\n\n<p>The thing I want to do is create a function that takes the rows with the same <code>gameID</code> and joins them. As you can see in data example below, the two rows represents one game that is split up into a home team (row_1 ) and an away team (row_2). I want these two rows to sit on one row only.</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>Won/Lost  h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4\n1            10       10         10        10         10        10      1       0        0      0         0      1        0      0\n</code></pre>\n\n<p>How do I get this result?</p>\n\n<p>EDIT: I created too much confusion, posting my code so you can get a better grasp of the problem I want to solve.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\nimport requests\nimport json\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\n\nresults = []\nfor game_id in range(2017020001, 2017020010, 1):\n    url = 'https://statsapi.web.nhl.com/api/v1/game/{}/boxscore'.format(game_id)\nr = requests.get(url)\ngame_data = r.json()\n\nfor homeaway in ['home','away']:\n\n    game_dict = game_data.get('teams').get(homeaway).get('teamStats').get('teamSkaterStats')\n    game_dict['team'] = game_data.get('teams').get(homeaway).get('team').get('name')\n    game_dict['homeaway'] = homeaway\n    game_dict['game_id'] = game_id\n    results.append(game_dict)\n\ndf = pd.DataFrame(results)\n\ndf['Won/Lost'] = df.groupby('game_id')['goals'].apply(lambda g: (g == g.max()).map({True: 1, False: 0}))\n\ndf[\"faceOffWinPercentage\"] = df[\"faceOffWinPercentage\"].astype('float')\ndf[\"powerPlayPercentage\"] = df[\"powerPlayPercentage\"].astype('float')\ndf[\"team\"] = df[\"team\"].astype('category')\ndf = pd.get_dummies(df, columns=['homeaway'])\ndf = pd.get_dummies(df, columns=['team'])\n</code></pre>\n",
        "formatted_input": {
            "qid": 53973711,
            "link": "https://stackoverflow.com/questions/53973711/how-to-merge-rows-in-a-dataframe-based-on-a-column-value",
            "question": {
                "title": "How to merge rows in a dataframe based on a column value?",
                "ques_desc": "I have a data-set that is in the shape of this, where each row represents a in a specific match that is specified by the . The thing I want to do is create a function that takes the rows with the same and joins them. As you can see in data example below, the two rows represents one game that is split up into a home team (row_1 ) and an away team (row_2). I want these two rows to sit on one row only. How do I get this result? EDIT: I created too much confusion, posting my code so you can get a better grasp of the problem I want to solve. "
            },
            "io": [
                "  gameID          Won/Lost   Home   Away  metric2 metric3 metric4   team1 team2 team3 team4\n2017020001         1          1      0      10      10      10      1     0     0      0\n2017020001         0          0      1      10      10      10      0     1     0      0\n",
                "Won/Lost  h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4\n1            10       10         10        10         10        10      1       0        0      0         0      1        0      0\n"
            ],
            "answer": {
                "ans_desc": "This is under the assumption that you have exactly two rows per and that you want to group by that ID. (It also assumes that I understand the question.) Improved solution Given a dataframe such as you can use (and some data munging) like this: (I kept the prefix for because it indicates that it's the statistic for the home team. Also, if anybody knows how to add the prefixes more elegantly without having to re-rename the please leave a comment.) Original Attempt You can apply the following function after grouping ... like this: ",
                "code": [
                    "def munge(group): \n     is_home = group.Home == 1 \n     wonlost = group.loc[is_home, 'Won/Lost'].reset_index(drop=True) \n     group = group.loc[:, 'metric2':] \n     home = group[is_home].add_prefix('h_').reset_index(drop=True) \n     away = group[~is_home].add_prefix('a_').reset_index(drop=True) \n     return pd.concat([wonlost, home, away], axis=1) \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 469,
            "user_id": 8500756,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-aFKcbLRcSEo/AAAAAAAAAAI/AAAAAAAAAAw/VgAuhFrfk3o/photo.jpg?sz=128",
            "display_name": "theantomc",
            "link": "https://stackoverflow.com/users/8500756/theantomc"
        },
        "is_answered": true,
        "view_count": 21333,
        "accepted_answer_id": 53961961,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1546017202,
        "creation_date": 1546016614,
        "question_id": 53961914,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53961914/create-a-dataframe-from-arrays-python",
        "title": "Create a dataframe from arrays python",
        "body": "<p>I'm try to construct a dataframe (I'm using Pandas library)\nfrom some arrays and one matrix.</p>\n\n<p>in particular, \nif I have two array like this:</p>\n\n<pre><code>A=[A,B,C]\nB=[D,E,F]\n</code></pre>\n\n<p>And one matrix like this : </p>\n\n<pre><code>1 2 2\n3 3 3\n4 4 4\n</code></pre>\n\n<p>Can i create a dataset like this?</p>\n\n<pre><code>  A B C\nD 1 2 2\nE 3 3 3\nF 4 4 4\n</code></pre>\n\n<p>Maybe is a stupid question, but i m very new with Python and Pandas. </p>\n\n<p>I seen this : </p>\n\n<p><a href=\"https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html\" rel=\"noreferrer\">https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html</a></p>\n\n<p>but specify only 'colums'.</p>\n\n<p>I should read the matrix row for row and paste in my dataset, but I m think that exist a more easy solution with Pandas.</p>\n",
        "answer_body": "<p>This should do the trick for you.</p>\n\n<pre><code>columns = [\"A\", \"B\", \"C\"]\nrows = [\"D\", \"E\", \"F\"]\ndata = np.array([[1, 2, 2], [3, 3, 3],[4, 4, 4]])\ndf = pd.DataFrame(data=data, index=rows, columns=columns)\n</code></pre>\n",
        "question_body": "<p>I'm try to construct a dataframe (I'm using Pandas library)\nfrom some arrays and one matrix.</p>\n\n<p>in particular, \nif I have two array like this:</p>\n\n<pre><code>A=[A,B,C]\nB=[D,E,F]\n</code></pre>\n\n<p>And one matrix like this : </p>\n\n<pre><code>1 2 2\n3 3 3\n4 4 4\n</code></pre>\n\n<p>Can i create a dataset like this?</p>\n\n<pre><code>  A B C\nD 1 2 2\nE 3 3 3\nF 4 4 4\n</code></pre>\n\n<p>Maybe is a stupid question, but i m very new with Python and Pandas. </p>\n\n<p>I seen this : </p>\n\n<p><a href=\"https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html\" rel=\"noreferrer\">https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html</a></p>\n\n<p>but specify only 'colums'.</p>\n\n<p>I should read the matrix row for row and paste in my dataset, but I m think that exist a more easy solution with Pandas.</p>\n",
        "formatted_input": {
            "qid": 53961914,
            "link": "https://stackoverflow.com/questions/53961914/create-a-dataframe-from-arrays-python",
            "question": {
                "title": "Create a dataframe from arrays python",
                "ques_desc": "I'm try to construct a dataframe (I'm using Pandas library) from some arrays and one matrix. in particular, if I have two array like this: And one matrix like this : Can i create a dataset like this? Maybe is a stupid question, but i m very new with Python and Pandas. I seen this : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html but specify only 'colums'. I should read the matrix row for row and paste in my dataset, but I m think that exist a more easy solution with Pandas. "
            },
            "io": [
                "1 2 2\n3 3 3\n4 4 4\n",
                "  A B C\nD 1 2 2\nE 3 3 3\nF 4 4 4\n"
            ],
            "answer": {
                "ans_desc": "This should do the trick for you. ",
                "code": [
                    "columns = [\"A\", \"B\", \"C\"]\nrows = [\"D\", \"E\", \"F\"]\ndata = np.array([[1, 2, 2], [3, 3, 3],[4, 4, 4]])\ndf = pd.DataFrame(data=data, index=rows, columns=columns)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 1821,
            "user_id": 7148245,
            "user_type": "registered",
            "accept_rate": 72,
            "profile_image": "https://www.gravatar.com/avatar/a8d64725924830b6291f35eff98f646e?s=128&d=identicon&r=PG&f=1",
            "display_name": "madsthaks",
            "link": "https://stackoverflow.com/users/7148245/madsthaks"
        },
        "is_answered": true,
        "view_count": 143,
        "accepted_answer_id": 47402370,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1545972509,
        "creation_date": 1511217644,
        "last_edit_date": 1545972509,
        "question_id": 47402346,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/47402346/ranking-groups-based-on-size",
        "title": "Ranking groups based on size",
        "body": "<p>Sample Data:</p>\n\n<pre><code>id cluster\n1 3\n2 3\n3 3\n4 3\n5 1\n6 1\n7 2\n8 2\n9 2\n10 4\n11 4\n12 5\n13 6\n</code></pre>\n\n<p>What I would like to do is replace the largest cluster id with <code>0</code> and the second largest with <code>1</code> and so on and so forth. Output would be as shown below. </p>\n\n<pre><code>id cluster\n1 0\n2 0\n3 0\n4 0\n5 2\n6 2\n7 1\n8 1\n9 1\n10 3\n11 3\n12 4\n13 5\n</code></pre>\n\n<p>I'm not quite sure where to start with this. Any help would be much appreciated. </p>\n",
        "answer_body": "<p>The objective is to relabel groups defined in the <code>'cluster'</code> column by the corresponding rank of that group's total value count within the column.  We'll break this down into several steps:</p>\n\n<ol>\n<li>Integer factorization.  Find an integer representation where each unique value in the column gets its own integer.  We'll start with zero.</li>\n<li>We then need the counts of each of these unique values.</li>\n<li>We need to rank the unique values by their counts.</li>\n<li>We assign the ranks back to the positions of the original column.</li>\n</ol>\n\n<hr>\n\n<p><strong>Approach 1</strong><br>\nUsing Numpy's <code>numpy.unique</code> + <code>argsort</code>  </p>\n\n<p><strong>TL;DR</strong>  </p>\n\n<pre><code>u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_counts=True\n)\n(-c).argsort()[i]\n</code></pre>\n\n<p>Turns out, <code>numpy.unique</code> performs the task of integer factorization and counting values in one go.  In the process, we get unique values as well, but we don't really need those.  Also, the integer factorization isn't obvious.  That's because per the <code>numpy.unique</code> function, the return value we're looking for is called the <code>inverse</code>.  It's called the inverse because it was intended to act as a way to get back the original array given the array of unique values.  So if we let</p>\n\n<pre><code>u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_couns=True\n)\n</code></pre>\n\n<p>You'll see <code>i</code> looks like:</p>\n\n<pre><code>array([2, 2, 2, 2, 0, 0, 1, 1, 1, 3, 3, 4, 5])\n</code></pre>\n\n<p>And if we did <code>u[i]</code> we get back the original <code>df.cluster.values</code></p>\n\n<pre><code>array([3, 3, 3, 3, 1, 1, 2, 2, 2, 4, 4, 5, 6])\n</code></pre>\n\n<p>But we are going to use it as integer factorization.</p>\n\n<p>Next, we need the counts <code>c</code></p>\n\n<pre><code>array([2, 3, 4, 2, 1, 1])\n</code></pre>\n\n<p>I'm going to propose the use of <code>argsort</code> but it's confusing.  So I'll try to show it:</p>\n\n<pre><code>np.row_stack([c, (-c).argsort()])\n\narray([[2, 3, 4, 2, 1, 1],\n       [2, 1, 0, 3, 4, 5]])\n</code></pre>\n\n<p>What <code>argsort</code> does in general is to place the top spot (position 0), the position to draw from in the originating array.</p>\n\n<pre><code>#            position 2\n#            is best\n#                |\n#                v\n# array([[2, 3, 4, 2, 1, 1],\n#        [2, 1, 0, 3, 4, 5]])\n#         ^\n#         |\n#     top spot\n#     from\n#     position 2\n\n#        position 1\n#        goes to\n#        pen-ultimate spot\n#            |\n#            v\n# array([[2, 3, 4, 2, 1, 1],\n#        [2, 1, 0, 3, 4, 5]])\n#            ^\n#            |\n#        pen-ultimate spot\n#        from\n#        position 1\n</code></pre>\n\n<p>What this allows us to do is to slice this <code>argsort</code> result with our integer factorization to arrive at a remapping of the ranks.</p>\n\n<pre><code>#     i is\n#        [2 2 2 2 0 0 1 1 1 3 3 4 5]\n\n#     (-c).argsort() is \n#        [2 1 0 3 4 5]\n\n# argsort\n# slice\n#      \\   / This is our integer factorization\n#       a i\n#     [[0 2]  &lt;-- 0 is second position in argsort\n#      [0 2]  &lt;-- 0 is second position in argsort\n#      [0 2]  &lt;-- 0 is second position in argsort\n#      [0 2]  &lt;-- 0 is second position in argsort\n#      [2 0]  &lt;-- 2 is zeroth position in argsort\n#      [2 0]  &lt;-- 2 is zeroth position in argsort\n#      [1 1]  &lt;-- 1 is first position in argsort\n#      [1 1]  &lt;-- 1 is first position in argsort\n#      [1 1]  &lt;-- 1 is first position in argsort\n#      [3 3]  &lt;-- 3 is third position in argsort\n#      [3 3]  &lt;-- 3 is third position in argsort\n#      [4 4]  &lt;-- 4 is fourth position in argsort\n#      [5 5]] &lt;-- 5 is fifth position in argsort\n</code></pre>\n\n<p>We can then drop it into the column with <code>pd.DataFrame.assign</code>  </p>\n\n<pre><code>u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_counts=True\n)\ndf.assign(cluster=(-c).argsort()[i])\n\n    id  cluster\n0    1        0\n1    2        0\n2    3        0\n3    4        0\n4    5        2\n5    6        2\n6    7        1\n7    8        1\n8    9        1\n9   10        3\n10  11        3\n11  12        4\n12  13        5\n</code></pre>\n\n<hr>\n\n<p><strong>Approach 2</strong><br>\nI'm going to leverage the same concepts.  However, I'll use Pandas <code>pandas.factorize</code> to get integer factorization with <code>numpy.bincount</code> to count values.  The reason to use this approach is because Numpy's <code>unique</code> actually sorts the values in the midst of factorizing and counting.  <code>pandas.factorize</code> does not.  For larger data sets, big oh is our friend as this remains <code>O(n)</code> while the Numpy approach is <code>O(nlogn)</code>.</p>\n\n<pre><code>i, u = pd.factorize(df.cluster.values)\nc = np.bincount(i)\ndf.assign(cluster=(-c).argsort()[i])\n\n    id  cluster\n0    1        0\n1    2        0\n2    3        0\n3    4        0\n4    5        2\n5    6        2\n6    7        1\n7    8        1\n8    9        1\n9   10        3\n10  11        3\n11  12        4\n12  13        5\n</code></pre>\n",
        "question_body": "<p>Sample Data:</p>\n\n<pre><code>id cluster\n1 3\n2 3\n3 3\n4 3\n5 1\n6 1\n7 2\n8 2\n9 2\n10 4\n11 4\n12 5\n13 6\n</code></pre>\n\n<p>What I would like to do is replace the largest cluster id with <code>0</code> and the second largest with <code>1</code> and so on and so forth. Output would be as shown below. </p>\n\n<pre><code>id cluster\n1 0\n2 0\n3 0\n4 0\n5 2\n6 2\n7 1\n8 1\n9 1\n10 3\n11 3\n12 4\n13 5\n</code></pre>\n\n<p>I'm not quite sure where to start with this. Any help would be much appreciated. </p>\n",
        "formatted_input": {
            "qid": 47402346,
            "link": "https://stackoverflow.com/questions/47402346/ranking-groups-based-on-size",
            "question": {
                "title": "Ranking groups based on size",
                "ques_desc": "Sample Data: What I would like to do is replace the largest cluster id with and the second largest with and so on and so forth. Output would be as shown below. I'm not quite sure where to start with this. Any help would be much appreciated. "
            },
            "io": [
                "id cluster\n1 3\n2 3\n3 3\n4 3\n5 1\n6 1\n7 2\n8 2\n9 2\n10 4\n11 4\n12 5\n13 6\n",
                "id cluster\n1 0\n2 0\n3 0\n4 0\n5 2\n6 2\n7 1\n8 1\n9 1\n10 3\n11 3\n12 4\n13 5\n"
            ],
            "answer": {
                "ans_desc": "The objective is to relabel groups defined in the column by the corresponding rank of that group's total value count within the column. We'll break this down into several steps: Integer factorization. Find an integer representation where each unique value in the column gets its own integer. We'll start with zero. We then need the counts of each of these unique values. We need to rank the unique values by their counts. We assign the ranks back to the positions of the original column. Approach 1 Using Numpy's + TL;DR Turns out, performs the task of integer factorization and counting values in one go. In the process, we get unique values as well, but we don't really need those. Also, the integer factorization isn't obvious. That's because per the function, the return value we're looking for is called the . It's called the inverse because it was intended to act as a way to get back the original array given the array of unique values. So if we let You'll see looks like: And if we did we get back the original But we are going to use it as integer factorization. Next, we need the counts I'm going to propose the use of but it's confusing. So I'll try to show it: What does in general is to place the top spot (position 0), the position to draw from in the originating array. What this allows us to do is to slice this result with our integer factorization to arrive at a remapping of the ranks. We can then drop it into the column with Approach 2 I'm going to leverage the same concepts. However, I'll use Pandas to get integer factorization with to count values. The reason to use this approach is because Numpy's actually sorts the values in the midst of factorizing and counting. does not. For larger data sets, big oh is our friend as this remains while the Numpy approach is . ",
                "code": [
                    "u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_counts=True\n)\n(-c).argsort()[i]\n",
                    "u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_couns=True\n)\n",
                    "array([2, 2, 2, 2, 0, 0, 1, 1, 1, 3, 3, 4, 5])\n",
                    "array([3, 3, 3, 3, 1, 1, 2, 2, 2, 4, 4, 5, 6])\n",
                    "array([2, 3, 4, 2, 1, 1])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15222,
            "user_id": 25197,
            "user_type": "registered",
            "accept_rate": 67,
            "profile_image": "https://www.gravatar.com/avatar/cf458a6960715c6015e5324ef18a9350?s=128&d=identicon&r=PG",
            "display_name": "GollyJer",
            "link": "https://stackoverflow.com/users/25197/gollyjer"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 53953157,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1545967233,
        "creation_date": 1545965037,
        "question_id": 53953121,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53953121/dataframe-summary-math-based-on-condition-from-another-dataframe",
        "title": "Dataframe summary math based on condition from another dataframe?",
        "body": "<p>I have what amounts to 3D data but can't install the Pandas recommended <a href=\"http://xarray.pydata.org/en/stable/\" rel=\"nofollow noreferrer\">xarray package</a>.</p>\n\n<h3>df_values</h3>\n\n<pre><code>   | a    b    c\n-----------------\n0  | 5    9    2\n1  | 6    9    5\n2  | 1    6    8  \n</code></pre>\n\n<h3>df_condition</h3>\n\n<pre><code>   | a    b    c\n-----------------\n0  | y    y    y\n1  | y    n    y\n2  | n    n    y\n</code></pre>\n\n<p>I know I can get the average of all values in <code>df_values</code> like this.</p>\n\n<pre><code>df_values.stack().mean()\n</code></pre>\n\n<p><br>\nQuestion...  \ud83d\udc47<br>\nWhat is the simplest way to find the <code>average of df_values</code> where <code>df_condition == \"y\"</code>?</p>\n",
        "answer_body": "<p>Assuming you wish to find the mean of all values where <code>df_condition == 'y'</code>:</p>\n\n<pre><code>res = np.nanmean(df_values[df_condition.eq('y')])  #5.833333333333333\n</code></pre>\n\n<p>Using NumPy is substantially cheaper than Pandas <code>stack</code> or <code>where</code>:</p>\n\n<pre><code># Pandas 0.23.0, NumPy 1.14.3\nn = 10**5\ndf_values = pd.concat([df_values]*n, ignore_index=True)\ndf_condition = pd.concat([df_condition]*n, ignore_index=True)\n\n%timeit np.nanmean(df_values.values[df_condition.eq('y')])       # 32 ms\n%timeit np.nanmean(df_values.where(df_condition == 'y').values)  # 88 ms\n%timeit df_values[df_condition.eq('y')].stack().mean()           # 107 ms\n</code></pre>\n",
        "question_body": "<p>I have what amounts to 3D data but can't install the Pandas recommended <a href=\"http://xarray.pydata.org/en/stable/\" rel=\"nofollow noreferrer\">xarray package</a>.</p>\n\n<h3>df_values</h3>\n\n<pre><code>   | a    b    c\n-----------------\n0  | 5    9    2\n1  | 6    9    5\n2  | 1    6    8  \n</code></pre>\n\n<h3>df_condition</h3>\n\n<pre><code>   | a    b    c\n-----------------\n0  | y    y    y\n1  | y    n    y\n2  | n    n    y\n</code></pre>\n\n<p>I know I can get the average of all values in <code>df_values</code> like this.</p>\n\n<pre><code>df_values.stack().mean()\n</code></pre>\n\n<p><br>\nQuestion...  \ud83d\udc47<br>\nWhat is the simplest way to find the <code>average of df_values</code> where <code>df_condition == \"y\"</code>?</p>\n",
        "formatted_input": {
            "qid": 53953121,
            "link": "https://stackoverflow.com/questions/53953121/dataframe-summary-math-based-on-condition-from-another-dataframe",
            "question": {
                "title": "Dataframe summary math based on condition from another dataframe?",
                "ques_desc": "I have what amounts to 3D data but can't install the Pandas recommended xarray package. df_values df_condition I know I can get the average of all values in like this. Question... \ud83d\udc47 What is the simplest way to find the where ? "
            },
            "io": [
                "   | a    b    c\n-----------------\n0  | 5    9    2\n1  | 6    9    5\n2  | 1    6    8  \n",
                "   | a    b    c\n-----------------\n0  | y    y    y\n1  | y    n    y\n2  | n    n    y\n"
            ],
            "answer": {
                "ans_desc": "Assuming you wish to find the mean of all values where : Using NumPy is substantially cheaper than Pandas or : ",
                "code": [
                    "# Pandas 0.23.0, NumPy 1.14.3\nn = 10**5\ndf_values = pd.concat([df_values]*n, ignore_index=True)\ndf_condition = pd.concat([df_condition]*n, ignore_index=True)\n\n%timeit np.nanmean(df_values.values[df_condition.eq('y')])       # 32 ms\n%timeit np.nanmean(df_values.where(df_condition == 'y').values)  # 88 ms\n%timeit df_values[df_condition.eq('y')].stack().mean()           # 107 ms\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe",
            "indexing"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user10332687"
        },
        "is_answered": true,
        "view_count": 323,
        "accepted_answer_id": 53939093,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1545916653,
        "creation_date": 1545877219,
        "last_edit_date": 1545916653,
        "question_id": 53939072,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53939072/column-dupe-renaming-in-pandas",
        "title": "Column dupe renaming in pandas",
        "body": "<p>I have the following csv file of data:</p>\n\n<pre><code>id,number,id\n132605,1,1\n132750,2,1\n</code></pre>\n\n<p>Pandas currently renames this to:</p>\n\n<pre><code>       id number id.1\n0  132605      1    1\n1  132750      2    1\n</code></pre>\n\n<p>Is there a way to customize how this is renamed? For example, I would prefer:</p>\n\n<pre><code>           id number id2\n0  132605      1    1\n1  132750      2    1\n</code></pre>\n",
        "answer_body": "<h3><code>rename</code>: use period delimiter</h3>\n\n<p>Assuming duplicate column labels are the <em>only</em> instances where a column name contains a period (<code>.</code>), you can use a custom function with <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html\" rel=\"nofollow noreferrer\"><code>pd.DataFrame.rename</code></a>:</p>\n\n<pre><code>from io import StringIO\n\nfile = \"\"\"id,number,id\n132605,1,1\n132750,2,1\"\"\"\n\ndef rename_func(x):\n    if '.' not in x:\n        return x\n    name, num = x.split('.')\n    return f'{name}{int(num)+1}'\n\n# replace StringIO(file) with 'file.csv'\ndf = pd.read_csv(StringIO(file))\\\n       .rename(columns=rename_func)\n\nprint(df)\n\n       id  number  id2\n0  132605       1    1\n1  132750       2    1\n</code></pre>\n\n<h3><code>csv.reader</code>: robust solution</h3>\n\n<p>A robust solution is possible with the <code>csv</code> module from the standard library:</p>\n\n<pre><code>from collections import defaultdict\nimport csv\n\n# replace StringIO(file) with open('file.csv', 'r')\nwith StringIO(file) as fin:\n    headers = next(csv.reader(fin))\n\ndef rename_duplicates(original_cols):\n    count = defaultdict(int)\n    for x in original_cols:\n        count[x] += 1\n        yield f'{x}{count[x]}' if count[x] &gt; 1 else x\n\ndf.columns = rename_duplicates(headers)\n</code></pre>\n",
        "question_body": "<p>I have the following csv file of data:</p>\n\n<pre><code>id,number,id\n132605,1,1\n132750,2,1\n</code></pre>\n\n<p>Pandas currently renames this to:</p>\n\n<pre><code>       id number id.1\n0  132605      1    1\n1  132750      2    1\n</code></pre>\n\n<p>Is there a way to customize how this is renamed? For example, I would prefer:</p>\n\n<pre><code>           id number id2\n0  132605      1    1\n1  132750      2    1\n</code></pre>\n",
        "formatted_input": {
            "qid": 53939072,
            "link": "https://stackoverflow.com/questions/53939072/column-dupe-renaming-in-pandas",
            "question": {
                "title": "Column dupe renaming in pandas",
                "ques_desc": "I have the following csv file of data: Pandas currently renames this to: Is there a way to customize how this is renamed? For example, I would prefer: "
            },
            "io": [
                "       id number id.1\n0  132605      1    1\n1  132750      2    1\n",
                "           id number id2\n0  132605      1    1\n1  132750      2    1\n"
            ],
            "answer": {
                "ans_desc": ": use period delimiter Assuming duplicate column labels are the only instances where a column name contains a period (), you can use a custom function with : : robust solution A robust solution is possible with the module from the standard library: ",
                "code": [
                    "from collections import defaultdict\nimport csv\n\n# replace StringIO(file) with open('file.csv', 'r')\nwith StringIO(file) as fin:\n    headers = next(csv.reader(fin))\n\ndef rename_duplicates(original_cols):\n    count = defaultdict(int)\n    for x in original_cols:\n        count[x] += 1\n        yield f'{x}{count[x]}' if count[x] > 1 else x\n\ndf.columns = rename_duplicates(headers)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy-ndarray"
        ],
        "owner": {
            "reputation": 341,
            "user_id": 7617379,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/792a62849d77db3408d1f99fa6a08fcd?s=128&d=identicon&r=PG",
            "display_name": "Vibhu",
            "link": "https://stackoverflow.com/users/7617379/vibhu"
        },
        "is_answered": true,
        "view_count": 132,
        "accepted_answer_id": 53933413,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1545836062,
        "creation_date": 1545827442,
        "last_edit_date": 1545830786,
        "question_id": 53932155,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53932155/how-to-append-ndarray-values-into-dataframe-rows-of-particular-columns",
        "title": "How to append ndarray values into dataframe rows of particular columns?",
        "body": "<p>I have a function that returns an <code>ndarray</code> like this</p>\n\n<pre><code>[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0] \n</code></pre>\n\n<p>Now, I have a data frame <code>df</code> with columns A,B,C,...,Z ; but the array we are getting has only 20 values. Hence I want to find a way such that for every array I get as output, I am able to store it in <code>df</code> like this (A,B,W,X,Y,Z are to be left blank):</p>\n\n<pre><code>__| A | B | C | D | E | F | ...\n0 |nan|nan| 0 | 1 | 0 | 0 | ...\n1 |nan|nan| 1 | 1 | 0 | 1 | ...\n.\n. \n.\n</code></pre>\n",
        "answer_body": "<p>I couldn't get what I wanted through the suggestions posted here. However, I did figure it out myself. I'm sharing it here for the community's reference.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(columns=[chr(i) for i in range(ord('A'),ord('Z')+1)])\n\nprint(df)\n</code></pre>\n\n<blockquote>\n<pre><code>Empty DataFrame\nColumns: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z]\nIndex: []\n\n[0 rows x 26 columns]\n</code></pre>\n</blockquote>\n\n<pre><code>list1 = [i for i in range(101,121)]\narr1d = np.array(list1)\n\narr1d\n</code></pre>\n\n<blockquote>\n<pre><code>array([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n       114, 115, 116, 117, 118, 119, 120])\n</code></pre>\n</blockquote>\n\n<pre><code># Create alphabet list of uppercase letters\nalphabet = []\nfor letter in range(ord('C'),ord('W')):\n    alphabet.append(chr(letter))\nalphabet\n</code></pre>\n\n<blockquote>\n<pre><code>['C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'K',\n 'L',\n 'M',\n 'N',\n 'O',\n 'P',\n 'Q',\n 'R',\n 'S',\n 'T',\n 'U',\n 'V']\n</code></pre>\n</blockquote>\n\n<pre><code>df = df.append(pd.Series(arr1d, index=alphabet), ignore_index=True)\n#This line of code can be used for every new value of arr1d \n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/sRjPx.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/sRjPx.png\" alt=\"enter image description here\"></a></p>\n",
        "question_body": "<p>I have a function that returns an <code>ndarray</code> like this</p>\n\n<pre><code>[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0] \n</code></pre>\n\n<p>Now, I have a data frame <code>df</code> with columns A,B,C,...,Z ; but the array we are getting has only 20 values. Hence I want to find a way such that for every array I get as output, I am able to store it in <code>df</code> like this (A,B,W,X,Y,Z are to be left blank):</p>\n\n<pre><code>__| A | B | C | D | E | F | ...\n0 |nan|nan| 0 | 1 | 0 | 0 | ...\n1 |nan|nan| 1 | 1 | 0 | 1 | ...\n.\n. \n.\n</code></pre>\n",
        "formatted_input": {
            "qid": 53932155,
            "link": "https://stackoverflow.com/questions/53932155/how-to-append-ndarray-values-into-dataframe-rows-of-particular-columns",
            "question": {
                "title": "How to append ndarray values into dataframe rows of particular columns?",
                "ques_desc": "I have a function that returns an like this Now, I have a data frame with columns A,B,C,...,Z ; but the array we are getting has only 20 values. Hence I want to find a way such that for every array I get as output, I am able to store it in like this (A,B,W,X,Y,Z are to be left blank): "
            },
            "io": [
                "[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0] \n",
                "__| A | B | C | D | E | F | ...\n0 |nan|nan| 0 | 1 | 0 | 0 | ...\n1 |nan|nan| 1 | 1 | 0 | 1 | ...\n.\n. \n.\n"
            ],
            "answer": {
                "ans_desc": "I couldn't get what I wanted through the suggestions posted here. However, I did figure it out myself. I'm sharing it here for the community's reference. ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(columns=[chr(i) for i in range(ord('A'),ord('Z')+1)])\n\nprint(df)\n",
                    "Empty DataFrame\nColumns: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z]\nIndex: []\n\n[0 rows x 26 columns]\n",
                    "list1 = [i for i in range(101,121)]\narr1d = np.array(list1)\n\narr1d\n",
                    "# Create alphabet list of uppercase letters\nalphabet = []\nfor letter in range(ord('C'),ord('W')):\n    alphabet.append(chr(letter))\nalphabet\n",
                    "['C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'K',\n 'L',\n 'M',\n 'N',\n 'O',\n 'P',\n 'Q',\n 'R',\n 'S',\n 'T',\n 'U',\n 'V']\n",
                    "df = df.append(pd.Series(arr1d, index=alphabet), ignore_index=True)\n#This line of code can be used for every new value of arr1d \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 18861,
            "user_id": 1610626,
            "user_type": "registered",
            "accept_rate": 70,
            "profile_image": "https://www.gravatar.com/avatar/d4738de350bcaf1777b12e8b77f7d018?s=128&d=identicon&r=PG",
            "display_name": "user1234440",
            "link": "https://stackoverflow.com/users/1610626/user1234440"
        },
        "is_answered": true,
        "view_count": 104,
        "accepted_answer_id": 53885651,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1545401856,
        "creation_date": 1545397996,
        "last_edit_date": 1545399606,
        "question_id": 53885404,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53885404/python-setting-values-without-loop",
        "title": "Python Setting Values Without Loop",
        "body": "<p>I have a time series dataframe where there is 1 or 0 in it (true/false). I wrote a function that loops through all rows with values 1 in them. Given user defined integer parameter called <code>n_hold</code>, I will set values 1 to n rows forward from the initial row.</p>\n\n<p>For example, in the dataframe below I will be loop to row <code>2016-08-05</code>. If <code>n_hold = 2</code>, then I will set both <code>2016-08-08</code> and <code>2016-08-09</code> to 1 too.:</p>\n\n<pre><code>2016-08-03    0\n2016-08-04    0\n2016-08-05    1\n2016-08-08    0\n2016-08-09    0\n2016-08-10    0\n</code></pre>\n\n<p>The resulting <code>df</code> will then is</p>\n\n<pre><code>2016-08-03    0\n2016-08-04    0\n2016-08-05    1\n2016-08-08    1\n2016-08-09    1\n2016-08-10    0\n</code></pre>\n\n<p>The problem I have is this is being run 10s of thousands of times and my current solution where I am looping over rows where there are ones and subsetting is way too slow. I was wondering if there are any solutions to the above problem that is really fast.</p>\n\n<p>Here is my (slow) solution, <code>x</code> is the initial signal dataframe:</p>\n\n<pre><code>n_hold = 2\nentry_sig_diff = x.diff()\nentry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index\nfinal_signal = x * 0\nfor i in range(0, len(entry_sig_dt)):\n    row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])\n\n    if (row_idx + n_hold) &gt;= len(x):\n        break\n\n    final_signal[row_idx:(row_idx + n_hold + 1)] = 1\n</code></pre>\n",
        "answer_body": "<p>Completely changed answer, because working differently with consecutive <code>1</code> values:</p>\n\n<p><strong>Explanation</strong>:</p>\n\n<p>Solution remove each consecutive <code>1</code> first by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.where.html\" rel=\"nofollow noreferrer\"><code>where</code></a> with chained boolean mask by comparing with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.ne.html\" rel=\"nofollow noreferrer\"><code>ne</code></a> (not equal <code>!=</code>) with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.shift.html\" rel=\"nofollow noreferrer\"><code>shift</code></a> to <code>NaN</code>s, forward filling them by <code>ffill</code> with <code>limit</code> parameter and last replace <code>0</code> back:</p>\n\n<pre><code>n_hold = 2\ns = x.where(x.ne(x.shift()) &amp; (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n</code></pre>\n\n<p>Timings and comparing outputs:</p>\n\n<pre><code>np.random.seed(123)\nx = pd.Series(np.random.choice([0,1], p=(.8,.2), size=1000))\nx1 = x.copy()\n#print (x)\n\n\ndef orig(x):\n    n_hold = 2\n    entry_sig_diff = x.diff()\n    entry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index\n    final_signal = x * 0\n    for i in range(0, len(entry_sig_dt)):\n        row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])\n\n        if (row_idx + n_hold) &gt;= len(x):\n            break\n\n        final_signal[row_idx:(row_idx + n_hold + 1)] = 1\n    return final_signal\n\n#print (orig(x))\n</code></pre>\n\n<hr>\n\n<pre><code>n_hold = 2\ns = x.where(x.ne(x.shift()) &amp; (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n#print (s)\n\ndf = pd.concat([x,orig(x1), s], axis=1, keys=('input', 'orig', 'new'))\nprint (df.head(20))\n    input  orig  new\n0       0     0    0\n1       0     0    0\n2       0     0    0\n3       0     0    0\n4       0     0    0\n5       0     0    0\n6       1     1    1\n7       0     1    1\n8       0     1    1\n9       0     0    0\n10      0     0    0\n11      0     0    0\n12      0     0    0\n13      0     0    0\n14      0     0    0\n15      0     0    0\n16      0     0    0\n17      0     0    0\n18      0     0    0\n19      0     0    0\n\n#check outputs\n#print (s.values == orig(x).values)\n</code></pre>\n\n<p><strong>Timings</strong>:</p>\n\n<pre><code>%timeit (orig(x))\n24.8 ms \u00b1 653 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit x.where(x.ne(x.shift()) &amp; (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n1.36 ms \u00b1 12.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n",
        "question_body": "<p>I have a time series dataframe where there is 1 or 0 in it (true/false). I wrote a function that loops through all rows with values 1 in them. Given user defined integer parameter called <code>n_hold</code>, I will set values 1 to n rows forward from the initial row.</p>\n\n<p>For example, in the dataframe below I will be loop to row <code>2016-08-05</code>. If <code>n_hold = 2</code>, then I will set both <code>2016-08-08</code> and <code>2016-08-09</code> to 1 too.:</p>\n\n<pre><code>2016-08-03    0\n2016-08-04    0\n2016-08-05    1\n2016-08-08    0\n2016-08-09    0\n2016-08-10    0\n</code></pre>\n\n<p>The resulting <code>df</code> will then is</p>\n\n<pre><code>2016-08-03    0\n2016-08-04    0\n2016-08-05    1\n2016-08-08    1\n2016-08-09    1\n2016-08-10    0\n</code></pre>\n\n<p>The problem I have is this is being run 10s of thousands of times and my current solution where I am looping over rows where there are ones and subsetting is way too slow. I was wondering if there are any solutions to the above problem that is really fast.</p>\n\n<p>Here is my (slow) solution, <code>x</code> is the initial signal dataframe:</p>\n\n<pre><code>n_hold = 2\nentry_sig_diff = x.diff()\nentry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index\nfinal_signal = x * 0\nfor i in range(0, len(entry_sig_dt)):\n    row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])\n\n    if (row_idx + n_hold) &gt;= len(x):\n        break\n\n    final_signal[row_idx:(row_idx + n_hold + 1)] = 1\n</code></pre>\n",
        "formatted_input": {
            "qid": 53885404,
            "link": "https://stackoverflow.com/questions/53885404/python-setting-values-without-loop",
            "question": {
                "title": "Python Setting Values Without Loop",
                "ques_desc": "I have a time series dataframe where there is 1 or 0 in it (true/false). I wrote a function that loops through all rows with values 1 in them. Given user defined integer parameter called , I will set values 1 to n rows forward from the initial row. For example, in the dataframe below I will be loop to row . If , then I will set both and to 1 too.: The resulting will then is The problem I have is this is being run 10s of thousands of times and my current solution where I am looping over rows where there are ones and subsetting is way too slow. I was wondering if there are any solutions to the above problem that is really fast. Here is my (slow) solution, is the initial signal dataframe: "
            },
            "io": [
                "2016-08-03    0\n2016-08-04    0\n2016-08-05    1\n2016-08-08    0\n2016-08-09    0\n2016-08-10    0\n",
                "2016-08-03    0\n2016-08-04    0\n2016-08-05    1\n2016-08-08    1\n2016-08-09    1\n2016-08-10    0\n"
            ],
            "answer": {
                "ans_desc": "Completely changed answer, because working differently with consecutive values: Explanation: Solution remove each consecutive first by with chained boolean mask by comparing with (not equal ) with to s, forward filling them by with parameter and last replace back: Timings and comparing outputs: Timings: ",
                "code": [
                    "n_hold = 2\ns = x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n",
                    "np.random.seed(123)\nx = pd.Series(np.random.choice([0,1], p=(.8,.2), size=1000))\nx1 = x.copy()\n#print (x)\n\n\ndef orig(x):\n    n_hold = 2\n    entry_sig_diff = x.diff()\n    entry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index\n    final_signal = x * 0\n    for i in range(0, len(entry_sig_dt)):\n        row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])\n\n        if (row_idx + n_hold) >= len(x):\n            break\n\n        final_signal[row_idx:(row_idx + n_hold + 1)] = 1\n    return final_signal\n\n#print (orig(x))\n",
                    "%timeit (orig(x))\n24.8 ms \u00b1 653 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n1.36 ms \u00b1 12.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "iterator"
        ],
        "owner": {
            "reputation": 802,
            "user_id": 5941778,
            "user_type": "registered",
            "accept_rate": 46,
            "profile_image": "https://www.gravatar.com/avatar/71db03dc71a827b53bc696cb2f4d1536?s=128&d=identicon&r=PG&f=1",
            "display_name": "Deepak M",
            "link": "https://stackoverflow.com/users/5941778/deepak-m"
        },
        "is_answered": true,
        "view_count": 3574,
        "accepted_answer_id": 51645946,
        "answer_count": 2,
        "score": 5,
        "last_activity_date": 1545398718,
        "creation_date": 1533186848,
        "last_edit_date": 1533187123,
        "question_id": 51645934,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51645934/using-iterrows-with-series-nlargest-to-get-the-highest-number-in-a-row-in-a",
        "title": "Using .iterrows() with series.nlargest() to get the highest number in a row in a Dataframe",
        "body": "<p>I am trying to create a function that uses <code>df.iterrows()</code> and <code>Series.nlargest</code>. I want to iterate over each row and find the largest number and then mark it as a <code>1</code>. This is the data frame:</p>\n\n<pre><code>A   B    C\n9   6    5\n3   7    2\n</code></pre>\n\n<p>Here is the output I wish to have:</p>\n\n<pre><code>A    B   C\n1    0   0\n0    1   0\n</code></pre>\n\n<p>This is the function I wish to use here:</p>\n\n<pre><code>def get_top_n(df, top_n):\n    \"\"\"\n\n\n    Parameters\n    ----------\n    df : DataFrame\n\n    top_n : int\n        The top number to get\n    Returns\n    -------\n    top_numbers : DataFrame\n    Returns the top number marked with a 1\n\n    \"\"\"\n    # Implement Function\n    for row in df.iterrows():\n        top_numbers = row.nlargest(top_n).sum()\n\n    return top_numbers\n</code></pre>\n\n<p>I get the following error:\nAttributeError: 'tuple' object has no attribute 'nlargest'</p>\n\n<p>Help would be appreciated on how to re-write my function in a neater way and to actually work! Thanks in advance</p>\n",
        "answer_body": "<p>Add <code>i</code> variable, because <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\" rel=\"noreferrer\"><code>iterrows</code></a> return indices with <code>Series</code> for each row:</p>\n\n<pre><code>for i, row in df.iterrows():\n    top_numbers = row.nlargest(top_n).sum()\n</code></pre>\n\n<p>General solution with <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\" rel=\"noreferrer\"><code>numpy.argsort</code></a> for positions in <a href=\"https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\">descending order</a>, then compare and convert boolean array to integers:</p>\n\n<pre><code>def get_top_n(df, top_n):\n    if top_n &gt; len(df.columns):\n        raise ValueError(\"Value is higher as number of columns\")\n    elif not isinstance(top_n, int):\n        raise ValueError(\"Value is not integer\")\n\n    else:\n        arr = ((-df.values).argsort(axis=1) &lt; top_n).astype(int)\n        df1 = pd.DataFrame(arr, index=df.index, columns=df.columns)\n        return (df1)\n\ndf1 = get_top_n(df, 2)\nprint (df1)\n   A  B  C\n0  1  1  0\n1  1  1  0\n\ndf1 = get_top_n(df, 1)\nprint (df1)\n   A  B  C\n0  1  0  0\n1  0  1  0\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>Solution with <code>iterrows</code> is possible, but not recommended, because slow:</p>\n\n<pre><code>top_n = 2\nfor i, row in df.iterrows():\n    top = row.nlargest(top_n).index\n    df.loc[i] = 0\n    df.loc[i, top] = 1\n\nprint (df)\n   A  B  C\n0  1  1  0\n1  1  1  0\n</code></pre>\n",
        "question_body": "<p>I am trying to create a function that uses <code>df.iterrows()</code> and <code>Series.nlargest</code>. I want to iterate over each row and find the largest number and then mark it as a <code>1</code>. This is the data frame:</p>\n\n<pre><code>A   B    C\n9   6    5\n3   7    2\n</code></pre>\n\n<p>Here is the output I wish to have:</p>\n\n<pre><code>A    B   C\n1    0   0\n0    1   0\n</code></pre>\n\n<p>This is the function I wish to use here:</p>\n\n<pre><code>def get_top_n(df, top_n):\n    \"\"\"\n\n\n    Parameters\n    ----------\n    df : DataFrame\n\n    top_n : int\n        The top number to get\n    Returns\n    -------\n    top_numbers : DataFrame\n    Returns the top number marked with a 1\n\n    \"\"\"\n    # Implement Function\n    for row in df.iterrows():\n        top_numbers = row.nlargest(top_n).sum()\n\n    return top_numbers\n</code></pre>\n\n<p>I get the following error:\nAttributeError: 'tuple' object has no attribute 'nlargest'</p>\n\n<p>Help would be appreciated on how to re-write my function in a neater way and to actually work! Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 51645934,
            "link": "https://stackoverflow.com/questions/51645934/using-iterrows-with-series-nlargest-to-get-the-highest-number-in-a-row-in-a",
            "question": {
                "title": "Using .iterrows() with series.nlargest() to get the highest number in a row in a Dataframe",
                "ques_desc": "I am trying to create a function that uses and . I want to iterate over each row and find the largest number and then mark it as a . This is the data frame: Here is the output I wish to have: This is the function I wish to use here: I get the following error: AttributeError: 'tuple' object has no attribute 'nlargest' Help would be appreciated on how to re-write my function in a neater way and to actually work! Thanks in advance "
            },
            "io": [
                "A   B    C\n9   6    5\n3   7    2\n",
                "A    B   C\n1    0   0\n0    1   0\n"
            ],
            "answer": {
                "ans_desc": "Add variable, because return indices with for each row: General solution with for positions in descending order, then compare and convert boolean array to integers: EDIT: Solution with is possible, but not recommended, because slow: ",
                "code": [
                    "for i, row in df.iterrows():\n    top_numbers = row.nlargest(top_n).sum()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 155,
            "user_id": 7644267,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1419426528099359/picture?type=large",
            "display_name": "Edu Galindo",
            "link": "https://stackoverflow.com/users/7644267/edu-galindo"
        },
        "is_answered": true,
        "view_count": 2189,
        "accepted_answer_id": 53820180,
        "answer_count": 7,
        "score": 3,
        "last_activity_date": 1545188153,
        "creation_date": 1545067138,
        "last_edit_date": 1545188153,
        "question_id": 53820131,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53820131/how-to-create-a-dataframe-from-numpy-arrays",
        "title": "How to create a dataframe from numpy arrays?",
        "body": "<p>I am trying to create a matrix / DataFrame with the numbers stored in 2 variables</p>\n\n<pre><code>x = np.linspace(0,50)\ny = np.exp(x)\n</code></pre>\n\n<p>and I would like them to look like this:</p>\n\n<pre><code>x    |     y\n___________________\n0    |     1.0...\n1    |     2.77...             \n2    |     7.6...                      \n...  |     ...             \n50   |     5.18e+21...    \n</code></pre>\n\n<p>I would like it to be in a <code>DataFrame</code> so I can work with it with the <code>pandas</code> library. </p>\n\n<p>Thanks in advance     </p>\n",
        "answer_body": "<p>With <code>pandas</code>:</p>\n\n<p>You can issue</p>\n\n<pre><code>&gt;&gt;&gt; xs = np.arange(51)                                                                                                 \n&gt;&gt;&gt; ys = np.exp(xs) \n</code></pre>\n\n<p>to get the x and y values and then build your dataframe with</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'x': xs, 'y': ys})\n&gt;&gt;&gt; df                                                                                                                 \n     x             y\n0    0  1.000000e+00\n1    1  2.718282e+00\n2    2  7.389056e+00\n3    3  2.008554e+01\n...\n</code></pre>\n\n<p>In this case, you can also use the x-values as the index of a series without losing any information.</p>\n\n<pre><code>&gt;&gt;&gt; index = pd.RangeIndex(0, 51, name='x')                                                                             \n&gt;&gt;&gt; exps = pd.Series(data=np.exp(index), index=index, name='y')                                                        \n&gt;&gt;&gt; exps                                                                                                               \nx\n0     1.000000e+00\n1     2.718282e+00\n2     7.389056e+00\n3     2.008554e+01\n...\nName: y, dtype: float64\n</code></pre>\n\n<hr>\n\n<p>Without <code>pandas</code>:</p>\n\n<p>Consider if you truly need a dataframe or series. You could just leave it at </p>\n\n<pre><code>&gt;&gt;&gt; xs = np.arange(51)                                                                                                 \n&gt;&gt;&gt; ys = np.exp(xs)\n</code></pre>\n\n<p>and then index into <code>ys</code> with the integers <code>0</code>, <code>1</code>, <code>2</code>, ... to get the values of <code>exp(0)</code>, <code>exp(1)</code>, <code>exp(2)</code>, ...</p>\n",
        "question_body": "<p>I am trying to create a matrix / DataFrame with the numbers stored in 2 variables</p>\n\n<pre><code>x = np.linspace(0,50)\ny = np.exp(x)\n</code></pre>\n\n<p>and I would like them to look like this:</p>\n\n<pre><code>x    |     y\n___________________\n0    |     1.0...\n1    |     2.77...             \n2    |     7.6...                      \n...  |     ...             \n50   |     5.18e+21...    \n</code></pre>\n\n<p>I would like it to be in a <code>DataFrame</code> so I can work with it with the <code>pandas</code> library. </p>\n\n<p>Thanks in advance     </p>\n",
        "formatted_input": {
            "qid": 53820131,
            "link": "https://stackoverflow.com/questions/53820131/how-to-create-a-dataframe-from-numpy-arrays",
            "question": {
                "title": "How to create a dataframe from numpy arrays?",
                "ques_desc": "I am trying to create a matrix / DataFrame with the numbers stored in 2 variables and I would like them to look like this: I would like it to be in a so I can work with it with the library. Thanks in advance "
            },
            "io": [
                "x = np.linspace(0,50)\ny = np.exp(x)\n",
                "x    |     y\n___________________\n0    |     1.0...\n1    |     2.77...             \n2    |     7.6...                      \n...  |     ...             \n50   |     5.18e+21...    \n"
            ],
            "answer": {
                "ans_desc": "With : You can issue to get the x and y values and then build your dataframe with In this case, you can also use the x-values as the index of a series without losing any information. Without : Consider if you truly need a dataframe or series. You could just leave it at and then index into with the integers , , , ... to get the values of , , , ... ",
                "code": [
                    ">>> xs = np.arange(51)                                                                                                 \n>>> ys = np.exp(xs) \n",
                    ">>> xs = np.arange(51)                                                                                                 \n>>> ys = np.exp(xs)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "multi-index",
            "fillna"
        ],
        "owner": {
            "reputation": 238,
            "user_id": 10701542,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-kTg1XAvA9K8/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-jtu_0PM87HwVyeUFVDsC8r1tO4rg/mo/photo.jpg?sz=128",
            "display_name": "David",
            "link": "https://stackoverflow.com/users/10701542/david"
        },
        "is_answered": true,
        "view_count": 449,
        "accepted_answer_id": 53733583,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1544738749,
        "creation_date": 1544567907,
        "last_edit_date": 1544598980,
        "question_id": 53733422,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53733422/replace-nan-value-by-last-valid-value-for-only-one-column-in-a-dataframe-with",
        "title": "Replace &quot;NaN&quot; value by last valid value for only one column in a dataframe with column multi-index (df.fillna)",
        "body": "<p>I'm working with Python 3.6.5.</p>\n\n<p>Here is a little script to generate a multi index dataframe with some \"NaN\" value.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\natt_1 = ['X', 'Y']\natt_2 = ['a', 'b']\n\ndf_1 = pd.DataFrame(np.random.randint(10,19,size=(5, 2)), columns=att_2, \nindex=[10,20,30,35,40])\ndf_2 = pd.DataFrame(np.random.randint(20,29,size=(5, 2)), columns=att_2, \nindex=[20,25,40,50,80])\n\n# Concat df with new key dimension for column attribute\ndf = pd.concat([df_1, df_2], keys=att_1, axis=1)\n</code></pre>\n\n<p>I get this dataframe</p>\n\n<pre><code>print(df)\n       X           Y      \n       a     b     a     b\n10  17.0  17.0   NaN   NaN\n20  15.0  11.0  20.0  28.0\n25   NaN   NaN  23.0  24.0\n30  12.0  16.0   NaN   NaN\n35  10.0  10.0   NaN   NaN\n40  15.0  14.0  25.0  28.0\n50   NaN   NaN  22.0  22.0\n80   NaN   NaN  23.0  21.0\n</code></pre>\n\n<p>And I would like to replace the \"NaN\" value with the last valid value, BUT ONLY FOR ONE COLUMN. For example, I would like to get this (for column named 'X','b')</p>\n\n<pre><code>print(df)\n       X           Y      \n       a     b     a     b\n10  17.0  17.0   NaN   NaN\n20  15.0  11.0  20.0  28.0\n25   NaN  11.0  23.0  24.0\n30  12.0  16.0   NaN   NaN\n35  10.0  10.0   NaN   NaN\n40  15.0  14.0  25.0  28.0\n50   NaN  14.0  22.0  22.0\n80   NaN  14.0  23.0  21.0\n</code></pre>\n\n<p>I tried this :</p>\n\n<pre><code># Replace NaN value by last valid value for column named 'X','b'\ndf['X']['b'].fillna(method='ffill', inplace=True)\n</code></pre>\n\n<p>But I get this error <strong>\"A value is trying to be set on a copy of a slice from a DataFrame\"</strong></p>\n\n<p>I can not find any solution for a dataframe with multi-index of column.\nI found this link that gives me no hope. (<a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.MultiIndex.fillna.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.MultiIndex.fillna.html</a>)</p>\n\n<p>Does anyone have an idea to help me?</p>\n",
        "answer_body": "<p>After some digging, I found that there's a more appropriate way of referencing columns that we want to edit specifically. Check <a href=\"https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\">How to deal with SettingWithCopyWarning in Pandas?</a> out for more info. Another resource: <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\" rel=\"nofollow noreferrer\">http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy</a>. </p>\n\n<p>It is recommended that we use <code>.loc</code> to alter columns. Using the line below removed any errors.</p>\n\n<pre><code>df.loc[df['X']['b'].isnull(), ('X', 'b')] = df['X']['b'].ffill()\n</code></pre>\n\n<p>However, here I am using the max of the column to replace every <code>NaN</code> with. I'm uncertain on what is meant by the last valid value.</p>\n",
        "question_body": "<p>I'm working with Python 3.6.5.</p>\n\n<p>Here is a little script to generate a multi index dataframe with some \"NaN\" value.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\natt_1 = ['X', 'Y']\natt_2 = ['a', 'b']\n\ndf_1 = pd.DataFrame(np.random.randint(10,19,size=(5, 2)), columns=att_2, \nindex=[10,20,30,35,40])\ndf_2 = pd.DataFrame(np.random.randint(20,29,size=(5, 2)), columns=att_2, \nindex=[20,25,40,50,80])\n\n# Concat df with new key dimension for column attribute\ndf = pd.concat([df_1, df_2], keys=att_1, axis=1)\n</code></pre>\n\n<p>I get this dataframe</p>\n\n<pre><code>print(df)\n       X           Y      \n       a     b     a     b\n10  17.0  17.0   NaN   NaN\n20  15.0  11.0  20.0  28.0\n25   NaN   NaN  23.0  24.0\n30  12.0  16.0   NaN   NaN\n35  10.0  10.0   NaN   NaN\n40  15.0  14.0  25.0  28.0\n50   NaN   NaN  22.0  22.0\n80   NaN   NaN  23.0  21.0\n</code></pre>\n\n<p>And I would like to replace the \"NaN\" value with the last valid value, BUT ONLY FOR ONE COLUMN. For example, I would like to get this (for column named 'X','b')</p>\n\n<pre><code>print(df)\n       X           Y      \n       a     b     a     b\n10  17.0  17.0   NaN   NaN\n20  15.0  11.0  20.0  28.0\n25   NaN  11.0  23.0  24.0\n30  12.0  16.0   NaN   NaN\n35  10.0  10.0   NaN   NaN\n40  15.0  14.0  25.0  28.0\n50   NaN  14.0  22.0  22.0\n80   NaN  14.0  23.0  21.0\n</code></pre>\n\n<p>I tried this :</p>\n\n<pre><code># Replace NaN value by last valid value for column named 'X','b'\ndf['X']['b'].fillna(method='ffill', inplace=True)\n</code></pre>\n\n<p>But I get this error <strong>\"A value is trying to be set on a copy of a slice from a DataFrame\"</strong></p>\n\n<p>I can not find any solution for a dataframe with multi-index of column.\nI found this link that gives me no hope. (<a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.MultiIndex.fillna.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.MultiIndex.fillna.html</a>)</p>\n\n<p>Does anyone have an idea to help me?</p>\n",
        "formatted_input": {
            "qid": 53733422,
            "link": "https://stackoverflow.com/questions/53733422/replace-nan-value-by-last-valid-value-for-only-one-column-in-a-dataframe-with",
            "question": {
                "title": "Replace &quot;NaN&quot; value by last valid value for only one column in a dataframe with column multi-index (df.fillna)",
                "ques_desc": "I'm working with Python 3.6.5. Here is a little script to generate a multi index dataframe with some \"NaN\" value. I get this dataframe And I would like to replace the \"NaN\" value with the last valid value, BUT ONLY FOR ONE COLUMN. For example, I would like to get this (for column named 'X','b') I tried this : But I get this error \"A value is trying to be set on a copy of a slice from a DataFrame\" I can not find any solution for a dataframe with multi-index of column. I found this link that gives me no hope. (https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.MultiIndex.fillna.html) Does anyone have an idea to help me? "
            },
            "io": [
                "print(df)\n       X           Y      \n       a     b     a     b\n10  17.0  17.0   NaN   NaN\n20  15.0  11.0  20.0  28.0\n25   NaN   NaN  23.0  24.0\n30  12.0  16.0   NaN   NaN\n35  10.0  10.0   NaN   NaN\n40  15.0  14.0  25.0  28.0\n50   NaN   NaN  22.0  22.0\n80   NaN   NaN  23.0  21.0\n",
                "print(df)\n       X           Y      \n       a     b     a     b\n10  17.0  17.0   NaN   NaN\n20  15.0  11.0  20.0  28.0\n25   NaN  11.0  23.0  24.0\n30  12.0  16.0   NaN   NaN\n35  10.0  10.0   NaN   NaN\n40  15.0  14.0  25.0  28.0\n50   NaN  14.0  22.0  22.0\n80   NaN  14.0  23.0  21.0\n"
            ],
            "answer": {
                "ans_desc": "After some digging, I found that there's a more appropriate way of referencing columns that we want to edit specifically. Check How to deal with SettingWithCopyWarning in Pandas? out for more info. Another resource: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy. It is recommended that we use to alter columns. Using the line below removed any errors. However, here I am using the max of the column to replace every with. I'm uncertain on what is meant by the last valid value. ",
                "code": [
                    "df.loc[df['X']['b'].isnull(), ('X', 'b')] = df['X']['b'].ffill()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "sorting",
            "dataframe",
            "indexing"
        ],
        "owner": {
            "reputation": 1165,
            "user_id": 5831856,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0e49bfa0a7ca52a00f071552366eb6fd?s=128&d=identicon&r=PG&f=1",
            "display_name": "mathguy",
            "link": "https://stackoverflow.com/users/5831856/mathguy"
        },
        "is_answered": true,
        "view_count": 61,
        "accepted_answer_id": 53769385,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1544732354,
        "creation_date": 1544730935,
        "last_edit_date": 1544731794,
        "question_id": 53769189,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53769189/rearrange-rows-of-dataframe-alternatively",
        "title": "Rearrange rows of Dataframe alternatively",
        "body": "<p>I have a dataframe looks like this:</p>\n\n<pre><code>df = pd.DataFrame({'col1': [i+1 for i in range(10)] + [-i-1 for i in range(10)],\n                   'col2': ['random string'] *20})\nprint(df)\n    col1           col2\n0      1  random string\n1      2  random string\n2      3  random string\n3      4  random string\n4      5  random string\n5      6  random string\n6      7  random string\n7      8  random string\n8      9  random string\n9     10  random string\n10    -1  random string\n11    -2  random string\n12    -3  random string\n13    -4  random string\n14    -5  random string\n15    -6  random string\n16    -7  random string\n17    -8  random string\n18    -9  random string\n19   -10  random string\n</code></pre>\n\n<p>and I want to make it look like this:</p>\n\n<pre><code>   col1           col2\n0     1  random string\n1    -1  random string\n2     2  random string\n3    -2  random string\n4     3  random string\n5    -3  random string\n6     4  random string\n7    -4  random string\n8     5  random string\n9    -5  random string\n10    6  random string\n11   -6  random string\n12    7  random string\n13   -7  random string\n14    8  random string\n15   -8  random string\n16    9  random string\n17   -9  random string\n18   10  random string\n19  -10  random string\n</code></pre>\n\n<p>My own way to do it seems to take quite a few lines, aka not pythonic. My code:</p>\n\n<pre><code>df2 = pd.DataFrame(index = df.index,columns = df.columns)\n\nYpos = df[df['col1'] &gt; 0]\nYneg = df[df['col1'] &lt; 0]\n\nind_pos = [2*i for i in range(10)]\nind_neg = [2*i+1 for i in range(10)]\n\ndf2.loc[ind_pos] = Ypos.rename({k:v for k,v in zip(Ypos.index, ind_pos)})\ndf2.loc[ind_neg] = Yneg.rename({k:v for k,v in zip(Yneg.index, ind_neg)})\nprint(df2)\n</code></pre>\n\n<p>Is there any more pythonic way to accomplish the same result? Thank you in advance.</p>\n\n<p>EDIT: I'd like a more general method to deal with dataframe like this</p>\n\n<pre><code>   col1           col2\n0     1  random string\n1     2  random string\n2     3  random string\n3     4  random string\n4     5  random string\n5    1x  random string\n6    2x  random string\n7    3x  random string\n8    4x  random string\n9    5x  random string\n10   1y  random string\n11   2y  random string\n12   3y  random string\n13   4y  random string\n14   5y  random string\n</code></pre>\n",
        "answer_body": "<p>If the size of the subgroups is known, let's call it <code>n</code>, and your <code>DataFrame</code> is chunked with each group following the other, we just need some math:</p>\n\n<pre><code>n=5\n\ndf.index = df.index%n + (df.index//n)/(len(df)/n)\ndf = df.sort_index().reset_index(drop=True)\n</code></pre>\n\n<h3>Output:</h3>\n\n<pre><code>   col1           col2\n0     1  random_string\n1    1x  random_string\n2    1y  random_string\n3     2  random_string\n4    2x  random_string\n5    2y  random_string\n6     3  random_string\n7    3x  random_string\n8    3y  random_string\n9     4  random_string\n10   4x  random_string\n11   4y  random_string\n12    5  random_string\n13   5x  random_string\n14   5y  random_string\n</code></pre>\n",
        "question_body": "<p>I have a dataframe looks like this:</p>\n\n<pre><code>df = pd.DataFrame({'col1': [i+1 for i in range(10)] + [-i-1 for i in range(10)],\n                   'col2': ['random string'] *20})\nprint(df)\n    col1           col2\n0      1  random string\n1      2  random string\n2      3  random string\n3      4  random string\n4      5  random string\n5      6  random string\n6      7  random string\n7      8  random string\n8      9  random string\n9     10  random string\n10    -1  random string\n11    -2  random string\n12    -3  random string\n13    -4  random string\n14    -5  random string\n15    -6  random string\n16    -7  random string\n17    -8  random string\n18    -9  random string\n19   -10  random string\n</code></pre>\n\n<p>and I want to make it look like this:</p>\n\n<pre><code>   col1           col2\n0     1  random string\n1    -1  random string\n2     2  random string\n3    -2  random string\n4     3  random string\n5    -3  random string\n6     4  random string\n7    -4  random string\n8     5  random string\n9    -5  random string\n10    6  random string\n11   -6  random string\n12    7  random string\n13   -7  random string\n14    8  random string\n15   -8  random string\n16    9  random string\n17   -9  random string\n18   10  random string\n19  -10  random string\n</code></pre>\n\n<p>My own way to do it seems to take quite a few lines, aka not pythonic. My code:</p>\n\n<pre><code>df2 = pd.DataFrame(index = df.index,columns = df.columns)\n\nYpos = df[df['col1'] &gt; 0]\nYneg = df[df['col1'] &lt; 0]\n\nind_pos = [2*i for i in range(10)]\nind_neg = [2*i+1 for i in range(10)]\n\ndf2.loc[ind_pos] = Ypos.rename({k:v for k,v in zip(Ypos.index, ind_pos)})\ndf2.loc[ind_neg] = Yneg.rename({k:v for k,v in zip(Yneg.index, ind_neg)})\nprint(df2)\n</code></pre>\n\n<p>Is there any more pythonic way to accomplish the same result? Thank you in advance.</p>\n\n<p>EDIT: I'd like a more general method to deal with dataframe like this</p>\n\n<pre><code>   col1           col2\n0     1  random string\n1     2  random string\n2     3  random string\n3     4  random string\n4     5  random string\n5    1x  random string\n6    2x  random string\n7    3x  random string\n8    4x  random string\n9    5x  random string\n10   1y  random string\n11   2y  random string\n12   3y  random string\n13   4y  random string\n14   5y  random string\n</code></pre>\n",
        "formatted_input": {
            "qid": 53769189,
            "link": "https://stackoverflow.com/questions/53769189/rearrange-rows-of-dataframe-alternatively",
            "question": {
                "title": "Rearrange rows of Dataframe alternatively",
                "ques_desc": "I have a dataframe looks like this: and I want to make it look like this: My own way to do it seems to take quite a few lines, aka not pythonic. My code: Is there any more pythonic way to accomplish the same result? Thank you in advance. EDIT: I'd like a more general method to deal with dataframe like this "
            },
            "io": [
                "   col1           col2\n0     1  random string\n1    -1  random string\n2     2  random string\n3    -2  random string\n4     3  random string\n5    -3  random string\n6     4  random string\n7    -4  random string\n8     5  random string\n9    -5  random string\n10    6  random string\n11   -6  random string\n12    7  random string\n13   -7  random string\n14    8  random string\n15   -8  random string\n16    9  random string\n17   -9  random string\n18   10  random string\n19  -10  random string\n",
                "   col1           col2\n0     1  random string\n1     2  random string\n2     3  random string\n3     4  random string\n4     5  random string\n5    1x  random string\n6    2x  random string\n7    3x  random string\n8    4x  random string\n9    5x  random string\n10   1y  random string\n11   2y  random string\n12   3y  random string\n13   4y  random string\n14   5y  random string\n"
            ],
            "answer": {
                "ans_desc": "If the size of the subgroups is known, let's call it , and your is chunked with each group following the other, we just need some math: Output: ",
                "code": [
                    "n=5\n\ndf.index = df.index%n + (df.index//n)/(len(df)/n)\ndf = df.sort_index().reset_index(drop=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 7586893,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/eb6e472ad80fe6a0dd9b3c8c27606019?s=128&d=identicon&r=PG&f=1",
            "display_name": "SereneWizard",
            "link": "https://stackoverflow.com/users/7586893/serenewizard"
        },
        "is_answered": true,
        "view_count": 1562,
        "accepted_answer_id": 53687460,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1544320063,
        "creation_date": 1544305012,
        "last_edit_date": 1544320063,
        "question_id": 53687204,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53687204/check-one-on-one-relationship-between-two-columns",
        "title": "Check one-on-one relationship between two columns",
        "body": "<p>I have two columns A and B in a pandas dataframe, where values are repeated multiple times. For a unique value in A, B is expected to have \"another\" unique value too. And each unique value of A has a corresponding unique value in B (See example below in the form of two lists). But since each value in each column is repeated multiple times, I would like to check if any one-to-one relationship exists between two columns or not. Is there any inbuilt function in pandas to check that? If not, is there an efficient way of achieving that task? </p>\n\n<p>Example: </p>\n\n<pre><code>A = [1, 3, 3, 2, 1, 2, 1, 1]\nB = [5, 12, 12, 10, 5, 10, 5, 5]\n</code></pre>\n\n<p>Here, for each 1 in A, the corresponding value in B is always 5, and nothing else. Similarly, for 2-->10, and for 3-->12. Hence, each number in A has only one/unique corresponding number in B (and no other number). I have called this one-on-one relationship. Now I want to check if such relationship exists between two columns in pandas dataframe or not. </p>\n\n<p>An example where this relationship is not satisfied: </p>\n\n<pre><code>A = [1, 3, 3, 2, 1, 2, 1, 1]\nB = [5, 12, 12, 10, 5, 10, 7, 5]\n</code></pre>\n\n<p>Here, 1 in A doesn't have a unique corresponding value in B. It has two corresponding values - 5 and 7. Hence, the relationship is not satisfied. </p>\n",
        "answer_body": "<p>Consider you have some dataframe:</p>\n\n<pre><code> d = df({'A': [1, 3, 1, 2, 1, 3, 2], 'B': [4, 6, 4, 5, 4, 6, 5]})\n</code></pre>\n\n<p><code>d</code> has <code>groupby</code> method, which returns <a href=\"https://pandas.pydata.org/pandas-docs/stable/groupby.html\" rel=\"nofollow noreferrer\"><code>GroupBy</code> object</a>. This is the interface to group some rows by equal column value, for example.</p>\n\n<pre><code> gb = d.groupby('A')\n grouped_b_column = gb['B']\n</code></pre>\n\n<p>On grouped rows you could perform an aggregation. Lets find min and max value in every group.</p>\n\n<pre><code>res = grouped_b_column.agg([np.min, np.max])\n\n&gt;&gt;&gt; print(res)\n   amin  amax\nA            \n1     4     4\n2     5     5\n3     6     6\n</code></pre>\n\n<p>Now we just should check that <code>amin</code> and <code>amax</code> are equal in every group, so every group consists of equal <code>B</code> fields:</p>\n\n<pre><code>res['amin'].equals(res['amax'])\n</code></pre>\n\n<p>If this check is OK, then for every <code>A</code> you have unique <code>B</code>. Now you should check the same criteria for <code>A</code> and <code>B</code> columns swapped.</p>\n",
        "question_body": "<p>I have two columns A and B in a pandas dataframe, where values are repeated multiple times. For a unique value in A, B is expected to have \"another\" unique value too. And each unique value of A has a corresponding unique value in B (See example below in the form of two lists). But since each value in each column is repeated multiple times, I would like to check if any one-to-one relationship exists between two columns or not. Is there any inbuilt function in pandas to check that? If not, is there an efficient way of achieving that task? </p>\n\n<p>Example: </p>\n\n<pre><code>A = [1, 3, 3, 2, 1, 2, 1, 1]\nB = [5, 12, 12, 10, 5, 10, 5, 5]\n</code></pre>\n\n<p>Here, for each 1 in A, the corresponding value in B is always 5, and nothing else. Similarly, for 2-->10, and for 3-->12. Hence, each number in A has only one/unique corresponding number in B (and no other number). I have called this one-on-one relationship. Now I want to check if such relationship exists between two columns in pandas dataframe or not. </p>\n\n<p>An example where this relationship is not satisfied: </p>\n\n<pre><code>A = [1, 3, 3, 2, 1, 2, 1, 1]\nB = [5, 12, 12, 10, 5, 10, 7, 5]\n</code></pre>\n\n<p>Here, 1 in A doesn't have a unique corresponding value in B. It has two corresponding values - 5 and 7. Hence, the relationship is not satisfied. </p>\n",
        "formatted_input": {
            "qid": 53687204,
            "link": "https://stackoverflow.com/questions/53687204/check-one-on-one-relationship-between-two-columns",
            "question": {
                "title": "Check one-on-one relationship between two columns",
                "ques_desc": "I have two columns A and B in a pandas dataframe, where values are repeated multiple times. For a unique value in A, B is expected to have \"another\" unique value too. And each unique value of A has a corresponding unique value in B (See example below in the form of two lists). But since each value in each column is repeated multiple times, I would like to check if any one-to-one relationship exists between two columns or not. Is there any inbuilt function in pandas to check that? If not, is there an efficient way of achieving that task? Example: Here, for each 1 in A, the corresponding value in B is always 5, and nothing else. Similarly, for 2-->10, and for 3-->12. Hence, each number in A has only one/unique corresponding number in B (and no other number). I have called this one-on-one relationship. Now I want to check if such relationship exists between two columns in pandas dataframe or not. An example where this relationship is not satisfied: Here, 1 in A doesn't have a unique corresponding value in B. It has two corresponding values - 5 and 7. Hence, the relationship is not satisfied. "
            },
            "io": [
                "A = [1, 3, 3, 2, 1, 2, 1, 1]\nB = [5, 12, 12, 10, 5, 10, 5, 5]\n",
                "A = [1, 3, 3, 2, 1, 2, 1, 1]\nB = [5, 12, 12, 10, 5, 10, 7, 5]\n"
            ],
            "answer": {
                "ans_desc": "Consider you have some dataframe: has method, which returns object. This is the interface to group some rows by equal column value, for example. On grouped rows you could perform an aggregation. Lets find min and max value in every group. Now we just should check that and are equal in every group, so every group consists of equal fields: If this check is OK, then for every you have unique . Now you should check the same criteria for and columns swapped. ",
                "code": [
                    " gb = d.groupby('A')\n grouped_b_column = gb['B']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2005,
            "user_id": 3879858,
            "user_type": "registered",
            "accept_rate": 74,
            "profile_image": "https://www.gravatar.com/avatar/50664a4ca094f147d526e739406e6726?s=128&d=identicon&r=PG&f=1",
            "display_name": "s900n",
            "link": "https://stackoverflow.com/users/3879858/s900n"
        },
        "is_answered": true,
        "view_count": 190,
        "accepted_answer_id": 53654809,
        "answer_count": 8,
        "score": 11,
        "last_activity_date": 1544113789,
        "creation_date": 1544110405,
        "last_edit_date": 1544111209,
        "question_id": 53654729,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53654729/pandas-dataframe-remove-secondary-upcoming-same-value",
        "title": "Pandas dataframe: Remove secondary upcoming same value",
        "body": "<p>I have a dataframe:</p>\n\n<pre><code>col1  col2\n a     0\n b     1\n c     1\n d     0\n c     1\n d     0\n</code></pre>\n\n<p>On <code>'col2'</code> I want to keep only the first <code>1</code> from the top and replace every <code>1</code> below the first one with a <code>0</code>, such that the output is:</p>\n\n<pre><code>col1  col2\n a     0\n b     1\n c     0\n d     0\n c     0\n d     0\n</code></pre>\n\n<p>Thank you very much. </p>\n",
        "answer_body": "<p>You can find the index of the first <code>1</code> and set others to <code>0</code>:</p>\n\n<pre><code>mask = df['col2'].eq(1)\ndf.loc[mask &amp; (df.index != mask.idxmax()), 'col2'] = 0\n</code></pre>\n\n<p>For better performance, see <a href=\"https://stackoverflow.com/questions/53020764/efficiently-return-the-index-of-the-first-value-satisfying-condition-in-array\">Efficiently return the index of the first value satisfying condition in array</a>.</p>\n",
        "question_body": "<p>I have a dataframe:</p>\n\n<pre><code>col1  col2\n a     0\n b     1\n c     1\n d     0\n c     1\n d     0\n</code></pre>\n\n<p>On <code>'col2'</code> I want to keep only the first <code>1</code> from the top and replace every <code>1</code> below the first one with a <code>0</code>, such that the output is:</p>\n\n<pre><code>col1  col2\n a     0\n b     1\n c     0\n d     0\n c     0\n d     0\n</code></pre>\n\n<p>Thank you very much. </p>\n",
        "formatted_input": {
            "qid": 53654729,
            "link": "https://stackoverflow.com/questions/53654729/pandas-dataframe-remove-secondary-upcoming-same-value",
            "question": {
                "title": "Pandas dataframe: Remove secondary upcoming same value",
                "ques_desc": "I have a dataframe: On I want to keep only the first from the top and replace every below the first one with a , such that the output is: Thank you very much. "
            },
            "io": [
                "col1  col2\n a     0\n b     1\n c     1\n d     0\n c     1\n d     0\n",
                "col1  col2\n a     0\n b     1\n c     0\n d     0\n c     0\n d     0\n"
            ],
            "answer": {
                "ans_desc": "You can find the index of the first and set others to : For better performance, see Efficiently return the index of the first value satisfying condition in array. ",
                "code": [
                    "mask = df['col2'].eq(1)\ndf.loc[mask & (df.index != mask.idxmax()), 'col2'] = 0\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "file-io"
        ],
        "owner": {
            "reputation": 475,
            "user_id": 9296990,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/dc468e3e5ffb875693abe69e5f379f97?s=128&d=identicon&r=PG&f=1",
            "display_name": "mrsquid",
            "link": "https://stackoverflow.com/users/9296990/mrsquid"
        },
        "is_answered": true,
        "view_count": 13763,
        "accepted_answer_id": 48832098,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1544026006,
        "creation_date": 1518801294,
        "last_edit_date": 1518810185,
        "question_id": 48831802,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/48831802/outputting-pandas-series-to-txt-file",
        "title": "Outputting pandas series to txt file",
        "body": "<p>I have a pandas series object </p>\n\n<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre>\n\n<p>that look like this:</p>\n\n<pre><code>userId\n1          3072 1196 838 2278 1259\n2               648 475 1 151 1035\n3               457 150 300 21 339\n4          1035 7153 953 4993 2571\n5           260 671 1210 2628 7153\n6          4993 1210 2291 589 1196\n7               150 457 111 246 25\n8       1221 8132 30749 44191 1721\n9           296 377 2858 3578 3256\n10          2762 377 2858 1617 858\n11           527 593 2396 318 1258\n12        3578 2683 2762 2571 2580\n13        7153 150 5952 35836 2028\n14        1197 2580 2712 2762 1968\n15        1245 1090 1080 2529 1261\n16         296 2324 4993 7153 1203\n17       1208 1234 6796 55820 1060\n18            1377 1 1073 1356 592\n19           778 1173 272 3022 909\n20              329 534 377 73 272\n21            608 904 903 1204 111\n22       1221 1136 1258 4973 48516\n23        1214 1200 1148 2761 2791\n24             593 318 162 480 733\n25               314 969 25 85 766\n26        293 253 4878 46578 64614\n27          1193 2716 24 2959 2841\n28         318 260 58559 8961 4226\n29            318 260 1196 2959 50\n30        1077 1136 1230 1203 3481\n\n642            123 593 750 1212 50\n643         750 671 1663 2427 5618\n644            780 3114 1584 11 62\n645         912 2858 1617 1035 903\n646           608 527 21 2710 1704\n647         1196 720 5060 2599 594\n648         46578 50 745 1223 5995\n649            318 300 110 529 246\n650            733 110 151 318 364\n651         1240 1210 541 589 1247\n652      4993 296 95510 122900 736\n653            858 1225 1961 25 36\n654        333 1221 3039 1610 4011\n655           318 47 6377 527 2028\n656          527 1193 1073 1265 73\n657             527 349 454 357 97\n658            457 590 480 589 329\n659              474 508 1 288 477\n660         904 1197 1247 858 1221\n661           780 1527 3 1376 5481\n662             110 590 50 593 733\n663          2028 919 527 2791 110\n664    1201 64839 1228 122886 1203\n665        1197 858 7153 1221 6539\n666            318 300 161 500 337\n667            527 260 318 593 223\n668            161 527 151 110 300\n669          50 2858 4993 318 2628\n670          296 5952 508 272 1196\n671         1210 1200 7153 593 110\n</code></pre>\n\n<p>What is the best way to go about outputting this to a txt file (e.g. output.txt) such that the format look like this?</p>\n\n<pre><code>User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\nUser-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\n</code></pre>\n\n<p>The values on the far left are the userId's and the other values are the movieId's.</p>\n\n<p>Here is the code that generated the above:</p>\n\n<pre><code>import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef predict(l):\n    # finds the userIds corresponding to the top 5 similarities\n    # calculate the prediction according to the formula\n    return (df[l.index] * l).sum(axis=1) / l.sum()\n\n\n# use userID as columns for convinience when interpretering the forumla\ndf = pd.read_csv('ratings.csv').pivot(columns='userId',\n                                                index='movieId',\n                                                values='rating')\ndf = df - df.mean()\nsimilarity = pd.DataFrame(cosine_similarity(\n    df.T.fillna(0)), index=df.columns, columns=df.columns)\n\nres = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in (0 * col).fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\n\n\n\n#Do not understand why this does not work for me but works below\ndf = pd.DataFrame.from_items(zip(res.index, res.str.split(' ')))\n#print(df)\ndf.columns = ['movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']\ndf['customer_id'] = df.index\ndf = df[['customer_id', 'movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']]\ndf.to_csv('filepath.txt', sep=' ', index=False)\n</code></pre>\n\n<p>I tried implementing @emmet02 solution but got this error, I do not understand why I got it though:</p>\n\n<pre><code>ValueError: Length mismatch: Expected axis has 671 elements, new values have 5 elements\n</code></pre>\n\n<p>Any advice is appreciated, please let me know if you need any more information or clarification.</p>\n",
        "answer_body": "<p>I would suggest turning your pd.Series into a pd.DataFrame first.</p>\n\n<pre><code>df = pd.DataFrame.from_items(zip(series.index, series.str.split(' '))).T\n</code></pre>\n\n<p>So long as the Series has the same number of values (for every entry!), separated by a space, this will return a dataframe in this format</p>\n\n<pre><code>Out[49]: \n      0     1    2     3     4\n0  3072   648  457  1035   260\n1  1196   475  150  7153   671\n2   838     1  300   953  1210\n3  2278   151   21  4993  2628\n4  1259  1035  339  2571  7153\n</code></pre>\n\n<p>Next I would name the columns appropriately</p>\n\n<pre><code>df.columns = ['movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']\n</code></pre>\n\n<p>Finally, the dataframe is indexed by customer id (I am supposing this based upon your series index). We want to move that into the dataframe, and then reorganise the columns.</p>\n\n<pre><code>df['customer_id'] = df.index\ndf = df[['customer_id', 'movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']]\n</code></pre>\n\n<p>This now leaves you with a dataframe like this </p>\n\n<pre><code>  customer_id movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\n0            0      3072       648       457      1035       260\n1            1      1196       475       150      7153       671\n2            2       838         1       300       953      1210\n3            3      2278       151        21      4993      2628\n4            4      1259      1035       339      2571      7153\n</code></pre>\n\n<p>which I would recommend you write to disk as a csv using </p>\n\n<pre><code>df.to_csv('filepath.csv', index=False)\n</code></pre>\n\n<p>If however you want to write it as a text file, with only spaces separating, you can use the same function but pass the separator.</p>\n\n<pre><code>df.to_csv('filepath.txt', sep=' ', index=False)\n</code></pre>\n\n<p>I don't think that the Series object is the correct choice of data structure for the problem you want to solve. Treating numerical data as numerical data (and in a DataFrame) is far easier than maintaining 'space delimited string' conversions imo.</p>\n",
        "question_body": "<p>I have a pandas series object </p>\n\n<pre><code>&lt;class 'pandas.core.series.Series'&gt;\n</code></pre>\n\n<p>that look like this:</p>\n\n<pre><code>userId\n1          3072 1196 838 2278 1259\n2               648 475 1 151 1035\n3               457 150 300 21 339\n4          1035 7153 953 4993 2571\n5           260 671 1210 2628 7153\n6          4993 1210 2291 589 1196\n7               150 457 111 246 25\n8       1221 8132 30749 44191 1721\n9           296 377 2858 3578 3256\n10          2762 377 2858 1617 858\n11           527 593 2396 318 1258\n12        3578 2683 2762 2571 2580\n13        7153 150 5952 35836 2028\n14        1197 2580 2712 2762 1968\n15        1245 1090 1080 2529 1261\n16         296 2324 4993 7153 1203\n17       1208 1234 6796 55820 1060\n18            1377 1 1073 1356 592\n19           778 1173 272 3022 909\n20              329 534 377 73 272\n21            608 904 903 1204 111\n22       1221 1136 1258 4973 48516\n23        1214 1200 1148 2761 2791\n24             593 318 162 480 733\n25               314 969 25 85 766\n26        293 253 4878 46578 64614\n27          1193 2716 24 2959 2841\n28         318 260 58559 8961 4226\n29            318 260 1196 2959 50\n30        1077 1136 1230 1203 3481\n\n642            123 593 750 1212 50\n643         750 671 1663 2427 5618\n644            780 3114 1584 11 62\n645         912 2858 1617 1035 903\n646           608 527 21 2710 1704\n647         1196 720 5060 2599 594\n648         46578 50 745 1223 5995\n649            318 300 110 529 246\n650            733 110 151 318 364\n651         1240 1210 541 589 1247\n652      4993 296 95510 122900 736\n653            858 1225 1961 25 36\n654        333 1221 3039 1610 4011\n655           318 47 6377 527 2028\n656          527 1193 1073 1265 73\n657             527 349 454 357 97\n658            457 590 480 589 329\n659              474 508 1 288 477\n660         904 1197 1247 858 1221\n661           780 1527 3 1376 5481\n662             110 590 50 593 733\n663          2028 919 527 2791 110\n664    1201 64839 1228 122886 1203\n665        1197 858 7153 1221 6539\n666            318 300 161 500 337\n667            527 260 318 593 223\n668            161 527 151 110 300\n669          50 2858 4993 318 2628\n670          296 5952 508 272 1196\n671         1210 1200 7153 593 110\n</code></pre>\n\n<p>What is the best way to go about outputting this to a txt file (e.g. output.txt) such that the format look like this?</p>\n\n<pre><code>User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\nUser-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\n</code></pre>\n\n<p>The values on the far left are the userId's and the other values are the movieId's.</p>\n\n<p>Here is the code that generated the above:</p>\n\n<pre><code>import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef predict(l):\n    # finds the userIds corresponding to the top 5 similarities\n    # calculate the prediction according to the formula\n    return (df[l.index] * l).sum(axis=1) / l.sum()\n\n\n# use userID as columns for convinience when interpretering the forumla\ndf = pd.read_csv('ratings.csv').pivot(columns='userId',\n                                                index='movieId',\n                                                values='rating')\ndf = df - df.mean()\nsimilarity = pd.DataFrame(cosine_similarity(\n    df.T.fillna(0)), index=df.columns, columns=df.columns)\n\nres = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in (0 * col).fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\n\n\n\n#Do not understand why this does not work for me but works below\ndf = pd.DataFrame.from_items(zip(res.index, res.str.split(' ')))\n#print(df)\ndf.columns = ['movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']\ndf['customer_id'] = df.index\ndf = df[['customer_id', 'movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']]\ndf.to_csv('filepath.txt', sep=' ', index=False)\n</code></pre>\n\n<p>I tried implementing @emmet02 solution but got this error, I do not understand why I got it though:</p>\n\n<pre><code>ValueError: Length mismatch: Expected axis has 671 elements, new values have 5 elements\n</code></pre>\n\n<p>Any advice is appreciated, please let me know if you need any more information or clarification.</p>\n",
        "formatted_input": {
            "qid": 48831802,
            "link": "https://stackoverflow.com/questions/48831802/outputting-pandas-series-to-txt-file",
            "question": {
                "title": "Outputting pandas series to txt file",
                "ques_desc": "I have a pandas series object that look like this: What is the best way to go about outputting this to a txt file (e.g. output.txt) such that the format look like this? The values on the far left are the userId's and the other values are the movieId's. Here is the code that generated the above: I tried implementing @emmet02 solution but got this error, I do not understand why I got it though: Any advice is appreciated, please let me know if you need any more information or clarification. "
            },
            "io": [
                "userId\n1          3072 1196 838 2278 1259\n2               648 475 1 151 1035\n3               457 150 300 21 339\n4          1035 7153 953 4993 2571\n5           260 671 1210 2628 7153\n6          4993 1210 2291 589 1196\n7               150 457 111 246 25\n8       1221 8132 30749 44191 1721\n9           296 377 2858 3578 3256\n10          2762 377 2858 1617 858\n11           527 593 2396 318 1258\n12        3578 2683 2762 2571 2580\n13        7153 150 5952 35836 2028\n14        1197 2580 2712 2762 1968\n15        1245 1090 1080 2529 1261\n16         296 2324 4993 7153 1203\n17       1208 1234 6796 55820 1060\n18            1377 1 1073 1356 592\n19           778 1173 272 3022 909\n20              329 534 377 73 272\n21            608 904 903 1204 111\n22       1221 1136 1258 4973 48516\n23        1214 1200 1148 2761 2791\n24             593 318 162 480 733\n25               314 969 25 85 766\n26        293 253 4878 46578 64614\n27          1193 2716 24 2959 2841\n28         318 260 58559 8961 4226\n29            318 260 1196 2959 50\n30        1077 1136 1230 1203 3481\n\n642            123 593 750 1212 50\n643         750 671 1663 2427 5618\n644            780 3114 1584 11 62\n645         912 2858 1617 1035 903\n646           608 527 21 2710 1704\n647         1196 720 5060 2599 594\n648         46578 50 745 1223 5995\n649            318 300 110 529 246\n650            733 110 151 318 364\n651         1240 1210 541 589 1247\n652      4993 296 95510 122900 736\n653            858 1225 1961 25 36\n654        333 1221 3039 1610 4011\n655           318 47 6377 527 2028\n656          527 1193 1073 1265 73\n657             527 349 454 357 97\n658            457 590 480 589 329\n659              474 508 1 288 477\n660         904 1197 1247 858 1221\n661           780 1527 3 1376 5481\n662             110 590 50 593 733\n663          2028 919 527 2791 110\n664    1201 64839 1228 122886 1203\n665        1197 858 7153 1221 6539\n666            318 300 161 500 337\n667            527 260 318 593 223\n668            161 527 151 110 300\n669          50 2858 4993 318 2628\n670          296 5952 508 272 1196\n671         1210 1200 7153 593 110\n",
                "User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\nUser-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5\n"
            ],
            "answer": {
                "ans_desc": "I would suggest turning your pd.Series into a pd.DataFrame first. So long as the Series has the same number of values (for every entry!), separated by a space, this will return a dataframe in this format Next I would name the columns appropriately Finally, the dataframe is indexed by customer id (I am supposing this based upon your series index). We want to move that into the dataframe, and then reorganise the columns. This now leaves you with a dataframe like this which I would recommend you write to disk as a csv using If however you want to write it as a text file, with only spaces separating, you can use the same function but pass the separator. I don't think that the Series object is the correct choice of data structure for the problem you want to solve. Treating numerical data as numerical data (and in a DataFrame) is far easier than maintaining 'space delimited string' conversions imo. ",
                "code": [
                    "df = pd.DataFrame.from_items(zip(series.index, series.str.split(' '))).T\n",
                    "df.columns = ['movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']\n",
                    "df['customer_id'] = df.index\ndf = df[['customer_id', 'movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 312,
            "user_id": 10133617,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dd05020b136e880da69323a46b6b8716?s=128&d=identicon&r=PG&f=1",
            "display_name": "Siddharth Bachoti",
            "link": "https://stackoverflow.com/users/10133617/siddharth-bachoti"
        },
        "is_answered": true,
        "view_count": 286,
        "accepted_answer_id": 53611971,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1543923580,
        "creation_date": 1543922714,
        "question_id": 53611917,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53611917/swapping-of-elements-in-a-pandas-dataframe",
        "title": "Swapping of elements in a PANDAS dataframe",
        "body": "<p>Given below is a table :</p>\n\n<pre><code>    A NUMBER    B NUMBER\n    7042967611  9999574081\n    12320       9999574081\n    9999574081  9810256463\n    9999574081  9716551924\n    9716551924  9999574081  \n    9999574081  8130945859\n</code></pre>\n\n<p>This was originally an excel sheet which has been converted into a dataframe. I wish to swap some of the elements such that the A number column has only 9999574081.\nTherefore the output should look like :</p>\n\n<pre><code>    A NUMBER    B NUMBER\n    9999574081  7042967611  \n    9999574081  12320       \n    9999574081  9810256463\n    9999574081  9716551924\n    9999574081  9716551924  \n    9999574081  8130945859\n</code></pre>\n\n<p>This is the code I have used :</p>\n\n<pre><code>for i in list(df['A NUMBER']):\n    j=0\n    if i!= 9999574081:\n        temp = df['B NUMBER'][j]\n        df['B NUMBER'][j] = i\n        df['A NUMBER'][j] = temp\n    j+=1\n</code></pre>\n\n<p>However, I am not getting the desired result. Please help me out. Thanks:)</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>DataFrame.loc</code></a> for swap only rows matched boolean mask, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.values.html\" rel=\"nofollow noreferrer\"><code>values</code></a> is necessary for avoid align index values:</p>\n\n<pre><code>m = df['A NUMBER'] != 9999574081\n\ndf.loc[m, ['A NUMBER','B NUMBER']] = df.loc[m, ['B NUMBER','A NUMBER']].values\n</code></pre>\n\n<p>Another solution with <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\" rel=\"nofollow noreferrer\"><code>numpy.where</code></a>:</p>\n\n<pre><code>df['B NUMBER'] = np.where(df['A NUMBER'] != 9999574081, df['A NUMBER'], df['B NUMBER'])\ndf['A NUMBER'] = 9999574081\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n     A NUMBER    B NUMBER\n0  9999574081  7042967611\n1  9999574081       12320\n2  9999574081  9810256463\n3  9999574081  9716551924\n4  9999574081  9716551924\n5  9999574081  8130945859\n</code></pre>\n",
        "question_body": "<p>Given below is a table :</p>\n\n<pre><code>    A NUMBER    B NUMBER\n    7042967611  9999574081\n    12320       9999574081\n    9999574081  9810256463\n    9999574081  9716551924\n    9716551924  9999574081  \n    9999574081  8130945859\n</code></pre>\n\n<p>This was originally an excel sheet which has been converted into a dataframe. I wish to swap some of the elements such that the A number column has only 9999574081.\nTherefore the output should look like :</p>\n\n<pre><code>    A NUMBER    B NUMBER\n    9999574081  7042967611  \n    9999574081  12320       \n    9999574081  9810256463\n    9999574081  9716551924\n    9999574081  9716551924  \n    9999574081  8130945859\n</code></pre>\n\n<p>This is the code I have used :</p>\n\n<pre><code>for i in list(df['A NUMBER']):\n    j=0\n    if i!= 9999574081:\n        temp = df['B NUMBER'][j]\n        df['B NUMBER'][j] = i\n        df['A NUMBER'][j] = temp\n    j+=1\n</code></pre>\n\n<p>However, I am not getting the desired result. Please help me out. Thanks:)</p>\n",
        "formatted_input": {
            "qid": 53611917,
            "link": "https://stackoverflow.com/questions/53611917/swapping-of-elements-in-a-pandas-dataframe",
            "question": {
                "title": "Swapping of elements in a PANDAS dataframe",
                "ques_desc": "Given below is a table : This was originally an excel sheet which has been converted into a dataframe. I wish to swap some of the elements such that the A number column has only 9999574081. Therefore the output should look like : This is the code I have used : However, I am not getting the desired result. Please help me out. Thanks:) "
            },
            "io": [
                "    A NUMBER    B NUMBER\n    7042967611  9999574081\n    12320       9999574081\n    9999574081  9810256463\n    9999574081  9716551924\n    9716551924  9999574081  \n    9999574081  8130945859\n",
                "    A NUMBER    B NUMBER\n    9999574081  7042967611  \n    9999574081  12320       \n    9999574081  9810256463\n    9999574081  9716551924\n    9999574081  9716551924  \n    9999574081  8130945859\n"
            ],
            "answer": {
                "ans_desc": "Use for swap only rows matched boolean mask, is necessary for avoid align index values: Another solution with : ",
                "code": [
                    "m = df['A NUMBER'] != 9999574081\n\ndf.loc[m, ['A NUMBER','B NUMBER']] = df.loc[m, ['B NUMBER','A NUMBER']].values\n",
                    "df['B NUMBER'] = np.where(df['A NUMBER'] != 9999574081, df['A NUMBER'], df['B NUMBER'])\ndf['A NUMBER'] = 9999574081\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "search",
            "pattern-matching"
        ],
        "owner": {
            "reputation": 509,
            "user_id": 7507925,
            "user_type": "registered",
            "accept_rate": 69,
            "profile_image": "https://graph.facebook.com/10212060276686137/picture?type=large",
            "display_name": "Cdhippen",
            "link": "https://stackoverflow.com/users/7507925/cdhippen"
        },
        "is_answered": true,
        "view_count": 506,
        "accepted_answer_id": 53611654,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1543921919,
        "creation_date": 1543875734,
        "question_id": 53602854,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53602854/how-to-identify-string-repetition-throughout-rows-of-a-column-in-a-pandas-datafr",
        "title": "How to identify string repetition throughout rows of a column in a Pandas DataFrame?",
        "body": "<p>I'm trying to think of a way to best handle this. If I have a data frame like this:</p>\n\n<pre><code>Module---|-Line Item---|---Formula-----------------------------------------|-repetition?|--What repeated--------------------------------|---Where repeated\nModule 1-|Line Item 1--|---hello[SUM: hello2]------------------------------|----yes-----|--hello[SUM: hello2]---------------------------|---Module 1 Line item 2\nModule 1-|Line Item 2--|---goodbye[LOOKUP: blue123] + hello[SUM: hello2]---|----yes-----|--hello[SUM: hello2], goodbye[LOOKUP: blue123]-|---Module 1 Line item 1, Module 2 Line Item 1\nModule 2-|Line Item 1--|---goodbye[LOOKUP: blue123] + some other line item-|----yes-----|--goodbye[LOOKUP: blue123]---------------------|---Module 1 Line item 2\n</code></pre>\n\n<p>How would I go about setting up a search and find to locate and identify repetition in the middle or on edges or complete strings?</p>\n\n<p>Sorry the formatting looks bad\nBasically I have the module, line item, and formula columns filled in, but I need to figure out some sort of search function that I can apply to each of the last 3 columns. I'm not sure where to start with this.</p>\n\n<p>I want to match any repetition that occurs between 3 or more words, including if for example a formula was <code>1 + 2 + 3 + 4</code> and that occurred 4 times in the Formula column, I'd want to give a yes to the boolean column \"repetition\" return <code>1 + 2 + 3 + 4</code> on the \"Where repeated\" column and a list of every module/line item combination where it occurred on the last column. I'm sure I can tweak it more to fit my needs once I get started.</p>\n",
        "answer_body": "<p>This one was a bit messy, is surely some more straight forward way to do some of the steps, but it worked for your data.</p>\n\n<p>Step 1: I just reset_index() (assuming index uses row numbers) to get row numbers into a column.</p>\n\n<p><code>df.reset_index(inplace=True)</code></p>\n\n<p>I then wrote a for loop which aim was to check for each given value, if that value is at any place in the given column (using the <code>.str.contains()</code> function, and if so, where. And then store that information in a dictionary. Note that here I used <code>+</code> to split the various values you search by as that looked to be a valid separator in your dataset, but you can adjust this accordingly</p>\n\n<pre><code>#the dictionary will have a key containing row number and the value we searched for\n#the value will contain the module and line item values\nresult = {}\n#create a rownumber variable so we know where in the dataset we are\nrownumber = -1\n#now we just iterate over every row of the Formula series\nfor row in df['Formula']:\n    rownumber +=1\n    #and also every relevant value within that cell\n    for value in row.split('+'):\n        #we clean the value from trailing/preceding whitespace\n        value = value.strip()\n        #and then we return our key and value and update our dictionary\n        key = 'row:|:'+str(rownumber)+':|:'+value\n        value = (df.loc[((df.Formula.str.contains(value,regex=False))) &amp; (df.index!=rownumber),['Module','Line Item']])\n        result.update({key:value})\n</code></pre>\n\n<p>We can now unpack the dictionary into list, where we had a match:</p>\n\n<pre><code>where_raw = []\nwhat_raw = []\nrows_raw = []\nfor key,value in zip(result.keys(),result.values()):\n    if 'Empty' in str(value):\n        continue\n    else:\n        where_raw.append(list(value['Module']+' '+value['Line Item']))\n        what_raw.append(key.split(':|:')[2])\n        rows_raw.append(int(key.split(':|:')[1]))\n\ntempdf = pd.DataFrame({'row':rows_raw,'where':where_raw,'what':what_raw})\n</code></pre>\n\n<p><code>tempdf</code> now contains one row per match, however, we want to have one row per original row in the df, so we combine all matches for each main row into one</p>\n\n<pre><code>where = []\nwhat = []\nrows = []        \n\nfor row in tempdf.row.unique():\n    where.append(list(tempdf.loc[tempdf.row==row,'where']))\n    what.append(list(tempdf.loc[tempdf.row==row,'what']))\n    rows.append(row)\nresult = df.merge(pd.DataFrame({'index':rows,'where':where,'what':what}))\n</code></pre>\n\n<p>lastly we can now get the result by merging the result with our original dataframe</p>\n\n<p><code>result = df.merge(pd.DataFrame({'index':rows,'where':where,'what':what}),how='left',on='index').drop('index',axis=1)</code></p>\n\n<p>and lastly we can add the <code>repeated</code> column like this:\n<code>result['repeated'] = (result['what']!='')</code></p>\n\n<pre><code> print(result)\n Module     Line Item   Formula                                         what                                                 where\n Module 1   Line Item 1 hello[SUM: hello2]                              ['hello[SUM: hello2]']                               [['Module 1 Line Item 2']]\n Module 1   Line Item 2 goodbye[LOOKUP: blue123] + hello[SUM: hello2]   ['goodbye[LOOKUP: blue123]', 'hello[SUM: hello2]']   [['Module 2 Line Item 1'], ['Module 1 Line Item 1']]\n Module 2   Line Item 1 goodbye[LOOKUP: blue123] + some other line item ['goodbye[LOOKUP: blue123]']                         [['Module 1 Line Item 2']]\n</code></pre>\n",
        "question_body": "<p>I'm trying to think of a way to best handle this. If I have a data frame like this:</p>\n\n<pre><code>Module---|-Line Item---|---Formula-----------------------------------------|-repetition?|--What repeated--------------------------------|---Where repeated\nModule 1-|Line Item 1--|---hello[SUM: hello2]------------------------------|----yes-----|--hello[SUM: hello2]---------------------------|---Module 1 Line item 2\nModule 1-|Line Item 2--|---goodbye[LOOKUP: blue123] + hello[SUM: hello2]---|----yes-----|--hello[SUM: hello2], goodbye[LOOKUP: blue123]-|---Module 1 Line item 1, Module 2 Line Item 1\nModule 2-|Line Item 1--|---goodbye[LOOKUP: blue123] + some other line item-|----yes-----|--goodbye[LOOKUP: blue123]---------------------|---Module 1 Line item 2\n</code></pre>\n\n<p>How would I go about setting up a search and find to locate and identify repetition in the middle or on edges or complete strings?</p>\n\n<p>Sorry the formatting looks bad\nBasically I have the module, line item, and formula columns filled in, but I need to figure out some sort of search function that I can apply to each of the last 3 columns. I'm not sure where to start with this.</p>\n\n<p>I want to match any repetition that occurs between 3 or more words, including if for example a formula was <code>1 + 2 + 3 + 4</code> and that occurred 4 times in the Formula column, I'd want to give a yes to the boolean column \"repetition\" return <code>1 + 2 + 3 + 4</code> on the \"Where repeated\" column and a list of every module/line item combination where it occurred on the last column. I'm sure I can tweak it more to fit my needs once I get started.</p>\n",
        "formatted_input": {
            "qid": 53602854,
            "link": "https://stackoverflow.com/questions/53602854/how-to-identify-string-repetition-throughout-rows-of-a-column-in-a-pandas-datafr",
            "question": {
                "title": "How to identify string repetition throughout rows of a column in a Pandas DataFrame?",
                "ques_desc": "I'm trying to think of a way to best handle this. If I have a data frame like this: How would I go about setting up a search and find to locate and identify repetition in the middle or on edges or complete strings? Sorry the formatting looks bad Basically I have the module, line item, and formula columns filled in, but I need to figure out some sort of search function that I can apply to each of the last 3 columns. I'm not sure where to start with this. I want to match any repetition that occurs between 3 or more words, including if for example a formula was and that occurred 4 times in the Formula column, I'd want to give a yes to the boolean column \"repetition\" return on the \"Where repeated\" column and a list of every module/line item combination where it occurred on the last column. I'm sure I can tweak it more to fit my needs once I get started. "
            },
            "io": [
                "1 + 2 + 3 + 4",
                "1 + 2 + 3 + 4"
            ],
            "answer": {
                "ans_desc": "This one was a bit messy, is surely some more straight forward way to do some of the steps, but it worked for your data. Step 1: I just reset_index() (assuming index uses row numbers) to get row numbers into a column. I then wrote a for loop which aim was to check for each given value, if that value is at any place in the given column (using the function, and if so, where. And then store that information in a dictionary. Note that here I used to split the various values you search by as that looked to be a valid separator in your dataset, but you can adjust this accordingly We can now unpack the dictionary into list, where we had a match: now contains one row per match, however, we want to have one row per original row in the df, so we combine all matches for each main row into one lastly we can now get the result by merging the result with our original dataframe and lastly we can add the column like this: ",
                "code": [
                    "#the dictionary will have a key containing row number and the value we searched for\n#the value will contain the module and line item values\nresult = {}\n#create a rownumber variable so we know where in the dataset we are\nrownumber = -1\n#now we just iterate over every row of the Formula series\nfor row in df['Formula']:\n    rownumber +=1\n    #and also every relevant value within that cell\n    for value in row.split('+'):\n        #we clean the value from trailing/preceding whitespace\n        value = value.strip()\n        #and then we return our key and value and update our dictionary\n        key = 'row:|:'+str(rownumber)+':|:'+value\n        value = (df.loc[((df.Formula.str.contains(value,regex=False))) & (df.index!=rownumber),['Module','Line Item']])\n        result.update({key:value})\n",
                    "where_raw = []\nwhat_raw = []\nrows_raw = []\nfor key,value in zip(result.keys(),result.values()):\n    if 'Empty' in str(value):\n        continue\n    else:\n        where_raw.append(list(value['Module']+' '+value['Line Item']))\n        what_raw.append(key.split(':|:')[2])\n        rows_raw.append(int(key.split(':|:')[1]))\n\ntempdf = pd.DataFrame({'row':rows_raw,'where':where_raw,'what':what_raw})\n",
                    "where = []\nwhat = []\nrows = []        \n\nfor row in tempdf.row.unique():\n    where.append(list(tempdf.loc[tempdf.row==row,'where']))\n    what.append(list(tempdf.loc[tempdf.row==row,'what']))\n    rows.append(row)\nresult = df.merge(pd.DataFrame({'index':rows,'where':where,'what':what}))\n",
                    " print(result)\n Module     Line Item   Formula                                         what                                                 where\n Module 1   Line Item 1 hello[SUM: hello2]                              ['hello[SUM: hello2]']                               [['Module 1 Line Item 2']]\n Module 1   Line Item 2 goodbye[LOOKUP: blue123] + hello[SUM: hello2]   ['goodbye[LOOKUP: blue123]', 'hello[SUM: hello2]']   [['Module 2 Line Item 1'], ['Module 1 Line Item 1']]\n Module 2   Line Item 1 goodbye[LOOKUP: blue123] + some other line item ['goodbye[LOOKUP: blue123]']                         [['Module 1 Line Item 2']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "flatten",
            "columnsorting"
        ],
        "owner": {
            "reputation": 329,
            "user_id": 10444642,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/2137151472964826/picture?type=large",
            "display_name": "Yannick",
            "link": "https://stackoverflow.com/users/10444642/yannick"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 53592969,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1543836782,
        "creation_date": 1543834084,
        "question_id": 53592186,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53592186/modify-and-flatten-values-from-pandas-dataframe",
        "title": "Modify and flatten values from Pandas dataframe",
        "body": "<p>Here is the dataframe I am working with: </p>\n\n<pre><code>            0\n0  380.143752\n1  379.942595\n2  379.589472\n3  379.816187\n4  379.622086\n5  379.299071\n6  379.559615\n</code></pre>\n\n<p>dtypes gives this:</p>\n\n<pre><code>0    float64\ndtype: object\n</code></pre>\n\n<p>You can get a sample of the data by click on the link below:</p>\n\n<p><a href=\"https://ufile.io/x534q\" rel=\"nofollow noreferrer\">https://ufile.io/x534q</a></p>\n\n<p>What I would like to do now is to get rid of the header, the first column (0 to 6) and to flatten the rest of values so that the end result looks like this:</p>\n\n<pre><code>380.143752 379.942595 379.589472 379.816187 379.622086 379.299071 379.559615\n</code></pre>\n\n<p>Could you please help me? Thanks in advance. </p>\n",
        "answer_body": "<p>A <strong>Transpose of the dataframe</strong> should give you what you need.</p>\n\n<pre><code>newdf = (pd.DataFrame(df)).T\n</code></pre>\n\n<p>This will still have headers though but you can use the data in your normal operations  ignoring the header and index</p>\n\n<p>e.g. write to a csv file</p>\n\n<pre><code>newdf.to_csv(file,mode='w', sep=',',header=False,index=False) # you can use any separator here 'tabs' if you don't want commas.\n</code></pre>\n",
        "question_body": "<p>Here is the dataframe I am working with: </p>\n\n<pre><code>            0\n0  380.143752\n1  379.942595\n2  379.589472\n3  379.816187\n4  379.622086\n5  379.299071\n6  379.559615\n</code></pre>\n\n<p>dtypes gives this:</p>\n\n<pre><code>0    float64\ndtype: object\n</code></pre>\n\n<p>You can get a sample of the data by click on the link below:</p>\n\n<p><a href=\"https://ufile.io/x534q\" rel=\"nofollow noreferrer\">https://ufile.io/x534q</a></p>\n\n<p>What I would like to do now is to get rid of the header, the first column (0 to 6) and to flatten the rest of values so that the end result looks like this:</p>\n\n<pre><code>380.143752 379.942595 379.589472 379.816187 379.622086 379.299071 379.559615\n</code></pre>\n\n<p>Could you please help me? Thanks in advance. </p>\n",
        "formatted_input": {
            "qid": 53592186,
            "link": "https://stackoverflow.com/questions/53592186/modify-and-flatten-values-from-pandas-dataframe",
            "question": {
                "title": "Modify and flatten values from Pandas dataframe",
                "ques_desc": "Here is the dataframe I am working with: dtypes gives this: You can get a sample of the data by click on the link below: https://ufile.io/x534q What I would like to do now is to get rid of the header, the first column (0 to 6) and to flatten the rest of values so that the end result looks like this: Could you please help me? Thanks in advance. "
            },
            "io": [
                "            0\n0  380.143752\n1  379.942595\n2  379.589472\n3  379.816187\n4  379.622086\n5  379.299071\n6  379.559615\n",
                "380.143752 379.942595 379.589472 379.816187 379.622086 379.299071 379.559615\n"
            ],
            "answer": {
                "ans_desc": "A Transpose of the dataframe should give you what you need. This will still have headers though but you can use the data in your normal operations ignoring the header and index e.g. write to a csv file ",
                "code": [
                    "newdf.to_csv(file,mode='w', sep=',',header=False,index=False) # you can use any separator here 'tabs' if you don't want commas.\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2720,
            "user_id": 5424617,
            "user_type": "registered",
            "accept_rate": 83,
            "profile_image": "https://www.gravatar.com/avatar/f86a85c89417fa560070c4aeb5aecc56?s=128&d=identicon&r=PG",
            "display_name": "Md. Rezwanul Haque",
            "link": "https://stackoverflow.com/users/5424617/md-rezwanul-haque"
        },
        "is_answered": true,
        "view_count": 468,
        "accepted_answer_id": 53590024,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1543828035,
        "creation_date": 1543825410,
        "question_id": 53589894,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53589894/how-to-handle-missing-data-with-respect-to-type-of-dataset",
        "title": "How to handle missing data with respect to type of dataset?",
        "body": "<p>I have a dataset where has column <strong>types</strong> that has type like <code>Primary</code> , <code>Secondary</code>.</p>\n\n<blockquote>\n  <p>df</p>\n</blockquote>\n\n<pre><code>   ID    types        C   D\n0  101   Primary      2   3\n1  103   Primary      6   3\n2  108   Primary     10   ?\n3  109   Primary      3  12\n4  118   Secondary    5   2\n5  122   Secondary    ?   6\n6  123   Secondary    5   6\n7  125   Secondary    2   5 \n</code></pre>\n\n<p>I want to replace missing value with <code>median</code> for each type. Such as-</p>\n\n<blockquote>\n  <p>result_df</p>\n</blockquote>\n\n<pre><code>   ID    types        C   D\n0  101   Primary      2   3\n1  103   Primary      6   3\n2  108   Primary     10   3\n3  109   Primary      3  12\n4  118   Secondary    5   2\n5  122   Secondary    5   6\n6  123   Secondary    5   6\n7  125   Secondary    2   5 \n</code></pre>\n\n<p>How can do it with Python?</p>\n",
        "answer_body": "<p>Something like this should work:</p>\n\n<p>First replace <code>?</code> in your df with actual <code>np.nan</code> values:</p>\n\n<pre><code>In [1268]: df = df.replace('?',np.nan)\nIn [1273]: df\nOut[1273]: \n    ID      types    C    D\n0  101    Primary    2    3\n1  103    Primary    6    3\n2  108    Primary   10  NaN\n3  109    Primary    3   12\n4  118  Secondary    5    2\n5  122  Secondary  NaN    6\n6  123  Secondary    5    6\n7  125  Secondary    2    5\n</code></pre>\n\n<blockquote>\n  <p><strong>For me the <code>dtypes</code> is showing as <code>object</code> for columns <code>C</code> and <code>D</code>. Hence, I convert these into numeric before finding median. If this is\n  not applicable for you, skip this step and directly run the below\n  command with <code>transform</code> function.</strong></p>\n</blockquote>\n\n<pre><code>In [1274]: df.dtypes\nOut[1274]: \nID        int64\ntypes    object\nC        object\nD        object\ndtype: object\n</code></pre>\n\n<p>In order to find <code>median</code>, convert columns <code>C</code> and <code>D</code> into pandas numeric type:</p>\n\n<pre><code>In [1256]: df.C = df.C.apply(pd.to_numeric)\nIn [1258]: df.D = df.D.apply(pd.to_numeric)\n\nIn [1279]: df.dtypes\nOut[1279]: \nID         int64\ntypes     object\nC        float64\nD        float64\ndtype: object\n</code></pre>\n\n<p>Now, you can fill nulls with <code>median</code> of types in both columns <code>C</code> and <code>D</code> like below, using <code>groupby</code> and <code>transform</code> functions:</p>\n\n<pre><code>In [1265]: df.C = df.C.fillna(df.groupby('types')['C'].transform('median'))\n\nIn [1266]: df.D = df.D.fillna(df.groupby('types')['D'].transform('median'))\n\nIn [1267]: df\nOut[1267]: \n    ID      types     C     D\n0  101    Primary   2.0   3.0\n1  103    Primary   6.0   3.0\n2  108    Primary  10.0   3.0\n3  109    Primary   3.0  12.0\n4  118  Secondary   5.0   2.0\n5  122  Secondary   5.0   6.0\n6  123  Secondary   5.0   6.0\n7  125  Secondary   2.0   5.0\n</code></pre>\n\n<p>Let me know if this helps.</p>\n",
        "question_body": "<p>I have a dataset where has column <strong>types</strong> that has type like <code>Primary</code> , <code>Secondary</code>.</p>\n\n<blockquote>\n  <p>df</p>\n</blockquote>\n\n<pre><code>   ID    types        C   D\n0  101   Primary      2   3\n1  103   Primary      6   3\n2  108   Primary     10   ?\n3  109   Primary      3  12\n4  118   Secondary    5   2\n5  122   Secondary    ?   6\n6  123   Secondary    5   6\n7  125   Secondary    2   5 \n</code></pre>\n\n<p>I want to replace missing value with <code>median</code> for each type. Such as-</p>\n\n<blockquote>\n  <p>result_df</p>\n</blockquote>\n\n<pre><code>   ID    types        C   D\n0  101   Primary      2   3\n1  103   Primary      6   3\n2  108   Primary     10   3\n3  109   Primary      3  12\n4  118   Secondary    5   2\n5  122   Secondary    5   6\n6  123   Secondary    5   6\n7  125   Secondary    2   5 \n</code></pre>\n\n<p>How can do it with Python?</p>\n",
        "formatted_input": {
            "qid": 53589894,
            "link": "https://stackoverflow.com/questions/53589894/how-to-handle-missing-data-with-respect-to-type-of-dataset",
            "question": {
                "title": "How to handle missing data with respect to type of dataset?",
                "ques_desc": "I have a dataset where has column types that has type like , . df I want to replace missing value with for each type. Such as- result_df How can do it with Python? "
            },
            "io": [
                "   ID    types        C   D\n0  101   Primary      2   3\n1  103   Primary      6   3\n2  108   Primary     10   ?\n3  109   Primary      3  12\n4  118   Secondary    5   2\n5  122   Secondary    ?   6\n6  123   Secondary    5   6\n7  125   Secondary    2   5 \n",
                "   ID    types        C   D\n0  101   Primary      2   3\n1  103   Primary      6   3\n2  108   Primary     10   3\n3  109   Primary      3  12\n4  118   Secondary    5   2\n5  122   Secondary    5   6\n6  123   Secondary    5   6\n7  125   Secondary    2   5 \n"
            ],
            "answer": {
                "ans_desc": "Something like this should work: First replace in your df with actual values: For me the is showing as for columns and . Hence, I convert these into numeric before finding median. If this is not applicable for you, skip this step and directly run the below command with function. In order to find , convert columns and into pandas numeric type: Now, you can fill nulls with of types in both columns and like below, using and functions: Let me know if this helps. ",
                "code": [
                    "In [1274]: df.dtypes\nOut[1274]: \nID        int64\ntypes    object\nC        object\nD        object\ndtype: object\n",
                    "In [1256]: df.C = df.C.apply(pd.to_numeric)\nIn [1258]: df.D = df.D.apply(pd.to_numeric)\n\nIn [1279]: df.dtypes\nOut[1279]: \nID         int64\ntypes     object\nC        float64\nD        float64\ndtype: object\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "statistics",
            "correlation"
        ],
        "owner": {
            "reputation": 6492,
            "user_id": 6163621,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://i.stack.imgur.com/yyhNK.jpg?s=128&g=1",
            "display_name": "elPastor",
            "link": "https://stackoverflow.com/users/6163621/elpastor"
        },
        "is_answered": true,
        "view_count": 9622,
        "accepted_answer_id": 52482007,
        "answer_count": 1,
        "score": 8,
        "last_activity_date": 1543356798,
        "creation_date": 1537711198,
        "last_edit_date": 1543356798,
        "question_id": 52466844,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52466844/pandas-corr-returning-nan-too-often",
        "title": "Pandas corr() returning NaN too often",
        "body": "<p>I'm attempting to run what I think should be a simple correlation function on a dataframe but it is returning NaN in places where I don't believe it should.  </p>\n\n<p><strong>Code:</strong></p>\n\n<pre><code># setup\nimport pandas as pd\nimport io\n\ncsv = io.StringIO(u'''\nid  date    num\nA   2018-08-01  99\nA   2018-08-02  50\nA   2018-08-03  100\nA   2018-08-04  100\nA   2018-08-05  100\nB   2018-07-31  500\nB   2018-08-01  100\nB   2018-08-02  100\nB   2018-08-03  0\nB   2018-08-05  100\nB   2018-08-06  500\nB   2018-08-07  500\nB   2018-08-08  100\nC   2018-08-01  100\nC   2018-08-02  50\nC   2018-08-03  100\nC   2018-08-06  300\n''')\n\ndf = pd.read_csv(csv, sep = '\\t')\n\n# Format manipulation\ndf = df[df['num'] &gt; 50]\ndf = df.pivot(index = 'date', columns = 'id', values = 'num')\ndf = pd.DataFrame(df.to_records())\n\n# Main correlation calculations\nprint df.iloc[:, 1:].corr()\n</code></pre>\n\n<p><strong>Subject DataFrame:</strong></p>\n\n<pre><code>       A      B      C\n0    NaN  500.0    NaN\n1   99.0  100.0  100.0\n2    NaN  100.0    NaN\n3  100.0    NaN  100.0\n4  100.0    NaN    NaN\n5  100.0  100.0    NaN\n6    NaN  500.0  300.0\n7    NaN  500.0    NaN\n8    NaN  100.0    NaN\n</code></pre>\n\n<p><strong>corr() Result:</strong></p>\n\n<pre><code>    A    B    C\nA  1.0  NaN  NaN\nB  NaN  1.0  1.0\nC  NaN  1.0  1.0\n</code></pre>\n\n<p>According to the (limited) <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html\" rel=\"noreferrer\">documentation</a> on the function, it should exclude \"NA/null values\".  Since there are overlapping values for each column, should the result not all be non-NaN?</p>\n\n<p>There are good discussions <a href=\"https://stackoverflow.com/questions/42579908/use-corr-to-get-the-correlation-between-two-columns\">here</a> and <a href=\"https://stackoverflow.com/questions/31619578/numpy-corrcoef-compute-correlation-matrix-while-ignoring-missing-data\">here</a>, but neither answered my question.  I've tried the <code>float64</code> idea discussed <a href=\"https://stackoverflow.com/questions/40453337/python-cannot-make-corr-work\">here</a>, but that failed as well.</p>\n\n<p>@hellpanderr's comment brought up a good point, I'm using 0.22.0</p>\n\n<p><em>Bonus question - I'm no mathematician, but how is there a 1:1 correlation between B and C in this result?</em></p>\n",
        "answer_body": "<p>The result seems to be an artefact of the data you work with. As you write, <code>NA</code>s are ignored, so it basically boils down to:</p>\n\n<pre><code>df[['B', 'C']].dropna()\n\n       B      C\n1  100.0  100.0\n6  500.0  300.0\n</code></pre>\n\n<p>So, there are only two values per column left for the calculation which should therefore <a href=\"https://stats.stackexchange.com/questions/94150/why-is-the-pearson-correlation-1-when-only-two-data-values-are-available/94152#94152\">lead to to correlation coefficients of <code>1</code></a>:</p>\n\n<pre><code>df[['B', 'C']].dropna().corr()\n\n     B    C\nB  1.0  1.0\nC  1.0  1.0\n</code></pre>\n\n<p>So, where do the <code>NA</code>s then come from for the remaining combinations?</p>\n\n<pre><code>df[['A', 'B']].dropna()\n\n       A      B\n1   99.0  100.0\n5  100.0  100.0\n\n\ndf[['A', 'C']].dropna()\n\n       A      C\n1   99.0  100.0\n3  100.0  100.0\n</code></pre>\n\n<p>So, also here you end up with only two values per column. The difference is that the columns <code>B</code> and <code>C</code> contain only one value (<code>100</code>) which gives a standard deviation of <code>0</code>:</p>\n\n<pre><code>df[['A', 'C']].dropna().std()\n\nA    0.707107\nC    0.000000\n</code></pre>\n\n<p>When the correlation coefficient is calculated, you divide by the standard deviation, which leads to a <code>NA</code>.</p>\n",
        "question_body": "<p>I'm attempting to run what I think should be a simple correlation function on a dataframe but it is returning NaN in places where I don't believe it should.  </p>\n\n<p><strong>Code:</strong></p>\n\n<pre><code># setup\nimport pandas as pd\nimport io\n\ncsv = io.StringIO(u'''\nid  date    num\nA   2018-08-01  99\nA   2018-08-02  50\nA   2018-08-03  100\nA   2018-08-04  100\nA   2018-08-05  100\nB   2018-07-31  500\nB   2018-08-01  100\nB   2018-08-02  100\nB   2018-08-03  0\nB   2018-08-05  100\nB   2018-08-06  500\nB   2018-08-07  500\nB   2018-08-08  100\nC   2018-08-01  100\nC   2018-08-02  50\nC   2018-08-03  100\nC   2018-08-06  300\n''')\n\ndf = pd.read_csv(csv, sep = '\\t')\n\n# Format manipulation\ndf = df[df['num'] &gt; 50]\ndf = df.pivot(index = 'date', columns = 'id', values = 'num')\ndf = pd.DataFrame(df.to_records())\n\n# Main correlation calculations\nprint df.iloc[:, 1:].corr()\n</code></pre>\n\n<p><strong>Subject DataFrame:</strong></p>\n\n<pre><code>       A      B      C\n0    NaN  500.0    NaN\n1   99.0  100.0  100.0\n2    NaN  100.0    NaN\n3  100.0    NaN  100.0\n4  100.0    NaN    NaN\n5  100.0  100.0    NaN\n6    NaN  500.0  300.0\n7    NaN  500.0    NaN\n8    NaN  100.0    NaN\n</code></pre>\n\n<p><strong>corr() Result:</strong></p>\n\n<pre><code>    A    B    C\nA  1.0  NaN  NaN\nB  NaN  1.0  1.0\nC  NaN  1.0  1.0\n</code></pre>\n\n<p>According to the (limited) <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html\" rel=\"noreferrer\">documentation</a> on the function, it should exclude \"NA/null values\".  Since there are overlapping values for each column, should the result not all be non-NaN?</p>\n\n<p>There are good discussions <a href=\"https://stackoverflow.com/questions/42579908/use-corr-to-get-the-correlation-between-two-columns\">here</a> and <a href=\"https://stackoverflow.com/questions/31619578/numpy-corrcoef-compute-correlation-matrix-while-ignoring-missing-data\">here</a>, but neither answered my question.  I've tried the <code>float64</code> idea discussed <a href=\"https://stackoverflow.com/questions/40453337/python-cannot-make-corr-work\">here</a>, but that failed as well.</p>\n\n<p>@hellpanderr's comment brought up a good point, I'm using 0.22.0</p>\n\n<p><em>Bonus question - I'm no mathematician, but how is there a 1:1 correlation between B and C in this result?</em></p>\n",
        "formatted_input": {
            "qid": 52466844,
            "link": "https://stackoverflow.com/questions/52466844/pandas-corr-returning-nan-too-often",
            "question": {
                "title": "Pandas corr() returning NaN too often",
                "ques_desc": "I'm attempting to run what I think should be a simple correlation function on a dataframe but it is returning NaN in places where I don't believe it should. Code: Subject DataFrame: corr() Result: According to the (limited) documentation on the function, it should exclude \"NA/null values\". Since there are overlapping values for each column, should the result not all be non-NaN? There are good discussions here and here, but neither answered my question. I've tried the idea discussed here, but that failed as well. @hellpanderr's comment brought up a good point, I'm using 0.22.0 Bonus question - I'm no mathematician, but how is there a 1:1 correlation between B and C in this result? "
            },
            "io": [
                "       A      B      C\n0    NaN  500.0    NaN\n1   99.0  100.0  100.0\n2    NaN  100.0    NaN\n3  100.0    NaN  100.0\n4  100.0    NaN    NaN\n5  100.0  100.0    NaN\n6    NaN  500.0  300.0\n7    NaN  500.0    NaN\n8    NaN  100.0    NaN\n",
                "    A    B    C\nA  1.0  NaN  NaN\nB  NaN  1.0  1.0\nC  NaN  1.0  1.0\n"
            ],
            "answer": {
                "ans_desc": "The result seems to be an artefact of the data you work with. As you write, s are ignored, so it basically boils down to: So, there are only two values per column left for the calculation which should therefore lead to to correlation coefficients of : So, where do the s then come from for the remaining combinations? So, also here you end up with only two values per column. The difference is that the columns and contain only one value () which gives a standard deviation of : When the correlation coefficient is calculated, you divide by the standard deviation, which leads to a . ",
                "code": [
                    "df[['A', 'C']].dropna().std()\n\nA    0.707107\nC    0.000000\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "counter",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 97,
            "user_id": 10565650,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a7e6d9f596f73390151f135670d51315?s=128&d=identicon&r=PG&f=1",
            "display_name": "LSF",
            "link": "https://stackoverflow.com/users/10565650/lsf"
        },
        "is_answered": true,
        "view_count": 361,
        "accepted_answer_id": 53471595,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1543178617,
        "creation_date": 1543176373,
        "last_edit_date": 1543178617,
        "question_id": 53471422,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53471422/drop-group-by-number-of-occurrence",
        "title": "drop group by number of occurrence",
        "body": "<p>Hi I want to delete the rows with the entries whose number of occurrence is smaller than a number, for example:</p>\n\n<pre><code>df = pd.DataFrame({'a': [1,2,3,2], 'b':[4,5,6,7], 'c':[0,1,3,2]})\ndf\n</code></pre>\n\n<pre class=\"lang-none prettyprint-override\"><code>   a  b  c\n0  1  4  0\n1  2  5  1\n2  3  6  3\n3  2  7  2\n</code></pre>\n\n<p>Here I want to delete all the rows if the number of occurrence in column 'a' is less than twice.<br>\nWanted output:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>   a  b  c\n1  2  5  1\n3  2  7  2\n</code></pre>\n\n<p>What I know: \nwe can find the number of occurrence by <code>condition = df['a'].value_counts() &lt; 2</code>, and it will give me something like:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>2    False\n3    True\n1    True\nName: a, dtype: int64\n</code></pre>\n\n<p>But I don't know how I should approach from here to delete the rows.<br>\nThanks in advance!</p>\n",
        "answer_body": "<h3><a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html\" rel=\"nofollow noreferrer\"><code>size</code></a></h3>\n\n<pre><code>res = df[df.groupby('a')['b'].transform('size') &gt;= 2]\n</code></pre>\n\n<p>The <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html\" rel=\"nofollow noreferrer\"><code>transform</code></a> method maps <code>df.groupby('a')['b'].size()</code> to <code>df</code> aligned with <code>df['a']</code>.</p>\n\n<h3><a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\" rel=\"nofollow noreferrer\"><code>value_counts</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\" rel=\"nofollow noreferrer\"><code>map</code></a></h3>\n\n<pre><code>s = df['a'].value_counts()\nres = df[df['a'].map(s) &gt;= 2]\n\nprint(res)\n\n   a  b  c\n1  2  5  1\n3  2  7  2\n</code></pre>\n",
        "question_body": "<p>Hi I want to delete the rows with the entries whose number of occurrence is smaller than a number, for example:</p>\n\n<pre><code>df = pd.DataFrame({'a': [1,2,3,2], 'b':[4,5,6,7], 'c':[0,1,3,2]})\ndf\n</code></pre>\n\n<pre class=\"lang-none prettyprint-override\"><code>   a  b  c\n0  1  4  0\n1  2  5  1\n2  3  6  3\n3  2  7  2\n</code></pre>\n\n<p>Here I want to delete all the rows if the number of occurrence in column 'a' is less than twice.<br>\nWanted output:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>   a  b  c\n1  2  5  1\n3  2  7  2\n</code></pre>\n\n<p>What I know: \nwe can find the number of occurrence by <code>condition = df['a'].value_counts() &lt; 2</code>, and it will give me something like:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>2    False\n3    True\n1    True\nName: a, dtype: int64\n</code></pre>\n\n<p>But I don't know how I should approach from here to delete the rows.<br>\nThanks in advance!</p>\n",
        "formatted_input": {
            "qid": 53471422,
            "link": "https://stackoverflow.com/questions/53471422/drop-group-by-number-of-occurrence",
            "question": {
                "title": "drop group by number of occurrence",
                "ques_desc": "Hi I want to delete the rows with the entries whose number of occurrence is smaller than a number, for example: Here I want to delete all the rows if the number of occurrence in column 'a' is less than twice. Wanted output: What I know: we can find the number of occurrence by , and it will give me something like: But I don't know how I should approach from here to delete the rows. Thanks in advance! "
            },
            "io": [
                "   a  b  c\n0  1  4  0\n1  2  5  1\n2  3  6  3\n3  2  7  2\n",
                "   a  b  c\n1  2  5  1\n3  2  7  2\n"
            ],
            "answer": {
                "ans_desc": " + The method maps to aligned with . + ",
                "code": [
                    "res = df[df.groupby('a')['b'].transform('size') >= 2]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 309,
            "user_id": 10226862,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-uiX-cuuSAqM/AAAAAAAAAAI/AAAAAAAAAAA/AAnnY7r7hdhBGNOe6PQ2GS2Crjrc_TNPSw/mo/photo.jpg?sz=128",
            "display_name": "Arthur Coimbra",
            "link": "https://stackoverflow.com/users/10226862/arthur-coimbra"
        },
        "is_answered": true,
        "view_count": 409,
        "accepted_answer_id": 53357683,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1542513083,
        "creation_date": 1542505533,
        "last_edit_date": 1542511375,
        "question_id": 53357204,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53357204/how-to-do-a-calculation-only-at-some-rows-of-my-dataframe",
        "title": "How to do a calculation only at some rows of my dataframe?",
        "body": "<p>Let's say I have a dataframe with only two columns and 20 rows, where all values from the first column are equal to 10, and all values from the second row are random percentage numbers.</p>\n\n<p>Now, I want to multiply the first column with the percentage values of the second column +1, but only at some intervals, and copy the last value to the next row.</p>\n\n<p>E.g. I want to do this multiplication operation from row 5 to 10.</p>\n\n<p>The problem Is that I don't know to start and end the calculation in arbitrary spots based on the df's index.</p>\n\n<p>Example input data:</p>\n\n<pre><code>df = pd.DataFrame(np.random.randint(0,10,size=(20, 2)), columns=list('AB'))\ndf['A'] = 10\ndf['B'] = df['B'] /100\n</code></pre>\n\n<p>Which produces:</p>\n\n<pre><code>      A     B\n0   10  0.07\n1   10  0.02\n2   10  0.05\n3   10  0.00\n4   10  0.01\n5   10  0.09\n6   10  0.00\n7   10  0.02\n8   10  0.03\n9   10  0.05\n10  10  0.05\n11  10  0.03\n12  10  0.01\n13  10  0.09\n14  10  0.06\n15  10  0.07\n16  10  0.01\n17  10  0.01\n18  10  0.01\n19  10  0.07\n</code></pre>\n\n<p>An output I would like to get, is where the first row go thorugh a comulative multiplication only at sow rows, like this:</p>\n\n<pre><code>      C       B\n0   10     0.07\n1   10     0.02\n2   10     0.05\n3   10     0.00\n4   10     0.01\n5   10,9   0.09\n6   10,9   0.00\n7   11,11  0.02\n8   11,45  0.03\n9   12,02  0.05\n10  12,62  0.05\n11  12,62  0.03\n12  12,62  0.01\n13  12,62  0.09\n14  12,62  0.06\n15  12,62  0.07\n16  12,62  0.01\n17  12,62  0.01\n18  12,62  0.01\n19  12,62  0.07\n</code></pre>\n\n<p>Thank you!</p>\n",
        "answer_body": "<p>To get the recursive product you can do the following:</p>\n\n<pre><code>start = 5\nend = 10\n\ndf['C'] = ((1+df.B)[start:end+1].cumprod().reindex(df.index[:end+1]).fillna(1)*df.A).ffill()\n</code></pre>\n\n<h3>Output:</h3>\n\n<pre><code>     A     B          C\n0   10  0.07  10.000000\n1   10  0.02  10.000000\n2   10  0.05  10.000000\n3   10  0.00  10.000000\n4   10  0.01  10.000000\n5   10  0.09  10.900000\n6   10  0.00  10.900000\n7   10  0.02  11.118000\n8   10  0.03  11.451540\n9   10  0.05  12.024117\n10  10  0.05  12.625323\n11  10  0.03  12.625323\n12  10  0.01  12.625323\n13  10  0.09  12.625323\n14  10  0.06  12.625323\n15  10  0.07  12.625323\n16  10  0.01  12.625323\n17  10  0.01  12.625323\n18  10  0.01  12.625323\n19  10  0.07  12.625323\n</code></pre>\n\n<hr>\n\n<h3>Explanation:</h3>\n\n<p>Calculate the cumulative product of <code>(1 + df.B)</code>, which is the factor to mulitply by <code>df.A</code> to obtain the recursive product. Do this only over the range specified. <code>reindex</code> and fill the the rows before <code>start</code> with 1, so the value remains constant before this range.</p>\n\n<p>Multiply by <code>df.A</code> to get the actual value, forward filling values after the range you specify. </p>\n",
        "question_body": "<p>Let's say I have a dataframe with only two columns and 20 rows, where all values from the first column are equal to 10, and all values from the second row are random percentage numbers.</p>\n\n<p>Now, I want to multiply the first column with the percentage values of the second column +1, but only at some intervals, and copy the last value to the next row.</p>\n\n<p>E.g. I want to do this multiplication operation from row 5 to 10.</p>\n\n<p>The problem Is that I don't know to start and end the calculation in arbitrary spots based on the df's index.</p>\n\n<p>Example input data:</p>\n\n<pre><code>df = pd.DataFrame(np.random.randint(0,10,size=(20, 2)), columns=list('AB'))\ndf['A'] = 10\ndf['B'] = df['B'] /100\n</code></pre>\n\n<p>Which produces:</p>\n\n<pre><code>      A     B\n0   10  0.07\n1   10  0.02\n2   10  0.05\n3   10  0.00\n4   10  0.01\n5   10  0.09\n6   10  0.00\n7   10  0.02\n8   10  0.03\n9   10  0.05\n10  10  0.05\n11  10  0.03\n12  10  0.01\n13  10  0.09\n14  10  0.06\n15  10  0.07\n16  10  0.01\n17  10  0.01\n18  10  0.01\n19  10  0.07\n</code></pre>\n\n<p>An output I would like to get, is where the first row go thorugh a comulative multiplication only at sow rows, like this:</p>\n\n<pre><code>      C       B\n0   10     0.07\n1   10     0.02\n2   10     0.05\n3   10     0.00\n4   10     0.01\n5   10,9   0.09\n6   10,9   0.00\n7   11,11  0.02\n8   11,45  0.03\n9   12,02  0.05\n10  12,62  0.05\n11  12,62  0.03\n12  12,62  0.01\n13  12,62  0.09\n14  12,62  0.06\n15  12,62  0.07\n16  12,62  0.01\n17  12,62  0.01\n18  12,62  0.01\n19  12,62  0.07\n</code></pre>\n\n<p>Thank you!</p>\n",
        "formatted_input": {
            "qid": 53357204,
            "link": "https://stackoverflow.com/questions/53357204/how-to-do-a-calculation-only-at-some-rows-of-my-dataframe",
            "question": {
                "title": "How to do a calculation only at some rows of my dataframe?",
                "ques_desc": "Let's say I have a dataframe with only two columns and 20 rows, where all values from the first column are equal to 10, and all values from the second row are random percentage numbers. Now, I want to multiply the first column with the percentage values of the second column +1, but only at some intervals, and copy the last value to the next row. E.g. I want to do this multiplication operation from row 5 to 10. The problem Is that I don't know to start and end the calculation in arbitrary spots based on the df's index. Example input data: Which produces: An output I would like to get, is where the first row go thorugh a comulative multiplication only at sow rows, like this: Thank you! "
            },
            "io": [
                "      A     B\n0   10  0.07\n1   10  0.02\n2   10  0.05\n3   10  0.00\n4   10  0.01\n5   10  0.09\n6   10  0.00\n7   10  0.02\n8   10  0.03\n9   10  0.05\n10  10  0.05\n11  10  0.03\n12  10  0.01\n13  10  0.09\n14  10  0.06\n15  10  0.07\n16  10  0.01\n17  10  0.01\n18  10  0.01\n19  10  0.07\n",
                "      C       B\n0   10     0.07\n1   10     0.02\n2   10     0.05\n3   10     0.00\n4   10     0.01\n5   10,9   0.09\n6   10,9   0.00\n7   11,11  0.02\n8   11,45  0.03\n9   12,02  0.05\n10  12,62  0.05\n11  12,62  0.03\n12  12,62  0.01\n13  12,62  0.09\n14  12,62  0.06\n15  12,62  0.07\n16  12,62  0.01\n17  12,62  0.01\n18  12,62  0.01\n19  12,62  0.07\n"
            ],
            "answer": {
                "ans_desc": "To get the recursive product you can do the following: Output: Explanation: Calculate the cumulative product of , which is the factor to mulitply by to obtain the recursive product. Do this only over the range specified. and fill the the rows before with 1, so the value remains constant before this range. Multiply by to get the actual value, forward filling values after the range you specify. ",
                "code": [
                    "start = 5\nend = 10\n\ndf['C'] = ((1+df.B)[start:end+1].cumprod().reindex(df.index[:end+1]).fillna(1)*df.A).ffill()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 962,
            "user_id": 604365,
            "user_type": "registered",
            "accept_rate": 71,
            "profile_image": "https://www.gravatar.com/avatar/03c3a4e855b186ab51c7c996c3087d0a?s=128&d=identicon&r=PG",
            "display_name": "Aesir",
            "link": "https://stackoverflow.com/users/604365/aesir"
        },
        "is_answered": true,
        "view_count": 215,
        "accepted_answer_id": 53328177,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1542317639,
        "creation_date": 1542316157,
        "last_edit_date": 1542316978,
        "question_id": 53327932,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53327932/creating-a-pandas-dataframe-based-on-cell-content-of-two-other-dataframes",
        "title": "creating a pandas dataframe based on cell content of two other dataframes",
        "body": "<p>I have wo dataframes with the same number of rows and columns. I would like to create a third dataframe based on these two dataframes that has the same dimensions as the other two dataframes.  Each cell in the third dataframe should be the result by a function applied to the corresponding cell values in df1 and df2 respectively.</p>\n\n<p>i.e. if I have </p>\n\n<pre><code>df1 = | 1 | 2 |\n      | 3 | 4 |\n\ndf2 = | 5 | 6 |\n      | 7 | 8 |\n</code></pre>\n\n<p>then df3 should be like this</p>\n\n<pre><code>df3 = | func(1, 5) | func(2, 6) |\n      | func(3, 7) | func(4, 8) |\n</code></pre>\n\n<p>I have a way to do this that I do not think is very pythonic nor appropriate for large dataframes and would like to know if there is an efficient way to do such a thing?</p>\n\n<p>The function I wish to apply is:</p>\n\n<pre><code>def smape3(y, yhat, axis=0):\n    all_zeros = not (np.any(y) and np.any(yhat))\n    if all_zeros:\n        return 0.0\n    return np.sum(np.abs(yhat - y), axis) / np.sum(np.abs(yhat + y), axis)\n</code></pre>\n\n<p>It can be used to produce a single scalar value OR an array of values.  In my use case above the input to the function would be two scalar values.  So smape(1, 5) = 0.66.</p>\n",
        "answer_body": "<p>You can use a vectorised approach:</p>\n\n<pre><code>df1 = pd.DataFrame([[1, 2], [3, 4]])\ndf2 = pd.DataFrame([[5, 6], [7, 8]])\n\narr = np.where(df1.eq(0) &amp; df2.eq(0), 0, (df2 - df1).abs() / (df2 + df1).abs())\n\ndf = pd.DataFrame(arr)\n\nprint(df)\n\n          0         1\n0  0.666667  0.500000\n1  0.400000  0.333333\n</code></pre>\n\n<p>Or if you want to separate some of the logic in a function:</p>\n\n<pre><code>def smape3(df1, df2):\n    return (df2 - df1).abs() / (df2 + df1).abs()\n\ndf = pd.DataFrame(np.where(df1.eq(0) &amp; df2.eq(0), 0, smape3(df1, df2)))\n</code></pre>\n",
        "question_body": "<p>I have wo dataframes with the same number of rows and columns. I would like to create a third dataframe based on these two dataframes that has the same dimensions as the other two dataframes.  Each cell in the third dataframe should be the result by a function applied to the corresponding cell values in df1 and df2 respectively.</p>\n\n<p>i.e. if I have </p>\n\n<pre><code>df1 = | 1 | 2 |\n      | 3 | 4 |\n\ndf2 = | 5 | 6 |\n      | 7 | 8 |\n</code></pre>\n\n<p>then df3 should be like this</p>\n\n<pre><code>df3 = | func(1, 5) | func(2, 6) |\n      | func(3, 7) | func(4, 8) |\n</code></pre>\n\n<p>I have a way to do this that I do not think is very pythonic nor appropriate for large dataframes and would like to know if there is an efficient way to do such a thing?</p>\n\n<p>The function I wish to apply is:</p>\n\n<pre><code>def smape3(y, yhat, axis=0):\n    all_zeros = not (np.any(y) and np.any(yhat))\n    if all_zeros:\n        return 0.0\n    return np.sum(np.abs(yhat - y), axis) / np.sum(np.abs(yhat + y), axis)\n</code></pre>\n\n<p>It can be used to produce a single scalar value OR an array of values.  In my use case above the input to the function would be two scalar values.  So smape(1, 5) = 0.66.</p>\n",
        "formatted_input": {
            "qid": 53327932,
            "link": "https://stackoverflow.com/questions/53327932/creating-a-pandas-dataframe-based-on-cell-content-of-two-other-dataframes",
            "question": {
                "title": "creating a pandas dataframe based on cell content of two other dataframes",
                "ques_desc": "I have wo dataframes with the same number of rows and columns. I would like to create a third dataframe based on these two dataframes that has the same dimensions as the other two dataframes. Each cell in the third dataframe should be the result by a function applied to the corresponding cell values in df1 and df2 respectively. i.e. if I have then df3 should be like this I have a way to do this that I do not think is very pythonic nor appropriate for large dataframes and would like to know if there is an efficient way to do such a thing? The function I wish to apply is: It can be used to produce a single scalar value OR an array of values. In my use case above the input to the function would be two scalar values. So smape(1, 5) = 0.66. "
            },
            "io": [
                "df1 = | 1 | 2 |\n      | 3 | 4 |\n\ndf2 = | 5 | 6 |\n      | 7 | 8 |\n",
                "df3 = | func(1, 5) | func(2, 6) |\n      | func(3, 7) | func(4, 8) |\n"
            ],
            "answer": {
                "ans_desc": "You can use a vectorised approach: Or if you want to separate some of the logic in a function: ",
                "code": [
                    "def smape3(df1, df2):\n    return (df2 - df1).abs() / (df2 + df1).abs()\n\ndf = pd.DataFrame(np.where(df1.eq(0) & df2.eq(0), 0, smape3(df1, df2)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 175,
            "user_id": 7672132,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1425284914159457/picture?type=large",
            "display_name": "xiumpt",
            "link": "https://stackoverflow.com/users/7672132/xiumpt"
        },
        "is_answered": true,
        "view_count": 92,
        "accepted_answer_id": 53315305,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1542276783,
        "creation_date": 1542267266,
        "last_edit_date": 1542274785,
        "question_id": 53314441,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53314441/python-pandas-explode-rows-by-turns",
        "title": "Python - pandas explode rows by turns",
        "body": "<p>I have a dataframe as below. </p>\n\n<pre><code>df = DataFrame([{'B1': '1C', 'B2': '', 'B3': '', },\n                {'B1': '3A', 'B2': '1A', 'B3': ''},\n                {'B1': '41A', 'B2': '28A', 'B3': '3A'}])\n\n    B1   B2  B3    \n0   1C             \n1   3A   1A      \n2  41A  28A  3A  \n</code></pre>\n\n<p>Now I extracted letters from <strong>B1-B3</strong> and add to new columns <strong>U1-U3</strong> get:</p>\n\n<pre><code>    B1   B2  B3  U1 U2 U3 \n0   1C            C         \n1   3A   1A       A  A      \n2  41A  28A  3A   A  A  A   \n</code></pre>\n\n<p>and I want to let the row to explode like this:</p>\n\n<pre><code>    B1   B2  B3   U1  U2  U3 \n0   1C            C         \n1   3A   1A       A         \n2   3A   1A            A      \n3  41A  28A  3A   A         \n4  41A  28A  3A        A      \n5  41A  28A  3A            A    \n</code></pre>\n\n<p>Thanks in advance</p>\n",
        "answer_body": "<p>I think, it needs 3 step solution of </p>\n\n<p>1) extracting the Alphabates from data and creating new columns,</p>\n\n<p>2) duplicating the rows w.r.t values and </p>\n\n<p>3) masking with identity matrix.</p>\n\n<pre><code>df = pd.DataFrame([{'B1': '1C', 'B2': '', 'B3': '', },\n            {'B1': '3A', 'B2': '1A', 'B3': ''},\n            {'B1': '41A', 'B2': '28A', 'B3': '3A'}])\n\n    B1  B2  B3\n0   1C      \n1   3A  1A  \n2   41A 28A 3A\n</code></pre>\n\n<p>1) Extracting the Alphabates from the rows and assigning as columns</p>\n\n<pre><code>df = df.merge(df.apply(lambda x: x.str.extract('([A-Za-z])')).add_prefix('U_'), left_index=True,right_index=True,how='outer')\n</code></pre>\n\n<p>Out:</p>\n\n<pre><code>  B1    B2  B3  U_B1    U_B2    U_B3\n0   1C          C   NaN NaN\n1   3A  1A      A   A   NaN\n2   41A 28A 3A  A   A   A\n</code></pre>\n\n<p>2) You can try of <code>duplicating the rows</code> of dataframe wherever it has more than 1 value </p>\n\n<pre><code># Duplicating the rows of dataframe\nval = df[['U_B1','U_B2','U_B3']].notnull().sum(axis=1)\ndf1 = df.loc[np.repeat(val.index,val)]\n</code></pre>\n\n<p>-> 3) then by grouping with index, pick only <code>masked values of identity matrix</code>(<strong>np.identity</strong>) w.r.t length of each group.</p>\n\n<pre><code>df1[['U_B1','U_B2','U_B3']] = df1.groupby(df1.index)['U_B1','U_B2','U_B3'].apply(lambda x: x.dropna(axis=1).mask(np.identity(len(x))==0))\n</code></pre>\n\n<p>Out:</p>\n\n<pre><code>   B1   B2  B3  U_B1 U_B2 U_B3\n0   1C          C       \n1   3A  1A      A       \n1   3A  1A          A   \n2   41A 28A 3A  A       \n2   41A 28A 3A      A   \n2   41A 28A 3A          A\n</code></pre>\n",
        "question_body": "<p>I have a dataframe as below. </p>\n\n<pre><code>df = DataFrame([{'B1': '1C', 'B2': '', 'B3': '', },\n                {'B1': '3A', 'B2': '1A', 'B3': ''},\n                {'B1': '41A', 'B2': '28A', 'B3': '3A'}])\n\n    B1   B2  B3    \n0   1C             \n1   3A   1A      \n2  41A  28A  3A  \n</code></pre>\n\n<p>Now I extracted letters from <strong>B1-B3</strong> and add to new columns <strong>U1-U3</strong> get:</p>\n\n<pre><code>    B1   B2  B3  U1 U2 U3 \n0   1C            C         \n1   3A   1A       A  A      \n2  41A  28A  3A   A  A  A   \n</code></pre>\n\n<p>and I want to let the row to explode like this:</p>\n\n<pre><code>    B1   B2  B3   U1  U2  U3 \n0   1C            C         \n1   3A   1A       A         \n2   3A   1A            A      \n3  41A  28A  3A   A         \n4  41A  28A  3A        A      \n5  41A  28A  3A            A    \n</code></pre>\n\n<p>Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 53314441,
            "link": "https://stackoverflow.com/questions/53314441/python-pandas-explode-rows-by-turns",
            "question": {
                "title": "Python - pandas explode rows by turns",
                "ques_desc": "I have a dataframe as below. Now I extracted letters from B1-B3 and add to new columns U1-U3 get: and I want to let the row to explode like this: Thanks in advance "
            },
            "io": [
                "    B1   B2  B3  U1 U2 U3 \n0   1C            C         \n1   3A   1A       A  A      \n2  41A  28A  3A   A  A  A   \n",
                "    B1   B2  B3   U1  U2  U3 \n0   1C            C         \n1   3A   1A       A         \n2   3A   1A            A      \n3  41A  28A  3A   A         \n4  41A  28A  3A        A      \n5  41A  28A  3A            A    \n"
            ],
            "answer": {
                "ans_desc": "I think, it needs 3 step solution of 1) extracting the Alphabates from data and creating new columns, 2) duplicating the rows w.r.t values and 3) masking with identity matrix. 1) Extracting the Alphabates from the rows and assigning as columns Out: 2) You can try of of dataframe wherever it has more than 1 value -> 3) then by grouping with index, pick only (np.identity) w.r.t length of each group. Out: ",
                "code": [
                    "df = df.merge(df.apply(lambda x: x.str.extract('([A-Za-z])')).add_prefix('U_'), left_index=True,right_index=True,how='outer')\n",
                    "# Duplicating the rows of dataframe\nval = df[['U_B1','U_B2','U_B3']].notnull().sum(axis=1)\ndf1 = df.loc[np.repeat(val.index,val)]\n",
                    "masked values of identity matrix",
                    "df1[['U_B1','U_B2','U_B3']] = df1.groupby(df1.index)['U_B1','U_B2','U_B3'].apply(lambda x: x.dropna(axis=1).mask(np.identity(len(x))==0))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "types"
        ],
        "owner": {
            "reputation": 93,
            "user_id": 10645913,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "Georg B",
            "link": "https://stackoverflow.com/users/10645913/georg-b"
        },
        "is_answered": true,
        "view_count": 9618,
        "accepted_answer_id": 53280930,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1542114973,
        "creation_date": 1542110609,
        "last_edit_date": 1542114973,
        "question_id": 53280650,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53280650/pandas-dataframe-interpreting-columns-as-float-instead-of-string",
        "title": "Pandas Dataframe interpreting columns as float instead of String",
        "body": "<p>I want to import a csv file into a pandas dataframe. There is a column with IDs, which consist of only numbers, but not every row has an ID.</p>\n\n<pre><code>   ID      xyz\n0  12345     4.56\n1           45.60\n2  54231   987.00\n</code></pre>\n\n<p>I want to read this column as String, but even if I specifiy it with </p>\n\n<p><code>df=pd.read_csv(filename,dtype={'ID': str})</code></p>\n\n<p>I get </p>\n\n<pre><code>   ID         xyz\n0  '12345.0'    4.56\n1   NaN        45.60\n2  '54231.0'  987.00\n</code></pre>\n\n<p>Is there an easy way get the ID as a string without decimal like <code>'12345'</code>without having to edit the Strings after importing the table?</p>\n",
        "answer_body": "<p>A solution could be this, but after you have imported the df:</p>\n\n<pre><code>df = pd.read_csv(filename)\ndf['ID'] = df['ID'].astype(int).astype(str)\n</code></pre>\n\n<p>Or since there are <code>NaN</code> with:</p>\n\n<pre><code>df['ID'] = df['ID'].apply(lambda x: x if pd.isnull(x) else str(int(x)))\n</code></pre>\n",
        "question_body": "<p>I want to import a csv file into a pandas dataframe. There is a column with IDs, which consist of only numbers, but not every row has an ID.</p>\n\n<pre><code>   ID      xyz\n0  12345     4.56\n1           45.60\n2  54231   987.00\n</code></pre>\n\n<p>I want to read this column as String, but even if I specifiy it with </p>\n\n<p><code>df=pd.read_csv(filename,dtype={'ID': str})</code></p>\n\n<p>I get </p>\n\n<pre><code>   ID         xyz\n0  '12345.0'    4.56\n1   NaN        45.60\n2  '54231.0'  987.00\n</code></pre>\n\n<p>Is there an easy way get the ID as a string without decimal like <code>'12345'</code>without having to edit the Strings after importing the table?</p>\n",
        "formatted_input": {
            "qid": 53280650,
            "link": "https://stackoverflow.com/questions/53280650/pandas-dataframe-interpreting-columns-as-float-instead-of-string",
            "question": {
                "title": "Pandas Dataframe interpreting columns as float instead of String",
                "ques_desc": "I want to import a csv file into a pandas dataframe. There is a column with IDs, which consist of only numbers, but not every row has an ID. I want to read this column as String, but even if I specifiy it with I get Is there an easy way get the ID as a string without decimal like without having to edit the Strings after importing the table? "
            },
            "io": [
                "   ID      xyz\n0  12345     4.56\n1           45.60\n2  54231   987.00\n",
                "   ID         xyz\n0  '12345.0'    4.56\n1   NaN        45.60\n2  '54231.0'  987.00\n"
            ],
            "answer": {
                "ans_desc": "A solution could be this, but after you have imported the df: Or since there are with: ",
                "code": [
                    "df = pd.read_csv(filename)\ndf['ID'] = df['ID'].astype(int).astype(str)\n",
                    "df['ID'] = df['ID'].apply(lambda x: x if pd.isnull(x) else str(int(x)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 744,
            "user_id": 7936266,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/15f5348176b560d232a662a4777c7284?s=128&d=identicon&r=PG&f=1",
            "display_name": "w4bo",
            "link": "https://stackoverflow.com/users/7936266/w4bo"
        },
        "is_answered": true,
        "view_count": 151,
        "accepted_answer_id": 53278504,
        "answer_count": 5,
        "score": 3,
        "last_activity_date": 1542104849,
        "creation_date": 1542100072,
        "last_edit_date": 1542104173,
        "question_id": 53277378,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53277378/estimate-the-mean-of-a-dataframegroupby-by-only-considering-values-in-a-percenti",
        "title": "Estimate the mean of a DataFrameGroupBy by only considering values in a percentile range",
        "body": "<p>I need to estimate the mean of a pandas DataFrameGroupBy by only considering the values between a given percentile range.</p>\n\n<p>For instance, given the snippet</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\na = np.matrix('1 1; 1 2; 1 4; 2 1; 2 2; 2 4')\ndata = pd.DataFrame(a)\ngroupby = data.groupby(0)\nm1 = groupby.mean()\n</code></pre>\n\n<p>the result is</p>\n\n<pre><code>m1 =            1\n      0          \n      1  2.333333\n      2  2.333333\n</code></pre>\n\n<p>However, if a percentile range is picked to exclude the maximum and minimum values the result should be</p>\n\n<pre><code>m1 =     1\n      0          \n      1  2\n      2  2\n</code></pre>\n\n<p>How can I filter, for each group, the values between an <strong>arbitrary</strong> percentile range before estimating the mean? For instance, only considering the values between the 20th and 80th percentiles.</p>\n",
        "answer_body": "<p>You can use a custom function with either <a href=\"https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.percentile.html\" rel=\"nofollow noreferrer\"><code>np.percentile</code></a> or <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.quantile.html\" rel=\"nofollow noreferrer\"><code>pd.Series.quantile</code></a>. The performance difference is marginal. The below example includes values only above the 20th and below the 80th percentile in calculating groupwise mean.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\na = np.matrix('1 1; 1 2; 1 4; 2 1; 2 2; 2 4')\ndata = pd.DataFrame(a)\n\ndef jpp_np(df):\n    def meaner(x, lowperc, highperc):\n        low, high = np.percentile(x, [lowperc, highperc])\n        return x[(x &gt; low) &amp; (x &lt; high)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndef jpp_pd(df):\n    def meaner(x, lowperc, highperc):\n        low, high = x.quantile([lowperc/100, highperc/100]).values\n        return x[x.between(low, high, inclusive=False)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndata = pd.concat([data]*10000)\n\nassert np.array_equal(jpp_np(data), jpp_pd(data))\n\n%timeit jpp_np(data)  # 11.2 ms per loop\n%timeit jpp_pd(data)  # 12.5 ms per loop\n</code></pre>\n",
        "question_body": "<p>I need to estimate the mean of a pandas DataFrameGroupBy by only considering the values between a given percentile range.</p>\n\n<p>For instance, given the snippet</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\na = np.matrix('1 1; 1 2; 1 4; 2 1; 2 2; 2 4')\ndata = pd.DataFrame(a)\ngroupby = data.groupby(0)\nm1 = groupby.mean()\n</code></pre>\n\n<p>the result is</p>\n\n<pre><code>m1 =            1\n      0          \n      1  2.333333\n      2  2.333333\n</code></pre>\n\n<p>However, if a percentile range is picked to exclude the maximum and minimum values the result should be</p>\n\n<pre><code>m1 =     1\n      0          \n      1  2\n      2  2\n</code></pre>\n\n<p>How can I filter, for each group, the values between an <strong>arbitrary</strong> percentile range before estimating the mean? For instance, only considering the values between the 20th and 80th percentiles.</p>\n",
        "formatted_input": {
            "qid": 53277378,
            "link": "https://stackoverflow.com/questions/53277378/estimate-the-mean-of-a-dataframegroupby-by-only-considering-values-in-a-percenti",
            "question": {
                "title": "Estimate the mean of a DataFrameGroupBy by only considering values in a percentile range",
                "ques_desc": "I need to estimate the mean of a pandas DataFrameGroupBy by only considering the values between a given percentile range. For instance, given the snippet the result is However, if a percentile range is picked to exclude the maximum and minimum values the result should be How can I filter, for each group, the values between an arbitrary percentile range before estimating the mean? For instance, only considering the values between the 20th and 80th percentiles. "
            },
            "io": [
                "m1 =            1\n      0          \n      1  2.333333\n      2  2.333333\n",
                "m1 =     1\n      0          \n      1  2\n      2  2\n"
            ],
            "answer": {
                "ans_desc": "You can use a custom function with either or . The performance difference is marginal. The below example includes values only above the 20th and below the 80th percentile in calculating groupwise mean. ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\na = np.matrix('1 1; 1 2; 1 4; 2 1; 2 2; 2 4')\ndata = pd.DataFrame(a)\n\ndef jpp_np(df):\n    def meaner(x, lowperc, highperc):\n        low, high = np.percentile(x, [lowperc, highperc])\n        return x[(x > low) & (x < high)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndef jpp_pd(df):\n    def meaner(x, lowperc, highperc):\n        low, high = x.quantile([lowperc/100, highperc/100]).values\n        return x[x.between(low, high, inclusive=False)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndata = pd.concat([data]*10000)\n\nassert np.array_equal(jpp_np(data), jpp_pd(data))\n\n%timeit jpp_np(data)  # 11.2 ms per loop\n%timeit jpp_pd(data)  # 12.5 ms per loop\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2720,
            "user_id": 5424617,
            "user_type": "registered",
            "accept_rate": 83,
            "profile_image": "https://www.gravatar.com/avatar/f86a85c89417fa560070c4aeb5aecc56?s=128&d=identicon&r=PG",
            "display_name": "Md. Rezwanul Haque",
            "link": "https://stackoverflow.com/users/5424617/md-rezwanul-haque"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 53252091,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1541962595,
        "creation_date": 1541962403,
        "question_id": 53252056,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53252056/how-to-drop-rows-with-respect-to-a-column-values-in-python",
        "title": "How to drop rows with respect to a column values in Python?",
        "body": "<p>I wantto remove rows with respect <code>ID</code> column values.</p>\n\n<blockquote>\n  <p>df</p>\n</blockquote>\n\n<pre><code>   ID    B   C   D\n0  101   1   2   3\n1  103   5   6   7\n2  108   9  10  11\n3  109   5   3  12\n4  118  11  15   2\n5  121   2   5   6\n</code></pre>\n\n<p>Here the <code>remove_id</code> list of <code>ID</code> value those I want to remove.</p>\n\n<pre><code>remove_id = [103,108, 121]\n</code></pre>\n\n<p>I want to output like following:</p>\n\n<blockquote>\n  <p>df</p>\n</blockquote>\n\n<pre><code>   ID    B   C   D\n0  101   1   2   3\n3  109   5   3  12\n4  118  11  15   2\n</code></pre>\n\n<p>How can I do this?</p>\n",
        "answer_body": "<p>You can check which IDs are in <code>remove_id</code> with the <code>isin</code> method, negate the result with <code>~</code> and use the resulting <code>Series</code> for boolean indexing.</p>\n\n<pre><code>&gt;&gt;&gt; df[~df['ID'].isin(remove_id)]\n&gt;&gt;&gt; \n    ID   B   C   D\n0  101   1   2   3\n3  109   5   3  12\n4  118  11  15   2\n</code></pre>\n\n<p>Details:</p>\n\n<pre><code>&gt;&gt;&gt; df['ID'].isin(remove_id)\n&gt;&gt;&gt; \n0    False\n1     True\n2     True\n3    False\n4    False\n5     True\nName: ID, dtype: bool\n&gt;&gt;&gt; ~df['ID'].isin(remove_id)\n&gt;&gt;&gt; \n0     True\n1    False\n2    False\n3     True\n4     True\n5    False\nName: ID, dtype: bool\n</code></pre>\n",
        "question_body": "<p>I wantto remove rows with respect <code>ID</code> column values.</p>\n\n<blockquote>\n  <p>df</p>\n</blockquote>\n\n<pre><code>   ID    B   C   D\n0  101   1   2   3\n1  103   5   6   7\n2  108   9  10  11\n3  109   5   3  12\n4  118  11  15   2\n5  121   2   5   6\n</code></pre>\n\n<p>Here the <code>remove_id</code> list of <code>ID</code> value those I want to remove.</p>\n\n<pre><code>remove_id = [103,108, 121]\n</code></pre>\n\n<p>I want to output like following:</p>\n\n<blockquote>\n  <p>df</p>\n</blockquote>\n\n<pre><code>   ID    B   C   D\n0  101   1   2   3\n3  109   5   3  12\n4  118  11  15   2\n</code></pre>\n\n<p>How can I do this?</p>\n",
        "formatted_input": {
            "qid": 53252056,
            "link": "https://stackoverflow.com/questions/53252056/how-to-drop-rows-with-respect-to-a-column-values-in-python",
            "question": {
                "title": "How to drop rows with respect to a column values in Python?",
                "ques_desc": "I wantto remove rows with respect column values. df Here the list of value those I want to remove. I want to output like following: df How can I do this? "
            },
            "io": [
                "   ID    B   C   D\n0  101   1   2   3\n1  103   5   6   7\n2  108   9  10  11\n3  109   5   3  12\n4  118  11  15   2\n5  121   2   5   6\n",
                "   ID    B   C   D\n0  101   1   2   3\n3  109   5   3  12\n4  118  11  15   2\n"
            ],
            "answer": {
                "ans_desc": "You can check which IDs are in with the method, negate the result with and use the resulting for boolean indexing. Details: ",
                "code": [
                    ">>> df['ID'].isin(remove_id)\n>>> \n0    False\n1     True\n2     True\n3    False\n4    False\n5     True\nName: ID, dtype: bool\n>>> ~df['ID'].isin(remove_id)\n>>> \n0     True\n1    False\n2    False\n3     True\n4     True\n5    False\nName: ID, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-analysis"
        ],
        "owner": {
            "reputation": 35,
            "user_id": 10591246,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e3aadd5a6ceb5cab28801980d67030f7?s=128&d=identicon&r=PG&f=1",
            "display_name": "Rithvik K",
            "link": "https://stackoverflow.com/users/10591246/rithvik-k"
        },
        "is_answered": true,
        "view_count": 64,
        "accepted_answer_id": 53185390,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1541594064,
        "creation_date": 1541574626,
        "last_edit_date": 1541594064,
        "question_id": 53184917,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53184917/interpolation-of-a-dataframe-with-immediate-data-appearing-before-and-after-it",
        "title": "Interpolation of a dataframe with immediate data appearing before and after it - Pandas",
        "body": "<p>Let's say I've a dataframe like this - </p>\n\n<pre><code>ID  Weight Height\n1   80.0   180.0\n2   60.0   170.0\n3   NaN    NaN\n4   NaN    NaN\n5   82.0   185.0\n</code></pre>\n\n<p>I want the dataframe to be transormed to -</p>\n\n<pre><code>ID  Weight  Height\n1   80.0    180.0\n2   60.0    170.0\n3   71.0    177.5\n4   76.5    181.25\n5   82.0    185.0\n</code></pre>\n\n<p>It takes the average of the immediate data available before and after a NaN and updates the missing/NaN value accordingly.</p>\n",
        "answer_body": "<p>You can use interpolation from the <code>pandas</code> library by using the following:</p>\n\n<pre><code>df['Weight'], df['Height'] = df.Weight.interpolate(), df.Height.interpolate()\n</code></pre>\n\n<p>Check the arguments on the documentation for the method of interpolation to tune this to your problem case: <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html</a></p>\n",
        "question_body": "<p>Let's say I've a dataframe like this - </p>\n\n<pre><code>ID  Weight Height\n1   80.0   180.0\n2   60.0   170.0\n3   NaN    NaN\n4   NaN    NaN\n5   82.0   185.0\n</code></pre>\n\n<p>I want the dataframe to be transormed to -</p>\n\n<pre><code>ID  Weight  Height\n1   80.0    180.0\n2   60.0    170.0\n3   71.0    177.5\n4   76.5    181.25\n5   82.0    185.0\n</code></pre>\n\n<p>It takes the average of the immediate data available before and after a NaN and updates the missing/NaN value accordingly.</p>\n",
        "formatted_input": {
            "qid": 53184917,
            "link": "https://stackoverflow.com/questions/53184917/interpolation-of-a-dataframe-with-immediate-data-appearing-before-and-after-it",
            "question": {
                "title": "Interpolation of a dataframe with immediate data appearing before and after it - Pandas",
                "ques_desc": "Let's say I've a dataframe like this - I want the dataframe to be transormed to - It takes the average of the immediate data available before and after a NaN and updates the missing/NaN value accordingly. "
            },
            "io": [
                "ID  Weight Height\n1   80.0   180.0\n2   60.0   170.0\n3   NaN    NaN\n4   NaN    NaN\n5   82.0   185.0\n",
                "ID  Weight  Height\n1   80.0    180.0\n2   60.0    170.0\n3   71.0    177.5\n4   76.5    181.25\n5   82.0    185.0\n"
            ],
            "answer": {
                "ans_desc": "You can use interpolation from the library by using the following: Check the arguments on the documentation for the method of interpolation to tune this to your problem case: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html ",
                "code": [
                    "df['Weight'], df['Height'] = df.Weight.interpolate(), df.Height.interpolate()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "functools"
        ],
        "owner": {
            "reputation": 660,
            "user_id": 1833326,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://i.stack.imgur.com/4YBBM.png?s=128&g=1",
            "display_name": "Lazloo Xp",
            "link": "https://stackoverflow.com/users/1833326/lazloo-xp"
        },
        "is_answered": true,
        "view_count": 465,
        "accepted_answer_id": 53149622,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1541402808,
        "creation_date": 1541400408,
        "question_id": 53149552,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53149552/functools-reduce-in-place-modifies-original-dataframe",
        "title": "functools reduce In-Place modifies original dataframe",
        "body": "<p>I currently facing the issue that \"functools.reduce(operator.iadd,...)\" alters the original input. E.g.</p>\n\n<p>I have a simple dataframe\n    df = pd.DataFrame([[['A', 'B']], [['C', 'D']]])</p>\n\n<pre><code>        0\n0  [A, B]\n1  [C, D]\n</code></pre>\n\n<p>Applying the iadd operator leads to following result:</p>\n\n<pre><code>functools.reduce(operator.iadd, df[0])\n['A', 'B', 'C', 'D']\n</code></pre>\n\n<p>Now, the original df changed to </p>\n\n<pre><code>              0\n0  [A, B, C, D]\n1        [C, D]\n</code></pre>\n\n<p>Also copying the df using df.copy(deep=True) beforehand does not help.</p>\n\n<p>Has anyone an idea to overcome this issue? \nTHX, Lazloo</p>\n",
        "answer_body": "<p>Use <code>operator.add</code> instead of <code>operator.iadd</code>:</p>\n\n<pre><code>In [8]: functools.reduce(operator.add, df[0])\nOut[8]: ['A', 'B', 'C', 'D']\n\nIn [9]: df\nOut[9]: \n        0\n0  [A, B]\n1  [C, D]\n</code></pre>\n\n<p>After all, <code>operator.iadd(a, b)</code> is the same as <code>a += b</code>. So it modifies <code>df[0]</code>. In contrast, <code>operator.add(a, b)</code> <em>returns</em> <code>a + b</code>, so there is no modification of <code>df[0]</code>.</p>\n\n<hr>\n\n<p>Or, you could compute the same quantity using <code>df[0].sum()</code>:</p>\n\n<pre><code>In [39]: df[0].sum()\nOut[39]: ['A', 'B', 'C', 'D']\n</code></pre>\n\n<hr>\n\n<p>The <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html\" rel=\"nofollow noreferrer\">docs for <code>df.copy</code></a> warns: </p>\n\n<blockquote>\n  <p>When <code>deep=True</code>, data is copied <strong>but actual Python objects\n  will not be copied recursively</strong>, only the reference to the object.</p>\n</blockquote>\n\n<p>Since <code>df[0]</code> contains Python lists, the lists are not copied even with <code>df.copy(deep=True)</code>. This is why modifying the copy still affects <code>df</code>.</p>\n",
        "question_body": "<p>I currently facing the issue that \"functools.reduce(operator.iadd,...)\" alters the original input. E.g.</p>\n\n<p>I have a simple dataframe\n    df = pd.DataFrame([[['A', 'B']], [['C', 'D']]])</p>\n\n<pre><code>        0\n0  [A, B]\n1  [C, D]\n</code></pre>\n\n<p>Applying the iadd operator leads to following result:</p>\n\n<pre><code>functools.reduce(operator.iadd, df[0])\n['A', 'B', 'C', 'D']\n</code></pre>\n\n<p>Now, the original df changed to </p>\n\n<pre><code>              0\n0  [A, B, C, D]\n1        [C, D]\n</code></pre>\n\n<p>Also copying the df using df.copy(deep=True) beforehand does not help.</p>\n\n<p>Has anyone an idea to overcome this issue? \nTHX, Lazloo</p>\n",
        "formatted_input": {
            "qid": 53149552,
            "link": "https://stackoverflow.com/questions/53149552/functools-reduce-in-place-modifies-original-dataframe",
            "question": {
                "title": "functools reduce In-Place modifies original dataframe",
                "ques_desc": "I currently facing the issue that \"functools.reduce(operator.iadd,...)\" alters the original input. E.g. I have a simple dataframe df = pd.DataFrame([[['A', 'B']], [['C', 'D']]]) Applying the iadd operator leads to following result: Now, the original df changed to Also copying the df using df.copy(deep=True) beforehand does not help. Has anyone an idea to overcome this issue? THX, Lazloo "
            },
            "io": [
                "        0\n0  [A, B]\n1  [C, D]\n",
                "              0\n0  [A, B, C, D]\n1        [C, D]\n"
            ],
            "answer": {
                "ans_desc": "Use instead of : After all, is the same as . So it modifies . In contrast, returns , so there is no modification of . Or, you could compute the same quantity using : The docs for warns: When , data is copied but actual Python objects will not be copied recursively, only the reference to the object. Since contains Python lists, the lists are not copied even with . This is why modifying the copy still affects . ",
                "code": [
                    "In [39]: df[0].sum()\nOut[39]: ['A', 'B', 'C', 'D']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2500,
            "user_id": 3848207,
            "user_type": "registered",
            "accept_rate": 97,
            "profile_image": "https://www.gravatar.com/avatar/dd7ca7300cf6cc34291660db34152ff4?s=128&d=identicon&r=PG&f=1",
            "display_name": "user3848207",
            "link": "https://stackoverflow.com/users/3848207/user3848207"
        },
        "is_answered": true,
        "view_count": 83,
        "accepted_answer_id": 52107696,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1541261252,
        "creation_date": 1535681430,
        "last_edit_date": 1535682292,
        "question_id": 52107572,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52107572/swap-contents-of-columns-inside-dataframe",
        "title": "Swap contents of columns inside dataframe",
        "body": "<p>I have a pandas dataframe <code>df</code> with this contents;</p>\n\n<pre><code>Column1   Column2    Column3  \nC11         C21        C31\nC12         C22        C32\nC13         C23        C33\n</code></pre>\n\n<p>I would like to swap the contents between Column 1 and Column 2.</p>\n\n<p>The output dataframe should look like this;</p>\n\n<pre><code>Column1   Column2    Column3  \nC21         C11        C31\nC22         C12        C32\nC23         C13        C33\n</code></pre>\n\n<p>I am using python v3.6</p>\n",
        "answer_body": "<p>I'm sure there's a better answer, but you can swap the column names and then reorder:</p>\n\n<pre><code>df = pd.DataFrame({\"Column1\": [\"C11\", \"C12\", \"C13\"],\n                   \"Column2\": [\"C21\", \"C22\", \"C23\"],\n                   \"Column3\": [\"C31\", \"C32\", \"C33\"]})\ndf.columns = [\"Column2\", \"Column1\", \"Column3\"]\ndf[[\"Column1\", \"Column2\", \"Column3\"]]\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe <code>df</code> with this contents;</p>\n\n<pre><code>Column1   Column2    Column3  \nC11         C21        C31\nC12         C22        C32\nC13         C23        C33\n</code></pre>\n\n<p>I would like to swap the contents between Column 1 and Column 2.</p>\n\n<p>The output dataframe should look like this;</p>\n\n<pre><code>Column1   Column2    Column3  \nC21         C11        C31\nC22         C12        C32\nC23         C13        C33\n</code></pre>\n\n<p>I am using python v3.6</p>\n",
        "formatted_input": {
            "qid": 52107572,
            "link": "https://stackoverflow.com/questions/52107572/swap-contents-of-columns-inside-dataframe",
            "question": {
                "title": "Swap contents of columns inside dataframe",
                "ques_desc": "I have a pandas dataframe with this contents; I would like to swap the contents between Column 1 and Column 2. The output dataframe should look like this; I am using python v3.6 "
            },
            "io": [
                "Column1   Column2    Column3  \nC11         C21        C31\nC12         C22        C32\nC13         C23        C33\n",
                "Column1   Column2    Column3  \nC21         C11        C31\nC22         C12        C32\nC23         C13        C33\n"
            ],
            "answer": {
                "ans_desc": "I'm sure there's a better answer, but you can swap the column names and then reorder: ",
                "code": [
                    "df = pd.DataFrame({\"Column1\": [\"C11\", \"C12\", \"C13\"],\n                   \"Column2\": [\"C21\", \"C22\", \"C23\"],\n                   \"Column3\": [\"C31\", \"C32\", \"C33\"]})\ndf.columns = [\"Column2\", \"Column1\", \"Column3\"]\ndf[[\"Column1\", \"Column2\", \"Column3\"]]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 925,
            "user_id": 7246047,
            "user_type": "registered",
            "accept_rate": 76,
            "profile_image": "https://www.gravatar.com/avatar/13b4a7a1a95ac675dfd54ef2a229d81f?s=128&d=identicon&r=PG&f=1",
            "display_name": " owise",
            "link": "https://stackoverflow.com/users/7246047/owise"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 53062285,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1540897142,
        "creation_date": 1540895244,
        "last_edit_date": 1540897142,
        "question_id": 53062225,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53062225/how-can-check-the-duplication-on-the-group-level",
        "title": "How can check the duplication on the group level?",
        "body": "<p>How can I check for duplicated groups and remove them? Here is my data frame:</p>\n\n<pre><code>Group     Value_1      Value_2\n A          17           0.1\n A          20           0.8\n A          22           0.9\n A          24           0.13\n\n B          17           0.1\n B          20           0.8\n B          22           0.9\n B          24           0.13\n\n C          17           0.1\n C          20           0.8\n C          22           0.9\n C          26           0.11    \n</code></pre>\n\n<p>In this data frame group A and B are duplicate where as C is not because its forth element is different and thus it is deeper to be unique not duplicate, the resultant data frame should look like this:</p>\n\n<pre><code>Group     Value_1      Value_2\n A          17           0.1\n A          20           0.8\n A          22           0.9\n A          24           0.13\n\n\n C          17           0.1\n C          20           0.8\n C          22           0.9\n C          26           0.11    \n</code></pre>\n\n<p>I tried to groupby and check for duplicates, but this will check the values on the observational level. How can check the duplication on the group level?</p>\n",
        "answer_body": "<p>You can use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and aggregate by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html\" rel=\"nofollow noreferrer\"><code>agg</code></a> with <code>frozenset</code>, then remove duplicates by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html\" rel=\"nofollow noreferrer\"><code>drop_duplicates</code></a> (by default by all columns) and get indices - all groups names:</p>\n\n<pre><code>idx = df.groupby('Group').agg(frozenset).drop_duplicates().index\n#alternative solution\nidx = df.groupby('Group').agg(tuple).drop_duplicates().index\n</code></pre>\n\n<p>Or reshape to by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.cumcount.html\" rel=\"nofollow noreferrer\"><code>cumcount</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html\" rel=\"nofollow noreferrer\"><code>set_index</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unstack.html\" rel=\"nofollow noreferrer\"><code>unstack</code></a>:</p>\n\n<pre><code>g = df.groupby('Group').cumcount()\nidx = df.set_index(['Group',g]).unstack().drop_duplicates().index\n</code></pre>\n\n<p>Last filter by <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html\" rel=\"nofollow noreferrer\"><code>isin</code></a>:</p>\n\n<pre><code>df = df[df['Group'].isin(idx)]\nprint (df)\n   Group  Value_1  Value_2\n0      A       17     0.10\n1      A       20     0.80\n2      A       22     0.90\n3      A       24     0.13\n8      C       17     0.10\n9      C       20     0.80\n10     C       22     0.90\n11     C       26     0.11\n</code></pre>\n",
        "question_body": "<p>How can I check for duplicated groups and remove them? Here is my data frame:</p>\n\n<pre><code>Group     Value_1      Value_2\n A          17           0.1\n A          20           0.8\n A          22           0.9\n A          24           0.13\n\n B          17           0.1\n B          20           0.8\n B          22           0.9\n B          24           0.13\n\n C          17           0.1\n C          20           0.8\n C          22           0.9\n C          26           0.11    \n</code></pre>\n\n<p>In this data frame group A and B are duplicate where as C is not because its forth element is different and thus it is deeper to be unique not duplicate, the resultant data frame should look like this:</p>\n\n<pre><code>Group     Value_1      Value_2\n A          17           0.1\n A          20           0.8\n A          22           0.9\n A          24           0.13\n\n\n C          17           0.1\n C          20           0.8\n C          22           0.9\n C          26           0.11    \n</code></pre>\n\n<p>I tried to groupby and check for duplicates, but this will check the values on the observational level. How can check the duplication on the group level?</p>\n",
        "formatted_input": {
            "qid": 53062225,
            "link": "https://stackoverflow.com/questions/53062225/how-can-check-the-duplication-on-the-group-level",
            "question": {
                "title": "How can check the duplication on the group level?",
                "ques_desc": "How can I check for duplicated groups and remove them? Here is my data frame: In this data frame group A and B are duplicate where as C is not because its forth element is different and thus it is deeper to be unique not duplicate, the resultant data frame should look like this: I tried to groupby and check for duplicates, but this will check the values on the observational level. How can check the duplication on the group level? "
            },
            "io": [
                "Group     Value_1      Value_2\n A          17           0.1\n A          20           0.8\n A          22           0.9\n A          24           0.13\n\n B          17           0.1\n B          20           0.8\n B          22           0.9\n B          24           0.13\n\n C          17           0.1\n C          20           0.8\n C          22           0.9\n C          26           0.11    \n",
                "Group     Value_1      Value_2\n A          17           0.1\n A          20           0.8\n A          22           0.9\n A          24           0.13\n\n\n C          17           0.1\n C          20           0.8\n C          22           0.9\n C          26           0.11    \n"
            ],
            "answer": {
                "ans_desc": "You can use and aggregate by with , then remove duplicates by (by default by all columns) and get indices - all groups names: Or reshape to by with and : Last filter by with : ",
                "code": [
                    "idx = df.groupby('Group').agg(frozenset).drop_duplicates().index\n#alternative solution\nidx = df.groupby('Group').agg(tuple).drop_duplicates().index\n",
                    "g = df.groupby('Group').cumcount()\nidx = df.set_index(['Group',g]).unstack().drop_duplicates().index\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 708,
            "user_id": 10321210,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6440f6207ef22d725e12b1fb77e5d96d?s=128&d=identicon&r=PG&f=1",
            "display_name": "jxpython",
            "link": "https://stackoverflow.com/users/10321210/jxpython"
        },
        "is_answered": true,
        "view_count": 306,
        "accepted_answer_id": 52952912,
        "answer_count": 7,
        "score": -1,
        "last_activity_date": 1540571014,
        "creation_date": 1538810274,
        "last_edit_date": 1540122934,
        "question_id": 52676653,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52676653/create-a-column-that-has-the-same-length-of-the-longest-column-in-the-data-at-th",
        "title": "Create a column that has the same length of the longest column in the data at the same time",
        "body": "<p>I have the following data:</p>\n\n<pre><code>data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]]\ndataFrame = pandas.DataFrame(data).transpose()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>     0    1    2\n0  1.0  1.0  1.0\n1  2.0  2.0  2.0\n2  3.0  3.0  3.0\n3  NaN  4.0  4.0\n4  NaN  5.0  5.0\n5  NaN  NaN  6.0\n6  NaN  NaN  7.0\n</code></pre>\n\n<p>Is it possible to create a 4th column AT THE SAME TIME the others columns are created in data, which has the same length as the longest column of this dataframe (3rd one)?</p>\n\n<p>The data of this column doesn't matter. Assume it's 8. So this is the desired output can be:</p>\n\n<pre><code>     0    1    2    3\n0  1.0  1.0  1.0  8.0\n1  2.0  2.0  2.0  8.0\n2  3.0  3.0  3.0  8.0\n3  NaN  4.0  4.0  8.0\n4  NaN  5.0  5.0  8.0\n5  NaN  NaN  6.0  8.0\n6  NaN  NaN  7.0  8.0\n</code></pre>\n\n<p>In my script the dataframe keeps changing every time. This means the longest columns keeps changing with it.</p>\n\n<p>Thanks for reading</p>\n",
        "answer_body": "<p>This is quite similar to answers from @jpp, @Cleb, and maybe some other answers here, just slightly simpler:</p>\n\n<pre><code>data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]] + [[]]\n</code></pre>\n\n<p>This will automatically give you a column of NaNs that is the same length as the longest columnn, so you don't need the extra work of calculating the length of the longest column.  Resulting dataframe:</p>\n\n<pre><code>     0    1    2   3\n0  1.0  1.0  1.0 NaN\n1  2.0  2.0  2.0 NaN\n2  3.0  3.0  3.0 NaN\n3  NaN  4.0  4.0 NaN\n4  NaN  5.0  5.0 NaN\n5  NaN  NaN  6.0 NaN\n6  NaN  NaN  7.0 NaN\n</code></pre>\n\n<p>Note that this answer is less general than some others here (such as by @jpp &amp; @Cleb) in that it will only fill with NaNs.  If you want some default fill values other than NaN, you should use one of their answers.</p>\n",
        "question_body": "<p>I have the following data:</p>\n\n<pre><code>data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]]\ndataFrame = pandas.DataFrame(data).transpose()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>     0    1    2\n0  1.0  1.0  1.0\n1  2.0  2.0  2.0\n2  3.0  3.0  3.0\n3  NaN  4.0  4.0\n4  NaN  5.0  5.0\n5  NaN  NaN  6.0\n6  NaN  NaN  7.0\n</code></pre>\n\n<p>Is it possible to create a 4th column AT THE SAME TIME the others columns are created in data, which has the same length as the longest column of this dataframe (3rd one)?</p>\n\n<p>The data of this column doesn't matter. Assume it's 8. So this is the desired output can be:</p>\n\n<pre><code>     0    1    2    3\n0  1.0  1.0  1.0  8.0\n1  2.0  2.0  2.0  8.0\n2  3.0  3.0  3.0  8.0\n3  NaN  4.0  4.0  8.0\n4  NaN  5.0  5.0  8.0\n5  NaN  NaN  6.0  8.0\n6  NaN  NaN  7.0  8.0\n</code></pre>\n\n<p>In my script the dataframe keeps changing every time. This means the longest columns keeps changing with it.</p>\n\n<p>Thanks for reading</p>\n",
        "formatted_input": {
            "qid": 52676653,
            "link": "https://stackoverflow.com/questions/52676653/create-a-column-that-has-the-same-length-of-the-longest-column-in-the-data-at-th",
            "question": {
                "title": "Create a column that has the same length of the longest column in the data at the same time",
                "ques_desc": "I have the following data: Output: Is it possible to create a 4th column AT THE SAME TIME the others columns are created in data, which has the same length as the longest column of this dataframe (3rd one)? The data of this column doesn't matter. Assume it's 8. So this is the desired output can be: In my script the dataframe keeps changing every time. This means the longest columns keeps changing with it. Thanks for reading "
            },
            "io": [
                "     0    1    2\n0  1.0  1.0  1.0\n1  2.0  2.0  2.0\n2  3.0  3.0  3.0\n3  NaN  4.0  4.0\n4  NaN  5.0  5.0\n5  NaN  NaN  6.0\n6  NaN  NaN  7.0\n",
                "     0    1    2    3\n0  1.0  1.0  1.0  8.0\n1  2.0  2.0  2.0  8.0\n2  3.0  3.0  3.0  8.0\n3  NaN  4.0  4.0  8.0\n4  NaN  5.0  5.0  8.0\n5  NaN  NaN  6.0  8.0\n6  NaN  NaN  7.0  8.0\n"
            ],
            "answer": {
                "ans_desc": "This is quite similar to answers from @jpp, @Cleb, and maybe some other answers here, just slightly simpler: This will automatically give you a column of NaNs that is the same length as the longest columnn, so you don't need the extra work of calculating the length of the longest column. Resulting dataframe: Note that this answer is less general than some others here (such as by @jpp & @Cleb) in that it will only fill with NaNs. If you want some default fill values other than NaN, you should use one of their answers. ",
                "code": [
                    "data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]] + [[]]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 443,
            "user_id": 9998989,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c594ed699a93005ce5fe0a93d91b8106?s=128&d=identicon&r=PG&f=1",
            "display_name": "Noob Programmer",
            "link": "https://stackoverflow.com/users/9998989/noob-programmer"
        },
        "is_answered": true,
        "view_count": 56,
        "accepted_answer_id": 52985818,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1540459649,
        "creation_date": 1540459373,
        "question_id": 52985798,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52985798/get-minimum-value-from-index-in-data-frame",
        "title": "Get minimum value from index in data frame",
        "body": "<p>I have a dataframe like this. </p>\n\n<pre><code>column1 column2\n\n1         2\n1         3\n1         4\n2         3\n2         1\n2         4\n</code></pre>\n\n<p>I would like to get minimum values for each value in column1. So my output would be</p>\n\n<pre><code>column1    column2\n1             2\n2             1\n</code></pre>\n\n<p>When I try the code </p>\n\n<pre><code>df = df[df['column2'].isin(df.groupby('column1').idxmin(['column2']).values)]\n</code></pre>\n\n<p>It gives me an empty dataframe and if I try</p>\n\n<pre><code>a = df[df['column2'].isin(df.groupby('column1').min()['column2'].values)]\n</code></pre>\n\n<p>it deletes some values, for reasons I don't understand. </p>\n\n<p>I use python 2.7 </p>\n",
        "answer_body": "<p>Function <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.idxmax.html\" rel=\"nofollow noreferrer\"><code>DataFrameGroupBy.idxmax</code></a> return index values of minimal values of column <code>column2</code> per groups, so is necessary <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a> for selecting:</p>\n\n<pre><code>df = df.loc[df.groupby('column1')['column2'].idxmin()]\nprint (df)\n   column1  column2\n0        1        2\n4        2        1\n</code></pre>\n\n<p>Another solution is use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html\" rel=\"nofollow noreferrer\"><code>drop_duplicates</code></a>:</p>\n\n<pre><code>df = df.sort_values('column2', ascending=False).drop_duplicates('column1', keep='last')\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>If possible multiple minimal values and want select all of them use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a>:</p>\n\n<pre><code>print (df)\n   column1  column2\n0        1        2\n1        1        3\n2        1        4\n3        2        1\n4        2        1\n5        2        4\n\n\ndf2 = df[df.groupby('column1')['column2'].transform('min') == df['column2']]\nprint (df2)\n   column1  column2\n0        1        2\n3        2        1\n4        2        1\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this. </p>\n\n<pre><code>column1 column2\n\n1         2\n1         3\n1         4\n2         3\n2         1\n2         4\n</code></pre>\n\n<p>I would like to get minimum values for each value in column1. So my output would be</p>\n\n<pre><code>column1    column2\n1             2\n2             1\n</code></pre>\n\n<p>When I try the code </p>\n\n<pre><code>df = df[df['column2'].isin(df.groupby('column1').idxmin(['column2']).values)]\n</code></pre>\n\n<p>It gives me an empty dataframe and if I try</p>\n\n<pre><code>a = df[df['column2'].isin(df.groupby('column1').min()['column2'].values)]\n</code></pre>\n\n<p>it deletes some values, for reasons I don't understand. </p>\n\n<p>I use python 2.7 </p>\n",
        "formatted_input": {
            "qid": 52985798,
            "link": "https://stackoverflow.com/questions/52985798/get-minimum-value-from-index-in-data-frame",
            "question": {
                "title": "Get minimum value from index in data frame",
                "ques_desc": "I have a dataframe like this. I would like to get minimum values for each value in column1. So my output would be When I try the code It gives me an empty dataframe and if I try it deletes some values, for reasons I don't understand. I use python 2.7 "
            },
            "io": [
                "column1 column2\n\n1         2\n1         3\n1         4\n2         3\n2         1\n2         4\n",
                "column1    column2\n1             2\n2             1\n"
            ],
            "answer": {
                "ans_desc": "Function return index values of minimal values of column per groups, so is necessary for selecting: Another solution is use with : EDIT: If possible multiple minimal values and want select all of them use with : ",
                "code": [
                    "df = df.sort_values('column2', ascending=False).drop_duplicates('column1', keep='last')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "lambda",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 7019904,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-Y_W2p9rxmIM/AAAAAAAAAAI/AAAAAAAAAVk/UM8nlgZ9mt8/photo.jpg?sz=128",
            "display_name": "kartriter",
            "link": "https://stackoverflow.com/users/7019904/kartriter"
        },
        "is_answered": true,
        "view_count": 127,
        "accepted_answer_id": 52940940,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1540352341,
        "creation_date": 1540261760,
        "last_edit_date": 1540352341,
        "question_id": 52940317,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52940317/pandas-variable-shifting-within-groups",
        "title": "Pandas variable shifting within groups",
        "body": "<p>I have a dataframe:</p>\n\n<pre><code>c1   Lag  Val1  \nA    3    10\nA    1    5\nA    2    20\nA    2    15\nA    1    10\nB    1    25\nB    2    10\n</code></pre>\n\n<p>I want to create a new field val2 such that each value in val2 is the value in val2 shifted by Lag number of rows. The tricky part here is that the shift should happen within the groups defined in field c1, such that the output looks something like  </p>\n\n<pre><code>c1   Lag  Val1  Val2\nA    3    10    15\nA    1    5     20\nA    2    20    10\nA    2    15    NaN\nA    1    10    NaN\nB    1    25    10\nB    2    10    NaN\n</code></pre>\n\n<p>I have been trying with along the lines of</p>\n\n<pre><code>df['Val2'] = df.groupby(['c1'])['Val1'].apply(lambda x:x.shift(df.Lag))\n</code></pre>\n\n<p>to no avail and getting a \"The truth value of a Series is ambiguous.\" error. Appreciate any help. Thanks!</p>\n",
        "answer_body": "<p>You can can accomplish this with self-<code>merge</code> and a little manipulation of the index:</p>\n\n<pre><code># Copy and keep only the columns that are relevant\ndf2 = df.rename(columns={'Val1': 'Val2'}).drop(columns='Lag').copy()\n\n# Shift the index\ndf.index = df.index+df.Lag\n\n# Merge, requiring match on shifted index and within group.\ndf.reset_index().merge(df2.reset_index(), on=['index', 'c1'], how='left').drop(columns='index')\n</code></pre>\n\n<h3>Output:</h3>\n\n<pre><code>  c1  Lag  Val1  Val2\n0  A    3    10  15.0\n1  A    1     5  20.0\n2  A    2    20  10.0\n3  A    2    15   NaN\n4  A    1    10   NaN\n5  B    1    25  10.0\n6  B    2    10   NaN\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n\n<pre><code>c1   Lag  Val1  \nA    3    10\nA    1    5\nA    2    20\nA    2    15\nA    1    10\nB    1    25\nB    2    10\n</code></pre>\n\n<p>I want to create a new field val2 such that each value in val2 is the value in val2 shifted by Lag number of rows. The tricky part here is that the shift should happen within the groups defined in field c1, such that the output looks something like  </p>\n\n<pre><code>c1   Lag  Val1  Val2\nA    3    10    15\nA    1    5     20\nA    2    20    10\nA    2    15    NaN\nA    1    10    NaN\nB    1    25    10\nB    2    10    NaN\n</code></pre>\n\n<p>I have been trying with along the lines of</p>\n\n<pre><code>df['Val2'] = df.groupby(['c1'])['Val1'].apply(lambda x:x.shift(df.Lag))\n</code></pre>\n\n<p>to no avail and getting a \"The truth value of a Series is ambiguous.\" error. Appreciate any help. Thanks!</p>\n",
        "formatted_input": {
            "qid": 52940317,
            "link": "https://stackoverflow.com/questions/52940317/pandas-variable-shifting-within-groups",
            "question": {
                "title": "Pandas variable shifting within groups",
                "ques_desc": "I have a dataframe: I want to create a new field val2 such that each value in val2 is the value in val2 shifted by Lag number of rows. The tricky part here is that the shift should happen within the groups defined in field c1, such that the output looks something like I have been trying with along the lines of to no avail and getting a \"The truth value of a Series is ambiguous.\" error. Appreciate any help. Thanks! "
            },
            "io": [
                "c1   Lag  Val1  \nA    3    10\nA    1    5\nA    2    20\nA    2    15\nA    1    10\nB    1    25\nB    2    10\n",
                "c1   Lag  Val1  Val2\nA    3    10    15\nA    1    5     20\nA    2    20    10\nA    2    15    NaN\nA    1    10    NaN\nB    1    25    10\nB    2    10    NaN\n"
            ],
            "answer": {
                "ans_desc": "You can can accomplish this with self- and a little manipulation of the index: Output: ",
                "code": [
                    "# Copy and keep only the columns that are relevant\ndf2 = df.rename(columns={'Val1': 'Val2'}).drop(columns='Lag').copy()\n\n# Shift the index\ndf.index = df.index+df.Lag\n\n# Merge, requiring match on shifted index and within group.\ndf.reset_index().merge(df2.reset_index(), on=['index', 'c1'], how='left').drop(columns='index')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "recursion",
            "series"
        ],
        "owner": {
            "reputation": 517,
            "user_id": 8706181,
            "user_type": "registered",
            "accept_rate": 73,
            "profile_image": "https://www.gravatar.com/avatar/567132ef7e59ab952f827ea6f15e2667?s=128&d=identicon&r=PG&f=1",
            "display_name": "J Ng",
            "link": "https://stackoverflow.com/users/8706181/j-ng"
        },
        "is_answered": true,
        "view_count": 70,
        "accepted_answer_id": 52906601,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1540045055,
        "creation_date": 1540011345,
        "last_edit_date": 1540045055,
        "question_id": 52902568,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52902568/series-calculation-based-on-shifted-values-recursive-algorithm",
        "title": "Series calculation based on shifted values / recursive algorithm",
        "body": "<p>I have the following:</p>\n\n<pre><code>df['PositionLong'] = 0\ndf['PositionLong'] = np.where(df['Alpha'] == 1, 1, (np.where(np.logical_and(df['PositionLong'].shift(1) == 1, df['Bravo'] == 1), 1, 0)))\n</code></pre>\n\n<p>This lines basically only take in df['Alpha'] but not the df['PositionLong'].shift(1).. It cannot recognize it but I dont understand why?</p>\n\n<p>It produces this:</p>\n\n<pre><code>df['Alpha']  df['Bravo']   df['PositionLong']\n0               0             0\n1               1             1\n0               1             0\n1               1             1\n1               1             1\n</code></pre>\n\n<p>However what I wanted the code to do is this:</p>\n\n<pre><code>df['Alpha']  df['Bravo']   df['PositionLong']\n0               0             0\n1               1             1\n0               1             1\n1               1             1\n1               1             1\n</code></pre>\n\n<p>I believe the solution is to loop each row, but this will take very long.</p>\n\n<p>Can you help me please?</p>\n",
        "answer_body": "<p>You are looking for a <strong>recursive function</strong>, since a previous <code>PositionLong</code> value depends on <code>Alpha</code>, which itself is used to determine <code>PositionLong</code>.</p>\n\n<p>But <code>numpy.where</code> is a regular function, so <code>df['PositionLong'].shift(1)</code> is evaluated as a series of <code>0</code> values, since you initialise the series with <code>0</code>.</p>\n\n<p>A manual loop need not be expensive. You can use <a href=\"http://numba.pydata.org/\" rel=\"nofollow noreferrer\"><code>numba</code></a> to efficiently implement your recursive algorithm:</p>\n\n<pre><code>from numba import njit\n\n@njit\ndef rec_algo(alpha, bravo):\n    res = np.empty(alpha.shape)\n    res[0] = 1 if alpha[0] == 1 else 0\n    for i in range(1, len(res)):\n        if (alpha[i] == 1) or ((res[i-1] == 1) and bravo[i] == 1):\n            res[i] = 1\n        else:\n            res[i] = 0\n    return res\n\ndf['PositionLong'] = rec_algo(df['Alpha'].values, df['Bravo'].values).astype(int)\n</code></pre>\n\n<p>Result:</p>\n\n<pre><code>print(df)\n\n   Alpha  Bravo  PositionLong\n0      0      0             0\n1      1      1             1\n2      0      1             1\n3      1      1             1\n4      1      1             1\n</code></pre>\n",
        "question_body": "<p>I have the following:</p>\n\n<pre><code>df['PositionLong'] = 0\ndf['PositionLong'] = np.where(df['Alpha'] == 1, 1, (np.where(np.logical_and(df['PositionLong'].shift(1) == 1, df['Bravo'] == 1), 1, 0)))\n</code></pre>\n\n<p>This lines basically only take in df['Alpha'] but not the df['PositionLong'].shift(1).. It cannot recognize it but I dont understand why?</p>\n\n<p>It produces this:</p>\n\n<pre><code>df['Alpha']  df['Bravo']   df['PositionLong']\n0               0             0\n1               1             1\n0               1             0\n1               1             1\n1               1             1\n</code></pre>\n\n<p>However what I wanted the code to do is this:</p>\n\n<pre><code>df['Alpha']  df['Bravo']   df['PositionLong']\n0               0             0\n1               1             1\n0               1             1\n1               1             1\n1               1             1\n</code></pre>\n\n<p>I believe the solution is to loop each row, but this will take very long.</p>\n\n<p>Can you help me please?</p>\n",
        "formatted_input": {
            "qid": 52902568,
            "link": "https://stackoverflow.com/questions/52902568/series-calculation-based-on-shifted-values-recursive-algorithm",
            "question": {
                "title": "Series calculation based on shifted values / recursive algorithm",
                "ques_desc": "I have the following: This lines basically only take in df['Alpha'] but not the df['PositionLong'].shift(1).. It cannot recognize it but I dont understand why? It produces this: However what I wanted the code to do is this: I believe the solution is to loop each row, but this will take very long. Can you help me please? "
            },
            "io": [
                "df['Alpha']  df['Bravo']   df['PositionLong']\n0               0             0\n1               1             1\n0               1             0\n1               1             1\n1               1             1\n",
                "df['Alpha']  df['Bravo']   df['PositionLong']\n0               0             0\n1               1             1\n0               1             1\n1               1             1\n1               1             1\n"
            ],
            "answer": {
                "ans_desc": "You are looking for a recursive function, since a previous value depends on , which itself is used to determine . But is a regular function, so is evaluated as a series of values, since you initialise the series with . A manual loop need not be expensive. You can use to efficiently implement your recursive algorithm: Result: ",
                "code": [
                    "from numba import njit\n\n@njit\ndef rec_algo(alpha, bravo):\n    res = np.empty(alpha.shape)\n    res[0] = 1 if alpha[0] == 1 else 0\n    for i in range(1, len(res)):\n        if (alpha[i] == 1) or ((res[i-1] == 1) and bravo[i] == 1):\n            res[i] = 1\n        else:\n            res[i] = 0\n    return res\n\ndf['PositionLong'] = rec_algo(df['Alpha'].values, df['Bravo'].values).astype(int)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 379,
            "user_id": 3579151,
            "user_type": "registered",
            "accept_rate": 82,
            "profile_image": "https://www.gravatar.com/avatar/dacea2b9f6d752a47eb430dd42464ff0?s=128&d=identicon&r=PG",
            "display_name": "energyMax",
            "link": "https://stackoverflow.com/users/3579151/energymax"
        },
        "is_answered": true,
        "view_count": 428,
        "accepted_answer_id": 52877111,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1539892896,
        "creation_date": 1539864140,
        "last_edit_date": 1539892896,
        "question_id": 52873547,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52873547/calculating-grid-values-given-the-distance-in-python",
        "title": "Calculating grid values given the distance in python",
        "body": "<p>I have a cell grid of big dimensions. Each cell has an ID (<code>p1</code>), cell value (<code>p3</code>) and coordinates in actual measures (<code>X</code>, <code>Y</code>). This is how first 10 rows/cells look like</p>\n\n<pre><code>      p1     p2          p3     X  Y\n0      0     0.0         0.0    0  0\n1      1     0.0         0.0  100  0\n2      2     0.0        12.0  200  0\n3      3     0.0         0.0  300  0\n4      4     0.0        70.0  400  0\n5      5     0.0        40.0  500  0\n6      6     0.0        20.0  600  0\n7      7     0.0         0.0  700  0\n8      8     0.0         0.0  800  0\n9      9     0.0         0.0  900  0\n</code></pre>\n\n<p>Neighbouring cells of cell <code>i</code> in the <code>p1</code> can be determined as (<code>i-500+1</code>, <code>i-500-1</code>, <code>i-1</code>, <code>i+1</code>, <code>i+500+1</code>, <code>i+500-1</code>).\nFor example: <code>p1</code> of 5 has neighbours - 4,6,504,505,506. (these are the ID of rows in the upper table - <code>p1</code>).</p>\n\n<p>What I am trying to is:\nFor the chosen value/row <code>i</code> in <code>p1</code>, I would like to know all neighbours in the chosen distance from <code>i</code> and sum all their <code>p3</code> values.</p>\n\n<p>I tried to apply this solution (<a href=\"https://stackoverflow.com/questions/1620940/determining-neighbours-of-cell-two-dimensional-list\">link</a>), but I don't know how to incorporate the distance parameter. The cell value can be taken with <code>df.iloc</code>, but the steps before this are a bit tricky for me.</p>\n\n<p>Can you give me any advice?</p>\n\n<p>EDIT:\nUsing the solution from Thomas and having df called <code>CO</code>:</p>\n\n<pre><code>      p3\n0     45\n1    580\n2  12000\n3  12531\n4  22456\n</code></pre>\n\n<p>I'd like to add another column and use the values from <code>p3</code> columns</p>\n\n<pre><code>CO['new'] = format(sum_neighbors(data, CO['p3']))\n</code></pre>\n\n<p>But it doesn't work. If I add a number instead of a reference to row <code>CO['p3']</code> it works like charm. But how can I use values from p3 column automatically in <code>format</code> function?</p>\n\n<p>SOLVED:\nIt worked with:</p>\n\n<pre><code>CO['new'] = CO.apply(lambda row: sum_neighbors(data, row.p3), axis=1)\n</code></pre>\n",
        "answer_body": "<p>Solution:</p>\n\n<pre><code>import numpy as np\nimport pandas\n\n# Generating toy data\nN = 10\ndata = pandas.DataFrame({'p3': np.random.randn(N)})\nprint(data)\n\n# Finding neighbours\nget_candidates = lambda i: [i-500+1, i-500-1, i-1, i+1, i+500+1, i+500-1]\nfilter = lambda neighbors, N: [n for n in neighbors if 0&lt;=n&lt;N]\nget_neighbors = lambda i, N: filter(get_candidates(i), N)\n\nprint(\"Neighbors of 5: {}\".format(get_neighbors(5, len(data))))\n\n# Summing p3 on neighbors\ndef sum_neighbors(data, i, col='p3'):\n  return data.iloc[get_neighbors(i, len(data))][col].sum()\n\nprint(\"p3 sum on neighbors of 5: {}\".format(sum_neighbors(data, 5)))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>         p3\n0 -1.106541\n1 -0.760620\n2  1.282252\n3  0.204436\n4 -1.147042\n5  1.363007\n6 -0.030772\n7 -0.461756\n8 -1.110459\n9 -0.491368\n\nNeighbors of 5: [4, 6]\n\np3 sum on neighbors of 5: -1.1778133703169344\n</code></pre>\n\n<p>Notes:</p>\n\n<ul>\n<li>I assumed <code>p1</code> was <code>range(N)</code> as seemed to be implied (so we don't need it at all).</li>\n<li>I don't think that <code>505</code> is a neighbour of <code>5</code> given the list of neighbors of <code>i</code> defined by the OP. </li>\n</ul>\n",
        "question_body": "<p>I have a cell grid of big dimensions. Each cell has an ID (<code>p1</code>), cell value (<code>p3</code>) and coordinates in actual measures (<code>X</code>, <code>Y</code>). This is how first 10 rows/cells look like</p>\n\n<pre><code>      p1     p2          p3     X  Y\n0      0     0.0         0.0    0  0\n1      1     0.0         0.0  100  0\n2      2     0.0        12.0  200  0\n3      3     0.0         0.0  300  0\n4      4     0.0        70.0  400  0\n5      5     0.0        40.0  500  0\n6      6     0.0        20.0  600  0\n7      7     0.0         0.0  700  0\n8      8     0.0         0.0  800  0\n9      9     0.0         0.0  900  0\n</code></pre>\n\n<p>Neighbouring cells of cell <code>i</code> in the <code>p1</code> can be determined as (<code>i-500+1</code>, <code>i-500-1</code>, <code>i-1</code>, <code>i+1</code>, <code>i+500+1</code>, <code>i+500-1</code>).\nFor example: <code>p1</code> of 5 has neighbours - 4,6,504,505,506. (these are the ID of rows in the upper table - <code>p1</code>).</p>\n\n<p>What I am trying to is:\nFor the chosen value/row <code>i</code> in <code>p1</code>, I would like to know all neighbours in the chosen distance from <code>i</code> and sum all their <code>p3</code> values.</p>\n\n<p>I tried to apply this solution (<a href=\"https://stackoverflow.com/questions/1620940/determining-neighbours-of-cell-two-dimensional-list\">link</a>), but I don't know how to incorporate the distance parameter. The cell value can be taken with <code>df.iloc</code>, but the steps before this are a bit tricky for me.</p>\n\n<p>Can you give me any advice?</p>\n\n<p>EDIT:\nUsing the solution from Thomas and having df called <code>CO</code>:</p>\n\n<pre><code>      p3\n0     45\n1    580\n2  12000\n3  12531\n4  22456\n</code></pre>\n\n<p>I'd like to add another column and use the values from <code>p3</code> columns</p>\n\n<pre><code>CO['new'] = format(sum_neighbors(data, CO['p3']))\n</code></pre>\n\n<p>But it doesn't work. If I add a number instead of a reference to row <code>CO['p3']</code> it works like charm. But how can I use values from p3 column automatically in <code>format</code> function?</p>\n\n<p>SOLVED:\nIt worked with:</p>\n\n<pre><code>CO['new'] = CO.apply(lambda row: sum_neighbors(data, row.p3), axis=1)\n</code></pre>\n",
        "formatted_input": {
            "qid": 52873547,
            "link": "https://stackoverflow.com/questions/52873547/calculating-grid-values-given-the-distance-in-python",
            "question": {
                "title": "Calculating grid values given the distance in python",
                "ques_desc": "I have a cell grid of big dimensions. Each cell has an ID (), cell value () and coordinates in actual measures (, ). This is how first 10 rows/cells look like Neighbouring cells of cell in the can be determined as (, , , , , ). For example: of 5 has neighbours - 4,6,504,505,506. (these are the ID of rows in the upper table - ). What I am trying to is: For the chosen value/row in , I would like to know all neighbours in the chosen distance from and sum all their values. I tried to apply this solution (link), but I don't know how to incorporate the distance parameter. The cell value can be taken with , but the steps before this are a bit tricky for me. Can you give me any advice? EDIT: Using the solution from Thomas and having df called : I'd like to add another column and use the values from columns But it doesn't work. If I add a number instead of a reference to row it works like charm. But how can I use values from p3 column automatically in function? SOLVED: It worked with: "
            },
            "io": [
                "      p1     p2          p3     X  Y\n0      0     0.0         0.0    0  0\n1      1     0.0         0.0  100  0\n2      2     0.0        12.0  200  0\n3      3     0.0         0.0  300  0\n4      4     0.0        70.0  400  0\n5      5     0.0        40.0  500  0\n6      6     0.0        20.0  600  0\n7      7     0.0         0.0  700  0\n8      8     0.0         0.0  800  0\n9      9     0.0         0.0  900  0\n",
                "      p3\n0     45\n1    580\n2  12000\n3  12531\n4  22456\n"
            ],
            "answer": {
                "ans_desc": "Solution: Output: Notes: I assumed was as seemed to be implied (so we don't need it at all). I don't think that is a neighbour of given the list of neighbors of defined by the OP. ",
                "code": [
                    "import numpy as np\nimport pandas\n\n# Generating toy data\nN = 10\ndata = pandas.DataFrame({'p3': np.random.randn(N)})\nprint(data)\n\n# Finding neighbours\nget_candidates = lambda i: [i-500+1, i-500-1, i-1, i+1, i+500+1, i+500-1]\nfilter = lambda neighbors, N: [n for n in neighbors if 0<=n<N]\nget_neighbors = lambda i, N: filter(get_candidates(i), N)\n\nprint(\"Neighbors of 5: {}\".format(get_neighbors(5, len(data))))\n\n# Summing p3 on neighbors\ndef sum_neighbors(data, i, col='p3'):\n  return data.iloc[get_neighbors(i, len(data))][col].sum()\n\nprint(\"p3 sum on neighbors of 5: {}\".format(sum_neighbors(data, 5)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 2707200,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/822ac545e3a27fe9aa6fd976c23774c4?s=128&d=identicon&r=PG&f=1",
            "display_name": "thangaraj1980",
            "link": "https://stackoverflow.com/users/2707200/thangaraj1980"
        },
        "is_answered": true,
        "view_count": 112,
        "accepted_answer_id": 52800778,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1539505027,
        "creation_date": 1539502131,
        "last_edit_date": 1539502499,
        "question_id": 52800453,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52800453/how-to-remove-unwanted-data-from-python-panda-data-frame",
        "title": "How to remove unwanted data from python panda data frame?",
        "body": "<p>After reading my txt file:  <a href=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale\" rel=\"nofollow noreferrer\">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale</a></p>\n\n<p>The panda datadframe like as below:</p>\n\n<pre><code>1                 2                     3      4            \n\n-0.4302012 2     -0.3233208 3    0.576837 4   0.426791 5 \n</code></pre>\n\n<p>But I need the data as below( Space and extra letter should be removed)</p>\n\n<pre><code>1                 2                     3      4            \n\n-0.4302012      -0.3233208     0.576837   0.426791 \n</code></pre>\n",
        "answer_body": "<p>I got the answer: Thanks for all trying</p>\n\n<pre><code>data = pd.read_csv(filepath,delimiter=':',header= None)\ndata = data.drop([0],axis=1)\nfor i in range(1,9):\n   search_string = str(i+1)\n   data[i] = data[i].map(lambda x: x.replace(search_string.rjust(1),''))\nprint(data)\ndata.sample(n=5)\n</code></pre>\n",
        "question_body": "<p>After reading my txt file:  <a href=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale\" rel=\"nofollow noreferrer\">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale</a></p>\n\n<p>The panda datadframe like as below:</p>\n\n<pre><code>1                 2                     3      4            \n\n-0.4302012 2     -0.3233208 3    0.576837 4   0.426791 5 \n</code></pre>\n\n<p>But I need the data as below( Space and extra letter should be removed)</p>\n\n<pre><code>1                 2                     3      4            \n\n-0.4302012      -0.3233208     0.576837   0.426791 \n</code></pre>\n",
        "formatted_input": {
            "qid": 52800453,
            "link": "https://stackoverflow.com/questions/52800453/how-to-remove-unwanted-data-from-python-panda-data-frame",
            "question": {
                "title": "How to remove unwanted data from python panda data frame?",
                "ques_desc": "After reading my txt file: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale The panda datadframe like as below: But I need the data as below( Space and extra letter should be removed) "
            },
            "io": [
                "1                 2                     3      4            \n\n-0.4302012 2     -0.3233208 3    0.576837 4   0.426791 5 \n",
                "1                 2                     3      4            \n\n-0.4302012      -0.3233208     0.576837   0.426791 \n"
            ],
            "answer": {
                "ans_desc": "I got the answer: Thanks for all trying ",
                "code": [
                    "data = pd.read_csv(filepath,delimiter=':',header= None)\ndata = data.drop([0],axis=1)\nfor i in range(1,9):\n   search_string = str(i+1)\n   data[i] = data[i].map(lambda x: x.replace(search_string.rjust(1),''))\nprint(data)\ndata.sample(n=5)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 602,
            "user_id": 10431629,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/762cd3a520e3aa51db0a5c4c25540e1e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Stan",
            "link": "https://stackoverflow.com/users/10431629/stan"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 52746813,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1539196917,
        "creation_date": 1539187053,
        "last_edit_date": 1539191363,
        "question_id": 52744334,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52744334/creating-a-function-to-perform-grouping-and-sorting-based-on-columns-in-pandas-d",
        "title": "Creating a function to perform grouping and sorting based on columns in Pandas dataframe and Labeling",
        "body": "<pre><code>import pandas as pd\n\nimport numpy as np\n\ndf = pd.DataFrame([\n[100,     'm1',   1, 4],\n[200,     'm2',   7, 5], \n[120,     'm1',   4, 4],\n[240,     'm2',   8, 5],\n[300,     'm3',   5, 4],\n[330,     'm3',   2, 4],\n[350,     'm3',   11, 4],\n[200,     'm4',    9, 4]],\ncolumns=['Col1',  'Col2',   'Col3', 'Col4'])\n</code></pre>\n\n<p>I am wanting to group the data into two groups based on the Col2 group. However the first match should be assigned one value and the rest of the matches should be assigned a different value. Rahlf helped me to get \na function created </p>\n\n<pre><code>def my_function(x, val):\n\n    if x.shape[0]==1:\n        if x.iloc[0]&gt;val:\n            return 'high'\n        else:\n            return 'low'\n\n    if x.iloc[0]&gt;val and any(i&lt;=val for i in x.iloc[1:]):\n        return 'high'\n    elif x.iloc[0]&gt;val:\n        return 'med'\n    elif x.iloc[0]&lt;=val:\n        return 'low'\n    else:\n        return np.nan\n</code></pre>\n\n<p>and then do </p>\n\n<pre><code>df['Col5'] = df.sort_values(['Col2','Col1']).groupby('Col2')['Col3'].transform(my_function, (4))\n</code></pre>\n\n<p>However, I need two modifications to the function. Instead of the val, it will take the corresponding value from the Col 4 and then return one value (like 'low' to the first match within a group (based on the sorted col1) and then say 'low_red' for the rest of the matches in the group.</p>\n\n<p>So my question is how can I modify the function to do that?</p>\n\n<p>My Input:</p>\n\n<pre><code>   Col1 Col2  Col3  Col4    \n   100   m1     1     4    \n   200   m2     7     5    \n   120   m1     4     4   \n   240   m2     8     5   \n   300   m3     5     4   \n   330   m3     2     4    \n   350   m3    11     4    \n   200   m4     9     4\n</code></pre>\n\n<p>Expected Output:</p>\n\n<pre><code>   Col1 Col2  Col3  Col4   Col 5    \n   100   m1     1     4    low    \n   200   m2     7     5    med    \n   120   m1     4     4    low_red    \n   240   m2     8     5    med_red    \n   300   m3     5     4    high    \n   330   m3     2     4    high_red    \n   350   m3    11     4    high_red    \n   200   m4     9     4    high\n</code></pre>\n",
        "answer_body": "<p>You can create a higher level function (let's call it <code>my_function()</code>) that is called by <code>transform()</code>, which then calls a lower level function (let's call it <code>deeper_logic()</code>) that applies the previous logic outlined in your question, like so:</p>\n\n<pre><code>def my_function(group):\n\n    val = df.iloc[group.index]['Col4']\n\n    value = deeper_logic(group.iloc[0], val.iloc[0], group)\n\n    return [value if i==0 else value + '_red' for i in range(group.shape[0])]\n\ndef deeper_logic(x, val, group):\n\n    if group.shape[0]==1:\n        if x&gt;val:\n            return 'high'\n        else:\n            return 'low'\n\n    if x&gt;val and any(i&lt;=val for i in group.iloc[1:]):\n        return 'high'\n    elif x&gt;val:\n        return 'med'\n    elif x&lt;=val:\n        return 'low'\n    else:\n        return np.nan\n\ndf['Col5'] = df.sort_values(['Col2','Col1']).groupby('Col2')['Col3'].transform(my_function)\n</code></pre>\n\n<p>This yields:</p>\n\n<pre><code>   Col1 Col2  Col3  Col4      Col5\n0   100   m1     1     4       low\n1   200   m2     7     5       med\n2   120   m1     4     4   low_red\n3   240   m2     8     5   med_red\n4   300   m3     5     4      high\n5   330   m3     2     4  high_red\n6   350   m3    11     4  high_red\n7   200   m4     9     4      high\n</code></pre>\n\n<p>Note that <code>transform()</code> operates on series and returns a like-indexed NDFrame, which is the result that we want (i.e. retain the index of the original dataframe). Therefore, we can call <code>transform()</code> with our <code>Col3</code> column, and then extract the corresponding <code>Col4</code> column values from the original index using <code>iloc</code> in the function being called from <code>transform()</code>.</p>\n",
        "question_body": "<pre><code>import pandas as pd\n\nimport numpy as np\n\ndf = pd.DataFrame([\n[100,     'm1',   1, 4],\n[200,     'm2',   7, 5], \n[120,     'm1',   4, 4],\n[240,     'm2',   8, 5],\n[300,     'm3',   5, 4],\n[330,     'm3',   2, 4],\n[350,     'm3',   11, 4],\n[200,     'm4',    9, 4]],\ncolumns=['Col1',  'Col2',   'Col3', 'Col4'])\n</code></pre>\n\n<p>I am wanting to group the data into two groups based on the Col2 group. However the first match should be assigned one value and the rest of the matches should be assigned a different value. Rahlf helped me to get \na function created </p>\n\n<pre><code>def my_function(x, val):\n\n    if x.shape[0]==1:\n        if x.iloc[0]&gt;val:\n            return 'high'\n        else:\n            return 'low'\n\n    if x.iloc[0]&gt;val and any(i&lt;=val for i in x.iloc[1:]):\n        return 'high'\n    elif x.iloc[0]&gt;val:\n        return 'med'\n    elif x.iloc[0]&lt;=val:\n        return 'low'\n    else:\n        return np.nan\n</code></pre>\n\n<p>and then do </p>\n\n<pre><code>df['Col5'] = df.sort_values(['Col2','Col1']).groupby('Col2')['Col3'].transform(my_function, (4))\n</code></pre>\n\n<p>However, I need two modifications to the function. Instead of the val, it will take the corresponding value from the Col 4 and then return one value (like 'low' to the first match within a group (based on the sorted col1) and then say 'low_red' for the rest of the matches in the group.</p>\n\n<p>So my question is how can I modify the function to do that?</p>\n\n<p>My Input:</p>\n\n<pre><code>   Col1 Col2  Col3  Col4    \n   100   m1     1     4    \n   200   m2     7     5    \n   120   m1     4     4   \n   240   m2     8     5   \n   300   m3     5     4   \n   330   m3     2     4    \n   350   m3    11     4    \n   200   m4     9     4\n</code></pre>\n\n<p>Expected Output:</p>\n\n<pre><code>   Col1 Col2  Col3  Col4   Col 5    \n   100   m1     1     4    low    \n   200   m2     7     5    med    \n   120   m1     4     4    low_red    \n   240   m2     8     5    med_red    \n   300   m3     5     4    high    \n   330   m3     2     4    high_red    \n   350   m3    11     4    high_red    \n   200   m4     9     4    high\n</code></pre>\n",
        "formatted_input": {
            "qid": 52744334,
            "link": "https://stackoverflow.com/questions/52744334/creating-a-function-to-perform-grouping-and-sorting-based-on-columns-in-pandas-d",
            "question": {
                "title": "Creating a function to perform grouping and sorting based on columns in Pandas dataframe and Labeling",
                "ques_desc": " I am wanting to group the data into two groups based on the Col2 group. However the first match should be assigned one value and the rest of the matches should be assigned a different value. Rahlf helped me to get a function created and then do However, I need two modifications to the function. Instead of the val, it will take the corresponding value from the Col 4 and then return one value (like 'low' to the first match within a group (based on the sorted col1) and then say 'low_red' for the rest of the matches in the group. So my question is how can I modify the function to do that? My Input: Expected Output: "
            },
            "io": [
                "   Col1 Col2  Col3  Col4    \n   100   m1     1     4    \n   200   m2     7     5    \n   120   m1     4     4   \n   240   m2     8     5   \n   300   m3     5     4   \n   330   m3     2     4    \n   350   m3    11     4    \n   200   m4     9     4\n",
                "   Col1 Col2  Col3  Col4   Col 5    \n   100   m1     1     4    low    \n   200   m2     7     5    med    \n   120   m1     4     4    low_red    \n   240   m2     8     5    med_red    \n   300   m3     5     4    high    \n   330   m3     2     4    high_red    \n   350   m3    11     4    high_red    \n   200   m4     9     4    high\n"
            ],
            "answer": {
                "ans_desc": "You can create a higher level function (let's call it ) that is called by , which then calls a lower level function (let's call it ) that applies the previous logic outlined in your question, like so: This yields: Note that operates on series and returns a like-indexed NDFrame, which is the result that we want (i.e. retain the index of the original dataframe). Therefore, we can call with our column, and then extract the corresponding column values from the original index using in the function being called from . ",
                "code": [
                    "def my_function(group):\n\n    val = df.iloc[group.index]['Col4']\n\n    value = deeper_logic(group.iloc[0], val.iloc[0], group)\n\n    return [value if i==0 else value + '_red' for i in range(group.shape[0])]\n\ndef deeper_logic(x, val, group):\n\n    if group.shape[0]==1:\n        if x>val:\n            return 'high'\n        else:\n            return 'low'\n\n    if x>val and any(i<=val for i in group.iloc[1:]):\n        return 'high'\n    elif x>val:\n        return 'med'\n    elif x<=val:\n        return 'low'\n    else:\n        return np.nan\n\ndf['Col5'] = df.sort_values(['Col2','Col1']).groupby('Col2')['Col3'].transform(my_function)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 101,
            "user_id": 9984938,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-trXOptLFKag/AAAAAAAAAAI/AAAAAAAAAAA/AB6qoq2kizw4ipC6dIyTHrSm1VLhP1V0ug/mo/photo.jpg?sz=128",
            "display_name": "Rawan 2018",
            "link": "https://stackoverflow.com/users/9984938/rawan-2018"
        },
        "is_answered": true,
        "view_count": 227,
        "accepted_answer_id": 52745226,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1539190950,
        "creation_date": 1539190171,
        "last_edit_date": 1539190493,
        "question_id": 52745166,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52745166/how-to-take-first-one-or-two-digits-from-float-number",
        "title": "How to take first one or two digits from float number?",
        "body": "<p>I have a Pandas dataframe. In a series, I have time in hour and minute represented as <code>float</code>, as below. I want only the hours:</p>\n\n<p><strong>Example of time column values from (1 to 12) :</strong>    </p>\n\n<pre><code>1000.0 -&gt; 10\n901.0 -&gt;  9\n</code></pre>\n\n<p><strong>Example of time column values from (13 to 24) :</strong>    </p>\n\n<pre><code>1850.0 -&gt; 18\n2301.0 -&gt; 23\n</code></pre>\n\n<p>I have tried this code but it takes very long time until I close the editor so I didn't see the result</p>\n\n<pre><code>for index,row in df.iterrows():\n    if(row['time']&lt;=959.0):\n        row['hour']= int(str(row['dep_time'])[:1])\n    elif row['dep_time']&gt;959.0: \n         row['dep_hour']=int(str(row['dep_time'])[:2])\n</code></pre>\n",
        "answer_body": "<p>With Pandas, don't iterate rows when vectorised methods are available. In this case, you can use floor division followed by <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html\" rel=\"nofollow noreferrer\"><code>pd.Series.astype</code></a>:</p>\n\n<pre><code>df['hour'] = (df['dep_time'] // 100).astype(int)\n</code></pre>\n",
        "question_body": "<p>I have a Pandas dataframe. In a series, I have time in hour and minute represented as <code>float</code>, as below. I want only the hours:</p>\n\n<p><strong>Example of time column values from (1 to 12) :</strong>    </p>\n\n<pre><code>1000.0 -&gt; 10\n901.0 -&gt;  9\n</code></pre>\n\n<p><strong>Example of time column values from (13 to 24) :</strong>    </p>\n\n<pre><code>1850.0 -&gt; 18\n2301.0 -&gt; 23\n</code></pre>\n\n<p>I have tried this code but it takes very long time until I close the editor so I didn't see the result</p>\n\n<pre><code>for index,row in df.iterrows():\n    if(row['time']&lt;=959.0):\n        row['hour']= int(str(row['dep_time'])[:1])\n    elif row['dep_time']&gt;959.0: \n         row['dep_hour']=int(str(row['dep_time'])[:2])\n</code></pre>\n",
        "formatted_input": {
            "qid": 52745166,
            "link": "https://stackoverflow.com/questions/52745166/how-to-take-first-one-or-two-digits-from-float-number",
            "question": {
                "title": "How to take first one or two digits from float number?",
                "ques_desc": "I have a Pandas dataframe. In a series, I have time in hour and minute represented as , as below. I want only the hours: Example of time column values from (1 to 12) : Example of time column values from (13 to 24) : I have tried this code but it takes very long time until I close the editor so I didn't see the result "
            },
            "io": [
                "1000.0 -> 10\n901.0 ->  9\n",
                "1850.0 -> 18\n2301.0 -> 23\n"
            ],
            "answer": {
                "ans_desc": "With Pandas, don't iterate rows when vectorised methods are available. In this case, you can use floor division followed by : ",
                "code": [
                    "df['hour'] = (df['dep_time'] // 100).astype(int)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "string",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 528,
            "user_id": 8919443,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/512b0b6982152bef9c7103e34f8b4126?s=128&d=identicon&r=PG&f=1",
            "display_name": "LamaMo",
            "link": "https://stackoverflow.com/users/8919443/lamamo"
        },
        "is_answered": true,
        "view_count": 2462,
        "accepted_answer_id": 52705698,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1539031104,
        "creation_date": 1539012042,
        "last_edit_date": 1539031104,
        "question_id": 52705423,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52705423/match-a-value-in-the-column-and-return-another-column-in-pandas-python",
        "title": "Match a value in the column and return another column in pandas | python",
        "body": "<p>I have an <code>input file1</code> of two columns(tab-separted):</p>\n\n<pre><code>c1\\tc2\naaa\\t232 65 19 32\nbbew\\t32 22 20\njhsi\\t986 1 32 463 221\n</code></pre>\n\n<p>And <code>input file2</code> which have one column:</p>\n\n<pre><code>c1\n19\n1\n32\n277\n</code></pre>\n\n<p>what I want is to search an element from <code>file2</code> in <code>file1</code>, and return the corresponding value in <code>c1</code>. If there is more than one matched value, then return all together in one column.</p>\n\n<p>Here is what should the output file be like:</p>\n\n<pre><code>19       aaa\n1        jhsi\n32       aaa bbew jhsi\n277      \n</code></pre>\n\n<p><code>277</code> would be left empty because it does not exist.</p>\n\n<p>Any suggestion will be helpful. </p>\n",
        "answer_body": "<p>This isn't easily vectorisable. For performance, I suggest you perform your transformation before you put data in a Pandas dataframe. Here is a solution using <a href=\"https://docs.python.org/3/library/collections.html#collections.defaultdict\" rel=\"nofollow noreferrer\"><code>collections.defaultdict</code></a>:</p>\n\n<pre><code># use set for O(1) lookup\nscope_set = set(df2['c1'])\n\n# initialise defualtdict of lists\ndd = defaultdict(list)\n\n# iterate and create dictionary mapping numbers to keys\nfor row in df1.itertuples(index=False):\n    for num in map(int, row.c2.split()):\n        if num in scope_set:\n            dd[num].append(row.c1)\n\n# construct dataframe from defaultdict\ndf = pd.DataFrame({'num': list(dd), 'keys': list(map(' '.join, dd.values()))})\n\n# reindex to include blanks\ndf = df.set_index('num').reindex(sorted(scope_set)).reset_index()\n</code></pre>\n\n<p><strong>Result</strong></p>\n\n<pre><code>print(df)\n\n   num           keys\n0    1           jhsi\n1   19            aaa\n2   32  aaa bbew jhsi\n3  277            NaN\n</code></pre>\n\n<p><strong>Setup</strong></p>\n\n<pre><code>from io import StringIO\nfrom collections import defaultdict\n\nfile1 = StringIO(\"\"\"c1\\tc2\naaa\\t232 65 19 32\nbbew\\t32 22 20\njhsi\\t986 1 32 463 221\"\"\")\n\nfile2 = StringIO(\"\"\"c1\n19\n1\n32\n277\"\"\")\n\ndf1 = pd.read_csv(file1, sep='\\t')\ndf2 = pd.read_csv(file2)\n</code></pre>\n",
        "question_body": "<p>I have an <code>input file1</code> of two columns(tab-separted):</p>\n\n<pre><code>c1\\tc2\naaa\\t232 65 19 32\nbbew\\t32 22 20\njhsi\\t986 1 32 463 221\n</code></pre>\n\n<p>And <code>input file2</code> which have one column:</p>\n\n<pre><code>c1\n19\n1\n32\n277\n</code></pre>\n\n<p>what I want is to search an element from <code>file2</code> in <code>file1</code>, and return the corresponding value in <code>c1</code>. If there is more than one matched value, then return all together in one column.</p>\n\n<p>Here is what should the output file be like:</p>\n\n<pre><code>19       aaa\n1        jhsi\n32       aaa bbew jhsi\n277      \n</code></pre>\n\n<p><code>277</code> would be left empty because it does not exist.</p>\n\n<p>Any suggestion will be helpful. </p>\n",
        "formatted_input": {
            "qid": 52705423,
            "link": "https://stackoverflow.com/questions/52705423/match-a-value-in-the-column-and-return-another-column-in-pandas-python",
            "question": {
                "title": "Match a value in the column and return another column in pandas | python",
                "ques_desc": "I have an of two columns(tab-separted): And which have one column: what I want is to search an element from in , and return the corresponding value in . If there is more than one matched value, then return all together in one column. Here is what should the output file be like: would be left empty because it does not exist. Any suggestion will be helpful. "
            },
            "io": [
                "c1\\tc2\naaa\\t232 65 19 32\nbbew\\t32 22 20\njhsi\\t986 1 32 463 221\n",
                "19       aaa\n1        jhsi\n32       aaa bbew jhsi\n277      \n"
            ],
            "answer": {
                "ans_desc": "This isn't easily vectorisable. For performance, I suggest you perform your transformation before you put data in a Pandas dataframe. Here is a solution using : Result Setup ",
                "code": [
                    "# use set for O(1) lookup\nscope_set = set(df2['c1'])\n\n# initialise defualtdict of lists\ndd = defaultdict(list)\n\n# iterate and create dictionary mapping numbers to keys\nfor row in df1.itertuples(index=False):\n    for num in map(int, row.c2.split()):\n        if num in scope_set:\n            dd[num].append(row.c1)\n\n# construct dataframe from defaultdict\ndf = pd.DataFrame({'num': list(dd), 'keys': list(map(' '.join, dd.values()))})\n\n# reindex to include blanks\ndf = df.set_index('num').reindex(sorted(scope_set)).reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 111,
            "user_id": 9674523,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/lhEja.jpg?s=128&g=1",
            "display_name": "aadcg",
            "link": "https://stackoverflow.com/users/9674523/aadcg"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 52591337,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1538397490,
        "creation_date": 1538397386,
        "question_id": 52591303,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52591303/pandas-dataframe-filter",
        "title": "Pandas DataFrame filter",
        "body": "<p>My question is about the pandas.DataFrame.filter command. It seems that pandas creates a copy of the data frame to write any changes. How am I able to write on the data frame itself?</p>\n\n<p>In other words:</p>\n\n<pre><code>d = {'col1': [1, 2], 'col2': [3, 4]}\ndf = pd.DataFrame(data=d)\ndf.filter(regex='col1').iloc[0]=10\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   col1  col2\n0     1     3\n1     2     4\n</code></pre>\n\n<p>Desired Output:</p>\n\n<pre><code>   col1  col2\n0    10     3\n1     2     4\n</code></pre>\n",
        "answer_body": "<p>I think you need extract columns names and then use <code>loc</code> or <code>iloc</code> functions:</p>\n\n<pre><code>cols = df.filter(regex='col1').columns \ndf.loc[0, cols]=10\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df.iloc[0, df.columns.get_indexer(cols)] = 10\n\nprint (df)\n   col1  col2\n0    10     3\n1     2     4\n</code></pre>\n\n<p>You cannnot use <code>filter</code> function, because subset returns a Series/DataFrame which may have its data as a view. That's why <code>SettingWithCopyWarning</code> is possible there (or raise if you set the option).</p>\n",
        "question_body": "<p>My question is about the pandas.DataFrame.filter command. It seems that pandas creates a copy of the data frame to write any changes. How am I able to write on the data frame itself?</p>\n\n<p>In other words:</p>\n\n<pre><code>d = {'col1': [1, 2], 'col2': [3, 4]}\ndf = pd.DataFrame(data=d)\ndf.filter(regex='col1').iloc[0]=10\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   col1  col2\n0     1     3\n1     2     4\n</code></pre>\n\n<p>Desired Output:</p>\n\n<pre><code>   col1  col2\n0    10     3\n1     2     4\n</code></pre>\n",
        "formatted_input": {
            "qid": 52591303,
            "link": "https://stackoverflow.com/questions/52591303/pandas-dataframe-filter",
            "question": {
                "title": "Pandas DataFrame filter",
                "ques_desc": "My question is about the pandas.DataFrame.filter command. It seems that pandas creates a copy of the data frame to write any changes. How am I able to write on the data frame itself? In other words: Output: Desired Output: "
            },
            "io": [
                "   col1  col2\n0     1     3\n1     2     4\n",
                "   col1  col2\n0    10     3\n1     2     4\n"
            ],
            "answer": {
                "ans_desc": "I think you need extract columns names and then use or functions: Or: You cannnot use function, because subset returns a Series/DataFrame which may have its data as a view. That's why is possible there (or raise if you set the option). ",
                "code": [
                    "cols = df.filter(regex='col1').columns \ndf.loc[0, cols]=10\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "apache-spark",
            "dataframe"
        ],
        "owner": {
            "reputation": 133,
            "user_id": 8684363,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/fa9768e47e53391b53e9b846a1bc7c84?s=128&d=identicon&r=PG&f=1",
            "display_name": "kkumar",
            "link": "https://stackoverflow.com/users/8684363/kkumar"
        },
        "is_answered": true,
        "view_count": 4589,
        "accepted_answer_id": 52557525,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1538146841,
        "creation_date": 1538083943,
        "last_edit_date": 1538144840,
        "question_id": 52545438,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52545438/convert-a-dense-vector-to-a-dataframe-using-pyspark",
        "title": "Convert a Dense Vector to a Dataframe using Pyspark",
        "body": "<p>Firstly I tried everything in the link below to fix my error but none of them worked.</p>\n\n<p><a href=\"https://stackoverflow.com/questions/41328799/how-to-convert-rdd-of-dense-vector-into-dataframe-in-pyspark/41330850\">How to convert RDD of dense vector into DataFrame in pyspark?</a></p>\n\n<p>I am trying to convert a dense vector into a dataframe (Spark preferably) along with column names and running into issues.</p>\n\n<p>My column in spark dataframe is a vector that was created using Vector Assembler and I now want to convert it back to a dataframe as I would like to create plots on some of the variables in the vector. </p>\n\n<p>Approach 1: </p>\n\n<pre><code>from pyspark.ml.linalg import SparseVector, DenseVector\nfrom pyspark.ml.linalg import Vectors\n\ntemp=output.select(\"all_features\")\ntemp.rdd.map(\n    lambda row: (DenseVector(row[0].toArray()))\n).toDF()\n</code></pre>\n\n<p>Below is the Error</p>\n\n<pre><code>TypeError: not supported type: &lt;type 'numpy.ndarray'&gt;\n</code></pre>\n\n<p>Approach 2:</p>\n\n<pre><code>from pyspark.ml.linalg import VectorUDT\nfrom pyspark.sql.functions import udf\nfrom pyspark.ml.linalg import *\n\nas_ml = udf(lambda v: v.asML() if v is not None else None, VectorUDT())\nresult = output.withColumn(\"all_features\", as_ml(\"all_features\"))\nresult.head(5)\n</code></pre>\n\n<p>Error:</p>\n\n<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'asML'\n</code></pre>\n\n<p>I also tried to convert the dataframe into a Pandas dataframe and after that I am not able to split the values into separate columns</p>\n\n<p>Approach 3:</p>\n\n<pre><code>pandas_df=temp.toPandas()\npandas_df1=pd.DataFrame(pandas_df.all_features.values.tolist())\n</code></pre>\n\n<p>Above code runs fine but I still have only one column in my dataframe with all the values separated by commas as a list.</p>\n\n<p>Any help is greatly appreciated!</p>\n\n<p>EDIT:</p>\n\n<p>Here is how my temp dataframe looks like. It just has one column all_features. I am trying to create a dataframe that splits all of these values into separate columns (all_features is a vector that was created using 200 columns)</p>\n\n<pre><code>+--------------------+\n|        all_features|\n+--------------------+\n|[0.01193689934723...|\n|[0.04774759738895...|\n|[0.0,0.0,0.194417...|\n|[0.02387379869447...|\n|[1.89796699621085...|\n+--------------------+\nonly showing top 5 rows\n</code></pre>\n\n<p>Expected output is a dataframe with all 200 columns separated out in a dataframe</p>\n\n<pre><code>+----------------------------+\n|        col1| col2| col3|...\n+----------------------------+\n|0.01193689934723|0.0|0.5049431301173817...\n|0.04774759738895|0.0|0.1657316216149636...\n|0.0|0.0|7.213126372469...\n|0.02387379869447|0.0|0.1866693496827619|...\n|1.89796699621085|0.0|0.3192169213385746|...\n+----------------------------+\nonly showing top 5 rows\n</code></pre>\n\n<p>Here is how my Pandas DF output looks like</p>\n\n<pre><code>              0\n0   [0.011936899347238104, 0.0, 0.5049431301173817...\n1   [0.047747597388952415, 0.0, 0.1657316216149636...\n2   [0.0, 0.0, 0.19441761495525278, 7.213126372469...\n3   [0.023873798694476207, 0.0, 0.1866693496827619...\n4   [1.8979669962108585, 0.0, 0.3192169213385746, ...\n</code></pre>\n",
        "answer_body": "<p>Since you want all the features in separate columns (as I got from your EDIT), the link to the answer you provided is not your solution.</p>\n\n<p>Try this,</p>\n\n<pre><code>#column_names\ntemp = temp.rdd.map(lambda x:[float(y) for y in x['all_features']]).toDF(column_names)\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>Since your <code>temp</code> is originally a dataframe, you can also use this method without converting it to <code>rdd</code>,\n</p>\n\n<pre><code>import pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\nsplits = [F.udf(lambda val: float(val[i].item()),FloatType()) for i in range(200)]\ntemp = temp.select(*[s(F.col('all_features')).alias(c) for c,s in zip(column_names,splits)])\ntemp.show()\n</code></pre>\n",
        "question_body": "<p>Firstly I tried everything in the link below to fix my error but none of them worked.</p>\n\n<p><a href=\"https://stackoverflow.com/questions/41328799/how-to-convert-rdd-of-dense-vector-into-dataframe-in-pyspark/41330850\">How to convert RDD of dense vector into DataFrame in pyspark?</a></p>\n\n<p>I am trying to convert a dense vector into a dataframe (Spark preferably) along with column names and running into issues.</p>\n\n<p>My column in spark dataframe is a vector that was created using Vector Assembler and I now want to convert it back to a dataframe as I would like to create plots on some of the variables in the vector. </p>\n\n<p>Approach 1: </p>\n\n<pre><code>from pyspark.ml.linalg import SparseVector, DenseVector\nfrom pyspark.ml.linalg import Vectors\n\ntemp=output.select(\"all_features\")\ntemp.rdd.map(\n    lambda row: (DenseVector(row[0].toArray()))\n).toDF()\n</code></pre>\n\n<p>Below is the Error</p>\n\n<pre><code>TypeError: not supported type: &lt;type 'numpy.ndarray'&gt;\n</code></pre>\n\n<p>Approach 2:</p>\n\n<pre><code>from pyspark.ml.linalg import VectorUDT\nfrom pyspark.sql.functions import udf\nfrom pyspark.ml.linalg import *\n\nas_ml = udf(lambda v: v.asML() if v is not None else None, VectorUDT())\nresult = output.withColumn(\"all_features\", as_ml(\"all_features\"))\nresult.head(5)\n</code></pre>\n\n<p>Error:</p>\n\n<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'asML'\n</code></pre>\n\n<p>I also tried to convert the dataframe into a Pandas dataframe and after that I am not able to split the values into separate columns</p>\n\n<p>Approach 3:</p>\n\n<pre><code>pandas_df=temp.toPandas()\npandas_df1=pd.DataFrame(pandas_df.all_features.values.tolist())\n</code></pre>\n\n<p>Above code runs fine but I still have only one column in my dataframe with all the values separated by commas as a list.</p>\n\n<p>Any help is greatly appreciated!</p>\n\n<p>EDIT:</p>\n\n<p>Here is how my temp dataframe looks like. It just has one column all_features. I am trying to create a dataframe that splits all of these values into separate columns (all_features is a vector that was created using 200 columns)</p>\n\n<pre><code>+--------------------+\n|        all_features|\n+--------------------+\n|[0.01193689934723...|\n|[0.04774759738895...|\n|[0.0,0.0,0.194417...|\n|[0.02387379869447...|\n|[1.89796699621085...|\n+--------------------+\nonly showing top 5 rows\n</code></pre>\n\n<p>Expected output is a dataframe with all 200 columns separated out in a dataframe</p>\n\n<pre><code>+----------------------------+\n|        col1| col2| col3|...\n+----------------------------+\n|0.01193689934723|0.0|0.5049431301173817...\n|0.04774759738895|0.0|0.1657316216149636...\n|0.0|0.0|7.213126372469...\n|0.02387379869447|0.0|0.1866693496827619|...\n|1.89796699621085|0.0|0.3192169213385746|...\n+----------------------------+\nonly showing top 5 rows\n</code></pre>\n\n<p>Here is how my Pandas DF output looks like</p>\n\n<pre><code>              0\n0   [0.011936899347238104, 0.0, 0.5049431301173817...\n1   [0.047747597388952415, 0.0, 0.1657316216149636...\n2   [0.0, 0.0, 0.19441761495525278, 7.213126372469...\n3   [0.023873798694476207, 0.0, 0.1866693496827619...\n4   [1.8979669962108585, 0.0, 0.3192169213385746, ...\n</code></pre>\n",
        "formatted_input": {
            "qid": 52545438,
            "link": "https://stackoverflow.com/questions/52545438/convert-a-dense-vector-to-a-dataframe-using-pyspark",
            "question": {
                "title": "Convert a Dense Vector to a Dataframe using Pyspark",
                "ques_desc": "Firstly I tried everything in the link below to fix my error but none of them worked. How to convert RDD of dense vector into DataFrame in pyspark? I am trying to convert a dense vector into a dataframe (Spark preferably) along with column names and running into issues. My column in spark dataframe is a vector that was created using Vector Assembler and I now want to convert it back to a dataframe as I would like to create plots on some of the variables in the vector. Approach 1: Below is the Error Approach 2: Error: I also tried to convert the dataframe into a Pandas dataframe and after that I am not able to split the values into separate columns Approach 3: Above code runs fine but I still have only one column in my dataframe with all the values separated by commas as a list. Any help is greatly appreciated! EDIT: Here is how my temp dataframe looks like. It just has one column all_features. I am trying to create a dataframe that splits all of these values into separate columns (all_features is a vector that was created using 200 columns) Expected output is a dataframe with all 200 columns separated out in a dataframe Here is how my Pandas DF output looks like "
            },
            "io": [
                "+----------------------------+\n|        col1| col2| col3|...\n+----------------------------+\n|0.01193689934723|0.0|0.5049431301173817...\n|0.04774759738895|0.0|0.1657316216149636...\n|0.0|0.0|7.213126372469...\n|0.02387379869447|0.0|0.1866693496827619|...\n|1.89796699621085|0.0|0.3192169213385746|...\n+----------------------------+\nonly showing top 5 rows\n",
                "              0\n0   [0.011936899347238104, 0.0, 0.5049431301173817...\n1   [0.047747597388952415, 0.0, 0.1657316216149636...\n2   [0.0, 0.0, 0.19441761495525278, 7.213126372469...\n3   [0.023873798694476207, 0.0, 0.1866693496827619...\n4   [1.8979669962108585, 0.0, 0.3192169213385746, ...\n"
            ],
            "answer": {
                "ans_desc": "Since you want all the features in separate columns (as I got from your EDIT), the link to the answer you provided is not your solution. Try this, EDIT: Since your is originally a dataframe, you can also use this method without converting it to , ",
                "code": [
                    "#column_names\ntemp = temp.rdd.map(lambda x:[float(y) for y in x['all_features']]).toDF(column_names)\n",
                    "import pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\nsplits = [F.udf(lambda val: float(val[i].item()),FloatType()) for i in range(200)]\ntemp = temp.select(*[s(F.col('all_features')).alias(c) for c,s in zip(column_names,splits)])\ntemp.show()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 342,
            "user_id": 9793651,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ec0c98d11272927a5c54f4de47556123?s=128&d=identicon&r=PG",
            "display_name": "Can H. Tartanoglu",
            "link": "https://stackoverflow.com/users/9793651/can-h-tartanoglu"
        },
        "is_answered": true,
        "view_count": 1141,
        "accepted_answer_id": 52506978,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1537911135,
        "creation_date": 1537909596,
        "question_id": 52506702,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52506702/how-to-select-and-replace-specific-values-with-nan-in-pandas-dataframe-how-to",
        "title": "How to select, and replace specific values with NaN in pandas dataframe. How to remove a column from each level 1 multiindex",
        "body": "<p>I have a csv file, which I read into a pandas frame:</p>\n\n<pre><code>import pandas as pd\n\n\ncsv_file = pd.read_csv('hello.csv', engine='c', delimiter=',', index_col=0,\n                       skiprows=1, header=[0, 1])\n</code></pre>\n\n<p>This is the view of the csv file (print(csv_file)):</p>\n\n<pre><code>bodyparts        nose                  ...        right_ear              \ncoords              x           y      ...                y    likelihood\n0          197.486369    4.545954      ...       206.351233  1.280000e-06\n1          319.946460  191.035224      ...       206.321893  9.680000e-07\n2          319.880388  191.012984      ...       206.322207  9.520000e-07\n3          320.286005  190.843329      ...       206.227396  1.020000e-06\n4          320.210989  190.863304      ...         3.106570  8.350000e-07\n5          320.212529  190.867178      ...         3.116692  8.460000e-07\n6           -0.794705    2.462400      ...         3.112797  8.500000e-07\n7           -0.785404    2.485562      ...         3.117945  8.430000e-07\n8          319.786777  191.003882      ...         3.125062  8.820000e-07\n9          319.947064  191.030201      ...       206.202980  9.210000e-07\n10         319.845807  191.002510      ...       206.177779  8.660000e-07\n11         320.135816  190.967408      ...       206.190732  8.910000e-07\n12          -0.935765    2.568168      ...       206.260773  8.860000e-07\n13          -0.932833    2.525062      ...       206.273504  8.780000e-07\n14          -0.960939    2.500079      ...       206.272811  8.680000e-07\n15          -0.832561    2.442907      ...       206.266416  8.720000e-07\n16          -0.838884    2.421689      ...       206.242941  9.440000e-07\n17          -0.857173    2.421467      ...       206.243972  9.950000e-07\n18          -0.841627    2.414854      ...       206.225004  9.820000e-07\n...               ...         ...      ...              ...           ...\n10459      349.556703  301.995042      ...       307.018688  9.999745e-01\n10460      348.608277  301.098244      ...       309.648986  9.999962e-01\n10461      349.995217  303.397438      ...       311.149967  9.999974e-01\n10462      349.109666  305.710711      ...       311.893106  9.999955e-01\n10463      352.142571  310.081763      ...       317.420410  9.907742e-01\n10464      351.916488  317.078128      ...       319.407211  2.706501e-01\n10465      353.809847  320.086683      ...       323.478481  9.911720e-01\n10466      349.233529  321.859424      ...       323.383276  8.724346e-01\n</code></pre>\n\n<p>The resulting dataframe is MultiIndexed with two levels:</p>\n\n<pre><code>tuple(('body_part1', 'body_part2', ..., 'body_partn'), ('x', 'y', 'likelihood')\n</code></pre>\n\n<p>print(df.column()):</p>\n\n<pre><code>MultiIndex(levels=[['left_ear', 'nose', 'right_ear', 'tail'], ['likelihood', 'x', 'y']],\n           labels=[[1, 1, 1, 3, 3, 3, 0, 0, 0, 2, 2, 2], [1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0]],\n           names=['bodyparts', 'coords'])\n</code></pre>\n\n<p>If a coordinate has a lower likelihood, I wan't the coordinates to be replaced by NaN. The new dataframe shan't have a likelihood column(s). An example with the first row from 'nose':</p>\n\n<pre><code>coords           x           y    likelihood\n0       197.486369    4.545954  3.890000e-07\n</code></pre>\n\n<p>After function should be like this:</p>\n\n<pre><code>coords           x           y\n0              NaN         NaN\n</code></pre>\n\n<p>Note that outstanding values remain unchanged during this process!</p>\n",
        "answer_body": "<p>Assuming you have a threshold for defining \"lower\" likelihood:</p>\n\n<pre><code>for col in df.columns.levels[0]:\n    df.loc[df[(col, 'likelihood')] &lt; threshold, [(col, 'x'), (col, 'y')]] = np.nan\n</code></pre>\n\n<p>I also think there might be a more optimal way to do this (without looping through the columns) but this should work too.</p>\n",
        "question_body": "<p>I have a csv file, which I read into a pandas frame:</p>\n\n<pre><code>import pandas as pd\n\n\ncsv_file = pd.read_csv('hello.csv', engine='c', delimiter=',', index_col=0,\n                       skiprows=1, header=[0, 1])\n</code></pre>\n\n<p>This is the view of the csv file (print(csv_file)):</p>\n\n<pre><code>bodyparts        nose                  ...        right_ear              \ncoords              x           y      ...                y    likelihood\n0          197.486369    4.545954      ...       206.351233  1.280000e-06\n1          319.946460  191.035224      ...       206.321893  9.680000e-07\n2          319.880388  191.012984      ...       206.322207  9.520000e-07\n3          320.286005  190.843329      ...       206.227396  1.020000e-06\n4          320.210989  190.863304      ...         3.106570  8.350000e-07\n5          320.212529  190.867178      ...         3.116692  8.460000e-07\n6           -0.794705    2.462400      ...         3.112797  8.500000e-07\n7           -0.785404    2.485562      ...         3.117945  8.430000e-07\n8          319.786777  191.003882      ...         3.125062  8.820000e-07\n9          319.947064  191.030201      ...       206.202980  9.210000e-07\n10         319.845807  191.002510      ...       206.177779  8.660000e-07\n11         320.135816  190.967408      ...       206.190732  8.910000e-07\n12          -0.935765    2.568168      ...       206.260773  8.860000e-07\n13          -0.932833    2.525062      ...       206.273504  8.780000e-07\n14          -0.960939    2.500079      ...       206.272811  8.680000e-07\n15          -0.832561    2.442907      ...       206.266416  8.720000e-07\n16          -0.838884    2.421689      ...       206.242941  9.440000e-07\n17          -0.857173    2.421467      ...       206.243972  9.950000e-07\n18          -0.841627    2.414854      ...       206.225004  9.820000e-07\n...               ...         ...      ...              ...           ...\n10459      349.556703  301.995042      ...       307.018688  9.999745e-01\n10460      348.608277  301.098244      ...       309.648986  9.999962e-01\n10461      349.995217  303.397438      ...       311.149967  9.999974e-01\n10462      349.109666  305.710711      ...       311.893106  9.999955e-01\n10463      352.142571  310.081763      ...       317.420410  9.907742e-01\n10464      351.916488  317.078128      ...       319.407211  2.706501e-01\n10465      353.809847  320.086683      ...       323.478481  9.911720e-01\n10466      349.233529  321.859424      ...       323.383276  8.724346e-01\n</code></pre>\n\n<p>The resulting dataframe is MultiIndexed with two levels:</p>\n\n<pre><code>tuple(('body_part1', 'body_part2', ..., 'body_partn'), ('x', 'y', 'likelihood')\n</code></pre>\n\n<p>print(df.column()):</p>\n\n<pre><code>MultiIndex(levels=[['left_ear', 'nose', 'right_ear', 'tail'], ['likelihood', 'x', 'y']],\n           labels=[[1, 1, 1, 3, 3, 3, 0, 0, 0, 2, 2, 2], [1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0]],\n           names=['bodyparts', 'coords'])\n</code></pre>\n\n<p>If a coordinate has a lower likelihood, I wan't the coordinates to be replaced by NaN. The new dataframe shan't have a likelihood column(s). An example with the first row from 'nose':</p>\n\n<pre><code>coords           x           y    likelihood\n0       197.486369    4.545954  3.890000e-07\n</code></pre>\n\n<p>After function should be like this:</p>\n\n<pre><code>coords           x           y\n0              NaN         NaN\n</code></pre>\n\n<p>Note that outstanding values remain unchanged during this process!</p>\n",
        "formatted_input": {
            "qid": 52506702,
            "link": "https://stackoverflow.com/questions/52506702/how-to-select-and-replace-specific-values-with-nan-in-pandas-dataframe-how-to",
            "question": {
                "title": "How to select, and replace specific values with NaN in pandas dataframe. How to remove a column from each level 1 multiindex",
                "ques_desc": "I have a csv file, which I read into a pandas frame: This is the view of the csv file (print(csv_file)): The resulting dataframe is MultiIndexed with two levels: print(df.column()): If a coordinate has a lower likelihood, I wan't the coordinates to be replaced by NaN. The new dataframe shan't have a likelihood column(s). An example with the first row from 'nose': After function should be like this: Note that outstanding values remain unchanged during this process! "
            },
            "io": [
                "coords           x           y    likelihood\n0       197.486369    4.545954  3.890000e-07\n",
                "coords           x           y\n0              NaN         NaN\n"
            ],
            "answer": {
                "ans_desc": "Assuming you have a threshold for defining \"lower\" likelihood: I also think there might be a more optimal way to do this (without looping through the columns) but this should work too. ",
                "code": [
                    "for col in df.columns.levels[0]:\n    df.loc[df[(col, 'likelihood')] < threshold, [(col, 'x'), (col, 'y')]] = np.nan\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 203,
            "user_id": 4314229,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XTcBA6Z6L7s/AAAAAAAAAAI/AAAAAAAAA_c/O9TVeUPPO6k/photo.jpg?sz=128",
            "display_name": "Yen",
            "link": "https://stackoverflow.com/users/4314229/yen"
        },
        "is_answered": true,
        "view_count": 877,
        "accepted_answer_id": 52502214,
        "answer_count": 2,
        "score": 13,
        "last_activity_date": 1537893025,
        "creation_date": 1537890547,
        "last_edit_date": 1537892178,
        "question_id": 52502179,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52502179/split-pandas-dataframe-into-multiple-dataframes-based-on-null-columns",
        "title": "Split pandas dataframe into multiple dataframes based on null columns",
        "body": "<p>I have a pandas dataframe as follows:</p>\n\n<pre><code>        a       b       c\n    0   1.0     NaN     NaN\n    1   NaN     7.0     5.0\n    2   3.0     8.0     3.0\n    3   4.0     9.0     2.0\n    4   5.0     0.0     NaN\n</code></pre>\n\n<p>Is there a simple way to split the dataframe into multiple dataframes based on non-null values?</p>\n\n<pre><code>        a   \n    0   1.0     \n\n         b      c\n    1    7.0    5.0\n\n        a       b       c\n    2   3.0     8.0     3.0\n    3   4.0     9.0     2.0\n\n        a       b      \n    4   5.0     0.0\n</code></pre>\n",
        "answer_body": "<p>Using <code>groupby</code> with <code>dropna</code> </p>\n\n<pre><code>for _, x in df.groupby(df.isnull().dot(df.columns)):\n      print(x.dropna(1))\n\n     a    b    c\n2  3.0  8.0  3.0\n3  4.0  9.0  2.0\n     b    c\n1  7.0  5.0\n     a\n0  1.0\n     a    b\n4  5.0  0.0\n</code></pre>\n\n<p>We can save them in dict </p>\n\n<pre><code>d = {y : x.dropna(1) for y, x in df.groupby(df.isnull().dot(df.columns))}\n</code></pre>\n\n<p>More Info using the <code>dot</code> to get the null column , if they are same we should combine them together </p>\n\n<pre><code>df.isnull().dot(df.columns)\nOut[1250]: \n0    bc\n1     a\n2      \n3      \n4     c\ndtype: object\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as follows:</p>\n\n<pre><code>        a       b       c\n    0   1.0     NaN     NaN\n    1   NaN     7.0     5.0\n    2   3.0     8.0     3.0\n    3   4.0     9.0     2.0\n    4   5.0     0.0     NaN\n</code></pre>\n\n<p>Is there a simple way to split the dataframe into multiple dataframes based on non-null values?</p>\n\n<pre><code>        a   \n    0   1.0     \n\n         b      c\n    1    7.0    5.0\n\n        a       b       c\n    2   3.0     8.0     3.0\n    3   4.0     9.0     2.0\n\n        a       b      \n    4   5.0     0.0\n</code></pre>\n",
        "formatted_input": {
            "qid": 52502179,
            "link": "https://stackoverflow.com/questions/52502179/split-pandas-dataframe-into-multiple-dataframes-based-on-null-columns",
            "question": {
                "title": "Split pandas dataframe into multiple dataframes based on null columns",
                "ques_desc": "I have a pandas dataframe as follows: Is there a simple way to split the dataframe into multiple dataframes based on non-null values? "
            },
            "io": [
                "        a       b       c\n    0   1.0     NaN     NaN\n    1   NaN     7.0     5.0\n    2   3.0     8.0     3.0\n    3   4.0     9.0     2.0\n    4   5.0     0.0     NaN\n",
                "        a   \n    0   1.0     \n\n         b      c\n    1    7.0    5.0\n\n        a       b       c\n    2   3.0     8.0     3.0\n    3   4.0     9.0     2.0\n\n        a       b      \n    4   5.0     0.0\n"
            ],
            "answer": {
                "ans_desc": "Using with We can save them in dict More Info using the to get the null column , if they are same we should combine them together ",
                "code": [
                    "d = {y : x.dropna(1) for y, x in df.groupby(df.isnull().dot(df.columns))}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "function",
            "dataframe"
        ],
        "owner": {
            "reputation": 331,
            "user_id": 9601725,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4b4cb37091a75c39f5ff958c7e812b8f?s=128&d=identicon&r=PG&f=1",
            "display_name": "NZ_DJ",
            "link": "https://stackoverflow.com/users/9601725/nz-dj"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 52493373,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1537863614,
        "creation_date": 1537861089,
        "last_edit_date": 1537861434,
        "question_id": 52492961,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52492961/compare-each-of-the-column-values-and-return-final-value-based-on-conditions",
        "title": "Compare each of the column values and return final value based on conditions",
        "body": "<p>I currently have a dataframe which looks like this:</p>\n\n<pre><code>col1  col2  col3\n 1      2     3\n 2      3     NaN\n 3      4     NaN\n 2      NaN   NaN\n 0      2     NaN\n</code></pre>\n\n<p>What I want to do is apply some condition to the column values and return the final result in a new column.</p>\n\n<p>The condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4]</p>\n\n<p>I tried to define a function to append the final results but wasnt really getting anywhere...any thoughts?</p>\n\n<p>The desired outcome would look something like:</p>\n\n<pre><code>col1  col2  col3  col4\n 1     2     3     2\n 2     3     NaN   2\n 3     4    NaN    3\n 2     NaN   NaN   2\n 0     2    NaN    2\n</code></pre>\n\n<p>where col4 is the new column created.</p>\n\n<p>Thanks</p>\n",
        "answer_body": "<p>first you may want to get ride of the NaNs:</p>\n\n<pre><code>df.fillna(5)\n</code></pre>\n\n<p>and then apply a function to every row to find your value:</p>\n\n<pre><code>def func(x,l=[2,1,3,0,4,5]):\n    for j in l:\n      if(j in x):\n         return j\n\ndf['new'] = df.apply(lambda x: func(list(x)),axis =1)\n</code></pre>\n\n<p><strong>Output</strong>:</p>\n\n<pre><code>     col1   col2    col3    new\n  0   1      2        3      2    \n  1   2      3        5      2\n  2   3      4        5      3\n  3   2      5        5      2\n  4   0      2        5      2\n</code></pre>\n",
        "question_body": "<p>I currently have a dataframe which looks like this:</p>\n\n<pre><code>col1  col2  col3\n 1      2     3\n 2      3     NaN\n 3      4     NaN\n 2      NaN   NaN\n 0      2     NaN\n</code></pre>\n\n<p>What I want to do is apply some condition to the column values and return the final result in a new column.</p>\n\n<p>The condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4]</p>\n\n<p>I tried to define a function to append the final results but wasnt really getting anywhere...any thoughts?</p>\n\n<p>The desired outcome would look something like:</p>\n\n<pre><code>col1  col2  col3  col4\n 1     2     3     2\n 2     3     NaN   2\n 3     4    NaN    3\n 2     NaN   NaN   2\n 0     2    NaN    2\n</code></pre>\n\n<p>where col4 is the new column created.</p>\n\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 52492961,
            "link": "https://stackoverflow.com/questions/52492961/compare-each-of-the-column-values-and-return-final-value-based-on-conditions",
            "question": {
                "title": "Compare each of the column values and return final value based on conditions",
                "ques_desc": "I currently have a dataframe which looks like this: What I want to do is apply some condition to the column values and return the final result in a new column. The condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4] I tried to define a function to append the final results but wasnt really getting anywhere...any thoughts? The desired outcome would look something like: where col4 is the new column created. Thanks "
            },
            "io": [
                "col1  col2  col3\n 1      2     3\n 2      3     NaN\n 3      4     NaN\n 2      NaN   NaN\n 0      2     NaN\n",
                "col1  col2  col3  col4\n 1     2     3     2\n 2     3     NaN   2\n 3     4    NaN    3\n 2     NaN   NaN   2\n 0     2    NaN    2\n"
            ],
            "answer": {
                "ans_desc": "first you may want to get ride of the NaNs: and then apply a function to every row to find your value: Output: ",
                "code": [
                    "def func(x,l=[2,1,3,0,4,5]):\n    for j in l:\n      if(j in x):\n         return j\n\ndf['new'] = df.apply(lambda x: func(list(x)),axis =1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 332,
            "user_id": 9952195,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/KwO3A.png?s=128&g=1",
            "display_name": "lil-wolf",
            "link": "https://stackoverflow.com/users/9952195/lil-wolf"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 52491361,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1537854840,
        "creation_date": 1537854388,
        "question_id": 52491327,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52491327/pandas-dataframe-data-are-same-or-new",
        "title": "Pandas Dataframe data are same or new?",
        "body": "<p>In Python, Pandas dataframes are used : <br><br>\n dataframe_1 :</p>\n\n<pre><code>     id\n0  AB17\n1  AB18\n2  AB19\n3  AB20\n4  AB10\n</code></pre>\n\n<p>dataframe_2 :</p>\n\n<pre><code>     id\n0  AB20\n1  AB10\n2  AB17\n3  AB21\n4  AB09\n</code></pre>\n\n<p>Here, dataframe_2 contains <b>AB20</b>,<b> AB10</b> and <b>AB17</b> same as dataframe_1 in random order.<br><br>\nHow to check which elements in dataframe_2 <b>are new and which are same </b>as dataframe_1 ???</p>\n",
        "answer_body": "<p>I think need <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html\" rel=\"nofollow noreferrer\"><code>isin</code></a> for boolean mask and filter by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\" rel=\"nofollow noreferrer\"><code>boolean indexing</code></a>, if necessary convert output <code>Series</code> to <code>list</code>:</p>\n\n<pre><code>mask = dataframe_2['id'].isin(dataframe_1['id'])\nprint (mask)\n0     True\n1     True\n2     True\n3    False\n4    False\nName: id, dtype: bool\n\nsame = dataframe_2.loc[mask, 'id'].tolist()\ndiff = dataframe_2.loc[~mask, 'id'].tolist()\n\n#if want unique values\n#same = dataframe_2.loc[mask, 'id'].unique().tolist()\n#diff = dataframe_2.loc[~mask, 'id'].unique().tolist()\n\nprint (same)\n['AB20', 'AB10', 'AB17']\n\nprint (diff)\n['AB21', 'AB09']\n</code></pre>\n",
        "question_body": "<p>In Python, Pandas dataframes are used : <br><br>\n dataframe_1 :</p>\n\n<pre><code>     id\n0  AB17\n1  AB18\n2  AB19\n3  AB20\n4  AB10\n</code></pre>\n\n<p>dataframe_2 :</p>\n\n<pre><code>     id\n0  AB20\n1  AB10\n2  AB17\n3  AB21\n4  AB09\n</code></pre>\n\n<p>Here, dataframe_2 contains <b>AB20</b>,<b> AB10</b> and <b>AB17</b> same as dataframe_1 in random order.<br><br>\nHow to check which elements in dataframe_2 <b>are new and which are same </b>as dataframe_1 ???</p>\n",
        "formatted_input": {
            "qid": 52491327,
            "link": "https://stackoverflow.com/questions/52491327/pandas-dataframe-data-are-same-or-new",
            "question": {
                "title": "Pandas Dataframe data are same or new?",
                "ques_desc": "In Python, Pandas dataframes are used : dataframe_1 : dataframe_2 : Here, dataframe_2 contains AB20, AB10 and AB17 same as dataframe_1 in random order. How to check which elements in dataframe_2 are new and which are same as dataframe_1 ??? "
            },
            "io": [
                "     id\n0  AB17\n1  AB18\n2  AB19\n3  AB20\n4  AB10\n",
                "     id\n0  AB20\n1  AB10\n2  AB17\n3  AB21\n4  AB09\n"
            ],
            "answer": {
                "ans_desc": "I think need for boolean mask and filter by with , if necessary convert output to : ",
                "code": [
                    "mask = dataframe_2['id'].isin(dataframe_1['id'])\nprint (mask)\n0     True\n1     True\n2     True\n3    False\n4    False\nName: id, dtype: bool\n\nsame = dataframe_2.loc[mask, 'id'].tolist()\ndiff = dataframe_2.loc[~mask, 'id'].tolist()\n\n#if want unique values\n#same = dataframe_2.loc[mask, 'id'].unique().tolist()\n#diff = dataframe_2.loc[~mask, 'id'].unique().tolist()\n\nprint (same)\n['AB20', 'AB10', 'AB17']\n\nprint (diff)\n['AB21', 'AB09']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dictionary",
            "dataframe",
            "python-2.x"
        ],
        "owner": {
            "reputation": 404,
            "user_id": 9966212,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/97d3d5fa55149a03b87c7c42a5f5ccb5?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ruven Guna",
            "link": "https://stackoverflow.com/users/9966212/ruven-guna"
        },
        "is_answered": true,
        "view_count": 378,
        "accepted_answer_id": 52489929,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1537844022,
        "creation_date": 1537840728,
        "last_edit_date": 1537842894,
        "question_id": 52489565,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52489565/convert-list-of-dict-in-dataframe-to-csv",
        "title": "Convert list of dict in dataframe to CSV",
        "body": "<p>I have a dataframe that looks like this (df1):</p>\n\n<pre><code>id  detail\n78  [{}{}{}{}{}]\n120 [{}{}{}{}{}]\n110 [{}{}{}{}{}]\n109 [{}{}{}{}{}]\n109 [{}{}{}{}{}]\n79  [{}{}{}{}{}]\n</code></pre>\n\n<p>The detail column contains a list of dictionaries and each dictionary looks like this:</p>\n\n<pre><code>{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}\n</code></pre>\n\n<p>I need to extract this information into a CSV with this format:</p>\n\n<pre><code>frame, id, x1, y1, x2, y2, score, class, visibility\n</code></pre>\n\n<p>In addition, the x2 and y2 in the extracted data should be like this:</p>\n\n<pre><code>x2_new = x2 + x1 = 630 + 602 = 1232\ny2_new = y2 + y1 = 564 + 549 = 1113\n</code></pre>\n\n<p>Expected output (Assuming the dict provided is in the first row of df1):</p>\n\n<pre><code>1054, 78, 602, 549, 1232, 1113, 1, 5, 0.0\n</code></pre>\n\n<p>I have tried this code to create a new df based off the detail column but I got an error:</p>\n\n<pre><code>for i in finaldftoconvert['id']:\n    for k in finaldftoconvert[['detail'][['id']==i]]:\n        df = pd.DataFrame(k)\nprint df\n</code></pre>\n\n<p>Error:</p>\n\n<pre><code>main.py:267: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  for k in finaldftoconvert[['detail'][['id']==i]]:\nTraceback (most recent call last):\n  File \"main.py\", line 268, in &lt;module&gt;\n    df = pd.DataFrame(k)\n  File \"/usr/lib/python2.7/dist-packages/pandas/core/frame.py\", line 305, in __init__\n    raise PandasError('DataFrame constructor not properly called!')\npandas.core.common.PandasError: DataFrame constructor not properly called!\n</code></pre>\n",
        "answer_body": "<pre><code>a = pd.DataFrame(index=[78],columns=[\"detail\"])\na.loc[78,\"detail\"] = [{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\na.loc[188,\"detail\"] = [{'y1': 649, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\n</code></pre>\n\n<p>For each dict, you should use <code>pd.DataFrame.from_dict</code>.\nActually, I don't exactly that you want to print it out? or convert it into multiple dataframes. </p>\n\n<p>Here is some simple solutions.</p>\n\n<pre><code># print it \na.applymap(lambda x:print(pd.DataFrame.from_dict({0:x[0]})))\n# convert it\nfor i in a.index:\n    tmp = pd.DataFrame.from_dict({0:a.loc[i,\"detail\"][0]}).T\n    tmp.x2 = tmp.x2+tmp.x1\n    tmp.y2 = tmp.y2 + tmp.y1\n    # this function you could storge in any dict/list. Or you could storge into a list. And using pd.concat to concate them together.\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like this (df1):</p>\n\n<pre><code>id  detail\n78  [{}{}{}{}{}]\n120 [{}{}{}{}{}]\n110 [{}{}{}{}{}]\n109 [{}{}{}{}{}]\n109 [{}{}{}{}{}]\n79  [{}{}{}{}{}]\n</code></pre>\n\n<p>The detail column contains a list of dictionaries and each dictionary looks like this:</p>\n\n<pre><code>{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}\n</code></pre>\n\n<p>I need to extract this information into a CSV with this format:</p>\n\n<pre><code>frame, id, x1, y1, x2, y2, score, class, visibility\n</code></pre>\n\n<p>In addition, the x2 and y2 in the extracted data should be like this:</p>\n\n<pre><code>x2_new = x2 + x1 = 630 + 602 = 1232\ny2_new = y2 + y1 = 564 + 549 = 1113\n</code></pre>\n\n<p>Expected output (Assuming the dict provided is in the first row of df1):</p>\n\n<pre><code>1054, 78, 602, 549, 1232, 1113, 1, 5, 0.0\n</code></pre>\n\n<p>I have tried this code to create a new df based off the detail column but I got an error:</p>\n\n<pre><code>for i in finaldftoconvert['id']:\n    for k in finaldftoconvert[['detail'][['id']==i]]:\n        df = pd.DataFrame(k)\nprint df\n</code></pre>\n\n<p>Error:</p>\n\n<pre><code>main.py:267: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  for k in finaldftoconvert[['detail'][['id']==i]]:\nTraceback (most recent call last):\n  File \"main.py\", line 268, in &lt;module&gt;\n    df = pd.DataFrame(k)\n  File \"/usr/lib/python2.7/dist-packages/pandas/core/frame.py\", line 305, in __init__\n    raise PandasError('DataFrame constructor not properly called!')\npandas.core.common.PandasError: DataFrame constructor not properly called!\n</code></pre>\n",
        "formatted_input": {
            "qid": 52489565,
            "link": "https://stackoverflow.com/questions/52489565/convert-list-of-dict-in-dataframe-to-csv",
            "question": {
                "title": "Convert list of dict in dataframe to CSV",
                "ques_desc": "I have a dataframe that looks like this (df1): The detail column contains a list of dictionaries and each dictionary looks like this: I need to extract this information into a CSV with this format: In addition, the x2 and y2 in the extracted data should be like this: Expected output (Assuming the dict provided is in the first row of df1): I have tried this code to create a new df based off the detail column but I got an error: Error: "
            },
            "io": [
                "{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}\n",
                "1054, 78, 602, 549, 1232, 1113, 1, 5, 0.0\n"
            ],
            "answer": {
                "ans_desc": " For each dict, you should use . Actually, I don't exactly that you want to print it out? or convert it into multiple dataframes. Here is some simple solutions. ",
                "code": [
                    "a = pd.DataFrame(index=[78],columns=[\"detail\"])\na.loc[78,\"detail\"] = [{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\na.loc[188,\"detail\"] = [{'y1': 649, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 1377,
            "user_id": 9276708,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/Cwbyr.png?s=128&g=1",
            "display_name": "rosefun",
            "link": "https://stackoverflow.com/users/9276708/rosefun"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 52439392,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1537518786,
        "creation_date": 1537516898,
        "question_id": 52439222,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52439222/how-to-get-the-top-frequency-elements-after-grouping-by-columns",
        "title": "How to get the top frequency elements after grouping by columns?",
        "body": "<p>I have a DataFrame named <code>df</code>, and I want to count the top frequency elements in column <code>app_0</code>, <code>app_1</code> and <code>app_2</code> on different <code>sex</code>.</p>\n\n<pre><code>import pandas as pd \nimport numpy as np \ndf=pd.DataFrame({'id':[1,2,3,4],'app_0':['a','b','c','d'],\n'app_1':['b','c','d',np.nan],'app_2':['c','b','a','a'],'sex':[0,0,1,1]})\n</code></pre>\n\n<p><code>Input:</code></p>\n\n<pre><code>df\n    id app_0 app_1 app_2  sex\n0   1     a     b     c    0\n1   2     b     c     b    0\n2   3     c     d     a    1\n3   4     d   NaN     a    1\n</code></pre>\n\n<p>As you see, the <code>sex</code> of both <code>id 1</code> and <code>id 2</code> is <code>0</code>. For <code>sex 0</code>, <code>b</code> appears the most in column <code>app_0</code>, <code>app_1</code> and <code>app_2</code>, <code>c</code> appears the second most. So for <code>id 1</code> and <code>id 2</code>, the most frequency element is <code>b</code>, and the second most is <code>c</code>.</p>\n\n<p><code>Expected:</code></p>\n\n<pre><code>df\n    id app_0 app_1 app_2  sex  top_1  top_2\n0   1     a     b     c    0      b      c\n1   2     b     c     b    0      b      c\n2   3     c     d     a    1      a      d\n3   4     d   NaN     a    1      a      d\n</code></pre>\n",
        "answer_body": "<p>Use custom function with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>stack</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\" rel=\"nofollow noreferrer\"><code>value_counts</code></a>:</p>\n\n<pre><code>def f(x):\n    s = x.stack().value_counts()\n    return pd.Series([s.index[0], s.index[1]], index=['top_1','top_2'])\n</code></pre>\n\n<p>Or use <code>Counter</code> with flattened values with <a href=\"https://docs.python.org/3.6/library/collections.html#collections.Counter.most_common\" rel=\"nofollow noreferrer\"><code>Counter.most_common</code></a>:</p>\n\n<pre><code>from collections import Counter\n\ndef f(x):\n    c = Counter([y for x in x.values.tolist() for y in x])\n    a = c.most_common(2)\n    return pd.Series([a[0][0], a[1][0]], index=['top_1','top_2'])\n\ndf1 = df.groupby('sex')['app_0','app_1','app_2'].apply(f)\n\ndf = df.join(df1, on='sex')\nprint (df)\n   id app_0 app_1 app_2  sex top_1 top_2\n0   1     a     b     c    0     b     c\n1   2     b     c     b    0     b     c\n2   3     c     d     a    1     a     d\n3   4     d   NaN     a    1     a     d\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>More general solution working if second top values not exist with <code>next</code>:</p>\n\n<pre><code>df=pd.DataFrame({'id':[1,2,3,4],'app_0':['a','a','a','a'],\n'app_1':['a','a','a',np.nan],'app_2':['a','a','a','a'],'sex':[0,0,1,1]})\nprint (df)\n   id app_0 app_1 app_2  sex\n0   1     a     a     a    0\n1   2     a     a     a    0\n2   3     a     a     a    1\n3   4     a   NaN     a    1\n\ndef f(x):\n    c = Counter([y for x in x.values.tolist() for y in x])\n    a = iter(c.most_common(2))\n\n    return pd.Series([next(a, ['no top1'])[0],\n                      next(a, ['no top2'])[0]], index=['top_1','top_2'])\n\ndf1 = df.groupby('sex')['app_0','app_1','app_2'].apply(f)\n\ndf = df.join(df1, on='sex')\nprint (df)\n   id app_0 app_1 app_2  sex top_1    top_2\n0   1     a     a     a    0     a  no top2\n1   2     a     a     a    0     a  no top2\n2   3     a     a     a    1     a      NaN\n3   4     a   NaN     a    1     a      NaN\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame named <code>df</code>, and I want to count the top frequency elements in column <code>app_0</code>, <code>app_1</code> and <code>app_2</code> on different <code>sex</code>.</p>\n\n<pre><code>import pandas as pd \nimport numpy as np \ndf=pd.DataFrame({'id':[1,2,3,4],'app_0':['a','b','c','d'],\n'app_1':['b','c','d',np.nan],'app_2':['c','b','a','a'],'sex':[0,0,1,1]})\n</code></pre>\n\n<p><code>Input:</code></p>\n\n<pre><code>df\n    id app_0 app_1 app_2  sex\n0   1     a     b     c    0\n1   2     b     c     b    0\n2   3     c     d     a    1\n3   4     d   NaN     a    1\n</code></pre>\n\n<p>As you see, the <code>sex</code> of both <code>id 1</code> and <code>id 2</code> is <code>0</code>. For <code>sex 0</code>, <code>b</code> appears the most in column <code>app_0</code>, <code>app_1</code> and <code>app_2</code>, <code>c</code> appears the second most. So for <code>id 1</code> and <code>id 2</code>, the most frequency element is <code>b</code>, and the second most is <code>c</code>.</p>\n\n<p><code>Expected:</code></p>\n\n<pre><code>df\n    id app_0 app_1 app_2  sex  top_1  top_2\n0   1     a     b     c    0      b      c\n1   2     b     c     b    0      b      c\n2   3     c     d     a    1      a      d\n3   4     d   NaN     a    1      a      d\n</code></pre>\n",
        "formatted_input": {
            "qid": 52439222,
            "link": "https://stackoverflow.com/questions/52439222/how-to-get-the-top-frequency-elements-after-grouping-by-columns",
            "question": {
                "title": "How to get the top frequency elements after grouping by columns?",
                "ques_desc": "I have a DataFrame named , and I want to count the top frequency elements in column , and on different . As you see, the of both and is . For , appears the most in column , and , appears the second most. So for and , the most frequency element is , and the second most is . "
            },
            "io": [
                "df\n    id app_0 app_1 app_2  sex\n0   1     a     b     c    0\n1   2     b     c     b    0\n2   3     c     d     a    1\n3   4     d   NaN     a    1\n",
                "df\n    id app_0 app_1 app_2  sex  top_1  top_2\n0   1     a     b     c    0      b      c\n1   2     b     c     b    0      b      c\n2   3     c     d     a    1      a      d\n3   4     d   NaN     a    1      a      d\n"
            ],
            "answer": {
                "ans_desc": "Use custom function with and : Or use with flattened values with : EDIT: More general solution working if second top values not exist with : ",
                "code": [
                    "def f(x):\n    s = x.stack().value_counts()\n    return pd.Series([s.index[0], s.index[1]], index=['top_1','top_2'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-processing",
            "data-munging"
        ],
        "owner": {
            "reputation": 607,
            "user_id": 10117402,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/219cc8aa594874e43e85530babe92166?s=128&d=identicon&r=PG&f=1",
            "display_name": "oren_isp",
            "link": "https://stackoverflow.com/users/10117402/oren-isp"
        },
        "is_answered": true,
        "view_count": 228,
        "accepted_answer_id": 52354748,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1537108455,
        "creation_date": 1537100651,
        "last_edit_date": 1537102486,
        "question_id": 52354096,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52354096/pandas-dataframe-take-rows-before-certain-indexes",
        "title": "pandas dataframe take rows before certain indexes",
        "body": "<p>I have a dataframe and a list of indexes, and I want to get a new dataframe such that for each index (from the given last), I will take the all the preceding  rows that matches in the value of the given column at the index.</p>\n\n<pre><code>      C1 C2 C3\n0     1  2  A\n1     3  4  A\n2     5  4  A\n3     7  5  B\n4     9  7  C\n5     2  3  D\n6     1  1  D\n</code></pre>\n\n<p>The column c3 the indexes (row numbers) 2, 4 , 5 my new dataframe will be:</p>\n\n<pre><code>     C1 C2 C3\n0     1  2  A\n1     3  4  A\n2     5  4  A\n4     9  7  C\n5     2  3  D\n</code></pre>\n\n<p>Explanation:</p>\n\n<p>For index 2, rows 0,1,2 were selected because C3 equals in all of them.</p>\n\n<p>For index 4, no preceding row is valid.</p>\n\n<p>And for index 5 also no preceding row is valid, and row 6 is irrelevant because it is not preceding.\nWhat is the best way to do so?</p>\n",
        "answer_body": "<p>you can make conditions to filter data,if you want just preceding rows match to condition.</p>\n\n<pre><code>ind= 2\ncol ='C3'\n# \".loc[np.arange(ind+1)]\" creates indexes till preceding row, so rest of matching conditions can be ignored \ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n</code></pre>\n\n<p>Out:</p>\n\n<pre><code>   C1   C2  C3\n0   1   2   A\n1   3   4   A\n2   5   4   A\n</code></pre>\n\n<p>appying on other column</p>\n\n<pre><code>ind= 2\ncol ='C2'\ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n</code></pre>\n\n<p>Out:</p>\n\n<pre><code>   C1   C2  C3\n1   3.0 4.0 A\n2   5.0 4.0 A\n</code></pre>\n",
        "question_body": "<p>I have a dataframe and a list of indexes, and I want to get a new dataframe such that for each index (from the given last), I will take the all the preceding  rows that matches in the value of the given column at the index.</p>\n\n<pre><code>      C1 C2 C3\n0     1  2  A\n1     3  4  A\n2     5  4  A\n3     7  5  B\n4     9  7  C\n5     2  3  D\n6     1  1  D\n</code></pre>\n\n<p>The column c3 the indexes (row numbers) 2, 4 , 5 my new dataframe will be:</p>\n\n<pre><code>     C1 C2 C3\n0     1  2  A\n1     3  4  A\n2     5  4  A\n4     9  7  C\n5     2  3  D\n</code></pre>\n\n<p>Explanation:</p>\n\n<p>For index 2, rows 0,1,2 were selected because C3 equals in all of them.</p>\n\n<p>For index 4, no preceding row is valid.</p>\n\n<p>And for index 5 also no preceding row is valid, and row 6 is irrelevant because it is not preceding.\nWhat is the best way to do so?</p>\n",
        "formatted_input": {
            "qid": 52354096,
            "link": "https://stackoverflow.com/questions/52354096/pandas-dataframe-take-rows-before-certain-indexes",
            "question": {
                "title": "pandas dataframe take rows before certain indexes",
                "ques_desc": "I have a dataframe and a list of indexes, and I want to get a new dataframe such that for each index (from the given last), I will take the all the preceding rows that matches in the value of the given column at the index. The column c3 the indexes (row numbers) 2, 4 , 5 my new dataframe will be: Explanation: For index 2, rows 0,1,2 were selected because C3 equals in all of them. For index 4, no preceding row is valid. And for index 5 also no preceding row is valid, and row 6 is irrelevant because it is not preceding. What is the best way to do so? "
            },
            "io": [
                "      C1 C2 C3\n0     1  2  A\n1     3  4  A\n2     5  4  A\n3     7  5  B\n4     9  7  C\n5     2  3  D\n6     1  1  D\n",
                "     C1 C2 C3\n0     1  2  A\n1     3  4  A\n2     5  4  A\n4     9  7  C\n5     2  3  D\n"
            ],
            "answer": {
                "ans_desc": "you can make conditions to filter data,if you want just preceding rows match to condition. Out: appying on other column Out: ",
                "code": [
                    "ind= 2\ncol ='C3'\n# \".loc[np.arange(ind+1)]\" creates indexes till preceding row, so rest of matching conditions can be ignored \ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n",
                    "ind= 2\ncol ='C2'\ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 65,
            "user_id": 9969070,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a63d50a5f5a59c0e443ce9a0deddd3b3?s=128&d=identicon&r=PG&f=1",
            "display_name": "GGiacomo",
            "link": "https://stackoverflow.com/users/9969070/ggiacomo"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 52313208,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1536869275,
        "creation_date": 1536836073,
        "last_edit_date": 1536859771,
        "question_id": 52312211,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52312211/mean-of-only-two-consecutive-rows-with-conditional-statements",
        "title": "Mean of only two consecutive rows with conditional statements",
        "body": "<p>After having searched for similar questions I found out with <a href=\"https://stackoverflow.com/questions/39477393/finding-the-average-of-two-consecutive-rows-in-pandas\">this</a> and <a href=\"https://stackoverflow.com/questions/36269323/take-the-average-only-of-two-consecutive-values-in-pandas\">this</a> questions. Unfortunately neither of them works with me.</p>\n\n<p>The first works on all the columns, the second does not work on my column of <code>True</code> and <code>False</code> and returns error (I also have not understood it completely).</p>\n\n<p>Here's a description of the problem:</p>\n\n<p>I am working with a dataframe of ~54k rows. Here's an example of 24 values:</p>\n\n<pre><code>+----+---------------------+---------------------+----------------------+--------------------+-------+\n|    |        date         |       omegasr       |        omega         |      omegass       | isday |\n+----+---------------------+---------------------+----------------------+--------------------+-------+\n|  1 | 2012-03-27 00:00:00 | -1.5707963267948966 |    -3.32335035194977 | 1.5707963267948966 | False |\n|  2 | 2012-03-27 01:00:00 | -1.5707963267948966 |  -3.0615509641506207 | 1.5707963267948966 | False |\n|  3 | 2012-03-27 02:00:00 | -1.5707963267948966 |   -2.799751576351471 | 1.5707963267948966 | False |\n|  4 | 2012-03-27 03:00:00 | -1.5707963267948966 |  -2.5379521885523215 | 1.5707963267948966 | False |\n|  5 | 2012-03-27 04:00:00 | -1.5707963267948966 |  -2.2761528007531724 | 1.5707963267948966 | False |\n|  6 | 2012-03-27 05:00:00 | -1.5707963267948966 |   -2.014353412954023 | 1.5707963267948966 | False |\n|  7 | 2012-03-27 06:00:00 | -1.5707963267948966 |  -1.7525540251548732 | 1.5707963267948966 | False |\n|  8 | 2012-03-27 07:00:00 | -1.5707963267948966 |  -1.4907546373557239 | 1.5707963267948966 | True  |\n|  9 | 2012-03-27 08:00:00 | -1.5707963267948966 |  -1.2289552495565745 | 1.5707963267948966 | True  |\n| 10 | 2012-03-27 09:00:00 | -1.5707963267948966 |  -0.9671558617574253 | 1.5707963267948966 | True  |\n| 11 | 2012-03-27 10:00:00 | -1.5707963267948966 |  -0.7053564739582756 | 1.5707963267948966 | True  |\n| 12 | 2012-03-27 11:00:00 | -1.5707963267948966 | -0.44355708615912615 | 1.5707963267948966 | True  |\n| 13 | 2012-03-27 12:00:00 | -1.5707963267948966 |  -0.1817576983599767 | 1.5707963267948966 | True  |\n| 14 | 2012-03-27 13:00:00 | -1.5707963267948966 |  0.08004168943917273 | 1.5707963267948966 | True  |\n| 15 | 2012-03-27 14:00:00 | -1.5707963267948966 |  0.34184107723832213 | 1.5707963267948966 | True  |\n| 16 | 2012-03-27 15:00:00 | -1.5707963267948966 |   0.6036404650374716 | 1.5707963267948966 | True  |\n| 17 | 2012-03-27 16:00:00 | -1.5707963267948966 |   0.8654398528366211 | 1.5707963267948966 | True  |\n| 18 | 2012-03-27 17:00:00 | -1.5707963267948966 |    1.127239240635771 | 1.5707963267948966 | True  |\n| 19 | 2012-03-27 18:00:00 | -1.5707963267948966 |   1.3890386284349199 | 1.5707963267948966 | True  |\n| 20 | 2012-03-27 19:00:00 | -1.5707963267948966 |   1.6508380162340692 | 1.5707963267948966 | False |\n| 21 | 2012-03-27 20:00:00 | -1.5707963267948966 |   1.9126374040332188 | 1.5707963267948966 | False |\n| 22 | 2012-03-27 21:00:00 | -1.5707963267948966 |    2.174436791832368 | 1.5707963267948966 | False |\n| 23 | 2012-03-27 22:00:00 | -1.5707963267948966 |   2.4362361796315177 | 1.5707963267948966 | False |\n| 24 | 2012-03-27 23:00:00 | -1.5707963267948966 |    2.698035567430667 | 1.5707963267948966 | False |\n+----+---------------------+---------------------+----------------------+--------------------+-------+\n</code></pre>\n\n<p><code>omega</code> is the solar hour angle in radians. It ranges from -pi/2 to +pi/2 for the hours 00:00 and 24:00 respectively. At midday its value is 0.</p>\n\n<p><code>omegass</code> is the hour angle to which the sunset occurs. Due to the symmetry of the sun-earth system, <code>omegasr = -omegass</code>. These values are constant along one day, but change for every day.</p>\n\n<p>The column <code>isday</code> is a result of a conditional expression: when <code>omegasr &lt; omega &lt; omegasr</code> then it's day and further calculations can be made.</p>\n\n<p>In order to do further calculations I need to associate for each hour the midpoint of the time span that the measure covers. So, for example, the midday measure was recorded at 12:00 but in order to represent all of that hour I want to have the hour angle of 12:30. Therefore I need a </p>\n\n<pre><code>omegam[i] = (omega[i],omega[i+1]).mean() \n</code></pre>\n\n<p>where <code>i</code> represents the index.</p>\n\n<p>But here a new problem arises: if the sunset occurs, let's say, at 6:40 am then the midpoint hour has to be calculated like this: </p>\n\n<pre><code>omegam[i] = (omegasr[i],omega[i+1]).mean() #sunrise\nomegam[i] = (omega[i],omegass[i+1]).mean() #sunset\n</code></pre>\n\n<p>Thus the hourly radian angle will correspond to 6:50am. I created the column <code>isday</code> to help perform this task, but unfortunately I can't really use it.</p>\n\n<p>Thank you. </p>\n\n<p><strong>EDIT:</strong> </p>\n\n<p>The solution proposed by @Mabel Villaba is not correct, for the <code>new_omega</code> column only has sunrise and sunset values.</p>\n\n<p>A coorect <code>new_omega</code> column would be:</p>\n\n<pre><code> new_omega  \n... \n7   #here the mean is between omegasr and omega[8], therefore this new_omega value can't have a correct value, according to the proposed solution.\n\n8   -1.2289552495565745 # = omega[9]       \n9   omega[10]  #                  \n10  omega[11]\n... \n17   omega[18] \n18   omega[19] \n19   1.570796  #omegass\n...\n</code></pre>\n\n<p>I hope that it is clear enough</p>\n\n<p><strong>EDIT2:</strong></p>\n\n<p>Thank you again, but the values are still not correct: the mean values are still calculated wrongly. I have calculated manually the correct values, I will post them here:</p>\n\n<pre><code>     omegam\n\n...\n7    -1.530775\n8    -1.359855\n9    -1.098058\n...\n13   -0.05256705\n...\n19   1.47992\n...\n</code></pre>\n\n<p><strong>EDIT3:</strong></p>\n\n<p>I think the column <code>df['isday']</code> obtained thanks to the boolean mask might be misleading.</p>\n\n<p>In fact: the sunrise always occurs between two rows, let them be called <code>omega1</code> and <code>omega2</code>, whom belong to <code>row1</code> and <code>row2</code> respectively. The same happens with the sunset, but with<code>omega3</code> and <code>omega4</code>. What happens is that the correct <code>omegam</code> of <code>row1</code> is calculated as:</p>\n\n<pre><code>omegam(row1) = (omegasr + omega2)/2\n</code></pre>\n\n<p>but <code>row1</code> hase a <code>False</code> attribute in the <code>isday</code> column.</p>\n\n<p>For the sunset it's the opposite: occurring between <code>row3</code> and <code>row4</code> it is calculated as:</p>\n\n<pre><code>omegam(row3) = (omega3 + omegass)/2\n</code></pre>\n\n<p>and <code>row3</code> has a <code>True</code> attribute.</p>\n",
        "answer_body": "<p><strong>EDIT</strong></p>\n\n<p>In the case you mention, it is a little more complicated but I think I came to a workaround. There is some misleading, since the operation at sunrise and sunset is not always done in the same direction.</p>\n\n<p>Let us create two omegas, <code>omega1</code> that does <code>omegam[i] = 0.5 * (omega[i] + omegasr[i+1])</code> and another <code>omega2</code> that does <code>omegam[i] = 0.5 * (omega[i-1] + omegass[i])</code>:</p>\n\n<pre><code>df['omega1'] = .5*((df['omega'] + df['omegasr'].shift(-1)))   \ndf['omega2'] = .5*((df['omega'].shift(1) + df['omegass']))\n</code></pre>\n\n<p>Then, we need to create a mask that tells us whether it is sunset or sunrise, or none of them:</p>\n\n<pre><code>df['mask'] =  (df['isday'] * 1).diff().bfill()\n\n&gt;&gt; df[['date','mask', 'isday']]\n\n                     date  mask  isday\n0    2012-03-27 00:00:00    0.0  False\n1    2012-03-27 01:00:00    0.0  False\n2    2012-03-27 02:00:00    0.0  False\n3    2012-03-27 03:00:00    0.0  False\n4    2012-03-27 04:00:00    0.0  False\n5    2012-03-27 05:00:00    0.0  False\n6    2012-03-27 06:00:00    0.0  False\n7    2012-03-27 07:00:00    1.0   True\n8    2012-03-27 08:00:00    0.0   True\n9    2012-03-27 09:00:00    0.0   True\n10   2012-03-27 10:00:00    0.0   True\n11   2012-03-27 11:00:00    0.0   True\n12   2012-03-27 12:00:00    0.0   True\n13   2012-03-27 13:00:00    0.0   True\n14   2012-03-27 14:00:00    0.0   True\n15   2012-03-27 15:00:00    0.0   True\n16   2012-03-27 16:00:00    0.0   True\n17   2012-03-27 17:00:00    0.0   True\n18   2012-03-27 18:00:00    0.0   True\n19   2012-03-27 19:00:00   -1.0  False\n20   2012-03-27 20:00:00    0.0  False\n21   2012-03-27 21:00:00    0.0  False\n22   2012-03-27 22:00:00    0.0  False\n23   2012-03-27 23:00:00    0.0  False\n</code></pre>\n\n<p>This way, <code>df['mask']==1</code> corresponds to sunrise, <code>df['mask']==-1</code> to sunset and <code>df['mask']==0</code> corresponds to the rest.</p>\n\n<p>Based on these conditions, we can create <code>omegam</code>:</p>\n\n<pre><code>df['omegam'] = df['omega'].rolling(2).mean() * (df['mask'] == 0) + \\\n               df['omega1'] * (df['mask']==1) + \\\n               df['omega2'] * (df['mask']==-1)\n\n&gt;&gt; df[['date','omegam']]\n\n                     date    omegam\n0    2012-03-27 00:00:00        NaN\n1    2012-03-27 01:00:00  -3.192451\n2    2012-03-27 02:00:00  -2.930651\n3    2012-03-27 03:00:00  -2.668852\n4    2012-03-27 04:00:00  -2.407052\n5    2012-03-27 05:00:00  -2.145253\n6    2012-03-27 06:00:00  -1.883454\n7    2012-03-27 07:00:00  -1.530775\n8    2012-03-27 08:00:00  -1.359855\n9    2012-03-27 09:00:00  -1.098056\n10   2012-03-27 10:00:00  -0.836256\n11   2012-03-27 11:00:00  -0.574457\n12   2012-03-27 12:00:00  -0.312657\n13   2012-03-27 13:00:00  -0.050858\n14   2012-03-27 14:00:00   0.210941\n15   2012-03-27 15:00:00   0.472741\n16   2012-03-27 16:00:00   0.734540\n17   2012-03-27 17:00:00   0.996340\n18   2012-03-27 18:00:00   1.258139\n19   2012-03-27 19:00:00   1.479917\n20   2012-03-27 20:00:00   1.781738\n21   2012-03-27 21:00:00   2.043537\n22   2012-03-27 22:00:00   2.305336\n23   2012-03-27 23:00:00        NaN\n</code></pre>\n\n<p><strong>OLD SOLUTION</strong>:</p>\n\n<p>As you mention, since <code>omegasr = -omegass</code>, then you could create a new column in your pandas based on the hour so you can get the <code>omega</code> you need for the mean operation (if sunrise (hour&lt;12): omegasr, else: - omegasr):</p>\n\n<pre><code>df['new_omega'] = df.apply(lambda x: x['omegasr'] if pd.to_datetime(x['date']).hour &lt; 12 else -x['omegasr'], axis=1).shift(-1)\n\n&gt;&gt; df\n\n                     date   omegasr     omega   omegass  isday  new_omega\n\n1    2012-03-27 00:00:00  -1.570796 -3.323350  1.570796  False  -1.570796\n2    2012-03-27 01:00:00  -1.570796 -3.061551  1.570796  False  -1.570796\n3    2012-03-27 02:00:00  -1.570796 -2.799752  1.570796  False  -1.570796\n4    2012-03-27 03:00:00  -1.570796 -2.537952  1.570796  False  -1.570796\n5    2012-03-27 04:00:00  -1.570796 -2.276153  1.570796  False  -1.570796\n6    2012-03-27 05:00:00  -1.570796 -2.014353  1.570796  False  -1.570796\n7    2012-03-27 06:00:00  -1.570796 -1.752554  1.570796  False  -1.570796\n8    2012-03-27 07:00:00  -1.570796 -1.490755  1.570796   True  -1.570796\n9    2012-03-27 08:00:00  -1.570796 -1.228955  1.570796   True  -1.570796\n10   2012-03-27 09:00:00  -1.570796 -0.967156  1.570796   True  -1.570796\n11   2012-03-27 10:00:00  -1.570796 -0.705356  1.570796   True  -1.570796\n12   2012-03-27 11:00:00  -1.570796 -0.443557  1.570796   True   1.570796\n13   2012-03-27 12:00:00  -1.570796 -0.181758  1.570796   True   1.570796\n14   2012-03-27 13:00:00  -1.570796  0.080042  1.570796   True   1.570796\n15   2012-03-27 14:00:00  -1.570796  0.341841  1.570796   True   1.570796\n16   2012-03-27 15:00:00  -1.570796  0.603640  1.570796   True   1.570796\n17   2012-03-27 16:00:00  -1.570796  0.865440  1.570796   True   1.570796\n18   2012-03-27 17:00:00  -1.570796  1.127239  1.570796   True   1.570796\n19   2012-03-27 18:00:00  -1.570796  1.389039  1.570796   True   1.570796\n20   2012-03-27 19:00:00  -1.570796  1.650838  1.570796  False   1.570796\n21   2012-03-27 20:00:00  -1.570796  1.912637  1.570796  False   1.570796\n22   2012-03-27 21:00:00  -1.570796  2.174437  1.570796  False   1.570796\n23   2012-03-27 22:00:00  -1.570796  2.436236  1.570796  False   1.570796\n24   2012-03-27 23:00:00  -1.570796  2.698036  1.570796  False        NaN\n</code></pre>\n\n<p>Data in 'new_omega' is shifted  to comply with </p>\n\n<pre><code>omegam[i] = (omegasr[i],omega[i+1]).mean() #sunrise\nomegam[i] = (omega[i],omegass[i+1]).mean() #sunset\n</code></pre>\n\n<p>Then, <code>omegam</code> can be obtained just by applying the mean to the columns <code>omega</code> and <code>new_omega</code> when the condition <code>df['isday']==True</code> is satisfied or the <code>mean(omega[i], omega[i+1])</code> when <code>df['isday']==False</code> :</p>\n\n<pre><code># Calculate the rolling mean with a window=2 and then shift backwards \n# (by default pd.rollling(2).mean() would result in \n# omegam[i]= mean(omega[i-1], omega[i]))\n\n\ndf['omegam'] = df['omega'].rolling(2).mean().shift(-1)\n\n\ndf['omegam'][df['isday']] = df[['omega', 'new_omega']][df['isday']].mean(axis=1).values\n\n&gt;&gt; df['omegam']\n\n1    -3.192451\n2    -2.930651\n3    -2.668852\n4    -2.407052\n5    -2.145253\n6    -1.883454\n7    -1.621654\n8    -1.530775\n9    -1.399876\n10   -1.268976\n11   -1.138076\n12    0.563620\n13    0.694519\n14    0.825419\n15    0.956319\n16    1.087218\n17    1.218118\n18    1.349018\n19    1.479917\n20    1.781738\n21    2.043537\n22    2.305336\n23    2.567136\n24         NaN\nName: omegam, dtype: float64\n</code></pre>\n\n<p>Hope it serves.</p>\n",
        "question_body": "<p>After having searched for similar questions I found out with <a href=\"https://stackoverflow.com/questions/39477393/finding-the-average-of-two-consecutive-rows-in-pandas\">this</a> and <a href=\"https://stackoverflow.com/questions/36269323/take-the-average-only-of-two-consecutive-values-in-pandas\">this</a> questions. Unfortunately neither of them works with me.</p>\n\n<p>The first works on all the columns, the second does not work on my column of <code>True</code> and <code>False</code> and returns error (I also have not understood it completely).</p>\n\n<p>Here's a description of the problem:</p>\n\n<p>I am working with a dataframe of ~54k rows. Here's an example of 24 values:</p>\n\n<pre><code>+----+---------------------+---------------------+----------------------+--------------------+-------+\n|    |        date         |       omegasr       |        omega         |      omegass       | isday |\n+----+---------------------+---------------------+----------------------+--------------------+-------+\n|  1 | 2012-03-27 00:00:00 | -1.5707963267948966 |    -3.32335035194977 | 1.5707963267948966 | False |\n|  2 | 2012-03-27 01:00:00 | -1.5707963267948966 |  -3.0615509641506207 | 1.5707963267948966 | False |\n|  3 | 2012-03-27 02:00:00 | -1.5707963267948966 |   -2.799751576351471 | 1.5707963267948966 | False |\n|  4 | 2012-03-27 03:00:00 | -1.5707963267948966 |  -2.5379521885523215 | 1.5707963267948966 | False |\n|  5 | 2012-03-27 04:00:00 | -1.5707963267948966 |  -2.2761528007531724 | 1.5707963267948966 | False |\n|  6 | 2012-03-27 05:00:00 | -1.5707963267948966 |   -2.014353412954023 | 1.5707963267948966 | False |\n|  7 | 2012-03-27 06:00:00 | -1.5707963267948966 |  -1.7525540251548732 | 1.5707963267948966 | False |\n|  8 | 2012-03-27 07:00:00 | -1.5707963267948966 |  -1.4907546373557239 | 1.5707963267948966 | True  |\n|  9 | 2012-03-27 08:00:00 | -1.5707963267948966 |  -1.2289552495565745 | 1.5707963267948966 | True  |\n| 10 | 2012-03-27 09:00:00 | -1.5707963267948966 |  -0.9671558617574253 | 1.5707963267948966 | True  |\n| 11 | 2012-03-27 10:00:00 | -1.5707963267948966 |  -0.7053564739582756 | 1.5707963267948966 | True  |\n| 12 | 2012-03-27 11:00:00 | -1.5707963267948966 | -0.44355708615912615 | 1.5707963267948966 | True  |\n| 13 | 2012-03-27 12:00:00 | -1.5707963267948966 |  -0.1817576983599767 | 1.5707963267948966 | True  |\n| 14 | 2012-03-27 13:00:00 | -1.5707963267948966 |  0.08004168943917273 | 1.5707963267948966 | True  |\n| 15 | 2012-03-27 14:00:00 | -1.5707963267948966 |  0.34184107723832213 | 1.5707963267948966 | True  |\n| 16 | 2012-03-27 15:00:00 | -1.5707963267948966 |   0.6036404650374716 | 1.5707963267948966 | True  |\n| 17 | 2012-03-27 16:00:00 | -1.5707963267948966 |   0.8654398528366211 | 1.5707963267948966 | True  |\n| 18 | 2012-03-27 17:00:00 | -1.5707963267948966 |    1.127239240635771 | 1.5707963267948966 | True  |\n| 19 | 2012-03-27 18:00:00 | -1.5707963267948966 |   1.3890386284349199 | 1.5707963267948966 | True  |\n| 20 | 2012-03-27 19:00:00 | -1.5707963267948966 |   1.6508380162340692 | 1.5707963267948966 | False |\n| 21 | 2012-03-27 20:00:00 | -1.5707963267948966 |   1.9126374040332188 | 1.5707963267948966 | False |\n| 22 | 2012-03-27 21:00:00 | -1.5707963267948966 |    2.174436791832368 | 1.5707963267948966 | False |\n| 23 | 2012-03-27 22:00:00 | -1.5707963267948966 |   2.4362361796315177 | 1.5707963267948966 | False |\n| 24 | 2012-03-27 23:00:00 | -1.5707963267948966 |    2.698035567430667 | 1.5707963267948966 | False |\n+----+---------------------+---------------------+----------------------+--------------------+-------+\n</code></pre>\n\n<p><code>omega</code> is the solar hour angle in radians. It ranges from -pi/2 to +pi/2 for the hours 00:00 and 24:00 respectively. At midday its value is 0.</p>\n\n<p><code>omegass</code> is the hour angle to which the sunset occurs. Due to the symmetry of the sun-earth system, <code>omegasr = -omegass</code>. These values are constant along one day, but change for every day.</p>\n\n<p>The column <code>isday</code> is a result of a conditional expression: when <code>omegasr &lt; omega &lt; omegasr</code> then it's day and further calculations can be made.</p>\n\n<p>In order to do further calculations I need to associate for each hour the midpoint of the time span that the measure covers. So, for example, the midday measure was recorded at 12:00 but in order to represent all of that hour I want to have the hour angle of 12:30. Therefore I need a </p>\n\n<pre><code>omegam[i] = (omega[i],omega[i+1]).mean() \n</code></pre>\n\n<p>where <code>i</code> represents the index.</p>\n\n<p>But here a new problem arises: if the sunset occurs, let's say, at 6:40 am then the midpoint hour has to be calculated like this: </p>\n\n<pre><code>omegam[i] = (omegasr[i],omega[i+1]).mean() #sunrise\nomegam[i] = (omega[i],omegass[i+1]).mean() #sunset\n</code></pre>\n\n<p>Thus the hourly radian angle will correspond to 6:50am. I created the column <code>isday</code> to help perform this task, but unfortunately I can't really use it.</p>\n\n<p>Thank you. </p>\n\n<p><strong>EDIT:</strong> </p>\n\n<p>The solution proposed by @Mabel Villaba is not correct, for the <code>new_omega</code> column only has sunrise and sunset values.</p>\n\n<p>A coorect <code>new_omega</code> column would be:</p>\n\n<pre><code> new_omega  \n... \n7   #here the mean is between omegasr and omega[8], therefore this new_omega value can't have a correct value, according to the proposed solution.\n\n8   -1.2289552495565745 # = omega[9]       \n9   omega[10]  #                  \n10  omega[11]\n... \n17   omega[18] \n18   omega[19] \n19   1.570796  #omegass\n...\n</code></pre>\n\n<p>I hope that it is clear enough</p>\n\n<p><strong>EDIT2:</strong></p>\n\n<p>Thank you again, but the values are still not correct: the mean values are still calculated wrongly. I have calculated manually the correct values, I will post them here:</p>\n\n<pre><code>     omegam\n\n...\n7    -1.530775\n8    -1.359855\n9    -1.098058\n...\n13   -0.05256705\n...\n19   1.47992\n...\n</code></pre>\n\n<p><strong>EDIT3:</strong></p>\n\n<p>I think the column <code>df['isday']</code> obtained thanks to the boolean mask might be misleading.</p>\n\n<p>In fact: the sunrise always occurs between two rows, let them be called <code>omega1</code> and <code>omega2</code>, whom belong to <code>row1</code> and <code>row2</code> respectively. The same happens with the sunset, but with<code>omega3</code> and <code>omega4</code>. What happens is that the correct <code>omegam</code> of <code>row1</code> is calculated as:</p>\n\n<pre><code>omegam(row1) = (omegasr + omega2)/2\n</code></pre>\n\n<p>but <code>row1</code> hase a <code>False</code> attribute in the <code>isday</code> column.</p>\n\n<p>For the sunset it's the opposite: occurring between <code>row3</code> and <code>row4</code> it is calculated as:</p>\n\n<pre><code>omegam(row3) = (omega3 + omegass)/2\n</code></pre>\n\n<p>and <code>row3</code> has a <code>True</code> attribute.</p>\n",
        "formatted_input": {
            "qid": 52312211,
            "link": "https://stackoverflow.com/questions/52312211/mean-of-only-two-consecutive-rows-with-conditional-statements",
            "question": {
                "title": "Mean of only two consecutive rows with conditional statements",
                "ques_desc": "After having searched for similar questions I found out with this and this questions. Unfortunately neither of them works with me. The first works on all the columns, the second does not work on my column of and and returns error (I also have not understood it completely). Here's a description of the problem: I am working with a dataframe of ~54k rows. Here's an example of 24 values: is the solar hour angle in radians. It ranges from -pi/2 to +pi/2 for the hours 00:00 and 24:00 respectively. At midday its value is 0. is the hour angle to which the sunset occurs. Due to the symmetry of the sun-earth system, . These values are constant along one day, but change for every day. The column is a result of a conditional expression: when then it's day and further calculations can be made. In order to do further calculations I need to associate for each hour the midpoint of the time span that the measure covers. So, for example, the midday measure was recorded at 12:00 but in order to represent all of that hour I want to have the hour angle of 12:30. Therefore I need a where represents the index. But here a new problem arises: if the sunset occurs, let's say, at 6:40 am then the midpoint hour has to be calculated like this: Thus the hourly radian angle will correspond to 6:50am. I created the column to help perform this task, but unfortunately I can't really use it. Thank you. EDIT: The solution proposed by @Mabel Villaba is not correct, for the column only has sunrise and sunset values. A coorect column would be: I hope that it is clear enough EDIT2: Thank you again, but the values are still not correct: the mean values are still calculated wrongly. I have calculated manually the correct values, I will post them here: EDIT3: I think the column obtained thanks to the boolean mask might be misleading. In fact: the sunrise always occurs between two rows, let them be called and , whom belong to and respectively. The same happens with the sunset, but with and . What happens is that the correct of is calculated as: but hase a attribute in the column. For the sunset it's the opposite: occurring between and it is calculated as: and has a attribute. "
            },
            "io": [
                "     omegam\n\n...\n7    -1.530775\n8    -1.359855\n9    -1.098058\n...\n13   -0.05256705\n...\n19   1.47992\n...\n",
                "omegam(row3) = (omega3 + omegass)/2\n"
            ],
            "answer": {
                "ans_desc": "EDIT In the case you mention, it is a little more complicated but I think I came to a workaround. There is some misleading, since the operation at sunrise and sunset is not always done in the same direction. Let us create two omegas, that does and another that does : Then, we need to create a mask that tells us whether it is sunset or sunrise, or none of them: This way, corresponds to sunrise, to sunset and corresponds to the rest. Based on these conditions, we can create : OLD SOLUTION: As you mention, since , then you could create a new column in your pandas based on the hour so you can get the you need for the mean operation (if sunrise (hour<12): omegasr, else: - omegasr): Data in 'new_omega' is shifted to comply with Then, can be obtained just by applying the mean to the columns and when the condition is satisfied or the when : Hope it serves. ",
                "code": [
                    "df['omega1'] = .5*((df['omega'] + df['omegasr'].shift(-1)))   \ndf['omega2'] = .5*((df['omega'].shift(1) + df['omegass']))\n",
                    "df['mask'] =  (df['isday'] * 1).diff().bfill()\n\n>> df[['date','mask', 'isday']]\n\n                     date  mask  isday\n0    2012-03-27 00:00:00    0.0  False\n1    2012-03-27 01:00:00    0.0  False\n2    2012-03-27 02:00:00    0.0  False\n3    2012-03-27 03:00:00    0.0  False\n4    2012-03-27 04:00:00    0.0  False\n5    2012-03-27 05:00:00    0.0  False\n6    2012-03-27 06:00:00    0.0  False\n7    2012-03-27 07:00:00    1.0   True\n8    2012-03-27 08:00:00    0.0   True\n9    2012-03-27 09:00:00    0.0   True\n10   2012-03-27 10:00:00    0.0   True\n11   2012-03-27 11:00:00    0.0   True\n12   2012-03-27 12:00:00    0.0   True\n13   2012-03-27 13:00:00    0.0   True\n14   2012-03-27 14:00:00    0.0   True\n15   2012-03-27 15:00:00    0.0   True\n16   2012-03-27 16:00:00    0.0   True\n17   2012-03-27 17:00:00    0.0   True\n18   2012-03-27 18:00:00    0.0   True\n19   2012-03-27 19:00:00   -1.0  False\n20   2012-03-27 20:00:00    0.0  False\n21   2012-03-27 21:00:00    0.0  False\n22   2012-03-27 22:00:00    0.0  False\n23   2012-03-27 23:00:00    0.0  False\n",
                    "df['new_omega'] = df.apply(lambda x: x['omegasr'] if pd.to_datetime(x['date']).hour < 12 else -x['omegasr'], axis=1).shift(-1)\n\n>> df\n\n                     date   omegasr     omega   omegass  isday  new_omega\n\n1    2012-03-27 00:00:00  -1.570796 -3.323350  1.570796  False  -1.570796\n2    2012-03-27 01:00:00  -1.570796 -3.061551  1.570796  False  -1.570796\n3    2012-03-27 02:00:00  -1.570796 -2.799752  1.570796  False  -1.570796\n4    2012-03-27 03:00:00  -1.570796 -2.537952  1.570796  False  -1.570796\n5    2012-03-27 04:00:00  -1.570796 -2.276153  1.570796  False  -1.570796\n6    2012-03-27 05:00:00  -1.570796 -2.014353  1.570796  False  -1.570796\n7    2012-03-27 06:00:00  -1.570796 -1.752554  1.570796  False  -1.570796\n8    2012-03-27 07:00:00  -1.570796 -1.490755  1.570796   True  -1.570796\n9    2012-03-27 08:00:00  -1.570796 -1.228955  1.570796   True  -1.570796\n10   2012-03-27 09:00:00  -1.570796 -0.967156  1.570796   True  -1.570796\n11   2012-03-27 10:00:00  -1.570796 -0.705356  1.570796   True  -1.570796\n12   2012-03-27 11:00:00  -1.570796 -0.443557  1.570796   True   1.570796\n13   2012-03-27 12:00:00  -1.570796 -0.181758  1.570796   True   1.570796\n14   2012-03-27 13:00:00  -1.570796  0.080042  1.570796   True   1.570796\n15   2012-03-27 14:00:00  -1.570796  0.341841  1.570796   True   1.570796\n16   2012-03-27 15:00:00  -1.570796  0.603640  1.570796   True   1.570796\n17   2012-03-27 16:00:00  -1.570796  0.865440  1.570796   True   1.570796\n18   2012-03-27 17:00:00  -1.570796  1.127239  1.570796   True   1.570796\n19   2012-03-27 18:00:00  -1.570796  1.389039  1.570796   True   1.570796\n20   2012-03-27 19:00:00  -1.570796  1.650838  1.570796  False   1.570796\n21   2012-03-27 20:00:00  -1.570796  1.912637  1.570796  False   1.570796\n22   2012-03-27 21:00:00  -1.570796  2.174437  1.570796  False   1.570796\n23   2012-03-27 22:00:00  -1.570796  2.436236  1.570796  False   1.570796\n24   2012-03-27 23:00:00  -1.570796  2.698036  1.570796  False        NaN\n",
                    "omegam[i] = (omegasr[i],omega[i+1]).mean() #sunrise\nomegam[i] = (omega[i],omegass[i+1]).mean() #sunset\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 377,
            "user_id": 10114490,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-BbsU7vdQuxM/AAAAAAAAAAI/AAAAAAAAACc/UWzpHftXqoU/photo.jpg?sz=128",
            "display_name": "Junaid Mohammad",
            "link": "https://stackoverflow.com/users/10114490/junaid-mohammad"
        },
        "is_answered": true,
        "view_count": 507,
        "accepted_answer_id": 52152038,
        "answer_count": 5,
        "score": 2,
        "last_activity_date": 1536020479,
        "creation_date": 1535985374,
        "last_edit_date": 1535987023,
        "question_id": 52151831,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52151831/repeating-a-series-to-create-a-df-in-python",
        "title": "Repeating a series to create a df in python",
        "body": "<p>I have a series, <code>s</code></p>\n\n<pre><code>       A   \n0     1.5         \n1     2.5                  \n2     1.3          \n</code></pre>\n\n<p>How do I repeat this column 9 times to create a dataframe. \nExpected output:</p>\n\n<pre><code>      A             A         ...       A\n0     1.5          1.5        ...      1.5         \n1     2.5          2.5        ...      2.5                 \n2     1.3          1.3        ...      1.3         \n</code></pre>\n\n<p><code>df.shape</code> is <code>(3, 9)</code></p>\n\n<p>I can use <code>pd.concat</code> but this is a bit messy. I tried <code>np.repeat</code> but it wouldn't work along <code>axis=1</code>, and <code>axis=0</code> isn't what I need</p>\n\n<p>Thanks</p>\n",
        "answer_body": "<p>Not sure, what you meant by <strong>messy</strong> but <code>pd.concat</code>\nhandles it pretty well here:  </p>\n\n<pre><code>pd.concat([s for i in range(9)], axis=1)\n</code></pre>\n\n<p>you can pass   </p>\n\n<pre><code>keys=[f'A{i}' for i in range(9)]\n</code></pre>\n\n<p>to <code>pd.concat</code> to make distinct column names.</p>\n",
        "question_body": "<p>I have a series, <code>s</code></p>\n\n<pre><code>       A   \n0     1.5         \n1     2.5                  \n2     1.3          \n</code></pre>\n\n<p>How do I repeat this column 9 times to create a dataframe. \nExpected output:</p>\n\n<pre><code>      A             A         ...       A\n0     1.5          1.5        ...      1.5         \n1     2.5          2.5        ...      2.5                 \n2     1.3          1.3        ...      1.3         \n</code></pre>\n\n<p><code>df.shape</code> is <code>(3, 9)</code></p>\n\n<p>I can use <code>pd.concat</code> but this is a bit messy. I tried <code>np.repeat</code> but it wouldn't work along <code>axis=1</code>, and <code>axis=0</code> isn't what I need</p>\n\n<p>Thanks</p>\n",
        "formatted_input": {
            "qid": 52151831,
            "link": "https://stackoverflow.com/questions/52151831/repeating-a-series-to-create-a-df-in-python",
            "question": {
                "title": "Repeating a series to create a df in python",
                "ques_desc": "I have a series, How do I repeat this column 9 times to create a dataframe. Expected output: is I can use but this is a bit messy. I tried but it wouldn't work along , and isn't what I need Thanks "
            },
            "io": [
                "       A   \n0     1.5         \n1     2.5                  \n2     1.3          \n",
                "      A             A         ...       A\n0     1.5          1.5        ...      1.5         \n1     2.5          2.5        ...      2.5                 \n2     1.3          1.3        ...      1.3         \n"
            ],
            "answer": {
                "ans_desc": "Not sure, what you meant by messy but handles it pretty well here: you can pass to to make distinct column names. ",
                "code": [
                    "pd.concat([s for i in range(9)], axis=1)\n",
                    "keys=[f'A{i}' for i in range(9)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 247,
            "user_id": 8473002,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ccaad9925fcd1cd9fc7af944df8b7604?s=128&d=identicon&r=PG&f=1",
            "display_name": "arqchicago",
            "link": "https://stackoverflow.com/users/8473002/arqchicago"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 52155196,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1536002568,
        "creation_date": 1536001626,
        "last_edit_date": 1536002568,
        "question_id": 52155125,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52155125/rearranging-python-data-frame-index-and-columns",
        "title": "Rearranging python data frame index and columns",
        "body": "<p>I want to convert this dataframe (note that 'ABC' is the index name):</p>\n\n<pre><code>       t1    t2    t3 \nABC\ngp      7    11    26\nfp      6    14    23\npm      3    -1     7\nwm      2    -2     9\n</code></pre>\n\n<p>to this dataframe:</p>\n\n<pre><code>     s1   tx    gp   fp   pm   wm \n0   ABC   t1     7    6    3    2\n1   ABC   t2    11   14   -1   -2\n2   ABC   t3    26   23    7    9\n</code></pre>\n\n<p>What's the best way to perform this function?</p>\n",
        "answer_body": "<p>How about this:</p>\n\n<pre><code>df = df.T\ndf.reset_index(inplace=True)\ndf.rename(columns={\"index\":\"tx\"}, inplace=True)\ndf[\"s1\"] = \"ABC\"\n</code></pre>\n",
        "question_body": "<p>I want to convert this dataframe (note that 'ABC' is the index name):</p>\n\n<pre><code>       t1    t2    t3 \nABC\ngp      7    11    26\nfp      6    14    23\npm      3    -1     7\nwm      2    -2     9\n</code></pre>\n\n<p>to this dataframe:</p>\n\n<pre><code>     s1   tx    gp   fp   pm   wm \n0   ABC   t1     7    6    3    2\n1   ABC   t2    11   14   -1   -2\n2   ABC   t3    26   23    7    9\n</code></pre>\n\n<p>What's the best way to perform this function?</p>\n",
        "formatted_input": {
            "qid": 52155125,
            "link": "https://stackoverflow.com/questions/52155125/rearranging-python-data-frame-index-and-columns",
            "question": {
                "title": "Rearranging python data frame index and columns",
                "ques_desc": "I want to convert this dataframe (note that 'ABC' is the index name): to this dataframe: What's the best way to perform this function? "
            },
            "io": [
                "       t1    t2    t3 \nABC\ngp      7    11    26\nfp      6    14    23\npm      3    -1     7\nwm      2    -2     9\n",
                "     s1   tx    gp   fp   pm   wm \n0   ABC   t1     7    6    3    2\n1   ABC   t2    11   14   -1   -2\n2   ABC   t3    26   23    7    9\n"
            ],
            "answer": {
                "ans_desc": "How about this: ",
                "code": [
                    "df = df.T\ndf.reset_index(inplace=True)\ndf.rename(columns={\"index\":\"tx\"}, inplace=True)\ndf[\"s1\"] = \"ABC\"\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dictionary",
            "dataframe"
        ],
        "owner": {
            "reputation": 12928,
            "user_id": 4596596,
            "user_type": "registered",
            "accept_rate": 93,
            "profile_image": "https://www.gravatar.com/avatar/b00c839c9e28eb73aeb299bf3ef69a76?s=128&d=identicon&r=PG&f=1",
            "display_name": "ShanZhengYang",
            "link": "https://stackoverflow.com/users/4596596/shanzhengyang"
        },
        "is_answered": true,
        "view_count": 147,
        "accepted_answer_id": 52144040,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1535957351,
        "creation_date": 1535950007,
        "question_id": 52143288,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52143288/how-to-calculate-dictionaries-of-lists-using-pandas-dataframe",
        "title": "How to calculate dictionaries of lists using pandas DataFrame?",
        "body": "<p>I have two strings in Python3.x, which are defined to be the same length:</p>\n\n<pre><code>string1 = 'WGWFTSJKPGP'\nstring2 = 'DORKSRQKYJG'\n</code></pre>\n\n<p>I am also given an integer which is meant to represent the \"starting index\" of <code>string2</code>. In this case, <code>start_pos = 51</code>. </p>\n\n<p>The goal is to create a dictionary based on the indices. So, <code>string1</code> begins at <code>0</code>, <code>string2</code> begins at <code>51</code>. The dictionary \"converting\" these coordinates is as follows:</p>\n\n<pre><code>{0: 51, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61}\n</code></pre>\n\n<p>which can be constructed (give the variables above) with:</p>\n\n<pre><code>convert_dict = {i: i + start_pos for i, _ in enumerate(string1)}\n</code></pre>\n\n<p>I currently have this data in the form of a pandas DataFrame:</p>\n\n<pre><code>import pandas as pd\n\ndict1 = {'column1':['MXRBMVQDHF', 'LJNVTJOY', 'LJNVTJOY', 'LJNVTJOY', 'WHLAOECVQR'], 'column2':['DPBVNJYANX', 'UWRAWDOB', 'PEKUYUQR', 'WPMLFVFZ', 'CUTQVWHRIJ'], 'start':[79, 31, 52, 84, 18]}\n\ndf = pd.DataFrame(dict1)\nprint(df)\n#       column1     column2  start\n# 0  MXRBMVQDHF  DPBVNJYANX     79\n# 1    LJNVTJOY    UWRAWDOB     31\n# 2    LJNVTJOY    PEKUYUQR     52\n# 3    LJNVTJOY    WPMLFVFZ     84\n# 4  WHLAOECVQR  CUTQVWHRIJ     18\n</code></pre>\n\n<p>There are multiple entries of the same string in column <code>column1</code>. In this case, the dictionary for the coordinates with <code>LJNVTJOY</code> should be:</p>\n\n<pre><code>{0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], \n     4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]}\n</code></pre>\n\n<p>I would like to take this DataFrame and calculate similar dictionaries of the coordinates. Such a <code>.groupby('column1')</code> statement looks like one should somehow use <code>.apply()</code>? I'm not sure how to populate dictionary lists like this...</p>\n\n<p>Here is the correct output (keeping the DataFrame structure). Here the DataFrame <code>df2</code> has the column <code>'new_column'</code> such that it looks like the following:</p>\n\n<pre><code>df2.new_column\n0    {0: 79, 1: 80, 2: 81, 3: 82, 4: 83, 5: 84, 6: ...\n1    {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], 4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]}\n2    {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: ...\nName: new, dtype: object\n</code></pre>\n",
        "answer_body": "<p>Use - </p>\n\n<pre><code>def dict_op(x):\n    string1 = x['column1']\n    string2 = x['column2']\n    start_pos = x['start']\n    x['val'] = {i: i + start_pos for i, _ in enumerate(string1)}\n    return x\n\ndef zip_dict(x):\n    b=pd.DataFrame(x)\n    return {i:b.loc[:,i].tolist() for i in b.columns }\n\nop = df.apply(dict_op, axis=1).groupby('column1')['val'].apply(list).apply(zip_dict)\nprint(op)\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>column1\nLJNVTJOY      {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54,...\nMXRBMVQDHF    {0: [79], 1: [80], 2: [81], 3: [82], 4: [83], ...\nWHLAOECVQR    {0: [18], 1: [19], 2: [20], 3: [21], 4: [22], ...\nName: val, dtype: object\n</code></pre>\n\n<p><strong>Explanation</strong></p>\n\n<p>The <code>dict_op</code> reuses your code to create the dict for every row and then the <code>.apply(list)</code> zips the dicts together to form a list of dicts.</p>\n\n<p>The <code>zip_dict()</code> then creates the output <code>dict</code> out of the interim output.</p>\n\n<p>The last piece that I haven't included is the part where if the length of the list is 1 then you can include the first element only, taking the output from <code>{0: [79], 1: [80], 2: [81], 3: [82], 4: [83], ...</code> to <code>{0: 79, 1: 80, 2: 81, 3: 82, 4: 83, ...</code> </p>\n",
        "question_body": "<p>I have two strings in Python3.x, which are defined to be the same length:</p>\n\n<pre><code>string1 = 'WGWFTSJKPGP'\nstring2 = 'DORKSRQKYJG'\n</code></pre>\n\n<p>I am also given an integer which is meant to represent the \"starting index\" of <code>string2</code>. In this case, <code>start_pos = 51</code>. </p>\n\n<p>The goal is to create a dictionary based on the indices. So, <code>string1</code> begins at <code>0</code>, <code>string2</code> begins at <code>51</code>. The dictionary \"converting\" these coordinates is as follows:</p>\n\n<pre><code>{0: 51, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61}\n</code></pre>\n\n<p>which can be constructed (give the variables above) with:</p>\n\n<pre><code>convert_dict = {i: i + start_pos for i, _ in enumerate(string1)}\n</code></pre>\n\n<p>I currently have this data in the form of a pandas DataFrame:</p>\n\n<pre><code>import pandas as pd\n\ndict1 = {'column1':['MXRBMVQDHF', 'LJNVTJOY', 'LJNVTJOY', 'LJNVTJOY', 'WHLAOECVQR'], 'column2':['DPBVNJYANX', 'UWRAWDOB', 'PEKUYUQR', 'WPMLFVFZ', 'CUTQVWHRIJ'], 'start':[79, 31, 52, 84, 18]}\n\ndf = pd.DataFrame(dict1)\nprint(df)\n#       column1     column2  start\n# 0  MXRBMVQDHF  DPBVNJYANX     79\n# 1    LJNVTJOY    UWRAWDOB     31\n# 2    LJNVTJOY    PEKUYUQR     52\n# 3    LJNVTJOY    WPMLFVFZ     84\n# 4  WHLAOECVQR  CUTQVWHRIJ     18\n</code></pre>\n\n<p>There are multiple entries of the same string in column <code>column1</code>. In this case, the dictionary for the coordinates with <code>LJNVTJOY</code> should be:</p>\n\n<pre><code>{0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], \n     4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]}\n</code></pre>\n\n<p>I would like to take this DataFrame and calculate similar dictionaries of the coordinates. Such a <code>.groupby('column1')</code> statement looks like one should somehow use <code>.apply()</code>? I'm not sure how to populate dictionary lists like this...</p>\n\n<p>Here is the correct output (keeping the DataFrame structure). Here the DataFrame <code>df2</code> has the column <code>'new_column'</code> such that it looks like the following:</p>\n\n<pre><code>df2.new_column\n0    {0: 79, 1: 80, 2: 81, 3: 82, 4: 83, 5: 84, 6: ...\n1    {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], 4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]}\n2    {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: ...\nName: new, dtype: object\n</code></pre>\n",
        "formatted_input": {
            "qid": 52143288,
            "link": "https://stackoverflow.com/questions/52143288/how-to-calculate-dictionaries-of-lists-using-pandas-dataframe",
            "question": {
                "title": "How to calculate dictionaries of lists using pandas DataFrame?",
                "ques_desc": "I have two strings in Python3.x, which are defined to be the same length: I am also given an integer which is meant to represent the \"starting index\" of . In this case, . The goal is to create a dictionary based on the indices. So, begins at , begins at . The dictionary \"converting\" these coordinates is as follows: which can be constructed (give the variables above) with: I currently have this data in the form of a pandas DataFrame: There are multiple entries of the same string in column . In this case, the dictionary for the coordinates with should be: I would like to take this DataFrame and calculate similar dictionaries of the coordinates. Such a statement looks like one should somehow use ? I'm not sure how to populate dictionary lists like this... Here is the correct output (keeping the DataFrame structure). Here the DataFrame has the column such that it looks like the following: "
            },
            "io": [
                "{0: 51, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61}\n",
                "{0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], \n     4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]}\n"
            ],
            "answer": {
                "ans_desc": "Use - Output Explanation The reuses your code to create the dict for every row and then the zips the dicts together to form a list of dicts. The then creates the output out of the interim output. The last piece that I haven't included is the part where if the length of the list is 1 then you can include the first element only, taking the output from to ",
                "code": [
                    "column1\nLJNVTJOY      {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54,...\nMXRBMVQDHF    {0: [79], 1: [80], 2: [81], 3: [82], 4: [83], ...\nWHLAOECVQR    {0: [18], 1: [19], 2: [20], 3: [21], 4: [22], ...\nName: val, dtype: object\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 11,
            "user_id": 10299635,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-Cyf31Pvavtw/AAAAAAAAAAI/AAAAAAAABMc/0cnWvq_FZgM/photo.jpg?sz=128",
            "display_name": "Karthikai Vasan",
            "link": "https://stackoverflow.com/users/10299635/karthikai-vasan"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 52114505,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1535815711,
        "creation_date": 1535712535,
        "last_edit_date": 1535815419,
        "question_id": 52113834,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52113834/conditional-removal-of-duplicates-entries-in-pandas",
        "title": "Conditional removal of duplicates entries in Pandas",
        "body": "<p>How can I remove the duplicate entries in the Pandas DataFrame given below.</p>\n\n<pre><code>a   b   c   d\n11216   08-08-2018  2000    SIP\n40277   28-08-2018  1000    SIP\n44165   02-08-2018  8000    Lump\n44165   03-08-2018  5000    Lump\n45845   16-08-2018  25000   Lump\n45845   18-08-2018  50000   Lump\n52730   13-08-2018  10000   Lump\n52730   27-08-2018  10000   Lump\n53390   20-08-2018  400000  Lump\n56180   02-08-2018  1000    Lump\n58537   11-07-2018  5000    Lump\n58537   22-08-2018  2000    SIP\n912813  15-08-2018  160001  Lump\n912813  15-08-2018  6000    SIP\n85606   16-08-2018  3500    SIP\n88327   06-08-2018  5000    SIP\n90240   07-08-2018  2000    SIP\n</code></pre>\n\n<p>Desired result:</p>\n\n<pre><code>a   b   c   d\n11216   08-08-2018  2000    SIP\n40277   28-08-2018  1000    SIP\n44165   02-08-2018  8000    Lump\n45845   16-08-2018  25000   Lump\n52730   13-08-2018  10000   Lump\n53390   20-08-2018  400000  Lump\n58537   11-07-2018  5000    Lump\n912813  15-08-2018  160001  Lump\n912813  15-08-2018  6000    SIP\n85606   16-08-2018  3500    SIP\n88327   06-08-2018  5000    SIP\n90240   07-08-2018  2000    SIP\n</code></pre>\n\n<p>The condition is: remove if <code>a2==a1</code> and <code>b2&lt;&gt;b1</code>.</p>\n",
        "answer_body": "<p>First you need to add them to a list and then this code can remove the duplicated items with your conditions.  </p>\n\n<pre><code>i = 0 \nwhile i &lt; len(a)-1 :\n    if a[i] == a[i+1] and if b[i] != b[s] :\n        del a[i]\n        del b[i]\n        del c[i]\n        del d[i]\n        i -= 1 \n    i += 1 \n</code></pre>\n",
        "question_body": "<p>How can I remove the duplicate entries in the Pandas DataFrame given below.</p>\n\n<pre><code>a   b   c   d\n11216   08-08-2018  2000    SIP\n40277   28-08-2018  1000    SIP\n44165   02-08-2018  8000    Lump\n44165   03-08-2018  5000    Lump\n45845   16-08-2018  25000   Lump\n45845   18-08-2018  50000   Lump\n52730   13-08-2018  10000   Lump\n52730   27-08-2018  10000   Lump\n53390   20-08-2018  400000  Lump\n56180   02-08-2018  1000    Lump\n58537   11-07-2018  5000    Lump\n58537   22-08-2018  2000    SIP\n912813  15-08-2018  160001  Lump\n912813  15-08-2018  6000    SIP\n85606   16-08-2018  3500    SIP\n88327   06-08-2018  5000    SIP\n90240   07-08-2018  2000    SIP\n</code></pre>\n\n<p>Desired result:</p>\n\n<pre><code>a   b   c   d\n11216   08-08-2018  2000    SIP\n40277   28-08-2018  1000    SIP\n44165   02-08-2018  8000    Lump\n45845   16-08-2018  25000   Lump\n52730   13-08-2018  10000   Lump\n53390   20-08-2018  400000  Lump\n58537   11-07-2018  5000    Lump\n912813  15-08-2018  160001  Lump\n912813  15-08-2018  6000    SIP\n85606   16-08-2018  3500    SIP\n88327   06-08-2018  5000    SIP\n90240   07-08-2018  2000    SIP\n</code></pre>\n\n<p>The condition is: remove if <code>a2==a1</code> and <code>b2&lt;&gt;b1</code>.</p>\n",
        "formatted_input": {
            "qid": 52113834,
            "link": "https://stackoverflow.com/questions/52113834/conditional-removal-of-duplicates-entries-in-pandas",
            "question": {
                "title": "Conditional removal of duplicates entries in Pandas",
                "ques_desc": "How can I remove the duplicate entries in the Pandas DataFrame given below. Desired result: The condition is: remove if and . "
            },
            "io": [
                "a   b   c   d\n11216   08-08-2018  2000    SIP\n40277   28-08-2018  1000    SIP\n44165   02-08-2018  8000    Lump\n44165   03-08-2018  5000    Lump\n45845   16-08-2018  25000   Lump\n45845   18-08-2018  50000   Lump\n52730   13-08-2018  10000   Lump\n52730   27-08-2018  10000   Lump\n53390   20-08-2018  400000  Lump\n56180   02-08-2018  1000    Lump\n58537   11-07-2018  5000    Lump\n58537   22-08-2018  2000    SIP\n912813  15-08-2018  160001  Lump\n912813  15-08-2018  6000    SIP\n85606   16-08-2018  3500    SIP\n88327   06-08-2018  5000    SIP\n90240   07-08-2018  2000    SIP\n",
                "a   b   c   d\n11216   08-08-2018  2000    SIP\n40277   28-08-2018  1000    SIP\n44165   02-08-2018  8000    Lump\n45845   16-08-2018  25000   Lump\n52730   13-08-2018  10000   Lump\n53390   20-08-2018  400000  Lump\n58537   11-07-2018  5000    Lump\n912813  15-08-2018  160001  Lump\n912813  15-08-2018  6000    SIP\n85606   16-08-2018  3500    SIP\n88327   06-08-2018  5000    SIP\n90240   07-08-2018  2000    SIP\n"
            ],
            "answer": {
                "ans_desc": "First you need to add them to a list and then this code can remove the duplicated items with your conditions. ",
                "code": [
                    "i = 0 \nwhile i < len(a)-1 :\n    if a[i] == a[i+1] and if b[i] != b[s] :\n        del a[i]\n        del b[i]\n        del c[i]\n        del d[i]\n        i -= 1 \n    i += 1 \n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 400,
            "user_id": 7859096,
            "user_type": "registered",
            "accept_rate": 69,
            "profile_image": "https://lh3.googleusercontent.com/-p2JSfLd_OtQ/AAAAAAAAAAI/AAAAAAAAABI/UJhDkT-Ahus/photo.jpg?sz=128",
            "display_name": "Jonathan Pacheco",
            "link": "https://stackoverflow.com/users/7859096/jonathan-pacheco"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 52118572,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1535730360,
        "creation_date": 1535727941,
        "last_edit_date": 1535728436,
        "question_id": 52118251,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52118251/easy-tool-to-filtering-columns-with-specific-conditions-using-pandas",
        "title": "easy tool to filtering columns with specific conditions using pandas",
        "body": "<p>I'm wondering if exist a tool in python to filter data between columns that follow an specific condition. I need to generate a clean dataframe where all the data in column 'A' must have the same consecutive number in column 'E'(and this number is repeated at least twice). Here an example:</p>\n\n<pre><code>df\nOut[30]: \n         A         B           C          D           E\n6        1       2.366       8.621      10.835        1\n7        1       2.489       8.586      10.890        2\n8        1       2.279       8.460      10.945        2\n9        1       2.296       8.559      11.000        2\n10       2       2.275       8.620      11.055        2\n11       2       2.539       8.528      11.110        2\n50       2      3.346       5.979      10.175         5\n51       3       3.359       5.910      10.230        1\n52       3       3.416       5.936      10.285        1 \n</code></pre>\n\n<p>The output will be:</p>\n\n<pre><code>df\nOut[31]: \n         A         B           C          D           E\n7        1       2.489       8.586      10.890        2\n8        1       2.279       8.460      10.945        2\n9        1       2.296       8.559      11.000        2\n10       2       2.275       8.620      11.055        2\n11       2       2.539       8.528      11.110        2\n51       3       3.359       5.910      10.230        1\n52       3       3.416       5.936      10.285        1 \n</code></pre>\n",
        "answer_body": "<p>What you are looking for is:</p>\n\n<pre><code>import numpy as np\n\ndf.groupby((df.E != df.E.shift(1)).cumsum()).filter(lambda x: np.size(x.E) &gt;= 2)\n# or\ndf[df.groupby((df.E != df.E.shift(1)).cumsum()).E.transform('size') &gt;= 2]\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>    A      B      C       D  E\n7   1  2.489  8.586  10.890  2\n8   1  2.279  8.460  10.945  2\n9   1  2.296  8.559  11.000  2\n10  2  2.275  8.620  11.055  2\n11  2  2.539  8.528  11.110  2\n51  3  3.359  5.910  10.230  1\n52  3  3.416  5.936  10.285  1\n</code></pre>\n\n<p>Explanation:</p>\n\n<p>You want to keep all records where there is a consecutive group in <code>E</code> which has a size of more than 2.</p>\n\n<p>The first part <code>(df.E != df.E.shift(1)).cumsum()</code> allows you to label consecutive groups in column <code>E</code>, and then you group by that label and filter the <code>DataFrame</code>, keeping only groups where the size is 2 or more. </p>\n",
        "question_body": "<p>I'm wondering if exist a tool in python to filter data between columns that follow an specific condition. I need to generate a clean dataframe where all the data in column 'A' must have the same consecutive number in column 'E'(and this number is repeated at least twice). Here an example:</p>\n\n<pre><code>df\nOut[30]: \n         A         B           C          D           E\n6        1       2.366       8.621      10.835        1\n7        1       2.489       8.586      10.890        2\n8        1       2.279       8.460      10.945        2\n9        1       2.296       8.559      11.000        2\n10       2       2.275       8.620      11.055        2\n11       2       2.539       8.528      11.110        2\n50       2      3.346       5.979      10.175         5\n51       3       3.359       5.910      10.230        1\n52       3       3.416       5.936      10.285        1 \n</code></pre>\n\n<p>The output will be:</p>\n\n<pre><code>df\nOut[31]: \n         A         B           C          D           E\n7        1       2.489       8.586      10.890        2\n8        1       2.279       8.460      10.945        2\n9        1       2.296       8.559      11.000        2\n10       2       2.275       8.620      11.055        2\n11       2       2.539       8.528      11.110        2\n51       3       3.359       5.910      10.230        1\n52       3       3.416       5.936      10.285        1 \n</code></pre>\n",
        "formatted_input": {
            "qid": 52118251,
            "link": "https://stackoverflow.com/questions/52118251/easy-tool-to-filtering-columns-with-specific-conditions-using-pandas",
            "question": {
                "title": "easy tool to filtering columns with specific conditions using pandas",
                "ques_desc": "I'm wondering if exist a tool in python to filter data between columns that follow an specific condition. I need to generate a clean dataframe where all the data in column 'A' must have the same consecutive number in column 'E'(and this number is repeated at least twice). Here an example: The output will be: "
            },
            "io": [
                "df\nOut[30]: \n         A         B           C          D           E\n6        1       2.366       8.621      10.835        1\n7        1       2.489       8.586      10.890        2\n8        1       2.279       8.460      10.945        2\n9        1       2.296       8.559      11.000        2\n10       2       2.275       8.620      11.055        2\n11       2       2.539       8.528      11.110        2\n50       2      3.346       5.979      10.175         5\n51       3       3.359       5.910      10.230        1\n52       3       3.416       5.936      10.285        1 \n",
                "df\nOut[31]: \n         A         B           C          D           E\n7        1       2.489       8.586      10.890        2\n8        1       2.279       8.460      10.945        2\n9        1       2.296       8.559      11.000        2\n10       2       2.275       8.620      11.055        2\n11       2       2.539       8.528      11.110        2\n51       3       3.359       5.910      10.230        1\n52       3       3.416       5.936      10.285        1 \n"
            ],
            "answer": {
                "ans_desc": "What you are looking for is: Output: Explanation: You want to keep all records where there is a consecutive group in which has a size of more than 2. The first part allows you to label consecutive groups in column , and then you group by that label and filter the , keeping only groups where the size is 2 or more. ",
                "code": [
                    "import numpy as np\n\ndf.groupby((df.E != df.E.shift(1)).cumsum()).filter(lambda x: np.size(x.E) >= 2)\n# or\ndf[df.groupby((df.E != df.E.shift(1)).cumsum()).E.transform('size') >= 2]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3443,
            "user_id": 7607701,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://lh3.googleusercontent.com/-0eLxeTB_SdQ/AAAAAAAAAAI/AAAAAAAAABg/XZT1oaa0wqA/photo.jpg?sz=128",
            "display_name": "Aaron N. Brock",
            "link": "https://stackoverflow.com/users/7607701/aaron-n-brock"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 52086279,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1535581109,
        "creation_date": 1535572649,
        "question_id": 52085200,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52085200/in-dataframe-of-strings-and-floats-cast-floats-to-integer-then-string",
        "title": "In dataframe of strings AND floats, cast floats to integer then string",
        "body": "<p>I have the following dataframe of dtype <code>object</code>:</p>\n\n<pre><code>  col1 col2  col3\n0  1.1  3.3  spam\n1  2.2  foo  eggs\n2  bar  4.4   5.5\n</code></pre>\n\n<p>I would like to cast all the floats to integers then convert everything to strings, so the output would be of dtype <code>string</code>: </p>\n\n<pre><code>  col1 col2  col3\n0    1    3  spam\n1    2  foo  eggs\n2  bar    4     5\n</code></pre>\n\n<p>Is there something that allows me to cast a dataframe to an int but ignore errors?  Or achieve this in a different way?  (using the <code>errors = 'ignore'</code>, seems to ignore the entire thing)</p>\n",
        "answer_body": "<p>You can use a helper function that:</p>\n\n<ul>\n<li>tries to convert what's in your object to a <code>float</code> - so \"2.5\" and \"2\" will be able to be translated (as well as anything that Python's <code>float</code> function can interpret as a <code>float</code> value), but \"hello there. how are you?\" won't...</li>\n<li>then tries to convert that <code>float</code> to an <code>int</code></li>\n<li>then returns its <code>str</code> value</li>\n</ul>\n\n<p>If the <code>float</code> conversion fails - then it'll just return your original object as technically only the <code>float</code> conversation can fail, as if that succeeds you can always <code>int(some_float)</code> and <code>str</code> will always work (failing some weird custom class - that deliberately causes it to fail).</p>\n\n<p>eg:</p>\n\n<pre><code>def try_to_int(obj):\n    try:\n        return str(int(float(obj)))\n    except (ValueError, TypeError):\n        return obj\n</code></pre>\n\n<p>Then use it with <code>new_df = df.applymap(try_to_int)</code></p>\n",
        "question_body": "<p>I have the following dataframe of dtype <code>object</code>:</p>\n\n<pre><code>  col1 col2  col3\n0  1.1  3.3  spam\n1  2.2  foo  eggs\n2  bar  4.4   5.5\n</code></pre>\n\n<p>I would like to cast all the floats to integers then convert everything to strings, so the output would be of dtype <code>string</code>: </p>\n\n<pre><code>  col1 col2  col3\n0    1    3  spam\n1    2  foo  eggs\n2  bar    4     5\n</code></pre>\n\n<p>Is there something that allows me to cast a dataframe to an int but ignore errors?  Or achieve this in a different way?  (using the <code>errors = 'ignore'</code>, seems to ignore the entire thing)</p>\n",
        "formatted_input": {
            "qid": 52085200,
            "link": "https://stackoverflow.com/questions/52085200/in-dataframe-of-strings-and-floats-cast-floats-to-integer-then-string",
            "question": {
                "title": "In dataframe of strings AND floats, cast floats to integer then string",
                "ques_desc": "I have the following dataframe of dtype : I would like to cast all the floats to integers then convert everything to strings, so the output would be of dtype : Is there something that allows me to cast a dataframe to an int but ignore errors? Or achieve this in a different way? (using the , seems to ignore the entire thing) "
            },
            "io": [
                "  col1 col2  col3\n0  1.1  3.3  spam\n1  2.2  foo  eggs\n2  bar  4.4   5.5\n",
                "  col1 col2  col3\n0    1    3  spam\n1    2  foo  eggs\n2  bar    4     5\n"
            ],
            "answer": {
                "ans_desc": "You can use a helper function that: tries to convert what's in your object to a - so \"2.5\" and \"2\" will be able to be translated (as well as anything that Python's function can interpret as a value), but \"hello there. how are you?\" won't... then tries to convert that to an then returns its value If the conversion fails - then it'll just return your original object as technically only the conversation can fail, as if that succeeds you can always and will always work (failing some weird custom class - that deliberately causes it to fail). eg: Then use it with ",
                "code": [
                    "def try_to_int(obj):\n    try:\n        return str(int(float(obj)))\n    except (ValueError, TypeError):\n        return obj\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "lambda",
            "apply"
        ],
        "owner": {
            "reputation": 609,
            "user_id": 7396306,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/033bc19f1f9658adc87f877fcfa29bf5?s=128&d=identicon&r=PG&f=1",
            "display_name": "DrakeMurdoch",
            "link": "https://stackoverflow.com/users/7396306/drakemurdoch"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 52064386,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1535481853,
        "creation_date": 1535481376,
        "last_edit_date": 1535481853,
        "question_id": 52064324,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/52064324/add-a-different-item-from-a-list-to-each-cell-in-a-dataframe-with-pandas",
        "title": "Add a different item from a list to each cell in a dataframe with Pandas",
        "body": "<p>Given a dataframe</p>\n\n<pre><code>df = pd.DataFrame(np.random.randint(0,100,size=(15, 4)), columns=list('ABCD'))\n</code></pre>\n\n<p>I want to make a list of sequential that has as many elements as there are rows in the dataframe</p>\n\n<pre><code>l = [i for i in range(df['A'].count())]\n</code></pre>\n\n<p>And then add each element of the list (as a string) onto the end of a single column in the dataframe.</p>\n\n<pre><code>df['A'] = df['A'].apply(lambda x: str(x) + str(l[i]))\n</code></pre>\n\n<p>Does not work, as it adds the whole list as a string to each value as opposed to one value of the list per value in the dataframe.</p>\n\n<p>Essentially, I want to transform</p>\n\n<pre><code>A    B    C\n23   16   85\n 9   74   12\n99   24   83\n</code></pre>\n\n<p>to</p>\n\n<pre><code>A     B    C\n231   16   85\n 92   74   12\n993   24   83  \n</code></pre>\n\n<p>So anyway to do that would be fine. </p>\n\n<p>Thanks for the help!</p>\n",
        "answer_body": "<p>Try:</p>\n\n<pre><code>df['A'] = df.A.astype(str) + (df.index+1).astype(str)\n\n&gt;&gt;&gt; df\n     A   B   C\n0  231  16  85\n1   92  74  12\n2  993  24  83\n</code></pre>\n\n<p>That's assuming your dataframe has a regular <code>RangeIndex</code>. If not, you can use:</p>\n\n<pre><code>df['A'] = df.A.astype(str) + (df.reset_index().index + 1).astype(str)\n</code></pre>\n\n<p>Or </p>\n\n<pre><code>df['A'] = df.A.astype(str) + (pd.RangeIndex(len(df)) + 1).astype(str)\n</code></pre>\n",
        "question_body": "<p>Given a dataframe</p>\n\n<pre><code>df = pd.DataFrame(np.random.randint(0,100,size=(15, 4)), columns=list('ABCD'))\n</code></pre>\n\n<p>I want to make a list of sequential that has as many elements as there are rows in the dataframe</p>\n\n<pre><code>l = [i for i in range(df['A'].count())]\n</code></pre>\n\n<p>And then add each element of the list (as a string) onto the end of a single column in the dataframe.</p>\n\n<pre><code>df['A'] = df['A'].apply(lambda x: str(x) + str(l[i]))\n</code></pre>\n\n<p>Does not work, as it adds the whole list as a string to each value as opposed to one value of the list per value in the dataframe.</p>\n\n<p>Essentially, I want to transform</p>\n\n<pre><code>A    B    C\n23   16   85\n 9   74   12\n99   24   83\n</code></pre>\n\n<p>to</p>\n\n<pre><code>A     B    C\n231   16   85\n 92   74   12\n993   24   83  \n</code></pre>\n\n<p>So anyway to do that would be fine. </p>\n\n<p>Thanks for the help!</p>\n",
        "formatted_input": {
            "qid": 52064324,
            "link": "https://stackoverflow.com/questions/52064324/add-a-different-item-from-a-list-to-each-cell-in-a-dataframe-with-pandas",
            "question": {
                "title": "Add a different item from a list to each cell in a dataframe with Pandas",
                "ques_desc": "Given a dataframe I want to make a list of sequential that has as many elements as there are rows in the dataframe And then add each element of the list (as a string) onto the end of a single column in the dataframe. Does not work, as it adds the whole list as a string to each value as opposed to one value of the list per value in the dataframe. Essentially, I want to transform to So anyway to do that would be fine. Thanks for the help! "
            },
            "io": [
                "A    B    C\n23   16   85\n 9   74   12\n99   24   83\n",
                "A     B    C\n231   16   85\n 92   74   12\n993   24   83  \n"
            ],
            "answer": {
                "ans_desc": "Try: That's assuming your dataframe has a regular . If not, you can use: Or ",
                "code": [
                    "df['A'] = df.A.astype(str) + (df.reset_index().index + 1).astype(str)\n",
                    "df['A'] = df.A.astype(str) + (pd.RangeIndex(len(df)) + 1).astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 21,
            "user_id": 10268792,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ba7ff85e0e2054dbe36f91cccf1100e0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Amoxz",
            "link": "https://stackoverflow.com/users/10268792/amoxz"
        },
        "is_answered": true,
        "view_count": 95,
        "closed_date": 1535350804,
        "accepted_answer_id": 52033393,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1535350284,
        "creation_date": 1535349772,
        "last_edit_date": 1535350284,
        "question_id": 52033359,
        "link": "https://stackoverflow.com/questions/52033359/transform-a-large-dataframe-takes-too-long",
        "closed_reason": "Duplicate",
        "title": "Transform a large dataframe - takes too long",
        "body": "<p>I have a dataframe loaded from a CSV in the following format: </p>\n\n<pre><code>           stock_code    price \n20180827     001          10\n20180827     002          11\n20180827     003          12\n20180827     004          13\n20180826     001          14\n20180826     002          15\n20180826     003          11\n20180826     004          10\n20180826     005          19\n</code></pre>\n\n<p>I want to transform it to the following format: </p>\n\n<pre><code>            001     002     003     004     005\n20180827    10      11      12      13      nan\n20180826    14      15      11      10      19\n</code></pre>\n\n<p>This is my function ( <code>oracle_data</code> is the original data frame) that does the transformation, but it takes 7 minutes for 547500 row dataframe. Is there a way to speed it up?</p>\n\n<pre><code>def transform_data(oracle_data):\n    data_code = oracle_data[0]  \n    data_date = oracle_data[1] \n    factor_date = sorted(data_date.unique()) \n    stock_list =  sorted(data_code.unique())     \n    factor_data = pd.DataFrame(index = factor_date, columns = stock_list)\n    sort = oracle_data.sort_index()\n    for n in oracle_data.index:\n        factor_data.at[oracle_data.at[n,1],oracle_data.at[n,0]]=oracle_data.at[n,2]\n    return factor_data\n</code></pre>\n",
        "answer_body": "<p>I believe here is possible use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot.html\" rel=\"nofollow noreferrer\"><code>pandas.pivot</code></a>:</p>\n\n<pre><code>df = pd.pivot(df.index, df['stock_code'], df['price'])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe loaded from a CSV in the following format: </p>\n\n<pre><code>           stock_code    price \n20180827     001          10\n20180827     002          11\n20180827     003          12\n20180827     004          13\n20180826     001          14\n20180826     002          15\n20180826     003          11\n20180826     004          10\n20180826     005          19\n</code></pre>\n\n<p>I want to transform it to the following format: </p>\n\n<pre><code>            001     002     003     004     005\n20180827    10      11      12      13      nan\n20180826    14      15      11      10      19\n</code></pre>\n\n<p>This is my function ( <code>oracle_data</code> is the original data frame) that does the transformation, but it takes 7 minutes for 547500 row dataframe. Is there a way to speed it up?</p>\n\n<pre><code>def transform_data(oracle_data):\n    data_code = oracle_data[0]  \n    data_date = oracle_data[1] \n    factor_date = sorted(data_date.unique()) \n    stock_list =  sorted(data_code.unique())     \n    factor_data = pd.DataFrame(index = factor_date, columns = stock_list)\n    sort = oracle_data.sort_index()\n    for n in oracle_data.index:\n        factor_data.at[oracle_data.at[n,1],oracle_data.at[n,0]]=oracle_data.at[n,2]\n    return factor_data\n</code></pre>\n",
        "formatted_input": {
            "qid": 52033359,
            "link": "https://stackoverflow.com/questions/52033359/transform-a-large-dataframe-takes-too-long",
            "question": {
                "title": "Transform a large dataframe - takes too long",
                "ques_desc": "I have a dataframe loaded from a CSV in the following format: I want to transform it to the following format: This is my function ( is the original data frame) that does the transformation, but it takes 7 minutes for 547500 row dataframe. Is there a way to speed it up? "
            },
            "io": [
                "           stock_code    price \n20180827     001          10\n20180827     002          11\n20180827     003          12\n20180827     004          13\n20180826     001          14\n20180826     002          15\n20180826     003          11\n20180826     004          10\n20180826     005          19\n",
                "            001     002     003     004     005\n20180827    10      11      12      13      nan\n20180826    14      15      11      10      19\n"
            ],
            "answer": {
                "ans_desc": "I believe here is possible use : ",
                "code": [
                    "df = pd.pivot(df.index, df['stock_code'], df['price'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matplotlib",
            "legend"
        ],
        "owner": {
            "reputation": 307,
            "user_id": 10113213,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-_HkX7M2jWzM/AAAAAAAAAAI/AAAAAAAAAeY/sO4p6-w4xEI/photo.jpg?sz=128",
            "display_name": "Bhanu Tez",
            "link": "https://stackoverflow.com/users/10113213/bhanu-tez"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 51903335,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1534543777,
        "creation_date": 1534540011,
        "last_edit_date": 1534540578,
        "question_id": 51902887,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51902887/python-subplot-plot-bar-from-one-dataframe-and-legend-from-a-different-dataframe",
        "title": "python subplot plot.bar from one dataframe and legend from a different dataframe",
        "body": "<p>I have two data sets below</p>\n\n<p>Df1:</p>\n\n<pre><code>    Cluster     HPE     FRE     UNE\n0        0  176617  255282   55881\n1        1  126130    7752  252045\n2        2   12613   52326    7434\n</code></pre>\n\n<p>I draw a bar diagram. (This is not an exact code of mine, but it will give you an idea)</p>\n\n<pre><code>Hd=list(Df1.columns)\nfor i in range(1,4):\n  subp=Fig.add_subplot(3,1,i) \n  plt.bar(Df1[Hd[0]],DataFrame[Hd[i]],width=0.4)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/GyXQV.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/GyXQV.png\" alt=\"df1.bar plot\"></a></p>\n\n<p>Now I want a legend based on a second data set of centroids.</p>\n\n<p>Df2:</p>\n\n<pre><code>   Cluster          HPE         FRE          UNE\n0        0    19.282091  106.470162  1620.005037\n1        1  1790.500000  367.625000   537.856177\n2        2  1500.000000  180.148148  4729.275913\n</code></pre>\n\n<p>HPE subplot should have HPE column values (19.282091,1790.500000,1500.000000)\nas below.</p>\n\n<p><a href=\"https://i.stack.imgur.com/0Kwfq.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/0Kwfq.png\" alt=\"The requirement\"></a></p>\n\n<p>How can I do that?</p>\n",
        "answer_body": "<p>Explicitly set the legend to strings obtained from second dataframe (if you want the color boxes of the bars): </p>\n\n<pre><code>subp.legend([str(a) + ' - ' + str(b) for a, b in zip(df2['Cluster'].tolist(), df2['HPE'].tolist())])\n</code></pre>\n\n<p>Or just use a table:</p>\n\n<pre><code>plt.table(cellText=df2[['Cluster', 'HPE']].values, loc='upper right') \n</code></pre>\n",
        "question_body": "<p>I have two data sets below</p>\n\n<p>Df1:</p>\n\n<pre><code>    Cluster     HPE     FRE     UNE\n0        0  176617  255282   55881\n1        1  126130    7752  252045\n2        2   12613   52326    7434\n</code></pre>\n\n<p>I draw a bar diagram. (This is not an exact code of mine, but it will give you an idea)</p>\n\n<pre><code>Hd=list(Df1.columns)\nfor i in range(1,4):\n  subp=Fig.add_subplot(3,1,i) \n  plt.bar(Df1[Hd[0]],DataFrame[Hd[i]],width=0.4)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/GyXQV.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/GyXQV.png\" alt=\"df1.bar plot\"></a></p>\n\n<p>Now I want a legend based on a second data set of centroids.</p>\n\n<p>Df2:</p>\n\n<pre><code>   Cluster          HPE         FRE          UNE\n0        0    19.282091  106.470162  1620.005037\n1        1  1790.500000  367.625000   537.856177\n2        2  1500.000000  180.148148  4729.275913\n</code></pre>\n\n<p>HPE subplot should have HPE column values (19.282091,1790.500000,1500.000000)\nas below.</p>\n\n<p><a href=\"https://i.stack.imgur.com/0Kwfq.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/0Kwfq.png\" alt=\"The requirement\"></a></p>\n\n<p>How can I do that?</p>\n",
        "formatted_input": {
            "qid": 51902887,
            "link": "https://stackoverflow.com/questions/51902887/python-subplot-plot-bar-from-one-dataframe-and-legend-from-a-different-dataframe",
            "question": {
                "title": "python subplot plot.bar from one dataframe and legend from a different dataframe",
                "ques_desc": "I have two data sets below Df1: I draw a bar diagram. (This is not an exact code of mine, but it will give you an idea) Now I want a legend based on a second data set of centroids. Df2: HPE subplot should have HPE column values (19.282091,1790.500000,1500.000000) as below. How can I do that? "
            },
            "io": [
                "    Cluster     HPE     FRE     UNE\n0        0  176617  255282   55881\n1        1  126130    7752  252045\n2        2   12613   52326    7434\n",
                "   Cluster          HPE         FRE          UNE\n0        0    19.282091  106.470162  1620.005037\n1        1  1790.500000  367.625000   537.856177\n2        2  1500.000000  180.148148  4729.275913\n"
            ],
            "answer": {
                "ans_desc": "Explicitly set the legend to strings obtained from second dataframe (if you want the color boxes of the bars): Or just use a table: ",
                "code": [
                    "subp.legend([str(a) + ' - ' + str(b) for a, b in zip(df2['Cluster'].tolist(), df2['HPE'].tolist())])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 121,
            "user_id": 6316512,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/80a16a9af7c09c87315445815ebbb6fc?s=128&d=identicon&r=PG&f=1",
            "display_name": "sindhuja",
            "link": "https://stackoverflow.com/users/6316512/sindhuja"
        },
        "is_answered": true,
        "view_count": 66,
        "accepted_answer_id": 51872414,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1534406912,
        "creation_date": 1534405838,
        "last_edit_date": 1534406770,
        "question_id": 51872125,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51872125/creating-list-from-dataframe",
        "title": "creating list from dataframe",
        "body": "<p>I am a newbie to python. I am trying iterate over rows of individual columns of a dataframe in python. I am trying to create an adjacency list using the first two columns of the dataframe taken from csv data (which has 3 columns). </p>\n\n<p>The following is the code to iterate over the dataframe and create a dictionary for adjacency list:</p>\n\n<pre><code>df1 = pd.read_csv('person_knows_person_0_0_sample.csv', sep=',', index_col=False, skiprows=1) \n\nsrc_list = list(df1.iloc[:, 0:1])\ntgt_list = list(df1.iloc[:, 1:2])\n    adj_list = {}\n\n    for src in src_list:\n        for tgt in tgt_list:\n            adj_list[src] = tgt\n\n\n    print(src_list) \n    print(tgt_list)\n    print(adj_list)\n</code></pre>\n\n<p>and the following is the output I am getting:</p>\n\n<pre><code>['933']\n['4139']\n{'933': '4139'}\n</code></pre>\n\n<p>I see that I am not getting the entire list when I use the <code>list()</code> constructor.\nHence I am not able to loop over the entire data.</p>\n\n<p>Could anyone tell me where I am going wrong?</p>\n\n<p>To summarize, Here is the input data: </p>\n\n<pre><code>A,B,C\n933,4139,20100313073721718\n933,6597069777240,20100920094243187\n933,10995116284808,20110102064341955\n933,32985348833579,20120907011130195\n933,32985348838375,20120717080449463\n1129,1242,20100202163844119\n1129,2199023262543,20100331220757321\n1129,6597069771886,20100724111548162\n1129,6597069776731,20100804033836982\n</code></pre>\n\n<p>the output that I am expecting: </p>\n\n<pre><code>933: [4139,6597069777240, 10995116284808, 32985348833579, 32985348838375]\n1129: [1242, 2199023262543, 6597069771886, 6597069776731]\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and create <code>Series</code> of <code>list</code>s and then <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.to_dict.html\" rel=\"nofollow noreferrer\"><code>to_dict</code></a>:</p>\n\n<pre><code>#selecting by columns names\nd = df1.groupby('A')['B'].apply(list).to_dict()\n\n#seelcting columns by positions\nd = df1.iloc[:, 1].groupby(df1.iloc[:, 0]).apply(list).to_dict()\n</code></pre>\n\n<hr>\n\n<pre><code>print (d)\n{933: [4139, 6597069777240, 10995116284808, 32985348833579, 32985348838375],\n 1129: [1242, 2199023262543, 6597069771886, 6597069776731]}\n</code></pre>\n",
        "question_body": "<p>I am a newbie to python. I am trying iterate over rows of individual columns of a dataframe in python. I am trying to create an adjacency list using the first two columns of the dataframe taken from csv data (which has 3 columns). </p>\n\n<p>The following is the code to iterate over the dataframe and create a dictionary for adjacency list:</p>\n\n<pre><code>df1 = pd.read_csv('person_knows_person_0_0_sample.csv', sep=',', index_col=False, skiprows=1) \n\nsrc_list = list(df1.iloc[:, 0:1])\ntgt_list = list(df1.iloc[:, 1:2])\n    adj_list = {}\n\n    for src in src_list:\n        for tgt in tgt_list:\n            adj_list[src] = tgt\n\n\n    print(src_list) \n    print(tgt_list)\n    print(adj_list)\n</code></pre>\n\n<p>and the following is the output I am getting:</p>\n\n<pre><code>['933']\n['4139']\n{'933': '4139'}\n</code></pre>\n\n<p>I see that I am not getting the entire list when I use the <code>list()</code> constructor.\nHence I am not able to loop over the entire data.</p>\n\n<p>Could anyone tell me where I am going wrong?</p>\n\n<p>To summarize, Here is the input data: </p>\n\n<pre><code>A,B,C\n933,4139,20100313073721718\n933,6597069777240,20100920094243187\n933,10995116284808,20110102064341955\n933,32985348833579,20120907011130195\n933,32985348838375,20120717080449463\n1129,1242,20100202163844119\n1129,2199023262543,20100331220757321\n1129,6597069771886,20100724111548162\n1129,6597069776731,20100804033836982\n</code></pre>\n\n<p>the output that I am expecting: </p>\n\n<pre><code>933: [4139,6597069777240, 10995116284808, 32985348833579, 32985348838375]\n1129: [1242, 2199023262543, 6597069771886, 6597069776731]\n</code></pre>\n",
        "formatted_input": {
            "qid": 51872125,
            "link": "https://stackoverflow.com/questions/51872125/creating-list-from-dataframe",
            "question": {
                "title": "creating list from dataframe",
                "ques_desc": "I am a newbie to python. I am trying iterate over rows of individual columns of a dataframe in python. I am trying to create an adjacency list using the first two columns of the dataframe taken from csv data (which has 3 columns). The following is the code to iterate over the dataframe and create a dictionary for adjacency list: and the following is the output I am getting: I see that I am not getting the entire list when I use the constructor. Hence I am not able to loop over the entire data. Could anyone tell me where I am going wrong? To summarize, Here is the input data: the output that I am expecting: "
            },
            "io": [
                "A,B,C\n933,4139,20100313073721718\n933,6597069777240,20100920094243187\n933,10995116284808,20110102064341955\n933,32985348833579,20120907011130195\n933,32985348838375,20120717080449463\n1129,1242,20100202163844119\n1129,2199023262543,20100331220757321\n1129,6597069771886,20100724111548162\n1129,6597069776731,20100804033836982\n",
                "933: [4139,6597069777240, 10995116284808, 32985348833579, 32985348838375]\n1129: [1242, 2199023262543, 6597069771886, 6597069776731]\n"
            ],
            "answer": {
                "ans_desc": "Use and create of s and then : ",
                "code": [
                    "#selecting by columns names\nd = df1.groupby('A')['B'].apply(list).to_dict()\n\n#seelcting columns by positions\nd = df1.iloc[:, 1].groupby(df1.iloc[:, 0]).apply(list).to_dict()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 10226740,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/bf02ac4892effa4e87ebd5e3a406dd1e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Chris ",
            "link": "https://stackoverflow.com/users/10226740/chris"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 51850251,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1534284369,
        "creation_date": 1534282952,
        "last_edit_date": 1534284369,
        "question_id": 51850165,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51850165/pandas-dataframe-filter-column-a-depending-on-if-column-b-contains-x-for-group-o",
        "title": "Pandas DataFrame filter column A depending on if column B contains x for group of values in A",
        "body": "<p>I would like to filter the below DataFrame <code>df</code> on column <code>ref</code>, based on if for the value in <code>ref</code>, column <code>type</code> contains the value <code>'P'</code>.</p>\n\n<pre><code>In [32]: df\nOut[32]: \n   ref type\n0    1    P\n1    1    C\n2    1    A\n3    2    C\n4    3    P\n5    3    P\n6    4    P\n7    4    A\n8    5    C\n9    5    A\n</code></pre>\n\n<p>Here, <code>ref</code> values 1, 3, and 4 contain <strong>at least one row</strong> with value <code>'P'</code> in column <code>type</code>, while 2 and 5 do not.</p>\n\n<p>I am trying to filter out any rows with <code>ref</code> 2 and 5 so that the final output is:</p>\n\n<pre><code>In [34]: df\nOut[34]: \n   ref type\n0    1    P\n1    1    C\n2    1    A\n4    3    P\n5    3    P\n6    4    P\n7    4    A\n</code></pre>\n\n<p>How could I do this (preferably in one step)?</p>\n",
        "answer_body": "<p>Use <code>groupby</code> and <code>filter</code>:</p>\n\n<pre><code>df.groupby('ref').filter(lambda x : ('P' in x['type'].values))\n</code></pre>\n\n<p>returns:</p>\n\n<pre><code>   ref type\n0    1    P\n1    1    C\n2    1    A\n4    3    P\n5    3    P\n6    4    P\n7    4    A\n</code></pre>\n",
        "question_body": "<p>I would like to filter the below DataFrame <code>df</code> on column <code>ref</code>, based on if for the value in <code>ref</code>, column <code>type</code> contains the value <code>'P'</code>.</p>\n\n<pre><code>In [32]: df\nOut[32]: \n   ref type\n0    1    P\n1    1    C\n2    1    A\n3    2    C\n4    3    P\n5    3    P\n6    4    P\n7    4    A\n8    5    C\n9    5    A\n</code></pre>\n\n<p>Here, <code>ref</code> values 1, 3, and 4 contain <strong>at least one row</strong> with value <code>'P'</code> in column <code>type</code>, while 2 and 5 do not.</p>\n\n<p>I am trying to filter out any rows with <code>ref</code> 2 and 5 so that the final output is:</p>\n\n<pre><code>In [34]: df\nOut[34]: \n   ref type\n0    1    P\n1    1    C\n2    1    A\n4    3    P\n5    3    P\n6    4    P\n7    4    A\n</code></pre>\n\n<p>How could I do this (preferably in one step)?</p>\n",
        "formatted_input": {
            "qid": 51850165,
            "link": "https://stackoverflow.com/questions/51850165/pandas-dataframe-filter-column-a-depending-on-if-column-b-contains-x-for-group-o",
            "question": {
                "title": "Pandas DataFrame filter column A depending on if column B contains x for group of values in A",
                "ques_desc": "I would like to filter the below DataFrame on column , based on if for the value in , column contains the value . Here, values 1, 3, and 4 contain at least one row with value in column , while 2 and 5 do not. I am trying to filter out any rows with 2 and 5 so that the final output is: How could I do this (preferably in one step)? "
            },
            "io": [
                "In [32]: df\nOut[32]: \n   ref type\n0    1    P\n1    1    C\n2    1    A\n3    2    C\n4    3    P\n5    3    P\n6    4    P\n7    4    A\n8    5    C\n9    5    A\n",
                "In [34]: df\nOut[34]: \n   ref type\n0    1    P\n1    1    C\n2    1    A\n4    3    P\n5    3    P\n6    4    P\n7    4    A\n"
            ],
            "answer": {
                "ans_desc": "Use and : returns: ",
                "code": [
                    "df.groupby('ref').filter(lambda x : ('P' in x['type'].values))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 331,
            "user_id": 7900723,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/9IPso.jpg?s=128&g=1",
            "display_name": "Daniel Bourke",
            "link": "https://stackoverflow.com/users/7900723/daniel-bourke"
        },
        "is_answered": true,
        "view_count": 2896,
        "accepted_answer_id": 51784878,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1533900818,
        "creation_date": 1533898183,
        "question_id": 51784855,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51784855/remove-decimals-in-pandas-column-names",
        "title": "Remove decimals in Pandas column names",
        "body": "<p>I have a dataframe with <code>numpy.float64</code> as the column names.</p>\n\n<pre><code>df =\n     2006.0   2007.0   2008.0   2009.0\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n</code></pre>\n\n<p>I'd like to change them to be strings and remove the decimal places:</p>\n\n<pre><code>df =\n       2006     2007     2008     2009\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n</code></pre>\n\n<p>I've tried saving the columns out to a list with <code>df.columns.tolist()</code> and changing them there but I've had no luck.</p>\n\n<p>Any help would be greatly appreciated!</p>\n",
        "answer_body": "<p>You can convert the type with <code>.astype</code></p>\n\n<pre><code>In [312]: df.columns = df.columns.astype(int)\n\nIn [313]: df\nOut[313]:\n  2006 2007 2008 2009\n0  foo  foo  bar  bar\n1  foo  foo  bar  bar\n</code></pre>\n\n<p>Or use <code>.map</code> and convert to string type.</p>\n\n<pre><code>In [338]: df.columns.map('{:g}'.format)\nOut[338]: Index(['2006', '2007', '2008', '2009'], dtype='object')\n\nIn [319]: df.columns.map(int)\nOut[319]: Int64Index([2006, 2007, 2008, 2009], dtype='int64')\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with <code>numpy.float64</code> as the column names.</p>\n\n<pre><code>df =\n     2006.0   2007.0   2008.0   2009.0\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n</code></pre>\n\n<p>I'd like to change them to be strings and remove the decimal places:</p>\n\n<pre><code>df =\n       2006     2007     2008     2009\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n</code></pre>\n\n<p>I've tried saving the columns out to a list with <code>df.columns.tolist()</code> and changing them there but I've had no luck.</p>\n\n<p>Any help would be greatly appreciated!</p>\n",
        "formatted_input": {
            "qid": 51784855,
            "link": "https://stackoverflow.com/questions/51784855/remove-decimals-in-pandas-column-names",
            "question": {
                "title": "Remove decimals in Pandas column names",
                "ques_desc": "I have a dataframe with as the column names. I'd like to change them to be strings and remove the decimal places: I've tried saving the columns out to a list with and changing them there but I've had no luck. Any help would be greatly appreciated! "
            },
            "io": [
                "df =\n     2006.0   2007.0   2008.0   2009.0\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n",
                "df =\n       2006     2007     2008     2009\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n"
            ],
            "answer": {
                "ans_desc": "You can convert the type with Or use and convert to string type. ",
                "code": [
                    "In [338]: df.columns.map('{:g}'.format)\nOut[338]: Index(['2006', '2007', '2008', '2009'], dtype='object')\n\nIn [319]: df.columns.map(int)\nOut[319]: Int64Index([2006, 2007, 2008, 2009], dtype='int64')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1688,
            "user_id": 6750211,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/tmOvh.gif?s=128&g=1",
            "display_name": "Kevin Fang",
            "link": "https://stackoverflow.com/users/6750211/kevin-fang"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 51781120,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1533886914,
        "creation_date": 1533884216,
        "last_edit_date": 1533884765,
        "question_id": 51780710,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51780710/how-to-slice-rows-from-two-pandas-dataframes-then-merge-them-with-some-other-val",
        "title": "How to slice rows from two pandas dataframes then merge them with some other value",
        "body": "<p>I got two pandas dataframes and two indexes, and one datetime variable. What I would like to do is: </p>\n\n<ol>\n<li><p>slice the dataframes with the indexes, then I got two rows.</p></li>\n<li><p>combine the two rows to one row.</p></li>\n<li><p>add the variable to the row.</p></li>\n<li><p>then I can get new indexes and datetime values to form more rows, and assemble the rows to a new dataframe.</p></li>\n</ol>\n\n<p>Example:</p>\n\n<p>df1:</p>\n\n<pre><code>    A   B\n0   0   10\n1   1   11\n2   2   12\n3   3   13\n4   4   14\n5   5   15\n6   6   16\n7   7   17\n8   8   18\n9   9   19\n</code></pre>\n\n<p>df2:</p>\n\n<pre><code>    C   D\n0   10  110\n1   11  111\n2   12  112\n3   13  113\n4   14  114\n5   15  115\n6   16  116\n7   17  117\n8   18  118\n9   19  119\n</code></pre>\n\n<p>index: 3, 5, datetime: <code>datetime.datetime(2018, 8, 10, 16, 53, 52, 760014)</code></p>\n\n<p>Output:</p>\n\n<pre><code>    A   B   C   D   time\n0   3   13  15  115 20180810-16:53:52:760014\n... # More rows when there's more indexes and datetimes\n</code></pre>\n",
        "answer_body": "<p>You can try :</p>\n\n<pre><code>index = [3,5]\ndata = np.r_[df1.iloc[index[0]].values,df2.iloc[index[1]].values]\ndf = pd.DataFrame([data],columns = list('ABCD'))\ndt = datetime.datetime(2018, 8, 10, 16, 53, 52, 760014)\ndf['date'] = dt\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>    A   B   C   D   date\n0   3   13  15  115 2018-08-10 16:53:52.760014\n</code></pre>\n",
        "question_body": "<p>I got two pandas dataframes and two indexes, and one datetime variable. What I would like to do is: </p>\n\n<ol>\n<li><p>slice the dataframes with the indexes, then I got two rows.</p></li>\n<li><p>combine the two rows to one row.</p></li>\n<li><p>add the variable to the row.</p></li>\n<li><p>then I can get new indexes and datetime values to form more rows, and assemble the rows to a new dataframe.</p></li>\n</ol>\n\n<p>Example:</p>\n\n<p>df1:</p>\n\n<pre><code>    A   B\n0   0   10\n1   1   11\n2   2   12\n3   3   13\n4   4   14\n5   5   15\n6   6   16\n7   7   17\n8   8   18\n9   9   19\n</code></pre>\n\n<p>df2:</p>\n\n<pre><code>    C   D\n0   10  110\n1   11  111\n2   12  112\n3   13  113\n4   14  114\n5   15  115\n6   16  116\n7   17  117\n8   18  118\n9   19  119\n</code></pre>\n\n<p>index: 3, 5, datetime: <code>datetime.datetime(2018, 8, 10, 16, 53, 52, 760014)</code></p>\n\n<p>Output:</p>\n\n<pre><code>    A   B   C   D   time\n0   3   13  15  115 20180810-16:53:52:760014\n... # More rows when there's more indexes and datetimes\n</code></pre>\n",
        "formatted_input": {
            "qid": 51780710,
            "link": "https://stackoverflow.com/questions/51780710/how-to-slice-rows-from-two-pandas-dataframes-then-merge-them-with-some-other-val",
            "question": {
                "title": "How to slice rows from two pandas dataframes then merge them with some other value",
                "ques_desc": "I got two pandas dataframes and two indexes, and one datetime variable. What I would like to do is: slice the dataframes with the indexes, then I got two rows. combine the two rows to one row. add the variable to the row. then I can get new indexes and datetime values to form more rows, and assemble the rows to a new dataframe. Example: df1: df2: index: 3, 5, datetime: Output: "
            },
            "io": [
                "    A   B\n0   0   10\n1   1   11\n2   2   12\n3   3   13\n4   4   14\n5   5   15\n6   6   16\n7   7   17\n8   8   18\n9   9   19\n",
                "    C   D\n0   10  110\n1   11  111\n2   12  112\n3   13  113\n4   14  114\n5   15  115\n6   16  116\n7   17  117\n8   18  118\n9   19  119\n"
            ],
            "answer": {
                "ans_desc": "You can try : Output: ",
                "code": [
                    "index = [3,5]\ndata = np.r_[df1.iloc[index[0]].values,df2.iloc[index[1]].values]\ndf = pd.DataFrame([data],columns = list('ABCD'))\ndt = datetime.datetime(2018, 8, 10, 16, 53, 52, 760014)\ndf['date'] = dt\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "combinations"
        ],
        "owner": {
            "reputation": 528,
            "user_id": 8919443,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/512b0b6982152bef9c7103e34f8b4126?s=128&d=identicon&r=PG&f=1",
            "display_name": "LamaMo",
            "link": "https://stackoverflow.com/users/8919443/lamamo"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 51734908,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1533677914,
        "creation_date": 1533673327,
        "last_edit_date": 1533674592,
        "question_id": 51734814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51734814/combination-of-two-dataframes-without-duplicate-and-reversion-in-efficient-way",
        "title": "Combination of two dataframes without duplicate and reversion in efficient way | python",
        "body": "<p>I have two dataframes with thousands of rows, I need to combine both into one dataframe without duplicate and reversion. for example:</p>\n\n<p>Dataframe 1</p>\n\n<pre><code>drug1\ndrug2\ndrug3\n</code></pre>\n\n<p>Dataframe 2</p>\n\n<pre><code>disease1\ndisease2\ndisease3\n</code></pre>\n\n<p>So, the output dataframe will be:</p>\n\n<p>output-dataframe</p>\n\n<pre><code>drug1 disease1\ndrug1 disease2\ndrug1 disease3\ndrug2 disease1\ndrug2 disease2\ndrug2 disease3 \ndrug3 disease1\ndrug3 disease2\ndrug3 disease3\n</code></pre>\n\n<p><strong>I don't want</strong> the output combination containing something like:</p>\n\n<pre><code>disease1 drug1\ndrug1 drug1\ndisease1 disease1 \n</code></pre>\n\n<p>I actually try it using <code>pd.merge</code> but it return duplicate and reversion and also took long time because I have thousands in Dataframes 1 and 2 </p>\n\n<p>Any help please ?</p>\n",
        "answer_body": "<p>Try this solution:</p>\n\n<pre><code>from pandas import DataFrame, merge\n\ndf1['key'] = 1\ndf2['key'] = 1\n\nresult = df1.merge(df2, on='key').drop('key', axis=1)\n</code></pre>\n",
        "question_body": "<p>I have two dataframes with thousands of rows, I need to combine both into one dataframe without duplicate and reversion. for example:</p>\n\n<p>Dataframe 1</p>\n\n<pre><code>drug1\ndrug2\ndrug3\n</code></pre>\n\n<p>Dataframe 2</p>\n\n<pre><code>disease1\ndisease2\ndisease3\n</code></pre>\n\n<p>So, the output dataframe will be:</p>\n\n<p>output-dataframe</p>\n\n<pre><code>drug1 disease1\ndrug1 disease2\ndrug1 disease3\ndrug2 disease1\ndrug2 disease2\ndrug2 disease3 \ndrug3 disease1\ndrug3 disease2\ndrug3 disease3\n</code></pre>\n\n<p><strong>I don't want</strong> the output combination containing something like:</p>\n\n<pre><code>disease1 drug1\ndrug1 drug1\ndisease1 disease1 \n</code></pre>\n\n<p>I actually try it using <code>pd.merge</code> but it return duplicate and reversion and also took long time because I have thousands in Dataframes 1 and 2 </p>\n\n<p>Any help please ?</p>\n",
        "formatted_input": {
            "qid": 51734814,
            "link": "https://stackoverflow.com/questions/51734814/combination-of-two-dataframes-without-duplicate-and-reversion-in-efficient-way",
            "question": {
                "title": "Combination of two dataframes without duplicate and reversion in efficient way | python",
                "ques_desc": "I have two dataframes with thousands of rows, I need to combine both into one dataframe without duplicate and reversion. for example: Dataframe 1 Dataframe 2 So, the output dataframe will be: output-dataframe I don't want the output combination containing something like: I actually try it using but it return duplicate and reversion and also took long time because I have thousands in Dataframes 1 and 2 Any help please ? "
            },
            "io": [
                "drug1 disease1\ndrug1 disease2\ndrug1 disease3\ndrug2 disease1\ndrug2 disease2\ndrug2 disease3 \ndrug3 disease1\ndrug3 disease2\ndrug3 disease3\n",
                "disease1 drug1\ndrug1 drug1\ndisease1 disease1 \n"
            ],
            "answer": {
                "ans_desc": "Try this solution: ",
                "code": [
                    "from pandas import DataFrame, merge\n\ndf1['key'] = 1\ndf2['key'] = 1\n\nresult = df1.merge(df2, on='key').drop('key', axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "random"
        ],
        "owner": {
            "reputation": 35,
            "user_id": 9855667,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a0d15cca697d88b83b52a42c149f6ffb?s=128&d=identicon&r=PG&f=1",
            "display_name": "lukalxn",
            "link": "https://stackoverflow.com/users/9855667/lukalxn"
        },
        "is_answered": true,
        "view_count": 2226,
        "accepted_answer_id": 51735167,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1533675429,
        "creation_date": 1533674508,
        "question_id": 51735106,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51735106/pandas-assign-random-numbers-in-given-range-to-equal-column-values",
        "title": "pandas: assign random numbers in given range to equal column values",
        "body": "<p>I am working with a large dataset, and one of the columns has very long integers, like below:</p>\n\n<pre><code>       Column_1        Column_2\n  1     A              12345123451\n  2     B              12345123451\n  3     C              12345123451\n  4     D              23456789234\n  5     E              23456789234\n  6     F              34567893456\n</code></pre>\n\n<p>What is important here is not the actual number in Column_2, but when those numbers are the same while Column_1 is different. I would like to reassign the values of Column_2 randomly from a range of smaller numbers, say (1, 999).</p>\n\n<pre><code>       Column_1        Column_2\n  1     A              120\n  2     B              120\n  3     C              120\n  4     D              54\n  5     E              54\n  6     F              567\n</code></pre>\n\n<p>My issue is figuring a way to describe in a lambda function that each equal value in Column_2 needs the same random number. </p>\n",
        "answer_body": "<p>You can create an array of random numbers between 1 and 999 using <code>np.random.choice</code>, making sure to say <code>replace=False</code> so you don't get any duplicates, and then map <code>Column_2</code> to a dictionary mapping of <code>Column_2</code> unique values with your array of random numbers:</p>\n\n<pre><code>import numpy as np \n\nnums = np.random.choice(range(1,999), size = df['Column_2'].nunique(), replace=False)\n\n# If you prefer to use the random package rather than numpy, uncomment the following:\n# import random\n# nums = random.sample(range(1,999), df['Column_2'].nunique())\n\ndf['Column_2'] = df['Column_2'].map(dict(zip(df['Column_2'].unique(), nums)))\n\n&gt;&gt;&gt; df\n  Column_1  Column_2\n1        A       274\n2        B       274\n3        C       274\n4        D       842\n5        E       842\n6        F       860\n</code></pre>\n\n<p><strong>Explanation:</strong></p>\n\n<p>Your array of numbers looks like:</p>\n\n<pre><code>&gt;&gt;&gt; nums\narray([274, 842, 860])\n</code></pre>\n\n<p>And your mapping dictionary looks like:</p>\n\n<pre><code>&gt;&gt;&gt; dict(zip(df['Column_2'].unique(), nums))\n{12345123451: 274, 23456789234: 842, 34567893456: 860}\n</code></pre>\n\n<p>So when you map, you are saying to replace <code>12345123451</code> with <code>274</code>, <code>23456789234</code> with <code>842</code>, and so on...</p>\n",
        "question_body": "<p>I am working with a large dataset, and one of the columns has very long integers, like below:</p>\n\n<pre><code>       Column_1        Column_2\n  1     A              12345123451\n  2     B              12345123451\n  3     C              12345123451\n  4     D              23456789234\n  5     E              23456789234\n  6     F              34567893456\n</code></pre>\n\n<p>What is important here is not the actual number in Column_2, but when those numbers are the same while Column_1 is different. I would like to reassign the values of Column_2 randomly from a range of smaller numbers, say (1, 999).</p>\n\n<pre><code>       Column_1        Column_2\n  1     A              120\n  2     B              120\n  3     C              120\n  4     D              54\n  5     E              54\n  6     F              567\n</code></pre>\n\n<p>My issue is figuring a way to describe in a lambda function that each equal value in Column_2 needs the same random number. </p>\n",
        "formatted_input": {
            "qid": 51735106,
            "link": "https://stackoverflow.com/questions/51735106/pandas-assign-random-numbers-in-given-range-to-equal-column-values",
            "question": {
                "title": "pandas: assign random numbers in given range to equal column values",
                "ques_desc": "I am working with a large dataset, and one of the columns has very long integers, like below: What is important here is not the actual number in Column_2, but when those numbers are the same while Column_1 is different. I would like to reassign the values of Column_2 randomly from a range of smaller numbers, say (1, 999). My issue is figuring a way to describe in a lambda function that each equal value in Column_2 needs the same random number. "
            },
            "io": [
                "       Column_1        Column_2\n  1     A              12345123451\n  2     B              12345123451\n  3     C              12345123451\n  4     D              23456789234\n  5     E              23456789234\n  6     F              34567893456\n",
                "       Column_1        Column_2\n  1     A              120\n  2     B              120\n  3     C              120\n  4     D              54\n  5     E              54\n  6     F              567\n"
            ],
            "answer": {
                "ans_desc": "You can create an array of random numbers between 1 and 999 using , making sure to say so you don't get any duplicates, and then map to a dictionary mapping of unique values with your array of random numbers: Explanation: Your array of numbers looks like: And your mapping dictionary looks like: So when you map, you are saying to replace with , with , and so on... ",
                "code": [
                    ">>> nums\narray([274, 842, 860])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 2843,
            "user_id": 7372165,
            "user_type": "registered",
            "accept_rate": 98,
            "profile_image": "https://i.stack.imgur.com/PCyvJ.png?s=128&g=1",
            "display_name": "JAG2024",
            "link": "https://stackoverflow.com/users/7372165/jag2024"
        },
        "is_answered": true,
        "view_count": 1996,
        "accepted_answer_id": 51588589,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1532944177,
        "creation_date": 1532929810,
        "last_edit_date": 1532930939,
        "question_id": 51587685,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51587685/pandas-dataframe-replace-multiple-rows-based-on-values-in-another-column",
        "title": "Pandas dataframe: Replace multiple rows based on values in another column",
        "body": "<p>I'm trying to replace some values in one dataframe's column with values from another data frame's column. Here's what the data frames look like. <code>df2</code> has a lot of rows and columns.</p>\n\n<pre><code>df1\n\n    0                   1029\n0   aaaaa               Green\n1   bbbbb               Green\n2   fffff               Blue\n3   xxxxx               Blue\n4   zzzzz               Green\n\ndf2\n    0       1   2     3  ....    1029\n0   aaaaa   1   NaN   14         NaN\n1   bbbbb   1   NaN   14         NaN\n2   ccccc   1   NaN   14         Blue\n3   ddddd   1   NaN   14         Blue\n...    \n25  yyyyy   1   NaN   14         Blue\n26  zzzzz   1   NaN   14         Blue\n</code></pre>\n\n<p>The final df should look like this</p>\n\n<pre><code>    0       1   2     3  ....    1029\n0   aaaaa   1   NaN   14         Green \n1   bbbbb   1   NaN   14         Green\n2   ccccc   1   NaN   14         Blue\n3   ddddd   1   NaN   14         Blue\n...    \n25  yyyyy   1   NaN   14         Blue\n26  zzzzz   1   NaN   14         Green\n</code></pre>\n\n<p>So basically what needs to happen is that <code>df1[0]</code> and <code>df[2]</code> need to be matched and then <code>df2[1029]</code> needs to have values replaced by the corresponding row in <code>df1[1029]</code> for the rows that matched. I don't want to lose any values in <code>df2['1029']</code> which are not in <code>df1['1029']</code></p>\n\n<p>I believe the <code>re</code> module in python can do that? This is what I have so far:</p>\n\n<pre><code>import re\nfor line in replace:\nline = re.sub(df1['1029'], \n              '1029',\n              line.rstrip())\n\nprint(line)\n</code></pre>\n\n<p>But it definitely doesn't work. </p>\n\n<p>I could also use merge as in <code>merged1 = df1.merge(df2, left_index=True, right_index=True, how='inner')</code> but that doesn't replace the values inline.</p>\n",
        "answer_body": "<p>You need:</p>\n\n<pre><code>df1 = pd.DataFrame({'0':['aaaaa','bbbbb','fffff','xxxxx','zzzzz'], '1029':['Green','Green','Blue','Blue','Green']})\n\ndf2 = pd.DataFrame({'0':['aaaa','bbbb','ccccc','ddddd','yyyyy','zzzzz',], '1029':[None,None,'Blue','Blue','Blue','Blue']})\n\n\n# Fill NaNs\ndf2['1029'] = df2['1029'].fillna(df1['1029'])\n\n# Merge the dataframes \ndf_ = df2.merge(df1, how='left', on=['0'])\n\ndf_['1029'] = np.where(df_['1029_y'].isna(), df_['1029_x'], df_['1029_y'])\n\ndf_.drop(['1029_y','1029_x'],1,inplace=True)\nprint(df_)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>       0   1029\n0   aaaa  Green\n1   bbbb  Green\n2  ccccc   Blue\n3  ddddd   Blue\n4  yyyyy   Blue\n5  zzzzz  Green\n</code></pre>\n",
        "question_body": "<p>I'm trying to replace some values in one dataframe's column with values from another data frame's column. Here's what the data frames look like. <code>df2</code> has a lot of rows and columns.</p>\n\n<pre><code>df1\n\n    0                   1029\n0   aaaaa               Green\n1   bbbbb               Green\n2   fffff               Blue\n3   xxxxx               Blue\n4   zzzzz               Green\n\ndf2\n    0       1   2     3  ....    1029\n0   aaaaa   1   NaN   14         NaN\n1   bbbbb   1   NaN   14         NaN\n2   ccccc   1   NaN   14         Blue\n3   ddddd   1   NaN   14         Blue\n...    \n25  yyyyy   1   NaN   14         Blue\n26  zzzzz   1   NaN   14         Blue\n</code></pre>\n\n<p>The final df should look like this</p>\n\n<pre><code>    0       1   2     3  ....    1029\n0   aaaaa   1   NaN   14         Green \n1   bbbbb   1   NaN   14         Green\n2   ccccc   1   NaN   14         Blue\n3   ddddd   1   NaN   14         Blue\n...    \n25  yyyyy   1   NaN   14         Blue\n26  zzzzz   1   NaN   14         Green\n</code></pre>\n\n<p>So basically what needs to happen is that <code>df1[0]</code> and <code>df[2]</code> need to be matched and then <code>df2[1029]</code> needs to have values replaced by the corresponding row in <code>df1[1029]</code> for the rows that matched. I don't want to lose any values in <code>df2['1029']</code> which are not in <code>df1['1029']</code></p>\n\n<p>I believe the <code>re</code> module in python can do that? This is what I have so far:</p>\n\n<pre><code>import re\nfor line in replace:\nline = re.sub(df1['1029'], \n              '1029',\n              line.rstrip())\n\nprint(line)\n</code></pre>\n\n<p>But it definitely doesn't work. </p>\n\n<p>I could also use merge as in <code>merged1 = df1.merge(df2, left_index=True, right_index=True, how='inner')</code> but that doesn't replace the values inline.</p>\n",
        "formatted_input": {
            "qid": 51587685,
            "link": "https://stackoverflow.com/questions/51587685/pandas-dataframe-replace-multiple-rows-based-on-values-in-another-column",
            "question": {
                "title": "Pandas dataframe: Replace multiple rows based on values in another column",
                "ques_desc": "I'm trying to replace some values in one dataframe's column with values from another data frame's column. Here's what the data frames look like. has a lot of rows and columns. The final df should look like this So basically what needs to happen is that and need to be matched and then needs to have values replaced by the corresponding row in for the rows that matched. I don't want to lose any values in which are not in I believe the module in python can do that? This is what I have so far: But it definitely doesn't work. I could also use merge as in but that doesn't replace the values inline. "
            },
            "io": [
                "df1\n\n    0                   1029\n0   aaaaa               Green\n1   bbbbb               Green\n2   fffff               Blue\n3   xxxxx               Blue\n4   zzzzz               Green\n\ndf2\n    0       1   2     3  ....    1029\n0   aaaaa   1   NaN   14         NaN\n1   bbbbb   1   NaN   14         NaN\n2   ccccc   1   NaN   14         Blue\n3   ddddd   1   NaN   14         Blue\n...    \n25  yyyyy   1   NaN   14         Blue\n26  zzzzz   1   NaN   14         Blue\n",
                "    0       1   2     3  ....    1029\n0   aaaaa   1   NaN   14         Green \n1   bbbbb   1   NaN   14         Green\n2   ccccc   1   NaN   14         Blue\n3   ddddd   1   NaN   14         Blue\n...    \n25  yyyyy   1   NaN   14         Blue\n26  zzzzz   1   NaN   14         Green\n"
            ],
            "answer": {
                "ans_desc": "You need: Output: ",
                "code": [
                    "df1 = pd.DataFrame({'0':['aaaaa','bbbbb','fffff','xxxxx','zzzzz'], '1029':['Green','Green','Blue','Blue','Green']})\n\ndf2 = pd.DataFrame({'0':['aaaa','bbbb','ccccc','ddddd','yyyyy','zzzzz',], '1029':[None,None,'Blue','Blue','Blue','Blue']})\n\n\n# Fill NaNs\ndf2['1029'] = df2['1029'].fillna(df1['1029'])\n\n# Merge the dataframes \ndf_ = df2.merge(df1, how='left', on=['0'])\n\ndf_['1029'] = np.where(df_['1029_y'].isna(), df_['1029_x'], df_['1029_y'])\n\ndf_.drop(['1029_y','1029_x'],1,inplace=True)\nprint(df_)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 579,
            "user_id": 5571914,
            "user_type": "registered",
            "accept_rate": 79,
            "profile_image": "https://www.gravatar.com/avatar/0585097f26069beb3b696cb0d7dba241?s=128&d=identicon&r=PG&f=1",
            "display_name": "Arun",
            "link": "https://stackoverflow.com/users/5571914/arun"
        },
        "is_answered": true,
        "view_count": 2637,
        "accepted_answer_id": 51578887,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1532856070,
        "creation_date": 1532855503,
        "last_edit_date": 1532856070,
        "question_id": 51578845,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51578845/indexing-with-multidimensional-key-pandas-error",
        "title": "indexing with multidimensional key pandas error",
        "body": "<p>I have a dataframe as the following:</p>\n\n<pre><code>A   B  C  D  E\ng1  1 -10 20 text1\ng2  0  0  0  text2\ng3  0  1  0  text3\ng4  0  0  0  text4\n</code></pre>\n\n<p>I want to filter out g2 &amp; g4 where all of 'B', 'C', 'D' values are 0</p>\n\n<p>I tried </p>\n\n<p><code>df = df[df[['B','C','D']]==0]</code> returns output same as input,</p>\n\n<p><code>df = df[df[['B','C','D']].all(axis=1)==0]</code> also returns output same as input,</p>\n\n<p><code>df = df.loc[df[['B','C','D']].isin(['0'])]</code> returns 'Cannot index with multi-dimensional key.</p>\n\n<p>The expected output is,</p>\n\n<pre><code>A   B  C  D  E\ng1  1 -10 20 text1\ng3  0  1  0  text3\n</code></pre>\n\n<p>What is the error here?</p>\n\n<p>Thanks in Advance.</p>\n",
        "answer_body": "<p>You were closest with your second attempt. Compare to zero before you use <code>all</code>. Then take the negative via the <code>~</code> operator:</p>\n\n<pre><code>mask = (df[['B', 'C', 'D']] == 0).all(1)\ndf = df[~mask]\n\nprint(df)\n\n    A  B   C   D      E\n0  g1  1 -10  20  text1\n2  g3  0   1   0  text3\n</code></pre>\n\n<p>To understand how this logic works, notice that comparing a dataframe to a scalar returns a Boolean dataframe:</p>\n\n<pre><code>print(df[['B', 'C', 'D']] == 0)\n\n       B      C      D\n0  False  False  False\n2   True  False   True\n</code></pre>\n\n<p>You can then collapse by dimension 1 via <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.all.html\" rel=\"nofollow noreferrer\"><code>pd.DataFrame.all</code></a>. The syntax and methods are borrowed from NumPy array functionality.</p>\n",
        "question_body": "<p>I have a dataframe as the following:</p>\n\n<pre><code>A   B  C  D  E\ng1  1 -10 20 text1\ng2  0  0  0  text2\ng3  0  1  0  text3\ng4  0  0  0  text4\n</code></pre>\n\n<p>I want to filter out g2 &amp; g4 where all of 'B', 'C', 'D' values are 0</p>\n\n<p>I tried </p>\n\n<p><code>df = df[df[['B','C','D']]==0]</code> returns output same as input,</p>\n\n<p><code>df = df[df[['B','C','D']].all(axis=1)==0]</code> also returns output same as input,</p>\n\n<p><code>df = df.loc[df[['B','C','D']].isin(['0'])]</code> returns 'Cannot index with multi-dimensional key.</p>\n\n<p>The expected output is,</p>\n\n<pre><code>A   B  C  D  E\ng1  1 -10 20 text1\ng3  0  1  0  text3\n</code></pre>\n\n<p>What is the error here?</p>\n\n<p>Thanks in Advance.</p>\n",
        "formatted_input": {
            "qid": 51578845,
            "link": "https://stackoverflow.com/questions/51578845/indexing-with-multidimensional-key-pandas-error",
            "question": {
                "title": "indexing with multidimensional key pandas error",
                "ques_desc": "I have a dataframe as the following: I want to filter out g2 & g4 where all of 'B', 'C', 'D' values are 0 I tried returns output same as input, also returns output same as input, returns 'Cannot index with multi-dimensional key. The expected output is, What is the error here? Thanks in Advance. "
            },
            "io": [
                "A   B  C  D  E\ng1  1 -10 20 text1\ng2  0  0  0  text2\ng3  0  1  0  text3\ng4  0  0  0  text4\n",
                "A   B  C  D  E\ng1  1 -10 20 text1\ng3  0  1  0  text3\n"
            ],
            "answer": {
                "ans_desc": "You were closest with your second attempt. Compare to zero before you use . Then take the negative via the operator: To understand how this logic works, notice that comparing a dataframe to a scalar returns a Boolean dataframe: You can then collapse by dimension 1 via . The syntax and methods are borrowed from NumPy array functionality. ",
                "code": [
                    "print(df[['B', 'C', 'D']] == 0)\n\n       B      C      D\n0  False  False  False\n2   True  False   True\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 675,
            "user_id": 7058823,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a515f96ca304a2a3a0e29fc2f42e4cfd?s=128&d=identicon&r=PG&f=1",
            "display_name": "Platalea Minor",
            "link": "https://stackoverflow.com/users/7058823/platalea-minor"
        },
        "is_answered": true,
        "view_count": 259,
        "accepted_answer_id": 51357540,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1531973669,
        "creation_date": 1531729396,
        "last_edit_date": 1531973669,
        "question_id": 51357485,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51357485/combine-multi-columns-to-one-column-pandas",
        "title": "Combine multi columns to one column Pandas",
        "body": "<p>Hi I have the following dataframe</p>\n\n<pre><code>   z  a   b   c \n   a  1   NaN NaN\n   ss NaN 2   NaN\n   cc 3   NaN NaN\n   aa NaN 4   NaN\n   ww NaN 5   NaN\n   ss NaN NaN 6\n   aa NaN NaN 7\n   g  NaN NaN 8\n   j  9   NaN NaN\n</code></pre>\n\n<p>I would like to create a new column d to do something like this</p>\n\n<pre><code>z  a   b   c    d\na  1   NaN NaN  1\nss NaN 2   NaN  2\ncc 3  NaN NaN  3\naa NaN 4   NaN  4\nww NaN 5   NaN  5\nss NaN NaN 6    6\naa NaN NaN 7    7\ng  NaN NaN 8    8\nj  9   NaN NaN  9\n</code></pre>\n\n<p>For the numbers, it is not in integer. It is in np.float64. The integers are for clear example. you may assume the numbers are like 32065431243556.62, 763835218962767.8 Thank you for your help</p>\n",
        "answer_body": "<p>We can replace the NA by 0 and sum up the rows.</p>\n\n<pre><code>df['d'] = df[['a', 'b', 'c']].fillna(0).sum(axis=1)\n</code></pre>\n",
        "question_body": "<p>Hi I have the following dataframe</p>\n\n<pre><code>   z  a   b   c \n   a  1   NaN NaN\n   ss NaN 2   NaN\n   cc 3   NaN NaN\n   aa NaN 4   NaN\n   ww NaN 5   NaN\n   ss NaN NaN 6\n   aa NaN NaN 7\n   g  NaN NaN 8\n   j  9   NaN NaN\n</code></pre>\n\n<p>I would like to create a new column d to do something like this</p>\n\n<pre><code>z  a   b   c    d\na  1   NaN NaN  1\nss NaN 2   NaN  2\ncc 3  NaN NaN  3\naa NaN 4   NaN  4\nww NaN 5   NaN  5\nss NaN NaN 6    6\naa NaN NaN 7    7\ng  NaN NaN 8    8\nj  9   NaN NaN  9\n</code></pre>\n\n<p>For the numbers, it is not in integer. It is in np.float64. The integers are for clear example. you may assume the numbers are like 32065431243556.62, 763835218962767.8 Thank you for your help</p>\n",
        "formatted_input": {
            "qid": 51357485,
            "link": "https://stackoverflow.com/questions/51357485/combine-multi-columns-to-one-column-pandas",
            "question": {
                "title": "Combine multi columns to one column Pandas",
                "ques_desc": "Hi I have the following dataframe I would like to create a new column d to do something like this For the numbers, it is not in integer. It is in np.float64. The integers are for clear example. you may assume the numbers are like 32065431243556.62, 763835218962767.8 Thank you for your help "
            },
            "io": [
                "   z  a   b   c \n   a  1   NaN NaN\n   ss NaN 2   NaN\n   cc 3   NaN NaN\n   aa NaN 4   NaN\n   ww NaN 5   NaN\n   ss NaN NaN 6\n   aa NaN NaN 7\n   g  NaN NaN 8\n   j  9   NaN NaN\n",
                "z  a   b   c    d\na  1   NaN NaN  1\nss NaN 2   NaN  2\ncc 3  NaN NaN  3\naa NaN 4   NaN  4\nww NaN 5   NaN  5\nss NaN NaN 6    6\naa NaN NaN 7    7\ng  NaN NaN 8    8\nj  9   NaN NaN  9\n"
            ],
            "answer": {
                "ans_desc": "We can replace the NA by 0 and sum up the rows. ",
                "code": [
                    "df['d'] = df[['a', 'b', 'c']].fillna(0).sum(axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 528,
            "user_id": 8919443,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/512b0b6982152bef9c7103e34f8b4126?s=128&d=identicon&r=PG&f=1",
            "display_name": "LamaMo",
            "link": "https://stackoverflow.com/users/8919443/lamamo"
        },
        "is_answered": true,
        "view_count": 214,
        "closed_date": 1531940059,
        "accepted_answer_id": 51409150,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1531940033,
        "creation_date": 1531939497,
        "last_edit_date": 1531940032,
        "question_id": 51408990,
        "link": "https://stackoverflow.com/questions/51408990/how-to-fill-column-value-with-another-column-and-keep-existing",
        "closed_reason": "Duplicate",
        "title": "How to fill column&#39; value with another column and keep existing?",
        "body": "<p>I have this dataframe with two column<code>['c1','c2']</code>, and I want to fill <code>c2</code> with its corresponding <code>c1</code> if exist, also if <code>c1</code> already have a value keep it don't change it with <code>c2</code>. Here is an example:</p>\n\n<p>input </p>\n\n<pre><code>c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\n              MP_0000231\n              MP_0000231\n</code></pre>\n\n<p>output</p>\n\n<pre><code>c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\nMP_0000231    MP_0000231\nMP_0000231    MP_0000231\n</code></pre>\n\n<p>It's look easy, but I'm beginner in python :(\nAny help please.</p>\n",
        "answer_body": "<p>You can do so:</p>\n\n<pre><code>df['c1'] = df['c1'].replace('',np.NaN).fillna(df['c2'])\ndf['c2'] = df['c2'].replace('',np.NaN).fillna(df['c1'])\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>           c1          c2\n0  HP_0003470  HP_0003470\n1  HP_8362789  HP_0093723\n2  MP_0000231  MP_0000231\n3  MP_0000231  MP_0000231\n</code></pre>\n",
        "question_body": "<p>I have this dataframe with two column<code>['c1','c2']</code>, and I want to fill <code>c2</code> with its corresponding <code>c1</code> if exist, also if <code>c1</code> already have a value keep it don't change it with <code>c2</code>. Here is an example:</p>\n\n<p>input </p>\n\n<pre><code>c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\n              MP_0000231\n              MP_0000231\n</code></pre>\n\n<p>output</p>\n\n<pre><code>c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\nMP_0000231    MP_0000231\nMP_0000231    MP_0000231\n</code></pre>\n\n<p>It's look easy, but I'm beginner in python :(\nAny help please.</p>\n",
        "formatted_input": {
            "qid": 51408990,
            "link": "https://stackoverflow.com/questions/51408990/how-to-fill-column-value-with-another-column-and-keep-existing",
            "question": {
                "title": "How to fill column&#39; value with another column and keep existing?",
                "ques_desc": "I have this dataframe with two column, and I want to fill with its corresponding if exist, also if already have a value keep it don't change it with . Here is an example: input output It's look easy, but I'm beginner in python :( Any help please. "
            },
            "io": [
                "c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\n              MP_0000231\n              MP_0000231\n",
                "c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\nMP_0000231    MP_0000231\nMP_0000231    MP_0000231\n"
            ],
            "answer": {
                "ans_desc": "You can do so: Output: ",
                "code": [
                    "df['c1'] = df['c1'].replace('',np.NaN).fillna(df['c2'])\ndf['c2'] = df['c2'].replace('',np.NaN).fillna(df['c1'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 664,
            "user_id": 7577404,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/682c5ac7d6e758a648c98087663ea705?s=128&d=identicon&r=PG&f=1",
            "display_name": "MrClean",
            "link": "https://stackoverflow.com/users/7577404/mrclean"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 51371558,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1531867634,
        "creation_date": 1531785881,
        "last_edit_date": 1531788798,
        "question_id": 51371507,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51371507/pulling-multiple-ranges-from-dataframe-pandas",
        "title": "Pulling Multiple Ranges from Dataframe Pandas",
        "body": "<p>Lets say I have the following data set:</p>\n\n<pre><code>A      B    \n10.1   53\n12.5   42\n16.0   37\n20.7   03\n25.6   16\n30.1   01\n40.9   19\n60.5   99  \n</code></pre>\n\n<p>I have a the following list of ranges. </p>\n\n<pre><code>[[9,15],[19,22],[39,50]]  \n</code></pre>\n\n<p>How do I efficiently pull rows that lie in those ranges? </p>\n\n<p>Wanted Output</p>\n\n<pre><code>A      B    \n10.1   53\n12.5   42\n20.7   03\n40.9   19\n</code></pre>\n\n<p>Edit:\nNeeds to work for floating points</p>\n",
        "answer_body": "<p><strong>Update for modified question</strong></p>\n\n<p>For floats, you can construct a mask using NumPy array operations:</p>\n\n<pre><code>L = np.array([[9,15],[19,22],[39,50]])\nA = df['A'].values\n\nmask = ((A &gt;= L[:, 0][:, None]) &amp; (A &lt;= L[:, 1][:, None])).any(0)\n\nres = df[mask]\n\nprint(res)\n\n      A   B\n0  10.1  53\n1  12.5  42\n3  20.7   3\n6  40.9  19\n</code></pre>\n\n<hr>\n\n<p><strong>Previous answer to original question</strong></p>\n\n<p>For integers, you can use <code>numpy.concatenate</code> with <code>numpy.arange</code>:</p>\n\n<pre><code>L = [[9,15],[19,22],[39,50]]\n\nvals = np.concatenate([np.arange(i, j) for i, j in L])\n\nres = df[df['A'].isin(vals)]\n\nprint(res)\n\n    A   B\n0  10  53\n1  12  42\n3  20   3\n6  40  19\n</code></pre>\n\n<p>An alternative solution with <code>itertools.chain</code> and <code>range</code>:</p>\n\n<pre><code>from itertools import chain\n\nvals = set(chain.from_iterable(range(i, j) for i, j in L))\n\nres = df[df['A'].isin(vals)]\n</code></pre>\n",
        "question_body": "<p>Lets say I have the following data set:</p>\n\n<pre><code>A      B    \n10.1   53\n12.5   42\n16.0   37\n20.7   03\n25.6   16\n30.1   01\n40.9   19\n60.5   99  \n</code></pre>\n\n<p>I have a the following list of ranges. </p>\n\n<pre><code>[[9,15],[19,22],[39,50]]  \n</code></pre>\n\n<p>How do I efficiently pull rows that lie in those ranges? </p>\n\n<p>Wanted Output</p>\n\n<pre><code>A      B    \n10.1   53\n12.5   42\n20.7   03\n40.9   19\n</code></pre>\n\n<p>Edit:\nNeeds to work for floating points</p>\n",
        "formatted_input": {
            "qid": 51371507,
            "link": "https://stackoverflow.com/questions/51371507/pulling-multiple-ranges-from-dataframe-pandas",
            "question": {
                "title": "Pulling Multiple Ranges from Dataframe Pandas",
                "ques_desc": "Lets say I have the following data set: I have a the following list of ranges. How do I efficiently pull rows that lie in those ranges? Wanted Output Edit: Needs to work for floating points "
            },
            "io": [
                "A      B    \n10.1   53\n12.5   42\n16.0   37\n20.7   03\n25.6   16\n30.1   01\n40.9   19\n60.5   99  \n",
                "A      B    \n10.1   53\n12.5   42\n20.7   03\n40.9   19\n"
            ],
            "answer": {
                "ans_desc": "Update for modified question For floats, you can construct a mask using NumPy array operations: Previous answer to original question For integers, you can use with : An alternative solution with and : ",
                "code": [
                    "from itertools import chain\n\nvals = set(chain.from_iterable(range(i, j) for i, j in L))\n\nres = df[df['A'].isin(vals)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3971,
            "user_id": 7585973,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/1cf9f40e4f8e5e17076d55814a9c2ad9?s=128&d=identicon&r=PG",
            "display_name": "Nabih Bawazir",
            "link": "https://stackoverflow.com/users/7585973/nabih-bawazir"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 51374881,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1531811230,
        "creation_date": 1531809946,
        "last_edit_date": 1531811230,
        "question_id": 51374862,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51374862/how-to-map-new-variable-in-pandas-in-effective-way",
        "title": "How to map new variable in pandas in effective way",
        "body": "<p>Here's my data</p>\n\n<pre><code>Id  Amount\n1   6\n2   2\n3   0\n4   6\n</code></pre>\n\n<p>What I need, is to map : if <code>Amount</code> is more than <code>3</code> , <code>Map</code> is <code>1</code>. But,if  <code>Amount</code> is less than <code>3</code>, <code>Map</code> is <code>0</code></p>\n\n<pre><code>Id  Amount   Map\n1   6        1\n2   2        0\n3   0        0\n4   5        1\n</code></pre>\n\n<p>What I did</p>\n\n<pre><code>a = df[['Id','Amount']]\na = a[a['Amount'] &gt;= 3]\na['Map'] = 1\na = a[['Id', 'Map']]\ndf=  df.merge(a, on='Id', how='left')\ndf['Amount'].fillna(0)\n</code></pre>\n\n<p>It works, but not highly configurable and not effective.</p>\n",
        "answer_body": "<p>Convert boolean mask to integer:</p>\n\n<pre><code>#for better performance convert to numpy array\ndf['Map'] = (df['Amount'].values &gt;= 3).astype(int)\n#pure pandas solution\ndf['Map'] = (df['Amount'] &gt;= 3).astype(int)\nprint (df)\n   Id  Amount  Map\n0   1       6    1\n1   2       2    0\n2   3       0    0\n3   4       6    1\n</code></pre>\n\n<p><strong>Performance</strong>:</p>\n\n<pre><code>#[400000 rows x 3 columns]\ndf = pd.concat([df] * 100000, ignore_index=True)\n\nIn [133]: %timeit df['Map'] = (df['Amount'].values &gt;= 3).astype(int)\n2.44 ms \u00b1 97.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [134]: %timeit df['Map'] = (df['Amount'] &gt;= 3).astype(int)\n2.6 ms \u00b1 66.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n",
        "question_body": "<p>Here's my data</p>\n\n<pre><code>Id  Amount\n1   6\n2   2\n3   0\n4   6\n</code></pre>\n\n<p>What I need, is to map : if <code>Amount</code> is more than <code>3</code> , <code>Map</code> is <code>1</code>. But,if  <code>Amount</code> is less than <code>3</code>, <code>Map</code> is <code>0</code></p>\n\n<pre><code>Id  Amount   Map\n1   6        1\n2   2        0\n3   0        0\n4   5        1\n</code></pre>\n\n<p>What I did</p>\n\n<pre><code>a = df[['Id','Amount']]\na = a[a['Amount'] &gt;= 3]\na['Map'] = 1\na = a[['Id', 'Map']]\ndf=  df.merge(a, on='Id', how='left')\ndf['Amount'].fillna(0)\n</code></pre>\n\n<p>It works, but not highly configurable and not effective.</p>\n",
        "formatted_input": {
            "qid": 51374862,
            "link": "https://stackoverflow.com/questions/51374862/how-to-map-new-variable-in-pandas-in-effective-way",
            "question": {
                "title": "How to map new variable in pandas in effective way",
                "ques_desc": "Here's my data What I need, is to map : if is more than , is . But,if is less than , is What I did It works, but not highly configurable and not effective. "
            },
            "io": [
                "Id  Amount\n1   6\n2   2\n3   0\n4   6\n",
                "Id  Amount   Map\n1   6        1\n2   2        0\n3   0        0\n4   5        1\n"
            ],
            "answer": {
                "ans_desc": "Convert boolean mask to integer: Performance: ",
                "code": [
                    "#[400000 rows x 3 columns]\ndf = pd.concat([df] * 100000, ignore_index=True)\n\nIn [133]: %timeit df['Map'] = (df['Amount'].values >= 3).astype(int)\n2.44 ms \u00b1 97.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [134]: %timeit df['Map'] = (df['Amount'] >= 3).astype(int)\n2.6 ms \u00b1 66.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "apriori"
        ],
        "owner": {
            "reputation": 1127,
            "user_id": 9228407,
            "user_type": "registered",
            "accept_rate": 44,
            "profile_image": "https://www.gravatar.com/avatar/2afa510ffdff9be4f0ab5d75256fd178?s=128&d=identicon&r=PG&f=1",
            "display_name": "qwww",
            "link": "https://stackoverflow.com/users/9228407/qwww"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 51357379,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1531729334,
        "creation_date": 1531728499,
        "question_id": 51357253,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51357253/how-to-find-which-row-items-are-appearing-most-in-a-pandas-dataframe",
        "title": "How to find which row items are appearing most in a pandas dataframe",
        "body": "<p>I have a dataframe something like this :</p>\n\n<pre><code>    a   b   c   d   e   f\n  ------------------------\n0   0   0   1   1   0   1\n1   1   0   1   1   0   0\n2   0   0   1   1   0   1\n3   1   0   1   0   0   0\n4   0   0   1   1   0   1\n5   0   1   1   0   0   0\n6   1   0   1   0   1   1\n7   0   0   1   1   0   1\n8   1   0   1   1   1   0\n9   0   0   1   1   0   1\n</code></pre>\n\n<p>How to find which row is appearing the most number of times and unique items count?\nHere <code>0  0  1   1   0   1</code> this is appearing most times in rows <code>0,2,4,7,9</code>.</p>\n\n<p>I tried <code>apriori algorithm</code> ,but it is giving me 100+ rules if my data is big.\n.NB : My real data is not <code>0</code> and <code>1</code>. This is mock data.</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> by all columns with <code>size</code> and for index by max value add <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html\" rel=\"nofollow noreferrer\"><code>idxmax</code></a>:</p>\n\n<pre><code>out = df.groupby(df.columns.tolist()).size().idxmax()\nprint (out)\n(0, 0, 1, 1, 0, 1)\n</code></pre>\n\n<p>And for index values <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> with compare by <code>max</code> value:</p>\n\n<pre><code>s = df.groupby(df.columns.tolist())[df.columns[0]].transform('size')\nidx = s.index[s == s.max()]\nprint (idx)\nInt64Index([0, 2, 4, 7, 9], dtype='int64')\n</code></pre>\n",
        "question_body": "<p>I have a dataframe something like this :</p>\n\n<pre><code>    a   b   c   d   e   f\n  ------------------------\n0   0   0   1   1   0   1\n1   1   0   1   1   0   0\n2   0   0   1   1   0   1\n3   1   0   1   0   0   0\n4   0   0   1   1   0   1\n5   0   1   1   0   0   0\n6   1   0   1   0   1   1\n7   0   0   1   1   0   1\n8   1   0   1   1   1   0\n9   0   0   1   1   0   1\n</code></pre>\n\n<p>How to find which row is appearing the most number of times and unique items count?\nHere <code>0  0  1   1   0   1</code> this is appearing most times in rows <code>0,2,4,7,9</code>.</p>\n\n<p>I tried <code>apriori algorithm</code> ,but it is giving me 100+ rules if my data is big.\n.NB : My real data is not <code>0</code> and <code>1</code>. This is mock data.</p>\n",
        "formatted_input": {
            "qid": 51357253,
            "link": "https://stackoverflow.com/questions/51357253/how-to-find-which-row-items-are-appearing-most-in-a-pandas-dataframe",
            "question": {
                "title": "How to find which row items are appearing most in a pandas dataframe",
                "ques_desc": "I have a dataframe something like this : How to find which row is appearing the most number of times and unique items count? Here this is appearing most times in rows . I tried ,but it is giving me 100+ rules if my data is big. .NB : My real data is not and . This is mock data. "
            },
            "io": [
                "    a   b   c   d   e   f\n  ------------------------\n0   0   0   1   1   0   1\n1   1   0   1   1   0   0\n2   0   0   1   1   0   1\n3   1   0   1   0   0   0\n4   0   0   1   1   0   1\n5   0   1   1   0   0   0\n6   1   0   1   0   1   1\n7   0   0   1   1   0   1\n8   1   0   1   1   1   0\n9   0   0   1   1   0   1\n",
                "0  0  1   1   0   1"
            ],
            "answer": {
                "ans_desc": "Use by all columns with and for index by max value add : And for index values with compare by value: ",
                "code": [
                    "s = df.groupby(df.columns.tolist())[df.columns[0]].transform('size')\nidx = s.index[s == s.max()]\nprint (idx)\nInt64Index([0, 2, 4, 7, 9], dtype='int64')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "apply",
            "series"
        ],
        "owner": {
            "reputation": 1127,
            "user_id": 9228407,
            "user_type": "registered",
            "accept_rate": 44,
            "profile_image": "https://www.gravatar.com/avatar/2afa510ffdff9be4f0ab5d75256fd178?s=128&d=identicon&r=PG&f=1",
            "display_name": "qwww",
            "link": "https://stackoverflow.com/users/9228407/qwww"
        },
        "is_answered": true,
        "view_count": 968,
        "accepted_answer_id": 51279994,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1531315956,
        "creation_date": 1531295141,
        "last_edit_date": 1531315956,
        "question_id": 51279903,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51279903/pandas-apply-and-applymap-functions-are-taking-long-time-to-run-on-large-dataset",
        "title": "pandas apply and applymap functions are taking long time to run on large dataset",
        "body": "<p>I have two functions applied on a dataframe</p>\n\n<pre><code>res = df.apply(lambda x:pd.Series(list(x)))  \nres = res.applymap(lambda x: x.strip('\"') if isinstance(x, str) else x)\n</code></pre>\n\n<p>{{Update}} Dataframe has got almost 700 000 rows. This is taking much time to run.</p>\n\n<p>How to reduce the running time?</p>\n\n<p>Sample data :</p>\n\n<pre><code>   A        \n ----------\n0 [1,4,3,c] \n1 [t,g,h,j]  \n2 [d,g,e,w]  \n3 [f,i,j,h] \n4 [m,z,s,e] \n5 [q,f,d,s] \n</code></pre>\n\n<p>output:</p>\n\n<pre><code>   A         B   C   D  E\n-------------------------\n0 [1,4,3,c]  1   4   3  c\n1 [t,g,h,j]  t   g   h  j\n2 [d,g,e,w]  d   g   e  w\n3 [f,i,j,h]  f   i   j  h\n4 [m,z,s,e]  m   z   s  e\n5 [q,f,d,s]  q   f   d  s\n</code></pre>\n\n<p>This line of code <code>res = df.apply(lambda x:pd.Series(list(x)))</code> takes items from a list and fill one by one to each column as shown above. There will be almost 38 columns.</p>\n",
        "answer_body": "<p>I think:</p>\n\n<pre><code>res = df.apply(lambda x:pd.Series(list(x)))  \n</code></pre>\n\n<p>should be changed to:</p>\n\n<pre><code>df1 = pd.DataFrame(df['A'].values.tolist())\nprint (df1)\n   0  1  2  3\n0  1  4  3  c\n1  t  g  h  j\n2  d  g  e  w\n3  f  i  j  h\n4  m  z  s  e\n5  q  f  d  s\n</code></pre>\n\n<p>And second if not mixed columns values - numeric with strings:</p>\n\n<pre><code>cols = res.select_dtypes(object).columns\nres[cols] = res[cols].apply(lambda x: x.str.strip('\"'))\n</code></pre>\n",
        "question_body": "<p>I have two functions applied on a dataframe</p>\n\n<pre><code>res = df.apply(lambda x:pd.Series(list(x)))  \nres = res.applymap(lambda x: x.strip('\"') if isinstance(x, str) else x)\n</code></pre>\n\n<p>{{Update}} Dataframe has got almost 700 000 rows. This is taking much time to run.</p>\n\n<p>How to reduce the running time?</p>\n\n<p>Sample data :</p>\n\n<pre><code>   A        \n ----------\n0 [1,4,3,c] \n1 [t,g,h,j]  \n2 [d,g,e,w]  \n3 [f,i,j,h] \n4 [m,z,s,e] \n5 [q,f,d,s] \n</code></pre>\n\n<p>output:</p>\n\n<pre><code>   A         B   C   D  E\n-------------------------\n0 [1,4,3,c]  1   4   3  c\n1 [t,g,h,j]  t   g   h  j\n2 [d,g,e,w]  d   g   e  w\n3 [f,i,j,h]  f   i   j  h\n4 [m,z,s,e]  m   z   s  e\n5 [q,f,d,s]  q   f   d  s\n</code></pre>\n\n<p>This line of code <code>res = df.apply(lambda x:pd.Series(list(x)))</code> takes items from a list and fill one by one to each column as shown above. There will be almost 38 columns.</p>\n",
        "formatted_input": {
            "qid": 51279903,
            "link": "https://stackoverflow.com/questions/51279903/pandas-apply-and-applymap-functions-are-taking-long-time-to-run-on-large-dataset",
            "question": {
                "title": "pandas apply and applymap functions are taking long time to run on large dataset",
                "ques_desc": "I have two functions applied on a dataframe {{Update}} Dataframe has got almost 700 000 rows. This is taking much time to run. How to reduce the running time? Sample data : output: This line of code takes items from a list and fill one by one to each column as shown above. There will be almost 38 columns. "
            },
            "io": [
                "   A        \n ----------\n0 [1,4,3,c] \n1 [t,g,h,j]  \n2 [d,g,e,w]  \n3 [f,i,j,h] \n4 [m,z,s,e] \n5 [q,f,d,s] \n",
                "   A         B   C   D  E\n-------------------------\n0 [1,4,3,c]  1   4   3  c\n1 [t,g,h,j]  t   g   h  j\n2 [d,g,e,w]  d   g   e  w\n3 [f,i,j,h]  f   i   j  h\n4 [m,z,s,e]  m   z   s  e\n5 [q,f,d,s]  q   f   d  s\n"
            ],
            "answer": {
                "ans_desc": "I think: should be changed to: And second if not mixed columns values - numeric with strings: ",
                "code": [
                    "cols = res.select_dtypes(object).columns\nres[cols] = res[cols].apply(lambda x: x.str.strip('\"'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1377,
            "user_id": 9276708,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/Cwbyr.png?s=128&g=1",
            "display_name": "rosefun",
            "link": "https://stackoverflow.com/users/9276708/rosefun"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 51265966,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1531228032,
        "creation_date": 1531227476,
        "question_id": 51265888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51265888/python-how-to-sum-unique-elements-respectively-of-a-dataframe-column-based-on-a",
        "title": "python: how to sum unique elements respectively of a dataframe column based on another column",
        "body": "<p>For example, I have a df with two columns.</p>\n\n<p><strong>Input</strong></p>\n\n<pre><code>df = pd.DataFrame({'user_id':list('aaabbbccc'),'label':[0,0,1,0,0,2,0,1,2]})\nprint('df\\n',df)\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>df\n    label user_id\n0      0       a\n1      0       a\n2      1       a\n3      0       b\n4      0       b\n5      2       b\n6      0       c\n7      1       c\n8      2       c\n</code></pre>\n\n<p>I want to count the element in <code>label</code> group by user_id respectively. \nThe expected output is shown as follow.</p>\n\n<p><strong>Expected</strong></p>\n\n<pre><code>  df\n    label user_id  label_0  label_1  label_2\n0      0       a        2         1         0\n1      0       a        2         1         0\n2      1       a        2         1         0\n3      0       b        2         0         1\n4      0       b        2         0         1\n5      2       b        2         0         1\n6      0       c        1         1         1 \n7      1       c        1         1         1\n8      2       c        1         1         1\n</code></pre>\n\n<p>Briefly, in column <code>label_0</code>, I count the number of <code>0</code> in column <code>label</code> based on column <code>user_id</code>.</p>\n\n<p>Hopefully for help!</p>\n",
        "answer_body": "<p>Idea is create helper <code>DataFrame</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html\" rel=\"nofollow noreferrer\"><code>size</code></a> or <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.SeriesGroupBy.value_counts.html\" rel=\"nofollow noreferrer\"><code>value_counts</code></a> and then <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unstack.html\" rel=\"nofollow noreferrer\"><code>unstack</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html\" rel=\"nofollow noreferrer\"><code>join</code></a> to original <code>df</code>:</p>\n\n<pre><code>df = (df.join(df.groupby(['user_id', 'label'])\n                .size()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n</code></pre>\n\n<hr>\n\n<pre><code>df = (df.join(df.groupby('user_id')['label']\n                .value_counts()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n</code></pre>\n\n<p>Or using <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\" rel=\"nofollow noreferrer\"><code>crosstab</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\" rel=\"nofollow noreferrer\"><code>merge</code></a> with left join:</p>\n\n<pre><code>df = (df.merge(pd.crosstab(df['user_id'], df['label'])\n                 .add_prefix('label_'), on='user_id', how='left'))\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n  user_id  label  label_0  label_1  label_2\n0       a      0        1        2        0\n1       a      1        1        2        0\n2       a      1        1        2        0\n3       b      1        1        1        1\n4       b      2        1        1        1\n5       b      0        1        1        1\n6       c      0        1        1        1\n7       c      1        1        1        1\n8       c      2        1        1        1\n</code></pre>\n",
        "question_body": "<p>For example, I have a df with two columns.</p>\n\n<p><strong>Input</strong></p>\n\n<pre><code>df = pd.DataFrame({'user_id':list('aaabbbccc'),'label':[0,0,1,0,0,2,0,1,2]})\nprint('df\\n',df)\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>df\n    label user_id\n0      0       a\n1      0       a\n2      1       a\n3      0       b\n4      0       b\n5      2       b\n6      0       c\n7      1       c\n8      2       c\n</code></pre>\n\n<p>I want to count the element in <code>label</code> group by user_id respectively. \nThe expected output is shown as follow.</p>\n\n<p><strong>Expected</strong></p>\n\n<pre><code>  df\n    label user_id  label_0  label_1  label_2\n0      0       a        2         1         0\n1      0       a        2         1         0\n2      1       a        2         1         0\n3      0       b        2         0         1\n4      0       b        2         0         1\n5      2       b        2         0         1\n6      0       c        1         1         1 \n7      1       c        1         1         1\n8      2       c        1         1         1\n</code></pre>\n\n<p>Briefly, in column <code>label_0</code>, I count the number of <code>0</code> in column <code>label</code> based on column <code>user_id</code>.</p>\n\n<p>Hopefully for help!</p>\n",
        "formatted_input": {
            "qid": 51265888,
            "link": "https://stackoverflow.com/questions/51265888/python-how-to-sum-unique-elements-respectively-of-a-dataframe-column-based-on-a",
            "question": {
                "title": "python: how to sum unique elements respectively of a dataframe column based on another column",
                "ques_desc": "For example, I have a df with two columns. Input Output I want to count the element in group by user_id respectively. The expected output is shown as follow. Expected Briefly, in column , I count the number of in column based on column . Hopefully for help! "
            },
            "io": [
                "df\n    label user_id\n0      0       a\n1      0       a\n2      1       a\n3      0       b\n4      0       b\n5      2       b\n6      0       c\n7      1       c\n8      2       c\n",
                "  df\n    label user_id  label_0  label_1  label_2\n0      0       a        2         1         0\n1      0       a        2         1         0\n2      1       a        2         1         0\n3      0       b        2         0         1\n4      0       b        2         0         1\n5      2       b        2         0         1\n6      0       c        1         1         1 \n7      1       c        1         1         1\n8      2       c        1         1         1\n"
            ],
            "answer": {
                "ans_desc": "Idea is create helper by with or and then and to original : Or using and with left join: ",
                "code": [
                    "df = (df.join(df.groupby(['user_id', 'label'])\n                .size()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n",
                    "df = (df.join(df.groupby('user_id')['label']\n                .value_counts()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n",
                    "df = (df.merge(pd.crosstab(df['user_id'], df['label'])\n                 .add_prefix('label_'), on='user_id', how='left'))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "series"
        ],
        "owner": {
            "reputation": 1127,
            "user_id": 9228407,
            "user_type": "registered",
            "accept_rate": 44,
            "profile_image": "https://www.gravatar.com/avatar/2afa510ffdff9be4f0ab5d75256fd178?s=128&d=identicon&r=PG&f=1",
            "display_name": "qwww",
            "link": "https://stackoverflow.com/users/9228407/qwww"
        },
        "is_answered": true,
        "view_count": 350,
        "accepted_answer_id": 51259324,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1531207216,
        "creation_date": 1531206889,
        "question_id": 51259229,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51259229/how-to-select-specific-column-items-as-list-from-pandas-dataframe",
        "title": "How to select specific column items as list from pandas dataframe?",
        "body": "<p>I have a dataframe like this :</p>\n\n<pre><code>  A  B  C   D \n---------------\n0  A  0  C  D\n1  A  0  C  D\n2  0  B  C  0\n3  A  0  0  D\n4  0  B  C  0\n5  A  0  0  0\n</code></pre>\n\n<p>How to convert it into this form (All zeros appearing are not considered) : </p>\n\n<pre><code>   A  B  C  D    E\n----------------------\n0  A  0  C  D  [A,C,D]\n1  A  0  C  D  [A,C,D]\n2  0  A  C  0  [A,C]\n3  A  0  0  D  [A,D]\n4  0  A  C  0  [A,C]\n5  A  0  0  0  [A]\n</code></pre>\n\n<p>And finally into a set of items like :</p>\n\n<pre><code>[{A,C,D},{A,C,D},{A,C},{A,D},{A,C},{A}]\n</code></pre>\n",
        "answer_body": "<p>Use nested list comprehension with filtering <code>0</code>:</p>\n\n<pre><code>#if 0 is number change '0' to 0\ndf['E'] = [[y for y in x if y != '0'] for x in df.values.tolist()]\nprint (df)\n   A  B  C  D          E\n0  A  0  C  D  [A, C, D]\n1  A  0  C  D  [A, C, D]\n2  0  B  C  0     [B, C]\n3  A  0  0  D     [A, D]\n4  0  B  C  0     [B, C]\n5  A  0  0  0        [A]\n</code></pre>\n\n<p>And for <code>set</code>s:</p>\n\n<pre><code>s = [set([y for y in x if y != '0']) for x in df.values.tolist()]\nprint (s)\n[{'A', 'D', 'C'}, {'A', 'D', 'C'}, {'C', 'B'}, {'A', 'D'}, {'C', 'B'}, {'A'}]\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this :</p>\n\n<pre><code>  A  B  C   D \n---------------\n0  A  0  C  D\n1  A  0  C  D\n2  0  B  C  0\n3  A  0  0  D\n4  0  B  C  0\n5  A  0  0  0\n</code></pre>\n\n<p>How to convert it into this form (All zeros appearing are not considered) : </p>\n\n<pre><code>   A  B  C  D    E\n----------------------\n0  A  0  C  D  [A,C,D]\n1  A  0  C  D  [A,C,D]\n2  0  A  C  0  [A,C]\n3  A  0  0  D  [A,D]\n4  0  A  C  0  [A,C]\n5  A  0  0  0  [A]\n</code></pre>\n\n<p>And finally into a set of items like :</p>\n\n<pre><code>[{A,C,D},{A,C,D},{A,C},{A,D},{A,C},{A}]\n</code></pre>\n",
        "formatted_input": {
            "qid": 51259229,
            "link": "https://stackoverflow.com/questions/51259229/how-to-select-specific-column-items-as-list-from-pandas-dataframe",
            "question": {
                "title": "How to select specific column items as list from pandas dataframe?",
                "ques_desc": "I have a dataframe like this : How to convert it into this form (All zeros appearing are not considered) : And finally into a set of items like : "
            },
            "io": [
                "  A  B  C   D \n---------------\n0  A  0  C  D\n1  A  0  C  D\n2  0  B  C  0\n3  A  0  0  D\n4  0  B  C  0\n5  A  0  0  0\n",
                "   A  B  C  D    E\n----------------------\n0  A  0  C  D  [A,C,D]\n1  A  0  C  D  [A,C,D]\n2  0  A  C  0  [A,C]\n3  A  0  0  D  [A,D]\n4  0  A  C  0  [A,C]\n5  A  0  0  0  [A]\n"
            ],
            "answer": {
                "ans_desc": "Use nested list comprehension with filtering : And for s: ",
                "code": [
                    "s = [set([y for y in x if y != '0']) for x in df.values.tolist()]\nprint (s)\n[{'A', 'D', 'C'}, {'A', 'D', 'C'}, {'C', 'B'}, {'A', 'D'}, {'C', 'B'}, {'A'}]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "list",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 857,
            "user_id": 9618242,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2e46a949858c559c6fcdcd5375afa2fb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Marisa",
            "link": "https://stackoverflow.com/users/9618242/marisa"
        },
        "is_answered": true,
        "view_count": 2096,
        "accepted_answer_id": 51186726,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1530791096,
        "creation_date": 1530779315,
        "question_id": 51186619,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51186619/convert-list-of-dictionaries-to-dataframe-with-one-column-for-keys-and-one-for-v",
        "title": "Convert list of dictionaries to dataframe with one column for keys and one for values",
        "body": "<p>Let's suppose I have the following list:</p>\n\n<p><code>list1 = [{'a': 1}, {'b': 2}, {'c': 3}]</code></p>\n\n<p>Which I want to convert it to a panda dataframe that have two columns: one for the keys, and one for the values.</p>\n\n<pre><code>    keys    values\n0    'a'      1\n1    'b'      2\n2    'c'      3\n</code></pre>\n\n<p>To do so, I have tried to use <code>pd.DataFrame(list1)</code> and also <code>pd.DataFrame.from_records(list1)</code>, but, in both cases, I get a dataframe like:</p>\n\n<pre><code>     a    b    c\n0  1.0  NaN  NaN\n1  NaN  2.0  NaN\n2  NaN  NaN  3.0\n</code></pre>\n\n<p>Is there any way to specify what I want? By doing research I could only find the way I am describing above.</p>\n",
        "answer_body": "<p>Use <code>list comprehension</code> with flattening for list of tuples:</p>\n\n<pre><code>df = pd.DataFrame([(i, j) for a in list1 for i, j in a.items()], \n                   columns=['keys','values'])\nprint (df)\n  keys  values\n0    a       1\n1    b       2\n2    c       3\n</code></pre>\n\n<p><strong>Detail</strong>:</p>\n\n<pre><code>print ([(i, j) for a in list1 for i, j in a.items()])\n\n[('a', 1), ('b', 2), ('c', 3)]\n</code></pre>\n",
        "question_body": "<p>Let's suppose I have the following list:</p>\n\n<p><code>list1 = [{'a': 1}, {'b': 2}, {'c': 3}]</code></p>\n\n<p>Which I want to convert it to a panda dataframe that have two columns: one for the keys, and one for the values.</p>\n\n<pre><code>    keys    values\n0    'a'      1\n1    'b'      2\n2    'c'      3\n</code></pre>\n\n<p>To do so, I have tried to use <code>pd.DataFrame(list1)</code> and also <code>pd.DataFrame.from_records(list1)</code>, but, in both cases, I get a dataframe like:</p>\n\n<pre><code>     a    b    c\n0  1.0  NaN  NaN\n1  NaN  2.0  NaN\n2  NaN  NaN  3.0\n</code></pre>\n\n<p>Is there any way to specify what I want? By doing research I could only find the way I am describing above.</p>\n",
        "formatted_input": {
            "qid": 51186619,
            "link": "https://stackoverflow.com/questions/51186619/convert-list-of-dictionaries-to-dataframe-with-one-column-for-keys-and-one-for-v",
            "question": {
                "title": "Convert list of dictionaries to dataframe with one column for keys and one for values",
                "ques_desc": "Let's suppose I have the following list: Which I want to convert it to a panda dataframe that have two columns: one for the keys, and one for the values. To do so, I have tried to use and also , but, in both cases, I get a dataframe like: Is there any way to specify what I want? By doing research I could only find the way I am describing above. "
            },
            "io": [
                "list1 = [{'a': 1}, {'b': 2}, {'c': 3}]",
                "     a    b    c\n0  1.0  NaN  NaN\n1  NaN  2.0  NaN\n2  NaN  NaN  3.0\n"
            ],
            "answer": {
                "ans_desc": "Use with flattening for list of tuples: Detail: ",
                "code": [
                    "print ([(i, j) for a in list1 for i, j in a.items()])\n\n[('a', 1), ('b', 2), ('c', 3)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 77,
            "user_id": 8069886,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c5a879565f5464605c8fa2bfd0cd8212?s=128&d=identicon&r=PG&f=1",
            "display_name": "sym",
            "link": "https://stackoverflow.com/users/8069886/sym"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 51143326,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1530564020,
        "creation_date": 1530562623,
        "question_id": 51143187,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51143187/create-new-column-depending-on-values-from-other-column",
        "title": "Create new column depending on values from other column",
        "body": "<p>I have a DataFrame that looks something like this:</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf=pd.DataFrame([['vt 40462',5,6],[5,6,6],[5,5,8],[4,3,1],['vl 6450',5,6],[5,6,7],\n             [1,2,3],['vt 40462',5,6],[5,5,8],['vl 658',6,7],[5,5,8],[4,3,1],['vt 40461',5,6],[5,5,8],\n             [7,8,5]],columns=['A','B','C'])\n</code></pre>\n\n<p>df</p>\n\n<pre><code>         A  B  C\n0 vt 40462  5  6\n1        5  6  6\n2        5  5  8\n3        4  3  1\n4  vl 6450  5  6\n5        5  6  7\n6        1  2  3\n7  vt 40462  5  6\n8        5  5  8\n9   vl 658  6  7\n10       5  5  8\n11       4  3  1\n12 vt 40461 5  6\n13        5 5  8\n14        7 8  5\n</code></pre>\n\n<p>I want to give indexes the values that are between  <code>vt</code> and <code>vl</code> in column <code>A</code> and create a new columns as :</p>\n\n<pre><code>         A  B  C  D\n0 vt 40462  5  6  vt 40462\n1        5  6  6  vt 40462\n2        5  5  8  vt 40462\n3        4  3  1  vt 40462\n4  vl 6450  5  6  vl 6450\n5        5  6  7  vl 6450\n6        1  2  3  vl 6450\n7 vt 40462  5  6  vt 40462\n8        5  5  8  vt 40462\n9   vl 658  6  7  vl 658\n10       5  5  8  vl 658\n11       4  3  1  vl 658\n12 vt 40461 5  6  vt 40461\n13        5 5  8  vt 40461\n14        7 8  5  vt 40461\n</code></pre>\n",
        "answer_body": "<p>Another way would be to <code>assign</code> column <code>D</code> to be all values of <code>A</code> that start with a letter, and then use <code>df.ffill()</code> to get rid of <code>NaN</code>s:</p>\n\n<pre><code>df.assign(D=df.loc[df.A.str.contains('^[A-Za-z]', na=False), 'A']).ffill()\n\n\n           A  B  C         D\n0   vt 40462  5  6  vt 40462\n1          5  6  6  vt 40462\n2          5  5  8  vt 40462\n3          4  3  1  vt 40462\n4    vl 6450  5  6   vl 6450\n5          5  6  7   vl 6450\n6          1  2  3   vl 6450\n7   vt 40462  5  6  vt 40462\n8          5  5  8  vt 40462\n9     vl 658  6  7    vl 658\n10         5  5  8    vl 658\n11         4  3  1    vl 658\n12  vt 40461  5  6  vt 40461\n13         5  5  8  vt 40461\n14         7  8  5  vt 40461\n</code></pre>\n\n<p>Or, more or less equivalently, but in 2 steps:</p>\n\n<pre><code>df.loc[df.A.astype(str).str.contains('^[A-Za-z]'), 'D'] = df.A\n\ndf.ffill()\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame that looks something like this:</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf=pd.DataFrame([['vt 40462',5,6],[5,6,6],[5,5,8],[4,3,1],['vl 6450',5,6],[5,6,7],\n             [1,2,3],['vt 40462',5,6],[5,5,8],['vl 658',6,7],[5,5,8],[4,3,1],['vt 40461',5,6],[5,5,8],\n             [7,8,5]],columns=['A','B','C'])\n</code></pre>\n\n<p>df</p>\n\n<pre><code>         A  B  C\n0 vt 40462  5  6\n1        5  6  6\n2        5  5  8\n3        4  3  1\n4  vl 6450  5  6\n5        5  6  7\n6        1  2  3\n7  vt 40462  5  6\n8        5  5  8\n9   vl 658  6  7\n10       5  5  8\n11       4  3  1\n12 vt 40461 5  6\n13        5 5  8\n14        7 8  5\n</code></pre>\n\n<p>I want to give indexes the values that are between  <code>vt</code> and <code>vl</code> in column <code>A</code> and create a new columns as :</p>\n\n<pre><code>         A  B  C  D\n0 vt 40462  5  6  vt 40462\n1        5  6  6  vt 40462\n2        5  5  8  vt 40462\n3        4  3  1  vt 40462\n4  vl 6450  5  6  vl 6450\n5        5  6  7  vl 6450\n6        1  2  3  vl 6450\n7 vt 40462  5  6  vt 40462\n8        5  5  8  vt 40462\n9   vl 658  6  7  vl 658\n10       5  5  8  vl 658\n11       4  3  1  vl 658\n12 vt 40461 5  6  vt 40461\n13        5 5  8  vt 40461\n14        7 8  5  vt 40461\n</code></pre>\n",
        "formatted_input": {
            "qid": 51143187,
            "link": "https://stackoverflow.com/questions/51143187/create-new-column-depending-on-values-from-other-column",
            "question": {
                "title": "Create new column depending on values from other column",
                "ques_desc": "I have a DataFrame that looks something like this: df I want to give indexes the values that are between and in column and create a new columns as : "
            },
            "io": [
                "         A  B  C\n0 vt 40462  5  6\n1        5  6  6\n2        5  5  8\n3        4  3  1\n4  vl 6450  5  6\n5        5  6  7\n6        1  2  3\n7  vt 40462  5  6\n8        5  5  8\n9   vl 658  6  7\n10       5  5  8\n11       4  3  1\n12 vt 40461 5  6\n13        5 5  8\n14        7 8  5\n",
                "         A  B  C  D\n0 vt 40462  5  6  vt 40462\n1        5  6  6  vt 40462\n2        5  5  8  vt 40462\n3        4  3  1  vt 40462\n4  vl 6450  5  6  vl 6450\n5        5  6  7  vl 6450\n6        1  2  3  vl 6450\n7 vt 40462  5  6  vt 40462\n8        5  5  8  vt 40462\n9   vl 658  6  7  vl 658\n10       5  5  8  vl 658\n11       4  3  1  vl 658\n12 vt 40461 5  6  vt 40461\n13        5 5  8  vt 40461\n14        7 8  5  vt 40461\n"
            ],
            "answer": {
                "ans_desc": "Another way would be to column to be all values of that start with a letter, and then use to get rid of s: Or, more or less equivalently, but in 2 steps: ",
                "code": [
                    "df.loc[df.A.astype(str).str.contains('^[A-Za-z]'), 'D'] = df.A\n\ndf.ffill()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 317,
            "user_id": 7298979,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ce95d42fe078a662325ff3a501763684?s=128&d=identicon&r=PG&f=1",
            "display_name": "user7298979",
            "link": "https://stackoverflow.com/users/7298979/user7298979"
        },
        "is_answered": true,
        "view_count": 54,
        "accepted_answer_id": 51141582,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1530554945,
        "creation_date": 1530554276,
        "last_edit_date": 1530554945,
        "question_id": 51141459,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51141459/map-filter-and-reduce-procedures-in-python",
        "title": "Map, Filter and Reduce procedures in Python",
        "body": "<p>I am working through understanding the concepts of map, filter and reduce in Python. I am working in Spyder IDE with Python v3.6. I have a data frame:</p>\n\n<pre><code>Cap    OC_y       GMWB         PE        Acc\n0.01    0.0065  0.560840708 0.646683673 0.515243902\n0.0105  0.0068  0.586725664 0.676530612 0.53902439\n0.011   0.0071  0.612610619 0.706377551 0.562804878\n0.0115  0.0073  0.629867257 0.72627551  0.578658537\n0.012   0.0076  0.655752212 0.756122449 0.602439024\n0.0125  0.0079  0.681637168 0.785969388 0.626219512\n0.013   0.0082  0.707522124 0.815816327 0.65\n0.0135  0.0085  0.73340708  0.845663265 0.673780488\n0.014   0.0087  0.750663717 0.865561224 0.689634146\n0.0145  0.009   0.776548673 0.895408163 0.713414634\n0.015   0.0093  0.802433628 0.925255102 0.737195122\n</code></pre>\n\n<p>I want to select Cap records in increments of .005. Please see below:</p>\n\n<pre><code>Cap    OC_y        GMWB          PE         Acc\n0.01    0.0065  0.560840708 0.646683673 0.515243902\n0.015   0.0093  0.802433628 0.925255102 0.737195122\n</code></pre>\n\n<p>In this case, wouldn't a map function work in this case? </p>\n\n<pre><code>map(lambda Cap: Cap + 0.05, data)\n</code></pre>\n\n<p>Any other option would be great. I ideally need it to work in a way where I can incrementally select the records based on a certain value. </p>\n",
        "answer_body": "<p>Assuming these are multiples of 0.005, you can divide and then use <code>np.isclose</code> to select.</p>\n\n<pre><code>v = df.Cap / 0.005\nout = df[np.isclose(v, v.astype(int))]\n</code></pre>\n\n<p>If this isn't the case, then I recommend subtracting the delta until it is, and then repeating the above:</p>\n\n<pre><code>v = (df.Cap - (df.Cap % 0.005).iat[0]) / 0.005\nout = df[np.isclose(v, v.astype(int))]\n</code></pre>\n\n<p></p>\n\n<pre><code>print(out)\n\n      Cap    OC_y      GMWB        PE       Acc\n0   0.010  0.0065  0.560841  0.646684  0.515244\n10  0.015  0.0093  0.802434  0.925255  0.737195\n</code></pre>\n",
        "question_body": "<p>I am working through understanding the concepts of map, filter and reduce in Python. I am working in Spyder IDE with Python v3.6. I have a data frame:</p>\n\n<pre><code>Cap    OC_y       GMWB         PE        Acc\n0.01    0.0065  0.560840708 0.646683673 0.515243902\n0.0105  0.0068  0.586725664 0.676530612 0.53902439\n0.011   0.0071  0.612610619 0.706377551 0.562804878\n0.0115  0.0073  0.629867257 0.72627551  0.578658537\n0.012   0.0076  0.655752212 0.756122449 0.602439024\n0.0125  0.0079  0.681637168 0.785969388 0.626219512\n0.013   0.0082  0.707522124 0.815816327 0.65\n0.0135  0.0085  0.73340708  0.845663265 0.673780488\n0.014   0.0087  0.750663717 0.865561224 0.689634146\n0.0145  0.009   0.776548673 0.895408163 0.713414634\n0.015   0.0093  0.802433628 0.925255102 0.737195122\n</code></pre>\n\n<p>I want to select Cap records in increments of .005. Please see below:</p>\n\n<pre><code>Cap    OC_y        GMWB          PE         Acc\n0.01    0.0065  0.560840708 0.646683673 0.515243902\n0.015   0.0093  0.802433628 0.925255102 0.737195122\n</code></pre>\n\n<p>In this case, wouldn't a map function work in this case? </p>\n\n<pre><code>map(lambda Cap: Cap + 0.05, data)\n</code></pre>\n\n<p>Any other option would be great. I ideally need it to work in a way where I can incrementally select the records based on a certain value. </p>\n",
        "formatted_input": {
            "qid": 51141459,
            "link": "https://stackoverflow.com/questions/51141459/map-filter-and-reduce-procedures-in-python",
            "question": {
                "title": "Map, Filter and Reduce procedures in Python",
                "ques_desc": "I am working through understanding the concepts of map, filter and reduce in Python. I am working in Spyder IDE with Python v3.6. I have a data frame: I want to select Cap records in increments of .005. Please see below: In this case, wouldn't a map function work in this case? Any other option would be great. I ideally need it to work in a way where I can incrementally select the records based on a certain value. "
            },
            "io": [
                "Cap    OC_y       GMWB         PE        Acc\n0.01    0.0065  0.560840708 0.646683673 0.515243902\n0.0105  0.0068  0.586725664 0.676530612 0.53902439\n0.011   0.0071  0.612610619 0.706377551 0.562804878\n0.0115  0.0073  0.629867257 0.72627551  0.578658537\n0.012   0.0076  0.655752212 0.756122449 0.602439024\n0.0125  0.0079  0.681637168 0.785969388 0.626219512\n0.013   0.0082  0.707522124 0.815816327 0.65\n0.0135  0.0085  0.73340708  0.845663265 0.673780488\n0.014   0.0087  0.750663717 0.865561224 0.689634146\n0.0145  0.009   0.776548673 0.895408163 0.713414634\n0.015   0.0093  0.802433628 0.925255102 0.737195122\n",
                "Cap    OC_y        GMWB          PE         Acc\n0.01    0.0065  0.560840708 0.646683673 0.515243902\n0.015   0.0093  0.802433628 0.925255102 0.737195122\n"
            ],
            "answer": {
                "ans_desc": "Assuming these are multiples of 0.005, you can divide and then use to select. If this isn't the case, then I recommend subtracting the delta until it is, and then repeating the above: ",
                "code": [
                    "v = df.Cap / 0.005\nout = df[np.isclose(v, v.astype(int))]\n",
                    "v = (df.Cap - (df.Cap % 0.005).iat[0]) / 0.005\nout = df[np.isclose(v, v.astype(int))]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-2.7",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 91,
            "user_id": 9969189,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c3fb17367f2ebd0be2be1478cc1a1a1b?s=128&d=identicon&r=PG&f=1",
            "display_name": "J.Calc",
            "link": "https://stackoverflow.com/users/9969189/j-calc"
        },
        "is_answered": true,
        "view_count": 60,
        "accepted_answer_id": 51054932,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1530078004,
        "creation_date": 1530075798,
        "question_id": 51054844,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51054844/python-pandas-find-maximum-value-only-from-a-specific-part-of-a-column",
        "title": "Python Pandas | Find maximum value only from a specific part of a column",
        "body": "<p>I have been trying to do this. Pandas max() would find the maximum value from the entire column. What I need is:</p>\n\n<p>My input csv file:</p>\n\n<pre><code>Id  Param1          Param2              Val1\n1  -5.00138282776   2.04990620034e-08   1.738e-05\n1  -4.80147838593   2.01516989762e-08   1.628e-05\n1  -4.60159301758   1.98263165885e-08   1.671e-05\n1  -4.40133094788   1.94918392538e-08   1.576e-05\n1  -4.20143127441   1.91767686175e-08   \n2  -5.00141859055   6.88369405921e-09   5.512e-06\n2  -4.80152130126   6.77335965093e-09   5.964e-06\n2  -4.60163593292   6.65415056389e-09\n3  -5.00138044357   1.16316911658e-08   4.008e-06\n3  -4.80148792267   1.15515588206e-08   7.347e-06\n3  -4.60160970681   1.14048361866e-08   8.446e-06\n3  -4.40137386322   1.12357021465e-08   \n</code></pre>\n\n<p>Output needed:</p>\n\n<pre><code>Id  Param1          Param2              Val1        Max_Val1_for_each_Id\n1  -5.00138282776   2.04990620034e-08   1.738e-05   1.738e-05\n1  -4.80147838593   2.01516989762e-08   1.628e-05\n1  -4.60159301758   1.98263165885e-08   1.671e-05\n1  -4.40133094788   1.94918392538e-08   1.576e-05\n1  -4.20143127441   1.91767686175e-08   \n2  -5.00141859055   6.88369405921e-09   5.512e-06   5.964e-06\n2  -4.80152130126   6.77335965093e-09   5.964e-06\n2  -4.60163593292   6.65415056389e-09\n3  -5.00138044357   1.16316911658e-08   4.008e-06   8.446e-06\n3  -4.80148792267   1.15515588206e-08   7.347e-06\n3  -4.60160970681   1.14048361866e-08   8.446e-06\n3  -4.40137386322   1.12357021465e-08 \n</code></pre>\n\n<p>I am not sure how to select/group values from Val1 column with the same Id and then find their maximum value. Also, I have some blanks in the Val1 column, rendering its datatype as object. I don't know how to go about this. Any help would be most welcome.</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>GroupBy.transform</code></a> for new column of <code>max</code> values per group:</p>\n\n<pre><code>df['Max_Val1_for_each_Id'] = df.groupby('Id')['Val1'].transform('max')\nprint (df)\n    Id    Param1        Param2      Val1  Max_Val1_for_each_Id\n0    1 -5.001383  2.049906e-08  0.000017              0.000017\n1    1 -4.801478  2.015170e-08  0.000016              0.000017\n2    1 -4.601593  1.982632e-08  0.000017              0.000017\n3    1 -4.401331  1.949184e-08  0.000016              0.000017\n4    1 -4.201431  1.917677e-08       NaN              0.000017\n5    2 -5.001419  6.883694e-09  0.000006              0.000006\n6    2 -4.801521  6.773360e-09  0.000006              0.000006\n7    2 -4.601636  6.654151e-09       NaN              0.000006\n8    3 -5.001380  1.163169e-08  0.000004              0.000008\n9    3 -4.801488  1.155156e-08  0.000007              0.000008\n10   3 -4.601610  1.140484e-08  0.000008              0.000008\n11   3 -4.401374  1.123570e-08       NaN              0.000008\n</code></pre>\n\n<p>And then if need only first value add <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.where.html\" rel=\"nofollow noreferrer\"><code>where</code></a> with mask created by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.duplicated.html\" rel=\"nofollow noreferrer\"><code>duplicated</code></a> with <code>~</code> for invert mask:</p>\n\n<pre><code>df['Max_Val1_for_each_Id'] = df['Max_Val1_for_each_Id'].where(~df['Id'].duplicated())\nprint (df)\n    Id    Param1        Param2      Val1  Max_Val1_for_each_Id\n0    1 -5.001383  2.049906e-08  0.000017              0.000017\n1    1 -4.801478  2.015170e-08  0.000016                   NaN\n2    1 -4.601593  1.982632e-08  0.000017                   NaN\n3    1 -4.401331  1.949184e-08  0.000016                   NaN\n4    1 -4.201431  1.917677e-08       NaN                   NaN\n5    2 -5.001419  6.883694e-09  0.000006              0.000006\n6    2 -4.801521  6.773360e-09  0.000006                   NaN\n7    2 -4.601636  6.654151e-09       NaN                   NaN\n8    3 -5.001380  1.163169e-08  0.000004              0.000008\n9    3 -4.801488  1.155156e-08  0.000007                   NaN\n10   3 -4.601610  1.140484e-08  0.000008                   NaN\n11   3 -4.401374  1.123570e-08       NaN                   NaN\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>If <code>Val1</code> have no <code>NaN</code> values and solution above raise error:</p>\n\n<blockquote>\n  <p>TypeError: '>=' not supported between instances of 'float' and 'str'</p>\n</blockquote>\n\n<p>first step is convert non numeric to <code>NaN</code>s:</p>\n\n<pre><code>df['Val1'] = pd.to_numeric(df['Val1'], errors='coerce')\ndf['Max_Val1_for_each_Id'] = df.groupby('Id')['Val1'].transform('max')\ndf['Max_Val1_for_each_Id'] = df['Max_Val1_for_each_Id'].where(~df['Id'].duplicated())\n</code></pre>\n",
        "question_body": "<p>I have been trying to do this. Pandas max() would find the maximum value from the entire column. What I need is:</p>\n\n<p>My input csv file:</p>\n\n<pre><code>Id  Param1          Param2              Val1\n1  -5.00138282776   2.04990620034e-08   1.738e-05\n1  -4.80147838593   2.01516989762e-08   1.628e-05\n1  -4.60159301758   1.98263165885e-08   1.671e-05\n1  -4.40133094788   1.94918392538e-08   1.576e-05\n1  -4.20143127441   1.91767686175e-08   \n2  -5.00141859055   6.88369405921e-09   5.512e-06\n2  -4.80152130126   6.77335965093e-09   5.964e-06\n2  -4.60163593292   6.65415056389e-09\n3  -5.00138044357   1.16316911658e-08   4.008e-06\n3  -4.80148792267   1.15515588206e-08   7.347e-06\n3  -4.60160970681   1.14048361866e-08   8.446e-06\n3  -4.40137386322   1.12357021465e-08   \n</code></pre>\n\n<p>Output needed:</p>\n\n<pre><code>Id  Param1          Param2              Val1        Max_Val1_for_each_Id\n1  -5.00138282776   2.04990620034e-08   1.738e-05   1.738e-05\n1  -4.80147838593   2.01516989762e-08   1.628e-05\n1  -4.60159301758   1.98263165885e-08   1.671e-05\n1  -4.40133094788   1.94918392538e-08   1.576e-05\n1  -4.20143127441   1.91767686175e-08   \n2  -5.00141859055   6.88369405921e-09   5.512e-06   5.964e-06\n2  -4.80152130126   6.77335965093e-09   5.964e-06\n2  -4.60163593292   6.65415056389e-09\n3  -5.00138044357   1.16316911658e-08   4.008e-06   8.446e-06\n3  -4.80148792267   1.15515588206e-08   7.347e-06\n3  -4.60160970681   1.14048361866e-08   8.446e-06\n3  -4.40137386322   1.12357021465e-08 \n</code></pre>\n\n<p>I am not sure how to select/group values from Val1 column with the same Id and then find their maximum value. Also, I have some blanks in the Val1 column, rendering its datatype as object. I don't know how to go about this. Any help would be most welcome.</p>\n",
        "formatted_input": {
            "qid": 51054844,
            "link": "https://stackoverflow.com/questions/51054844/python-pandas-find-maximum-value-only-from-a-specific-part-of-a-column",
            "question": {
                "title": "Python Pandas | Find maximum value only from a specific part of a column",
                "ques_desc": "I have been trying to do this. Pandas max() would find the maximum value from the entire column. What I need is: My input csv file: Output needed: I am not sure how to select/group values from Val1 column with the same Id and then find their maximum value. Also, I have some blanks in the Val1 column, rendering its datatype as object. I don't know how to go about this. Any help would be most welcome. "
            },
            "io": [
                "Id  Param1          Param2              Val1\n1  -5.00138282776   2.04990620034e-08   1.738e-05\n1  -4.80147838593   2.01516989762e-08   1.628e-05\n1  -4.60159301758   1.98263165885e-08   1.671e-05\n1  -4.40133094788   1.94918392538e-08   1.576e-05\n1  -4.20143127441   1.91767686175e-08   \n2  -5.00141859055   6.88369405921e-09   5.512e-06\n2  -4.80152130126   6.77335965093e-09   5.964e-06\n2  -4.60163593292   6.65415056389e-09\n3  -5.00138044357   1.16316911658e-08   4.008e-06\n3  -4.80148792267   1.15515588206e-08   7.347e-06\n3  -4.60160970681   1.14048361866e-08   8.446e-06\n3  -4.40137386322   1.12357021465e-08   \n",
                "Id  Param1          Param2              Val1        Max_Val1_for_each_Id\n1  -5.00138282776   2.04990620034e-08   1.738e-05   1.738e-05\n1  -4.80147838593   2.01516989762e-08   1.628e-05\n1  -4.60159301758   1.98263165885e-08   1.671e-05\n1  -4.40133094788   1.94918392538e-08   1.576e-05\n1  -4.20143127441   1.91767686175e-08   \n2  -5.00141859055   6.88369405921e-09   5.512e-06   5.964e-06\n2  -4.80152130126   6.77335965093e-09   5.964e-06\n2  -4.60163593292   6.65415056389e-09\n3  -5.00138044357   1.16316911658e-08   4.008e-06   8.446e-06\n3  -4.80148792267   1.15515588206e-08   7.347e-06\n3  -4.60160970681   1.14048361866e-08   8.446e-06\n3  -4.40137386322   1.12357021465e-08 \n"
            ],
            "answer": {
                "ans_desc": "Use for new column of values per group: And then if need only first value add with mask created by with for invert mask: EDIT: If have no values and solution above raise error: TypeError: '>=' not supported between instances of 'float' and 'str' first step is convert non numeric to s: ",
                "code": [
                    "df['Val1'] = pd.to_numeric(df['Val1'], errors='coerce')\ndf['Max_Val1_for_each_Id'] = df.groupby('Id')['Val1'].transform('max')\ndf['Max_Val1_for_each_Id'] = df['Max_Val1_for_each_Id'].where(~df['Id'].duplicated())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "postgresql",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 115,
            "user_id": 1783974,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://www.gravatar.com/avatar/71e4cdc52d146c27ee9121ebc7a5ccdb?s=128&d=identicon&r=PG",
            "display_name": "O.D.P",
            "link": "https://stackoverflow.com/users/1783974/o-d-p"
        },
        "is_answered": true,
        "view_count": 486,
        "accepted_answer_id": 51051689,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1530049099,
        "creation_date": 1530024806,
        "last_edit_date": 1530049099,
        "question_id": 51045848,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51045848/improve-pandas-filter-speed-by-storing-indices",
        "title": "Improve pandas filter speed by storing indices?",
        "body": "<p>I have the following df:</p>\n\n<pre><code>df = pd.DataFrame({'ID1':[1,2,3,4,5,6],'ID2':[2,6,6,2,1,2],'AREA':[1,1,1,1,1,1]})\n...\n\n    ID1 ID2 AREA\n0   1   2   1\n1   2   6   1\n2   3   6   1\n3   4   2   1\n4   5   1   1\n5   6   2   1\n</code></pre>\n\n<p>I accumulate the AREA column as so:</p>\n\n<pre><code>for id_ in df.ID1:   \n    id1_filter = df.ID1 == id_\n    id2_filter = (df.ID1 == id_) | (df.ID2 == id_)\n    df.loc[id1_filter, 'AREA'] = df.loc[id2_filter].AREA.sum()\n\nprint(df)\n...\nID1 ID2 AREA\n0   1   2   2\n1   2   6   5\n2   3   6   1\n3   4   2   1\n4   5   1   1\n5   6   2   7\n</code></pre>\n\n<p>For each <code>id_</code> in <code>ID1</code>, the <code>AREA</code> is summed where <code>ID1</code> == <code>id_</code> or <code>ID2 == id_</code>, \nand it is always run when <code>df</code> is sorted on <code>ID1</code>.</p>\n\n<p>The real dataframe I'm working on though is 150,000 records, each row belonging to a unique ID1.\nRunning the above on this dataframe takes 2.5 hours.  Since this operation will take place repeatedly\nfor the foreseeable future, I decided to store the indices of the True values in <code>id1_filter</code> and <code>id2_filter</code>\nin a DB with the following schema.</p>\n\n<p><strong>Table ID1:</strong> </p>\n\n<pre><code>ID_,INDEX_\n1  ,   0\n2  ,   1\netc, ect\n</code></pre>\n\n<p><strong>Table ID2:</strong></p>\n\n<pre><code>ID_,INDEX_\n1  ,   0\n1  ,   4\n2  ,   0\n2  ,   1\n2  ,   3\n2  ,   5\netc, etc\n</code></pre>\n\n<p>The next time I run the accumulation on the <code>AREA</code> column (now filled with different <code>AREA</code> values)\nI read in the sql tables and the convert them to dicts.  I then use these dicts\nto grab the records I need during the summing loop.</p>\n\n<pre><code>id1_dict = pd.read_sql('select * from ID1',db_engine).groupby('ID_').INDEX_.unique().to_dict()\nid2_dict = pd.read_sql('select * from ID2',db_engine).groupby('ID_').INDEX_.unique().to_dict()\n\n# print indices for id1_filter and id2_fillter for id 1\nprint(id1_dict[1])\nprint(id2_dict[1])\n...\n[0]\n[0, 4]\n\n for id_ in df.ID1:\n        df.loc[id1_dict[id_], 'AREA'] = df.loc[id2_dict[id_]].AREA.sum()\n</code></pre>\n\n<p>When run this way it only takes 6 minutes!</p>\n\n<p>My question: Is there a better/standard way to handle this scenario, i.e  storing dataframe selections for \nlater use?  Side note, I have set an index on the SQL table's ID columns and tried getting the\nindices by querying the table for each id, and it works well, but still takes a little longer than the above (9 mins).</p>\n",
        "answer_body": "<p>One way to do it is like this:</p>\n\n<pre><code>df = df.set_index('ID1') \nfor row in df.join(df.groupby('ID2')['AREA'].apply(lambda x: x.index.tolist()),rsuffix='_').dropna().itertuples():\n    df.loc[row[0],'AREA'] += df.loc[row[3],'AREA'].sum()\ndf = df.reset_index()\n</code></pre>\n\n<p>and you get the result expected </p>\n\n<pre><code>   ID1  ID2  AREA\n0    1    2     2\n1    2    6     5\n2    3    6     1\n3    4    2     1\n4    5    1     1\n5    6    2     7\n</code></pre>\n\n<p>Now on a bigger <code>df</code> like:</p>\n\n<pre><code>df = pd.DataFrame( {'ID1':range(1,1501),'ID2': np.random.randint(1,1501,(1500,)),'AREA':[1]*1500}, \n                   columns = ['ID1','ID2','AREA'])\n</code></pre>\n\n<p>The method presented here turns in about 0.76 s on my computer while your first is running in 6.5 s.</p>\n\n<p>Ultimately, you could create a <code>df_list</code> such as:</p>\n\n<pre><code>df_list = (df.set_index('ID1')\n             .join(df.set_index('ID1').groupby('ID2')['AREA']\n                     .apply(lambda x: x.index.tolist()),rsuffix='_ID2')\n             .dropna().drop(['AREA','ID2'],1))\n</code></pre>\n\n<p>to keep somewhere the information that linked ID1 and ID2: here you can see the id is equal to 2 in the column ID2, where the value of ID1 = 1, 4 and 6 </p>\n\n<pre><code>      AREA_ID2\nID1           \n1          [5]\n2    [1, 4, 6]\n6       [2, 3]\n</code></pre>\n\n<p>and then you can run to not re-create the <code>df_list</code>, with a small difference in the code:</p>\n\n<pre><code>df = df.set_index('ID1') \nfor row in df_list.itertuples():\n    df.loc[row[0],'AREA'] += df.loc[row[1],'AREA'].sum()\ndf = df.reset_index()\n</code></pre>\n\n<p>Hope it's faster</p>\n",
        "question_body": "<p>I have the following df:</p>\n\n<pre><code>df = pd.DataFrame({'ID1':[1,2,3,4,5,6],'ID2':[2,6,6,2,1,2],'AREA':[1,1,1,1,1,1]})\n...\n\n    ID1 ID2 AREA\n0   1   2   1\n1   2   6   1\n2   3   6   1\n3   4   2   1\n4   5   1   1\n5   6   2   1\n</code></pre>\n\n<p>I accumulate the AREA column as so:</p>\n\n<pre><code>for id_ in df.ID1:   \n    id1_filter = df.ID1 == id_\n    id2_filter = (df.ID1 == id_) | (df.ID2 == id_)\n    df.loc[id1_filter, 'AREA'] = df.loc[id2_filter].AREA.sum()\n\nprint(df)\n...\nID1 ID2 AREA\n0   1   2   2\n1   2   6   5\n2   3   6   1\n3   4   2   1\n4   5   1   1\n5   6   2   7\n</code></pre>\n\n<p>For each <code>id_</code> in <code>ID1</code>, the <code>AREA</code> is summed where <code>ID1</code> == <code>id_</code> or <code>ID2 == id_</code>, \nand it is always run when <code>df</code> is sorted on <code>ID1</code>.</p>\n\n<p>The real dataframe I'm working on though is 150,000 records, each row belonging to a unique ID1.\nRunning the above on this dataframe takes 2.5 hours.  Since this operation will take place repeatedly\nfor the foreseeable future, I decided to store the indices of the True values in <code>id1_filter</code> and <code>id2_filter</code>\nin a DB with the following schema.</p>\n\n<p><strong>Table ID1:</strong> </p>\n\n<pre><code>ID_,INDEX_\n1  ,   0\n2  ,   1\netc, ect\n</code></pre>\n\n<p><strong>Table ID2:</strong></p>\n\n<pre><code>ID_,INDEX_\n1  ,   0\n1  ,   4\n2  ,   0\n2  ,   1\n2  ,   3\n2  ,   5\netc, etc\n</code></pre>\n\n<p>The next time I run the accumulation on the <code>AREA</code> column (now filled with different <code>AREA</code> values)\nI read in the sql tables and the convert them to dicts.  I then use these dicts\nto grab the records I need during the summing loop.</p>\n\n<pre><code>id1_dict = pd.read_sql('select * from ID1',db_engine).groupby('ID_').INDEX_.unique().to_dict()\nid2_dict = pd.read_sql('select * from ID2',db_engine).groupby('ID_').INDEX_.unique().to_dict()\n\n# print indices for id1_filter and id2_fillter for id 1\nprint(id1_dict[1])\nprint(id2_dict[1])\n...\n[0]\n[0, 4]\n\n for id_ in df.ID1:\n        df.loc[id1_dict[id_], 'AREA'] = df.loc[id2_dict[id_]].AREA.sum()\n</code></pre>\n\n<p>When run this way it only takes 6 minutes!</p>\n\n<p>My question: Is there a better/standard way to handle this scenario, i.e  storing dataframe selections for \nlater use?  Side note, I have set an index on the SQL table's ID columns and tried getting the\nindices by querying the table for each id, and it works well, but still takes a little longer than the above (9 mins).</p>\n",
        "formatted_input": {
            "qid": 51045848,
            "link": "https://stackoverflow.com/questions/51045848/improve-pandas-filter-speed-by-storing-indices",
            "question": {
                "title": "Improve pandas filter speed by storing indices?",
                "ques_desc": "I have the following df: I accumulate the AREA column as so: For each in , the is summed where == or , and it is always run when is sorted on . The real dataframe I'm working on though is 150,000 records, each row belonging to a unique ID1. Running the above on this dataframe takes 2.5 hours. Since this operation will take place repeatedly for the foreseeable future, I decided to store the indices of the True values in and in a DB with the following schema. Table ID1: Table ID2: The next time I run the accumulation on the column (now filled with different values) I read in the sql tables and the convert them to dicts. I then use these dicts to grab the records I need during the summing loop. When run this way it only takes 6 minutes! My question: Is there a better/standard way to handle this scenario, i.e storing dataframe selections for later use? Side note, I have set an index on the SQL table's ID columns and tried getting the indices by querying the table for each id, and it works well, but still takes a little longer than the above (9 mins). "
            },
            "io": [
                "ID_,INDEX_\n1  ,   0\n2  ,   1\netc, ect\n",
                "ID_,INDEX_\n1  ,   0\n1  ,   4\n2  ,   0\n2  ,   1\n2  ,   3\n2  ,   5\netc, etc\n"
            ],
            "answer": {
                "ans_desc": "One way to do it is like this: and you get the result expected Now on a bigger like: The method presented here turns in about 0.76 s on my computer while your first is running in 6.5 s. Ultimately, you could create a such as: to keep somewhere the information that linked ID1 and ID2: here you can see the id is equal to 2 in the column ID2, where the value of ID1 = 1, 4 and 6 and then you can run to not re-create the , with a small difference in the code: Hope it's faster ",
                "code": [
                    "df = df.set_index('ID1') \nfor row in df.join(df.groupby('ID2')['AREA'].apply(lambda x: x.index.tolist()),rsuffix='_').dropna().itertuples():\n    df.loc[row[0],'AREA'] += df.loc[row[3],'AREA'].sum()\ndf = df.reset_index()\n",
                    "df = pd.DataFrame( {'ID1':range(1,1501),'ID2': np.random.randint(1,1501,(1500,)),'AREA':[1]*1500}, \n                   columns = ['ID1','ID2','AREA'])\n",
                    "df_list = (df.set_index('ID1')\n             .join(df.set_index('ID1').groupby('ID2')['AREA']\n                     .apply(lambda x: x.index.tolist()),rsuffix='_ID2')\n             .dropna().drop(['AREA','ID2'],1))\n",
                    "df = df.set_index('ID1') \nfor row in df_list.itertuples():\n    df.loc[row[0],'AREA'] += df.loc[row[1],'AREA'].sum()\ndf = df.reset_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 98,
            "user_id": 3945974,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/xcNti.png?s=128&g=1",
            "display_name": "koray1396",
            "link": "https://stackoverflow.com/users/3945974/koray1396"
        },
        "is_answered": true,
        "view_count": 1405,
        "accepted_answer_id": 51041864,
        "answer_count": 3,
        "score": 5,
        "last_activity_date": 1530018453,
        "creation_date": 1530006571,
        "last_edit_date": 1530007010,
        "question_id": 51039857,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51039857/pandas-count-values-greater-than-current-row-in-the-last-n-rows",
        "title": "Pandas count values greater than current row in the last n rows",
        "body": "<p>How to get count of values greater than current row in the last n rows?</p>\n\n<p>Imagine we have a dataframe as following:</p>\n\n<pre><code>    col_a\n0    8.4\n1   11.3\n2    7.2\n3    6.5\n4    4.5\n5    8.9\n</code></pre>\n\n<p>I am trying to get a table such as following where n=3.</p>\n\n<pre><code>    col_a   col_b\n0     8.4       0\n1    11.3       0\n2     7.2       2\n3     6.5       3\n4     4.5       3\n5     8.9       0\n</code></pre>\n\n<p>Thanks in advance.</p>\n",
        "answer_body": "<p>In pandas is best dont loop because slow, here is better use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html\" rel=\"noreferrer\"><code>rolling</code></a> with custom function:</p>\n\n<pre><code>n = 3\ndf['new'] = (df['col_a'].rolling(n+1, min_periods=1)\n                        .apply(lambda x: (x[-1] &lt; x[:-1]).sum())\n                        .astype(int))\nprint (df)\n   col_a  new\n0    8.4    0\n1   11.3    0\n2    7.2    2\n3    6.5    3\n4    4.5    3\n5    8.9    0\n</code></pre>\n\n<p>If performance is important, use <a href=\"https://stackoverflow.com/a/48884154/2901002\">strides</a>:</p>\n\n<pre><code>n = 3\nx = np.concatenate([[np.nan] * (n), df['col_a'].values])\n\ndef rolling_window(a, window):\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\narr = rolling_window(x, n + 1)\n\ndf['new'] = (arr[:, :-1] &gt; arr[:, [-1]]).sum(axis=1)\nprint (df)\n   col_a  new\n0    8.4    0\n1   11.3    0\n2    7.2    2\n3    6.5    3\n4    4.5    3\n5    8.9    0\n</code></pre>\n\n<p><strong>Performance</strong>: Here is used <a href=\"https://github.com/nschloe/perfplot\" rel=\"noreferrer\"><code>perfplot</code></a> in small window <code>n = 3</code>:</p>\n\n<p><a href=\"https://i.stack.imgur.com/BpkDU.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/BpkDU.png\" alt=\"g1\"></a></p>\n\n<pre><code>np.random.seed(1256)\nn = 3\n\ndef rolling_window(a, window):\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\ndef roll(df):\n    df['new'] = (df['col_a'].rolling(n+1, min_periods=1).apply(lambda x: (x[-1] &lt; x[:-1]).sum(), raw=True).astype(int))\n    return df\n\ndef list_comp(df):\n    df['count'] = [(j &lt; df['col_a'].iloc[max(0, i-3):i]).sum() for i, j in df['col_a'].items()]\n    return df\n\ndef strides(df):\n    x = np.concatenate([[np.nan] * (n), df['col_a'].values])\n    arr = rolling_window(x, n + 1)\n    df['new1'] = (arr[:, :-1] &gt; arr[:, [-1]]).sum(axis=1)\n    return df\n\n\ndef make_df(n):\n    df = pd.DataFrame(np.random.randint(20, size=n), columns=['col_a'])\n    return df\n\nperfplot.show(\n    setup=make_df,\n    kernels=[list_comp, roll, strides],\n    n_range=[2**k for k in range(2, 15)],\n    logx=True,\n    logy=True,\n    xlabel='len(df)')\n</code></pre>\n\n<p>Also I was curious about performance in large window, <code>n = 100</code>:</p>\n\n<p><a href=\"https://i.stack.imgur.com/qjp1j.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/qjp1j.png\" alt=\"g2\"></a></p>\n",
        "question_body": "<p>How to get count of values greater than current row in the last n rows?</p>\n\n<p>Imagine we have a dataframe as following:</p>\n\n<pre><code>    col_a\n0    8.4\n1   11.3\n2    7.2\n3    6.5\n4    4.5\n5    8.9\n</code></pre>\n\n<p>I am trying to get a table such as following where n=3.</p>\n\n<pre><code>    col_a   col_b\n0     8.4       0\n1    11.3       0\n2     7.2       2\n3     6.5       3\n4     4.5       3\n5     8.9       0\n</code></pre>\n\n<p>Thanks in advance.</p>\n",
        "formatted_input": {
            "qid": 51039857,
            "link": "https://stackoverflow.com/questions/51039857/pandas-count-values-greater-than-current-row-in-the-last-n-rows",
            "question": {
                "title": "Pandas count values greater than current row in the last n rows",
                "ques_desc": "How to get count of values greater than current row in the last n rows? Imagine we have a dataframe as following: I am trying to get a table such as following where n=3. Thanks in advance. "
            },
            "io": [
                "    col_a\n0    8.4\n1   11.3\n2    7.2\n3    6.5\n4    4.5\n5    8.9\n",
                "    col_a   col_b\n0     8.4       0\n1    11.3       0\n2     7.2       2\n3     6.5       3\n4     4.5       3\n5     8.9       0\n"
            ],
            "answer": {
                "ans_desc": "In pandas is best dont loop because slow, here is better use with custom function: If performance is important, use strides: Performance: Here is used in small window : Also I was curious about performance in large window, : ",
                "code": [
                    "np.random.seed(1256)\nn = 3\n\ndef rolling_window(a, window):\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\ndef roll(df):\n    df['new'] = (df['col_a'].rolling(n+1, min_periods=1).apply(lambda x: (x[-1] < x[:-1]).sum(), raw=True).astype(int))\n    return df\n\ndef list_comp(df):\n    df['count'] = [(j < df['col_a'].iloc[max(0, i-3):i]).sum() for i, j in df['col_a'].items()]\n    return df\n\ndef strides(df):\n    x = np.concatenate([[np.nan] * (n), df['col_a'].values])\n    arr = rolling_window(x, n + 1)\n    df['new1'] = (arr[:, :-1] > arr[:, [-1]]).sum(axis=1)\n    return df\n\n\ndef make_df(n):\n    df = pd.DataFrame(np.random.randint(20, size=n), columns=['col_a'])\n    return df\n\nperfplot.show(\n    setup=make_df,\n    kernels=[list_comp, roll, strides],\n    n_range=[2**k for k in range(2, 15)],\n    logx=True,\n    logy=True,\n    xlabel='len(df)')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3971,
            "user_id": 7585973,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/1cf9f40e4f8e5e17076d55814a9c2ad9?s=128&d=identicon&r=PG",
            "display_name": "Nabih Bawazir",
            "link": "https://stackoverflow.com/users/7585973/nabih-bawazir"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 51018126,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1529912091,
        "creation_date": 1529911074,
        "last_edit_date": 1529912091,
        "question_id": 51018104,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51018104/how-to-group-phone-number-with-and-without-country-code",
        "title": "How to group phone number with and without country code",
        "body": "<p>I am trying to detect phone number, my country code is <code>+62</code> but some phone manufacturer or operator use <code>0</code> and <code>+62</code>, after query and pivoting I get pivoted data. But, the pivoted data is out of context</p>\n\n<p>Here's the pivoted data</p>\n\n<pre><code>Id    +623684682   03684682   +623684684   03684684\n1              1          0            1          1\n2              1          1            2          1\n</code></pre>\n\n<p>Here's what I need to group, but I don't want to group manually (<code>+623684682</code> and <code>03684682</code> is same, etc)</p>\n\n<pre><code>Id      03684682   03684684\n1              1          2\n2              2          3\n</code></pre>\n",
        "answer_body": "<p>I think need <code>replace</code> with aggregate <code>sum</code>:</p>\n\n<pre><code>df = df.groupby(lambda x: x.replace('+62','0'), axis=1).sum()\n</code></pre>\n\n<p>Or <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.replace.html\" rel=\"nofollow noreferrer\"><code>replace</code></a> columns names and <code>sum</code>:</p>\n\n<pre><code>df.columns = df.columns.str.replace('\\+62','0')\ndf = df.sum(level=0, axis=1)\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n    03684682  03684684\nId                    \n1          1         2\n2          2         3\n</code></pre>\n",
        "question_body": "<p>I am trying to detect phone number, my country code is <code>+62</code> but some phone manufacturer or operator use <code>0</code> and <code>+62</code>, after query and pivoting I get pivoted data. But, the pivoted data is out of context</p>\n\n<p>Here's the pivoted data</p>\n\n<pre><code>Id    +623684682   03684682   +623684684   03684684\n1              1          0            1          1\n2              1          1            2          1\n</code></pre>\n\n<p>Here's what I need to group, but I don't want to group manually (<code>+623684682</code> and <code>03684682</code> is same, etc)</p>\n\n<pre><code>Id      03684682   03684684\n1              1          2\n2              2          3\n</code></pre>\n",
        "formatted_input": {
            "qid": 51018104,
            "link": "https://stackoverflow.com/questions/51018104/how-to-group-phone-number-with-and-without-country-code",
            "question": {
                "title": "How to group phone number with and without country code",
                "ques_desc": "I am trying to detect phone number, my country code is but some phone manufacturer or operator use and , after query and pivoting I get pivoted data. But, the pivoted data is out of context Here's the pivoted data Here's what I need to group, but I don't want to group manually ( and is same, etc) "
            },
            "io": [
                "Id    +623684682   03684682   +623684684   03684684\n1              1          0            1          1\n2              1          1            2          1\n",
                "Id      03684682   03684684\n1              1          2\n2              2          3\n"
            ],
            "answer": {
                "ans_desc": "I think need with aggregate : Or columns names and : ",
                "code": [
                    "df = df.groupby(lambda x: x.replace('+62','0'), axis=1).sum()\n",
                    "df.columns = df.columns.str.replace('\\+62','0')\ndf = df.sum(level=0, axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 1795,
            "user_id": 5059335,
            "user_type": "registered",
            "accept_rate": 74,
            "profile_image": "https://www.gravatar.com/avatar/b0ca397cbec3c1264f0f22c6cab4e280?s=128&d=identicon&r=PG&f=1",
            "display_name": "haimen",
            "link": "https://stackoverflow.com/users/5059335/haimen"
        },
        "is_answered": true,
        "view_count": 972,
        "accepted_answer_id": 50958247,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1529731312,
        "creation_date": 1529534100,
        "last_edit_date": 1529731312,
        "question_id": 50957993,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50957993/fill-all-values-in-a-group-with-the-first-non-null-value-in-that-group",
        "title": "Fill all values in a group with the first non-null value in that group",
        "body": "<p>The following is the pandas dataframe I have:</p>\n\n<pre><code>cluster Value\n1         A\n1        NaN\n1        NaN\n1        NaN\n1        NaN\n2        NaN\n2        NaN\n2         B\n2        NaN\n3        NaN\n3        NaN\n3         C\n3        NaN\n4        NaN\n4         S\n4        NaN\n5        NaN\n5         A\n5        NaN\n5        NaN\n</code></pre>\n\n<p>If we look into the data, cluster 1 has Value 'A' for one row and remain all are NA values. I want to fill 'A' value for all the rows of cluster 1. Similarly for all the clusters. Based on one of the values of the cluster, I want to fill the remaining rows of the cluster. The output should be like,</p>\n\n<pre><code>cluster Value\n1         A\n1         A\n1         A\n1         A\n1         A\n2         B\n2         B\n2         B\n2         B\n3         C\n3         C\n3         C\n3         C\n4         S\n4         S\n4         S\n5         A\n5         A\n5         A\n5         A\n</code></pre>\n\n<p>I am new to python and not sure how to proceed with this. Can anybody help with this ?</p>\n",
        "answer_body": "<p><strong>Edit</strong></p>\n\n<p>The following seems better:</p>\n\n<pre><code>nan_map = df.dropna().set_index('cluster').to_dict()['Value']\ndf['Value'] = df['cluster'].map(nan_map)\n\nprint(df)\n</code></pre>\n\n<p><strong>Original</strong></p>\n\n<p>I can't think of a better way to do this than iterate over all the rows, but one might exist.  First I built your DataFrame:</p>\n\n<pre><code>import pandas as pd\nimport math\n\n# Build your DataFrame\ndf = pd.DataFrame.from_items([\n    ('cluster', [1,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,5,5,5,5]),\n    ('Value', [float('nan') for _ in range(20)]),\n])\ndf['Value'] = df['Value'].astype(object)\ndf.at[ 0,'Value'] = 'A'\ndf.at[ 7,'Value'] = 'B'\ndf.at[11,'Value'] = 'C'\ndf.at[14,'Value'] = 'S'\ndf.at[17,'Value'] = 'A'\n</code></pre>\n\n<p>Now here's an approach that first creates a <code>nan_map</code> dict, then sets the values in <code>Value</code> as specified in the dict.</p>\n\n<pre><code># Create a dict to map clusters to unique values\nnan_map = df.dropna().set_index('cluster').to_dict()['Value']\n# nan_map: {1: 'A', 2: 'B', 3: 'C', 4: 'S', 5: 'A'}\n\n# Apply\nfor i, row in df.iterrows():\n    df.at[i,'Value'] = nan_map[row['cluster']]\n\nprint(df)\n</code></pre>\n\n<p>Output:</p>\n\n<pre>\n    cluster Value\n0         1     A\n1         1     A\n2         1     A\n3         1     A\n4         1     A\n5         2     B\n6         2     B\n7         2     B\n8         2     B\n9         3     C\n10        3     C\n11        3     C\n12        3     C\n13        4     S\n14        4     S\n15        4     S\n16        5     A\n17        5     A\n18        5     A\n19        5     A\n</pre>\n\n<p>Note: This sets <em>all</em> values based on the cluster and doesn't check for NaN-ness.  You may want to experiment with something like:</p>\n\n<pre><code># Apply\nfor i, row in df.iterrows():\n    if isinstance(df.at[i,'Value'], float) and math.isnan(df.at[i,'Value']):\n        df.at[i,'Value'] = nan_map[row['cluster']]\n</code></pre>\n\n<p>to see which is more efficient (my guess is the former, without the checks).</p>\n",
        "question_body": "<p>The following is the pandas dataframe I have:</p>\n\n<pre><code>cluster Value\n1         A\n1        NaN\n1        NaN\n1        NaN\n1        NaN\n2        NaN\n2        NaN\n2         B\n2        NaN\n3        NaN\n3        NaN\n3         C\n3        NaN\n4        NaN\n4         S\n4        NaN\n5        NaN\n5         A\n5        NaN\n5        NaN\n</code></pre>\n\n<p>If we look into the data, cluster 1 has Value 'A' for one row and remain all are NA values. I want to fill 'A' value for all the rows of cluster 1. Similarly for all the clusters. Based on one of the values of the cluster, I want to fill the remaining rows of the cluster. The output should be like,</p>\n\n<pre><code>cluster Value\n1         A\n1         A\n1         A\n1         A\n1         A\n2         B\n2         B\n2         B\n2         B\n3         C\n3         C\n3         C\n3         C\n4         S\n4         S\n4         S\n5         A\n5         A\n5         A\n5         A\n</code></pre>\n\n<p>I am new to python and not sure how to proceed with this. Can anybody help with this ?</p>\n",
        "formatted_input": {
            "qid": 50957993,
            "link": "https://stackoverflow.com/questions/50957993/fill-all-values-in-a-group-with-the-first-non-null-value-in-that-group",
            "question": {
                "title": "Fill all values in a group with the first non-null value in that group",
                "ques_desc": "The following is the pandas dataframe I have: If we look into the data, cluster 1 has Value 'A' for one row and remain all are NA values. I want to fill 'A' value for all the rows of cluster 1. Similarly for all the clusters. Based on one of the values of the cluster, I want to fill the remaining rows of the cluster. The output should be like, I am new to python and not sure how to proceed with this. Can anybody help with this ? "
            },
            "io": [
                "cluster Value\n1         A\n1        NaN\n1        NaN\n1        NaN\n1        NaN\n2        NaN\n2        NaN\n2         B\n2        NaN\n3        NaN\n3        NaN\n3         C\n3        NaN\n4        NaN\n4         S\n4        NaN\n5        NaN\n5         A\n5        NaN\n5        NaN\n",
                "cluster Value\n1         A\n1         A\n1         A\n1         A\n1         A\n2         B\n2         B\n2         B\n2         B\n3         C\n3         C\n3         C\n3         C\n4         S\n4         S\n4         S\n5         A\n5         A\n5         A\n5         A\n"
            ],
            "answer": {
                "ans_desc": "Edit The following seems better: Original I can't think of a better way to do this than iterate over all the rows, but one might exist. First I built your DataFrame: Now here's an approach that first creates a dict, then sets the values in as specified in the dict. Output: cluster Value 0 1 A 1 1 A 2 1 A 3 1 A 4 1 A 5 2 B 6 2 B 7 2 B 8 2 B 9 3 C 10 3 C 11 3 C 12 3 C 13 4 S 14 4 S 15 4 S 16 5 A 17 5 A 18 5 A 19 5 A Note: This sets all values based on the cluster and doesn't check for NaN-ness. You may want to experiment with something like: to see which is more efficient (my guess is the former, without the checks). ",
                "code": [
                    "nan_map = df.dropna().set_index('cluster').to_dict()['Value']\ndf['Value'] = df['cluster'].map(nan_map)\n\nprint(df)\n",
                    "# Create a dict to map clusters to unique values\nnan_map = df.dropna().set_index('cluster').to_dict()['Value']\n# nan_map: {1: 'A', 2: 'B', 3: 'C', 4: 'S', 5: 'A'}\n\n# Apply\nfor i, row in df.iterrows():\n    df.at[i,'Value'] = nan_map[row['cluster']]\n\nprint(df)\n",
                    "# Apply\nfor i, row in df.iterrows():\n    if isinstance(df.at[i,'Value'], float) and math.isnan(df.at[i,'Value']):\n        df.at[i,'Value'] = nan_map[row['cluster']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "sum",
            "aggregate"
        ],
        "owner": {
            "reputation": 227,
            "user_id": 4997551,
            "user_type": "registered",
            "accept_rate": 40,
            "profile_image": "https://lh6.googleusercontent.com/-2gg7FXJwyBo/AAAAAAAAAAI/AAAAAAAAAAw/FT5HRpd3mi4/photo.jpg?sz=128",
            "display_name": "DBinJP",
            "link": "https://stackoverflow.com/users/4997551/dbinjp"
        },
        "is_answered": true,
        "view_count": 1573,
        "accepted_answer_id": 50983921,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1529657068,
        "creation_date": 1529654554,
        "last_edit_date": 1529655501,
        "question_id": 50983386,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50983386/how-do-i-sum-time-series-data-by-day-in-python-resample-sum-has-no-effect",
        "title": "How do I sum time series data by day in Python? resample.sum() has no effect",
        "body": "<p>I am new to Python. How do I sum data based on date and plot the result?</p>\n\n<p>I have a Series object with data like:</p>\n\n<pre><code>2017-11-03 07:30:00      NaN\n2017-11-03 09:18:00      NaN\n2017-11-03 10:00:00      NaN\n2017-11-03 11:08:00      NaN\n2017-11-03 14:39:00      NaN\n2017-11-03 14:53:00      NaN\n2017-11-03 15:00:00      NaN\n2017-11-03 16:00:00      NaN\n2017-11-03 17:03:00      NaN\n2017-11-03 17:42:00    800.0\n2017-11-04 07:27:00    600.0\n2017-11-04 10:10:00      NaN\n2017-11-04 11:48:00      NaN\n2017-11-04 12:58:00    500.0\n2017-11-04 13:40:00      NaN\n2017-11-04 15:15:00      NaN\n2017-11-04 16:21:00      NaN\n2017-11-04 17:37:00    500.0\n2017-11-04 21:37:00      NaN\n2017-11-05 03:00:00      NaN\n2017-11-05 06:30:00      NaN\n2017-11-05 07:19:00      NaN\n2017-11-05 08:31:00    200.0\n2017-11-05 09:31:00    500.0\n2017-11-05 12:03:00      NaN\n2017-11-05 12:25:00    200.0\n2017-11-05 13:11:00    500.0\n2017-11-05 16:31:00      NaN\n2017-11-05 19:00:00    500.0\n2017-11-06 08:08:00      NaN\n</code></pre>\n\n<p>I have the following code:</p>\n\n<pre><code># load packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# import painkiller data\ndf = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')\n\n# plot bar graph of date and painkiller amount\ntimes = pd.to_datetime(df.loc[:,'Time'])\n\nts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,\n               name = 'Painkiller over Time')\nts.plot()\n</code></pre>\n\n<p>This gives me the following line(?) graph:</p>\n\n<p><a href=\"https://i.stack.imgur.com/hfhbo.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/hfhbo.png\" alt=\"line graph of raw data\"></a></p>\n\n<p>It's a start; now I want to sum the doses by date. However, this code fails to effect any change: The resulting plot is the same. What is wrong?</p>\n\n<pre><code>ts.resample('D',closed='left', label='right').sum()\nts.plot()\n</code></pre>\n\n<p>I have also tried <code>ts.resample('D').sum()</code>, <code>ts.resample('1d').sum()</code>, <code>ts.resample('1D').sum()</code>, but there is no change in the plot. </p>\n\n<p>Is <code>.resample</code> even the correct function? I understand resampling to be <em>sampling</em> from the data, e.g. randomly taking one point per day, whereas I want to sum each day's values.</p>\n\n<p>Namely, I'm hoping for some result (based on the above data) like:</p>\n\n<pre><code>2017-11-03 800\n2017-11-04 1600\n2017-11-05 1900\n2017-11-06 NaN\n</code></pre>\n",
        "answer_body": "<p><a href=\"https://stats.stackexchange.com/a/121690/197523\">This answer</a> helped me see that I needed to assign it to a new object (if that's the right terminology):</p>\n\n<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')\n# plot bar graph of date and painkiller amount\ntimes = pd.to_datetime(df.loc[:,'Time'])\n\n# raw plot of data\nts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,\n               name = 'Painkiller over Time')\nfig1 = ts.plot()\n\n# combine data by day\ntest2 = ts.resample('D').sum()\nfig2 = test2.plot()\n</code></pre>\n\n<p>That produces the following plots:</p>\n\n<p><a href=\"https://i.stack.imgur.com/y54p6.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/y54p6.png\" alt=\"first plot\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/rxJXg.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/rxJXg.png\" alt=\"second plot\"></a></p>\n\n<p>Is this method not better than the 'groupby' function?</p>\n\n<p>Now how do I make a scatter or bar plot instead of this line plot...?</p>\n",
        "question_body": "<p>I am new to Python. How do I sum data based on date and plot the result?</p>\n\n<p>I have a Series object with data like:</p>\n\n<pre><code>2017-11-03 07:30:00      NaN\n2017-11-03 09:18:00      NaN\n2017-11-03 10:00:00      NaN\n2017-11-03 11:08:00      NaN\n2017-11-03 14:39:00      NaN\n2017-11-03 14:53:00      NaN\n2017-11-03 15:00:00      NaN\n2017-11-03 16:00:00      NaN\n2017-11-03 17:03:00      NaN\n2017-11-03 17:42:00    800.0\n2017-11-04 07:27:00    600.0\n2017-11-04 10:10:00      NaN\n2017-11-04 11:48:00      NaN\n2017-11-04 12:58:00    500.0\n2017-11-04 13:40:00      NaN\n2017-11-04 15:15:00      NaN\n2017-11-04 16:21:00      NaN\n2017-11-04 17:37:00    500.0\n2017-11-04 21:37:00      NaN\n2017-11-05 03:00:00      NaN\n2017-11-05 06:30:00      NaN\n2017-11-05 07:19:00      NaN\n2017-11-05 08:31:00    200.0\n2017-11-05 09:31:00    500.0\n2017-11-05 12:03:00      NaN\n2017-11-05 12:25:00    200.0\n2017-11-05 13:11:00    500.0\n2017-11-05 16:31:00      NaN\n2017-11-05 19:00:00    500.0\n2017-11-06 08:08:00      NaN\n</code></pre>\n\n<p>I have the following code:</p>\n\n<pre><code># load packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# import painkiller data\ndf = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')\n\n# plot bar graph of date and painkiller amount\ntimes = pd.to_datetime(df.loc[:,'Time'])\n\nts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,\n               name = 'Painkiller over Time')\nts.plot()\n</code></pre>\n\n<p>This gives me the following line(?) graph:</p>\n\n<p><a href=\"https://i.stack.imgur.com/hfhbo.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/hfhbo.png\" alt=\"line graph of raw data\"></a></p>\n\n<p>It's a start; now I want to sum the doses by date. However, this code fails to effect any change: The resulting plot is the same. What is wrong?</p>\n\n<pre><code>ts.resample('D',closed='left', label='right').sum()\nts.plot()\n</code></pre>\n\n<p>I have also tried <code>ts.resample('D').sum()</code>, <code>ts.resample('1d').sum()</code>, <code>ts.resample('1D').sum()</code>, but there is no change in the plot. </p>\n\n<p>Is <code>.resample</code> even the correct function? I understand resampling to be <em>sampling</em> from the data, e.g. randomly taking one point per day, whereas I want to sum each day's values.</p>\n\n<p>Namely, I'm hoping for some result (based on the above data) like:</p>\n\n<pre><code>2017-11-03 800\n2017-11-04 1600\n2017-11-05 1900\n2017-11-06 NaN\n</code></pre>\n",
        "formatted_input": {
            "qid": 50983386,
            "link": "https://stackoverflow.com/questions/50983386/how-do-i-sum-time-series-data-by-day-in-python-resample-sum-has-no-effect",
            "question": {
                "title": "How do I sum time series data by day in Python? resample.sum() has no effect",
                "ques_desc": "I am new to Python. How do I sum data based on date and plot the result? I have a Series object with data like: I have the following code: This gives me the following line(?) graph: It's a start; now I want to sum the doses by date. However, this code fails to effect any change: The resulting plot is the same. What is wrong? I have also tried , , , but there is no change in the plot. Is even the correct function? I understand resampling to be sampling from the data, e.g. randomly taking one point per day, whereas I want to sum each day's values. Namely, I'm hoping for some result (based on the above data) like: "
            },
            "io": [
                "2017-11-03 07:30:00      NaN\n2017-11-03 09:18:00      NaN\n2017-11-03 10:00:00      NaN\n2017-11-03 11:08:00      NaN\n2017-11-03 14:39:00      NaN\n2017-11-03 14:53:00      NaN\n2017-11-03 15:00:00      NaN\n2017-11-03 16:00:00      NaN\n2017-11-03 17:03:00      NaN\n2017-11-03 17:42:00    800.0\n2017-11-04 07:27:00    600.0\n2017-11-04 10:10:00      NaN\n2017-11-04 11:48:00      NaN\n2017-11-04 12:58:00    500.0\n2017-11-04 13:40:00      NaN\n2017-11-04 15:15:00      NaN\n2017-11-04 16:21:00      NaN\n2017-11-04 17:37:00    500.0\n2017-11-04 21:37:00      NaN\n2017-11-05 03:00:00      NaN\n2017-11-05 06:30:00      NaN\n2017-11-05 07:19:00      NaN\n2017-11-05 08:31:00    200.0\n2017-11-05 09:31:00    500.0\n2017-11-05 12:03:00      NaN\n2017-11-05 12:25:00    200.0\n2017-11-05 13:11:00    500.0\n2017-11-05 16:31:00      NaN\n2017-11-05 19:00:00    500.0\n2017-11-06 08:08:00      NaN\n",
                "2017-11-03 800\n2017-11-04 1600\n2017-11-05 1900\n2017-11-06 NaN\n"
            ],
            "answer": {
                "ans_desc": "This answer helped me see that I needed to assign it to a new object (if that's the right terminology): That produces the following plots: Is this method not better than the 'groupby' function? Now how do I make a scatter or bar plot instead of this line plot...? ",
                "code": [
                    "import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')\n# plot bar graph of date and painkiller amount\ntimes = pd.to_datetime(df.loc[:,'Time'])\n\n# raw plot of data\nts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,\n               name = 'Painkiller over Time')\nfig1 = ts.plot()\n\n# combine data by day\ntest2 = ts.resample('D').sum()\nfig2 = test2.plot()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "indexing",
            "multi-index"
        ],
        "owner": {
            "reputation": 699,
            "user_id": 3877781,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/fd37c8b5d076e8eaa03e191475924360?s=128&d=identicon&r=PG&f=1",
            "display_name": "Christian",
            "link": "https://stackoverflow.com/users/3877781/christian"
        },
        "is_answered": true,
        "view_count": 97,
        "closed_date": 1529184345,
        "accepted_answer_id": 50890904,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1529357897,
        "creation_date": 1529175544,
        "last_edit_date": 1529179815,
        "question_id": 50890844,
        "link": "https://stackoverflow.com/questions/50890844/select-individual-rows-from-multiindex-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "select individual rows from multiindex pandas dataframe",
        "body": "<p>I am trying to select individual rows from a multiindex dataframe using a list of multiindices.</p>\n\n<p>For example. I have got the following dataframe:</p>\n\n<pre><code>           Col1\nA B C\n1 1 1 -0.148593\n    2  2.043589\n  2 3 -1.696572\n    4 -0.249049\n2 1 5  2.012294\n    6 -1.756410\n  2 7  0.476035\n    8 -0.531612\n</code></pre>\n\n<p>I would like to select all 'C' with (A,B) = [(1,1), (2,2)]</p>\n\n<pre><code>           Col1\nA B C\n1 1 1 -0.148593\n    2  2.043589\n2 2 7  0.476035\n    8 -0.531612\n</code></pre>\n\n<p>My flawed code for this is as follows:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\narrays = [np.array([1, 1, 1, 1, 2, 2, 2, 2]), np.array([1, 1, 2, 2, 1, 1, 2, 2]), np.array([1, 2, 3, 4, 5, 6, 7, 8])]\ndf = pd.DataFrame(np.random.randn(8), index=arrays, columns=['Col1'])\ndf.rename_axis(['A','B','C'], inplace=True)\nprint(df)\n\nidx_lst = [(1,1), (2,2)]\ntest = df.loc(axis=0)[idx_lst]\nprint(test)\n</code></pre>\n",
        "answer_body": "<p>One option is to use <a href=\"http://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.query.html\" rel=\"nofollow noreferrer\"><code>pd.DataFrame.query</code></a>:</p>\n\n<pre><code>res = df.query('((A == 1) &amp; (B == 1)) | ((A == 2) &amp; (B == 2))')\n\nprint(res)\n\n           Col1\nA B C          \n1 1 1  0.981483\n    2  0.851543\n2 2 7 -0.522760\n    8 -0.332099\n</code></pre>\n\n<p>For a more generic solution, you can use f-strings (Python 3.6+), which should perform better than <code>str.format</code> or manual concatenation.</p>\n\n<pre><code>filters = [(1,1), (2,2)]\nfilterstr = '|'.join(f'(A=={i})&amp;(B=={j})' for i, j in filters)\nres = df.query(filterstr)\n\nprint(filterstr)\n\n(A==1)&amp;(B==1)|(A==2)&amp;(B==2)\n</code></pre>\n",
        "question_body": "<p>I am trying to select individual rows from a multiindex dataframe using a list of multiindices.</p>\n\n<p>For example. I have got the following dataframe:</p>\n\n<pre><code>           Col1\nA B C\n1 1 1 -0.148593\n    2  2.043589\n  2 3 -1.696572\n    4 -0.249049\n2 1 5  2.012294\n    6 -1.756410\n  2 7  0.476035\n    8 -0.531612\n</code></pre>\n\n<p>I would like to select all 'C' with (A,B) = [(1,1), (2,2)]</p>\n\n<pre><code>           Col1\nA B C\n1 1 1 -0.148593\n    2  2.043589\n2 2 7  0.476035\n    8 -0.531612\n</code></pre>\n\n<p>My flawed code for this is as follows:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\narrays = [np.array([1, 1, 1, 1, 2, 2, 2, 2]), np.array([1, 1, 2, 2, 1, 1, 2, 2]), np.array([1, 2, 3, 4, 5, 6, 7, 8])]\ndf = pd.DataFrame(np.random.randn(8), index=arrays, columns=['Col1'])\ndf.rename_axis(['A','B','C'], inplace=True)\nprint(df)\n\nidx_lst = [(1,1), (2,2)]\ntest = df.loc(axis=0)[idx_lst]\nprint(test)\n</code></pre>\n",
        "formatted_input": {
            "qid": 50890844,
            "link": "https://stackoverflow.com/questions/50890844/select-individual-rows-from-multiindex-pandas-dataframe",
            "question": {
                "title": "select individual rows from multiindex pandas dataframe",
                "ques_desc": "I am trying to select individual rows from a multiindex dataframe using a list of multiindices. For example. I have got the following dataframe: I would like to select all 'C' with (A,B) = [(1,1), (2,2)] My flawed code for this is as follows: "
            },
            "io": [
                "           Col1\nA B C\n1 1 1 -0.148593\n    2  2.043589\n  2 3 -1.696572\n    4 -0.249049\n2 1 5  2.012294\n    6 -1.756410\n  2 7  0.476035\n    8 -0.531612\n",
                "           Col1\nA B C\n1 1 1 -0.148593\n    2  2.043589\n2 2 7  0.476035\n    8 -0.531612\n"
            ],
            "answer": {
                "ans_desc": "One option is to use : For a more generic solution, you can use f-strings (Python 3.6+), which should perform better than or manual concatenation. ",
                "code": [
                    "filters = [(1,1), (2,2)]\nfilterstr = '|'.join(f'(A=={i})&(B=={j})' for i, j in filters)\nres = df.query(filterstr)\n\nprint(filterstr)\n\n(A==1)&(B==1)|(A==2)&(B==2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 379,
            "user_id": 9283258,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/2c95b5aca3c1a96ed210a03982d2633a?s=128&d=identicon&r=PG&f=1",
            "display_name": "MaMo",
            "link": "https://stackoverflow.com/users/9283258/mamo"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 50861880,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1528993530,
        "creation_date": 1528987912,
        "question_id": 50860328,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50860328/python-add-a-column-to-a-dataframe-containing-a-value-from-another-row-based-o",
        "title": "Python - Add a column to a dataframe containing a value from another row based on condition",
        "body": "<p>My dataframe looks like this:</p>\n\n<pre><code>+-----+-------+----------+-------+\n| No  | Group | refGroup | Value |\n+-----+-------+----------+-------+\n| 123 | A1    | A1       |   5.0 |\n| 123 | B1    | A1       |   7.3 |\n| 123 | B2    | A1       |   8.9 |\n| 123 | B3    | B1       |   7.9 |\n| 465 | A1    | A1       |   1.4 |\n| 465 | B1    | A1       |   4.5 |\n| 465 | B2    | B1       |   7.3 |\n+-----+-------+----------+-------+\n</code></pre>\n\n<p>Now I need to add another column which conatains the difference between the value of column <code>Value</code> from the current row and the value of column <code>Value</code> from the row with the same number (<code>No</code>) and the group (<code>Group</code>) that is written in <code>refGroup</code>. </p>\n\n<p>Exeption: If <code>refGroup</code> equals <code>Group</code>, <code>Value</code> and <code>refValue</code> are the same.</p>\n\n<p>So the result should be:</p>\n\n<pre><code>+-----+-------+----------+-------+----------+\n| No  | Group | refGroup | Value | refValue |\n+-----+-------+----------+-------+----------+\n| 123 | A1    | A1       |   5.0 |      5.0 |\n| 123 | B1    | A1       |   7.3 |      2.3 |\n| 123 | B2    | A1       |   8.9 |      3.9 |\n| 123 | B3    | B1       |   7.9 |      0.6 |\n| 465 | A1    | A1       |   1.4 |      1.4 |\n| 465 | B1    | A1       |   4.5 |      3.1 |\n| 465 | B2    | B1       |   7.3 |      2.8 |\n+-----+-------+----------+-------+----------+\n</code></pre>\n\n<p>Explanation for the first two rows:</p>\n\n<p>First row: <code>refGroup</code> equals <code>Group</code> -> <code>refValue</code> = <code>Value</code></p>\n\n<p>Second row: search for the row with the same <code>No</code> (123) and <code>refGroup</code> as <code>Group</code> (A1) and calculate <code>Value</code> of the current row minus <code>Value</code> of the referenced row (7.3 - 5.0 = 2.3). </p>\n\n<p>I thought I might need to use groupby() and apply(), but how?</p>\n\n<p>Hope my example is detailed enough, if you need any further information, please ask :)</p>\n",
        "answer_body": "<p>One way is to use a database SQL like technique; use 'self-join' with <code>merge</code>.  You merge/join a dataframe to itself using <code>left_on</code> and  <code>right_on</code> to line up 'Group' with 'refGroup' then subtract the value from each dataframe record:</p>\n\n<pre><code>df_out = df.merge(df, \n                  left_on=['No','refGroup'], \n                  right_on=['No','Group'], \n                  suffixes=('','_ref'))\n\ndf['refValue'] = np.where(df_out['Group'] == df_out['refGroup'],\n                          df_out['value'],\n                          df_out['value'] - df_out['value_ref'])\n\ndf\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>    No Group refGroup  value  refValue\n0  123    A1       A1    5.0       5.0\n1  123    B1       A1    7.3       2.3\n2  123    B2       A1    8.9       3.9\n3  123    B3       B1    7.9       0.6\n4  465    A1       A1    1.4       1.4\n5  465    B1       A1    4.5       3.1\n6  465    B2       B1    7.3       2.8\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like this:</p>\n\n<pre><code>+-----+-------+----------+-------+\n| No  | Group | refGroup | Value |\n+-----+-------+----------+-------+\n| 123 | A1    | A1       |   5.0 |\n| 123 | B1    | A1       |   7.3 |\n| 123 | B2    | A1       |   8.9 |\n| 123 | B3    | B1       |   7.9 |\n| 465 | A1    | A1       |   1.4 |\n| 465 | B1    | A1       |   4.5 |\n| 465 | B2    | B1       |   7.3 |\n+-----+-------+----------+-------+\n</code></pre>\n\n<p>Now I need to add another column which conatains the difference between the value of column <code>Value</code> from the current row and the value of column <code>Value</code> from the row with the same number (<code>No</code>) and the group (<code>Group</code>) that is written in <code>refGroup</code>. </p>\n\n<p>Exeption: If <code>refGroup</code> equals <code>Group</code>, <code>Value</code> and <code>refValue</code> are the same.</p>\n\n<p>So the result should be:</p>\n\n<pre><code>+-----+-------+----------+-------+----------+\n| No  | Group | refGroup | Value | refValue |\n+-----+-------+----------+-------+----------+\n| 123 | A1    | A1       |   5.0 |      5.0 |\n| 123 | B1    | A1       |   7.3 |      2.3 |\n| 123 | B2    | A1       |   8.9 |      3.9 |\n| 123 | B3    | B1       |   7.9 |      0.6 |\n| 465 | A1    | A1       |   1.4 |      1.4 |\n| 465 | B1    | A1       |   4.5 |      3.1 |\n| 465 | B2    | B1       |   7.3 |      2.8 |\n+-----+-------+----------+-------+----------+\n</code></pre>\n\n<p>Explanation for the first two rows:</p>\n\n<p>First row: <code>refGroup</code> equals <code>Group</code> -> <code>refValue</code> = <code>Value</code></p>\n\n<p>Second row: search for the row with the same <code>No</code> (123) and <code>refGroup</code> as <code>Group</code> (A1) and calculate <code>Value</code> of the current row minus <code>Value</code> of the referenced row (7.3 - 5.0 = 2.3). </p>\n\n<p>I thought I might need to use groupby() and apply(), but how?</p>\n\n<p>Hope my example is detailed enough, if you need any further information, please ask :)</p>\n",
        "formatted_input": {
            "qid": 50860328,
            "link": "https://stackoverflow.com/questions/50860328/python-add-a-column-to-a-dataframe-containing-a-value-from-another-row-based-o",
            "question": {
                "title": "Python - Add a column to a dataframe containing a value from another row based on condition",
                "ques_desc": "My dataframe looks like this: Now I need to add another column which conatains the difference between the value of column from the current row and the value of column from the row with the same number () and the group () that is written in . Exeption: If equals , and are the same. So the result should be: Explanation for the first two rows: First row: equals -> = Second row: search for the row with the same (123) and as (A1) and calculate of the current row minus of the referenced row (7.3 - 5.0 = 2.3). I thought I might need to use groupby() and apply(), but how? Hope my example is detailed enough, if you need any further information, please ask :) "
            },
            "io": [
                "+-----+-------+----------+-------+\n| No  | Group | refGroup | Value |\n+-----+-------+----------+-------+\n| 123 | A1    | A1       |   5.0 |\n| 123 | B1    | A1       |   7.3 |\n| 123 | B2    | A1       |   8.9 |\n| 123 | B3    | B1       |   7.9 |\n| 465 | A1    | A1       |   1.4 |\n| 465 | B1    | A1       |   4.5 |\n| 465 | B2    | B1       |   7.3 |\n+-----+-------+----------+-------+\n",
                "+-----+-------+----------+-------+----------+\n| No  | Group | refGroup | Value | refValue |\n+-----+-------+----------+-------+----------+\n| 123 | A1    | A1       |   5.0 |      5.0 |\n| 123 | B1    | A1       |   7.3 |      2.3 |\n| 123 | B2    | A1       |   8.9 |      3.9 |\n| 123 | B3    | B1       |   7.9 |      0.6 |\n| 465 | A1    | A1       |   1.4 |      1.4 |\n| 465 | B1    | A1       |   4.5 |      3.1 |\n| 465 | B2    | B1       |   7.3 |      2.8 |\n+-----+-------+----------+-------+----------+\n"
            ],
            "answer": {
                "ans_desc": "One way is to use a database SQL like technique; use 'self-join' with . You merge/join a dataframe to itself using and to line up 'Group' with 'refGroup' then subtract the value from each dataframe record: Output: ",
                "code": [
                    "df_out = df.merge(df, \n                  left_on=['No','refGroup'], \n                  right_on=['No','Group'], \n                  suffixes=('','_ref'))\n\ndf['refValue'] = np.where(df_out['Group'] == df_out['refGroup'],\n                          df_out['value'],\n                          df_out['value'] - df_out['value_ref'])\n\ndf\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3747,
            "user_id": 4447920,
            "user_type": "registered",
            "accept_rate": 55,
            "profile_image": "https://i.stack.imgur.com/BI8Ts.jpg?s=128&g=1",
            "display_name": "Piyush S. Wanare",
            "link": "https://stackoverflow.com/users/4447920/piyush-s-wanare"
        },
        "is_answered": true,
        "view_count": 437,
        "accepted_answer_id": 50717966,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1528291722,
        "creation_date": 1528280208,
        "last_edit_date": 1528283364,
        "question_id": 50717904,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50717904/pandas-delete-all-rows-which-contains-required-value-in-all-column",
        "title": "Pandas delete all rows which contains &quot;required value&quot; in all column",
        "body": "<p>I have the following dataframe</p>\n\n<pre><code> A     B    C    D\nBUY   150   Q   2018\nSELL  63    Q   2018\nN      N    N    N\n\nV      v    v    v\nSELL  53    Q   2018\n</code></pre>\n\n<p>I want to delete all rows which contains all column a V or N</p>\n\n<p>Output data frame will be :-</p>\n\n<pre><code>    A     B     C    D\n   BUY   150    Q   2018\n   SELL  63     Q   2018\n\n   SELL  53     Q   2018\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\" rel=\"noreferrer\"><code>boolean indexing</code></a>:</p>\n\n<pre><code>df = df[~df.isin(['V', 'v', 'N', 'n']).all(axis=1)]\nprint (df)\n      A    B  C     D\n0   BUY  150  Q  2018\n1  SELL   63  Q  2018\n4  SELL   53  Q  2018\n</code></pre>\n\n<p><strong>Detail</strong>:</p>\n\n<p>First compare by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isin.html\" rel=\"noreferrer\"><code>isin</code></a>:</p>\n\n<pre><code>print (df.isin(['V', 'v', 'N', 'n']))\n       A      B      C      D\n0  False  False  False  False\n1  False  False  False  False\n2   True   True   True   True\n3   True   True   True   True\n4  False  False  False  False\n</code></pre>\n\n<p>Get rows if <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.all.html\" rel=\"noreferrer\"><code>all</code></a> <code>True</code>s per rows:</p>\n\n<pre><code>print (df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0    False\n1    False\n2     True\n3     True\n4    False\ndtype: bool\n</code></pre>\n\n<p>Invert condition by <code>~</code>:</p>\n\n<pre><code>print (~df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe</p>\n\n<pre><code> A     B    C    D\nBUY   150   Q   2018\nSELL  63    Q   2018\nN      N    N    N\n\nV      v    v    v\nSELL  53    Q   2018\n</code></pre>\n\n<p>I want to delete all rows which contains all column a V or N</p>\n\n<p>Output data frame will be :-</p>\n\n<pre><code>    A     B     C    D\n   BUY   150    Q   2018\n   SELL  63     Q   2018\n\n   SELL  53     Q   2018\n</code></pre>\n",
        "formatted_input": {
            "qid": 50717904,
            "link": "https://stackoverflow.com/questions/50717904/pandas-delete-all-rows-which-contains-required-value-in-all-column",
            "question": {
                "title": "Pandas delete all rows which contains &quot;required value&quot; in all column",
                "ques_desc": "I have the following dataframe I want to delete all rows which contains all column a V or N Output data frame will be :- "
            },
            "io": [
                " A     B    C    D\nBUY   150   Q   2018\nSELL  63    Q   2018\nN      N    N    N\n\nV      v    v    v\nSELL  53    Q   2018\n",
                "    A     B     C    D\n   BUY   150    Q   2018\n   SELL  63     Q   2018\n\n   SELL  53     Q   2018\n"
            ],
            "answer": {
                "ans_desc": "Use : Detail: First compare by : Get rows if s per rows: Invert condition by : ",
                "code": [
                    "print (df.isin(['V', 'v', 'N', 'n']))\n       A      B      C      D\n0  False  False  False  False\n1  False  False  False  False\n2   True   True   True   True\n3   True   True   True   True\n4  False  False  False  False\n",
                    "print (df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0    False\n1    False\n2     True\n3     True\n4    False\ndtype: bool\n",
                    "print (~df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dictionary",
            "dataframe"
        ],
        "owner": {
            "reputation": 611,
            "user_id": 5775504,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/77e044fffe183e599866a8594cf7fd01?s=128&d=identicon&r=PG&f=1",
            "display_name": "F.Lira",
            "link": "https://stackoverflow.com/users/5775504/f-lira"
        },
        "is_answered": true,
        "view_count": 76,
        "accepted_answer_id": 50644457,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1528194549,
        "creation_date": 1527858911,
        "last_edit_date": 1528120671,
        "question_id": 50644315,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50644315/merge-dictionaries-to-dataframe-get-dummies",
        "title": "Merge dictionaries to dataframe get_dummies",
        "body": "<p>In a dictionary with information about a string in a text file, where keys are the strings and values are the names of the files. </p>\n\n<pre><code>Dict1 = {'str1A':'file1', 'str1B':'file1', 'str1C':'file1', 'str1D':'file1', 'str2A':'file2', 'str2B':'file2', 'str2C':'file2', 'str2D':'file2', 'str2D':'file2', 'str3A':'file3', \n</code></pre>\n\n<p>'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'}</p>\n\n<p>Another dictionary contains information about the best match for the strings from the text.</p>\n\n<pre><code>Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}\n</code></pre>\n\n<p>The third dictionary contains information about the percentage of occurrence of the string in the text.</p>\n\n<pre><code>Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}\n</code></pre>\n\n<p>Now my aims are to use the information of these different dictionaries to generate a dataframe like this:</p>\n\n<pre><code>       jump     fly     swim    run\nfile1   90      60      30      NA\nfile2   70      30      60      40\nfile3   10      90      70      90\n</code></pre>\n\n<p>To this, I started the script but I am stuck:</p>\n\n<pre><code>col_file = ['str', 'file']\ndf_origin = pd.DataFrame(Dict1.items(), columns=col_file)\n#print df_origin\n\ncol_bmatch = ['str', 'text']\ndf_bmatch =  pd.DataFrame(Dict2.items(), columns=col_bmatch)\n#print df_bmatch\n\ncol_percent = ['str', 'percent']\ndf_percent = pd.DataFrame(Dict3.items(), columns=col_percent)\n#print df_percent\n</code></pre>\n\n<p>This block was removed from script: </p>\n\n<blockquote>\n<pre><code>df_origin['text'] = df_origin['str'].map(df_bmatch.set_index('str')['text'])\n\ndf_origin['percent'] = df_origin['str'].map(df_percent.set_index('str')['percent'])\n</code></pre>\n</blockquote>\n\n<p>And substituted to: </p>\n\n<pre><code>data = {}\nfor k, col in Dict1.items():\n    if k in Dict1 and k not in Dict3:\n        data.setdefault(k, {})[col] = \"NA\"\n    elif k in Dict1 and k in Dict3:\n        data.setdefault(k, {})[col] = Dict3[k]\n\n    df = pd.DataFrame(data)\n\nprint(df)\n</code></pre>\n\n<p>But the final result was not very exact:</p>\n\n<pre><code>      str1A str1B str1C str1D str2A str2B str2C str2D str3A str3B  \\\nfile1     90     60     30     NO    NaN    NaN    NaN    NaN    NaN    NaN   \nfile2    NaN    NaN    NaN    NaN     70     30     60     40    NaN    NaN   \nfile3    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN     10     90   \nfile4    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n\n      str3C str3D str4A str4B stre4C str4D str4E  \nfile1    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \nfile2    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \nfile3     70     90    NaN    NaN    NaN    NaN    NaN  \nfile4    NaN    NaN     NO     NO     NO     NO     NO  \n</code></pre>\n\n<p>But the expected table is:</p>\n\n<pre><code>         jump   fly    swim   run   sit\nfile1    90     60     30     NA    NA\nfile2    70     30     60     40    NA\nfile3    10     90     70     90    NA\nfile4    NA     NA     NA     NA    NA\n</code></pre>\n\n<p>Where the string in file4 where not detected.</p>\n\n<p>Blosk removed</p>\n\n<blockquote>\n<pre><code>print df_origin\n\n#          str   file  text percent\n#    0   str2B  file2   fly      30\n#    1   str2C  file2  swim      60\n#    2   str3C  file3  swim      70\n#    3   str3B  file3   fly      90\n#    4   str3D  file3   run      90\n#    5   str2D  file2   run      40\n#    6   str3A  file3  jump      10\n#    7   str1D  file1   NaN     NaN\n#    8   str1C  file1  swim      30\n#    9   str1B  file1   fly      60\n#    10  str1A  file1  jump      90\n#    11  str2A  file2  jump      70\n</code></pre>\n</blockquote>\n\n<p>Here relies the problem</p>\n\n<pre><code>print pd.get_dummies(df_origin.set_index('file')['text']).max(level=0).max(level=0, axis=1)\n</code></pre>\n\n<p>But the only result that I get is this:</p>\n\n<pre><code>       fly  jump  run  swim\nfile                       \nfile2    1     1    1     1\nfile3    1     1    1     1\nfile1    1     1    0     1\n</code></pre>\n\n<p>As I can understand, pd.getdummies groups the field 'file' from my df_origin and uses 'text' to check their presence. </p>\n\n<p>How can I redirect the command to plot the columns 'percent' in my df_origin dataframe?</p>\n",
        "answer_body": "<p>Try this:</p>\n\n<pre><code>import pandas as pd\n\nDict1 = {'str1A':'file1', 'str1B':'file1', 'str1C':'file1', 'str1D':'file1', 'str2A':'file2', 'str2B':'file2', 'str2C':'file2', 'str2D':'file2', 'str2D':'file2', 'str3A':'file3', 'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'}\nDict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}\nDict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}\n\ndata = {}\nfor k, col in Dict2.items():\n    if k not in Dict1 or k not in Dict3:\n        continue\n    data.setdefault(col, {})[Dict1[k]] = Dict3[k]\ndf = pd.DataFrame(data, index=sorted(set(Dict1.values())), columns=sorted(set(Dict2.values())))\n\nprint(df)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>       fly jump  run swim\nfile1   60   90  NaN   30\nfile2   30   70   40   60\nfile3   90   10   90   70\nfile4  NaN  NaN  NaN  NaN\n</code></pre>\n",
        "question_body": "<p>In a dictionary with information about a string in a text file, where keys are the strings and values are the names of the files. </p>\n\n<pre><code>Dict1 = {'str1A':'file1', 'str1B':'file1', 'str1C':'file1', 'str1D':'file1', 'str2A':'file2', 'str2B':'file2', 'str2C':'file2', 'str2D':'file2', 'str2D':'file2', 'str3A':'file3', \n</code></pre>\n\n<p>'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'}</p>\n\n<p>Another dictionary contains information about the best match for the strings from the text.</p>\n\n<pre><code>Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}\n</code></pre>\n\n<p>The third dictionary contains information about the percentage of occurrence of the string in the text.</p>\n\n<pre><code>Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}\n</code></pre>\n\n<p>Now my aims are to use the information of these different dictionaries to generate a dataframe like this:</p>\n\n<pre><code>       jump     fly     swim    run\nfile1   90      60      30      NA\nfile2   70      30      60      40\nfile3   10      90      70      90\n</code></pre>\n\n<p>To this, I started the script but I am stuck:</p>\n\n<pre><code>col_file = ['str', 'file']\ndf_origin = pd.DataFrame(Dict1.items(), columns=col_file)\n#print df_origin\n\ncol_bmatch = ['str', 'text']\ndf_bmatch =  pd.DataFrame(Dict2.items(), columns=col_bmatch)\n#print df_bmatch\n\ncol_percent = ['str', 'percent']\ndf_percent = pd.DataFrame(Dict3.items(), columns=col_percent)\n#print df_percent\n</code></pre>\n\n<p>This block was removed from script: </p>\n\n<blockquote>\n<pre><code>df_origin['text'] = df_origin['str'].map(df_bmatch.set_index('str')['text'])\n\ndf_origin['percent'] = df_origin['str'].map(df_percent.set_index('str')['percent'])\n</code></pre>\n</blockquote>\n\n<p>And substituted to: </p>\n\n<pre><code>data = {}\nfor k, col in Dict1.items():\n    if k in Dict1 and k not in Dict3:\n        data.setdefault(k, {})[col] = \"NA\"\n    elif k in Dict1 and k in Dict3:\n        data.setdefault(k, {})[col] = Dict3[k]\n\n    df = pd.DataFrame(data)\n\nprint(df)\n</code></pre>\n\n<p>But the final result was not very exact:</p>\n\n<pre><code>      str1A str1B str1C str1D str2A str2B str2C str2D str3A str3B  \\\nfile1     90     60     30     NO    NaN    NaN    NaN    NaN    NaN    NaN   \nfile2    NaN    NaN    NaN    NaN     70     30     60     40    NaN    NaN   \nfile3    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN     10     90   \nfile4    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n\n      str3C str3D str4A str4B stre4C str4D str4E  \nfile1    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \nfile2    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \nfile3     70     90    NaN    NaN    NaN    NaN    NaN  \nfile4    NaN    NaN     NO     NO     NO     NO     NO  \n</code></pre>\n\n<p>But the expected table is:</p>\n\n<pre><code>         jump   fly    swim   run   sit\nfile1    90     60     30     NA    NA\nfile2    70     30     60     40    NA\nfile3    10     90     70     90    NA\nfile4    NA     NA     NA     NA    NA\n</code></pre>\n\n<p>Where the string in file4 where not detected.</p>\n\n<p>Blosk removed</p>\n\n<blockquote>\n<pre><code>print df_origin\n\n#          str   file  text percent\n#    0   str2B  file2   fly      30\n#    1   str2C  file2  swim      60\n#    2   str3C  file3  swim      70\n#    3   str3B  file3   fly      90\n#    4   str3D  file3   run      90\n#    5   str2D  file2   run      40\n#    6   str3A  file3  jump      10\n#    7   str1D  file1   NaN     NaN\n#    8   str1C  file1  swim      30\n#    9   str1B  file1   fly      60\n#    10  str1A  file1  jump      90\n#    11  str2A  file2  jump      70\n</code></pre>\n</blockquote>\n\n<p>Here relies the problem</p>\n\n<pre><code>print pd.get_dummies(df_origin.set_index('file')['text']).max(level=0).max(level=0, axis=1)\n</code></pre>\n\n<p>But the only result that I get is this:</p>\n\n<pre><code>       fly  jump  run  swim\nfile                       \nfile2    1     1    1     1\nfile3    1     1    1     1\nfile1    1     1    0     1\n</code></pre>\n\n<p>As I can understand, pd.getdummies groups the field 'file' from my df_origin and uses 'text' to check their presence. </p>\n\n<p>How can I redirect the command to plot the columns 'percent' in my df_origin dataframe?</p>\n",
        "formatted_input": {
            "qid": 50644315,
            "link": "https://stackoverflow.com/questions/50644315/merge-dictionaries-to-dataframe-get-dummies",
            "question": {
                "title": "Merge dictionaries to dataframe get_dummies",
                "ques_desc": "In a dictionary with information about a string in a text file, where keys are the strings and values are the names of the files. 'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'} Another dictionary contains information about the best match for the strings from the text. The third dictionary contains information about the percentage of occurrence of the string in the text. Now my aims are to use the information of these different dictionaries to generate a dataframe like this: To this, I started the script but I am stuck: This block was removed from script: And substituted to: But the final result was not very exact: But the expected table is: Where the string in file4 where not detected. Blosk removed Here relies the problem But the only result that I get is this: As I can understand, pd.getdummies groups the field 'file' from my df_origin and uses 'text' to check their presence. How can I redirect the command to plot the columns 'percent' in my df_origin dataframe? "
            },
            "io": [
                "Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}\n",
                "Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}\n"
            ],
            "answer": {
                "ans_desc": "Try this: Output: ",
                "code": [
                    "       fly jump  run swim\nfile1   60   90  NaN   30\nfile2   30   70   40   60\nfile3   90   10   90   70\nfile4  NaN  NaN  NaN  NaN\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "python-2.7",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 8084647,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1428413980553713/picture?type=large",
            "display_name": "Mehul Bhatia",
            "link": "https://stackoverflow.com/users/8084647/mehul-bhatia"
        },
        "is_answered": true,
        "view_count": 1136,
        "accepted_answer_id": 50534025,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1528152208,
        "creation_date": 1527266049,
        "last_edit_date": 1527267762,
        "question_id": 50533522,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50533522/copying-columns-within-pandas-dataframe",
        "title": "Copying columns within pandas dataframe",
        "body": "<p>I want to slice and copy columns in a Python Dataframe. My data frame looks like the following:</p>\n\n<pre><code>     1928  1928.1  1929  1929.1  1930  1930.1\n 0    0     0       0     0       0     0\n 1    1     3       3     2       2     2\n 2    4     1       3     0       1     2\n</code></pre>\n\n<p>I want to make it of the form </p>\n\n<pre><code>     1928  1928.1  1929 1929.1 1930 1930.1\n 0   0     0            \n 1   1     3          \n 2   4     1                    \n 3   0     0\n 4   3     2\n 5   3     0\n 6   0     0\n 7   2     2\n 8   1     2 \n</code></pre>\n\n<p>Which basically means that I want to shift the values in Columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1'</p>\n\n<p>For the same, I wrote the code as</p>\n\n<pre><code>   [In]x=2\n       y=2\n       b=3\n       c=x-1\n       for a in range(0,2):\n            df.iloc[b:(b+3),0:x]=df.iloc[0:3,x:(x+y)]\n            x=x+2\n            b=b+3\n   [In] df\n   [Out] \n     1928  1928.1  1929  1929.1  1930  1930.1\n 0    0     0       0     0       0     0\n 1    1     3       3     2       2     2\n 2    4     1       3     0       1     2\n</code></pre>\n\n<p>No copying takes places within the columns. How shall I modify my code??</p>\n",
        "answer_body": "<p><strong><em>Setup</em></strong></p>\n\n<pre><code>cols = ['1929', '1929.1', '1930', '1930.1']\nvals = df[cols].values.reshape(-1, 2)\n</code></pre>\n\n<p><strong><code>numpy.vstack</code></strong> with <strong><code>append</code></strong>:</p>\n\n<pre><code>df[['1928', '1928.1']].append(\n    pd.DataFrame(\n        np.vstack([vals[::2], vals[1::2]]), columns = ['1928', '1928.1']\n    )\n)\n\n   1928  1928.1\n0     0       0\n1     1       3\n2     4       1\n0     0       0\n1     3       2\n2     3       0\n3     0       0\n4     2       2\n5     1       2\n</code></pre>\n",
        "question_body": "<p>I want to slice and copy columns in a Python Dataframe. My data frame looks like the following:</p>\n\n<pre><code>     1928  1928.1  1929  1929.1  1930  1930.1\n 0    0     0       0     0       0     0\n 1    1     3       3     2       2     2\n 2    4     1       3     0       1     2\n</code></pre>\n\n<p>I want to make it of the form </p>\n\n<pre><code>     1928  1928.1  1929 1929.1 1930 1930.1\n 0   0     0            \n 1   1     3          \n 2   4     1                    \n 3   0     0\n 4   3     2\n 5   3     0\n 6   0     0\n 7   2     2\n 8   1     2 \n</code></pre>\n\n<p>Which basically means that I want to shift the values in Columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1'</p>\n\n<p>For the same, I wrote the code as</p>\n\n<pre><code>   [In]x=2\n       y=2\n       b=3\n       c=x-1\n       for a in range(0,2):\n            df.iloc[b:(b+3),0:x]=df.iloc[0:3,x:(x+y)]\n            x=x+2\n            b=b+3\n   [In] df\n   [Out] \n     1928  1928.1  1929  1929.1  1930  1930.1\n 0    0     0       0     0       0     0\n 1    1     3       3     2       2     2\n 2    4     1       3     0       1     2\n</code></pre>\n\n<p>No copying takes places within the columns. How shall I modify my code??</p>\n",
        "formatted_input": {
            "qid": 50533522,
            "link": "https://stackoverflow.com/questions/50533522/copying-columns-within-pandas-dataframe",
            "question": {
                "title": "Copying columns within pandas dataframe",
                "ques_desc": "I want to slice and copy columns in a Python Dataframe. My data frame looks like the following: I want to make it of the form Which basically means that I want to shift the values in Columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1' For the same, I wrote the code as No copying takes places within the columns. How shall I modify my code?? "
            },
            "io": [
                "     1928  1928.1  1929  1929.1  1930  1930.1\n 0    0     0       0     0       0     0\n 1    1     3       3     2       2     2\n 2    4     1       3     0       1     2\n",
                "     1928  1928.1  1929 1929.1 1930 1930.1\n 0   0     0            \n 1   1     3          \n 2   4     1                    \n 3   0     0\n 4   3     2\n 5   3     0\n 6   0     0\n 7   2     2\n 8   1     2 \n"
            ],
            "answer": {
                "ans_desc": "Setup with : ",
                "code": [
                    "cols = ['1929', '1929.1', '1930', '1930.1']\nvals = df[cols].values.reshape(-1, 2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1655,
            "user_id": 2691630,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/5c145b89a0b5d74d36e305a578254f63?s=128&d=identicon&r=PG&f=1",
            "display_name": "Santiago Munez",
            "link": "https://stackoverflow.com/users/2691630/santiago-munez"
        },
        "is_answered": true,
        "view_count": 138158,
        "closed_date": 1548311850,
        "accepted_answer_id": 18942558,
        "answer_count": 2,
        "score": 51,
        "last_activity_date": 1528056725,
        "creation_date": 1379843444,
        "last_edit_date": 1477436058,
        "question_id": 18942506,
        "link": "https://stackoverflow.com/questions/18942506/add-new-column-in-pandas-dataframe-python",
        "closed_reason": "Duplicate",
        "title": "Add new column in Pandas DataFrame Python",
        "body": "<p>I have dataframe in Pandas for example:</p>\n\n<pre><code>Col1 Col2\nA     1 \nB     2\nC     3\n</code></pre>\n\n<p>Now if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be:</p>\n\n<pre><code>Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n</code></pre>\n\n<p>Any idea on how to achieve this?</p>\n",
        "answer_body": "<p>You just do an opposite comparison. <code>if Col2 &lt;= 1</code>. This will return a boolean Series with <code>False</code> values for those greater than 1 and <code>True</code> values for the other. If you convert it to an <code>int64</code> dtype, <code>True</code> becomes <code>1</code> and <code>False</code> become <code>0</code>,</p>\n\n<pre><code>df['Col3'] = (df['Col2'] &lt;= 1).astype(int)\n</code></pre>\n\n<p>If you want a more general solution, where you can assign any number to <code>Col3</code> depending on the value of <code>Col2</code> you should do something like:</p>\n\n<pre><code>df['Col3'] = df['Col2'].map(lambda x: 42 if x &gt; 1 else 55)\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>df['Col3'] = 0\ncondition = df['Col2'] &gt; 1\ndf.loc[condition, 'Col3'] = 42\ndf.loc[~condition, 'Col3'] = 55\n</code></pre>\n",
        "question_body": "<p>I have dataframe in Pandas for example:</p>\n\n<pre><code>Col1 Col2\nA     1 \nB     2\nC     3\n</code></pre>\n\n<p>Now if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be:</p>\n\n<pre><code>Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n</code></pre>\n\n<p>Any idea on how to achieve this?</p>\n",
        "formatted_input": {
            "qid": 18942506,
            "link": "https://stackoverflow.com/questions/18942506/add-new-column-in-pandas-dataframe-python",
            "question": {
                "title": "Add new column in Pandas DataFrame Python",
                "ques_desc": "I have dataframe in Pandas for example: Now if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be: Any idea on how to achieve this? "
            },
            "io": [
                "Col1 Col2\nA     1 \nB     2\nC     3\n",
                "Col1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n"
            ],
            "answer": {
                "ans_desc": "You just do an opposite comparison. . This will return a boolean Series with values for those greater than 1 and values for the other. If you convert it to an dtype, becomes and become , If you want a more general solution, where you can assign any number to depending on the value of you should do something like: Or: ",
                "code": [
                    "df['Col3'] = (df['Col2'] <= 1).astype(int)\n",
                    "df['Col3'] = df['Col2'].map(lambda x: 42 if x > 1 else 55)\n",
                    "df['Col3'] = 0\ncondition = df['Col2'] > 1\ndf.loc[condition, 'Col3'] = 42\ndf.loc[~condition, 'Col3'] = 55\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2075,
            "user_id": 8660280,
            "user_type": "registered",
            "accept_rate": 74,
            "profile_image": "https://www.gravatar.com/avatar/e55c666d520a7858d53707429f785ead?s=128&d=identicon&r=PG&f=1",
            "display_name": "ubuntu_noob",
            "link": "https://stackoverflow.com/users/8660280/ubuntu-noob"
        },
        "is_answered": true,
        "view_count": 298,
        "accepted_answer_id": 50642413,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1527864778,
        "creation_date": 1527851838,
        "last_edit_date": 1527864778,
        "question_id": 50642233,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50642233/creating-single-row-pandas-dataframe",
        "title": "Creating single row pandas dataframe",
        "body": "<p>I have a dataframe like this-</p>\n\n<pre><code>                 0\n0            a  43\n1            b  630\n2            r  587\n3            i  462\n4            g  153\n5            t  266\n</code></pre>\n\n<p>I want to create a new dataframe which looks like this-</p>\n\n<pre><code>     a         b     r       i     g        t\n0    43       630   587    462    153      266\n</code></pre>\n",
        "answer_body": "<p>If you have just one column with this format <code>a 43</code>, this should work:</p>\n\n<pre><code>df.columns = ['col']\ndf = pd.DataFrame(df.col.str.split(' ',1).tolist(), columns = ['col1','col2']).T.reset_index(drop=True)\ndf = df.rename(columns=df.iloc[0]).drop(df.index[0])\n</code></pre>\n\n<p>Input:</p>\n\n<pre><code>df = pd.DataFrame(data=[\n     ['a 43',],\n     ['b 630'],\n     ['r 587']],\ncolumns=['col'])\n\n     col\n0   a 43\n1  b 630\n2  r 587\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>    a    b    r\n1  43  630  587\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this-</p>\n\n<pre><code>                 0\n0            a  43\n1            b  630\n2            r  587\n3            i  462\n4            g  153\n5            t  266\n</code></pre>\n\n<p>I want to create a new dataframe which looks like this-</p>\n\n<pre><code>     a         b     r       i     g        t\n0    43       630   587    462    153      266\n</code></pre>\n",
        "formatted_input": {
            "qid": 50642233,
            "link": "https://stackoverflow.com/questions/50642233/creating-single-row-pandas-dataframe",
            "question": {
                "title": "Creating single row pandas dataframe",
                "ques_desc": "I have a dataframe like this- I want to create a new dataframe which looks like this- "
            },
            "io": [
                "                 0\n0            a  43\n1            b  630\n2            r  587\n3            i  462\n4            g  153\n5            t  266\n",
                "     a         b     r       i     g        t\n0    43       630   587    462    153      266\n"
            ],
            "answer": {
                "ans_desc": "If you have just one column with this format , this should work: Input: Output: ",
                "code": [
                    "df.columns = ['col']\ndf = pd.DataFrame(df.col.str.split(' ',1).tolist(), columns = ['col1','col2']).T.reset_index(drop=True)\ndf = df.rename(columns=df.iloc[0]).drop(df.index[0])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matrix"
        ],
        "owner": {
            "reputation": 1026,
            "user_id": 7258898,
            "user_type": "registered",
            "accept_rate": 32,
            "profile_image": "https://www.gravatar.com/avatar/6161c57d2988e99274a8ae0bbd6f0c29?s=128&d=identicon&r=PG&f=1",
            "display_name": "Christopher Costello",
            "link": "https://stackoverflow.com/users/7258898/christopher-costello"
        },
        "is_answered": true,
        "view_count": 273,
        "accepted_answer_id": 50594842,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1527688418,
        "creation_date": 1527633357,
        "question_id": 50593948,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50593948/python-cbind-previous-and-next-row-to-current-row",
        "title": "Python - Cbind previous and next row to current row",
        "body": "<p>I have a Pandas data frame like so:</p>\n\n<pre><code>d = {'col1': [1, 2], 'col2': [3, 4], 'col3': [5, 6]}\ndf = pd.DataFrame(data=d)\n</code></pre>\n\n<p>Which looks like:</p>\n\n<pre><code>  doc  sent col1 col2 col3\n0   0    0    5   4    8\n1   0    1    6   3    2\n2   0    2    1   2    9\n3   1    0    6   1    6\n4   1    1    5   1    5\n</code></pre>\n\n<p>I'd like to bind the previous row and the next next row to each column like so (accounting for \"doc\" and \"sent\" column in my example, which count as indices that nothing can come before or after as seen below):</p>\n\n<pre><code>  doc  sent col1 col2 col3 p_col1 p_col2 p_col3 n_col1 n_col2 n_col3\n0   0    0    5   4    8    0      0      0      6       3      2  \n1   0    1    6   3    2    5      4      8      1       2      9\n2   0    2    1   2    9    6      3      2      6       1      6\n3   1    0    6   1    6    0      0      0      5       1      5\n4   1    1    5   1    5    6      1      6      0       0      0\n</code></pre>\n",
        "answer_body": "<p>use <a href=\"http://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.shift.html\" rel=\"nofollow noreferrer\"><code>pd.DataFrame.shift</code></a> to get the prev / next rows, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>pd.concat</code></a> to merge the dataframes &amp; <a href=\"http://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.fillna.html\" rel=\"nofollow noreferrer\"><code>fillna</code></a> to set nulls to zero</p>\n\n<p>The presence of nulls upcasts the ints to floats, since numpy integer arrays cannot contain null values, which are cast back to ints after replacing nulls with 0.</p>\n\n<pre><code>cs = ['col1', 'col2', 'col3']\ng = df.groupby('doc')\n\npd.concat([\n   df, \n   g[cs].shift(-1).add_prefix('n'), \n   g[cs].shift().add_prefix('p')\n], axis=1).fillna(0).astype(int)\n</code></pre>\n\n<p>outputs:</p>\n\n<pre><code>   doc  sent  col1  col2  col3  ncol1  ncol2  ncol3  pcol1  pcol2  pcol3\n0    0     0     5     4     8      6      3      2      0      0      0\n1    0     1     6     3     2      1      2      9      5      4      8\n2    0     2     1     2     9      0      0      0      6      3      2\n3    1     0     6     1     6      5      1      5      0      0      0\n4    1     1     5     1     5      0      0      0      6      1      6\n</code></pre>\n",
        "question_body": "<p>I have a Pandas data frame like so:</p>\n\n<pre><code>d = {'col1': [1, 2], 'col2': [3, 4], 'col3': [5, 6]}\ndf = pd.DataFrame(data=d)\n</code></pre>\n\n<p>Which looks like:</p>\n\n<pre><code>  doc  sent col1 col2 col3\n0   0    0    5   4    8\n1   0    1    6   3    2\n2   0    2    1   2    9\n3   1    0    6   1    6\n4   1    1    5   1    5\n</code></pre>\n\n<p>I'd like to bind the previous row and the next next row to each column like so (accounting for \"doc\" and \"sent\" column in my example, which count as indices that nothing can come before or after as seen below):</p>\n\n<pre><code>  doc  sent col1 col2 col3 p_col1 p_col2 p_col3 n_col1 n_col2 n_col3\n0   0    0    5   4    8    0      0      0      6       3      2  \n1   0    1    6   3    2    5      4      8      1       2      9\n2   0    2    1   2    9    6      3      2      6       1      6\n3   1    0    6   1    6    0      0      0      5       1      5\n4   1    1    5   1    5    6      1      6      0       0      0\n</code></pre>\n",
        "formatted_input": {
            "qid": 50593948,
            "link": "https://stackoverflow.com/questions/50593948/python-cbind-previous-and-next-row-to-current-row",
            "question": {
                "title": "Python - Cbind previous and next row to current row",
                "ques_desc": "I have a Pandas data frame like so: Which looks like: I'd like to bind the previous row and the next next row to each column like so (accounting for \"doc\" and \"sent\" column in my example, which count as indices that nothing can come before or after as seen below): "
            },
            "io": [
                "  doc  sent col1 col2 col3\n0   0    0    5   4    8\n1   0    1    6   3    2\n2   0    2    1   2    9\n3   1    0    6   1    6\n4   1    1    5   1    5\n",
                "  doc  sent col1 col2 col3 p_col1 p_col2 p_col3 n_col1 n_col2 n_col3\n0   0    0    5   4    8    0      0      0      6       3      2  \n1   0    1    6   3    2    5      4      8      1       2      9\n2   0    2    1   2    9    6      3      2      6       1      6\n3   1    0    6   1    6    0      0      0      5       1      5\n4   1    1    5   1    5    6      1      6      0       0      0\n"
            ],
            "answer": {
                "ans_desc": "use to get the prev / next rows, to merge the dataframes & to set nulls to zero The presence of nulls upcasts the ints to floats, since numpy integer arrays cannot contain null values, which are cast back to ints after replacing nulls with 0. outputs: ",
                "code": [
                    "cs = ['col1', 'col2', 'col3']\ng = df.groupby('doc')\n\npd.concat([\n   df, \n   g[cs].shift(-1).add_prefix('n'), \n   g[cs].shift().add_prefix('p')\n], axis=1).fillna(0).astype(int)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 400,
            "user_id": 7859096,
            "user_type": "registered",
            "accept_rate": 69,
            "profile_image": "https://lh3.googleusercontent.com/-p2JSfLd_OtQ/AAAAAAAAAAI/AAAAAAAAABI/UJhDkT-Ahus/photo.jpg?sz=128",
            "display_name": "Jonathan Pacheco",
            "link": "https://stackoverflow.com/users/7859096/jonathan-pacheco"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 50592751,
        "answer_count": 2,
        "score": -2,
        "last_activity_date": 1527627996,
        "creation_date": 1527626462,
        "last_edit_date": 1527626678,
        "question_id": 50592595,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50592595/merge-dataframes-including-extreme-values",
        "title": "Merge dataframes including extreme values",
        "body": "<p>I have 2 data frames, df1 and df2:</p>\n\n<pre><code>df1\nOut[66]: \n    A   B\n0   1  11\n1   1   2\n2   1  32\n3   1  42\n4   1  54\n5   1  66\n6   2  16\n7   2  23\n8   3  13\n9   3  24\n10  3  35\n11  3  46\n12  3  51\n13  4  12\n14  4  28\n15  4  39\n16  4  49\n\ndf2\nOut[80]: \n    B\n0  32\n1  42\n2  13\n3  24\n4  35\n5  39\n6  49\n</code></pre>\n\n<p>I want to merge dataframes but at the same time including the first and/or last value of the set in column A. This is an example of the desired outcome:</p>\n\n<pre><code>df3\nOut[93]: \n    A   B\n0   1   2\n1   1  32\n2   1  42\n3   1  54\n4   3  13\n5   3  24\n6   3  35\n7   3  46\n8   4  28\n9   4  39\n10  4  49\n</code></pre>\n\n<p>I'm trying to use <code>merge</code> but that only slice the portion of data frames that coincides. Someone have an idea to deal with this? thanks!</p>\n",
        "answer_body": "<p>Here's one way to do it using <code>merge</code> with indicator, <code>groupby</code>, and <code>rolling</code>:</p>\n\n<pre><code>df[df.merge(df2, on='B', how='left', indicator='Ind').eval('Found=Ind == \"both\"')\n     .groupby('A')['Found']\n     .apply(lambda x: x.rolling(3, center=True, min_periods=2).max()).astype(bool)]\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>    A   B\n1   1   2\n2   1  32\n3   1  42\n4   1  54\n8   3  13\n9   3  24\n10  3  35\n11  3  46\n14  4  28\n15  4  39\n16  4  49\n</code></pre>\n",
        "question_body": "<p>I have 2 data frames, df1 and df2:</p>\n\n<pre><code>df1\nOut[66]: \n    A   B\n0   1  11\n1   1   2\n2   1  32\n3   1  42\n4   1  54\n5   1  66\n6   2  16\n7   2  23\n8   3  13\n9   3  24\n10  3  35\n11  3  46\n12  3  51\n13  4  12\n14  4  28\n15  4  39\n16  4  49\n\ndf2\nOut[80]: \n    B\n0  32\n1  42\n2  13\n3  24\n4  35\n5  39\n6  49\n</code></pre>\n\n<p>I want to merge dataframes but at the same time including the first and/or last value of the set in column A. This is an example of the desired outcome:</p>\n\n<pre><code>df3\nOut[93]: \n    A   B\n0   1   2\n1   1  32\n2   1  42\n3   1  54\n4   3  13\n5   3  24\n6   3  35\n7   3  46\n8   4  28\n9   4  39\n10  4  49\n</code></pre>\n\n<p>I'm trying to use <code>merge</code> but that only slice the portion of data frames that coincides. Someone have an idea to deal with this? thanks!</p>\n",
        "formatted_input": {
            "qid": 50592595,
            "link": "https://stackoverflow.com/questions/50592595/merge-dataframes-including-extreme-values",
            "question": {
                "title": "Merge dataframes including extreme values",
                "ques_desc": "I have 2 data frames, df1 and df2: I want to merge dataframes but at the same time including the first and/or last value of the set in column A. This is an example of the desired outcome: I'm trying to use but that only slice the portion of data frames that coincides. Someone have an idea to deal with this? thanks! "
            },
            "io": [
                "df1\nOut[66]: \n    A   B\n0   1  11\n1   1   2\n2   1  32\n3   1  42\n4   1  54\n5   1  66\n6   2  16\n7   2  23\n8   3  13\n9   3  24\n10  3  35\n11  3  46\n12  3  51\n13  4  12\n14  4  28\n15  4  39\n16  4  49\n\ndf2\nOut[80]: \n    B\n0  32\n1  42\n2  13\n3  24\n4  35\n5  39\n6  49\n",
                "df3\nOut[93]: \n    A   B\n0   1   2\n1   1  32\n2   1  42\n3   1  54\n4   3  13\n5   3  24\n6   3  35\n7   3  46\n8   4  28\n9   4  39\n10  4  49\n"
            ],
            "answer": {
                "ans_desc": "Here's one way to do it using with indicator, , and : Output: ",
                "code": [
                    "df[df.merge(df2, on='B', how='left', indicator='Ind').eval('Found=Ind == \"both\"')\n     .groupby('A')['Found']\n     .apply(lambda x: x.rolling(3, center=True, min_periods=2).max()).astype(bool)]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "performance",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 7343160,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/770a3e8e1281b2a5ed578f0352e9a212?s=128&d=identicon&r=PG&f=1",
            "display_name": "Maganna Dev",
            "link": "https://stackoverflow.com/users/7343160/maganna-dev"
        },
        "is_answered": true,
        "view_count": 56,
        "accepted_answer_id": 50545167,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1527354574,
        "creation_date": 1527353611,
        "question_id": 50545075,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50545075/create-extra-columns-in-pandas-time-indexed-dataframe",
        "title": "Create extra columns in pandas time-indexed DataFrame",
        "body": "<p>I currenty have a Datetime-Indexed dataframe with three columns:</p>\n\n<pre><code>                     Glucosa   Insulina  Carbs\nHour\n2018-05-16 06:43:00    156.0       7.0   65.0\n2018-05-16 07:43:00    170.0       0.0   65.0\n2018-05-16 08:45:00    185.0       2.0    0.0\n2018-05-16 09:45:00    150.0       0.0    0.0\n2018-05-16 10:45:00     80.0       0.0    0.0\n     ...\n</code></pre>\n\n<p>I would like to create three extra columns that hold the values indexed one hour from the current index to end up with something like this:</p>\n\n<pre><code>                     Glucosa   Insulina  Carbs  Glucosa1  Insulina1  Carbs1\nHour\n2018-05-16 06:43:00    156.0       7.0   65.0      170.0        0.0   65.0\n2018-05-16 07:43:00    170.0       0.0   65.0      185.0        2.0    0.0\n2018-05-16 08:45:00    185.0       2.0    0.0      150.0        0.0    0.0\n2018-05-16 09:45:00    150.0       0.0    0.0       80.0        0.0    0.0\n2018-05-16 10:45:00     80.0       0.0    0.0       ...         ...    ...\n     ...\n</code></pre>\n\n<p>I have already defined a function that creates a dataframe with the columns 'Glucosa1',  'Insulina1',  'Carbs1' but it is very poorly performing and I would like to make it run faster.</p>\n\n<p>I profile the time used by different functions on my code using the following:</p>\n\n<pre><code>start = time.time()\n  # foo() \nend = time.time()\nprint(f' Time required to execute foo() : {end - start}')\n</code></pre>\n\n<p>This outputs a time of 8.331165 seconds (on average) for the function nn_format_df() compared to similar functions (which iterate over the rows of the dataframe) wicht output 0.366158 seconds .</p>\n\n<p>After creating a new dataframe calling my function on the original I merge them to get the desired dataframe.</p>\n\n<pre><code>df2 = nn_format_df(df)\ndf = df.join([df2])\n</code></pre>\n\n<p>The function: </p>\n\n<pre><code>def nn_format_df( df : pd.core.frame.DataFrame ) -&gt; pd.core.frame.DataFrame:\n\n  _indices   : pd.core.indexes.datetimes.DatetimeIndex = [ idx for idx in df.index ]\n  indices    = _indices[:-60]\n  _df        : pd.core.frame.DataFrame = df.copy()\n  _df1       : pd.core.frame.DataFrame\n  _glc1      : pd.core.series.Series   = pd.Series(pd.np.nan, index=_indices)\n  _insu1     : pd.core.series.Series   = pd.Series(pd.np.nan, index=_indices)\n  _carbs1    : pd.core.series.Series   = pd.Series(pd.np.nan, index=_indices)\n\n  aux        : pd._libs.tslibs.timestamps.Timestamp\n  aux1       : pd._libs.tslibs.timestamps.Timestamp\n  one        : datetime.timedelta = datetime.timedelta(hours=1) \n\n  for idx in indices:\n    aux  = _df.ix[ idx, : ].name\n    aux1 = aux + one\n    _glc1[   idx ]  = _df.ix[ aux1, 'Glucosa' ]\n    _insu1[  idx ]  = _df.ix[ aux1, 'Insulina' ]\n    _carbs1[ idx ]  = _df.ix[ aux1, 'Carbs' ]\n\n  _df1 = pd.DataFrame({ 'Glucosa1': _glc1,\\\n                       'Insulina1': _insu1,\\\n                          'Carbs1': _carbs1\n                      }, index=_indices)\n\n  return _df1\n</code></pre>\n\n<p>To sum it up:</p>\n\n<ul>\n<li>I would appreciate any comments on how to improve the function so that it doesn't take so long.</li>\n<li>A better, more Pythonic or pandas-y way of getting the desired dataframe is welcome. I am new to pandas and I understand my implementation of the function is a completely na\u00efve approach.</li>\n</ul>\n",
        "answer_body": "<p>You can accomplish this very quickly with <code>.shift</code>, which shifts an entire <code>DataFrame</code>. Just use <code>pd.concat</code> to combine them together; the <code>axis=1</code> argument specifies that you want to append new columns instead of rows.</p>\n\n<pre><code>import pandas as pd\npd.concat([df, df.shift(-1).rename(columns=dict((elem, elem+'1') for elem in df.columns))], axis=1)\n</code></pre>\n\n<p>The above code gives you the following output:</p>\n\n<pre><code>                     Glucosa  Insulina  Carbs  Glucosa1  Insulina1  Carbs1\nHour                                                                      \n2018-05-16 06:43:00    156.0       7.0   65.0     170.0        0.0    65.0\n2018-05-16 07:43:00    170.0       0.0   65.0     185.0        2.0     0.0\n2018-05-16 08:45:00    185.0       2.0    0.0     150.0        0.0     0.0\n2018-05-16 09:45:00    150.0       0.0    0.0      80.0        0.0     0.0\n2018-05-16 10:45:00     80.0       0.0    0.0       NaN        NaN     NaN\n</code></pre>\n",
        "question_body": "<p>I currenty have a Datetime-Indexed dataframe with three columns:</p>\n\n<pre><code>                     Glucosa   Insulina  Carbs\nHour\n2018-05-16 06:43:00    156.0       7.0   65.0\n2018-05-16 07:43:00    170.0       0.0   65.0\n2018-05-16 08:45:00    185.0       2.0    0.0\n2018-05-16 09:45:00    150.0       0.0    0.0\n2018-05-16 10:45:00     80.0       0.0    0.0\n     ...\n</code></pre>\n\n<p>I would like to create three extra columns that hold the values indexed one hour from the current index to end up with something like this:</p>\n\n<pre><code>                     Glucosa   Insulina  Carbs  Glucosa1  Insulina1  Carbs1\nHour\n2018-05-16 06:43:00    156.0       7.0   65.0      170.0        0.0   65.0\n2018-05-16 07:43:00    170.0       0.0   65.0      185.0        2.0    0.0\n2018-05-16 08:45:00    185.0       2.0    0.0      150.0        0.0    0.0\n2018-05-16 09:45:00    150.0       0.0    0.0       80.0        0.0    0.0\n2018-05-16 10:45:00     80.0       0.0    0.0       ...         ...    ...\n     ...\n</code></pre>\n\n<p>I have already defined a function that creates a dataframe with the columns 'Glucosa1',  'Insulina1',  'Carbs1' but it is very poorly performing and I would like to make it run faster.</p>\n\n<p>I profile the time used by different functions on my code using the following:</p>\n\n<pre><code>start = time.time()\n  # foo() \nend = time.time()\nprint(f' Time required to execute foo() : {end - start}')\n</code></pre>\n\n<p>This outputs a time of 8.331165 seconds (on average) for the function nn_format_df() compared to similar functions (which iterate over the rows of the dataframe) wicht output 0.366158 seconds .</p>\n\n<p>After creating a new dataframe calling my function on the original I merge them to get the desired dataframe.</p>\n\n<pre><code>df2 = nn_format_df(df)\ndf = df.join([df2])\n</code></pre>\n\n<p>The function: </p>\n\n<pre><code>def nn_format_df( df : pd.core.frame.DataFrame ) -&gt; pd.core.frame.DataFrame:\n\n  _indices   : pd.core.indexes.datetimes.DatetimeIndex = [ idx for idx in df.index ]\n  indices    = _indices[:-60]\n  _df        : pd.core.frame.DataFrame = df.copy()\n  _df1       : pd.core.frame.DataFrame\n  _glc1      : pd.core.series.Series   = pd.Series(pd.np.nan, index=_indices)\n  _insu1     : pd.core.series.Series   = pd.Series(pd.np.nan, index=_indices)\n  _carbs1    : pd.core.series.Series   = pd.Series(pd.np.nan, index=_indices)\n\n  aux        : pd._libs.tslibs.timestamps.Timestamp\n  aux1       : pd._libs.tslibs.timestamps.Timestamp\n  one        : datetime.timedelta = datetime.timedelta(hours=1) \n\n  for idx in indices:\n    aux  = _df.ix[ idx, : ].name\n    aux1 = aux + one\n    _glc1[   idx ]  = _df.ix[ aux1, 'Glucosa' ]\n    _insu1[  idx ]  = _df.ix[ aux1, 'Insulina' ]\n    _carbs1[ idx ]  = _df.ix[ aux1, 'Carbs' ]\n\n  _df1 = pd.DataFrame({ 'Glucosa1': _glc1,\\\n                       'Insulina1': _insu1,\\\n                          'Carbs1': _carbs1\n                      }, index=_indices)\n\n  return _df1\n</code></pre>\n\n<p>To sum it up:</p>\n\n<ul>\n<li>I would appreciate any comments on how to improve the function so that it doesn't take so long.</li>\n<li>A better, more Pythonic or pandas-y way of getting the desired dataframe is welcome. I am new to pandas and I understand my implementation of the function is a completely na\u00efve approach.</li>\n</ul>\n",
        "formatted_input": {
            "qid": 50545075,
            "link": "https://stackoverflow.com/questions/50545075/create-extra-columns-in-pandas-time-indexed-dataframe",
            "question": {
                "title": "Create extra columns in pandas time-indexed DataFrame",
                "ques_desc": "I currenty have a Datetime-Indexed dataframe with three columns: I would like to create three extra columns that hold the values indexed one hour from the current index to end up with something like this: I have already defined a function that creates a dataframe with the columns 'Glucosa1', 'Insulina1', 'Carbs1' but it is very poorly performing and I would like to make it run faster. I profile the time used by different functions on my code using the following: This outputs a time of 8.331165 seconds (on average) for the function nn_format_df() compared to similar functions (which iterate over the rows of the dataframe) wicht output 0.366158 seconds . After creating a new dataframe calling my function on the original I merge them to get the desired dataframe. The function: To sum it up: I would appreciate any comments on how to improve the function so that it doesn't take so long. A better, more Pythonic or pandas-y way of getting the desired dataframe is welcome. I am new to pandas and I understand my implementation of the function is a completely na\u00efve approach. "
            },
            "io": [
                "                     Glucosa   Insulina  Carbs\nHour\n2018-05-16 06:43:00    156.0       7.0   65.0\n2018-05-16 07:43:00    170.0       0.0   65.0\n2018-05-16 08:45:00    185.0       2.0    0.0\n2018-05-16 09:45:00    150.0       0.0    0.0\n2018-05-16 10:45:00     80.0       0.0    0.0\n     ...\n",
                "                     Glucosa   Insulina  Carbs  Glucosa1  Insulina1  Carbs1\nHour\n2018-05-16 06:43:00    156.0       7.0   65.0      170.0        0.0   65.0\n2018-05-16 07:43:00    170.0       0.0   65.0      185.0        2.0    0.0\n2018-05-16 08:45:00    185.0       2.0    0.0      150.0        0.0    0.0\n2018-05-16 09:45:00    150.0       0.0    0.0       80.0        0.0    0.0\n2018-05-16 10:45:00     80.0       0.0    0.0       ...         ...    ...\n     ...\n"
            ],
            "answer": {
                "ans_desc": "You can accomplish this very quickly with , which shifts an entire . Just use to combine them together; the argument specifies that you want to append new columns instead of rows. The above code gives you the following output: ",
                "code": [
                    "import pandas as pd\npd.concat([df, df.shift(-1).rename(columns=dict((elem, elem+'1') for elem in df.columns))], axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 3139,
            "user_id": 6204900,
            "user_type": "registered",
            "accept_rate": 94,
            "profile_image": "https://www.gravatar.com/avatar/4c88f41c450074eebb6fe844184601fe?s=128&d=identicon&r=PG&f=1",
            "display_name": "splinter",
            "link": "https://stackoverflow.com/users/6204900/splinter"
        },
        "is_answered": true,
        "view_count": 182,
        "accepted_answer_id": 50481749,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1527059820,
        "creation_date": 1527057415,
        "question_id": 50481372,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50481372/lengthening-a-dataframe-based-on-stacking-columns-within-it-in-pandas",
        "title": "Lengthening a DataFrame based on stacking columns within it in Pandas",
        "body": "<p>I am looking for a function that achieves the following. It is best shown in an example. Consider:</p>\n\n<pre><code>pd.DataFrame([ [1, 2, 3 ], [4, 5, np.nan ]], columns=['x', 'y1', 'y2'])\n</code></pre>\n\n<p>which looks like:</p>\n\n<pre><code>   x  y1   y2\n0  1   2  3\n1  4   5  NaN\n</code></pre>\n\n<p>I would like to collapase the <code>y1</code> and <code>y2</code> columns, lengthening the DataFame where necessary, so that the output is:</p>\n\n<pre><code>   x  y\n0  1   2  \n1  1   3  \n2  4   5  \n</code></pre>\n\n<p>That is, one row for each combination between either <code>x</code> and <code>y1</code>,  or <code>x</code> and <code>y2</code>. I am looking for a function that does this relatively efficiently, as I have multiple <code>y</code>s and many rows.</p>\n",
        "answer_body": "<p>Here's one based on NumPy, as you were looking for performance -</p>\n\n<pre><code>def gather_columns(df):\n    col_mask = [i.startswith('y') for i in df.columns]\n    ally_vals = df.iloc[:,col_mask].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n</code></pre>\n\n<p>Sample run -</p>\n\n<pre><code>In [78]: df #(added more cols for variety)\nOut[78]: \n   x  y1   y2   y5   y7\n0  1   2  3.0  NaN  NaN\n1  4   5  NaN  6.0  7.0\n\nIn [79]: gather_columns(df)\nOut[79]: \n   x    y\n0  1  2.0\n1  1  3.0\n2  4  5.0\n3  4  6.0\n4  4  7.0\n</code></pre>\n\n<p>If the <code>y</code> columns are always starting from the second column onwards until the end, we can simply slice the dataframe and hence get further performance boost, like so -</p>\n\n<pre><code>def gather_columns_v2(df):\n    ally_vals = df.iloc[:,1:].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n</code></pre>\n",
        "question_body": "<p>I am looking for a function that achieves the following. It is best shown in an example. Consider:</p>\n\n<pre><code>pd.DataFrame([ [1, 2, 3 ], [4, 5, np.nan ]], columns=['x', 'y1', 'y2'])\n</code></pre>\n\n<p>which looks like:</p>\n\n<pre><code>   x  y1   y2\n0  1   2  3\n1  4   5  NaN\n</code></pre>\n\n<p>I would like to collapase the <code>y1</code> and <code>y2</code> columns, lengthening the DataFame where necessary, so that the output is:</p>\n\n<pre><code>   x  y\n0  1   2  \n1  1   3  \n2  4   5  \n</code></pre>\n\n<p>That is, one row for each combination between either <code>x</code> and <code>y1</code>,  or <code>x</code> and <code>y2</code>. I am looking for a function that does this relatively efficiently, as I have multiple <code>y</code>s and many rows.</p>\n",
        "formatted_input": {
            "qid": 50481372,
            "link": "https://stackoverflow.com/questions/50481372/lengthening-a-dataframe-based-on-stacking-columns-within-it-in-pandas",
            "question": {
                "title": "Lengthening a DataFrame based on stacking columns within it in Pandas",
                "ques_desc": "I am looking for a function that achieves the following. It is best shown in an example. Consider: which looks like: I would like to collapase the and columns, lengthening the DataFame where necessary, so that the output is: That is, one row for each combination between either and , or and . I am looking for a function that does this relatively efficiently, as I have multiple s and many rows. "
            },
            "io": [
                "   x  y1   y2\n0  1   2  3\n1  4   5  NaN\n",
                "   x  y\n0  1   2  \n1  1   3  \n2  4   5  \n"
            ],
            "answer": {
                "ans_desc": "Here's one based on NumPy, as you were looking for performance - Sample run - If the columns are always starting from the second column onwards until the end, we can simply slice the dataframe and hence get further performance boost, like so - ",
                "code": [
                    "def gather_columns(df):\n    col_mask = [i.startswith('y') for i in df.columns]\n    ally_vals = df.iloc[:,col_mask].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n",
                    "def gather_columns_v2(df):\n    ally_vals = df.iloc[:,1:].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 5445395,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://graph.facebook.com/891844017569723/picture?type=large",
            "display_name": "Vadim  K",
            "link": "https://stackoverflow.com/users/5445395/vadim-k"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 50409509,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1526649183,
        "creation_date": 1526639833,
        "question_id": 50409452,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50409452/python-pandas-apply-on-separated-values",
        "title": "Python pandas: apply on separated values",
        "body": "<p>How can I sum values in dataframe that a separated by semicolon?</p>\n\n<p>Got:</p>\n\n<pre><code>                  col1            col2\n2018-03-05         2.1               8\n2018-03-06           8           3.1;2\n2018-03-07         1;1             8;1\n</code></pre>\n\n<p>Need:</p>\n\n<pre><code>                  col1            col2\n2018-03-05         2.1               8\n2018-03-06           8             5.1\n2018-03-07           2               9\n</code></pre>\n",
        "answer_body": "<p>You can use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> for processes each column with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html\" rel=\"nofollow noreferrer\"><code>split</code></a>, cast to <code>float</code> and <code>sum</code> per columns:</p>\n\n<pre><code>df = df.apply(lambda x: x.str.split(';', expand=True).astype(float).sum(axis=1))\n</code></pre>\n\n<p>Or process each value separately by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html\" rel=\"nofollow noreferrer\"><code>applymap</code></a>:</p>\n\n<pre><code>df = df.applymap(lambda x: sum(map(float, x.split(';'))))\nprint (df)\n            col1  col2\n2018-03-05   2.1   8.0\n2018-03-06   8.0   5.1\n2018-03-07   2.0   9.0\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>If numeric with strings columns is possible use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html\" rel=\"nofollow noreferrer\"><code>select_dtypes</code></a> for exclude numeric and working only with <code>string</code>s columns with <code>;</code>:</p>\n\n<pre><code>print (df)\n           col1   col2  col3\n2018-03-05  2.1      8     1\n2018-03-06    8  3.1;2     2\n2018-03-07  1;1    8;1     8\n\ncols = df.select_dtypes(exclude=np.number).columns\ndf[cols] = df[cols].apply(lambda x: x.str.split(';', expand=True).astype(float).sum(axis=1))\nprint (df)\n            col1  col2  col3\n2018-03-05   2.1   8.0     1\n2018-03-06   8.0   5.1     2\n2018-03-07   2.0   9.0     8\n</code></pre>\n",
        "question_body": "<p>How can I sum values in dataframe that a separated by semicolon?</p>\n\n<p>Got:</p>\n\n<pre><code>                  col1            col2\n2018-03-05         2.1               8\n2018-03-06           8           3.1;2\n2018-03-07         1;1             8;1\n</code></pre>\n\n<p>Need:</p>\n\n<pre><code>                  col1            col2\n2018-03-05         2.1               8\n2018-03-06           8             5.1\n2018-03-07           2               9\n</code></pre>\n",
        "formatted_input": {
            "qid": 50409452,
            "link": "https://stackoverflow.com/questions/50409452/python-pandas-apply-on-separated-values",
            "question": {
                "title": "Python pandas: apply on separated values",
                "ques_desc": "How can I sum values in dataframe that a separated by semicolon? Got: Need: "
            },
            "io": [
                "                  col1            col2\n2018-03-05         2.1               8\n2018-03-06           8           3.1;2\n2018-03-07         1;1             8;1\n",
                "                  col1            col2\n2018-03-05         2.1               8\n2018-03-06           8             5.1\n2018-03-07           2               9\n"
            ],
            "answer": {
                "ans_desc": "You can use for processes each column with , cast to and per columns: Or process each value separately by : EDIT: If numeric with strings columns is possible use for exclude numeric and working only with s columns with : ",
                "code": [
                    "df = df.apply(lambda x: x.str.split(';', expand=True).astype(float).sum(axis=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "datetime",
            "dataframe"
        ],
        "owner": {
            "reputation": 3653,
            "user_id": 1506850,
            "user_type": "registered",
            "accept_rate": 94,
            "profile_image": "https://www.gravatar.com/avatar/529adde7e6c422b0ab9b990d9b09e8da?s=128&d=identicon&r=PG&f=1",
            "display_name": "00__00__00",
            "link": "https://stackoverflow.com/users/1506850/00-00-00"
        },
        "is_answered": true,
        "view_count": 187,
        "accepted_answer_id": 50336535,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1526321405,
        "creation_date": 1526320281,
        "question_id": 50336251,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50336251/split-dataframe-entries-at-midnight",
        "title": "split dataframe entries at midnight",
        "body": "<p>I have a <code>pandas</code> <code>dataframe</code>, with <code>Start</code> and <code>End</code> datatime.</p>\n\n<pre><code>df=pd.DataFrame(data=pd.date_range('20100201', periods=10, freq='5h3min'),columns=['Start'])\ndf.loc[:,'End']=df.loc[:,'Start']+pd.Timedelta(4,'h')\n</code></pre>\n\n<p><code>Start</code> and <code>End</code> can be expected to be sorted interally, but gaps/overlaps may occur between consecutive rows.</p>\n\n<p>I would like to create a new dataframe with the difference that if row contains midnight (e.g. midnight is contained in [<code>Start</code>,<code>End</code>]), the row is then split in two parts before and after midnight\nex: </p>\n\n<pre><code> Start                 End\n0 2010-02-01 00:00:00 2010-02-01 04:00:00\n1 2010-02-01 05:03:00 2010-02-01 09:03:00\n2 2010-02-01 10:06:00 2010-02-01 14:06:00\n3 2010-02-01 15:09:00 2010-02-01 19:09:00\n4 2010-02-01 20:12:00 2010-02-02 00:12:00\n5 2010-02-02 01:15:00 2010-02-02 05:15:00\n</code></pre>\n\n<p>should be</p>\n\n<pre><code>Start                 End\n    0 2010-02-01 00:00:00 2010-02-01 04:00:00\n    1 2010-02-01 05:03:00 2010-02-01 09:03:00\n    2 2010-02-01 10:06:00 2010-02-01 14:06:00\n    3 2010-02-01 15:09:00 2010-02-01 19:09:00\n    -----------------------------------------\n    4 2010-02-01 20:12:00 2010-02-01 23:59:00\n    5 2010-02-02 00:00:00 2010-02-02 00:12:00\n    -----------------------------------------\n    6 2010-02-02 01:15:00 2010-02-02 05:15:00\n</code></pre>\n",
        "answer_body": "<p>You can concat the DataFrame of new pairs, then erase the old ones.</p>\n\n<p>First find the splits:</p>\n\n<pre><code>splits = df[df.End.dt.date &gt; df.Start.dt.date].copy()\n</code></pre>\n\n<p>Now concatenate and drop:</p>\n\n<pre><code>&gt;&gt;&gt; pd.concat([\n    df,\n    pd.DataFrame({\n        'Start': list(splits.Start) + list(splits.End.dt.floor(freq='1D')),\n        'End': list(splits.Start.dt.ceil(freq='1D')) + list(splits.End)})\n]).drop(splits.index).sort_values(by='Start')\n    End Start\n0   2010-02-01 04:00:00 2010-02-01 00:00:00\n1   2010-02-01 09:03:00 2010-02-01 05:03:00\n2   2010-02-01 14:06:00 2010-02-01 10:06:00\n3   2010-02-01 19:09:00 2010-02-01 15:09:00\n0   2010-02-02 00:00:00 2010-02-01 20:12:00\n2   2010-02-02 00:12:00 2010-02-02 00:00:00\n5   2010-02-02 05:15:00 2010-02-02 01:15:00\n6   2010-02-02 10:18:00 2010-02-02 06:18:00\n7   2010-02-02 15:21:00 2010-02-02 11:21:00\n8   2010-02-02 20:24:00 2010-02-02 16:24:00\n1   2010-02-03 00:00:00 2010-02-02 21:27:00\n3   2010-02-03 01:27:00 2010-02-03 00:00:00\n</code></pre>\n",
        "question_body": "<p>I have a <code>pandas</code> <code>dataframe</code>, with <code>Start</code> and <code>End</code> datatime.</p>\n\n<pre><code>df=pd.DataFrame(data=pd.date_range('20100201', periods=10, freq='5h3min'),columns=['Start'])\ndf.loc[:,'End']=df.loc[:,'Start']+pd.Timedelta(4,'h')\n</code></pre>\n\n<p><code>Start</code> and <code>End</code> can be expected to be sorted interally, but gaps/overlaps may occur between consecutive rows.</p>\n\n<p>I would like to create a new dataframe with the difference that if row contains midnight (e.g. midnight is contained in [<code>Start</code>,<code>End</code>]), the row is then split in two parts before and after midnight\nex: </p>\n\n<pre><code> Start                 End\n0 2010-02-01 00:00:00 2010-02-01 04:00:00\n1 2010-02-01 05:03:00 2010-02-01 09:03:00\n2 2010-02-01 10:06:00 2010-02-01 14:06:00\n3 2010-02-01 15:09:00 2010-02-01 19:09:00\n4 2010-02-01 20:12:00 2010-02-02 00:12:00\n5 2010-02-02 01:15:00 2010-02-02 05:15:00\n</code></pre>\n\n<p>should be</p>\n\n<pre><code>Start                 End\n    0 2010-02-01 00:00:00 2010-02-01 04:00:00\n    1 2010-02-01 05:03:00 2010-02-01 09:03:00\n    2 2010-02-01 10:06:00 2010-02-01 14:06:00\n    3 2010-02-01 15:09:00 2010-02-01 19:09:00\n    -----------------------------------------\n    4 2010-02-01 20:12:00 2010-02-01 23:59:00\n    5 2010-02-02 00:00:00 2010-02-02 00:12:00\n    -----------------------------------------\n    6 2010-02-02 01:15:00 2010-02-02 05:15:00\n</code></pre>\n",
        "formatted_input": {
            "qid": 50336251,
            "link": "https://stackoverflow.com/questions/50336251/split-dataframe-entries-at-midnight",
            "question": {
                "title": "split dataframe entries at midnight",
                "ques_desc": "I have a , with and datatime. and can be expected to be sorted interally, but gaps/overlaps may occur between consecutive rows. I would like to create a new dataframe with the difference that if row contains midnight (e.g. midnight is contained in [,]), the row is then split in two parts before and after midnight ex: should be "
            },
            "io": [
                " Start                 End\n0 2010-02-01 00:00:00 2010-02-01 04:00:00\n1 2010-02-01 05:03:00 2010-02-01 09:03:00\n2 2010-02-01 10:06:00 2010-02-01 14:06:00\n3 2010-02-01 15:09:00 2010-02-01 19:09:00\n4 2010-02-01 20:12:00 2010-02-02 00:12:00\n5 2010-02-02 01:15:00 2010-02-02 05:15:00\n",
                "Start                 End\n    0 2010-02-01 00:00:00 2010-02-01 04:00:00\n    1 2010-02-01 05:03:00 2010-02-01 09:03:00\n    2 2010-02-01 10:06:00 2010-02-01 14:06:00\n    3 2010-02-01 15:09:00 2010-02-01 19:09:00\n    -----------------------------------------\n    4 2010-02-01 20:12:00 2010-02-01 23:59:00\n    5 2010-02-02 00:00:00 2010-02-02 00:12:00\n    -----------------------------------------\n    6 2010-02-02 01:15:00 2010-02-02 05:15:00\n"
            ],
            "answer": {
                "ans_desc": "You can concat the DataFrame of new pairs, then erase the old ones. First find the splits: Now concatenate and drop: ",
                "code": [
                    "splits = df[df.End.dt.date > df.Start.dt.date].copy()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "list",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1313,
            "user_id": 7492420,
            "user_type": "registered",
            "accept_rate": 68,
            "profile_image": "https://www.gravatar.com/avatar/ca34e32c5b0e1aae0c1d5976cdd8ad60?s=128&d=identicon&r=PG&f=1",
            "display_name": "jovicbg",
            "link": "https://stackoverflow.com/users/7492420/jovicbg"
        },
        "is_answered": true,
        "view_count": 62,
        "accepted_answer_id": 50269637,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1525945582,
        "creation_date": 1525944317,
        "last_edit_date": 1525945304,
        "question_id": 50269584,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50269584/append-list-from-pandas-column-to-python-list",
        "title": "Append list from pandas column to python list",
        "body": "<p>I have values in a list in pandas column, for example:</p>\n\n<p>df </p>\n\n<pre><code>id       col1\n1       [51.97559, 4.12565]\n2       [52.97559, 3.12565]\n3       [49.97559, 5.12565]\n</code></pre>\n\n<p>But when I append col1 to list I got quote around the first element in each sublist.</p>\n\n<pre><code>new_list = []\nfor val in df['col1'].values:\n    new_list.append(val)\n</code></pre>\n\n<p>And I got:</p>\n\n<pre><code>[['51.97559', 4.12565]\n['52.97559', 3.12565]\n['49.97559', 5.12565]]\n</code></pre>\n\n<p>But I need:</p>\n\n<pre><code>[[51.97559, 4.12565]\n[52.97559, 3.12565]\n[49.97559, 5.12565]]\n</code></pre>\n",
        "answer_body": "<p>You can cast all the elements of the list into <code>float</code></p>\n\n<pre><code>new_list = []\nfor val in df['col1'].values:\n    new_list.append(list(map(float, val)))\n</code></pre>\n",
        "question_body": "<p>I have values in a list in pandas column, for example:</p>\n\n<p>df </p>\n\n<pre><code>id       col1\n1       [51.97559, 4.12565]\n2       [52.97559, 3.12565]\n3       [49.97559, 5.12565]\n</code></pre>\n\n<p>But when I append col1 to list I got quote around the first element in each sublist.</p>\n\n<pre><code>new_list = []\nfor val in df['col1'].values:\n    new_list.append(val)\n</code></pre>\n\n<p>And I got:</p>\n\n<pre><code>[['51.97559', 4.12565]\n['52.97559', 3.12565]\n['49.97559', 5.12565]]\n</code></pre>\n\n<p>But I need:</p>\n\n<pre><code>[[51.97559, 4.12565]\n[52.97559, 3.12565]\n[49.97559, 5.12565]]\n</code></pre>\n",
        "formatted_input": {
            "qid": 50269584,
            "link": "https://stackoverflow.com/questions/50269584/append-list-from-pandas-column-to-python-list",
            "question": {
                "title": "Append list from pandas column to python list",
                "ques_desc": "I have values in a list in pandas column, for example: df But when I append col1 to list I got quote around the first element in each sublist. And I got: But I need: "
            },
            "io": [
                "id       col1\n1       [51.97559, 4.12565]\n2       [52.97559, 3.12565]\n3       [49.97559, 5.12565]\n",
                "[[51.97559, 4.12565]\n[52.97559, 3.12565]\n[49.97559, 5.12565]]\n"
            ],
            "answer": {
                "ans_desc": "You can cast all the elements of the list into ",
                "code": [
                    "new_list = []\nfor val in df['col1'].values:\n    new_list.append(list(map(float, val)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "string",
            "list",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 3653,
            "user_id": 1506850,
            "user_type": "registered",
            "accept_rate": 94,
            "profile_image": "https://www.gravatar.com/avatar/529adde7e6c422b0ab9b990d9b09e8da?s=128&d=identicon&r=PG&f=1",
            "display_name": "00__00__00",
            "link": "https://stackoverflow.com/users/1506850/00-00-00"
        },
        "is_answered": true,
        "view_count": 253,
        "accepted_answer_id": 50152188,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1525342585,
        "creation_date": 1525341400,
        "last_edit_date": 1525341740,
        "question_id": 50152137,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50152137/how-to-extract-a-2d-array-encoded-in-a-list-of-strings-in-a-pandas-dataframe",
        "title": "how to extract a 2D array encoded in a list of strings in a pandas dataframe?",
        "body": "<p>I have messed up a dataframe.\nI have a columns which contain strings which encode a list of numbers </p>\n\n<p>e.g.</p>\n\n<pre><code>df=\n                                    mycol\n0   '[ 0.5497076,   0.59722222,  0.42361111]'  \n1   '[ 0.8030303,   0.69090909,  0.52727273]'  \n2   '[ 0.51461988,  0.38194444,  0.66666667]'\n</code></pre>\n\n<p>EDIT: actually, the commas are missing as well</p>\n\n<pre><code>df=\n                                    mycol\n0   '[ 0.5497076   0.59722222  0.42361111]'  \n1   '[ 0.8030303   0.69090909  0.52727273]'  \n2   '[ 0.51461988  0.38194444  0.66666667]'\n</code></pre>\n\n<p>Each of the strings encodes a list with a fixed number of elements.\nI would like to convert this <code>mycol</code> into 3 (in general N, where <code>N=len(df[mycol][0])</code> <code>columns</code> each of them numeric, containing one element from the original list in mycol</p>\n\n<p>I have tried the following, without success</p>\n\n<pre><code>df[mycol]=df[mycol].apply(lambda s: s.split())\ndf[mycol]=df[mycol].apply(lambda s: np.fromstring(s))\n\ndf[['mycol1','mycol2','mycol3']] = pd.DataFrame(df[mycol].values.tolist(), index= df.index)\n</code></pre>\n",
        "answer_body": "<p>This should help. </p>\n\n<p><strong>Ex:</strong></p>\n\n<pre><code>import pandas as pd\ndf = pd.DataFrame({\"mycol\": ['[ 0.5497076   0.59722222  0.42361111]', '[ 0.8030303   0.69090909  0.52727273]']})\ndf[['mycol1','mycol2','mycol3']]  = df[\"mycol\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").split()).apply(pd.Series)\nprint(df)\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>                                   mycol     mycol1      mycol2      mycol3\n0  [ 0.5497076   0.59722222  0.42361111]  0.5497076  0.59722222  0.42361111\n1  [ 0.8030303   0.69090909  0.52727273]  0.8030303  0.69090909  0.52727273\n</code></pre>\n",
        "question_body": "<p>I have messed up a dataframe.\nI have a columns which contain strings which encode a list of numbers </p>\n\n<p>e.g.</p>\n\n<pre><code>df=\n                                    mycol\n0   '[ 0.5497076,   0.59722222,  0.42361111]'  \n1   '[ 0.8030303,   0.69090909,  0.52727273]'  \n2   '[ 0.51461988,  0.38194444,  0.66666667]'\n</code></pre>\n\n<p>EDIT: actually, the commas are missing as well</p>\n\n<pre><code>df=\n                                    mycol\n0   '[ 0.5497076   0.59722222  0.42361111]'  \n1   '[ 0.8030303   0.69090909  0.52727273]'  \n2   '[ 0.51461988  0.38194444  0.66666667]'\n</code></pre>\n\n<p>Each of the strings encodes a list with a fixed number of elements.\nI would like to convert this <code>mycol</code> into 3 (in general N, where <code>N=len(df[mycol][0])</code> <code>columns</code> each of them numeric, containing one element from the original list in mycol</p>\n\n<p>I have tried the following, without success</p>\n\n<pre><code>df[mycol]=df[mycol].apply(lambda s: s.split())\ndf[mycol]=df[mycol].apply(lambda s: np.fromstring(s))\n\ndf[['mycol1','mycol2','mycol3']] = pd.DataFrame(df[mycol].values.tolist(), index= df.index)\n</code></pre>\n",
        "formatted_input": {
            "qid": 50152137,
            "link": "https://stackoverflow.com/questions/50152137/how-to-extract-a-2d-array-encoded-in-a-list-of-strings-in-a-pandas-dataframe",
            "question": {
                "title": "how to extract a 2D array encoded in a list of strings in a pandas dataframe?",
                "ques_desc": "I have messed up a dataframe. I have a columns which contain strings which encode a list of numbers e.g. EDIT: actually, the commas are missing as well Each of the strings encodes a list with a fixed number of elements. I would like to convert this into 3 (in general N, where each of them numeric, containing one element from the original list in mycol I have tried the following, without success "
            },
            "io": [
                "df=\n                                    mycol\n0   '[ 0.5497076,   0.59722222,  0.42361111]'  \n1   '[ 0.8030303,   0.69090909,  0.52727273]'  \n2   '[ 0.51461988,  0.38194444,  0.66666667]'\n",
                "df=\n                                    mycol\n0   '[ 0.5497076   0.59722222  0.42361111]'  \n1   '[ 0.8030303   0.69090909  0.52727273]'  \n2   '[ 0.51461988  0.38194444  0.66666667]'\n"
            ],
            "answer": {
                "ans_desc": "This should help. Ex: Output: ",
                "code": [
                    "import pandas as pd\ndf = pd.DataFrame({\"mycol\": ['[ 0.5497076   0.59722222  0.42361111]', '[ 0.8030303   0.69090909  0.52727273]']})\ndf[['mycol1','mycol2','mycol3']]  = df[\"mycol\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").split()).apply(pd.Series)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 295,
            "user_id": 1169058,
            "user_type": "registered",
            "accept_rate": 56,
            "profile_image": "https://www.gravatar.com/avatar/d18a86672a2e803af25eabec65bb01f6?s=128&d=identicon&r=PG",
            "display_name": "stavrop",
            "link": "https://stackoverflow.com/users/1169058/stavrop"
        },
        "is_answered": true,
        "view_count": 192,
        "accepted_answer_id": 50150151,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1525339694,
        "creation_date": 1525335174,
        "question_id": 50150090,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50150090/pandas-create-dataframe-from-data-and-column-order",
        "title": "Pandas: Create dataframe from data and column order",
        "body": "<p>what i'm asking must be something very easy, but i honestly can't see it.... :(</p>\n\n<p>I have an array, lets say</p>\n\n<pre><code>data = [[1, 2, 3], \n        [4, 5, 6],\n        [7, 8, 9],\n        [10,11,12]]\n</code></pre>\n\n<p>and i want to put it in a dataframe.<br>\nI do <code>df = pd.Dataframe(data, columns={'col1', 'col2', 'col3'})</code></p>\n\n<p>aiming for:</p>\n\n<pre><code>col1 col2 col3\n1    2    3\n4    5    6\n7    8    9\n10   11   12\n</code></pre>\n\n<p>but i am getting:</p>\n\n<pre><code>col3 col1 col2\n1    2    3\n4    5    6\n7    8    9\n10   11   12\n</code></pre>\n\n<p><strong>(notice the discrepancy between column names and data)</strong></p>\n\n<p>I know i can re-arrange the column names order in the dataframe creation, but i'm trying to understand how it works.</p>\n\n<p>Am i doing something wrong, or it's normal behaviour? (why though?)</p>\n",
        "answer_body": "<p>You are are using a <code>{set}</code> of columns, which is NOT an ordered collection (neither are dictionaries).\nTry with a <code>(tuple)</code>, o simply a <code>[list]</code></p>\n\n<pre><code>df = pd.Dataframe(data, columns=['col1', 'col2', 'col3'])\n</code></pre>\n",
        "question_body": "<p>what i'm asking must be something very easy, but i honestly can't see it.... :(</p>\n\n<p>I have an array, lets say</p>\n\n<pre><code>data = [[1, 2, 3], \n        [4, 5, 6],\n        [7, 8, 9],\n        [10,11,12]]\n</code></pre>\n\n<p>and i want to put it in a dataframe.<br>\nI do <code>df = pd.Dataframe(data, columns={'col1', 'col2', 'col3'})</code></p>\n\n<p>aiming for:</p>\n\n<pre><code>col1 col2 col3\n1    2    3\n4    5    6\n7    8    9\n10   11   12\n</code></pre>\n\n<p>but i am getting:</p>\n\n<pre><code>col3 col1 col2\n1    2    3\n4    5    6\n7    8    9\n10   11   12\n</code></pre>\n\n<p><strong>(notice the discrepancy between column names and data)</strong></p>\n\n<p>I know i can re-arrange the column names order in the dataframe creation, but i'm trying to understand how it works.</p>\n\n<p>Am i doing something wrong, or it's normal behaviour? (why though?)</p>\n",
        "formatted_input": {
            "qid": 50150090,
            "link": "https://stackoverflow.com/questions/50150090/pandas-create-dataframe-from-data-and-column-order",
            "question": {
                "title": "Pandas: Create dataframe from data and column order",
                "ques_desc": "what i'm asking must be something very easy, but i honestly can't see it.... :( I have an array, lets say and i want to put it in a dataframe. I do aiming for: but i am getting: (notice the discrepancy between column names and data) I know i can re-arrange the column names order in the dataframe creation, but i'm trying to understand how it works. Am i doing something wrong, or it's normal behaviour? (why though?) "
            },
            "io": [
                "col1 col2 col3\n1    2    3\n4    5    6\n7    8    9\n10   11   12\n",
                "col3 col1 col2\n1    2    3\n4    5    6\n7    8    9\n10   11   12\n"
            ],
            "answer": {
                "ans_desc": "You are are using a of columns, which is NOT an ordered collection (neither are dictionaries). Try with a , o simply a ",
                "code": [
                    "df = pd.Dataframe(data, columns=['col1', 'col2', 'col3'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "fixed-width"
        ],
        "owner": {
            "reputation": 67,
            "user_id": 4868597,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/i4bRr.jpg?s=128&g=1",
            "display_name": "Javier Bhardt",
            "link": "https://stackoverflow.com/users/4868597/javier-bhardt"
        },
        "is_answered": true,
        "view_count": 1574,
        "accepted_answer_id": 50068278,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1524853629,
        "creation_date": 1524852230,
        "question_id": 50067957,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/50067957/saving-a-pandas-dataframe-in-fixed-format-with-different-column-widths",
        "title": "Saving a Pandas dataframe in fixed format with different column widths",
        "body": "<p>I have a pandas dataframe (df) that looks like this:</p>\n\n<pre><code>   A   B  C\n0  1  10  1234\n1  2  20  0\n</code></pre>\n\n<p>I want to save this dataframe in a fixed format. The fixed format I have in mind has different column width and is as follows: </p>\n\n<p>\"one space for column A's value then a comma then four spaces for column B's values and a comma and then five spaces for column C's values\"</p>\n\n<p>Or symbolically:</p>\n\n<pre><code>-,----,-----\n</code></pre>\n\n<p>My dataframe above (df) would look like the following in my desired fixed format:</p>\n\n<pre><code>1,  10, 1234\n2,  20,    0\n</code></pre>\n\n<p>How can I write a command in Python that saves my dataframe into this format?</p>\n",
        "answer_body": "<pre><code>df['B'] = df['B'].apply(lambda t: (' '*(4-len(str(t)))+str(t)))\ndf['C'] = df['C'].apply(lambda t: (' '*(5-len(str(t)))+str(t)))\ndf.to_csv('path_to_file.csv', index=False)\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe (df) that looks like this:</p>\n\n<pre><code>   A   B  C\n0  1  10  1234\n1  2  20  0\n</code></pre>\n\n<p>I want to save this dataframe in a fixed format. The fixed format I have in mind has different column width and is as follows: </p>\n\n<p>\"one space for column A's value then a comma then four spaces for column B's values and a comma and then five spaces for column C's values\"</p>\n\n<p>Or symbolically:</p>\n\n<pre><code>-,----,-----\n</code></pre>\n\n<p>My dataframe above (df) would look like the following in my desired fixed format:</p>\n\n<pre><code>1,  10, 1234\n2,  20,    0\n</code></pre>\n\n<p>How can I write a command in Python that saves my dataframe into this format?</p>\n",
        "formatted_input": {
            "qid": 50067957,
            "link": "https://stackoverflow.com/questions/50067957/saving-a-pandas-dataframe-in-fixed-format-with-different-column-widths",
            "question": {
                "title": "Saving a Pandas dataframe in fixed format with different column widths",
                "ques_desc": "I have a pandas dataframe (df) that looks like this: I want to save this dataframe in a fixed format. The fixed format I have in mind has different column width and is as follows: \"one space for column A's value then a comma then four spaces for column B's values and a comma and then five spaces for column C's values\" Or symbolically: My dataframe above (df) would look like the following in my desired fixed format: How can I write a command in Python that saves my dataframe into this format? "
            },
            "io": [
                "   A   B  C\n0  1  10  1234\n1  2  20  0\n",
                "1,  10, 1234\n2,  20,    0\n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "df['B'] = df['B'].apply(lambda t: (' '*(4-len(str(t)))+str(t)))\ndf['C'] = df['C'].apply(lambda t: (' '*(5-len(str(t)))+str(t)))\ndf.to_csv('path_to_file.csv', index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 9691640,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/bb11e069847cf93a44cfb389b5a7cdd7?s=128&d=identicon&r=PG&f=1",
            "display_name": "Fupp2",
            "link": "https://stackoverflow.com/users/9691640/fupp2"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 50003960,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1524579340,
        "creation_date": 1524578866,
        "question_id": 50003791,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/50003791/removing-random-rows-from-a-data-frame-until-count-is-equal-some-criteria",
        "title": "Removing random rows from a data frame until count is equal some criteria",
        "body": "<p>I have a dataframe with data that I feed to a ML library in python. The data I have is categorized into 5 different tasks, t1,t2,t3,t4,t5. The data I have right now for every task is uneven, to simplify things here is an example.</p>\n\n<pre><code>task, someValue\nt1,   XXX\nt1,   XXX\nt1,   XXX\nt1,   XXX\nt2,   XXX\nt2,   XXX\n</code></pre>\n\n<p>In the case above, I want to remove random rows with the task label of \"t1\" until there is an equal amount of \"t1\" as there is \"t2\"\nSo after the code is run, it should look like this:</p>\n\n<pre><code>task, someValue\nt1,   XXX\nt1,   XXX\nt2,   XXX\nt2,   XXX\n</code></pre>\n\n<p>What is the most clean way to do this? I could of course just do for loops and if conditions and use random numbers and count the occurances for each iteration, but that solution would not be very elegant. Surely there must be a way using functions of dataframe? So far, this is what I got:</p>\n\n<pre><code>def equalize_rows(df):\n    t = df['task'].value_counts()\n    mininmum_occurance = min(t)\n</code></pre>\n",
        "answer_body": "<p>You can calculate the smallest number of tasks in your dataFrame, and then use <code>groupby</code> + <code>head</code> to get the top N rows per task.</p>\n\n<pre><code>v = df['task'].value_counts().min()\ndf = df.groupby('task', as_index=False).head(v)\n</code></pre>\n\n<p></p>\n\n<pre><code>df\n  task someValue\n0   t1       XXX\n1   t1       XXX\n4   t2       XXX\n5   t2       XXX\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with data that I feed to a ML library in python. The data I have is categorized into 5 different tasks, t1,t2,t3,t4,t5. The data I have right now for every task is uneven, to simplify things here is an example.</p>\n\n<pre><code>task, someValue\nt1,   XXX\nt1,   XXX\nt1,   XXX\nt1,   XXX\nt2,   XXX\nt2,   XXX\n</code></pre>\n\n<p>In the case above, I want to remove random rows with the task label of \"t1\" until there is an equal amount of \"t1\" as there is \"t2\"\nSo after the code is run, it should look like this:</p>\n\n<pre><code>task, someValue\nt1,   XXX\nt1,   XXX\nt2,   XXX\nt2,   XXX\n</code></pre>\n\n<p>What is the most clean way to do this? I could of course just do for loops and if conditions and use random numbers and count the occurances for each iteration, but that solution would not be very elegant. Surely there must be a way using functions of dataframe? So far, this is what I got:</p>\n\n<pre><code>def equalize_rows(df):\n    t = df['task'].value_counts()\n    mininmum_occurance = min(t)\n</code></pre>\n",
        "formatted_input": {
            "qid": 50003791,
            "link": "https://stackoverflow.com/questions/50003791/removing-random-rows-from-a-data-frame-until-count-is-equal-some-criteria",
            "question": {
                "title": "Removing random rows from a data frame until count is equal some criteria",
                "ques_desc": "I have a dataframe with data that I feed to a ML library in python. The data I have is categorized into 5 different tasks, t1,t2,t3,t4,t5. The data I have right now for every task is uneven, to simplify things here is an example. In the case above, I want to remove random rows with the task label of \"t1\" until there is an equal amount of \"t1\" as there is \"t2\" So after the code is run, it should look like this: What is the most clean way to do this? I could of course just do for loops and if conditions and use random numbers and count the occurances for each iteration, but that solution would not be very elegant. Surely there must be a way using functions of dataframe? So far, this is what I got: "
            },
            "io": [
                "task, someValue\nt1,   XXX\nt1,   XXX\nt1,   XXX\nt1,   XXX\nt2,   XXX\nt2,   XXX\n",
                "task, someValue\nt1,   XXX\nt1,   XXX\nt2,   XXX\nt2,   XXX\n"
            ],
            "answer": {
                "ans_desc": "You can calculate the smallest number of tasks in your dataFrame, and then use + to get the top N rows per task. ",
                "code": [
                    "v = df['task'].value_counts().min()\ndf = df.groupby('task', as_index=False).head(v)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 67,
            "user_id": 8863566,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5e5837715055974f58019b8d6fad7141?s=128&d=identicon&r=PG&f=1",
            "display_name": "lucky_but_stupid",
            "link": "https://stackoverflow.com/users/8863566/lucky-but-stupid"
        },
        "is_answered": true,
        "view_count": 250,
        "accepted_answer_id": 49927450,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1524170069,
        "creation_date": 1524154975,
        "last_edit_date": 1524162981,
        "question_id": 49925888,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49925888/pandas-get-the-least-number-of-records-so-all-columns-have-at-least-one-non-nu",
        "title": "Pandas : Get the least number of records so all columns have at least one non null value",
        "body": "<p>I have a dataframe with 62 columns that are largely null. Some records have multiple columns with non-null values, others just a single non-null. I'm wondering if there's a way to use .dropna or other strategy <strong>to return the least number of rows with each column having at least one non-null value.</strong></p>\n\n<p>For a simplified example</p>\n\n<pre><code>       a          b         c\n      NaN        1         NaN\n      1          NaN       NaN\n      NaN        NaN       NaN\n      NaN        1         1\n</code></pre>\n\n<p>Would return </p>\n\n<pre><code>      a          b         c\n      1          NaN       NaN\n      NaN        1         1\n</code></pre>\n\n<p>...</p>\n",
        "answer_body": "<p>Here's a simple greedy solution that should do the job but won't guarantee you have the lowest number of rows (as @chthonicdaemon said the problem is NP-hard)</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\n# sample dataframe\ndf = pd.DataFrame({'a':[np.nan,1,np.nan,np.nan],'b':[1, np.nan, np.nan, 1],'c':[np.nan, np.nan, np.nan, 1]})\ndf_orig = df\ncols = df.columns.tolist()\n\nrows = []\nwhile not df.empty:\n    ## Find the row with most non-null column entries\n    x = df.notnull().sum(axis=1).idxmax() # edit - fix for null/nonnull\n    ## Add the row to our list and continue\n    rows.append(x)\n    ## Remove the columns from our dataframe\n    df = df.drop(columns=df.columns[df.loc[[x]].notnull().any()].tolist())\n\n## Access the dataframe with only 'essential' rows\ndf_orig.loc[rows]\n</code></pre>\n\n<p>Out:</p>\n\n<pre><code>    a   b   c\n3   NaN 1.0 1.0\n1   1.0 NaN NaN\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with 62 columns that are largely null. Some records have multiple columns with non-null values, others just a single non-null. I'm wondering if there's a way to use .dropna or other strategy <strong>to return the least number of rows with each column having at least one non-null value.</strong></p>\n\n<p>For a simplified example</p>\n\n<pre><code>       a          b         c\n      NaN        1         NaN\n      1          NaN       NaN\n      NaN        NaN       NaN\n      NaN        1         1\n</code></pre>\n\n<p>Would return </p>\n\n<pre><code>      a          b         c\n      1          NaN       NaN\n      NaN        1         1\n</code></pre>\n\n<p>...</p>\n",
        "formatted_input": {
            "qid": 49925888,
            "link": "https://stackoverflow.com/questions/49925888/pandas-get-the-least-number-of-records-so-all-columns-have-at-least-one-non-nu",
            "question": {
                "title": "Pandas : Get the least number of records so all columns have at least one non null value",
                "ques_desc": "I have a dataframe with 62 columns that are largely null. Some records have multiple columns with non-null values, others just a single non-null. I'm wondering if there's a way to use .dropna or other strategy to return the least number of rows with each column having at least one non-null value. For a simplified example Would return ... "
            },
            "io": [
                "       a          b         c\n      NaN        1         NaN\n      1          NaN       NaN\n      NaN        NaN       NaN\n      NaN        1         1\n",
                "      a          b         c\n      1          NaN       NaN\n      NaN        1         1\n"
            ],
            "answer": {
                "ans_desc": "Here's a simple greedy solution that should do the job but won't guarantee you have the lowest number of rows (as @chthonicdaemon said the problem is NP-hard) Out: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\n# sample dataframe\ndf = pd.DataFrame({'a':[np.nan,1,np.nan,np.nan],'b':[1, np.nan, np.nan, 1],'c':[np.nan, np.nan, np.nan, 1]})\ndf_orig = df\ncols = df.columns.tolist()\n\nrows = []\nwhile not df.empty:\n    ## Find the row with most non-null column entries\n    x = df.notnull().sum(axis=1).idxmax() # edit - fix for null/nonnull\n    ## Add the row to our list and continue\n    rows.append(x)\n    ## Remove the columns from our dataframe\n    df = df.drop(columns=df.columns[df.loc[[x]].notnull().any()].tolist())\n\n## Access the dataframe with only 'essential' rows\ndf_orig.loc[rows]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "list",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 163,
            "user_id": 8620084,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3a30953c36975de0f65487e88dff378b?s=128&d=identicon&r=PG&f=1",
            "display_name": "guru",
            "link": "https://stackoverflow.com/users/8620084/guru"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 49885318,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1523993136,
        "creation_date": 1523989646,
        "last_edit_date": 1523993136,
        "question_id": 49885060,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49885060/cross-reference-list-of-ids-to-index",
        "title": "Cross reference list of ids to index",
        "body": "<p>I have grouped together a list of ids that are associated with a certain value and placed all these lists of ids into a dataframe. It looks like this: (with index = id)</p>\n\n<pre><code>    phase  list_ids\nid  \na1  1      [a1,a2,c3] \na2  3      [a1,b2,c3]  \nb1  3      [a2,b2] \nb2  2      [b1,b2,c1] \nb3  3      [b2,c1] \nc1  1      [a1,a2,c3] \nc2  1      [a1,b1,c4] \nc3  2      [c1,c2,c4] \nc4  1      [c1,c2]\n</code></pre>\n\n<p>I want to iterate through these lists and cross reference them to the id index where phase equals either a 2 or 3, then just keep the ids that match within the original list (or if not possible, create a new column with modified lists). Something like this below:</p>\n\n<pre><code>    phase  list_ids\nid  \na1  1      [a2,c3] #ids whose phase != 2|3 not kept in list\na2  3      [b2,c3]  \nb1  3      [a2,b2] \nb2  2      [b1,b2] \nb3  3      [b2] \nc1  1      [a2,c3] \nc2  1      [b1] \nc3  2      [] \nc4  1      []\n</code></pre>\n\n<p>If possible I'd like to do this within the dataframe object as there are multiple features/dependencies for each row. Any tips on how to go about this?</p>\n\n<p>My actual data:</p>\n\n<pre><code>               phase  ids\nStudy_id             \nACP-103-006    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... \nACP-103-008    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10...  \nACP-103-010    2.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  \nACP-103-012    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  \nACP-103-014    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...   \n</code></pre>\n\n<p>And the dtypes:</p>\n\n<pre><code>phase float64 \nids object\ndtype: object \n</code></pre>\n\n<p>And the good_ids output:</p>\n\n<pre><code>print(good_ids)\n{'CLS1001-301', 'EFC13799', 'AG120-C-009', 'IRBES_R_04320', 'LTS11298', 'CLS1003-302', '13621', 'TMC-ORI-10-01', '11935', 'C_8428', 'ACP-103-008', 'SFY13476', 'MNTX 301EXT', '14-OBE001-016', '812P310', 'V01-126A-201', 'VX06-770-101', 'EFC11603', ...}\n</code></pre>\n",
        "answer_body": "<p>Assuming that each element in the column <code>list_ids</code> is a list of strings, you could do the following:</p>\n\n<p>First get a <code>set</code> of the \"good\" <code>ids</code> (where phase is 2 or 3):</p>\n\n<pre><code>good_ids = set(df[df[\"phase\"].isin([2,3])].index)\nprint(good_ids)\n#{'a2', 'b1', 'b2', 'b3', 'c3'}\n</code></pre>\n\n<p>Next use <code>apply</code> to filter the <code>list_ids</code> using <code>good_ids</code>:</p>\n\n<pre><code>df[\"list_ids\"] = df[\"list_ids\"].apply(lambda x: [val for val in x if val in good_ids])\nprint(df)\n#    phase  list_ids\n#id                 \n#a1      1  [a2, c3]\n#a2      3  [b2, c3]\n#b1      3  [a2, b2]\n#b2      2  [b1, b2]\n#b3      3      [b2]\n#c1      1  [a2, c3]\n#c2      1      [b1]\n#c3      2        []\n#c4      1        []\n</code></pre>\n",
        "question_body": "<p>I have grouped together a list of ids that are associated with a certain value and placed all these lists of ids into a dataframe. It looks like this: (with index = id)</p>\n\n<pre><code>    phase  list_ids\nid  \na1  1      [a1,a2,c3] \na2  3      [a1,b2,c3]  \nb1  3      [a2,b2] \nb2  2      [b1,b2,c1] \nb3  3      [b2,c1] \nc1  1      [a1,a2,c3] \nc2  1      [a1,b1,c4] \nc3  2      [c1,c2,c4] \nc4  1      [c1,c2]\n</code></pre>\n\n<p>I want to iterate through these lists and cross reference them to the id index where phase equals either a 2 or 3, then just keep the ids that match within the original list (or if not possible, create a new column with modified lists). Something like this below:</p>\n\n<pre><code>    phase  list_ids\nid  \na1  1      [a2,c3] #ids whose phase != 2|3 not kept in list\na2  3      [b2,c3]  \nb1  3      [a2,b2] \nb2  2      [b1,b2] \nb3  3      [b2] \nc1  1      [a2,c3] \nc2  1      [b1] \nc3  2      [] \nc4  1      []\n</code></pre>\n\n<p>If possible I'd like to do this within the dataframe object as there are multiple features/dependencies for each row. Any tips on how to go about this?</p>\n\n<p>My actual data:</p>\n\n<pre><code>               phase  ids\nStudy_id             \nACP-103-006    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... \nACP-103-008    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10...  \nACP-103-010    2.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  \nACP-103-012    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  \nACP-103-014    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...   \n</code></pre>\n\n<p>And the dtypes:</p>\n\n<pre><code>phase float64 \nids object\ndtype: object \n</code></pre>\n\n<p>And the good_ids output:</p>\n\n<pre><code>print(good_ids)\n{'CLS1001-301', 'EFC13799', 'AG120-C-009', 'IRBES_R_04320', 'LTS11298', 'CLS1003-302', '13621', 'TMC-ORI-10-01', '11935', 'C_8428', 'ACP-103-008', 'SFY13476', 'MNTX 301EXT', '14-OBE001-016', '812P310', 'V01-126A-201', 'VX06-770-101', 'EFC11603', ...}\n</code></pre>\n",
        "formatted_input": {
            "qid": 49885060,
            "link": "https://stackoverflow.com/questions/49885060/cross-reference-list-of-ids-to-index",
            "question": {
                "title": "Cross reference list of ids to index",
                "ques_desc": "I have grouped together a list of ids that are associated with a certain value and placed all these lists of ids into a dataframe. It looks like this: (with index = id) I want to iterate through these lists and cross reference them to the id index where phase equals either a 2 or 3, then just keep the ids that match within the original list (or if not possible, create a new column with modified lists). Something like this below: If possible I'd like to do this within the dataframe object as there are multiple features/dependencies for each row. Any tips on how to go about this? My actual data: And the dtypes: And the good_ids output: "
            },
            "io": [
                "    phase  list_ids\nid  \na1  1      [a1,a2,c3] \na2  3      [a1,b2,c3]  \nb1  3      [a2,b2] \nb2  2      [b1,b2,c1] \nb3  3      [b2,c1] \nc1  1      [a1,a2,c3] \nc2  1      [a1,b1,c4] \nc3  2      [c1,c2,c4] \nc4  1      [c1,c2]\n",
                "               phase  ids\nStudy_id             \nACP-103-006    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... \nACP-103-008    2.0   [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10...  \nACP-103-010    2.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  \nACP-103-012    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...  \nACP-103-014    3.0   [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10...   \n"
            ],
            "answer": {
                "ans_desc": "Assuming that each element in the column is a list of strings, you could do the following: First get a of the \"good\" (where phase is 2 or 3): Next use to filter the using : ",
                "code": [
                    "good_ids = set(df[df[\"phase\"].isin([2,3])].index)\nprint(good_ids)\n#{'a2', 'b1', 'b2', 'b3', 'c3'}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 9305276,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/4454f9382b18f9bda7e3d51133f606b4?s=128&d=identicon&r=PG&f=1",
            "display_name": "Cullen DuYaw",
            "link": "https://stackoverflow.com/users/9305276/cullen-duyaw"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 49073607,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1523983871,
        "creation_date": 1520008927,
        "last_edit_date": 1523983871,
        "question_id": 49073547,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49073547/sum-values-in-third-column-while-putting-together-corespondinng-values-in-first",
        "title": "Sum values in third column while putting together corespondinng values in first and second columns",
        "body": "<p>I have 3 columns of data.  I have data stored in three columns (k, v, t) in csv. For instance, </p>\n\n<p>Data:</p>\n\n<pre><code>k v t    \n\na 1 2    \nb 2 3    \nc 3 4    \na 2 3    \nb 3 2    \nb 3 4    \nc 3 5    \nb 2 3\n</code></pre>\n\n<p>I want to get as the following data. Basically, sum all the values of t that has the same k and v. </p>\n\n<pre><code>a 1 5\nb 2 6\nb 3 6\nc 3 9\n</code></pre>\n\n<p>this is the code I have so far:</p>\n\n<pre><code>aList = []\naList2 = []\naList3 = []\n\nfor i in range(len(data)):\n    if data['k'][i] == 'a':\n        if data['v'][i] == 1:\n            aList.append(data['t'][i])\n        elif data['v'][i] == 2:\n            aList2.append(data['t'][i])\n        else:\n            aList3.append(data['t'][i])\n</code></pre>\n\n<p>and it keeps going until the end.</p>\n\n<p>I use \"for loop\" and \"if\" but it is too long. Can I use numpy in a short and clean way? or any other better way?</p>\n",
        "answer_body": "<p>Here is one solution using <code>pandas</code>.</p>\n\n<p>First create a dataframe, then perform a <code>groupby</code> operation. The below code assumes your data is stored in a csv file.</p>\n\n<pre><code>df = pd.read_csv('file.csv')\n\ng = df.groupby(['k', 'v'], as_index=False)['t'].sum()\n</code></pre>\n\n<p><strong>Result</strong></p>\n\n<pre><code>   k  v  t\n0  a  1  2\n1  a  2  3\n2  b  2  6\n3  b  3  6\n4  c  3  9\n</code></pre>\n",
        "question_body": "<p>I have 3 columns of data.  I have data stored in three columns (k, v, t) in csv. For instance, </p>\n\n<p>Data:</p>\n\n<pre><code>k v t    \n\na 1 2    \nb 2 3    \nc 3 4    \na 2 3    \nb 3 2    \nb 3 4    \nc 3 5    \nb 2 3\n</code></pre>\n\n<p>I want to get as the following data. Basically, sum all the values of t that has the same k and v. </p>\n\n<pre><code>a 1 5\nb 2 6\nb 3 6\nc 3 9\n</code></pre>\n\n<p>this is the code I have so far:</p>\n\n<pre><code>aList = []\naList2 = []\naList3 = []\n\nfor i in range(len(data)):\n    if data['k'][i] == 'a':\n        if data['v'][i] == 1:\n            aList.append(data['t'][i])\n        elif data['v'][i] == 2:\n            aList2.append(data['t'][i])\n        else:\n            aList3.append(data['t'][i])\n</code></pre>\n\n<p>and it keeps going until the end.</p>\n\n<p>I use \"for loop\" and \"if\" but it is too long. Can I use numpy in a short and clean way? or any other better way?</p>\n",
        "formatted_input": {
            "qid": 49073547,
            "link": "https://stackoverflow.com/questions/49073547/sum-values-in-third-column-while-putting-together-corespondinng-values-in-first",
            "question": {
                "title": "Sum values in third column while putting together corespondinng values in first and second columns",
                "ques_desc": "I have 3 columns of data. I have data stored in three columns (k, v, t) in csv. For instance, Data: I want to get as the following data. Basically, sum all the values of t that has the same k and v. this is the code I have so far: and it keeps going until the end. I use \"for loop\" and \"if\" but it is too long. Can I use numpy in a short and clean way? or any other better way? "
            },
            "io": [
                "k v t    \n\na 1 2    \nb 2 3    \nc 3 4    \na 2 3    \nb 3 2    \nb 3 4    \nc 3 5    \nb 2 3\n",
                "a 1 5\nb 2 6\nb 3 6\nc 3 9\n"
            ],
            "answer": {
                "ans_desc": "Here is one solution using . First create a dataframe, then perform a operation. The below code assumes your data is stored in a csv file. Result ",
                "code": [
                    "df = pd.read_csv('file.csv')\n\ng = df.groupby(['k', 'v'], as_index=False)['t'].sum()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "time-series"
        ],
        "owner": {
            "reputation": 187,
            "user_id": 9408795,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://i.stack.imgur.com/fUTB8.jpg?s=128&g=1",
            "display_name": "Chi",
            "link": "https://stackoverflow.com/users/9408795/chi"
        },
        "is_answered": true,
        "view_count": 2364,
        "accepted_answer_id": 49352987,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1523983530,
        "creation_date": 1521403161,
        "last_edit_date": 1523983530,
        "question_id": 49352279,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49352279/calculating-the-duration-an-event-in-a-time-series-python",
        "title": "Calculating the duration an event in a time series python",
        "body": "<p>I have a dataframe as show below:</p>\n\n<pre><code>index                value\n2003-01-01 00:00:00  14.5\n2003-01-01 01:00:00  15.8\n2003-01-01 02:00:00     0\n2003-01-01 03:00:00     0\n2003-01-01 04:00:00  13.6\n2003-01-01 05:00:00   4.3\n2003-01-01 06:00:00  13.7\n2003-01-01 07:00:00  14.4\n2003-01-01 08:00:00     0\n2003-01-01 09:00:00     0\n2003-01-01 10:00:00     0\n2003-01-01 11:00:00  17.2\n2003-01-01 12:00:00     0\n2003-01-01 13:00:00   5.3\n2003-01-01 14:00:00     0\n2003-01-01 15:00:00   2.0\n2003-01-01 16:00:00   4.0\n2003-01-01 17:00:00     0\n2003-01-01 18:00:00     0\n2003-01-01 19:00:00   3.9\n2003-01-01 20:00:00   7.2\n2003-01-01 21:00:00   1.0\n2003-01-01 22:00:00   1.0\n2003-01-01 23:00:00  10.0\n</code></pre>\n\n<p>The index is datetime and have column record the rainfall value(unit:mm) in each hour,I would like to calculate the \"Average wet spell duration\", which means the\naverage of continuous hours that exist values (not zero) in a day, so the calculation is </p>\n\n<pre><code>2 + 4 + 1 + 1 + 2 + 5 / 6 (events) = 2.5 (hr)\n</code></pre>\n\n<p>and the \"average wet spell amount\", which means the average of sum of the values in continuous hours in a day.</p>\n\n<pre><code>{ (14.5 + 15.8) + ( 13.6 + 4.3 + 13.7 + 14.4 ) + (17.2) + (5.3) + (2 + 4)+ (3.9 + 7.2 + 1 + 1 + 10) } /  6 (events) = 21.32 (mm)\n</code></pre>\n\n<p>The datafame above is just a example, the dataframe which I have have more longer time series (more than one year for example), how can I write a function so it could calculate the two value mentioned above in a better way? thanks in advance!</p>\n\n<p>P.S. the values may be NaN, and I would like to just ignore it.</p>\n",
        "answer_body": "<p>I believe this is what you are looking for. I have added explanations to the code for each step.</p>\n\n<pre><code># create helper columns defining contiguous blocks and day\ndf['block'] = (df['value'].astype(bool).shift() != df['value'].astype(bool)).cumsum()\ndf['day'] = df['index'].dt.normalize()\n\n# group by day to get unique block count and value count\nsession_map = df[df['value'].astype(bool)].groupby('day')['block'].nunique()\nhour_map = df[df['value'].astype(bool)].groupby('day')['value'].count()\n\n# map to original dataframe\ndf['sessions'] = df['day'].map(session_map)\ndf['hours'] = df['day'].map(hour_map)\n\n# calculate result\nres = df.groupby(['day', 'hours', 'sessions'], as_index=False)['value'].sum()\nres['duration'] = res['hours'] / res['sessions']\nres['amount'] = res['value'] / res['sessions']\n</code></pre>\n\n<p><strong>Result</strong></p>\n\n<pre><code>         day  sessions  duration  value     amount\n0 2003-01-01         6       2.5  127.9  21.316667\n</code></pre>\n",
        "question_body": "<p>I have a dataframe as show below:</p>\n\n<pre><code>index                value\n2003-01-01 00:00:00  14.5\n2003-01-01 01:00:00  15.8\n2003-01-01 02:00:00     0\n2003-01-01 03:00:00     0\n2003-01-01 04:00:00  13.6\n2003-01-01 05:00:00   4.3\n2003-01-01 06:00:00  13.7\n2003-01-01 07:00:00  14.4\n2003-01-01 08:00:00     0\n2003-01-01 09:00:00     0\n2003-01-01 10:00:00     0\n2003-01-01 11:00:00  17.2\n2003-01-01 12:00:00     0\n2003-01-01 13:00:00   5.3\n2003-01-01 14:00:00     0\n2003-01-01 15:00:00   2.0\n2003-01-01 16:00:00   4.0\n2003-01-01 17:00:00     0\n2003-01-01 18:00:00     0\n2003-01-01 19:00:00   3.9\n2003-01-01 20:00:00   7.2\n2003-01-01 21:00:00   1.0\n2003-01-01 22:00:00   1.0\n2003-01-01 23:00:00  10.0\n</code></pre>\n\n<p>The index is datetime and have column record the rainfall value(unit:mm) in each hour,I would like to calculate the \"Average wet spell duration\", which means the\naverage of continuous hours that exist values (not zero) in a day, so the calculation is </p>\n\n<pre><code>2 + 4 + 1 + 1 + 2 + 5 / 6 (events) = 2.5 (hr)\n</code></pre>\n\n<p>and the \"average wet spell amount\", which means the average of sum of the values in continuous hours in a day.</p>\n\n<pre><code>{ (14.5 + 15.8) + ( 13.6 + 4.3 + 13.7 + 14.4 ) + (17.2) + (5.3) + (2 + 4)+ (3.9 + 7.2 + 1 + 1 + 10) } /  6 (events) = 21.32 (mm)\n</code></pre>\n\n<p>The datafame above is just a example, the dataframe which I have have more longer time series (more than one year for example), how can I write a function so it could calculate the two value mentioned above in a better way? thanks in advance!</p>\n\n<p>P.S. the values may be NaN, and I would like to just ignore it.</p>\n",
        "formatted_input": {
            "qid": 49352279,
            "link": "https://stackoverflow.com/questions/49352279/calculating-the-duration-an-event-in-a-time-series-python",
            "question": {
                "title": "Calculating the duration an event in a time series python",
                "ques_desc": "I have a dataframe as show below: The index is datetime and have column record the rainfall value(unit:mm) in each hour,I would like to calculate the \"Average wet spell duration\", which means the average of continuous hours that exist values (not zero) in a day, so the calculation is and the \"average wet spell amount\", which means the average of sum of the values in continuous hours in a day. The datafame above is just a example, the dataframe which I have have more longer time series (more than one year for example), how can I write a function so it could calculate the two value mentioned above in a better way? thanks in advance! P.S. the values may be NaN, and I would like to just ignore it. "
            },
            "io": [
                "2 + 4 + 1 + 1 + 2 + 5 / 6 (events) = 2.5 (hr)\n",
                "{ (14.5 + 15.8) + ( 13.6 + 4.3 + 13.7 + 14.4 ) + (17.2) + (5.3) + (2 + 4)+ (3.9 + 7.2 + 1 + 1 + 10) } /  6 (events) = 21.32 (mm)\n"
            ],
            "answer": {
                "ans_desc": "I believe this is what you are looking for. I have added explanations to the code for each step. Result ",
                "code": [
                    "# create helper columns defining contiguous blocks and day\ndf['block'] = (df['value'].astype(bool).shift() != df['value'].astype(bool)).cumsum()\ndf['day'] = df['index'].dt.normalize()\n\n# group by day to get unique block count and value count\nsession_map = df[df['value'].astype(bool)].groupby('day')['block'].nunique()\nhour_map = df[df['value'].astype(bool)].groupby('day')['value'].count()\n\n# map to original dataframe\ndf['sessions'] = df['day'].map(session_map)\ndf['hours'] = df['day'].map(hour_map)\n\n# calculate result\nres = df.groupby(['day', 'hours', 'sessions'], as_index=False)['value'].sum()\nres['duration'] = res['hours'] / res['sessions']\nres['amount'] = res['value'] / res['sessions']\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "replace",
            "conditional-statements"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 6459417,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/0u15u.jpg?s=128&g=1",
            "display_name": "AndrewDAG",
            "link": "https://stackoverflow.com/users/6459417/andrewdag"
        },
        "is_answered": true,
        "view_count": 6819,
        "accepted_answer_id": 49857505,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1523882436,
        "creation_date": 1523881968,
        "question_id": 49857470,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49857470/replace-value-in-pandas-dataframe-based-on-condition",
        "title": "Replace value in Pandas Dataframe based on condition",
        "body": "<p>I have a dataframe column with some numeric values. I want that these values get replaced by 1 and 0 based on a given condition. The condition is that if the value is above the mean of the column, then change the numeric value to 1, else set it to 0.</p>\n\n<p>Here is the code I have now:</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndataset = pd.read_csv('data.csv')\ndataset = dataset.dropna(axis=0, how='any')\n\nX = dataset.drop(['myCol'], axis=1)\ny = dataset.iloc[:, 4:5].values\n\nmean_y = np.mean(dataset.myCol)\n</code></pre>\n\n<p>The target is the dataframe y. y is like so:</p>\n\n<pre><code>      0\n0    16\n1    13\n2    12.5\n3    12\n</code></pre>\n\n<p>and so on. mean_y is equal to 3.55. \nTherefore, I need that all values greater than 3.55 to become ones, and the rest 0.</p>\n\n<p>I applied this loop, but without success:</p>\n\n<pre><code>for i in dataset.myCol:\n    if dataset.myCol[i] &gt; mean_y:\n        dataset.myCol[i] = 1\n    else:\n        dataset.myCol[i] = 0\n</code></pre>\n\n<p>The output is the following:</p>\n\n<pre><code>      0\n0    16\n1    13\n2    0\n3    12\n</code></pre>\n\n<p>What am I doing wrong? Can someone please explain me the mistake?</p>\n\n<p>Thank you!</p>\n",
        "answer_body": "<p>Try this vectorized approach:</p>\n\n<pre><code>dataset.myCol = np.where(dataset.myCol &gt; dataset.myCol.mean(), 1, 0)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe column with some numeric values. I want that these values get replaced by 1 and 0 based on a given condition. The condition is that if the value is above the mean of the column, then change the numeric value to 1, else set it to 0.</p>\n\n<p>Here is the code I have now:</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndataset = pd.read_csv('data.csv')\ndataset = dataset.dropna(axis=0, how='any')\n\nX = dataset.drop(['myCol'], axis=1)\ny = dataset.iloc[:, 4:5].values\n\nmean_y = np.mean(dataset.myCol)\n</code></pre>\n\n<p>The target is the dataframe y. y is like so:</p>\n\n<pre><code>      0\n0    16\n1    13\n2    12.5\n3    12\n</code></pre>\n\n<p>and so on. mean_y is equal to 3.55. \nTherefore, I need that all values greater than 3.55 to become ones, and the rest 0.</p>\n\n<p>I applied this loop, but without success:</p>\n\n<pre><code>for i in dataset.myCol:\n    if dataset.myCol[i] &gt; mean_y:\n        dataset.myCol[i] = 1\n    else:\n        dataset.myCol[i] = 0\n</code></pre>\n\n<p>The output is the following:</p>\n\n<pre><code>      0\n0    16\n1    13\n2    0\n3    12\n</code></pre>\n\n<p>What am I doing wrong? Can someone please explain me the mistake?</p>\n\n<p>Thank you!</p>\n",
        "formatted_input": {
            "qid": 49857470,
            "link": "https://stackoverflow.com/questions/49857470/replace-value-in-pandas-dataframe-based-on-condition",
            "question": {
                "title": "Replace value in Pandas Dataframe based on condition",
                "ques_desc": "I have a dataframe column with some numeric values. I want that these values get replaced by 1 and 0 based on a given condition. The condition is that if the value is above the mean of the column, then change the numeric value to 1, else set it to 0. Here is the code I have now: The target is the dataframe y. y is like so: and so on. mean_y is equal to 3.55. Therefore, I need that all values greater than 3.55 to become ones, and the rest 0. I applied this loop, but without success: The output is the following: What am I doing wrong? Can someone please explain me the mistake? Thank you! "
            },
            "io": [
                "      0\n0    16\n1    13\n2    12.5\n3    12\n",
                "      0\n0    16\n1    13\n2    0\n3    12\n"
            ],
            "answer": {
                "ans_desc": "Try this vectorized approach: ",
                "code": [
                    "dataset.myCol = np.where(dataset.myCol > dataset.myCol.mean(), 1, 0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 264370,
            "user_id": 7964527,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://i.stack.imgur.com/HPW8R.jpg?s=128&g=1",
            "display_name": "BENY",
            "link": "https://stackoverflow.com/users/7964527/beny"
        },
        "is_answered": true,
        "view_count": 1478,
        "accepted_answer_id": 45970911,
        "answer_count": 2,
        "score": 6,
        "last_activity_date": 1523535101,
        "creation_date": 1504132897,
        "last_edit_date": 1523535101,
        "question_id": 45970751,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/45970751/shift-nans-to-the-end-of-their-respective-rows",
        "title": "Shift NaNs to the end of their respective rows",
        "body": "<p>I have a DataFrame like :</p>\n\n<pre><code>     0    1    2\n0  0.0  1.0  2.0\n1  NaN  1.0  2.0\n2  NaN  NaN  2.0\n</code></pre>\n\n<hr>\n\n<p>What I want to get is </p>\n\n<pre><code>Out[116]: \n     0    1    2\n0  0.0  1.0  2.0\n1  1.0  2.0  NaN\n2  2.0  NaN  NaN\n</code></pre>\n\n<hr>\n\n<p>This is my approach as of now.</p>\n\n<pre><code>df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\nOut[117]: \n     0    1    2\n0  0.0  1.0  2.0\n1  1.0  2.0  NaN\n2  2.0  NaN  NaN\n</code></pre>\n\n<hr>\n\n<p>Is there any efficient way to achieve this ? <code>apply</code> Here is way to slow .\nThank you for your assistant!:) </p>\n\n<hr>\n\n<p>My real data size</p>\n\n<pre><code>df.shape\nOut[117]: (54812040, 1522)\n</code></pre>\n",
        "answer_body": "<p>Here's a NumPy solution using <a href=\"https://stackoverflow.com/a/44559180/\"><code>justify</code></a> -</p>\n\n<pre><code>In [455]: df\nOut[455]: \n     0    1    2\n0  0.0  1.0  2.0\n1  NaN  1.0  2.0\n2  NaN  NaN  2.0\n\nIn [456]: pd.DataFrame(justify(df.values, invalid_val=np.nan, axis=1, side='left'))\nOut[456]: \n     0    1    2\n0  0.0  1.0  2.0\n1  1.0  2.0  NaN\n2  2.0  NaN  NaN\n</code></pre>\n\n<p>If you want to save memory, assign it back instead -</p>\n\n<pre><code>df[:] = justify(df.values, invalid_val=np.nan, axis=1, side='left')\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame like :</p>\n\n<pre><code>     0    1    2\n0  0.0  1.0  2.0\n1  NaN  1.0  2.0\n2  NaN  NaN  2.0\n</code></pre>\n\n<hr>\n\n<p>What I want to get is </p>\n\n<pre><code>Out[116]: \n     0    1    2\n0  0.0  1.0  2.0\n1  1.0  2.0  NaN\n2  2.0  NaN  NaN\n</code></pre>\n\n<hr>\n\n<p>This is my approach as of now.</p>\n\n<pre><code>df.apply(lambda x : (x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),1)\nOut[117]: \n     0    1    2\n0  0.0  1.0  2.0\n1  1.0  2.0  NaN\n2  2.0  NaN  NaN\n</code></pre>\n\n<hr>\n\n<p>Is there any efficient way to achieve this ? <code>apply</code> Here is way to slow .\nThank you for your assistant!:) </p>\n\n<hr>\n\n<p>My real data size</p>\n\n<pre><code>df.shape\nOut[117]: (54812040, 1522)\n</code></pre>\n",
        "formatted_input": {
            "qid": 45970751,
            "link": "https://stackoverflow.com/questions/45970751/shift-nans-to-the-end-of-their-respective-rows",
            "question": {
                "title": "Shift NaNs to the end of their respective rows",
                "ques_desc": "I have a DataFrame like : What I want to get is This is my approach as of now. Is there any efficient way to achieve this ? Here is way to slow . Thank you for your assistant!:) My real data size "
            },
            "io": [
                "     0    1    2\n0  0.0  1.0  2.0\n1  NaN  1.0  2.0\n2  NaN  NaN  2.0\n",
                "Out[116]: \n     0    1    2\n0  0.0  1.0  2.0\n1  1.0  2.0  NaN\n2  2.0  NaN  NaN\n"
            ],
            "answer": {
                "ans_desc": "Here's a NumPy solution using - If you want to save memory, assign it back instead - ",
                "code": [
                    "df[:] = justify(df.values, invalid_val=np.nan, axis=1, side='left')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 3446927,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/CiCFy.jpg?s=128&g=1",
            "display_name": "Joe Plumb",
            "link": "https://stackoverflow.com/users/3446927/joe-plumb"
        },
        "is_answered": true,
        "view_count": 159,
        "closed_date": 1523190492,
        "accepted_answer_id": 49717743,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1523190577,
        "creation_date": 1523189855,
        "question_id": 49717720,
        "link": "https://stackoverflow.com/questions/49717720/how-to-append-multiple-columns-into-two",
        "closed_reason": "Duplicate",
        "title": "How to append multiple columns into two?",
        "body": "<p>The data I am working with is in a large set of columns, with related values - for example:</p>\n\n<pre><code>| YearQ  | Area A | Area B | Area C |\n+--------+--------+--------+--------+\n| 2017Q1 | 1234.0 | 9252.0 | 3421.0 |\n| 2017Q2 | 1245.0 | 9368.0 | 3321.0 |\n| 2017Q3 | 1350.0 | 9440.0 | 3225.0 |\n| 2017Q4 | 1333.0 | 9501.0 | 3625.0 |\n</code></pre>\n\n<p>In order to join this data with another data set, I need to append these values into one column, preserving the <code>Area</code> column data, as well as the <code>YearQ</code> data:</p>\n\n<pre><code>| YearQ  |  Area  |  Value  |\n+--------+--------+---------+\n| 2017Q1 | Area A | 1234.0  |\n| 2017Q1 | Area B | 9252.0  |\n| 2017Q1 | Area C | 3421.0  |\n| 2017Q2 | Area A | 1245.0  |\n| 2017Q2 | Area B | 9368.0  |\n| 2017Q2 | Area C | 3321.0  |\n</code></pre>\n\n<p>I've tried using <code>df.append</code> and <code>pivot_table</code>, but am so far unable to get the required result .. which pandas function should I be using here?</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.melt.html\" rel=\"nofollow noreferrer\"><code>melt</code></a> with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\" rel=\"nofollow noreferrer\"><code>sort_values</code></a>:</p>\n\n<pre><code>df = df.melt('YearQ', var_name='Area', value_name='Value').sort_values(['YearQ','Area'])\n</code></pre>\n\n<p>A bit slowier alternative with <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html\" rel=\"nofollow noreferrer\"><code>set_index</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>stack</code></a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reset_index.html\" rel=\"nofollow noreferrer\"><code>reset_index</code></a>:</p>\n\n<pre><code>df = df.set_index('YearQ').stack().rename_axis(('YearQ','Area')).reset_index(name='Value')\n\nprint (df)\n     YearQ    Area   Value\n0   2017Q1  Area A  1234.0\n4   2017Q1  Area B  9252.0\n8   2017Q1  Area C  3421.0\n1   2017Q2  Area A  1245.0\n5   2017Q2  Area B  9368.0\n9   2017Q2  Area C  3321.0\n2   2017Q3  Area A  1350.0\n6   2017Q3  Area B  9440.0\n10  2017Q3  Area C  3225.0\n3   2017Q4  Area A  1333.0\n7   2017Q4  Area B  9501.0\n11  2017Q4  Area C  3625.0\n</code></pre>\n",
        "question_body": "<p>The data I am working with is in a large set of columns, with related values - for example:</p>\n\n<pre><code>| YearQ  | Area A | Area B | Area C |\n+--------+--------+--------+--------+\n| 2017Q1 | 1234.0 | 9252.0 | 3421.0 |\n| 2017Q2 | 1245.0 | 9368.0 | 3321.0 |\n| 2017Q3 | 1350.0 | 9440.0 | 3225.0 |\n| 2017Q4 | 1333.0 | 9501.0 | 3625.0 |\n</code></pre>\n\n<p>In order to join this data with another data set, I need to append these values into one column, preserving the <code>Area</code> column data, as well as the <code>YearQ</code> data:</p>\n\n<pre><code>| YearQ  |  Area  |  Value  |\n+--------+--------+---------+\n| 2017Q1 | Area A | 1234.0  |\n| 2017Q1 | Area B | 9252.0  |\n| 2017Q1 | Area C | 3421.0  |\n| 2017Q2 | Area A | 1245.0  |\n| 2017Q2 | Area B | 9368.0  |\n| 2017Q2 | Area C | 3321.0  |\n</code></pre>\n\n<p>I've tried using <code>df.append</code> and <code>pivot_table</code>, but am so far unable to get the required result .. which pandas function should I be using here?</p>\n",
        "formatted_input": {
            "qid": 49717720,
            "link": "https://stackoverflow.com/questions/49717720/how-to-append-multiple-columns-into-two",
            "question": {
                "title": "How to append multiple columns into two?",
                "ques_desc": "The data I am working with is in a large set of columns, with related values - for example: In order to join this data with another data set, I need to append these values into one column, preserving the column data, as well as the data: I've tried using and , but am so far unable to get the required result .. which pandas function should I be using here? "
            },
            "io": [
                "| YearQ  | Area A | Area B | Area C |\n+--------+--------+--------+--------+\n| 2017Q1 | 1234.0 | 9252.0 | 3421.0 |\n| 2017Q2 | 1245.0 | 9368.0 | 3321.0 |\n| 2017Q3 | 1350.0 | 9440.0 | 3225.0 |\n| 2017Q4 | 1333.0 | 9501.0 | 3625.0 |\n",
                "| YearQ  |  Area  |  Value  |\n+--------+--------+---------+\n| 2017Q1 | Area A | 1234.0  |\n| 2017Q1 | Area B | 9252.0  |\n| 2017Q1 | Area C | 3421.0  |\n| 2017Q2 | Area A | 1245.0  |\n| 2017Q2 | Area B | 9368.0  |\n| 2017Q2 | Area C | 3321.0  |\n"
            ],
            "answer": {
                "ans_desc": "Use with : A bit slowier alternative with , and : ",
                "code": [
                    "df = df.melt('YearQ', var_name='Area', value_name='Value').sort_values(['YearQ','Area'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dynamic",
            "subquery"
        ],
        "owner": {
            "reputation": 609,
            "user_id": 3877165,
            "user_type": "registered",
            "accept_rate": 54,
            "profile_image": "https://www.gravatar.com/avatar/4f5b7a157319a9d89906ae5d04614599?s=128&d=identicon&r=PG&f=1",
            "display_name": "eternity1",
            "link": "https://stackoverflow.com/users/3877165/eternity1"
        },
        "is_answered": true,
        "view_count": 60,
        "accepted_answer_id": 49715123,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1523172348,
        "creation_date": 1523168755,
        "question_id": 49715104,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49715104/dynamically-accessing-subset-of-pandas-dataf-rame-perform-calculation-and-write",
        "title": "Dynamically accessing subset of pandas dataf rame, perform calculation and write to new data frame",
        "body": "<p>I have a very large data frame from which I would like to pull a subsample, perform some calculation and then write these results into a new data frame. For the sample, please consider:</p>\n\n<pre><code>df_test = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),\n                    columns=['a', 'b', 'c', 'd', 'e'])\ndf_test\n</code></pre>\n\n<p>returning this:</p>\n\n<pre><code>    a   b   c   d   e\n0   1   9   0   3   0\n1   5   4   1   0   3\n2   9   3   6   3   5\n3   6   2   5   9   7\n4   9   0   7   9   5\n</code></pre>\n\n<p>Now I would like \"extract\" always 3 rows, rolling from the beginning and calculate the averages (as an example, other calculations would work too) of each column:</p>\n\n<pre><code>df_1\n    a   b   c   d   e\n0   1   9   0   3   0\n1   5   4   1   0   3\n2   9   3   6   3   5\n\ndf_2 \n    a   b   c   d   e\n1   5   4   1   0   3\n2   9   3   6   3   5\n3   6   2   5   9   7\n\ndf_3 \n    a   b   c   d   e\n2   9   3   6   3   5\n3   6   2   5   9   7\n4   9   0   7   9   5\n</code></pre>\n\n<p>the result data frame is then</p>\n\n<pre><code>result\n    a   b   c   d   e\n0   5   5.3 2.3 3   2.7\n1   6.7 3   4   4   5\n2   8   1.7 6   7   5.3\n</code></pre>\n\n<p>How can I do that?</p>\n",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html\" rel=\"nofollow noreferrer\"><code>rolling</code></a> and remove first <code>NaN</code>s rows by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html\" rel=\"nofollow noreferrer\"><code>iloc</code></a> or <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\" rel=\"nofollow noreferrer\"><code>dropna</code></a>:</p>\n\n<pre><code>N = 3\ndf = df.rolling(N).mean().iloc[N-1:]\n</code></pre>\n\n<hr>\n\n<pre><code>df = df.rolling(3).mean().dropna(how='all')\n\nprint (df)\n          a         b         c    d         e\n2  5.000000  5.333333  2.333333  2.0  2.666667\n3  6.666667  3.000000  4.000000  4.0  5.000000\n4  8.000000  1.666667  6.000000  7.0  5.666667\n</code></pre>\n\n<p>If need also <code>mean</code> of first, first + second rows add parameter <code>min_periods</code>:</p>\n\n<pre><code>df1 = df.rolling(3, min_periods=1).mean()\nprint (df1)\n          a         b         c    d         e\n0  1.000000  9.000000  0.000000  3.0  0.000000\n1  3.000000  6.500000  0.500000  1.5  1.500000\n2  5.000000  5.333333  2.333333  2.0  2.666667\n3  6.666667  3.000000  4.000000  4.0  5.000000\n4  8.000000  1.666667  6.000000  7.0  5.666667\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>Manual aproach should be create one line <code>DataFrame</code>s and then join all together:</p>\n\n<pre><code>dfs = []\nN = 3\nfor x in np.arange(len(df)+1)[N:]:\n    df1 = df.iloc[np.arange(x - N, x)]\n    #print (df1)\n    s = df1.mean().to_frame().T\n    #print (s)\n    dfs.append(s)\n\ndf2 = pd.concat(dfs, ignore_index=True)\nprint (df2)\n          a         b         c    d         e\n0  5.000000  5.333333  2.333333  2.0  2.666667\n1  6.666667  3.000000  4.000000  4.0  5.000000\n2  8.000000  1.666667  6.000000  7.0  5.666667\n</code></pre>\n",
        "question_body": "<p>I have a very large data frame from which I would like to pull a subsample, perform some calculation and then write these results into a new data frame. For the sample, please consider:</p>\n\n<pre><code>df_test = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),\n                    columns=['a', 'b', 'c', 'd', 'e'])\ndf_test\n</code></pre>\n\n<p>returning this:</p>\n\n<pre><code>    a   b   c   d   e\n0   1   9   0   3   0\n1   5   4   1   0   3\n2   9   3   6   3   5\n3   6   2   5   9   7\n4   9   0   7   9   5\n</code></pre>\n\n<p>Now I would like \"extract\" always 3 rows, rolling from the beginning and calculate the averages (as an example, other calculations would work too) of each column:</p>\n\n<pre><code>df_1\n    a   b   c   d   e\n0   1   9   0   3   0\n1   5   4   1   0   3\n2   9   3   6   3   5\n\ndf_2 \n    a   b   c   d   e\n1   5   4   1   0   3\n2   9   3   6   3   5\n3   6   2   5   9   7\n\ndf_3 \n    a   b   c   d   e\n2   9   3   6   3   5\n3   6   2   5   9   7\n4   9   0   7   9   5\n</code></pre>\n\n<p>the result data frame is then</p>\n\n<pre><code>result\n    a   b   c   d   e\n0   5   5.3 2.3 3   2.7\n1   6.7 3   4   4   5\n2   8   1.7 6   7   5.3\n</code></pre>\n\n<p>How can I do that?</p>\n",
        "formatted_input": {
            "qid": 49715104,
            "link": "https://stackoverflow.com/questions/49715104/dynamically-accessing-subset-of-pandas-dataf-rame-perform-calculation-and-write",
            "question": {
                "title": "Dynamically accessing subset of pandas dataf rame, perform calculation and write to new data frame",
                "ques_desc": "I have a very large data frame from which I would like to pull a subsample, perform some calculation and then write these results into a new data frame. For the sample, please consider: returning this: Now I would like \"extract\" always 3 rows, rolling from the beginning and calculate the averages (as an example, other calculations would work too) of each column: the result data frame is then How can I do that? "
            },
            "io": [
                "    a   b   c   d   e\n0   1   9   0   3   0\n1   5   4   1   0   3\n2   9   3   6   3   5\n3   6   2   5   9   7\n4   9   0   7   9   5\n",
                "df_1\n    a   b   c   d   e\n0   1   9   0   3   0\n1   5   4   1   0   3\n2   9   3   6   3   5\n\ndf_2 \n    a   b   c   d   e\n1   5   4   1   0   3\n2   9   3   6   3   5\n3   6   2   5   9   7\n\ndf_3 \n    a   b   c   d   e\n2   9   3   6   3   5\n3   6   2   5   9   7\n4   9   0   7   9   5\n"
            ],
            "answer": {
                "ans_desc": "Use and remove first s rows by or : If need also of first, first + second rows add parameter : EDIT: Manual aproach should be create one line s and then join all together: ",
                "code": [
                    "N = 3\ndf = df.rolling(N).mean().iloc[N-1:]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "csv",
            "dataframe"
        ],
        "owner": {
            "reputation": 77,
            "user_id": 5113601,
            "user_type": "registered",
            "accept_rate": 80,
            "profile_image": "https://www.gravatar.com/avatar/a0b1d2135f6da349a1f66877a67046c1?s=128&d=identicon&r=PG&f=1",
            "display_name": "sithara",
            "link": "https://stackoverflow.com/users/5113601/sithara"
        },
        "is_answered": true,
        "view_count": 64,
        "accepted_answer_id": 49274723,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1522855831,
        "creation_date": 1521021379,
        "last_edit_date": 1522855831,
        "question_id": 49274575,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49274575/merge-2-csv-files-with-mapped-values-in-another-file-separated-by-comma",
        "title": "Merge 2 CSV files with mapped values in another file separated by comma",
        "body": "<p>here is my problem:</p>\n\n<p>I have tow csv files as follows:</p>\n\n<p><strong>Book1.csv</strong></p>\n\n<pre><code>Id  Product\n0   aaaa\n1   bbbb\n2   cccc\n3   dddd\n</code></pre>\n\n<p><strong>Book2.csv</strong></p>\n\n<pre><code>Id  Attribute\n0   aaad\n0   sssd\n1   fffd\n1   gggd\n1   cccd\n2   bbbd\n3   hhhd\n3   bbbd\n</code></pre>\n\n<p>I want merge above files and get an output file as this:</p>\n\n<pre><code>Product Attributes\naaaa    aaad, sssd\nbbbb    fffd, gggd, cccd\ncccc    bbbd\ndddd    hhhd, bbbd\n</code></pre>\n\n<p>The code I am using right now is:</p>\n\n<pre><code>import pandas as pd\n\na = pd.read_csv(\"Book1.csv\")\nb = pd.read_csv(\"Book2.csv\")\nb = b.dropna(axis=0)\nmerged = a.merge(b, how='left', left_on='Id', right_on='Id' )\nmerged.rename(columns={\n                 'Product': 'Product',\n                 'Attribute': 'Attributes'}, inplace=True)\nmerged = merged[['Product','Attributes']]\nmerged.to_csv(\"output.csv\", index=False)\n</code></pre>\n\n<p>what I get from this is :</p>\n\n<pre><code>Product Attributes\naaaa    aaad\naaaa    sssd\nbbbb    fffd\nbbbb    gggd\nbbbb    cccd\ncccc    bbbd\ndddd    hhhd\ndddd    bbbd\n</code></pre>\n\n<p>All the Attributes and Products are merged correctly. But what I want is merge Attibutes into one string and separate by comma (not line by line). How do I do this? Thank you in advance!</p>\n",
        "answer_body": "<p>This is one way.</p>\n\n<pre><code>g = df2.groupby('Id')['Attribute'].apply(', '.join)\ndf1['Attributes'] = df1['Id'].map(g)\n</code></pre>\n\n<p><strong>Result</strong></p>\n\n<pre><code>   Id Product          Attributes\n0   0    aaaa          aaad, sssd\n1   1    bbbb    fffd, gggd, cccd\n2   2    cccc                bbbd\n3   3    dddd          hhhd, bbbd\n</code></pre>\n\n<p>If you want to just combine to <code>list</code>, you can use this instead, though it won't print nicely:</p>\n\n<pre><code>g = df2.groupby('Id')['Attribute'].apply(list)\n</code></pre>\n\n<p><strong>Explanation</strong></p>\n\n<ul>\n<li>Group <code>df2</code> Attributes by Id and aggregate to list.</li>\n<li>Map to column in <code>df1</code> via <code>pd.Series.map</code>.</li>\n</ul>\n",
        "question_body": "<p>here is my problem:</p>\n\n<p>I have tow csv files as follows:</p>\n\n<p><strong>Book1.csv</strong></p>\n\n<pre><code>Id  Product\n0   aaaa\n1   bbbb\n2   cccc\n3   dddd\n</code></pre>\n\n<p><strong>Book2.csv</strong></p>\n\n<pre><code>Id  Attribute\n0   aaad\n0   sssd\n1   fffd\n1   gggd\n1   cccd\n2   bbbd\n3   hhhd\n3   bbbd\n</code></pre>\n\n<p>I want merge above files and get an output file as this:</p>\n\n<pre><code>Product Attributes\naaaa    aaad, sssd\nbbbb    fffd, gggd, cccd\ncccc    bbbd\ndddd    hhhd, bbbd\n</code></pre>\n\n<p>The code I am using right now is:</p>\n\n<pre><code>import pandas as pd\n\na = pd.read_csv(\"Book1.csv\")\nb = pd.read_csv(\"Book2.csv\")\nb = b.dropna(axis=0)\nmerged = a.merge(b, how='left', left_on='Id', right_on='Id' )\nmerged.rename(columns={\n                 'Product': 'Product',\n                 'Attribute': 'Attributes'}, inplace=True)\nmerged = merged[['Product','Attributes']]\nmerged.to_csv(\"output.csv\", index=False)\n</code></pre>\n\n<p>what I get from this is :</p>\n\n<pre><code>Product Attributes\naaaa    aaad\naaaa    sssd\nbbbb    fffd\nbbbb    gggd\nbbbb    cccd\ncccc    bbbd\ndddd    hhhd\ndddd    bbbd\n</code></pre>\n\n<p>All the Attributes and Products are merged correctly. But what I want is merge Attibutes into one string and separate by comma (not line by line). How do I do this? Thank you in advance!</p>\n",
        "formatted_input": {
            "qid": 49274575,
            "link": "https://stackoverflow.com/questions/49274575/merge-2-csv-files-with-mapped-values-in-another-file-separated-by-comma",
            "question": {
                "title": "Merge 2 CSV files with mapped values in another file separated by comma",
                "ques_desc": "here is my problem: I have tow csv files as follows: Book1.csv Book2.csv I want merge above files and get an output file as this: The code I am using right now is: what I get from this is : All the Attributes and Products are merged correctly. But what I want is merge Attibutes into one string and separate by comma (not line by line). How do I do this? Thank you in advance! "
            },
            "io": [
                "Id  Product\n0   aaaa\n1   bbbb\n2   cccc\n3   dddd\n",
                "Id  Attribute\n0   aaad\n0   sssd\n1   fffd\n1   gggd\n1   cccd\n2   bbbd\n3   hhhd\n3   bbbd\n"
            ],
            "answer": {
                "ans_desc": "This is one way. Result If you want to just combine to , you can use this instead, though it won't print nicely: Explanation Group Attributes by Id and aggregate to list. Map to column in via . ",
                "code": [
                    "g = df2.groupby('Id')['Attribute'].apply(', '.join)\ndf1['Attributes'] = df1['Id'].map(g)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 1382,
            "user_id": 6428488,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/5bb14e33b2cf4a065f6bb82d0116f907?s=128&d=identicon&r=PG&f=1",
            "display_name": "tktktk0711",
            "link": "https://stackoverflow.com/users/6428488/tktktk0711"
        },
        "is_answered": true,
        "view_count": 232,
        "accepted_answer_id": 49525379,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1522223131,
        "creation_date": 1522205677,
        "last_edit_date": 1522207940,
        "question_id": 49525337,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49525337/python-pandas-how-to-fast-process-the-value-in-columns",
        "title": "Python pandas: how to fast process the value in columns",
        "body": "<p>Hi there is a dataframe like the following dataframes df1. The data type is string.</p>\n\n<pre><code>    eye         nose       mouse       ear\n  34_35_a      45_66_b    45_64_a     78_87_a\n  35_38_a      75_76_b    95_37_a     38_79_a\n  64_43_a      85_66_b    65_45_a     87_45_a\n</code></pre>\n\n<p>I want to get the dataframe like the following dataframe. The eye data is divided into eye_x, eye_y, the other columns is the same, the data type is float.</p>\n\n<pre><code> eye_x   eye_y    nose_x   nose_y     mouse_x  mouse_y     ear_x   ear_y        \n    34       35       45       66         45        64        78       87\n    35       38       75       76         95        37        38       79\n    64       43       85       66         65        45        87       45\n</code></pre>\n\n<p>Until now I know how to get the (x, y) value together  with the following code:</p>\n\n<pre><code> eye           nose       mouse       ear\n  (34, 35)      (45,66)    (45,64)     (78,87)\n  (35, 38)      (75,76)    (95,37)     (38,79)\n  (64, 43)      (85,66)    (65,45)     (87,45)\n</code></pre>\n\n<p> </p>\n\n<pre><code>def process_xy(val_str):\n    s = val_str.split('_')\n    x = float(s[0])\n    y = float(s[1])\n    label = int(s[2])\n    return np.array([x, y])\n\nkeypoint_cols = list(df.columns)\nd = None\nfor col in keypoint_cols:\n    df[col+'_xy'] = df[col].apply(process_xy)\n\ndf2 = df.drop(keypoint_cols, axis=1)\n</code></pre>\n",
        "answer_body": "<p>You may try <code>stack</code>ing and <code>unstack</code>ing again. </p>\n\n<pre><code>v = df.stack().str.split('_', expand=True).iloc[:, :-1]\nv.columns = ['x', 'y']\n\nv = v.unstack().swaplevel(0, 1, axis=1)\nv.columns = v.columns.map('_'.join)\n</code></pre>\n\n<p></p>\n\n<pre><code>v.sort_index(axis=1)\n\n  ear_x ear_y eye_x eye_y mouse_x mouse_y nose_x nose_y\n0    78    87    34    35      45      64     45     66\n1    38    79    35    38      95      37     75     76\n2    87    45    64    43      65      45     85     66\n</code></pre>\n",
        "question_body": "<p>Hi there is a dataframe like the following dataframes df1. The data type is string.</p>\n\n<pre><code>    eye         nose       mouse       ear\n  34_35_a      45_66_b    45_64_a     78_87_a\n  35_38_a      75_76_b    95_37_a     38_79_a\n  64_43_a      85_66_b    65_45_a     87_45_a\n</code></pre>\n\n<p>I want to get the dataframe like the following dataframe. The eye data is divided into eye_x, eye_y, the other columns is the same, the data type is float.</p>\n\n<pre><code> eye_x   eye_y    nose_x   nose_y     mouse_x  mouse_y     ear_x   ear_y        \n    34       35       45       66         45        64        78       87\n    35       38       75       76         95        37        38       79\n    64       43       85       66         65        45        87       45\n</code></pre>\n\n<p>Until now I know how to get the (x, y) value together  with the following code:</p>\n\n<pre><code> eye           nose       mouse       ear\n  (34, 35)      (45,66)    (45,64)     (78,87)\n  (35, 38)      (75,76)    (95,37)     (38,79)\n  (64, 43)      (85,66)    (65,45)     (87,45)\n</code></pre>\n\n<p> </p>\n\n<pre><code>def process_xy(val_str):\n    s = val_str.split('_')\n    x = float(s[0])\n    y = float(s[1])\n    label = int(s[2])\n    return np.array([x, y])\n\nkeypoint_cols = list(df.columns)\nd = None\nfor col in keypoint_cols:\n    df[col+'_xy'] = df[col].apply(process_xy)\n\ndf2 = df.drop(keypoint_cols, axis=1)\n</code></pre>\n",
        "formatted_input": {
            "qid": 49525337,
            "link": "https://stackoverflow.com/questions/49525337/python-pandas-how-to-fast-process-the-value-in-columns",
            "question": {
                "title": "Python pandas: how to fast process the value in columns",
                "ques_desc": "Hi there is a dataframe like the following dataframes df1. The data type is string. I want to get the dataframe like the following dataframe. The eye data is divided into eye_x, eye_y, the other columns is the same, the data type is float. Until now I know how to get the (x, y) value together with the following code: "
            },
            "io": [
                " eye_x   eye_y    nose_x   nose_y     mouse_x  mouse_y     ear_x   ear_y        \n    34       35       45       66         45        64        78       87\n    35       38       75       76         95        37        38       79\n    64       43       85       66         65        45        87       45\n",
                " eye           nose       mouse       ear\n  (34, 35)      (45,66)    (45,64)     (78,87)\n  (35, 38)      (75,76)    (95,37)     (38,79)\n  (64, 43)      (85,66)    (65,45)     (87,45)\n"
            ],
            "answer": {
                "ans_desc": "You may try ing and ing again. ",
                "code": [
                    "v = df.stack().str.split('_', expand=True).iloc[:, :-1]\nv.columns = ['x', 'y']\n\nv = v.unstack().swaplevel(0, 1, axis=1)\nv.columns = v.columns.map('_'.join)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 371,
            "user_id": 9470893,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1f361b128718cc148e896ac8852e7675?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ronnie",
            "link": "https://stackoverflow.com/users/9470893/ronnie"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 49514025,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1522156867,
        "creation_date": 1522155958,
        "question_id": 49513688,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49513688/check-if-string-contains-sub-string-from-the-same-column-in-pandas-dataframe",
        "title": "check if string contains sub string from the same column in pandas dataframe",
        "body": "<p>Hi I have the following dataframe:</p>\n\n<pre><code>&gt; df1\n  col1  \n0 donald     \n1 mike\n2 donald trump\n3 trump\n4 mike pence\n5 pence\n6 jarred\n</code></pre>\n\n<p>i want to check for the strings that contain sub string from this column and create a new column that holds the bigger strings if the condition is full filled</p>\n\n<p>something like this:</p>\n\n<pre><code>&gt; df1\n  col1           col2\n0 donald        donald trump\n1 mike          mike pence\n2 donald trump  donald trump\n3 trump         donald trump\n4 mike pence    mike pence\n5 pence         mike pence\n6 jarred        jarred\n</code></pre>\n\n<p>Thanks in advance</p>\n",
        "answer_body": "<p>This should do it:</p>\n\n<pre><code>df['Col2'] = df['Col1'].apply(lambda x: max([i for i in df['Col1'] if x in i], key=len))\n</code></pre>\n",
        "question_body": "<p>Hi I have the following dataframe:</p>\n\n<pre><code>&gt; df1\n  col1  \n0 donald     \n1 mike\n2 donald trump\n3 trump\n4 mike pence\n5 pence\n6 jarred\n</code></pre>\n\n<p>i want to check for the strings that contain sub string from this column and create a new column that holds the bigger strings if the condition is full filled</p>\n\n<p>something like this:</p>\n\n<pre><code>&gt; df1\n  col1           col2\n0 donald        donald trump\n1 mike          mike pence\n2 donald trump  donald trump\n3 trump         donald trump\n4 mike pence    mike pence\n5 pence         mike pence\n6 jarred        jarred\n</code></pre>\n\n<p>Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 49513688,
            "link": "https://stackoverflow.com/questions/49513688/check-if-string-contains-sub-string-from-the-same-column-in-pandas-dataframe",
            "question": {
                "title": "check if string contains sub string from the same column in pandas dataframe",
                "ques_desc": "Hi I have the following dataframe: i want to check for the strings that contain sub string from this column and create a new column that holds the bigger strings if the condition is full filled something like this: Thanks in advance "
            },
            "io": [
                "> df1\n  col1  \n0 donald     \n1 mike\n2 donald trump\n3 trump\n4 mike pence\n5 pence\n6 jarred\n",
                "> df1\n  col1           col2\n0 donald        donald trump\n1 mike          mike pence\n2 donald trump  donald trump\n3 trump         donald trump\n4 mike pence    mike pence\n5 pence         mike pence\n6 jarred        jarred\n"
            ],
            "answer": {
                "ans_desc": "This should do it: ",
                "code": [
                    "df['Col2'] = df['Col1'].apply(lambda x: max([i for i in df['Col1'] if x in i], key=len))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "string",
            "pandas",
            "dataframe",
            "replace"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 8974931,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-_YdKzgUUTas/AAAAAAAAAAI/AAAAAAAAAxA/s52ExJgvCVU/photo.jpg?sz=128",
            "display_name": "Matheus Morselli Gysi",
            "link": "https://stackoverflow.com/users/8974931/matheus-morselli-gysi"
        },
        "is_answered": true,
        "view_count": 915,
        "accepted_answer_id": 49499458,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1522095084,
        "creation_date": 1522093203,
        "last_edit_date": 1522095084,
        "question_id": 49499352,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49499352/insert-characters-at-multiple-positions-in-a-dataframe-column",
        "title": "Insert characters at multiple positions in a dataframe column",
        "body": "<p>I have a table with a lot of rows, and i need to change all the first \" \" space for \"h\" the second space for \"m\" and add at last char \"s\"</p>\n\n<pre><code>import pandas as pd\n\nd = {'RA' : ['11 50 10.4747', \"11 50 10.2641\",\"11 50 10.0534\", \"11 50 09.8428\"],'DEC':[\"+26 01 09.559\",\"+26 01 10.770\", \"+26 01 11.980\",\"+26 01 13.191\"]}\ndf=pd.DataFrame(d)\nfor i in range(len(df)):\n    RA = df['RA'][i]\n    RA = str.replace(RA, \" \", \"h\", 1)\n    RA = str.replace(RA, \" \", \"m\", 1)\n    RA += \"s\"\n    df['RA'][i] = RA\n    DEC = df['DEC'][i]\n    DEC = str.replace(DEC, \" \", \"d\", 1)\n    DEC = str.replace(DEC, \" \", \"m\", 1)\n    DEC += \"s\"\n    df['DEC'][i] = DEC\n</code></pre>\n\n<p>I have made this code, but for my use is getting a slow.\nWith this code I change my data frame from:</p>\n\n<pre><code>     DEC             RA\n+26 01 09.559  11 50 10.4747 \n+26 01 10.770  11 50 10.2641  \n+26 01 11.980  11 50 10.0534 \n+26 01 13.191  11 50 09.8428\n</code></pre>\n\n<p>To This:</p>\n\n<pre><code>         DEC              RA\n0  +26d01m09.559s  11h50m10.4747s\n1  +26d01m10.770s  11h50m10.2641s\n2  +26d01m11.980s  11h50m10.0534s\n3  +26d01m13.191s  11h50m09.8428s\n</code></pre>\n\n<p>Is there any function that i can do this?\nI tried using df.replace, but it replaces all \" \" in the table...</p>\n\n<p>thanks for now</p>\n",
        "answer_body": "<p>Define a function to handle splitting and recombination. You may use <code>str.split</code>, followed by some super simple string concatenation, respectively.</p>\n\n<pre><code>def split_combine(v, letters=list('dms')):\n    v = v.str.split(expand=True)\n    return (\n       v[0] + letters[0] \n     + v[1] + letters[1] \n     + v[2] + letters[2]\n    )\n</code></pre>\n\n<p>Now, call it with your appropriate parameters.</p>\n\n<pre><code>df['DEC'] = split_combine(df.DEC, list('dms'))\ndf['RA'] = split_combine(df.RA, list('hms'))\n\ndf\n              DEC              RA\n0  +26d01m09.559s  11h50m10.4747s\n1  +26d01m10.770s  11h50m10.2641s\n2  +26d01m11.980s  11h50m10.0534s\n3  +26d01m13.191s  11h50m09.8428s\n</code></pre>\n",
        "question_body": "<p>I have a table with a lot of rows, and i need to change all the first \" \" space for \"h\" the second space for \"m\" and add at last char \"s\"</p>\n\n<pre><code>import pandas as pd\n\nd = {'RA' : ['11 50 10.4747', \"11 50 10.2641\",\"11 50 10.0534\", \"11 50 09.8428\"],'DEC':[\"+26 01 09.559\",\"+26 01 10.770\", \"+26 01 11.980\",\"+26 01 13.191\"]}\ndf=pd.DataFrame(d)\nfor i in range(len(df)):\n    RA = df['RA'][i]\n    RA = str.replace(RA, \" \", \"h\", 1)\n    RA = str.replace(RA, \" \", \"m\", 1)\n    RA += \"s\"\n    df['RA'][i] = RA\n    DEC = df['DEC'][i]\n    DEC = str.replace(DEC, \" \", \"d\", 1)\n    DEC = str.replace(DEC, \" \", \"m\", 1)\n    DEC += \"s\"\n    df['DEC'][i] = DEC\n</code></pre>\n\n<p>I have made this code, but for my use is getting a slow.\nWith this code I change my data frame from:</p>\n\n<pre><code>     DEC             RA\n+26 01 09.559  11 50 10.4747 \n+26 01 10.770  11 50 10.2641  \n+26 01 11.980  11 50 10.0534 \n+26 01 13.191  11 50 09.8428\n</code></pre>\n\n<p>To This:</p>\n\n<pre><code>         DEC              RA\n0  +26d01m09.559s  11h50m10.4747s\n1  +26d01m10.770s  11h50m10.2641s\n2  +26d01m11.980s  11h50m10.0534s\n3  +26d01m13.191s  11h50m09.8428s\n</code></pre>\n\n<p>Is there any function that i can do this?\nI tried using df.replace, but it replaces all \" \" in the table...</p>\n\n<p>thanks for now</p>\n",
        "formatted_input": {
            "qid": 49499352,
            "link": "https://stackoverflow.com/questions/49499352/insert-characters-at-multiple-positions-in-a-dataframe-column",
            "question": {
                "title": "Insert characters at multiple positions in a dataframe column",
                "ques_desc": "I have a table with a lot of rows, and i need to change all the first \" \" space for \"h\" the second space for \"m\" and add at last char \"s\" I have made this code, but for my use is getting a slow. With this code I change my data frame from: To This: Is there any function that i can do this? I tried using df.replace, but it replaces all \" \" in the table... thanks for now "
            },
            "io": [
                "     DEC             RA\n+26 01 09.559  11 50 10.4747 \n+26 01 10.770  11 50 10.2641  \n+26 01 11.980  11 50 10.0534 \n+26 01 13.191  11 50 09.8428\n",
                "         DEC              RA\n0  +26d01m09.559s  11h50m10.4747s\n1  +26d01m10.770s  11h50m10.2641s\n2  +26d01m11.980s  11h50m10.0534s\n3  +26d01m13.191s  11h50m09.8428s\n"
            ],
            "answer": {
                "ans_desc": "Define a function to handle splitting and recombination. You may use , followed by some super simple string concatenation, respectively. Now, call it with your appropriate parameters. ",
                "code": [
                    "def split_combine(v, letters=list('dms')):\n    v = v.str.split(expand=True)\n    return (\n       v[0] + letters[0] \n     + v[1] + letters[1] \n     + v[2] + letters[2]\n    )\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "append",
            "concatenation"
        ],
        "owner": {
            "reputation": 695,
            "user_id": 6934489,
            "user_type": "registered",
            "accept_rate": 74,
            "profile_image": "https://www.gravatar.com/avatar/aa1df7a1a5d874f48bf8250542baee32?s=128&d=identicon&r=PG&f=1",
            "display_name": "DPdl",
            "link": "https://stackoverflow.com/users/6934489/dpdl"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 49494178,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1522075307,
        "creation_date": 1522074097,
        "last_edit_date": 1522074847,
        "question_id": 49493720,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49493720/merging-combining-dataframes-in-pandas",
        "title": "Merging/Combining Dataframes in Pandas",
        "body": "<p>I have a df1, example:</p>\n\n<pre><code>     B    A    C\nB         1\nA              1\nC    2\n</code></pre>\n\n<p>,and a df2, example:</p>\n\n<pre><code>    C    E    D\nC        2    3\nE             1\nD   2\n</code></pre>\n\n<p>The column and row 'C' is common in both dataframes. </p>\n\n<p>I would like to combine these dataframes such that I get,</p>\n\n<pre><code>    B    A    C    D    E\nB        1\nA             1\nC   2              2    3\nD                       1\nE   2  \n</code></pre>\n\n<p>Is there an easy way to do this? pd.concat and pd.append do not seem to work. Thanks!</p>\n\n<p>Edit: df1.combine_first(df2) works (thanks @jezarel), but can we keep the original ordering?</p>\n",
        "answer_body": "<p>There is problem <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combine_first.html\" rel=\"nofollow noreferrer\"><code>combine_first</code></a> always sorted columns namd index, so need <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html\" rel=\"nofollow noreferrer\"><code>reindex</code></a> with combine columns names:</p>\n\n<pre><code>idx = df1.columns.append(df2.columns).unique()\nprint (idx)\nIndex(['B', 'A', 'C', 'E', 'D'], dtype='object')\n\ndf = df1.combine_first(df2).reindex(index=idx, columns=idx)\nprint (df)\n     B    A    C    E    D\nB  NaN  1.0  NaN  NaN  NaN\nA  NaN  NaN  1.0  NaN  NaN\nC  2.0  NaN  NaN  2.0  3.0\nE  NaN  NaN  NaN  NaN  1.0\nD  NaN  NaN  2.0  NaN  NaN\n</code></pre>\n\n<p>More general solution:</p>\n\n<pre><code>c = df1.columns.append(df2.columns).unique()\ni = df1.index.append(df2.index).unique()\n\ndf = df1.combine_first(df2).reindex(index=i, columns=c)\n</code></pre>\n",
        "question_body": "<p>I have a df1, example:</p>\n\n<pre><code>     B    A    C\nB         1\nA              1\nC    2\n</code></pre>\n\n<p>,and a df2, example:</p>\n\n<pre><code>    C    E    D\nC        2    3\nE             1\nD   2\n</code></pre>\n\n<p>The column and row 'C' is common in both dataframes. </p>\n\n<p>I would like to combine these dataframes such that I get,</p>\n\n<pre><code>    B    A    C    D    E\nB        1\nA             1\nC   2              2    3\nD                       1\nE   2  \n</code></pre>\n\n<p>Is there an easy way to do this? pd.concat and pd.append do not seem to work. Thanks!</p>\n\n<p>Edit: df1.combine_first(df2) works (thanks @jezarel), but can we keep the original ordering?</p>\n",
        "formatted_input": {
            "qid": 49493720,
            "link": "https://stackoverflow.com/questions/49493720/merging-combining-dataframes-in-pandas",
            "question": {
                "title": "Merging/Combining Dataframes in Pandas",
                "ques_desc": "I have a df1, example: ,and a df2, example: The column and row 'C' is common in both dataframes. I would like to combine these dataframes such that I get, Is there an easy way to do this? pd.concat and pd.append do not seem to work. Thanks! Edit: df1.combine_first(df2) works (thanks @jezarel), but can we keep the original ordering? "
            },
            "io": [
                "    C    E    D\nC        2    3\nE             1\nD   2\n",
                "    B    A    C    D    E\nB        1\nA             1\nC   2              2    3\nD                       1\nE   2  \n"
            ],
            "answer": {
                "ans_desc": "There is problem always sorted columns namd index, so need with combine columns names: More general solution: ",
                "code": [
                    "c = df1.columns.append(df2.columns).unique()\ni = df1.index.append(df2.index).unique()\n\ndf = df1.combine_first(df2).reindex(index=i, columns=c)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "timestamp"
        ],
        "owner": {
            "reputation": 885,
            "user_id": 8826524,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/709c267b2ae4fc621fd988fe346d39af?s=128&d=identicon&r=PG&f=1",
            "display_name": "may",
            "link": "https://stackoverflow.com/users/8826524/may"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 49367652,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1521476663,
        "creation_date": 1521475791,
        "last_edit_date": 1521476275,
        "question_id": 49367519,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49367519/select-consecultive-times-in-a-dataframe",
        "title": "Select consecultive times in a dataframe",
        "body": "<p>I have a data frame where I would like to select the consecutive timestamp. I mean times that happen one after the other, in this case, that happened 15 minutes consecutively. \nFor example,</p>\n\n<pre><code>2017-07-19 17:45:00+02:00    16\n2017-07-23 02:45:00+02:00    23\n2017-07-25 14:15:00+02:00    23\n2017-07-27 07:00:00+02:00    25\n2017-07-28 09:30:00+02:00    22\n2017-07-28 18:00:00+02:00    17\n2017-07-29 04:00:00+02:00    28\n2017-07-29 04:15:00+02:00    19\n2017-07-29 11:30:00+02:00    20\n2017-07-30 09:00:00+02:00    11\n2017-08-03 02:45:00+02:00    22\n2017-08-04 06:45:00+02:00    27\n2017-08-06 01:45:00+02:00    21\n2017-08-08 19:30:00+02:00    27\n2017-08-08 19:45:00+02:00    27\n2017-08-08 20:00:00+02:00    15\n2017-08-08 21:45:00+02:00    25\n</code></pre>\n\n<p>I would select only those from the above dataframe</p>\n\n<pre><code>2017-07-29 04:00:00+02:00    28\n2017-07-29 04:15:00+02:00    19\n2017-08-08 19:30:00+02:00    27\n2017-08-08 19:45:00+02:00    27\n2017-08-08 20:00:00+02:00    15 \n</code></pre>\n\n<p>I have This is just an example but I am dealing with many timestamps. How can I do this with python commands?</p>\n",
        "answer_body": "<p>You could</p>\n\n<pre><code>In [202]: s = df.time.diff().dt.total_seconds().eq(900)\n\nIn [203]: df[s.shift(-1) | s]\nOut[203]:\n                  time   v\n6  2017-07-29 02:00:00  28\n7  2017-07-29 02:15:00  19\n13 2017-08-08 17:30:00  27\n14 2017-08-08 17:45:00  27\n15 2017-08-08 18:00:00  15\n</code></pre>\n\n<hr>\n\n<pre><code>In [205]: df.time.diff().dt.total_seconds().eq(900)\nOut[205]:\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7      True\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14     True\n15     True\n16    False\nName: time, dtype: bool\n</code></pre>\n",
        "question_body": "<p>I have a data frame where I would like to select the consecutive timestamp. I mean times that happen one after the other, in this case, that happened 15 minutes consecutively. \nFor example,</p>\n\n<pre><code>2017-07-19 17:45:00+02:00    16\n2017-07-23 02:45:00+02:00    23\n2017-07-25 14:15:00+02:00    23\n2017-07-27 07:00:00+02:00    25\n2017-07-28 09:30:00+02:00    22\n2017-07-28 18:00:00+02:00    17\n2017-07-29 04:00:00+02:00    28\n2017-07-29 04:15:00+02:00    19\n2017-07-29 11:30:00+02:00    20\n2017-07-30 09:00:00+02:00    11\n2017-08-03 02:45:00+02:00    22\n2017-08-04 06:45:00+02:00    27\n2017-08-06 01:45:00+02:00    21\n2017-08-08 19:30:00+02:00    27\n2017-08-08 19:45:00+02:00    27\n2017-08-08 20:00:00+02:00    15\n2017-08-08 21:45:00+02:00    25\n</code></pre>\n\n<p>I would select only those from the above dataframe</p>\n\n<pre><code>2017-07-29 04:00:00+02:00    28\n2017-07-29 04:15:00+02:00    19\n2017-08-08 19:30:00+02:00    27\n2017-08-08 19:45:00+02:00    27\n2017-08-08 20:00:00+02:00    15 \n</code></pre>\n\n<p>I have This is just an example but I am dealing with many timestamps. How can I do this with python commands?</p>\n",
        "formatted_input": {
            "qid": 49367519,
            "link": "https://stackoverflow.com/questions/49367519/select-consecultive-times-in-a-dataframe",
            "question": {
                "title": "Select consecultive times in a dataframe",
                "ques_desc": "I have a data frame where I would like to select the consecutive timestamp. I mean times that happen one after the other, in this case, that happened 15 minutes consecutively. For example, I would select only those from the above dataframe I have This is just an example but I am dealing with many timestamps. How can I do this with python commands? "
            },
            "io": [
                "2017-07-19 17:45:00+02:00    16\n2017-07-23 02:45:00+02:00    23\n2017-07-25 14:15:00+02:00    23\n2017-07-27 07:00:00+02:00    25\n2017-07-28 09:30:00+02:00    22\n2017-07-28 18:00:00+02:00    17\n2017-07-29 04:00:00+02:00    28\n2017-07-29 04:15:00+02:00    19\n2017-07-29 11:30:00+02:00    20\n2017-07-30 09:00:00+02:00    11\n2017-08-03 02:45:00+02:00    22\n2017-08-04 06:45:00+02:00    27\n2017-08-06 01:45:00+02:00    21\n2017-08-08 19:30:00+02:00    27\n2017-08-08 19:45:00+02:00    27\n2017-08-08 20:00:00+02:00    15\n2017-08-08 21:45:00+02:00    25\n",
                "2017-07-29 04:00:00+02:00    28\n2017-07-29 04:15:00+02:00    19\n2017-08-08 19:30:00+02:00    27\n2017-08-08 19:45:00+02:00    27\n2017-08-08 20:00:00+02:00    15 \n"
            ],
            "answer": {
                "ans_desc": "You could ",
                "code": [
                    "In [205]: df.time.diff().dt.total_seconds().eq(900)\nOut[205]:\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7      True\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14     True\n15     True\n16    False\nName: time, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 249,
            "user_id": 7944566,
            "user_type": "registered",
            "accept_rate": 67,
            "profile_image": "https://www.gravatar.com/avatar/4699d332a1d0685a3baa08672fa64f72?s=128&d=identicon&r=PG&f=1",
            "display_name": "hyon",
            "link": "https://stackoverflow.com/users/7944566/hyon"
        },
        "is_answered": true,
        "view_count": 36,
        "closed_date": 1494232793,
        "accepted_answer_id": 43828263,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1521474859,
        "creation_date": 1494134663,
        "last_edit_date": 1521474859,
        "question_id": 43828124,
        "link": "https://stackoverflow.com/questions/43828124/how-can-i-change-the-structure-of-a-dataframe",
        "closed_reason": "Needs more focus",
        "title": "How can I change the structure of a dataframe?",
        "body": "<p>How can I change the structure of a dataframe?  I need series data of each row. I tried unstack but failed.</p>\n\n<p>Example dataframe:</p>\n\n<pre><code>    df:\n\n        c1   c2     c3\n\n   0     a     b     c\n   1     d     e     f\n</code></pre>\n\n<p>Output Series:</p>\n\n<pre><code>S1 \n0   a\n1   b\n2   c\n\nS2\n0   d\n1   e\n2   f\n</code></pre>\n",
        "answer_body": "<p>You can transpose the data frame, and, if you so like, rename the indices, to a range:</p>\n\n<pre><code>dfT = df.T\ndfT.index = pd.RangeIndex(dfT.shape[0]) # Rename the indicies\ndfT.columns=['S{}'.format(i+1) for i in range(dfT.shape[1])] # Rename the columns\ndfT.unstack()\n</code></pre>\n",
        "question_body": "<p>How can I change the structure of a dataframe?  I need series data of each row. I tried unstack but failed.</p>\n\n<p>Example dataframe:</p>\n\n<pre><code>    df:\n\n        c1   c2     c3\n\n   0     a     b     c\n   1     d     e     f\n</code></pre>\n\n<p>Output Series:</p>\n\n<pre><code>S1 \n0   a\n1   b\n2   c\n\nS2\n0   d\n1   e\n2   f\n</code></pre>\n",
        "formatted_input": {
            "qid": 43828124,
            "link": "https://stackoverflow.com/questions/43828124/how-can-i-change-the-structure-of-a-dataframe",
            "question": {
                "title": "How can I change the structure of a dataframe?",
                "ques_desc": "How can I change the structure of a dataframe? I need series data of each row. I tried unstack but failed. Example dataframe: Output Series: "
            },
            "io": [
                "    df:\n\n        c1   c2     c3\n\n   0     a     b     c\n   1     d     e     f\n",
                "S1 \n0   a\n1   b\n2   c\n\nS2\n0   d\n1   e\n2   f\n"
            ],
            "answer": {
                "ans_desc": "You can transpose the data frame, and, if you so like, rename the indices, to a range: ",
                "code": [
                    "dfT = df.T\ndfT.index = pd.RangeIndex(dfT.shape[0]) # Rename the indicies\ndfT.columns=['S{}'.format(i+1) for i in range(dfT.shape[1])] # Rename the columns\ndfT.unstack()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 4072,
            "user_id": 973158,
            "user_type": "registered",
            "accept_rate": 78,
            "profile_image": "https://www.gravatar.com/avatar/d8f132b1c508947f8c6858790796f1a4?s=128&d=identicon&r=PG&f=1",
            "display_name": "beta",
            "link": "https://stackoverflow.com/users/973158/beta"
        },
        "is_answered": true,
        "view_count": 133,
        "accepted_answer_id": 49352831,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1521458457,
        "creation_date": 1521407307,
        "question_id": 49352824,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49352824/python-pandas-delete-every-row-after-specific-value-in-cell",
        "title": "Python Pandas delete every row after specific value in cell",
        "body": "<p>I have a Pandas Dataframe that looks as follows:</p>\n\n<pre><code>    streak \n0      1.0 \n1      2.0 \n2      0.0 \n3      1.0 \n4      2.0 \n5      0.0 \n6      0.0 \n</code></pre>\n\n<p>I want to delete every row after the first <code>0.0</code> in the <code>streak</code> column.</p>\n\n<p>The result should look like this:</p>\n\n<pre><code>    streak \n0      1.0 \n1      2.0 \n</code></pre>\n",
        "answer_body": "<p>Get index of first <code>0</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html\" rel=\"nofollow noreferrer\"><code>idxmax</code></a> and slice by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.iloc.html\" rel=\"nofollow noreferrer\"><code>iloc</code></a>, only need default unique indices:</p>\n\n<pre><code>#df = df.reset_index(drop=True)\ndf = df.iloc[:df['streak'].eq(0).idxmax()]\nprint (df)\n   streak\n0     1.0\n1     2.0\n</code></pre>\n\n<p><strong>Detail</strong>:</p>\n\n<pre><code>print (df['streak'].eq(0).idxmax())\n2\n</code></pre>\n\n<p>EDIT: For more general solution is necessary use <code>numpy</code> - get position by <a href=\"https://docs.scipy.org/doc/numpy-1.9.3/reference/generated/numpy.argmax.html\" rel=\"nofollow noreferrer\"><code>numpy.argmax</code></a>:</p>\n\n<pre><code>print (df)\n   streak\na     1.0\nb     2.0\nc     0.0\nd     1.0\ne     2.0\nf     0.0\ng     0.0\n\ndf = df.iloc[:df['streak'].eq(0).values.argmax()]\nprint (df)\n   streak\na     1.0\nb     2.0\n</code></pre>\n",
        "question_body": "<p>I have a Pandas Dataframe that looks as follows:</p>\n\n<pre><code>    streak \n0      1.0 \n1      2.0 \n2      0.0 \n3      1.0 \n4      2.0 \n5      0.0 \n6      0.0 \n</code></pre>\n\n<p>I want to delete every row after the first <code>0.0</code> in the <code>streak</code> column.</p>\n\n<p>The result should look like this:</p>\n\n<pre><code>    streak \n0      1.0 \n1      2.0 \n</code></pre>\n",
        "formatted_input": {
            "qid": 49352824,
            "link": "https://stackoverflow.com/questions/49352824/python-pandas-delete-every-row-after-specific-value-in-cell",
            "question": {
                "title": "Python Pandas delete every row after specific value in cell",
                "ques_desc": "I have a Pandas Dataframe that looks as follows: I want to delete every row after the first in the column. The result should look like this: "
            },
            "io": [
                "    streak \n0      1.0 \n1      2.0 \n2      0.0 \n3      1.0 \n4      2.0 \n5      0.0 \n6      0.0 \n",
                "    streak \n0      1.0 \n1      2.0 \n"
            ],
            "answer": {
                "ans_desc": "Get index of first by and slice by , only need default unique indices: Detail: EDIT: For more general solution is necessary use - get position by : ",
                "code": [
                    "print (df)\n   streak\na     1.0\nb     2.0\nc     0.0\nd     1.0\ne     2.0\nf     0.0\ng     0.0\n\ndf = df.iloc[:df['streak'].eq(0).values.argmax()]\nprint (df)\n   streak\na     1.0\nb     2.0\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 9987,
            "user_id": 5178905,
            "user_type": "registered",
            "accept_rate": 68,
            "profile_image": "https://www.gravatar.com/avatar/1c0748e7186e5a37822dbba51012e5b4?s=128&d=identicon&r=PG&f=1",
            "display_name": "Joe",
            "link": "https://stackoverflow.com/users/5178905/joe"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 49358809,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1521451416,
        "creation_date": 1521446844,
        "last_edit_date": 1521447442,
        "question_id": 49358261,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49358261/compare-2-consecutive-cells-in-a-dataframe",
        "title": "Compare 2 consecutive cells in a dataframe",
        "body": "<p>I have a dataframe (more than 150 rows and 16 columns)  with <code>multiindex</code> like this:</p>\n\n<pre><code>              a001          a002          a003        a004         a005  \nYear Week                                                                    \n2017  1          0            1            1            3            0   \n      2          1            2            2            4            0   \n      3          2            0            3            5            0   \n      4          0            0            4            0            0   \n      5          0            1            5            0            0   \n      6          0            2            6            1            0   \n      7          0            0            7            2            0   \n      8          1            0            0            3            0   \n      9          2            0            0            0            0   \n     10          3            2            0            0            0  \n</code></pre>\n\n<p>What I would like is to have only the last numbers per column before the 0 in a following row:</p>\n\n<pre><code>              a001          a002          a003        a004         a005  \nYear Week                                                                    \n2017  1          0            0            0            0            0   \n      2          0            0            0            0            0   \n      3          0            2            0            0            0   \n      4          2            0            0            5            0   \n      5          0            0            0            0            0   \n      6          0            0            0            0            0   \n      7          0            2            0            0            0   \n      8          0            0            7            0            0   \n      9          0            0            0            3            0   \n     10          0            0            0            0            0  \n</code></pre>\n\n<p>I started to try with <code>mask</code>, but then I got stucked</p>\n\n<pre><code>for i in column:\n    mask = (df[i] &lt; df[i].shift())\n    print mask\n</code></pre>\n\n<p>Can anyone help in this direction or with any other solution? Thanks in advance</p>\n",
        "answer_body": "<p>I think need compare 2 consecutive <code>0</code>, replace another values to <code>0</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html\" rel=\"nofollow noreferrer\"><code>where</code></a>, <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shift.html\" rel=\"nofollow noreferrer\"><code>shift</code></a>, convert <code>NaN</code>s to <code>0</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\" rel=\"nofollow noreferrer\"><code>fillna</code></a> and last to <code>integer</code>:</p>\n\n<pre><code>mask = (df != 0) &amp; (df.shift(-1) == 0) &amp; (df.shift(-2) == 0)\ndf1 = df.where(mask).shift().fillna(0).astype(int)\n\nprint (df1)\n           a001  a002  a003  a004  a005\nYear Week                              \n2017 1        0     0     0     0     0\n     2        0     0     0     0     0\n     3        0     2     0     0     0\n     4        2     0     0     5     0\n     5        0     0     0     0     0\n     6        0     0     0     0     0\n     7        0     2     0     0     0\n     8        0     0     7     0     0\n     9        0     0     0     3     0\n     10       0     0     0     0     0\n</code></pre>\n\n<p>EDIT:</p>\n\n<p>Thanks @Joe for simplify code:</p>\n\n<pre><code>df1 = df.where((df != 0) &amp; (df.shift(-1) == 0)).shift().fillna(0).astype(int)\n</code></pre>\n",
        "question_body": "<p>I have a dataframe (more than 150 rows and 16 columns)  with <code>multiindex</code> like this:</p>\n\n<pre><code>              a001          a002          a003        a004         a005  \nYear Week                                                                    \n2017  1          0            1            1            3            0   \n      2          1            2            2            4            0   \n      3          2            0            3            5            0   \n      4          0            0            4            0            0   \n      5          0            1            5            0            0   \n      6          0            2            6            1            0   \n      7          0            0            7            2            0   \n      8          1            0            0            3            0   \n      9          2            0            0            0            0   \n     10          3            2            0            0            0  \n</code></pre>\n\n<p>What I would like is to have only the last numbers per column before the 0 in a following row:</p>\n\n<pre><code>              a001          a002          a003        a004         a005  \nYear Week                                                                    \n2017  1          0            0            0            0            0   \n      2          0            0            0            0            0   \n      3          0            2            0            0            0   \n      4          2            0            0            5            0   \n      5          0            0            0            0            0   \n      6          0            0            0            0            0   \n      7          0            2            0            0            0   \n      8          0            0            7            0            0   \n      9          0            0            0            3            0   \n     10          0            0            0            0            0  \n</code></pre>\n\n<p>I started to try with <code>mask</code>, but then I got stucked</p>\n\n<pre><code>for i in column:\n    mask = (df[i] &lt; df[i].shift())\n    print mask\n</code></pre>\n\n<p>Can anyone help in this direction or with any other solution? Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 49358261,
            "link": "https://stackoverflow.com/questions/49358261/compare-2-consecutive-cells-in-a-dataframe",
            "question": {
                "title": "Compare 2 consecutive cells in a dataframe",
                "ques_desc": "I have a dataframe (more than 150 rows and 16 columns) with like this: What I would like is to have only the last numbers per column before the 0 in a following row: I started to try with , but then I got stucked Can anyone help in this direction or with any other solution? Thanks in advance "
            },
            "io": [
                "              a001          a002          a003        a004         a005  \nYear Week                                                                    \n2017  1          0            1            1            3            0   \n      2          1            2            2            4            0   \n      3          2            0            3            5            0   \n      4          0            0            4            0            0   \n      5          0            1            5            0            0   \n      6          0            2            6            1            0   \n      7          0            0            7            2            0   \n      8          1            0            0            3            0   \n      9          2            0            0            0            0   \n     10          3            2            0            0            0  \n",
                "              a001          a002          a003        a004         a005  \nYear Week                                                                    \n2017  1          0            0            0            0            0   \n      2          0            0            0            0            0   \n      3          0            2            0            0            0   \n      4          2            0            0            5            0   \n      5          0            0            0            0            0   \n      6          0            0            0            0            0   \n      7          0            2            0            0            0   \n      8          0            0            7            0            0   \n      9          0            0            0            3            0   \n     10          0            0            0            0            0  \n"
            ],
            "answer": {
                "ans_desc": "I think need compare 2 consecutive , replace another values to by , , convert s to by and last to : EDIT: Thanks @Joe for simplify code: ",
                "code": [
                    "df1 = df.where((df != 0) & (df.shift(-1) == 0)).shift().fillna(0).astype(int)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 6234,
            "user_id": 766708,
            "user_type": "registered",
            "accept_rate": 81,
            "profile_image": "https://www.gravatar.com/avatar/55fd0f7d95a4938975026ff886c3a563?s=128&d=identicon&r=PG",
            "display_name": "daiyue",
            "link": "https://stackoverflow.com/users/766708/daiyue"
        },
        "is_answered": true,
        "view_count": 2422,
        "accepted_answer_id": 49325042,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1521216391,
        "creation_date": 1521216014,
        "last_edit_date": 1521216291,
        "question_id": 49324988,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49324988/pandas-sum-the-differences-between-two-columns-in-each-group",
        "title": "pandas sum the differences between two columns in each group",
        "body": "<p>I have a <code>df</code> looks like,</p>\n\n<pre><code>A               B              C    D\n2017-10-01      2017-10-11     M    2017-10\n2017-10-02      2017-10-03     M    2017-10\n2017-11-01      2017-11-04     B    2017-11\n2017-11-08      2017-11-09     B    2017-11\n2018-01-01      2018-01-03     A    2018-01\n</code></pre>\n\n<p>the <code>dtype</code> of <code>A</code> and <code>B</code> are <code>datetime64</code>, <code>C</code> and <code>D</code> are of <code>strings</code>;</p>\n\n<p>I like to <code>groupby</code> <code>C</code> and <code>D</code> and get the differences between <code>B</code> and <code>A</code>,</p>\n\n<pre><code>df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A'])\n</code></pre>\n\n<p>but I don't know how to sum such differences in each group and assign the values to a new column say <code>E</code>, possibly in a new <code>df</code>,</p>\n\n<pre><code>C    D          E\nM    2017-10    11\nM    2017-10    11\nB    2017-11    4\nB    2017-11    4\nA    2018-01    2\n</code></pre>\n",
        "answer_body": "<p>Base on you code </p>\n\n<pre><code>df.merge(df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A']).sum(level=[0,1]).reset_index())\nOut[292]: \n           A          B  C        D       0\n0 2017-10-01 2017-10-11  M  2017-10 11 days\n1 2017-10-02 2017-10-03  M  2017-10 11 days\n2 2017-11-01 2017-11-04  B  2017-11  4 days\n3 2017-11-08 2017-11-09  B  2017-11  4 days\n4 2018-01-01 2018-01-03  A  2018-01  2 days\n</code></pre>\n",
        "question_body": "<p>I have a <code>df</code> looks like,</p>\n\n<pre><code>A               B              C    D\n2017-10-01      2017-10-11     M    2017-10\n2017-10-02      2017-10-03     M    2017-10\n2017-11-01      2017-11-04     B    2017-11\n2017-11-08      2017-11-09     B    2017-11\n2018-01-01      2018-01-03     A    2018-01\n</code></pre>\n\n<p>the <code>dtype</code> of <code>A</code> and <code>B</code> are <code>datetime64</code>, <code>C</code> and <code>D</code> are of <code>strings</code>;</p>\n\n<p>I like to <code>groupby</code> <code>C</code> and <code>D</code> and get the differences between <code>B</code> and <code>A</code>,</p>\n\n<pre><code>df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A'])\n</code></pre>\n\n<p>but I don't know how to sum such differences in each group and assign the values to a new column say <code>E</code>, possibly in a new <code>df</code>,</p>\n\n<pre><code>C    D          E\nM    2017-10    11\nM    2017-10    11\nB    2017-11    4\nB    2017-11    4\nA    2018-01    2\n</code></pre>\n",
        "formatted_input": {
            "qid": 49324988,
            "link": "https://stackoverflow.com/questions/49324988/pandas-sum-the-differences-between-two-columns-in-each-group",
            "question": {
                "title": "pandas sum the differences between two columns in each group",
                "ques_desc": "I have a looks like, the of and are , and are of ; I like to and and get the differences between and , but I don't know how to sum such differences in each group and assign the values to a new column say , possibly in a new , "
            },
            "io": [
                "A               B              C    D\n2017-10-01      2017-10-11     M    2017-10\n2017-10-02      2017-10-03     M    2017-10\n2017-11-01      2017-11-04     B    2017-11\n2017-11-08      2017-11-09     B    2017-11\n2018-01-01      2018-01-03     A    2018-01\n",
                "C    D          E\nM    2017-10    11\nM    2017-10    11\nB    2017-11    4\nB    2017-11    4\nA    2018-01    2\n"
            ],
            "answer": {
                "ans_desc": "Base on you code ",
                "code": [
                    "df.merge(df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A']).sum(level=[0,1]).reset_index())\nOut[292]: \n           A          B  C        D       0\n0 2017-10-01 2017-10-11  M  2017-10 11 days\n1 2017-10-02 2017-10-03  M  2017-10 11 days\n2 2017-11-01 2017-11-04  B  2017-11  4 days\n3 2017-11-08 2017-11-09  B  2017-11  4 days\n4 2018-01-01 2018-01-03  A  2018-01  2 days\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 135,
            "user_id": 3897862,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/100006566321484/picture?type=large",
            "display_name": "PacmanKX",
            "link": "https://stackoverflow.com/users/3897862/pacmankx"
        },
        "is_answered": true,
        "view_count": 187,
        "accepted_answer_id": 49301434,
        "answer_count": 5,
        "score": 2,
        "last_activity_date": 1521123490,
        "creation_date": 1521122045,
        "question_id": 49301344,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49301344/pass-different-columns-in-pandas-dataframe-in-a-custom-function-in-df-apply",
        "title": "Pass Different Columns in Pandas DataFrame in a Custom Function in df.apply()",
        "body": "<p>Say I have a dataframe <code>df</code>:</p>\n\n<pre><code>  x y z\n0 1 2 3\n1 4 5 6\n2 7 8 9\n</code></pre>\n\n<p>I wanna have two new columns that are x * y and x * z:</p>\n\n<pre><code>  x y z xy xz\n0 1 2 3  2  3\n1 4 5 6 20 24\n2 7 8 9 56 63\n</code></pre>\n\n<p>So I define a function <code>func</code> (just for example) that takes either the string <code>'y'</code> or the string <code>'z'</code> as an argument to indicate which column I want to multiply with the column x:</p>\n\n<pre><code>def func(row, colName):\n    return row['x'] * row[colName]\n</code></pre>\n\n<p>And apply the function to the dataframe <code>df</code>:</p>\n\n<pre><code>df['xz'] = df.apply(func, axis=1)\n</code></pre>\n\n<p>Apparently it is wrong here because I didn't specify the <code>colName</code>, <code>'y'</code> or <code>'z'</code>. Question is, <code>df.apply()</code> just takes function name, how do I tell it to take the two arguments?</p>\n",
        "answer_body": "<p>You can use lambda function with specify columns, but also is necessary change <code>func</code>:</p>\n\n<pre><code>def func(row, colName):\n    return row * colName\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x['x'], x[c]), axis=1)\n</code></pre>\n\n<p>If is not possible change <code>func</code>:</p>\n\n<pre><code>def func(row, colName):\n    return row['x'] * row[colName]\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x, c), axis=1)\n</code></pre>\n\n<hr>\n\n<pre><code>print (df)\n   x  y  z  xy  xz\n0  1  2  3   2   3\n1  4  5  6  20  24\n2  7  8  9  56  63\n</code></pre>\n",
        "question_body": "<p>Say I have a dataframe <code>df</code>:</p>\n\n<pre><code>  x y z\n0 1 2 3\n1 4 5 6\n2 7 8 9\n</code></pre>\n\n<p>I wanna have two new columns that are x * y and x * z:</p>\n\n<pre><code>  x y z xy xz\n0 1 2 3  2  3\n1 4 5 6 20 24\n2 7 8 9 56 63\n</code></pre>\n\n<p>So I define a function <code>func</code> (just for example) that takes either the string <code>'y'</code> or the string <code>'z'</code> as an argument to indicate which column I want to multiply with the column x:</p>\n\n<pre><code>def func(row, colName):\n    return row['x'] * row[colName]\n</code></pre>\n\n<p>And apply the function to the dataframe <code>df</code>:</p>\n\n<pre><code>df['xz'] = df.apply(func, axis=1)\n</code></pre>\n\n<p>Apparently it is wrong here because I didn't specify the <code>colName</code>, <code>'y'</code> or <code>'z'</code>. Question is, <code>df.apply()</code> just takes function name, how do I tell it to take the two arguments?</p>\n",
        "formatted_input": {
            "qid": 49301344,
            "link": "https://stackoverflow.com/questions/49301344/pass-different-columns-in-pandas-dataframe-in-a-custom-function-in-df-apply",
            "question": {
                "title": "Pass Different Columns in Pandas DataFrame in a Custom Function in df.apply()",
                "ques_desc": "Say I have a dataframe : I wanna have two new columns that are x * y and x * z: So I define a function (just for example) that takes either the string or the string as an argument to indicate which column I want to multiply with the column x: And apply the function to the dataframe : Apparently it is wrong here because I didn't specify the , or . Question is, just takes function name, how do I tell it to take the two arguments? "
            },
            "io": [
                "  x y z\n0 1 2 3\n1 4 5 6\n2 7 8 9\n",
                "  x y z xy xz\n0 1 2 3  2  3\n1 4 5 6 20 24\n2 7 8 9 56 63\n"
            ],
            "answer": {
                "ans_desc": "You can use lambda function with specify columns, but also is necessary change : If is not possible change : ",
                "code": [
                    "def func(row, colName):\n    return row * colName\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x['x'], x[c]), axis=1)\n",
                    "def func(row, colName):\n    return row['x'] * row[colName]\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x, c), axis=1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "reputation": 93,
            "user_id": 9294498,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d23c03afa0ea54459233cb361b749323?s=128&d=identicon&r=PG&f=1",
            "display_name": "rafat.ch",
            "link": "https://stackoverflow.com/users/9294498/rafat-ch"
        },
        "is_answered": true,
        "view_count": 222,
        "accepted_answer_id": 48565063,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1520472784,
        "creation_date": 1517489401,
        "last_edit_date": 1520472784,
        "question_id": 48563186,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/48563186/python-dataframe-get-the-nan-columns-for-each-row",
        "title": "Python Dataframe get the NaN columns for each row",
        "body": "<p>I have a pandas dataframe which look like the following: </p>\n\n<pre><code>  a          b       c     \n NaN         2       165     \n NaN         9       NaN     \n NaN         NaN     NaN    \n  15        15       NaN      \n   5         NaN     11 \n</code></pre>\n\n<p>I would like to add a column that gives me something like a summary of Null values. So I need a command which gives me for every row which columns are NULL. Something like this:</p>\n\n<pre><code>  a          b       c     Summary\n NaN         2       165      a\n NaN         9       NaN     a + c\n NaN         NaN     NaN    a + b + c\n  15        15       NaN      c      \n   5         NaN     11       b\n</code></pre>\n\n<p>I could not find anything which satisfies my need on the internet.</p>\n",
        "answer_body": "<p>Here is one way.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[np.nan, 2, 165], [np.nan, 9, np.nan], [np.nan, np.nan, np.nan],\n                   [15, 15, np.nan], [5, np.nan, 11]], columns=['a', 'b', 'c'])\n\ndf['Errors'] = df.apply(lambda row: ' + '.join(i for i in ['a', 'b', 'c'] if np.isnan(row[i])), axis=1)\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe which look like the following: </p>\n\n<pre><code>  a          b       c     \n NaN         2       165     \n NaN         9       NaN     \n NaN         NaN     NaN    \n  15        15       NaN      \n   5         NaN     11 \n</code></pre>\n\n<p>I would like to add a column that gives me something like a summary of Null values. So I need a command which gives me for every row which columns are NULL. Something like this:</p>\n\n<pre><code>  a          b       c     Summary\n NaN         2       165      a\n NaN         9       NaN     a + c\n NaN         NaN     NaN    a + b + c\n  15        15       NaN      c      \n   5         NaN     11       b\n</code></pre>\n\n<p>I could not find anything which satisfies my need on the internet.</p>\n",
        "formatted_input": {
            "qid": 48563186,
            "link": "https://stackoverflow.com/questions/48563186/python-dataframe-get-the-nan-columns-for-each-row",
            "question": {
                "title": "Python Dataframe get the NaN columns for each row",
                "ques_desc": "I have a pandas dataframe which look like the following: I would like to add a column that gives me something like a summary of Null values. So I need a command which gives me for every row which columns are NULL. Something like this: I could not find anything which satisfies my need on the internet. "
            },
            "io": [
                "  a          b       c     \n NaN         2       165     \n NaN         9       NaN     \n NaN         NaN     NaN    \n  15        15       NaN      \n   5         NaN     11 \n",
                "  a          b       c     Summary\n NaN         2       165      a\n NaN         9       NaN     a + c\n NaN         NaN     NaN    a + b + c\n  15        15       NaN      c      \n   5         NaN     11       b\n"
            ],
            "answer": {
                "ans_desc": "Here is one way. ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[np.nan, 2, 165], [np.nan, 9, np.nan], [np.nan, np.nan, np.nan],\n                   [15, 15, np.nan], [5, np.nan, 11]], columns=['a', 'b', 'c'])\n\ndf['Errors'] = df.apply(lambda row: ' + '.join(i for i in ['a', 'b', 'c'] if np.isnan(row[i])), axis=1)\n"
                ]
            }
        }
    }
]