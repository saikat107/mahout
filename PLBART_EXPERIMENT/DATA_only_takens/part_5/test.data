{"id": 273, "q": "A better way to map data in multiple datasets, with multiple data mapping rules", "d": "I have three datasets (, , ), and I wish to add a new column called in dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes. final_NN: ppt_code: herd_id: Expected output: The rules is: if in final_NN is not , = in ; if in final_NN is but in is not Null, then search the ppt_code dataframe, and use the and its corresponding \"number\" to map and fill in the \"MapValue\" in ; if both and in are and null respectively, but in is not Null, then search dataframe, and use the and its corresponding to fill in the in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result?", "q_apis": "map add value codes map first", "io": " code number 0 AA 11 1 AA 11 2 BB 22 3 BB 22 4 CC 33 <s> ID number 0 799 678 1 813 789 ", "apis": "drop_duplicates set_index drop_duplicates set_index replace fillna map fillna map", "code": ["ppt_map = ppt_code.drop_duplicates(subset=['code']).set_index('code')['number']\nhrd_map = herd_id.drop_duplicates(subset=['ID']).set_index('ID')['number']\n\nfinal_NN['MapNumber'] = final_NN['number'].replace({'Unknown': np.nan})\nfinal_NN['MapNumber'] = (\n    final_NN['MapNumber']\n    .fillna(final_NN['code'].map(ppt_map))\n    .fillna(final_NN['ID'].map(hrd_map))\n)\n"], "link": "https://stackoverflow.com/questions/62527486/a-better-way-to-map-data-in-multiple-datasets-with-multiple-data-mapping-rules"}
{"id": 540, "q": "Create a dataframe from arrays python", "d": "I'm try to construct a dataframe (I'm using Pandas library) from some arrays and one matrix. in particular, if I have two array like this: And one matrix like this : Can i create a dataset like this? Maybe is a stupid question, but i m very new with Python and Pandas. I seen this : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html but specify only 'colums'. I should read the matrix row for row and paste in my dataset, but I m think that exist a more easy solution with Pandas.", "q_apis": "array DataFrame", "io": "1 2 2 3 3 3 4 4 4 <s> A B C D 1 2 2 E 3 3 3 F 4 4 4 ", "apis": "columns array DataFrame index columns columns", "code": ["columns = [\"A\", \"B\", \"C\"]\nrows = [\"D\", \"E\", \"F\"]\ndata = np.array([[1, 2, 2], [3, 3, 3],[4, 4, 4]])\ndf = pd.DataFrame(data=data, index=rows, columns=columns)\n"], "link": "https://stackoverflow.com/questions/53961914/create-a-dataframe-from-arrays-python"}
{"id": 453, "q": "Can not make desired pandas dataframe", "d": "I am trying to make a pandas dataframe using 2 paramters as columns. But it makes a dataframe transpose of what I need. I have and as column parameters as follows: This gives the following dataframe: However, I want the dataframe as:", "q_apis": "columns transpose", "io": " 0 1 2 3 4 0 1 2 3 4 5 1 11 22 33 44 55 <s> 0 1 0 1 11 1 2 22 2 3 33 3 4 44 4 5 55 ", "apis": "DataFrame DataFrame", "code": ["df = pd.DataFrame(list(zip(a, b)))\n#pandas 0.24+\n#df = pd.DataFrame(zip(a, b))\n"], "link": "https://stackoverflow.com/questions/56929277/can-not-make-desired-pandas-dataframe"}
{"id": 556, "q": "How to handle missing data with respect to type of dataset?", "d": "I have a dataset where has column types that has type like , . df I want to replace missing value with for each type. Such as- result_df How can do it with Python?", "q_apis": "where replace value", "io": " ID types C D 0 101 Primary 2 3 1 103 Primary 6 3 2 108 Primary 10 ? 3 109 Primary 3 12 4 118 Secondary 5 2 5 122 Secondary ? 6 6 123 Secondary 5 6 7 125 Secondary 2 5 <s> ID types C D 0 101 Primary 2 3 1 103 Primary 6 3 2 108 Primary 10 3 3 109 Primary 3 12 4 118 Secondary 5 2 5 122 Secondary 5 6 6 123 Secondary 5 6 7 125 Secondary 2 5 ", "apis": "dtypes dtype apply to_numeric apply to_numeric dtypes dtype", "code": ["In [1274]: df.dtypes\nOut[1274]: \nID        int64\ntypes    object\nC        object\nD        object\ndtype: object\n", "In [1256]: df.C = df.C.apply(pd.to_numeric)\nIn [1258]: df.D = df.D.apply(pd.to_numeric)\n\nIn [1279]: df.dtypes\nOut[1279]: \nID         int64\ntypes     object\nC        float64\nD        float64\ndtype: object\n"], "link": "https://stackoverflow.com/questions/53589894/how-to-handle-missing-data-with-respect-to-type-of-dataset"}
{"id": 294, "q": "Python: Convert matrices to permutations table", "d": "Given a set of ids, I need to get the values from a matrix (time A & B) for each id combination, and create a dataframe appending the values for all the permutations. I have been able to do it by creating the permutations dataframe and then iterating through it while looking & filling the values. However I need to do this for ~3000 ids, not 3, and I don't know how to do it efficiently. Can I generate a Time A/B dataframe as my example without having to iterate through 9000000* rows? I know I shouldn't be iterating though a dataframe however I haven't found an alternative yet. Ids (3): Time A matrix (3x3): Time B matrix (3x3): Time A/B dataframe (6):", "q_apis": "get values time values all values", "io": "id 15 24 38 15 0 1.8 1.7 24 1.2 0 1.9 38 1.5 1.3 0 <s> id 15 24 38 15 0 88.7 87.3 24 42.2 0 32.7 38 65.6 13.5 0 ", "apis": "drop set_index index concat set_index stack to_frame name set_index stack to_frame name rename_axis index query reset_index", "code": ["# drop set_index('id') if `id` is already index\n(pd.concat([timeA.set_index('id').stack().to_frame(name='A'),\n           timeB.set_index('id').stack().to_frame(name='B')], axis=1)\n   .rename_axis(index=['id_start','id_end'])\n   .query('id_start != id_end')\n   .reset_index()\n)\n"], "link": "https://stackoverflow.com/questions/61926146/python-convert-matrices-to-permutations-table"}
{"id": 137, "q": "python/pandas: update a column based on a series holding sums of that same column", "d": "I have a dataframe with a non-unique col1 like the following Some of the values of col1 repeat lots of times and others not so. I'd like to take the bottom (80%/50%/10%) and change the value to 'other' ahead of plotting. I've got a series which contains the codes in col1 (as the index) and the amount of times that they appear in the df in descending order by doing the following: I've also got my cut-off point (bottom 80%) I'd like to update col1 in df with the value 'others' when col1 appears after the cutOff in the index of the series df2. I don't know how to go about checking and updating. I figured that the best way would be to do a groupby on col1 and then loop through, but it starts to fall apart, should I create a new groupby object? Or do I call this as an .apply() for each row? Can you update a column that is being used as the index for a dataframe? I could do with some help about how to start. edit to add: So if the 'b's in col1 were not in the top 20% most populous values in col1 then I'd expect to see:", "q_apis": "update unique values repeat take value contains codes index cut update value index groupby groupby apply update index start add values", "io": " col1 col2 0 a 1 1 a 1 2 a 2 3 b 3 4 b 3 5 c 2 6 c 2 <s> col1 col2 0 a 1 1 a 1 2 a 2 3 others 3 4 others 3 5 c 2 6 c 2 ", "apis": "DataFrame columns groupby size sort_values round iloc copy loc isin index", "code": ["data = [[\"a \", 1],\n        [\"a \", 1],\n        [\"a \", 2],\n        [\"b \", 3],\n        [\"b \", 3],\n        [\"c \", 2],\n        [\"c \", 2], ]\ndf = pd.DataFrame(data, columns=[\"col1\", \"col2\"])\nprint(df)\n\ndf2 = df.groupby(['col1']).size().sort_values(ascending=False)\nprint(df2)\n\ncutOff = round(len(df2) / 5)\nothers = df2.iloc[cutOff + 1:]\nprint(others)\n\nresult = df.copy()\nresult.loc[result[\"col1\"].isin(others.index), \"col1\"] = \"others\"\nprint(result)\n"], "link": "https://stackoverflow.com/questions/65385779/python-pandas-update-a-column-based-on-a-series-holding-sums-of-that-same-colum"}
{"id": 66, "q": "Assign unique ID to Pandas group but add one if repeated", "d": "I couldn't find a solution and want something faster than what I already have. So, the idea is to assign a unique ID for 'fruit' column, e.g. However, if repeated, add 1 to the last result, so that instead of: I will end up with: So it adds up until the end, even if there may only be 4 fruits changing their positions. Here is my solution but it's really slow and I bet there is something that Pandas can do, inherently: Any ideas?", "q_apis": "unique add assign unique add last", "io": "df['id'] = [0, 0, 1, 1, 2, 0, 0, 2, 2] <s> df['id'] = [0, 0, 1, 1, 2, 3, 3, 4, 4] ", "apis": "groupby shift cumsum ngroup groupby groupby sum", "code": ["df[\"id\"] = df.groupby((df[\"fruit\"] != df[\"fruit\"].shift(1)).cumsum()).ngroup()\nprint(df)\n", "    fruit  id\n0   apple   0\n1   apple   0\n2  orange   1\n3  orange   1\n4   lemon   2\n5   apple   3\n6   apple   3\n7   lemon   4\n8   lemon   4\n", "from itertools import groupby\n\ndata, i = [], 0\nfor _, g in groupby(df[\"fruit\"]):\n    data.extend([i] * sum(1 for _ in g))\n    i += 1\n\ndf[\"id\"] = data\nprint(df)\n"], "link": "https://stackoverflow.com/questions/66846548/assign-unique-id-to-pandas-group-but-add-one-if-repeated"}
{"id": 544, "q": "How to append ndarray values into dataframe rows of particular columns?", "d": "I have a function that returns an like this Now, I have a data frame with columns A,B,C,...,Z ; but the array we are getting has only 20 values. Hence I want to find a way such that for every array I get as output, I am able to store it in like this (A,B,W,X,Y,Z are to be left blank):", "q_apis": "append values columns columns array values array get left", "io": "[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0] <s> __| A | B | C | D | E | F | ... 0 |nan|nan| 0 | 1 | 0 | 0 | ... 1 |nan|nan| 1 | 1 | 0 | 1 | ... . . . ", "apis": "DataFrame columns DataFrame T Index columns array append T append Series index value", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(columns=[chr(i) for i in range(ord('A'),ord('Z')+1)])\n\nprint(df)\n", "Empty DataFrame\nColumns: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z]\nIndex: []\n\n[0 rows x 26 columns]\n", "list1 = [i for i in range(101,121)]\narr1d = np.array(list1)\n\narr1d\n", "# Create alphabet list of uppercase letters\nalphabet = []\nfor letter in range(ord('C'),ord('W')):\n    alphabet.append(chr(letter))\nalphabet\n", "['C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'K',\n 'L',\n 'M',\n 'N',\n 'O',\n 'P',\n 'Q',\n 'R',\n 'S',\n 'T',\n 'U',\n 'V']\n", "df = df.append(pd.Series(arr1d, index=alphabet), ignore_index=True)\n#This line of code can be used for every new value of arr1d \n"], "link": "https://stackoverflow.com/questions/53932155/how-to-append-ndarray-values-into-dataframe-rows-of-particular-columns"}
{"id": 509, "q": "Pandas:Calculate mean of a group of n values of each columns of a dataframe", "d": "I have a dataframe of the following type: I want to calculate the mean of the first 3 element of each column and then next 3 elements and so on and then store in a dataframe. Desired Output- Using Group By was one of the approach I thought of but I am unable to figure out how to use Group by in this case.", "q_apis": "mean values columns mean first", "io": " A B 0 1 2 1 4 5 2 7 8 3 10 11 4 13 14 5 16 17 <s> A B 0 4 5 1 12 14 ", "apis": "index Int64Index dtype groupby mean", "code": ["print (df.index // 3)\nInt64Index([0, 0, 0, 1, 1, 1], dtype='int64')\n", "df = df.groupby(np.arange(len(df)) // 3).mean()\n"], "link": "https://stackoverflow.com/questions/54806672/pandascalculate-mean-of-a-group-of-n-values-of-each-columns-of-a-dataframe"}
{"id": 39, "q": "how to extract each numbers from pandas string column to list?", "d": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list:", "q_apis": "", "io": "Column_A 11.2 some text 17 some text 21 some text 25.2 4.1 some text 53 17 78 121.1 bla bla bla 14 some text 12 some text <s> listA[0] = 11.2 listA[1] = 17 listA[2] = 21 listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78 listC[0] = 121.1 listC[1] = 14 listD[0] = 12 ", "apis": "apply map", "code": ["df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n", "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"], "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list"}
{"id": 307, "q": "How to fill elements between intervals of a list", "d": "I have a list like this: So there are intervals that begin with and end with . How can I replace the values in those intervals, say with 1? The outcome will look like this: I use in this example, but a generalized solution that can apply to any value will also be great", "q_apis": "between replace values apply any value", "io": "list_1 = [np.NaN, np.NaN, 1, np.NaN, np.NaN, np.NaN, 0, np.NaN, 1, np.NaN, 0, 1, np.NaN, 0, np.NaN, 1, np.NaN] <s> list_2 = [np.NaN, np.NaN, 1, 1, 1, 1, 0, np.NaN, 1, 1, 0, 1, 1, 0, np.NaN, 1, np.NaN] ", "apis": "Series fillna ffill where eq", "code": ["s = pd.Series(list_1)\ns.fillna(s.ffill().where(lambda x: x.eq(1))).tolist()\n"], "link": "https://stackoverflow.com/questions/61292759/how-to-fill-elements-between-intervals-of-a-list"}
{"id": 111, "q": "Creating new columns within a dataframe, based on the latest value from previous columns", "d": "I've just completed a beginner's course in python, so please bear with me if the code below doesn't make sense or my issue is because of some rookie mistake. I've been trying to put the learning to use by working with college production of NFL players, with a view to understanding which statistics can be predictive or at least correlate to NFL production. It turns out that there's a lot of data out there so I have about 200 columns of data for 600 odd prospects from the last 20 years (just for running backs so far). However, one of the problems with this data is that each stat is only provided by the age the prospect was in that season giving me something like this: What I want to do at the moment is to be able to take the last year of college production and put it into a new column (for 17 different statistics). I've therefore defined the following function: Which I think should go backwards through the columns until I find a value which isn't NaN, and then take that value as the output. I've then defined the columns via a list: and have then run the function through a for loop based on this list: The result I'm getting back is a slightly bizarre one - the for loop appears to work, as all the new columns I'm expecting are created, however they are only populated with data where the player had an age 23 season. The remainder of indexes are filled with 'NaN': This suggests to me that the first 'if' statement in my function is working fine, but that all of the 'elif' statements aren't triggering and I can't work out why. I'm wondering whether it's because I need to be more explicit about why they would trigger, rather than just relying on a logical test of 'if the column is not, not equal to NaN, go to the next one', or if I'm misunderstanding the elif aspect all together. I've put the whole segment of code in also, just because when I've run into issues so far the problem has often not been where I originally thought. By all means tell me if you think I've gone about this in a weird way - this just seemed like a logical approach to the problem but open to other ways of getting the desired result. Thanks in advance!", "q_apis": "columns value columns put view at columns last at take last year put columns value take value columns all columns where first all test all put where all", "io": " GP 18 GP 19 GP 20 GP 21 GP 22 GP 23 50 14.0 13.0 14.0 NaN NaN NaN 51 14.0 14.0 14.0 NaN NaN NaN 53 13.0 12.0 11.0 NaN NaN NaN 56 10.0 13.0 9.0 13.0 NaN NaN 59 10.0 13.0 15.0 NaN NaN NaN 61 NaN NaN 11.0 11.0 NaN NaN 66 NaN 12.0 13.0 12.0 2.0 13.0 <s> GP Last 50 NaN 51 NaN 53 NaN 56 NaN 59 NaN 61 NaN 66 13.0 ", "apis": "mask columns loc mask ffill iloc", "code": ["stats_list = ['GP']\nfor column in stats_list:\n    mask = df.columns.str.startswith(column)\n    df[f'{column} Last'] = df.loc[:, mask].ffill(axis=1).iloc[:, -1]\n"], "link": "https://stackoverflow.com/questions/65917323/creating-new-columns-within-a-dataframe-based-on-the-latest-value-from-previous"}
{"id": 5, "q": "Extract part of a 3 D dataframe", "d": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this:", "q_apis": "take", "io": " d1 d2 d3 A B C D... A B C D... A B C D.. 0 1 2 <s> d1 d2 d3 A B A B A B 0 1 2 ", "apis": "loc columns isin DataFrame columns MultiIndex from_product", "code": ["filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n", "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"], "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe"}
{"id": 380, "q": "Iterative comparison with pandas", "d": "I don't know to approach this issue. I have a data frame that looks like this What I need to do is to iterate between each row per usuario_id and check if there's a difference between each row, and create a new data set with the row changed and the usuario_web in charge of this change, to generate a data frame that looks like this: Is there any way to do this? I'm working with pandas on python and this dataset could be a little big, let's say around 10000 rows, sorted by usuario_id. Thanks for any advice.", "q_apis": "between difference between any any", "io": "cuenta_bancaria nombre_empresa perfil_cobranza usuario_id usuario_web 5545 a 123 500199 5012 5551 a 123 500199 3321 5551 a 55 500199 5541 5551 b 55 500199 5246 <s> usuario_id cambio usuario_web 500199 cuenta_bancaria 3321 500199 perfil_cobranza 5541 500199 nombre_empresa 5246 ", "apis": "columns intersection ne shift sum eq", "code": ["c = df.columns.intersection(\n        ['nombre_empresa', 'perfil_cobranza', 'cuenta_bancaria']\n)\n\ni = df[c].ne(df[c].shift())\nj = i.sum(1).eq(1)\n"], "link": "https://stackoverflow.com/questions/47872271/iterative-comparison-with-pandas"}
{"id": 524, "q": "Check if group contains same value in Pandas", "d": "I am curious if there is a pre-built function in Pandas to check if all members of a group (factors in a column) contain the same value in another column. i.e. if my dataframe was similar to below it would return an empty list. However, if my dataframe appeared as such (notice the 1 in Col1): Then the output would be a list containing the object \"B\" since the group B has different values in Col1.", "q_apis": "contains value all value empty values", "io": "Col1 Col2 2 A 2 A 0 B 0 B <s> Col1 Col2 2 A 2 A 0 B 1 B ", "apis": "groupby nunique index", "code": ["a = df.groupby('Col2').Col1.nunique() > 1\na[a].index.tolist()\n"], "link": "https://stackoverflow.com/questions/54518504/check-if-group-contains-same-value-in-pandas"}
{"id": 467, "q": "Re-arranging a single column of strings based on text containing different dates, by date", "d": "I am looking to arrange a dataframe by dates, however, the dates are a part of a string within each row. The rows must be rearranged in order by day. Other solutions from stack overflow show how to sort based on a column of dates alone, this example is different because other information is a part of each string and is mixed with the dates. The dataframe is one column with an index, but the rows are not arranged in order from the dates contained on the far right side of each string. The score numbers are random and do not require any attention. The expected dataframe should look like this (repeated dates have no preference for order between each other and index doesn't matter). The respective scores must stay with their associated dates. What is a way to do this?", "q_apis": "date day stack index right any between index", "io": " 0 __________________________ 0 score17 6-20-19.xlsx 1 score23 6-7-19.xlsx 2 score4 6-17-19.xlsx 3 score34 6-8-19.xlsx 4 score10 6-7-19.xlsx <s> 0 __________________________ 1 score23 6-7-19.xlsx 4 score10 6-7-19.xlsx 3 score34 6-8-19.xlsx 2 score4 6-17-19.xlsx 0 score17 6-20-19.xlsx ", "apis": "date date date", "code": ["df[['score', 'date']] = df['column_name'].str.split(' ', n=1, expand=True)\n", "df['date'] = df['date'].str.split('.', expand = True)\n"], "link": "https://stackoverflow.com/questions/56693262/re-arranging-a-single-column-of-strings-based-on-text-containing-different-dates"}
{"id": 36, "q": "Convert dictionary with sub-list of dictionaries into pandas dataframe", "d": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance!", "q_apis": "sub", "io": " 0 2000 {'team': 'Manchester United', 'points': '91'} 2001 {'team': 'Manchester United', 'points': '80'} 2002 {'team': 'Arsenal', 'points': '87'} <s> team points 2000 Manchester United 91 2001 Manchester United 80 2002 Arsenal 87 ", "apis": "items DataFrame T", "code": ["\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"], "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe"}
{"id": 637, "q": "Pandas: Create dataframe from data and column order", "d": "what i'm asking must be something very easy, but i honestly can't see it.... :( I have an array, lets say and i want to put it in a dataframe. I do aiming for: but i am getting: (notice the discrepancy between column names and data) I know i can re-arrange the column names order in the dataframe creation, but i'm trying to understand how it works. Am i doing something wrong, or it's normal behaviour? (why though?)", "q_apis": "array put between names names", "io": "col1 col2 col3 1 2 3 4 5 6 7 8 9 10 11 12 <s> col3 col1 col2 1 2 3 4 5 6 7 8 9 10 11 12 ", "apis": "columns", "code": ["df = pd.Dataframe(data, columns=['col1', 'col2', 'col3'])\n"], "link": "https://stackoverflow.com/questions/50150090/pandas-create-dataframe-from-data-and-column-order"}
{"id": 214, "q": "How to split dataframe made from objects?", "d": "I want to split one column pandas dataframe that look like this: into two columns: So it can look like this: But its showing type as:", "q_apis": "columns", "io": " 0 0 38 A 1 35 B 2 14 B <s> Number Letter 0 38 A 1 35 B 2 14 B ", "apis": "set_axis", "code": ["df['0'].str.split(' ', expand=True).set_axis(['Number', 'Letter'], axis=1)\n"], "link": "https://stackoverflow.com/questions/63849171/how-to-split-dataframe-made-from-objects"}
{"id": 144, "q": "How to read list of json objects from Pandas DataFrame?", "d": "I want just want to loop through the array of json objects, and get the values of 'box'..... I have a DataFrame which looks like this and the column 'facesJson' (dstype = object) contain array of json objects which look like this: when i run this code i get this error:", "q_apis": "DataFrame array get values DataFrame array get", "io": " img facesJson 0 2b26mn4.jpg [{'box': [57, 255, 91, 103], 'confidence': 0.7... 1 cd7ntf.jpg [{'box': [510, 85, 58, 87], 'confidence': 0.99... 2 m9kf3e.jpg [{'box': [328, 78, 93, 123], 'confidence': 0.9... 3 b4hx0n.jpg [{'box': [129, 30, 38, 54], 'confidence': 0.99... 4 afx0fm.jpg [{'box': [86, 126, 221, 298], 'confidence': 0.... <s> [ { \"box\":[ 158,115,84,112 ], \"confidence\":0.9998929500579834, }, { \"box\":[ 404,105, 86,114 ], \"confidence\":0.9996863603591919, } ] ", "apis": "keys", "code": ["import ast\n\nline='[ { \"box\":[ 158,115,84,112 ], \"confidence\":0.9998929500579834, }, { \"box\":[ 404,105, 86,114 ], \"confidence\":0.9996863603591919, } ]'\n\n\nparsed=ast.literal_eval(line)\n\nprint(parsed[0].keys())\n"], "link": "https://stackoverflow.com/questions/63282322/how-to-read-list-of-json-objects-from-pandas-dataframe"}
{"id": 213, "q": "Drop a pandas DataFrame row that comes after a row that contains a particular value", "d": "I am trying to drop all rows that come after a row which has inside the column df: Required output df: Look at the following code: Returns a error message I have tried many different variations of this using different methods like and but I can't seem to figure it out anyway. I have also tried truncate: This returns: IndexError: index 1 is out of bounds for axis 0 with size 1", "q_apis": "DataFrame contains value drop all at truncate index size", "io": " Ammend 0 no 1 yes 2 no 3 no 4 yes 5 no <s> Ammend 0 no 1 yes 3 no 4 yes ", "apis": "DataFrame", "code": ["import pandas as pd\nimport numpy as np\n\nnp.random.seed(525)\ndf = pd.DataFrame({'Other': np.random.rand(10), 'Ammend': np.random.choice(['yes', 'no'], 10)})\ndf\n"], "link": "https://stackoverflow.com/questions/63855152/drop-a-pandas-dataframe-row-that-comes-after-a-row-that-contains-a-particular-va"}
{"id": 425, "q": "Replicate multiple rows of events for specific IDs multiple times", "d": "I have a call log data made on customers. Which looks something like below, where ID is customer ID and A and B are log attributes: I want to replicate each set of event for each ID based on some slots. For e.g. if slot value is 2 then all events for ID \"A\" should be replicated slot-1 times. and a new Index should be created indicating which slot does replicated values belong to: I have tried following solution: it gives me the expected output but is not scalable when slots are increased and number of customers increases in order of 10k. I think its taking a long time because of the loop. Any solution which is vectorized will be really helpful.", "q_apis": "where value all Index values time", "io": " ID A B A A 46 31 A A 99 54 A A 34 9 B B 46 48 B B 7 75 C C 1 25 C C 71 40 C C 74 53 D D 57 17 D D 19 78 <s> ID A B A A 46 31 A A 99 54 A A 34 9 A A 46 31 A A 99 54 A A 34 9 ", "apis": "concat assign Index Index Index", "code": ["slots = 2\nnew_df = pd.concat(df.assign(Index=f'_{i}') for i in range(1, slots+1))\n\nnew_df['Index'] = new_df['ID'] + new_df['Index']\n"], "link": "https://stackoverflow.com/questions/58086144/replicate-multiple-rows-of-events-for-specific-ids-multiple-times"}
{"id": 275, "q": "from two arrays to one dataframe python", "d": "I am trying to put my values into two arrays and then to make them a dataframe. I am using python, numpy and pandas to do so. my arrays are: and I would like to put them into a pandas dataframe. When I print my dataframe, I would like to see this: How can I do that? I read some related questions, but I can't get it right. One of the errors says that indexes must not be tuples, but, as you can see, I don't have tuples", "q_apis": "put values put get right", "io": "k = [7.0, 8.0, 6.55, 7.0000001, 10.12] p = [6.94, 9.0, 4.44444, 13.0, 9.0876] <s> a b c d e k 7.0 8.0 6.6 7.0 10.1 p 6.9 9.0 4.4 13.0 9.1 ", "apis": "DataFrame columns index round", "code": ["from string import ascii_lowercase\npd.DataFrame([k,p],columns=list(ascii_lowercase[:len(k)]),index=['k','p']).round()\n"], "link": "https://stackoverflow.com/questions/62455905/from-two-arrays-to-one-dataframe-python"}
{"id": 199, "q": "Shifting and reverting multiple rows in pandas dataframe", "d": "I have the following dataframe and wish to shift over the 0 values to the right and then revert each row: This is the result I would like to get: I've tried varius shift and apply combinations without any success. Is there a simple way of achieving this?", "q_apis": "shift values right get shift apply any", "io": " H00 H01 H02 H03 H04 H05 H06 NR 1 33 28 98 97 0 0 0 2 29 24 22 98 97 0 0 3 78 76 98 97 0 0 0 4 16 15 98 97 0 0 0 5 81 72 70 98 97 0 0 <s> H00 H01 H02 H03 H04 H05 H06 NR 1 97 98 28 33 0 0 0 2 97 98 22 24 29 0 0 3 97 98 76 78 0 0 0 4 97 98 15 16 0 0 0 5 97 98 70 72 81 0 0 ", "apis": "apply", "code": ["def reverse_part(series):\n  series[series > 0] = series[series > 0][::-1]\n  return series\n\ndf.apply(reverse_part, axis=1, raw=True)\n"], "link": "https://stackoverflow.com/questions/64230159/shifting-and-reverting-multiple-rows-in-pandas-dataframe"}
{"id": 68, "q": "Merge pandas dataframes by timestamps", "d": "I've got a few pandas dataframes indexed with timestamps and I would like to merge them into one dataframe, matching nearest timestamp. So I would like to have for example: What exact timestamp there is going to be in final DataFrame is not important to me. BTW. Is there an easy way to leter convert \"absolute\" timestamps into time from start (either in seconds or miliseconds)? So for this example:", "q_apis": "merge timestamp timestamp DataFrame time start seconds", "io": "a = CPU 2021-03-25 13:40:44.208 70.571797 2021-03-25 13:40:44.723 14.126870 2021-03-25 13:40:45.228 17.182844 b = X Y 2021-03-25 13:40:44.193 45 1 2021-03-25 13:40:44.707 46 1 2021-03-25 13:40:45.216 50 2 a + b = CPU X Y 2021-03-25 13:40:44.208 70.571797 45 1 2021-03-25 13:40:44.723 14.126870 46 1 2021-03-25 13:40:45.228 17.182844 50 2 <s> CPU X Y 0.0 70.571797 45 1 0.5 14.126870 46 1 1.0 17.182844 50 2 ", "apis": "merge_asof", "code": ["pd.merge_asof(df1, df2, left_index=True, right_index=True, direction='nearest')\n"], "link": "https://stackoverflow.com/questions/66801447/merge-pandas-dataframes-by-timestamps"}
{"id": 523, "q": "Pandas DataFrame from Dictionary", "d": "Let's assume that I have a JSON file like below, and I want to convert this file into a data frame with 2 columns: This is what I already tried, but I am unable to create DF and probably there is a more efficient way of doing this task: Expecting output like this for all elements in a Json file Also I want to create a directed graph for each parent node with child nodes as clusters because parent nodes has the same child nodes which are also present in other parent nodes. Thanks in Advance", "q_apis": "DataFrame columns all", "io": "{\"1087\": [4,5,6,7,8,9,10,12,13,21,22,23,24,25,26,27,28,34,35 ,37,39,40,42,44,45,46,47,48,51,52,54,55,56,59,60,61,63,64,65,66,67,68,72,73,74,75,78,80,81,82,83,84,85,87,88,92,94,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,149,180,181,196,198,200,202,206,222,223,226,227,230,231,232,233,234,235,242,255,257,258,259,261,263,264,265,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,302,303,304,305,306,307,308,309,310,311,313,314,316,318,319,320,323,325,326,327,328,330,334,336,337,339,340,342,343,350,351,354,355,362,363,365,366,367,368,369,370,371,372,374,375,376,377,378,379,380,383,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,427,428,429,430,431,432,433,434,435,437,438,444,446,449,451,455,457,461,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,494,496,498,499,500,502,504,506,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,559,560,561,565,567,569,571,573,574,576,579,580,581,583,586,587,588,590,593,594,597,598,600,601,602,604,605,606,607,608,609,611,612,613,614,615,616,617,620,621,622,624,625,626,629,631,633,634,636,638,639,640,641,643,644,647,649,650,651,652,653,654,657,658,664,665,666,667,669,671,674,675,676,677,678,682,683,685,686,687,688,692,694,695,702,703,705,708,712,713,714,715,716,717,718,720,728,732,734,735,739,740,742,743,745,746,751,752,759,769,770,772,778,779,780,783,784,786,792,805,815,823,831,832,834,835,836,837,838,839,852,854,855,856,867,875,877,879,888,890,891,896,900,908,909,910,911,912,913,914,915,916,917,918,919,934,935,936,937,938,939,944,945,946,950,951,952,953,957,958,959,960,964,965,966,967,971,975,977,978,980,981,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1027,1028,1033,1034,1035,1036,1037,1038,1039,1049,1061,1063,1065,1067,1069,1070,1071,1072,1073,1074,1076,1077,1078,1080,1081,1084,1088,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1127,1128,1129,1130,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1151,1155,1156,1201,1202,1203,1204,1207,1208,1209,1213,1214,1215,1216,1217,1220,1221,1222,1223,1224,1232,1233,1235,1237,1238,1241,1243,1244,1245,1248,1249,1251,1254,1269,1271,1273,1274,1275,1284,1289,1298,1301,1302,1303,1470,1495,1500,1501,1508,1509,1517,1518,1572,1573,1574,1575,1614,1619,1620,1625,1633,1639,1661,1669,1670,1671,1692,1693,1694,1695,1696,1698,1699,1700,1701,1706,1707,1708,1709,1711,1712,1713,1715,1720,1726,1728,1729,1730,1731,1732,1734,1755,1771,1780,1781,1785,1788,1794,1795,1797,1801,1802,1803,1805,1827,1829,1830,1836,1838,1843,1845,1847,1849,1851,1852,1853,1854,1855,1897,1899,1901,1920,1922,1923,1974,1987,1988,1989,1990,1991,1993,1994,2013,2014,2038,2039,2040,2044,2057,2086,2108,2144,2150,2215,2216,2218,2219,2220,2227,2228,2229,2230,2250,2258,2271,2279,2283,2285,2286,2287,2295,2302,2327,2390,2397,2406,2407,2411,2413,2414,2415,2419,2421,2429,2441,2471,2472,2490,2493,2507,2514,2519,2524,2525,2531,2532,2535,2538,2541,2551,2552,2553,2555,2560,2561,2562,2564,2570,2577,2578,2579,2580,2581,2586,2587,2588,2589,2591,2592,2594,2595,2596,2597,2599,2600,2601,2602,2603,2604,2605,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2625,2626,2627,2628,2629,2630,2631,2634,2635,2665,2668,2669,2671,2673,2681,2682,2683,2684,2705,2706,2707,2708,2709,2710,2711,2712,2713,2750,2766,2768,2769,2770,2798,2804,2817,2821,2822,2823,2824,2825,2826,2844,2847,2853,2855,2858,2860,2861,2862,2863,2864,2865,2880,2900,2901,2902,2903,2906,2911,2912,2913,2916,2918,2922,2925,2926,2932,2935,2941,2943,2945,2947,2948,2949,2950,2958,2959,2966,2967,2972,2976,2977,2978,2979,2980,2981,2982,2987,2988,2991,2992,2993,2994,2995,2996,2999,3001,3003,3007,3008,3011,3012,3015,3016,3017,3018,3024,3030,3031,3033,3034,3045,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3093,3099,3105,3112,3113,3114,3115,3116,3117,3127,3128,3154,3155,3156,3157,3297,3299,3300,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3336,3339,3416,3417,3420,3424,3550,3587,3588,3589,3590,3591,3592,3593,3598,3599,3600,3602,3603,3604,3605,3606,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3625,3655,3656,3657,3718,3721,3724,3725,3726,3730,3732,3733,3736,3738,3741,3743,3744,3747,3748,3750,3752,3754,3756,3762,3763,3764,3765,3766,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3795,3797,3798,3800,3806,3811,3864,3866,3867,3871,3881,3883,3884,3885,3886,3925,3926,3929,3930,3935,3936,3940,4018,4030,4045,4049,4050,4051,4054,4058,4059,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4104,4109,4110,4113,4116,4117,4118,4119,4161,4267,4285,4310,4317,4335,4358,4359,4365,4366,4467,4471,4475,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4638,4639,4764,4765,4766,4782,4803,4824,4827,4828,4830,4888,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4926,4990,6998,7026,7027,7028],\"1096\": [25,26,27,28,45,46,63,64,65,66,67,80,81,82,83,84,85,128,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,330,366,367,376,385,386,387,388,391,417,418,419,420,437,449,451,553,555,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,641,643,644,779,780,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1273,1274,1275,1278,1280,1289,1292,1670,1671,1713,1730,1731,1847,1849,1993,2086,2218,2219,2220,2258,2421,2586,2587,2608,2610,2611,2629,2631,2673,2708,2709,2710,2711,2712,2713,2821,2822,2823,2825,2844,2847,2858,2860,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2959,2976,2977,2978,2979,2980,2981,2982,2991,2992,2993,2994,2995,3001,3003,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3112,3113,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3416,3417,3589,3590,3591,3592,3593,3598,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3656,3657,3732,3738,3743,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3797,3798,3871,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4267,4358,4359,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4764,4765,4766,7026,7027,7028],\"1144\": [25,26,27,28,144,372,374,422,768,1005,1051,1052,1053,1054,1057,1058,1060,1062,1064,1066,1068,1098,1101,1146,1703,1704,1705,1713,1716,1994,2086,2382,3095,3096,3097,3114,3115,3116,3339,3619,3620,3621,3732,3738,3743,3881,3883,3884,3885,3886,4113,4116,4117,4118,4119,4267,4285,4365,4370,4371,4372,4373,4374,4375,4471,4764,4765,4766,4803,4824,4828,4830,4990],\"-1\": [40,63,64,65,66,67,68,80,81,82,83,84,85,87,130,131,132,133,134,135,136,137,138,139,140,141,234,261,263,264,267,268,269,271,272,293,308,314,318,319,337,366,367,375,376,385,386,387,388,391,407,416,417,418,419,420,435,461,489,559,561,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,623,624,625,626,632,634,636,644,666,682,683,685,686,687,688,694,695,696,720,737,854,855,870,882,883,888,896,916,917,918,919,930,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,971,978,982,986,987,988,993,994,995,996,1000,1001,1002,1036,1037,1038,1039,1081,1132,1133,1134,1135,1136,1210,1317,1321,1341,1347,1377,1378,1380,1383,1384,1386,1396,1398,1408,1410,1432,1456,1458,1473,1500,1501,1525,1614,1670,1730,1808,1838,1982,1983,1984,1985,1993,2033,2034,2038,2039,2040,2069,2150,2151,2258,2355,2356,2571,2596,2692,2729,2737,2821,2822,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2976,2977,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3150,3151,3316,3416,3417,3594,3744,3769,3770,3773,3776,3785,3786,3787,3871,4064,4066,4068,4070,4072,4074,4076,4078,4080,4082,4084,4086,4088,4092,4094,4096,4098,4109,4288,4321,4330,4466,4991,5567,6913],\"2060\": [47,65,67,80,81,148,155,156,166,167,168,226,227,267,268,269,580,594,597,600,601,602,604,607,608,609,611,614,634,636,682,683,685,686,687,688,694,695,696,728,733,738,744,944,945,946,993,994,995,1317,1321,1341,1347,1377,1378,1380,1383,1384,1385,1386,1387,1396,1398,1408,1410,1432,1456,1458,1473,1525,1696,1736,1737,1738,1739,1754,1808,1859,1865,1873,1879,1885,1892,1922,1982,1983,1984,1985,1993,1994,2038,2039,2040,2150,2151,2254,2300,2355,2356,2377,2391,2448,2478,2530,2564,2723,2742,2745,2746,2747,2882,2922,2925,2947,2948,2949,2950,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3341,3383,3482,3493,3494,3506,3530,3672,3675,3821,4332,4439,4440,4459,4908,4913,4914,4915,4916,4917,4919,4920,4921,4922,4923,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5177,5178,5179,5180,5181,5182,5183,5184,5188,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5217,5218,5219,5220,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5486,5487,5488,5489,5490,5491,5492,5493,5494,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5559,5566,5567,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,7037],\"1742\": [47,60,61,63,64,65,66,67,80,81,82,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,226,227,232,233,242,267,268,269,271,314,319,323,366,367,376,385,388,417,418,419,420,554,559,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,633,634,636,643,700,701,702,703,705,717,728,745,746,834,835,836,837,838,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1088,1119,1120,1121,1122,1123,1207,1208,1209,1213,1214,1215,1216,1269,1386,1508,1509,1692,1693,1694,1695,1698,1699,1700,1701,1720,1726,1727,1729,1780,1781,1830,1851,1920,1993,2127,2216,2258,2295,2390,2564,2621,2821,2823,2844,2847,2862,2863,2864,2865,2911,2912,2913,2916,2922,2925,2935,2943,2945,2947,2948,2949,2950,2978,2979,2980,2981,2982,2987,2988,2996,3007,3008,3011,3012,3015,3016,3017,3018,3072,3099,3112,3113,3154,3155,3156,3157,3311,3312,3315,3318,3353,3355,3420,3422,3423,3590,3591,3592,3625,3732,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3794,3801,3871,3940,4049,4050,4058,4063,4169,4170,4476,4477,4482,4616,4906,6864,6865],\"1125\": [47,53,65,67,80,81,172,174,187,190,196,198,200,202,246,267,268,269,309,313,316,319,320,323,324,325,326,370,372,374,421,448,594,597,600,634,636,657,658,673,679,692,708,735,860,944,945,946,993,994,995,1061,1063,1065,1067,1220,1222,1223,1277,1502,1517,1518,1572,1573,1574,1575,1621,1622,1623,1632,1635,1637,1661,1696,1849,1897,1899,1901,1968,1993,2032,2033,2034,2069,2283,2421,2422,2423,2445,2471,2472,2490,2493,2527,2529,2609,2623,2627,2669,2671,2729,2738,2739,2804,2825,2826,2853,2854,2855,2856,2894,2895,2901,2902,2903,2904,2918,2926,2932,3024,3107,3114,3115,3116,3299,3353,3354,3355,3356,3364,3365,3373,3390,3391,3392,3393,3415,3422,3423,3425,3541,3550,3715,3719,3720,3721,3792,3793,3794,3795,3800,3835,3836,3844,3845,3846,3847,3864,3866,3867,3920,3925,3926,3929,3930,3935,3936,4030,4059,4090,4111,4112,4138,4143,4145,4161,4162,4165,4306,4311,4351,4361,4368,4397,4457,4467,4471,4480,4638,4639,4754,4764,4765,4766,4770,4803,4806,4807,4808,4824,4830,4888,4904,4905,4911,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,6892,6893],\"1095\": [64,65,66,67,80,81,187,190,196,198,200,202,219,267,268,269,319,320,324,385,388,559,573,576,580,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,644,679,689,690,691,751,756,842,843,844,845,846,847,848,937,938,939,944,945,946,951,952,953,958,959,960,964,965,966,967,986,987,988,993,994,995,1000,1001,1002,1993,2098,2250,2258,2354,2421,2495,2496,2821,2823,2839,2844,2847,2854,2856,2862,2863,2864,2865,2922,2925,2947,2948,2949,2950,2978,2980,3007,3011,3015,3016,3017,3018,3113,3299,3306,3310,3315,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3368,3422,3423,3590,3591,3592,3748,3750,3752,3754,3756,3762,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3792,3793,3794,3795,3801,3871],\"1145\": [64,65,66,67,80,81,82,84,85,125,129,130,131,132,133,134,135,140,141,263,264,267,268,269,327,334,351,367,388,446,553,594,597,600,601,602,604,607,608,609,611,614,620,622,629,631,634,643,692,735,786,867,896,937,938,939,944,945,946,950,951,952,953,958,959,960,975,977,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1077,1088,1119,1120,1121,1122,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1204,1224,1232,1233,1244,1245,1275,1517,1518,1572,1573,1574,1575,1661,1669,1670,1671,1709,1711,1729,1730,1731,1843,1845,1897,1899,1901,1993,1994,2013,2057,2218,2219,2220,2227,2228,2229,2230,2250,2258,2283,2327,2471,2472,2490,2493,2560,2562,2587,2596,2608,2609,2610,2611,2630,2631,2673,2682,2683,2684,2705,2706,2708,2709,2710,2711,2712,2713,2817,2821,2825,2844,2862,2863,2880,2900,2901,2902,2916,2918,2922,2925,2926,2941,2947,2948,2949,2950,2977,2978,2982,2991,2992,2993,3007,3011,3015,3016,3017,3018,3033,3034,3072,3297,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3417,3588,3721,3744,3864,3866,3867,4030,4034,4059,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4161,4358,4359,4366,4638,4639],\"1966\": [64,65,66,67,80,81,82,83,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,366,367,376,385,386,387,388,391,417,418,419,420,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,616,624,625,626,634,636,639,640,641,644,778,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1075,1114,1115,1116,1117,1118,1728,1993,2258,2596,2673,2821,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3112,3113,3590,3591,3592,3744,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3800,3871,4062,4764,4765,4766],\"2148\": [159,220,696,1050,1277,1278,1280,1696,1994,2032,2033,2034,2151,2319,2429,2432,2433,2434,2435,2436,2437,2441,2445,3299,3310,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3622,4913,6945,6946,6947,6948,6949,6950,6951,6952,6953,6990,6991],\"2387\": [159,1050,1994,2708,2709,2710,2711,2712,2713,2976,2977,3416,3417,3622,4358,4359],\"865\": [216,217,851,860,2053,2054,2055,2056,2131,2132,2422,2423],\"2442\": [220,1277,1621,1622,1623,1632,1635,1637,1859,1865,1873,1879,1885,1892,1994,2032,2033,2034,2432,2433,2434,2435,2436,2437,2445,2780,2789,3299],\"2370\": [321,692,1245,1517,1518,1572,1573,1574,1575,1661,1897,1899,1901,2068,2094,2095,2096,2106,2109,2263,2264,2270,2283,2284,2303,2327,2366,2367,2390,2428,2490,2493,2719,2722,2726,2735,2736,2738,2739,2740,2741,2765,2827,2894,2895,2901,2902,2903,2926,3019,3024,3031,3077,3078,3079,3081,3083,3084,3085,3349,3590,3591,3592,3605,3606,3715,3716,3853,3854,3855,3856,3857,3861,4112,4120,4284,4306,4398,4620,4621,6902,6903],\"1950\": [684,816,1285,1286,1656,1657,2405,2512,2527,3651],\"3852\": [779,780,1213,1214,1289,1847,1849,3339,3732,3738,3743,3790,3797,3800,4054,4113,4765,4766],\"2381\": [781,782,810,2023,2024,2632,2633,4365],\"1108\": [920,921],\"1105\": [1276,1502,1994,2032,2269,2319,2342,2343,2344,2348,2349,2350,2420,2421,2429,2441,3310,3321,3322,3323,3326,3327,3328],\"2725\": [2723,4161],\"2727\": [2729],\"2728\": [2730],\"2820\": [2858,2860]} <s> Parent Child 1087 4 1087 5 1087 6 ..... ...... 1096 25 1096 26 1096 27 1096 28 ...... ...... 1144 25 1144 26 1144 27 ..... ..... ", "apis": "items items", "code": ["[[k, i] for k, v in data.items() for i in v]", "In [14]: [[k, i] for k, v in data.items() for i in v]                                                                                                                                                                                                        \nOut[14]:\n[['1108', 920],\n ['1108', 921],\n ['2381', 781],\n ['2381', 782],\n ['2381', 810],\n ['2381', 2023],\n ['2381', 2024],\n ['2381', 2632],\n ['2381', 2633],\n ['2381', 4365],\n ['2728', 2730]]\n"], "link": "https://stackoverflow.com/questions/54577752/pandas-dataframe-from-dictionary"}
{"id": 455, "q": "How to manipulate column entries using only one specific output of a function that returns several values?", "d": "I have a dataframe like this: and I have a function that returns several values. Here I just use a dummy function that returns the minimum and maximum for a certain input iterable: Now I want to e.g. add the maximum of each column to each value in the respective column. So gives and then yields the desired outcome I am wondering whether there is a more straightforward way that avoids the two chained 's. Just to make sure: I am NOT interested in a type solution. I highlighted the to illustrate that this not my actual function but just serves as a minimal example function that has several outputs.", "q_apis": "values values add value", "io": "a (0, 3) b (2, 5) <s> a b 0 3 7 1 4 8 2 5 9 3 6 10 ", "apis": "min max add apply Series min max index min max add apply loc max max add apply", "code": ["def return_min_max(x):\n    return (np.min(x), np.max(x))\n\ndf.add(df.apply(return_min_max).str[1])\n", "def return_min_max(x):\n    return pd.Series([np.min(x), np.max(x)], index=['min', 'max'])\n\ndf.add(df.apply(return_min_max).loc['max'])\n", "def return_max(x):\n    return np.max(x)\n\ndf.add(df.apply(return_max))\n"], "link": "https://stackoverflow.com/questions/56891370/how-to-manipulate-column-entries-using-only-one-specific-output-of-a-function-th"}
{"id": 295, "q": "pandas groupby expanding df based on unique values", "d": "I have below: I want to achieve the following. For each unique , the bottom row is (this is ). I want to count how many times each unique value of occurs where . This part would be achieved by something like: However, I also want to add, for each unique value of , a column indicating how many times that value occurred after the point where for the value indicated by the row. I understand this might sound confusing; here's how the output woud look like in this example: It is important that the solution is general enough to be applicable on a similar case with more unique values than just , and . UPDATE As a bonus, I am also interested in how, instead of the count, one can instead return the sum of some value column, under the same conditions, divided by the corresponding in the rows. Example: suppose we now depart from below instead: The output would need to sum for the cases indicated by the counts in the solution by @jezrael, and divide that number by . The output would instead look like:", "q_apis": "groupby expanding unique values unique count unique value where add unique value value where value unique values count sum value now sum", "io": "df V2 V1 A B C 0 A 0 0 0 0 1 B 1 0 0 0 2 C 2 1 2 0 <s> df V2 V1 A B C 0 A 0 0 0 0 1 B 1 0 0 0 2 C 2 1 3.5 0 ", "apis": "pivot_table index columns mean reindex columns index", "code": ["df = df.pivot_table(\n    index='n',\n    columns='V2',\n    aggfunc=({\n        'V3': 'mean'\n    })\n).V3.reindex(columns=v, index=v, fill_value=0)\n"], "link": "https://stackoverflow.com/questions/61868871/pandas-groupby-expanding-df-based-on-unique-values"}
{"id": 38, "q": "Working with list inside a Pandas dataframe", "d": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here?", "q_apis": "length", "io": "ID | Column1 | 0 | [] | 1 | [1,2] | 2 | [] | <s> ID | Column1 | Column2 | 0 | [] | 0 | 1 | [1,2] | 2 | 2 | [] | 0 | ", "apis": "", "code": ["df['Column2'] = [len(x) for x in df['Column1']]\n"], "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe"}
{"id": 322, "q": "Add ID found in list to new column in pandas dataframe", "d": "Say I have the following dataframe (a column of integers and a column with a list of integers)... And also a separate list of IDs... Given that, and ignoring the column and any index, I want to see if any of the IDs in the list are mentioned in the column. The code I have so far is: This works but only if the list is longer than the dataframe and for the real dataset the list is going to be a lot shorter than the dataframe. If I set the list to only two elements... I get a very popular error (I have read many questions with the same error)... I have tried converting the list to a series (no change in the error). I have also tried adding the new column and setting all values to before doing the comprehension line (again no change in the error). Two questions: How do I get my code (below) to work for a list that is shorter than a dataframe? How would I get the code to write the actual ID found back to the column (more useful than True/False)? Expected output for : Ideal output for (ID(s) are written to a new column or columns): Code:", "q_apis": "any index any get all values get get columns", "io": " ID Found_IDs 0 12345 [15443, 15533, 3433] 1 15533 [2234, 16608, 12002, 7654] 2 6789 [43322, 876544, 36789] <s> bad_ids = [15533, 876544, 36789, 11111] ", "apis": "apply", "code": ["bad_ids_set = set(bad_ids)\ndf['Found_IDs'].apply(lambda x: list(set(x) & bad_ids_set))\n"], "link": "https://stackoverflow.com/questions/60989914/add-id-found-in-list-to-new-column-in-pandas-dataframe"}
{"id": 426, "q": "How to change values of rows based on conditions in dataframe?", "d": "I have dataframe that is shown below, (Need some magic to be done) The change should be made in type column such that, in a row if is and is then next row of should be assigned . Full dataframe should look like this:", "q_apis": "values", "io": " type label 0 0 0 1 0 0 2 0 0 3 0 0 4 2 1 5 2 1 6 2 1 7 2 1 8 2 1 9 2 1 10 0 0 11 0 0 12 0 0 13 0 0 14 0 0 15 0 0 16 0 0 17 0 0 18 0 0 19 0 0 <s> type label 0 0 0 1 2 0 2 2 0 3 0 0 4 2 1 5 2 1 6 2 1 7 2 1 8 2 1 9 2 1 10 0 0 11 2 0 12 2 0 13 2 0 14 2 0 15 2 0 16 2 0 17 2 0 18 2 0 19 2 0 ", "apis": "eq eq loc shift", "code": ["m = df['type'].eq(0) & df['label'].eq(0)\ndf.loc[m == m.shift(1),'type'] = 2\nprint(df)\n"], "link": "https://stackoverflow.com/questions/58060206/how-to-change-values-of-rows-based-on-conditions-in-dataframe"}
{"id": 119, "q": "Adding value from one pandas dataframe to another dataframe by matching a variable", "d": "Suppose I have a pandas dataframe with 2 columns A second dataframe, contains c1, c2 and a few other columns. My goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. What is the best way to do this?", "q_apis": "value columns second contains columns replace empty values values first values", "io": " c1 c2 0 v1 b1 1 v2 b2 2 v3 b3 3 v4 b4 4 v5 b5 <s> c1 c2 c3 c4 0 \"\" b5 500 3 1 \"\" b2 420 7 2 \"\" b1 380 5 3 \"\" b2 470 9 4 \"\" b3 290 2 ", "apis": "merge left", "code": ["main_df = pd.merge(df2, df, on=\"c2\", how=\"left\")"], "link": "https://stackoverflow.com/questions/65750044/adding-value-from-one-pandas-dataframe-to-another-dataframe-by-matching-a-variab"}
{"id": 626, "q": "Copying columns within pandas dataframe", "d": "I want to slice and copy columns in a Python Dataframe. My data frame looks like the following: I want to make it of the form Which basically means that I want to shift the values in Columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1' For the same, I wrote the code as No copying takes places within the columns. How shall I modify my code??", "q_apis": "columns copy columns shift values columns", "io": " 1928 1928.1 1929 1929.1 1930 1930.1 0 0 0 0 0 0 0 1 1 3 3 2 2 2 2 4 1 3 0 1 2 <s> 1928 1928.1 1929 1929.1 1930 1930.1 0 0 0 1 1 3 2 4 1 3 0 0 4 3 2 5 3 0 6 0 0 7 2 2 8 1 2 ", "apis": "values", "code": ["cols = ['1929', '1929.1', '1930', '1930.1']\nvals = df[cols].values.reshape(-1, 2)\n"], "link": "https://stackoverflow.com/questions/50533522/copying-columns-within-pandas-dataframe"}
{"id": 478, "q": "Append rows to groups in pandas", "d": "I'm trying to append a number of NaN rows to each group in a pandas dataframe. Essentially I want to pad each group to be 5 rows long. Ordering is important. I have: I want:", "q_apis": "groups append pad", "io": " Rank id 0 1 a 1 2 a 2 3 a 3 4 a 4 5 a 5 1 c 6 2 c 7 1 e 8 2 e 9 3 e <s> Rank id 0 1 a 1 2 a 2 3 a 3 4 a 4 5 a 5 1 c 6 2 c 7 NaN c 8 NaN c 9 NaN c 10 1 e 11 2 e 12 3 e 13 NaN e 14 NaN e ", "apis": "crosstab iloc unstack reset_index loc", "code": ["df = pd.crosstab(df.Rank, df.ID).iloc[:5].unstack().reset_index()\ndf.loc[(df[0]==0),'Rank'] = np.nan\ndel df[0]\n"], "link": "https://stackoverflow.com/questions/50529459/append-rows-to-groups-in-pandas"}
{"id": 24, "q": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas", "d": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance!", "q_apis": "replace replace all value any last value", "io": " L1 D1 L2 D2 L3 1.0 ABC 1.1 4.1 NaN NaN NaN 1.7 NaN NaN NaN 4.1 NaN NaN NaN NaN 1.8 3.2 PQR NaN NaN NaN 1.6 NaN NaN <s> L1 D1 L2 D2 L3 1.0 ABC 1.1 4.1 - NaN NaN 1.7 - - NaN 4.1 - - - NaN 1.8 3.2 PQR - NaN NaN 1.6 - - ", "apis": "loc notna cumsum eq mask", "code": ["minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"], "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas"}
{"id": 88, "q": "transform a pandas dataframe in a pandas with multicolumns", "d": "I have the following pandas dataframe, where the columna is the dataframe index And i want to convert this datframe in to a multi column data frame, that looks like this I've tried transforming my old pandas dataframe in to a dict this way: But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome.", "q_apis": "transform where index", "io": "+----+-----------+------------+-----------+------------+ | | price_A | amount_A | price_B | amount_b | |----+-----------+------------+-----------+------------| | 0 | 0.652826 | 0.941421 | 0.823048 | 0.728427 | | 1 | 0.400078 | 0.600585 | 0.194912 | 0.269842 | | 2 | 0.223524 | 0.146675 | 0.375459 | 0.177165 | | 3 | 0.330626 | 0.214981 | 0.389855 | 0.541666 | | 4 | 0.578132 | 0.30478 | 0.789573 | 0.268851 | | 5 | 0.0943601 | 0.514878 | 0.419333 | 0.0170096 | | 6 | 0.279122 | 0.401132 | 0.722363 | 0.337094 | | 7 | 0.444977 | 0.333254 | 0.643878 | 0.371528 | | 8 | 0.724673 | 0.0632807 | 0.345225 | 0.935403 | | 9 | 0.905482 | 0.8465 | 0.585653 | 0.364495 | +----+-----------+------------+-----------+------------+ <s> +----+-----------+------------+-----------+------------+ | | A | B | +----+-----------+------------+-----------+------------+ | id | price | amount | price | amount | |----+-----------+------------+-----------+------------| | 0 | 0.652826 | 0.941421 | 0.823048 | 0.728427 | | 1 | 0.400078 | 0.600585 | 0.194912 | 0.269842 | | 2 | 0.223524 | 0.146675 | 0.375459 | 0.177165 | | 3 | 0.330626 | 0.214981 | 0.389855 | 0.541666 | | 4 | 0.578132 | 0.30478 | 0.789573 | 0.268851 | | 5 | 0.0943601 | 0.514878 | 0.419333 | 0.0170096 | | 6 | 0.279122 | 0.401132 | 0.722363 | 0.337094 | | 7 | 0.444977 | 0.333254 | 0.643878 | 0.371528 | | 8 | 0.724673 | 0.0632807 | 0.345225 | 0.935403 | | 9 | 0.905482 | 0.8465 | 0.585653 | 0.364495 | +----+-----------+------------+-----------+------------+ ", "apis": "columns MultiIndex from_tuples columns index name", "code": ["df.columns=pd.MultiIndex.from_tuples([x.split('_')[::-1] for x in df.columns])\ndf.index.name='id'\n"], "link": "https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns"}
{"id": 339, "q": "How to drop pandas consecutive column by column name simultaneously?", "d": "Here's my data The Output I expected, What I did But this is not efficient, how to do this effectively?", "q_apis": "drop name", "io": "Id Column1 Column2 Column3 Column4 .... Column112 Column113 ... Column143 1 67 89 86 43 56 72 67 <s> Id Column1 Column113 ... Column143 1 67 72 67 ", "apis": "drop loc columns", "code": ["df1 = df.drop(df.loc[:, 'Column2':'Column112'].columns, axis=1)\n"], "link": "https://stackoverflow.com/questions/60502832/how-to-drop-pandas-consecutive-column-by-column-name-simultaneously"}
{"id": 411, "q": "Pandas: Drop duplicates based on row value", "d": "I have a dataframe and I want to drop duplicates based on different conditions.... I want to drop all the duplicates from column A except rows with \"-\". After this, I want to drop duplicates from column A with \"-\" as a value based on their column B value. Given the input dataframe, this should return the following:- I have the following code but it's not very efficient for very large amounts of data, how can I improve this....", "q_apis": "value drop drop all drop value value", "io": " A B 0 1 1.0 1 1 1.0 2 2 2.0 3 2 2.0 4 3 3.0 5 4 4.0 6 5 5.0 7 - 5.1 8 - 5.1 9 - 5.3 <s> A B 0 1 1.0 2 2 2.0 4 3 3.0 5 4 4.0 6 5 5.0 7 - 5.1 9 - 5.3 ", "apis": "duplicated eq duplicated", "code": ["df[~df.duplicated('A')            # keep those not duplicates in A\n   | (df['A'].eq('-')             # or those '-' in A\n      & ~df['B'].duplicated())]   # which are not duplicates in B\n"], "link": "https://stackoverflow.com/questions/58490071/pandas-drop-duplicates-based-on-row-value"}
{"id": 290, "q": "How can I find and store how many columns it takes to reach a value greater than the first value in each row?", "d": "Original dataframe is df1. For each row, I want to find the first time a value is bigger than the value in the first column and store it in a new dataframe. df2 is the resulting dataframe. For example, for df1 row 1; the first value is 3 and the first value bigger than 3 is 4 (column c). Hence in df2 row 1, we store 2 (there are two columns from column a to c). For df1 row 2, the first value is 4 and the first value bigger than 4 is 5 (column d). Hence in df2 row 2, we store 3 (there are three columns from column a to d). For df1 row 3, the first value is 5 and the first value bigger than 5 is 6 (column e). Hence in df2 row 3, we store 4 (there are four columns from column a to e). I would appreciate the help.", "q_apis": "columns value first value first time value value first first value first value columns first value first value columns first value first value columns", "io": "df1 a b c d e 0 3 1 4 1 9 1 4 2 3 5 4 2 5 3 3 4 6 <s> df2 b 0 2 1 3 2 4 ", "apis": "columns get_indexer drop sub ge idxmax array", "code": ["s=df1.columns.get_indexer(df1.drop('a',1).sub(df1.a,0).ge(0).idxmax(1))\narray([1, 1, 3])\ndf['New']=s\n"], "link": "https://stackoverflow.com/questions/61979568/how-can-i-find-and-store-how-many-columns-it-takes-to-reach-a-value-greater-than"}
{"id": 459, "q": "Pandas: Keep values in a set of columns if they exist in another set of columns in the same row, otherwise set it to NaN", "d": "I have a couple of columns in my dataframe that have values in them. I want to only keep those values in those columns if they exist in another set of columns in the same row. Otherwise, I want to set the value to . Here's an example dataframe: In this case, I want and to be changed based on and : It's been difficult to form a query to google this, and the closest I've gotten is to use like this: Which gives me this: Which appears to be somewhat useful, but I'm not sure if this is the right path or what to do with it from here.", "q_apis": "values columns columns columns values values columns columns value query right", "io": " A B C D 0 1 30 1 29 1 5 42 99 5 2 64 67 12 22 3 2 22 22 0 4 43 6 9 43 <s> A B C D 0 1 30 1.0 NaN 1 5 42 NaN 5.0 2 64 67 NaN NaN 3 2 22 22.0 NaN 4 43 6 NaN 43.0 ", "apis": "map get", "code": ["df[['C', 'D']] = [\n    (c if c in ab else np.nan, d if d in ab else np.nan)\n    for *ab, c, d in zip(*map(df.get, ['A', 'B', 'C', 'D']))\n]\n"], "link": "https://stackoverflow.com/questions/56810353/pandas-keep-values-in-a-set-of-columns-if-they-exist-in-another-set-of-columns"}
{"id": 129, "q": "put only elements into a list with a certian number", "d": "is the sales ID and is the sold itemid. I would like to use all unique i_ids to find all purchases that have interacted with i_id. I also implemented this in the loop. What I would like that I only want to add something to the list when the has more of a 1 item. How do I do that so that I only add the purchases to the list if it contains more than one item? Output But what I want means that the element does not exist, I only wrote for a better understanding", "q_apis": "put all unique all add item add contains item", "io": "[[1], [1, 2, 3], [1], [4, 1, 2]] [[1, 2, 3], [4, 1, 2]] [[1, 2, 3], [3, 5]] [[4, 1, 2]] [[3, 5]] <s> [[REMOVED], [1, 2, 3], [REMOVED], [4, 1, 2]] [[1, 2, 3], [4, 1, 2]] [[1, 2, 3], [3, 5]] [[4, 1, 2]] [[3, 5]] ", "apis": "mean std mean std", "code": ["6.71 ms \u00b1 91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)", "12.3 ms \u00b1 201 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)"], "link": "https://stackoverflow.com/questions/65642545/put-only-elements-into-a-list-with-a-certian-number"}
{"id": 357, "q": "Multi-column to single column in Pandas", "d": "I have the following data frame : And I need this : This is just a basic example, the real deal can have over 60 children.", "q_apis": "", "io": " parent 0 1 2 3 0 14026529 14062504 0 0 0 1 14103793 14036094 0 0 0 2 14025454 14036094 0 0 0 3 14030252 14030253 14062647 0 0 4 14034704 14086964 0 0 0 <s> parent_id child_id 0 14026529 14062504 1 14025454 14036094 2 14030252 14030253 3 14030252 14062647 4 14103793 14036094 5 14034704 14086964 ", "apis": "astype where ne set_index stack reset_index name", "code": ["(df.astype('Int64').where(df.ne(0))\n .set_index('parent')\n .stack()\n .reset_index(level=0, name='child'))\n"], "link": "https://stackoverflow.com/questions/60042289/multi-column-to-single-column-in-pandas"}
{"id": 405, "q": "Groupby and perform row-wise calculation using a custom function", "d": "Following on from this question: python - Group by and add new row which is calculation of other rows I have a pandas dataframe as follows: And I want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to X and Z from col_2 and create a new row with these values. So the output would be as below: Where are the outputs of the function. Original question (which only requires a simple addition) was answered with: I'm now looking for a way to use a custom function, such as or , rather than . How can I modify this code to work with my new requirements?", "q_apis": "add value apply values columns values now", "io": "col_1 col_2 col_3 col_4 a X 5 1 a Y 3 2 a Z 6 4 b X 7 8 b Y 4 3 b Z 6 5 <s> col_1 col_2 col_3 col_4 a X 5 1 a Y 3 2 a Z 6 4 a NEW * * b X 7 8 b Y 4 3 b Z 6 5 b NEW * * ", "apis": "values replace", "code": ["def f(x):\n    y = x.values\n    return y[0] / y[1] # replace with your function\n"], "link": "https://stackoverflow.com/questions/46451284/groupby-and-perform-row-wise-calculation-using-a-custom-function"}
{"id": 249, "q": "How can I remove a certain type of values in a group in pandas?", "d": "I have the following dataframe which is a small part of a bigger one: I'd like to delete all rows where the last items are \"d\". So my desired dataframe would look like this: So the point is, that a group shouldn't have \"d\" as the last item. There is a code that deletes the last row in the groups where the last item is \"d\". But in this case, I have to run the code twice to delete all last \"d\"-s in group 3 for example. Is there a better solution to this problem?", "q_apis": "values delete all where last items last item last groups where last item delete all last", "io": " acc_num trans_cdi 0 1 c 1 1 d 3 3 d 4 3 c 5 3 d 6 3 d <s> acc_num trans_cdi 0 1 c 3 3 d 4 3 c ", "apis": "shift iloc iloc", "code": ["df = df[~((df['trans_cdi'] == 'd') & (df.shift(1)['trans_cdi'] == 'd'))]\nif df['trans_cdi'].iloc[-1] == 'd': df = df.iloc[0:-1]\ndf\n"], "link": "https://stackoverflow.com/questions/62967408/how-can-i-remove-a-certain-type-of-values-in-a-group-in-pandas"}
{"id": 595, "q": "Transform a large dataframe - takes too long", "d": "I have a dataframe loaded from a CSV in the following format: I want to transform it to the following format: This is my function ( is the original data frame) that does the transformation, but it takes 7 minutes for 547500 row dataframe. Is there a way to speed it up?", "q_apis": "transform", "io": " stock_code price 20180827 001 10 20180827 002 11 20180827 003 12 20180827 004 13 20180826 001 14 20180826 002 15 20180826 003 11 20180826 004 10 20180826 005 19 <s> 001 002 003 004 005 20180827 10 11 12 13 nan 20180826 14 15 11 10 19 ", "apis": "pivot index", "code": ["df = pd.pivot(df.index, df['stock_code'], df['price'])\n"], "link": "https://stackoverflow.com/questions/52033359/transform-a-large-dataframe-takes-too-long"}
{"id": 261, "q": "Drop last n rows within pandas dataframe groupby", "d": "I have a dataframe where I want to drop last rows within a group of columns. For example, say is defined as below the group is of columns and : Desired output for is as follows: Desired output for is as follows:", "q_apis": "last groupby where drop last columns columns", "io": ">>> df a b c d 0 abd john 0 1000 1 abd john 1 1001 4 pqr john 4 1004 9 xyz doe 9 1009 10 xyz doe 10 1010 11 xyz doe 11 1011 12 xyz doe 12 1012 13 xyz doe 13 1013 >>> <s> >>> df a b c d 0 abd john 0 1000 9 xyz doe 9 1009 10 xyz doe 10 1010 11 xyz doe 11 1011 12 xyz doe 12 1012 >>> ", "apis": "drop groupby tail index", "code": ["n = 2\ndf.drop(df.groupby(['a','b']).tail(n).index, axis=0)\n"], "link": "https://stackoverflow.com/questions/62882426/drop-last-n-rows-within-pandas-dataframe-groupby"}
{"id": 83, "q": "Python transposing multiple dataframes in a list", "d": "I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you.", "q_apis": "columns item get transpose get any", "io": "0 email factor1_final factor2_final factor3_final 1 john@abc.com 85% 90% 50% 2 peter@abc.com 80% 60% 60% 3 shelby@abc.com 50% 70% 60% 4 jess@abc.com 60% 65% 50% 5 mark@abc.com 98% 50% 60% <s> email john@abc.com peter@abc.com shelby@abc.com jess@abc.com mark@abc.com factor1 85% 80% 50% 60% 98% factor2 90% 60% 70% 65% 50% factor3 50% 60% 60% 50% 60% ", "apis": "set_index T", "code": ["dfs = [df.set_index('email').T for df in df_list]\n"], "link": "https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list"}
{"id": 451, "q": "pandas - downsample a more frequent DataFrame to the frequency of a less frequent DataFrame", "d": "I have two DataFrames that have different data measured at different frequencies, as in those csv examples: df1: df2: I would like to obtain a single df that have all the measures of both dfs at the times of the first one (which get data less frequently). I tried to do that with a for loop averaging over the df2 measures between two timestamps of df1 but it was extremely slow.", "q_apis": "DataFrame DataFrame at all at first get between", "io": "i,m1,m2,t 0,0.556529,6.863255,43564.844 1,0.5565576199999884,6.86327749999999,43564.863999999994 2,0.5565559400000003,6.8632764,43564.884 3,0.5565699799999941,6.863286799999996,43564.903999999995 4,0.5565570200000007,6.863277200000001,43564.924 5,0.5565316400000097,6.863257100000007,43564.944 ... <s> i,m3,m4,t 0,306.81162500000596,-1.2126870045404683,43564.878125 1,306.86175000000725,-1.1705838272666433,43564.928250000004 2,306.77552454544787,-1.1240195386446195,43564.97837499999 3,306.85900545454086,-1.0210345363692084,43565.0285 4,306.8354250000052,-1.0052431772666657,43565.078625 5,306.88397499999286,-0.9468344809917896,43565.12875 ... ", "apis": "groups cut right copy columns groupby get groupby groups mean reset_index", "code": ["groups =pd.cut(df2.t, bins= list(df1.t) + [np.inf],\n               right=False,\n               labels=df1['t'])\n\n# cols to copy\ncols = [col for col in df2.columns if col != 't']\n\n# groupby and get the average\nnew_df = (df2[cols].groupby(groups)\n                   .mean()\n                   .reset_index()\n         )\n"], "link": "https://stackoverflow.com/questions/57043695/pandas-downsample-a-more-frequent-dataframe-to-the-frequency-of-a-less-frequen"}
{"id": 259, "q": "Merge two columns of a dataframe into an already existing column of dictionaries as a key value pair", "d": "If we have 3 columns of a dataframe as : I want the column3 to be something like : I have tried a few things from using lambda functions with apply to iterating over rows but all were unsuccessful.", "q_apis": "columns value columns apply all", "io": "column1 : ['A','A','B','C'] column2 : [12,13,14,15] column3 : [{\"key1\":\"val1\"},{\"key2\":\"val2\"},{\"key3\":\"val3\"},{\"key4\":\"val4\"}] <s> column3 : [{\"key1\":\"val1\", \"A\":12},{\"key2\":\"val2\", \"A\":13},{\"key3\":\"val3\", \"B\":14},{\"key4\":\"val4\", \"C\":15}] ", "apis": "values", "code": ["df['col3'] = [{**d, k:v} for k,v,d in df.values.tolist()]\n"], "link": "https://stackoverflow.com/questions/62912823/merge-two-columns-of-a-dataframe-into-an-already-existing-column-of-dictionaries"}
{"id": 567, "q": "Swap contents of columns inside dataframe", "d": "I have a pandas dataframe with this contents; I would like to swap the contents between Column 1 and Column 2. The output dataframe should look like this; I am using python v3.6", "q_apis": "columns between", "io": "Column1 Column2 Column3 C11 C21 C31 C12 C22 C32 C13 C23 C33 <s> Column1 Column2 Column3 C21 C11 C31 C22 C12 C32 C23 C13 C33 ", "apis": "DataFrame columns", "code": ["df = pd.DataFrame({\"Column1\": [\"C11\", \"C12\", \"C13\"],\n                   \"Column2\": [\"C21\", \"C22\", \"C23\"],\n                   \"Column3\": [\"C31\", \"C32\", \"C33\"]})\ndf.columns = [\"Column2\", \"Column1\", \"Column3\"]\ndf[[\"Column1\", \"Column2\", \"Column3\"]]\n"], "link": "https://stackoverflow.com/questions/52107572/swap-contents-of-columns-inside-dataframe"}
{"id": 480, "q": "How does pandas convert one column of data into another?", "d": "I have a dataframe generated by pandas, as follows\uff1a I want to convert the CODE column data to get the NUM column. The encoding rules are as follows: thank you\uff01", "q_apis": "get", "io": "NO CODE 1 a 2 a 3 a 4 a 5 a 6 a 7 b 8 b 9 a 10 a 11 a 12 a 13 b 14 a 15 a 16 a <s> NO CODE NUM 1 a 1 2 a 2 3 a 3 4 a 4 5 a 5 6 a 6 7 b b 8 b b 9 a 1 10 a 2 11 a 3 12 a 4 13 b b 14 a 1 15 a 2 16 a 3 ", "apis": "eq where groupby ne shift cumsum cumcount", "code": ["a_group = df.CODE.eq('a')\n\ndf['NUM'] = np.where(a_group, \n                     df.groupby(a_group.ne(a_group.shift()).cumsum())\n                       .CODE.cumcount()+1, \n                     df.CODE)\n"], "link": "https://stackoverflow.com/questions/56086666/how-does-pandas-convert-one-column-of-data-into-another"}
{"id": 594, "q": "Add a different item from a list to each cell in a dataframe with Pandas", "d": "Given a dataframe I want to make a list of sequential that has as many elements as there are rows in the dataframe And then add each element of the list (as a string) onto the end of a single column in the dataframe. Does not work, as it adds the whole list as a string to each value as opposed to one value of the list per value in the dataframe. Essentially, I want to transform to So anyway to do that would be fine. Thanks for the help!", "q_apis": "item add value value value transform", "io": "A B C 23 16 85 9 74 12 99 24 83 <s> A B C 231 16 85 92 74 12 993 24 83 ", "apis": "astype reset_index index astype astype RangeIndex astype", "code": ["df['A'] = df.A.astype(str) + (df.reset_index().index + 1).astype(str)\n", "df['A'] = df.A.astype(str) + (pd.RangeIndex(len(df)) + 1).astype(str)\n"], "link": "https://stackoverflow.com/questions/52064324/add-a-different-item-from-a-list-to-each-cell-in-a-dataframe-with-pandas"}
{"id": 377, "q": "Pandas: Swap rows between columns", "d": "Some rows were input in the wrong columns so now I need to swap them. My current approach Expected output It works but this is only possible because it was 4 columns. Is there a better way to do this? Note the dtypes can cause issues with sorting is Maybe there is something like", "q_apis": "between columns columns now columns dtypes", "io": " a b c d 0 0 10 22:58:00 23:27:00 1 10 17 23:03:00 23:39:00 2 22:58:00 23:27:00 0 10 3 23:03:00 23:39:00 10 17 <s> a b c d 0 0 10 22:58:00 23:27:00 1 10 17 23:03:00 23:39:00 2 0 10 22:58:00 23:27:00 3 10 17 23:03:00 23:39:00 ", "apis": "DataFrame to_numpy", "code": ["AttributeError: 'DataFrame' object has no attribute 'to_numpy'"], "link": "https://stackoverflow.com/questions/59505475/pandas-swap-rows-between-columns"}
{"id": 121, "q": "Turn columns&#39; values to headers of columns with values 1 and 0 ( accordingly) [python]", "d": "I got a column of the form : The column represents the answers of users to a question of 5 choices (1-5). I want to turn this into a matrix of 5 columns where the indexes are the 5 possible answers and the values are 1 or 0 according to the user's given answer. Visualy i want a matrix of the form:", "q_apis": "columns values columns values columns where values", "io": "0 q4 1 4 2 3 3 1 4 2 5 1 6 5 7 1 8 3 <s> 0 q4_1 q4_2 q4_3 q4_4 q4_5 1 Nan Nan Nan 1 Nan 2 Nan Nan 1 Nan Nan 3 1 Nan Nan Nan Nan 4 Nan 1 Nan Nan Nan 5 1 Nan Nan Nan Nan ", "apis": "where", "code": ["for i in range(1,6):\n    df['q4_'+str(i)]=np.where(df.q4==i, 1, 0)\n\ndef df['q4']\n"], "link": "https://stackoverflow.com/questions/65739584/turn-columns-values-to-headers-of-columns-with-values-1-and-0-accordingly-p"}
{"id": 345, "q": "Use duplicated values to increment column", "d": "I have a Pandas dataframe and I want to increment a column based on the amount of duplicated values. So when a duplicate is found, all other occurrences is incremented. So given this input dataframe return I tried this line of code but I don't know how to increment", "q_apis": "duplicated values duplicated values all", "io": " SM 0 AB 1 AC 2 AD 3 AB 4 AB 5 AC 6 AE 7 AD <s> SM DM 0 AB AB 1 AC AC 2 AD AD 3 AB AB_1 4 AB AB_2 5 AC AC_1 6 AE AE 7 AD AD_1 ", "apis": "groupby cumcount where eq astype", "code": ["s = df.groupby('SM').cumcount()\n\ndf['DM'] = df['SM'].where(s.eq(0), df['SM'] + '_' + s.astype(str))\n"], "link": "https://stackoverflow.com/questions/60336550/use-duplicated-values-to-increment-column"}
{"id": 270, "q": "Append list to list in specific cell in pandas", "d": "I have a pandas dataframe, which looks like this: And I want to append a new list c = [5,6] to the row, where So the result would be: Right now my attempt looks like this: Any help is appreciated!", "q_apis": "append where now", "io": " key arr a 't1' [1,2] b 't2' [3,4] <s> key arr a 't1' [1,2,5,6] b 't2' [3,4] ", "apis": "loc Series repeat sum index index", "code": ["m = df['key'] == 't1'\n\ndf.loc[m, 'arr'] += pd.Series(np.repeat([c], m.sum(), axis=0).tolist(), index=df.index[m])\n"], "link": "https://stackoverflow.com/questions/62696143/append-list-to-list-in-specific-cell-in-pandas"}
{"id": 70, "q": "Group a dataframe on one column and take max from one column and its corresponding value from the other col", "d": "I have a large dataframe which has a similar pattern as below: And can be constructed as: Now I want to group this dataframe by the first column i.e., and take from the column and its corresponding value from . And if there are two max values in , then I would like to take alphabetically first value from . So my expected result would look like: I have tried but this selects max from and first from both at the same time. Additionally I know there is a approach, but this would take a lot of time for my dataset. Any suggestions on how could I proceed would be appreciated. Thanks in advance:)", "q_apis": "take max value first take value max values take first value max first at time take time", "io": " X Y Z 0 a p 2 1 a q 5 2 a r 6 3 a s 3 4 b w 10 5 b z 20 6 b y 9 7 b x 20 <s> df = { 'X': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], 'Y': ['p', 'q', 'r', 's', 'w', 'x', 'y', 'z'], 'Z': [2, 5, 6, 3, 10, 20, 9, 5] } ", "apis": "sort_values drop_duplicates", "code": ["df.sort_values(['X', 'Z', 'Y'], ascending=[True, False, True]).drop_duplicates('X')\n"], "link": "https://stackoverflow.com/questions/66638218/group-a-dataframe-on-one-column-and-take-max-from-one-column-and-its-correspondi"}
{"id": 19, "q": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?", "d": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result:", "q_apis": "name name columns values value concat value all values columns", "io": "(a) (b) (c) (d) a1 b1 c1 d1 a2 b2 c2 d2 <s> (a) (b) (e) (f) a3 b3 e1 f1 a4 b4 e1 f1 a5 b5 e2 f2 a6 b6 e4 f4 a7 a8 e4 f5 ", "apis": "ExcelWriter ExcelWriter read_excel items drop concat apply join groupby concat apply drop_duplicates last concat to_excel index", "code": ["import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"], "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da"}
{"id": 596, "q": "python subplot plot.bar from one dataframe and legend from a different dataframe", "d": "I have two data sets below Df1: I draw a bar diagram. (This is not an exact code of mine, but it will give you an idea) Now I want a legend based on a second data set of centroids. Df2: HPE subplot should have HPE column values (19.282091,1790.500000,1500.000000) as below. How can I do that?", "q_apis": "plot second values", "io": " Cluster HPE FRE UNE 0 0 176617 255282 55881 1 1 126130 7752 252045 2 2 12613 52326 7434 <s> Cluster HPE FRE UNE 0 0 19.282091 106.470162 1620.005037 1 1 1790.500000 367.625000 537.856177 2 2 1500.000000 180.148148 4729.275913 ", "apis": "", "code": ["subp.legend([str(a) + ' - ' + str(b) for a, b in zip(df2['Cluster'].tolist(), df2['HPE'].tolist())])\n"], "link": "https://stackoverflow.com/questions/51902887/python-subplot-plot-bar-from-one-dataframe-and-legend-from-a-different-dataframe"}
{"id": 289, "q": "How to pick some values of a column and make another one with them?", "d": "This is a table similar to the one I'm working with And what I'm trying to do is take some values of the column A that follow a certain pattern and create another column with such values. For example, the column C would have only the values from A that are bigger than 12, and column D the ones smaller or equal: I've tried making a list for each group of values, but I can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. I think there's an esasier way to do that, but I can't seem to find it. How can I do that?", "q_apis": "values take values values values values merge repeat columns", "io": " A B 0 12.2 43 1 10.1 32 2 3.4 34 3 12.0 55 4 40.6 31 <s> A B C D 0 12.2 43 12.2 NaN 1 10.1 32 NaN 10.1 2 3.4 34 NaN 3.4 3 12.0 55 NaN 12.0 4 40.6 31 40.6 NaN ", "apis": "loc loc", "code": ["df['C'] = df.loc[df.A > 12, 'A']\ndf['D'] = df.loc[df.A <= 12, 'A']\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/62011520/how-to-pick-some-values-of-a-column-and-make-another-one-with-them"}
{"id": 448, "q": "Using pandas.interpolate()", "d": "Suppose I would like to apply following command: which returns Question: How can i apply a restriction on the minimum number of valid numbers (i.e not NaN) before AND after a group of NaNs, so as to apply the interpolation In this example, i would like to fill first group of NaNs because there are minimum 3 valid numbers before AND after, but NOT interpolate the second group of NaNs, as there are only two valid numbers after the NaNs (and not 3 as i would prefer) Expected result:", "q_apis": "interpolate apply apply apply first interpolate second", "io": " 0 0 1.000000 1 2.000000 2 3.000000 3 3.166667 4 3.333333 5 NaN 6 3.666667 7 3.833333 8 4.000000 9 5.000000 10 6.000000 11 5.500000 12 5.000000 13 NaN 14 4.000000 15 3.500000 16 3.000000 17 4.000000 18 NaN <s> 0 0 1.000000 1 2.000000 2 3.000000 3 3.166667 4 3.333333 5 NaN 6 3.666667 7 3.833333 8 4.000000 9 5.000000 10 6.000000 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 3.000000 17 4.000000 18 NaN ", "apis": "copy array DataFrame values size notnull mask mask mask start stop indices mask mask mask mask size size DataFrame interpolate update", "code": ["    import numpy as np\n    import pandas as pd\n    from copy import deepcopy\n\n    a = np.array([1,2,3,np.nan,np.nan,np.nan,np.nan,np.nan,4,5,6,np.nan,np.nan,np.nan,np.nan,np.nan,3,4,np.nan,1])\n\n    df = pd.DataFrame(a)\n    # store values for later, to keep information from blocks that are below size limit:\n    temp = deepcopy(df[df[0].notnull()]) \n\n    mask = np.concatenate(([False],np.isfinite(a),[False]))\n    idx = np.nonzero(mask[1:] != mask[:-1])[0] # start and stop indices of your blocks of finite numbers\n    counts = (np.flatnonzero(mask[1:] < mask[:-1]) - np.flatnonzero(mask[1:] > mask[:-1])) # n finite numbers per block\n\n    sz_limit = 2 # set limit, exclusive in this case\n    for i, size in enumerate(counts):\n        if size <= sz_limit:\n            a[idx[i*2]:idx[i*2+1]] = np.nan\n\n", "\n    a_inter = pd.DataFrame(a).interpolate(limit = 2, limit_direction = 'both', limit_area = 'inside') \n    a_inter.update(other = temp)  \n\n"], "link": "https://stackoverflow.com/questions/57143033/using-pandas-interpolate"}
{"id": 4, "q": "Compare two columns that contains timestamps in pandas", "d": "Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0", "q_apis": "columns contains compare timestamp columns timestamp timestamp columns", "io": " Col0 Col1 Col2 Col3 Col4 1.txt 2021-06-23 15:04:30 2021-06-23 14:10:30 2021-06-23 14:15:30 2021-06-23 14:20:30 2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30 2021-06-23 14:35:30 2021-06-23 14:40:30 <s> Col0 Col1 Col2 Col3 Col4 1.txt 2021-06-23 15:04:30 NaN NaN NaN 2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30 NaN NaN ", "apis": "select_dtypes mask lt shift cumsum astype bool loc columns", "code": ["dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n"], "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas"}
{"id": 342, "q": "remove unnamed colums pandas dataframe", "d": "i'm a student and have a problem that i cant figure it out how to solve it.i have csv data like this : code for reading csv like this : SMT print out : expected output : i already trying but it will be like this : i already trying to put or //is not working, and the last time I tried it like this : and i got i just want to get rid the Unnamed: 5 ~ Unnamed: 8, how the correct way to get rid of this Unnamed thing ?", "q_apis": "put last time get get", "io": " 1 2 3 4 6 7 8 9 11 12 13 14 <s> 1 2 3 4 0 6 7 8 9 1 11 12 13 14 2 0 0 0 0 ", "apis": "read_csv test names", "code": ["pd.read_csv(\"test.csv\", usecols=(5,6,7,8), skiprows=3, nrows=3, header=0, names=[\"c1\", \"c2\", \"c3\", \"c4\"])\n"], "link": "https://stackoverflow.com/questions/60440882/remove-unnamed-colums-pandas-dataframe"}
{"id": 335, "q": "Using values from dataframe for calculation", "d": "I am selecting dedicated data from a dataframe and would like to make a linear interpolation based on my defined formula: I would like to interpolate e.g. between rank 2.0 and 3.0, where the needed rank is 2.5. The calculation looks like the following: y = -9.080002 + (-9.039993 - (-9.080002))*[(2.5-2)/(3-2)] = -9.059997500000 where the values are defined in the code as the following: The code looks like the following: The result looks like the following: The Excel file looks like the following:", "q_apis": "values interpolate between rank where rank where values", "io": "y = y0 + (y1 - y0) * [(x-x0)/(x1-x0)] <s> -9.080002000000007 2.0 -9.360001000000011 1.0 -9.22000150000001 ", "apis": "index index", "code": ["def Calc(data):\n\n    Rank = 2.5\n    Rank_Up = 3.0\n    Rank_Down = 2.0\n\n    Calculation = 0.0\n\n    check_int = isinstance(Rank, int)\n    if not check_int:\n        for ind in data.Rank:\n            if (ind in (Rank_Down, Rank_Up)):\n                index0 = data.Rank.index[ind-1]\n                index1 = data.Rank.index[ind]\n                Calculation = data['DataPoint'][index0] + (data['DataPoint'][index1]- data['DataPoint'][index0]) *((Rank-Rank_Down)/(Rank_Up-Rank_Down))\n                print(data['DataPoint'][index0], ind)\n                print(data['DataPoint'][index1], ind+1)\n                break\n\n    return Calculation\n"], "link": "https://stackoverflow.com/questions/60578553/using-values-from-dataframe-for-calculation"}
{"id": 222, "q": "How to average DataFrame row with another row only if the first row is a substring of other next row", "d": "I have a dataframe called 'data': I want to combine 'ABC-1' with 'ABC-1B' into a single row using the first USER name and then averaging the two values to arrive here: The dataframe may not be in order and there are other values in there as well that are unrelated that don't need averaging. I only want to average the two rows where 'XXX-X' is in 'XXX-XB'", "q_apis": "DataFrame first combine first name values values where", "io": "USER VALUE XOXO 21 ABC-1 2 ABC-1B 4 ABC-2 4 ABC-2B 6 PEPE 12 <s> USER VALUE XOXO 21 ABC-1 3 ABC-2 5 PEPE 12 ", "apis": "replace groupby mean", "code": ["df.USER = df.USER.str.replace('(-\\d)B', r\"\\1\")\ndf = df.groupby(\"USER\", as_index=False, sort=False).VALUE.mean()\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/63621901/how-to-average-dataframe-row-with-another-row-only-if-the-first-row-is-a-substri"}
{"id": 643, "q": "Calculating the duration an event in a time series python", "d": "I have a dataframe as show below: The index is datetime and have column record the rainfall value(unit:mm) in each hour,I would like to calculate the \"Average wet spell duration\", which means the average of continuous hours that exist values (not zero) in a day, so the calculation is and the \"average wet spell amount\", which means the average of sum of the values in continuous hours in a day. The datafame above is just a example, the dataframe which I have have more longer time series (more than one year for example), how can I write a function so it could calculate the two value mentioned above in a better way? thanks in advance! P.S. the values may be NaN, and I would like to just ignore it.", "q_apis": "time index value hour values day sum values day time year value values", "io": "2 + 4 + 1 + 1 + 2 + 5 / 6 (events) = 2.5 (hr) <s> { (14.5 + 15.8) + ( 13.6 + 4.3 + 13.7 + 14.4 ) + (17.2) + (5.3) + (2 + 4)+ (3.9 + 7.2 + 1 + 1 + 10) } / 6 (events) = 21.32 (mm) ", "apis": "columns day value astype bool shift value astype bool cumsum day index normalize day get unique count value count value astype bool groupby day nunique value astype bool groupby day value count map day map day map groupby day value sum value", "code": ["# create helper columns defining contiguous blocks and day\ndf['block'] = (df['value'].astype(bool).shift() != df['value'].astype(bool)).cumsum()\ndf['day'] = df['index'].dt.normalize()\n\n# group by day to get unique block count and value count\nsession_map = df[df['value'].astype(bool)].groupby('day')['block'].nunique()\nhour_map = df[df['value'].astype(bool)].groupby('day')['value'].count()\n\n# map to original dataframe\ndf['sessions'] = df['day'].map(session_map)\ndf['hours'] = df['day'].map(hour_map)\n\n# calculate result\nres = df.groupby(['day', 'hours', 'sessions'], as_index=False)['value'].sum()\nres['duration'] = res['hours'] / res['sessions']\nres['amount'] = res['value'] / res['sessions']\n"], "link": "https://stackoverflow.com/questions/49352279/calculating-the-duration-an-event-in-a-time-series-python"}
