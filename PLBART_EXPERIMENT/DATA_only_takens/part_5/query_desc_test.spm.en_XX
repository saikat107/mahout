▁A ▁better ▁way ▁to ▁map ▁data ▁in ▁multiple ▁datasets , ▁with ▁multiple ▁data ▁mapping ▁rules ▁< s > ▁I ▁have ▁three ▁datasets ▁( , ▁, ▁), ▁and ▁I ▁wish ▁to ▁add ▁a ▁new ▁column ▁called ▁in ▁dataframe , ▁and ▁the ▁value ▁to ▁be ▁added ▁can ▁be ▁retrieved ▁from ▁the ▁other ▁two ▁dataframes , ▁the ▁rule ▁is ▁in ▁the ▁bottom ▁after ▁codes . ▁final _ NN : ▁p pt _ code : ▁her d _ id : ▁Expected ▁output : ▁The ▁rules ▁is : ▁if ▁in ▁final _ NN ▁is ▁not ▁, ▁= ▁in ▁; ▁if ▁in ▁final _ NN ▁is ▁but ▁in ▁is ▁not ▁Null , ▁then ▁search ▁the ▁p pt _ code ▁dataframe , ▁and ▁use ▁the ▁and ▁its ▁corresponding ▁" number " ▁to ▁map ▁and ▁fill ▁in ▁the ▁" Map Value " ▁in ▁; ▁if ▁both ▁and ▁in ▁are ▁and ▁null ▁respectively , ▁but ▁in ▁is ▁not ▁Null , ▁then ▁search ▁dataframe , ▁and ▁use ▁the ▁and ▁its ▁corresponding ▁to ▁fill ▁in ▁the ▁in ▁the ▁first ▁dataframe . ▁I ▁applied ▁a ▁loop ▁through ▁the ▁dataframe ▁which ▁is ▁a ▁slow ▁way ▁to ▁achieve ▁this , ▁as ▁above . ▁But ▁I ▁understand ▁there ▁could ▁be ▁a ▁faster ▁way ▁to ▁do ▁this . ▁Just ▁wondering ▁would ▁anyone ▁help ▁me ▁to ▁have ▁a ▁fast ▁and ▁easier ▁way ▁to ▁achieve ▁the ▁same ▁result ?
▁Create ▁a ▁dataframe ▁from ▁arrays ▁python ▁< s > ▁I ' m ▁try ▁to ▁construct ▁a ▁dataframe ▁( I ' m ▁using ▁Pandas ▁library ) ▁from ▁some ▁arrays ▁and ▁one ▁matrix . ▁in ▁particular , ▁if ▁I ▁have ▁two ▁array ▁like ▁this : ▁And ▁one ▁matrix ▁like ▁this ▁: ▁Can ▁i ▁create ▁a ▁dataset ▁like ▁this ? ▁Maybe ▁is ▁a ▁stupid ▁question , ▁but ▁i ▁m ▁very ▁new ▁with ▁Python ▁and ▁Pandas . ▁I ▁seen ▁this ▁: ▁https :// pandas . py data . org / pandas - docs / version / 0. 2 3.4/ generated / pandas . DataFrame . html ▁but ▁specify ▁only ▁' col ums '. ▁I ▁should ▁read ▁the ▁matrix ▁row ▁for ▁row ▁and ▁paste ▁in ▁my ▁dataset , ▁but ▁I ▁m ▁think ▁that ▁exist ▁a ▁more ▁easy ▁solution ▁with ▁Pandas .
▁Can ▁not ▁make ▁desired ▁pandas ▁dataframe ▁< s > ▁I ▁am ▁trying ▁to ▁make ▁a ▁pandas ▁dataframe ▁using ▁2 ▁param ters ▁as ▁columns . ▁But ▁it ▁makes ▁a ▁dataframe ▁transpose ▁of ▁what ▁I ▁need . ▁I ▁have ▁and ▁as ▁column ▁parameters ▁as ▁follows : ▁This ▁gives ▁the ▁following ▁dataframe : ▁However , ▁I ▁want ▁the ▁dataframe ▁as :
▁How ▁to ▁handle ▁missing ▁data ▁with ▁respect ▁to ▁type ▁of ▁dataset ? ▁< s > ▁I ▁have ▁a ▁dataset ▁where ▁has ▁column ▁types ▁that ▁has ▁type ▁like ▁, ▁. ▁df ▁I ▁want ▁to ▁replace ▁missing ▁value ▁with ▁for ▁each ▁type . ▁Such ▁as - ▁result _ df ▁How ▁can ▁do ▁it ▁with ▁Python ?
▁Python : ▁Convert ▁matrices ▁to ▁permutations ▁table ▁< s > ▁Given ▁a ▁set ▁of ▁ids , ▁I ▁need ▁to ▁get ▁the ▁values ▁from ▁a ▁matrix ▁( time ▁A ▁& ▁B ) ▁for ▁each ▁id ▁combination , ▁and ▁create ▁a ▁dataframe ▁appending ▁the ▁values ▁for ▁all ▁the ▁permutations . ▁I ▁have ▁been ▁able ▁to ▁do ▁it ▁by ▁creating ▁the ▁permutations ▁dataframe ▁and ▁then ▁iterating ▁through ▁it ▁while ▁looking ▁& ▁filling ▁the ▁values . ▁However ▁I ▁need ▁to ▁do ▁this ▁for ▁~ 3 000 ▁ids , ▁not ▁3, ▁and ▁I ▁don ' t ▁know ▁how ▁to ▁do ▁it ▁efficiently . ▁Can ▁I ▁generate ▁a ▁Time ▁A / B ▁dataframe ▁as ▁my ▁example ▁without ▁having ▁to ▁iterate ▁through ▁9 000000 * ▁rows ? ▁I ▁know ▁I ▁shouldn ' t ▁be ▁iterating ▁though ▁a ▁dataframe ▁however ▁I ▁haven ' t ▁found ▁an ▁alternative ▁yet . ▁I ds ▁(3 ): ▁Time ▁A ▁matrix ▁(3 x 3): ▁Time ▁B ▁matrix ▁(3 x 3): ▁Time ▁A / B ▁dataframe ▁(6 ):
▁python / pandas : ▁update ▁a ▁column ▁based ▁on ▁a ▁series ▁holding ▁sums ▁of ▁that ▁same ▁column ▁< s > ▁I ▁have ▁a ▁dataframe ▁with ▁a ▁non - unique ▁col 1 ▁like ▁the ▁following ▁Some ▁of ▁the ▁values ▁of ▁col 1 ▁repeat ▁lots ▁of ▁times ▁and ▁others ▁not ▁so . ▁I ' d ▁like ▁to ▁take ▁the ▁bottom ▁( 80 % / 50% /10 %) ▁and ▁change ▁the ▁value ▁to ▁' other ' ▁ahead ▁of ▁plotting . ▁I ' ve ▁got ▁a ▁series ▁which ▁contains ▁the ▁codes ▁in ▁col 1 ▁( as ▁the ▁index ) ▁and ▁the ▁amount ▁of ▁times ▁that ▁they ▁appear ▁in ▁the ▁df ▁in ▁descending ▁order ▁by ▁doing ▁the ▁following : ▁I ' ve ▁also ▁got ▁my ▁cut - off ▁point ▁( bottom ▁80 %) ▁I ' d ▁like ▁to ▁update ▁col 1 ▁in ▁df ▁with ▁the ▁value ▁' oth ers ' ▁when ▁col 1 ▁appears ▁after ▁the ▁cut Off ▁in ▁the ▁index ▁of ▁the ▁series ▁df 2. ▁I ▁don ' t ▁know ▁how ▁to ▁go ▁about ▁checking ▁and ▁updating . ▁I ▁figured ▁that ▁the ▁best ▁way ▁would ▁be ▁to ▁do ▁a ▁groupby ▁on ▁col 1 ▁and ▁then ▁loop ▁through , ▁but ▁it ▁starts ▁to ▁fall ▁apart , ▁should ▁I ▁create ▁a ▁new ▁groupby ▁object ? ▁Or ▁do ▁I ▁call ▁this ▁as ▁an ▁. apply () ▁for ▁each ▁row ? ▁Can ▁you ▁update ▁a ▁column ▁that ▁is ▁being ▁used ▁as ▁the ▁index ▁for ▁a ▁dataframe ? ▁I ▁could ▁do ▁with ▁some ▁help ▁about ▁how ▁to ▁start . ▁edit ▁to ▁add : ▁So ▁if ▁the ▁' b ' s ▁in ▁col 1 ▁were ▁not ▁in ▁the ▁top ▁20 % ▁most ▁pop ul ous ▁values ▁in ▁col 1 ▁then ▁I ' d ▁expect ▁to ▁see :
▁Assign ▁unique ▁ID ▁to ▁Pandas ▁group ▁but ▁add ▁one ▁if ▁repeated ▁< s > ▁I ▁couldn ' t ▁find ▁a ▁solution ▁and ▁want ▁something ▁faster ▁than ▁what ▁I ▁already ▁have . ▁So , ▁the ▁idea ▁is ▁to ▁assign ▁a ▁unique ▁ID ▁for ▁' fruit ' ▁column , ▁e . g . ▁However , ▁if ▁repeated , ▁add ▁1 ▁to ▁the ▁last ▁result , ▁so ▁that ▁instead ▁of : ▁I ▁will ▁end ▁up ▁with : ▁So ▁it ▁adds ▁up ▁until ▁the ▁end , ▁even ▁if ▁there ▁may ▁only ▁be ▁4 ▁fruits ▁changing ▁their ▁positions . ▁Here ▁is ▁my ▁solution ▁but ▁it ' s ▁really ▁slow ▁and ▁I ▁bet ▁there ▁is ▁something ▁that ▁Pandas ▁can ▁do , ▁inher ently : ▁Any ▁ideas ?
▁How ▁to ▁append ▁ndarray ▁values ▁into ▁dataframe ▁rows ▁of ▁particular ▁columns ? ▁< s > ▁I ▁have ▁a ▁function ▁that ▁returns ▁an ▁like ▁this ▁Now , ▁I ▁have ▁a ▁data ▁frame ▁with ▁columns ▁A , B , C , ..., Z ▁; ▁but ▁the ▁array ▁we ▁are ▁getting ▁has ▁only ▁20 ▁values . ▁Hence ▁I ▁want ▁to ▁find ▁a ▁way ▁such ▁that ▁for ▁every ▁array ▁I ▁get ▁as ▁output , ▁I ▁am ▁able ▁to ▁store ▁it ▁in ▁like ▁this ▁( A , B , W , X , Y , Z ▁are ▁to ▁be ▁left ▁blank ):
▁Pandas : Calculate ▁mean ▁of ▁a ▁group ▁of ▁n ▁values ▁of ▁each ▁columns ▁of ▁a ▁dataframe ▁< s > ▁I ▁have ▁a ▁dataframe ▁of ▁the ▁following ▁type : ▁I ▁want ▁to ▁calculate ▁the ▁mean ▁of ▁the ▁first ▁3 ▁element ▁of ▁each ▁column ▁and ▁then ▁next ▁3 ▁elements ▁and ▁so ▁on ▁and ▁then ▁store ▁in ▁a ▁dataframe . ▁Desired ▁Output - ▁Using ▁Group ▁By ▁was ▁one ▁of ▁the ▁approach ▁I ▁thought ▁of ▁but ▁I ▁am ▁unable ▁to ▁figure ▁out ▁how ▁to ▁use ▁Group ▁by ▁in ▁this ▁case .
▁how ▁to ▁extract ▁each ▁numbers ▁from ▁pandas ▁string ▁column ▁to ▁list ? ▁< s > ▁How ▁to ▁do ▁that ? ▁I ▁have ▁pandas ▁dataframe ▁looks ▁like : ▁I ▁need ▁to ▁transfer ▁this ▁each ▁row ▁to ▁separated ▁list :
▁How ▁to ▁fill ▁elements ▁between ▁intervals ▁of ▁a ▁list ▁< s > ▁I ▁have ▁a ▁list ▁like ▁this : ▁So ▁there ▁are ▁intervals ▁that ▁begin ▁with ▁and ▁end ▁with ▁. ▁How ▁can ▁I ▁replace ▁the ▁values ▁in ▁those ▁intervals , ▁say ▁with ▁1 ? ▁The ▁outcome ▁will ▁look ▁like ▁this : ▁I ▁use ▁in ▁this ▁example , ▁but ▁a ▁general ized ▁solution ▁that ▁can ▁apply ▁to ▁any ▁value ▁will ▁also ▁be ▁great
▁Creating ▁new ▁columns ▁within ▁a ▁dataframe , ▁based ▁on ▁the ▁latest ▁value ▁from ▁previous ▁columns ▁< s > ▁I ' ve ▁just ▁completed ▁a ▁beginner ' s ▁course ▁in ▁python , ▁so ▁please ▁bear ▁with ▁me ▁if ▁the ▁code ▁below ▁doesn ' t ▁make ▁sense ▁or ▁my ▁issue ▁is ▁because ▁of ▁some ▁ro ok ie ▁mistake . ▁I ' ve ▁been ▁trying ▁to ▁put ▁the ▁learning ▁to ▁use ▁by ▁working ▁with ▁colle ge ▁production ▁of ▁N FL ▁players , ▁with ▁a ▁view ▁to ▁understanding ▁which ▁statistics ▁can ▁be ▁predict ive ▁or ▁at ▁least ▁corre late ▁to ▁N FL ▁production . ▁It ▁turns ▁out ▁that ▁there ' s ▁a ▁lot ▁of ▁data ▁out ▁there ▁so ▁I ▁have ▁about ▁200 ▁columns ▁of ▁data ▁for ▁600 ▁odd ▁prospect s ▁from ▁the ▁last ▁20 ▁years ▁( just ▁for ▁running ▁back s ▁so ▁far ). ▁However , ▁one ▁of ▁the ▁problems ▁with ▁this ▁data ▁is ▁that ▁each ▁stat ▁is ▁only ▁provided ▁by ▁the ▁age ▁the ▁prospect ▁was ▁in ▁that ▁season ▁giving ▁me ▁something ▁like ▁this : ▁What ▁I ▁want ▁to ▁do ▁at ▁the ▁moment ▁is ▁to ▁be ▁able ▁to ▁take ▁the ▁last ▁year ▁of ▁colle ge ▁production ▁and ▁put ▁it ▁into ▁a ▁new ▁column ▁( for ▁17 ▁different ▁statistics ). ▁I ' ve ▁therefore ▁defined ▁the ▁following ▁function : ▁Which ▁I ▁think ▁should ▁go ▁backwards ▁through ▁the ▁columns ▁until ▁I ▁find ▁a ▁value ▁which ▁isn ' t ▁NaN , ▁and ▁then ▁take ▁that ▁value ▁as ▁the ▁output . ▁I ' ve ▁then ▁defined ▁the ▁columns ▁via ▁a ▁list : ▁and ▁have ▁then ▁run ▁the ▁function ▁through ▁a ▁for ▁loop ▁based ▁on ▁this ▁list : ▁The ▁result ▁I ' m ▁getting ▁back ▁is ▁a ▁slightly ▁biz ar re ▁one ▁- ▁the ▁for ▁loop ▁appears ▁to ▁work , ▁as ▁all ▁the ▁new ▁columns ▁I ' m ▁expecting ▁are ▁created , ▁however ▁they ▁are ▁only ▁populated ▁with ▁data ▁where ▁the ▁player ▁had ▁an ▁age ▁23 ▁season . ▁The ▁remainder ▁of ▁indexes ▁are ▁filled ▁with ▁' NaN ': ▁This ▁suggests ▁to ▁me ▁that ▁the ▁first ▁' if ' ▁statement ▁in ▁my ▁function ▁is ▁working ▁fine , ▁but ▁that ▁all ▁of ▁the ▁' el if ' ▁statements ▁aren ' t ▁triggering ▁and ▁I ▁can ' t ▁work ▁out ▁why . ▁I ' m ▁wondering ▁whether ▁it ' s ▁because ▁I ▁need ▁to ▁be ▁more ▁explicit ▁about ▁why ▁they ▁would ▁trigger , ▁rather ▁than ▁just ▁relying ▁on ▁a ▁logical ▁test ▁of ▁' if ▁the ▁column ▁is ▁not , ▁not ▁equal ▁to ▁NaN , ▁go ▁to ▁the ▁next ▁one ', ▁or ▁if ▁I ' m ▁misunder standing ▁the ▁elif ▁aspect ▁all ▁together . ▁I ' ve ▁put ▁the ▁whole ▁segment ▁of ▁code ▁in ▁also , ▁just ▁because ▁when ▁I ' ve ▁run ▁into ▁issues ▁so ▁far ▁the ▁problem ▁has ▁often ▁not ▁been ▁where ▁I ▁originally ▁thought . ▁By ▁all ▁means ▁tell ▁me ▁if ▁you ▁think ▁I ' ve ▁gone ▁about ▁this ▁in ▁a ▁weird ▁way ▁- ▁this ▁just ▁seemed ▁like ▁a ▁logical ▁approach ▁to ▁the ▁problem ▁but ▁open ▁to ▁other ▁ways ▁of ▁getting ▁the ▁desired ▁result . ▁Thanks ▁in ▁advance !
▁Extract ▁part ▁of ▁a ▁3 ▁D ▁dataframe ▁< s > ▁I ▁have ▁a ▁3 d ▁dataframe . ▁looks ▁like ▁this : ▁How ▁could ▁I ▁extract ▁only ▁column ▁A ▁& ▁B ▁from ▁every ▁d 1, d 2 ..... ? ▁I ▁desire ▁to ▁take ▁the ▁dataframe ▁like ▁this :
▁Iter ative ▁comparison ▁with ▁pandas ▁< s > ▁I ▁don ' t ▁know ▁to ▁approach ▁this ▁issue . ▁I ▁have ▁a ▁data ▁frame ▁that ▁looks ▁like ▁this ▁What ▁I ▁need ▁to ▁do ▁is ▁to ▁iterate ▁between ▁each ▁row ▁per ▁usuario _ id ▁and ▁check ▁if ▁there ' s ▁a ▁difference ▁between ▁each ▁row , ▁and ▁create ▁a ▁new ▁data ▁set ▁with ▁the ▁row ▁changed ▁and ▁the ▁usuario _ web ▁in ▁charge ▁of ▁this ▁change , ▁to ▁generate ▁a ▁data ▁frame ▁that ▁looks ▁like ▁this : ▁Is ▁there ▁any ▁way ▁to ▁do ▁this ? ▁I ' m ▁working ▁with ▁pandas ▁on ▁python ▁and ▁this ▁dataset ▁could ▁be ▁a ▁little ▁big , ▁let ' s ▁say ▁around ▁10000 ▁rows , ▁sorted ▁by ▁usuario _ id . ▁Thanks ▁for ▁any ▁advice .
▁Check ▁if ▁group ▁contains ▁same ▁value ▁in ▁Pandas ▁< s > ▁I ▁am ▁curious ▁if ▁there ▁is ▁a ▁pre - built ▁function ▁in ▁Pandas ▁to ▁check ▁if ▁all ▁members ▁of ▁a ▁group ▁( factors ▁in ▁a ▁column ) ▁contain ▁the ▁same ▁value ▁in ▁another ▁column . ▁i . e . ▁if ▁my ▁dataframe ▁was ▁similar ▁to ▁below ▁it ▁would ▁return ▁an ▁empty ▁list . ▁However , ▁if ▁my ▁dataframe ▁appeared ▁as ▁such ▁( notice ▁the ▁1 ▁in ▁Col 1): ▁Then ▁the ▁output ▁would ▁be ▁a ▁list ▁containing ▁the ▁object ▁" B " ▁since ▁the ▁group ▁B ▁has ▁different ▁values ▁in ▁Col 1.
▁Re - arr anging ▁a ▁single ▁column ▁of ▁strings ▁based ▁on ▁text ▁containing ▁different ▁dates , ▁by ▁date ▁< s > ▁I ▁am ▁looking ▁to ▁arrange ▁a ▁dataframe ▁by ▁dates , ▁however , ▁the ▁dates ▁are ▁a ▁part ▁of ▁a ▁string ▁within ▁each ▁row . ▁The ▁rows ▁must ▁be ▁re arr anged ▁in ▁order ▁by ▁day . ▁Other ▁solutions ▁from ▁stack ▁overflow ▁show ▁how ▁to ▁sort ▁based ▁on ▁a ▁column ▁of ▁dates ▁alone , ▁this ▁example ▁is ▁different ▁because ▁other ▁information ▁is ▁a ▁part ▁of ▁each ▁string ▁and ▁is ▁mixed ▁with ▁the ▁dates . ▁The ▁dataframe ▁is ▁one ▁column ▁with ▁an ▁index , ▁but ▁the ▁rows ▁are ▁not ▁arr anged ▁in ▁order ▁from ▁the ▁dates ▁contained ▁on ▁the ▁far ▁right ▁side ▁of ▁each ▁string . ▁The ▁score ▁numbers ▁are ▁random ▁and ▁do ▁not ▁require ▁any ▁attention . ▁The ▁expected ▁dataframe ▁should ▁look ▁like ▁this ▁( repeated ▁dates ▁have ▁no ▁preference ▁for ▁order ▁between ▁each ▁other ▁and ▁index ▁doesn ' t ▁matter ). ▁The ▁respective ▁scores ▁must ▁stay ▁with ▁their ▁associated ▁dates . ▁What ▁is ▁a ▁way ▁to ▁do ▁this ?
▁Convert ▁dictionary ▁with ▁sub - list ▁of ▁dictionaries ▁into ▁pandas ▁dataframe ▁< s > ▁I ▁have ▁this ▁code ▁with ▁a ▁dictionary ▁" dict ": ▁The ▁result ▁is : ▁But ▁what ▁I ▁want ▁is : ▁I ▁would ▁like ▁to ▁obtain ▁this , ▁without ▁using ▁loops ▁in ▁python , ▁and ▁by ▁using ▁pandas . ▁Can ▁anyone ▁help ▁me ▁out ? ▁Thanks ▁in ▁advance !
▁Pandas : ▁Create ▁dataframe ▁from ▁data ▁and ▁column ▁order ▁< s > ▁what ▁i ' m ▁asking ▁must ▁be ▁something ▁very ▁easy , ▁but ▁i ▁honestly ▁can ' t ▁see ▁it .... ▁:( ▁I ▁have ▁an ▁array , ▁lets ▁say ▁and ▁i ▁want ▁to ▁put ▁it ▁in ▁a ▁dataframe . ▁I ▁do ▁aim ing ▁for : ▁but ▁i ▁am ▁getting : ▁( notice ▁the ▁dis cre p ancy ▁between ▁column ▁names ▁and ▁data ) ▁I ▁know ▁i ▁can ▁re - arrange ▁the ▁column ▁names ▁order ▁in ▁the ▁dataframe ▁creation , ▁but ▁i ' m ▁trying ▁to ▁understand ▁how ▁it ▁works . ▁Am ▁i ▁doing ▁something ▁wrong , ▁or ▁it ' s ▁normal ▁behaviour ? ▁( why ▁though ?)
▁How ▁to ▁split ▁dataframe ▁made ▁from ▁objects ? ▁< s > ▁I ▁want ▁to ▁split ▁one ▁column ▁pandas ▁dataframe ▁that ▁look ▁like ▁this : ▁into ▁two ▁columns : ▁So ▁it ▁can ▁look ▁like ▁this : ▁But ▁its ▁showing ▁type ▁as :
▁How ▁to ▁read ▁list ▁of ▁json ▁objects ▁from ▁Pandas ▁DataFrame ? ▁< s > ▁I ▁want ▁just ▁want ▁to ▁loop ▁through ▁the ▁array ▁of ▁json ▁objects , ▁and ▁get ▁the ▁values ▁of ▁' box ' ..... ▁I ▁have ▁a ▁DataFrame ▁which ▁looks ▁like ▁this ▁and ▁the ▁column ▁' faces Json ' ▁( dst ype ▁= ▁object ) ▁contain ▁array ▁of ▁json ▁objects ▁which ▁look ▁like ▁this : ▁when ▁i ▁run ▁this ▁code ▁i ▁get ▁this ▁error :
▁Drop ▁a ▁pandas ▁DataFrame ▁row ▁that ▁comes ▁after ▁a ▁row ▁that ▁contains ▁a ▁particular ▁value ▁< s > ▁I ▁am ▁trying ▁to ▁drop ▁all ▁rows ▁that ▁come ▁after ▁a ▁row ▁which ▁has ▁inside ▁the ▁column ▁df : ▁Required ▁output ▁df : ▁Look ▁at ▁the ▁following ▁code : ▁Returns ▁a ▁error ▁message ▁I ▁have ▁tried ▁many ▁different ▁variations ▁of ▁this ▁using ▁different ▁methods ▁like ▁and ▁but ▁I ▁can ' t ▁seem ▁to ▁figure ▁it ▁out ▁anyway . ▁I ▁have ▁also ▁tried ▁truncate : ▁This ▁returns : ▁IndexError : ▁index ▁1 ▁is ▁out ▁of ▁bounds ▁for ▁axis ▁0 ▁with ▁size ▁1
▁Rep lic ate ▁multiple ▁rows ▁of ▁events ▁for ▁specific ▁IDs ▁multiple ▁times ▁< s > ▁I ▁have ▁a ▁call ▁log ▁data ▁made ▁on ▁customers . ▁Which ▁looks ▁something ▁like ▁below , ▁where ▁ID ▁is ▁customer ▁ID ▁and ▁A ▁and ▁B ▁are ▁log ▁attributes : ▁I ▁want ▁to ▁replicate ▁each ▁set ▁of ▁event ▁for ▁each ▁ID ▁based ▁on ▁some ▁slots . ▁For ▁e . g . ▁if ▁slot ▁value ▁is ▁2 ▁then ▁all ▁events ▁for ▁ID ▁" A " ▁should ▁be ▁replic ated ▁slot -1 ▁times . ▁and ▁a ▁new ▁Index ▁should ▁be ▁created ▁indicating ▁which ▁slot ▁does ▁replic ated ▁values ▁belong ▁to : ▁I ▁have ▁tried ▁following ▁solution : ▁it ▁gives ▁me ▁the ▁expected ▁output ▁but ▁is ▁not ▁scal able ▁when ▁slots ▁are ▁increased ▁and ▁number ▁of ▁customers ▁increases ▁in ▁order ▁of ▁10 k . ▁I ▁think ▁its ▁taking ▁a ▁long ▁time ▁because ▁of ▁the ▁loop . ▁Any ▁solution ▁which ▁is ▁vectorized ▁will ▁be ▁really ▁helpful .
▁from ▁two ▁arrays ▁to ▁one ▁dataframe ▁python ▁< s > ▁I ▁am ▁trying ▁to ▁put ▁my ▁values ▁into ▁two ▁arrays ▁and ▁then ▁to ▁make ▁them ▁a ▁dataframe . ▁I ▁am ▁using ▁python , ▁numpy ▁and ▁pandas ▁to ▁do ▁so . ▁my ▁arrays ▁are : ▁and ▁I ▁would ▁like ▁to ▁put ▁them ▁into ▁a ▁pandas ▁dataframe . ▁When ▁I ▁print ▁my ▁dataframe , ▁I ▁would ▁like ▁to ▁see ▁this : ▁How ▁can ▁I ▁do ▁that ? ▁I ▁read ▁some ▁related ▁questions , ▁but ▁I ▁can ' t ▁get ▁it ▁right . ▁One ▁of ▁the ▁errors ▁says ▁that ▁indexes ▁must ▁not ▁be ▁tuples , ▁but , ▁as ▁you ▁can ▁see , ▁I ▁don ' t ▁have ▁tuples
▁Sh if ting ▁and ▁revert ing ▁multiple ▁rows ▁in ▁pandas ▁dataframe ▁< s > ▁I ▁have ▁the ▁following ▁dataframe ▁and ▁wish ▁to ▁shift ▁over ▁the ▁0 ▁values ▁to ▁the ▁right ▁and ▁then ▁revert ▁each ▁row : ▁This ▁is ▁the ▁result ▁I ▁would ▁like ▁to ▁get : ▁I ' ve ▁tried ▁vari us ▁shift ▁and ▁apply ▁combinations ▁without ▁any ▁success . ▁Is ▁there ▁a ▁simple ▁way ▁of ▁achieving ▁this ?
▁Merge ▁pandas ▁dataframes ▁by ▁timestamps ▁< s > ▁I ' ve ▁got ▁a ▁few ▁pandas ▁dataframes ▁indexed ▁with ▁timestamps ▁and ▁I ▁would ▁like ▁to ▁merge ▁them ▁into ▁one ▁dataframe , ▁matching ▁nearest ▁timestamp . ▁So ▁I ▁would ▁like ▁to ▁have ▁for ▁example : ▁What ▁exact ▁timestamp ▁there ▁is ▁going ▁to ▁be ▁in ▁final ▁DataFrame ▁is ▁not ▁important ▁to ▁me . ▁BTW . ▁Is ▁there ▁an ▁easy ▁way ▁to ▁l eter ▁convert ▁" absolute " ▁timestamps ▁into ▁time ▁from ▁start ▁( either ▁in ▁seconds ▁or ▁mil ise conds )? ▁So ▁for ▁this ▁example :
▁Pandas ▁DataFrame ▁from ▁Dictionary ▁< s > ▁Let ' s ▁assume ▁that ▁I ▁have ▁a ▁JSON ▁file ▁like ▁below , ▁and ▁I ▁want ▁to ▁convert ▁this ▁file ▁into ▁a ▁data ▁frame ▁with ▁2 ▁columns : ▁This ▁is ▁what ▁I ▁already ▁tried , ▁but ▁I ▁am ▁unable ▁to ▁create ▁DF ▁and ▁probably ▁there ▁is ▁a ▁more ▁efficient ▁way ▁of ▁doing ▁this ▁task : ▁Expecting ▁output ▁like ▁this ▁for ▁all ▁elements ▁in ▁a ▁Json ▁file ▁Also ▁I ▁want ▁to ▁create ▁a ▁directed ▁graph ▁for ▁each ▁parent ▁node ▁with ▁child ▁nodes ▁as ▁clusters ▁because ▁parent ▁nodes ▁has ▁the ▁same ▁child ▁nodes ▁which ▁are ▁also ▁present ▁in ▁other ▁parent ▁nodes . ▁Thanks ▁in ▁Advance
▁How ▁to ▁manipulate ▁column ▁entries ▁using ▁only ▁one ▁specific ▁output ▁of ▁a ▁function ▁that ▁returns ▁several ▁values ? ▁< s > ▁I ▁have ▁a ▁dataframe ▁like ▁this : ▁and ▁I ▁have ▁a ▁function ▁that ▁returns ▁several ▁values . ▁Here ▁I ▁just ▁use ▁a ▁dummy ▁function ▁that ▁returns ▁the ▁minimum ▁and ▁maximum ▁for ▁a ▁certain ▁input ▁iterable : ▁Now ▁I ▁want ▁to ▁e . g . ▁add ▁the ▁maximum ▁of ▁each ▁column ▁to ▁each ▁value ▁in ▁the ▁respective ▁column . ▁So ▁gives ▁and ▁then ▁yields ▁the ▁desired ▁outcome ▁I ▁am ▁wondering ▁whether ▁there ▁is ▁a ▁more ▁straightforward ▁way ▁that ▁avoids ▁the ▁two ▁chained ▁' s . ▁Just ▁to ▁make ▁sure : ▁I ▁am ▁NOT ▁interested ▁in ▁a ▁type ▁solution . ▁I ▁highlighted ▁the ▁to ▁illustrate ▁that ▁this ▁not ▁my ▁actual ▁function ▁but ▁just ▁serves ▁as ▁a ▁minimal ▁example ▁function ▁that ▁has ▁several ▁outputs .
▁pandas ▁groupby ▁expanding ▁df ▁based ▁on ▁unique ▁values ▁< s > ▁I ▁have ▁below : ▁I ▁want ▁to ▁achieve ▁the ▁following . ▁For ▁each ▁unique ▁, ▁the ▁bottom ▁row ▁is ▁( this ▁is ▁). ▁I ▁want ▁to ▁count ▁how ▁many ▁times ▁each ▁unique ▁value ▁of ▁occurs ▁where ▁. ▁This ▁part ▁would ▁be ▁achieved ▁by ▁something ▁like : ▁However , ▁I ▁also ▁want ▁to ▁add , ▁for ▁each ▁unique ▁value ▁of ▁, ▁a ▁column ▁indicating ▁how ▁many ▁times ▁that ▁value ▁occurred ▁after ▁the ▁point ▁where ▁for ▁the ▁value ▁indicated ▁by ▁the ▁row . ▁I ▁understand ▁this ▁might ▁sound ▁confusing ; ▁here ' s ▁how ▁the ▁output ▁w oud ▁look ▁like ▁in ▁this ▁example : ▁It ▁is ▁important ▁that ▁the ▁solution ▁is ▁general ▁enough ▁to ▁be ▁applicable ▁on ▁a ▁similar ▁case ▁with ▁more ▁unique ▁values ▁than ▁just ▁, ▁and ▁. ▁UPDATE ▁As ▁a ▁bonus , ▁I ▁am ▁also ▁interested ▁in ▁how , ▁instead ▁of ▁the ▁count , ▁one ▁can ▁instead ▁return ▁the ▁sum ▁of ▁some ▁value ▁column , ▁under ▁the ▁same ▁conditions , ▁divided ▁by ▁the ▁corresponding ▁in ▁the ▁rows . ▁Example : ▁suppose ▁we ▁now ▁depart ▁from ▁below ▁instead : ▁The ▁output ▁would ▁need ▁to ▁sum ▁for ▁the ▁cases ▁indicated ▁by ▁the ▁counts ▁in ▁the ▁solution ▁by ▁@ j ez ra el , ▁and ▁divide ▁that ▁number ▁by ▁. ▁The ▁output ▁would ▁instead ▁look ▁like :
▁Add ▁ID ▁found ▁in ▁list ▁to ▁new ▁column ▁in ▁pandas ▁dataframe ▁< s > ▁Say ▁I ▁have ▁the ▁following ▁dataframe ▁( a ▁column ▁of ▁integers ▁and ▁a ▁column ▁with ▁a ▁list ▁of ▁integers )... ▁And ▁also ▁a ▁separate ▁list ▁of ▁IDs ... ▁Given ▁that , ▁and ▁ignoring ▁the ▁column ▁and ▁any ▁index , ▁I ▁want ▁to ▁see ▁if ▁any ▁of ▁the ▁IDs ▁in ▁the ▁list ▁are ▁mentioned ▁in ▁the ▁column . ▁The ▁code ▁I ▁have ▁so ▁far ▁is : ▁This ▁works ▁but ▁only ▁if ▁the ▁list ▁is ▁longer ▁than ▁the ▁dataframe ▁and ▁for ▁the ▁real ▁dataset ▁the ▁list ▁is ▁going ▁to ▁be ▁a ▁lot ▁shorter ▁than ▁the ▁dataframe . ▁If ▁I ▁set ▁the ▁list ▁to ▁only ▁two ▁elements ... ▁I ▁get ▁a ▁very ▁popular ▁error ▁( I ▁have ▁read ▁many ▁questions ▁with ▁the ▁same ▁error )... ▁I ▁have ▁tried ▁converting ▁the ▁list ▁to ▁a ▁series ▁( no ▁change ▁in ▁the ▁error ). ▁I ▁have ▁also ▁tried ▁adding ▁the ▁new ▁column ▁and ▁setting ▁all ▁values ▁to ▁before ▁doing ▁the ▁comprehension ▁line ▁( again ▁no ▁change ▁in ▁the ▁error ). ▁Two ▁questions : ▁How ▁do ▁I ▁get ▁my ▁code ▁( below ) ▁to ▁work ▁for ▁a ▁list ▁that ▁is ▁shorter ▁than ▁a ▁dataframe ? ▁How ▁would ▁I ▁get ▁the ▁code ▁to ▁write ▁the ▁actual ▁ID ▁found ▁back ▁to ▁the ▁column ▁( more ▁useful ▁than ▁True / False )? ▁Expected ▁output ▁for ▁: ▁Ide al ▁output ▁for ▁( ID ( s ) ▁are ▁written ▁to ▁a ▁new ▁column ▁or ▁columns ): ▁Code :
▁How ▁to ▁change ▁values ▁of ▁rows ▁based ▁on ▁conditions ▁in ▁dataframe ? ▁< s > ▁I ▁have ▁dataframe ▁that ▁is ▁shown ▁below , ▁( Need ▁some ▁magic ▁to ▁be ▁done ) ▁The ▁change ▁should ▁be ▁made ▁in ▁type ▁column ▁such ▁that , ▁in ▁a ▁row ▁if ▁is ▁and ▁is ▁then ▁next ▁row ▁of ▁should ▁be ▁assigned ▁. ▁Full ▁dataframe ▁should ▁look ▁like ▁this :
▁Adding ▁value ▁from ▁one ▁pandas ▁dataframe ▁to ▁another ▁dataframe ▁by ▁matching ▁a ▁variable ▁< s > ▁Suppose ▁I ▁have ▁a ▁pandas ▁dataframe ▁with ▁2 ▁columns ▁A ▁second ▁dataframe , ▁contains ▁c 1, ▁c 2 ▁and ▁a ▁few ▁other ▁columns . ▁My ▁goal ▁is ▁to ▁replace ▁the ▁empty ▁values ▁for ▁c 1 ▁in ▁df 2, ▁with ▁those ▁in ▁df , ▁corresponding ▁to ▁the ▁values ▁in ▁c 2, ▁so ▁the ▁first ▁five ▁values ▁for ▁c 1 ▁in ▁df 2, ▁should ▁be ▁v 5, v 2, v 1, v 2 ▁and ▁v 3 ▁respectively . ▁What ▁is ▁the ▁best ▁way ▁to ▁do ▁this ?
▁Copying ▁columns ▁within ▁pandas ▁dataframe ▁< s > ▁I ▁want ▁to ▁slice ▁and ▁copy ▁columns ▁in ▁a ▁Python ▁Dataframe . ▁My ▁data ▁frame ▁looks ▁like ▁the ▁following : ▁I ▁want ▁to ▁make ▁it ▁of ▁the ▁form ▁Which ▁basically ▁means ▁that ▁I ▁want ▁to ▁shift ▁the ▁values ▁in ▁Columns ▁'19 29 ',' 19 29 .1 ',' 19 30 ',' 19 30 .1' ▁under ▁the ▁column ▁'19 28 ' ▁and ▁'19 28 .1' ▁For ▁the ▁same , ▁I ▁wrote ▁the ▁code ▁as ▁No ▁copying ▁takes ▁places ▁within ▁the ▁columns . ▁How ▁shall ▁I ▁modify ▁my ▁code ??
▁Append ▁rows ▁to ▁groups ▁in ▁pandas ▁< s > ▁I ' m ▁trying ▁to ▁append ▁a ▁number ▁of ▁NaN ▁rows ▁to ▁each ▁group ▁in ▁a ▁pandas ▁dataframe . ▁Essentially ▁I ▁want ▁to ▁pad ▁each ▁group ▁to ▁be ▁5 ▁rows ▁long . ▁Ordering ▁is ▁important . ▁I ▁have : ▁I ▁want :
▁replace ▁NaN ▁with ▁& # 39 ; - &# 39 ; ▁sign ▁only ▁in ▁spe ce f ic ▁condition ▁, Python - P andas ▁< s > ▁I ▁have ▁a ▁dataframe ▁I ▁want ▁to ▁replace ▁all ▁the ▁NaN ▁with ▁'-' ▁( only ▁when ▁the ▁value ▁in ▁any ▁column ▁is ▁last ▁value ▁in ▁that ▁row ) ▁so ▁basically ▁my ▁desired ▁output ▁will ▁be ▁Can ▁someone ▁help , ▁Thank ▁you ▁in ▁advance !
▁transform ▁a ▁pandas ▁dataframe ▁in ▁a ▁pandas ▁with ▁mult icol umn s ▁< s > ▁I ▁have ▁the ▁following ▁pandas ▁dataframe , ▁where ▁the ▁column a ▁is ▁the ▁dataframe ▁index ▁And ▁i ▁want ▁to ▁convert ▁this ▁dat frame ▁in ▁to ▁a ▁multi ▁column ▁data ▁frame , ▁that ▁looks ▁like ▁this ▁I ' ve ▁tried ▁transform ing ▁my ▁old ▁pandas ▁dataframe ▁in ▁to ▁a ▁dict ▁this ▁way : ▁But ▁i ▁had ▁no ▁success , ▁can ▁someone ▁give ▁me ▁tips ▁and ▁adv ices ▁on ▁how ▁to ▁do ▁that ? ▁Any ▁help ▁is ▁more ▁than ▁welcome .
▁How ▁to ▁drop ▁pandas ▁consecutive ▁column ▁by ▁column ▁name ▁simultaneously ? ▁< s > ▁Here ' s ▁my ▁data ▁The ▁Output ▁I ▁expected , ▁What ▁I ▁did ▁But ▁this ▁is ▁not ▁efficient , ▁how ▁to ▁do ▁this ▁effectively ?
▁Pandas : ▁Drop ▁duplicates ▁based ▁on ▁row ▁value ▁< s > ▁I ▁have ▁a ▁dataframe ▁and ▁I ▁want ▁to ▁drop ▁duplicates ▁based ▁on ▁different ▁conditions .... ▁I ▁want ▁to ▁drop ▁all ▁the ▁duplicates ▁from ▁column ▁A ▁except ▁rows ▁with ▁"- ". ▁After ▁this , ▁I ▁want ▁to ▁drop ▁duplicates ▁from ▁column ▁A ▁with ▁"-" ▁as ▁a ▁value ▁based ▁on ▁their ▁column ▁B ▁value . ▁Given ▁the ▁input ▁dataframe , ▁this ▁should ▁return ▁the ▁following :- ▁I ▁have ▁the ▁following ▁code ▁but ▁it ' s ▁not ▁very ▁efficient ▁for ▁very ▁large ▁amounts ▁of ▁data , ▁how ▁can ▁I ▁improve ▁this ....
▁How ▁can ▁I ▁find ▁and ▁store ▁how ▁many ▁columns ▁it ▁takes ▁to ▁reach ▁a ▁value ▁greater ▁than ▁the ▁first ▁value ▁in ▁each ▁row ? ▁< s > ▁Original ▁dataframe ▁is ▁df 1. ▁For ▁each ▁row , ▁I ▁want ▁to ▁find ▁the ▁first ▁time ▁a ▁value ▁is ▁bigger ▁than ▁the ▁value ▁in ▁the ▁first ▁column ▁and ▁store ▁it ▁in ▁a ▁new ▁dataframe . ▁df 2 ▁is ▁the ▁resulting ▁dataframe . ▁For ▁example , ▁for ▁df 1 ▁row ▁1; ▁the ▁first ▁value ▁is ▁3 ▁and ▁the ▁first ▁value ▁bigger ▁than ▁3 ▁is ▁4 ▁( column ▁c ). ▁Hence ▁in ▁df 2 ▁row ▁1, ▁we ▁store ▁2 ▁( there ▁are ▁two ▁columns ▁from ▁column ▁a ▁to ▁c ). ▁For ▁df 1 ▁row ▁2, ▁the ▁first ▁value ▁is ▁4 ▁and ▁the ▁first ▁value ▁bigger ▁than ▁4 ▁is ▁5 ▁( column ▁d ). ▁Hence ▁in ▁df 2 ▁row ▁2, ▁we ▁store ▁3 ▁( there ▁are ▁three ▁columns ▁from ▁column ▁a ▁to ▁d ). ▁For ▁df 1 ▁row ▁3, ▁the ▁first ▁value ▁is ▁5 ▁and ▁the ▁first ▁value ▁bigger ▁than ▁5 ▁is ▁6 ▁( column ▁e ). ▁Hence ▁in ▁df 2 ▁row ▁3, ▁we ▁store ▁4 ▁( there ▁are ▁four ▁columns ▁from ▁column ▁a ▁to ▁e ). ▁I ▁would ▁appreciate ▁the ▁help .
▁Pandas : ▁Keep ▁values ▁in ▁a ▁set ▁of ▁columns ▁if ▁they ▁exist ▁in ▁another ▁set ▁of ▁columns ▁in ▁the ▁same ▁row , ▁otherwise ▁set ▁it ▁to ▁NaN ▁< s > ▁I ▁have ▁a ▁couple ▁of ▁columns ▁in ▁my ▁dataframe ▁that ▁have ▁values ▁in ▁them . ▁I ▁want ▁to ▁only ▁keep ▁those ▁values ▁in ▁those ▁columns ▁if ▁they ▁exist ▁in ▁another ▁set ▁of ▁columns ▁in ▁the ▁same ▁row . ▁Otherwise , ▁I ▁want ▁to ▁set ▁the ▁value ▁to ▁. ▁Here ' s ▁an ▁example ▁dataframe : ▁In ▁this ▁case , ▁I ▁want ▁and ▁to ▁be ▁changed ▁based ▁on ▁and ▁: ▁It ' s ▁been ▁difficult ▁to ▁form ▁a ▁query ▁to ▁google ▁this , ▁and ▁the ▁closest ▁I ' ve ▁gotten ▁is ▁to ▁use ▁like ▁this : ▁Which ▁gives ▁me ▁this : ▁Which ▁appears ▁to ▁be ▁somewhat ▁useful , ▁but ▁I ' m ▁not ▁sure ▁if ▁this ▁is ▁the ▁right ▁path ▁or ▁what ▁to ▁do ▁with ▁it ▁from ▁here .
▁put ▁only ▁elements ▁into ▁a ▁list ▁with ▁a ▁cert ian ▁number ▁< s > ▁is ▁the ▁sales ▁ID ▁and ▁is ▁the ▁sol d ▁item id . ▁I ▁would ▁like ▁to ▁use ▁all ▁unique ▁i _ ids ▁to ▁find ▁all ▁purch ases ▁that ▁have ▁interact ed ▁with ▁i _ id . ▁I ▁also ▁implemented ▁this ▁in ▁the ▁loop . ▁What ▁I ▁would ▁like ▁that ▁I ▁only ▁want ▁to ▁add ▁something ▁to ▁the ▁list ▁when ▁the ▁has ▁more ▁of ▁a ▁1 ▁item . ▁How ▁do ▁I ▁do ▁that ▁so ▁that ▁I ▁only ▁add ▁the ▁purch ases ▁to ▁the ▁list ▁if ▁it ▁contains ▁more ▁than ▁one ▁item ? ▁Output ▁But ▁what ▁I ▁want ▁means ▁that ▁the ▁element ▁does ▁not ▁exist , ▁I ▁only ▁wrote ▁for ▁a ▁better ▁understanding
▁Multi - column ▁to ▁single ▁column ▁in ▁Pandas ▁< s > ▁I ▁have ▁the ▁following ▁data ▁frame ▁: ▁And ▁I ▁need ▁this ▁: ▁This ▁is ▁just ▁a ▁basic ▁example , ▁the ▁real ▁deal ▁can ▁have ▁over ▁60 ▁children .
▁Group by ▁and ▁perform ▁row - wise ▁calculation ▁using ▁a ▁custom ▁function ▁< s > ▁Following ▁on ▁from ▁this ▁question : ▁python ▁- ▁Group ▁by ▁and ▁add ▁new ▁row ▁which ▁is ▁calculation ▁of ▁other ▁rows ▁I ▁have ▁a ▁pandas ▁dataframe ▁as ▁follows : ▁And ▁I ▁want ▁to , ▁for ▁each ▁value ▁in ▁col _1, ▁apply ▁a ▁function ▁with ▁the ▁values ▁in ▁col _3 ▁and ▁col _4 ▁( and ▁many ▁more ▁columns ) ▁that ▁correspond ▁to ▁X ▁and ▁Z ▁from ▁col _2 ▁and ▁create ▁a ▁new ▁row ▁with ▁these ▁values . ▁So ▁the ▁output ▁would ▁be ▁as ▁below : ▁Where ▁are ▁the ▁outputs ▁of ▁the ▁function . ▁Original ▁question ▁( which ▁only ▁requires ▁a ▁simple ▁addition ) ▁was ▁answered ▁with : ▁I ' m ▁now ▁looking ▁for ▁a ▁way ▁to ▁use ▁a ▁custom ▁function , ▁such ▁as ▁or ▁, ▁rather ▁than ▁. ▁How ▁can ▁I ▁modify ▁this ▁code ▁to ▁work ▁with ▁my ▁new ▁requirements ?
▁How ▁can ▁I ▁remove ▁a ▁certain ▁type ▁of ▁values ▁in ▁a ▁group ▁in ▁pandas ? ▁< s > ▁I ▁have ▁the ▁following ▁dataframe ▁which ▁is ▁a ▁small ▁part ▁of ▁a ▁bigger ▁one : ▁I ' d ▁like ▁to ▁delete ▁all ▁rows ▁where ▁the ▁last ▁items ▁are ▁" d ". ▁So ▁my ▁desired ▁dataframe ▁would ▁look ▁like ▁this : ▁So ▁the ▁point ▁is , ▁that ▁a ▁group ▁shouldn ' t ▁have ▁" d " ▁as ▁the ▁last ▁item . ▁There ▁is ▁a ▁code ▁that ▁deletes ▁the ▁last ▁row ▁in ▁the ▁groups ▁where ▁the ▁last ▁item ▁is ▁" d ". ▁But ▁in ▁this ▁case , ▁I ▁have ▁to ▁run ▁the ▁code ▁twice ▁to ▁delete ▁all ▁last ▁" d "- s ▁in ▁group ▁3 ▁for ▁example . ▁Is ▁there ▁a ▁better ▁solution ▁to ▁this ▁problem ?
▁Transform ▁a ▁large ▁dataframe ▁- ▁takes ▁too ▁long ▁< s > ▁I ▁have ▁a ▁dataframe ▁loaded ▁from ▁a ▁CSV ▁in ▁the ▁following ▁format : ▁I ▁want ▁to ▁transform ▁it ▁to ▁the ▁following ▁format : ▁This ▁is ▁my ▁function ▁( ▁is ▁the ▁original ▁data ▁frame ) ▁that ▁does ▁the ▁transformation , ▁but ▁it ▁takes ▁7 ▁minutes ▁for ▁54 75 00 ▁row ▁dataframe . ▁Is ▁there ▁a ▁way ▁to ▁speed ▁it ▁up ?
▁Drop ▁last ▁n ▁rows ▁within ▁pandas ▁dataframe ▁groupby ▁< s > ▁I ▁have ▁a ▁dataframe ▁where ▁I ▁want ▁to ▁drop ▁last ▁rows ▁within ▁a ▁group ▁of ▁columns . ▁For ▁example , ▁say ▁is ▁defined ▁as ▁below ▁the ▁group ▁is ▁of ▁columns ▁and ▁: ▁Desired ▁output ▁for ▁is ▁as ▁follows : ▁Desired ▁output ▁for ▁is ▁as ▁follows :
▁Python ▁trans posing ▁multiple ▁dataframes ▁in ▁a ▁list ▁< s > ▁I ▁have ▁a ▁few ▁dataframes ▁which ▁are ▁similar ▁( in ▁terms ▁of ▁number ▁of ▁rows ▁and ▁columns ) ▁to ▁the ▁2 ▁dataframes ▁listed ▁below ▁my ▁desired ▁output ▁is ▁to ▁have ▁multiple ▁dataframes ▁with ▁the ▁email ▁as ▁column ▁header ▁and ▁the ▁factor ▁or ▁item ▁as ▁rows ▁I ▁am ▁able ▁to ▁get ▁the ▁result ▁by ▁trans posing ▁each ▁dataframe ▁individually ▁using ▁this ▁but ▁i ' d ▁like ▁to ▁create ▁a ▁for ▁loop ▁as ▁i ▁have ▁several ▁dataframes ▁to ▁transpose ▁wrote ▁something ▁like ▁this ▁but ▁the ▁dataframes ▁do ▁not ▁get ▁trans posed . ▁Would ▁like ▁to ▁directly ▁change ▁the ▁dataframes ▁in ▁the ▁list ▁of ▁dataframes ▁( som ewhere ▁along ▁the ▁lines ▁of ▁inplace = True ). ▁Was ▁wondering ▁if ▁there ▁is ▁something ▁i ▁am ▁missing , ▁appreciate ▁any ▁form ▁of ▁help , ▁thank ▁you .
▁pandas ▁- ▁down sample ▁a ▁more ▁frequent ▁DataFrame ▁to ▁the ▁frequency ▁of ▁a ▁less ▁frequent ▁DataFrame ▁< s > ▁I ▁have ▁two ▁DataFrames ▁that ▁have ▁different ▁data ▁measured ▁at ▁different ▁frequencies , ▁as ▁in ▁those ▁csv ▁examples : ▁df 1: ▁df 2: ▁I ▁would ▁like ▁to ▁obtain ▁a ▁single ▁df ▁that ▁have ▁all ▁the ▁measures ▁of ▁both ▁dfs ▁at ▁the ▁times ▁of ▁the ▁first ▁one ▁( which ▁get ▁data ▁less ▁frequently ). ▁I ▁tried ▁to ▁do ▁that ▁with ▁a ▁for ▁loop ▁aver aging ▁over ▁the ▁df 2 ▁measures ▁between ▁two ▁timestamps ▁of ▁df 1 ▁but ▁it ▁was ▁extremely ▁slow .
▁Merge ▁two ▁columns ▁of ▁a ▁dataframe ▁into ▁an ▁already ▁existing ▁column ▁of ▁dictionaries ▁as ▁a ▁key ▁value ▁pair ▁< s > ▁If ▁we ▁have ▁3 ▁columns ▁of ▁a ▁dataframe ▁as ▁: ▁I ▁want ▁the ▁column 3 ▁to ▁be ▁something ▁like ▁: ▁I ▁have ▁tried ▁a ▁few ▁things ▁from ▁using ▁lambda ▁functions ▁with ▁apply ▁to ▁iterating ▁over ▁rows ▁but ▁all ▁were ▁unsuccessful .
▁Sw ap ▁contents ▁of ▁columns ▁inside ▁dataframe ▁< s > ▁I ▁have ▁a ▁pandas ▁dataframe ▁with ▁this ▁contents ; ▁I ▁would ▁like ▁to ▁swap ▁the ▁contents ▁between ▁Column ▁1 ▁and ▁Column ▁2. ▁The ▁output ▁dataframe ▁should ▁look ▁like ▁this ; ▁I ▁am ▁using ▁python ▁v 3.6
▁How ▁does ▁pandas ▁convert ▁one ▁column ▁of ▁data ▁into ▁another ? ▁< s > ▁I ▁have ▁a ▁dataframe ▁generated ▁by ▁pandas , ▁as ▁follows : ▁I ▁want ▁to ▁convert ▁the ▁CODE ▁column ▁data ▁to ▁get ▁the ▁NUM ▁column . ▁The ▁encoding ▁rules ▁are ▁as ▁follows : ▁thank ▁you !
▁Add ▁a ▁different ▁item ▁from ▁a ▁list ▁to ▁each ▁cell ▁in ▁a ▁dataframe ▁with ▁Pandas ▁< s > ▁Given ▁a ▁dataframe ▁I ▁want ▁to ▁make ▁a ▁list ▁of ▁sequential ▁that ▁has ▁as ▁many ▁elements ▁as ▁there ▁are ▁rows ▁in ▁the ▁dataframe ▁And ▁then ▁add ▁each ▁element ▁of ▁the ▁list ▁( as ▁a ▁string ) ▁onto ▁the ▁end ▁of ▁a ▁single ▁column ▁in ▁the ▁dataframe . ▁Does ▁not ▁work , ▁as ▁it ▁adds ▁the ▁whole ▁list ▁as ▁a ▁string ▁to ▁each ▁value ▁as ▁opposed ▁to ▁one ▁value ▁of ▁the ▁list ▁per ▁value ▁in ▁the ▁dataframe . ▁Essentially , ▁I ▁want ▁to ▁transform ▁to ▁So ▁anyway ▁to ▁do ▁that ▁would ▁be ▁fine . ▁Thanks ▁for ▁the ▁help !
▁Pandas : ▁Sw ap ▁rows ▁between ▁columns ▁< s > ▁Some ▁rows ▁were ▁input ▁in ▁the ▁wrong ▁columns ▁so ▁now ▁I ▁need ▁to ▁swap ▁them . ▁My ▁current ▁approach ▁Expected ▁output ▁It ▁works ▁but ▁this ▁is ▁only ▁possible ▁because ▁it ▁was ▁4 ▁columns . ▁Is ▁there ▁a ▁better ▁way ▁to ▁do ▁this ? ▁Note ▁the ▁dtypes ▁can ▁cause ▁issues ▁with ▁sorting ▁is ▁Maybe ▁there ▁is ▁something ▁like
▁Turn ▁columns &# 39 ; ▁values ▁to ▁headers ▁of ▁columns ▁with ▁values ▁1 ▁and ▁0 ▁( ▁accordingly ) ▁[ python ] ▁< s > ▁I ▁got ▁a ▁column ▁of ▁the ▁form ▁: ▁The ▁column ▁represents ▁the ▁answers ▁of ▁users ▁to ▁a ▁question ▁of ▁5 ▁choices ▁(1 -5 ). ▁I ▁want ▁to ▁turn ▁this ▁into ▁a ▁matrix ▁of ▁5 ▁columns ▁where ▁the ▁indexes ▁are ▁the ▁5 ▁possible ▁answers ▁and ▁the ▁values ▁are ▁1 ▁or ▁0 ▁according ▁to ▁the ▁user ' s ▁given ▁answer . ▁Visual y ▁i ▁want ▁a ▁matrix ▁of ▁the ▁form :
▁Use ▁duplicated ▁values ▁to ▁increment ▁column ▁< s > ▁I ▁have ▁a ▁Pandas ▁dataframe ▁and ▁I ▁want ▁to ▁increment ▁a ▁column ▁based ▁on ▁the ▁amount ▁of ▁duplicated ▁values . ▁So ▁when ▁a ▁duplicate ▁is ▁found , ▁all ▁other ▁occurrences ▁is ▁incremented . ▁So ▁given ▁this ▁input ▁dataframe ▁return ▁I ▁tried ▁this ▁line ▁of ▁code ▁but ▁I ▁don ' t ▁know ▁how ▁to ▁increment
▁Append ▁list ▁to ▁list ▁in ▁specific ▁cell ▁in ▁pandas ▁< s > ▁I ▁have ▁a ▁pandas ▁dataframe , ▁which ▁looks ▁like ▁this : ▁And ▁I ▁want ▁to ▁append ▁a ▁new ▁list ▁c ▁= ▁[5, 6] ▁to ▁the ▁row , ▁where ▁So ▁the ▁result ▁would ▁be : ▁Right ▁now ▁my ▁attempt ▁looks ▁like ▁this : ▁Any ▁help ▁is ▁appreciated !
▁Group ▁a ▁dataframe ▁on ▁one ▁column ▁and ▁take ▁max ▁from ▁one ▁column ▁and ▁its ▁corresponding ▁value ▁from ▁the ▁other ▁col ▁< s > ▁I ▁have ▁a ▁large ▁dataframe ▁which ▁has ▁a ▁similar ▁pattern ▁as ▁below : ▁And ▁can ▁be ▁constructed ▁as : ▁Now ▁I ▁want ▁to ▁group ▁this ▁dataframe ▁by ▁the ▁first ▁column ▁i . e ., ▁and ▁take ▁from ▁the ▁column ▁and ▁its ▁corresponding ▁value ▁from ▁. ▁And ▁if ▁there ▁are ▁two ▁max ▁values ▁in ▁, ▁then ▁I ▁would ▁like ▁to ▁take ▁alphabetically ▁first ▁value ▁from ▁. ▁So ▁my ▁expected ▁result ▁would ▁look ▁like : ▁I ▁have ▁tried ▁but ▁this ▁selects ▁max ▁from ▁and ▁first ▁from ▁both ▁at ▁the ▁same ▁time . ▁Additionally ▁I ▁know ▁there ▁is ▁a ▁approach , ▁but ▁this ▁would ▁take ▁a ▁lot ▁of ▁time ▁for ▁my ▁dataset . ▁Any ▁suggestions ▁on ▁how ▁could ▁I ▁proceed ▁would ▁be ▁appreciated . ▁Thanks ▁in ▁advance :)
▁is ▁there ▁a ▁way ▁to ▁read ▁multiple ▁excel ▁tab / sheets ▁from ▁single ▁xlsx ▁to ▁multiple ▁dataframes ▁with ▁each ▁dataframe ▁named ▁with ▁sheet ▁name ? ▁< s > ▁I ▁am ▁not ▁good ▁in ▁python ▁please ▁forg ive ▁me ▁for ▁this ▁question ▁but ▁I ▁need ▁to ▁create ▁a ▁function ▁which ▁does ▁the ▁following ▁thing : ▁Create ▁multiple ▁data ▁frames ▁from ▁multiple ▁excel ▁tab / sheet ▁present ▁in ▁a ▁single ▁xlsx ▁file ▁and ▁be ▁named ▁on ▁the ▁sheet ▁name . ▁The ▁columns ' ▁values ▁should ▁be ▁concatenated ▁and ▁checked ▁if ▁there ▁is ▁no ▁duplicate ▁value . ▁if ▁the ▁concat ▁value ▁has ▁a ▁duplicate ▁then ▁it ▁should ▁be ▁told ▁as ▁yes / No ▁in ▁another ▁column . ▁all ▁the ▁dataframes ▁then ▁should ▁be ▁written ▁into ▁a ▁single ▁workbook ▁as ▁different ▁work sheets ▁inside . ▁values ▁inside ▁() ▁are ▁columns ▁for ▁better ▁understanding ▁example : ▁sheet 1 ▁result : ▁sheet 2 ▁result :
▁How ▁to ▁pick ▁some ▁values ▁of ▁a ▁column ▁and ▁make ▁another ▁one ▁with ▁them ? ▁< s > ▁This ▁is ▁a ▁table ▁similar ▁to ▁the ▁one ▁I ' m ▁working ▁with ▁And ▁what ▁I ' m ▁trying ▁to ▁do ▁is ▁take ▁some ▁values ▁of ▁the ▁column ▁A ▁that ▁follow ▁a ▁certain ▁pattern ▁and ▁create ▁another ▁column ▁with ▁such ▁values . ▁For ▁example , ▁the ▁column ▁C ▁would ▁have ▁only ▁the ▁values ▁from ▁A ▁that ▁are ▁bigger ▁than ▁12, ▁and ▁column ▁D ▁the ▁ones ▁smaller ▁or ▁equal : ▁I ' ve ▁tried ▁making ▁a ▁list ▁for ▁each ▁group ▁of ▁values , ▁but ▁I ▁can ' t ▁merge ▁them ▁back ▁with ▁the ▁original ▁table , ▁since ▁there ▁are ▁some ▁numbers ▁that ▁repeat ▁and ▁the ▁number ▁of ▁columns ▁grow . ▁I ▁think ▁there ' s ▁an ▁es as ier ▁way ▁to ▁do ▁that , ▁but ▁I ▁can ' t ▁seem ▁to ▁find ▁it . ▁How ▁can ▁I ▁do ▁that ?
▁Using ▁pandas . interpolate () ▁< s > ▁Suppose ▁I ▁would ▁like ▁to ▁apply ▁following ▁command : ▁which ▁returns ▁Question : ▁How ▁can ▁i ▁apply ▁a ▁restriction ▁on ▁the ▁minimum ▁number ▁of ▁valid ▁numbers ▁( i . e ▁not ▁NaN ) ▁before ▁AND ▁after ▁a ▁group ▁of ▁NaN s , ▁so ▁as ▁to ▁apply ▁the ▁interpolation ▁In ▁this ▁example , ▁i ▁would ▁like ▁to ▁fill ▁first ▁group ▁of ▁NaN s ▁because ▁there ▁are ▁minimum ▁3 ▁valid ▁numbers ▁before ▁AND ▁after , ▁but ▁NOT ▁interpolate ▁the ▁second ▁group ▁of ▁NaN s , ▁as ▁there ▁are ▁only ▁two ▁valid ▁numbers ▁after ▁the ▁NaN s ▁( and ▁not ▁3 ▁as ▁i ▁would ▁prefer ) ▁Expected ▁result :
▁Compare ▁two ▁columns ▁that ▁contains ▁timestamps ▁in ▁pandas ▁< s > ▁Lets ▁say ▁I ▁have ▁a ▁dataframe ▁like ▁this ▁one : ▁I ▁want ▁to ▁compare ▁if ▁the ▁timestamp ▁in ▁Col 1 ▁is ▁greater ▁than ▁in ▁Col 2 ▁and ▁if ▁that ▁is ▁true ▁I ▁want ▁to ▁remove ▁the ▁timestamps ▁from ▁the ▁other ▁columns ▁( Col 2, ▁Col 3, ▁Col 4 ). ▁I ▁also ▁want ▁to ▁check ▁if ▁timestamp ▁in ▁Col 2 ▁is ▁greater ▁than ▁in ▁Col 3 ▁and ▁if ▁that ▁is ▁true ▁I ▁want ▁to ▁remove ▁timestamp ▁from ▁other ▁columns ▁Col 3, ▁Col 4 ). ▁I ▁tried ▁this ▁one : ▁But ▁it ▁is ▁showing ▁me ▁this ▁error : ▁My ▁des irable ▁output ▁would ▁look ▁like ▁this : ▁EDIT ED : ▁Added ▁Col 0
▁remove ▁un named ▁col ums ▁pandas ▁dataframe ▁< s > ▁i ' m ▁a ▁student ▁and ▁have ▁a ▁problem ▁that ▁i ▁cant ▁figure ▁it ▁out ▁how ▁to ▁solve ▁it . i ▁have ▁csv ▁data ▁like ▁this ▁: ▁code ▁for ▁reading ▁csv ▁like ▁this ▁: ▁S MT ▁print ▁out ▁: ▁expected ▁output ▁: ▁i ▁already ▁trying ▁but ▁it ▁will ▁be ▁like ▁this ▁: ▁i ▁already ▁trying ▁to ▁put ▁or ▁// is ▁not ▁working , ▁and ▁the ▁last ▁time ▁I ▁tried ▁it ▁like ▁this ▁: ▁and ▁i ▁got ▁i ▁just ▁want ▁to ▁get ▁rid ▁the ▁Un named : ▁5 ▁~ ▁Un named : ▁8, ▁how ▁the ▁correct ▁way ▁to ▁get ▁rid ▁of ▁this ▁Un named ▁thing ▁?
▁Using ▁values ▁from ▁dataframe ▁for ▁calculation ▁< s > ▁I ▁am ▁selecting ▁dedicated ▁data ▁from ▁a ▁dataframe ▁and ▁would ▁like ▁to ▁make ▁a ▁linear ▁interpolation ▁based ▁on ▁my ▁defined ▁formula : ▁I ▁would ▁like ▁to ▁interpolate ▁e . g . ▁between ▁rank ▁2.0 ▁and ▁3.0 , ▁where ▁the ▁needed ▁rank ▁is ▁2. 5. ▁The ▁calculation ▁looks ▁like ▁the ▁following : ▁y ▁= ▁- 9.0 8 0002 ▁+ ▁( -9 .0 3 999 3 ▁- ▁( -9 .0 8 000 2)) * [( 2. 5- 2) /( 3- 2) ] ▁= ▁- 9.0 5 999 75 00000 ▁where ▁the ▁values ▁are ▁defined ▁in ▁the ▁code ▁as ▁the ▁following : ▁The ▁code ▁looks ▁like ▁the ▁following : ▁The ▁result ▁looks ▁like ▁the ▁following : ▁The ▁Excel ▁file ▁looks ▁like ▁the ▁following :
▁How ▁to ▁average ▁DataFrame ▁row ▁with ▁another ▁row ▁only ▁if ▁the ▁first ▁row ▁is ▁a ▁substring ▁of ▁other ▁next ▁row ▁< s > ▁I ▁have ▁a ▁dataframe ▁called ▁' data ': ▁I ▁want ▁to ▁combine ▁' ABC -1 ' ▁with ▁' ABC -1 B ' ▁into ▁a ▁single ▁row ▁using ▁the ▁first ▁USER ▁name ▁and ▁then ▁aver aging ▁the ▁two ▁values ▁to ▁arrive ▁here : ▁The ▁dataframe ▁may ▁not ▁be ▁in ▁order ▁and ▁there ▁are ▁other ▁values ▁in ▁there ▁as ▁well ▁that ▁are ▁unrelated ▁that ▁don ' t ▁need ▁aver aging . ▁I ▁only ▁want ▁to ▁average ▁the ▁two ▁rows ▁where ▁' XXX - X ' ▁is ▁in ▁' XXX - X B '
▁Cal culating ▁the ▁duration ▁an ▁event ▁in ▁a ▁time ▁series ▁python ▁< s > ▁I ▁have ▁a ▁dataframe ▁as ▁show ▁below : ▁The ▁index ▁is ▁datetime ▁and ▁have ▁column ▁record ▁the ▁ra inf all ▁value ( unit : mm ) ▁in ▁each ▁hour , I ▁would ▁like ▁to ▁calculate ▁the ▁" Average ▁w et ▁spell ▁duration ", ▁which ▁means ▁the ▁average ▁of ▁continuous ▁hours ▁that ▁exist ▁values ▁( not ▁zero ) ▁in ▁a ▁day , ▁so ▁the ▁calculation ▁is ▁and ▁the ▁" average ▁w et ▁spell ▁amount ", ▁which ▁means ▁the ▁average ▁of ▁sum ▁of ▁the ▁values ▁in ▁continuous ▁hours ▁in ▁a ▁day . ▁The ▁data f ame ▁above ▁is ▁just ▁a ▁example , ▁the ▁dataframe ▁which ▁I ▁have ▁have ▁more ▁longer ▁time ▁series ▁( more ▁than ▁one ▁year ▁for ▁example ), ▁how ▁can ▁I ▁write ▁a ▁function ▁so ▁it ▁could ▁calculate ▁the ▁two ▁value ▁mentioned ▁above ▁in ▁a ▁better ▁way ? ▁thanks ▁in ▁advance ! ▁P . S . ▁the ▁values ▁may ▁be ▁NaN , ▁and ▁I ▁would ▁like ▁to ▁just ▁ignore ▁it .
