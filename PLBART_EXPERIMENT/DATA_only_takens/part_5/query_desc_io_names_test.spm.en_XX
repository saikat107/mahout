▁A ▁better ▁way ▁to ▁map ▁data ▁in ▁multiple ▁datasets , ▁with ▁multiple ▁data ▁mapping ▁rules ▁< s > ▁I ▁have ▁three ▁datasets ▁( , ▁, ▁), ▁and ▁I ▁wish ▁to ▁add ▁a ▁new ▁column ▁called ▁in ▁dataframe , ▁and ▁the ▁value ▁to ▁be ▁added ▁can ▁be ▁retrieved ▁from ▁the ▁other ▁two ▁dataframes , ▁the ▁rule ▁is ▁in ▁the ▁bottom ▁after ▁codes . ▁final _ NN : ▁p pt _ code : ▁her d _ id : ▁Expected ▁output : ▁The ▁rules ▁is : ▁if ▁in ▁final _ NN ▁is ▁not ▁, ▁= ▁in ▁; ▁if ▁in ▁final _ NN ▁is ▁but ▁in ▁is ▁not ▁Null , ▁then ▁search ▁the ▁p pt _ code ▁dataframe , ▁and ▁use ▁the ▁and ▁its ▁corresponding ▁" number " ▁to ▁map ▁and ▁fill ▁in ▁the ▁" Map Value " ▁in ▁; ▁if ▁both ▁and ▁in ▁are ▁and ▁null ▁respectively , ▁but ▁in ▁is ▁not ▁Null , ▁then ▁search ▁dataframe , ▁and ▁use ▁the ▁and ▁its ▁corresponding ▁to ▁fill ▁in ▁the ▁in ▁the ▁first ▁dataframe . ▁I ▁applied ▁a ▁loop ▁through ▁the ▁dataframe ▁which ▁is ▁a ▁slow ▁way ▁to ▁achieve ▁this , ▁as ▁above . ▁But ▁I ▁understand ▁there ▁could ▁be ▁a ▁faster ▁way ▁to ▁do ▁this . ▁Just ▁wondering ▁would ▁anyone ▁help ▁me ▁to ▁have ▁a ▁fast ▁and ▁easier ▁way ▁to ▁achieve ▁the ▁same ▁result ? ▁< s > ▁code ▁number ▁0 ▁AA ▁11 ▁1 ▁AA ▁11 ▁2 ▁BB ▁22 ▁3 ▁BB ▁22 ▁4 ▁CC ▁33 ▁< s > ▁ID ▁number ▁0 ▁7 99 ▁6 78 ▁1 ▁8 13 ▁7 89 ▁< s > ▁map ▁add ▁value ▁codes ▁map ▁first
▁Create ▁a ▁dataframe ▁from ▁arrays ▁python ▁< s > ▁I ' m ▁try ▁to ▁construct ▁a ▁dataframe ▁( I ' m ▁using ▁Pandas ▁library ) ▁from ▁some ▁arrays ▁and ▁one ▁matrix . ▁in ▁particular , ▁if ▁I ▁have ▁two ▁array ▁like ▁this : ▁And ▁one ▁matrix ▁like ▁this ▁: ▁Can ▁i ▁create ▁a ▁dataset ▁like ▁this ? ▁Maybe ▁is ▁a ▁stupid ▁question , ▁but ▁i ▁m ▁very ▁new ▁with ▁Python ▁and ▁Pandas . ▁I ▁seen ▁this ▁: ▁https :// pandas . py data . org / pandas - docs / version / 0. 2 3.4/ generated / pandas . DataFrame . html ▁but ▁specify ▁only ▁' col ums '. ▁I ▁should ▁read ▁the ▁matrix ▁row ▁for ▁row ▁and ▁paste ▁in ▁my ▁dataset , ▁but ▁I ▁m ▁think ▁that ▁exist ▁a ▁more ▁easy ▁solution ▁with ▁Pandas . ▁< s > ▁1 ▁2 ▁2 ▁3 ▁3 ▁3 ▁4 ▁4 ▁4 ▁< s > ▁A ▁B ▁C ▁D ▁1 ▁2 ▁2 ▁E ▁3 ▁3 ▁3 ▁F ▁4 ▁4 ▁4 ▁< s > ▁array ▁DataFrame
▁Can ▁not ▁make ▁desired ▁pandas ▁dataframe ▁< s > ▁I ▁am ▁trying ▁to ▁make ▁a ▁pandas ▁dataframe ▁using ▁2 ▁param ters ▁as ▁columns . ▁But ▁it ▁makes ▁a ▁dataframe ▁transpose ▁of ▁what ▁I ▁need . ▁I ▁have ▁and ▁as ▁column ▁parameters ▁as ▁follows : ▁This ▁gives ▁the ▁following ▁dataframe : ▁However , ▁I ▁want ▁the ▁dataframe ▁as : ▁< s > ▁0 ▁1 ▁2 ▁3 ▁4 ▁0 ▁1 ▁2 ▁3 ▁4 ▁5 ▁1 ▁11 ▁22 ▁33 ▁44 ▁55 ▁< s > ▁0 ▁1 ▁0 ▁1 ▁11 ▁1 ▁2 ▁22 ▁2 ▁3 ▁33 ▁3 ▁4 ▁44 ▁4 ▁5 ▁55 ▁< s > ▁columns ▁transpose
▁How ▁to ▁handle ▁missing ▁data ▁with ▁respect ▁to ▁type ▁of ▁dataset ? ▁< s > ▁I ▁have ▁a ▁dataset ▁where ▁has ▁column ▁types ▁that ▁has ▁type ▁like ▁, ▁. ▁df ▁I ▁want ▁to ▁replace ▁missing ▁value ▁with ▁for ▁each ▁type . ▁Such ▁as - ▁result _ df ▁How ▁can ▁do ▁it ▁with ▁Python ? ▁< s > ▁ID ▁types ▁C ▁D ▁0 ▁101 ▁Primary ▁2 ▁3 ▁1 ▁103 ▁Primary ▁6 ▁3 ▁2 ▁108 ▁Primary ▁10 ▁? ▁3 ▁109 ▁Primary ▁3 ▁12 ▁4 ▁118 ▁Second ary ▁5 ▁2 ▁5 ▁12 2 ▁Second ary ▁? ▁6 ▁6 ▁123 ▁Second ary ▁5 ▁6 ▁7 ▁125 ▁Second ary ▁2 ▁5 ▁< s > ▁ID ▁types ▁C ▁D ▁0 ▁101 ▁Primary ▁2 ▁3 ▁1 ▁103 ▁Primary ▁6 ▁3 ▁2 ▁108 ▁Primary ▁10 ▁3 ▁3 ▁109 ▁Primary ▁3 ▁12 ▁4 ▁118 ▁Second ary ▁5 ▁2 ▁5 ▁12 2 ▁Second ary ▁5 ▁6 ▁6 ▁123 ▁Second ary ▁5 ▁6 ▁7 ▁125 ▁Second ary ▁2 ▁5 ▁< s > ▁where ▁replace ▁value
▁Python : ▁Convert ▁matrices ▁to ▁permutations ▁table ▁< s > ▁Given ▁a ▁set ▁of ▁ids , ▁I ▁need ▁to ▁get ▁the ▁values ▁from ▁a ▁matrix ▁( time ▁A ▁& ▁B ) ▁for ▁each ▁id ▁combination , ▁and ▁create ▁a ▁dataframe ▁appending ▁the ▁values ▁for ▁all ▁the ▁permutations . ▁I ▁have ▁been ▁able ▁to ▁do ▁it ▁by ▁creating ▁the ▁permutations ▁dataframe ▁and ▁then ▁iterating ▁through ▁it ▁while ▁looking ▁& ▁filling ▁the ▁values . ▁However ▁I ▁need ▁to ▁do ▁this ▁for ▁~ 3 000 ▁ids , ▁not ▁3, ▁and ▁I ▁don ' t ▁know ▁how ▁to ▁do ▁it ▁efficiently . ▁Can ▁I ▁generate ▁a ▁Time ▁A / B ▁dataframe ▁as ▁my ▁example ▁without ▁having ▁to ▁iterate ▁through ▁9 000000 * ▁rows ? ▁I ▁know ▁I ▁shouldn ' t ▁be ▁iterating ▁though ▁a ▁dataframe ▁however ▁I ▁haven ' t ▁found ▁an ▁alternative ▁yet . ▁I ds ▁(3 ): ▁Time ▁A ▁matrix ▁(3 x 3): ▁Time ▁B ▁matrix ▁(3 x 3): ▁Time ▁A / B ▁dataframe ▁(6 ): ▁< s > ▁id ▁15 ▁24 ▁38 ▁15 ▁0 ▁1.8 ▁1.7 ▁24 ▁1.2 ▁0 ▁1.9 ▁38 ▁1.5 ▁1.3 ▁0 ▁< s > ▁id ▁15 ▁24 ▁38 ▁15 ▁0 ▁8 8. 7 ▁8 7. 3 ▁24 ▁4 2.2 ▁0 ▁3 2.7 ▁38 ▁6 5. 6 ▁1 3.5 ▁0 ▁< s > ▁get ▁values ▁time ▁values ▁all ▁values
▁python / pandas : ▁update ▁a ▁column ▁based ▁on ▁a ▁series ▁holding ▁sums ▁of ▁that ▁same ▁column ▁< s > ▁I ▁have ▁a ▁dataframe ▁with ▁a ▁non - unique ▁col 1 ▁like ▁the ▁following ▁Some ▁of ▁the ▁values ▁of ▁col 1 ▁repeat ▁lots ▁of ▁times ▁and ▁others ▁not ▁so . ▁I ' d ▁like ▁to ▁take ▁the ▁bottom ▁( 80 % / 50% /10 %) ▁and ▁change ▁the ▁value ▁to ▁' other ' ▁ahead ▁of ▁plotting . ▁I ' ve ▁got ▁a ▁series ▁which ▁contains ▁the ▁codes ▁in ▁col 1 ▁( as ▁the ▁index ) ▁and ▁the ▁amount ▁of ▁times ▁that ▁they ▁appear ▁in ▁the ▁df ▁in ▁descending ▁order ▁by ▁doing ▁the ▁following : ▁I ' ve ▁also ▁got ▁my ▁cut - off ▁point ▁( bottom ▁80 %) ▁I ' d ▁like ▁to ▁update ▁col 1 ▁in ▁df ▁with ▁the ▁value ▁' oth ers ' ▁when ▁col 1 ▁appears ▁after ▁the ▁cut Off ▁in ▁the ▁index ▁of ▁the ▁series ▁df 2. ▁I ▁don ' t ▁know ▁how ▁to ▁go ▁about ▁checking ▁and ▁updating . ▁I ▁figured ▁that ▁the ▁best ▁way ▁would ▁be ▁to ▁do ▁a ▁groupby ▁on ▁col 1 ▁and ▁then ▁loop ▁through , ▁but ▁it ▁starts ▁to ▁fall ▁apart , ▁should ▁I ▁create ▁a ▁new ▁groupby ▁object ? ▁Or ▁do ▁I ▁call ▁this ▁as ▁an ▁. apply () ▁for ▁each ▁row ? ▁Can ▁you ▁update ▁a ▁column ▁that ▁is ▁being ▁used ▁as ▁the ▁index ▁for ▁a ▁dataframe ? ▁I ▁could ▁do ▁with ▁some ▁help ▁about ▁how ▁to ▁start . ▁edit ▁to ▁add : ▁So ▁if ▁the ▁' b ' s ▁in ▁col 1 ▁were ▁not ▁in ▁the ▁top ▁20 % ▁most ▁pop ul ous ▁values ▁in ▁col 1 ▁then ▁I ' d ▁expect ▁to ▁see : ▁< s > ▁col 1 ▁col 2 ▁0 ▁a ▁1 ▁1 ▁a ▁1 ▁2 ▁a ▁2 ▁3 ▁b ▁3 ▁4 ▁b ▁3 ▁5 ▁c ▁2 ▁6 ▁c ▁2 ▁< s > ▁col 1 ▁col 2 ▁0 ▁a ▁1 ▁1 ▁a ▁1 ▁2 ▁a ▁2 ▁3 ▁others ▁3 ▁4 ▁others ▁3 ▁5 ▁c ▁2 ▁6 ▁c ▁2 ▁< s > ▁update ▁unique ▁values ▁repeat ▁take ▁value ▁contains ▁codes ▁index ▁cut ▁update ▁value ▁index ▁groupby ▁groupby ▁apply ▁update ▁index ▁start ▁add ▁values
▁Assign ▁unique ▁ID ▁to ▁Pandas ▁group ▁but ▁add ▁one ▁if ▁repeated ▁< s > ▁I ▁couldn ' t ▁find ▁a ▁solution ▁and ▁want ▁something ▁faster ▁than ▁what ▁I ▁already ▁have . ▁So , ▁the ▁idea ▁is ▁to ▁assign ▁a ▁unique ▁ID ▁for ▁' fruit ' ▁column , ▁e . g . ▁However , ▁if ▁repeated , ▁add ▁1 ▁to ▁the ▁last ▁result , ▁so ▁that ▁instead ▁of : ▁I ▁will ▁end ▁up ▁with : ▁So ▁it ▁adds ▁up ▁until ▁the ▁end , ▁even ▁if ▁there ▁may ▁only ▁be ▁4 ▁fruits ▁changing ▁their ▁positions . ▁Here ▁is ▁my ▁solution ▁but ▁it ' s ▁really ▁slow ▁and ▁I ▁bet ▁there ▁is ▁something ▁that ▁Pandas ▁can ▁do , ▁inher ently : ▁Any ▁ideas ? ▁< s > ▁df [' id '] ▁= ▁[0, ▁0, ▁1, ▁1, ▁2, ▁0, ▁0, ▁2, ▁2] ▁< s > ▁df [' id '] ▁= ▁[0, ▁0, ▁1, ▁1, ▁2, ▁3, ▁3, ▁4, ▁4] ▁< s > ▁unique ▁add ▁assign ▁unique ▁add ▁last
▁How ▁to ▁append ▁ndarray ▁values ▁into ▁dataframe ▁rows ▁of ▁particular ▁columns ? ▁< s > ▁I ▁have ▁a ▁function ▁that ▁returns ▁an ▁like ▁this ▁Now , ▁I ▁have ▁a ▁data ▁frame ▁with ▁columns ▁A , B , C , ..., Z ▁; ▁but ▁the ▁array ▁we ▁are ▁getting ▁has ▁only ▁20 ▁values . ▁Hence ▁I ▁want ▁to ▁find ▁a ▁way ▁such ▁that ▁for ▁every ▁array ▁I ▁get ▁as ▁output , ▁I ▁am ▁able ▁to ▁store ▁it ▁in ▁like ▁this ▁( A , B , W , X , Y , Z ▁are ▁to ▁be ▁left ▁blank ): ▁< s > ▁[0 ▁1 ▁0 ▁0 ▁0 ▁0 ▁1 ▁0 ▁1 ▁0 ▁1 ▁0 ▁0 ▁1 ▁0 ▁0 ▁0 ▁0 ▁0 ▁0] ▁< s > ▁__ | ▁A ▁| ▁B ▁| ▁C ▁| ▁D ▁| ▁E ▁| ▁F ▁| ▁... ▁0 ▁| nan | nan | ▁0 ▁| ▁1 ▁| ▁0 ▁| ▁0 ▁| ▁... ▁1 ▁| nan | nan | ▁1 ▁| ▁1 ▁| ▁0 ▁| ▁1 ▁| ▁... ▁. ▁. ▁. ▁< s > ▁append ▁values ▁columns ▁columns ▁array ▁values ▁array ▁get ▁left
▁Pandas : Calculate ▁mean ▁of ▁a ▁group ▁of ▁n ▁values ▁of ▁each ▁columns ▁of ▁a ▁dataframe ▁< s > ▁I ▁have ▁a ▁dataframe ▁of ▁the ▁following ▁type : ▁I ▁want ▁to ▁calculate ▁the ▁mean ▁of ▁the ▁first ▁3 ▁element ▁of ▁each ▁column ▁and ▁then ▁next ▁3 ▁elements ▁and ▁so ▁on ▁and ▁then ▁store ▁in ▁a ▁dataframe . ▁Desired ▁Output - ▁Using ▁Group ▁By ▁was ▁one ▁of ▁the ▁approach ▁I ▁thought ▁of ▁but ▁I ▁am ▁unable ▁to ▁figure ▁out ▁how ▁to ▁use ▁Group ▁by ▁in ▁this ▁case . ▁< s > ▁A ▁B ▁0 ▁1 ▁2 ▁1 ▁4 ▁5 ▁2 ▁7 ▁8 ▁3 ▁10 ▁11 ▁4 ▁13 ▁14 ▁5 ▁16 ▁17 ▁< s > ▁A ▁B ▁0 ▁4 ▁5 ▁1 ▁12 ▁14 ▁< s > ▁mean ▁values ▁columns ▁mean ▁first
▁how ▁to ▁extract ▁each ▁numbers ▁from ▁pandas ▁string ▁column ▁to ▁list ? ▁< s > ▁How ▁to ▁do ▁that ? ▁I ▁have ▁pandas ▁dataframe ▁looks ▁like : ▁I ▁need ▁to ▁transfer ▁this ▁each ▁row ▁to ▁separated ▁list : ▁< s > ▁Column _ A ▁11. 2 ▁some ▁text ▁17 ▁some ▁text ▁21 ▁some ▁text ▁25. 2 ▁4.1 ▁some ▁text ▁53 ▁17 ▁78 ▁12 1.1 ▁bla ▁bla ▁bla ▁14 ▁some ▁text ▁12 ▁some ▁text ▁< s > ▁list A [0] ▁= ▁11. 2 ▁list A [1] ▁= ▁17 ▁list A [2] ▁= ▁21 ▁list B [0] ▁= ▁25. 2 ▁list B [1] ▁= ▁4.1 ▁list B [2] ▁= ▁53 ▁list B [3] ▁= ▁17 ▁list B [4] ▁= ▁78 ▁list C [0] ▁= ▁12 1.1 ▁list C [1] ▁= ▁14 ▁list D [0] ▁= ▁12
▁How ▁to ▁fill ▁elements ▁between ▁intervals ▁of ▁a ▁list ▁< s > ▁I ▁have ▁a ▁list ▁like ▁this : ▁So ▁there ▁are ▁intervals ▁that ▁begin ▁with ▁and ▁end ▁with ▁. ▁How ▁can ▁I ▁replace ▁the ▁values ▁in ▁those ▁intervals , ▁say ▁with ▁1 ? ▁The ▁outcome ▁will ▁look ▁like ▁this : ▁I ▁use ▁in ▁this ▁example , ▁but ▁a ▁general ized ▁solution ▁that ▁can ▁apply ▁to ▁any ▁value ▁will ▁also ▁be ▁great ▁< s > ▁list _1 ▁= ▁[ np . NaN , ▁np . NaN , ▁1, ▁np . NaN , ▁np . NaN , ▁np . NaN , ▁0, ▁np . NaN , ▁1, ▁np . NaN , ▁0, ▁1, ▁np . NaN , ▁0, ▁np . NaN , ▁1, ▁np . NaN ] ▁< s > ▁list _2 ▁= ▁[ np . NaN , ▁np . NaN , ▁1, ▁1, ▁1, ▁1, ▁0, ▁np . NaN , ▁1, ▁1, ▁0, ▁1, ▁1, ▁0, ▁np . NaN , ▁1, ▁np . NaN ] ▁< s > ▁between ▁replace ▁values ▁apply ▁any ▁value
▁Creating ▁new ▁columns ▁within ▁a ▁dataframe , ▁based ▁on ▁the ▁latest ▁value ▁from ▁previous ▁columns ▁< s > ▁I ' ve ▁just ▁completed ▁a ▁beginner ' s ▁course ▁in ▁python , ▁so ▁please ▁bear ▁with ▁me ▁if ▁the ▁code ▁below ▁doesn ' t ▁make ▁sense ▁or ▁my ▁issue ▁is ▁because ▁of ▁some ▁ro ok ie ▁mistake . ▁I ' ve ▁been ▁trying ▁to ▁put ▁the ▁learning ▁to ▁use ▁by ▁working ▁with ▁colle ge ▁production ▁of ▁N FL ▁players , ▁with ▁a ▁view ▁to ▁understanding ▁which ▁statistics ▁can ▁be ▁predict ive ▁or ▁at ▁least ▁corre late ▁to ▁N FL ▁production . ▁It ▁turns ▁out ▁that ▁there ' s ▁a ▁lot ▁of ▁data ▁out ▁there ▁so ▁I ▁have ▁about ▁200 ▁columns ▁of ▁data ▁for ▁600 ▁odd ▁prospect s ▁from ▁the ▁last ▁20 ▁years ▁( just ▁for ▁running ▁back s ▁so ▁far ). ▁However , ▁one ▁of ▁the ▁problems ▁with ▁this ▁data ▁is ▁that ▁each ▁stat ▁is ▁only ▁provided ▁by ▁the ▁age ▁the ▁prospect ▁was ▁in ▁that ▁season ▁giving ▁me ▁something ▁like ▁this : ▁What ▁I ▁want ▁to ▁do ▁at ▁the ▁moment ▁is ▁to ▁be ▁able ▁to ▁take ▁the ▁last ▁year ▁of ▁colle ge ▁production ▁and ▁put ▁it ▁into ▁a ▁new ▁column ▁( for ▁17 ▁different ▁statistics ). ▁I ' ve ▁therefore ▁defined ▁the ▁following ▁function : ▁Which ▁I ▁think ▁should ▁go ▁backwards ▁through ▁the ▁columns ▁until ▁I ▁find ▁a ▁value ▁which ▁isn ' t ▁NaN , ▁and ▁then ▁take ▁that ▁value ▁as ▁the ▁output . ▁I ' ve ▁then ▁defined ▁the ▁columns ▁via ▁a ▁list : ▁and ▁have ▁then ▁run ▁the ▁function ▁through ▁a ▁for ▁loop ▁based ▁on ▁this ▁list : ▁The ▁result ▁I ' m ▁getting ▁back ▁is ▁a ▁slightly ▁biz ar re ▁one ▁- ▁the ▁for ▁loop ▁appears ▁to ▁work , ▁as ▁all ▁the ▁new ▁columns ▁I ' m ▁expecting ▁are ▁created , ▁however ▁they ▁are ▁only ▁populated ▁with ▁data ▁where ▁the ▁player ▁had ▁an ▁age ▁23 ▁season . ▁The ▁remainder ▁of ▁indexes ▁are ▁filled ▁with ▁' NaN ': ▁This ▁suggests ▁to ▁me ▁that ▁the ▁first ▁' if ' ▁statement ▁in ▁my ▁function ▁is ▁working ▁fine , ▁but ▁that ▁all ▁of ▁the ▁' el if ' ▁statements ▁aren ' t ▁triggering ▁and ▁I ▁can ' t ▁work ▁out ▁why . ▁I ' m ▁wondering ▁whether ▁it ' s ▁because ▁I ▁need ▁to ▁be ▁more ▁explicit ▁about ▁why ▁they ▁would ▁trigger , ▁rather ▁than ▁just ▁relying ▁on ▁a ▁logical ▁test ▁of ▁' if ▁the ▁column ▁is ▁not , ▁not ▁equal ▁to ▁NaN , ▁go ▁to ▁the ▁next ▁one ', ▁or ▁if ▁I ' m ▁misunder standing ▁the ▁elif ▁aspect ▁all ▁together . ▁I ' ve ▁put ▁the ▁whole ▁segment ▁of ▁code ▁in ▁also , ▁just ▁because ▁when ▁I ' ve ▁run ▁into ▁issues ▁so ▁far ▁the ▁problem ▁has ▁often ▁not ▁been ▁where ▁I ▁originally ▁thought . ▁By ▁all ▁means ▁tell ▁me ▁if ▁you ▁think ▁I ' ve ▁gone ▁about ▁this ▁in ▁a ▁weird ▁way ▁- ▁this ▁just ▁seemed ▁like ▁a ▁logical ▁approach ▁to ▁the ▁problem ▁but ▁open ▁to ▁other ▁ways ▁of ▁getting ▁the ▁desired ▁result . ▁Thanks ▁in ▁advance ! ▁< s > ▁GP ▁18 ▁GP ▁19 ▁GP ▁20 ▁GP ▁21 ▁GP ▁22 ▁GP ▁23 ▁50 ▁14.0 ▁13 .0 ▁14.0 ▁NaN ▁NaN ▁NaN ▁51 ▁14.0 ▁14.0 ▁14.0 ▁NaN ▁NaN ▁NaN ▁53 ▁13 .0 ▁12.0 ▁11.0 ▁NaN ▁NaN ▁NaN ▁56 ▁10.0 ▁13 .0 ▁9.0 ▁13 .0 ▁NaN ▁NaN ▁59 ▁10.0 ▁13 .0 ▁15.0 ▁NaN ▁NaN ▁NaN ▁61 ▁NaN ▁NaN ▁11.0 ▁11.0 ▁NaN ▁NaN ▁66 ▁NaN ▁12.0 ▁13 .0 ▁12.0 ▁2.0 ▁13 .0 ▁< s > ▁GP ▁Last ▁50 ▁NaN ▁51 ▁NaN ▁53 ▁NaN ▁56 ▁NaN ▁59 ▁NaN ▁61 ▁NaN ▁66 ▁13 .0 ▁< s > ▁columns ▁value ▁columns ▁put ▁view ▁at ▁columns ▁last ▁at ▁take ▁last ▁year ▁put ▁columns ▁value ▁take ▁value ▁columns ▁all ▁columns ▁where ▁first ▁all ▁test ▁all ▁put ▁where ▁all
▁Extract ▁part ▁of ▁a ▁3 ▁D ▁dataframe ▁< s > ▁I ▁have ▁a ▁3 d ▁dataframe . ▁looks ▁like ▁this : ▁How ▁could ▁I ▁extract ▁only ▁column ▁A ▁& ▁B ▁from ▁every ▁d 1, d 2 ..... ? ▁I ▁desire ▁to ▁take ▁the ▁dataframe ▁like ▁this : ▁< s > ▁d 1 ▁d 2 ▁d 3 ▁A ▁B ▁C ▁D ... ▁A ▁B ▁C ▁D ... ▁A ▁B ▁C ▁D .. ▁0 ▁1 ▁2 ▁< s > ▁d 1 ▁d 2 ▁d 3 ▁A ▁B ▁A ▁B ▁A ▁B ▁0 ▁1 ▁2 ▁< s > ▁take
▁Iter ative ▁comparison ▁with ▁pandas ▁< s > ▁I ▁don ' t ▁know ▁to ▁approach ▁this ▁issue . ▁I ▁have ▁a ▁data ▁frame ▁that ▁looks ▁like ▁this ▁What ▁I ▁need ▁to ▁do ▁is ▁to ▁iterate ▁between ▁each ▁row ▁per ▁usuario _ id ▁and ▁check ▁if ▁there ' s ▁a ▁difference ▁between ▁each ▁row , ▁and ▁create ▁a ▁new ▁data ▁set ▁with ▁the ▁row ▁changed ▁and ▁the ▁usuario _ web ▁in ▁charge ▁of ▁this ▁change , ▁to ▁generate ▁a ▁data ▁frame ▁that ▁looks ▁like ▁this : ▁Is ▁there ▁any ▁way ▁to ▁do ▁this ? ▁I ' m ▁working ▁with ▁pandas ▁on ▁python ▁and ▁this ▁dataset ▁could ▁be ▁a ▁little ▁big , ▁let ' s ▁say ▁around ▁10000 ▁rows , ▁sorted ▁by ▁usuario _ id . ▁Thanks ▁for ▁any ▁advice . ▁< s > ▁cu ent a _ ban car ia ▁nombre _ emp res a ▁per fil _ c ob r anza ▁usuario _ id ▁usuario _ web ▁55 45 ▁a ▁123 ▁500 199 ▁5 012 ▁5 55 1 ▁a ▁123 ▁500 199 ▁3 321 ▁5 55 1 ▁a ▁55 ▁500 199 ▁5 54 1 ▁5 55 1 ▁b ▁55 ▁500 199 ▁5 246 ▁< s > ▁usuario _ id ▁c amb io ▁usuario _ web ▁500 199 ▁cu ent a _ ban car ia ▁3 321 ▁500 199 ▁per fil _ c ob r anza ▁5 54 1 ▁500 199 ▁nombre _ emp res a ▁5 246 ▁< s > ▁between ▁difference ▁between ▁any ▁any
▁Check ▁if ▁group ▁contains ▁same ▁value ▁in ▁Pandas ▁< s > ▁I ▁am ▁curious ▁if ▁there ▁is ▁a ▁pre - built ▁function ▁in ▁Pandas ▁to ▁check ▁if ▁all ▁members ▁of ▁a ▁group ▁( factors ▁in ▁a ▁column ) ▁contain ▁the ▁same ▁value ▁in ▁another ▁column . ▁i . e . ▁if ▁my ▁dataframe ▁was ▁similar ▁to ▁below ▁it ▁would ▁return ▁an ▁empty ▁list . ▁However , ▁if ▁my ▁dataframe ▁appeared ▁as ▁such ▁( notice ▁the ▁1 ▁in ▁Col 1): ▁Then ▁the ▁output ▁would ▁be ▁a ▁list ▁containing ▁the ▁object ▁" B " ▁since ▁the ▁group ▁B ▁has ▁different ▁values ▁in ▁Col 1. ▁< s > ▁Col 1 ▁Col 2 ▁2 ▁A ▁2 ▁A ▁0 ▁B ▁0 ▁B ▁< s > ▁Col 1 ▁Col 2 ▁2 ▁A ▁2 ▁A ▁0 ▁B ▁1 ▁B ▁< s > ▁contains ▁value ▁all ▁value ▁empty ▁values
▁Re - arr anging ▁a ▁single ▁column ▁of ▁strings ▁based ▁on ▁text ▁containing ▁different ▁dates , ▁by ▁date ▁< s > ▁I ▁am ▁looking ▁to ▁arrange ▁a ▁dataframe ▁by ▁dates , ▁however , ▁the ▁dates ▁are ▁a ▁part ▁of ▁a ▁string ▁within ▁each ▁row . ▁The ▁rows ▁must ▁be ▁re arr anged ▁in ▁order ▁by ▁day . ▁Other ▁solutions ▁from ▁stack ▁overflow ▁show ▁how ▁to ▁sort ▁based ▁on ▁a ▁column ▁of ▁dates ▁alone , ▁this ▁example ▁is ▁different ▁because ▁other ▁information ▁is ▁a ▁part ▁of ▁each ▁string ▁and ▁is ▁mixed ▁with ▁the ▁dates . ▁The ▁dataframe ▁is ▁one ▁column ▁with ▁an ▁index , ▁but ▁the ▁rows ▁are ▁not ▁arr anged ▁in ▁order ▁from ▁the ▁dates ▁contained ▁on ▁the ▁far ▁right ▁side ▁of ▁each ▁string . ▁The ▁score ▁numbers ▁are ▁random ▁and ▁do ▁not ▁require ▁any ▁attention . ▁The ▁expected ▁dataframe ▁should ▁look ▁like ▁this ▁( repeated ▁dates ▁have ▁no ▁preference ▁for ▁order ▁between ▁each ▁other ▁and ▁index ▁doesn ' t ▁matter ). ▁The ▁respective ▁scores ▁must ▁stay ▁with ▁their ▁associated ▁dates . ▁What ▁is ▁a ▁way ▁to ▁do ▁this ? ▁< s > ▁0 ▁_ ________________ ________ _ ▁0 ▁score 17 ▁6 -20 -1 9. xlsx ▁1 ▁score 23 ▁6 -7 -1 9. xlsx ▁2 ▁score 4 ▁6 -17 -1 9. xlsx ▁3 ▁score 34 ▁6 -8 -1 9. xlsx ▁4 ▁score 10 ▁6 -7 -1 9. xlsx ▁< s > ▁0 ▁_ ________________ ________ _ ▁1 ▁score 23 ▁6 -7 -1 9. xlsx ▁4 ▁score 10 ▁6 -7 -1 9. xlsx ▁3 ▁score 34 ▁6 -8 -1 9. xlsx ▁2 ▁score 4 ▁6 -17 -1 9. xlsx ▁0 ▁score 17 ▁6 -20 -1 9. xlsx ▁< s > ▁date ▁day ▁stack ▁index ▁right ▁any ▁between ▁index
▁Convert ▁dictionary ▁with ▁sub - list ▁of ▁dictionaries ▁into ▁pandas ▁dataframe ▁< s > ▁I ▁have ▁this ▁code ▁with ▁a ▁dictionary ▁" dict ": ▁The ▁result ▁is : ▁But ▁what ▁I ▁want ▁is : ▁I ▁would ▁like ▁to ▁obtain ▁this , ▁without ▁using ▁loops ▁in ▁python , ▁and ▁by ▁using ▁pandas . ▁Can ▁anyone ▁help ▁me ▁out ? ▁Thanks ▁in ▁advance ! ▁< s > ▁0 ▁2000 ▁{' team ': ▁' M anch ester ▁Unit ed ', ▁' points ': ▁'9 1' } ▁2001 ▁{' team ': ▁' M anch ester ▁Unit ed ', ▁' points ': ▁' 80 '} ▁2002 ▁{' team ': ▁' Ar sen al ', ▁' points ': ▁' 87 '} ▁< s > ▁team ▁points ▁2000 ▁M anch ester ▁Unit ed ▁91 ▁2001 ▁M anch ester ▁Unit ed ▁80 ▁2002 ▁Ar sen al ▁87 ▁< s > ▁sub
▁Pandas : ▁Create ▁dataframe ▁from ▁data ▁and ▁column ▁order ▁< s > ▁what ▁i ' m ▁asking ▁must ▁be ▁something ▁very ▁easy , ▁but ▁i ▁honestly ▁can ' t ▁see ▁it .... ▁:( ▁I ▁have ▁an ▁array , ▁lets ▁say ▁and ▁i ▁want ▁to ▁put ▁it ▁in ▁a ▁dataframe . ▁I ▁do ▁aim ing ▁for : ▁but ▁i ▁am ▁getting : ▁( notice ▁the ▁dis cre p ancy ▁between ▁column ▁names ▁and ▁data ) ▁I ▁know ▁i ▁can ▁re - arrange ▁the ▁column ▁names ▁order ▁in ▁the ▁dataframe ▁creation , ▁but ▁i ' m ▁trying ▁to ▁understand ▁how ▁it ▁works . ▁Am ▁i ▁doing ▁something ▁wrong , ▁or ▁it ' s ▁normal ▁behaviour ? ▁( why ▁though ?) ▁< s > ▁col 1 ▁col 2 ▁col 3 ▁1 ▁2 ▁3 ▁4 ▁5 ▁6 ▁7 ▁8 ▁9 ▁10 ▁11 ▁12 ▁< s > ▁col 3 ▁col 1 ▁col 2 ▁1 ▁2 ▁3 ▁4 ▁5 ▁6 ▁7 ▁8 ▁9 ▁10 ▁11 ▁12 ▁< s > ▁array ▁put ▁between ▁names ▁names
▁How ▁to ▁split ▁dataframe ▁made ▁from ▁objects ? ▁< s > ▁I ▁want ▁to ▁split ▁one ▁column ▁pandas ▁dataframe ▁that ▁look ▁like ▁this : ▁into ▁two ▁columns : ▁So ▁it ▁can ▁look ▁like ▁this : ▁But ▁its ▁showing ▁type ▁as : ▁< s > ▁0 ▁0 ▁38 ▁A ▁1 ▁35 ▁B ▁2 ▁14 ▁B ▁< s > ▁Number ▁Let ter ▁0 ▁38 ▁A ▁1 ▁35 ▁B ▁2 ▁14 ▁B ▁< s > ▁columns
▁How ▁to ▁read ▁list ▁of ▁json ▁objects ▁from ▁Pandas ▁DataFrame ? ▁< s > ▁I ▁want ▁just ▁want ▁to ▁loop ▁through ▁the ▁array ▁of ▁json ▁objects , ▁and ▁get ▁the ▁values ▁of ▁' box ' ..... ▁I ▁have ▁a ▁DataFrame ▁which ▁looks ▁like ▁this ▁and ▁the ▁column ▁' faces Json ' ▁( dst ype ▁= ▁object ) ▁contain ▁array ▁of ▁json ▁objects ▁which ▁look ▁like ▁this : ▁when ▁i ▁run ▁this ▁code ▁i ▁get ▁this ▁error : ▁< s > ▁img ▁faces Json ▁0 ▁2 b 26 mn 4. jpg ▁[{' box ': ▁[5 7, ▁255, ▁9 1, ▁103 ], ▁' confidence ': ▁0.7 ... ▁1 ▁cd 7 nt f . jpg ▁[{' box ': ▁[5 10, ▁8 5, ▁5 8, ▁87 ], ▁' confidence ': ▁0.99 ... ▁2 ▁m 9 k f 3 e . jpg ▁[{' box ': ▁[ 32 8, ▁7 8, ▁9 3, ▁123 ], ▁' confidence ': ▁0.9 ... ▁3 ▁b 4 h x 0 n . jpg ▁[{' box ': ▁[12 9, ▁30, ▁3 8, ▁54 ], ▁' confidence ': ▁0.99 ... ▁4 ▁a fx 0 fm . jpg ▁[{' box ': ▁[ 86, ▁12 6, ▁22 1, ▁2 98 ], ▁' confidence ': ▁0 .... ▁< s > ▁[ ▁{ ▁" box ":[ ▁15 8, 11 5, 8 4, 112 ▁], ▁" confidence ": 0. 999 89 29 500 5 798 34, ▁}, ▁{ ▁" box ":[ ▁40 4, 10 5, ▁8 6, 114 ▁], ▁" confidence ": 0. 99 96 86 360 359 19 19, ▁} ▁] ▁< s > ▁DataFrame ▁array ▁get ▁values ▁DataFrame ▁array ▁get
▁Drop ▁a ▁pandas ▁DataFrame ▁row ▁that ▁comes ▁after ▁a ▁row ▁that ▁contains ▁a ▁particular ▁value ▁< s > ▁I ▁am ▁trying ▁to ▁drop ▁all ▁rows ▁that ▁come ▁after ▁a ▁row ▁which ▁has ▁inside ▁the ▁column ▁df : ▁Required ▁output ▁df : ▁Look ▁at ▁the ▁following ▁code : ▁Returns ▁a ▁error ▁message ▁I ▁have ▁tried ▁many ▁different ▁variations ▁of ▁this ▁using ▁different ▁methods ▁like ▁and ▁but ▁I ▁can ' t ▁seem ▁to ▁figure ▁it ▁out ▁anyway . ▁I ▁have ▁also ▁tried ▁truncate : ▁This ▁returns : ▁IndexError : ▁index ▁1 ▁is ▁out ▁of ▁bounds ▁for ▁axis ▁0 ▁with ▁size ▁1 ▁< s > ▁Am m end ▁0 ▁no ▁1 ▁yes ▁2 ▁no ▁3 ▁no ▁4 ▁yes ▁5 ▁no ▁< s > ▁Am m end ▁0 ▁no ▁1 ▁yes ▁3 ▁no ▁4 ▁yes ▁< s > ▁DataFrame ▁contains ▁value ▁drop ▁all ▁at ▁truncate ▁index ▁size
▁Rep lic ate ▁multiple ▁rows ▁of ▁events ▁for ▁specific ▁IDs ▁multiple ▁times ▁< s > ▁I ▁have ▁a ▁call ▁log ▁data ▁made ▁on ▁customers . ▁Which ▁looks ▁something ▁like ▁below , ▁where ▁ID ▁is ▁customer ▁ID ▁and ▁A ▁and ▁B ▁are ▁log ▁attributes : ▁I ▁want ▁to ▁replicate ▁each ▁set ▁of ▁event ▁for ▁each ▁ID ▁based ▁on ▁some ▁slots . ▁For ▁e . g . ▁if ▁slot ▁value ▁is ▁2 ▁then ▁all ▁events ▁for ▁ID ▁" A " ▁should ▁be ▁replic ated ▁slot -1 ▁times . ▁and ▁a ▁new ▁Index ▁should ▁be ▁created ▁indicating ▁which ▁slot ▁does ▁replic ated ▁values ▁belong ▁to : ▁I ▁have ▁tried ▁following ▁solution : ▁it ▁gives ▁me ▁the ▁expected ▁output ▁but ▁is ▁not ▁scal able ▁when ▁slots ▁are ▁increased ▁and ▁number ▁of ▁customers ▁increases ▁in ▁order ▁of ▁10 k . ▁I ▁think ▁its ▁taking ▁a ▁long ▁time ▁because ▁of ▁the ▁loop . ▁Any ▁solution ▁which ▁is ▁vectorized ▁will ▁be ▁really ▁helpful . ▁< s > ▁ID ▁A ▁B ▁A ▁A ▁46 ▁31 ▁A ▁A ▁99 ▁54 ▁A ▁A ▁34 ▁9 ▁B ▁B ▁46 ▁48 ▁B ▁B ▁7 ▁75 ▁C ▁C ▁1 ▁25 ▁C ▁C ▁71 ▁40 ▁C ▁C ▁74 ▁53 ▁D ▁D ▁57 ▁17 ▁D ▁D ▁19 ▁78 ▁< s > ▁ID ▁A ▁B ▁A ▁A ▁46 ▁31 ▁A ▁A ▁99 ▁54 ▁A ▁A ▁34 ▁9 ▁A ▁A ▁46 ▁31 ▁A ▁A ▁99 ▁54 ▁A ▁A ▁34 ▁9 ▁< s > ▁where ▁value ▁all ▁Index ▁values ▁time
▁from ▁two ▁arrays ▁to ▁one ▁dataframe ▁python ▁< s > ▁I ▁am ▁trying ▁to ▁put ▁my ▁values ▁into ▁two ▁arrays ▁and ▁then ▁to ▁make ▁them ▁a ▁dataframe . ▁I ▁am ▁using ▁python , ▁numpy ▁and ▁pandas ▁to ▁do ▁so . ▁my ▁arrays ▁are : ▁and ▁I ▁would ▁like ▁to ▁put ▁them ▁into ▁a ▁pandas ▁dataframe . ▁When ▁I ▁print ▁my ▁dataframe , ▁I ▁would ▁like ▁to ▁see ▁this : ▁How ▁can ▁I ▁do ▁that ? ▁I ▁read ▁some ▁related ▁questions , ▁but ▁I ▁can ' t ▁get ▁it ▁right . ▁One ▁of ▁the ▁errors ▁says ▁that ▁indexes ▁must ▁not ▁be ▁tuples , ▁but , ▁as ▁you ▁can ▁see , ▁I ▁don ' t ▁have ▁tuples ▁< s > ▁k ▁= ▁[7 .0, ▁8 .0, ▁6. 55, ▁7. 000000 1, ▁10.1 2] ▁p ▁= ▁[ 6. 9 4, ▁9 .0, ▁4. 4444 4, ▁13 .0, ▁9.0 8 76 ] ▁< s > ▁a ▁b ▁c ▁d ▁e ▁k ▁7.0 ▁8.0 ▁6. 6 ▁7.0 ▁10.1 ▁p ▁6. 9 ▁9.0 ▁4. 4 ▁13 .0 ▁9 .1 ▁< s > ▁put ▁values ▁put ▁get ▁right
▁Sh if ting ▁and ▁revert ing ▁multiple ▁rows ▁in ▁pandas ▁dataframe ▁< s > ▁I ▁have ▁the ▁following ▁dataframe ▁and ▁wish ▁to ▁shift ▁over ▁the ▁0 ▁values ▁to ▁the ▁right ▁and ▁then ▁revert ▁each ▁row : ▁This ▁is ▁the ▁result ▁I ▁would ▁like ▁to ▁get : ▁I ' ve ▁tried ▁vari us ▁shift ▁and ▁apply ▁combinations ▁without ▁any ▁success . ▁Is ▁there ▁a ▁simple ▁way ▁of ▁achieving ▁this ? ▁< s > ▁H 00 ▁H 01 ▁H 02 ▁H 03 ▁H 04 ▁H 05 ▁H 06 ▁N R ▁1 ▁33 ▁28 ▁98 ▁97 ▁0 ▁0 ▁0 ▁2 ▁29 ▁24 ▁22 ▁98 ▁97 ▁0 ▁0 ▁3 ▁78 ▁76 ▁98 ▁97 ▁0 ▁0 ▁0 ▁4 ▁16 ▁15 ▁98 ▁97 ▁0 ▁0 ▁0 ▁5 ▁81 ▁72 ▁70 ▁98 ▁97 ▁0 ▁0 ▁< s > ▁H 00 ▁H 01 ▁H 02 ▁H 03 ▁H 04 ▁H 05 ▁H 06 ▁N R ▁1 ▁97 ▁98 ▁28 ▁33 ▁0 ▁0 ▁0 ▁2 ▁97 ▁98 ▁22 ▁24 ▁29 ▁0 ▁0 ▁3 ▁97 ▁98 ▁76 ▁78 ▁0 ▁0 ▁0 ▁4 ▁97 ▁98 ▁15 ▁16 ▁0 ▁0 ▁0 ▁5 ▁97 ▁98 ▁70 ▁72 ▁81 ▁0 ▁0 ▁< s > ▁shift ▁values ▁right ▁get ▁shift ▁apply ▁any
▁Merge ▁pandas ▁dataframes ▁by ▁timestamps ▁< s > ▁I ' ve ▁got ▁a ▁few ▁pandas ▁dataframes ▁indexed ▁with ▁timestamps ▁and ▁I ▁would ▁like ▁to ▁merge ▁them ▁into ▁one ▁dataframe , ▁matching ▁nearest ▁timestamp . ▁So ▁I ▁would ▁like ▁to ▁have ▁for ▁example : ▁What ▁exact ▁timestamp ▁there ▁is ▁going ▁to ▁be ▁in ▁final ▁DataFrame ▁is ▁not ▁important ▁to ▁me . ▁BTW . ▁Is ▁there ▁an ▁easy ▁way ▁to ▁l eter ▁convert ▁" absolute " ▁timestamps ▁into ▁time ▁from ▁start ▁( either ▁in ▁seconds ▁or ▁mil ise conds )? ▁So ▁for ▁this ▁example : ▁< s > ▁a ▁= ▁CPU ▁20 21 -03 -25 ▁13 :40 :4 4. 208 ▁7 0. 57 17 97 ▁20 21 -03 -25 ▁13 :40 :4 4. 7 23 ▁14 .1 268 70 ▁20 21 -03 -25 ▁13 :40 :45 . 228 ▁17 .1 8 28 44 ▁b ▁= ▁X ▁Y ▁20 21 -03 -25 ▁13 :40 :44 .1 93 ▁45 ▁1 ▁20 21 -03 -25 ▁13 :40 :4 4. 707 ▁46 ▁1 ▁20 21 -03 -25 ▁13 :40 :45 .2 16 ▁50 ▁2 ▁a ▁+ ▁b ▁= ▁CPU ▁X ▁Y ▁20 21 -03 -25 ▁13 :40 :4 4. 208 ▁7 0. 57 17 97 ▁45 ▁1 ▁20 21 -03 -25 ▁13 :40 :4 4. 7 23 ▁14 .1 268 70 ▁46 ▁1 ▁20 21 -03 -25 ▁13 :40 :45 . 228 ▁17 .1 8 28 44 ▁50 ▁2 ▁< s > ▁CPU ▁X ▁Y ▁0.0 ▁7 0. 57 17 97 ▁45 ▁1 ▁0.5 ▁14 .1 268 70 ▁46 ▁1 ▁1.0 ▁17 .1 8 28 44 ▁50 ▁2 ▁< s > ▁merge ▁timestamp ▁timestamp ▁DataFrame ▁time ▁start ▁seconds
▁Pandas ▁DataFrame ▁from ▁Dictionary ▁< s > ▁Let ' s ▁assume ▁that ▁I ▁have ▁a ▁JSON ▁file ▁like ▁below , ▁and ▁I ▁want ▁to ▁convert ▁this ▁file ▁into ▁a ▁data ▁frame ▁with ▁2 ▁columns : ▁This ▁is ▁what ▁I ▁already ▁tried , ▁but ▁I ▁am ▁unable ▁to ▁create ▁DF ▁and ▁probably ▁there ▁is ▁a ▁more ▁efficient ▁way ▁of ▁doing ▁this ▁task : ▁Expecting ▁output ▁like ▁this ▁for ▁all ▁elements ▁in ▁a ▁Json ▁file ▁Also ▁I ▁want ▁to ▁create ▁a ▁directed ▁graph ▁for ▁each ▁parent ▁node ▁with ▁child ▁nodes ▁as ▁clusters ▁because ▁parent ▁nodes ▁has ▁the ▁same ▁child ▁nodes ▁which ▁are ▁also ▁present ▁in ▁other ▁parent ▁nodes . ▁Thanks ▁in ▁Advance ▁< s > ▁{" 10 87 ": ▁[4, 5,6, 7,8, 9, 10, 12, 13, 21, 22, 23, 24, 25, 26, 27, 28, 34, 35 ▁, 37, 39, 4 0, 4 2, 4 4, 4 5, 4 6, 4 7, 48, 5 1, 5 2, 5 4, 55, 56, 59 , 6 0, 6 1, 6 3, 64, 6 5,6 6, 6 7, 68 , 7 2, 7 3, 7 4, 7 5, 78, 8 0, 8 1, 8 2, 8 3, 8 4, 8 5, 8 7, 88 , 9 2, 9 4, 9 6, 9 7, 9 8, 99, 100, 101 , 10 2, 10 3, 10 4, 10 5, 10 6, 10 7, 10 8, 10 9, 1 10, 11 1, 1 12, 11 3, 11 4, 11 5, 11 6, 11 7, 11 8, 11 9, 12 0, 12 1, 12 2, 12 3, 12 5, 12 7, 12 8, 12 9, 13 0, 13 1, 1 32, 13 3, 1 34, 13 5, 1 36, 1 37, 1 38, 1 39, 14 0, 14 1, 14 2, 14 3, 14 4, 14 5, 14 9, 18 0, 18 1, 19 6, 19 8, 200, 20 2, 20 6, 22 2, 22 3, 22 6, 22 7, 23 0, 23 1, 2 32, 23 3, 234 , 23 5, 24 2, 255, 25 7, 25 8, 25 9, 26 1, 26 3, 2 64, 26 5, 26 7, 26 8, 26 9, 27 0, 27 1, 27 2, 27 3, 27 4, 27 5, 27 6, 27 7, 2 78, 27 9, 28 0, 28 1, 28 2, 28 3, 28 4, 28 5, 28 6, 28 7, 28 8, 28 9, 29 0, 29 1, 29 2, 29 3, 29 4, 29 5, 29 6, 29 7, 29 8, 2 99, 3 00, 30 2, 30 3, 30 4, 30 5, 30 6, 30 7, 30 8, 30 9, 3 10, 31 1, 31 3, 3 14, 3 16, 3 18, 3 19, 32 0, 32 3, 32 5, 32 6, 32 7, 32 8, 33 0, 3 34, 33 6, 33 7, 33 9, 34 0, 34 2, 34 3, 35 0, 35 1, 35 4, 3 55, 36 2, 36 3, 36 5, 36 6, 36 7, 36 8, 36 9, 37 0, 37 1, 37 2, 37 4, 37 5, 37 6, 37 7, 3 78, 37 9, 38 0, 38 3, 38 5, 38 6, 38 7, 38 8, 38 9, 39 0, 39 1, 39 2, 39 3, 39 4, 39 5, 39 6, 39 7, 39 8, 3 99, 4 00, 4 01, 40 2, 40 3, 40 4, 40 5, 40 6, 40 7, 40 8, 40 9, 4 10, 41 1, 4 12, 41 3, 4 14, 4 15, 4 16, 4 17, 4 18, 4 19, 4 20, 42 1, 42 2, 42 3, 4 24, 4 27, 4 28, 4 29, 43 0, 43 1, 4 32, 43 3, 4 34, 43 5, 4 37, 4 38, 44 4, 44 6, 44 9, 45 1, 4 55, 45 7, 46 1, 4 64, 46 5, 46 6, 46 7, 4 68 , 46 9, 47 0, 47 1, 47 2, 47 3, 47 4, 47 5, 4 76, 47 7, 4 78, 4 79, 48 0, 48 1, 48 2, 48 3, 48 4, 48 5, 48 6, 48 7, 48 8, 48 9, 49 0, 49 1, 49 2, 49 4, 49 6, 49 8, 4 99, 500, 50 2, 50 4, 50 6, 50 8, 50 9, 5 10, 51 1, 5 12, 51 3, 5 14, 5 15, 5 16, 5 17, 5 18, 5 19, 5 20, 52 1, 52 2, 52 3, 5 24, 5 25, 5 26, 5 27, 5 28, 5 29, 53 0, 53 1, 5 32, 53 3, 5 34, 53 5, 5 36, 5 37, 5 38, 5 39, 54 1, 54 2, 54 3, 54 4, 54 5, 54 6, 54 7, 5 48, 5 49, 55 0, 55 1, 55 2, 55 3, 55 4, 55 5, 55 9, 56 0, 56 1, 56 5, 56 7, 56 9, 57 1, 57 3, 57 4, 5 76, 5 79, 58 0, 58 1, 58 3, 5 86, 58 7, 5 88 , 59 0, 59 3, 59 4, 59 7, 59 8, 6 00, 6 01, 60 2, 60 4, 60 5, 60 6, 60 7, 60 8, 60 9, 61 1, 6 12, 61 3, 6 14, 6 15, 6 16, 6 17, 6 20, 62 1, 62 2, 6 24, 6 25, 6 26, 6 29, 63 1, 63 3, 6 34, 6 36, 6 38, 6 39, 64 0, 64 1, 64 3, 64 4, 64 7, 64 9, 65 0, 65 1, 65 2, 65 3, 65 4, 65 7, 65 8, 6 64, 66 5, 66 6, 66 7, 66 9, 67 1, 67 4, 67 5,6 76, 67 7, 6 78, 68 2, 68 3, 68 5,6 86, 68 7, 68 8, 69 2, 69 4, 69 5, 70 2, 70 3, 70 5, 70 8, 7 12, 71 3, 7 14, 7 15, 7 16, 7 17, 7 18, 7 20, 7 28, 7 32, 7 34, 73 5, 7 39, 74 0, 74 2, 74 3, 74 5, 74 6, 75 1, 75 2, 7 59 , 76 9, 77 0, 77 2, 7 78, 7 79, 78 0, 78 3, 78 4, 78 6, 79 2, 80 5, 8 15, 82 3, 83 1, 8 32, 8 34, 83 5, 8 36, 8 37, 8 38, 8 39, 85 2, 85 4, 8 55, 8 56, 86 7, 87 5, 87 7, 8 79, 88 8, 89 0, 89 1, 89 6, 9 00, 90 8, 90 9, 9 10, 91 1, 9 12, 91 3, 9 14, 9 15, 9 16, 9 17, 9 18, 9 19, 9 34, 93 5, 9 36, 9 37, 9 38, 9 39, 94 4, 94 5, 94 6, 95 0, 95 1, 95 2, 95 3, 95 7, 95 8, 9 59 , 96 0, 9 64, 96 5, 96 6, 96 7, 97 1, 97 5, 97 7, 9 78, 98 0, 98 1, 98 2, 9 86, 98 7, 9 88 , 99 3, 99 4, 99 5, 99 6, 1 000, 100 1, 100 2, 100 3, 10 27, 10 28, 103 3, 10 34, 103 5, 10 36, 10 37, 10 38, 10 39, 10 49, 106 1, 106 3, 106 5, 106 7, 106 9, 107 0, 107 1, 107 2, 107 3, 107 4, 10 76, 107 7, 10 78, 108 0, 108 1, 108 4, 10 88 , 11 14, 11 15, 11 16, 11 17, 11 18, 11 19, 112 0, 112 1, 112 2, 112 3, 112 7, 112 8, 112 9, 11 30, 11 32, 11 3 3, 11 34, 11 3 5, 11 36, 11 37, 11 38, 11 39, 114 0, 114 1, 115 1, 11 55, 11 56, 12 01, 120 2, 120 3, 120 4, 120 7, 120 8, 120 9, 12 13, 12 14, 12 15, 12 16, 12 17, 12 20, 122 1, 122 2, 122 3, 12 24, 12 32, 123 3, 123 5, 12 37, 12 38, 124 1, 124 3, 124 4, 124 5, 12 48, 12 49, 125 1, 125 4, 126 9, 127 1, 127 3, 127 4, 127 5, 128 4, 12 89 , 129 8, 13 01, 1 30 2, 1 30 3, 147 0, 149 5, 15 00, 15 01, 150 8, 150 9, 15 17, 15 18, 157 2, 157 3, 157 4, 157 5, 16 14, 16 19, 16 20, 16 25, 16 3 3, 16 39, 166 1, 166 9, 167 0, 167 1, 169 2, 169 3, 169 4, 169 5, 169 6, 169 8, 16 99, 17 00, 17 01, 170 6, 170 7, 170 8, 170 9, 17 11, 17 12, 17 13, 17 15, 17 20, 17 26, 17 28, 17 29, 17 30, 17 3 1, 17 32, 17 34, 17 55, 177 1, 1 78 0, 1 78 1, 1 78 5, 1 78 8, 179 4, 179 5, 179 7, 18 01, 180 2, 180 3, 180 5, 18 27, 18 29, 18 30, 18 36, 18 38, 184 3, 184 5, 184 7, 18 49, 185 1, 185 2, 185 3, 185 4, 18 55, 189 7, 18 99, 19 01, 19 20, 192 2, 192 3, 197 4, 198 7, 19 88 , 19 89 , 199 0, 199 1, 199 3, 199 4, 201 3, 201 4, 20 38, 20 39, 204 0, 204 4, 205 7, 20 86, 210 8, 214 4, 2 15 0, 22 15, 22 16, 22 18, 22 19, 22 20, 22 27, 22 28, 22 29, 22 30, 225 0, 225 8, 227 1, 22 79, 228 3, 228 5, 22 86, 228 7, 229 5, 2 30 2, 232 7, 239 0, 239 7, 240 6, 240 7, 24 11, 24 13, 24 14, 24 15, 24 19, 24 21, 24 29, 244 1, 247 1, 247 2, 249 0, 249 3, 250 7, 25 14, 25 19, 25 24, 25 25, 253 1, 25 32, 253 5, 25 38, 254 1, 255 1, 255 2, 255 3, 25 55, 256 0, 256 1, 256 2, 25 64, 257 0, 257 7, 25 78, 25 79, 258 0, 258 1, 25 86, 258 7, 25 88 , 25 89 , 259 1, 259 2, 259 4, 259 5, 259 6, 259 7, 25 99, 26 00, 26 01, 260 2, 260 3, 260 4, 260 5, 260 8, 260 9, 26 10, 26 11, 26 12, 26 13, 26 14, 26 15, 26 16, 26 17, 26 18, 26 19, 26 20, 26 21, 26 22, 26 23, 26 25, 26 26, 26 27, 26 28, 26 29, 26 30, 26 3 1, 26 34, 26 3 5, 266 5, 26 68 , 266 9, 267 1, 267 3, 268 1, 268 2, 268 3, 268 4, 270 5, 270 6, 270 7, 270 8, 270 9, 27 10, 27 11, 27 12, 27 13, 27 50, 276 6, 27 68 , 276 9, 277 0, 279 8, 2 80 4, 28 17, 28 21, 28 22, 28 23, 28 24, 28 25, 28 26, 284 4, 284 7, 285 3, 28 55, 285 8, 286 0, 286 1, 286 2, 286 3, 28 64, 286 5, 288 0, 29 00, 29 01, 290 2, 290 3, 290 6, 29 11, 29 12, 29 13, 29 16, 29 18, 29 22, 29 25, 29 26, 29 32, 29 3 5, 294 1, 294 3, 294 5, 294 7, 29 48, 29 49, 29 50, 295 8, 29 59 , 296 6, 296 7, 29 7 2, 29 76, 29 7 7, 29 78, 29 79, 298 0, 298 1, 298 2, 298 7, 29 88 , 2 99 1, 2 99 2, 2 99 3, 2 99 4, 2 99 5, 2 99 6, 2 99 9, 300 1, 300 3, 300 7, 300 8, 301 1, 301 2,3 01 5, 301 6, 301 7, 301 8, 30 24, 30 30, 303 1, 303 3, 30 34, 304 5, 305 3, 305 4, 30 55, 30 56, 305 7, 305 8, 30 59 , 306 0, 306 1, 306 2, 306 3, 30 64, 306 5, 306 6, 306 7, 30 68 , 306 9, 307 0, 307 1, 307 2, 309 3, 30 99, 3 10 5, 31 12, 3 11 3, 3 11 4, 3 11 5, 3 11 6, 3 11 7, 312 7, 312 8, 315 4, 31 55, 31 56, 315 7, 329 7, 32 99, 33 00, 3 30 6, 33 10, 33 11, 33 12, 33 17, 33 18, 33 19, 3 32 0, 3 32 1, 3 32 2,3 32 3, 3 32 4, 3 32 5, 3 32 6, 3 32 7, 3 32 8, 3 32 9, 333 0, 33 36, 33 39, 34 16, 34 17, 34 20, 34 24, 3 55 0, 358 7, 35 88 , 35 89 , 359 0, 359 1, 359 2, 359 3, 359 8, 35 99, 36 00, 360 2, 360 3, 360 4, 360 5, 360 6, 360 8, 360 9, 36 10, 36 12, 36 13, 36 14, 36 15, 36 16, 36 17, 36 18, 36 25, 36 55, 36 56, 365 7, 37 18, 37 21, 37 24, 37 25, 37 26, 37 30, 37 32, 37 3 3, 37 36, 37 38, 374 1, 374 3, 374 4, 374 7, 37 48, 375 0, 375 2, 375 4, 37 56, 376 2, 376 3, 37 64, 376 5, 376 6, 377 0, 377 3, 37 76, 37 79, 3 78 0, 3 78 1, 3 78 2,3 78 3, 3 78 4, 3 78 5, 3 78 6, 3 78 7, 3 78 8, 3 78 9, 379 0, 379 5, 379 7, 379 8, 38 00, 3 80 6, 38 11, 38 64, 386 6, 386 7, 387 1, 388 1, 388 3, 388 4, 388 5, 38 86, 39 25, 39 26, 39 29, 39 30, 39 3 5, 39 36, 39 4 0, 4 01 8, 40 30, 404 5, 40 49, 40 50, 405 1, 405 4, 405 8, 40 59 , 406 2, 406 3, 40 64, 406 5, 406 6, 406 7, 40 68 , 406 9, 407 0, 407 1, 407 2, 407 3, 407 4, 407 5, 40 76, 407 7, 40 78, 40 79, 408 0, 408 1, 408 2, 408 3, 408 4, 408 5, 40 86, 408 7, 40 88 , 40 89 , 409 1, 409 2, 409 3, 409 4, 409 5, 409 6, 409 7, 409 8, 40 99, 4 10 4, 4 10 9, 41 10, 4 11 3, 4 11 6, 4 11 7, 4 11 8, 4 11 9, 4 16 1, 4 26 7, 4 28 5, 43 10, 43 17, 4 33 5, 4 35 8, 4 35 9, 4 36 5, 4 36 6, 44 6 7, 44 7 1, 44 7 5, 45 00, 45 01, 4 50 2, 4 50 3, 4 50 4, 4 50 5, 4 50 6, 4 50 7, 4 50 8, 4 50 9, 45 10, 45 11, 45 12, 45 13, 45 14, 45 15, 45 16, 45 17, 45 18, 45 19, 45 20, 45 21, 45 22, 45 23, 45 24, 45 25, 45 26, 45 27, 45 28, 45 29, 45 30, 45 3 1, 45 32, 45 3 3, 45 34, 45 3 5, 45 36, 45 37, 45 38, 45 39, 454 0, 454 1, 454 2, 454 3, 454 4, 454 5, 454 6, 454 7, 45 48, 45 49, 4 55 0, 4 55 1, 4 55 2, 4 55 3, 4 55 4, 4 55 5, 45 56, 4 55 7, 4 55 8, 4 55 9, 456 0, 456 1, 456 2, 456 3, 45 64, 456 5, 456 6, 456 7, 456 8, 456 9, 457 0, 457 1, 457 2, 457 3, 457 4, 457 5, 45 76, 457 7, 45 78, 45 79, 458 0, 458 1, 458 2, 458 3, 458 4, 458 5, 45 86, 458 7, 45 88 , 45 89 , 459 0, 459 1, 459 2, 459 3, 459 4, 459 5, 459 6, 459 7, 459 8, 45 99, 46 16, 46 38, 46 39, 47 64, 4 76 5, 4 76 6, 4 78 2, 4 80 3, 48 24, 48 27, 48 28, 48 30, 48 88 , 49 13, 49 14, 49 15, 49 16, 49 17, 49 18, 49 19, 49 20, 49 21, 49 22, 49 23, 49 26, 4 99 0, 6 99 8, 70 26, 70 27, 70 28 ], " 10 96 ": ▁[ 25, 26, 27, 28, 4 5, 4 6, 6 3, 64, 6 5,6 6, 6 7, 8 0, 8 1, 8 2, 8 3, 8 4, 8 5, 12 8, 12 9, 13 0, 13 1, 1 32, 13 3, 1 34, 13 5, 1 36, 1 37, 1 38, 1 39, 14 0, 14 1, 26 3, 2 64, 26 7, 26 8, 26 9, 27 1, 27 2,3 14, 33 0, 36 6, 36 7, 37 6, 38 5, 38 6, 38 7, 38 8, 39 1, 4 17, 4 18, 4 19, 4 20, 4 37, 44 9, 45 1, 55 3, 55 5, 55 9, 56 9, 57 3, 57 4, 5 76, 5 79, 58 0, 58 1, 58 3, 5 86, 58 7, 5 88 , 59 0, 59 3, 59 4, 59 7, 6 00, 6 01, 60 2, 60 4, 60 7, 60 8, 60 9, 61 1, 6 14, 6 15, 6 16, 6 24, 6 25, 6 26, 6 34, 6 36, 6 39, 64 0, 64 1, 64 3, 64 4, 7 79, 78 0, 9 36, 9 37, 9 38, 9 39, 94 4, 94 5, 94 6, 95 0, 95 1, 95 2, 95 3, 95 8, 9 59 , 96 0, 9 64, 96 5, 96 6, 96 7, 98 2, 9 86, 98 7, 9 88 , 99 3, 99 4, 99 5, 99 6, 1 000, 100 1, 100 2, 10 76, 11 32, 11 3 3, 11 34, 11 3 5, 11 36, 11 37, 11 38, 11 39, 114 0, 114 1, 127 3, 127 4, 127 5, 12 78, 128 0, 12 89 , 129 2, 167 0, 167 1, 17 13, 17 30, 17 3 1, 184 7, 18 49, 199 3, 20 86, 22 18, 22 19, 22 20, 225 8, 24 21, 25 86, 258 7, 260 8, 26 10, 26 11, 26 29, 26 3 1, 267 3, 270 8, 270 9, 27 10, 27 11, 27 12, 27 13, 28 21, 28 22, 28 23, 28 25, 284 4, 284 7, 285 8, 286 0, 286 2, 286 3, 28 64, 286 5, 29 16, 29 22, 29 25, 294 7, 29 48, 29 49, 29 50, 29 59 , 29 76, 29 7 7, 29 78, 29 79, 298 0, 298 1, 298 2, 2 99 1, 2 99 2, 2 99 3, 2 99 4, 2 99 5, 300 1, 300 3, 300 7, 301 1, 301 5, 301 6, 301 7, 301 8, 305 3, 305 4, 30 55, 30 56, 305 7, 305 8, 30 59 , 306 0, 306 1, 306 2, 306 3, 30 64, 306 5, 306 6, 306 7, 30 68 , 306 9, 307 0, 307 1, 307 2, 31 12, 3 11 3, 33 19, 3 32 0, 3 32 1, 3 32 2,3 32 3, 3 32 4, 3 32 5, 3 32 6, 3 32 7, 3 32 8, 34 16, 34 17, 35 89 , 359 0, 359 1, 359 2, 359 3, 359 8, 360 8, 360 9, 36 10, 36 12, 36 13, 36 14, 36 15, 36 16, 36 17, 36 18, 36 56, 365 7, 37 32, 37 38, 374 3, 37 48, 375 0, 375 2, 375 4, 37 56, 376 2, 376 3, 37 64, 377 0, 377 3, 37 76, 37 79, 3 78 0, 3 78 1, 3 78 2,3 78 3, 3 78 4, 3 78 5, 3 78 6, 3 78 7, 3 78 8, 3 78 9, 379 0, 379 7, 379 8, 387 1, 40 64, 406 5, 406 6, 406 7, 40 68 , 406 9, 407 0, 407 1, 407 2, 407 3, 407 4, 407 5, 40 76, 407 7, 40 78, 40 79, 408 0, 408 1, 408 2, 408 3, 408 4, 408 5, 40 86, 408 7, 40 88 , 40 89 , 409 1, 409 2, 409 3, 409 4, 409 5, 409 6, 409 7, 409 8, 40 99, 4 10 9, 41 10, 4 26 7, 4 35 8, 4 35 9, 45 00, 45 01, 4 50 2, 4 50 3, 4 50 4, 4 50 5, 4 50 6, 4 50 7, 4 50 8, 4 50 9, 45 10, 45 11, 45 12, 45 13, 45 14, 45 15, 45 16, 45 17, 45 18, 45 19, 45 20, 45 21, 45 22, 45 23, 45 24, 45 25, 45 26, 45 27, 45 28, 45 29, 45 30, 45 3 1, 45 32, 45 3 3, 45 34, 45 3 5, 45 36, 45 37, 45 38, 45 39, 454 0, 454 1, 454 2, 454 3, 454 4, 454 5, 454 6, 454 7, 45 48, 45 49, 4 55 0, 4 55 1, 4 55 2, 4 55 3, 4 55 4, 4 55 5, 45 56, 4 55 7, 4 55 8, 4 55 9, 456 0, 456 1, 456 2, 456 3, 45 64, 456 5, 456 6, 456 7, 456 8, 456 9, 457 0, 457 1, 457 2, 457 3, 457 4, 457 5, 45 76, 457 7, 45 78, 45 79, 458 0, 458 1, 458 2, 458 3, 458 4, 458 5, 45 86, 458 7, 45 88 , 45 89 , 459 0, 459 1, 459 2, 459 3, 459 4, 459 5, 459 6, 459 7, 459 8, 45 99, 46 16, 47 64, 4 76 5, 4 76 6, 70 26, 70 27, 70 28 ], " 11 44 ": ▁[ 25, 26, 27, 28, 14 4, 37 2, 37 4, 42 2, 76 8, 100 5, 105 1, 105 2, 105 3, 105 4, 105 7, 105 8, 106 0, 106 2, 10 64, 106 6, 10 68 , 109 8, 11 01, 114 6, 170 3, 170 4, 170 5, 17 13, 17 16, 199 4, 20 86, 2 38 2, 309 5, 309 6, 309 7, 3 11 4, 3 11 5, 3 11 6, 33 39, 36 19, 36 20, 36 21, 37 32, 37 38, 374 3, 388 1, 388 3, 388 4, 388 5, 38 86, 4 11 3, 4 11 6, 4 11 7, 4 11 8, 4 11 9, 4 26 7, 4 28 5, 4 36 5, 4 37 0, 4 37 1, 4 37 2, 4 37 3, 4 37 4, 4 37 5, 44 7 1, 47 64, 4 76 5, 4 76 6, 4 80 3, 48 24, 48 28, 48 30, 4 990 ], " -1 ": ▁[4 0, 6 3, 64, 6 5,6 6, 6 7, 68 , 8 0, 8 1, 8 2, 8 3, 8 4, 8 5, 8 7, 13 0, 13 1, 1 32, 13 3, 1 34, 13 5, 1 36, 1 37, 1 38, 1 39, 14 0, 14 1, 234 , 26 1, 26 3, 2 64, 26 7, 26 8, 26 9, 27 1, 27 2, 29 3, 30 8, 3 14, 3 18, 3 19, 33 7, 36 6, 36 7, 37 5, 37 6, 38 5, 38 6, 38 7, 38 8, 39 1, 40 7, 4 16, 4 17, 4 18, 4 19, 4 20, 43 5, 46 1, 48 9, 55 9, 56 1, 57 3, 57 4, 5 76, 5 79, 58 0, 58 1, 58 3, 5 86, 58 7, 5 88 , 59 0, 59 3, 59 4, 59 7, 6 00, 6 01, 60 2, 60 4, 60 7, 60 8, 60 9, 61 1, 6 14, 6 15, 6 16, 62 3, 6 24, 6 25, 6 26, 6 32, 6 34, 6 36, 64 4, 66 6, 68 2, 68 3, 68 5,6 86, 68 7, 68 8, 69 4, 69 5, 69 6, 7 20, 7 37, 85 4, 8 55, 87 0, 88 2, 88 3, 88 8, 89 6, 9 16, 9 17, 9 18, 9 19, 93 0, 9 36, 9 37, 9 38, 9 39, 94 4, 94 5, 94 6, 95 0, 95 1, 95 2, 95 3, 95 8, 9 59 , 96 0, 9 64, 96 5, 96 6, 96 7, 97 1, 9 78, 98 2, 9 86, 98 7, 9 88 , 99 3, 99 4, 99 5, 99 6, 1 000, 100 1, 100 2, 10 36, 10 37, 10 38, 10 39, 108 1, 11 32, 11 3 3, 11 34, 11 3 5, 11 36, 12 10, 13 17, 132 1, 134 1, 134 7, 1 37 7, 13 78, 138 0, 138 3, 138 4, 138 6, 139 6, 139 8, 140 8, 14 10, 14 32, 14 56, 145 8, 147 3, 15 00, 15 01, 15 25, 16 14, 167 0, 17 30, 180 8, 18 38, 198 2, 198 3, 198 4, 198 5, 199 3, 203 3, 20 34, 20 38, 20 39, 204 0, 206 9, 2 15 0, 2 15 1, 225 8, 23 55, 23 56, 257 1, 259 6, 269 2, 27 29, 27 37, 28 21, 28 22, 28 23, 284 4, 284 7, 286 2, 286 3, 28 64, 286 5, 29 16, 29 22, 29 25, 294 7, 29 48, 29 49, 29 50, 29 76, 29 7 7, 29 78, 29 79, 298 0, 298 1, 298 2, 300 7, 301 1, 301 5, 301 6, 301 7, 301 8, 305 3, 305 4, 30 55, 30 56, 305 7, 305 8, 30 59 , 306 0, 306 1, 306 2, 306 3, 30 64, 306 5, 306 6, 306 7, 30 68 , 306 9, 307 0, 315 0, 315 1, 33 16, 34 16, 34 17, 359 4, 374 4, 376 9, 377 0, 377 3, 37 76, 3 78 5, 3 78 6, 3 78 7, 387 1, 40 64, 406 6, 40 68 , 407 0, 407 2, 407 4, 40 76, 40 78, 408 0, 408 2, 408 4, 40 86, 40 88 , 409 2, 409 4, 409 6, 409 8, 4 10 9, 4 28 8, 4 32 1, 4 33 0, 44 6 6, 4 99 1, 5 56 7, 69 13 ], " 20 60 ": ▁[4 7, 6 5,6 7, 8 0, 8 1, 14 8, 1 55, 1 56, 16 6, 16 7, 16 8, 22 6, 22 7, 26 7, 26 8, 26 9, 58 0, 59 4, 59 7, 6 00, 6 01, 60 2, 60 4, 60 7, 60 8, 60 9, 61 1, 6 14, 6 34, 6 36, 68 2, 68 3, 68 5,6 86, 68 7, 68 8, 69 4, 69 5, 69 6, 7 28, 73 3, 7 38, 74 4, 94 4, 94 5, 94 6, 99 3, 99 4, 99 5, 13 17, 132 1, 134 1, 134 7, 1 37 7, 13 78, 138 0, 138 3, 138 4, 138 5, 138 6, 138 7, 139 6, 139 8, 140 8, 14 10, 14 32, 14 56, 145 8, 147 3, 15 25, 169 6, 17 36, 17 37, 17 38, 17 39, 175 4, 180 8, 18 59 , 186 5, 187 3, 18 79, 188 5, 189 2, 192 2, 198 2, 198 3, 198 4, 198 5, 199 3, 199 4, 20 38, 20 39, 204 0, 2 15 0, 2 15 1, 225 4, 23 00, 23 55, 23 56, 2 37 7, 239 1, 24 48, 24 78, 25 30, 25 64, 27 23, 274 2, 274 5, 274 6, 274 7, 288 2, 29 22, 29 25, 294 7, 29 48, 29 49, 29 50, 33 18, 33 19, 3 32 0, 3 32 1, 3 32 2,3 32 3, 3 32 4, 3 32 5, 3 32 6, 3 32 7, 3 32 8, 3 34 1, 338 3, 348 2, 349 3, 349 4, 350 6, 35 30, 367 2, 367 5, 38 21, 43 32, 44 39, 444 0, 4 45 9, 4 90 8, 49 13, 49 14, 49 15, 49 16, 49 17, 49 19, 49 20, 49 21, 49 22, 49 23, 5 000, 500 1, 500 2, 500 3, 500 4, 500 5, 500 6, 500 7, 500 8, 500 9, 5 01 1, 5 01 2, 5 01 3, 5 01 4, 5 01 5, 5 01 6, 5 01 7, 5 01 8, 5 01 9, 50 20, 502 1, 50 22, 50 23, 50 24, 50 25, 50 26, 50 27, 50 28, 50 29, 50 30, 50 3 1, 50 32, 50 3 3, 50 34, 50 3 5, 50 36, 50 37, 50 38, 50 39, 504 0, 504 1, 504 2, 504 3, 504 4, 504 5, 504 6, 504 7, 50 48, 50 49, 50 50, 505 1, 505 2, 505 3, 505 4, 50 55, 50 56, 505 7, 505 8, 50 59 , 506 0, 506 1, 506 2, 506 3, 50 64, 506 5, 506 6, 506 7, 50 68 , 506 9, 507 0, 507 1, 507 2, 507 3, 507 4, 507 5, 50 76, 507 7, 50 78, 50 79, 508 0, 508 1, 508 2, 508 3, 508 4, 508 5, 50 86, 508 7, 50 88 , 50 89 , 509 0, 509 1, 509 2, 509 3, 509 4, 509 5, 509 6, 509 7, 509 8, 50 99, 5 100, 5 101 , 5 10 2, 5 10 3, 5 10 4, 5 10 5, 5 11 5, 5 11 6, 5 11 7, 5 11 8, 5 11 9, 512 0, 512 1, 512 2, 512 3, 512 4, 512 5, 512 6, 512 7, 512 8, 512 9, 5 13 0, 5 13 1, 51 32, 5 13 3, 51 34, 5 13 5, 51 39, 5 14 0, 5 14 1, 5 14 2, 5 14 3, 5 14 4, 5 14 5, 5 14 6, 5 14 7, 5 14 8, 5 14 9, 5 15 0, 5 15 1, 5 15 2, 5 15 3, 5 15 4, 51 55, 51 56, 5 15 7, 5 15 8, 5 15 9, 5 16 0, 5 16 1, 5 16 2, 5 16 3, 51 64, 5 16 5, 5 16 6, 5 16 7, 5 16 8, 5 16 9, 5 17 0, 5 17 1, 5 17 2, 5 17 3, 5 17 4, 5 17 7, 51 78, 5 17 9, 5 18 0, 5 18 1, 5 18 2, 5 18 3, 5 18 4, 5 18 8, 5 19 0, 5 19 1, 5 19 2, 5 19 3, 5 19 4, 5 19 5, 5 19 6, 5 19 7, 5 19 8, 51 99, 5 200, 5 201 , 5 20 2, 5 20 3, 5 20 4, 5 20 5, 5 20 6, 5 20 7, 5 20 8, 5 20 9, 52 10, 5 21 1, 52 12, 5 21 3, 52 14, 52 15, 52 17, 52 18, 52 19, 52 20, 5 28 4, 5 28 5, 5 28 6, 5 28 7, 5 28 8, 5 28 9, 5 29 0, 5 29 1, 5 29 2, 5 29 3, 5 29 4, 5 29 5, 5 29 6, 5 29 7, 5 29 8, 52 99, 53 00, 5 30 3, 5 30 4, 5 30 5, 5 30 6, 5 30 7, 5 30 8, 5 30 9, 53 10, 5 31 1, 53 12, 5 31 3, 53 14, 53 15, 53 16, 53 17, 53 18, 53 19, 5 32 0, 5 32 1, 5 32 2, 5 32 3, 5 32 4, 5 32 5, 5 32 6, 5 32 7, 5 32 8, 5 32 9, 5 33 0, 5 33 1, 53 32, 5 33 3, 53 34, 5 33 5, 5 33 6, 5 33 7, 5 33 8, 5 33 9, 5 34 0, 5 34 1, 5 34 2, 5 34 3, 5 34 4, 5 34 5, 5 34 6, 5 34 7, 5 34 8, 5 34 9, 5 35 0, 5 35 1, 5 35 2, 5 35 3, 5 35 4, 53 55, 53 56, 5 35 7, 5 35 8, 5 35 9, 5 36 0, 5 36 1, 5 36 2, 5 36 3, 53 64, 5 36 5, 5 36 6, 5 36 7, 5 36 8, 5 36 9, 5 37 0, 5 37 1, 5 37 2, 5 37 3, 5 37 4, 5 37 5, 5 37 6, 5 37 7, 53 78, 5 37 9, 5 38 0, 5 38 1, 5 38 2, 5 38 3, 5 38 4, 5 38 5, 5 38 6, 5 38 7, 5 38 8, 5 38 9, 5 39 0, 5 39 1, 5 39 2, 5 39 3, 5 39 4, 5 39 5, 5 39 6, 5 39 7, 53 99, 54 00, 54 01, 5 40 2, 5 40 3, 5 40 4, 5 40 5, 5 40 6, 5 40 7, 5 40 8, 54 10, 54 11, 54 12, 54 13, 54 14, 54 15, 54 16, 54 17, 54 18, 54 19, 54 20, 54 21, 54 22, 54 23, 54 24, 54 25, 54 26, 54 27, 54 28, 54 29, 54 30, 54 3 1, 54 32, 54 3 3, 54 34, 54 3 5, 54 36, 54 37, 54 38, 54 39, 5 44 0, 5 44 1, 5 44 2, 5 44 3, 5 44 4, 5 44 5, 5 44 6, 5 44 7, 54 48, 5 44 9, 5 45 0, 5 45 1, 5 45 2, 5 45 3, 5 45 4, 54 55, 54 56, 5 45 7, 5 45 8, 5 45 9, 5 46 0, 5 46 1, 5 46 2, 5 46 3, 54 64, 5 46 5, 5 46 6, 5 46 7, 54 68 , 5 46 9, 5 47 0, 5 47 1, 5 47 2, 5 47 3, 5 47 4, 5 47 5, 54 76, 5 47 7, 54 78, 54 79, 5 48 0, 5 48 1, 5 48 2, 5 48 3, 5 48 4, 5 48 6, 5 48 7, 5 48 8, 5 48 9, 5 49 0, 5 49 1, 5 49 2, 5 49 3, 5 49 4, 55 01, 5 50 2, 5 50 3, 5 50 4, 5 50 5, 5 50 6, 5 50 7, 5 50 8, 5 50 9, 55 10, 55 11, 55 12, 55 13, 55 14, 55 15, 55 16, 55 17, 55 18, 55 19, 55 20, 55 21, 55 22, 55 23, 55 24, 55 25, 55 26, 55 27, 55 28, 55 29, 55 30, 55 3 1, 55 32, 55 3 3, 55 34, 55 3 5, 55 36, 55 37, 55 38, 55 39, 554 0, 554 1, 55 59 , 5 56 6, 5 56 7, 5 56 9, 557 0, 557 1, 557 2, 557 3, 557 4, 557 5, 55 76, 557 7, 55 78, 55 79, 558 0, 558 1, 558 2, 558 3, 558 4, 558 5, 55 86, 558 7, 70 37 ], " 17 42 ": ▁[4 7, 6 0, 6 1, 6 3, 64, 6 5,6 6, 6 7, 8 0, 8 1, 8 2, 8 4, 8 5, 12 9, 13 0, 13 1, 1 32, 13 3, 1 34, 13 5, 1 36, 1 37, 1 38, 1 39, 14 0, 14 1, 22 6, 22 7, 2 32, 23 3, 24 2, 26 7, 26 8, 26 9, 27 1, 3 14, 3 19, 32 3, 36 6, 36 7, 37 6, 38 5, 38 8, 4 17, 4 18, 4 19, 4 20, 55 4, 55 9, 57 3, 57 4, 5 76, 5 79, 58 0, 58 1, 58 3, 5 86, 58 7, 5 88 , 59 0, 59 3, 59 4, 59 7, 6 00, 6 01, 60 2, 60 4, 60 7, 60 8, 60 9, 61 1, 6 14, 6 15, 6 16, 6 24, 6 25, 6 26, 63 3, 6 34, 6 36, 64 3, 7 00, 7 01, 70 2, 70 3, 70 5, 7 17, 7 28, 74 5, 74 6, 8 34, 83 5, 8 36, 8 37, 8 38, 9 36, 9 37, 9 38, 9 39, 94 4, 94 5, 94 6, 95 0, 95 1, 95 2, 95 3, 95 8, 9 59 , 96 0, 9 64, 96 5, 96 6, 96 7, 98 2, 9 86, 98 7, 9 88 , 99 3, 99 4, 99 5, 99 6, 1 000, 100 1, 100 2, 100 3, 10 88 , 11 19, 112 0, 112 1, 112 2, 112 3, 120 7, 120 8, 120 9, 12 13, 12 14, 12 15, 12 16, 126 9, 138 6, 150 8, 150 9, 169 2, 169 3, 169 4, 169 5, 169 8, 16 99, 17 00, 17 01, 17 20, 17 26, 17 27, 17 29, 1 78 0, 1 78 1, 18 30, 185 1, 19 20, 199 3, 212 7, 22 16, 225 8, 229 5, 239 0, 25 64, 26 21, 28 21, 28 23, 284 4, 284 7, 286 2, 286 3, 28 64, 286 5, 29 11, 29 12, 29 13, 29 16, 29 22, 29 25, 29 3 5, 294 3, 294 5, 294 7, 29 48, 29 49, 29 50, 29 78, 29 79, 298 0, 298 1, 298 2, 298 7, 29 88 , 2 99 6, 300 7, 300 8, 301 1, 301 2,3 01 5, 301 6, 301 7, 301 8, 307 2, 30 99, 31 12, 3 11 3, 315 4, 31 55, 31 56, 315 7, 33 11, 33 12, 33 15, 33 18, 335 3, 33 55, 34 20, 34 22, 34 23, 359 0, 359 1, 359 2, 36 25, 37 32, 37 48, 375 0, 375 2, 375 4, 37 56, 376 2, 376 3, 37 64, 377 0, 377 3, 37 76, 37 79, 3 78 0, 3 78 1, 3 78 2,3 78 3, 3 78 4, 3 78 5, 3 78 6, 3 78 7, 3 78 8, 3 78 9, 379 4, 38 01, 387 1, 39 4 0, 40 49, 40 50, 405 8, 406 3, 4 16 9, 4 17 0, 44 76, 44 7 7, 4 48 2, 46 16, 4 90 6, 68 64, 6 86 5 ], " 1 125 ": ▁[4 7, 5 3, 6 5,6 7, 8 0, 8 1, 17 2, 17 4, 18 7, 19 0, 19 6, 19 8, 200, 20 2, 24 6, 26 7, 26 8, 26 9, 30 9, 31 3, 3 16, 3 19, 32 0, 32 3, 32 4, 32 5, 32 6, 37 0, 37 2, 37 4, 42 1, 4 48, 59 4, 59 7, 6 00, 6 34, 6 36, 65 7, 65 8, 67 3, 6 79, 69 2, 70 8, 73 5, 86 0, 94 4, 94 5, 94 6, 99 3, 99 4, 99 5, 106 1, 106 3, 106 5, 106 7, 12 20, 122 2, 122 3, 127 7, 150 2, 15 17, 15 18, 157 2, 157 3, 157 4, 157 5, 16 21, 16 22, 16 23, 16 32, 16 3 5, 16 37, 166 1, 169 6, 18 49, 189 7, 18 99, 19 01, 19 68 , 199 3, 20 32, 203 3, 20 34, 206 9, 228 3, 24 21, 24 22, 24 23, 244 5, 247 1, 247 2, 249 0, 249 3, 25 27, 25 29, 260 9, 26 23, 26 27, 266 9, 267 1, 27 29, 27 38, 27 39, 2 80 4, 28 25, 28 26, 285 3, 285 4, 28 55, 28 56, 289 4, 289 5, 29 01, 290 2, 290 3, 290 4, 29 18, 29 26, 29 32, 30 24, 3 10 7, 3 11 4, 3 11 5, 3 11 6, 32 99, 335 3, 335 4, 33 55, 33 56, 33 64, 336 5, 337 3, 339 0, 339 1, 339 2, 339 3, 34 15, 34 22, 34 23, 34 25, 354 1, 3 55 0, 37 15, 37 19, 37 20, 37 21, 379 2, 379 3, 379 4, 379 5, 38 00, 38 3 5, 38 36, 384 4, 384 5, 384 6, 384 7, 38 64, 386 6, 386 7, 39 20, 39 25, 39 26, 39 29, 39 30, 39 3 5, 39 36, 40 30, 40 59 , 409 0, 4 11 1, 41 12, 41 38, 4 14 3, 4 14 5, 4 16 1, 4 16 2, 4 16 5, 4 30 6, 4 31 1, 4 35 1, 4 36 1, 4 36 8, 4 39 7, 4 45 7, 44 6 7, 44 7 1, 4 48 0, 46 38, 46 39, 4 75 4, 47 64, 4 76 5, 4 76 6, 4 77 0, 4 80 3, 4 80 6, 4 80 7, 4 80 8, 48 24, 48 30, 48 88 , 4 90 4, 4 90 5, 49 11, 49 13, 49 14, 49 15, 49 16, 49 17, 49 18, 49 19, 49 20, 49 21, 49 22, 49 23, 68 9 2, 68 93 ], " 10 95 ": ▁[ 64, 6 5,6 6, 6 7, 8 0, 8 1, 18 7, 19 0, 19 6, 19 8, 200, 20 2, 2 19, 26 7, 26 8, 26 9, 3 19, 32 0, 32 4, 38 5, 38 8, 55 9, 57 3, 5 76, 58 0, 58 3, 5 86, 58 7, 5 88 , 59 0, 59 3, 59 4, 59 7, 6 00, 6 01, 60 2, 60 4, 60 7, 60 8, 60 9, 61 1, 6 14, 6 15, 6 16, 6 24, 6 25, 6 26, 6 34, 6 36, 6 39, 64 0, 64 4, 6 79, 68 9, 69 0, 69 1, 75 1, 7 56, 84 2, 84 3, 84 4, 84 5, 84 6, 84 7, 8 48, 9 37, 9 38, 9 39, 94 4, 94 5, 94 6, 95 1, 95 2, 95 3, 95 8, 9 59 , 96 0, 9 64, 96 5, 96 6, 96 7, 9 86, 98 7, 9 88 , 99 3, 99 4, 99 5, 1 000, 100 1, 100 2, 199 3, 209 8, 225 0, 225 8, 235 4, 24 21, 249 5, 249 6, 28 21, 28 23, 28 39, 284 4, 284 7, 285 4, 28 56, 286 2, 286 3, 28 64, 286 5, 29 22, 29 25, 294 7, 29 48, 29 49, 29 50, 29 78, 298 0, 300 7, 301 1, 301 5, 301 6, 301 7, 301 8, 3 11 3, 32 99, 3 30 6, 33 10, 33 15, 33 18, 33 19, 3 32 0, 3 32 1, 3 32 2,3 32 3, 3 32 4, 3 32 5, 3 32 6, 3 32 7, 3 32 8, 3 32 9, 33 68 , 34 22, 34 23, 359 0, 359 1, 359 2, 37 48, 375 0, 375 2, 375 4, 37 56, 376 2, 377 0, 377 3, 37 76, 37 79, 3 78 0, 3 78 1, 3 78 2,3 78 3, 3 78 4, 3 78 5, 3 78 6, 3 78 7, 3 78 9, 379 2, 379 3, 379 4, 379 5, 38 01, 38 71 ], " 11 45 ": ▁[ 64, 6 5,6 6, 6 7, 8 0, 8 1, 8 2, 8 4, 8 5, 12 5, 12 9, 13 0, 13 1, 1 32, 13 3, 1 34, 13 5, 14 0, 14 1, 26 3, 2 64, 26 7, 26 8, 26 9, 32 7, 3 34, 35 1, 36 7, 38 8, 44 6, 55 3, 59 4, 59 7, 6 00, 6 01, 60 2, 60 4, 60 7, 60 8, 60 9, 61 1, 6 14, 6 20, 62 2, 6 29, 63 1, 6 34, 64 3, 69 2, 73 5, 78 6, 86 7, 89 6, 9 37, 9 38, 9 39, 94 4, 94 5, 94 6, 95 0, 95 1, 95 2, 95 3, 95 8, 9 59 , 96 0, 97 5, 97 7, 98 2, 9 86, 98 7, 9 88 , 99 3, 99 4, 99 5, 99 6, 1 000, 100 1, 100 2, 10 76, 107 7, 10 88 , 11 19, 112 0, 112 1, 112 2, 11 32, 11 3 3, 11 34, 11 3 5, 11 36, 11 37, 11 38, 11 39, 114 0, 114 1, 120 4, 12 24, 12 32, 123 3, 124 4, 124 5, 127 5, 15 17, 15 18, 157 2, 157 3, 157 4, 157 5, 166 1, 166 9, 167 0, 167 1, 170 9, 17 11, 17 29, 17 30, 17 3 1, 184 3, 184 5, 189 7, 18 99, 19 01, 199 3, 199 4, 201 3, 205 7, 22 18, 22 19, 22 20, 22 27, 22 28, 22 29, 22 30, 225 0, 225 8, 228 3, 232 7, 247 1, 247 2, 249 0, 249 3, 256 0, 256 2, 258 7, 259 6, 260 8, 260 9, 26 10, 26 11, 26 30, 26 3 1, 267 3, 268 2, 268 3, 268 4, 270 5, 270 6, 270 8, 270 9, 27 10, 27 11, 27 12, 27 13, 28 17, 28 21, 28 25, 284 4, 286 2, 286 3, 288 0, 29 00, 29 01, 290 2, 29 16, 29 18, 29 22, 29 25, 29 26, 294 1, 294 7, 29 48, 29 49, 29 50, 29 7 7, 29 78, 298 2, 2 99 1, 2 99 2, 2 99 3, 300 7, 301 1, 301 5, 301 6, 301 7, 301 8, 303 3, 30 34, 307 2, 329 7, 3 30 6, 33 10, 33 11, 33 12, 33 17, 33 18, 33 19, 3 32 0, 3 32 1, 3 32 2,3 32 3, 3 32 4, 3 32 5, 3 32 6, 3 32 7, 3 32 8, 3 32 9, 34 17, 35 88 , 37 21, 374 4, 38 64, 386 6, 386 7, 40 30, 40 34, 40 59 , 406 3, 40 64, 406 5, 406 6, 406 7, 40 68 , 406 9, 407 0, 407 1, 407 2, 407 3, 407 4, 407 5, 40 76, 407 7, 40 78, 40 79, 408 0, 408 1, 408 2, 408 3, 408 4, 408 5, 40 86, 408 7, 40 88 , 40 89 , 409 2, 409 3, 409 4, 409 5, 409 6, 409 7, 409 8, 40 99, 4 10 9, 41 10, 4 16 1, 4 35 8, 4 35 9, 4 36 6, 46 38, 46 39 ], " 19 66 ": ▁[ 64, 6 5,6 6, 6 7, 8 0, 8 1, 8 2, 8 3, 8 4, 8 5, 12 9, 13 0, 13 1, 1 32, 13 3, 1 34, 13 5, 1 36, 1 37, 1 38, 1 39, 14 0, 14 1, 26 3, 2 64, 26 7, 26 8, 26 9, 27 1, 27 2,3 14, 36 6, 36 7, 37 6, 38 5, 38 6, 38 7, 38 8, 39 1, 4 17, 4 18, 4 19, 4 20, 55 9, 56 9, 57 3, 57 4, 5 76, 5 79, 58 0, 58 1, 58 3, 5 86, 58 7, 5 88 , 59 0, 59 3, 59 4, 59 7, 6 00, 6 01, 60 2, 60 4, 60 7, 60 8, 60 9, 61 1, 6 14, 6 16, 6 24, 6 25, 6 26, 6 34, 6 36, 6 39, 64 0, 64 1, 64 4, 7 78, 9 36, 9 37, 9 38, 9 39, 94 4, 94 5, 94 6, 95 0, 95 1, 95 2, 95 3, 95 8, 9 59 , 96 0, 9 64, 96 5, 96 6, 96 7, 98 2, 9 86, 98 7, 9 88 , 99 3, 99 4, 99 5, 99 6, 1 000, 100 1, 100 2, 107 5, 11 14, 11 15, 11 16, 11 17, 11 18, 17 28, 199 3, 225 8, 259 6, 267 3, 28 21, 28 23, 284 4, 284 7, 286 2, 286 3, 28 64, 286 5, 29 16, 29 22, 29 25, 294 7, 29 48, 29 49, 29 50, 29 78, 29 79, 298 0, 298 1, 298 2, 300 7, 301 1, 301 5, 301 6, 301 7, 301 8, 31 12, 3 11 3, 359 0, 359 1, 359 2, 374 4, 37 48, 375 0, 375 2, 375 4, 37 56, 376 2, 376 3, 37 64, 377 0, 377 3, 37 76, 37 79, 3 78 0, 3 78 1, 3 78 2,3 78 3, 3 78 4, 3 78 5, 3 78 6, 3 78 7, 3 78 9, 38 00, 387 1, 406 2, 47 64, 4 76 5, 4 766 ], " 2 148 ": ▁[ 15 9, 2 20, 69 6, 10 50, 127 7, 12 78, 128 0, 169 6, 199 4, 20 32, 203 3, 20 34, 2 15 1, 23 19, 24 29, 24 32, 24 3 3, 24 34, 24 3 5, 24 36, 24 37, 244 1, 244 5, 32 99, 33 10, 33 19, 3 32 0, 3 32 1, 3 32 2,3 32 3, 3 32 4, 3 32 5, 3 32 6, 3 32 7, 3 32 8, 36 22, 49 13, 69 4 5, 69 4 6, 69 4 7, 69 48, 69 49, 6 95 0, 6 95 1, 6 95 2, 6 95 3, 6 99 0, 6 99 1], " 2 387 ": ▁[ 15 9, 10 50, 199 4, 270 8, 270 9, 27 10, 27 11, 27 12, 27 13, 29 76, 29 7 7, 34 16, 34 17, 36 22, 4 35 8, 4 359 ], " 86 5 ": ▁[2 16, 2 17, 85 1, 86 0, 205 3, 205 4, 20 55, 20 56, 2 13 1, 21 32, 24 22, 24 23 ], " 24 42 ": ▁[2 20, 127 7, 16 21, 16 22, 16 23, 16 32, 16 3 5, 16 37, 18 59 , 186 5, 187 3, 18 79, 188 5, 189 2, 199 4, 20 32, 203 3, 20 34, 24 32, 24 3 3, 24 34, 24 3 5, 24 36, 24 37, 244 5, 2 78 0, 2 78 9, 32 99 ], " 2 370 ": ▁[ 32 1, 69 2, 124 5, 15 17, 15 18, 157 2, 157 3, 157 4, 157 5, 166 1, 189 7, 18 99, 19 01, 20 68 , 209 4, 209 5, 209 6, 210 6, 210 9, 226 3, 22 64, 227 0, 228 3, 228 4, 2 30 3, 232 7, 2 36 6, 2 36 7, 239 0, 24 28, 249 0, 249 3, 27 19, 27 22, 27 26, 27 3 5, 27 36, 27 38, 27 39, 274 0, 274 1, 276 5, 28 27, 289 4, 289 5, 29 01, 290 2, 290 3, 29 26, 301 9, 30 24, 303 1, 307 7, 30 78, 30 79, 308 1, 308 3, 308 4, 308 5, 3 34 9, 359 0, 359 1, 359 2, 360 5, 360 6, 37 15, 37 16, 385 3, 385 4, 38 55, 38 56, 385 7, 386 1, 41 12, 4 12 0, 4 28 4, 4 30 6, 4 39 8, 46 20, 46 21, 6 90 2, 6 90 3], " 19 50 ": ▁[ 68 4, 8 16, 128 5, 12 86, 16 56, 165 7, 240 5, 25 12, 25 27, 36 51 ], " 38 52 ": ▁[7 79, 78 0, 12 13, 12 14, 12 89 , 184 7, 18 49, 33 39, 37 32, 37 38, 374 3, 379 0, 379 7, 38 00, 405 4, 4 11 3, 4 76 5, 4 766 ], " 2 38 1": ▁[ 78 1, 78 2, 8 10, 202 3, 20 24, 26 32, 26 3 3, 4 365 ], " 110 8 ": ▁[ 9 20, 9 21 ], " 110 5 ": ▁[12 76, 150 2, 199 4, 20 32, 226 9, 23 19, 234 2, 234 3, 234 4, 234 8, 234 9, 235 0, 24 20, 24 21, 24 29, 244 1, 33 10, 3 32 1, 3 32 2,3 32 3, 3 32 6, 3 32 7, 3 328 ], " 27 25 ": ▁[ 27 23, 4 16 1], " 27 27 ": ▁[ 27 29 ], " 27 28 ": ▁[ 27 30 ], " 28 20 ": ▁[ 285 8, 28 60 ]} ▁< s > ▁Parent ▁Child ▁10 87 ▁4 ▁10 87 ▁5 ▁10 87 ▁6 ▁..... ▁...... ▁10 96 ▁25 ▁10 96 ▁26 ▁10 96 ▁27 ▁10 96 ▁28 ▁...... ▁...... ▁1 144 ▁25 ▁1 144 ▁26 ▁1 144 ▁27 ▁..... ▁..... ▁< s > ▁DataFrame ▁columns ▁all
▁How ▁to ▁manipulate ▁column ▁entries ▁using ▁only ▁one ▁specific ▁output ▁of ▁a ▁function ▁that ▁returns ▁several ▁values ? ▁< s > ▁I ▁have ▁a ▁dataframe ▁like ▁this : ▁and ▁I ▁have ▁a ▁function ▁that ▁returns ▁several ▁values . ▁Here ▁I ▁just ▁use ▁a ▁dummy ▁function ▁that ▁returns ▁the ▁minimum ▁and ▁maximum ▁for ▁a ▁certain ▁input ▁iterable : ▁Now ▁I ▁want ▁to ▁e . g . ▁add ▁the ▁maximum ▁of ▁each ▁column ▁to ▁each ▁value ▁in ▁the ▁respective ▁column . ▁So ▁gives ▁and ▁then ▁yields ▁the ▁desired ▁outcome ▁I ▁am ▁wondering ▁whether ▁there ▁is ▁a ▁more ▁straightforward ▁way ▁that ▁avoids ▁the ▁two ▁chained ▁' s . ▁Just ▁to ▁make ▁sure : ▁I ▁am ▁NOT ▁interested ▁in ▁a ▁type ▁solution . ▁I ▁highlighted ▁the ▁to ▁illustrate ▁that ▁this ▁not ▁my ▁actual ▁function ▁but ▁just ▁serves ▁as ▁a ▁minimal ▁example ▁function ▁that ▁has ▁several ▁outputs . ▁< s > ▁a ▁(0, ▁3) ▁b ▁(2, ▁5) ▁< s > ▁a ▁b ▁0 ▁3 ▁7 ▁1 ▁4 ▁8 ▁2 ▁5 ▁9 ▁3 ▁6 ▁10 ▁< s > ▁values ▁values ▁add ▁value
▁pandas ▁groupby ▁expanding ▁df ▁based ▁on ▁unique ▁values ▁< s > ▁I ▁have ▁below : ▁I ▁want ▁to ▁achieve ▁the ▁following . ▁For ▁each ▁unique ▁, ▁the ▁bottom ▁row ▁is ▁( this ▁is ▁). ▁I ▁want ▁to ▁count ▁how ▁many ▁times ▁each ▁unique ▁value ▁of ▁occurs ▁where ▁. ▁This ▁part ▁would ▁be ▁achieved ▁by ▁something ▁like : ▁However , ▁I ▁also ▁want ▁to ▁add , ▁for ▁each ▁unique ▁value ▁of ▁, ▁a ▁column ▁indicating ▁how ▁many ▁times ▁that ▁value ▁occurred ▁after ▁the ▁point ▁where ▁for ▁the ▁value ▁indicated ▁by ▁the ▁row . ▁I ▁understand ▁this ▁might ▁sound ▁confusing ; ▁here ' s ▁how ▁the ▁output ▁w oud ▁look ▁like ▁in ▁this ▁example : ▁It ▁is ▁important ▁that ▁the ▁solution ▁is ▁general ▁enough ▁to ▁be ▁applicable ▁on ▁a ▁similar ▁case ▁with ▁more ▁unique ▁values ▁than ▁just ▁, ▁and ▁. ▁UPDATE ▁As ▁a ▁bonus , ▁I ▁am ▁also ▁interested ▁in ▁how , ▁instead ▁of ▁the ▁count , ▁one ▁can ▁instead ▁return ▁the ▁sum ▁of ▁some ▁value ▁column , ▁under ▁the ▁same ▁conditions , ▁divided ▁by ▁the ▁corresponding ▁in ▁the ▁rows . ▁Example : ▁suppose ▁we ▁now ▁depart ▁from ▁below ▁instead : ▁The ▁output ▁would ▁need ▁to ▁sum ▁for ▁the ▁cases ▁indicated ▁by ▁the ▁counts ▁in ▁the ▁solution ▁by ▁@ j ez ra el , ▁and ▁divide ▁that ▁number ▁by ▁. ▁The ▁output ▁would ▁instead ▁look ▁like : ▁< s > ▁df ▁V 2 ▁V 1 ▁A ▁B ▁C ▁0 ▁A ▁0 ▁0 ▁0 ▁0 ▁1 ▁B ▁1 ▁0 ▁0 ▁0 ▁2 ▁C ▁2 ▁1 ▁2 ▁0 ▁< s > ▁df ▁V 2 ▁V 1 ▁A ▁B ▁C ▁0 ▁A ▁0 ▁0 ▁0 ▁0 ▁1 ▁B ▁1 ▁0 ▁0 ▁0 ▁2 ▁C ▁2 ▁1 ▁3.5 ▁0 ▁< s > ▁groupby ▁expanding ▁unique ▁values ▁unique ▁count ▁unique ▁value ▁where ▁add ▁unique ▁value ▁value ▁where ▁value ▁unique ▁values ▁count ▁sum ▁value ▁now ▁sum
▁Add ▁ID ▁found ▁in ▁list ▁to ▁new ▁column ▁in ▁pandas ▁dataframe ▁< s > ▁Say ▁I ▁have ▁the ▁following ▁dataframe ▁( a ▁column ▁of ▁integers ▁and ▁a ▁column ▁with ▁a ▁list ▁of ▁integers )... ▁And ▁also ▁a ▁separate ▁list ▁of ▁IDs ... ▁Given ▁that , ▁and ▁ignoring ▁the ▁column ▁and ▁any ▁index , ▁I ▁want ▁to ▁see ▁if ▁any ▁of ▁the ▁IDs ▁in ▁the ▁list ▁are ▁mentioned ▁in ▁the ▁column . ▁The ▁code ▁I ▁have ▁so ▁far ▁is : ▁This ▁works ▁but ▁only ▁if ▁the ▁list ▁is ▁longer ▁than ▁the ▁dataframe ▁and ▁for ▁the ▁real ▁dataset ▁the ▁list ▁is ▁going ▁to ▁be ▁a ▁lot ▁shorter ▁than ▁the ▁dataframe . ▁If ▁I ▁set ▁the ▁list ▁to ▁only ▁two ▁elements ... ▁I ▁get ▁a ▁very ▁popular ▁error ▁( I ▁have ▁read ▁many ▁questions ▁with ▁the ▁same ▁error )... ▁I ▁have ▁tried ▁converting ▁the ▁list ▁to ▁a ▁series ▁( no ▁change ▁in ▁the ▁error ). ▁I ▁have ▁also ▁tried ▁adding ▁the ▁new ▁column ▁and ▁setting ▁all ▁values ▁to ▁before ▁doing ▁the ▁comprehension ▁line ▁( again ▁no ▁change ▁in ▁the ▁error ). ▁Two ▁questions : ▁How ▁do ▁I ▁get ▁my ▁code ▁( below ) ▁to ▁work ▁for ▁a ▁list ▁that ▁is ▁shorter ▁than ▁a ▁dataframe ? ▁How ▁would ▁I ▁get ▁the ▁code ▁to ▁write ▁the ▁actual ▁ID ▁found ▁back ▁to ▁the ▁column ▁( more ▁useful ▁than ▁True / False )? ▁Expected ▁output ▁for ▁: ▁Ide al ▁output ▁for ▁( ID ( s ) ▁are ▁written ▁to ▁a ▁new ▁column ▁or ▁columns ): ▁Code : ▁< s > ▁ID ▁Found _ IDs ▁0 ▁12345 ▁[ 15 44 3, ▁155 3 3, ▁34 33 ] ▁1 ▁155 33 ▁[ 22 34, ▁16 60 8, ▁1200 2, ▁7 65 4] ▁2 ▁6 789 ▁[ 43 32 2, ▁8 7654 4, ▁36 789 ] ▁< s > ▁bad _ ids ▁= ▁[1 55 3 3, ▁8 7654 4, ▁36 78 9, ▁1 1111 ] ▁< s > ▁any ▁index ▁any ▁get ▁all ▁values ▁get ▁get ▁columns
▁How ▁to ▁change ▁values ▁of ▁rows ▁based ▁on ▁conditions ▁in ▁dataframe ? ▁< s > ▁I ▁have ▁dataframe ▁that ▁is ▁shown ▁below , ▁( Need ▁some ▁magic ▁to ▁be ▁done ) ▁The ▁change ▁should ▁be ▁made ▁in ▁type ▁column ▁such ▁that , ▁in ▁a ▁row ▁if ▁is ▁and ▁is ▁then ▁next ▁row ▁of ▁should ▁be ▁assigned ▁. ▁Full ▁dataframe ▁should ▁look ▁like ▁this : ▁< s > ▁type ▁label ▁0 ▁0 ▁0 ▁1 ▁0 ▁0 ▁2 ▁0 ▁0 ▁3 ▁0 ▁0 ▁4 ▁2 ▁1 ▁5 ▁2 ▁1 ▁6 ▁2 ▁1 ▁7 ▁2 ▁1 ▁8 ▁2 ▁1 ▁9 ▁2 ▁1 ▁10 ▁0 ▁0 ▁11 ▁0 ▁0 ▁12 ▁0 ▁0 ▁13 ▁0 ▁0 ▁14 ▁0 ▁0 ▁15 ▁0 ▁0 ▁16 ▁0 ▁0 ▁17 ▁0 ▁0 ▁18 ▁0 ▁0 ▁19 ▁0 ▁0 ▁< s > ▁type ▁label ▁0 ▁0 ▁0 ▁1 ▁2 ▁0 ▁2 ▁2 ▁0 ▁3 ▁0 ▁0 ▁4 ▁2 ▁1 ▁5 ▁2 ▁1 ▁6 ▁2 ▁1 ▁7 ▁2 ▁1 ▁8 ▁2 ▁1 ▁9 ▁2 ▁1 ▁10 ▁0 ▁0 ▁11 ▁2 ▁0 ▁12 ▁2 ▁0 ▁13 ▁2 ▁0 ▁14 ▁2 ▁0 ▁15 ▁2 ▁0 ▁16 ▁2 ▁0 ▁17 ▁2 ▁0 ▁18 ▁2 ▁0 ▁19 ▁2 ▁0 ▁< s > ▁values
▁Adding ▁value ▁from ▁one ▁pandas ▁dataframe ▁to ▁another ▁dataframe ▁by ▁matching ▁a ▁variable ▁< s > ▁Suppose ▁I ▁have ▁a ▁pandas ▁dataframe ▁with ▁2 ▁columns ▁A ▁second ▁dataframe , ▁contains ▁c 1, ▁c 2 ▁and ▁a ▁few ▁other ▁columns . ▁My ▁goal ▁is ▁to ▁replace ▁the ▁empty ▁values ▁for ▁c 1 ▁in ▁df 2, ▁with ▁those ▁in ▁df , ▁corresponding ▁to ▁the ▁values ▁in ▁c 2, ▁so ▁the ▁first ▁five ▁values ▁for ▁c 1 ▁in ▁df 2, ▁should ▁be ▁v 5, v 2, v 1, v 2 ▁and ▁v 3 ▁respectively . ▁What ▁is ▁the ▁best ▁way ▁to ▁do ▁this ? ▁< s > ▁c 1 ▁c 2 ▁0 ▁v 1 ▁b 1 ▁1 ▁v 2 ▁b 2 ▁2 ▁v 3 ▁b 3 ▁3 ▁v 4 ▁b 4 ▁4 ▁v 5 ▁b 5 ▁< s > ▁c 1 ▁c 2 ▁c 3 ▁c 4 ▁0 ▁"" ▁b 5 ▁500 ▁3 ▁1 ▁"" ▁b 2 ▁4 20 ▁7 ▁2 ▁"" ▁b 1 ▁3 80 ▁5 ▁3 ▁"" ▁b 2 ▁4 70 ▁9 ▁4 ▁"" ▁b 3 ▁2 90 ▁2 ▁< s > ▁value ▁columns ▁second ▁contains ▁columns ▁replace ▁empty ▁values ▁values ▁first ▁values
▁Copying ▁columns ▁within ▁pandas ▁dataframe ▁< s > ▁I ▁want ▁to ▁slice ▁and ▁copy ▁columns ▁in ▁a ▁Python ▁Dataframe . ▁My ▁data ▁frame ▁looks ▁like ▁the ▁following : ▁I ▁want ▁to ▁make ▁it ▁of ▁the ▁form ▁Which ▁basically ▁means ▁that ▁I ▁want ▁to ▁shift ▁the ▁values ▁in ▁Columns ▁'19 29 ',' 19 29 .1 ',' 19 30 ',' 19 30 .1' ▁under ▁the ▁column ▁'19 28 ' ▁and ▁'19 28 .1' ▁For ▁the ▁same , ▁I ▁wrote ▁the ▁code ▁as ▁No ▁copying ▁takes ▁places ▁within ▁the ▁columns . ▁How ▁shall ▁I ▁modify ▁my ▁code ?? ▁< s > ▁19 28 ▁19 28 .1 ▁19 29 ▁19 29 .1 ▁19 30 ▁19 30 .1 ▁0 ▁0 ▁0 ▁0 ▁0 ▁0 ▁0 ▁1 ▁1 ▁3 ▁3 ▁2 ▁2 ▁2 ▁2 ▁4 ▁1 ▁3 ▁0 ▁1 ▁2 ▁< s > ▁19 28 ▁19 28 .1 ▁19 29 ▁19 29 .1 ▁19 30 ▁19 30 .1 ▁0 ▁0 ▁0 ▁1 ▁1 ▁3 ▁2 ▁4 ▁1 ▁3 ▁0 ▁0 ▁4 ▁3 ▁2 ▁5 ▁3 ▁0 ▁6 ▁0 ▁0 ▁7 ▁2 ▁2 ▁8 ▁1 ▁2 ▁< s > ▁columns ▁copy ▁columns ▁shift ▁values ▁columns
▁Append ▁rows ▁to ▁groups ▁in ▁pandas ▁< s > ▁I ' m ▁trying ▁to ▁append ▁a ▁number ▁of ▁NaN ▁rows ▁to ▁each ▁group ▁in ▁a ▁pandas ▁dataframe . ▁Essentially ▁I ▁want ▁to ▁pad ▁each ▁group ▁to ▁be ▁5 ▁rows ▁long . ▁Ordering ▁is ▁important . ▁I ▁have : ▁I ▁want : ▁< s > ▁Rank ▁id ▁0 ▁1 ▁a ▁1 ▁2 ▁a ▁2 ▁3 ▁a ▁3 ▁4 ▁a ▁4 ▁5 ▁a ▁5 ▁1 ▁c ▁6 ▁2 ▁c ▁7 ▁1 ▁e ▁8 ▁2 ▁e ▁9 ▁3 ▁e ▁< s > ▁Rank ▁id ▁0 ▁1 ▁a ▁1 ▁2 ▁a ▁2 ▁3 ▁a ▁3 ▁4 ▁a ▁4 ▁5 ▁a ▁5 ▁1 ▁c ▁6 ▁2 ▁c ▁7 ▁NaN ▁c ▁8 ▁NaN ▁c ▁9 ▁NaN ▁c ▁10 ▁1 ▁e ▁11 ▁2 ▁e ▁12 ▁3 ▁e ▁13 ▁NaN ▁e ▁14 ▁NaN ▁e ▁< s > ▁groups ▁append ▁pad
▁replace ▁NaN ▁with ▁& # 39 ; - &# 39 ; ▁sign ▁only ▁in ▁spe ce f ic ▁condition ▁, Python - P andas ▁< s > ▁I ▁have ▁a ▁dataframe ▁I ▁want ▁to ▁replace ▁all ▁the ▁NaN ▁with ▁'-' ▁( only ▁when ▁the ▁value ▁in ▁any ▁column ▁is ▁last ▁value ▁in ▁that ▁row ) ▁so ▁basically ▁my ▁desired ▁output ▁will ▁be ▁Can ▁someone ▁help , ▁Thank ▁you ▁in ▁advance ! ▁< s > ▁L 1 ▁D 1 ▁L 2 ▁D 2 ▁L 3 ▁1.0 ▁ABC ▁1.1 ▁4.1 ▁NaN ▁NaN ▁NaN ▁1.7 ▁NaN ▁NaN ▁NaN ▁4.1 ▁NaN ▁NaN ▁NaN ▁NaN ▁1.8 ▁3.2 ▁P QR ▁NaN ▁NaN ▁NaN ▁1.6 ▁NaN ▁NaN ▁< s > ▁L 1 ▁D 1 ▁L 2 ▁D 2 ▁L 3 ▁1.0 ▁ABC ▁1.1 ▁4.1 ▁- ▁NaN ▁NaN ▁1.7 ▁- ▁- ▁NaN ▁4.1 ▁- ▁- ▁- ▁NaN ▁1.8 ▁3.2 ▁P QR ▁- ▁NaN ▁NaN ▁1.6 ▁- ▁- ▁< s > ▁replace ▁replace ▁all ▁value ▁any ▁last ▁value
▁transform ▁a ▁pandas ▁dataframe ▁in ▁a ▁pandas ▁with ▁mult icol umn s ▁< s > ▁I ▁have ▁the ▁following ▁pandas ▁dataframe , ▁where ▁the ▁column a ▁is ▁the ▁dataframe ▁index ▁And ▁i ▁want ▁to ▁convert ▁this ▁dat frame ▁in ▁to ▁a ▁multi ▁column ▁data ▁frame , ▁that ▁looks ▁like ▁this ▁I ' ve ▁tried ▁transform ing ▁my ▁old ▁pandas ▁dataframe ▁in ▁to ▁a ▁dict ▁this ▁way : ▁But ▁i ▁had ▁no ▁success , ▁can ▁someone ▁give ▁me ▁tips ▁and ▁adv ices ▁on ▁how ▁to ▁do ▁that ? ▁Any ▁help ▁is ▁more ▁than ▁welcome . ▁< s > ▁+ ----+ -------- ---+ ------------ + -------- ---+ ------------ + ▁| ▁| ▁price _ A ▁| ▁amount _ A ▁| ▁price _ B ▁| ▁amount _ b ▁| ▁| ----+ -------- ---+ ------------ + -------- ---+ ------------ | ▁| ▁0 ▁| ▁0. 65 28 26 ▁| ▁0. 94 14 21 ▁| ▁0. 82 30 48 ▁| ▁0.7 284 27 ▁| ▁| ▁1 ▁| ▁0.4 000 78 ▁| ▁0.6 005 85 ▁| ▁0.1 9 49 12 ▁| ▁0. 26 98 42 ▁| ▁| ▁2 ▁| ▁0. 22 35 24 ▁| ▁0.1 4 66 75 ▁| ▁0. 375 459 ▁| ▁0.1 77 165 ▁| ▁| ▁3 ▁| ▁0.3 306 26 ▁| ▁0.2 14 98 1 ▁| ▁0. 38 98 55 ▁| ▁0.5 4 16 66 ▁| ▁| ▁4 ▁| ▁0.5 78 132 ▁| ▁0. 304 78 ▁| ▁0. 78 95 73 ▁| ▁0. 26 88 51 ▁| ▁| ▁5 ▁| ▁0.0 94 36 01 ▁| ▁0.5 148 78 ▁| ▁0.4 19 333 ▁| ▁0.0 17 00 96 ▁| ▁| ▁6 ▁| ▁0. 279 122 ▁| ▁0.4 011 32 ▁| ▁0.7 22 36 3 ▁| ▁0. 33 70 94 ▁| ▁| ▁7 ▁| ▁0. 44 49 77 ▁| ▁0. 33 32 54 ▁| ▁0. 64 38 78 ▁| ▁0. 37 15 28 ▁| ▁| ▁8 ▁| ▁0.7 24 67 3 ▁| ▁0.0 6 32 807 ▁| ▁0. 34 52 25 ▁| ▁0.9 35 403 ▁| ▁| ▁9 ▁| ▁0. 90 5 48 2 ▁| ▁0.8 465 ▁| ▁0.5 8 56 53 ▁| ▁0.3 64 49 5 ▁| ▁+ ----+ -------- ---+ ------------ + -------- ---+ ------------ + ▁< s > ▁+ ----+ -------- ---+ ------------ + -------- ---+ ------------ + ▁| ▁| ▁A ▁| ▁B ▁| ▁+ ----+ -------- ---+ ------------ + -------- ---+ ------------ + ▁| ▁id ▁| ▁price ▁| ▁amount ▁| ▁price ▁| ▁amount ▁| ▁| ----+ -------- ---+ ------------ + -------- ---+ ------------ | ▁| ▁0 ▁| ▁0. 65 28 26 ▁| ▁0. 94 14 21 ▁| ▁0. 82 30 48 ▁| ▁0.7 284 27 ▁| ▁| ▁1 ▁| ▁0.4 000 78 ▁| ▁0.6 005 85 ▁| ▁0.1 9 49 12 ▁| ▁0. 26 98 42 ▁| ▁| ▁2 ▁| ▁0. 22 35 24 ▁| ▁0.1 4 66 75 ▁| ▁0. 375 459 ▁| ▁0.1 77 165 ▁| ▁| ▁3 ▁| ▁0.3 306 26 ▁| ▁0.2 14 98 1 ▁| ▁0. 38 98 55 ▁| ▁0.5 4 16 66 ▁| ▁| ▁4 ▁| ▁0.5 78 132 ▁| ▁0. 304 78 ▁| ▁0. 78 95 73 ▁| ▁0. 26 88 51 ▁| ▁| ▁5 ▁| ▁0.0 94 36 01 ▁| ▁0.5 148 78 ▁| ▁0.4 19 333 ▁| ▁0.0 17 00 96 ▁| ▁| ▁6 ▁| ▁0. 279 122 ▁| ▁0.4 011 32 ▁| ▁0.7 22 36 3 ▁| ▁0. 33 70 94 ▁| ▁| ▁7 ▁| ▁0. 44 49 77 ▁| ▁0. 33 32 54 ▁| ▁0. 64 38 78 ▁| ▁0. 37 15 28 ▁| ▁| ▁8 ▁| ▁0.7 24 67 3 ▁| ▁0.0 6 32 807 ▁| ▁0. 34 52 25 ▁| ▁0.9 35 403 ▁| ▁| ▁9 ▁| ▁0. 90 5 48 2 ▁| ▁0.8 465 ▁| ▁0.5 8 56 53 ▁| ▁0.3 64 49 5 ▁| ▁+ ----+ -------- ---+ ------------ + -------- ---+ ------------ + ▁< s > ▁transform ▁where ▁index
▁How ▁to ▁drop ▁pandas ▁consecutive ▁column ▁by ▁column ▁name ▁simultaneously ? ▁< s > ▁Here ' s ▁my ▁data ▁The ▁Output ▁I ▁expected , ▁What ▁I ▁did ▁But ▁this ▁is ▁not ▁efficient , ▁how ▁to ▁do ▁this ▁effectively ? ▁< s > ▁Id ▁Column 1 ▁Column 2 ▁Column 3 ▁Column 4 ▁.... ▁Column 112 ▁Column 11 3 ▁... ▁Column 14 3 ▁1 ▁67 ▁89 ▁86 ▁43 ▁56 ▁72 ▁67 ▁< s > ▁Id ▁Column 1 ▁Column 11 3 ▁... ▁Column 14 3 ▁1 ▁67 ▁72 ▁67 ▁< s > ▁drop ▁name
▁Pandas : ▁Drop ▁duplicates ▁based ▁on ▁row ▁value ▁< s > ▁I ▁have ▁a ▁dataframe ▁and ▁I ▁want ▁to ▁drop ▁duplicates ▁based ▁on ▁different ▁conditions .... ▁I ▁want ▁to ▁drop ▁all ▁the ▁duplicates ▁from ▁column ▁A ▁except ▁rows ▁with ▁"- ". ▁After ▁this , ▁I ▁want ▁to ▁drop ▁duplicates ▁from ▁column ▁A ▁with ▁"-" ▁as ▁a ▁value ▁based ▁on ▁their ▁column ▁B ▁value . ▁Given ▁the ▁input ▁dataframe , ▁this ▁should ▁return ▁the ▁following :- ▁I ▁have ▁the ▁following ▁code ▁but ▁it ' s ▁not ▁very ▁efficient ▁for ▁very ▁large ▁amounts ▁of ▁data , ▁how ▁can ▁I ▁improve ▁this .... ▁< s > ▁A ▁B ▁0 ▁1 ▁1.0 ▁1 ▁1 ▁1.0 ▁2 ▁2 ▁2.0 ▁3 ▁2 ▁2.0 ▁4 ▁3 ▁3.0 ▁5 ▁4 ▁4.0 ▁6 ▁5 ▁5.0 ▁7 ▁- ▁5.1 ▁8 ▁- ▁5.1 ▁9 ▁- ▁5. 3 ▁< s > ▁A ▁B ▁0 ▁1 ▁1.0 ▁2 ▁2 ▁2.0 ▁4 ▁3 ▁3.0 ▁5 ▁4 ▁4.0 ▁6 ▁5 ▁5.0 ▁7 ▁- ▁5.1 ▁9 ▁- ▁5. 3 ▁< s > ▁value ▁drop ▁drop ▁all ▁drop ▁value ▁value
▁How ▁can ▁I ▁find ▁and ▁store ▁how ▁many ▁columns ▁it ▁takes ▁to ▁reach ▁a ▁value ▁greater ▁than ▁the ▁first ▁value ▁in ▁each ▁row ? ▁< s > ▁Original ▁dataframe ▁is ▁df 1. ▁For ▁each ▁row , ▁I ▁want ▁to ▁find ▁the ▁first ▁time ▁a ▁value ▁is ▁bigger ▁than ▁the ▁value ▁in ▁the ▁first ▁column ▁and ▁store ▁it ▁in ▁a ▁new ▁dataframe . ▁df 2 ▁is ▁the ▁resulting ▁dataframe . ▁For ▁example , ▁for ▁df 1 ▁row ▁1; ▁the ▁first ▁value ▁is ▁3 ▁and ▁the ▁first ▁value ▁bigger ▁than ▁3 ▁is ▁4 ▁( column ▁c ). ▁Hence ▁in ▁df 2 ▁row ▁1, ▁we ▁store ▁2 ▁( there ▁are ▁two ▁columns ▁from ▁column ▁a ▁to ▁c ). ▁For ▁df 1 ▁row ▁2, ▁the ▁first ▁value ▁is ▁4 ▁and ▁the ▁first ▁value ▁bigger ▁than ▁4 ▁is ▁5 ▁( column ▁d ). ▁Hence ▁in ▁df 2 ▁row ▁2, ▁we ▁store ▁3 ▁( there ▁are ▁three ▁columns ▁from ▁column ▁a ▁to ▁d ). ▁For ▁df 1 ▁row ▁3, ▁the ▁first ▁value ▁is ▁5 ▁and ▁the ▁first ▁value ▁bigger ▁than ▁5 ▁is ▁6 ▁( column ▁e ). ▁Hence ▁in ▁df 2 ▁row ▁3, ▁we ▁store ▁4 ▁( there ▁are ▁four ▁columns ▁from ▁column ▁a ▁to ▁e ). ▁I ▁would ▁appreciate ▁the ▁help . ▁< s > ▁df 1 ▁a ▁b ▁c ▁d ▁e ▁0 ▁3 ▁1 ▁4 ▁1 ▁9 ▁1 ▁4 ▁2 ▁3 ▁5 ▁4 ▁2 ▁5 ▁3 ▁3 ▁4 ▁6 ▁< s > ▁df 2 ▁b ▁0 ▁2 ▁1 ▁3 ▁2 ▁4 ▁< s > ▁columns ▁value ▁first ▁value ▁first ▁time ▁value ▁value ▁first ▁first ▁value ▁first ▁value ▁columns ▁first ▁value ▁first ▁value ▁columns ▁first ▁value ▁first ▁value ▁columns
▁Pandas : ▁Keep ▁values ▁in ▁a ▁set ▁of ▁columns ▁if ▁they ▁exist ▁in ▁another ▁set ▁of ▁columns ▁in ▁the ▁same ▁row , ▁otherwise ▁set ▁it ▁to ▁NaN ▁< s > ▁I ▁have ▁a ▁couple ▁of ▁columns ▁in ▁my ▁dataframe ▁that ▁have ▁values ▁in ▁them . ▁I ▁want ▁to ▁only ▁keep ▁those ▁values ▁in ▁those ▁columns ▁if ▁they ▁exist ▁in ▁another ▁set ▁of ▁columns ▁in ▁the ▁same ▁row . ▁Otherwise , ▁I ▁want ▁to ▁set ▁the ▁value ▁to ▁. ▁Here ' s ▁an ▁example ▁dataframe : ▁In ▁this ▁case , ▁I ▁want ▁and ▁to ▁be ▁changed ▁based ▁on ▁and ▁: ▁It ' s ▁been ▁difficult ▁to ▁form ▁a ▁query ▁to ▁google ▁this , ▁and ▁the ▁closest ▁I ' ve ▁gotten ▁is ▁to ▁use ▁like ▁this : ▁Which ▁gives ▁me ▁this : ▁Which ▁appears ▁to ▁be ▁somewhat ▁useful , ▁but ▁I ' m ▁not ▁sure ▁if ▁this ▁is ▁the ▁right ▁path ▁or ▁what ▁to ▁do ▁with ▁it ▁from ▁here . ▁< s > ▁A ▁B ▁C ▁D ▁0 ▁1 ▁30 ▁1 ▁29 ▁1 ▁5 ▁42 ▁99 ▁5 ▁2 ▁64 ▁67 ▁12 ▁22 ▁3 ▁2 ▁22 ▁22 ▁0 ▁4 ▁43 ▁6 ▁9 ▁43 ▁< s > ▁A ▁B ▁C ▁D ▁0 ▁1 ▁30 ▁1.0 ▁NaN ▁1 ▁5 ▁42 ▁NaN ▁5.0 ▁2 ▁64 ▁67 ▁NaN ▁NaN ▁3 ▁2 ▁22 ▁22 .0 ▁NaN ▁4 ▁43 ▁6 ▁NaN ▁4 3.0 ▁< s > ▁values ▁columns ▁columns ▁columns ▁values ▁values ▁columns ▁columns ▁value ▁query ▁right
▁put ▁only ▁elements ▁into ▁a ▁list ▁with ▁a ▁cert ian ▁number ▁< s > ▁is ▁the ▁sales ▁ID ▁and ▁is ▁the ▁sol d ▁item id . ▁I ▁would ▁like ▁to ▁use ▁all ▁unique ▁i _ ids ▁to ▁find ▁all ▁purch ases ▁that ▁have ▁interact ed ▁with ▁i _ id . ▁I ▁also ▁implemented ▁this ▁in ▁the ▁loop . ▁What ▁I ▁would ▁like ▁that ▁I ▁only ▁want ▁to ▁add ▁something ▁to ▁the ▁list ▁when ▁the ▁has ▁more ▁of ▁a ▁1 ▁item . ▁How ▁do ▁I ▁do ▁that ▁so ▁that ▁I ▁only ▁add ▁the ▁purch ases ▁to ▁the ▁list ▁if ▁it ▁contains ▁more ▁than ▁one ▁item ? ▁Output ▁But ▁what ▁I ▁want ▁means ▁that ▁the ▁element ▁does ▁not ▁exist , ▁I ▁only ▁wrote ▁for ▁a ▁better ▁understanding ▁< s > ▁[ [1], ▁[1, ▁2, ▁3], ▁[1 ], ▁[4, ▁1, ▁2 ]] ▁[[1, ▁2, ▁3], ▁[4, ▁1, ▁2 ]] ▁[[1, ▁2, ▁3], ▁[3, ▁5 ]] ▁[[ 4, ▁1, ▁2 ]] ▁[[ 3, ▁5 ]] ▁< s > ▁[[ REMO VED ], ▁[1, ▁2, ▁3], ▁[ REMO VED ], ▁[4, ▁1, ▁2 ]] ▁[[1, ▁2, ▁3], ▁[4, ▁1, ▁2 ]] ▁[[1, ▁2, ▁3], ▁[3, ▁5 ]] ▁[[ 4, ▁1, ▁2 ]] ▁[[ 3, ▁5 ]] ▁< s > ▁put ▁all ▁unique ▁all ▁add ▁item ▁add ▁contains ▁item
▁Multi - column ▁to ▁single ▁column ▁in ▁Pandas ▁< s > ▁I ▁have ▁the ▁following ▁data ▁frame ▁: ▁And ▁I ▁need ▁this ▁: ▁This ▁is ▁just ▁a ▁basic ▁example , ▁the ▁real ▁deal ▁can ▁have ▁over ▁60 ▁children . ▁< s > ▁parent ▁0 ▁1 ▁2 ▁3 ▁0 ▁140 265 29 ▁140 6 25 04 ▁0 ▁0 ▁0 ▁1 ▁14 10 37 93 ▁140 36 09 4 ▁0 ▁0 ▁0 ▁2 ▁140 25 454 ▁140 36 09 4 ▁0 ▁0 ▁0 ▁3 ▁140 30 252 ▁140 30 253 ▁140 62 647 ▁0 ▁0 ▁4 ▁140 34 704 ▁140 86 9 64 ▁0 ▁0 ▁0 ▁< s > ▁parent _ id ▁child _ id ▁0 ▁140 265 29 ▁140 6 25 04 ▁1 ▁140 25 454 ▁140 36 09 4 ▁2 ▁140 30 252 ▁140 30 253 ▁3 ▁140 30 252 ▁140 62 647 ▁4 ▁14 10 37 93 ▁140 36 09 4 ▁5 ▁140 34 704 ▁140 86 9 64
▁Group by ▁and ▁perform ▁row - wise ▁calculation ▁using ▁a ▁custom ▁function ▁< s > ▁Following ▁on ▁from ▁this ▁question : ▁python ▁- ▁Group ▁by ▁and ▁add ▁new ▁row ▁which ▁is ▁calculation ▁of ▁other ▁rows ▁I ▁have ▁a ▁pandas ▁dataframe ▁as ▁follows : ▁And ▁I ▁want ▁to , ▁for ▁each ▁value ▁in ▁col _1, ▁apply ▁a ▁function ▁with ▁the ▁values ▁in ▁col _3 ▁and ▁col _4 ▁( and ▁many ▁more ▁columns ) ▁that ▁correspond ▁to ▁X ▁and ▁Z ▁from ▁col _2 ▁and ▁create ▁a ▁new ▁row ▁with ▁these ▁values . ▁So ▁the ▁output ▁would ▁be ▁as ▁below : ▁Where ▁are ▁the ▁outputs ▁of ▁the ▁function . ▁Original ▁question ▁( which ▁only ▁requires ▁a ▁simple ▁addition ) ▁was ▁answered ▁with : ▁I ' m ▁now ▁looking ▁for ▁a ▁way ▁to ▁use ▁a ▁custom ▁function , ▁such ▁as ▁or ▁, ▁rather ▁than ▁. ▁How ▁can ▁I ▁modify ▁this ▁code ▁to ▁work ▁with ▁my ▁new ▁requirements ? ▁< s > ▁col _1 ▁col _2 ▁col _3 ▁col _4 ▁a ▁X ▁5 ▁1 ▁a ▁Y ▁3 ▁2 ▁a ▁Z ▁6 ▁4 ▁b ▁X ▁7 ▁8 ▁b ▁Y ▁4 ▁3 ▁b ▁Z ▁6 ▁5 ▁< s > ▁col _1 ▁col _2 ▁col _3 ▁col _4 ▁a ▁X ▁5 ▁1 ▁a ▁Y ▁3 ▁2 ▁a ▁Z ▁6 ▁4 ▁a ▁NEW ▁* ▁* ▁b ▁X ▁7 ▁8 ▁b ▁Y ▁4 ▁3 ▁b ▁Z ▁6 ▁5 ▁b ▁NEW ▁* ▁* ▁< s > ▁add ▁value ▁apply ▁values ▁columns ▁values ▁now
▁How ▁can ▁I ▁remove ▁a ▁certain ▁type ▁of ▁values ▁in ▁a ▁group ▁in ▁pandas ? ▁< s > ▁I ▁have ▁the ▁following ▁dataframe ▁which ▁is ▁a ▁small ▁part ▁of ▁a ▁bigger ▁one : ▁I ' d ▁like ▁to ▁delete ▁all ▁rows ▁where ▁the ▁last ▁items ▁are ▁" d ". ▁So ▁my ▁desired ▁dataframe ▁would ▁look ▁like ▁this : ▁So ▁the ▁point ▁is , ▁that ▁a ▁group ▁shouldn ' t ▁have ▁" d " ▁as ▁the ▁last ▁item . ▁There ▁is ▁a ▁code ▁that ▁deletes ▁the ▁last ▁row ▁in ▁the ▁groups ▁where ▁the ▁last ▁item ▁is ▁" d ". ▁But ▁in ▁this ▁case , ▁I ▁have ▁to ▁run ▁the ▁code ▁twice ▁to ▁delete ▁all ▁last ▁" d "- s ▁in ▁group ▁3 ▁for ▁example . ▁Is ▁there ▁a ▁better ▁solution ▁to ▁this ▁problem ? ▁< s > ▁acc _ num ▁trans _ cd i ▁0 ▁1 ▁c ▁1 ▁1 ▁d ▁3 ▁3 ▁d ▁4 ▁3 ▁c ▁5 ▁3 ▁d ▁6 ▁3 ▁d ▁< s > ▁acc _ num ▁trans _ cd i ▁0 ▁1 ▁c ▁3 ▁3 ▁d ▁4 ▁3 ▁c ▁< s > ▁values ▁delete ▁all ▁where ▁last ▁items ▁last ▁item ▁last ▁groups ▁where ▁last ▁item ▁delete ▁all ▁last
▁Transform ▁a ▁large ▁dataframe ▁- ▁takes ▁too ▁long ▁< s > ▁I ▁have ▁a ▁dataframe ▁loaded ▁from ▁a ▁CSV ▁in ▁the ▁following ▁format : ▁I ▁want ▁to ▁transform ▁it ▁to ▁the ▁following ▁format : ▁This ▁is ▁my ▁function ▁( ▁is ▁the ▁original ▁data ▁frame ) ▁that ▁does ▁the ▁transformation , ▁but ▁it ▁takes ▁7 ▁minutes ▁for ▁54 75 00 ▁row ▁dataframe . ▁Is ▁there ▁a ▁way ▁to ▁speed ▁it ▁up ? ▁< s > ▁stock _ code ▁price ▁201 808 27 ▁001 ▁10 ▁201 808 27 ▁00 2 ▁11 ▁201 808 27 ▁00 3 ▁12 ▁201 808 27 ▁00 4 ▁13 ▁201 808 26 ▁001 ▁14 ▁201 808 26 ▁00 2 ▁15 ▁201 808 26 ▁00 3 ▁11 ▁201 808 26 ▁00 4 ▁10 ▁201 808 26 ▁00 5 ▁19 ▁< s > ▁001 ▁00 2 ▁00 3 ▁00 4 ▁00 5 ▁201 808 27 ▁10 ▁11 ▁12 ▁13 ▁nan ▁201 808 26 ▁14 ▁15 ▁11 ▁10 ▁19 ▁< s > ▁transform
▁Drop ▁last ▁n ▁rows ▁within ▁pandas ▁dataframe ▁groupby ▁< s > ▁I ▁have ▁a ▁dataframe ▁where ▁I ▁want ▁to ▁drop ▁last ▁rows ▁within ▁a ▁group ▁of ▁columns . ▁For ▁example , ▁say ▁is ▁defined ▁as ▁below ▁the ▁group ▁is ▁of ▁columns ▁and ▁: ▁Desired ▁output ▁for ▁is ▁as ▁follows : ▁Desired ▁output ▁for ▁is ▁as ▁follows : ▁< s > ▁>>> ▁df ▁a ▁b ▁c ▁d ▁0 ▁ab d ▁john ▁0 ▁1000 ▁1 ▁ab d ▁john ▁1 ▁1001 ▁4 ▁p qr ▁john ▁4 ▁100 4 ▁9 ▁xyz ▁do e ▁9 ▁100 9 ▁10 ▁xyz ▁do e ▁10 ▁101 0 ▁11 ▁xyz ▁do e ▁11 ▁101 1 ▁12 ▁xyz ▁do e ▁12 ▁101 2 ▁13 ▁xyz ▁do e ▁13 ▁101 3 ▁>>> ▁< s > ▁>>> ▁df ▁a ▁b ▁c ▁d ▁0 ▁ab d ▁john ▁0 ▁1000 ▁9 ▁xyz ▁do e ▁9 ▁100 9 ▁10 ▁xyz ▁do e ▁10 ▁101 0 ▁11 ▁xyz ▁do e ▁11 ▁101 1 ▁12 ▁xyz ▁do e ▁12 ▁101 2 ▁>>> ▁< s > ▁last ▁groupby ▁where ▁drop ▁last ▁columns ▁columns
▁Python ▁trans posing ▁multiple ▁dataframes ▁in ▁a ▁list ▁< s > ▁I ▁have ▁a ▁few ▁dataframes ▁which ▁are ▁similar ▁( in ▁terms ▁of ▁number ▁of ▁rows ▁and ▁columns ) ▁to ▁the ▁2 ▁dataframes ▁listed ▁below ▁my ▁desired ▁output ▁is ▁to ▁have ▁multiple ▁dataframes ▁with ▁the ▁email ▁as ▁column ▁header ▁and ▁the ▁factor ▁or ▁item ▁as ▁rows ▁I ▁am ▁able ▁to ▁get ▁the ▁result ▁by ▁trans posing ▁each ▁dataframe ▁individually ▁using ▁this ▁but ▁i ' d ▁like ▁to ▁create ▁a ▁for ▁loop ▁as ▁i ▁have ▁several ▁dataframes ▁to ▁transpose ▁wrote ▁something ▁like ▁this ▁but ▁the ▁dataframes ▁do ▁not ▁get ▁trans posed . ▁Would ▁like ▁to ▁directly ▁change ▁the ▁dataframes ▁in ▁the ▁list ▁of ▁dataframes ▁( som ewhere ▁along ▁the ▁lines ▁of ▁inplace = True ). ▁Was ▁wondering ▁if ▁there ▁is ▁something ▁i ▁am ▁missing , ▁appreciate ▁any ▁form ▁of ▁help , ▁thank ▁you . ▁< s > ▁0 ▁email ▁factor 1_ final ▁factor 2_ final ▁factor 3_ final ▁1 ▁john @ abc . com ▁85 % ▁90 % ▁50% ▁2 ▁p eter @ abc . com ▁80 % ▁60 % ▁60 % ▁3 ▁sh el by @ abc . com ▁50% ▁70 % ▁60 % ▁4 ▁j ess @ abc . com ▁60 % ▁65 % ▁50% ▁5 ▁mark @ abc . com ▁98 % ▁50% ▁60 % ▁< s > ▁email ▁john @ abc . com ▁p eter @ abc . com ▁sh el by @ abc . com ▁j ess @ abc . com ▁mark @ abc . com ▁factor 1 ▁85 % ▁80 % ▁50% ▁60 % ▁98 % ▁factor 2 ▁90 % ▁60 % ▁70 % ▁65 % ▁50% ▁factor 3 ▁50% ▁60 % ▁60 % ▁50% ▁60 % ▁< s > ▁columns ▁item ▁get ▁transpose ▁get ▁any
▁pandas ▁- ▁down sample ▁a ▁more ▁frequent ▁DataFrame ▁to ▁the ▁frequency ▁of ▁a ▁less ▁frequent ▁DataFrame ▁< s > ▁I ▁have ▁two ▁DataFrames ▁that ▁have ▁different ▁data ▁measured ▁at ▁different ▁frequencies , ▁as ▁in ▁those ▁csv ▁examples : ▁df 1: ▁df 2: ▁I ▁would ▁like ▁to ▁obtain ▁a ▁single ▁df ▁that ▁have ▁all ▁the ▁measures ▁of ▁both ▁dfs ▁at ▁the ▁times ▁of ▁the ▁first ▁one ▁( which ▁get ▁data ▁less ▁frequently ). ▁I ▁tried ▁to ▁do ▁that ▁with ▁a ▁for ▁loop ▁aver aging ▁over ▁the ▁df 2 ▁measures ▁between ▁two ▁timestamps ▁of ▁df 1 ▁but ▁it ▁was ▁extremely ▁slow . ▁< s > ▁i , m 1, m 2, t ▁0, 0.5 565 29, 6. 86 32 55, 4 35 64. 8 44 ▁1, 0.5 56 55 76 1 9999 9 88 4, 6. 86 32 774 999999 9, 4 35 64. 86 3 99999999 4 ▁2, 0.5 56 55 59 4 0000000 3, 6. 86 327 64, 4 35 64. 88 4 ▁3, 0.5 56 56 997 999999 4 1, 6. 86 32 86 7 9999 999 6, 4 35 64. 90 3 99999999 5 ▁4, 0.5 56 55 70 2 0000000 7, 6. 86 32 77 2 00000001 , 4 35 64. 9 24 ▁5, 0.5 565 31 64 000000 9 7, 6. 86 32 57 1 0000000 7, 4 35 64. 9 44 ▁... ▁< s > ▁i , m 3, m 4, t ▁0, 30 6. 8 116 25 00000 59 6, -1. 212 68 700 45 404 68 3, 4 35 64. 8 78 125 ▁1, 30 6. 86 175 000000 7 25, -1.1 70 58 38 27 266 64 3 3, 4 35 64. 9 28 25 000000 4 ▁2, 30 6. 77 55 245 45 44 78 7, -1.1 24 01 95 38 64 46 19 5, 4 35 64. 9 78 374 9999 9 ▁3, 30 6. 859 005 45 45 40 86, -1 .0 210 345 36 369 208 4, 43 56 5.0 285 ▁4, 30 6. 8 354 25 00000 5 2, -1. 005 24 3 177 26 66 65 7, 43 56 5.0 786 25 ▁5, 30 6. 88 39 74 9999 9 28 6, -0. 94 68 344 80 99 1 789 6, 43 56 5.1 28 75 ▁... ▁< s > ▁DataFrame ▁DataFrame ▁at ▁all ▁at ▁first ▁get ▁between
▁Merge ▁two ▁columns ▁of ▁a ▁dataframe ▁into ▁an ▁already ▁existing ▁column ▁of ▁dictionaries ▁as ▁a ▁key ▁value ▁pair ▁< s > ▁If ▁we ▁have ▁3 ▁columns ▁of ▁a ▁dataframe ▁as ▁: ▁I ▁want ▁the ▁column 3 ▁to ▁be ▁something ▁like ▁: ▁I ▁have ▁tried ▁a ▁few ▁things ▁from ▁using ▁lambda ▁functions ▁with ▁apply ▁to ▁iterating ▁over ▁rows ▁but ▁all ▁were ▁unsuccessful . ▁< s > ▁column 1 ▁: ▁[' A ',' A ',' B ',' C '] ▁column 2 ▁: ▁[ 12, 13, 14, 15 ] ▁column 3 ▁: ▁[{" key 1 ":" val 1" },{" key 2 ":" val 2" },{" key 3 ":" val 3" },{" key 4 ":" val 4" }] ▁< s > ▁column 3 ▁: ▁[{" key 1 ":" val 1", ▁" A ": 12 },{" key 2 ":" val 2", ▁" A ": 13 },{" key 3 ":" val 3", ▁" B ": 14 },{" key 4 ":" val 4", ▁" C ": 15 }] ▁< s > ▁columns ▁value ▁columns ▁apply ▁all
▁Sw ap ▁contents ▁of ▁columns ▁inside ▁dataframe ▁< s > ▁I ▁have ▁a ▁pandas ▁dataframe ▁with ▁this ▁contents ; ▁I ▁would ▁like ▁to ▁swap ▁the ▁contents ▁between ▁Column ▁1 ▁and ▁Column ▁2. ▁The ▁output ▁dataframe ▁should ▁look ▁like ▁this ; ▁I ▁am ▁using ▁python ▁v 3.6 ▁< s > ▁Column 1 ▁Column 2 ▁Column 3 ▁C 11 ▁C 21 ▁C 31 ▁C 12 ▁C 22 ▁C 32 ▁C 13 ▁C 23 ▁C 33 ▁< s > ▁Column 1 ▁Column 2 ▁Column 3 ▁C 21 ▁C 11 ▁C 31 ▁C 22 ▁C 12 ▁C 32 ▁C 23 ▁C 13 ▁C 33 ▁< s > ▁columns ▁between
▁How ▁does ▁pandas ▁convert ▁one ▁column ▁of ▁data ▁into ▁another ? ▁< s > ▁I ▁have ▁a ▁dataframe ▁generated ▁by ▁pandas , ▁as ▁follows : ▁I ▁want ▁to ▁convert ▁the ▁CODE ▁column ▁data ▁to ▁get ▁the ▁NUM ▁column . ▁The ▁encoding ▁rules ▁are ▁as ▁follows : ▁thank ▁you ! ▁< s > ▁NO ▁CODE ▁1 ▁a ▁2 ▁a ▁3 ▁a ▁4 ▁a ▁5 ▁a ▁6 ▁a ▁7 ▁b ▁8 ▁b ▁9 ▁a ▁10 ▁a ▁11 ▁a ▁12 ▁a ▁13 ▁b ▁14 ▁a ▁15 ▁a ▁16 ▁a ▁< s > ▁NO ▁CODE ▁NUM ▁1 ▁a ▁1 ▁2 ▁a ▁2 ▁3 ▁a ▁3 ▁4 ▁a ▁4 ▁5 ▁a ▁5 ▁6 ▁a ▁6 ▁7 ▁b ▁b ▁8 ▁b ▁b ▁9 ▁a ▁1 ▁10 ▁a ▁2 ▁11 ▁a ▁3 ▁12 ▁a ▁4 ▁13 ▁b ▁b ▁14 ▁a ▁1 ▁15 ▁a ▁2 ▁16 ▁a ▁3 ▁< s > ▁get
▁Add ▁a ▁different ▁item ▁from ▁a ▁list ▁to ▁each ▁cell ▁in ▁a ▁dataframe ▁with ▁Pandas ▁< s > ▁Given ▁a ▁dataframe ▁I ▁want ▁to ▁make ▁a ▁list ▁of ▁sequential ▁that ▁has ▁as ▁many ▁elements ▁as ▁there ▁are ▁rows ▁in ▁the ▁dataframe ▁And ▁then ▁add ▁each ▁element ▁of ▁the ▁list ▁( as ▁a ▁string ) ▁onto ▁the ▁end ▁of ▁a ▁single ▁column ▁in ▁the ▁dataframe . ▁Does ▁not ▁work , ▁as ▁it ▁adds ▁the ▁whole ▁list ▁as ▁a ▁string ▁to ▁each ▁value ▁as ▁opposed ▁to ▁one ▁value ▁of ▁the ▁list ▁per ▁value ▁in ▁the ▁dataframe . ▁Essentially , ▁I ▁want ▁to ▁transform ▁to ▁So ▁anyway ▁to ▁do ▁that ▁would ▁be ▁fine . ▁Thanks ▁for ▁the ▁help ! ▁< s > ▁A ▁B ▁C ▁23 ▁16 ▁85 ▁9 ▁74 ▁12 ▁99 ▁24 ▁83 ▁< s > ▁A ▁B ▁C ▁2 31 ▁16 ▁85 ▁92 ▁74 ▁12 ▁99 3 ▁24 ▁83 ▁< s > ▁item ▁add ▁value ▁value ▁value ▁transform
▁Pandas : ▁Sw ap ▁rows ▁between ▁columns ▁< s > ▁Some ▁rows ▁were ▁input ▁in ▁the ▁wrong ▁columns ▁so ▁now ▁I ▁need ▁to ▁swap ▁them . ▁My ▁current ▁approach ▁Expected ▁output ▁It ▁works ▁but ▁this ▁is ▁only ▁possible ▁because ▁it ▁was ▁4 ▁columns . ▁Is ▁there ▁a ▁better ▁way ▁to ▁do ▁this ? ▁Note ▁the ▁dtypes ▁can ▁cause ▁issues ▁with ▁sorting ▁is ▁Maybe ▁there ▁is ▁something ▁like ▁< s > ▁a ▁b ▁c ▁d ▁0 ▁0 ▁10 ▁22 :58 :00 ▁23 :27 :00 ▁1 ▁10 ▁17 ▁23 :03 :00 ▁23 :39 :00 ▁2 ▁22 :58 :00 ▁23 :27 :00 ▁0 ▁10 ▁3 ▁23 :03 :00 ▁23 :39 :00 ▁10 ▁17 ▁< s > ▁a ▁b ▁c ▁d ▁0 ▁0 ▁10 ▁22 :58 :00 ▁23 :27 :00 ▁1 ▁10 ▁17 ▁23 :03 :00 ▁23 :39 :00 ▁2 ▁0 ▁10 ▁22 :58 :00 ▁23 :27 :00 ▁3 ▁10 ▁17 ▁23 :03 :00 ▁23 :39 :00 ▁< s > ▁between ▁columns ▁columns ▁now ▁columns ▁dtypes
▁Turn ▁columns &# 39 ; ▁values ▁to ▁headers ▁of ▁columns ▁with ▁values ▁1 ▁and ▁0 ▁( ▁accordingly ) ▁[ python ] ▁< s > ▁I ▁got ▁a ▁column ▁of ▁the ▁form ▁: ▁The ▁column ▁represents ▁the ▁answers ▁of ▁users ▁to ▁a ▁question ▁of ▁5 ▁choices ▁(1 -5 ). ▁I ▁want ▁to ▁turn ▁this ▁into ▁a ▁matrix ▁of ▁5 ▁columns ▁where ▁the ▁indexes ▁are ▁the ▁5 ▁possible ▁answers ▁and ▁the ▁values ▁are ▁1 ▁or ▁0 ▁according ▁to ▁the ▁user ' s ▁given ▁answer . ▁Visual y ▁i ▁want ▁a ▁matrix ▁of ▁the ▁form : ▁< s > ▁0 ▁q 4 ▁1 ▁4 ▁2 ▁3 ▁3 ▁1 ▁4 ▁2 ▁5 ▁1 ▁6 ▁5 ▁7 ▁1 ▁8 ▁3 ▁< s > ▁0 ▁q 4 _1 ▁q 4 _2 ▁q 4 _3 ▁q 4 _4 ▁q 4_ 5 ▁1 ▁N an ▁N an ▁N an ▁1 ▁N an ▁2 ▁N an ▁N an ▁1 ▁N an ▁N an ▁3 ▁1 ▁N an ▁N an ▁N an ▁N an ▁4 ▁N an ▁1 ▁N an ▁N an ▁N an ▁5 ▁1 ▁N an ▁N an ▁N an ▁N an ▁< s > ▁columns ▁values ▁columns ▁values ▁columns ▁where ▁values
▁Use ▁duplicated ▁values ▁to ▁increment ▁column ▁< s > ▁I ▁have ▁a ▁Pandas ▁dataframe ▁and ▁I ▁want ▁to ▁increment ▁a ▁column ▁based ▁on ▁the ▁amount ▁of ▁duplicated ▁values . ▁So ▁when ▁a ▁duplicate ▁is ▁found , ▁all ▁other ▁occurrences ▁is ▁incremented . ▁So ▁given ▁this ▁input ▁dataframe ▁return ▁I ▁tried ▁this ▁line ▁of ▁code ▁but ▁I ▁don ' t ▁know ▁how ▁to ▁increment ▁< s > ▁SM ▁0 ▁AB ▁1 ▁AC ▁2 ▁AD ▁3 ▁AB ▁4 ▁AB ▁5 ▁AC ▁6 ▁A E ▁7 ▁AD ▁< s > ▁SM ▁DM ▁0 ▁AB ▁AB ▁1 ▁AC ▁AC ▁2 ▁AD ▁AD ▁3 ▁AB ▁AB _1 ▁4 ▁AB ▁AB _2 ▁5 ▁AC ▁AC _1 ▁6 ▁A E ▁A E ▁7 ▁AD ▁AD _1 ▁< s > ▁duplicated ▁values ▁duplicated ▁values ▁all
▁Append ▁list ▁to ▁list ▁in ▁specific ▁cell ▁in ▁pandas ▁< s > ▁I ▁have ▁a ▁pandas ▁dataframe , ▁which ▁looks ▁like ▁this : ▁And ▁I ▁want ▁to ▁append ▁a ▁new ▁list ▁c ▁= ▁[5, 6] ▁to ▁the ▁row , ▁where ▁So ▁the ▁result ▁would ▁be : ▁Right ▁now ▁my ▁attempt ▁looks ▁like ▁this : ▁Any ▁help ▁is ▁appreciated ! ▁< s > ▁key ▁arr ▁a ▁' t 1' ▁[1, 2] ▁b ▁' t 2' ▁[3, 4] ▁< s > ▁key ▁arr ▁a ▁' t 1' ▁[1, 2, 5, 6] ▁b ▁' t 2' ▁[3, 4] ▁< s > ▁append ▁where ▁now
▁Group ▁a ▁dataframe ▁on ▁one ▁column ▁and ▁take ▁max ▁from ▁one ▁column ▁and ▁its ▁corresponding ▁value ▁from ▁the ▁other ▁col ▁< s > ▁I ▁have ▁a ▁large ▁dataframe ▁which ▁has ▁a ▁similar ▁pattern ▁as ▁below : ▁And ▁can ▁be ▁constructed ▁as : ▁Now ▁I ▁want ▁to ▁group ▁this ▁dataframe ▁by ▁the ▁first ▁column ▁i . e ., ▁and ▁take ▁from ▁the ▁column ▁and ▁its ▁corresponding ▁value ▁from ▁. ▁And ▁if ▁there ▁are ▁two ▁max ▁values ▁in ▁, ▁then ▁I ▁would ▁like ▁to ▁take ▁alphabetically ▁first ▁value ▁from ▁. ▁So ▁my ▁expected ▁result ▁would ▁look ▁like : ▁I ▁have ▁tried ▁but ▁this ▁selects ▁max ▁from ▁and ▁first ▁from ▁both ▁at ▁the ▁same ▁time . ▁Additionally ▁I ▁know ▁there ▁is ▁a ▁approach , ▁but ▁this ▁would ▁take ▁a ▁lot ▁of ▁time ▁for ▁my ▁dataset . ▁Any ▁suggestions ▁on ▁how ▁could ▁I ▁proceed ▁would ▁be ▁appreciated . ▁Thanks ▁in ▁advance :) ▁< s > ▁X ▁Y ▁Z ▁0 ▁a ▁p ▁2 ▁1 ▁a ▁q ▁5 ▁2 ▁a ▁r ▁6 ▁3 ▁a ▁s ▁3 ▁4 ▁b ▁w ▁10 ▁5 ▁b ▁z ▁20 ▁6 ▁b ▁y ▁9 ▁7 ▁b ▁x ▁20 ▁< s > ▁df ▁= ▁{ ▁' X ': ▁[' a ', ▁' a ', ▁' a ', ▁' a ', ▁' b ', ▁' b ', ▁' b ', ▁' b '], ▁' Y ': ▁[' p ', ▁' q ', ▁' r ', ▁' s ', ▁' w ', ▁' x ', ▁' y ', ▁' z '], ▁' Z ': ▁[2, ▁5, ▁6, ▁3, ▁10, ▁20, ▁9, ▁5] ▁} ▁< s > ▁take ▁max ▁value ▁first ▁take ▁value ▁max ▁values ▁take ▁first ▁value ▁max ▁first ▁at ▁time ▁take ▁time
▁is ▁there ▁a ▁way ▁to ▁read ▁multiple ▁excel ▁tab / sheets ▁from ▁single ▁xlsx ▁to ▁multiple ▁dataframes ▁with ▁each ▁dataframe ▁named ▁with ▁sheet ▁name ? ▁< s > ▁I ▁am ▁not ▁good ▁in ▁python ▁please ▁forg ive ▁me ▁for ▁this ▁question ▁but ▁I ▁need ▁to ▁create ▁a ▁function ▁which ▁does ▁the ▁following ▁thing : ▁Create ▁multiple ▁data ▁frames ▁from ▁multiple ▁excel ▁tab / sheet ▁present ▁in ▁a ▁single ▁xlsx ▁file ▁and ▁be ▁named ▁on ▁the ▁sheet ▁name . ▁The ▁columns ' ▁values ▁should ▁be ▁concatenated ▁and ▁checked ▁if ▁there ▁is ▁no ▁duplicate ▁value . ▁if ▁the ▁concat ▁value ▁has ▁a ▁duplicate ▁then ▁it ▁should ▁be ▁told ▁as ▁yes / No ▁in ▁another ▁column . ▁all ▁the ▁dataframes ▁then ▁should ▁be ▁written ▁into ▁a ▁single ▁workbook ▁as ▁different ▁work sheets ▁inside . ▁values ▁inside ▁() ▁are ▁columns ▁for ▁better ▁understanding ▁example : ▁sheet 1 ▁result : ▁sheet 2 ▁result : ▁< s > ▁( a ) ▁( b ) ▁( c ) ▁( d ) ▁a 1 ▁b 1 ▁c 1 ▁d 1 ▁a 2 ▁b 2 ▁c 2 ▁d 2 ▁< s > ▁( a ) ▁( b ) ▁( e ) ▁( f ) ▁a 3 ▁b 3 ▁e 1 ▁f 1 ▁a 4 ▁b 4 ▁e 1 ▁f 1 ▁a 5 ▁b 5 ▁e 2 ▁f 2 ▁a 6 ▁b 6 ▁e 4 ▁f 4 ▁a 7 ▁a 8 ▁e 4 ▁f 5 ▁< s > ▁name ▁name ▁columns ▁values ▁value ▁concat ▁value ▁all ▁values ▁columns
▁How ▁to ▁pick ▁some ▁values ▁of ▁a ▁column ▁and ▁make ▁another ▁one ▁with ▁them ? ▁< s > ▁This ▁is ▁a ▁table ▁similar ▁to ▁the ▁one ▁I ' m ▁working ▁with ▁And ▁what ▁I ' m ▁trying ▁to ▁do ▁is ▁take ▁some ▁values ▁of ▁the ▁column ▁A ▁that ▁follow ▁a ▁certain ▁pattern ▁and ▁create ▁another ▁column ▁with ▁such ▁values . ▁For ▁example , ▁the ▁column ▁C ▁would ▁have ▁only ▁the ▁values ▁from ▁A ▁that ▁are ▁bigger ▁than ▁12, ▁and ▁column ▁D ▁the ▁ones ▁smaller ▁or ▁equal : ▁I ' ve ▁tried ▁making ▁a ▁list ▁for ▁each ▁group ▁of ▁values , ▁but ▁I ▁can ' t ▁merge ▁them ▁back ▁with ▁the ▁original ▁table , ▁since ▁there ▁are ▁some ▁numbers ▁that ▁repeat ▁and ▁the ▁number ▁of ▁columns ▁grow . ▁I ▁think ▁there ' s ▁an ▁es as ier ▁way ▁to ▁do ▁that , ▁but ▁I ▁can ' t ▁seem ▁to ▁find ▁it . ▁How ▁can ▁I ▁do ▁that ? ▁< s > ▁A ▁B ▁0 ▁12. 2 ▁43 ▁1 ▁10.1 ▁32 ▁2 ▁3.4 ▁34 ▁3 ▁12.0 ▁55 ▁4 ▁40. 6 ▁31 ▁< s > ▁A ▁B ▁C ▁D ▁0 ▁12. 2 ▁43 ▁12. 2 ▁NaN ▁1 ▁10.1 ▁32 ▁NaN ▁10.1 ▁2 ▁3.4 ▁34 ▁NaN ▁3.4 ▁3 ▁12.0 ▁55 ▁NaN ▁12.0 ▁4 ▁40. 6 ▁31 ▁40. 6 ▁NaN ▁< s > ▁values ▁take ▁values ▁values ▁values ▁values ▁merge ▁repeat ▁columns
▁Using ▁pandas . interpolate () ▁< s > ▁Suppose ▁I ▁would ▁like ▁to ▁apply ▁following ▁command : ▁which ▁returns ▁Question : ▁How ▁can ▁i ▁apply ▁a ▁restriction ▁on ▁the ▁minimum ▁number ▁of ▁valid ▁numbers ▁( i . e ▁not ▁NaN ) ▁before ▁AND ▁after ▁a ▁group ▁of ▁NaN s , ▁so ▁as ▁to ▁apply ▁the ▁interpolation ▁In ▁this ▁example , ▁i ▁would ▁like ▁to ▁fill ▁first ▁group ▁of ▁NaN s ▁because ▁there ▁are ▁minimum ▁3 ▁valid ▁numbers ▁before ▁AND ▁after , ▁but ▁NOT ▁interpolate ▁the ▁second ▁group ▁of ▁NaN s , ▁as ▁there ▁are ▁only ▁two ▁valid ▁numbers ▁after ▁the ▁NaN s ▁( and ▁not ▁3 ▁as ▁i ▁would ▁prefer ) ▁Expected ▁result : ▁< s > ▁0 ▁0 ▁1. 000000 ▁1 ▁2. 000000 ▁2 ▁3. 000000 ▁3 ▁3.1 6666 7 ▁4 ▁3. 333333 ▁5 ▁NaN ▁6 ▁3. 6666 67 ▁7 ▁3.8 3333 3 ▁8 ▁4. 000000 ▁9 ▁5. 000000 ▁10 ▁6. 000000 ▁11 ▁5. 500 000 ▁12 ▁5. 000000 ▁13 ▁NaN ▁14 ▁4. 000000 ▁15 ▁3. 500 000 ▁16 ▁3. 000000 ▁17 ▁4. 000000 ▁18 ▁NaN ▁< s > ▁0 ▁0 ▁1. 000000 ▁1 ▁2. 000000 ▁2 ▁3. 000000 ▁3 ▁3.1 6666 7 ▁4 ▁3. 333333 ▁5 ▁NaN ▁6 ▁3. 6666 67 ▁7 ▁3.8 3333 3 ▁8 ▁4. 000000 ▁9 ▁5. 000000 ▁10 ▁6. 000000 ▁11 ▁NaN ▁12 ▁NaN ▁13 ▁NaN ▁14 ▁NaN ▁15 ▁NaN ▁16 ▁3. 000000 ▁17 ▁4. 000000 ▁18 ▁NaN ▁< s > ▁interpolate ▁apply ▁apply ▁apply ▁first ▁interpolate ▁second
▁Compare ▁two ▁columns ▁that ▁contains ▁timestamps ▁in ▁pandas ▁< s > ▁Lets ▁say ▁I ▁have ▁a ▁dataframe ▁like ▁this ▁one : ▁I ▁want ▁to ▁compare ▁if ▁the ▁timestamp ▁in ▁Col 1 ▁is ▁greater ▁than ▁in ▁Col 2 ▁and ▁if ▁that ▁is ▁true ▁I ▁want ▁to ▁remove ▁the ▁timestamps ▁from ▁the ▁other ▁columns ▁( Col 2, ▁Col 3, ▁Col 4 ). ▁I ▁also ▁want ▁to ▁check ▁if ▁timestamp ▁in ▁Col 2 ▁is ▁greater ▁than ▁in ▁Col 3 ▁and ▁if ▁that ▁is ▁true ▁I ▁want ▁to ▁remove ▁timestamp ▁from ▁other ▁columns ▁Col 3, ▁Col 4 ). ▁I ▁tried ▁this ▁one : ▁But ▁it ▁is ▁showing ▁me ▁this ▁error : ▁My ▁des irable ▁output ▁would ▁look ▁like ▁this : ▁EDIT ED : ▁Added ▁Col 0 ▁< s > ▁Col 0 ▁Col 1 ▁Col 2 ▁Col 3 ▁Col 4 ▁1. txt ▁20 21 -06 -23 ▁15 :04 :30 ▁20 21 -06 -23 ▁14 :10 :30 ▁20 21 -06 -23 ▁14 :15 :30 ▁20 21 -06 -23 ▁14 :20 :30 ▁2. txt ▁20 21 -06 -23 ▁14 :25 :30 ▁20 21 -06 -23 ▁15 :30 :30 ▁20 21 -06 -23 ▁14 :35 :30 ▁20 21 -06 -23 ▁14 :40 :30 ▁< s > ▁Col 0 ▁Col 1 ▁Col 2 ▁Col 3 ▁Col 4 ▁1. txt ▁20 21 -06 -23 ▁15 :04 :30 ▁NaN ▁NaN ▁NaN ▁2. txt ▁20 21 -06 -23 ▁14 :25 :30 ▁20 21 -06 -23 ▁15 :30 :30 ▁NaN ▁NaN ▁< s > ▁columns ▁contains ▁compare ▁timestamp ▁columns ▁timestamp ▁timestamp ▁columns
▁remove ▁un named ▁col ums ▁pandas ▁dataframe ▁< s > ▁i ' m ▁a ▁student ▁and ▁have ▁a ▁problem ▁that ▁i ▁cant ▁figure ▁it ▁out ▁how ▁to ▁solve ▁it . i ▁have ▁csv ▁data ▁like ▁this ▁: ▁code ▁for ▁reading ▁csv ▁like ▁this ▁: ▁S MT ▁print ▁out ▁: ▁expected ▁output ▁: ▁i ▁already ▁trying ▁but ▁it ▁will ▁be ▁like ▁this ▁: ▁i ▁already ▁trying ▁to ▁put ▁or ▁// is ▁not ▁working , ▁and ▁the ▁last ▁time ▁I ▁tried ▁it ▁like ▁this ▁: ▁and ▁i ▁got ▁i ▁just ▁want ▁to ▁get ▁rid ▁the ▁Un named : ▁5 ▁~ ▁Un named : ▁8, ▁how ▁the ▁correct ▁way ▁to ▁get ▁rid ▁of ▁this ▁Un named ▁thing ▁? ▁< s > ▁1 ▁2 ▁3 ▁4 ▁6 ▁7 ▁8 ▁9 ▁11 ▁12 ▁13 ▁14 ▁< s > ▁1 ▁2 ▁3 ▁4 ▁0 ▁6 ▁7 ▁8 ▁9 ▁1 ▁11 ▁12 ▁13 ▁14 ▁2 ▁0 ▁0 ▁0 ▁0 ▁< s > ▁put ▁last ▁time ▁get ▁get
▁Using ▁values ▁from ▁dataframe ▁for ▁calculation ▁< s > ▁I ▁am ▁selecting ▁dedicated ▁data ▁from ▁a ▁dataframe ▁and ▁would ▁like ▁to ▁make ▁a ▁linear ▁interpolation ▁based ▁on ▁my ▁defined ▁formula : ▁I ▁would ▁like ▁to ▁interpolate ▁e . g . ▁between ▁rank ▁2.0 ▁and ▁3.0 , ▁where ▁the ▁needed ▁rank ▁is ▁2. 5. ▁The ▁calculation ▁looks ▁like ▁the ▁following : ▁y ▁= ▁- 9.0 8 0002 ▁+ ▁( -9 .0 3 999 3 ▁- ▁( -9 .0 8 000 2)) * [( 2. 5- 2) /( 3- 2) ] ▁= ▁- 9.0 5 999 75 00000 ▁where ▁the ▁values ▁are ▁defined ▁in ▁the ▁code ▁as ▁the ▁following : ▁The ▁code ▁looks ▁like ▁the ▁following : ▁The ▁result ▁looks ▁like ▁the ▁following : ▁The ▁Excel ▁file ▁looks ▁like ▁the ▁following : ▁< s > ▁y ▁= ▁y 0 ▁+ ▁( y 1 ▁- ▁y 0) ▁* ▁[( x - x 0) /( x 1- x 0 )] ▁< s > ▁- 9.0 8 0002 00000000 7 ▁2.0 ▁- 9. 36 0001 00000001 1 ▁1.0 ▁- 9. 22 0001 5 000000 1 ▁< s > ▁values ▁interpolate ▁between ▁rank ▁where ▁rank ▁where ▁values
▁How ▁to ▁average ▁DataFrame ▁row ▁with ▁another ▁row ▁only ▁if ▁the ▁first ▁row ▁is ▁a ▁substring ▁of ▁other ▁next ▁row ▁< s > ▁I ▁have ▁a ▁dataframe ▁called ▁' data ': ▁I ▁want ▁to ▁combine ▁' ABC -1 ' ▁with ▁' ABC -1 B ' ▁into ▁a ▁single ▁row ▁using ▁the ▁first ▁USER ▁name ▁and ▁then ▁aver aging ▁the ▁two ▁values ▁to ▁arrive ▁here : ▁The ▁dataframe ▁may ▁not ▁be ▁in ▁order ▁and ▁there ▁are ▁other ▁values ▁in ▁there ▁as ▁well ▁that ▁are ▁unrelated ▁that ▁don ' t ▁need ▁aver aging . ▁I ▁only ▁want ▁to ▁average ▁the ▁two ▁rows ▁where ▁' XXX - X ' ▁is ▁in ▁' XXX - X B ' ▁< s > ▁USER ▁VALUE ▁X O X O ▁21 ▁ABC -1 ▁2 ▁ABC -1 B ▁4 ▁ABC -2 ▁4 ▁ABC -2 B ▁6 ▁PE PE ▁12 ▁< s > ▁USER ▁VALUE ▁X O X O ▁21 ▁ABC -1 ▁3 ▁ABC -2 ▁5 ▁PE PE ▁12 ▁< s > ▁DataFrame ▁first ▁combine ▁first ▁name ▁values ▁values ▁where
▁Cal culating ▁the ▁duration ▁an ▁event ▁in ▁a ▁time ▁series ▁python ▁< s > ▁I ▁have ▁a ▁dataframe ▁as ▁show ▁below : ▁The ▁index ▁is ▁datetime ▁and ▁have ▁column ▁record ▁the ▁ra inf all ▁value ( unit : mm ) ▁in ▁each ▁hour , I ▁would ▁like ▁to ▁calculate ▁the ▁" Average ▁w et ▁spell ▁duration ", ▁which ▁means ▁the ▁average ▁of ▁continuous ▁hours ▁that ▁exist ▁values ▁( not ▁zero ) ▁in ▁a ▁day , ▁so ▁the ▁calculation ▁is ▁and ▁the ▁" average ▁w et ▁spell ▁amount ", ▁which ▁means ▁the ▁average ▁of ▁sum ▁of ▁the ▁values ▁in ▁continuous ▁hours ▁in ▁a ▁day . ▁The ▁data f ame ▁above ▁is ▁just ▁a ▁example , ▁the ▁dataframe ▁which ▁I ▁have ▁have ▁more ▁longer ▁time ▁series ▁( more ▁than ▁one ▁year ▁for ▁example ), ▁how ▁can ▁I ▁write ▁a ▁function ▁so ▁it ▁could ▁calculate ▁the ▁two ▁value ▁mentioned ▁above ▁in ▁a ▁better ▁way ? ▁thanks ▁in ▁advance ! ▁P . S . ▁the ▁values ▁may ▁be ▁NaN , ▁and ▁I ▁would ▁like ▁to ▁just ▁ignore ▁it . ▁< s > ▁2 ▁+ ▁4 ▁+ ▁1 ▁+ ▁1 ▁+ ▁2 ▁+ ▁5 ▁/ ▁6 ▁( events ) ▁= ▁2.5 ▁( hr ) ▁< s > ▁{ ▁( 14 .5 ▁+ ▁15. 8) ▁+ ▁( ▁1 3.6 ▁+ ▁4. 3 ▁+ ▁13. 7 ▁+ ▁14. 4 ▁) ▁+ ▁( 17 . 2) ▁+ ▁( 5. 3) ▁+ ▁(2 ▁+ ▁4) + ▁( 3.9 ▁+ ▁7. 2 ▁+ ▁1 ▁+ ▁1 ▁+ ▁10) ▁} ▁/ ▁6 ▁( events ) ▁= ▁2 1. 32 ▁( mm ) ▁< s > ▁time ▁index ▁value ▁hour ▁values ▁day ▁sum ▁values ▁day ▁time ▁year ▁value ▁values
