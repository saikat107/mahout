{"id": 441, "q": "Change value of datetime in pandas Dataframe", "d": "i want to change the value of a pandas dataframe column with datetime format. Basically i want to add always 20 seconds to a row. Im new to python/pandas so i dont know any ways to solve that issue. Here is my code so far: Dataframe df_date: original: ... expected: ... Any help on this would be appreciated!", "q_apis": "value value add seconds any", "io": "0 2017-03-10 01:00:00 1 2017-03-10 01:00:00 2 2017-03-10 01:00:00 3 2017-03-10 01:00:00 4 2017-03-10 01:00:00 <s> 0 2017-03-10 01:00:20 1 2017-03-10 01:00:40 2 2017-03-10 01:01:00 3 2017-03-10 01:00:20 4 2017-03-10 01:00:40 ", "apis": "to_timedelta timedelta_range freq", "code": ["td = pd.to_timedelta(np.arange(1, len(dataframe) + 1) * 20, unit='s')\n", "td = pd.timedelta_range(0, periods=len(dataframe), freq='20s') \n"], "link": "https://stackoverflow.com/questions/57428633/change-value-of-datetime-in-pandas-dataframe"}
{"id": 216, "q": "Pandas fill in missing data from columns in other rows", "d": "I have a df like below: Output: For ac are null, get prev's value, and then look up at the id column. Fill in the null with value at ac column. Expected output: How do I achieve this? Thanks.", "q_apis": "columns get value at value at", "io": " id ac prev 0 a 123 NaN 1 b 223 NaN 2 c NaN a 3 d NaN b <s> id ac prev 0 a 123 NaN 1 b 223 NaN 2 c 123 a 3 d 223 b ", "apis": "isna loc loc map set_index", "code": ["m = df['ac'].isna()\ndf.loc[m, 'ac'] = df.loc[m, 'prev'].map(df.set_index('id')['ac'])\n"], "link": "https://stackoverflow.com/questions/63779231/pandas-fill-in-missing-data-from-columns-in-other-rows"}
{"id": 527, "q": "Add column to DataFrame in a loop", "d": "Let's say I have a very simple pandas dataframe, containing a single indexed column with \"initial values\". I want to read in a loop N other dataframes to fill a single \"comparison\" column, with matching indices. For instance, with my inital dataframe as and the following two dataframes to read in a loop I would like to produce the following result Using , or , I only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with . What's the most pandas-pythonic way of achieving this? Below an example from the proposed duplicate solution: Second edit: @W-B, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. It also requires turning off warnings, which might be dangerous...", "q_apis": "DataFrame values indices", "io": " Initial 0 a 1 b 2 c 3 d <s> Initial Comparison 0 a e 1 b f 2 c g 3 d h ", "apis": "DataFrame array columns DataFrame array columns DataFrame array columns index", "code": ["import functools\ndf1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])\ndf1['Compare']=np.nan\ndf2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])\ndf3 = pd.DataFrame(np.array(['g','h','i']), columns=['Compare'],index=[2,3,4])\n"], "link": "https://stackoverflow.com/questions/54407275/add-column-to-dataframe-in-a-loop"}
{"id": 69, "q": "python pandas give comma separated values into columns with &quot;title&quot;", "d": "I have some comma-separated data in the same column and I wish to separate each value into different columns. I have done separation using below and the output I got is but I need something like below (has to give some titles for each column) How would I do that?", "q_apis": "values columns value columns", "io": "0 13.4119837, 42.082885, 13.4119837, 42.082885 1 11.6285463, 42.4193742, 11.6285463, 42.4193742 2 -3.606772, 39.460299, -3.606772, 39.460299 3 -0.515639, 38.988847, -0.515639, 38.988847 4 -2.403309, 37.241792, -2.403309, 37.241792 <s> 0 1 2 3 0 13.4119837 42.082885 13.4119837 42.082885 1 11.6285463 42.4193742 11.6285463 42.4193742 2 -3.606772 39.460299 -3.606772 39.460299 3 -0.515639 38.988847 -0.515639 38.988847 4 -2.403309 37.241792 -2.403309 37.241792 ", "apis": "columns", "code": ["data = data['column_name'].str.split(\",\", n = 3, expand = True)\ndata.columns = ['minLat', 'maxLat', 'minLong', 'maxLong']\n"], "link": "https://stackoverflow.com/questions/66782284/python-pandas-give-comma-separated-values-into-columns-with-title"}
{"id": 324, "q": "Pandas select only the first 3 YYYYMM per group", "d": "CGood afternoon, I have a datrame like the one below I need to get only the first three *CONSECUTIVE MMMMYY per USR. Im able to get the first 3 records using head(3) but of course it dont bring back what I need, neither using it gets the consecutive when ['check'] is true but in some cases I may need to get 200001 and 200003 only and they are not consecutive between them. Any guidance will be appreciated Thanks", "q_apis": "select first get first get first head get between", "io": "+---+---+--------+ | |USR| MMMMYY | +---+---+--------+ | 1 | A | 200002 | +---+---+--------+ | 2 | A | 200003 | +---+---+--------+ | 3 | A | 200004 | +---+---+--------+ | 4 | A | 200005 | +---+---+--------+ | 5 | B | 200001 | +---+---+--------+ | 6 | B | 200003 | +---+---+--------+ | 7 | B | 200008 | +---+---+--------+ | 8 | B | 200009 | +---+---+--------+ <s> +---+---+--------+ | |USR| MMMMYY | +---+---+--------+ | 1 | A | 200002 | +---+---+--------+ | 2 | A | 200003 | +---+---+--------+ | 3 | A | 200004 | +---+---+--------+ | 5 | B | 200001 | +---+---+--------+ | 6 | B | 200003 | +---+---+--------+ ", "apis": "to_datetime groupby transform min", "code": ["df['MMMMYY'] = pd.to_datetime(df.MMMMYY, format='%Y%m')\n\ns = df.groupby('USR')['MMMMYY'].transform('min') + pd.offsets.MonthOffset(3)\n\ndf[df.MMMMYY<s]\n"], "link": "https://stackoverflow.com/questions/60959528/pandas-select-only-the-first-3-yyyymm-per-group"}
{"id": 233, "q": "Pandas Dataframe split into seperate csv by column value", "d": "Hello i have dataset containing 4 columns i want to split my dataframe into separate csv files by their \"s\" column but I couldn't figure out a proper way of doing it. \"s\" column has arbitrary numbers of labels. We don't know how many 1's or 2's dataset has. it is until 30 but not every number is contained in this dataset. My desired output is: after I get this split I can easily write it to separate csv files. The problem I am having is that I don't know how many different \"s\" values I have and how much from each of them. Thank you", "q_apis": "value columns get values", "io": "x y z s 1 42.8 157.5 1 1 43.8 13.5 1 1 44.8 152 2 . . . 4 7528 157.5 2 4 45.8 13.5 3 8 72.8 152 3 <s> df1 x y z s 1 42.8 157.5 1 . 1 43.8 13.5 1 df2 1 44.8 152 2 . 4 7528 157.5 2 df3 4 45.8 13.5 3 . 8 72.8 152 3 ", "apis": "groupby to_csv index", "code": ["for i, x in df.groupby('s'): x.to_csv(f'df{i}.csv', index=False)\n"], "link": "https://stackoverflow.com/questions/63353050/pandas-dataframe-split-into-seperate-csv-by-column-value"}
{"id": 265, "q": "Turn column levels inside-out", "d": "I have a pandas DataFrame which looks like this (code to create it is at the bottom of the question): I would like to turn the and columns \"inside out\", i.e. my expected output is: Is there an efficient (i.e. that does not involve writing a python loop that goes through each row one-by-one) way to do this in pandas? Code to generate the starting DataFrame: Code to generate expected output:", "q_apis": "levels DataFrame at columns DataFrame", "io": " col_1 col_2 foo_1 foo_2 col_3 col_4 col_3 col_4 0 1 4 2 8 5 7 1 3 1 6 3 8 9 <s> col_1 col_2 col_3 col_4 0 1 4 {'foo_1': 2, 'foo_2': 5} {'foo_1': 8, 'foo_2': 7} 1 3 1 {'foo_1': 6, 'foo_2': 8} {'foo_1': 3, 'foo_2': 9} ", "apis": "filter droplevel agg filter droplevel agg drop droplevel", "code": ["df['col_3'] = df.filter(like='col_3').droplevel(1, 1).agg(dict, axis=1)\ndf['col_4'] = df.filter(like='col_4').droplevel(1, 1).agg(dict, axis=1)\n\ndf = df.drop(['foo_1', 'foo_2'], 1).droplevel(1, 1)\n"], "link": "https://stackoverflow.com/questions/62818462/turn-column-levels-inside-out"}
{"id": 598, "q": "Pandas DataFrame filter column A depending on if column B contains x for group of values in A", "d": "I would like to filter the below DataFrame on column , based on if for the value in , column contains the value . Here, values 1, 3, and 4 contain at least one row with value in column , while 2 and 5 do not. I am trying to filter out any rows with 2 and 5 so that the final output is: How could I do this (preferably in one step)?", "q_apis": "DataFrame filter contains values filter DataFrame value contains value values at value filter any step", "io": "In [32]: df Out[32]: ref type 0 1 P 1 1 C 2 1 A 3 2 C 4 3 P 5 3 P 6 4 P 7 4 A 8 5 C 9 5 A <s> In [34]: df Out[34]: ref type 0 1 P 1 1 C 2 1 A 4 3 P 5 3 P 6 4 P 7 4 A ", "apis": "groupby filter values", "code": ["df.groupby('ref').filter(lambda x : ('P' in x['type'].values))\n"], "link": "https://stackoverflow.com/questions/51850165/pandas-dataframe-filter-column-a-depending-on-if-column-b-contains-x-for-group-o"}
{"id": 101, "q": "Pandas: Clean up String column containing Single Quotes and Brackets using Regex?", "d": "I want to clean the following Pandas dataframe column, but in a single and efficient statement than the way I am trying to achieve it in the code below. Input: Output: Code:", "q_apis": "", "io": " string 0 ['string', '#string'] 1 ['#string'] 2 [] <s> string 0 string, #string 1 #string 2 NaN ", "apis": "astype replace", "code": ["df['string'] = df['string'].astype(str).str.replace(r\"^[][\\s]*$|(^\\[+|\\]+$|')\", lambda m: '' if m.group(1) else np.nan)\n"], "link": "https://stackoverflow.com/questions/66126525/pandas-clean-up-string-column-containing-single-quotes-and-brackets-using-regex"}
{"id": 557, "q": "Pandas corr() returning NaN too often", "d": "I'm attempting to run what I think should be a simple correlation function on a dataframe but it is returning NaN in places where I don't believe it should. Code: Subject DataFrame: corr() Result: According to the (limited) documentation on the function, it should exclude \"NA/null values\". Since there are overlapping values for each column, should the result not all be non-NaN? There are good discussions here and here, but neither answered my question. I've tried the idea discussed here, but that failed as well. @hellpanderr's comment brought up a good point, I'm using 0.22.0 Bonus question - I'm no mathematician, but how is there a 1:1 correlation between B and C in this result?", "q_apis": "corr where DataFrame corr values values all between", "io": " A B C 0 NaN 500.0 NaN 1 99.0 100.0 100.0 2 NaN 100.0 NaN 3 100.0 NaN 100.0 4 100.0 NaN NaN 5 100.0 100.0 NaN 6 NaN 500.0 300.0 7 NaN 500.0 NaN 8 NaN 100.0 NaN <s> A B C A 1.0 NaN NaN B NaN 1.0 1.0 C NaN 1.0 1.0 ", "apis": "dropna std", "code": ["df[['A', 'C']].dropna().std()\n\nA    0.707107\nC    0.000000\n"], "link": "https://stackoverflow.com/questions/52466844/pandas-corr-returning-nan-too-often"}
{"id": 576, "q": "How to take first one or two digits from float number?", "d": "I have a Pandas dataframe. In a series, I have time in hour and minute represented as , as below. I want only the hours: Example of time column values from (1 to 12) : Example of time column values from (13 to 24) : I have tried this code but it takes very long time until I close the editor so I didn't see the result", "q_apis": "take first time hour minute time values time values time", "io": "1000.0 -> 10 901.0 -> 9 <s> 1850.0 -> 18 2301.0 -> 23 ", "apis": "hour astype", "code": ["df['hour'] = (df['dep_time'] // 100).astype(int)\n"], "link": "https://stackoverflow.com/questions/52745166/how-to-take-first-one-or-two-digits-from-float-number"}
{"id": 358, "q": "Apply qcut for complete dataframe", "d": "I want to replace the column values with bin numbers based on quantiles but for each and every column present in the dataframe. I know how to do this with qcut and labels as its parameter for a single column, but do not know whether it can be applied for complete dataframe or not. say the dataframe looks like below.. The ID column should remain unchanged but the other columns should be replaced by bin numbers, like below.. the bin numbers I have provided here for CC, DD, EE are not exact and for understanding purpose only. And in the real dataset, there are more than 100 columns and 1000 rows, and I do not want to replace the 'ID' column, but all the other columns. How to do this?", "q_apis": "qcut replace values qcut columns columns replace all columns", "io": " ID CC DD EE 0 Q1 0 23 18 1 Q2 2 32 19 2 Q3 3 45 20 3 Q4 4 54 21 4 Q5 5 67 22 5 Q6 6 76 23 <s> ID CC DD EE 0 Q1 1 1 1 1 Q2 2 2 1 2 Q3 3 2 2 3 Q4 4 3 3 4 Q5 5 4 4 5 Q6 5 5 5 ", "apis": "cut", "code": ["import pandas as pd\n\ndf['CC'] = pd.cut(df['CC'], [0, 5, 10,20])\n"], "link": "https://stackoverflow.com/questions/59915944/apply-qcut-for-complete-dataframe"}
{"id": 352, "q": "Pandas.DataFrame: efficient way to add a column &quot;seconds since last event&quot;", "d": "I have a Pandas.DataFrame with a standard index representing seconds, and I want to add a column \"seconds elapsed since last event\" where the events are given in a list. Specifically, say and Then I want to obtain I tried so apparently to make it work I need to write . More importantly, when I then do I obtain That is: the previous has been overwritten. Is there a way to assign new values without overwriting existing values by nan ? Then, I can do something like Or is there a more efficient way ? For the record, here is my naive code. It works, but looks inefficient and of poor design:", "q_apis": "DataFrame add seconds last DataFrame index seconds add seconds last where assign values values", "io": "| | 0 | x | |---:|----:|-----:| | 0 | 0 | <NA> | | 1 | 0 | <NA> | | 2 | 0 | 0 | | 3 | 0 | 1 | | 4 | 0 | 2 | | 5 | 0 | 0 | | 6 | 0 | 1 | <s> | | 0 | x | |---:|----:|----:| | 0 | 0 | nan | | 1 | 0 | nan | | 2 | 0 | nan | | 3 | 0 | nan | | 4 | 0 | nan | | 5 | 0 | 0 | | 6 | 0 | 1 | ", "apis": "index isin cumsum loc reindex index isna cumsum where groupby cumcount", "code": ["s = df.index.isin(event).cumsum()\n# or equivalently\n# s = df.loc[event, 0].reindex(df.index).isna().cumsum()\n\ndf['x'] = np.where(s>0,df.groupby(s).cumcount(), np.nan)\n"], "link": "https://stackoverflow.com/questions/60157250/pandas-dataframe-efficient-way-to-add-a-column-seconds-since-last-event"}
{"id": 431, "q": "Print dataframe without float precision", "d": "I would like to print a dataframe in my console without displaying any float decimal precision. The output I got after printing is: Whereas what I expected is: There seems to be an issue to display the tuple without float precision. Any idea why ? Cheers", "q_apis": "any", "io": "0 (a, 2.01212121212) 1 1.12 2 8.12 <s> 0 (a, 2.01) 1 1.12 2 8.12 ", "apis": "DataFrame append round append append append round", "code": ["pd_tmp = pd.DataFrame()\npd_tmp[\"new_column\"] = [(\"a\", 2.01212121212), 1.123123, 8.1212]\n\n\ndef foo(pd_col):\n    pd_new_col = []\n    for i in pd_col:\n        if not isinstance(i, float):\n            lst = []\n            for j in i:\n                if not isinstance(j, float):\n                    k = j\n                    lst.append(k)\n                else:\n                    k = round(j, 2)\n                    lst.append(k)\n            pd_new_col.append(tuple(lst))\n\n        else:\n            pd_new_col.append(round(i, 2))\n\n   return pd_new_col\n\n\npd_tmp[\"new_column\"] = foo(pd_tmp[\"new_column\"])\nprint(pd_tmp)\n"], "link": "https://stackoverflow.com/questions/57864979/print-dataframe-without-float-precision"}
{"id": 25, "q": "move column above and delete rows in pandas python dataframe", "d": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be", "q_apis": "delete sample DataFrame start", "io": "A B C D E F G H a.1 b.1 c.1 d.1 c.2 d.2 e.1 f.1 g.1 h.1 <s> A B C D E F G H a.1 b.1 c.1 d.1 e.1 f.1 g.1 h.1 c.2 d.2 ", "apis": "apply shift first_valid_index dropna all fillna index index reset_index drop apply shift first_valid_index dropna all fillna index index name name columns get_indexer name shift shift first_valid_index apply", "code": ["out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n", "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n", "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"], "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe"}
{"id": 610, "q": "pandas apply and applymap functions are taking long time to run on large dataset", "d": "I have two functions applied on a dataframe {{Update}} Dataframe has got almost 700 000 rows. This is taking much time to run. How to reduce the running time? Sample data : output: This line of code takes items from a list and fill one by one to each column as shown above. There will be almost 38 columns.", "q_apis": "apply applymap time time time items columns", "io": " A ---------- 0 [1,4,3,c] 1 [t,g,h,j] 2 [d,g,e,w] 3 [f,i,j,h] 4 [m,z,s,e] 5 [q,f,d,s] <s> A B C D E ------------------------- 0 [1,4,3,c] 1 4 3 c 1 [t,g,h,j] t g h j 2 [d,g,e,w] d g e w 3 [f,i,j,h] f i j h 4 [m,z,s,e] m z s e 5 [q,f,d,s] q f d s ", "apis": "select_dtypes columns apply", "code": ["cols = res.select_dtypes(object).columns\nres[cols] = res[cols].apply(lambda x: x.str.strip('\"'))\n"], "link": "https://stackoverflow.com/questions/51279903/pandas-apply-and-applymap-functions-are-taking-long-time-to-run-on-large-dataset"}
{"id": 186, "q": "Drop rows and sort one dataframe according to another", "d": "I have two pandas data frames ( and ): My goal is to append the corresponding from to each in . However, the relationship is not one-to-one (this is my client's fault and there's nothing I can do about this). To solve this problem, I want to sort by such that is identical to . So basically, for any row in 0 to : if then keep row in . if then drop row from and repeat. The desired result is: This way, I can use to assign to . Is there a standardized way to do this? Does have a method for doing this? Before this gets voted as a duplicate, please realize that , so threads like this are not quite what I'm looking for.", "q_apis": "append identical any drop repeat assign", "io": "# df1 ID COL 1 A 2 F 2 A 3 A 3 S 3 D 4 D # df2 ID VAL 1 1 2 0 3 0 3 1 4 0 <s> ID COL 1 A 2 F 3 A 3 S 4 D ", "apis": "assign groupby cumcount merge assign groupby cumcount columns", "code": ["(df1.assign(idx=df1.groupby('ID').cumcount())\n    .merge(df2.assign(idx=df2.groupby('ID').cumcount()),\n           on=['ID','idx'],\n           suffixes=['','_drop'])\n    [df1.columns]\n)\n"], "link": "https://stackoverflow.com/questions/62778713/drop-rows-and-sort-one-dataframe-according-to-another"}
{"id": 239, "q": "Pandas merge and keep only non-matching records", "d": "How can I merge/join these two dataframes ONLY on \"id\". Produce 3 new dataframes: 1)R1 = Merged records 2)R2 = (DF1 - Merged records) 3)R3 = (DF2 - Merged records) Using pandas in Python. First dataframe (DF1) Second dataframe (DF2) My solution for I am unsure that is the easiest way to get R2 and R3 R2 should look like R3 should look like:", "q_apis": "merge merge join get", "io": "| id | salary | |-----------|--------| | 1 | 20 | | 2 | 30 | | 3 | 40 | | 4 | 50 | | 6 | 33 | | 7 | 23 | | 8 | 24 | | 9 | 28 | <s> | id | salary | |-----------|--------| | 6 | 33 | | 7 | 23 | | 8 | 24 | | 9 | 28 | ", "apis": "merge groupby", "code": ["total_merge = df1.merge(df2, on='id', how='outer', indicator=True)\n\nR1 = total_merge[total_merge['_merge']=='both']\nR2 = total_merge[total_merge['_merge']=='left_only']\nR3 = total_merge[total_merge['_merge']=='right_only']\n", "dfs = {k:v for k,v in total_merge.groupby('_merge')}\n"], "link": "https://stackoverflow.com/questions/63184392/pandas-merge-and-keep-only-non-matching-records"}
{"id": 72, "q": "How to convert data frame column list value to element", "d": "Hi I have a dataframe like this: I want to change it into: How can I do that? I tries with", "q_apis": "value", "io": " A 0 [] 1 [1234] 2 [] <s> A 0 0 1 1234 2 0 ", "apis": "where", "code": ["df['A'] = np.where(df['A'].str.len() == 0, 0, df['A'].str[0]) \n"], "link": "https://stackoverflow.com/questions/59225169/how-to-convert-data-frame-column-list-value-to-element"}
{"id": 614, "q": "Create new column depending on values from other column", "d": "I have a DataFrame that looks something like this: df I want to give indexes the values that are between and in column and create a new columns as :", "q_apis": "values DataFrame values between columns", "io": " A B C 0 vt 40462 5 6 1 5 6 6 2 5 5 8 3 4 3 1 4 vl 6450 5 6 5 5 6 7 6 1 2 3 7 vt 40462 5 6 8 5 5 8 9 vl 658 6 7 10 5 5 8 11 4 3 1 12 vt 40461 5 6 13 5 5 8 14 7 8 5 <s> A B C D 0 vt 40462 5 6 vt 40462 1 5 6 6 vt 40462 2 5 5 8 vt 40462 3 4 3 1 vt 40462 4 vl 6450 5 6 vl 6450 5 5 6 7 vl 6450 6 1 2 3 vl 6450 7 vt 40462 5 6 vt 40462 8 5 5 8 vt 40462 9 vl 658 6 7 vl 658 10 5 5 8 vl 658 11 4 3 1 vl 658 12 vt 40461 5 6 vt 40461 13 5 5 8 vt 40461 14 7 8 5 vt 40461 ", "apis": "loc astype contains ffill", "code": ["df.loc[df.A.astype(str).str.contains('^[A-Za-z]'), 'D'] = df.A\n\ndf.ffill()\n"], "link": "https://stackoverflow.com/questions/51143187/create-new-column-depending-on-values-from-other-column"}
{"id": 226, "q": "Applying a function to chunks of the Dataframe", "d": "I have a (for instance - simplified version) and generated 20 bootstrap resamples that are all now in the same df but differ in the Resample Nr. Now I want to apply a certain function on each Reample Nr. Say: The outlook would look like this: So there are 20 different new values. I know there is a df.iloc command where I can specify my row selection but I would like to find a command where I don't have to repeat the code for the 20 samples. My goal is to find a command that identifies the Resample Nr. automatically and then calculates the function for each Resample Nr. How can I do this? Thank you!", "q_apis": "all now apply values iloc where where repeat", "io": " A B 0 2.0 3.0 1 3.0 4.0 <s> A B 0 1 0 2.0 3.0 1 1 1 3.0 4.0 2 2 1 3.0 4.0 3 2 1 3.0 4.0 .. .. .. .. 39 20 0 2.0 3.0 40 20 0 2.0 3.0 ", "apis": "assign mul groupby transform sum div", "code": ["s = df.assign(x=df['A'].mul(df['B']), y=df['B']**2)\\\n      .groupby(level=1)[['x', 'y']].transform('sum')\ndf['C'] = s['x'].div(s['y'])\n"], "link": "https://stackoverflow.com/questions/63598638/applying-a-function-to-chunks-of-the-dataframe"}
{"id": 149, "q": "Find the latest occurrence of an class item and store how many values are between these two in a pandas DataFrame", "d": "I have a pandas DataFrame with some labels for classes. Now I want to add a column and store how many items are between two elements of the same class. and I want to get this: This is the code I used: This seems circuitous to me. Is there a more elegant way of producing this output?", "q_apis": "item values between DataFrame DataFrame add items between get", "io": " Class 0 0 1 1 2 1 3 1 4 0 <s> Class Shift 0 0 NaN 1 1 NaN 2 1 1.0 3 1 1.0 4 0 4.0 ", "apis": "shift shift groupby shift diff", "code": ["df['shift'] = np.arange(len(df))\ndf['shift'] = df.groupby('Class')['shift'].diff()\nprint(df)\n"], "link": "https://stackoverflow.com/questions/65087280/find-the-latest-occurrence-of-an-class-item-and-store-how-many-values-are-betwee"}
{"id": 471, "q": "How to create a json from pandas data frame where columns are the key", "d": "I have a data frame df The json I am looking for is: I have tries df.to_json but its not working This is not the output i was looking for How to do it in most effective way using pandas/python ?", "q_apis": "where columns to_json", "io": "df: col1 col2 col3 1 2 3 4 5 6 7 8 9 <s> { \"col1\": 1, \"col1\": 4, \"col1\": 7, }, { \"col2\": 2, \"col2\": 5, \"col2\": 8 }, { \"col3\": 3, \"col3\": 6, \"col3\": 9, } ", "apis": "DataFrame columns to_json columns join join index columns", "code": ["import json\nfrom io import StringIO\n\ndf = pd.DataFrame(np.arange(1,10).reshape((3,3)), columns=['col1','col2','col3'])\nio = StringIO()\ndf.to_json(io, orient='columns')\nparsed = json.loads(io.getvalue())\nwith open(\"pretty.json\", '+w') as of:\n    json.dump(parsed, of, indent=4)\n", "with open(\"exact.json\", \"w+\") as of:\n    of.write('[\\n\\t{\\n' + '\\t},\\n\\t{\\n'.join([\"\".join([\"\\t\\t\\\"%s\\\": %s,\\n\"%(c, df[c][i]) for i in df.index]) for c in df.columns])+'\\t}\\n]')\n"], "link": "https://stackoverflow.com/questions/56592574/how-to-create-a-json-from-pandas-data-frame-where-columns-are-the-key"}
{"id": 494, "q": "Choose a value from a set of columns based on value and create new column with the value?", "d": "so if I have a pandas Dataframe like: and want to insert row 'E' by choosing from columns 'A', 'B', or 'C' based on conditions in column 'D', how would I go about doing this? For example: if D == a, choose 'A', else choose 'B', outputting: Thanks in advance!", "q_apis": "value columns value value insert columns", "io": " A B C D 0 1 2 3 a 1 2 4 6 a 2 4 8 8 b 3 2 3 5 c <s> A B C D E 0 1 2 3 a 1 1 2 4 6 a 2 2 4 8 8 b 8 3 2 3 5 c 3 ", "apis": "lookup index array dtype lookup index", "code": ["df.lookup(df.index,df.D.str.upper())\nOut[749]: array([1, 2, 8, 5], dtype=int64)\n\ndf['E']=df.lookup(df.index,df.D.str.upper())\n"], "link": "https://stackoverflow.com/questions/55756126/choose-a-value-from-a-set-of-columns-based-on-value-and-create-new-column-with-t"}
{"id": 525, "q": "Matching dictionaries with columns and indices in DataFrame | python", "d": "I have a DataFrame with column names as on example and the indices from 0 to 1000. The dataframe is filled with zeros. Then, I have dictionary, e.g.: Dictionary name edited in order not to confuse anyone later. My goal is to: for every dict key match it with the column for every number in the list as dictionary value to match with the index if there is a match of index and column, then change the cell to 1 else: leave zero How would you do that?", "q_apis": "columns indices DataFrame DataFrame names indices name value index index", "io": "House 1 | House 2 | House 5 | House 8 | ... 0 1 2 3 4... <s> dict_of_houses = {'House 1':[100,201,306,387,500,900],'House 2':[31,87,254,675,987],'House 5':[23,45,67,123,345,654,789,808,864,987,999],'House 8':[23,675,786,858,868,912,934]} ", "apis": "indices items loc indices", "code": ["for house, indices in dict_.items():\n    df.loc[indices, house] = 1\n"], "link": "https://stackoverflow.com/questions/54460092/matching-dictionaries-with-columns-and-indices-in-dataframe-python"}
{"id": 327, "q": "Pandas groupby and change/reassign one element", "d": "I want to given dataframe, and then, for each group, for given column overwrite the value of its last element (of each group) to (with being the sum of all the elements apart from the last one). Note that after performing the operation, the sum of all values in for each group is equal to . For example, for this input dataframe (grouping by and ): the expected output would be: I managed to perform the operation using loop: but I am looking for more elegant way of doing this, without explicitly looping through each group. I tried this: But I don't really know how to assemble those outputs given that their indices differ.", "q_apis": "groupby value last sum all last sum all values indices", "io": " c1 c2 p 0 x a 0.4 1 y a 0.2 2 x a 0.3 3 y b 0.6 <s> c1 c2 p 0 x a 0.4 1 y a 1.0 2 x a 0.6 3 y b 1.0 ", "apis": "groupby apply assign iloc sum reset_index drop iloc sum iloc groupby apply", "code": ["df.groupby(['c1', 'c2']).apply(\n        lambda x: x.assign(p=x['p'][:-1].tolist()+[1 - x.iloc[:-1].sum()['p']])\n).reset_index(level=[0,1], drop=True)\n", "def func(row):\n    result = 1 - row.iloc[:-1].sum()['p']\n    row['p'].iloc[-1] = result\n    return row\n\ndf.groupby(['c1', 'c2']).apply(func)\n"], "link": "https://stackoverflow.com/questions/60875750/pandas-groupby-and-change-reassign-one-element"}
{"id": 337, "q": "Pandas dataframe compare columns with one value and get this row and previous row into another dataframe", "d": "I have a dataframe like this: I want to create another dataframe with and also store its previous row. It should look like this: What is the best way. Thanks in advance.", "q_apis": "compare columns value get", "io": ">>> df A B 0 1 56 1 2 75 2 3 102 3 4 15 4 5 19 5 6 116 <s> >>> df1 A B 1 2 75 2 3 102 4 5 19 5 6 116 ", "apis": "gt shift gt shift", "code": ["df1 = df[df.B.gt(100) | df.B.shift(-1).gt(100)]\n", "df1 = df[(df.B>100) | (df.B.shift(-1)>100)]\n"], "link": "https://stackoverflow.com/questions/60486231/pandas-dataframe-compare-columns-with-one-value-and-get-this-row-and-previous-ro"}
{"id": 657, "q": "pandas sum the differences between two columns in each group", "d": "I have a looks like, the of and are , and are of ; I like to and and get the differences between and , but I don't know how to sum such differences in each group and assign the values to a new column say , possibly in a new ,", "q_apis": "sum between columns get between sum assign values", "io": "A B C D 2017-10-01 2017-10-11 M 2017-10 2017-10-02 2017-10-03 M 2017-10 2017-11-01 2017-11-04 B 2017-11 2017-11-08 2017-11-09 B 2017-11 2018-01-01 2018-01-03 A 2018-01 <s> C D E M 2017-10 11 M 2017-10 11 B 2017-11 4 B 2017-11 4 A 2018-01 2 ", "apis": "merge groupby apply sum reset_index days days days days days", "code": ["df.merge(df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A']).sum(level=[0,1]).reset_index())\nOut[292]: \n           A          B  C        D       0\n0 2017-10-01 2017-10-11  M  2017-10 11 days\n1 2017-10-02 2017-10-03  M  2017-10 11 days\n2 2017-11-01 2017-11-04  B  2017-11  4 days\n3 2017-11-08 2017-11-09  B  2017-11  4 days\n4 2018-01-01 2018-01-03  A  2018-01  2 days\n"], "link": "https://stackoverflow.com/questions/49324988/pandas-sum-the-differences-between-two-columns-in-each-group"}
{"id": 156, "q": "Choose the next number from columns in Pandas", "d": "I want to make a new column that shows the next biggest value after within the columns. Following is my intended result: I have been researching it a little but no luck. Some help will be appreciated, Thanks!", "q_apis": "columns value columns", "io": "ID Op Cl V C R0 R1 R2 R3 R4 R5 UN 22.85 22.86 8830500 0.21 25 34 12 87 105 102 SS 55.01 52.67 6500 5.45 84 122 147 124 644 788 PN 90.00 90.99 1000 102 89 55 100 156 44 87 PI 184.99 182.38 15000 84 56 77 97 45 44 33 <s> ID Op Cl V C R0 R1 R2 R3 R4 R5 X UN 22.85 22.86 8830500 0.21 25 34 12 87 105 102 25 SS 55.01 52.67 6500 5.45 84 122 147 124 644 788 84 PN 90.00 90.99 1000 102 89 55 100 156 44 87 100 PI 184.99 182.38 15000 84 56 77 97 45 44 33 NaN ", "apis": "columns filter where columns mask gt values idxmax lookup where mask any lookup index mask idxmax filter values now mask values assign where mask any shape mask argmax", "code": ["# extract the `R` columns\ns = df.filter(like='R')\n\n# find out where these columns are larger than `Cl`:\nmask = s.gt(df['Cl'], axis='rows')\n\n# extract the values with `idxmax` and `lookup`:\ndf['X'] = np.where(mask.any(1), s.lookup(s.index,mask.idxmax(1)), np.nan)\n", "# extract and sort by rows\ns = np.sort(df.filter(like='R').values, axis=1)\n\n# now we work with numpy data:\nmask = s > df['Cl'].values[:,None]\n\n# check and assign\ndf['X'] = np.where(mask.any(1), s[np.arange(s.shape[0]),mask.argmax(1)], np.nan)\n"], "link": "https://stackoverflow.com/questions/64972959/choose-the-next-number-from-columns-in-pandas"}
{"id": 252, "q": "Pandas Conditional Rolling Sum of Two Columns", "d": "I have four columns in a data frame like so: And I want to conditionally sum by: D has a value, then D Else, take value from previous row for D plus C So for above, D would be . I've been able to do this successfully with a for loop But the for loop is obviously really expensive, anti-pattern and basically doesn't run over my whole data frame. I'm trying to copy an excel function here that has basically been written over a panel of data, and for the life of me I cannot figure out how to do this with: Simply calculating different column values I was attempting to do it with for a while, but I realized that I kept having to make a separate column for each row, and that's why I went with a for loop. Generalized to Groups Thanks to Scott Boston for the help!", "q_apis": "columns sum value take value copy values", "io": " A B C D 75472 d1 x -36.0 0.0 75555 d2 x -38.0 0.0 75638 d3 x -18.0 0.0 75721 d4 x -18.0 1836.0 75804 d5 x 1151.0 0.0 75887 d6 x 734.0 0.0 75970 d7 x -723.0 0.0 <s> [-36, -74, -92, 1836, 2987, 3721, 2998]", "apis": "cumsum mask combine_first groupby cumsum cumsum mask combine_first", "code": ["grp = (df['D'] != 0).cumsum()\ndf['D_new'] = df['D'].mask(df['D'] == 0).combine_first(df['C']).groupby(grp).cumsum()\ndf\n", "grp = (df['D'] != 0).cumsum()\n", "df['newCD'] = df['D'].mask(df['D'] == 0).combine_first(df['C'])\n"], "link": "https://stackoverflow.com/questions/62935300/pandas-conditional-rolling-sum-of-two-columns"}
{"id": 179, "q": "Subtract from every value in a DataFrame", "d": "I have a dataframe that looks like this: I want to subtract the movie scores from each movie so the output would look like this: The actual dataframe has thousands of movies and the movies are referenced by name so im trying to find a solution to comply with that. I should have also mention that the movies are not listed in order like [\"movie1\", \"movie2\", \"movie3\"], they are listed by their titles instead like [\"Star Wars\", \"Harry Potter\", \"Lord of the Rings\"]. The dataset could be changed so I wont know what the last movie in the list is.", "q_apis": "value DataFrame name last", "io": "userId movie1 movie2 movie3 movie4 score 0 4.1 2.1 1.0 NaN 2 1 3.1 1.1 3.4 1.4 1 2 2.8 NaN 1.7 NaN 3 3 NaN 5.0 NaN 2.3 4 4 NaN NaN NaN NaN 1 5 2.3 NaN 2.0 4.0 1 <s> userId movie1 movie2 movie3 movie4 score 0 2.1 0.1 -1.0 NaN 2 1 2.1 0.1 2.4 0.4 1 2 -0.2 NaN -2.3 NaN 3 3 NaN 1.0 NaN -1.7 4 4 NaN NaN NaN NaN 1 5 1.3 NaN 1.0 3.0 1 ", "apis": "columns columns isin values", "code": ["x = df.columns[~df.columns.isin(['userId', 'score'])]\ndf[x] = df[x] - df.score.values[:, None]\n"], "link": "https://stackoverflow.com/questions/64625342/subtract-from-every-value-in-a-dataframe"}
{"id": 372, "q": "Replace comma-separated values in a dataframe with values from another dataframe", "d": "this is my first question on StackOverflow, so please pardon if I am not clear enough. I usually find my answers here but this time I had no luck. Maybe I am being dense, but here we go. I have two pandas dataframes formatted as follows df1 df2 Basically, Ref_ID in df2 contains IDs that form the string contained in the field References in df1 What I would like to do is to replace values in the References field in df1 so it looks like this: So far, I had to deal with columns and IDs with a 1-1 relationship, and this works perfectly Pandas - Replacing Values by Looking Up in an Another Dataframe But I cannot get my mind around this slightly different problem. The only solution I could think of is to re-iterate a for and if cycles that compare every string of df1 to df2 and make the substitution. This would be, I am afraid, very slow as I have ca. 2000 unique Ref_IDs and I have to repeat this operation in several columns similar to the References one. Anyone is willing to point me in the right direction? Many thanks in advance.", "q_apis": "values values first time contains replace values columns get compare unique repeat columns right", "io": "+------------+-------------+ | References | Description | +------------+-------------+ | 1,2 | Descr 1 | | 3 | Descr 2 | | 2,3,5 | Descr 3 | +------------+-------------+ <s> +-------------------------------------+-------------+ | References | Description | +-------------------------------------+-------------+ | Smith (2006); Mike (2009) | Descr 1 | | John (2014) | Descr 2 | | Mike (2009);John (2014);Jill (2019) | Descr 3 | +-------------------------------------+-------------+ ", "apis": "DataFrame DataFrame explode map assign astype set_index groupby agg explode map assign astype set_index groupby agg join", "code": ["df1 = pd.DataFrame({'Reference':['1,2','3','1,3,5'], 'Description':['Descr 1', 'Descr 2', 'Descr 3']})\ndf2 = pd.DataFrame({'Ref_ID':[1,2,3,4,5,6], 'ShortRef':['Smith (2006)',\n                                                       'Mike (2009)',\n                                                       'John (2014)',\n                                                       'Cole (2007)',\n                                                       'Jill (2019)',\n                                                       'Tom (2007)']})\n\ndf1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(list))\n", "df1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(';'.join))\n"], "link": "https://stackoverflow.com/questions/59617019/replace-comma-separated-values-in-a-dataframe-with-values-from-another-dataframe"}
{"id": 245, "q": "How to find first occurrence of a significant difference in values of a pandas dataframe?", "d": "In a Pandas DataFrame, how would I find the first occurrence of a large difference between two values at two adjacent indices? As an example, if I have a DataFrame column A with data , I would want index holding 1.5, which would be 5. In my code below, it would give me the index holding 7.2, because . How should I fix this problem, so I get the index of the first 'large difference' occurrence?", "q_apis": "first difference values DataFrame first difference between values at indices DataFrame index index get index first difference", "io": "[1, 1.1, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1] <s> 15 - 7.2 > 7 - 1.5", "apis": "DataFrame diff abs index quantile index index mean values Series index index index eq squeeze argmax", "code": ["df = pd.DataFrame({'A': [1, 1.05, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]})\ndifferences = df['A'].diff(-1).abs()\nidx = differences.index[differences >= differences.quantile(.75)][0]\nprint(idx, differences[idx])\n", "idx = differences.index[differences >= 1.5][0]\n", "idx = differences.index[differences >= differences.mean()][0]\n", "from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2).fit(differences.values[:-1].reshape(-1, 1))\nclusters = pd.Series(kmeans.labels_, index=differences.index[:-1])\nidx = clusters.index[clusters.eq(np.squeeze(kmeans.cluster_centers_).argmax())][0]\n"], "link": "https://stackoverflow.com/questions/62989401/how-to-find-first-occurrence-of-a-significant-difference-in-values-of-a-pandas-d"}
{"id": 299, "q": "How to find and replace values of even-positioned elements in sequence", "d": "I have a list as follows: There are sequences of elements whose value's distances are equal. In this list, that distance is , for example, . How can I replace the value of the even-positioned elements for each of these sequences / sublists with a new value, say -1. The output will be:", "q_apis": "replace values value replace value value", "io": "list_1 = [12, 15, 18, 21, 6, 9, 7, 21, 38, 62, 65, 68, 81, 21, 25, 96, 101, 8, 11] <s> list_2 = [12, -1, 18, -1, 6, -1, 7, 21, 38, 62, -1, 68, 81, 21, 25, 96, 101, 8, -1] ", "apis": "Series loc diff index loc groupby diff ne cumsum cumcount", "code": ["s = pd.Series(list_1)\ns.loc[(s.diff() == 3) & (s.index % 2 == 1)] = -1\n", "s.loc[s.groupby(s.diff().ne(3).cumsum()).cumcount() % 2 == 1] = -1\n"], "link": "https://stackoverflow.com/questions/61615757/how-to-find-and-replace-values-of-even-positioned-elements-in-sequence"}
{"id": 632, "q": "Lengthening a DataFrame based on stacking columns within it in Pandas", "d": "I am looking for a function that achieves the following. It is best shown in an example. Consider: which looks like: I would like to collapase the and columns, lengthening the DataFame where necessary, so that the output is: That is, one row for each combination between either and , or and . I am looking for a function that does this relatively efficiently, as I have multiple s and many rows.", "q_apis": "DataFrame columns columns where between", "io": " x y1 y2 0 1 2 3 1 4 5 NaN <s> x y 0 1 2 1 1 3 2 4 5 ", "apis": "columns iloc values repeat values DataFrame iloc values repeat values DataFrame", "code": ["def gather_columns(df):\n    col_mask = [i.startswith('y') for i in df.columns]\n    ally_vals = df.iloc[:,col_mask].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n", "def gather_columns_v2(df):\n    ally_vals = df.iloc[:,1:].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n"], "link": "https://stackoverflow.com/questions/50481372/lengthening-a-dataframe-based-on-stacking-columns-within-it-in-pandas"}
{"id": 579, "q": "Convert a Dense Vector to a Dataframe using Pyspark", "d": "Firstly I tried everything in the link below to fix my error but none of them worked. How to convert RDD of dense vector into DataFrame in pyspark? I am trying to convert a dense vector into a dataframe (Spark preferably) along with column names and running into issues. My column in spark dataframe is a vector that was created using Vector Assembler and I now want to convert it back to a dataframe as I would like to create plots on some of the variables in the vector. Approach 1: Below is the Error Approach 2: Error: I also tried to convert the dataframe into a Pandas dataframe and after that I am not able to split the values into separate columns Approach 3: Above code runs fine but I still have only one column in my dataframe with all the values separated by commas as a list. Any help is greatly appreciated! EDIT: Here is how my temp dataframe looks like. It just has one column all_features. I am trying to create a dataframe that splits all of these values into separate columns (all_features is a vector that was created using 200 columns) Expected output is a dataframe with all 200 columns separated out in a dataframe Here is how my Pandas DF output looks like", "q_apis": "DataFrame names now values columns all values all values columns columns all columns", "io": "+----------------------------+ | col1| col2| col3|... +----------------------------+ |0.01193689934723|0.0|0.5049431301173817... |0.04774759738895|0.0|0.1657316216149636... |0.0|0.0|7.213126372469... |0.02387379869447|0.0|0.1866693496827619|... |1.89796699621085|0.0|0.3192169213385746|... +----------------------------+ only showing top 5 rows <s> 0 0 [0.011936899347238104, 0.0, 0.5049431301173817... 1 [0.047747597388952415, 0.0, 0.1657316216149636... 2 [0.0, 0.0, 0.19441761495525278, 7.213126372469... 3 [0.023873798694476207, 0.0, 0.1866693496827619... 4 [1.8979669962108585, 0.0, 0.3192169213385746, ... ", "apis": "map item select", "code": ["#column_names\ntemp = temp.rdd.map(lambda x:[float(y) for y in x['all_features']]).toDF(column_names)\n", "import pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\nsplits = [F.udf(lambda val: float(val[i].item()),FloatType()) for i in range(200)]\ntemp = temp.select(*[s(F.col('all_features')).alias(c) for c,s in zip(column_names,splits)])\ntemp.show()\n"], "link": "https://stackoverflow.com/questions/52545438/convert-a-dense-vector-to-a-dataframe-using-pyspark"}
{"id": 244, "q": "Filter dataframe rows which contribute to X% of values in one column", "d": "I have a dataframe: I want to see only those rows which contribute to 90% of Col3. In this case the expected output will be : I tried the below but is doesnt work as expected: Is there any solution for the same?", "q_apis": "values any", "io": "df Col1 Col2 Col3 A B 5 C D 4 E F 1 <s> Col1 Col2 Col3 A B 5 C D 4 ", "apis": "sort_values reset_index drop cumsum sum idxmin", "code": ["df = df[df.Col3 > 0] # optionally remove 0 valued rows\ndf = df.sort_values(by='Col3', ascending=False).reset_index(drop=True)\ntotals = df.Col3.cumsum()\ncutoff = totals[totals >= df.Col3.sum() * .7].idxmin()\nprint(df[:cutoff + 1])\n"], "link": "https://stackoverflow.com/questions/63059265/filter-dataframe-rows-which-contribute-to-x-of-values-in-one-column"}
{"id": 512, "q": "Pandas: Complex merge of DataFrames with date basis", "d": "I have 2 pandas 's that I need to merge in a bit of a complex manner so I am in need of some help. DataFrame to be inserted: DataFrame to receive insertion 1) The needs to establish a common basis for (notice column is different), thus the needs to be organized into , , , where the dates of , , and are used for assembling to correctly reflect the values in , , at a certain date. Sample output from step 1: 2) will need to be sorted by date according to and the columns , , will be added as columns in the . I imagine this will follow similar methods as in 1). Sample output from step 2: 3) The new columns , , will need to be filled forward with cumsum but I think I got that down: ~ Sample output from step 3: So the end goal would result in the values and shares held according to the index. For 's that will not have a column value (since column is \"missing\" some dates) in the resulting , it would be best to make that value but 0 would suffice. Happy to clarify anything, I appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! EDIT: Trying to merge in a loop now since number of - pairs may vary. I now have a list of separate for the - pairs: . Since number of pairs may vary, it seems best not to based on column labels hence .", "q_apis": "merge date merge DataFrame DataFrame where values at date step date columns columns step columns cumsum step values index value value any all at merge now now", "io": " 0 1 2 3 4 5 0 1998-01-02 16.25 2014-03-27 558.46 1998-01-02 131.13 1 1998-01-05 15.88 2014-03-28 559.99 1998-01-05 130.38 2 1998-01-06 18.94 2014-03-31 556.97 1998-01-06 131.13 3 1998-01-07 17.50 2014-04-01 567.16 1998-01-07 129.56 4 1998-01-08 18.19 2014-04-02 567.00 1998-01-08 130.50 5 1998-01-09 18.19 2014-04-03 569.74 1998-01-09 127.00 6 1998-01-12 18.25 2014-04-04 543.14 1998-01-12 129.50 7 1998-01-13 19.50 2014-04-07 538.15 1998-01-13 132.13 8 1998-01-14 19.75 2014-04-08 554.90 1998-01-14 131.13 9 1998-01-15 19.19 2014-04-09 564.14 1998-01-15 132.31 10 1998-01-16 18.81 2014-04-10 540.95 1998-01-16 135.25 11 1998-01-20 19.06 2014-04-11 530.60 1998-01-20 137.81 12 1998-01-21 18.91 2014-04-14 532.52 1998-01-21 137.00 13 1998-01-22 19.25 2014-04-15 536.44 1998-01-22 138.63 14 1998-01-23 19.50 2014-04-16 556.54 1998-01-23 138.25 15 1998-01-26 19.44 2014-04-17 536.10 1998-01-26 141.75 <s> 0 1 3 5 0 1998-01-02 16.25 NA 131.13 1 1998-01-05 15.88 NA 130.38 2 1998-01-06 18.94 NA 131.13 3 1998-01-07 17.50 NA 129.56 4 1998-01-08 18.19 NA 130.50 5 1998-01-09 18.19 NA 127.00 6 1998-01-12 18.25 NA 129.50 7 1998-01-13 19.50 NA 132.13 8 1998-01-14 19.75 NA 131.13 ... 10 2014-04-10 18.81 558.46 135.25 11 2014-04-11 19.06 559.99 137.81 12 2014-04-14 18.91 556.97 137.00 13 2014-04-15 19.25 567.16 138.63 14 2014-04-16 19.50 567.00 138.25 15 2014-04-17 19.44 569.74 141.75 ... ", "apis": "merge left right dropna columns", "code": ["final_df=pd.merge(left=rec_df, right=insert_df, left_index=True, right_index=True, dropna=False)\n", "df=df[[list of columns in order]]"], "link": "https://stackoverflow.com/questions/54752268/pandas-complex-merge-of-dataframes-with-date-basis"}
{"id": 279, "q": "extract a value from a pandas dataframe dict into another dataframe", "d": "df.head(10) How to convert the above dataframe into a new dataframe by selecting X: df.info() shows", "q_apis": "value head info", "io": " XYZVal 0 {\"X\":\"56.68\",\"Y\":\"51.56\",\"Z\":\"100\"} 1 {\"X\":\"58.05\",\"Y\":\"52.37\",\"Z\":\"62.6\"} 2 {\"X\":\"59.32\",\"Y\":\"54.48\",\"Z\":\"69.59\"} 3 {\"X\":\"58.51\",\"Y\":\"36.36\",\"Z\":\"82.76\"} 4 {\"X\":\"65.21\",\"Y\":\"60.26\",\"Z\":\"71.06\"} 5 {\"X\":\"57.64\",\"Y\":\"52.07\",\"Z\":\"67.89\"} 6 {\"X\":\"58.24\",\"Y\":\"50\",\"Z\":\"75\"} 7 {\"X\":\"57.69\",\"Y\":\"52.13\",\"Z\":\"68.64\"} 8 {\"X\":\"57.83\",\"Y\":\"53.05\",\"Z\":\"65.92\"} 9 {\"X\":\"60.87\",\"Y\":\"51.73\",\"Z\":\"71.35\"} <s> { 56.68 ,58.05 ,59.32 ,58.51 ,65.21 ,57.64 ,58.24 ,57.69 ,57.83 ,60.87 } ", "apis": "apply", "code": ["import json\ndf['XYZVal'].apply(lambda x: json.loads(x)[\"X\"])\n"], "link": "https://stackoverflow.com/questions/62351000/extract-a-value-from-a-pandas-dataframe-dict-into-another-dataframe"}
{"id": 618, "q": "Pandas count values greater than current row in the last n rows", "d": "How to get count of values greater than current row in the last n rows? Imagine we have a dataframe as following: I am trying to get a table such as following where n=3. Thanks in advance.", "q_apis": "count values last get count values last get where", "io": " col_a 0 8.4 1 11.3 2 7.2 3 6.5 4 4.5 5 8.9 <s> col_a col_b 0 8.4 0 1 11.3 0 2 7.2 2 3 6.5 3 4 4.5 3 5 8.9 0 ", "apis": "shape shape shape shape shape rolling apply sum astype count iloc max sum items values sum DataFrame size columns", "code": ["np.random.seed(1256)\nn = 3\n\ndef rolling_window(a, window):\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\ndef roll(df):\n    df['new'] = (df['col_a'].rolling(n+1, min_periods=1).apply(lambda x: (x[-1] < x[:-1]).sum(), raw=True).astype(int))\n    return df\n\ndef list_comp(df):\n    df['count'] = [(j < df['col_a'].iloc[max(0, i-3):i]).sum() for i, j in df['col_a'].items()]\n    return df\n\ndef strides(df):\n    x = np.concatenate([[np.nan] * (n), df['col_a'].values])\n    arr = rolling_window(x, n + 1)\n    df['new1'] = (arr[:, :-1] > arr[:, [-1]]).sum(axis=1)\n    return df\n\n\ndef make_df(n):\n    df = pd.DataFrame(np.random.randint(20, size=n), columns=['col_a'])\n    return df\n\nperfplot.show(\n    setup=make_df,\n    kernels=[list_comp, roll, strides],\n    n_range=[2**k for k in range(2, 15)],\n    logx=True,\n    logy=True,\n    xlabel='len(df)')\n"], "link": "https://stackoverflow.com/questions/51039857/pandas-count-values-greater-than-current-row-in-the-last-n-rows"}
{"id": 360, "q": "Multidimensional numpy.ndarray from multi-indexed pandas.DataFrame", "d": "I want to produce a 3-dimensional numpy.ndarray from a multi-indexed pandas.DataFrame. More precisely, say I have: which gives me and I want to write a function which returns, with the above argument, the numpy.ndarray Pandas multi-index looks like a substitute for multidimensional arrays, but it does not provide (or at least does not document) ways to go back and forth... Thanks.", "q_apis": "DataFrame DataFrame index at", "io": " z t x y 1 1 1 10 2 2 20 2 1 5 30 <s> [[[1, 10], [2, 20]], [[5, 30], [NaN, NaN]]] ", "apis": "to_xarray values transpose", "code": ["df.to_xarray().to_array().values.transpose(1,2,0)\n\n>>[[[ 1. 10.]\n  [ 2. 20.]]\n\n [[ 5. 30.]\n  [nan nan]]]\n"], "link": "https://stackoverflow.com/questions/59935954/multidimensional-numpy-ndarray-from-multi-indexed-pandas-dataframe"}
{"id": 187, "q": "Checking for NaNs in many columns in Pandas", "d": "I want to add a binary column to my dataframe based on whether given columns contain NaN or not. I have tried to do it with the below code. but I got a ValueError at the line before last. Sample input: Expected output: I want to check NaNs only for A, B, C columns.", "q_apis": "columns add columns at last columns", "io": "A B C D 10 NaN 40 NaN NaN NaN 80 90 20 45 NaN 89 NaN NaN NaN 46 <s> A B C D E 10 NaN 40 NaN 0 NaN NaN 80 90 0 20 45 NaN 89 0 NaN NaN NaN 46 1 ", "apis": "isna all mean std index isin drop dropna index mean std count eq view mean std where isnull all mean std", "code": ["In [1720]: %timeit df['ismissing'] = df[['A','B','C']].isna().all(axis=1)\n989 \u00b5s \u00b1 70 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "In [1719]: %timeit df['New']=~df.index.isin(df.drop('D',1).dropna(thresh=1).index)\n2.05 ms \u00b1 113 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "In [1724]: %timeit df['all_nan'] = df[['A','B','C']].count(axis=1).eq(0).view('i1')\n1.48 ms \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "In [1723]: %timeit dat['E'] = np.where(dat[['A','B','C']].isnull().all(1), 1, 0)\n914 \u00b5s \u00b1 18.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/62006098/checking-for-nans-in-many-columns-in-pandas"}
{"id": 155, "q": "Explode pandas dataframe singe row into multiple rows across multiple columns simultaneously", "d": "I have a dataframe as I want to break each record in such a way that values in column and explode into multiple rows but such that the first value in after splitting upon corresponds to the first value in after splitting upon . So my should look like this: NOTE: this is not the same as Split (explode) pandas dataframe string entry to separate rows as here the exploding/splitting of one record is not just across one column but the need is to split or explode one row into multiple rows, in two columns simultaneously. Any help is appreciated. Thanks", "q_apis": "columns values explode first value first value explode explode columns", "io": "df col1 act_id col2 -------------------- 0 40;30;30 act1 A;B;C 1 25;50;25 act2 D;E;F 2 70;30 act3 G;H <s> desired_df col1 act_id col2 --------------- 0 40 act1 A 1 30 act1 B 2 30 act1 C 3 25 act2 D 4 50 act2 E 5 25 act2 F 6 70 act3 G 7 30 act3 H ", "apis": "set_index apply Series stack dropna reset_index columns", "code": ["df2.set_index('act_id').apply(lambda x: pd.Series(x.col1.split(';'),x.col2.split(';')), axis=1).stack().dropna().reset_index()\n\ndf2.columns = ['col1','act_id','col2']\n"], "link": "https://stackoverflow.com/questions/56437405/explode-pandas-dataframe-singe-row-into-multiple-rows-across-multiple-columns-si"}
{"id": 230, "q": "How to combine rows into seperate dataframe python pandas", "d": "i have the following dataset: i want to combine x y z into another dataframe like this: and i want these dataframes for each x y z value like first, second third and so on. how can i select and combine them? desired output:", "q_apis": "combine combine value first second select combine", "io": "A B C D E F 154.6175111 148.0112337 155.7859835 1 1 x 255 253.960131 242.5382584 1 1 x 251.9665958 235.1105659 185.9121703 1 1 x 137.9974994 225.3985177 254.4420772 1 1 x 85.74722877 116.7060415 158.4608395 1 1 x 123.6969939 140.0524405 132.6798037 1 1 x 133.3251695 80.08976196 38.81201612 1 1 y 118.0718812 243.5927927 255 1 1 y 189.5557302 139.9046713 91.90519519 1 1 y 172.3117291 188.000268 129.8155501 1 1 y 48.07634611 21.9183119 25.99669279 1 1 y 23.40525987 8.395857933 25.62371342 1 1 y 228.753009 164.0697727 172.6624107 1 1 z 203.3405006 173.9368303 189.8103708 1 1 z 184.9801932 117.1591341 87.94739034 1 1 z 29.55251224 46.03945452 70.7433477 1 1 z 143.6159623 120.6170926 155.0736604 1 1 z 142.5421179 128.8916843 169.6013111 1 1 z <s> A B C D E F 154.6175111 148.0112337 155.7859835 1 1 x 133.3251695 80.08976196 38.81201612 1 1 y 228.753009 164.0697727 172.6624107 1 1 z A B C D E F 255 253.960131 242.5382584 1 1 x 118.0718812 243.5927927 255 1 1 y 203.3405006 173.9368303 189.8103708 1 1 z A B C D E F 251.9665958 235.1105659 185.9121703 1 1 x 189.5557302 139.9046713 91.90519519 1 1 y 184.9801932 117.1591341 87.94739034 1 1 z A B C D E F 137.9974994 225.3985177 254.4420772 1 1 x 172.3117291 188.000268 129.8155501 1 1 y 29.55251224 46.03945452 70.7433477 1 1 z A B C D E F 85.74722877 116.7060415 158.4608395 1 1 x 48.07634611 21.9183119 25.99669279 1 1 y 143.6159623 120.6170926 155.0736604 1 1 z A B C D E F 123.6969939 140.0524405 132.6798037 1 1 x 23.40525987 8.395857933 25.62371342 1 1 y 142.5421179 128.8916843 169.6013111 1 1 z ", "apis": "groupby cumcount groupby", "code": ["g = df.groupby('F').cumcount()\n\nfor i, g in df.groupby(g):\n    print (g)\n"], "link": "https://stackoverflow.com/questions/63451784/how-to-combine-rows-into-seperate-dataframe-python-pandas"}
{"id": 413, "q": "How to make a deepcopy of a dataframe with dataframes within it? (python)", "d": "I want a copy of a dataframe which contains a dataframe. When I change something in the nested dataframe, it shouldn't change in the original dataframe. I have a dataframe like this: Generated with the next code: When I make a deepcopy of the hole dataframe and the nested dataframe and change something in the nested dataframe in the copy, the value also changes in the original. output: but I want: What is the fix for this problem?", "q_apis": "copy contains copy value", "io": " 0 1 2 0 1 2 doei 1 4 5 6 0 1 2 0 1 2 doei 1 4 5 6 <s> 0 1 2 0 1 2 doei 1 4 5 6 0 1 2 0 1 2 3 1 4 5 6 ", "apis": "iloc iloc iloc iloc", "code": ["import pickle\ndeepCopy = pickle.loads(pickle.dumps(df))\n\ndeepCopy.iloc[0,2].dfCombinations.iloc[0,2] = \"doei\"\n\nprint(deepCopy.iloc[0,2].dfCombinations)\nprint(\" \")\nprint(df.iloc[0,2].dfCombinations)\n"], "link": "https://stackoverflow.com/questions/58431148/how-to-make-a-deepcopy-of-a-dataframe-with-dataframes-within-it-python"}
{"id": 318, "q": "How to break/pop a nested Dictionary inside a list, inside a pandas dataframe?", "d": "I have a dataframe which has a dictionary inside a nested list and i want to split the column 'C' : expected output :", "q_apis": "pop", "io": "A B C 1 a [ {\"id\":2,\"Col\":{\"x\":3,\"y\":4}}] 2 b [ {\"id\":5,\"Col\":{\"x\":6,\"y\":7}}] <s> A B C_id Col_x Col_y 1 a 2 3 4 2 b 5 6 7 ", "apis": "apply Series concat", "code": ["df[[\"Col\", \"id\"]] = df[\"C\"].apply(lambda x: pd.Series(x[0]))\n", "df = pd.concat([df, json_normalize(df.Col)], axis=1)\n"], "link": "https://stackoverflow.com/questions/60862810/how-to-break-pop-a-nested-dictionary-inside-a-list-inside-a-pandas-dataframe"}
{"id": 326, "q": "Sort pandas dataframe by index", "d": "I have the following pandas dataframe: I would like to order it from highest to lowest based on the index no matter if it is repeated, what I say would look like this: The maximum value corresponds to Cmpd6 = 0.79, followed by Cmpd4 = 0.69 ... at some point Cmpd6 = 0.56 which I would like to leave the list like this: This with each value of the array, no matter how many times the indexes are repeated, I was trying with but it does not produce any of this, I also tried but it does not give me the indexes. How can i fix this? Thanks! My solution:", "q_apis": "index index value at value array any", "io": " Cmpd1 Cmpd2 Cmpd3 Cmpd4 Cmpd5 Cmpd6 Cmpd1 1 Cmpd2 0.4 1 Cmpd3 0.6 0.3 1 Cmpd4 0.46 0.69 0.32 1 Cmpd5 0.57 0.44 0.41 0.51 1 Cmpd6 0.41 0.79 0.33 0.56 0.43 1 <s> Cmpd6 = 0.79 Cmpd4 = 0.69 Cmpdx = x Cmpd6 = 0.56 ", "apis": "stack apply reset_index drop sort_values astype replace stack reset_index drop sort_values", "code": ["s = df.stack()\ns[s.apply(lambda x: type(x) is not str and x < 1)]\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\\\n    .astype(float)\n", "s = df.replace(r'^\\s*$', np.nan, regex=True).stack()\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\ns[s < 1]\n"], "link": "https://stackoverflow.com/questions/60897173/sort-pandas-dataframe-by-index"}
{"id": 592, "q": "easy tool to filtering columns with specific conditions using pandas", "d": "I'm wondering if exist a tool in python to filter data between columns that follow an specific condition. I need to generate a clean dataframe where all the data in column 'A' must have the same consecutive number in column 'E'(and this number is repeated at least twice). Here an example: The output will be:", "q_apis": "columns filter between columns where all at", "io": "df Out[30]: A B C D E 6 1 2.366 8.621 10.835 1 7 1 2.489 8.586 10.890 2 8 1 2.279 8.460 10.945 2 9 1 2.296 8.559 11.000 2 10 2 2.275 8.620 11.055 2 11 2 2.539 8.528 11.110 2 50 2 3.346 5.979 10.175 5 51 3 3.359 5.910 10.230 1 52 3 3.416 5.936 10.285 1 <s> df Out[31]: A B C D E 7 1 2.489 8.586 10.890 2 8 1 2.279 8.460 10.945 2 9 1 2.296 8.559 11.000 2 10 2 2.275 8.620 11.055 2 11 2 2.539 8.528 11.110 2 51 3 3.359 5.910 10.230 1 52 3 3.416 5.936 10.285 1 ", "apis": "groupby shift cumsum filter size groupby shift cumsum transform size", "code": ["import numpy as np\n\ndf.groupby((df.E != df.E.shift(1)).cumsum()).filter(lambda x: np.size(x.E) >= 2)\n# or\ndf[df.groupby((df.E != df.E.shift(1)).cumsum()).E.transform('size') >= 2]\n"], "link": "https://stackoverflow.com/questions/52118251/easy-tool-to-filtering-columns-with-specific-conditions-using-pandas"}
{"id": 634, "q": "split dataframe entries at midnight", "d": "I have a , with and datatime. and can be expected to be sorted interally, but gaps/overlaps may occur between consecutive rows. I would like to create a new dataframe with the difference that if row contains midnight (e.g. midnight is contained in [,]), the row is then split in two parts before and after midnight ex: should be", "q_apis": "at overlaps between difference contains", "io": " Start End 0 2010-02-01 00:00:00 2010-02-01 04:00:00 1 2010-02-01 05:03:00 2010-02-01 09:03:00 2 2010-02-01 10:06:00 2010-02-01 14:06:00 3 2010-02-01 15:09:00 2010-02-01 19:09:00 4 2010-02-01 20:12:00 2010-02-02 00:12:00 5 2010-02-02 01:15:00 2010-02-02 05:15:00 <s> Start End 0 2010-02-01 00:00:00 2010-02-01 04:00:00 1 2010-02-01 05:03:00 2010-02-01 09:03:00 2 2010-02-01 10:06:00 2010-02-01 14:06:00 3 2010-02-01 15:09:00 2010-02-01 19:09:00 ----------------------------------------- 4 2010-02-01 20:12:00 2010-02-01 23:59:00 5 2010-02-02 00:00:00 2010-02-02 00:12:00 ----------------------------------------- 6 2010-02-02 01:15:00 2010-02-02 05:15:00 ", "apis": "date date copy", "code": ["splits = df[df.End.dt.date > df.Start.dt.date].copy()\n"], "link": "https://stackoverflow.com/questions/50336251/split-dataframe-entries-at-midnight"}
{"id": 50, "q": "Best way to change column data for all rows over multiple dataframes in pandas?", "d": "Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example :", "q_apis": "all all replace unique update where all apply", "io": "+---+----+ | | id | +---+----+ | 1 | a1 | +---+----+ | 2 | c1 | +---+----+ <s> +---+----+ | | id | +---+----+ | 1 | b1 | +---+----+ ", "apis": "replace map replace replace", "code": ["import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n"], "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas"}
{"id": 200, "q": "Using Python&#39;s max to return two equally large values across columns of a data frame", "d": "I would like to find the column of a data frame with the maximum value per row and if there are multiple equally large values, then return all the column names where those values are. I would like to store all of these values in the last column of the data frame. I have been referencing the following post, and am unsure of how to modify it to handle data frames: Using Python's max to return two equally large values So if my data looked like this My goal is an output that looks like this: I know how to use idxmax(axis=1,skipna = True) to return the first max and know that if I change 0 to Nan in the dataframe it will populate the last row correctly, just not sure how to do this when there are multiple max values. Any help is greatly appreciated ! I am an R programmer and this is my first time in Python.", "q_apis": "max values columns value values all names where values all values last max values idxmax first max last max values first time", "io": "Key Column_1 Column_2 Column_3 0 1 2 3 1 1 1 0 2 0 0 0 <s> Key Column_1 Column_2 Column_3 Column_4 0 1 2 3 Column_3 1 1 1 0 Column_1,Column_2 2 0 0 0 NA ", "apis": "set_index select_dtypes eq max dot columns mask eq all", "code": ["d = df.set_index('Key').select_dtypes('number')\nv = d.eq(d.max(axis=1), axis=0).dot(d.columns + ',').str.rstrip(',')\ndf['Column_4'] = v.mask(d.eq(0).all(axis=1)))\n"], "link": "https://stackoverflow.com/questions/64229332/using-pythons-max-to-return-two-equally-large-values-across-columns-of-a-data-f"}
{"id": 85, "q": "How to set new columns in a multi-column index from a dict with partially specified tuple keys?", "d": "I have a pandas dataframe initialized in the following way: which gives: Now I'd like to add a new column to this dataframe using partial key slicing BUT not in code, I'd like to do this from configuration i.e. a dictionary with partial tuple keys: which gives: notice that setting 'x' doesn't depend on the second component of the key and setting 'y1' and 'y2' do depend on the second component of the key. A possible solution is to fully specify the mapping but this is also not desirable if I have a 100 whose assignment doesn't depend on the second component. I wish to reach the above result but not hard-coding the sliced assignments, instead I'd like to do it from an externalized dictionary: My configuration dictionary would look like this: Is there a pythonic and pandas-tonic way to apply this dictionary with partially specified keys to reach the sliced assignment produced before?", "q_apis": "columns index keys add keys second second second apply keys", "io": "# col1 col2 # key1 key2 # a a1 1 2 # a2 3 4 # b b1 5 6 # b2 7 8 <s> # key1 key2 # a a1 1 2 x # a2 3 4 x # b b1 5 6 y1 # b2 7 8 y2 ", "apis": "value items loc value", "code": ["my_dict = {\n    ('a',): 'x',\n    ('b', 'b1'): 'y1',\n    ('b', 'b2'): 'y2'\n}\n\nfor idx, value in my_dict.items():\n    df.loc[idx, 'desc1'] = value\n"], "link": "https://stackoverflow.com/questions/66398540/how-to-set-new-columns-in-a-multi-column-index-from-a-dict-with-partially-specif"}
{"id": 197, "q": "Rename column in dataframe that contains digits in the middle", "d": "Say I have a dataframe columns as such : To: This need to be done dynamically. My plan is: Use regex to find digits in the MIDDLE of string. Replace to the back of the column name, iteratively. My current code :", "q_apis": "contains columns name", "io": " # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Action_3.@source 1 non-null object 1 Description_3.#text 1 non-null object 2 Code_3.@source 1 non-null object 3 Others 1 non-null object 4 Animal_1 1 non-null object <s> # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Action.@source_3 1 non-null object 1 Description.#text_3 1 non-null object 2 Code.@source_3 1 non-null object 3 Others 1 non-null object 4 Animal_1 1 non-null object ", "apis": "DataFrame columns first last first last columns replace Index dtype", "code": ["df = pd.DataFrame(\n    [],\n    columns=[\n        \"Action_3.@source\",\n        \"Description_3.#text\",\n        \"Code_3.@source\",\n        \"Others\",\n        \"Animal_1\",\n    ],\n)\n\npat = r\"(?P<first>.+)(?P<middle>_\\d)(?P<last>.+)\"\nrepl = lambda m: f\"{m.group('first')}{m.group('last')}{m.group('middle')}\"\n\ndf.columns.str.replace(pat, repl)\n\nIndex(['Action.@source_3', 'Description.#text_3', 'Code.@source_3', 'Others',\n       'Animal_1'],\n      dtype='object')\n"], "link": "https://stackoverflow.com/questions/64275557/rename-column-in-dataframe-that-contains-digits-in-the-middle"}
{"id": 375, "q": "Create a list in list comprehension and then create another list from that list inside the same list comprehension", "d": "That title is a mouthful, so it may be easier to show what I am trying to achieve via code. The above is not where I am having trouble with, but I wanted to provide as much context as possible. I am trying to iterate through the dataframe and retrieve the first element of the column name where the dataframe's element equals 1. I can do this using the following: This generates the first list I need to create: I can assign that output to a variable and perform another list comprehension to get my final output: This outputs my final list of: Is it possible to perform both of these inside the same list comprehension while only receiving that final list as the output? This is somewhat inelegant, but this is what it would look like in non-list comprehension: Thank you for taking the time to look this over!", "q_apis": "where first name where equals first assign get time", "io": "[1, 1, 1, 2, 3, 4] <s> [(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)] ", "apis": "eq dot columns eq dot columns iloc eq dot columns eq dot columns iloc", "code": ["[(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(df.columns.str[0])[1:])]\n#same with .iloc -> [(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(\n#                                          df.columns.str[0]).iloc[1:])]\n"], "link": "https://stackoverflow.com/questions/59548017/create-a-list-in-list-comprehension-and-then-create-another-list-from-that-list"}
{"id": 248, "q": "How to shuffle a dataframe while maintaining the order of a specific column", "d": "I have a pandas dataframe which I want to shuffle, but keep the order of 1 column. So imagine I have the following df: I want to shuffle the rows but keep the order of the ID column of the first df. My wanted result would be something like this: How do I do this?", "q_apis": "first", "io": "| i | val | val2| ID | | 0 | 2 | 2 |a | | 1 | 3 | 3 |b | | 2 | 4 | 4 |a | | 3 | 6 | 5 |b | | 4 | 5 | 6 |b | <s> | i | val | val2| ID | | 2 | 4 | 4 |a | | 4 | 5 | 6 |b | | 0 | 2 | 2 |a | | 3 | 6 | 5 |b | | 1 | 3 | 3 |b | ", "apis": "DataFrame groupby transform sample", "code": ["df = pd.DataFrame({\"val\": [1, 2, 3, 4, 5, 6, 7], \"ID\": [\"a\", \"b\", \"a\", \"b\", \"a\", \"a\", \"b\"]})\ndf[\"val\"] = df.groupby(\"ID\").transform(lambda x: x.sample(frac=1))\nprint(df)\n"], "link": "https://stackoverflow.com/questions/62972913/how-to-shuffle-a-dataframe-while-maintaining-the-order-of-a-specific-column"}
{"id": 150, "q": "How to filter rows in a dataframe to get only 3 most popular and delete others unused data?", "d": "Theory I have some data on car brands in the US. I have to arrange them on the map of individual states and after hovering over with the mouse, I have to display the 3 most popular brands for a given state. Question I have the following dataframe I need to achieve something like that (structure is probably wrong - I am not sure what kind of structure would be best - I just need data about name of column and its value): Current situation I was able to create something like this: So the situation is identical to the example I gave at the top. Now I have to somehow \"filter this data and keep information about the brand name and its percentage in a given state\". It is a bit difficult for me, can someone please help me?", "q_apis": "filter get delete map name value identical at filter name", "io": " A B C D E 1 20 0 40 10 30 2 0 60 15 5 20 3 50 30 20 0 0 <s> 1 (C: 40) (E: 30) (A: 20) 2 (B: 60) (E: 20) (C: 15) 3 (A: 50) (B: 30) (C: 20) ", "apis": "apply Series nlargest to_dict apply Series nlargest items", "code": ["res = df.apply(lambda x: pd.Series(x).nlargest(3).to_dict(), axis=1)\nprint(res)\n", "res = df.apply(lambda x: list(pd.Series(x).nlargest(3).items()), axis=1)\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65077340/how-to-filter-rows-in-a-dataframe-to-get-only-3-most-popular-and-delete-others-u"}
{"id": 99, "q": "Pandas regex comprehension - isolate single result", "d": "I have a dataframe that's been extracted from an SQL server, and I used regex to extract a string of three dimensions. I need all three dimensions in three new columns so I have used a regular expression for a number optionally separated by a period, and created a column from this findall result. But the result shows as a list and I am unable to index the second dimension. Due to the urgency I have been able to temporarily solve this with a lookaround. But how can I directly extract dimension column extract - not all are in this format code extract for finding dims sample output using the findall result I would need a column for 1493.4 and a second column for 204.2 - I can do the first one but how would I create a column for specific indexes in the regex results. I have tried lambda, list comprehension, and everything else I can think of. So far I cannot find a similar question online and I know it should be simple - but its taken me 2 days! Many thanks for all your help EDIT: To confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern. Additionally, my focus is on obtaining only list item n to a new column and not every item in the list", "q_apis": "all columns index second all sample second first days all where item item", "io": "[1103.5 x 308.8 x 25.4 mm] [33.3 x 13 x 9.5mm] [136.5 x 15 x 12.7 mm] <s> [1493.4, 204.2, 25.4, 0013, 900, 4] [136.5, 15, 12.7, 001, 900, 2] ", "apis": "DataFrame apply Series", "code": ["import pandas as pd\ndf = pd.DataFrame({'dim':['10x10','12x10','10x12']})\ndf['dim_list'] = df.dim.str.findall(r'\\d+')\ndf[['width','height']] = df.dim_list.apply(pd.Series)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/66138245/pandas-regex-comprehension-isolate-single-result"}
{"id": 306, "q": "Match on multiple columns using array", "d": "I'm working on a project where my original dataframe is: But, I have an array with new labels for certain points (for that I only used columns A and B) in the original dataframe. Something like this: My goal is to add the new labels to the original dataframe. I know that the combination of A and B unique is. What is the fastest way to assign the new label to the correct row? This is my try: Wanted output (rows with index 1 and 2 are changed): For small datasets may this be okay with I'm currently using it for datasets with more than 25000 labels. Is there a way that is faster? Also, in some cases I used all columns expect the column 'label'. That dataframe exists out of 64 columns so my method can not be used here. Has someone an idea to improve this? Thanks in advance", "q_apis": "columns array where array columns add unique assign index all columns columns", "io": " A B C label 0 1 2 2 Nan 1 2 4 5 7 2 3 6 5 Nan 3 4 8 7 Nan 4 5 10 3 8 5 6 12 4 8 <s> A B C label 0 1 2 2 Nan 1 2 4 5 5 2 3 6 5 9 3 4 8 7 Nan 4 5 10 3 8 5 6 12 4 8 ", "apis": "values astype view dtype dtype shape loc", "code": ["import numpy as np\n\nX_labeled = [[2, 4], [3,6]]\ny_labeled = [5,9]\n\na = df.values[:,:2].astype(int) #indexing on A and B\n\ndef view_as_1d(a):\n    a = np.ascontiguousarray(a)\n    return a.view(np.dtype((np.void, a.dtype.itemsize * a.shape[-1])))\n\nix = np.in1d(view_as_1d(a), view_as_1d(X_labeled))\ndf.loc[ix, 'label'] = y_labeled\n"], "link": "https://stackoverflow.com/questions/61293428/match-on-multiple-columns-using-array"}
{"id": 351, "q": "Renaming columns on slice of dataframe not performing as expected", "d": "I was trying to clean up column names in a dataframe but only a part of the columns. It doesn't work when trying to replace column names on a slice of the dataframe somehow, why is that? Lets say we have the following dataframe: Note, on the bottom is copy-able code to reproduce the data: I want to clean up the column names (expected output): Approach 1: I can get the clean column names like this: Or Approach 2: But when I try to overwrite the column names, nothing happens: Same for the second approach: This does work, but you have to manually concat the name of the first column, which is not ideal: Is there an easier way to achieve this? Am I missing something? Dataframe for reproduction:", "q_apis": "columns names columns replace names copy names get names names second concat name first", "io": " Value ColAfjkj ColBhuqwa ColCouiqw 0 1 a e i 1 2 b f j 2 3 c g k 3 4 d h l <s> Value ColA ColB ColC 0 1 a e i 1 2 b f j 2 3 c g k 3 4 d h l ", "apis": "columns columns iloc columns", "code": ["df.columns = [df.columns[0]] + list(df.iloc[:, 1:].columns.str[:4])\n"], "link": "https://stackoverflow.com/questions/56293392/renaming-columns-on-slice-of-dataframe-not-performing-as-expected"}
{"id": 43, "q": "Convert dataframe objects to float by iterating over columns", "d": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance", "q_apis": "columns Series Series DataFrame values", "io": " c1 c2 0 - 75.0% 1 -5.5% 65.8% . n - 6.9% <s> c1 c2 0 0.0 75.0 1 -5.5 65.8 . n 0.0 6.9 ", "apis": "replace", "code": ["# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"], "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns"}
{"id": 107, "q": "Join two pandas dataframes based on lists columns", "d": "I have 2 dataframes containing columns of lists. I would like to join them based on 2+ shared values on the lists. Example: In this case we can see that id1 matches id3 because there are 2+ shared values on the lists. So the output will be (columns name are not important and just for example): How can I achieve this result? I've tried to iterate each row in dataframe #1 but it doesn't seem a good idea. Thank you!", "q_apis": "columns columns join values values columns name", "io": "ColumnA ColumnB | ColumnA ColumnB id1 ['a','b','c'] | id3 ['a','b','c','x','y', 'z'] id2 ['a','d,'e'] | <s> ColumnA1 ColumnB1 ColumnA2 ColumnB2 id1 ['a','b','c'] id3 ['a','b','c','x','y', 'z'] ", "apis": "merge apply intersection mean std map intersection mean std DataFrame DataFrame merge map intersection", "code": ["    df = df1.merge(df2, how='cross')         # simplified cross joint for pandas >= 1.2.0\n", "%%timeit\ndf['overlap'] = df.apply(lambda x: \n                         len(set(x['ColumnB1']).intersection(\n                             set(x['ColumnB2']))), axis=1)\n\n\n800 \u00b5s \u00b1 59.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "%%timeit\ndf['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n217 \u00b5s \u00b1 25.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "    data = {'ColumnA1': ['id1', 'id2'], 'ColumnB1': [['a', 'b', 'c'], ['a', 'd', 'e']]}\n    df1 = pd.DataFrame(data)\n\n    data = {'ColumnA2': ['id3', 'id4'], 'ColumnB2': [['a','b','c','x','y', 'z'], ['d','e','f','p','q', 'r']]}\n    df2 = pd.DataFrame(data)\n\n    df = df1.merge(df2, how='cross')             # for pandas version >= 1.2.0\n\n    df['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n    df = df[df['overlap'] >= 2]\n    print (df)\n"], "link": "https://stackoverflow.com/questions/66060591/join-two-pandas-dataframes-based-on-lists-columns"}
{"id": 485, "q": "Is there a way to get the mean value of previous two columns in pandas?", "d": "I want to calculate the mean value of previous two rows and fill the NAN's in my dataframe. There are only few rows with missing values in the column. I tried using and but it only captures the previous or next row/column value and fill NAN. My example data set has 7 columns as below: The output I want:", "q_apis": "get mean value columns mean value values value columns", "io": "X 1990-2000 2000-2010 2010-19 1990-2000 2000-2010 2010-19 Hyderabad 10 20 NAN 1 3 NAN <s> X 1990-2000 2000-2010 2010-19 1990-2000 2000-2010 2010-19 Hyderabad 10 20 15 1 3 2 ", "apis": "iloc mean iloc fillna", "code": ["col_indices = [3, 6]\n\nfor i in col_indices:\n    means = df.iloc[:, [i-1, i-2]].mean(axis=1)\n    df.iloc[:, i].fillna(means, inplace=True)\n"], "link": "https://stackoverflow.com/questions/55986653/is-there-a-way-to-get-the-mean-value-of-previous-two-columns-in-pandas"}
{"id": 223, "q": "Replace NaN with values in a row from previous matching values in column", "d": "I have following data frame (df). And I want to get to this state: I want to go through both columns and replace NaN with appropriate zip_code or city. Here is what I have done but as you can see it didn't fully work. If columns 'zip_mapped' and 'city_mapped' were properly populated I would have just replaced them with original cols. Can anyone help me here?", "q_apis": "values values get columns replace columns", "io": "df city zip_code 0 city1 90287 1 city2 90288 2 city3 80023 3 city4 90210 4 city1 NaN 5 city4 NaN 6 city7 NaN 7 NaN 90210 8 NaN 80023 <s> city zip_code 0 city1 90287 1 city2 90288 2 city3 80023 3 city4 90210 4 city1 90287 5 city4 90210 6 city7 NaN 7 city4 90210 8 city3 80023 ", "apis": "fillna groupby transform first fillna groupby transform first", "code": ["df.zip_code = df.zip_code.fillna(df.zip_code.groupby(df.city).transform('first'))\n\ndf.city = df.city.fillna(df.city.groupby(df.zip_code).transform('first'))\n"], "link": "https://stackoverflow.com/questions/63620593/replace-nan-with-values-in-a-row-from-previous-matching-values-in-column"}
{"id": 606, "q": "How to fill column&#39; value with another column and keep existing?", "d": "I have this dataframe with two column, and I want to fill with its corresponding if exist, also if already have a value keep it don't change it with . Here is an example: input output It's look easy, but I'm beginner in python :( Any help please.", "q_apis": "value value", "io": "c1 c2 HP_0003470 HP_8362789 HP_0093723 MP_0000231 MP_0000231 <s> c1 c2 HP_0003470 HP_8362789 HP_0093723 MP_0000231 MP_0000231 MP_0000231 MP_0000231 ", "apis": "replace fillna replace fillna", "code": ["df['c1'] = df['c1'].replace('',np.NaN).fillna(df['c2'])\ndf['c2'] = df['c2'].replace('',np.NaN).fillna(df['c1'])\n"], "link": "https://stackoverflow.com/questions/51408990/how-to-fill-column-value-with-another-column-and-keep-existing"}
{"id": 477, "q": "pandas create multiple dataframes based on duplicate index dataframe", "d": "If I have a dataframe with duplicates in the index, how would I create a set of dataframes with no duplicates in the index? More precisely, given the dataframe: I would want as output, a list of dataframes: This needs to be scalable to as many dataframes as needed based on the number of duplicates.", "q_apis": "index index index", "io": " a b 1 1 6 1 2 7 2 3 8 2 4 9 2 5 0 <s> a b 1 1 6 2 3 8 a b 1 2 7 2 4 9 a b 2 5 0 ", "apis": "groupby index index max nth", "code": ["import numpy as np\n\ng = df.groupby(df.index)\ncnt = np.bincount(df.index).max()\ndfs = [g.nth(i) for i in range(cnt)]\n"], "link": "https://stackoverflow.com/questions/56271520/pandas-create-multiple-dataframes-based-on-duplicate-index-dataframe"}
{"id": 258, "q": "Replace last non NaN value in row", "d": "I'd like to replace all the last non NaNs in rows in data frame with NaN value. I have 300 rows and 1068 columns in my data frame. and each row have different number of valid values in them padded with NaNs. Here is an example of a row: a row in dataframe = output = How to replace last non NaN value in rows in CSV file?", "q_apis": "last value replace all last value columns values replace last value", "io": "[1 2 3 NaN NaN NaN] <s> [1 2 NaN NaN NaN NaN]", "apis": "DataFrame to_numpy shape argmax array argmax array dtype shape argmax array dtype", "code": ["import numpy as np\ndf = pd.DataFrame([[1, 2, 3, np.nan, np.nan, np.nan], [1, 2, 3, np.nan, np.nan, 2]])\n", "a = df.to_numpy()\nm = a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\nnp.put_along_axis(a, m[:,None], np.nan, axis=1)\ndf[:] = a\n", "np.isnan(a[:,::-1])\narray([[ True,  True,  True, False, False, False],\n       [False,  True,  True, False, False, False]])\n", "np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([3, 0], dtype=int64)\n", "a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([2, 5], dtype=int64)\n"], "link": "https://stackoverflow.com/questions/62913446/replace-last-non-nan-value-in-row"}
