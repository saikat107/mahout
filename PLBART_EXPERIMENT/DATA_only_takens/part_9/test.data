{"id": 164, "q": "How to reorder rows by a condition in pandas?", "d": "I have two dataframes and one of their orders is correct for me. I want to make the other's order the same as the correct one. Here is the point, it's not about index numbers, order depends on a variable. Like this df1 df2 I want the order of df2 to be same as df1, I put them in a for loop but it took long time (my real data is much greater than reproducible example) Is there any easier way to make my wish real ? Thanks in advice.", "q_apis": "index put time any", "io": "A B 13 2 20 5 15 3 . . . . <s> A B 15 3 13 2 20 5 . . . . ", "apis": "DataFrame DataFrame set_index loc", "code": ["df1 = pd.DataFrame({\"A\": [13,20,15], \"B\": [2,5,3]})\ndf2 = pd.DataFrame({\"A\": [15,13,20], \"B\": [3,2,5]})\ndf2 = df2.set_index(\"A\")\nprint(df2.loc[df1[\"A\"]])\n"], "link": "https://stackoverflow.com/questions/64826052/how-to-reorder-rows-by-a-condition-in-pandas"}
{"id": 574, "q": "How to remove unwanted data from python panda data frame?", "d": "After reading my txt file: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale The panda datadframe like as below: But I need the data as below( Space and extra letter should be removed)", "q_apis": "", "io": "1 2 3 4 -0.4302012 2 -0.3233208 3 0.576837 4 0.426791 5 <s> 1 2 3 4 -0.4302012 -0.3233208 0.576837 0.426791 ", "apis": "read_csv drop map replace sample", "code": ["data = pd.read_csv(filepath,delimiter=':',header= None)\ndata = data.drop([0],axis=1)\nfor i in range(1,9):\n   search_string = str(i+1)\n   data[i] = data[i].map(lambda x: x.replace(search_string.rjust(1),''))\nprint(data)\ndata.sample(n=5)\n"], "link": "https://stackoverflow.com/questions/52800453/how-to-remove-unwanted-data-from-python-panda-data-frame"}
{"id": 55, "q": "How to edit Excel file using DataFrame and save it back as Excel file?", "d": "I have this Excel file. I also put the screenshot of my the file below. I want to edit the data on column with this 2 criteria: removing mark between the text. removing values. removing mark. So, for example, from this text: I want to make it look like this: Of course, I can do this manually one by one, but unfortunately because I have about 20 similar files that I have to edit, I can't do it manually, so I think I might need help from Python. My idea to do it on Python is to load the Excel file to a DataFrame, edit the data row by row (maybe using and method), and put the edit result back to original Excel file, or maybe generate a new one consisting an edited data column. But, I kinda have no idea on how to do code it. So far, what I've tried to do is this: read the Excel files to Python. read column in that Excel file. load it to a dataframe. Below is my current code. My question is how can I edit the data per row and put the edit result back again to original or a new Excel file? I have difficulties accessing the data because I can't get the string value. Is there any way in Python to achieve it?", "q_apis": "DataFrame put between values DataFrame put put get value any", "io": "['0', 'E3', 'F3', 'F#3 / Gb3', 'G3', 'G#3 / Ab3', 'A3', 'A#3 / Bb3', 'B3', 'C4', 'C#4 / Db4', 'D4'] <s> [E3, F3, F#3 / Gb3, G3, G#3 / Ab3, A3, A#3 / Bb3, B3, C4, C#4 / Db4, D4] ", "apis": "read_excel astype replace astype replace to_excel", "code": ["df = pd.read_excel(\"014_twinkle_twinkle 300 0.0001 dataframe.xlsx\")\ndf[\"pitch-class\"] = df[\"pitch-class\"].astype(str).str.replace(\"'0', \", \"\")\ndf[\"pitch-class\"] = df[\"pitch-class\"].astype(str).str.replace(\"'\", \"\")\ndf.to_excel(\"results.xlsx\")\n"], "link": "https://stackoverflow.com/questions/67105986/how-to-edit-excel-file-using-dataframe-and-save-it-back-as-excel-file"}
{"id": 570, "q": "Get minimum value from index in data frame", "d": "I have a dataframe like this. I would like to get minimum values for each value in column1. So my output would be When I try the code It gives me an empty dataframe and if I try it deletes some values, for reasons I don't understand. I use python 2.7", "q_apis": "value index get values value empty values", "io": "column1 column2 1 2 1 3 1 4 2 3 2 1 2 4 <s> column1 column2 1 2 2 1 ", "apis": "sort_values drop_duplicates last", "code": ["df = df.sort_values('column2', ascending=False).drop_duplicates('column1', keep='last')\n"], "link": "https://stackoverflow.com/questions/52985798/get-minimum-value-from-index-in-data-frame"}
{"id": 541, "q": "Ranking groups based on size", "d": "Sample Data: What I would like to do is replace the largest cluster id with and the second largest with and so on and so forth. Output would be as shown below. I'm not quite sure where to start with this. Any help would be much appreciated.", "q_apis": "groups size replace second where start", "io": "id cluster 1 3 2 3 3 3 4 3 5 1 6 1 7 2 8 2 9 2 10 4 11 4 12 5 13 6 <s> id cluster 1 0 2 0 3 0 4 0 5 2 6 2 7 1 8 1 9 1 10 3 11 3 12 4 13 5 ", "apis": "unique values argsort unique values array array array", "code": ["u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_counts=True\n)\n(-c).argsort()[i]\n", "u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_couns=True\n)\n", "array([2, 2, 2, 2, 0, 0, 1, 1, 1, 3, 3, 4, 5])\n", "array([3, 3, 3, 3, 1, 1, 2, 2, 2, 4, 4, 5, 6])\n", "array([2, 3, 4, 2, 1, 1])\n"], "link": "https://stackoverflow.com/questions/47402346/ranking-groups-based-on-size"}
{"id": 54, "q": "Python, Pandas: check each element in list values of column to exist in other dataframe", "d": "I have dataframe column with values in lists, want to add new column with filtered values from list if they are in other dataframe. df: df2: I need to add new column with filtered column in so that it contains lists with only elements which are in column . Result: Speed is crucial, as there is a huge amount of records. What I did for now: created a set of possible values Try to use with comprehensive lists, but it's not quite working and too slow. Appreciate any help. UPD In lists and df2 not always integer values, sometimes it's strings.", "q_apis": "values values add values add contains now values any values", "io": "**a**|**b** :-----:|:-----: 1|[10, 1, 'xxx'] 2|[] 5|[1, 2, 3] 7|[5] 9|[25, 27] <s> **a**|**b**|**c** :-----:|:-----:|:-----: 1|[10, 1, 'xxx']|[1,'xxx'] 2|[]|[] 5|[1, 2, 3]|[1] 7|[5]|[5] ", "apis": "map unique astype join map unique astype join", "code": ["l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall('|'.join([fr\"\\b{i}\\b\" for i in l]))\n", "l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall(fr\"\\b({'|'.join(l)})\\b\")\n"], "link": "https://stackoverflow.com/questions/67111775/python-pandas-check-each-element-in-list-values-of-column-to-exist-in-other-da"}
{"id": 309, "q": "Pivot Table in Pandas with two column(Index and Value)", "d": "I have a CSV file with and column. I need to sum values for each and have output like below Input: Output: I have tried below code,As I have just two column to apply I added column with unique value to have pivot(pivot table need Index,Column and Value).Then column is just to help. However out put is sum thing weird!!! output of my code:", "q_apis": "Index sum values apply unique value pivot pivot Index put sum", "io": "+-----+------+ | obj | \u00a0VS \u00a0| +-----+------+ | B \u00a0 | 2048 | | A \u00a0 | 1024 | | B \u00a0 | \u00a0 10 | | A \u00a0 | 1024 | | B \u00a0 | 1025 | | A \u00a0 | 1026 | | B \u00a0 | 1027 | +-----+------+ <s> +---+------+ | A | 3074 | +---+------+ | B | 4110 | +---+------+ ", "apis": "read_csv dtype value dtypes to_numeric dtypes pivot_table values index columns value sum", "code": ["import pandas as pd \nimport numpy as np \n\nfilename='1test.csv'\ndf = pd.read_csv(filename, dtype='str')\ndf[\"value\"]=1\nprint(df.dtypes)\ndf['VS']=pd.to_numeric(df['VS'])\nprint(df.dtypes)\npd.pivot_table(df, values=\"VS\", index=\"obj\", columns=\"value\", aggfunc=np.sum)\n"], "link": "https://stackoverflow.com/questions/61195022/pivot-table-in-pandas-with-two-columnindex-and-value"}
{"id": 303, "q": "Pandas interpolate NaNs from zero to next valid value", "d": "I am looking for a way to linear interpolate missing values (NaN) from zero to the next valid value. E.g.: Given this table, i want the output to look like this: I've tried using fillna to fill only the next NaN to a valid value to 0 and to then linear interpolate the whole dataframe. The problem I'm facing here is that specifying a value and a limit with fillna won't affect consecutive NaNs, but limit the total amount of columns to be filled. If possible please only suggest solutions without iterating over each row manually since I'm working with large dataframes. Thanks in advance.", "q_apis": "interpolate value interpolate values value fillna value interpolate value fillna columns", "io": " A B C D E 0 NaN 2.0 NaN NaN 0 1 3.0 4.0 NaN NaN 1 2 NaN NaN NaN NaN 5 3 NaN 3.0 NaN NaN 4 <s> A B C D E 0 NaN 2.0 0 0 0 1 3.0 4.0 0 0.5 1 2 NaN NaN NaN NaN 5 3 NaN 3.0 0 2 4 ", "apis": "notnull cummax isnull astype diff fillna update where eq loc cummin eq replace update update interpolate", "code": ["m = (df.notnull().cummax(axis=1) & df.isnull()).astype(int).diff(axis=1).fillna(0)\nupdate = m.where(m.eq(1) & m.loc[:, ::-1].cummin(axis=1).eq(-1)).replace(1, 0)\n\ndf.update(update)  # Add in 0s\n\ndf = df.interpolate(axis=1, limit_area='inside')\n"], "link": "https://stackoverflow.com/questions/61530817/pandas-interpolate-nans-from-zero-to-next-valid-value"}
{"id": 412, "q": "Python List to Pandas DataFrame with Number &amp; Strings", "d": "If I have a following list (this list need the separator for each comma); And also another list; How can i get this desire output with python? Could you please help me about this?", "q_apis": "DataFrame get", "io": "[(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)] <s> A B C D E F G H 5461 1.20 A BR SK-EL 7 EP 146 E 52 0 ", "apis": "DataFrame columns DataFrame columns", "code": ["df = pd.DataFrame([list(x) for x in data], columns=cols)\n", "df = pd.DataFrame([[x for x in data]], columns=cols)\n"], "link": "https://stackoverflow.com/questions/58448671/python-list-to-pandas-dataframe-with-number-strings"}
{"id": 624, "q": "Pandas delete all rows which contains &quot;required value&quot; in all column", "d": "I have the following dataframe I want to delete all rows which contains all column a V or N Output data frame will be :-", "q_apis": "delete all contains value all delete all contains all", "io": " A B C D BUY 150 Q 2018 SELL 63 Q 2018 N N N N V v v v SELL 53 Q 2018 <s> A B C D BUY 150 Q 2018 SELL 63 Q 2018 SELL 53 Q 2018 ", "apis": "isin isin all dtype bool isin all dtype bool", "code": ["print (df.isin(['V', 'v', 'N', 'n']))\n       A      B      C      D\n0  False  False  False  False\n1  False  False  False  False\n2   True   True   True   True\n3   True   True   True   True\n4  False  False  False  False\n", "print (df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0    False\n1    False\n2     True\n3     True\n4    False\ndtype: bool\n", "print (~df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/50717904/pandas-delete-all-rows-which-contains-required-value-in-all-column"}
{"id": 361, "q": "Change value of only 1 cell based on criteria DataFrame", "d": "Based on a condition, I want to change the value of the first row on a certain column, so far this is what I have So I want to change only the first value of the column recibos by the value on a where (despesas['despesas']==a) & (despesas['recibos']=='') Edit 1 Example: And the result should be:", "q_apis": "value DataFrame value first first value value where", "io": "despesas['despesas'] = [11.95, 2.5, 1.2 , 0.6 , 2.66, 2.66, 3. , 47.5 , 16.95,17.56] recibos['recibos'] = [11.95, 1.2 , 1.2 , 0.2 , 2.66, 2.66, 3. , 47.5 , 16.95, 17.56] <s> [[11.95, 11.95], [2.5, null] , [1.2, 1.2] , [0.6, null] , [2.66, 2.66], [2.66, 2.66], [3., 3] , [47.5, 45.5 ], [16.95, 16.95], [17.56, 17.56]] ", "apis": "count index iterrows loc iloc index drop loc index", "code": ["from itertools import count, filterfalse\n\ndespesas['recibos'] =''\nfor index, a in despesas.iterrows():\n    if len(recibos.loc[recibos['recibos']==a['despesas']])>0:\n        despesas.iloc[index,1]=True\n        recibos.drop(recibos.loc[recibos['recibos']==a['despesas']][:1].index, inplace=True)\n"], "link": "https://stackoverflow.com/questions/59900864/change-value-of-only-1-cell-based-on-criteria-dataframe"}
{"id": 51, "q": "How to convert rows into columns and filter using the ID", "d": "I have a CSV file that looks like this: and I would like to use simple python or pandas to: Make each unique customer id in a separate row convert key_id to the columns titles and the values are the quantity The output table should look like this: I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips?", "q_apis": "columns filter unique columns values filter", "io": "customer_id | key_id. | quantity | 1 | 777 | 3 | 1 | 888 | 2 | 1 | 999 | 3 | 2 | 777 | 6 | 2 | 888 | 1 | <s> | 777 | 888 | 999 | 1 | 3 | 2 | 3 | 2 | 6 | 1 | 0 | ", "apis": "pivot_table index columns values max fillna", "code": ["df.pivot_table(\n    index='customer_id',\n    columns='key_id',\n    values='quantity',\n    aggfunc='max',\n).fillna(0)\n"], "link": "https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id"}
{"id": 553, "q": "Swapping of elements in a PANDAS dataframe", "d": "Given below is a table : This was originally an excel sheet which has been converted into a dataframe. I wish to swap some of the elements such that the A number column has only 9999574081. Therefore the output should look like : This is the code I have used : However, I am not getting the desired result. Please help me out. Thanks:)", "q_apis": "", "io": " A NUMBER B NUMBER 7042967611 9999574081 12320 9999574081 9999574081 9810256463 9999574081 9716551924 9716551924 9999574081 9999574081 8130945859 <s> A NUMBER B NUMBER 9999574081 7042967611 9999574081 12320 9999574081 9810256463 9999574081 9716551924 9999574081 9716551924 9999574081 8130945859 ", "apis": "loc loc values where", "code": ["m = df['A NUMBER'] != 9999574081\n\ndf.loc[m, ['A NUMBER','B NUMBER']] = df.loc[m, ['B NUMBER','A NUMBER']].values\n", "df['B NUMBER'] = np.where(df['A NUMBER'] != 9999574081, df['A NUMBER'], df['B NUMBER'])\ndf['A NUMBER'] = 9999574081\n"], "link": "https://stackoverflow.com/questions/53611917/swapping-of-elements-in-a-pandas-dataframe"}
{"id": 212, "q": "Interpolates one series, and outputs constant for the second (constant) series", "d": "I'm trying to create a function that fills in missing numbers in multiple series, with different numerical scales, and at the same time generates a constant column for each of the series. Is it possible to create the following function with Pandas? Sample dataframe: Expected output:", "q_apis": "second at time", "io": "1029 400 1035 400 1031 340 1039 340 1020 503 1025 503 <s> 1029 400 1030 400 1031 400 1032 400 1033 400 1034 400 1035 400 1031 340 1032 340 1033 340 1034 340 1035 340 1036 340 1037 340 1038 340 1039 340 1020 503 1021 503 1022 503 1023 503 1024 503 1025 503 ", "apis": "min max set_index reindex ffill reset_index lt shift cumsum concat groupby", "code": ["def fill(d):\n    idx = range(d['col1'].min(), d['col1'].max() + 1)\n    return d.set_index('col1').reindex(idx, method='ffill').reset_index()\n\n\ng = df['col1'].lt(df['col1'].shift()).cumsum()\ndf = pd.concat([fill(g) for k, g in df.groupby(g)], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/63867276/interpolates-one-series-and-outputs-constant-for-the-second-constant-series"}
{"id": 237, "q": "How to to fuzzy merge items from a list that repeat many times python pandas", "d": "I have a one column df called ```logos''' consisting of the following list: (note I have searched for similar questions on stackoverflow to no avail I would like to merge with the following df that consists of each item, minus the .png filename I would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df I am wondering how I should go about this considering the and aren't identical, and item in the df I would like to merge with is listed multiple times. Is there such thing as a fuzzy join in python like in R? Thanks in advance for any help.", "q_apis": "merge items repeat merge item merge item time identical item merge join any", "io": "0 ARI 1 ARI 2 ARI 3 DEN 4 DEN 5 DEN <s> 0 ARI ARI.png 1 ARI ARI.png 2 ARI ARI.png 3 DEN DEN.png 4 DEN DEN.png 5 DEN DEN.png ", "apis": "merge left", "code": ["df = df.merge(logos, how='left', left_on='column_name', right_on='no_ext')\n"], "link": "https://stackoverflow.com/questions/63241112/how-to-to-fuzzy-merge-items-from-a-list-that-repeat-many-times-python-pandas"}
{"id": 116, "q": "Pandas: Replace missing dataframe values / conditional calculation: fillna", "d": "I want to calculate a pandas dataframe, but some rows contain missing values. For those missing values, i want to use a diffent algorithm. Lets say: If column B contains a value, then substract A from B If column B does not contain a value, then subtract A from C results in: Approach 1: fill the NaN rows using : which results in SyntaxError: cannot assign to function call. Approach 2: fill the NaN rows using : is executed without errors and calculation is correct, these values are printed to the console: but the values are not written into , the datafram remains as is: What is the correct way of overwriting the values?", "q_apis": "values fillna values values contains value value assign values values values", "io": "print(df) a b c calc 0 1 1.0 2 0.0 1 2 1.0 2 -1.0 2 3 NaN 2 NaN 3 4 1.0 2 -3.0 <s> print(df['calc']) 0 0.0 1 -1.0 2 NaN 3 -3.0 ", "apis": "index iterrows iloc index isnull loc index where value where isnull", "code": ["for index, row in df.iterrows():\n    i = df['calc'].iloc[index]\n\n    if pd.isnull(row['b']):\n        i = row['c']-row['a']\n        print(i)\n    else:\n        i = row['b']-row['a']\n        print(i)\n    df.loc[index,'calc'] = i #<------------- here\n", "Pandas where() method is used to check a data frame for one or more condition and return the result accordingly. By default, The rows not satisfying the condition are filled with NaN value.", "df['calc'] = np.where(df['b'].isnull(), df['c']-df['a'], df['calc'])\n"], "link": "https://stackoverflow.com/questions/65811195/pandas-replace-missing-dataframe-values-conditional-calculation-fillna"}
{"id": 495, "q": "Python: Count combinations of values within two columns and find max frequency of each combination", "d": "My pandas dataframe looks like this: First, I need to add another column containing the frequency of each combination of Section and Group. It is important to keep all rows. Desired output: The second step would be marking the highest value within Count for each Section. For example, with a column like this: The original data frame has lots of rows. That is why I'm asking for an efficient way because I cannot think of one. Thank you very much!", "q_apis": "values columns max add all second step value", "io": "+-----+---------+-------+ | No. | Section | Group | +-----+---------+-------+ | 123 | 222 | 1 | | 234 | 222 | 1 | | 345 | 222 | 1 | | 456 | 222 | 3 | | 567 | 241 | 1 | | 678 | 241 | 2 | | 789 | 241 | 2 | | 890 | 241 | 3 | +-----+---------+-------+ <s> +-----+---------+-------+-------+ | No. | Section | Group | Count | +-----+---------+-------+-------+ | 123 | 222 | 1 | 3 | | 234 | 222 | 1 | 3 | | 345 | 222 | 1 | 3 | | 456 | 222 | 3 | 1 | | 567 | 241 | 1 | 1 | | 678 | 241 | 2 | 2 | | 789 | 241 | 2 | 2 | | 890 | 241 | 3 | 1 | +-----+---------+-------+-------+ ", "apis": "groupby transform size groupby transform max", "code": ["df['Count']=df.groupby(['Section','Group'])['Group'].transform('size')\ndf['Max']=df.groupby(['Section'])['Count'].transform('max')==df['Count']\ndf\nOut[508]: \n    No  Section  Group  Count    Max\n0  123      222      1      3   True\n1  234      222      1      3   True\n2  345      222      1      3   True\n3  456      222      3      1  False\n4  567      241      1      1  False\n5  678      241      2      2   True\n6  789      241      2      2   True\n7  890      241      3      1  False\n"], "link": "https://stackoverflow.com/questions/49266390/python-count-combinations-of-values-within-two-columns-and-find-max-frequency-o"}
{"id": 37, "q": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?", "d": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this?", "q_apis": "concat iterrows DataFrame columns add get where names", "io": "A B C 4 c 12 5 d 19 2 b 43 <s> 0 A B C A 2 NaN NaN NaN B b NaN NaN NaN C 43 NaN NaN NaN 0 NaN 4.0 c 12.0 1 NaN 5.0 d 19.0 ", "apis": "concat", "code": ["pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa"}
{"id": 519, "q": "Transfer pandas dataframe column names to dictionary", "d": "I'm trying to convert a pandas dataframe column names into a dictionary. Not so worried about the actual data in the dataframe. Say I have an example dataframe like this and I'm not too worried about index just now: I'd like to get an output of a dictionary like: Not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order.", "q_apis": "names names index now get get keys name", "io": "Col1 Col2 Col3 Col4 -------------------- a b c a b d e c <s> {'Col1': 0, 'Col2': 1, 'Col3': 2, 'Col4': 3} ", "apis": "columns", "code": ["{c: i for i, c in enumerate(df.columns)}\n"], "link": "https://stackoverflow.com/questions/54674956/transfer-pandas-dataframe-column-names-to-dictionary"}
{"id": 304, "q": "How to delete the first and last rows with NaN of a dataframe and replace the remaining NaN with the average of the values below and above?", "d": "Let's take this dataframe as a simple example: I would like first to remove first and last rows until there is no longer NaN in the first and last row. Intermediate expected output : Then, I would like to replace the remaining NaN by the mean of the nearest value below which is not a NaN, and the one above. Final expected output : I know I can have the positions of NaN in my dataframe through But I can't solve my problem. How please could I do ?", "q_apis": "delete first last replace values take first first last first last replace mean value", "io": " Col1 Col2 Col3 1 1.0 1.0 1.0 2 1.0 NaN NaN 3 2.0 NaN 5.0 4 3.0 3.0 1.0 <s> Col1 Col2 Col3 0 1.0 1.0 1.0 1 1.0 2.0 3.0 2 2.0 2.0 5.0 3 3.0 3.0 1.0 ", "apis": "notnull all at at loc idxmax idxmax ffill bfill", "code": ["# identify the rows with some NaN\ns = df.notnull().all(1)\n\n# remove those with NaN at beginning and at the end:\nnew_df = df.loc[s.idxmax():s[::-1].idxmax()]\n\n# average:\nnew_df = (new_df.ffill()+ new_df.bfill())/2\n"], "link": "https://stackoverflow.com/questions/61351795/how-to-delete-the-first-and-last-rows-with-nan-of-a-dataframe-and-replace-the-re"}
{"id": 454, "q": "python: populating tuples in tuples over dataframe range", "d": "I have 4 portfolios a,b,c,d which can take on values either \"no\" or \"own\" over a period of time. (code included below to facilitate replication) Summary of schedule: What I have tried: create a holding dataframe and filling in values based on the schedule. Unfortunately the first portfolio 'a' gets overridden desired output: I am sure there's an easier way of achieving this but probably this is an example I haven't encountered before. Many thanks in advance!", "q_apis": "take values time values first", "io": " portf base st end 0 a no 2018-01-01 2018-01-02 1 a own 2018-01-03 2018-01-04 2 b no 2018-01-01 2018-01-05 3 b own 2018-01-06 2018-01-07 4 c own 2018-01-09 2018-01-10 5 d own 2018-01-09 2018-01-09 <s> 2018-01-01 (('a','no'), ('b','no')) 2018-01-02 (('a','no'), ('b','no')) 2018-01-03 (('a','own'), ('b','no')) 2018-01-04 (('a','own'), ('b','no')) 2018-01-05 ('b','no') ... ", "apis": "reset_index melt index date set_index date groupby index resample ffill drop columns index reset_index pivot index date columns resample first between", "code": ["cols = ['portf', 'base']\ns = (df.reset_index()\n       .melt(cols+['index'], value_name='date')\n       .set_index('date')\n       .groupby(cols+['index'], group_keys=False)\n       .resample('D').ffill()\n       .drop(columns=['variable', 'index'])\n       .reset_index())\n\nres = s.pivot(index='date', columns='portf')\nres = res.resample('D').first()  # Recover missing dates between\n"], "link": "https://stackoverflow.com/questions/56915574/python-populating-tuples-in-tuples-over-dataframe-range"}
{"id": 325, "q": "Fastest way to calculate in Pandas?", "d": "Given these two dataframes: has no column names, but you can assume column 0 is an offset of and column 1 is an offset of . I would like to transpose onto to get the Start and End differences. The final dataframe should look like this: I have a solution that works, but I'm not satisfied with it because it takes too long to run when processing a dataframe that has millions of rows. Below is a sample test case to simulate processing 30,000 rows. As you can imagine, running the original solution (method_1) on a 1GB dataframe is going to be a problem. Is there a faster way to do this using Pandas, Numpy, or maybe another package? UPDATE: I've added the provided solutions to the benchmarks. Output:", "q_apis": "names transpose get sample test", "io": "df1 = Name Start End 0 A 10 20 1 B 20 30 2 C 30 40 df2 = 0 1 0 5 10 1 15 20 2 25 30 <s> Name Start End Start_Diff_0 End_Diff_0 Start_Diff_1 End_Diff_1 Start_Diff_2 End_Diff_2 0 A 10 20 5 10 -5 0 -15 -10 1 B 20 30 15 20 5 10 -5 0 2 C 30 40 25 30 15 20 5 10 ", "apis": "to_numpy to_numpy", "code": ["a = df1[['Start','End']].to_numpy()\nb = df2[[0,1]].to_numpy()\n"], "link": "https://stackoverflow.com/questions/60843541/fastest-way-to-calculate-in-pandas"}
{"id": 533, "q": "Add numbers with duplicate values for columns in pandas", "d": "I have a data frame like this: I found that there is duplicate value and its pqr. I want to add 1,2,3 where pqr occurs. The final data frame I want to achieve is: How to do it in efficient way", "q_apis": "values columns value add where", "io": "df: col1 col2 1 pqr 3 abc 2 pqr 4 xyz 1 pqr <s> df1 col1 col2 1 pqr1 3 abc 2 pqr2 4 xyz 1 pqr3 ", "apis": "mask duplicated loc mask groupby cumcount add astype", "code": ["mask = df['col2'].duplicated(keep=False)\ndf.loc[mask, 'col2'] += df.groupby('col2').cumcount().add(1).astype(str)\n"], "link": "https://stackoverflow.com/questions/54105419/add-numbers-with-duplicate-values-for-columns-in-pandas"}
{"id": 1, "q": "Pandas: Loop through rows to update column value", "d": "Here is sample dataframe look like: From this dataFrame I want to update value. Condition is when or is not immediate next value of will be replaced by previous value afterthat next point value should be reindexed(cycle .1 to .6). eg. in row index(2) when So, the next value should be also 0.3 instead of 0.4, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively) OUTPUT I want: Code I tried:", "q_apis": "update value sample update value value value value index value index", "io": ">>> df point x y 0 0.1 NaN NaN 1 0.2 NaN NaN 2 0.3 5.0 NaN 3 0.4 NaN NaN 4 0.5 NaN 1.0 5 0.6 NaN NaN 6 0.7 1.0 1.0 7 0.8 NaN NaN 8 0.9 NaN NaN 9 1.1 NaN NaN 10 1.2 NaN NaN 11 1.3 NaN NaN 12 1.4 NaN 2.0 13 1.5 NaN NaN 14 1.6 NaN NaN 15 1.7 NaN NaN 16 0.1 NaN NaN 17 0.2 NaN NaN 18 0.3 NaN NaN 19 0.4 NaN NaN 20 0.5 NaN NaN 21 0.6 2.0 NaN 22 0.7 NaN NaN 23 1.1 NaN NaN <s> point x y 0 0.1 NaN NaN 1 0.2 NaN NaN 2 0.3 5.0 NaN 3 0.3 NaN NaN 4 0.4 NaN 1.0 5 0.4 NaN NaN 6 0.5 1.0 1.0 7 0.5 NaN NaN 8 0.6 NaN NaN 9 1.1 NaN NaN 10 1.2 NaN NaN 11 1.3 NaN NaN 12 1.4 NaN 2.0 13 1.4 NaN NaN 14 1.5 NaN NaN 15 1.6 NaN NaN 16 0.1 NaN NaN 17 0.2 NaN NaN 18 0.3 NaN NaN 19 0.4 NaN NaN 20 0.5 NaN NaN 21 0.6 2.0 NaN 22 0.6 NaN NaN 23 1.1 NaN NaN ", "apis": "mask any shift astype sub shift ne cumsum sub mask groupby cumsum div", "code": ["mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)\npoint = df['point'].astype(int)\ngroup = point.sub(point.shift(1)).ne(0).cumsum()\n\ndf['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))\n"], "link": "https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value"}
{"id": 639, "q": "Removing random rows from a data frame until count is equal some criteria", "d": "I have a dataframe with data that I feed to a ML library in python. The data I have is categorized into 5 different tasks, t1,t2,t3,t4,t5. The data I have right now for every task is uneven, to simplify things here is an example. In the case above, I want to remove random rows with the task label of \"t1\" until there is an equal amount of \"t1\" as there is \"t2\" So after the code is run, it should look like this: What is the most clean way to do this? I could of course just do for loops and if conditions and use random numbers and count the occurances for each iteration, but that solution would not be very elegant. Surely there must be a way using functions of dataframe? So far, this is what I got:", "q_apis": "count right now count", "io": "task, someValue t1, XXX t1, XXX t1, XXX t1, XXX t2, XXX t2, XXX <s> task, someValue t1, XXX t1, XXX t2, XXX t2, XXX ", "apis": "value_counts min groupby head", "code": ["v = df['task'].value_counts().min()\ndf = df.groupby('task', as_index=False).head(v)\n"], "link": "https://stackoverflow.com/questions/50003791/removing-random-rows-from-a-data-frame-until-count-is-equal-some-criteria"}
{"id": 35, "q": "How to create columns from a string in a dataframe?", "d": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.", "q_apis": "columns get get", "io": " long string 0 ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho... 1 hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho... 2 ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha... <s> ha-tra ha-la hi-tra hi-la ho-tra ho-la 0 1 2 1 2 1 2 1 1 2 1 2 1 2 2 1 2 1 2 1 2 ", "apis": "droplevel set_index append set_axis unstack swaplevel columns columns map join", "code": ["ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"], "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe"}
{"id": 550, "q": "Check one-on-one relationship between two columns", "d": "I have two columns A and B in a pandas dataframe, where values are repeated multiple times. For a unique value in A, B is expected to have \"another\" unique value too. And each unique value of A has a corresponding unique value in B (See example below in the form of two lists). But since each value in each column is repeated multiple times, I would like to check if any one-to-one relationship exists between two columns or not. Is there any inbuilt function in pandas to check that? If not, is there an efficient way of achieving that task? Example: Here, for each 1 in A, the corresponding value in B is always 5, and nothing else. Similarly, for 2-->10, and for 3-->12. Hence, each number in A has only one/unique corresponding number in B (and no other number). I have called this one-on-one relationship. Now I want to check if such relationship exists between two columns in pandas dataframe or not. An example where this relationship is not satisfied: Here, 1 in A doesn't have a unique corresponding value in B. It has two corresponding values - 5 and 7. Hence, the relationship is not satisfied.", "q_apis": "between columns columns where values unique value unique value unique value unique value value any between columns any value unique between columns where unique value values", "io": "A = [1, 3, 3, 2, 1, 2, 1, 1] B = [5, 12, 12, 10, 5, 10, 5, 5] <s> A = [1, 3, 3, 2, 1, 2, 1, 1] B = [5, 12, 12, 10, 5, 10, 7, 5] ", "apis": "groupby", "code": [" gb = d.groupby('A')\n grouped_b_column = gb['B']\n"], "link": "https://stackoverflow.com/questions/53687204/check-one-on-one-relationship-between-two-columns"}
{"id": 64, "q": "How to convert pandas dataframe into the numpy array with column names?", "d": "How can I convert pandas into the following Numpy array with column names? This is my pandas DataFrame : I tried to convert it as follows: But it gives me the output as follows: For some reason, the rows of data are grouped instead of .", "q_apis": "array names array names DataFrame", "io": "col1 col2 3 5 3 1 4 5 1 5 2 2 <s> [(3, 5), (3, 1), (4, 5), (1, 5), (1, 2)]", "apis": "to_records index array dtype", "code": [">>> df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '<i8'), ('B', '<f8')])\n"], "link": "https://stackoverflow.com/questions/66934423/how-to-convert-pandas-dataframe-into-the-numpy-array-with-column-names"}
{"id": 447, "q": "group rows (dates) and summarize serveral columns (several measured values for eacht date) in Python Pandas", "d": "I use Python Pandas and load a table like this from Postgres: I want to group the Date rows using Pandas and summarize the values. The result should look like this I can group the dates and summarize the values separately, but not in one view. My results are And Thats the Code:", "q_apis": "columns values date values values view", "io": "2001-01-01 00:00:00 500 2001-02-01 00:00:00 160 <s> 1 220 2 280 3 160 ", "apis": "sum sum reset_index name sum sum sum reset_index name sum", "code": ["df1 = df.sum(axis=1).sum(level=0).reset_index(name='sum')\n\ndf1 = df.sum(level=0).sum(axis=1).reset_index(name='sum')\n"], "link": "https://stackoverflow.com/questions/57234137/group-rows-dates-and-summarize-serveral-columns-several-measured-values-for-e"}
{"id": 18, "q": "python pandas - creating a column which keeps a running count of consecutive values", "d": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.", "q_apis": "count values count values", "io": ". binary consec 1 0 0 2 1 1 3 1 2 4 1 3 5 1 4 5 0 0 6 1 1 7 1 2 8 0 0 <s> . binary consec 0 1 NaN 1 1 1 2 1 1 3 0 0 4 1 1 5 0 0 6 1 1 7 1 1 8 1 1 9 0 0 ", "apis": "dtype bool", "code": [">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"], "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val"}
{"id": 126, "q": "Python - Pandas: get row indices for a particular value in a column", "d": "Given a pandas dataframe, is there a way to get the indices of rows where a column has particular values? Consider the following toy example: CSV (save as test1.csv) What I currently have is this: Is there an option/functionality that can give me something like the following? (I want to be able to do this for large value lists, fast!) Desired output:", "q_apis": "get indices value get indices where values value", "io": "id,val1,val2 1,20,A 1,19,A 1,23,B 2,10,B 2,10,A 2,14,A <s> id val1 val2 0 1 20 A 1 1 19 A 2 1 23 B 3 2 10 B 4 2 10 A 5 2 14 A [0, 1, 2] [3, 4, 5] ", "apis": "index groupby", "code": ["{k: list(d.index) for k, d in df.groupby('id')}\n"], "link": "https://stackoverflow.com/questions/65669445/python-pandas-get-row-indices-for-a-particular-value-in-a-column"}
{"id": 264, "q": "Pandas merging rows / Dataframe Transformation", "d": "I have this example DataFrame: How would I be able to convert it to this: I am thinking maybe I should pivot by counting with lens or assigning a index that could be multiple of 3, but I really am not sure what would be the most efficient way.", "q_apis": "DataFrame pivot index", "io": "e col1 col2 col3 1 238.4 238.7 238.2 2 238.45 238.75 238.2 3 238.2 238.25 237.95 4 238.1 238.15 238.05 5 238.1 238.1 238 6 229.1 229.05 229.05 7 229.35 229.35 229.1 8 229.1 229.15 229 9 229.05 229.05 229 <s> 1 2 3 col1 col2 col3 col1 col2 col3 col1 col2 col3 1 238.4 238.7 238.2 238.45 238.75 238.2 238.2 238.25 237.95 2 238.1 238.15 238.05 238.1 238.1 238 229.1 229.05 229.05 3 229.35 229.35 229.1 229.1 229.15 229 229.05 229.05 229 ", "apis": "pop sub unique concat set_index groupby keys", "code": ["g, k = df.pop('e').sub(1) % 3 + 1, np.unique(g)\ndf1 = pd.concat([g.set_index(k) for _, g in df.groupby(g)], keys=k, axis=1)\n"], "link": "https://stackoverflow.com/questions/62848641/pandas-merging-rows-dataframe-transformation"}
{"id": 418, "q": "calculating percentile values for each columns group by another column values - Pandas dataframe", "d": "I have a dataframe that looks like below - Python script to get the dataframe below - I want to calculate certain percentile values for all the columns grouped by 'Year'. Desired output should look like - I am running below python script to perform the calculations to calculate certain percentile values- Output - How can I get the output in the required format without having to do extra data manipulation/formatting or in fewer lines of code?", "q_apis": "values columns values get values all columns values get", "io": " Year Salary Amount 0 2019 1200 53 1 2020 3443 455 2 2021 6777 123 3 2019 5466 313 4 2020 4656 545 5 2021 4565 775 6 2019 4654 567 7 2020 7867 657 8 2021 6766 567 <s> Name Value 0 Salary_0.05 1208.9720 1 Salary_0.1 1217.9440 2 Salary_0.25 1244.8600 3 Salary_0.5 1289.7200 4 Salary_0.75 1334.5800 5 Salary_0.95 1370.4680 6 Salary_0.99 1377.6456 7 Amount_0.05 53.2800 8 Amount_0.1 53.5600 9 Amount_0.25 54.4000 10 Amount_0.5 55.8000 11 Amount_0.75 57.2000 12 Amount_0.95 58.3200 13 Amount_0.99 58.5440 ", "apis": "pivot columns quantile stack", "code": ["(df.pivot(columns='Year')\n   .quantile([0.01,0.05,0.75, 0.95, 0.99])\n   .stack('Year')\n)\n"], "link": "https://stackoverflow.com/questions/58274940/calculating-percentile-values-for-each-columns-group-by-another-column-values"}
{"id": 174, "q": "Generate unique key from multiple dataframes based on name", "d": "I have two data frames. As you can see, the function merges it correctly, but it is wrong. Because the carid must be unique and must not be assigned twice. How can I solve this problem? It can appear several times in a data frame, but it must remain unique over two data records. So across all data records and not What I want", "q_apis": "unique name unique unique all", "io": "Carid = 1 = Mercedes-benz <s> Cardid = 1 = Mercedes-Benz & Citroen", "apis": "name any unique groupby columns groupby name first to_dict unique get name delta items values get max max keys delta name name delta", "code": ["import pandas as pd\nimport numpy as np\nfrom collections import ChainMap\n\n\ndef generate_new_keys(*args,key='Carid',name='Carname'):\n    \"\"\"\n    Takes in a number of dataframes and returns any duplicates with a new unique id. \n    groupby columns fixed to CarID and CarName.\n    \"\"\"\n    # adds dictionaries into a single list.\n    dicts_ = [arg.groupby(key)[name].first().to_dict() for arg in args]\n    #merges dicts on unique key, this will exclude duplicates.\n    merged_dicts = dict(ChainMap(*dicts_))\n    #get the duplicate and pass the name into a list.\n    delta = [v for each_dict in dicts_ for k,v in each_dict.items() if v not in merged_dicts.values()]\n    # get the max sequence key\n    start_key =  max(merged_dicts.keys()) + 1\n    # create a new sequence\n    sequence = range(start_key, start_key + len(delta) + 1)\n    # return a dictionary.\n    return {name : number for name,number in zip(delta,sequence)}\n    \n"], "link": "https://stackoverflow.com/questions/64663456/generate-unique-key-from-multiple-dataframes-based-on-name"}
{"id": 549, "q": "Rearrange rows of Dataframe alternatively", "d": "I have a dataframe looks like this: and I want to make it look like this: My own way to do it seems to take quite a few lines, aka not pythonic. My code: Is there any more pythonic way to accomplish the same result? Thank you in advance. EDIT: I'd like a more general method to deal with dataframe like this", "q_apis": "take any", "io": " col1 col2 0 1 random string 1 -1 random string 2 2 random string 3 -2 random string 4 3 random string 5 -3 random string 6 4 random string 7 -4 random string 8 5 random string 9 -5 random string 10 6 random string 11 -6 random string 12 7 random string 13 -7 random string 14 8 random string 15 -8 random string 16 9 random string 17 -9 random string 18 10 random string 19 -10 random string <s> col1 col2 0 1 random string 1 2 random string 2 3 random string 3 4 random string 4 5 random string 5 1x random string 6 2x random string 7 3x random string 8 4x random string 9 5x random string 10 1y random string 11 2y random string 12 3y random string 13 4y random string 14 5y random string ", "apis": "index index index sort_index reset_index drop", "code": ["n=5\n\ndf.index = df.index%n + (df.index//n)/(len(df)/n)\ndf = df.sort_index().reset_index(drop=True)\n"], "link": "https://stackoverflow.com/questions/53769189/rearrange-rows-of-dataframe-alternatively"}
{"id": 142, "q": "Unprecise values when using pd.Dataframe.values.tolist", "d": "When converting a pd.DataFrame to a nested list, some values are unprecise. pd.DataFrame examplary row: pd.DataFrame.values.tolist() of this row: How can this be explained and avoided?", "q_apis": "values values DataFrame values DataFrame DataFrame values", "io": "1.0 -3.0 -3.0 0.01 -3.0 -1.0 <s> [1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0] ", "apis": "values", "code": ["df.values.tolist()\n# [[1.0], [-3.0], [-3.0], [0.010000000000000009], [-3.0], [-1.0]]\n"], "link": "https://stackoverflow.com/questions/65223498/unprecise-values-when-using-pd-dataframe-values-tolist"}
{"id": 287, "q": "Read a Text file having row in parenthesis and values separated by comma using pandas", "d": "I want to read a text file which contains data in parenthesis as row and values in it as column.The format of txt file is below : I want the data in this format : When i am reading text file as csv file it reads all the data in one row only. It shows 1 row and all the columns. Please help me with this problem.", "q_apis": "values contains values all all columns", "io": "(a, b, c, d) (a1, b1, (c1,c12,c13), d1) (a2, b2, (c2,c22,c23), d2) (a3, b3, (c3,c32,c33), d3) (a4, b4, (c4,c42,c43), d4) <s> a b c d a1 b1 c1 d1 a2 b2 c2 d2 a3 b3 c3 d3 a4 b4 c4 d4 ", "apis": "read_csv read_csv rename columns values columns values columns iloc iloc replace last", "code": ["# Use the standard `read_csv` function of pandas.\n# Note the lineterminator option.\ndf = pd.read_csv('data.dat', sep=\",\", lineterminator=\")\")\n# rename the 1st column (remove 1st char)\ndf.columns.values[0] = df.columns.values[0][1:]\n# remove the opening parenthesis for the 1st columns:\ndf.iloc[:, 0] = df.iloc[:, 0].str.replace('^\\ ?\\(', '')\n# remove the last line:\ndf = df[:-1]  \nprint(df)\n"], "link": "https://stackoverflow.com/questions/62112525/read-a-text-file-having-row-in-parenthesis-and-values-separated-by-comma-using-p"}
{"id": 615, "q": "Map, Filter and Reduce procedures in Python", "d": "I am working through understanding the concepts of map, filter and reduce in Python. I am working in Spyder IDE with Python v3.6. I have a data frame: I want to select Cap records in increments of .005. Please see below: In this case, wouldn't a map function work in this case? Any other option would be great. I ideally need it to work in a way where I can incrementally select the records based on a certain value.", "q_apis": "map filter select map where select value", "io": "Cap OC_y GMWB PE Acc 0.01 0.0065 0.560840708 0.646683673 0.515243902 0.0105 0.0068 0.586725664 0.676530612 0.53902439 0.011 0.0071 0.612610619 0.706377551 0.562804878 0.0115 0.0073 0.629867257 0.72627551 0.578658537 0.012 0.0076 0.655752212 0.756122449 0.602439024 0.0125 0.0079 0.681637168 0.785969388 0.626219512 0.013 0.0082 0.707522124 0.815816327 0.65 0.0135 0.0085 0.73340708 0.845663265 0.673780488 0.014 0.0087 0.750663717 0.865561224 0.689634146 0.0145 0.009 0.776548673 0.895408163 0.713414634 0.015 0.0093 0.802433628 0.925255102 0.737195122 <s> Cap OC_y GMWB PE Acc 0.01 0.0065 0.560840708 0.646683673 0.515243902 0.015 0.0093 0.802433628 0.925255102 0.737195122 ", "apis": "astype iat astype", "code": ["v = df.Cap / 0.005\nout = df[np.isclose(v, v.astype(int))]\n", "v = (df.Cap - (df.Cap % 0.005).iat[0]) / 0.005\nout = df[np.isclose(v, v.astype(int))]\n"], "link": "https://stackoverflow.com/questions/51141459/map-filter-and-reduce-procedures-in-python"}
{"id": 65, "q": "Duplicate rows and rename DataFrame indexes using a list of suffixes", "d": "I have a pandas DataFrame object as follow: for which I'd like to duplicate each using a list of suffixes: The list of suffixes is: and the list of indexes that must be changed is: . Notice , and are left untouched from the original DataFrame. From this answer: https://stackoverflow.com/a/50490890/6630397 I was able to duplicate the desired rows of my initial DataFrame to the right number according to the length of the list : But now all my DataFrame indices are an array of 10 of each , , and (except for , and where there are only 1 row) where I'd like them to follow the pattern of suffixes from as shown here above. How could I elegantly achieve that with good performances? (note: if it's much better to work from a column containing the indexes it's also fine, because my objects , , , , , and in the index column previously come from a standalone column named ).", "q_apis": "rename DataFrame DataFrame left DataFrame DataFrame right length now all DataFrame indices array where where index", "io": " P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 object A NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 C NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 D NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 E NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 F NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 G NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 <s> P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 object A_XS NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_S NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_M NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_L NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_XL NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_XS NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_S NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_M NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_L NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_XL NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 C_XS NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_S NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_M NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_L NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_XL NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 D_XS NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_S NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_M NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_L NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_XL NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 E NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 F NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 G NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 ", "apis": "index reset_index assign explode set_index", "code": ["newvals = [['{}{}'.format(i,j) for j in ('_' + k if k != '' else k\n                                         for k in list_of_suffixes)]\n           if i in list_init else [i] for i in df.index]\ndf.reset_index().assign(object=newvals).explode('object').set_index('object')\n"], "link": "https://stackoverflow.com/questions/66929674/duplicate-rows-and-rename-dataframe-indexes-using-a-list-of-suffixes"}
{"id": 34, "q": "Python - Delete lines from dataframe (pandas)", "d": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should..", "q_apis": "delete delete drop drop", "io": " 0 1 2 0 9783630876672 12,35 2.62 1 9783423282789 11,67 6.07 2 9783833879500 17,25 12.40 3 9783898798822 6,91 1.16 4 9783453281417 12,93 2.84 5 9783630876672 12,35 4.08 6 9783423282789 11,67 6.07 7 9783833879500 17,25 9.94 8 9783898798822 6,91 2.96 9 9783453281417 12,93 2.68 10 3927905909 /// /// 11 3872948210 /// 0.15 12 9783293003781 /// 0.15 13 9783423246842 /// /// 14 9783423247146 /// /// 15 9783423246934 /// /// 16 387294116x /// /// 17 9783935597456 0,16 0.15 18 9783423204545 /// /// <s> 0 1 2 0 9783630876672 12,35 2.62 1 9783423282789 11,67 6.07 2 9783833879500 17,25 12.40 3 9783898798822 6,91 1.16 4 9783453281417 12,93 2.84 5 9783630876672 12,35 4.08 6 9783423282789 11,67 6.07 7 9783833879500 17,25 9.94 8 9783898798822 6,91 2.96 9 9783453281417 12,93 2.68 ", "apis": "replace astype loc ge all columns", "code": ["data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"], "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas"}
{"id": 462, "q": "Converting DataFrame to dictionary with header as key and column as array with values", "d": "I have a dataframe as follows: I want a dictionary like this: I have tried using the normal and it is no where close. If I use the transposed dataframe, hence it gets close, but I have something like this: The questions in stack overflow are limited to the dictionary having one value per key, not an array. It would be very valuable for me to use and avoid any for loop, since the database I am using is quite big and I want the computational complexity to be as low as possible.", "q_apis": "DataFrame array values where stack value array any", "io": "A B C 1 6 1 2 5 7 3 4 9 4 2 2 <s> {0: {A: 1, B: 6, C:1} , ... , 4:{A: 4, B: 2, C: 2 } } ", "apis": "to_numpy items", "code": ["{k: v.to_numpy() for k, v in df.items()}\n"], "link": "https://stackoverflow.com/questions/56762170/converting-dataframe-to-dictionary-with-header-as-key-and-column-as-array-with-v"}
{"id": 644, "q": "Replace value in Pandas Dataframe based on condition", "d": "I have a dataframe column with some numeric values. I want that these values get replaced by 1 and 0 based on a given condition. The condition is that if the value is above the mean of the column, then change the numeric value to 1, else set it to 0. Here is the code I have now: The target is the dataframe y. y is like so: and so on. mean_y is equal to 3.55. Therefore, I need that all values greater than 3.55 to become ones, and the rest 0. I applied this loop, but without success: The output is the following: What am I doing wrong? Can someone please explain me the mistake? Thank you!", "q_apis": "value values values get value mean value now all values", "io": " 0 0 16 1 13 2 12.5 3 12 <s> 0 0 16 1 13 2 0 3 12 ", "apis": "where mean", "code": ["dataset.myCol = np.where(dataset.myCol > dataset.myCol.mean(), 1, 0)\n"], "link": "https://stackoverflow.com/questions/49857470/replace-value-in-pandas-dataframe-based-on-condition"}
{"id": 288, "q": "How to sum single row to multiple rows in pandas dataframe using multiindex?", "d": "My dataframe with Quarter and Week as MultiIndex: I am trying to add the last row in Q1 (Q1-W04) to all the rows in Q2 (Q2-W15 through Q2-W18). This is what I would like the dataframe to look like: When I try to only specify the level 0 index and sumthe specific row, all Q2 values go to NaN. I have figured out that if I specify both the level 0 and level 1 index, there is no problem. Is there a way to sum the specific row to all the rows within the Q2 Level 0 index without having to call out each row individually by its level 1 index? Any insight/guidance would be greatly appreciated. Thank you.", "q_apis": "sum MultiIndex add last all index all values index sum all index index", "io": "Quarter Week X Y Z Q1 Q1-W01 1 1 1 Q1-W02 2 2 2 Q1-W03 3 3 3 Q1-W04 4 4 4 Q2 Q2-W15 15 15 15 Q2-W16 16 16 16 Q2-W17 17 17 17 Q2-W18 18 18 18 <s> Quarter Week X Y Z Q1 Q1-W01 1 1 1 Q1-W02 2 2 2 Q1-W03 3 3 3 Q1-W04 4 4 4 Q2 Q2-W15 19 19 19 Q2-W16 20 20 20 Q2-W17 21 21 21 Q2-W18 22 22 22 ", "apis": "loc loc loc values", "code": ["df.loc['Q2'] = (df.loc['Q2'] + df.loc['Q1', 'Q1-W04']).values.tolist()\n"], "link": "https://stackoverflow.com/questions/62053788/how-to-sum-single-row-to-multiple-rows-in-pandas-dataframe-using-multiindex"}
{"id": 236, "q": "pandas dataframe delete groups with more than n rows in groupby", "d": "I have a dataframe: I want to apply groupby based on columns type1, type2 and delete from the dataframe the groups with more than 2 rows. So the new dataframe will be: What is the best way to do so?", "q_apis": "delete groups groupby apply groupby columns delete groups", "io": "df = [type1 , type2 , type3 , val1, val2, val3 a b q 1 2 3 a c w 3 5 2 b c t 2 9 0 a b p 4 6 7 a c m 2 1 8 a b h 8 6 3 a b e 4 2 7] <s> df = [type1 , type2 , type3 , val1, val2, val3 a c w 3 5 2 b c t 2 9 0 a c m 2 1 8 ] ", "apis": "groupby filter", "code": ["df =df.groupby(['type1','type2']).filter(lambda x: len(x) <= 2) \n"], "link": "https://stackoverflow.com/questions/63259726/pandas-dataframe-delete-groups-with-more-than-n-rows-in-groupby"}
{"id": 392, "q": "How to create rows for unique values in columns in pandas?", "d": "I have a pandas dataframe with thousands of rows like so: I need all unique values in \"IntentName\" to have the same IntentID value like so: What is the easiest way to do this?", "q_apis": "unique values columns all unique values value", "io": "IntentID IntentName Query Response 1 Intent Name 1 Query 1 Response1 2 Intent Name 1 Query 1 Response2 3 Intent Name 2 Query 2 Response3 4 Intent Name 2 Query 2 Response4 5 Intent Name 3 Query 3 Response5 <s> IntentID IntentName Query Response 1 Intent Name 1 Query 1 Response1 1 Intent Name 1 Query 1 Response2 2 Intent Name 2 Query 2 Response3 2 Intent Name 2 Query 2 Response4 3 Intent Name 3 Query 3 Response5 ", "apis": "groupby transform first rank astype", "code": ["df['IntentID'] = df.groupby('IntentName') \\\n                    ['IntentID'].transform('first') \\\n                    .rank(method='dense') \\\n                    .astype('int')\n"], "link": "https://stackoverflow.com/questions/59149281/how-to-create-rows-for-unique-values-in-columns-in-pandas"}
{"id": 95, "q": "In panda python how do I &quot;blacklist&quot; or &quot;whitelist&quot; numbers in a DataFrame", "d": "I have two DataFrames, and I want to add a new column to the df with the first allowed numbers in df1, but only as many as are in each group, when it starts again at 1 it needs to look at the first number in Allowed_numbers. and want this: I have tried a few things and get this I was also considering some kind of mapping numbers against eachother, since 1 in order_out will always be 3 in y_goals. the order_out might also have different lengths of number row, not always up to 9.", "q_apis": "DataFrame add first at at first get", "io": " order_y y_goal 0 1 3 1 2 4 2 3 5 3 4 6 4 5 8 5 6 9 6 7 12 7 8 15 8 9 17 9 1 3 10 2 4 11 3 5 12 4 6 13 5 8 14 6 9 15 7 12 16 8 15 17 9 17 ... <s> order_out y_goal 0 1 3.0 1 2 4.0 2 3 5.0 3 4 6.0 4 5 8.0 5 6 9.0 6 7 12.0 7 8 15.0 8 9 17.0 9 1 24.0 10 2 28.0 11 3 29.0 12 4 30.0 13 5 NaN 14 6 NaN 15 7 NaN 16 8 NaN 17 9 NaN ... ", "apis": "reset_index drop", "code": ["df['y_goal'] = y_goal.reset_index(drop = True)\n"], "link": "https://stackoverflow.com/questions/66229269/in-panda-python-how-do-i-blacklist-or-whitelist-numbers-in-a-dataframe"}
{"id": 157, "q": "Merge multiple columns into one by placing one below the other based on column value pandas dataframe", "d": "I have the following dataframe df: Where the row is the row with the names of the columns in the dataframe (i.e. the row with bold font that states the names of each column). What I want is to rearrange this dataframe so that the output is this: I have tried searching different ways to append, concatenate, merge etc. the columns in the way that I want but I can't figure out how since there are multiple instances of each Video, i.e. multiple . So, for each of these multiple instances, I want to make one column of these with the Video number as the column name, and the rows be all the confidence values, as shown above. Is that possible?", "q_apis": "columns value names columns names append merge columns name all values", "io": "Video 1 1 1 1 1 1 1 1 1 1 ... 36 36 36 36 36 36 36 36 36 36 Confidence Value 3 3 4 4 4 5 5 3 5 3 ... 3 3 3 2 4 2 3 3 3 3 <s> Video 1 2 3 ... 36 0 3 5 4 ... 3 1 1 2 3 ... 2 2 2 4 4 ... 5 3 4 5 4 ... 3 ... ", "apis": "transpose groupby cumcount values transpose set_index pivot columns values transpose set_index pivot columns values", "code": ["idx = df.transpose().groupby(\"Video\").cumcount().values\nans = df.transpose().set_index(idx).pivot(columns=\"Video\", values=\"Confidence Value\")\n", "ans = df.transpose().set_index(np.tile(range(5), 4)).pivot(columns=\"Video\", values=\"Confidence Value\")\n"], "link": "https://stackoverflow.com/questions/64969344/merge-multiple-columns-into-one-by-placing-one-below-the-other-based-on-column-v"}
{"id": 188, "q": "Select highest member of close coordinates saved in pandas dataframe", "d": "I have a dataframe that has following columns: X and Y are Cartesian coordinates and Value is the value of element at these coordinates. What I want to achieve is to select only one coordinates out of that are close to other, lets say coordinates are close if distance is lower than some value , so the initial DF looks like this (example): distance is count with following function: lets say if we want to , the output dataframe would look like this: What is to be done: So I need to go through dataframe row by row, check the rest, select best match and then continue. I can't think about any simple method how to achieve this, this cant be use case of , since they are not duplicates, but looping over the whole DF will be very inefficient. One method I could think about was to loop just once, for each of rows finds close ones (probably apply countdistance()), select the best fitting row and replace rest with its values, in the end use . The other idea was to create a recursive function that would create a new DF, then while original df will have rows select first, find close ones, best match append to new DF, remove first row and all close from original DF and continue until empty, then return same function with new DF as to remove possible uncaught close points. These ideas are all kind of inefficient, is there a nice and efficient pythonic way to achieve this?", "q_apis": "columns value at select value count select any apply select replace values select first append first all empty all", "io": " X Y Value 0 0 0 6 1 0 1 7 2 0 4 4 3 1 2 5 4 1 6 6 5 5 5 5 6 6 6 6 7 7 4 4 8 8 8 8 <s> X Y Value 1 0 1 7 4 1 6 6 8 8 8 8 ", "apis": "columns copy copy empty all reset_index drop loc first loc add select select index select select idxmax select select loc select idxmax get append DataFrame select iloc add drop now overlaps all", "code": ["def recModif(self,df):\n  #columns=['','X','Y','Value']\n  new_df = df.copy()\n  new_df = new_df[new_df['Value']<0] #create copy to work with\n  changed = False\n  while not df.empty: #for all the data\n    df = df.reset_index(drop=True) #need to reset so 0 is always accessible\n    x = df.loc[0,'X'] #first row x and y\n    y = df.loc[0,'Y']\n    df['dist'] = self.countDistance(x,y,df['X'],df['Y']) #add column with distances\n    select = df[df['dist']<10] #number of meters that two elements cant be next to other \n    if(len(select.index)>1): #if there is more than one elem close\n      changed = True\n      #print(select,select['Value'].idxmax())\n    select = select.loc[[select['Value'].idxmax()]] #get the highest one\n    new_df = new_df.append(pd.DataFrame(select.iloc[:,:3]),ignore_index=True) #add it to new df\n    df = df[df['dist'] >= 10] #drop the elements now\n\n  if changed:\n    return self.recModif(new_df) #use recursion if possible overlaps\n  else: \n    return new_df #return new df if all was OK\n"], "link": "https://stackoverflow.com/questions/64464184/select-highest-member-of-close-coordinates-saved-in-pandas-dataframe"}
{"id": 8, "q": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column", "d": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.", "q_apis": "take names time get all names time all names at all all", "io": "NAME VAL1 VAL2 VAL3 player1 3 5 7 player2 2 6 8 player3 3 6 7 NAME VAL1 VAL2 VAL3 player2 5 7 7 player3 2 6 8 player5 3 6 7 <s> NAME VAL1 VAL2 VAL3 player1 3 5 7 NAME VAL1 VAL2 VAL3 player2 2 6 8 player2 5 7 7 NAME VAL1 VAL2 VAL3 player3 3 6 7 player3 2 6 8 NAME VAL1 VAL2 VAL3 player5 3 6 7 ", "apis": "concat groupby concat groupby", "code": ["dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n", "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"], "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base"}
{"id": 586, "q": "pandas dataframe take rows before certain indexes", "d": "I have a dataframe and a list of indexes, and I want to get a new dataframe such that for each index (from the given last), I will take the all the preceding rows that matches in the value of the given column at the index. The column c3 the indexes (row numbers) 2, 4 , 5 my new dataframe will be: Explanation: For index 2, rows 0,1,2 were selected because C3 equals in all of them. For index 4, no preceding row is valid. And for index 5 also no preceding row is valid, and row 6 is irrelevant because it is not preceding. What is the best way to do so?", "q_apis": "take get index last take all value at index index equals all index index", "io": " C1 C2 C3 0 1 2 A 1 3 4 A 2 5 4 A 3 7 5 B 4 9 7 C 5 2 3 D 6 1 1 D <s> C1 C2 C3 0 1 2 A 1 3 4 A 2 5 4 A 4 9 7 C 5 2 3 D ", "apis": "loc loc loc loc dropna loc loc loc dropna", "code": ["ind= 2\ncol ='C3'\n# \".loc[np.arange(ind+1)]\" creates indexes till preceding row, so rest of matching conditions can be ignored \ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n", "ind= 2\ncol ='C2'\ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n"], "link": "https://stackoverflow.com/questions/52354096/pandas-dataframe-take-rows-before-certain-indexes"}
{"id": 241, "q": "Way to show multiple spaces in Pandas Dataframe on Jupyter Notebook", "d": "When displaying Pandas Dataframe object on notebook, multiple spaces are shown as single space. And I cannot copy multiple spaces. I would like to show all spaces as they are and copy a string of them. Any way to do so? Actual My expectation", "q_apis": "copy all copy", "io": " 0 1 0 ab c ab c 1 ab c ab c <s> 0 1 0 ab c ab c 1 ab c ab c ", "apis": "style", "code": ["def print_df(df):\n    return df.style.set_properties(**{'white-space': 'pre'})\n\nprint_df(df)\n"], "link": "https://stackoverflow.com/questions/62283545/way-to-show-multiple-spaces-in-pandas-dataframe-on-jupyter-notebook"}
{"id": 90, "q": "Merge padas rows if the difference between consecutive rows are less than two", "d": "I have a data frame like this, the df is sorted by col1. Now for each col1 values of the previous or/and next row if difference between consecutive col3 values are less than 2 then merge col2 values in a single row. So the data frame would look like, This could be done using for loop by filtering col1 values each time but it will take more time to execute, looking for some pandas shortcuts to do it most efficiently.", "q_apis": "difference between values difference between values merge values values time take time", "io": "df col1 col2 col3 A [p,s] 2 A [q] 3 A [r,t] 4 A [p,x] 7 B [x,y] 8 C [s] 4 C [t,v] 6 C [u,x] 7 <s> df col1 col2 A [p,s,q,r,t] A [p,x] B [x,y] C [s] C [t,v,u,x] ", "apis": "groupby apply diff ge cumsum", "code": ["df['g'] = df.groupby('col1')['col3'].apply(lambda x: x.diff().ge(2).cumsum())\n"], "link": "https://stackoverflow.com/questions/66277212/merge-padas-rows-if-the-difference-between-consecutive-rows-are-less-than-two"}
{"id": 491, "q": "Split list element in dataframe over multiple rows", "d": "I have a df. I would like to get to a second dataframe, with only scalar values in . If and only if the original values was a list, I would like to spread it over multiple new rows, with the other values duplicated. e.g from: to: In Split set values from Pandas dataframe cell over multiple rows, I have learned how to do this for one columns, but how to handle the case where df has multiple columns which need to be duplicated as ?", "q_apis": "get second values values values duplicated values columns where columns duplicated", "io": " id foo 0 301 [a] 1 301 [b, c] 2 302 [e, f,33,'Z'] 3 303 42 <s> id foo 0 301 a 1 301 b 1 301 c 2 302 e 2 302 f 2 302 33 2 302 Z 3 303 42 ", "apis": "DataFrame values repeat pop apply Series stack reset_index drop rename join reset_index drop apply Series stack reset_index drop rename drop join reset_index drop DataFrame index T concat apply Series stack reset_index drop rename drop join reset_index drop mean std DataFrame values repeat mean std", "code": ["df['foo']  = [x if isinstance(x, list) else [x] for x in df['foo']]\n\nfrom itertools import chain\n\ndf = pd.DataFrame({\n    'id' : df['id'].values.repeat(df['foo'].str.len()),\n    'foo' : list(chain.from_iterable(df['foo'].tolist()))\n\n})\n", "s = df.pop('foo').apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.join(s).reset_index(drop=True)\n", "s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.drop('foo', axis=1).join(s).reset_index(drop=True)\n", "df=pd.DataFrame(data=[[301,301,302,303],[['a'],['b','c'],['e','f',33,'Z'],42]],index=['id','foo']).T\n\ndf = pd.concat([df] * 1000, ignore_index=True)\n\ndef f(df):\n    s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\n    return df.drop('foo', axis=1).join(s).reset_index(drop=True)\n\n\nIn [241]: %timeit (f(df))\n814 ms \u00b1 11.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [242]: %%timeit\n     ...: L  = [x if isinstance(x, list) else [x] for x in df['foo']]\n     ...: \n     ...: from itertools import chain\n     ...: \n     ...: pd.DataFrame({\n     ...:     'id' : df['id'].values.repeat([len(x) for x in L]),\n     ...:     'foo' : list(chain.from_iterable(L))\n     ...: \n     ...: })\n     ...: \n2.6 ms \u00b1 15.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/55829605/split-list-element-in-dataframe-over-multiple-rows"}
{"id": 298, "q": "Get column index nr from value in other column", "d": "I'm relativly new to python and pandas, so I might not have the full understanding of all possibilities and would appreciate a hint how to solve the following problem: I have a like this one: I want to construct a column which takes the column with the index according to and then multiplies the value in the selected column with the value in . I want to bulid a table like this ( beeing the column constructed): (row (), in row () and in row ()) The value in will always be an integer value, so i would love to use the position of a column according to the value in .", "q_apis": "index value all index value value value value value", "io": " Jan Feb Mar Apr i j a 100 200 250 100 1 0.3 b 120 130 90 100 3 0.7 c 10 30 10 20 2 0.25 <s> Jan Feb Mar Apr i j k a 100 200 250 100 1 0.3 60 b 120 130 90 100 3 0.7 70 c 10 30 10 20 2 0.25 2.5 ", "apis": "mul rename columns columns columns lookup index", "code": ["df['k'] = df['j'].mul(df.rename(columns = dict(zip(df.columns,\n                                                   range(len(df.columns)))))\n                        .lookup(df.index, df['i']))\nprint(df)\n"], "link": "https://stackoverflow.com/questions/60720542/get-column-index-nr-from-value-in-other-column"}
{"id": 600, "q": "How to slice rows from two pandas dataframes then merge them with some other value", "d": "I got two pandas dataframes and two indexes, and one datetime variable. What I would like to do is: slice the dataframes with the indexes, then I got two rows. combine the two rows to one row. add the variable to the row. then I can get new indexes and datetime values to form more rows, and assemble the rows to a new dataframe. Example: df1: df2: index: 3, 5, datetime: Output:", "q_apis": "merge value combine add get values index", "io": " A B 0 0 10 1 1 11 2 2 12 3 3 13 4 4 14 5 5 15 6 6 16 7 7 17 8 8 18 9 9 19 <s> C D 0 10 110 1 11 111 2 12 112 3 13 113 4 14 114 5 15 115 6 16 116 7 17 117 8 18 118 9 19 119 ", "apis": "index iloc index values iloc index values DataFrame columns date", "code": ["index = [3,5]\ndata = np.r_[df1.iloc[index[0]].values,df2.iloc[index[1]].values]\ndf = pd.DataFrame([data],columns = list('ABCD'))\ndt = datetime.datetime(2018, 8, 10, 16, 53, 52, 760014)\ndf['date'] = dt\n"], "link": "https://stackoverflow.com/questions/51780710/how-to-slice-rows-from-two-pandas-dataframes-then-merge-them-with-some-other-val"}
{"id": 133, "q": "Append loop output in column pandas python", "d": "I am working with the code below to append output to empty dataframe i am getting output as below but i want What i want the output to be How can i make 3 rows to 3 columns every time the loop repeats.", "q_apis": "append empty columns time", "io": " 0 0 30708 1 15 2 1800 0 19200 1 50 2 1180 <s> 0 1 2 0 30708 15 1800 1 19200 50 1180 ", "apis": "append DataFrame DataFrame", "code": ["# image_data starts as a list\nimage_data = []\n\nfor i in words:\n    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)\n    image_data.append(y)\n\n# And it ends as a DataFrame\nimage_data = pd.DataFrame(image_data)\n"], "link": "https://stackoverflow.com/questions/65530626/append-loop-output-in-column-pandas-python"}
{"id": 53, "q": "Moving data from rows to columns based on another column", "d": "I have a huge dataset with contents such as given below: HHID can be present in the file at a maximum of three times. If HHID is found once, then the VAL_CD64/VAL_CD32 should be moved to VAL1_CD64/VAL1_CD32 columns, if found 2nd time, second value should be moved to VAL2_CD64/VAL2_CD32 columns, and if found 3rd time, third value should be moved to VAL3_CD64/VAL3_CD32 columns. If value is not found, then these columns should be left blank. Output should look something like this: I tried using pivot/melt in pandas but unable to get an idea to implement it. Can anyone help in giving me a lead? Thanks", "q_apis": "columns at columns time second value columns time value columns value columns left pivot melt get", "io": "+------+------------------------------------------------------------------+----------------------------------+--+ | HHID | VAL_CD64 | VAL_CD32 | | +------+------------------------------------------------------------------+----------------------------------+--+ | 203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 373aeda34c0b4ab91a02ecf55af58e15 | | | 203 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | 6f3606577eadacef1b956307558a1efd | | | 203 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 332321ab150879e930869c15b1d10c83 | | | 720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | 2c4f97a04f02db5a36a85f48dab39b5b | | | 720 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | 2111293e946703652070968b224875c9 | | | 348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 5c80a555fcda02d028fc60afa29c4a40 | | | 348 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 6c10cd11b805fa57d2ca36df91654576 | | | 348 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 6040b29107adf1a41c4f5964e0ff6dcb | | | 403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | 71a91c4768bd314f3c9dc74e9c7937e8 | | +------+------------------------------------------------------------------+----------------------------------+--+ <s> +------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+ | HHID | VAL1_CD64 | VAL2_CD64 | VAL3_CD64 | VAL1_CD32 | VAL2_CD32 | VAL3_CD32 | | +------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+ | 203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 373aeda34c0b4ab91a02ecf55af58e15 | 6f3606577eadacef1b956307558a1efd | 332321ab150879e930869c15b1d10c83 | | | 720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | | 2c4f97a04f02db5a36a85f48dab39b5b | 2111293e946703652070968b224875c9 | | | | 348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 5c80a555fcda02d028fc60afa29c4a40 | 6c10cd11b805fa57d2ca36df91654576 | 6040b29107adf1a41c4f5964e0ff6dcb | | | 403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | | | 71a91c4768bd314f3c9dc74e9c7937e8 | | | | +------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+ ", "apis": "groupby agg columns DataFrame values index index columns map columns append concat", "code": ["df_ = df.groupby('HHID').agg({'VAL_CD32': list, 'VAL_CD64': list})\n\ndata = []\nfor col in df_.columns:\n    d = pd.DataFrame(df_[col].values.tolist(), index=df_.index)\n    d.columns = [f'{col}_{i}' for i in map(str, range(1, len(d.columns)+1))]\n    data.append(d)\n\nres = pd.concat(data, axis=1)\n"], "link": "https://stackoverflow.com/questions/67179293/moving-data-from-rows-to-columns-based-on-another-column"}
{"id": 154, "q": "Recovering DataFrame MultiIndex (from both row and column) after groupby", "d": "I have a dataframe that is multi indexed in that manner. I want to apply a function to each of its columns, for each date (so the 89.583458 and 49.828466 go together, 9.328360 and 10.058943 go together, and so forth) This gives me But now I need to recover the lost indices (to get back the same structure as the original), but failed at setting , unstacking or using pd.MultiIndex.from_frame. Any idea? Perhaps there's a better to get exactly that from the call?", "q_apis": "DataFrame MultiIndex groupby apply columns date now indices get at MultiIndex from_frame get", "io": " Value Size A B Market Cap 2019-07-01 AAPL 89.583458 9.328360 2.116356e+06 AMGN 49.828466 10.058943 1.395518e+05 2019-10-01 AAPL 74.297570 11.237253 2.116356e+06 AMGN 56.841946 10.237481 1.395518e+05 2019-12-31 AAPL 97.435257 14.736749 2.116356e+06 AMGN 71.400903 12.859612 1.395518e+05 <s> Market Cap ... B 2019-07-01 [[139551.76568603513], [139551.76568603513]] ... [[49.828465616227064], [49.828465616227064]] 2019-10-01 [[139551.76568603513], [139551.76568603513]] ... [[56.84194615992103], [56.84194615992103]] 2019-12-31 [[139551.76568603513], [139551.76568603513]] ... [[71.40090272484755], [71.40090272484755]] ", "apis": "date date date date date date index to_datetime get date get columns DataFrame get get get index index columns columns sub array sub sub values sub sub sub groupby apply groups groupby apply", "code": ["import pandas\nfrom scipy.stats.mstats import winsorize\n\n# Recreating your dataframe\ndata = [\n    {\"date\": \"2019-07-01\", \"group\": \"AAPL\", \"A\": 89.583458, \"B\": 9.328360, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-07-01\", \"group\": \"AMGN\", \"A\": 49.828466, \"B\": 10.058943, \"Market Cap\": 1.395518e+05},\n    {\"date\": \"2019-10-01\", \"group\": \"AAPL\", \"A\": 74.297570, \"B\": 11.237253, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-10-01\", \"group\": \"AMGN\", \"A\": 56.841946, \"B\": 10.237481, \"Market Cap\": 1.395518e+05},\n    {\"date\": \"2019-12-31\", \"group\": \"AAPL\", \"A\": 97.435257, \"B\": 14.736749, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-12-31\", \"group\": \"AMGN\", \"A\": 71.400903, \"B\": 12.859612, \"Market Cap\": 1.395518e+05},\n]\nindex = [\n    [pandas.to_datetime(line.get(\"date\")) for line in data],\n    [line.get(\"group\") for line in data],\n]\ncolumns = [\n    [\"Value\", \"Value\", \"Size\"],\n    [\"A\", \"B\", \"Market Cap\"]\n]\ndf = pandas.DataFrame(data=[[line.get(\"A\"), line.get(\"B\"), line.get(\"Market Cap\")] for line in data], index=index, columns=columns)\n\n\n# Your lambda function in a separate definition\ndef process_group(group):\n\n    # Nested\n    def _sub(sub):\n        # winsorize returns an numpy array, sub is a dataframe; sub[:] replaces the \"values\" of the dataframe, not the dataframe itself\n        sub[:] = winsorize(a=sub, limits=[0.4, 0.6])  # I didn't know your limits so I've guessed...\n        return sub\n\n    # Return the result of the processing on the nested group\n    return group.groupby(level=1, axis=1).apply(_sub)\n\n# Process the groups\ndf = df.groupby(level=0, axis=0).apply(process_group)\n"], "link": "https://stackoverflow.com/questions/64936694/recovering-dataframe-multiindex-from-both-row-and-column-after-groupby"}
{"id": 301, "q": "Parsing a list of lists to a data frame in pandas", "d": "I have list of lists. Below is how my list looks like, I want to parse it into a data frame with continuation of values with columns = A,B,C The expected data frame is as below Really appreciate the help.", "q_apis": "parse values columns", "io": "[ A B C 0 1 2 3 1 1 2 3 2 1 2 3 3 1 2 3 A B C 0 4 5 6 1 4 5 6 2 4 5 6 3 4 5 6 ] <s> A B C 0 1 2 3 1 1 2 3 2 1 2 3 3 1 2 3 4 4 5 6 5 4 5 6 6 4 5 6 7 4 5 6 ", "apis": "count count count append count concat", "code": ["import pandas as pd\nlist1=[]\ncount=0\nwhile count<len(myList):\n    list2= myList[count]\n    list1.append(list2)\n    #print(list2)\n    count+=1\ndf = pd.concat(list1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/61556827/parsing-a-list-of-lists-to-a-data-frame-in-pandas"}
{"id": 622, "q": "select individual rows from multiindex pandas dataframe", "d": "I am trying to select individual rows from a multiindex dataframe using a list of multiindices. For example. I have got the following dataframe: I would like to select all 'C' with (A,B) = [(1,1), (2,2)] My flawed code for this is as follows:", "q_apis": "select select select all", "io": " Col1 A B C 1 1 1 -0.148593 2 2.043589 2 3 -1.696572 4 -0.249049 2 1 5 2.012294 6 -1.756410 2 7 0.476035 8 -0.531612 <s> Col1 A B C 1 1 1 -0.148593 2 2.043589 2 2 7 0.476035 8 -0.531612 ", "apis": "join query", "code": ["filters = [(1,1), (2,2)]\nfilterstr = '|'.join(f'(A=={i})&(B=={j})' for i, j in filters)\nres = df.query(filterstr)\n\nprint(filterstr)\n\n(A==1)&(B==1)|(A==2)&(B==2)\n"], "link": "https://stackoverflow.com/questions/50890844/select-individual-rows-from-multiindex-pandas-dataframe"}
{"id": 584, "q": "Convert list of dict in dataframe to CSV", "d": "I have a dataframe that looks like this (df1): The detail column contains a list of dictionaries and each dictionary looks like this: I need to extract this information into a CSV with this format: In addition, the x2 and y2 in the extracted data should be like this: Expected output (Assuming the dict provided is in the first row of df1): I have tried this code to create a new df based off the detail column but I got an error: Error:", "q_apis": "contains first", "io": "{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5} <s> 1054, 78, 602, 549, 1232, 1113, 1, 5, 0.0 ", "apis": "DataFrame index columns loc loc", "code": ["a = pd.DataFrame(index=[78],columns=[\"detail\"])\na.loc[78,\"detail\"] = [{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\na.loc[188,\"detail\"] = [{'y1': 649, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\n"], "link": "https://stackoverflow.com/questions/52489565/convert-list-of-dict-in-dataframe-to-csv"}
{"id": 428, "q": "Conditional pairwise calculations in pandas", "d": "For example, I have 2 dfs: df1 and another df is df2 I want to calculate first pairwise subtraction from df2 to df1. I am using using a function Now, I want to check, these new columns name like and . I am checking if there is any values in this new less than 5. If there is, then I want to do further calculations. Like this. For example, here for columns name , less than 5 value is 4 which is at . Now in this case, I want to subtract columns name of but at row 3, in this case it would be value . I want to subtract this value 2 with but at row 1 (because column name ) was from value at row 1 in . My is so complex for this. It would be great, if there would be some easier way in pandas. Any help, suggestions would be great. The expected new dataframe is this", "q_apis": "first columns name any values columns name value at columns name at value value at name value at", "io": "ID,col1,col2 1,5,9 2,6,3 3,7,2 4,8,5 <s> 0,1,2 Nan,Nan,Nan Nan,Nan,Nan (2-9)=-7,Nan,Nan (5-9)=-4,(5-7)=-2,Nan ", "apis": "DataFrame where values values index index columns columns", "code": ["pd.DataFrame(np.where(dist_df<5, df1.col2.values[:,None] - df2.col2.values, np.nan),\n             index=dist_df.index,\n             columns=dist_df.columns)\n"], "link": "https://stackoverflow.com/questions/57926657/conditional-pairwise-calculations-in-pandas"}
{"id": 379, "q": "Python: Convert two columns of dataframe into one interposed list", "d": "How can I convert two columns in a dataframe into an interposed list? ex: I want to do something like Closest I've found is but that returns a bunch of tuples in the list like this:", "q_apis": "columns columns", "io": ">>> [1, 2, 3, 4, 5, 6, 0, -1] <s> [(1, 2), (3, 4), (5, 6), (0, -1)]", "apis": "values array", "code": ["df.values.flatten()                                                                                                                                                                  \n# array([ 1,  2,  3,  4,  5,  6,  0, -1])\n"], "link": "https://stackoverflow.com/questions/59482469/python-convert-two-columns-of-dataframe-into-one-interposed-list"}
{"id": 193, "q": "Find the number of mutual friends in Python", "d": "I have a dataframe of users and their friends that looks like: I want to write a function in to compute the number of mutual friends for each pair: Currently I have: It works fine for small datasets, but I'm running it on a dataset with millions of rows. It takes forever to run everything. I know it's not the ideal way to find the count. Is there a better algorithm in Python? Thanks in advance!", "q_apis": "count", "io": "user_id | friend_id 1 3 1 4 2 3 2 5 3 4 <s> user_id | friend_id | num_mutual 1 3 1 1 4 1 2 3 0 2 5 0 3 4 1 ", "apis": "", "code": ["import networkx as nx\ng = nx.from_pandas_edgelist(df, \"user_id\",\"friend_id\")\nnx.draw_networkx(g)\n"], "link": "https://stackoverflow.com/questions/64405516/find-the-number-of-mutual-friends-in-python"}
{"id": 180, "q": "Is there a way to combine 9,12 or 15 columns from a single df into just 3?", "d": "I'm trying to convert a df that has the data divided every 3 columns into just three. An example is from this: To this:", "q_apis": "combine columns columns", "io": "C1 C2 C3 C4 C5 C6 C7 C8 C9 1 6 9 A D G 1A 6A 9A 2 7 10 B E H 2A 7A 10A 3 8 11 C F I 3A 8A 11A <s> C1 C2 C3 1 6 9 2 7 10 3 8 11 C4 C5 C6 A D G B E H C F I C7 C8 C9 1A 6A 9A 2A 7A 10A 3A 8A 11A ", "apis": "columns values values DataFrame", "code": ["arr = np.hsplit(np.vstack([df.columns.values, df.values]), 3)\npd.DataFrame(np.vstack(arr))\n"], "link": "https://stackoverflow.com/questions/64599227/is-there-a-way-to-combine-9-12-or-15-columns-from-a-single-df-into-just-3"}
{"id": 284, "q": "How to merge or join a stacked dataframe in pandas?", "d": "I cannot find this question answered elsewhere; I would like to do a SQL-like join in pandas but with the slight twist that one dataframe is stacked. I have created a dataframe A with a stacked column index from a csv file in pandas that looks as follows: The original csv had repeated what is in the 1st column for every entry like so: The original csv was the transposed version of this. Pandas chose to stack that when converting to dataframe. (I used this code: pd.read_csv(file, header = [0,1], index_col=0).T) In another csv/dataframe B I have for all of those so-called ticker symbols another ID that I would rather like to use: CIK. Desired output: I would like to have the CIK instead of the ticker in a new dataframe otherwise identical to A. Now in SQL I could easily join on A.name_of_2nd_column = b.Ticker since the table would just have the entry in the 1st column repeated in every line (like the original csv) and the column would have a name but in pandas I cannot. I tried this code: How do I tell pandas to use the 2nd column as the key and/or interpret the first column just as repeated values?", "q_apis": "merge join join index stack read_csv T all identical join name first values", "io": "| | | 2013-01-04 | 2013-01-07 | |----------:|-----:|-----------:|-----------:| | Adj Close | OWW | NaN | NaN | | Close | OXLC | 4.155157 | 4.147217 | | | OXM | 40.318089 | 42.988800 | | | OXY | 50.416079 | 62.934800 | <s> | | | 2013-01-04 | 2013-01-07 | |----------:|-----:|-----------:|-----------:| | Adj Close | OWW | NaN | NaN | | Close | OXLC | 4.155157 | 4.147217 | | Close | OXM | 40.318089 | 42.988800 | | Close | OXY | 50.416079 | 62.934800 | ", "apis": "reset_index assign map value dropna astype", "code": ["ticker_2_CIK = dict(zip(tix.Ticker,tix.CIK))  # create a dict\n\ntmp = df.reset_index().assign(CIK=lambda x: x['ticker'].map(ticker_2_CIK)) # use dict to find the correct value for colum \n\n# data was unclean, some ticker symbols were created after the period my data is from \n# and data was incomplete with some tickers missing\nsolution = tmp.dropna(subset=['CIK']).astype({'CIK':int})\n"], "link": "https://stackoverflow.com/questions/62011392/how-to-merge-or-join-a-stacked-dataframe-in-pandas"}
