{"id": 332, "q": "Removing Duplicate values from a Cell of DataFramein python", "d": "DataFrame Output I want Any Help will be Appreciated", "q_apis": "values DataFrame", "io": "ID Source 1 [192.168.1.121, 10.1.161.10, 192.168.1.121, 192.168.1.121] 2 [192.168.1.121, 10.1.161.10, 10.1.161.10, 10.1.161.10, 192.168.1.121] 3 [192.168.1.121, 192.168.1.121, 192.168.1.121] 4 [10.1.161.10, 192.168.1.121, 10.1.161.10, 10.1.161.10] <s> ID Source 1 192.168.1.121, 10.1.161.10 2 192.168.1.121, 10.1.161.10 3 192.168.1.121 4 10.1.161.10, 192.168.1.121 ", "apis": "apply join unique", "code": ["df['source'] = df['source'].apply(\nlambda x: ', '.join(pd.unique(x)),\n)\n"], "link": "https://stackoverflow.com/questions/60709527/removing-duplicate-values-from-a-cell-of-dataframein-python"}
{"id": 274, "q": "Vectorizing for-loop", "d": "I have a very large dataframe (~10^8 rows) where I need to change some values. The algorithm I use is complex so I tried to break down the issue into a simple example below. I mostly programmed in C++, so I keep thinking in for-loops. I know I should vectorize but I am new to python and very new to pandas and cannot come up with a better solution. Any solutions which increase performance are welcome. Any ideas? EDIT: I was ask to explain what I do with my for-loops. For every eventID I want to know if all corresponding types contain a 1 or a 0 or both. If they contain a 1, all values which are equal to -1 should be changed to 1. If the values are 0, all values equal to -1 should be changed to 0. My problem is to do this efficiently for each eventID independently. There can be one or multiple entries per eventID. Input of example: Output of example:", "q_apis": "where values all all values values all values", "io": " eventID types 0 1 0 1 1 -1 2 1 -1 3 2 -1 4 2 1 5 3 0 6 4 0 7 5 0 8 6 -1 9 6 -1 10 6 -1 11 6 1 12 7 -1 13 8 -1 <s> eventID types 0 1 0 1 1 0 2 1 0 3 2 1 4 2 1 5 3 0 6 4 0 7 5 0 8 6 1 9 6 1 10 6 1 11 6 1 12 7 -1 13 8 -1 ", "apis": "eq groupby transform any eq groupby transform any select", "code": ["m1 = mydf['types'].eq(1).groupby(mydf['eventID']).transform('any')\nm2 = mydf['types'].eq(0).groupby(mydf['eventID']).transform('any')\nmydf['types'] = np.select([m1 , m2], [1, 0], mydf['types'])\n"], "link": "https://stackoverflow.com/questions/62536086/vectorizing-for-loop"}
{"id": 100, "q": "Python find closest neighbors to a value in a dataframe", "d": "I have a dataframe or list. I want to find the closest values and their index to a given value. My code: Present output (val_idx): Expected output (val_idx):", "q_apis": "value values index value", "io": " num 1 24 0 20 <s> num 2 35 1 24 ", "apis": "DataFrame get test get index argmin test where", "code": ["import numpy as np\nimport pandas as pd\n\n# setup\ndf = pd.DataFrame({'num':[20,24,35,38]})\nval = 26 \n\n# subtract to get differences\ntest = np.absolute(np.subtract(val, df[\"num\"]))\n\n# get index\nidx = np.argmin(test)\n# Condition\nidx = np.where(df[\"num\"][idx] > val, [idx-1, idx], [idx, idx+1])\n\nprint(idx)\n"], "link": "https://stackoverflow.com/questions/66128637/python-find-closest-neighbors-to-a-value-in-a-dataframe"}
{"id": 52, "q": "How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?", "d": "I have two dataframes: df1 df2 df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be: ABD123 = 1 (because it has 1 different character from ABC123) DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one) GHI789 = 3 (because it has an equivalent at df1) I know how to check for the differences of each code individually and save the \"length\" of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way?", "q_apis": "compare difference between values get item codes codes at length apply compare all", "io": " Code Number 0 ABC123 1 1 DEF456 2 2 GHI789 3 3 DEA456 4 <s> Code 0 ABD123 1 DEA458 2 GHI789 ", "apis": "iterrows iterrows difference compare difference", "code": ["for index2, row2 in df2.iterrows():\n    for index1, row1 in df1.iterrows():\n        difference = compare(row2,row1)\n        #do something with the difference.\n"], "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf"}
{"id": 543, "q": "Column dupe renaming in pandas", "d": "I have the following csv file of data: Pandas currently renames this to: Is there a way to customize how this is renamed? For example, I would prefer:", "q_apis": "", "io": " id number id.1 0 132605 1 1 1 132750 2 1 <s> id number id2 0 132605 1 1 1 132750 2 1 ", "apis": "replace count count count count columns", "code": ["from collections import defaultdict\nimport csv\n\n# replace StringIO(file) with open('file.csv', 'r')\nwith StringIO(file) as fin:\n    headers = next(csv.reader(fin))\n\ndef rename_duplicates(original_cols):\n    count = defaultdict(int)\n    for x in original_cols:\n        count[x] += 1\n        yield f'{x}{count[x]}' if count[x] > 1 else x\n\ndf.columns = rename_duplicates(headers)\n"], "link": "https://stackoverflow.com/questions/53939072/column-dupe-renaming-in-pandas"}
{"id": 167, "q": "Pandas resample column based on other column", "d": "I have a similar dataframe: And I want to resample this dataframe such that x values with the same y value is averaged. In other words: I've looked into the pandas.DataFrame.resample function, but not sure how to do this without timestamps.", "q_apis": "resample resample values value DataFrame resample", "io": "x | y 1 | 1 3 | 1 3 | 1 4 | 1 5 | 2 5 | 2 9 | 2 8 | 2 <s> x | y (1+3+3+4)/4 | 1 (5+5+9+8)/4 | 2 ", "apis": "DataFrame groupby mean reset_index", "code": ["import pandas\ndf = pandas.DataFrame({\"x\":[1,3,3,4,5,5,9,8],\"y\":[1,1,1,1,2,2,2,2]})\ndf.groupby([\"y\"]).mean().reset_index()\n"], "link": "https://stackoverflow.com/questions/64766331/pandas-resample-column-based-on-other-column"}
{"id": 465, "q": "Negating column values and adding particular values in only some columns in a Pandas Dataframe", "d": "Taking a Pandas dataframe df I would like to be able to both take away the value in the particular column for all rows/entries and also add another value. This value to be added is a fixed additive for each of the columns. I believe I could reproduce df, say dfcopy=df, set all cell values in dfcopy to the particular numbers and then subtract df from dfcopy but am hoping for a simpler way. I am thinking that I need to somehow modify So for example of how this should look: Then negating only those values in columns (0,3,4) and then adding 10 (for example) we would have: Thanks.", "q_apis": "values values columns take value all add value value columns all values values columns", "io": " A B C D E 0 1.0 3.0 1.0 2.0 7.0 1 2.0 1.0 8.0 5.0 3.0 2 1.0 1.0 1.0 1.0 6.0 <s> A B C D E 0 9.0 3.0 1.0 8.0 3.0 1 8.0 1.0 8.0 5.0 7.0 2 9.0 1.0 1.0 9.0 4.0 ", "apis": "iloc iloc iloc iloc", "code": ["df.iloc[:, [0,2,7,10,11] = -df.iloc[:, [0,2,7,10,11]\n", "df.iloc[:, [0,2,7,10,11] = df.iloc[:, [0,2,7,10,11]+c\n"], "link": "https://stackoverflow.com/questions/56723207/negating-column-values-and-adding-particular-values-in-only-some-columns-in-a-pa"}
{"id": 321, "q": "choosing rows by values in DataFrame", "d": "A post gives a way to choose rows by column value Here is a DataFrame with this code , I got when I run this , I got error this code gives This is close, what I am trying to get is a new DataFrame consists of rows at [2,4,6,8,9]. How to do that? Thanks to anyone who gives some inspiration.", "q_apis": "values DataFrame value DataFrame get DataFrame at", "io": " 0 1 0 877.443401 808.520962 1 826.300620 848.761594 2 824.403359 861.395174 3 866.732033 804.494156 4 853.461260 874.307851 5 822.906499 830.102249 6 852.605652 863.602725 7 893.421600 825.032893 8 863.768363 862.298227 9 899.976622 864.111539 <s> 0 1 0 NaN NaN 1 NaN NaN 2 NaN 861.395174 3 NaN NaN 4 NaN 874.307851 5 NaN NaN 6 NaN 863.602725 7 NaN NaN 8 NaN 862.298227 9 NaN 864.111539 ", "apis": "loc copy query", "code": ["new_df = df.loc[df[1] > 850].copy()\n", "new_df = df.query('a > 850')\n"], "link": "https://stackoverflow.com/questions/60994122/choosing-rows-by-values-in-dataframe"}
{"id": 628, "q": "Creating single row pandas dataframe", "d": "I have a dataframe like this- I want to create a new dataframe which looks like this-", "q_apis": "", "io": " 0 0 a 43 1 b 630 2 r 587 3 i 462 4 g 153 5 t 266 <s> a b r i g t 0 43 630 587 462 153 266 ", "apis": "columns DataFrame columns T reset_index drop rename columns iloc drop index", "code": ["df.columns = ['col']\ndf = pd.DataFrame(df.col.str.split(' ',1).tolist(), columns = ['col1','col2']).T.reset_index(drop=True)\ndf = df.rename(columns=df.iloc[0]).drop(df.index[0])\n"], "link": "https://stackoverflow.com/questions/50642233/creating-single-row-pandas-dataframe"}
{"id": 13, "q": "Getting first/second/third... value in row of numpy array after nan using vectorization", "d": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks.", "q_apis": "first second value array values left where first value left array left values array first value array second value", "io": "f(offset=0) | 0 | 1 | | -- | -- | | 1 | 25 | | 2 | 29 | | 3 | 33 | | 4 | 31 | | 5 | 30 | | 6 | 35 | | 7 | 31 | | 8 | 33 | | 9 | 26 | | 10 | 27 | | 11 | 35 | | 12 | 33 | | 13 | 28 | | 14 | 25 | | 15 | 25 | | 16 | 26 | | 17 | 34 | | 18 | 28 | | 19 | 34 | | 20 | 28 | <s> f(offset=1) | 0 | 1 | | -- | --- | | 1 | nan | | 2 | nan | | 3 | nan | | 4 | 35 | | 5 | 34 | | 6 | 34 | | 7 | 26 | | 8 | 25 | | 9 | 31 | | 10 | 26 | | 11 | 25 | | 12 | 35 | | 13 | 25 | | 14 | 25 | | 15 | 26 | | 16 | 31 | | 17 | 29 | | 18 | 29 | | 19 | 26 | | 20 | 30 | ", "apis": "argmax clip shape shape any shape stack groupby nth reindex index concat to_numpy mean std mean std mean std", "code": ["def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n", "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n", "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"], "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector"}
{"id": 456, "q": "Pandas: Remove index entry (and all it&#39;s rows) from multilevel index when all data in a column is NaN", "d": "I'd like to clean up some data I have in a dataframe with a multilevel index. I'd like to loose the complete group indexed by bar, because all of the data in column A is NaN. I'd like to keep foo, because only some of the data in column A is NaN (column B is not important here, even if it's all NaN). I'd like to keep baz, because not all of column Ais NaN. So my result should look like this: What's the best way to do this with pandas and python? I suppose there is a better way than looping through the data...", "q_apis": "index all index all index all all all", "io": " | A | B | ----------------+-----+-----+ foo 2019-01-01 | x | NaN | 2019-01-02 | x | NaN | 2019-01-03 | NaN | NaN | ................+.....+.....+ bar 2019-01-01 | NaN | x | 2019-01-02 | NaN | y | 2019-01-03 | NaN | z | ................+.....+.....+ baz 2019-01-01 | x | x | 2019-01-02 | x | x | 2019-01-03 | x | x | <s> | A | B | ----------------+-----+-----+ foo 2019-01-01 | x | NaN | 2019-01-02 | x | NaN | 2019-01-03 | NaN | NaN | ................+.....+.....+ baz 2019-01-01 | x | x | 2019-01-02 | x | x | 2019-01-03 | x | x | ", "apis": "notna groupby transform any dtype bool", "code": ["m = df['A'].notna().groupby(level=0).transform('any')\nprint(m)\n\nidx  idx2      \nfoo  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nbar  2019-01-01    False\n     2019-01-02    False\n     2019-01-03    False\nbaz  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nName: A, dtype: bool\n"], "link": "https://stackoverflow.com/questions/56869643/pandas-remove-index-entry-and-all-its-rows-from-multilevel-index-when-all-da"}
{"id": 328, "q": "Merge columns with have \\n", "d": "ex) I'm merging columns, but I want to give '\\n\\n' in the merging process. so output what I want I want 'nan' to drop. I tried However, this includes all nan values. thank you for reading.", "q_apis": "columns columns drop all values", "io": " C1 C2 C3 C4 C5 C6 0 A B nan C A nan 1 B C D nan B nan 2 D E F nan C nan 3 nan nan A nan nan B <s> C 0 A B C A 1 B C D B 2 D E F C 3. A B ", "apis": "merge stack groupby agg join filter columns merge filter stack groupby agg join merge apply join dropna filter columns merge filter apply join dropna", "code": ["df['merge'] = df.stack().groupby(level=0).agg('\\n\\n'.join)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').stack().groupby(level=0).agg('\\n\\n'.join)\n", "df['merge'] = df.apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n"], "link": "https://stackoverflow.com/questions/60825821/merge-columns-with-have-n"}
{"id": 198, "q": "how to create new dataframe by combining some columns together of existing one?", "d": "I am having a dataframe df like shown: where the explanation of the columns as the following: the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3. I would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups. The new dataframe I need is like the shown:", "q_apis": "columns where columns first second groups groups max any groups", "io": "1-1 1-2 1-3 2-1 2-2 3-1 3-2 4-1 5-1 10 3 9 1 3 9 33 10 11 21 31 3 22 21 13 11 7 13 33 22 61 31 35 34 8 10 16 6 9 32 5 4 8 9 6 8 <s> 1 2 3 4 5 10 3 33 10 11 31 22 13 7 13 61 35 34 10 16 32 5 9 6 8 ", "apis": "groupby max", "code": ["df1 = df.groupby(lambda x: x.split('-')[0], axis=1).max()\n"], "link": "https://stackoverflow.com/questions/64242508/how-to-create-new-dataframe-by-combining-some-columns-together-of-existing-one"}
{"id": 472, "q": "Combine pandas dataframes eliminating common columns with python", "d": "I have 3 dataframes: I want to combine them together to get the following results: When I try to combine them, I keep getting: The common column (A) is duplicated once for each dataframe used in the concat call. I have tried various combinations on: Some variations have been disastrous while some keep giving the undesired result. Any suggestions would be much appreciated. Thanks.", "q_apis": "columns combine get combine duplicated concat", "io": " A B C D E F 0 A0 B0 C0 D0 E0 F0 1 A1 B1 C1 D1 E1 F1 2 A2 B2 C2 D2 E2 F2 3 A3 B3 C3 D3 E3 F3 <s> A B C D A E A F 0 A0 B0 C0 D0 A0 E0 A0 F0 1 A1 B1 C1 D1 A1 E1 A1 F1 2 A2 B2 C2 D2 A2 E2 A2 F2 3 A3 B3 C3 D3 A3 E3 A3 F3 ", "apis": "concat set_index reset_index", "code": ["df4 = (pd.concat((df.set_index('A') for df in (df1,df2,df3)), axis=1)\n         .reset_index()\n      )\n"], "link": "https://stackoverflow.com/questions/56572543/combine-pandas-dataframes-eliminating-common-columns-with-python"}
{"id": 168, "q": "pandas dataframe select list value from another column", "d": "Everyone! I have a pandas dataframe like this: as we can see, the A column is a list and the B column is an index value. I want to get a C column which is index by B from A: Is there any elegant method to solve this? Thank you!", "q_apis": "select value index value get index any", "io": " A B 0 [1,2,3] 0 1 [2,3,4] 1 <s> A B C 0 [1,2,3] 0 1 1 [2,3,4] 1 3 ", "apis": "to_numpy apply", "code": ["df['C'] = [x[y] for x, y in df[['A','B']].to_numpy()]\n", "df['C'] = df.apply(lambda x: x.A[x.B], axis=1)\n"], "link": "https://stackoverflow.com/questions/64765350/pandas-dataframe-select-list-value-from-another-column"}
{"id": 207, "q": "How to create bins for a dataframe column if the range is given", "d": "This is an example data frame that I want to play with If I do this, I get the output as: Here's the twist: Let's say the age columns can take values between 18 to 58(the range of the column) and I want the bins(or the output) as: How can I do that? because 'cut' takes the values which are in the column. I got the desired result by doing it manually but if the values of bins were say 100 - how can I do it?", "q_apis": "get columns take values between cut values values", "io": "0 (17.964, 25.2] 1 (17.964, 25.2] 2 (25.2, 32.4] 3 (25.2, 32.4] 4 (32.4, 39.6] 5 (32.4, 39.6] 6 (39.6, 46.8] 7 (46.8, 54.0] 8 (46.8, 54.0] <s> 0 (18.0, 26.0] 1 (18.0, 26.0] 2 (26.0, 34.0] 3 (26.0, 34.0] 4 (34.0, 42.0] 5 (34.0, 42.0] 6 (34.0, 42.0] 7 (50.0, 58.0] 8 (42.0, 50.0] ", "apis": "cut", "code": ["pd.cut(df['Age'], bins=np.linspace(18, 58, 100), include_lowest=True)\n"], "link": "https://stackoverflow.com/questions/64110166/how-to-create-bins-for-a-dataframe-column-if-the-range-is-given"}
{"id": 231, "q": "Sort Pandas dataframe column index by date", "d": "I want to sort dataframe by column index. The issue is my columns are 'dates' dd/mm/yyyy directly imported from my excel. For ex: The output I want is: I am using It is giving me following error: TypeError: '<' not supported between instances of 'datetime.datetime' and 'str' I want to do it in panda dataframe. Any help will be appreciated. Thanks", "q_apis": "index date index columns between", "io": " 10/08/20 12/08/20 11/08/20 0 2.0 6.0 15.0 1 6.0 11.0 8.0 2 4.0 7.0 3.0 3 7.0 12.0 2.0 4 12.0 5.0 7.0 <s> 10/08/20 11/08/20 12/08/20 0 2.0 15.0 6.0 1 6.0 8.0 11.0 2 4.0 3.0 7.0 3 7.0 2.0 12.0 4 12.0 7.0 5.0 ", "apis": "columns Series columns apply strptime sort_index", "code": ["import datetime as dt\ndf.columns=pd.Series(df.columns).apply(lambda d: dt.datetime(d, dt.datetime.strptime(d, '%d/%m/%Y')))\ndf.sort_index(axis = 1)\n"], "link": "https://stackoverflow.com/questions/63440751/sort-pandas-dataframe-column-index-by-date"}
{"id": 408, "q": "Python Round Dataframe Columns with Specific Value If Exists", "d": "My input dataframe; I want to round my dataframe columns according to a specifi value if exists. My code is like below; Output is; The issue is if there is no \"rounding\" variable, it should be run automatically as default (0.5). I need a code that can run for both together. Something like this or different; I saw many topics about rounding with specific value but i couldn' t see for this. Could you please help me about this?", "q_apis": "round columns value value", "io": "A B 0.3 0.6 0.4 3.05 1.6 4.35 0.15 5.47 4.19 9.99 <s> A B 1 1 1 3 2 5 0 6 4 10 ", "apis": "isna", "code": ["rounding = np.nan\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0\n", "rounding = 0 if pd.isna(rounding) else rounding\nprint (rounding)\n0\n", "rounding = 0.25\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0.25\n"], "link": "https://stackoverflow.com/questions/58539757/python-round-dataframe-columns-with-specific-value-if-exists"}
{"id": 443, "q": "Identify increasing features in a data frame", "d": "I have a data frame that present some features with cumulative values. I need to identify those features in order to revert the cumulative values. This is how my dataset looks (plus about 50 variables): What I wish to achieve is: I've seem this answer, but it first revert the values and then try to identify the columns. Can't I do the other way around? First identify the features and then revert the values? Finding cumulative features in dataframe? What I do at the moment is run the following code in order to give me the feature's names with cumulative values: Afterwards, I save these features names manually in a list called cum_features and revert the values, creating the desired dataset: Is there a better way to solve my problem?", "q_apis": "values values first values columns values at names values names values", "io": "a b 346 17 76 52 459 70 680 96 679 167 246 180 <s> a b 346 17 76 35 459 18 680 26 679 71 246 13 ", "apis": "diff dropna all dtype bool", "code": ["out = (df.diff().dropna()>0).all()\n#Output:\na     True\nb    False\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/57376287/identify-increasing-features-in-a-data-frame"}
{"id": 300, "q": "Python DataFrame Data Analysis of Large Amount of Data from a Text File", "d": "I have the following code: I am using a text file (that is not formatted) to pull chunks of data from. When the text file is opened, it looks something like this, except on a way bigger scale: Here are the things I'm having trouble doing with this data: I only need the second, third, sixth, and seventh columns of data. The issue with this one, I believe I've solved with my code above by reading the individual lines and creating a dataframe with the columns necessary. I am open to suggestions if anyone has a better way of doing this. I need to skip the first row of data. This one, the open feature doesn't have a skiprows attribute, so when I drop the first row, I also lose my index starting at 0. Is there any way around this? I need the resulting dataframe to look like a nice clean dataframe. As of right now, it looks something like this: Everything is right-aligned under the column and it looks strange. Any ideas how to solve this? I also need to be able to perform Statistic Analysis on the columns of data, and to be able to find the Name with the highest data and the lowest data, but for some reason, I always get errors because I think that, even though I've got all the data set up as a dataframe, the values inside the dataframe are reading as objects instead of integers, strings, floats, etc. So, if my data is not analyzable using Python functions, does anyone know how I can fix this to make the data be able to run correctly? Any help would be greatly appreciated. I hope I've laid out all of my needs clearly. I am new to Python, and I'm not sure if I'm using all the proper terminology.", "q_apis": "DataFrame second columns columns first drop first index at any right now right columns get all values all all", "io": "00 2381 1.3 3.4 1.8 265879 Name 34 7879 7.6 4.2 2.1 254789 Name 45 65824 2.3 3.4 1.8 265879 Name 58 3450 1.3 3.4 1.8 183713 Name 69 37495 1.3 3.4 1.8 137632 Name 73 458913 1.3 3.4 1.8 138024 Name <s> Col1 Col2 Col3 Col4 2381 3.4 265879 Name 7879 4.2 254789 Name 65824 3.4 265879 Name 3450 3.4 183713 Name 37495 3.4 137632 Name 458913 3.4 138024 Name ", "apis": "read_csv names", "code": ["keep = ['col1', 'col3', 'col5', 'col6']\ndf = pd.read_csv('txt2pd.txt', \n                 sep='\\s+', \n                 names=['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6'], \n                 skiprows=1)\ndf = df[keep]\n"], "link": "https://stackoverflow.com/questions/61580297/python-dataframe-data-analysis-of-large-amount-of-data-from-a-text-file"}
{"id": 496, "q": "Sort DataFrame column with given input list", "d": "Hi I want to sort DataFrame column with given input list values. My list looks like : And DataFrame is : Here I want to sort DataFrame column 'val' on basis of given 'inputlist'. I am expecting following output :", "q_apis": "DataFrame DataFrame values DataFrame DataFrame", "io": " val kaywords 195 keyword3 221 keyword5 307 keyword8 309 keyword9 354 keyword0 426 keyword1 585 keyword2 698 keyword4 789 keyword33 <s> val kaywords 309 keyword9 585 keyword2 221 keyword5 789 keyword33 195 keyword3 354 keyword0 307 keyword8 698 keyword4 426 keyword1 ", "apis": "set_index reindex reset_index", "code": ["inputlist = [int(x) for x in inputlist]\ndf = df.set_index('val').reindex(inputlist).reset_index()\n"], "link": "https://stackoverflow.com/questions/55672247/sort-dataframe-column-with-given-input-list"}
{"id": 367, "q": "Replace outliers with median exept NaN", "d": "I would like to replace outliers with median in a dataframe but only outliers and not NaN. First : I would like to replace the -60 which is an outlier with the median using : It works fine but it also delete all rows containing a NaN how can I avoid that ? Output : As you can see, 3 rows have been deleted which is not very convenient. Any ideas ? Thanks !", "q_apis": "median replace median replace median delete all", "io": " January February 0 -5.0 -7.0 1 -6.0 -6.0 2 -5.0 -5.0 3 -3.0 -6.0 4 -6.0 -8.0 5 -11.0 -9.0 6 -6.0 5.0 7 -8.0 -11.0 8 -11.0 -12.0 9 -8.0 -9.0 10 -8.0 -6.0 11 -8.0 -5.0 12 -8.0 -4.0 13 -10.0 1.0 14 -10.0 3.0 15 -9.0 -9.0 16 -6.0 -6.0 17 -6.0 -6.0 18 -4.0 -4.0 19 -8.0 2.0 20 -9.0 3.0 21 -14.0 1.0 22 -15.0 -3.0 23 -17.0 -4.0 24 -19.0 -6.0 25 -60.0 -8.0 26 -8.0 -8.0 27 -9.0 -11.0 28 -5.0 NaN 29 -6.0 NaN 30 -7.0 NaN <s> January February 0 -5.0 -7.0 1 -6.0 -6.0 2 -5.0 -5.0 3 -3.0 -6.0 4 -6.0 -8.0 5 -11.0 -9.0 6 -6.0 5.0 7 -8.0 -11.0 8 -11.0 -12.0 9 -8.0 -9.0 10 -8.0 -6.0 11 -8.0 -5.0 12 -8.0 -4.0 13 -10.0 1.0 14 -10.0 3.0 15 -9.0 -9.0 16 -6.0 -6.0 17 -6.0 -6.0 18 -4.0 -4.0 19 -8.0 2.0 20 -9.0 3.0 21 -14.0 1.0 22 -15.0 -3.0 23 -17.0 -4.0 24 -19.0 -6.0 25 -10.0 -8.0 26 -8.0 -8.0 27 -9.0 -11.0 ", "apis": "apply abs mean std isna all", "code": ["df = df[df.apply(lambda x: (np.abs(x - x.mean()) / x.std() < 4) | x.isna()).all(axis=1)]\nprint(df)\n"], "link": "https://stackoverflow.com/questions/59760868/replace-outliers-with-median-exept-nan"}
{"id": 385, "q": "Jupyter Pandas - dropping items which have average over a threshold", "d": "I have a data frame with items and their prices, something like this: I want to exclude all rows from this df where the item has an average price over 200. So filtered df should look like this: I'm new to python and pandas but as a first step was thinking something like this to get a new df for avg prices: avg_prices_df = df.groupby('ItemID').Price.mean().reset_index and then not sure how to proceed from there. Not sure even that first step is correct. To further complicate the matter, I am using vaex to read the data in ndf5 form as I have over 400 million rows. Many thanks in advance for any advice. EDIT: So I got the following code working, though I am sure it is not optimised.. ` create dataframe of ItemIDs and their average prices df_item_avg_price = df.groupby(df.ItemID, agg=[vaex.agg.count('ItemID'), vaex.agg.mean('Price')]) filter this new dataframe by average price threshold df_item_avg_price = (df_item_avg_price[df_item_avg_price[\"P_r_i_c_e_mean\"] <= 50000000]) create list of ItemIDs which have average price under the threshold items_in_price_range = df_item_avg_price['ItemID'].tolist() filter the original dataframe to include rows only with the items in price range filtered_df = df[df.ItemID.isin(items_in_price_range)] ` Any better way to do this?", "q_apis": "items items all where item first step get groupby mean reset_index first step any groupby agg agg count agg mean filter filter items isin", "io": " \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Item \u2551 Day \u2551 Price \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 A \u2551 1 \u2551 10 \u2551 \u2551 B \u2551 1 \u2551 20 \u2551 \u2551 C \u2551 1 \u2551 30 \u2551 \u2551 D \u2551 1 \u2551 40 \u2551 \u2551 A \u2551 2 \u2551 100 \u2551 \u2551 B \u2551 2 \u2551 20 \u2551 \u2551 C \u2551 2 \u2551 30 \u2551 \u2551 D \u2551 2 \u2551 40 \u2551 \u2551 A \u2551 3 \u2551 500 \u2551 \u2551 B \u2551 3 \u2551 25 \u2551 \u2551 C \u2551 3 \u2551 35 \u2551 \u2551 D \u2551 3 \u2551 1000 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d <s> \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Item \u2551 Day \u2551 Price \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 B \u2551 1 \u2551 20 \u2551 \u2551 C \u2551 1 \u2551 30 \u2551 \u2551 B \u2551 2 \u2551 20 \u2551 \u2551 C \u2551 2 \u2551 30 \u2551 \u2551 B \u2551 3 \u2551 25 \u2551 \u2551 C \u2551 3 \u2551 35 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d", "apis": "groupby transform mean groupby filter mean", "code": ["avg_prices_df = df[df.groupby('Item')['Price'].transform('mean') < 200]\n", "avg_prices_df = df.groupby('Item').filter(lambda x: x['Price'].mean() < 200)\n"], "link": "https://stackoverflow.com/questions/59233282/jupyter-pandas-dropping-items-which-have-average-over-a-threshold"}
{"id": 346, "q": "How to replace the value of a dataframe column with the value of another column using groupby.first()?", "d": "I have a df like this: I want to check the first of every Year-Month. If it's < 0, I want value2 to replace with value1. How can I do that? In this example, the result should be: Because only first are negative, first are positive, just leave it. I used: it doesn't seem to work. Thanks.", "q_apis": "replace value value groupby first first replace first first", "io": " Value1 Value2 2008-01-01 -1 4 2008-01-01 -1 5 2008-01-03 -1 6 2008-02-25 0 7 2008-02-26 -1 8 2008-02-27 0 9 2008-03-02 5 10 2008-03-16 -1 11 2008-03-17 -1 12 2009-04-04 -1 13 2009-04-07 0 14 <s> Value1 Value2 2008-01-01 -1 -1 2008-01-01 -1 5 2008-01-03 -1 6 2008-02-25 0 7 2008-02-26 -1 8 2008-02-27 0 9 2008-03-02 5 10 2008-03-16 -1 11 2008-03-17 -1 12 2009-04-04 -1 -1 2009-04-07 0 14 ", "apis": "groupby index to_period head loc lt index", "code": ["s = df.groupby(df.index.to_period('M'), as_index=False).head(1)\ndf.loc[s[s['Value1'].lt(0)].index, 'Value1'] = df['Value2']\n"], "link": "https://stackoverflow.com/questions/60305830/how-to-replace-the-value-of-a-dataframe-column-with-the-value-of-another-column"}
{"id": 642, "q": "Sum values in third column while putting together corespondinng values in first and second columns", "d": "I have 3 columns of data. I have data stored in three columns (k, v, t) in csv. For instance, Data: I want to get as the following data. Basically, sum all the values of t that has the same k and v. this is the code I have so far: and it keeps going until the end. I use \"for loop\" and \"if\" but it is too long. Can I use numpy in a short and clean way? or any other better way?", "q_apis": "values values first second columns columns columns get sum all values any", "io": "k v t a 1 2 b 2 3 c 3 4 a 2 3 b 3 2 b 3 4 c 3 5 b 2 3 <s> a 1 5 b 2 6 b 3 6 c 3 9 ", "apis": "read_csv groupby sum", "code": ["df = pd.read_csv('file.csv')\n\ng = df.groupby(['k', 'v'], as_index=False)['t'].sum()\n"], "link": "https://stackoverflow.com/questions/49073547/sum-values-in-third-column-while-putting-together-corespondinng-values-in-first"}
{"id": 106, "q": "Transpose dataframe based on column list", "d": "I have a dataframe in the following structure: I would like to transpose - create columns from the names in cNames. But I can't manage to achieve this with transpose because I want a column for each value in the list. The needed output: How can I achieve this result? Thanks! The code to create the DF:", "q_apis": "transpose columns names transpose value", "io": "cNames | cValues | number [a,b,c] | [1,2,3] | 10 [a,b,d] | [55,66,77]| 20 <s> a | b | c | d | number 1 | 2 | 3 | NaN | 10 55 | 66 | NaN | 77 | 20 ", "apis": "concat Series name iterrows T join iloc DataFrame iterrows T join iloc DataFrame iterrows T join iloc mean std concat Series name iterrows T join iloc mean std apply concat apply Series mean std apply Series explode set_index append unstack reset_index drop mean std set_index apply explode reset_index astype pivot_table index values columns droplevel reset_index mean std explode explode to_numeric pivot_table columns index values mean std", "code": ["pd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n", "pd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n", "%%timeit\npd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n1.29 ms \u00b1 36.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "%%timeit\npd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n2.03 ms \u00b1 86.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) \n", "%%timeit\ndf['series'] = df.apply(lambda x: dict(zip(x['cNames'], x['cValues'])), axis=1)\npd.concat([df['number'], df['series'].apply(pd.Series)], axis=1)\n\n2.09 ms \u00b1 65.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "%%timeit\ndf.apply(pd.Series.explode)\\\n  .set_index(['number', 'cNames'], append=True)['cValues']\\\n  .unstack()\\\n  .reset_index()\\\n  .drop('level_0', axis=1)\n\n4.9 ms \u00b1 135 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "%%timeit\ng=df.set_index('number').apply(lambda x: x.explode()).reset_index()\ng['cValues']=g['cValues'].astype(int)\npd.pivot_table(g, index=[\"number\"],values=[\"cValues\"],columns=[\"cNames\"]).droplevel(0, axis=1).reset_index()\n\n7.27 ms \u00b1 162 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "%%timeit\ndf1 = df.explode('cNames').explode('cValues')\ndf1['cValues'] = pd.to_numeric(df1['cValues'])\ndf1.pivot_table(columns='cNames',index='number',values='cValues')\n\n9.42 ms \u00b1 189 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/66070517/transpose-dataframe-based-on-column-list"}
{"id": 98, "q": "Is there a way to apply a condition while using apply and lambda in a DataFrame?", "d": "I have a Pandas dataframe that looks like this: And I'm looking for a way to iter trough the Dyn column, generating another one that sums only the numbers that are bigger than a cutoff, i.e.: 0.150, assigning all the values that pass it a value of one. This is what the expected result should look like: I thought I could use apply, while ittering trough all of the rows: But I'm lost on how to apply the condition (only sum it if it's greater than 0.150) to all the values inside 'Dyn' and how to assign the value of 1 to them. All advice is accepted. Thanks!", "q_apis": "apply apply DataFrame all values value apply all apply sum all values assign value", "io": " ID Dyn 0 AA01 0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1 1 BG54 0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200 <s> ID Dyn Sum 0 AA01 0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1 2 1 BG54 0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200 7 ", "apis": "assign sum DataFrame explode sum sum sum astype values groupby sum sum gt groupby sum sum reset_index", "code": ["#Create temp column to hold Dyn convereted into list\ndf=df.assign(sum=df['Dyn'].str.split(','))\n\n#Explode DataFrame\ndf=df.explode('sum')\n#Convert to float\ndf['sum']=df['sum'].astype(float)\n#Filter out values greater that 0.015, groupby and sum\ndf[df['sum'].gt(0.150)].groupby(['ID','Dyn'])['sum'].sum().reset_index()\n"], "link": "https://stackoverflow.com/questions/66146761/is-there-a-way-to-apply-a-condition-while-using-apply-and-lambda-in-a-dataframe"}
{"id": 159, "q": "Process pandas group efficiently", "d": "I have a dataframe df with columns a,b,c,d and e. What I want is, group by df on the basis of a,b and c. And tthen for each group I want to remove NULL value of column d and e with most frequent value of that column in that group. And then finally drop duplicates for each group. I am doing the following procesing: But the iteration is making my processing really very slow. Can someone suggest me better way to do it? Sample input: Sample output:", "q_apis": "columns value value drop", "io": "a b c d e a1 b1 c1 NULL e2 a2 b2 c2 NULL NULL a2 b2 c2 NULL NULL a1 b1 c3 d4 e4 a1 b1 c1 NULL e2 a1 b1 c1 d1 e2 a1 b1 c1 d1 NULL <s> a b c d e a1 b1 c1 d1 e2 a2 b2 c2 NULL NULL a1 b1 c3 d4 e4 ", "apis": "mode iloc groupby agg", "code": ["def get_mode(series):\n    out = series.mode()\n    return out.iloc[0] if len(out) else np.nan\n\ndf.groupby(['a','b','c'], as_index=False, sort=False).agg(get_mode)\n"], "link": "https://stackoverflow.com/questions/64904669/process-pandas-group-efficiently"}
{"id": 227, "q": "Change the value of column based on quantity of equals rows", "d": "I have a dataframe like this: I need to change the value of column to 1 if value of row equals the actual quantity of rows, where columns and are equals (row0 and row1 in this example). Desired output:", "q_apis": "value equals value value equals where columns equals", "io": " id desc quantity 0 B668441DE83B Car 2 1 B668441DE83B Car 2 2 B668441DE83B Bus 1 3 89C26DEE41E2 Bus 3 4 89C26DEE41E2 Bus 3 <s> id desc quantity 0 B668441DE83B Car 1 1 B668441DE83B Car 1 2 B668441DE83B Bus 1 3 89C26DEE41E2 Bus 3 4 89C26DEE41E2 Bus 3 ", "apis": "mask groupby transform size eq loc mask", "code": ["mask = df.groupby(['id','desc'])['id'].transform('size').eq(df['quantity'])\n\ndf.loc[mask, 'quantity'] = 1\n"], "link": "https://stackoverflow.com/questions/63595559/change-the-value-of-column-based-on-quantity-of-equals-rows"}
{"id": 440, "q": "How to rearrange/reorder the rows and columns in python dataframe?", "d": "SCREEN SHOT OF ACTUAL DATA FRAMEDataframe of 5000 rows and 192 columns I want to change the size of my data frame of m rows and n columns (m= 5000 and n = 192) into a size of n/3 rows(64 rows) and m*5000 columns(15000 columns)?? existing data frame DESIRED data frame", "q_apis": "columns columns size columns size columns columns", "io": "0 A1 A2 A3 A4 A5 A6 A7 A8 A9.....A192 1 B1 B2 B3 B4 B5 B6 B7 B8 B9.....B192 . . . 5000 192 X1 X2 X3 X4 X5 X6 X7 X8 X9.....X192 <s> 0 A1 A2 A3 B1 B2 B3.....X1 X2 X3 1 A4 A5 A6 B4 B5 B6.....X4 X5 X6 2 A7 A8 A9 B7 B8 B9.....X7 X8 X9 . . 64 A190 A191 A192 B190 B191 B192.....X190 X191 X192 ", "apis": "DataFrame iloc values", "code": ["pd.DataFrame([df.iloc[:, e:e+3].values.flatten() for e in range(0, 192, 3)])\n"], "link": "https://stackoverflow.com/questions/57479385/how-to-rearrange-reorder-the-rows-and-columns-in-python-dataframe"}
{"id": 347, "q": "python dataframe merge columns according to other column values", "d": "What I want to do is merge columns according to values in another column It is better illustrated with a simple example: I have a dataframe with 5 columns: I want to get the following table: where the columns are filled with values from team_1.x and team_1.y for rows of players with number less than 5 and values from team_2.x and team_2.y for rows of players with number bigger than 5", "q_apis": "merge columns values merge columns values columns get where columns values values", "io": "| player_num | team_1.x | team_1.y | team_2.x | team_2.y | |------------ |---------- |---------- |---------- |---------- | | 1 | x_1 | y_1 | x_2 | y_2 | | 4 | x_3 | y_3 | x_4 | y_4 | | 8 | x_5 | y_5 | x_6 | y_6 | <s> | x | y | |----- |----- | | x_1 | y_1 | | x_3 | y_3 | | x_6 | y_6 | ", "apis": "where where", "code": ["import numpy as np\n...\ndf['x'] = np.where(df['player_num'] < 5, df['team_1.x'], df['team_2.x'])\ndf['y'] = np.where(df['player_num'] < 5, df['team_1.y'], df['team_2.y'])\n"], "link": "https://stackoverflow.com/questions/60279369/python-dataframe-merge-columns-according-to-other-column-values"}
{"id": 424, "q": "pandas create a column and assign values to it from a dictionary", "d": "I have a dictionary looks like this, I have a df looks like this, I like to create a column in whose values will be based on the values in , so the result will look like, I am wondering whats the best way to do this.", "q_apis": "assign values values values", "io": "id code 1 SA01 2 SA02 3 SA03 4 AP01 5 AP02 6 AP03 <s> id code region 1 SA01 South America 2 SA02 South America 3 SA03 South America 4 AP01 Asia Pacific 5 AP02 Asia Pacific 6 AP03 Asia Pacific ", "apis": "name map", "code": ["d_ = {code:sd['name'] for sd in d['regions'] for code in sd['code'].split(',')} \n# {'SA01': 'South America', 'SA02': 'South America', 'SA03': 'South America',...\ndf['region'] = df.code.map(d_)\n"], "link": "https://stackoverflow.com/questions/58085046/pandas-create-a-column-and-assign-values-to-it-from-a-dictionary"}
{"id": 30, "q": "Element wise numeric comparison in Pandas dataframe column value with list", "d": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code:", "q_apis": "value value value compare min max value", "io": " | A | B | C | | Val | Val | Val | |---------------------|-----------------------|--------------------| 0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] | 1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] | 2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] | <s> | A | B | C | | Max | Max | Max | |-------|-------|------| 0 | 28.68 | 18.42 | 1.37 | 1 | 29.50 | 17.31 | 1.47 | 2 | 29.87 | 20.45 | 1.39 | ", "apis": "name IndexSlice to_numpy IndexSlice to_numpy array name IndexSlice to_numpy IndexSlice to_numpy array apply rename columns", "code": ["def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n", "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"], "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list"}
{"id": 601, "q": "Combination of two dataframes without duplicate and reversion in efficient way | python", "d": "I have two dataframes with thousands of rows, I need to combine both into one dataframe without duplicate and reversion. for example: Dataframe 1 Dataframe 2 So, the output dataframe will be: output-dataframe I don't want the output combination containing something like: I actually try it using but it return duplicate and reversion and also took long time because I have thousands in Dataframes 1 and 2 Any help please ?", "q_apis": "combine time", "io": "drug1 disease1 drug1 disease2 drug1 disease3 drug2 disease1 drug2 disease2 drug2 disease3 drug3 disease1 drug3 disease2 drug3 disease3 <s> disease1 drug1 drug1 drug1 disease1 disease1 ", "apis": "DataFrame merge merge drop", "code": ["from pandas import DataFrame, merge\n\ndf1['key'] = 1\ndf2['key'] = 1\n\nresult = df1.merge(df2, on='key').drop('key', axis=1)\n"], "link": "https://stackoverflow.com/questions/51734814/combination-of-two-dataframes-without-duplicate-and-reversion-in-efficient-way"}
{"id": 313, "q": "how to split a nested dictionary inside a column of a dataframe into new rows?", "d": "I have a dataframe : I need to split col3 into new rows: expected output dataframe : This doesnt seem to work :", "q_apis": "", "io": "Col1 Col2 Col3 01 ABC {'link':'http://smthing1} 02 DEF {'link':'http://smthing2} <s> Col1 Col2 Col3 01 ABC 'http://smthing1' 02 DEF 'http://smthing2' ", "apis": "apply get", "code": ["#converting to dicts\n#import ast\n#df['Col3'] = df['Col3'].apply(ast.literal_eval)\ndf['Col3'] = df['Col3'].str.get('link')\n"], "link": "https://stackoverflow.com/questions/61117957/how-to-split-a-nested-dictionary-inside-a-column-of-a-dataframe-into-new-rows"}
{"id": 56, "q": "Match multiple columns on Python to a single value", "d": "I hope you are doing well. I am trying to perform a match based on multiple columns where my values of Column B of df1 is scattered in three to four columns in df2. The goal here is the the return the values of Column A of df2 if values of Column B matches any values in the columns C,D,E. What I did until now was actually to do multiple left merges (and changing the name of Column B to match the name of columns C,D,E of df2). I am trying to simplify the process but I am unsure how I am supposed to do this? My dataset looks like that: Df1: DF2: My goal is to have in df1: Thank you very much !", "q_apis": "columns value columns where values columns values values any values columns now left name name columns", "io": " ID 0 77 1 4859 2 LSP <s> ID X 0 77 AAAAA_XX 1 4859 BBBBB_XX 2 LSP CCCC_YY ", "apis": "concat reset_index merge left iloc rename columns", "code": ["df3 = pd.concat([df2.id1, df2.id2]).reset_index()\ndf1 = df2.merge(df3, how=\"left\", left_on = df1.ID, right_on = df3[0])\ndf1 = df1.iloc[:, :2]\ndf1 = df1.rename(columns={\"key_0\": \"ID\"})\n"], "link": "https://stackoverflow.com/questions/67087432/match-multiple-columns-on-python-to-a-single-value"}
{"id": 482, "q": "Function on dataframe rows to reduce duplicate pairs Python", "d": "I've got a dataframe that looks like: Each 'layer'/row has pairs that are duplicates that I want to reduce. The one problem is that there are repeating 0s as well so I cannot just simply remove duplicates per row or it will leave an uneven number of rows. My desired output would be a lambda function that I could apply to all rows of this dataframe to get this: Is there a simple function I could write to do this?", "q_apis": "apply all get", "io": "0 1 2 3 4 5 6 7 8 9 10 11 12 13 13 13.4 13.4 12.4 12.4 16 0 0 0 0 14 12.2 12.2 13.4 13.4 12.6 12.6 19 5 5 6.7 6.7 . . . <s> 0 1 2 3 4 5 6 12 13 13.4 12.4 16 0 0 14 12.2 13.4 12.6 19 5 6.7 . . . ", "apis": "columns iloc columns columns", "code": ["idxcols = [x-1 for x in range(len(df.columns)) if x % 2]\n\ndf = df.iloc[:, idxcols]\n\ndf.columns = range(len(df.columns))\n"], "link": "https://stackoverflow.com/questions/56044354/function-on-dataframe-rows-to-reduce-duplicate-pairs-python"}
{"id": 608, "q": "How to map new variable in pandas in effective way", "d": "Here's my data What I need, is to map : if is more than , is . But,if is less than , is What I did It works, but not highly configurable and not effective.", "q_apis": "map map", "io": "Id Amount 1 6 2 2 3 0 4 6 <s> Id Amount Map 1 6 1 2 2 0 3 0 0 4 5 1 ", "apis": "columns concat values astype mean std astype mean std", "code": ["#[400000 rows x 3 columns]\ndf = pd.concat([df] * 100000, ignore_index=True)\n\nIn [133]: %timeit df['Map'] = (df['Amount'].values >= 3).astype(int)\n2.44 ms \u00b1 97.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [134]: %timeit df['Map'] = (df['Amount'] >= 3).astype(int)\n2.6 ms \u00b1 66.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/51374862/how-to-map-new-variable-in-pandas-in-effective-way"}
{"id": 383, "q": "Symmetrical column values in pandas data frame", "d": "I have one set of variable as in below data frame: Another set of variable in below data frame: 1st columns are index columns. I want to add each row (v1+v2) to get v3. How do I make the index column values (0 to 4) and (41 to 45) symmetrical ( either 0-4) or (42-45) in both data fame? I am working on pandas (python) jupyter notebook.", "q_apis": "values columns index columns add get index values", "io": " v1 ---------- 0 0.036286 1 -0.018490 2 0.011699 3 0.028955 4 -0.000373 <s> v2 ---------- 41 12.31 42 12.20 43 12.12 44 12.31 45 12.47 ", "apis": "concat sum sum values values", "code": ["df = pd.concat(df1, df2, axis=1, ignore_index=True)\ndf['v3'] = df.sum(axis=1) # or df[['v1','v2']].sum(axis=1)\n", "v3 = df1['v1'].values + df2['v2'].values\n"], "link": "https://stackoverflow.com/questions/59345178/symmetrical-column-values-in-pandas-data-frame"}
{"id": 45, "q": "Checking if column headers match PYTHON", "d": "I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python?", "q_apis": "columns get any", "io": " ID Open High Low 1 64 66 52 <s> ID Open High Volume 1 33 45 30043 ", "apis": "DataFrame DataFrame columns columns", "code": ["import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [64],\n        \"High\": [66],\n        \"Low\": [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [33],\n        \"High\": [45],\n        \"Volume\": [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns & df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(\"Columns only available in df1\", df1_columns_only)\nprint(\"Columns only available in df2\", df2_columns_only)\n", "Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n"], "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python"}
{"id": 266, "q": "position or move pandas column to a specific column index", "d": "I have a DF and it has multiple columns (over 75 columns) with default numeric index: I need to arrange/change position to as follows: I can get the index of using: but I don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list.", "q_apis": "index columns columns index get index all columns", "io": "Col1 Col2 Col3 ... Coln <s> Col1 Col3 Col2 ... Coln ", "apis": "index index", "code": ["l = list(df)\n\ni1, i2 = l.index('Col2'), l.index('Col3')\nl[i2], l[i1] = l[i1], l[i2]\n\ndf = df[l]\n"], "link": "https://stackoverflow.com/questions/56044461/position-or-move-pandas-column-to-a-specific-column-index"}
{"id": 60, "q": "How to convert this DataFrame into Json", "d": "I have this with 2 columns when I try to convert it into it goes wrong: I don't even know ehere the numbers come from. My desired :", "q_apis": "DataFrame columns", "io": "print(df) a b 10 {'A': 'foo', ...} 20 {'B': 'faa', ...} 30 {'C': 'fee', ...} 40 {'D': 'fii', ...} 50 {'E': 'foo', ...} <s> [{ 'a': 10, 'b': { 'A': 'foo', ... }, ... 'a': 50, 'b': { 'E': 'foo', ... } } ] ", "apis": "append", "code": ["data = []\nfor i in df:\n    data.append({'a': df[i[0]], 'b': df(i[1])})\n", "with open(\"myjson.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n"], "link": "https://stackoverflow.com/questions/66991966/how-to-convert-this-dataframe-into-json"}
{"id": 475, "q": "How to apply a method to a Pandas Dataframe", "d": "I have this dataframe I would like to convert it to I know how to create a dataframe (with indexes) for 1 column, but not for multiple columns This code produces this result how can I amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally I wouln't want to have to enter the same code for every column)", "q_apis": "apply columns add", "io": " Col1 Col2 0 A (1000 EUR) C ( 3000 USD) 1 B (2000 CHF) D ( 4000 GBP) <s> Col1 Col2 0 1000 3000 1 2000 4000 ", "apis": "DataFrame apply apply all columns apply apply to_numeric", "code": ["# creates your dataframe\ndf = pd.DataFrame({'Col1':['A (1000 EUR)','B (2000 CHF)'], 'Col2':['C (3000 USD)', 'D (4000 GBP)']})\n\n# use the apply function to  apply your code to all elements of both columns\ndf = df.apply(lambda x: x.str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce'))\n"], "link": "https://stackoverflow.com/questions/56328404/how-to-apply-a-method-to-a-pandas-dataframe"}
{"id": 619, "q": "How to group phone number with and without country code", "d": "I am trying to detect phone number, my country code is but some phone manufacturer or operator use and , after query and pivoting I get pivoted data. But, the pivoted data is out of context Here's the pivoted data Here's what I need to group, but I don't want to group manually ( and is same, etc)", "q_apis": "query get", "io": "Id +623684682 03684682 +623684684 03684684 1 1 0 1 1 2 1 1 2 1 <s> Id 03684682 03684684 1 1 2 2 2 3 ", "apis": "groupby replace sum columns columns replace sum", "code": ["df = df.groupby(lambda x: x.replace('+62','0'), axis=1).sum()\n", "df.columns = df.columns.str.replace('\\+62','0')\ndf = df.sum(level=0, axis=1)\n"], "link": "https://stackoverflow.com/questions/51018104/how-to-group-phone-number-with-and-without-country-code"}
{"id": 235, "q": "Python: Append 2 columns of a dataframe together", "d": "I am loading a csv file into a data frame using pandas. My dataframe looks something like this: I wish to append 2 of the columns into a new column: col4 needs to be created by appending the contents of col1 and col2 together. How can I do this in pandas/python? EDIT", "q_apis": "columns append columns", "io": "col1 col2 col3 1 4 1 2 5 2 3 6 3 <s> col1 col2 col3 col4 1 4 1 1 2 5 2 2 3 6 3 3 4 5 6 ", "apis": "append rename concat rename join concat reset_index drop append rename append rename concat", "code": ["s = df['col1'].append(df['col2'], ignore_index=True).rename('col4')\n#alternative\n#s = pd.concat([df['col1'], df['col2']], ignore_index=True).rename('col4')\n\ndf1 = df.join(s, how='outer')\n#alternative\n#df1 = pd.concat([df, s], axis=1)\n", "df = df.reset_index(drop=True)\n\ns1 = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ns2 = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\n\ndf1 = pd.concat([df, s1, s2], axis=1)\n"], "link": "https://stackoverflow.com/questions/63264777/python-append-2-columns-of-a-dataframe-together"}
{"id": 26, "q": "Replicating the DataFrame row in a special manner", "d": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output:", "q_apis": "DataFrame", "io": "col1 mob_no col3 a 9382949201/3245622535 45 b 8383459345/4325562678 67 c 8976247543/1827472398 89 d 7844329432 09 <s> col1 mob_no col3 a 9382949201 45 a 3245622535 45 b 8383459345 67 b 4325562678 67 c 8976247543 89 c 1827472398 89 d 7844329432 09 ", "apis": "explode", "code": ["df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"], "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner"}
{"id": 209, "q": "Get &quot;Last Purchase Year&quot; from Sales Data Pivot in Pandas", "d": "I have pivoted the Customer ID against their year of purchase, so that I know how many times each customer purchased in different years: My desired result is to append the column names with the latest year of purchase, and thus the number of years since their last purchase: Here is what I tried: However what I got is \"TypeError: cannot convert the series to <class 'float'>\" Could anyone help me to get the result I need? Thanks a lot! Dennis", "q_apis": "year append names year last get", "io": "Customer ID 1996 1997 ... 2019 2020 100000000000001 7 7 ... NaN NaN 100000000000002 8 8 ... NaN NaN 100000000000003 7 4 ... NaN NaN 100000000000004 NaN NaN ... 21 24 100000000000005 17 11 ... 18 NaN <s> Customer ID 1996 1997 ... 2019 2020 Last Recency 100000000000001 7 7 ... NaN NaN 1997 23 100000000000002 8 8 ... NaN NaN 1997 23 100000000000003 7 4 ... NaN NaN 1997 23 100000000000004 NaN NaN ... 21 24 2020 0 100000000000005 17 11 ... 18 NaN 2019 1 ", "apis": "filter columns notna cumsum idxmax max", "code": ["c = df.filter(regex=r'\\d+').columns\ndf['Last'] = df[c].notna().cumsum(1).idxmax(1)\ndf['Recency'] = c.max() - df['Last']\n"], "link": "https://stackoverflow.com/questions/64032701/get-last-purchase-year-from-sales-data-pivot-in-pandas"}
{"id": 417, "q": "How to split a string in a column within a pandas dataframe?", "d": "This is an example of the file I have, So, in the column 'Name', where '_EN' is present, I want to remove the '_EN' part. The output should be as follows: This is what I was trying: However, this is not working. What is a good way to do this?", "q_apis": "where", "io": "Name Att1 Att2 Att3 AB_EN 1 2 3 CD 5 6 7 FG_EN 7 8 9 <s> Name Att1 Att2 Att3 AB 1 2 3 CD 5 6 7 FG 7 8 9 ", "apis": "DataFrame", "code": ["df = pd.DataFrame({\"Name\": [\"AB_EN\", \"CD\", \"FG_EN\"]})\ndf['Name'] = df['Name'].str.split(\"_\").str[0]\nprint(df)\n"], "link": "https://stackoverflow.com/questions/58321988/how-to-split-a-string-in-a-column-within-a-pandas-dataframe"}
{"id": 362, "q": "Sort a pandas DataFrame by a column in another dataframe - pandas", "d": "Let's say I have a Pandas DataFrame with two columns, like: And let's say I also have a Pandas Series, like: How can I sort the column to become the same order as the series, with the corresponding row values sorted together? My desired output would be: Is there any way to achieve this? Please check self-answer below.", "q_apis": "DataFrame DataFrame columns Series values any", "io": " a b 0 1 100 1 2 200 2 3 300 3 4 400 <s> a b 0 1 100 1 3 300 2 2 200 3 4 400 ", "apis": "DataFrame values index columns columns DataFrame Series set_index reindex rename_axis reset_index set_index loc reset_index iloc map index DataFrame values index columns columns", "code": ["print(pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns))\n", "import pandas as pd\nfrom timeit import timeit\ndf = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [100, 200, 300, 400]})\ns = pd.Series([1, 3, 2, 4])\ndef u10_1():\n    return df.set_index('a').reindex(s).rename_axis('a').reset_index('a')\ndef u10_2():\n    return df.set_index('a').loc[s].reset_index()\ndef u10_3():\n    return df.iloc[list(map(df['a'].tolist().index, s))]\ndef u10_4():\n    return pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns)\nprint('u10_1:', timeit(u10_1, number=1000))\nprint('u10_2:', timeit(u10_2, number=1000))\nprint('u10_3:', timeit(u10_3, number=1000))\nprint('u10_4:', timeit(u10_4, number=1000))\n"], "link": "https://stackoverflow.com/questions/59925197/sort-a-pandas-dataframe-by-a-column-in-another-dataframe-pandas"}
{"id": 7, "q": "How to combine rows in a dataframe in a pairwise fashion while applying some function", "d": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code:", "q_apis": "combine keys values combine left", "io": "ID Val1 Val2 id0 10 20 id0 11 19 id1 5 5 id1 1 1 id1 2 4 <s> ID Val1 Val2 id0_1 10.5 19.5 id1_1 3 3 id1_2 1.5 2.5 ", "apis": "groupby rolling mean dropna all reset_index drop index groupby cumcount add astype", "code": ["out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"], "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun"}
{"id": 254, "q": "How to split a DataFrame on each different value in a column?", "d": "Below is an example DataFrame. I want to split this into new dataframes when the row in column 0 changes. I've tried adapting the following solutions without any luck so far. Split array at value in numpy Split a large pandas dataframe", "q_apis": "DataFrame value DataFrame any array at value", "io": " 0 1 2 3 4 0 0.0 13.00 4.50 30.0 0.0,13.0 1 0.0 13.00 4.75 30.0 0.0,13.0 2 0.0 13.00 5.00 30.0 0.0,13.0 3 0.0 13.00 5.25 30.0 0.0,13.0 4 0.0 13.00 5.50 30.0 0.0,13.0 5 0.0 13.00 5.75 0.0 0.0,13.0 6 0.0 13.00 6.00 30.0 0.0,13.0 7 1.0 13.25 0.00 30.0 0.0,13.25 8 1.0 13.25 0.25 0.0 0.0,13.25 9 1.0 13.25 0.50 30.0 0.0,13.25 10 1.0 13.25 0.75 30.0 0.0,13.25 11 2.0 13.25 1.00 30.0 0.0,13.25 12 2.0 13.25 1.25 30.0 0.0,13.25 13 2.0 13.25 1.50 30.0 0.0,13.25 14 2.0 13.25 1.75 30.0 0.0,13.25 15 2.0 13.25 2.00 30.0 0.0,13.25 16 2.0 13.25 2.25 30.0 0.0,13.25 <s> 0 1 2 3 4 0 0.0 13.00 4.50 30.0 0.0,13.0 1 0.0 13.00 4.75 30.0 0.0,13.0 2 0.0 13.00 5.00 30.0 0.0,13.0 3 0.0 13.00 5.25 30.0 0.0,13.0 4 0.0 13.00 5.50 30.0 0.0,13.0 5 0.0 13.00 5.75 0.0 0.0,13.0 6 0.0 13.00 6.00 30.0 0.0,13.0 7 1.0 13.25 0.00 30.0 0.0,13.25 8 1.0 13.25 0.25 0.0 0.0,13.25 9 1.0 13.25 0.50 30.0 0.0,13.25 10 1.0 13.25 0.75 30.0 0.0,13.25 11 2.0 13.25 1.00 30.0 0.0,13.25 12 2.0 13.25 1.25 30.0 0.0,13.25 13 2.0 13.25 1.50 30.0 0.0,13.25 14 2.0 13.25 1.75 30.0 0.0,13.25 15 2.0 13.25 2.00 30.0 0.0,13.25 16 2.0 13.25 2.25 30.0 0.0,13.25 ", "apis": "groupby", "code": ["out = [sub_df for _, sub_df in df.groupby(0)]\n"], "link": "https://stackoverflow.com/questions/59649084/how-to-split-a-dataframe-on-each-different-value-in-a-column"}
{"id": 463, "q": "Is there a way to use previous row value in pandas&#39; apply function when previous value is iteratively summed ?or an efficient way?", "d": "I have a dataframe with some columns and I would like to apply the following transformation in an efficient manner. Given the Dataframe below: It should be transformed in such a way I can get the following output: Note that: C[i] = C[i] + C[i - 1] + ... + C[0] and D[i] = D[i] + C[i - 1] NaN values should be filtered. Thx!", "q_apis": "value apply value columns apply get values", "io": " C D =========== Nan 10 0 22 2 280 4 250 6 270 <s> C D =========== Nan 10 0 22 2 280 6 252 12 276 ", "apis": "cumsum add shift fillna", "code": ["df['C'] = df['C'].cumsum()\ndf['D'] = df['D'].add(df['C'].shift(1).fillna(0))\n"], "link": "https://stackoverflow.com/questions/56738691/is-there-a-way-to-use-previous-row-value-in-pandas-apply-function-when-previous"}
{"id": 75, "q": "How to transform rows of other columns to columns on the basis of unique values of a column?", "d": "Suppose I have a df in the following structure, relation between column1 to column2 - one to many relation between column2 to column1 - one to many Expected Output: Also, while transforming, for every column7 can I create an empty column right beside column6_yyyymm? Final Output, How can I achieve Final Output using a python function and/or pandas library? If there is anything unclear please let me know. UPDATE: For all empty_yyyymm columns I want to implement the following function, How can achieve this too? Note: yyyymm is generic way of referring column7. It is not actually a column.", "q_apis": "transform columns columns unique values between between empty right all columns", "io": "column1 | column2 | column3 | column4 | column5 | column6 | column7 A | B | C | 10 | 78 | 12 | 202001 A | B | D | 21 | 64 | 87 | 202001 A | B | E | 21 | 64 | 87 | 202001 X | K | C | 54 | 23 | 23 | 202001 X | K | D | 21 | 55 | 87 | 202001 X | K | E | 21 | 43 | 22 | 202001 A | B | C | 10 | 78 | 12 | 202002 A | B | D | 23 | 64 | 87 | 202002 A | B | E | 21 | 11 | 34 | 202002 Z | K | C | 10 | 78 | 12 | 202002 Z | K | D | 21 | 13 | 56 | 202002 Z | K | E | 12 | 77 | 34 | 202002 <s> column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 | column4_202002 | column5_202002 | column6_202002 | A | B | C | 10 | 78 | 12 | 10 | 78 | 12 | A | B | D | 21 | 64 | 87 | 23 | 64 | 87 | A | B | E | 21 | 64 | 87 | 21 | 11 | 34 | X | K | C | 54 | 23 | 23 | 0 | 0 | 0 | X | K | D | 21 | 55 | 87 | 0 | 0 | 0 | X | K | E | 21 | 43 | 22 | 0 | 0 | 0 | Z | K | C | 0 | 0 | 0 | 10 | 78 | 12 | Z | K | D | 0 | 0 | 0 | 21 | 13 | 56 | Z | K | E | 0 | 0 | 0 | 12 | 77 | 34 | ", "apis": "assign empty set_index unstack sort_index", "code": ["df = (df.assign(empty = np.nan)\n        .set_index(['column1','column2','column3','column7'])\n        .unstack(fill_value=0)\n        .sort_index(level=1, axis=1))\n"], "link": "https://stackoverflow.com/questions/66545100/how-to-transform-rows-of-other-columns-to-columns-on-the-basis-of-unique-values"}
{"id": 147, "q": "Parsing a txt file into data frame, filling columns based on the multiple separators", "d": "Having a .txt file structure as below trying to parse into dataframe of the following structure describing the rule: # i - 'i' is the row number n:data - 'n' is the column number to fill, 'data' is the value to fill into i'th row if the number of columns would be small enough it could be done manually, but txt considered has roughly 2000-3000 column values and some of them are missing. gives the following result I tried to remove the odd rows in data1 even in data2, then will hopefully figure out how to split the odd and merge the 2 df's, but there might be a faster and more beautiful method to do it, that's why asking here update, spent 3 hours figuring out how to work with dataframes, as I was not that familiar with them. now from that using It became this any suggestions on how to add unknown number of phantom columnsnd fill them using \"n:value\" from the list to fill the \"n\" column with the \"value\"?", "q_apis": "columns parse value columns values merge update now any add value value", "io": "#n 1 a 1:0.0002 3:0.0003... #n 2 b 2:0.0002 3:0.0003... #n 3 a 1:0.0002 2:0.0003... ... <s> # type 1 2 3 1 a 0.0002 null 0.0003 .... 2 b null 0.0002 0.0003 .... 3 a 0.0002 0.0003 null .... ... ", "apis": "all at time take second first value add DataFrame from_dict index reindex columns reindex index", "code": ["# read all lines\nlines = file.readlines()\n\n# here we will store the results, dictionary of dictionaries\nparsing_res = {}\n\n# a fancy way of processing two lines, odd and even, at the same time\nfor line1,line2 in zip(lines[::2],lines[1::2]):\n    # line1 has the form '#n  1', we split on whitespace and take the second tokem\n    row_index = line1.split()[1]\n    # line2 is the other type of lines, split into tokens by whitespace\n    tokens = line2.split()\n    # first one is 'type'\n    t = tokens[0]\n\n    # the others are pairs 'x:y', split them into x,y and stick into a dictionary with label x and value y\n    row_dict = {token.split(':')[0]:token.split(':')[1] for token in tokens[1:]}\n\n    # add type\n    row_dict['type'] = t\n   \n    # store the result for these two lines into the main dictionary\n    parsing_res[row_index] = row_dict\nparsing_res\n", "df = pd.DataFrame.from_dict(parsing_res, orient='index')\ndf.reindex(sorted(df.columns), axis=1).reindex(sorted(df.index), axis=0)\n"], "link": "https://stackoverflow.com/questions/65169011/parsing-a-txt-file-into-data-frame-filling-columns-based-on-the-multiple-separa"}
{"id": 398, "q": "pandas documentation example for append does not work (pandas.DataFrame.append)", "d": "I copied the example from the pandas documentation for the append method, but it isn't working for me. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html outputs: and not: What are possible reasons for this? Where is my mistake? Thanks for your help.", "q_apis": "append DataFrame append append DataFrame append", "io": " A B 0 1 2 1 3 4 <s> A B 0 1 2 1 3 4 0 5 6 1 7 8 ", "apis": "DataFrame columns DataFrame columns append", "code": ["df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n\ndf = df.append(df2)\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/58942218/pandas-documentation-example-for-append-does-not-work-pandas-dataframe-append"}
{"id": 564, "q": "How to drop rows with respect to a column values in Python?", "d": "I wantto remove rows with respect column values. df Here the list of value those I want to remove. I want to output like following: df How can I do this?", "q_apis": "drop values values value", "io": " ID B C D 0 101 1 2 3 1 103 5 6 7 2 108 9 10 11 3 109 5 3 12 4 118 11 15 2 5 121 2 5 6 <s> ID B C D 0 101 1 2 3 3 109 5 3 12 4 118 11 15 2 ", "apis": "isin dtype bool isin dtype bool", "code": [">>> df['ID'].isin(remove_id)\n>>> \n0    False\n1     True\n2     True\n3    False\n4    False\n5     True\nName: ID, dtype: bool\n>>> ~df['ID'].isin(remove_id)\n>>> \n0     True\n1    False\n2    False\n3     True\n4     True\n5    False\nName: ID, dtype: bool\n"], "link": "https://stackoverflow.com/questions/53252056/how-to-drop-rows-with-respect-to-a-column-values-in-python"}
{"id": 534, "q": "Report difference/change in values between two dataFrames of identical shape", "d": "The context is I want to compare two df's and find the difference. Here's df and df2 with a small difference: Comparing them yields a 2D boolean df of the same shape: I tried to extract the elements corresponding to the True's, but other elements (that I don't want) still occurs as NaN How to extract only the elements corresponding to the True's and the indices (so I know where in the df): update: the above example has only one True. In a general situation with multiple True's, I think are two cases: df is small and one may want to see: df is large and one may want to see: @U9-Forward's answer works nicely for case 1, and when there's only one True. @coldspeed provided a comprehensive solution. Thanks!", "q_apis": "difference values between identical shape compare difference difference shape indices where update", "io": "df[df != df2] Out[29]: a b 0 NaN NaN 1 NaN 1.0 2 NaN NaN <s> df[df != df2] # somehow? Out[30]: b 1 1.0 ", "apis": "values array agg dropna dtype", "code": ["df.values[df != df2]\n# array([1])\n", "df[df != df2].agg(lambda x: x.dropna().tolist())\n\na    [0.0]\nb    [1.0]\ndtype: object\n"], "link": "https://stackoverflow.com/questions/54059466/report-difference-change-in-values-between-two-dataframes-of-identical-shape"}
{"id": 251, "q": "python - pandas ffill with groupby", "d": "I am trying to forward fill the missing rows to complete the missing time-series rows in the dataset. The size of the dataset is huge. More than 100 million rows. The original source dataset is as shown below. desired output is as below I need to group on and to fill the missing time-series rows in for each of the combinations. Currently, I have the below code which is working but its extremely slow due to the for-loop. Is there any way I can avoid the for-loop and send the whole dataset for creating missing rows and ffill()? Thanks and Appreciate the help. Update: The above code is working but it's too slow. It takes more than 30 minutes for just 300k rows. Hence, I'm looking for help to make it faster and avoid the for-loop.", "q_apis": "ffill groupby time size time any ffill", "io": " col1 col2 col3 col4 col5 col6 0 2020-01-01 b1 c1 1 9 17 1 2020-01-05 b1 c1 2 10 18 2 2020-01-02 b2 c2 3 11 19 3 2020-01-04 b2 c2 4 12 20 4 2020-01-10 b3 c3 5 13 21 5 2020-01-15 b3 c3 6 14 22 6 2020-01-16 b4 c4 7 15 23 7 2020-01-30 b4 c4 8 16 24 <s> col1 col2 col3 col4 col5 col6 0 2020-01-01 b1 c1 1.0 9.0 17.0 1 2020-01-02 b1 c1 1.0 9.0 17.0 2 2020-01-03 b1 c1 1.0 9.0 17.0 3 2020-01-04 b1 c1 1.0 9.0 17.0 4 2020-01-05 b1 c1 2.0 10.0 18.0 5 2020-01-02 b2 c2 3.0 11.0 19.0 6 2020-01-03 b2 c2 3.0 11.0 19.0 7 2020-01-04 b2 c2 4.0 12.0 20.0 8 2020-01-10 b3 c3 5.0 13.0 21.0 9 2020-01-11 b3 c3 5.0 13.0 21.0 10 2020-01-12 b3 c3 5.0 13.0 21.0 11 2020-01-13 b3 c3 5.0 13.0 21.0 12 2020-01-14 b3 c3 5.0 13.0 21.0 13 2020-01-15 b3 c3 6.0 14.0 22.0 14 2020-01-16 b4 c4 7.0 15.0 23.0 15 2020-01-17 b4 c4 7.0 15.0 23.0 16 2020-01-18 b4 c4 7.0 15.0 23.0 17 2020-01-19 b4 c4 7.0 15.0 23.0 18 2020-01-20 b4 c4 7.0 15.0 23.0 19 2020-01-21 b4 c4 7.0 15.0 23.0 20 2020-01-22 b4 c4 7.0 15.0 23.0 21 2020-01-23 b4 c4 7.0 15.0 23.0 22 2020-01-24 b4 c4 7.0 15.0 23.0 23 2020-01-25 b4 c4 7.0 15.0 23.0 24 2020-01-26 b4 c4 7.0 15.0 23.0 25 2020-01-27 b4 c4 7.0 15.0 23.0 26 2020-01-28 b4 c4 7.0 15.0 23.0 27 2020-01-29 b4 c4 7.0 15.0 23.0 28 2020-01-30 b4 c4 8.0 16.0 24.0 ", "apis": "set_index groupby resample ffill reset_index drop reset_index", "code": ["(df.set_index('col1').groupby(['col2', 'col3'])\n   .resample('D').ffill()\n   .reset_index(['col2','col3'], drop=True)\n   .reset_index()\n)\n"], "link": "https://stackoverflow.com/questions/62941861/python-pandas-ffill-with-groupby"}
{"id": 165, "q": "How to replace duplicate dataframe column values with certain conditions in python", "d": "I have a dataframe with shape (10x401) having duplicate columns with same column names and values. Some of them have nulls while other have numeric values. The columns names are not in sorted order. A short example of dataframe is given below: By ignoring the null values, i need to replace each first occurrence of the numeric value (from 0 to 10) with 1 and the rest of the values with -1 for all 10 rows and 400 columns ignoring the ID column. The resulting dataframe will look like: I will be thankful for some help here.", "q_apis": "replace values shape columns names values values columns names values replace first value values all columns", "io": " ID#, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,.........,100, 100, 100, 100 1, , , , , 3, 3, 3, 3, , , , ,........., 0, 0, 0, 0 2, 0, 0, 0, 0, , , , , 10, 10, 10, 10,........., , , , 3, 9, 9, 9, 9, 1, 1, 1, 1, 4, 4, 4, 4,........., 1, 1, 1, 1 . . . 10, , , , , , , , , , , , ,........., 6, 6, 6, 6 <s> ID#, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,.........,100, 100, 100, 100 1, , , , , 1, -1, -1, -1, , , , ,........., 1, -1, -1, -1 2, 1, -1, -1, -1, , , , , 1, -1, -1, -1,........., , , , 3, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1,........., 1, -1, -1, -1 . . . 10, , , , , , , , , , , , ,........., 1, -1, -1, -1 ", "apis": "read_csv columns columns shift", "code": ["import pandas as pd\n\nfrom io import StringIO\n\ndf_string = '''\nID;1;1;1;1;2;2;2;2;3;3;3;3\n1;;;;;3;3;3;3;;;;\n2;0;0;0;0;;;;;10;10;10;10\n3;9;9;9;9;1;1;1;1;4;4;4;4\n4;;;;;;;;;6;6;6;6\n'''\n\ndf = pd.read_csv(StringIO(df_string), sep = \";\", index_col=\"ID\")\n\n# Removing the automatically added .1/.2/... suffixes. You don't need that for your data.\ndf.columns = df.columns.str[0]\n", "StartMask = (df.shift() != df) & ValueMask\n"], "link": "https://stackoverflow.com/questions/64789227/how-to-replace-duplicate-dataframe-column-values-with-certain-conditions-in-pyth"}
{"id": 403, "q": "Group identical consecutive values in pandas DataFrame", "d": "I have the following pandas dataframe : I want to store the values in another dataframe such as every group of consecutive indentical values make a labeled group like this : The column A represent the value of the group and B represents the number of occurences. this is what i've done so far: It works but it's a bit messy. Do you think of a shortest/better way of doing this ?", "q_apis": "identical values DataFrame values values value", "io": " a 0 0 1 0 2 1 3 2 4 2 5 2 6 3 7 2 8 2 9 1 <s> A B 0 0 2 1 1 1 2 2 3 3 3 1 4 2 2 5 1 1 ", "apis": "groupby ne shift cumsum agg first count groupby ne shift cumsum agg first count", "code": ["new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .agg(A=('a','first'),B=('a','count')) )\n\nprint(new_df)\n", "new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .a\n            .agg({'A':'first','B':'count'}) )\n"], "link": "https://stackoverflow.com/questions/58771645/group-identical-consecutive-values-in-pandas-dataframe"}
{"id": 341, "q": "Rolling over values from one column to other based on another dataframe", "d": "I have two dataframes: Now I have another dataframe which only have unique IDs from first dataframe, and dates that represent months: So based on first dataframe I need to fill the values based on the value that is in the first dataframe that is within the corresponding month ( so for example I take the last value for the from and put it in the column in . IF there are no other values for that ID just fill all the remaining columns in with the value in the most right filled column(roll over to the right). So the end result is exactly like this", "q_apis": "values unique first first values value first month take last value put values all columns value right right", "io": "ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31 A1 A2 A3 A4 A5 <s> ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31 A1 8500 8500 8500 8500 8500 8500 8500 A2 NA 1900 1900 1900 1900 1900 1900 A3 NA NA NA 3000 110 0 0 A4 NA NA NA NA NA 10 10 A5 NA NA NA NA NA NA 500 ", "apis": "to_datetime to_period to_datetime add groupby last unstack ffill reset_index rename_axis columns", "code": ["month_ends = pd.to_datetime(df1.DatePaid).dt.to_period('M')\n# also\n# month_ends = pd.to_datetime(df1.DatePaid).add(pd.offsets.MonthEnd(0))\n\n(df1.groupby(['ID', month_ends])\n    ['Remaining'].last()\n    .unstack(-1)\n    .ffill(1)\n    .reset_index()\n    .rename_axis(columns=None)\n)\n"], "link": "https://stackoverflow.com/questions/60455539/rolling-over-values-from-one-column-to-other-based-on-another-dataframe"}
{"id": 42, "q": "Reformat Dataframe / Add rows when condition is met", "d": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated!", "q_apis": "add value add length date any", "io": " ColA ColB 2021-03-09 1 2021-03-09 3 2021-03-10 2 2021-03-10 1 2021-03-10 2 2021-03-11 2 <s> ColA ColB 2021-03-09 1 2021-03-09 1 2021-03-09 1 2021-03-09 1 2021-03-10 1 2021-03-10 1 2021-03-10 1 2021-03-10 1 2021-03-10 1 2021-03-11 1 2021-03-11 1 ", "apis": "DataFrame repeat assign", "code": ["import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"], "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met"}
{"id": 152, "q": "Filtering DataFrame rows which have overlapping values cross-columns", "d": "I have a dataframe that reflects rows with at least one conflict inside that row. Rows 0-3 and rows 4-5 have overlapping values with other rows, but the overlap occurs across various columns. How can I: drop all but the first row of each overlap group, in a table-wise or series-wise manner, ie without using down the rows This would be the output (though don't care about index): Below snippet for easy repro", "q_apis": "DataFrame values columns at values columns drop all first index", "io": " email id1 id2 id3 0 de@l Z7 Q4 Q4 1 sco@g Q4 Z7 Q4 2 alpha@n Q4 Z7 Z7 3 numer@o Z7 Z7 Q4 4 endo@c D8 D8 L1 5 chrono@k L1 L1 D8 <s> email id1 id2 id3 0 de@l Z7 Q4 Q4 4 endo@c D8 D8 L1 ", "apis": "filter to_numpy Series index index duplicated", "code": ["L = [frozenset(x) for x in df.filter(like='id').to_numpy()]\ndf = df[~pd.Series(L, index=df.index).duplicated()]\n"], "link": "https://stackoverflow.com/questions/65035031/filtering-dataframe-rows-which-have-overlapping-values-cross-columns"}
{"id": 479, "q": "how to slice a dataframe and reassemble it into a new dataframe", "d": "I get a dataframe like this: Slice every two columns and then reorganize to form a new dataframe, as follows: I have tried but somethings wrong happened! Thank you.", "q_apis": "get columns", "io": "A YEAR2000 B YEAR2001 C YEAR2002 a 1 b 3 a 7 b 3 c 5 e 6 c 6 d 2 f 3 e 1 g 0 <s> type YEAR2000 YEAR2001 YEAR2002 a 1 7 b 3 3 c 6 5 d 2 e 1 6 f 3 g 0 ", "apis": "DataFrame columns DataFrame columns DataFrame columns merge merge merge merge", "code": ["df1 = pd.DataFrame([['a', 1], ['b', 3], ['c', 6]],columns=['letter', 'number'])\ndf2 = pd.DataFrame([['b', 3], ['c', 5], ['d', 2], ['e', 1]],columns=['letter', 'number'])\ndf3 = pd.DataFrame([['a', 7], ['e', 6], ['f', 3], ['g', 0]],columns=['letter', 'number'])\npd.merge(pd.merge(df1, df2, how='outer', on='letter'), df3, how='outer', on='letter')\n", "df1.merge(df2, how='outer', on='letter').merge(df3, how='outer', on='letter')\n"], "link": "https://stackoverflow.com/questions/56122209/how-to-slice-a-dataframe-and-reassemble-it-into-a-new-dataframe"}
{"id": 151, "q": "Organize data based on a weird column distribution in pandas", "d": "Is there an elegant way of segment data in a dataframe in which the first row includes the name of the data owner, and the second row includes headers, with all the data organized below? I have this: I need to order that so that I can analyze it in something like: I though about making different dataframes, but that would be a waste of resources. Is there a more elegant way of doing this? Thanks.", "q_apis": "first name second all", "io": "0 n_1 NaN NaN NaN NaN n_2 NaN NaN NaN NaN ... n_3 NaN NaN NaN NaN n_4 NaN NaN NaN NaN 1 V1 V2 V3 V4 V5 V1 V2 V3 V4 V5 ... V1 V2 V3 V4 V5 V1 V2 V3 V4 V5 2 45 43 30 32 NaN 45 52 47 47 NaN ... 45 57 51 50 NaN 45 51 47 50 NaN 3 50 53 38 38 NaN 50 55 50 41 NaN ... 50 51 48 49 NaN 50 53 52 52 1 4 50 54 37 41 NaN 50 53 49 49 1 ... 50 54 50 47 NaN 50 54 48 41 1 5 50 51 40 39 NaN 50 53 50 48 NaN ... 50 53 50 49 NaN 50 51 49 50 NaN 6 50 53 47 50 NaN 50 50 47 35 NaN ... 50 55 44 34 NaN 50 50 47 47 NaN 7 50 51 47 45 NaN 50 52 48 48 1 ... 50 51 48 46 NaN 50 51 47 50 NaN 8 50 52 50 50 NaN 50 50 47 50 NaN ... 50 51 47 48 NaN NaN NaN NaN NaN NaN 9 NaN NaN NaN NaN NaN 50 54 51 53 NaN ... 50 52 48 51 NaN NaN NaN NaN NaN NaN <s> 0 Own V1 V2 V3 V4 V5 1 n_1 45 43 30 32 NaN 2 n_1 50 53 38 38 NaN 3 n_1 50 54 37 41 NaN 4 n_1 50 51 40 39 NaN 5 n_1 50 53 47 50 NaN 6 n_1 50 51 47 45 NaN 7 n_1 50 52 50 50 NaN 8 n_2 45 52 47 47 NaN 9 n_2 50 55 50 41 NaN 10 n_2 50 53 49 49 1 11 n_2 50 53 50 48 NaN 12 n_2 50 50 47 35 NaN 13 n_2 50 52 48 48 1 14 n_2 50 50 47 50 NaN 15 n_2 50 54 51 53 NaN 16 n_3 45 57 51 50 NaN 17 n_3 50 51 48 49 NaN 18 n_3 50 54 50 47 NaN 19 n_3 50 53 50 49 NaN 20 n_3 50 55 44 34 NaN 21 n_3 50 51 48 46 NaN 22 n_3 50 51 47 48 NaN 23 n_3 50 52 48 51 NaN 24 n_4 45 51 47 50 NaN 25 n_4 50 53 52 52 1 26 n_4 50 54 48 41 1 27 n_4 50 51 49 50 NaN 28 n_4 50 50 47 47 NaN 29 n_4 50 50 51 47 NaN ", "apis": "read_csv iloc T replace ffill replace columns MultiIndex from_frame stack reset_index rename columns iloc", "code": ["df = pd.read_csv('your_file.csv',headers=None)\n\ns = df.iloc[:2].T.replace('NaN',np.nan).ffill() # you may need to be smart with your replace here. \ndf.columns = pd.MultiIndex.from_frame(s)\n\ndf1 = df.stack(0).reset_index(1).rename(columns={0 : 'own'}).iloc[2:]\n"], "link": "https://stackoverflow.com/questions/65043847/organize-data-based-on-a-weird-column-distribution-in-pandas"}
{"id": 554, "q": "How to identify string repetition throughout rows of a column in a Pandas DataFrame?", "d": "I'm trying to think of a way to best handle this. If I have a data frame like this: How would I go about setting up a search and find to locate and identify repetition in the middle or on edges or complete strings? Sorry the formatting looks bad Basically I have the module, line item, and formula columns filled in, but I need to figure out some sort of search function that I can apply to each of the last 3 columns. I'm not sure where to start with this. I want to match any repetition that occurs between 3 or more words, including if for example a formula was and that occurred 4 times in the Formula column, I'd want to give a yes to the boolean column \"repetition\" return on the \"Where repeated\" column and a list of every module/line item combination where it occurred on the last column. I'm sure I can tweak it more to fit my needs once I get started.", "q_apis": "DataFrame item columns apply last columns where start any between item where last get", "io": "1 + 2 + 3 + 4 <s> 1 + 2 + 3 + 4", "apis": "value value item values where now value value value value value value update value value loc contains value index update value value keys values value append value value append append DataFrame where where unique where append loc where append loc append merge DataFrame index where where where item", "code": ["#the dictionary will have a key containing row number and the value we searched for\n#the value will contain the module and line item values\nresult = {}\n#create a rownumber variable so we know where in the dataset we are\nrownumber = -1\n#now we just iterate over every row of the Formula series\nfor row in df['Formula']:\n    rownumber +=1\n    #and also every relevant value within that cell\n    for value in row.split('+'):\n        #we clean the value from trailing/preceding whitespace\n        value = value.strip()\n        #and then we return our key and value and update our dictionary\n        key = 'row:|:'+str(rownumber)+':|:'+value\n        value = (df.loc[((df.Formula.str.contains(value,regex=False))) & (df.index!=rownumber),['Module','Line Item']])\n        result.update({key:value})\n", "where_raw = []\nwhat_raw = []\nrows_raw = []\nfor key,value in zip(result.keys(),result.values()):\n    if 'Empty' in str(value):\n        continue\n    else:\n        where_raw.append(list(value['Module']+' '+value['Line Item']))\n        what_raw.append(key.split(':|:')[2])\n        rows_raw.append(int(key.split(':|:')[1]))\n\ntempdf = pd.DataFrame({'row':rows_raw,'where':where_raw,'what':what_raw})\n", "where = []\nwhat = []\nrows = []        \n\nfor row in tempdf.row.unique():\n    where.append(list(tempdf.loc[tempdf.row==row,'where']))\n    what.append(list(tempdf.loc[tempdf.row==row,'what']))\n    rows.append(row)\nresult = df.merge(pd.DataFrame({'index':rows,'where':where,'what':what}))\n", " print(result)\n Module     Line Item   Formula                                         what                                                 where\n Module 1   Line Item 1 hello[SUM: hello2]                              ['hello[SUM: hello2]']                               [['Module 1 Line Item 2']]\n Module 1   Line Item 2 goodbye[LOOKUP: blue123] + hello[SUM: hello2]   ['goodbye[LOOKUP: blue123]', 'hello[SUM: hello2]']   [['Module 2 Line Item 1'], ['Module 1 Line Item 1']]\n Module 2   Line Item 1 goodbye[LOOKUP: blue123] + some other line item ['goodbye[LOOKUP: blue123]']                         [['Module 1 Line Item 2']]\n"], "link": "https://stackoverflow.com/questions/53602854/how-to-identify-string-repetition-throughout-rows-of-a-column-in-a-pandas-datafr"}
