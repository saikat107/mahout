{"id": 114, "q": "Create new rows in Pandas, by adding to previous row, looping until x number of rows are made", "d": "Input: Output: Desired Output: Starting from an initial reference row of list \"tw\" I need to add 1 to the starting value and keep going. How do I loop and keep creating rows so that the next row is the previous row +1, I need to do this for 3000 rows. A lot of solutions I've seen require me to create lists and add to the pandas dataframe however I cannot manually create 3000 lists and then manually add them to my dataframe. There must be a way to loop over this, please help!", "q_apis": "add value add add", "io": " 1 2 3 4 0 4 7 3 5 <s> 1 2 3 4 0 4 7 3 5 1 5 8 4 6 2 6 9 5 7 ... ... ... ... ... 3000 3003 3006 3002 3004 ", "apis": "DataFrame columns reindex fillna cumsum", "code": ["df = (pd.DataFrame([tw], columns=[1,2,3,4])\n        .reindex(range(N))\n        .fillna(1, downcast='infer')\n        .cumsum())\n"], "link": "https://stackoverflow.com/questions/65863722/create-new-rows-in-pandas-by-adding-to-previous-row-looping-until-x-number-of"}
{"id": 108, "q": "Assigning column to larger DataFrame in specific positions", "d": "I have two lists. I want to create C: Basically, where in there is a I want to have a value of , where there is a , a . In reality, contains around 10k elements, around 40k and I have many of them. I am working with a pandas.DataFrame (each \"array\" is a column of two different dataframes, I have to \"fit\" in by transforming it into ). I have done it with a for loop, but I am wondering how to do it in a better way. Thank you.", "q_apis": "DataFrame where value where contains DataFrame array", "io": "A = [0, 1, 0, 0, 1, 1, 0, 1] B = [2, 3, 4, 5] <s> C = [NaN, 2, NaN, NaN, 3, 4, NaN, 5] ", "apis": "pop", "code": ["[None if i==0 else B.pop(0) for i in A]\n", "[None, 2, None, None, 3, 4, None, 5]\n"], "link": "https://stackoverflow.com/questions/66034318/assigning-column-to-larger-dataframe-in-specific-positions"}
{"id": 537, "q": "Change values in columns of dataframe depending on values of other columns (values come from lists)", "d": "I have a Data Frame in python, for example this one: The code for the creation of the dataframe: Let the list1 be ['A','C','E'] and list2 be ['B','D','F']. What I want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). I have tried which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. Could someone help please? This is my expected output:", "q_apis": "values columns values columns values last replace right", "io": " col1 col2 col3 col4 0 A C B D 1 C E E A 2 E A E A 3 A D D D 4 B B B B 5 D D D D 6 F F A F 7 E E E E 8 B B B B <s> col1 col2 col3 col4 0 A B D 1 C E E A 2 E A E A 3 A D D 4 B B B B 5 D D D D 6 F F A F 7 E E E E 8 B B B B ", "apis": "isin isin any loc", "code": ["m1 = df['col1'].isin(list1)\nm2 = df[['col2', 'col3', 'col4']].isin(list2).any(1)\n\ndf.loc[m1 & m2, 'col2'] = ''\n"], "link": "https://stackoverflow.com/questions/54005974/change-values-in-columns-of-dataframe-depending-on-values-of-other-columns-valu"}
{"id": 645, "q": "Shift NaNs to the end of their respective rows", "d": "I have a DataFrame like : What I want to get is This is my approach as of now. Is there any efficient way to achieve this ? Here is way to slow . Thank you for your assistant!:) My real data size", "q_apis": "DataFrame get now any size", "io": " 0 1 2 0 0.0 1.0 2.0 1 NaN 1.0 2.0 2 NaN NaN 2.0 <s> Out[116]: 0 1 2 0 0.0 1.0 2.0 1 1.0 2.0 NaN 2 2.0 NaN NaN ", "apis": "values left", "code": ["df[:] = justify(df.values, invalid_val=np.nan, axis=1, side='left')\n"], "link": "https://stackoverflow.com/questions/45970751/shift-nans-to-the-end-of-their-respective-rows"}
{"id": 501, "q": "Add row values of all columns when a particular column value is null until it gets the not null values?", "d": "I have a data frame like this: If col1 value is null I want to add all the values of other columns untill it finds the notnull element in col1 The data frame i am looking for should look like: How to do in in most efficient way using python/pandas", "q_apis": "values all columns value values value add all values columns notnull", "io": "df col1 col2 col3 col4 A 12 34 XX B 20 25 PP B nan nan nan nan P 54 nan nan R nan nan nan nan nan PQ C D 32 SS R S 32 RS <s> col1 col2 col3 col4 A 12 34 XX B 20 25 PP B PR 54 PQ C D 32 SS R S 32 RS ", "apis": "notna cumsum ffill select_dtypes columns fillna mean dtype join groupby agg reset_index drop reset_index", "code": ["df['new'] = df['col1'].notna().cumsum()\ndf['col1'] = df['col1'].ffill()\nc = df.select_dtypes(object).columns\ndf[c] = df[c].fillna('')\n\nf = lambda x: x.mean() if np.issubdtype(x.dtype, np.number) else ''.join(x)\ndf = df.groupby(['col1', 'new']).agg(f).reset_index(level=1, drop=True).reset_index()\n"], "link": "https://stackoverflow.com/questions/55356991/add-row-values-of-all-columns-when-a-particular-column-value-is-null-until-it-ge"}
{"id": 181, "q": "Pandas - shifting a rolling sum after grouping spills over to following groups", "d": "I might be doing something wrong, but I was trying to calculate a rolling average (let's use sum instead in this example for simplicity) after grouping the dataframe. Until here it all works well, but when I apply a shift I'm finding the values spill over to the group below. See example below: Expected result: Result I actually get: You can see the result of A2 gets passed to B3 and the result of B5 to C6. I'm not sure this is the intended behaviour and I'm doing something wrong or there is some bug in pandas? Thanks", "q_apis": "rolling sum groups rolling sum all apply shift values get", "io": "X A 0 NaN 1 NaN 2 3.0 B 3 NaN 4 NaN 5 3.0 C 6 NaN 7 NaN 8 3.0 <s> X A 0 NaN 1 NaN 2 3.0 B 3 5.0 4 NaN 5 3.0 C 6 5.0 7 NaN 8 3.0 ", "apis": "groupby rolling sum groupby shift groupby transform rolling sum shift", "code": ["grouped_df = (df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum()\n                .groupby(level=0).shift(periods=1)\n             )\n", "grouped_df = (df.groupby('X')['Y']\n                .transform(lambda x: x.rolling(window=2, min_periods=2)\n                                      .sum().shift(periods=1))\n             )\n"], "link": "https://stackoverflow.com/questions/64592950/pandas-shifting-a-rolling-sum-after-grouping-spills-over-to-following-groups"}
{"id": 183, "q": "merge pandas dataframes under new index level", "d": "I have 2 s and that I want to combine into a single dataframe : Dataframe should contain one more column index level than and , and contain each under its own level-0 identifier, like so: Any ideas as to how to do this? It's a bit like ing the two frames: ...but using an additional level, instead of a suffix, to prevent name collisions. I tried: I could use loops to build up the series-by-series, but that doesn't seem right. Many thanks.", "q_apis": "merge index combine index name right", "io": "act #have a b 0 0.853910 0.405463 1 0.822641 0.255832 2 0.673718 0.313768 exp #have a c 0 0.464781 0.325553 1 0.565531 0.269678 2 0.363693 0.775927 <s> df #want act exp a b a c 0 0.853910 0.405463 0.464781 0.325553 1 0.822641 0.255832 0.565531 0.269678 2 0.673718 0.313768 0.363693 0.775927 ", "apis": "concat keys", "code": ["pd.concat([act, exp], axis=1, keys=['act', 'exp'])\n"], "link": "https://stackoverflow.com/questions/64577375/merge-pandas-dataframes-under-new-index-level"}
{"id": 623, "q": "Python - Add a column to a dataframe containing a value from another row based on condition", "d": "My dataframe looks like this: Now I need to add another column which conatains the difference between the value of column from the current row and the value of column from the row with the same number () and the group () that is written in . Exeption: If equals , and are the same. So the result should be: Explanation for the first two rows: First row: equals -> = Second row: search for the row with the same (123) and as (A1) and calculate of the current row minus of the referenced row (7.3 - 5.0 = 2.3). I thought I might need to use groupby() and apply(), but how? Hope my example is detailed enough, if you need any further information, please ask :)", "q_apis": "value add difference between value value equals first equals groupby apply any", "io": "+-----+-------+----------+-------+ | No | Group | refGroup | Value | +-----+-------+----------+-------+ | 123 | A1 | A1 | 5.0 | | 123 | B1 | A1 | 7.3 | | 123 | B2 | A1 | 8.9 | | 123 | B3 | B1 | 7.9 | | 465 | A1 | A1 | 1.4 | | 465 | B1 | A1 | 4.5 | | 465 | B2 | B1 | 7.3 | +-----+-------+----------+-------+ <s> +-----+-------+----------+-------+----------+ | No | Group | refGroup | Value | refValue | +-----+-------+----------+-------+----------+ | 123 | A1 | A1 | 5.0 | 5.0 | | 123 | B1 | A1 | 7.3 | 2.3 | | 123 | B2 | A1 | 8.9 | 3.9 | | 123 | B3 | B1 | 7.9 | 0.6 | | 465 | A1 | A1 | 1.4 | 1.4 | | 465 | B1 | A1 | 4.5 | 3.1 | | 465 | B2 | B1 | 7.3 | 2.8 | +-----+-------+----------+-------+----------+ ", "apis": "merge where value value", "code": ["df_out = df.merge(df, \n                  left_on=['No','refGroup'], \n                  right_on=['No','Group'], \n                  suffixes=('','_ref'))\n\ndf['refValue'] = np.where(df_out['Group'] == df_out['refGroup'],\n                          df_out['value'],\n                          df_out['value'] - df_out['value_ref'])\n\ndf\n"], "link": "https://stackoverflow.com/questions/50860328/python-add-a-column-to-a-dataframe-containing-a-value-from-another-row-based-o"}
{"id": 390, "q": "New dataframe which contais a certain value within each group", "d": "I have a dataframe as below I want to group by & and get a new dataframe with all the groups that contains 6 or 14 When I use the code below I accurately get the groups which have either 6 or 14 as below I am just not able to use this information to get the new dataframe which has the groups that are . The expected output is the new dataframe as below. Can anyone guide?", "q_apis": "value get all groups contains get groups get groups", "io": "User eve Ses a 123 1 a 123 2 a 123 3 a 123 4 a 123 5 a 123 6 a 456 1 a 456 2 a 456 3 a 456 4 a 456 5 a 456 14 a 456 7 a 456 8 a 456 9 a 456 10 a 888 1 a 888 2 a 888 3 a 888 4 a 888 5 a 888 5 a 888 7 a 888 8 b 123 1 b 123 2 b 123 3 b 123 4 b 123 5 b 123 6 b 456 1 b 456 2 b 456 3 b 456 4 b 456 5 b 456 9 b 456 7 b 456 8 b 456 9 b 456 10 b 888 1 b 888 2 b 888 3 b 888 4 b 888 5 b 888 6 b 888 7 b 888 8 <s> User eve Ses a 123 1 a 123 2 a 123 3 a 123 4 a 123 5 a 123 6 a 456 1 a 456 2 a 456 3 a 456 4 a 456 5 a 456 14 a 456 7 a 456 8 a 456 9 a 456 10 b 123 1 b 123 2 b 123 3 b 123 4 b 123 5 b 123 6 b 888 1 b 888 2 b 888 3 b 888 4 b 888 5 b 888 6 b 888 7 b 888 8 ", "apis": "groupby filter any any", "code": ["df = df.groupby(['User','eve']).filter(lambda x: (x['Ses']==6).any()|(x['Ses']==14).any())\n"], "link": "https://stackoverflow.com/questions/59189297/new-dataframe-which-contais-a-certain-value-within-each-group"}
{"id": 588, "q": "Repeating a series to create a df in python", "d": "I have a series, How do I repeat this column 9 times to create a dataframe. Expected output: is I can use but this is a bit messy. I tried but it wouldn't work along , and isn't what I need Thanks", "q_apis": "repeat", "io": " A 0 1.5 1 2.5 2 1.3 <s> A A ... A 0 1.5 1.5 ... 1.5 1 2.5 2.5 ... 2.5 2 1.3 1.3 ... 1.3 ", "apis": "concat keys", "code": ["pd.concat([s for i in range(9)], axis=1)\n", "keys=[f'A{i}' for i in range(9)]\n"], "link": "https://stackoverflow.com/questions/52151831/repeating-a-series-to-create-a-df-in-python"}
{"id": 94, "q": "Create pandas duplicate rows based on the number of items in a list type column", "d": "I have a data frame like this, Now I want to create new rows based on the number of values in the col2 list where the col1 values will be same so the final data frame would look like, I am looking for some pandas short cuts to do it more efficiently", "q_apis": "items values where values", "io": " df col1 col2 A [1] B [1,2] A [2,3,4] C [1,2] B [4] <s> df col1 col2 A [1] B [1] B [2] A [2] A [3] A [4] C [1] C [2] B [4] ", "apis": "explode apply", "code": ["df2 = df.explode('col2')\n\ndf2['col2'] = df2['col2'].apply(lambda x: [x])\n"], "link": "https://stackoverflow.com/questions/66255188/create-pandas-duplicate-rows-based-on-the-number-of-items-in-a-list-type-column"}
{"id": 578, "q": "Pandas DataFrame filter", "d": "My question is about the pandas.DataFrame.filter command. It seems that pandas creates a copy of the data frame to write any changes. How am I able to write on the data frame itself? In other words: Output: Desired Output:", "q_apis": "DataFrame filter DataFrame filter copy any", "io": " col1 col2 0 1 3 1 2 4 <s> col1 col2 0 10 3 1 2 4 ", "apis": "filter columns loc", "code": ["cols = df.filter(regex='col1').columns \ndf.loc[0, cols]=10\n"], "link": "https://stackoverflow.com/questions/52591303/pandas-dataframe-filter"}
{"id": 370, "q": "Pandas conditionally creating a new dataframe using another", "d": "I have a list; I want to create another where entries corresponding to positive values above are sum of positives, and those corresponding to negative values above are sum negatives. So the desired output is: I am doing this: So basically i was trying to create an empty dataframe like raw, and then conditionally fill it. However, the above method is failing. Even when i try to create a new column instead of a new df, it fails: The best solution I've found so far is this: However, I dont understand what is wrong with the other approaches. Is there something I am missing here?", "q_apis": "where values sum values sum empty", "io": "orig= [2, 3, 4, -5, -6, -7] <s> final = [9, 9, 9, 18, 18, 18] ", "apis": "loc values", "code": ["final.loc[raw['raw'] > 0, 'final'] = sum_pos.values\n\nprint(final)\n"], "link": "https://stackoverflow.com/questions/50121789/pandas-conditionally-creating-a-new-dataframe-using-another"}
{"id": 225, "q": "Merge two dataframes with uncertainty on keys", "d": "I am trying to use 'merge' to combine two data frames with shape: Those looks as follows: What I am looking for is to merge columns 'A' and 'B' with a defined uncertainty. For example + - 0.5. I don't have clear how to handle this. What I was trying to do is to manually add an uncertainty: After this, I do the merge: But here I got stuck because I can not figure it out how to use a conditional merge. The idea is to merge all those rows were columns 'A' and 'B' be the same with a definite certainty The expected output would be:", "q_apis": "keys merge combine shape merge columns add merge merge merge all columns", "io": "In [29]: df1 Out[29]: ID1 A B 0 10 1 3 1 32 4 5 2 53 2 2 3 65 5 9 4 3 4 3 In [32]: df2 Out[32]: ID2 A B 0 68 9 6 1 35 5 5 2 93 3 1 3 5 7 9 4 23 4 3 <s> df3: ID1 A B ID2 A B 1 32 4 5 35 5 5 2 53 2 2 93 3 1 4 3 4 3 23 4 3 ", "apis": "assign merge assign assign assign query drop columns to_string index DataFrame DataFrame columns merge_asof merge_asof sort_values sort_values sort_values sort_values value value merge_asof assign sort_values assign sort_values to_string index to_string index", "code": ["dfs = (df1.assign(foo=1).merge(df2.assign(foo=1), on=\"foo\", suffixes=(\"\",\"_df2\"))\n .assign(adiff=lambda x: x[\"A\"]-x[\"A_df2\"])\n .assign(bdiff=lambda x: x[\"B\"]-x[\"B_df2\"])\n .query(\"adiff>=-1 and adiff<=1 and bdiff>=-1 and bdiff<=1\")\n .drop(columns=[\"adiff\",\"bdiff\",\"foo\"])\n)\n\nprint(dfs.to_string(index=False))\n", "df1 = pd.DataFrame({'ID1':[10,32,53,65,3],'A':[1,4,2,5,4],'B':[3,5,2,9,3]})\ndf2 = pd.DataFrame({'ID2':[68,35,93,5,23],'A':[9,5,3,7,4],'B':[6,5,1,9,3]})\n\n\n# find nearest row in df2 when matching on columns A or B\ndfmab = pd.merge_asof(\n    (pd.merge_asof(df1.sort_values(\"A\"), df2.sort_values(\"A\"), \n                   on=\"A\", direction=\"nearest\", suffixes=(\"\",\"_Adf2\"))\n    ).sort_values(\"B\") \n    ,df2.sort_values(\"B\"),\n    on=\"B\", direction=\"nearest\", suffixes=(\"\",\"_Bdf2\")\n)\n\n# calculate a value that represents value of row\ndfmd = pd.merge_asof(\n    df1.assign(d=lambda x: x[\"A\"]*x[\"B\"]).sort_values(\"d\"),\n    df2.assign(d=lambda x: x[\"A\"]*x[\"B\"]).sort_values(\"d\"),\n    on=\"d\", direction=\"nearest\", suffixes=(\"\",\"_Ddf2\")\n)\n\nprint(dfmab.to_string(index=False))\nprint(dfmd.to_string(index=False))\n"], "link": "https://stackoverflow.com/questions/63609893/merge-two-dataframes-with-uncertainty-on-keys"}
{"id": 640, "q": "Pandas : Get the least number of records so all columns have at least one non null value", "d": "I have a dataframe with 62 columns that are largely null. Some records have multiple columns with non-null values, others just a single non-null. I'm wondering if there's a way to use .dropna or other strategy to return the least number of rows with each column having at least one non-null value. For a simplified example Would return ...", "q_apis": "all columns at value columns columns values dropna at value", "io": " a b c NaN 1 NaN 1 NaN NaN NaN NaN NaN NaN 1 1 <s> a b c 1 NaN NaN NaN 1 1 ", "apis": "sample DataFrame columns empty notnull sum idxmax append columns drop columns columns loc notnull any loc", "code": ["import pandas as pd\nimport numpy as np\n\n# sample dataframe\ndf = pd.DataFrame({'a':[np.nan,1,np.nan,np.nan],'b':[1, np.nan, np.nan, 1],'c':[np.nan, np.nan, np.nan, 1]})\ndf_orig = df\ncols = df.columns.tolist()\n\nrows = []\nwhile not df.empty:\n    ## Find the row with most non-null column entries\n    x = df.notnull().sum(axis=1).idxmax() # edit - fix for null/nonnull\n    ## Add the row to our list and continue\n    rows.append(x)\n    ## Remove the columns from our dataframe\n    df = df.drop(columns=df.columns[df.loc[[x]].notnull().any()].tolist())\n\n## Access the dataframe with only 'essential' rows\ndf_orig.loc[rows]\n"], "link": "https://stackoverflow.com/questions/49925888/pandas-get-the-least-number-of-records-so-all-columns-have-at-least-one-non-nu"}
{"id": 457, "q": "Python[pandas]: Select certain rows by index of another dataframe", "d": "I have a dataframe and I would select only rows that contain index value into df1.index. for Example: and these indexes I would like this output: thanks", "q_apis": "index select index value index", "io": "In [96]: df Out[96]: A B C D 1 1 4 9 1 2 4 5 0 2 3 5 5 1 0 22 1 3 9 6 <s> In [96]: df Out[96]: A B C D 1 1 4 9 1 3 5 5 1 0 22 1 3 9 6 ", "apis": "loc index index loc index index loc index intersection index", "code": ["df = df.loc[df.index & df1.index]\ndf = df.loc[np.intersect1d(df.index, df1.index)]\ndf = df.loc[df.index.intersection(df1.index)]\n"], "link": "https://stackoverflow.com/questions/48864923/pythonpandas-select-certain-rows-by-index-of-another-dataframe"}
{"id": 316, "q": "Pandas assignment and copy", "d": "If we run the following code, We get Whereas, if we run the following, We get I know there's a concept of pass by object reference in python. Why don't the in the second code gets copied? Thanks", "q_apis": "copy get get second", "io": " 0 0 1.298967 1 -0.887922 2 1.913559 3 -0.082032 4 -0.466594 .. ... 95 -0.845137 96 0.628542 97 -0.588897 98 0.464374 99 0.267946 <s> 0 a 0 -0.510875 1 1 0.401580 1 2 -0.037484 1 3 -0.935115 1 4 -1.108471 1 .. ... .. 95 0.362075 1 96 -1.017991 1 97 1.881081 1 98 0.376828 1 99 0.771661 1 ", "apis": "assign DataFrame columns assign DataFrame DataFrame copy DataFrame copy DataFrame", "code": ["def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) #doesnot return the changed columns\n", "def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n    return df\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df))\n", "def f(df):\n    df = df\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n", "def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) # doesnot return the a column\n", "def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n    return df\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df)) #returns the column a\n"], "link": "https://stackoverflow.com/questions/61024520/pandas-assignment-and-copy"}
{"id": 409, "q": "Replace repetitive number with NAN values except the first, in pandas column", "d": "I have a data frame like this, Now we can see that there is continuous occurrence of A, B and C. I want only the rows where the occurrence is starting. And the other values of the same occurrence will be nan. The final data frame I am looking for will look like, I can do it using for loop and comparing, But the execution time will be more. I am looking for pythonic way to do it. Some panda shortcuts may be.", "q_apis": "values first where values time", "io": "df col1 col2 1 A 2 A 3 B 4 C 5 C 6 C 7 B 8 B 9 A <s> df col1 col2 1 A 2 NA 3 B 4 C 5 NA 6 NA 7 B 8 NA 9 A ", "apis": "where ne shift where ne shift ne shift dtype bool", "code": ["df['col2'] = df['col2'].where(df['col2'].ne(df['col2'].shift()))\n#alternative\n#df['col2'] = np.where(df['col2'].ne(df['col2'].shift()), df['col2'], np.nan)\n", "print (df['col2'].ne(df['col2'].shift()))\n0     True\n1    False\n2     True\n3     True\n4    False\n5    False\n6     True\n7    False\n8     True\nName: col2, dtype: bool\n"], "link": "https://stackoverflow.com/questions/58538845/replace-repetitive-number-with-nan-values-except-the-first-in-pandas-column"}
{"id": 329, "q": "How to get absolute difference amongst all the values of a column with each other?", "d": "I am trying to find difference among-st all values in key column keeping these 4 digit code as my index value. I tried using pivot for this operation but failed. It would be really helpful if I can get the approach for this presentation. df1 Expected df", "q_apis": "get difference all values difference all values index value pivot get", "io": "Name | Key | 1001 | 1002 | 1003 | Abb AB 5 8 10 Baa BA 10 11 33 Cbb CB 12 40 90 <s> Code | Key | AB | BA | CB | 1001 AB 0 5 7 1001 BA 5 0 2 1001 CB 7 2 0 1002 . . . 1003 ", "apis": "loc repeat index values columns columns columns droplevel", "code": ["df2 = df1.loc[np.repeat(df1.index.values,len(df1.columns))] \ndf2.columns = df2.columns.droplevel(0)\n"], "link": "https://stackoverflow.com/questions/60763203/how-to-get-absolute-difference-amongst-all-the-values-of-a-column-with-each-othe"}
{"id": 353, "q": "Group together matched pairs across multiple columns Python", "d": "Thank you for reading. I have a dataframe which looks like this: Each row consists of a match between two IDs (e.g. ID1 from Col_A matches to ID2 from Col_B on the first row). In the example above, all 5 IDs are connected (1 is connected to 2, 2 to 3, 2 to 4, 1 to 5). I therefore want to create a new column which clusters all of these rows together so that I can easily access each group of matched pairs: I have not yet been able to find a similar question, but apologies if this is a duplicate. Many thanks in advance for any advice.", "q_apis": "columns between first all all any", "io": "Col_A Col_B Col_C Col_D Col_E 1 2 null null null 1 null 3 null null null 2 3 null null null 2 null 4 null 1 null null null 5 <s> Col_A Col_B Col_C Col_D Col_E Group ID 1 2 null null null 1 1 null 3 null null 1 null 2 3 null null 1 null 2 null 4 null 1 1 null null null 5 1 ", "apis": "apply dropna to_numpy to_numpy apply", "code": ["import networkx as nx\nd_edge = df.apply(lambda x: x.dropna().to_numpy(), axis=1)\nG = nx.from_edgelist(d_edge.to_numpy().tolist())\ncc_list = list(nx.connected_components(G))\ndf['groupid'] = d_edge.apply(lambda  x: [n for n, i in enumerate(cc_list) if x[0] in i][0] + 1)\ndf\n"], "link": "https://stackoverflow.com/questions/60153802/group-together-matched-pairs-across-multiple-columns-python"}
{"id": 242, "q": "Easy way to find find itinerant max value in grouped pandas list", "d": "I have a dataset where I have multiple value entries per year and some properties per entry. I want to find the maximum value per year and return that as a new data frame (to keep the other properties in the data frame), but only if the value in a year is greater than what it was in the years before (something like the \"All-time record value per year\"). So far I can find the max value per year, e.g. where the output then is This is almost what I want, expect for 2014 where I would like the value of 2013 with its according properties to go (since the value was greater in 2013 than it was in 2014). So the desired outcome would be Is there a good way to achieve this with pandas?", "q_apis": "max value where value year value year value year time value year max value year where where value value", "io": " Year Value Property 0 2012 35 Property B 1 2013 43 Property D 2 2014 37 Property C 3 2015 41 Property F <s> Year Value Property 0 2012 35 Property B 1 2013 43 Property D 2 2014 43 Property D 3 2015 43 Property D ", "apis": "cummax merge groupby first", "code": ["df_sorted_max['cummax_value'] = df_sorted_max.Value.cummax()\n\nprint(\n    df_sorted_max.merge(\n        df_sorted_max.groupby('cummax_value')[['Property']].first(),\n        on=\"cummax_value\"\n    )\n)\n"], "link": "https://stackoverflow.com/questions/63091593/easy-way-to-find-find-itinerant-max-value-in-grouped-pandas-list"}
{"id": 399, "q": "python pandas time series year extraction", "d": "I have a DF containing timestamps: I would like to extract the year from each timestamp, creating additional column in the DF that would look like: Obviously I can go over all DF entries stripping off the first 4 characters of the date. Which is very slow. I wonder if there is a fast python-way to do this. I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime(DF,'%Y-%m-%d %H:%M:%S') but when I try to then apply datetime.datetime.year(DF) it doesn't work. I will also need to parse the timestamps to months and combinations of years-months and so on... Help please. Thanks.", "q_apis": "time year year timestamp all first date to_datetime apply year parse", "io": "0 2005-08-31 16:39:40 1 2005-12-28 16:00:34 2 2005-10-21 17:52:10 3 2014-01-28 12:23:15 4 2014-01-28 12:23:15 5 2011-02-04 18:32:34 6 2011-02-04 18:32:34 7 2011-02-04 18:32:34 <s> 0 2005-08-31 16:39:40 2005 1 2005-12-28 16:00:34 2005 2 2005-10-21 17:52:10 2005 3 2014-01-28 12:23:15 2014 4 2014-01-28 12:23:15 2014 5 2011-02-04 18:32:34 2011 6 2011-02-04 18:32:34 2011 7 2011-02-04 18:32:34 2011 ", "apis": "year timestamp apply year", "code": ["df1['year'] = df1['timestamp'].apply(lambda x: x.year)\n"], "link": "https://stackoverflow.com/questions/28990256/python-pandas-time-series-year-extraction"}
{"id": 220, "q": "Python: Selecting rows matching Feb till current month in pandas dataframe", "d": "I have below dataframe: Considering the current month is June, the expected output is as follows: Here i have done filtering of rows by months from Feb:Current Month (in this case June) and then group by to find all the names once per mode. (Example: F will be only once for actual and once for plan) I previously tried taking a transpose of columns and then using the below to summarize the data till current month: where : But this is getting very complicated since the actual data has lots and lots of modes and also many years of data. Is there any easier solution for the same where this complication can be avoided and the similar solution can be achieved? In the end i am expecting to have the below dataframe: I guess i can have this format using pivot_table in pandas:", "q_apis": "month month all names mode transpose columns month where any where pivot_table", "io": "Name1 Name2 Mode Value1 Value2 C N Actual 4 4 D N Plan 2 2 E N Actual 10 10 F N Actual 7 7 F N Plan 3 3 <s> Name1 Name2 Actual_Value1 Actual_Value2 Plan_Value1 Plan_Value1 C N 4 4 D N 2 2 E N 10 10 F N 7 7 3 3 ", "apis": "today month isin groupby agg sum sum pivot_table index columns values sum reset_index rename_axis columns replace", "code": ["lstAllMonths=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\ncurMth = datetime.today().month  # 7=July\ncurMth = 6  # Jun for testing\nlstMth = lstAllMonths[1:curMth]\n\ndf = df[df['Month'].isin(lstMth)][['Name1','Name2','Mode','Value1','Value2']]\ngb = df.groupby(['Name1','Name2','Mode'])\ndfagg = gb.agg({'Value1':sum, 'Value2':sum})\n\ndfpvt = pd.pivot_table(dfagg,index=['Name1', 'Name2'], \n                      columns=['Mode'],\n                      values=['Value1', 'Value2'], \n                      aggfunc=np.sum, fill_value=0).reset_index().rename_axis(1)\n                      \ndfpvt.columns=['Name1','Name2','Actual_Value1','Plan_Value1','Actual_Value2','Plan_Value2']\ndfpvt.replace(0,'', inplace=True)\ndfpvt = dfpvt[['Name1','Name2','Actual_Value1','Actual_Value2','Plan_Value1','Plan_Value2']]    \nprint(dfpvt)\n"], "link": "https://stackoverflow.com/questions/63000962/python-selecting-rows-matching-feb-till-current-month-in-pandas-dataframe"}
{"id": 125, "q": "How to spread a key-value pair across multiple columns and flatten the matrix based on another column?", "d": "Using Pandas 1.2.0, I want to transform this dataframe where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively: into: My attempt: What should I do next to flatten the diagonals of these sub-matrices and group by 'a'?", "q_apis": "value columns transform where contains groups value sub", "io": " a b c 0 x_1 1 6.00 1 x_1 2 3.00 2 x_1 3 0.00 3 x_1 4 1.00 4 x_1 5 3.40 5 j_2 1 4.50 6 j_2 2 0.10 7 j_2 3 0.20 8 j_2 5 0.88 <s> a 1 2 3 4 5 0 x_1 6.0 3.0 0.0 1.0 3.40 1 j_2 4.5 0.1 0.2 NaN 0.88 ", "apis": "pivot index columns values pivot", "code": ["df = df.pivot(index='a', columns='b', values='c')", "df = df.pivot('a', 'b', 'c')"], "link": "https://stackoverflow.com/questions/65692969/how-to-spread-a-key-value-pair-across-multiple-columns-and-flatten-the-matrix-ba"}
{"id": 10, "q": "Concatenate values and column names in a data frame to create a new data frame", "d": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used", "q_apis": "values names values names value name sample", "io": " Value col1 col2 col3 0 a aa ab ac 1 b ba bb bc 2 c ca cb cc 3 d da db dc 4 e ea eb ec <s> Value Col 1 0 a_Col 1 aa 1 a_Col 2 ab 2 a_Col 3 ac 3 b_Col 1 ba 4 b_Col 2 bb 5 b_Col 3 bc 6 c_Col 1 ca 7 c_Col 2 cb 8 c_Col 3 cc 9 d_Col 1 da 10 d_Col 2 db 11 d_Col 3 dc 12 e_Col 1 ea 13 e_Col 2 eb 14 e_Col 3 ec ", "apis": "melt drop columns", "code": ["x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n"], "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame"}
{"id": 658, "q": "Pass Different Columns in Pandas DataFrame in a Custom Function in df.apply()", "d": "Say I have a dataframe : I wanna have two new columns that are x * y and x * z: So I define a function (just for example) that takes either the string or the string as an argument to indicate which column I want to multiply with the column x: And apply the function to the dataframe : Apparently it is wrong here because I didn't specify the , or . Question is, just takes function name, how do I tell it to take the two arguments?", "q_apis": "DataFrame apply columns apply name take", "io": " x y z 0 1 2 3 1 4 5 6 2 7 8 9 <s> x y z xy xz 0 1 2 3 2 3 1 4 5 6 20 24 2 7 8 9 56 63 ", "apis": "apply apply", "code": ["def func(row, colName):\n    return row * colName\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x['x'], x[c]), axis=1)\n", "def func(row, colName):\n    return row['x'] * row[colName]\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x, c), axis=1)\n"], "link": "https://stackoverflow.com/questions/49301344/pass-different-columns-in-pandas-dataframe-in-a-custom-function-in-df-apply"}
{"id": 232, "q": "Python Dataframe rowwise min and max with NaN values", "d": "My dataframe has nans. But I am trying to find row wise min and max. How do I find it. I am tring to find min and max in each row and difference between them in column and . My code: Present output: Expected output:", "q_apis": "min max values min max min max difference between", "io": "df['dif'] = 0 66 1 66 2 66 <s> df['dif'] = 0 51 1 66 2 52 ", "apis": "array", "code": ["np.nanmax(df[poacols],1)-np.nanmin(df[poacols],1)\nOut[81]: array([51., 66., 52.])\n"], "link": "https://stackoverflow.com/questions/63387459/python-dataframe-rowwise-min-and-max-with-nan-values"}
{"id": 89, "q": "Convert a Pandas DataFrame to a dictionary", "d": "I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be and the elements of other columns in same row be . DataFrame: Output should be like this: Dictionary:", "q_apis": "DataFrame DataFrame columns DataFrame first columns DataFrame", "io": " ID A B C 0 p 1 3 2 1 q 4 3 2 2 r 4 0 9 <s> {'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]} ", "apis": "to_dict columns index to_dict to_dict index", "code": [">>> df.to_dict('split')\n{'columns': ['a', 'b'],\n 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],\n 'index': [0, 1, 2]}\n", ">>> df.to_dict('records')\n[{'a': 'red', 'b': 0.5}, \n {'a': 'yellow', 'b': 0.25}, \n {'a': 'blue', 'b': 0.125}]\n", ">>> df.to_dict('index')\n{0: {'a': 'red', 'b': 0.5},\n 1: {'a': 'yellow', 'b': 0.25},\n 2: {'a': 'blue', 'b': 0.125}}\n"], "link": "https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary"}
{"id": 297, "q": "Filter Series/DataFrame by another DataFrame", "d": "Let's suppose I have a Series (or DataFrame) , for example list of all Universities and Colleges in the USA: And another Series (od DataFrame) , for example list of all cities in the USA: And my desired output (bascially an intersection of and ): The thing is: I'd like to create a Series that consists of cities but only these, that have a university/college. My very first thought was to remove \"University\" or \"College\" parts from the , but it turns out that it is not enough, as in case of . Then I thought of leaving only the first word, but that excludes . Finally, I got a series of all the cities and now I'm trying to use it as a filter (something similiar to or ), so if a string (Uni name) contains any of the elements of (city name), then return only the city name. My question is: how to do it in a neat way?", "q_apis": "Series DataFrame DataFrame Series DataFrame all Series DataFrame all intersection Series first first all now filter name contains any name name", "io": " City 0 Searcy 1 Angwin 2 New York 3 Ann Arbor <s> Uni City 0 Searcy 1 Angwin 2 Fairbanks 3 Ann Arbor ", "apis": "Series contains any name", "code": ["pd.Series([i for i in s2 if s1.str.contains(i).any()], name='Uni City')\n"], "link": "https://stackoverflow.com/questions/61678886/filter-series-dataframe-by-another-dataframe"}
{"id": 620, "q": "Fill all values in a group with the first non-null value in that group", "d": "The following is the pandas dataframe I have: If we look into the data, cluster 1 has Value 'A' for one row and remain all are NA values. I want to fill 'A' value for all the rows of cluster 1. Similarly for all the clusters. Based on one of the values of the cluster, I want to fill the remaining rows of the cluster. The output should be like, I am new to python and not sure how to proceed with this. Can anybody help with this ?", "q_apis": "all values first value all values value all all values", "io": "cluster Value 1 A 1 NaN 1 NaN 1 NaN 1 NaN 2 NaN 2 NaN 2 B 2 NaN 3 NaN 3 NaN 3 C 3 NaN 4 NaN 4 S 4 NaN 5 NaN 5 A 5 NaN 5 NaN <s> cluster Value 1 A 1 A 1 A 1 A 1 A 2 B 2 B 2 B 2 B 3 C 3 C 3 C 3 C 4 S 4 S 4 S 5 A 5 A 5 A 5 A ", "apis": "dropna set_index to_dict map map unique values dropna set_index to_dict iterrows at iterrows at at at", "code": ["nan_map = df.dropna().set_index('cluster').to_dict()['Value']\ndf['Value'] = df['cluster'].map(nan_map)\n\nprint(df)\n", "# Create a dict to map clusters to unique values\nnan_map = df.dropna().set_index('cluster').to_dict()['Value']\n# nan_map: {1: 'A', 2: 'B', 3: 'C', 4: 'S', 5: 'A'}\n\n# Apply\nfor i, row in df.iterrows():\n    df.at[i,'Value'] = nan_map[row['cluster']]\n\nprint(df)\n", "# Apply\nfor i, row in df.iterrows():\n    if isinstance(df.at[i,'Value'], float) and math.isnan(df.at[i,'Value']):\n        df.at[i,'Value'] = nan_map[row['cluster']]\n"], "link": "https://stackoverflow.com/questions/50957993/fill-all-values-in-a-group-with-the-first-non-null-value-in-that-group"}
{"id": 282, "q": "Python Pandas Iterate over columns and also update columns based on apply conditions", "d": "I am trying to update dataframe columns based on consecutive columns values. If columns say col1 and col2 has >0 and <0 values, then same columns should get updated as col2=col1 + col2 and col1=0 and also counter +1 (gives how many fixes has been done throughout the column). Dataframe look like: Required Dataframe after applying logic: I tried: It failed I also looked into but I couldn't figure out the way. DDL to generate DataFrame: Thanks!", "q_apis": "columns update columns apply update columns columns values columns values columns get DataFrame", "io": "id col0 col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 1 0 5 -5 5 -5 0 0 1 4 3 -3 2 0 0 0 0 0 0 0 4 -4 0 0 3 0 0 1 2 3 0 0 0 5 6 0 <s> id col0 col1 col2 col3 col4 col6 col7 col8 col9 col10 fix 1 0 0 0 0 0 0 0 1 4 0 0 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0 0 1 2 3 0 0 0 5 6 0 9 0 ", "apis": "gt shift lt mask shift fillna shift mask assign sum", "code": ["c = df.gt(0) & df.shift(-1,axis=1).lt(0)\n\nout = (df.mask(c.shift(axis=1).fillna(False),df+df.shift(axis=1))\n      .mask(c,0).assign(Fix=c.sum(1)))\n\nprint(out)\n"], "link": "https://stackoverflow.com/questions/62215453/python-pandas-iterate-over-columns-and-also-update-columns-based-on-apply-condit"}
{"id": 545, "q": "Python Setting Values Without Loop", "d": "I have a time series dataframe where there is 1 or 0 in it (true/false). I wrote a function that loops through all rows with values 1 in them. Given user defined integer parameter called , I will set values 1 to n rows forward from the initial row. For example, in the dataframe below I will be loop to row . If , then I will set both and to 1 too.: The resulting will then is The problem I have is this is being run 10s of thousands of times and my current solution where I am looping over rows where there are ones and subsetting is way too slow. I was wondering if there are any solutions to the above problem that is really fast. Here is my (slow) solution, is the initial signal dataframe:", "q_apis": "time where all values values where where any", "io": "2016-08-03 0 2016-08-04 0 2016-08-05 1 2016-08-08 0 2016-08-09 0 2016-08-10 0 <s> 2016-08-03 0 2016-08-04 0 2016-08-05 1 2016-08-08 1 2016-08-09 1 2016-08-10 0 ", "apis": "where ne shift ffill fillna Series size copy diff index index get_loc mean std where ne shift ffill fillna mean std", "code": ["n_hold = 2\ns = x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n", "np.random.seed(123)\nx = pd.Series(np.random.choice([0,1], p=(.8,.2), size=1000))\nx1 = x.copy()\n#print (x)\n\n\ndef orig(x):\n    n_hold = 2\n    entry_sig_diff = x.diff()\n    entry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index\n    final_signal = x * 0\n    for i in range(0, len(entry_sig_dt)):\n        row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])\n\n        if (row_idx + n_hold) >= len(x):\n            break\n\n        final_signal[row_idx:(row_idx + n_hold + 1)] = 1\n    return final_signal\n\n#print (orig(x))\n", "%timeit (orig(x))\n24.8 ms \u00b1 653 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n1.36 ms \u00b1 12.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/53885404/python-setting-values-without-loop"}
{"id": 219, "q": "How to re order this DataFrame in Python wthout hard coding column values?", "d": "I have a df ('COL3 SUM' is the full name with a space): How can I re order this df so that 'COL3 SUM' always comes at the end of the dataframe like so without re ordering any of the rest of the df?", "q_apis": "DataFrame values name at any", "io": "COL1 COL2 COL3 SUM COL4 COL5 1 2 3 4 5 <s> COL1 COL2 COL4 COL5 COL3 SUM 1 2 4 5 3 ", "apis": "columns columns", "code": ["new_cols = [col for col in df.columns if \"SUM\" not in col]\nmoved_cols = list(set(df.columns) - set(new_cols)) \nnew_cols.extend(moved_cols)\ndf = df[new_cols]\n"], "link": "https://stackoverflow.com/questions/63712894/how-to-re-order-this-dataframe-in-python-wthout-hard-coding-column-values"}
{"id": 374, "q": "pandas group by the column values with all values less than certain numbers and assign the group numbers as new columns", "d": "I have a data frame like this, Now I want to create another column col3 with grouping all the col2 values which are under below 5 and keep col3 values as 1 to number of groups, so the final data frame would look like, I could do this comparing the the prev values with the current values and store into a list and make it the col3. But the execution time will be huge in this case, so looking for some shortcuts/pythonic way to do it most efficiently.", "q_apis": "values all values assign columns all values values groups values values time", "io": "df col1 col2 A 2 B 3 C 1 D 4 E 6 F 1 G 2 H 8 I 1 J 10 <s> col1 col2 col3 A 2 1 B 3 1 C 1 1 D 4 1 E 6 2 F 1 2 G 2 2 H 8 3 I 1 3 J 10 4 ", "apis": "gt cumsum loc", "code": ["N = 5\ndf['col3'] = df['col2'].gt(N).cumsum() + int(df.loc[0, 'col2'] < 5)\n"], "link": "https://stackoverflow.com/questions/59589861/pandas-group-by-the-column-values-with-all-values-less-than-certain-numbers-and"}
{"id": 253, "q": "Trying to append the sum of an existing dataframe into a new excel sheet", "d": "So I have been trying to append the of an existing dataframe into a new dataframe for a new the below example: I want this to be appended to a new excel as: Following is the code where I am merging the files in a particular location I need help with the summing part.", "q_apis": "append sum append where", "io": "A B C 10 10 10 10 10 10 10 10 10 <s> A B C 30 30 30 ", "apis": "sum dtype sum to_frame", "code": ["df.sum()\nA    30\nB    30\nC    30\ndtype: int64\n", "df.sum().to_frame()\n\n0 \nA 30\nB 30\nC 30\n \n"], "link": "https://stackoverflow.com/questions/62935487/trying-to-append-the-sum-of-an-existing-dataframe-into-a-new-excel-sheet"}
{"id": 145, "q": "Python: How do I merge rows that shares the same name of &quot;Ocode&quot; or &quot;Ccode in dataframe", "d": "I'm thinking of using pandas to merge several repetitive rows of \"Ocode\" and Ccode\". Ideally I want only one \"Ocode\" or \"Ccode\" per row. When there are repetitive dates under c## (I.E. c21), only the latest date is kept. Separate dates under different column with the same \"Ocode\"/\"Ccode\" should also be merged. (For reference purpose: O and C code correspondingly represents Organization Code and Corporation code.) This is the heading of the dataframe. Which should become ----> Attempt: and perform the merge one by one. However, this method tends to delete information under other column when dealing with one of the c##(I.E. c21) column df.to_excel(ic + '.xlsx', index=False)", "q_apis": "merge name merge date merge delete to_excel index", "io": "Num Ocode Ccode c21 c57 c58 c59 c70 c71 c74 c75 0 BK0001000 NaN NaN NaN NaN NaN NaN NaN NaN 2021-02-09 1 CU0030000 NaN NaN NaN NaN NaN NaN 2021-12-31 NaN NaN 2 CU0030000 NaN NaN NaN NaN NaN NaN 2021-12-31 NaN NaN 3 CU0048000 NaN NaN NaN 2018-06-19 NaN NaN NaN NaN NaN 4 CU0056000 NaN NaN NaN NaN NaN NaN NaN 2020-06-04 NaN ... ... ... ... ... ... ... ... ... ... ... 2384 NaN CU0280002 2017-12-31 NaN NaN NaN NaN NaN NaN NaN 2385 NaN CU0280002 2016-12-31 NaN NaN NaN NaN NaN NaN NaN 2386 NaN CU0280002 NaN NaN NaN NaN NaN NaN 2017-12-31 NaN 2387 NaN CU0659001 NaN NaN NaN NaN NaN NaN 2022-05-31 NaN <s> Num Ocode Ccode c21 c57 c58 c59 c70 c71 c74 c75 0 BK0001000 NaN NaN NaN NaN NaN NaN NaN NaN 2021-02-09 1 CU0030000 NaN NaN NaN NaN NaN NaN 2021-12-31 NaN NaN 3 CU0048000 NaN NaN NaN 2018-06-19 NaN NaN NaN NaN NaN 4 CU0056000 NaN NaN NaN NaN NaN NaN NaN 2020-06-04 NaN ... ... ... ... ... ... ... ... ... ... ... 2384 NaN CU0280002 2017-12-31 NaN NaN NaN NaN NaN 2017-12-31 NaN 2387 NaN CU0659001 NaN NaN NaN NaN NaN NaN 2022-05-31 NaN ", "apis": "assign fillna fillna groupby last reset_index replace groupby last reset_index loc notna notna assign fillna drop groupby last reset_index assign fillna drop groupby last reset_index concat replace", "code": ["df = (df.assign(Ocode = df['Ocode'].fillna('nan'),Ccode = df['Ccode'].fillna('nan'))\n        .groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index()\n        .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n", "df = (df.groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index())\n", "df.loc[df['Ocode'].notna() & df['Ocode'].notna(), 'Ccode'] = np.nan\n", "df1 = (df.assign(Ocode = df['Ocode'].fillna('nan'))\n         .drop('Ccode', axis=1)\n         .groupby('Ocode')\n         .last()\n         .reset_index())\n\ndf2 = (df.assign(Ccode = df['Ccode'].fillna('nan'))\n         .drop('Ocode', axis=1)\n         .groupby('Ccode')\n         .last()\n         .reset_index())\n\nboth = (pd.concat([df1, df2], sort=False)\n          .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n"], "link": "https://stackoverflow.com/questions/65194434/python-how-do-i-merge-rows-that-shares-the-same-name-of-ocode-or-ccode-in-da"}
{"id": 27, "q": "Dictionary making for a transportation model from a Dataframe", "d": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.", "q_apis": "name any value", "io": " d = {c1:80, c2:270, c3:250, c4:160, c5:180} # customer demand M = {p1:500, p2:500, p3:500} # factory capacity I = [c1,c2,c3,c4,c5] # Customers J = [p1,p2,p3] # Factories cost = {(p1,c1):4, (p1,c2):5, (p1,c3):6, (p1,c4):8, (p1,c5):10, ...... } <s> {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan} ", "apis": "set_index loc loc T squeeze dropna iteritems iteritems drop columns columns index drop columns stack iteritems", "code": ["df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"], "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe"}
{"id": 96, "q": "Retrieve rows with highest value with condition", "d": "I have a dataframe that looks like this: I want to write a function that takes the rows with same id and label A and filter it based on the highest width so the after applying the function the dataframe would be:", "q_apis": "value filter", "io": "| Id | Label | Width | |----|-------| ------| | 0 | A | 5 | | 0 | A | 3 | | 0 | B | 4 | | 1 | A | 7 | | 1 | A | 9 | <s> | Id | Label | Width | |----|-------| ------| | 0 | A | 5 | | 0 | B | 4 | | 1 | A | 9 | ", "apis": "eq loc groupby idxmax concat sort_index dtype bool", "code": ["m = df['Label'].eq('A')\ndf_a = df.loc[df[m].groupby(['Id', 'Label'])['Width'].idxmax()]\n\ndf_out = pd.concat([df[~m], df_a]).sort_index()\n", ">>> m\n\n0     True\n1     True\n2    False\n3     True\n4     True\nName: Label, dtype: bool\n"], "link": "https://stackoverflow.com/questions/66194784/retrieve-rows-with-highest-value-with-condition"}
{"id": 492, "q": "Add values to existing rows -DataFrame", "d": "I'm appending some weather data (from json- dict) - in Japanese to DataFrame. I would like to have something like this But I have this How Could I change the Codes to make it like that? Here is the code", "q_apis": "values DataFrame DataFrame", "io": " \u5929\u6c17 \u98a8 0 \u72b6\u614b: Clouds \u98a8\u901f: 2.1m 1 NaN \u5411\u304d: 230 <s> \u5929\u6c17 \u98a8 0 \u72b6\u614b: Clouds NaN 1 NaN \u98a8\u901f: 2.1m 2 NaN \u5411\u304d: 230 ", "apis": "DataFrame columns append append", "code": ["df = pd.DataFrame(columns=['\u5929\u6c17','\u98a8'])\ndf = df.append({'\u5929\u6c17': weather_status, '\u98a8': wind_speed}, ignore_index=True)\ndf = df.append({'\u98a8': wind_deg}, ignore_index=True)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/55803354/add-values-to-existing-rows-dataframe"}
{"id": 310, "q": "Error: None of [Index([&#39;...&#39;], dtype=&#39;object&#39;)] are in the [index]", "d": "I am trying to delete a grouped set of rows in pandas according to the following condition: If a group (grouped by col1) has more than 2 values 'c' in col2, then remove the whole group. What I have looks like this And I am trying to get here: Typically I do this for other very similar dataframes (and it works): But for this one is not working and I receive this error: Does someone have an idea on how I could do this? Thanks!", "q_apis": "Index dtype index delete values get", "io": " col1 col2 0 A 10:10 1 A 20:05 2 A c 3 A 00:10 4 B 04:15 2 B c 3 B c 4 B 13:40 <s> col1 col2 0 A 10:10 1 A 20:05 2 A c 3 A 00:10 ", "apis": "eq dtype bool eq groupby transform sum lt dtype bool", "code": ["print (df['col2'].eq('c'))\n0    False\n1    False\n2     True\n3    False\n4    False\n2     True\n3     True\n4    False\nName: col2, dtype: bool\n", "print (df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2))\n0     True\n1     True\n2     True\n3     True\n4    False\n2    False\n3    False\n4    False\nName: col2, dtype: bool\n"], "link": "https://stackoverflow.com/questions/61170036/error-none-of-index-dtype-object-are-in-the-index"}
{"id": 384, "q": "How to Concat 2 columns in single column with column value check", "d": "I want to concat two column from data frame where Column1 not equals to ANY: DataFrame : as a result I want dataframe as follows ANY is variable, could represent Null, EmptyString, String, Number. Thanks.", "q_apis": "columns value concat where equals DataFrame", "io": " COLUMN1 | COLUMN2 0 A | FOO 1 B | BAR 2 ANY | FOO 3 ANY | BAR 4 C | FOO <s> COLUMN1 | COLUMN2 0 A | FOO_A 1 B | BAR_B 2 ANY | FOO 3 ANY | BAR 4 C | FOO_C ", "apis": "apply", "code": ["df['COLUMN2']=df.apply(lambda row:row['COLUMN2']+'_'+row['COLUMN1'] if row['COLUMN1']!='ANY' else row['COLUMN2'],axis=1)\n"], "link": "https://stackoverflow.com/questions/59284049/how-to-concat-2-columns-in-single-column-with-column-value-check"}
{"id": 505, "q": "Top 3 Values Per Row in Pandas", "d": "I have a large Pandas dataframe that is in the vein of: and I would like to generate output that looks like this, taking the column names of the 3 highest values for each row: I have tried using df.idmax(axis=1) which returns the 1st maximum column name but am unsure how to compute the other two? Any help on this would be truly appreciated, thanks!", "q_apis": "names values name", "io": "| ID | Var1 | Var2 | Var3 | Var4 | Var5 | |----|------|------|------|------|------| | 1 | 1 | 2 | 3 | 4 | 5 | | 2 | 10 | 9 | 8 | 7 | 6 | | 3 | 25 | 37 | 41 | 24 | 21 | | 4 | 102 | 11 | 72 | 56 | 151 | ... <s> | ID | 1st Max | 2nd Max | 3rd Max | |----|---------|---------|---------| | 1 | Var5 | Var4 | Var3 | | 2 | Var1 | Var2 | Var3 | | 3 | Var3 | Var2 | Var1 | | 4 | Var5 | Var1 | Var3 | ... ", "apis": "set_index apply Series nlargest index index reset_index", "code": ["c = ['1st Max','2nd Max','3rd Max']\ndf = (df.set_index('ID')\n        .apply(lambda x: pd.Series(x.nlargest(3).index, index=c), axis=1)\n        .reset_index())\n"], "link": "https://stackoverflow.com/questions/54923349/top-3-values-per-row-in-pandas"}
{"id": 260, "q": "Find out intersection of 2 pandas DataFrame according to 2 columns", "d": "I would to find out intersection of 2 pandas DataFrame according to 2 columns 'x' and 'y' and combine them into 1 DataFrame. The data are: The expected output is something like (can ignore index): Thank you very much!", "q_apis": "intersection DataFrame columns intersection DataFrame columns combine DataFrame index", "io": "df[1]: x y id fa 0 4 5 9283222 3.1 1 4 5 9283222 3.1 2 10 12 9224221 3.2 3 4 5 9284332 1.2 4 6 1 51249 11.2 df[2]: x y id fa 0 4 5 19283222 1.1 1 9 3 39224221 5.2 2 10 12 29284332 6.2 3 6 1 51242 5.2 4 6 2 51241 9.2 5 1 1 51241 9.2 <s> x y id fa 0 4 5 9283222 3.1 1 4 5 9283222 3.1 2 10 12 9224221 3.2 3 4 5 9284332 1.2 4 6 1 51249 11.2 0 4 5 19283222 1.1 2 10 12 29284332 6.2 3 6 1 51242 5.2 ", "apis": "intersection merge drop_duplicates concat merge intersection merge intersection", "code": ["intersection = df1[['x', 'y']].merge(df2[['x', 'y']]).drop_duplicates()\npd.concat([df1.merge(intersection), df2.merge(intersection)])\n"], "link": "https://stackoverflow.com/questions/41529340/find-out-intersection-of-2-pandas-dataframe-according-to-2-columns"}
{"id": 580, "q": "How to select, and replace specific values with NaN in pandas dataframe. How to remove a column from each level 1 multiindex", "d": "I have a csv file, which I read into a pandas frame: This is the view of the csv file (print(csv_file)): The resulting dataframe is MultiIndexed with two levels: print(df.column()): If a coordinate has a lower likelihood, I wan't the coordinates to be replaced by NaN. The new dataframe shan't have a likelihood column(s). An example with the first row from 'nose': After function should be like this: Note that outstanding values remain unchanged during this process!", "q_apis": "select replace values view levels first values", "io": "coords x y likelihood 0 197.486369 4.545954 3.890000e-07 <s> coords x y 0 NaN NaN ", "apis": "columns levels loc", "code": ["for col in df.columns.levels[0]:\n    df.loc[df[(col, 'likelihood')] < threshold, [(col, 'x'), (col, 'y')]] = np.nan\n"], "link": "https://stackoverflow.com/questions/52506702/how-to-select-and-replace-specific-values-with-nan-in-pandas-dataframe-how-to"}
{"id": 389, "q": "Python: Sort subsection of columns", "d": "Suppose we have the following dataframe: Which can be computed as follows I was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. I would want the end result to look as", "q_apis": "columns last columns", "io": "Label1 2016-03-31 2016-05-31 2016-04-30 0 A A1 1 6 1 B B1 3 4 2 C C2 5 7 3 D D1 7 2 4 E E4 9 4 5 F F1 11 6 <s> Label1 2016-03-31 2016-04-30 2016-05-31 0 A A1 6 1 1 B B1 4 3 2 C C2 7 5 3 D D1 2 7 4 E E4 4 9 5 F F1 6 11 ", "apis": "columns columns sort_values", "code": ["new_cols = list(df.columns[:-3]) + list(df.columns[-3:].sort_values())\n\ndf[new_cols]\n"], "link": "https://stackoverflow.com/questions/59250863/python-sort-subsection-of-columns"}
{"id": 205, "q": "Add character to column based on ascending order of another column if condition met pandas", "d": "Stuck on a data problem in pandas. See data below: The rules are: Only one Cost for each unique (Product, Level) combination. If multiple Cost for each unique (Product, Level) combination, add a letter to the Level value (L1 A, L1 B, etc) based on the Cost value (L1 A being the smallest Cost). If (Product, Level) combination has a unique Cost then do nothing. Desired output:", "q_apis": "unique unique add value value unique", "io": "| Product | Level | Cost | --------- ------- ------ | Prod_A | L1 | 100 | | Prod_A | L1 | 100 | | Prod_A | L1 | 200 | | Prod_A | L2 | 100 | | Prod_A | L3 | 100 | | Prod_B | L1 | 150 | | Prod_B | L1 | 150 | | Prod_B | L2 | 200 | | Prod_B | L2 | 300 | | Prod_C | L3 | 100 | <s> | Product | Level | Cost | --------- ------- ------ | Prod_A | L1 A | 100 | | Prod_A | L1 A | 100 | | Prod_A | L1 B | 200 | | Prod_A | L2 | 100 | | Prod_A | L3 | 100 | | Prod_B | L1 | 150 | | Prod_B | L1 | 150 | | Prod_B | L2 A | 200 | | Prod_B | L2 B | 300 | | Prod_C | L3 | 100 | ", "apis": "groupby transform factorize nunique map fillna", "code": ["charlist='ABCDEFG'\ndd = {k:' '+v for k, v in enumerate(charlist)}\ndf['Level'] += df.groupby(['Product', 'Level'])['Cost']\\\n                 .transform(lambda x: x.factorize()[0] if x.nunique()>1 else -1)\\\n                 .map(dd).fillna('')\n"], "link": "https://stackoverflow.com/questions/64165393/add-character-to-column-based-on-ascending-order-of-another-column-if-condition"}
{"id": 532, "q": "Dropping duplicate rows but keeping certain values Pandas", "d": "I have 2 similar dataframes that I concatenated that have a lot of repeated values because they are basically the same data set but for different years. The problem is that one of the sets has some values missing whereas the other sometimes has these values. For example: I want to drop duplicates on the since some repetitions don't have years. However, I'm left with the data that has no and I'd like to keep the data with these values: How do I keep these values rather than the blanks?", "q_apis": "values values values values drop left values values", "io": "Name Unit Year Level Nik 1 2000 12 Nik 1 12 John 2 2001 11 John 2 2001 11 Stacy 1 8 Stacy 1 1999 8 . . <s> Name Unit Year Level Nik 1 2000 12 John 2 2001 11 Stacy 1 1999 8 . . ", "apis": "sort_values drop_duplicates", "code": ["df = df.sort_values(subset + ['Year']).drop_duplicates(subset)\n"], "link": "https://stackoverflow.com/questions/54219106/dropping-duplicate-rows-but-keeping-certain-values-pandas"}
{"id": 82, "q": "specify number of spaces between pandas DataFrame columns when printing", "d": "When you print a pandas DataFrame, which calls DataFrame.to_string, it normally inserts a minimum of 2 spaces between the columns. For example, this code outputs which has a minimum of 2 spaces between each column. I am copying DataFarames printed on the console and pasting it into documents, and I have received feedback that it is hard to read: people would like more spaces between the columns. Is there a standard way to do that? I see no option in either DataFrame.to_string or pandas.set_option. I have done a web search, and not found an answer. This question asks how to remove those 2 spaces, while this question asks why sometimes only 1 space is between columns instead of 2 (I also have seen this bug, hope someone answers that question). My hack solution is to define a function that converts a DataFrame's columns to type str, and then prepends each element with a string of the specified number of spaces. This code (added to the code above) outputs which is the desired effect. But I think that pandas surely must have some builtin simple standard way to do this. Did I miss how? Also, the solution needs to handle a DataFrame whose columns are a MultiIndex. To continue the code example, consider this modification:", "q_apis": "between DataFrame columns DataFrame DataFrame to_string between columns between between columns DataFrame to_string between columns DataFrame columns DataFrame columns MultiIndex", "io": " c1 c2 a3235235235 0 a 11 1 1 bb 22 2 2 ccc 33 3 3 dddd 44 4 4 eeeeee 55 5 <s> c1 c2 a3235235235 0 a 11 1 1 bb 22 2 2 ccc 33 3 3 dddd 44 4 4 eeeeee 55 5 ", "apis": "length astype agg max pad between iteritems max max pad", "code": ["from functools import partial\n\n# Formatting string \ndef get_fmt_str(x, fill):\n    return '{message: >{fill}}'.format(message=x, fill=fill)\n\n# Max character length per column\ns = df.astype(str).agg(lambda x: x.str.len()).max() \n\npad = 6  # How many spaces between \nfmts = {}\nfor idx, c_len in s.iteritems():\n    # Deal with MultIndex tuples or simple string labels. \n    if isinstance(idx, tuple):\n        lab_len = max([len(str(x)) for x in idx])\n    else:\n        lab_len = len(str(idx))\n\n    fill = max(lab_len, c_len) + pad - 1\n    fmts[idx] = partial(get_fmt_str, fill=fill)\n"], "link": "https://stackoverflow.com/questions/66375262/specify-number-of-spaces-between-pandas-dataframe-columns-when-printing"}
{"id": 338, "q": "Pandas Drop Columns with Only One Unique Value for a Group", "d": "I have a df that consists of duplicate : I want to remove columns where all values for an are the same. This could mean that all values in the column are the same () or all the values are the same for each (). Desired result: I used this to identify the counts of unique values in each column: If I drop all columns where this count is equal to 1, this would take care of the scenario. However, how should I take care of the scenario? The df has already been grouped by id to find duplicate, but do I need to use again? As a \"bonus\", I wouldn't mind knowing how to identify where even one has text that is all the same (i.e. ). I'm essentially trying to find which columns cause there to be duplicates. Thank you for any and all insight you all might have!", "q_apis": "columns where all values mean all values all values unique values drop all columns where count take take where all columns any all all", "io": "id text text2 text3 1 hello hello hello 1 hello hello hello 2 hello hello goodbye 2 goodbye hello goodbye 2 hello hello goodbye <s> id text 1 hello 1 hello 2 hello 2 goodbye 2 hello ", "apis": "nunique groupby agg nunique all", "code": ["In [1227]: u = df.nunique()\n", "In [1228]: gu = gu = (df.groupby('id').agg('nunique') == 1).all()\n"], "link": "https://stackoverflow.com/questions/45336415/pandas-drop-columns-with-only-one-unique-value-for-a-group"}
{"id": 547, "q": "How to create a dataframe from numpy arrays?", "d": "I am trying to create a matrix / DataFrame with the numbers stored in 2 variables and I would like them to look like this: I would like it to be in a so I can work with it with the library. Thanks in advance", "q_apis": "DataFrame", "io": "x = np.linspace(0,50) y = np.exp(x) <s> x | y ___________________ 0 | 1.0... 1 | 2.77... 2 | 7.6... ... | ... 50 | 5.18e+21... ", "apis": "xs xs xs xs", "code": [">>> xs = np.arange(51)                                                                                                 \n>>> ys = np.exp(xs) \n", ">>> xs = np.arange(51)                                                                                                 \n>>> ys = np.exp(xs)\n"], "link": "https://stackoverflow.com/questions/53820131/how-to-create-a-dataframe-from-numpy-arrays"}
{"id": 63, "q": "Python: How to pass Dataframe Columns as parameters to a function?", "d": "I have a dataframe with 2 columns of text embeddings namely and . I want to create a third column in named which should contain the cosine_similarity between every row of and . But when I try to implement this using the following code I get a . How to fix it? Dataframe Code to Calculate Cosine Similarity Error Required Dataframe", "q_apis": "columns between get", "io": " embedding_1 | embedding_2 [[-0.28876397, -0.6367827, ...]] | [[-0.49163356, -0.4877703,...]] [[-0.28876397, -0.6367827, ...]] | [[-0.06686627, -0.75147504...]] [[-0.28876397, -0.6367827, ...]] | [[-0.42776933, -0.88310856,...]] [[-0.28876397, -0.6367827, ...]] | [[-0.6520882, -1.049325,...]] [[-0.28876397, -0.6367827, ...]] | [[-1.4216679, -0.8930428,...]] <s> embedding_1 | embedding_2 | distances [[-0.28876397, -0.6367827, ...]] | [[-0.49163356, -0.4877703,...]] | 0.427 [[-0.28876397, -0.6367827, ...]] | [[-0.06686627, -0.75147504...]] | 0.673 [[-0.28876397, -0.6367827, ...]] | [[-0.42776933, -0.88310856,...]] | 0.882 [[-0.28876397, -0.6367827, ...]] | [[-0.6520882, -1.049325,...]] | 0.665 [[-0.28876397, -0.6367827, ...]] | [[-1.4216679, -0.8930428,...]] | 0.312 ", "apis": "apply apply", "code": ["def cal_cosine_similarity(row):\n    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])\n\ndf['distances'] = df.apply(cal_cosine_similarity, axis=1)\n", "df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)\n"], "link": "https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function"}
{"id": 272, "q": "Pandas: multiply column value by sum of group", "d": "I have a dataframe which looks like this: I want to add a column 'c' which multiplies the value of 'b' by the sum of the group it belongs to in column 'a'. So, first row should be 0.15 * 0.5 (sum of group A) = 0.075. This would be the excel formula for column 'c' =B1*SUMIF($A$1:$A$9,A1,$B$1:$B$9) Resulting dataframe should look like this:", "q_apis": "value sum add value sum first sum", "io": " a b 0 A 0.15 1 A 0.25 2 A 0.10 3 B 0.20 4 B 0.10 5 B 0.25 6 B 0.60 7 C 0.50 8 C 0.70 <s> a b c 0 A 0.15 0.075 1 A 0.25 0.125 2 A 0.10 0.05 3 B 0.20 0.23 4 B 0.10 0.115 5 B 0.25 0.2875 6 B 0.60 0.69 7 C 0.50 0.6 8 C 0.70 0.84 ", "apis": "groupby transform sum groupby transform sum", "code": ["df['b'] * df.groupby('a')['b'].transform('sum')\n#df['c'] = df['b'] * df.groupby('a')['b'].transform('sum')\n"], "link": "https://stackoverflow.com/questions/62611339/pandas-multiply-column-value-by-sum-of-group"}
{"id": 349, "q": "Efficiently replace values from a column to another column Pandas DataFrame", "d": "I have a Pandas DataFrame like this: I want to replace the values with the values in the second column () only if values are equal to 0, and after (for the zero values remaining), do it again but with the third column (). The Desired Result is the next one: I did it using the function, but it seems too slow.. I think must be a faster way to accomplish that. is there a faster way to do that?, using some other function instead of the function?", "q_apis": "replace values DataFrame DataFrame replace values values second values values", "io": " col1 col2 col3 1 0.2 0.3 0.3 2 0.2 0.3 0.3 3 0 0.4 0.4 4 0 0 0.3 5 0 0 0 6 0.1 0.4 0.4 <s> col1 col2 col3 1 0.2 0.3 0.3 2 0.2 0.3 0.3 3 0.4 0.4 0.4 4 0.3 0 0.3 5 0 0 0 6 0.1 0.4 0.4 ", "apis": "where where where where", "code": ["df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\ndf['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n", "df['col1'] = np.where(df['col1'] == 0, \n                      np.where(df['col2'] == 0, df['col3'], df['col2']),\n                      df['col1'])\n"], "link": "https://stackoverflow.com/questions/39903090/efficiently-replace-values-from-a-column-to-another-column-pandas-dataframe"}
{"id": 112, "q": "pandas.MultiIndex: assign all elements in first level", "d": "I have a dataframe with a multiindex, as per the following example: This gives me a dataframe like this: I have another 2-dimensional dataframe, as follows: Resulting in the following: I would like to assign to the cell, across all dates, and the cell, across all dates etc. Something akin to the following: Of course this doesn't work, because I am attempting to set a value on a copy of a slice : A value is trying to be set on a copy of a slice from a I tried several variations: How can I select index level 1 (eg: row 'a'), column 'a', across all index level 0 - and be able to set the values?", "q_apis": "MultiIndex assign all first assign all all value copy value copy select index all index values", "io": " a b c 2020-01-01 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN 2020-01-02 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN 2020-01-03 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN 2020-01-04 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN <s> a b c 2020-01-01 0.540867 0.426181 0.220182 2020-01-02 0.864340 0.432873 0.487878 2020-01-03 0.017099 0.181050 0.373139 2020-01-04 0.764557 0.097839 0.499788 ", "apis": "to_numpy DataFrame shape index index columns columns", "code": ["a = df.to_numpy()\n\npanel = pd.DataFrame((a[...,None] * a[:,None,:]).reshape(-1, df.shape[1]), \n                     index=panel.index, columns=panel.columns)\n"], "link": "https://stackoverflow.com/questions/65905694/pandas-multiindex-assign-all-elements-in-first-level"}
{"id": 250, "q": "Slicing each dataframe row into 3 windows with different slicing ranges", "d": "I want to slice each row of my dataframe into 3 windows with slice indices that are stored in another dataframe and change for each row of the dataframe. Afterwards i want to return a single dataframe containing the windows in form of a MultiIndex. The rows in each windows that are shorter than the longest row in the window should be filled with NaN values. Since my actual dataframe has around 100.000 rows and 600 columns, i am concerned about an efficient solution. Consider the following example: This is my dataframe which i want to slice into 3 windows And the second dataframe containing my slicing indices having the same count of rows as : I've tried slicing the windows, like so: Which gives me the following error: My expected output is something like this: Is there an efficient solution for my problem without iterating over each row of my dataframe?", "q_apis": "indices MultiIndex values columns second indices count", "io": ">>> df 0 1 2 3 4 5 6 7 0 0 1 2 3 4 5 6 7 1 8 9 10 11 12 13 14 15 2 16 17 18 19 20 21 22 23 <s> >>> df_slice 0 1 0 3 5 1 2 6 2 4 7 ", "apis": "reset_index melt index merge index to_numeric loc loc loc shift values left groupby index transform min groupby transform min groups index columns pivot_table index index columns values value columns MultiIndex from_product columns pivot_table index index columns values value columns MultiIndex from_product columns pivot_table index index columns values value columns MultiIndex from_product columns concat columns MultiIndex from_tuples columns", "code": ["    t = df.reset_index().melt(id_vars=\"index\")\n    t = pd.merge(t, df_slice, left_on=\"index\", right_index=True)\n    t.variable = pd.to_numeric(t.variable)\n    \n    t.loc[t.variable < t.c_0,\"group\"] = \"A\"\n    t.loc[(t.variable >= t.c_0) & (t.variable < t.c_1), \"group\"] = \"B\"\n    t.loc[t.variable >= t.c_1, \"group\"] = \"C\"\n\n    # shift relevant values to the left\n    shift_val = t.groupby([\"group\", \"index\"]).variable.transform(\"min\") - t.groupby([\"group\"]).variable.transform(\"min\")\n    t.variable = t.variable - shift_val\n    \n    # extract a, b, and c groups, and create a multi-level index for their\n    # columns\n    df_a = pd.pivot_table(t[t.group == \"A\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_a.columns = pd.MultiIndex.from_product([[\"a\"], df_a.columns])\n    \n    df_b = pd.pivot_table(t[t.group == \"B\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_b.columns = pd.MultiIndex.from_product([[\"b\"], df_b.columns])\n    \n    df_c = pd.pivot_table(t[t.group == \"C\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_c.columns = pd.MultiIndex.from_product([[\"c\"], df_c.columns])\n    \n    res = pd.concat([df_a, df_b, df_c], axis=1)\n    \n    res.columns = pd.MultiIndex.from_tuples([(c[0], i) for i, c in enumerate(res.columns)])\n    \n    print(res)\n"], "link": "https://stackoverflow.com/questions/62962437/slicing-each-dataframe-row-into-3-windows-with-different-slicing-ranges"}
{"id": 591, "q": "Conditional removal of duplicates entries in Pandas", "d": "How can I remove the duplicate entries in the Pandas DataFrame given below. Desired result: The condition is: remove if and .", "q_apis": "DataFrame", "io": "a b c d 11216 08-08-2018 2000 SIP 40277 28-08-2018 1000 SIP 44165 02-08-2018 8000 Lump 44165 03-08-2018 5000 Lump 45845 16-08-2018 25000 Lump 45845 18-08-2018 50000 Lump 52730 13-08-2018 10000 Lump 52730 27-08-2018 10000 Lump 53390 20-08-2018 400000 Lump 56180 02-08-2018 1000 Lump 58537 11-07-2018 5000 Lump 58537 22-08-2018 2000 SIP 912813 15-08-2018 160001 Lump 912813 15-08-2018 6000 SIP 85606 16-08-2018 3500 SIP 88327 06-08-2018 5000 SIP 90240 07-08-2018 2000 SIP <s> a b c d 11216 08-08-2018 2000 SIP 40277 28-08-2018 1000 SIP 44165 02-08-2018 8000 Lump 45845 16-08-2018 25000 Lump 52730 13-08-2018 10000 Lump 53390 20-08-2018 400000 Lump 58537 11-07-2018 5000 Lump 912813 15-08-2018 160001 Lump 912813 15-08-2018 6000 SIP 85606 16-08-2018 3500 SIP 88327 06-08-2018 5000 SIP 90240 07-08-2018 2000 SIP ", "apis": "", "code": ["i = 0 \nwhile i < len(a)-1 :\n    if a[i] == a[i+1] and if b[i] != b[s] :\n        del a[i]\n        del b[i]\n        del c[i]\n        del d[i]\n        i -= 1 \n    i += 1 \n"], "link": "https://stackoverflow.com/questions/52113834/conditional-removal-of-duplicates-entries-in-pandas"}
{"id": 603, "q": "Pandas dataframe: Replace multiple rows based on values in another column", "d": "I'm trying to replace some values in one dataframe's column with values from another data frame's column. Here's what the data frames look like. has a lot of rows and columns. The final df should look like this So basically what needs to happen is that and need to be matched and then needs to have values replaced by the corresponding row in for the rows that matched. I don't want to lose any values in which are not in I believe the module in python can do that? This is what I have so far: But it definitely doesn't work. I could also use merge as in but that doesn't replace the values inline.", "q_apis": "values replace values values columns values any values merge replace values", "io": "df1 0 1029 0 aaaaa Green 1 bbbbb Green 2 fffff Blue 3 xxxxx Blue 4 zzzzz Green df2 0 1 2 3 .... 1029 0 aaaaa 1 NaN 14 NaN 1 bbbbb 1 NaN 14 NaN 2 ccccc 1 NaN 14 Blue 3 ddddd 1 NaN 14 Blue ... 25 yyyyy 1 NaN 14 Blue 26 zzzzz 1 NaN 14 Blue <s> 0 1 2 3 .... 1029 0 aaaaa 1 NaN 14 Green 1 bbbbb 1 NaN 14 Green 2 ccccc 1 NaN 14 Blue 3 ddddd 1 NaN 14 Blue ... 25 yyyyy 1 NaN 14 Blue 26 zzzzz 1 NaN 14 Green ", "apis": "DataFrame DataFrame fillna merge left where isna drop", "code": ["df1 = pd.DataFrame({'0':['aaaaa','bbbbb','fffff','xxxxx','zzzzz'], '1029':['Green','Green','Blue','Blue','Green']})\n\ndf2 = pd.DataFrame({'0':['aaaa','bbbb','ccccc','ddddd','yyyyy','zzzzz',], '1029':[None,None,'Blue','Blue','Blue','Blue']})\n\n\n# Fill NaNs\ndf2['1029'] = df2['1029'].fillna(df1['1029'])\n\n# Merge the dataframes \ndf_ = df2.merge(df1, how='left', on=['0'])\n\ndf_['1029'] = np.where(df_['1029_y'].isna(), df_['1029_x'], df_['1029_y'])\n\ndf_.drop(['1029_y','1029_x'],1,inplace=True)\nprint(df_)\n"], "link": "https://stackoverflow.com/questions/51587685/pandas-dataframe-replace-multiple-rows-based-on-values-in-another-column"}
{"id": 638, "q": "Saving a Pandas dataframe in fixed format with different column widths", "d": "I have a pandas dataframe (df) that looks like this: I want to save this dataframe in a fixed format. The fixed format I have in mind has different column width and is as follows: \"one space for column A's value then a comma then four spaces for column B's values and a comma and then five spaces for column C's values\" Or symbolically: My dataframe above (df) would look like the following in my desired fixed format: How can I write a command in Python that saves my dataframe into this format?", "q_apis": "value values values", "io": " A B C 0 1 10 1234 1 2 20 0 <s> 1, 10, 1234 2, 20, 0 ", "apis": "apply apply to_csv index", "code": ["df['B'] = df['B'].apply(lambda t: (' '*(4-len(str(t)))+str(t)))\ndf['C'] = df['C'].apply(lambda t: (' '*(5-len(str(t)))+str(t)))\ndf.to_csv('path_to_file.csv', index=False)\n"], "link": "https://stackoverflow.com/questions/50067957/saving-a-pandas-dataframe-in-fixed-format-with-different-column-widths"}
{"id": 607, "q": "Pulling Multiple Ranges from Dataframe Pandas", "d": "Lets say I have the following data set: I have a the following list of ranges. How do I efficiently pull rows that lie in those ranges? Wanted Output Edit: Needs to work for floating points", "q_apis": "", "io": "A B 10.1 53 12.5 42 16.0 37 20.7 03 25.6 16 30.1 01 40.9 19 60.5 99 <s> A B 10.1 53 12.5 42 20.7 03 40.9 19 ", "apis": "isin", "code": ["from itertools import chain\n\nvals = set(chain.from_iterable(range(i, j) for i, j in L))\n\nres = df[df['A'].isin(vals)]\n"], "link": "https://stackoverflow.com/questions/51371507/pulling-multiple-ranges-from-dataframe-pandas"}
{"id": 201, "q": "Dropping rows from pandas dataframe based on value in column(s)", "d": "Suppose I have a dataframe which has Column 'A' and Column 'B' How do I drop rows where Column 'A' and 'B' are equal , but not in same row. I only wanto to drop rows where column 'B' is equal to column 'A' For example Column 'B' from Rows 4, 8 & 9 is equal to Rows 2,3&5 Column 'A'. I want to drop Rows 4, 8 & 9 Drop Rows 4, 8 & 9 since Column B from rows is equal to column A from row 2,3&5 Expected output Rows 4, 8 & 9 needs to be deleted Adding additional details: Column A and B will never be equal in same row. Multiple rows in Column B may have matching values in Column A. To illustrate I have expanded the dataframe Sorry if my originial row numbers are not matching. To summarize the requirement. Multiple rows will have column B matching with Column A and expectation is to delete all rows where column B is matching with Column A in any row. To reiterate Column A and Column B will not be equal in same row", "q_apis": "value drop where drop where drop values delete all where any", "io": " Column A Column B 1 10 62 2 10 72 3 20 75 4 20 10 5 30 35 6 30 45 7 40 55 8 40 20 9 40 30 <s> Column A Column B 1 10 62 2 10 72 3 20 75 5 30 35 6 30 45 7 40 55 ", "apis": "isin", "code": ["df[~(df['Column B'].isin(df['Column A']) & (df['Column B'] != df['Column A']))]\n"], "link": "https://stackoverflow.com/questions/64198934/dropping-rows-from-pandas-dataframe-based-on-value-in-columns"}
{"id": 590, "q": "How to calculate dictionaries of lists using pandas DataFrame?", "d": "I have two strings in Python3.x, which are defined to be the same length: I am also given an integer which is meant to represent the \"starting index\" of . In this case, . The goal is to create a dictionary based on the indices. So, begins at , begins at . The dictionary \"converting\" these coordinates is as follows: which can be constructed (give the variables above) with: I currently have this data in the form of a pandas DataFrame: There are multiple entries of the same string in column . In this case, the dictionary for the coordinates with should be: I would like to take this DataFrame and calculate similar dictionaries of the coordinates. Such a statement looks like one should somehow use ? I'm not sure how to populate dictionary lists like this... Here is the correct output (keeping the DataFrame structure). Here the DataFrame has the column such that it looks like the following:", "q_apis": "DataFrame length index indices at at DataFrame take DataFrame DataFrame DataFrame", "io": "{0: 51, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61} <s> {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], 4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]} ", "apis": "dtype", "code": ["column1\nLJNVTJOY      {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54,...\nMXRBMVQDHF    {0: [79], 1: [80], 2: [81], 3: [82], 4: [83], ...\nWHLAOECVQR    {0: [18], 1: [19], 2: [20], 3: [21], 4: [22], ...\nName: val, dtype: object\n"], "link": "https://stackoverflow.com/questions/52143288/how-to-calculate-dictionaries-of-lists-using-pandas-dataframe"}
{"id": 569, "q": "Create a column that has the same length of the longest column in the data at the same time", "d": "I have the following data: Output: Is it possible to create a 4th column AT THE SAME TIME the others columns are created in data, which has the same length as the longest column of this dataframe (3rd one)? The data of this column doesn't matter. Assume it's 8. So this is the desired output can be: In my script the dataframe keeps changing every time. This means the longest columns keeps changing with it. Thanks for reading", "q_apis": "length at time columns length time columns", "io": " 0 1 2 0 1.0 1.0 1.0 1 2.0 2.0 2.0 2 3.0 3.0 3.0 3 NaN 4.0 4.0 4 NaN 5.0 5.0 5 NaN NaN 6.0 6 NaN NaN 7.0 <s> 0 1 2 3 0 1.0 1.0 1.0 8.0 1 2.0 2.0 2.0 8.0 2 3.0 3.0 3.0 8.0 3 NaN 4.0 4.0 8.0 4 NaN 5.0 5.0 8.0 5 NaN NaN 6.0 8.0 6 NaN NaN 7.0 8.0 ", "apis": "", "code": ["data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]] + [[]]\n"], "link": "https://stackoverflow.com/questions/52676653/create-a-column-that-has-the-same-length-of-the-longest-column-in-the-data-at-th"}
{"id": 315, "q": "pandas data change based on condition", "d": "I have data which has special characters, I want to change the conditional cell values. Data is below first few lines df_orig: I want to change cell values where $ in D, A = 0 and B = C THE OUTPUT SHOULD BE change: I tried at my end with but it didn't work.", "q_apis": "values first values where at", "io": "idx A B C D 0 0.5 2 5 # 1 3 5 8 % 2 6 8 10 $ 3 9 10 15 $ 4 11 15 18 # <s> idx A B C D 0 0.5 2 5 # 1 3 5 8 % 2 0 10 10 $ 3 0 15 15 $ 4 11 15 18 # ", "apis": "eq loc assign loc values", "code": ["m = df['D'].eq('$')\ndf.loc[m, ['A','B']] = df.assign(E=0).loc[m, ['E','C']].values\n"], "link": "https://stackoverflow.com/questions/61075047/pandas-data-change-based-on-condition"}
{"id": 464, "q": "How should I construct this json return from a pandas dataframe", "d": "I have some data, organised by date, as a datetime index. I then subset it so it is effectively irregular: In my service (this is not for interactive use) I am given a string which I pass to to aggregate my time data to any level. The string is supplied directly to the argument, and can be values like , , , I would like to use the same string to create an json which is similar to the following structure: The array will be 'ragged' i.e. not all records will be present, and not all keys in the json will have the same length of array. The goals for a good solution are: is the level set by the same string parameter as given to the array in the aggregate level will always be the raw, datetime hourly values Ideally the string parameter is not associated with a bunch of 'translatation rules' such as \"If then use like this, but if use this and if use this would just return a single array, of the raw values So in practice if a is supplied: Note that there are two keys at daily level, with the values in the array split to the correct day. If is supplied: Note that this means the contents of the value array will be 3 in this example, as the 3 datetimes are all in the same month Things I've tried/looked at that I haven't made work well: , they look like they aggregate only, based on some rules. I specifically need to return the actual records Parseing a new column based on the argument would technically work, but it seems wrong as I would have to start converting each to a or similar. I have not yet found a function that accepts the same character string and does not also perform aggregations Is setting a multi-index a solution to this? It might be but I'm not certain how to populate it in regards to the point above about the , , etc. custom resampler: Which is not working. I understand, it may be that this is not solvable with the rules above, but I am probably not good enough at yet to realise it.", "q_apis": "date index aggregate time any values array all all keys length array array aggregate values array values keys at values array day value array all month at aggregate start index at", "io": "{'2018-01-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'}, '2018-01-08':{'2018-01-08 03:00:00'}} <s> {'2018-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00', '2018-01-08 03:00:00'}} ", "apis": "to_json resample apply", "code": ["def custom_resampler(array_like):\n    return array_like.to_json()\n\ndf.resample('D').apply(custom_resampler)\n"], "link": "https://stackoverflow.com/questions/56681714/how-should-i-construct-this-json-return-from-a-pandas-dataframe"}
{"id": 419, "q": "merging two rows of data in a single row with Python/Pandas", "d": "I have a dataframe like this: I need the rows belonging to the same ID to be merged in a single row, the merged dataframe will be like this I tried using np.random.permutation, np.roll etc but unable to get the desired result. The count of rows in my original data set is in thousands so loops and creating individual data sets and then merging is not helping", "q_apis": "get count", "io": " ID A1 A2 A3 A4 0 01 100 101 103 104 1 01 501 502 503 504 2 01 701 702 703 704 3 02 1001 1002 1003 1004 4 03 2001 2002 2003 2004 5 03 5001 5002 5003 5004 <s> ID A1 A2 A3 A4 B1 B2 B3 B4 C1 C2 C3 C4 0 01 101 102 103 104 501 502 503 504 701 702 703 704 1 02 1001 2001 1003 1004 2 03 2001 2002 2003 2004 5001 5002 5003 5004 ", "apis": "columns Series loc unstack values index groupby apply unstack", "code": ["import pandas as pd\n\n\ndef widen(x):\n    num_rows = len(x)\n    num_cols = len(x.columns)\n\n    new_index = [\n        chr(ord('A') + row_number) + str(col_number + 1)\n        for row_number in range(num_rows)\n        for col_number in range(num_cols)\n    ]\n\n    return pd.Series(x.loc[:, 'A1':].unstack().values, index=new_index)\n\nres = df.groupby('ID').apply(widen).unstack()\n"], "link": "https://stackoverflow.com/questions/58254668/merging-two-rows-of-data-in-a-single-row-with-python-pandas"}
{"id": 530, "q": "How can I remove the &#39;NaN&#39; not removing the data?", "d": "I'm trying to remove the 'NaN'. In detail, there is data on one line and 'NaN'. My data looks like the one below. I want to eliminate the NAN between the data and make one data for every 18 lines. I tried option 'dropna()' (using 'how = 'all'' or 'thread = '10''). But these are not what I want. How can I remove NaN and merge data? Add This is the code that I using(python2). The is the data that have NaN. If you look at the data, there are data in the 0th line from 1 to 10, and data in the 1st line from 11th to 21st. That is, there are two lines of data. I want to wrap this in a single line without NaN. Like this result. I tried to re-index the row to time to using resampling. And I save the start and end index. And I save the index_time to use resampling time. The result of 'df_time_merge' is like this. enter image description here It's working!! But if I have data(starting with Nan) like this, the code didn't working. enter image description here If I run same code, the and . Where did I miss?", "q_apis": "between dropna all merge at index time start index time", "io": " 01 02 03 04 05 06 07 08 09 10 ... 12 13 \\ 0 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 14 15 16 17 18 19 20 21 0 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 1 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 <s> 01 02 03 04 05 06 07 08 09 10 ... 12 13 \\ 0 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 14 15 16 17 18 19 20 21 0 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 1 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 ", "apis": "test test test test test test test test array test all array test array array", "code": ["def make_sample():\n    test=np.full((8,12), np.nan)\n    test[0,:6]=np.arange(6)\n    test[1,6:]=np.arange(6,18,2)\n    test[4:6,:]=2*test[:2,:]\n    return test\n\ntest=make_sample()\n\nIn [74]: test\nOut[74]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n", "filt=1^np.isnan(test).all(axis=1)\n\nIn [78]: filt\nOut[78]: array([1, 1, 0, 0, 1, 1, 0, 0])\n", "compress=np.compress(filt, test, axis=0)\n\nIn [80]: compress\nOut[80]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.]])\n", "compress[np.isnan(compress)]=0\n\nIn [83]: compress\nOut[83]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0., 12., 16., 20., 24., 28., 32.]])\n"], "link": "https://stackoverflow.com/questions/54273695/how-can-i-remove-the-nan-not-removing-the-data"}
