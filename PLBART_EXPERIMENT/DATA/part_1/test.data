{"id": 67, "q": "pandas: write df to text file - indent df to right by 5 white spaces", "d": "I am writing a df to a text file like so: This works fine but how can I indent my df so it sits 5 white spaces to the right. so from this: to: Is this possible? Thanks.", "q_apis": "right right", "io": " dim_pptx qp_pptx Absolute Radio 0.0739 0.0753 BBC Asian Network 0.0013 0.0013 BBC Radio 1 0.1441 0.1455 BBC Radio 1Xtra 0.0057 0.0058 BBC Radio 2 0.2336 0.2339 <s> dim_pptx qp_pptx Absolute Radio 0.0739 0.0753 BBC Asian Network 0.0013 0.0013 BBC Radio 1 0.1441 0.1455 BBC Radio 1Xtra 0.0057 0.0058 BBC Radio 2 0.2336 0.2339 ", "apis": "to_string replace", "code": ["\" \"*5 + df.to_string().replace(\"\\n\", \"\\n     \")"], "link": "https://stackoverflow.com/questions/33283249/pandas-write-df-to-text-file-indent-df-to-right-by-5-white-spaces"}
{"id": 551, "q": "Pandas dataframe: Remove secondary upcoming same value", "d": "I have a dataframe: On I want to keep only the first from the top and replace every below the first one with a , such that the output is: Thank you very much.", "q_apis": "value first replace first", "io": "col1 col2 a 0 b 1 c 1 d 0 c 1 d 0 <s> col1 col2 a 0 b 1 c 0 d 0 c 0 d 0 ", "apis": "mask eq loc mask index mask idxmax", "code": ["mask = df['col2'].eq(1)\ndf.loc[mask & (df.index != mask.idxmax()), 'col2'] = 0\n"], "link": "https://stackoverflow.com/questions/53654729/pandas-dataframe-remove-secondary-upcoming-same-value"}
{"id": 429, "q": "Pandas dataframe column headers to labels for data", "d": "SUMMARY: The output of my code gives me a dataframe of the following format. The column headers of the dataframe are the labels for the text in the column . The labels will be used as training data for a multilabel classifier in the next step. This is a snippet of actual data which is much larger. Since they are columns titles, it is not possible to use them as mapped to the text they are the labels for. UPDATE: Converting the df to csv shows the empty cells are blank( vs ): Where is the column where the text is, and , , , , and are the column headers that need to be turned into the labels. Only columns with 1s or 2s are relevant. The column with empty cells are not relevant and thus don't need to be converted as labels. UPDATE: After some digging, maybe the numbers might not be ints, but strings. I know that when entering the text + labels into a classifier for processing, the length of both arrays needs to be equal, else it is not accepted as valid input. Is there a way I can convert the columns titles to labels for the text in in the DF? EXPECTED OUTPUT:", "q_apis": "step columns empty where columns empty length columns", "io": "Content A B C D E zxy 1 2 1 wvu 1 2 1 tsr 1 2 2 qpo 1 1 1 nml 2 2 kji 1 1 2 hgf 1 2 edc 1 2 1 <s> >>Content A B C D E Labels 0 zxy 1 2 1 A, B, D 1 wvu 1 2 1 A, C, D 2 tsr 1 2 2 A, B, E 3 qpo 1 1 1 B, C, D 4 nml 2 2 C, D 5 kji 1 1 2 A, C, E 6 hgf 1 2 C, E 7 edc 1 2 1 A, B, D ", "apis": "names columns loc loc dot columns isin dot columns", "code": ["# make certain the label names match the appropriate columns \ns=df.loc[:, ['A', 'B', 'C', 'D', 'E']]  \n# or\ns=df.loc[:,'A':]\n\ndf['Labels']=(s>0).dot(s.columns+',').str[:-1]  # column A:E need to be numeric, not str\n# df['Labels']=(~s.isin(['']).dot(s.columns+',').str[:-1]\n"], "link": "https://stackoverflow.com/questions/57857068/pandas-dataframe-column-headers-to-labels-for-data"}
{"id": 654, "q": "How can I change the structure of a dataframe?", "d": "How can I change the structure of a dataframe? I need series data of each row. I tried unstack but failed. Example dataframe: Output Series:", "q_apis": "unstack Series", "io": " df: c1 c2 c3 0 a b c 1 d e f <s> S1 0 a 1 b 2 c S2 0 d 1 e 2 f ", "apis": "T index RangeIndex shape columns shape columns unstack", "code": ["dfT = df.T\ndfT.index = pd.RangeIndex(dfT.shape[0]) # Rename the indicies\ndfT.columns=['S{}'.format(i+1) for i in range(dfT.shape[1])] # Rename the columns\ndfT.unstack()\n"], "link": "https://stackoverflow.com/questions/43828124/how-can-i-change-the-structure-of-a-dataframe"}
{"id": 3, "q": "replace zero with value of an other column using pandas", "d": "I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become:", "q_apis": "replace value replace value", "io": " ref Name id Score 8400 John 0 12 3840 Peter 414 0 7400 David 612 64 5200 Karen 0 0 <s> ref Name id Score 8400 John 8400 12 3840 Peter 414 0 7400 David 612 64 5200 Karen 5200 0 ", "apis": "where eq", "code": ["#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n"], "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas"}
{"id": 507, "q": "Change day to specific entries in pandas dataframe", "d": "I have a dataframe in pandas which has an error in the index: each entry between 23:00:00 and 23:59:59 has a wrong date. I would need to subtract one day (i.e. 24 hours) to each entry between those two times. I know that I can obtain the entries between those two times as , where is my dataframe. However, can I modify the day only for those specific entries of the dataframe index? Resetting would take me more time, since my dataframe index is not evenly spaced as you can see from the figure below (the step between two consecutive entries is once 15 minutes and once 30 minutes). Note also from the figure the wrong date in the last three entries: it should be 2018-02-05 and not 2018-02-06. I tried to do this but I get Sample data: Expected output:", "q_apis": "day index between date day between between where day index take time index step between date last get", "io": "2018-02-05 22:00:00 271.8000 2018-02-05 22:30:00 271.5600 2018-02-05 22:45:00 271.4400 2018-02-06 23:15:00 271.3750 2018-02-06 23:30:00 271.3425 2018-02-06 00:00:00 271.2700 2018-02-06 00:15:00 271.2300 2018-02-06 00:45:00 271.1500 2018-02-06 01:00:00 271.1475 2018-02-06 01:30:00 271.1425 2018-02-06 01:45:00 271.1400 <s> 2018-02-05 22:00:00 271.8000 2018-02-05 22:30:00 271.5600 2018-02-05 22:45:00 271.4400 2018-02-05 23:15:00 271.3750 2018-02-05 23:30:00 271.3425 2018-02-06 00:00:00 271.2700 2018-02-06 00:15:00 271.2300 2018-02-06 00:45:00 271.1500 2018-02-06 01:00:00 271.1475 2018-02-06 01:30:00 271.1425 2018-02-06 01:45:00 271.1400 ", "apis": "index hour day append replace day day year month append replace day month month append index", "code": ["as_list = df.index.tolist()\nnew_index = []\nfor idx,entry in enumerate(as_list):\n    if entry.hour == 23:\n        if entry.day != 1:            \n            new_index.append(as_list[idx].replace(day = as_list[idx].day - 1))\n        else:\n            new_day = calendar.monthrange(as_list[idx].year, as_list[idx].month -1)[1]\n            new_index.append(as_list[idx].replace(day = new_day, month = entry.month -1))\n    else:\n        new_index.append(entry)\ndf.index = new_index\n"], "link": "https://stackoverflow.com/questions/54891381/change-day-to-specific-entries-in-pandas-dataframe"}
{"id": 528, "q": "python pandas - calculate percentage change using last non-na value", "d": "I am pretty new to python (mostly I use R) and I would like to perform a simple calculation but keep getting errors and incorrect results. I would like to calculate the percentage change for a column in a pandas df using the latest non-na value. A toy example is below. I keep getting a weird result: I guess this has to do with the Nan values. How do I tell python to use the latest non-na value. The desired result is as follows: Since I don't know very much python at all, any suggestions would be welcome, even more convoluted ones.", "q_apis": "last value value values value at all any", "io": "price_chg = [Nan, -0.2307, 0, 0, 0.4444, NaN] <s> price_chg = [Nan, -0.2307, 0.4444, 0, 0, NaN] ", "apis": "pct_change backfill", "code": ["df['price_chg'] = df.price.pct_change(periods = -1, fill_method='backfill')"], "link": "https://stackoverflow.com/questions/54372352/python-pandas-calculate-percentage-change-using-last-non-na-value"}
{"id": 218, "q": "Heatmap of counts of every value in every column", "d": "I have a dataframe that is like this: Where A B C D are categories, and the values are in the range [1, 10] (some values might not appear in a single column) I would like to have a dataframe that for every category shows the count of those values. Something like this: I tried using and but I can't seem to understand what parameters to give.", "q_apis": "value categories values values count values", "io": "| A | B | C | D | |---|---|----|---| | 1 | 3 | 10 | 4 | | 2 | 3 | 1 | 5 | | 1 | 7 | 9 | 3 | <s> | | A | B | C | D | |----|---|----|---|---| | 1 | 2 | 0 | 1 | 0 | | 2 | 1 | 0 | 0 | 0 | | 3 | 0 | 2 | 0 | 1 | | 4 | 0 | 0 | 0 | 1 | | 5 | 0 | 0 | 0 | 1 | | 6 | 0 | 0 | 0 | 0 | | 7 | 0 | 1 | 0 | 0 | | 8 | 0 | 0 | 0 | 0 | | 9 | 0 | 0 | 1 | 0 | | 10 | 0 | 0 | 1 | 0 | ", "apis": "apply value_counts fillna plot", "code": ["# counts\ncounts = df.apply(pd.value_counts).fillna(0)\n\n# plot\nsns.heatmap(counts, cmap=\"GnBu\", annot=True)\n"], "link": "https://stackoverflow.com/questions/63757556/heatmap-of-counts-of-every-value-in-every-column"}
{"id": 173, "q": "Python Pandas : pandas.to_datetime() is switching day &amp; month when day is less than 13", "d": "I wrote a code that reads multiple files, however on some of my files datetime swaps day & month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). I tried to fix it by doing this,but it doesn't work. My data frame looks like this: The actual datetime is from 12june2015 to 13june2015 when my I read my datetime column as a string the dates remain correct dd/mm/yyyy but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13. output: Here is my code : I loop through files : then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type by pd_datetime() it swaps the day and month when the day is less than 13. Post converting my datetime column the dates are correct (string type) But when I change the column type I get this: The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13? UPDATE This command swaps all the days and months of my column So in order to swap only the incorrect dates, I wrote a condition: But it doesn't work either", "q_apis": "to_datetime day month day day month day any day day day month day all day month day get stop day month day all days", "io": "tmp p1 p2 11/06/2015 00:56:55.060 0 1 11/06/2015 04:16:38.060 0 1 12/06/2015 16:13:30.060 0 1 12/06/2015 21:24:03.060 0 1 13/06/2015 02:31:44.060 0 1 13/06/2015 02:37:49.060 0 1 <s> print(df) tmp p1 p2 06/11/2015 00:56:55 0 1 06/11/2015 04:16:38 0 1 06/12/2015 16:13:30 0 1 06/12/2015 21:24:03 0 1 13/06/2015 02:31:44 0 1 13/06/2015 02:37:49 0 1 ", "apis": "to_datetime apply replace microsecond date date time time year month day date apply Series strftime day to_numeric day month to_numeric month year to_numeric year days day month index day loc index day loc index month loc index month loc index day day day astype month month astype year year astype date to_datetime year month day date date astype time time astype drop date year month day time", "code": ["df['tmp'] = pd.to_datetime(df['tmp'], unit='ns')\ndf['tmp'] = df['tmp'].apply(lambda x: x.replace(microsecond=0))\ndf['date'] = [d.date() for d in df['tmp']]\ndf['time'] = [d.time() for d in df['tmp']]\ndf[['year','month','day']] = df['date'].apply(lambda x: pd.Series(x.strftime(\"%Y-%m-%d\").split(\"-\")))\n\ndf['day'] = pd.to_numeric(df['day'], errors='coerce')\ndf['month'] = pd.to_numeric(df['month'], errors='coerce')\ndf['year'] = pd.to_numeric(df['year'], errors='coerce')\n\n\n#Loop to look for days less than 13 and then swap the day and month\nfor index, d in enumerate(df['day']):\n        if(d <13): \n df.loc[index,'day'],df.loc[index,'month']=df.loc[index,'month'],df.loc[index,'day'] \n", " df['day'] = df['day'].astype(str)\n df['month'] = df['month'].astype(str)\n df['year'] = df['year'].astype(str)\n df['date']=  pd.to_datetime(df[['year', 'month', 'day']])\n df['date'] = df['date'].astype(str)\n df['time'] = df['time'].astype(str)\n", "df.drop(df[['date','year', 'month', 'day','time']], axis=1, inplace = True)\n"], "link": "https://stackoverflow.com/questions/50367656/python-pandas-pandas-to-datetime-is-switching-day-month-when-day-is-less-t"}
{"id": 314, "q": "Find count of unique value of each column and save in CSV", "d": "I have data like this: Need to count unique value of each column and report it like below: I have no issue when number of column are limit and manually name them, when input file is big it become hard,need to have simple way to have output here is the code I have", "q_apis": "count unique value count unique value name", "io": "+---+---+---+ | A | B | C | +---+---+---+ | 1 | 2 | 7 | | 2 | 2 | 7 | | 3 | 2 | 1 | | 3 | 2 | 1 | | 3 | 2 | 1 | +---+---+---+ <s> +---+---+---+ | A | 3 | 3 | | A | 2 | 1 | | A | 1 | 1 | | B | 2 | 5 | | C | 1 | 3 | | C | 7 | 2 | +---+---+---+ ", "apis": "read_csv all columns all update value_counts concat all to_csv", "code": ["import pandas as pd \ndf=pd.read_csv('1.csv')\nall={}\ncolm=df.columns\nfor i in colm:\n    all.update({i:df[i].value_counts()})\n\nresult = pd.concat(all)\nresult.to_csv('out.csv')\n"], "link": "https://stackoverflow.com/questions/61077572/find-count-of-unique-value-of-each-column-and-save-in-csv"}
{"id": 539, "q": "How to merge rows in a dataframe based on a column value?", "d": "I have a data-set that is in the shape of this, where each row represents a in a specific match that is specified by the . The thing I want to do is create a function that takes the rows with the same and joins them. As you can see in data example below, the two rows represents one game that is split up into a home team (row_1 ) and an away team (row_2). I want these two rows to sit on one row only. How do I get this result? EDIT: I created too much confusion, posting my code so you can get a better grasp of the problem I want to solve.", "q_apis": "merge value shape where get get", "io": " gameID Won/Lost Home Away metric2 metric3 metric4 team1 team2 team3 team4 2017020001 1 1 0 10 10 10 1 0 0 0 2017020001 0 0 1 10 10 10 0 1 0 0 <s> Won/Lost h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4 1 10 10 10 10 10 10 1 0 0 0 0 1 0 0 ", "apis": "loc reset_index drop loc add_prefix reset_index drop add_prefix reset_index drop concat", "code": ["def munge(group): \n     is_home = group.Home == 1 \n     wonlost = group.loc[is_home, 'Won/Lost'].reset_index(drop=True) \n     group = group.loc[:, 'metric2':] \n     home = group[is_home].add_prefix('h_').reset_index(drop=True) \n     away = group[~is_home].add_prefix('a_').reset_index(drop=True) \n     return pd.concat([wonlost, home, away], axis=1) \n"], "link": "https://stackoverflow.com/questions/53973711/how-to-merge-rows-in-a-dataframe-based-on-a-column-value"}
{"id": 204, "q": "Drop rows based on condition pandas", "d": "Consider I have a dataframe that looks like this: What I need to do is to sum the C and D and if the sum is higher than 10 remove the entire row. Howerver I can't acess the columns by their names, I need to do it by their position. How can I do it in pandas? EDIT: Another problem. How can I keep the rows that have at least two values in the columns B, C and D?", "q_apis": "sum sum columns names at values columns", "io": " A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 <s> A B C D 0 0 NaN 2 3 1 4 5 NaN NaN 2 8 9 10 11 ", "apis": "DataFrame mask iloc isnull sum mask", "code": ["df = pd.DataFrame({'A': [0,4,8], 'B':[1, np.nan, 9], 'C':[2,np.nan, np.nan], 'D':[3, 7, 11]})\nmask = df.iloc[:,1:].isnull().sum(axis=1) < 2\nprint(df[mask])\n\n"], "link": "https://stackoverflow.com/questions/64176921/drop-rows-based-on-condition-pandas"}
{"id": 172, "q": ".max() for dataframe converts object type to float64", "d": "Have 3 columns namely Whenever I do , it ends up giving a float value. Something like Output: I wanted Tried using but no luck.", "q_apis": "max columns value", "io": " \"A\", \"B\" -7.480000e+01,-1.480000e+01 -7.410000e+01,-1.410000e+01 -7.370000e+01,-1.370000e+01 -7.360000e+01,-1.360000e+01 -7.370000e+01,-1.370000e+01 -7.390000e+01,-1.390000e+01 <s> \"C\" -7.480000e+01, -7.410000e+01 -7.370000e+01, -7.360000e+01, -7.370000e+01 -7.390000e+01 ", "apis": "apply max apply max", "code": ["df.apply(lambda x: '{:e}'.format(max(x['A'], x['B'])), axis=1)\n", "df.apply(lambda x: '{:e}'.format(max(float(x['A']), float(x['B']))), axis=1)\n"], "link": "https://stackoverflow.com/questions/64715870/max-for-dataframe-converts-object-type-to-float64"}
{"id": 617, "q": "Improve pandas filter speed by storing indices?", "d": "I have the following df: I accumulate the AREA column as so: For each in , the is summed where == or , and it is always run when is sorted on . The real dataframe I'm working on though is 150,000 records, each row belonging to a unique ID1. Running the above on this dataframe takes 2.5 hours. Since this operation will take place repeatedly for the foreseeable future, I decided to store the indices of the True values in and in a DB with the following schema. Table ID1: Table ID2: The next time I run the accumulation on the column (now filled with different values) I read in the sql tables and the convert them to dicts. I then use these dicts to grab the records I need during the summing loop. When run this way it only takes 6 minutes! My question: Is there a better/standard way to handle this scenario, i.e storing dataframe selections for later use? Side note, I have set an index on the SQL table's ID columns and tried getting the indices by querying the table for each id, and it works well, but still takes a little longer than the above (9 mins).", "q_apis": "filter indices where unique take indices values time now values index columns indices", "io": "ID_,INDEX_ 1 , 0 2 , 1 etc, ect <s> ID_,INDEX_ 1 , 0 1 , 4 2 , 0 2 , 1 2 , 3 2 , 5 etc, etc ", "apis": "set_index join groupby apply index dropna itertuples loc loc sum reset_index DataFrame columns set_index join set_index groupby apply index dropna drop set_index itertuples loc loc sum reset_index", "code": ["df = df.set_index('ID1') \nfor row in df.join(df.groupby('ID2')['AREA'].apply(lambda x: x.index.tolist()),rsuffix='_').dropna().itertuples():\n    df.loc[row[0],'AREA'] += df.loc[row[3],'AREA'].sum()\ndf = df.reset_index()\n", "df = pd.DataFrame( {'ID1':range(1,1501),'ID2': np.random.randint(1,1501,(1500,)),'AREA':[1]*1500}, \n                   columns = ['ID1','ID2','AREA'])\n", "df_list = (df.set_index('ID1')\n             .join(df.set_index('ID1').groupby('ID2')['AREA']\n                     .apply(lambda x: x.index.tolist()),rsuffix='_ID2')\n             .dropna().drop(['AREA','ID2'],1))\n", "df = df.set_index('ID1') \nfor row in df_list.itertuples():\n    df.loc[row[0],'AREA'] += df.loc[row[1],'AREA'].sum()\ndf = df.reset_index()\n"], "link": "https://stackoverflow.com/questions/51045848/improve-pandas-filter-speed-by-storing-indices"}
{"id": 369, "q": "Most efficient way to multiply every column of a large pandas dataframe with every other column of the same dataframe", "d": "Suppose I have a dataset that looks something like: I want to get a dataframe that looks like the following, with the original columns, and all possible interactions between columns: My actual datasets are pretty large (~100 columns). What is the fastest way to achieve this? I could, of course, do a nested loop, or similar, to achieve this but I was hoping there is a more efficient way.", "q_apis": "get columns all between columns columns", "io": "INDEX A B C 1 1 1 0.75 2 1 1 1 3 1 0 0.35 4 0 0 1 5 1 1 0 <s> INDEX A B C A_B A_C B_C 1 1 1 0.75 1 0.75 0.75 2 1 1 1 1 1 1 3 1 0 0.35 0 0.35 0 4 0 0 1 0 0 0 5 1 1 0 1 0 0 ", "apis": "columns", "code": ["import numpy as np\n\ndef fx(x, y):\n    return np.multiply(x, y)\n\nfor col1, col2 in combinations(df.columns, 2):\n    df[f\"{col1}_{col2}\"] = np.vectorize(fx)(df[col1], df[col2])\n"], "link": "https://stackoverflow.com/questions/59673066/most-efficient-way-to-multiply-every-column-of-a-large-pandas-dataframe-with-eve"}
{"id": 228, "q": "Multiply dataframe rows depends on value in this row", "d": "I have a dataframe like this: I need multiply rows depends on value in 'col3'. Desired output:", "q_apis": "value value", "io": " col1 col2 col3 0 69 bar34 4 1 77 barf30 2 2 88 barfoo29 5 <s> col1 col2 col3 0 69 bar34 4 1 69 bar34 4 2 69 bar34 4 3 69 bar34 4 4 77 barf30 2 5 77 barf30 2 6 88 barfoo29 5 7 88 barfoo29 5 8 88 barfoo29 5 9 88 barfoo29 5 10 88 barfoo29 5 ", "apis": "set_index reindex index repeat reset_index drop loc index repeat", "code": ["df = df.set_index(df.col3)\n\nprint(\n    df.reindex(df.index.repeat(df.col3))\n        .reset_index(drop=True)\n)\n", "# suggested by @anky,\n\ndf.loc[df.index.repeat(df.col3)]\n"], "link": "https://stackoverflow.com/questions/63583217/multiply-dataframe-rows-depends-on-value-in-this-row"}
{"id": 109, "q": "Pandas Dataframe, average non 0 value", "d": "I have the following Pandas Dataframe 'df': How do I get the average value of \"a\" for from a1, a2, a3, ignoring the 0 value? I'm stuck with manual approach where I convert the value > 0 to 1", "q_apis": "value get value value where value", "io": "a1 a2 a3 b1 0 0 0 1 1 2 0 2 3 0 0 3 2 4 0 4 <s> a1 a2 a3 b1 avg(a) 0 0 0 1 0 1 2 0 2 1.5 3 0 0 3 3.0 2 4 0 4 3.0 ", "apis": "filter mask eq mean fillna mean fillna", "code": ["a = df.filter(like='a')\ndf['avg'] = a.mask(a.eq(0)).mean(1).fillna(0)\n\n# OR df['avg'] = a[a > 0].mean(1).fillna(0)\n"], "link": "https://stackoverflow.com/questions/65995268/pandas-dataframe-average-non-0-value"}
{"id": 605, "q": "Combine multi columns to one column Pandas", "d": "Hi I have the following dataframe I would like to create a new column d to do something like this For the numbers, it is not in integer. It is in np.float64. The integers are for clear example. you may assume the numbers are like 32065431243556.62, 763835218962767.8 Thank you for your help", "q_apis": "columns", "io": " z a b c a 1 NaN NaN ss NaN 2 NaN cc 3 NaN NaN aa NaN 4 NaN ww NaN 5 NaN ss NaN NaN 6 aa NaN NaN 7 g NaN NaN 8 j 9 NaN NaN <s> z a b c d a 1 NaN NaN 1 ss NaN 2 NaN 2 cc 3 NaN NaN 3 aa NaN 4 NaN 4 ww NaN 5 NaN 5 ss NaN NaN 6 6 aa NaN NaN 7 7 g NaN NaN 8 8 j 9 NaN NaN 9 ", "apis": "fillna sum", "code": ["df['d'] = df[['a', 'b', 'c']].fillna(0).sum(axis=1)\n"], "link": "https://stackoverflow.com/questions/51357485/combine-multi-columns-to-one-column-pandas"}
{"id": 58, "q": "Pandas Dataframe: Calculate Shared Fraction", "d": "Suppose I have a dataframe, , consisting of a class of two objects, , a set of co-ordinates associated with them, and , and a value, , that was measured there. The dataframe looks like this: I would like to know the commands that allow me to go from this picture to the one where each is converted to a series of columns where: represents the sum of all the shared coordinates; and represent the fractions of the V for each possible class, . For example: I can sum and fraction calculate the fraction by using What are the next steps?", "q_apis": "value where columns where sum all sum", "io": "S X Y V 0 1 1 1 1 2 2 1 1 9 9 2 0 9 9 8 <s> X Y V_s F0 F1 1 1 1 1.0 0.0 2 2 1 0.0 1.0 9 9 10 0.2 0.8 ", "apis": "groupby sum unstack rename columns assign sum reset_index groupby sum unstack rename columns sum div assign reset_index", "code": ["(df.groupby(['X','Y','S']).sum()\n   .unstack('S', fill_value=0)['V']\n   .rename(columns=lambda x: f\"F{x}\")\n   .assign(V_s=lambda x: x.sum(1),\n           F0 =lambda x: x['F0']/x['V_s'],\n           F1 =lambda x: x['F1']/x['V_s'])\n   .reset_index()\n)\n", "new_df = (df.groupby(['X','Y','S']).sum()\n   .unstack('S', fill_value=0)['V']\n   .rename(columns=lambda x: f\"F{x}\")\n)\n\nvs = new_df.sum(1)\nnew_df = (new_df.div(vs,axis='rows')\n                .assign(V_s=vs)\n                .reset_index()\n         )\n          \n"], "link": "https://stackoverflow.com/questions/62050416/pandas-dataframe-calculate-shared-fraction"}
{"id": 202, "q": "How can I save DataFrame as list and not as string", "d": "I have this created. I want to create a and it is saving as this but by doing The problem is that each is now saved as . For example, row 3 is And for those skeptics, I've tried and it's . Column 2 is working well. How can I save as each row and not as ? That is, how can I save all rows of as and not as ?", "q_apis": "DataFrame now all", "io": " 0 1 0 [15921, 10, 82, 22, 202973, 368, 1055, 3135, 1... 0 1 [609, 226, 413, 363, 211, 241, 988, 80, 12, 19... 0 2 [22572, 3720, 233, 13, 827, 710, 512, 354, 1, ... 0 3 [345, 656, 25, 2589, 6, 866] 0 4 [29142, 8, 4141, 456, 24] 0 ... .. 1599995 [256, 8, 80, 110, 25, 152] 4 1599996 [609039, 22, 129, 184, 163, 9419, 769, 358, 10... 4 1599997 [140, 5715, 6540, 294, 1552] 4 1599998 [59, 22771, 189, 387, 4483, 13, 10305, 112231,... 4 1599999 [59, 15833, 200370, 609041, 609042] 4 <s> \"[345, 656, 25, 2589, 6, 866]\" ", "apis": "DataFrame apply eval", "code": ["import pandas as pd\n\ndf = pd.DataFrame({'a': [\"[1,2,3,4]\", \"[6,7,8,9]\"]})\ndf['b'] = df['a'].apply(eval)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/64183163/how-can-i-save-dataframe-as-list-and-not-as-string"}
{"id": 14, "q": "How to Extract Month Name and Year from Date column of DataFrame", "d": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format.", "q_apis": "DataFrame month name year", "io": "45 2018-01-01 73 2018-02-08 74 2018-02-08 75 2018-02-08 76 2018-02-08 <s> 45 Jan-2018 73 Feb-2018 74 Feb-2018 75 Feb-2018 76 Feb-2018 ", "apis": "DataFrame to_datetime date strftime to_period", "code": ["import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"], "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe"}
{"id": 344, "q": "Check for each value in column has only one corresponding value in another column pandas", "d": "I have my data in pandas data frame as follows: I would like to add another field in this dataframe(with True/False) such that. for each id value, there should be only one corresponding values. So, my expected output looks like this.. for the id - 1234 there are two corresponding values (AA and BB), the one with lesser count should be flagged.", "q_apis": "value value add value values values count", "io": " ID Name 0 1234 AA 1 1234 AA 2 1234 AA 3 5678 BB 4 5678 BB 5 5678 DD 6 9999 EE 7 9999 EE 8 1234 CC 9 1234 CC 10 1234 BB 11 1234 BB <s> ID Name 5 5678 DD 8 1234 CC 9 1234 CC 10 1234 BB 11 1234 BB ", "apis": "DataFrame filter names groupby transform nunique ne count groupby transform count filter groupby transform min", "code": ["# toy data\ndf = pd.DataFrame({'ID': [1234, 1234, 1234, 5678, 5678, 5678, 9999, 9999, 1234, 1234, 1234, 1234],\n 'Name': ['AA', 'AA', 'AA', 'BB', 'BB', 'DD', 'EE', 'EE', 'CC', 'CC', 'BB', 'BB']}\n)\n\n# filter those ID's that appear with multiple names\nnon_unique = df.groupby('ID').Name.transform('nunique').ne(1)\ndf = df[non_unique]\n\n# count the occurrences of each combination ['ID','Name']\ncounts = df[non_unique].groupby(['ID','Name']).Name.transform('count')\n\n# filter those with minimal occurrences within each ID\ndf[counts == counts.groupby(df['ID']).transform('min')]\n"], "link": "https://stackoverflow.com/questions/60381266/check-for-each-value-in-column-has-only-one-corresponding-value-in-another-colum"}
{"id": 277, "q": "Finding the frequency that each column is a row minimum", "d": "I have a dataframe that looks like: I would like to find the frequency that each column contains the row's minimum. So in some format: I am currently doing and then looping through each row using ... but there has to be a better way. In case it matters, I have a couple hundred columns, a couple thousand rows, and it represents a sample, so I have to perform the operation roughly a million times. I must be missing an obvious pandas or numpy method that will do this both pythonically and reasonably efficiently.", "q_apis": "contains columns sample", "io": " A B C D 0 1.2 0 1.1 3.2 1 2.3 2.2 2.2 2.5 2 1.1 1.5 0 1.7 3 0 1.1 1.4 1.2 4 3.3 3.0 1.7 1.7 5 1.1 1.0 2.2 2.5 6 5.0 5.0 5.0 5.0 <s> B: 2 # rows 0, 5 A: 1 # row 3 C: 1 # row 2 (B, C): 1 # row 1 (C, D): 1 # row 4 (A, B, C, D): 1 # row 6 ", "apis": "T eq min apply index value_counts", "code": ["df = df.T\n\nresult = (df.eq(df.min())\n            .apply(lambda x:tuple(x.index[x]))\n            .value_counts())\n"], "link": "https://stackoverflow.com/questions/62376848/finding-the-frequency-that-each-column-is-a-row-minimum"}
{"id": 548, "q": "Replace &quot;NaN&quot; value by last valid value for only one column in a dataframe with column multi-index (df.fillna)", "d": "I'm working with Python 3.6.5. Here is a little script to generate a multi index dataframe with some \"NaN\" value. I get this dataframe And I would like to replace the \"NaN\" value with the last valid value, BUT ONLY FOR ONE COLUMN. For example, I would like to get this (for column named 'X','b') I tried this : But I get this error \"A value is trying to be set on a copy of a slice from a DataFrame\" I can not find any solution for a dataframe with multi-index of column. I found this link that gives me no hope. (https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.MultiIndex.fillna.html) Does anyone have an idea to help me?", "q_apis": "value last value index fillna index value get replace value last value get get value copy DataFrame any index MultiIndex fillna", "io": "print(df) X Y a b a b 10 17.0 17.0 NaN NaN 20 15.0 11.0 20.0 28.0 25 NaN NaN 23.0 24.0 30 12.0 16.0 NaN NaN 35 10.0 10.0 NaN NaN 40 15.0 14.0 25.0 28.0 50 NaN NaN 22.0 22.0 80 NaN NaN 23.0 21.0 <s> print(df) X Y a b a b 10 17.0 17.0 NaN NaN 20 15.0 11.0 20.0 28.0 25 NaN 11.0 23.0 24.0 30 12.0 16.0 NaN NaN 35 10.0 10.0 NaN NaN 40 15.0 14.0 25.0 28.0 50 NaN 14.0 22.0 22.0 80 NaN 14.0 23.0 21.0 ", "apis": "loc isnull ffill", "code": ["df.loc[df['X']['b'].isnull(), ('X', 'b')] = df['X']['b'].ffill()\n"], "link": "https://stackoverflow.com/questions/53733422/replace-nan-value-by-last-valid-value-for-only-one-column-in-a-dataframe-with"}
{"id": 438, "q": "Changing the Increment Value in Data Frame at Certain Row", "d": "I have this data frame: I wanted to create another column called 'Seconds' that increments by 10 every row so I wrote this code: This produces the data frame: I now want to make the Seconds column increment by 10 until the 5th row. At the 5th row, I want to change the increment to 0.1 until the 7th row. At the 7th row, I want to change the increment back to 10. So, I want the data frame to look like this: How would I go about doing this? Should I change the index and multiply the index.values by a different value when I get to the row where the increment needs to change? Thanks in advance for your help.", "q_apis": "at now index index values value get where", "io": "Seconds Power 10 15 20 15 30 10 40 30 50 15 60 90 70 100 80 22 90 15 <s> Seconds Power 10 15 20 15 30 10 40 30 50 15 50.1 90 50.2 100 60.2 22 70.2 15 ", "apis": "repeat cumsum shape sum array diff sum repeat cumsum shape array", "code": ["np.repeat([10, 0.1, 10], [5, 2, 2]).cumsum()\n#                         ^  ^  ^\n#                        5th ^  ^\n#                           7th ^\n#                               Until end -> df.shape - increments[:-1].sum() more generally\n", "array([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])\n", "def cumsum_custom_increment(increments, points, n):\n    points[1:] = np.diff(points)\n    d = np.r_[points, n - np.sum(points)]\n    return np.repeat(increments, d).cumsum()\n\n>>> cumsum_custom_increment([10, 0.1, 10], [5, 7], df.shape[0])\narray([10. , 20. , 30. , 40. , 50. , 50.1, 50.2, 60.2, 70.2])\n"], "link": "https://stackoverflow.com/questions/57515994/changing-the-increment-value-in-data-frame-at-certain-row"}
{"id": 449, "q": "Add columns to a dataset without any columns", "d": "I want to load a dataset into a dataframe and then add columns to the dataset. Right now when I add columns, it removes the first line of data. To visualize for happens; Let's assume the following data from a csv is loaded to the dataframe 21,5,14 456,47,1 47,89,66 It will look like this So basically the first line of data is now shown as the columns, if your visualize the dataframe. When, I try to add columns Where, file_structure, is a list with the columns Does now look like this;", "q_apis": "columns any columns add columns now add columns first first now columns add columns columns now", "io": " 21 5 14 0 456 47 1 1 47 89 66 <s> x y z 0 456 47 1 1 47 89 66 ", "apis": "read_csv names", "code": ["    df = pd.read_csv(\n            io.StringIO(decoded.decode('utf-8')), index_col=False, low_memory=False, header=None, names=file_structure\n)\n"], "link": "https://stackoverflow.com/questions/57140598/add-columns-to-a-dataset-without-any-columns"}
{"id": 59, "q": "How to unlist a list with one value inside a pandas columns?", "d": "I have a pandas data frame: Is possible to convert the data frame into another data frame that look like this? I tried with this way but only get the . Thanks for your time!", "q_apis": "value columns get time", "io": "Id Col1 1 ['string'] 2 ['string2'] <s> Id Col1 1 string 2 string2 ", "apis": "apply explode", "code": ["import ast\n\ndf.Col1 = df.Col1.apply(ast.literal_eval).explode()\n"], "link": "https://stackoverflow.com/questions/67027649/how-to-unlist-a-list-with-one-value-inside-a-pandas-columns"}
{"id": 141, "q": "How do I turn categorical column values into different column names?", "d": "I'm not sure how to approach this problem since I'm a beginner with pandas. I have this dataframe: and I want to turn it into a dataframe or a matrix like this: How should I approach this in Python?", "q_apis": "values names", "io": " col1 col2 0 a 1 1 a 2 2 a 3 3 b 4 4 b 5 5 b 6 6 c 7 7 c 8 8 c 9 <s> cola colb colc 0 1 4 7 1 2 5 8 2 3 6 9 ", "apis": "DataFrame groupby", "code": ["pd.DataFrame({k: [*g['col2']] for k, g in df.groupby('col1')})\n"], "link": "https://stackoverflow.com/questions/65262268/how-do-i-turn-categorical-column-values-into-different-column-names"}
{"id": 350, "q": "Split multiple columns by numeric or alphabetic symbols", "d": "I'm working on splitting multiple columns by numeric or alphabetic symbols for columns to , then take the first part as the values of this column. For example, will be split by then take , will be splited by and take . The code I have tried: Output: My desired output will like this: Thanks for your help.", "q_apis": "columns columns columns take first values take take", "io": " id v1 v2 v3 0 1 \u6ce5\u5c97\u8def \u7ea2\u5c97\u82b1\u56ed12\u680b110\u623f NaN 1 2 \u6c99\u4e95\u8857\u9053 \u4e07\u4e30\u8def \u4e1c\u4fa7 2 3 \u4e2d\u5fc3\u533a N15\u533a \u5e78\u798f\u00b7\u6d77\u5cb810\u680bA\u5ea711A 3 4 \u9f99\u5c97\u9547 \u5357\u8054\u6751 \u957f\u6d77\u96c5\u56ed2\u680bD301D302\u623f\u4ea7 4 5 \u86c7\u53e3\u5de5\u4e1a\u533a \u5174\u534e\u8def \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c06\u680b504\u623f\u4ea7 5 6 \u5b9d\u5b89\u8def \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b103 NaN 6 7 \u5b9d\u5b89\u8def \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c10\u680b203 NaN 7 8 \u9f99\u5c97\u9547 \u4e2d\u5fc3\u57ce \u5c1a\u666f\u534e\u56ed12\u680b307\u623f 8 9 \u6c99\u6cb3\u897f\u8def \u897f\u535a\u6d77\u540d\u82d11\u680b30C\u623f\u4ea7 NaN 9 10 \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def \u5929\u9e45\u5821\u4e09\u671fP\u680b4D\u623f NaN 10 11 \u5e03\u5409\u9547 \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1C4\u680bC5\u680bC4\u5ea71403\u623f NaN <s> id v1 v2 v3 0 1 \u6ce5\u5c97\u8def \u7ea2\u5c97\u82b1\u56ed NaN 1 2 \u6c99\u4e95\u8857\u9053 \u4e07\u4e30\u8def \u4e1c\u4fa7 2 3 \u4e2d\u5fc3\u533a NaN \u5e78\u798f\u00b7\u6d77\u5cb8 3 4 \u9f99\u5c97\u9547 \u5357\u8054\u6751 \u957f\u6d77\u96c5\u56ed 4 5 \u86c7\u53e3\u5de5\u4e1a\u533a \u5174\u534e\u8def \u6d77\u6ee8\u82b1\u56ed\u591a\u5c42\u6d77\u6ee8\u82b1\u56ed\u5170\u5c71\u697c 5 6 \u5b9d\u5b89\u8def \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c NaN 6 7 \u5b9d\u5b89\u8def \u677e\u56ed\u00b7\u5357\u4e5d\u5df7\u7efc\u5408\u697c NaN 7 8 \u9f99\u5c97\u9547 \u4e2d\u5fc3\u57ce \u5c1a\u666f\u534e\u56ed 8 9 \u6c99\u6cb3\u897f\u8def \u897f\u535a\u6d77\u540d\u82d1 NaN 9 10 \u534e\u4fa8\u57ce\u9999\u5c71\u4e2d\u8def \u5929\u9e45\u5821\u4e09\u671f NaN 10 11 \u5e03\u5409\u9547 \u5fb7\u798f\u82b1\u56ed\u5fb7\u798f\u8c6a\u82d1 NaN ", "apis": "apply replace", "code": ["cols = ['v1', 'v2', 'v3']\ndf[cols] = df[cols].apply(lambda x: x.str.replace(r'[A-Za-z0-9].*', ''))\n"], "link": "https://stackoverflow.com/questions/60203245/split-multiple-columns-by-numeric-or-alphabetic-symbols"}
{"id": 320, "q": "How do i check that the unique values of a column exists in another column in dataframe?", "d": "I have a dataframe like this : What i want to is to check that the unique value of col1 exist in Col2 for the same ID ,The ID is not always the same number. the check must be done only among rows with the same id i want a result like : i tried I'm not sur it s the right command ,can anyone help please ?", "q_apis": "unique values unique value right", "io": " A= [ ID COL1 COL2 23 AA BB 23 AA AA 23 AA DD 23 BB BB 23 BB AA 23 BB DD 23 CC BB 23 CC AA 24 AA BB ] <s> A= [ ID COL1 COL2 check 23 AA BB OK 23 AA AA OK 23 AA DD OK 23 BB BB OK 23 BB AA OK 23 BB DD OK 23 CC BB KO 23 CC AA KO 24 AA BB KO ] ", "apis": "groupby apply isin where", "code": ["df['check'] = df.groupby(['ID', 'COL1'], group_keys=False\n                         ).apply(lambda x: x['COL1'].isin(x['COL2']))\n", "df['check'] = np.where(df['check'], 'OK', 'KO')\n"], "link": "https://stackoverflow.com/questions/60993781/how-do-i-check-that-the-unique-values-of-a-column-exists-in-another-column-in-da"}
{"id": 468, "q": "Pandas Concatenation not working properly", "d": "So I've been setting up a label archive on my deep learning classifier and I wanted to concatenate the labels of an already existing 2D archive into one I just made. The one that exists is 'y_trainvalid' (39209, 43), which stands for 39209 images in 43 classes. The new label archive I'm trying to add is 'new_file_label' (23, 43). On these archives, the number set to 1 if it matches the class and 0 if it doesn't. Here's a sample of both of them: When I tried to concatenate using this command: Something like this appeared: As if it doubled the amount of columns to fit the data instead of putting the new data just below it. I'm not sure why this is happening cause I'm pretty sure both label archives have the same number of columns. When I print use the 'y_trainvalid2.head().to_dict()' command, this appears: How do I solve this problem?", "q_apis": "add sample columns columns head to_dict", "io": " 0 1 2 3 4 5 6 ... 41 42 5 6 7 8 9 39204 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... NaN NaN NaN NaN NaN NaN NaN 39205 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... NaN NaN NaN NaN NaN NaN NaN 39206 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... NaN NaN NaN NaN NaN NaN NaN 39207 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... NaN NaN NaN NaN NaN NaN NaN 39208 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... NaN NaN NaN NaN NaN NaN NaN 39209 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39210 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39211 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39212 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39213 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39214 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39215 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39216 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39217 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39218 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39219 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39220 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39221 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39222 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39223 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39224 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39225 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39226 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39227 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39228 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39229 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39230 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39231 NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 <s> {0: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '0': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 1: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '1': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 10: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '10': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '11': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 12: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '12': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 13: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '13': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 14: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '14': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '15': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 16: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '16': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 17: {0: 0.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, '17': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 18: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '18': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 19: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '19': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 2: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '2': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 20: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '20': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 21: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '21': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 22: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '22': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 23: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '23': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 24: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '24': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 25: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '25': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 26: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '26': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 27: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '27': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 28: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '28': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 29: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '29': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '3': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 30: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '30': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 31: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '31': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 32: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, '32': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 33: {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, '33': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 34: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '34': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 35: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '35': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 36: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '36': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 37: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '37': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 38: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, '38': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 39: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '39': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 4: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '4': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 40: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '40': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 41: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '41': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 42: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '42': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '5': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 6: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '6': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '7': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 8: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '8': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}, 9: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}, '9': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan}} ", "apis": "columns columns columns columns concat", "code": ["y_trainvalid.columns = [str(x) for x in y_trainvalid.columns]\nnew_file_label.columns = [str(x) for x in new_file_label.columns]\ny_trainvalid2 = pd.concat([y_trainvalid, new_file_label])\n"], "link": "https://stackoverflow.com/questions/56677321/pandas-concatenation-not-working-properly"}
{"id": 240, "q": "justify data from right to left", "d": "I have a dataset of rows containing varying lengths of integer values in a series. I want to separate the series so each integer has its own column but align these values along the right-most column. I want the dataframe to resenble upper triangle of a matrix. Currently I have a dataset like: I apply this function and I get this: But what i want is this:", "q_apis": "right left values align values right apply get", "io": " 1 2 3 4 5 6 7 8 9 10 0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0 1 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 NaN 2 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 NaN NaN 3 1.0 2.0 3.0 4.0 5.0 6.0 7.0 NaN NaN NaN 4 1.0 2.0 3.0 4.0 5.0 6.0 NaN NaN NaN NaN 5 1.0 2.0 3.0 4.0 5.0 NaN NaN NaN NaN NaN 6 1.0 2.0 3.0 4.0 NaN NaN NaN NaN NaN NaN 7 1.0 2.0 3.0 NaN NaN NaN NaN NaN NaN NaN 8 1.0 2.0 NaN NaN NaN NaN NaN NaN NaN NaN 9 1.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN <s> 1 2 3 4 5 6 7 8 9 10 0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.0 1 NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 2 NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 3 NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 7.0 4 NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6.0 5 NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0 5.0 6 NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 4.0 7 NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 3.0 8 NaN NaN NaN NaN NaN NaN NaN NaN 1.0 2.0 9 NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 ", "apis": "value max DataFrame value columns", "code": ["lmax = df['value'].str.len().max()\ndf1 = pd.DataFrame([[np.nan] * (lmax - len(s)) + s\n                    for s in df['value']], columns=range(1, lmax + 1))\n"], "link": "https://stackoverflow.com/questions/63173736/justify-data-from-right-to-left"}
{"id": 461, "q": "Increase specific rows by multiplication until sum of columns fulfils criteria", "d": "I have a dataframe with 4 columns and I want to do the following steps (ideally in one code): - Filter rows where the sum of the 4 columns is lower than 0.9 - Multiply each cell in each row so that the sum of the row is 0.9 - In case there is a 0 in any cell, this cell stays unchanged (as multiplying 0 with anything remains 0) - At the end display all rows, also the ones that were not changed Here is an example dataframe: Now only rows 0 and 1 should be affected by the algorithm I had used this one which worked partly here: But I do now know how to work only with the columns A to C and how I can first filter by the rows where the sum is below 0.9 and then how to show all rows again. So my desired outcome is something like this: Important, all columns (including product column) and rows should still be there and the format be a dataframe with all of the rows. I only added the sum function below to see that they add up to 0.9 or more.", "q_apis": "sum columns columns where sum columns sum any all now columns first filter where sum all all columns product all sum add", "io": "print (df) Name A B C 0 Bread 0.0414 0.170292 0.690000 1 Butter 0.0000 0.452000 0.452000 2 Cheese 0.70 0.3330 0.0333 <s> Sum = df[\"A\"]+df[\"B\"]+df[\"C\"] print (Sum) 0 0.9 1 0.9 2 1.0663 ", "apis": "apply sum mul sum", "code": ["df2 = df.apply(lambda x : x if x.sum() > 0.9 else x.mul(0.9/x.sum()), axis=1)\n"], "link": "https://stackoverflow.com/questions/56757624/increase-specific-rows-by-multiplication-until-sum-of-columns-fulfils-criteria"}
{"id": 488, "q": "Add new columns to pandas dataframe based on other dataframe", "d": "I'm trying to set a new column (two columns in fact) in a pandas dataframe, with the data comes from other dataframe. I have the following two dataframes (they are example for this purpose, the original dataframes are so much bigger): And I want to have a new dataframe (or added to df0, whatever), as: As you can see, in the resulting dataframe isn't present the row with A=6 which is present in df1 but not in df0. Also the row with A=0 is duplicated in df1, but not in the result df2. Actually, I'm having trouble with the selection method. I can do this: But I'm not sure how to apply the part of keep with unique data (remember that df1 can contain duplicated data) and add the two columns to the df2 dataset (or add them to df0). I've search here and I don't know see how to apply something like groupby, or even map. Any idea? Thanks!", "q_apis": "columns columns duplicated apply unique duplicated add columns add apply groupby map", "io": "In [116]: df0 Out[116]: A B C 0 0 1 0 1 2 3 2 2 4 5 4 3 5 5 5 In [118]: df1 Out[118]: A D E 0 2 7 2 1 6 5 5 2 4 3 2 3 0 1 0 4 5 4 6 5 0 1 0 <s> df2: A B C D E 0 0 1 0 1 0 1 2 3 2 7 2 2 4 5 4 3 2 3 5 5 5 4 6 ", "apis": "merge", "code": ["import pandas as pd\ndf2 = pd.merge(df0,df1, left_index=True, right_index=True)\n"], "link": "https://stackoverflow.com/questions/39359272/add-new-columns-to-pandas-dataframe-based-on-other-dataframe"}
{"id": 206, "q": "Divide a pandas dataframe by the sum of its index column and row", "d": "Here is what I currently have: How can i transform this to df1 so that we divide each element in the row by the sum of the index columns? The output of df1 should look like this:", "q_apis": "sum index transform sum index columns", "io": " print(df) 10 25 26 10 530 1 46 25 1 61 61 26 46 61 330 <s> df1: 10 25 26 10 530/(530) 1/(530+61) 46/(530+330) 25 1/(61+530) 61/(61) 61/(61+330) 26 46/(330+530) 61/(330+61) 330/(330) print(df1) 10 25 26 10 1 0.0016 0.0534 25 0.0016 1 0.1560 26 0.0534 0.1560 1 ", "apis": "div", "code": ["a = np.diag(df)[None, :]\nb = np.diag(df)[:, None]\n\nc = a+b\nnp.fill_diagonal(c, np.diag(df))\n\ndf_out = df.div(c)\ndf_out\n"], "link": "https://stackoverflow.com/questions/64159489/divide-a-pandas-dataframe-by-the-sum-of-its-index-column-and-row"}
{"id": 650, "q": "check if string contains sub string from the same column in pandas dataframe", "d": "Hi I have the following dataframe: i want to check for the strings that contain sub string from this column and create a new column that holds the bigger strings if the condition is full filled something like this: Thanks in advance", "q_apis": "contains sub sub", "io": "> df1 col1 0 donald 1 mike 2 donald trump 3 trump 4 mike pence 5 pence 6 jarred <s> > df1 col1 col2 0 donald donald trump 1 mike mike pence 2 donald trump donald trump 3 trump donald trump 4 mike pence mike pence 5 pence mike pence 6 jarred jarred ", "apis": "apply max", "code": ["df['Col2'] = df['Col1'].apply(lambda x: max([i for i in df['Col1'] if x in i], key=len))\n"], "link": "https://stackoverflow.com/questions/49513688/check-if-string-contains-sub-string-from-the-same-column-in-pandas-dataframe"}
{"id": 210, "q": "DataFrames - Average Columns", "d": "I have the following dataframe in pandas I am looking to create a dataframe which contains averages of columns 1& 2, Columns 3 &4, and so on. I was using this, but it is averaging everything. Is there a way, that I can add the column headers, when averaging each row. If not, another way would be to create two arrays, average them and then create a new dataframe.", "q_apis": "contains columns add", "io": "Column 1 Column 2 Column3 Column 4 2 2 2 4 1 2 2 3 <s> ColumnAvg(12) ColumnAvg(34) 2 3 1.5 1.5 ", "apis": "groupby columns mean add_prefix iloc groupby iloc columns mean add_prefix concat iloc", "code": ["  df.groupby((np.arange(len(df.columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')\n", " dfgrp=  df.iloc[:,2:].groupby((np.arange(len(df.iloc[:,2:].columns)) // 2) + 1, axis=1).mean().add_prefix('ColumnAVg')\n dfnew = pd.concat([df.iloc[:,2:],dfgrp])\n"], "link": "https://stackoverflow.com/questions/63974864/dataframes-average-columns"}
{"id": 77, "q": "Fill a Dataframe column with list of values if condition is not satisfied based on some other column", "d": "I have a dataframe that looks like this - I also have a list of values like this I want to construct a third column which should have \"Yes\" if the colour is \"red\" in , else should have 1,3,2 on respective rows. Basically, should have values from the label one after the other if colour is \"blue\". Expected output - My Approach - I have tried to impute using like this , but the I believe this is due to the difference in the size of and (5 vs 3). Can anyone help me out, please? Thanks EDIT: Expected Output added Made some mistake in My Approach demonstration, corrected that.", "q_apis": "values values values difference size", "io": "col_1 | col_2 ------------------- \"red\" | 21 ------------------- \"blue\" | 31 ------------------- \"red\" | 12 ------------------- \"blue\" | 99 ------------------- \"blue\" | 102 <s> col_1 | col_2 | col_3 --------------------------- \"red\" | 21 | \"Yes\" ----------------------------- \"blue\" | 31 | \"1\" ------------------------------ \"red\" | 12 | \"Yes\" ------------------------------ \"blue\" | 99 | \"3\" ------------------------------ \"blue\" | 102 | \"2\" ", "apis": "ne ne loc", "code": ["df['col_3'] = 'Yes'\nm = df['col_1'].ne('Red') # ne -> not equal to\ndf.loc[m, 'col_3'] = label\n"], "link": "https://stackoverflow.com/questions/66517526/fill-a-dataframe-column-with-list-of-values-if-condition-is-not-satisfied-based"}
{"id": 333, "q": "Pandas search if full rows of a large df contain template rows from a another smaller df?", "d": "I have a large df (df1) with binary outputs in each column like so: I also have another smaller df (df2) with some \"template\" rows and I want to check if df1s rows contain. Templates looks like this: What I'm trying to do is to search the large df efficiently for these small number of templates, so in this example, rows 1, 3, 4, 6 would match, but 2 and 5 would not match. I want any row in the large df which has any extra 1s to pass the test (i.e. a template row is there but it also has some extra 1s in that row). I know that I could just have a nested loop and iterate over all the rows of the large and small dfs and match rows as np.arrays, but this seems like an extremely inefficient way to do this. I'm wondering if there are any non-iterative pd-based solutions to this problem? Thank you so much! Minor functionality edit: Along with searching and matching, I'm also trying to retain a list of which template row from df2 each row in df1 matched with so I can do statistics on how many templates show up in the large df and which ones they are. This is one of the reasons why this answer(Compare Python Pandas DataFrames for matching rows) doesn't work.", "q_apis": "any any test all any", "io": "df1: a b c d 1 1 0 1 0 2 0 0 0 0 3 0 1 0 1 4 1 1 0 0 5 1 0 0 0 6 1 0 1 1 ... <s> df2: a b c d 1 1 0 1 0 2 1 1 1 1 3 0 0 0 1 4 1 1 0 0 ", "apis": "index iterrows append all sum", "code": ["t_match =[]\nfor index, row in df2.iterrows():\n    t_match.append(((df1-row) >= 0).all(axis=1).sum())\n"], "link": "https://stackoverflow.com/questions/60644834/pandas-search-if-full-rows-of-a-large-df-contain-template-rows-from-a-another-sm"}
{"id": 285, "q": "read csv and Iterate through 10 row blocks", "d": "I am trying to read a CSV file and Iterate through 10-row blocks. The data is quite unusual, with two columns and 10-row blocks. 57485 rows x 2 columns in the format below: Every 10 rows consist of a grid reference and two records X/Y ref. The grid reference and X value is in column 1, the Y value is in column 2, and then 9 rows with 12 columns, in column one. The code below reads 10 rows, but keeps repeating the first row in all following 10-row blocks?? I don't understand why it keeps repeating the first row?? Any suggestions to resolve this would be appreciated.. The first two block:", "q_apis": "columns columns value value columns first all first first", "io": "Grid-ref= 1,148 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 3020 2820 3040 2880 1740 1360 980 990 1410 1770 2580 2630 Grid-ref= 1,311 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 490 290 280 230 200 250 440 530 460 420 530 450 Grid-ref= 1,312 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 460 280 260 220 190 240 430 520 450 400 520 410 <s> Grid-ref= 1 148 0 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 1 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 2 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 3 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 4 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 5 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 6 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 7 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 8 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN 9 3020 2820 3040 2880 1740 1360 980 990 1410 ... NaN Grid-ref= 1 148 10 Grid-ref= 1 311.0 11 490 290 280 230 200 250 440 530 460 ... NaN 12 490 290 280 230 200 250 440 530 460 ... NaN 13 490 290 280 230 200 250 440 530 460 ... NaN 14 490 290 280 230 200 250 440 530 460 ... NaN 15 490 290 280 230 200 250 440 530 460 ... NaN 16 490 290 280 230 200 250 440 530 460 ... NaN 17 490 290 280 230 200 250 440 530 460 ... NaN 18 490 290 280 230 200 250 440 530 460 ... NaN 19 490 290 280 230 200 250 440 530 460 ... NaN ", "apis": "test groups add columns append DataFrame", "code": ["import csv\nimport re\nimport pandas as  pd\n\ngrid_re = re.compile(r\"Grid-ref=\\s*(\\d+),(\\d+)\")\n\nwith open('test.csv') as fobj:\n    table = []\n    try:\n        while True:\n            # find next block\n            for line in fobj:\n                match = grid_re.search(line)\n                if match:\n                    grid_xy = list(match.groups())\n                    break\n            else:\n                raise StopIteration()\n            for _ in range(10):\n                line = next(fobj)\n                # add row plus grid columns\n                table.append(line.strip().split() + grid_xy)\n    except StopIteration:\n        pass\n\ndf = pd.DataFrame(table)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/62139372/read-csv-and-iterate-through-10-row-blocks"}
{"id": 6, "q": "Insert values from variable and DataFrame into another DataFrame", "d": "On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code", "q_apis": "values DataFrame DataFrame start map DataFrame all DataFrame value hour", "io": " id col0 col1 col2 0 1 3.0 13 23 1 1 NaN 14 24 2 1 NaN 15 25 <s> id col0 col1 col2 0 1 3.0 13 23 1 1 3.0 14 24 2 1 3.0 15 25 ", "apis": "DataFrame DataFrame insert insert", "code": ["import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, \"id\", id)\ndf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n"], "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe"}
{"id": 397, "q": "How to use row index and cell value in function applied to dataframe?", "d": "I have a table similar to this, with the blank spaces being empty strings and the numbers being floats: I want to replace the value of each cell with the output of a function which takes two arguments: the index of the row and the value of the cell. For example, the values in the first column should be replaced with the output of func(D, 2) and func(E, 0) and the empty cells should stay empty. The function output is a string. Expected output table: if func(D, 2) returns X and func(E, 0) returns Y, then column 1 should look like: How do I do this?", "q_apis": "index value empty replace value index value values first empty empty", "io": " 1 2 3 4 5 6 A B 8 5 C 5 7 D 2 3 5 E 0 <s> 1 2 3 4 5 6 A B 8 5 C 5 7 D X 3 5 E Y ", "apis": "index loc loc map", "code": ["for idx in df.index:\n   df.loc[idx] = df.loc[idx].map(lambda x: func(idx, x))\n"], "link": "https://stackoverflow.com/questions/58984851/how-to-use-row-index-and-cell-value-in-function-applied-to-dataframe"}
{"id": 508, "q": "Remove rows when the occurrence of a column value in the data frame is less than a certain number using pandas/python?", "d": "I have a data frame like this: I have seen that col1 values with B and D occurs more than one times in the data frame. I want to keep those values with occurrence more than one, the final data frame will look like: How to do this in most efficient way using pandas/python ?", "q_apis": "value values values", "io": "df col1 col2 A 1 B 1 C 2 D 3 D 2 B 1 D 5 <s> col1 col2 B 1 D 3 D 2 B 1 D 5 ", "apis": "map value_counts groupby filter", "code": ["df = df[df['col1'].map(df['col1'].value_counts()) > 1]\n", "df = df.groupby('col1').filter(lambda x: len(x) > 1)\n"], "link": "https://stackoverflow.com/questions/54884153/remove-rows-when-the-occurrence-of-a-column-value-in-the-data-frame-is-less-than"}
{"id": 635, "q": "Append list from pandas column to python list", "d": "I have values in a list in pandas column, for example: df But when I append col1 to list I got quote around the first element in each sublist. And I got: But I need:", "q_apis": "values append first", "io": "id col1 1 [51.97559, 4.12565] 2 [52.97559, 3.12565] 3 [49.97559, 5.12565] <s> [[51.97559, 4.12565] [52.97559, 3.12565] [49.97559, 5.12565]] ", "apis": "values append map", "code": ["new_list = []\nfor val in df['col1'].values:\n    new_list.append(list(map(float, val)))\n"], "link": "https://stackoverflow.com/questions/50269584/append-list-from-pandas-column-to-python-list"}
{"id": 84, "q": "update table information based on columns of another table", "d": "I am new in python have two dataframes, df1 contains information about all students with their group and score, and df2 contains updated information about few students when they change their group and score. How could I update the information in df1 based on the values of df2 (group and score)? df1 The result df: 3 my code to update df1 from df2 but I face the following error", "q_apis": "update columns contains all contains update values update", "io": " +----+----------+-----------+----------------+ | |student No| group | score | |----+----------+-----------+----------------| | 0 | 0 | 0 | 0.839626 | | 1 | 1 | 0 | 0.845435 | | 2 | 2 | 3 | 0.830778 | | 3 | 3 | 2 | 0.831565 | | 4 | 4 | 3 | 0.823569 | | 5 | 5 | 0 | 0.808109 | | 6 | 6 | 4 | 0.831645 | | 7 | 7 | 1 | 0.851048 | | 8 | 8 | 3 | 0.843209 | | 9 | 9 | 4 | 0.84902 | | 10 | 10 | 0 | 0.835143 | | 11 | 11 | 4 | 0.843228 | | 12 | 12 | 2 | 0.826949 | | 13 | 13 | 0 | 0.84196 | | 14 | 14 | 1 | 0.821634 | | 15 | 15 | 3 | 0.840702 | | 16 | 16 | 0 | 0.828994 | | 17 | 17 | 2 | 0.843043 | | 18 | 18 | 4 | 0.809093 | | 19 | 19 | 1 | 0.85426 | +----+----------+-----------+----------------+ df2 +----+-----------+----------+----------------+ | | group |student No| score | |----+-----------+----------+----------------| | 0 | 2 | 1 | 0.887435 | | 1 | 0 | 19 | 0.81214 | | 2 | 3 | 17 | 0.899041 | | 3 | 0 | 8 | 0.853333 | | 4 | 4 | 9 | 0.88512 | +----+-----------+----------+----------------+ <s> +----+----------+-----------+----------------+ | |student No| group | score | |----+----------+-----------+----------------| | 0 | 0 | 0 | 0.839626 | | 1 | 1 | 2 | 0.887435 | | 2 | 2 | 3 | 0.830778 | | 3 | 3 | 2 | 0.831565 | | 4 | 4 | 3 | 0.823569 | | 5 | 5 | 0 | 0.808109 | | 6 | 6 | 4 | 0.831645 | | 7 | 7 | 1 | 0.851048 | | 8 | 8 | 0 | 0.853333 | | 9 | 9 | 4 | 0.88512 | | 10 | 10 | 0 | 0.835143 | | 11 | 11 | 4 | 0.843228 | | 12 | 12 | 2 | 0.826949 | | 13 | 13 | 0 | 0.84196 | | 14 | 14 | 1 | 0.821634 | | 15 | 15 | 3 | 0.840702 | | 16 | 16 | 0 | 0.828994 | | 17 | 17 | 3 | 0.899041 | | 18 | 18 | 4 | 0.809093 | | 19 | 19 | 0 | 0.81214 | +----+----------+-----------+----------------+ ", "apis": "merge left fillna fillna drop", "code": ["dfupdated = df1.merge(df2, on='student No', how='left')\ndfupdated['group'] = dfupdated['group_y'].fillna(dfupdated['group_x'])\ndfupdated['score'] = dfupdated['score_y'].fillna(dfupdated['score_x'])\ndfupdated.drop(['group_x', 'group_y','score_x', 'score_y'], axis=1,inplace=True)\n"], "link": "https://stackoverflow.com/questions/66401782/update-table-information-based-on-columns-of-another-table"}
{"id": 311, "q": "Pandas: how to do value counts within groups", "d": "I have the following dataframe. I want to group by and first. Within each group, I need to do a value count based on and only pick the one with most counts. If there are more than one c values for one group with the most counts, just pick any one. The expected result would be What is the right way to do it? It would be even better if I can print out each group with c's value counts sorted as an intermediate step.", "q_apis": "value groups first value count values any right value step", "io": "a b c 1 1 x 1 1 y 1 1 y 1 2 y 1 2 y 1 2 z 2 1 z 2 1 z 2 1 a 2 1 a <s> a b c 1 1 y 1 2 y 2 1 z ", "apis": "groupby agg max max count count reset_index", "code": ["df.groupby(['a', 'b'])['c'].agg({'max': max, 'count': 'count'}).reset_index()\n"], "link": "https://stackoverflow.com/questions/61143729/pandas-how-to-do-value-counts-within-groups"}
{"id": 616, "q": "Python Pandas | Find maximum value only from a specific part of a column", "d": "I have been trying to do this. Pandas max() would find the maximum value from the entire column. What I need is: My input csv file: Output needed: I am not sure how to select/group values from Val1 column with the same Id and then find their maximum value. Also, I have some blanks in the Val1 column, rendering its datatype as object. I don't know how to go about this. Any help would be most welcome.", "q_apis": "value max value select values value", "io": "Id Param1 Param2 Val1 1 -5.00138282776 2.04990620034e-08 1.738e-05 1 -4.80147838593 2.01516989762e-08 1.628e-05 1 -4.60159301758 1.98263165885e-08 1.671e-05 1 -4.40133094788 1.94918392538e-08 1.576e-05 1 -4.20143127441 1.91767686175e-08 2 -5.00141859055 6.88369405921e-09 5.512e-06 2 -4.80152130126 6.77335965093e-09 5.964e-06 2 -4.60163593292 6.65415056389e-09 3 -5.00138044357 1.16316911658e-08 4.008e-06 3 -4.80148792267 1.15515588206e-08 7.347e-06 3 -4.60160970681 1.14048361866e-08 8.446e-06 3 -4.40137386322 1.12357021465e-08 <s> Id Param1 Param2 Val1 Max_Val1_for_each_Id 1 -5.00138282776 2.04990620034e-08 1.738e-05 1.738e-05 1 -4.80147838593 2.01516989762e-08 1.628e-05 1 -4.60159301758 1.98263165885e-08 1.671e-05 1 -4.40133094788 1.94918392538e-08 1.576e-05 1 -4.20143127441 1.91767686175e-08 2 -5.00141859055 6.88369405921e-09 5.512e-06 5.964e-06 2 -4.80152130126 6.77335965093e-09 5.964e-06 2 -4.60163593292 6.65415056389e-09 3 -5.00138044357 1.16316911658e-08 4.008e-06 8.446e-06 3 -4.80148792267 1.15515588206e-08 7.347e-06 3 -4.60160970681 1.14048361866e-08 8.446e-06 3 -4.40137386322 1.12357021465e-08 ", "apis": "to_numeric groupby transform max where duplicated", "code": ["df['Val1'] = pd.to_numeric(df['Val1'], errors='coerce')\ndf['Max_Val1_for_each_Id'] = df.groupby('Id')['Val1'].transform('max')\ndf['Max_Val1_for_each_Id'] = df['Max_Val1_for_each_Id'].where(~df['Id'].duplicated())\n"], "link": "https://stackoverflow.com/questions/51054844/python-pandas-find-maximum-value-only-from-a-specific-part-of-a-column"}
{"id": 562, "q": "Pandas Dataframe interpreting columns as float instead of String", "d": "I want to import a csv file into a pandas dataframe. There is a column with IDs, which consist of only numbers, but not every row has an ID. I want to read this column as String, but even if I specifiy it with I get Is there an easy way get the ID as a string without decimal like without having to edit the Strings after importing the table?", "q_apis": "columns get get", "io": " ID xyz 0 12345 4.56 1 45.60 2 54231 987.00 <s> ID xyz 0 '12345.0' 4.56 1 NaN 45.60 2 '54231.0' 987.00 ", "apis": "read_csv astype astype apply isnull", "code": ["df = pd.read_csv(filename)\ndf['ID'] = df['ID'].astype(int).astype(str)\n", "df['ID'] = df['ID'].apply(lambda x: x if pd.isnull(x) else str(int(x)))\n"], "link": "https://stackoverflow.com/questions/53280650/pandas-dataframe-interpreting-columns-as-float-instead-of-string"}
{"id": 196, "q": "Plotting a data frame of error bars onto a data frame in matplotlib Python", "d": "I've got a pandas data frame () of values as follows: I also have a data frame () with the error of each of those values: I have successfully been able to plot with matplotlib as I desired: However, I am struggling to get the error bars onto this graph. My code for plotting is currently as follows: As you can see, I am trying to pass the data frame into the flag, but it does not do anything, and I am getting the error: I have had a look online but it seems not many people are trying to add so many error bars like I am trying to. What do I need to change to allow this to work?", "q_apis": "values values plot get add", "io": " 0 1 2 0 100.000000 100.000000 100.000000 1 0.412497 0.668880 136.019498 2 5.144450 77.323610 163.496773 3 31.078457 78.151325 146.772621 <s> 0 1 2 0 0.083579 0.048520 0.082328 1 0.005855 0.005904 0.046494 2 0.009907 0.080799 0.083671 3 0.045831 0.075932 0.044581 ", "apis": "T plot values", "code": ["ax = df.T.plot(kind='bar', yerr=list(deviation.values*100) color=['C0', 'C3', 'C1', 'C2'])\n"], "link": "https://stackoverflow.com/questions/64326826/plotting-a-data-frame-of-error-bars-onto-a-data-frame-in-matplotlib-python"}
{"id": 41, "q": "splitting list in dataframe columns to separate columns", "d": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success", "q_apis": "columns columns apply Series values", "io": " col1 col2 col3 0 [1, a] [1, a1] [1, a2] 1 [2, b] [2, b1] [2, b2] 2 [3, c] [3, c1] [3, c2] <s> col1 col2 col3 col4 0 a a1 a2 1 1 b b1 b2 2 2 c c1 c2 3 ", "apis": "applymap assign map", "code": ["df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"], "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns"}
{"id": 87, "q": "sum of row in the same columns in pandas", "d": "i have a dataframe something like this how do i get the sum of values between the same column in a new column in a dataframe for example: i want a new column with the sum of d1[i] + d1[i+1] .i know .sum() in pandas but i cant do sum between the same column", "q_apis": "sum columns get sum values between sum sum sum between", "io": " d1 d2 d3 d4 780 37.0 21.4 122840.0 784 38.1 21.4 122860.0 846 38.1 21.4 122880.0 843 38.0 21.5 122900.0 820 36.3 22.9 133220.0 819 36.3 22.9 133240.0 819 36.4 22.9 133260.0 820 36.3 22.9 133280.0 822 36.4 22.9 133300.0 <s> d1 d2 d3 d4 d5 780 37.0 21.4 122840.0 1564 784 38.1 21.4 122860.0 1630 846 38.1 21.4 122880.0 1689 ", "apis": "shift", "code": ["df['d5'] = df['d1'] + df['d1'].shift(-1)\n"], "link": "https://stackoverflow.com/questions/66371637/sum-of-row-in-the-same-columns-in-pandas"}
{"id": 103, "q": "Use a categorical column to order the dataframe according to an array", "d": "I have an array like this: I also have a dataframe like this: I want to use to order the column dataframe according to the array. The expected output is:", "q_apis": "array array array", "io": "BIN CA SUM 100 B B 100 300 A A 300 300 B B 300 400 B B 400 400 A A 400 200 B B 200 100 A A 100 200 A A 200 <s> BIN CA SUM 100 A A 100 200 A A 200 300 A A 300 400 A A 400 100 B B 100 200 B B 200 300 B B 300 400 B B 400 ", "apis": "Categorical categories ordered sort_values iloc argsort map", "code": ["df['SUM'] = pd.Categorical(df['SUM'], categories=arr, ordered=True)\ndf.sort_values('SUM')\n", "dct = {v: i for i, v in enumerate(arr)}\ndf.iloc[np.argsort(df['SUM'].map(dct))]\n"], "link": "https://stackoverflow.com/questions/66088290/use-a-categorical-column-to-order-the-dataframe-according-to-an-array"}
{"id": 74, "q": "Can I shift specific values in one data column to another column while keeping the other values unchanged?", "d": "Here is an example dataset that I have: I want to take all the values that have \"1\" in them in the Column \"C2\" and shift them to replace the adjacent values in column \"C1\". So the output should look like: Alternatively, I could create a new column with these values replaced. Main point is, that I need all the \"1s\" in C2 TO replace the NaN values in C1. I can't do find all NaN and replace with 1, because there are some NaN values that should stay in C1. Is there a way to do this? Thanks for the help in advance.", "q_apis": "shift values values take all values shift replace values values all replace values all replace values", "io": "C1 C2 1 1 NaN 1 2 0 NaN 0 NaN 1 1 1 2 2 2 2 NaN 1 <s> C1 C2 1 1 1 1 2 0 NaN 0 1 1 1 1 2 2 2 2 1 1 ", "apis": "value value mask", "code": ["value = 1\nsource_col = 1\ntarget_col = 0\n\ncondition = df[source_col] == value\n\ndf[target_col] = df[target_col].mask(condition,\n                                     df[source_col])\n"], "link": "https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t"}
{"id": 189, "q": "Convert dictionary of dictionaries to dataframe with data types", "d": "What is the preferred way to convert dictionary of dictionaries into a data frame with data types? I have the following kind of dictionary which contains fact sets behind each key Converting this dictionary of dictionaries into a dataframe can be done in a quite straightforward way which yields the following version on the original dictionary of dictionaries and the following datatypes for columns However, I would like to have transposed version on . After doing so it seems like the expected representation on the data is shown in matrix form but the data types are all What is the preferred way to do such conversion from to so that would yield directly data types similar to converting to ?", "q_apis": "contains columns all", "io": " 1 2 3 a 1 NaN NaN b 2 1 NaN c b e NaN d NaN 1 NaN e NaN NaN 0.0 <s> a b c d e 1 1 2 b NaN NaN 2 NaN 1 e 1 NaN 3 NaN NaN NaN NaN 0 ", "apis": "dtype", "code": ["a    float64\nb    float64\nc     object\nd    float64\ne    float64\ndtype: object\n"], "link": "https://stackoverflow.com/questions/64449526/convert-dictionary-of-dictionaries-to-dataframe-with-data-types"}
{"id": 599, "q": "Remove decimals in Pandas column names", "d": "I have a dataframe with as the column names. I'd like to change them to be strings and remove the decimal places: I've tried saving the columns out to a list with and changing them there but I've had no luck. Any help would be greatly appreciated!", "q_apis": "names names columns", "io": "df = 2006.0 2007.0 2008.0 2009.0 0 foo foo bar bar 1 foo foo bar bar <s> df = 2006 2007 2008 2009 0 foo foo bar bar 1 foo foo bar bar ", "apis": "columns map Index dtype columns map Int64Index dtype", "code": ["In [338]: df.columns.map('{:g}'.format)\nOut[338]: Index(['2006', '2007', '2008', '2009'], dtype='object')\n\nIn [319]: df.columns.map(int)\nOut[319]: Int64Index([2006, 2007, 2008, 2009], dtype='int64')\n"], "link": "https://stackoverflow.com/questions/51784855/remove-decimals-in-pandas-column-names"}
{"id": 269, "q": "Replace specific values in multiindex dataframe", "d": "I have a multindex dataframe with 3 index levels and 2 numerical columns. I want to replace the values in first row of 3rd index level wherever a new second level index begins. For ex: every first row The dataframe is too big and doing it datframe by dataframe like gets time consuming. Is there some way where i can get a mask and replace with new values in these positions ?", "q_apis": "values index levels columns replace values first index second index first time where get mask replace values", "io": "A 1 2017-04-01 14.0 87.346878 2017-06-01 4.0 87.347504 2 2014-08-01 1.0 123.110001 2015-01-01 4.0 209.612503 B 3 2014-07-01 1.0 68.540001 2014-12-01 1.0 64.370003 4 2015-01-01 3.0 75.000000 <s> (A,1,2017-04-01)->0.0 0.0 (A,2,2014-08-01)->0.0 0.0 (B,3,2014-07-01)->0.0 0.0 (B,4,2015-01-01)->0.0 0.0 ", "apis": "reset_index groupby first MultiIndex from_arrays reset_index to_numpy T loc", "code": ["idx = df.reset_index(level=2).groupby(level=[0, 1])['level_2'].first()\nidx = pd.MultiIndex.from_arrays(idx.reset_index().to_numpy().T)\ndf.loc[idx, :] = 0\n"], "link": "https://stackoverflow.com/questions/62697810/replace-specific-values-in-multiindex-dataframe"}
{"id": 175, "q": "How to merge multiple columns containing numeric data in Pandas, but ignore empty cells", "d": "I have a table like this: where each column in the desired range has only one integer in its row. I want to merge these columns into a single new column that would look like this: I have been searching, but the closest solution I can find is doing something like: However, this also concatenates \"NaN\"s from the blank cells, which is obviously undesirable. How might I get my desired output?", "q_apis": "merge columns empty where merge columns get", "io": "|-----|-----|-----| | A | B | C | |-----|-----|-----| | | 5 | | |-----|-----|-----| | 1 | | | |-----|-----|-----| | | 5 | | |-----|-----|-----| | | | 2 | |-----|-----|-----| | | | 2 | |-----|-----|-----| <s> |-----|-----|-----| |-----| | A | B | C | | Z | |-----|-----|-----| |-----| | | 5 | | \u2192 | 5 | |-----|-----|-----| |-----| | 1 | | | \u2192 | 1 | |-----|-----|-----| |-----| | | 5 | | \u2192 | 5 | |-----|-----|-----| |-----| | | | 2 | \u2192 | 2 | |-----|-----|-----| |-----| | | | 2 | \u2192 | 2 | |-----|-----|-----| |-----| ", "apis": "max", "code": ["df['Z'] = df.max(axis = 1)\n"], "link": "https://stackoverflow.com/questions/64657045/how-to-merge-multiple-columns-containing-numeric-data-in-pandas-but-ignore-empt"}
{"id": 246, "q": "How can I sort dataframe based on a complicated string column?", "d": "I'm needing to sort a dataframe based on a string column, which is composed of a variety of letters, numbers, dashes, and string lengths. I'm not even sure sorting is the right method of what I want to do. Example below: df Desired DF: The order/sorting of either column does not matter, I just want the dataframe reordered and 'grouped' on column2. Grouped might not be the right expression here either cause I don't want to perform any sort of aggregated calculation. Any ideas? Thanks!", "q_apis": "right right any", "io": "Col1 Col2 A 80NX-265-DF23 B D-87-B-003 C 80NX-265-DF23 D 0333-DD-02 E D-87-B-003 F 80NX-265-DF23 <s> Col1 Col2 A 80NX-265-DF23 C 80NX-265-DF23 F 80NX-265-DF23 D 0333-DD-02 B D-87-B-003 E D-87-B-003 ", "apis": "concat unique", "code": ["pd.concat([df[df['Col2']==x] for x in df['Col2'].unique()])\n"], "link": "https://stackoverflow.com/questions/62996260/how-can-i-sort-dataframe-based-on-a-complicated-string-column"}
{"id": 262, "q": "Keep a single element in dataframe of lists", "d": "Given the following dataframe: How can I remove all but the first element in each column and then unlist so the dataframe becomes like this:", "q_apis": "all first", "io": " Movement Distance Speed Delay Loss 0 [1, 1] [1, 1] [25, 25] [0, 0] [0, 0] 1 [1, 1] [1, 1] [25, 25] [0, 0] [0, 0] 2 [1, 1] [1, 1] [25, 25] [0, 0] [0, 0] 3 [1, 1] [1, 1] [25, 25] [0, 0] [0, 0] 4 [1, 1] [1, 1] [25, 25] [0, 0] [0, 0] <s> Movement Distance Speed Delay Loss 0 1 1 25 0 0 1 1 1 25 0 0 2 1 1 25 0 0 3 1 1 25 0 0 4 1 1 25 0 0 ", "apis": "apply to_numeric", "code": ["df.apply(lambda x: pd.to_numeric(x.str[0], downcast='integer', errors='ignore'))\n"], "link": "https://stackoverflow.com/questions/62875221/keep-a-single-element-in-dataframe-of-lists"}
{"id": 91, "q": "Python - How to combine the rows into a single row in Pandas? (not group by)", "d": "I am working with a weird dataframe using Pandas: I want to combine the three rows into 1 row and the expected output is: I really don't know how to do this and appreciate your help! Thank you.", "q_apis": "combine combine", "io": "print(df) Active Dead Hold Product1 n/a n/a n/a Product2 n/a n/a n/a Product3 <s> Active Dead Hold Product1 Product2 Product3 ", "apis": "apply dropna values", "code": ["new = df.apply(lambda x: x.dropna().values)\n"], "link": "https://stackoverflow.com/questions/66266438/python-how-to-combine-the-rows-into-a-single-row-in-pandas-not-group-by"}
{"id": 21, "q": "How do I check for conflict between columns in a pandas dataframe?", "d": "I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to.", "q_apis": "between columns contains values item index between values index empty filter values index get value Series empty bool item any all", "io": " Item Local A Local B Local C 0 Item1 NaN 6.0 5 1 Item2 6.0 7.0 5 2 Item3 NaN NaN 5 3 Item4 5.0 5.0 5 4 Item5 5.0 NaN 5 <s> Item Local A Local B Local C Conflict 0 Item1 NaN 6.0 5 yes 1 Item2 6.0 7.0 5 yes 2 Item3 NaN NaN 5 NaN 3 Item4 5.0 5.0 5 NaN 4 Item5 5.0 NaN 5 NaN ", "apis": "where iloc nunique", "code": ["df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n"], "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe"}
{"id": 17, "q": "Merging more than two columns of the same dataframe in pandas", "d": "Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values?", "q_apis": "columns get get values columns all values", "io": "VAR 1 VAR 2 VAR 3 GROUP 3 [0-10] 1 3 [0-10] 1 3 [0-10] 1 2 [0-10] 2 [0-10] 3 3 [10-20] 3 1 [10-20] 1 [10-20] 2 [10-20] 2 [10-20] 2 [10-20] <s> VAR_MERGED GROUP 1 [0-10] 1 [0-10] 1 [0-10] 2 [0-10] 2 [0-10] 3 [0-10] 3 [0-10] 3 [0-10] 1 [10-20] 1 [10-20] 2 [10-20] 2 [10-20] 2 [10-20] 3 [10-20] 3 [10-20] 3 [10-20] ", "apis": "columns columns melt dropna sort_values reset_index drop", "code": ["id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n"], "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas"}
{"id": 499, "q": "How to shift pandas column elements for given index based on condition?", "d": "I have recently started using python and pandas, please bear with me on this. I have two columns (A, B) of data (dataframe) that should be arranged in particular sequence based on certain relation between two columns (let's say elements of column A should be smaller than elements column B for a given index), if relation is not satisfied data should shifted (only for A) by a row starting from the index where condition is not satisfied throughout the length of a column. And it should be replaced by NaN where is condition is not met. I have tried shift(1) function. This works only if the first element doesn't meet the condition but if there is any other element or multiple elements don't meet criteria it creates multiple NaNs at the start of column A instead of at the place where criteria is not met. Actual result Expected result", "q_apis": "shift index columns between columns index index where length where shift first any at start at where", "io": "A B NaN 2 NaN 4 3.0 6 5.0 7 8.0 9 10.0 11 <s> A B NaN 2 3.0 4 5.0 6 NaN 7 8.0 9 10.0 11 ", "apis": "loc shift", "code": ["for xt in range (0,len(mdf1)):\nif mdf1.A[xt]>mdf1.B[xt]:\n    mdf1.loc[xt:,'A'] = mdf1[xt:]['A'].shift(1)\n"], "link": "https://stackoverflow.com/questions/55420231/how-to-shift-pandas-column-elements-for-given-index-based-on-condition"}
{"id": 401, "q": "Drop values in specific condition in pandas dataframe", "d": "I have a dataframe like below: now If Column A > 7, I want to drop Column B and C like below: How can I achieve that?", "q_apis": "values now drop", "io": "A B C 4.43 NaN 1.11 3.70 0.48 0.79 2.78 -0.29 1.26 1.78 2.90 1.13 40.70 -0.03 0.55 51.75 0.29 1.45 3.65 1.74 0.37 2.93 1.56 1.64 3.43 NaN NaN 2.93 NaN NaN 10.37 NaN NaN <s> A B C 4.43 NaN 1.11 3.70 0.48 0.79 2.78 -0.29 1.26 1.78 2.90 1.13 40.70 NaN NaN 51.75 NaN NaN 3.65 1.74 0.37 2.93 1.56 1.64 3.43 NaN NaN 2.93 NaN NaN 10.37 NaN NaN ", "apis": "mask loc", "code": ["df[['B','C']] = df[['B','C']].mask(df.A > 7)\n", "df.loc[df.A > 7, ['B','C']] = np.nan\n"], "link": "https://stackoverflow.com/questions/58820383/drop-values-in-specific-condition-in-pandas-dataframe"}
{"id": 391, "q": "Create a new pandas dataframe column based on other column of the dataframe", "d": "I have a Dataframe that consists in 2 columns: 'String' -> numpy array like [47, 0, 49, 12, 46] 'Is Isogram' -> 1 or 0 I would like to create another column, with the value 'Is Isogram' appended in the 'String' array, something like this: I've tried using the apply function with a lambda: But it throws me a KeyError that i don't really understand How can i takle this problem?", "q_apis": "columns array value array apply", "io": " String Is Isogram 0 [47, 0, 49, 12, 46] 1 1 [43, 50, 22, 1, 13] 1 2 [10, 1, 24, 22, 16] 1 3 [2, 24, 3, 24, 51] 0 4 [40, 1, 41, 18, 3] 1 <s> String Is Isogram IsoString 0 [47, 0, 49, 12, 46] 1 [47, 0, 49, 12, 46, 1] 1 [43, 50, 22, 1, 13] 1 [43, 50, 22, 1, 13, 1] 2 [10, 1, 24, 22, 16] 1 [10, 1, 24, 22, 16, 1] 3 [2, 24, 3, 24, 51] 0 [2, 24, 3, 24, 51, 0] 4 [40, 1, 41, 18, 3] 1 [40, 1, 41, 18, 3, 1] ", "apis": "apply append", "code": ["df['IsoString'] = df.apply(lambda x: np.append(x['String'], x['Is Isogram']), axis=1)\n"], "link": "https://stackoverflow.com/questions/59171537/create-a-new-pandas-dataframe-column-based-on-other-column-of-the-dataframe"}
{"id": 497, "q": "how to convert one column in dataframe into a 2D array in python", "d": "I have an dataframe which contain the observed data as: how can I get an array from the value to form a 2D with shape:(3,3) I try but it gives me which is not what I want", "q_apis": "array get array value shape", "io": " [[1, 2, 1], [5, 4, 6], [7, 20, 9]] <s> [list([1, 2, 1]) list([5, 4, 6]) list([7, 20, 9])] ", "apis": "DataFrame values DataFrame to_numpy array", "code": ["# pd.DataFrame(df['Value'].tolist()).values   #  < v0.24\npd.DataFrame(df['Value'].tolist()).to_numpy() #  v0.24+\n\narray([[ 1.,  2., nan],\n       [ 3., nan, nan],\n       [ 4.,  5.,  6.]])\n"], "link": "https://stackoverflow.com/questions/55661343/how-to-convert-one-column-in-dataframe-into-a-2d-array-in-python"}
