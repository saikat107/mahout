{"id": 629, "q": "Python - Cbind previous and next row to current row", "d": "I have a Pandas data frame like so: Which looks like: I'd like to bind the previous row and the next next row to each column like so (accounting for \"doc\" and \"sent\" column in my example, which count as indices that nothing can come before or after as seen below):", "q_apis": "count indices", "io": " doc sent col1 col2 col3 0 0 0 5 4 8 1 0 1 6 3 2 2 0 2 1 2 9 3 1 0 6 1 6 4 1 1 5 1 5 <s> doc sent col1 col2 col3 p_col1 p_col2 p_col3 n_col1 n_col2 n_col3 0 0 0 5 4 8 0 0 0 6 3 2 1 0 1 6 3 2 5 4 8 1 2 9 2 0 2 1 2 9 6 3 2 6 1 6 3 1 0 6 1 6 0 0 0 5 1 5 4 1 1 5 1 5 6 1 6 0 0 0 ", "apis": "groupby concat shift add_prefix shift add_prefix fillna astype", "code": ["cs = ['col1', 'col2', 'col3']\ng = df.groupby('doc')\n\npd.concat([\n   df, \n   g[cs].shift(-1).add_prefix('n'), \n   g[cs].shift().add_prefix('p')\n], axis=1).fillna(0).astype(int)\n"], "link": "https://stackoverflow.com/questions/50593948/python-cbind-previous-and-next-row-to-current-row"}
{"id": 20, "q": "Pandas replacing values in a column by values in another column", "d": "Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:", "q_apis": "values values unique unique all where columns unique get", "io": " ppid col2 ... 1 'id1' '1' 2 'id2' '2' 3 'id3' '3' ... <s> ppid val 1 'id1' '5' 2 'id2' '6' ", "apis": "set_index set_index update reset_index", "code": ["df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n"], "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column"}
{"id": 396, "q": "Adding rows that have the same column value in a pandas dataframe", "d": "I have a pandas dataframe with dates and hours as columns. Now I want to add the hours of the same dates. For example to make this: Into this: Is there a quick way to do this on big files?", "q_apis": "value columns add", "io": "7-1-2016 | 4 7-1-2016 | 2 4-1-2016 | 5 <s> 7-1-2016 | 6 4-1-2016 | 5 ", "apis": "DataFrame groupby squeeze", "code": ["DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False)\n"], "link": "https://stackoverflow.com/questions/34657183/adding-rows-that-have-the-same-column-value-in-a-pandas-dataframe"}
{"id": 293, "q": "Add column in dataframe from make list", "d": "a*.csv b*.csv example I edited the content and then restored it because the answer didn't solve it and I solved it myself I'll end the question after commenting it solution but I chose one answer to close this question", "q_apis": "", "io": " D A B E F park KOREA 1 SUM1 hello1 michel France 3 SUM3 hello3 park2 USA 4 SUM4 hello4 <s> A B C KOREA 1 2020 KOREA 2 177 France 3 2020 USA 4 43 SPAIN 7 67 ", "apis": "append append concat concat set_index DataFrame merge reset_index", "code": ["try_a = glob.glob('a*.csv')\ntry_b = glob.glob('b*.csv')\nlst_a = []\nlst_b = []\nfor (i,j) in zip(try_a,try_b):\n lst_a.append(i)\n lst_b.append(j)\ndf_a = pd.concat(lst_a)\ndf_b = pd.concat(lst_b)\ndf_a.set_index('d', inplace= True)\ndf = pd.DataFrame.merge(df_a,df_b, how = 'inner', left_index = True).reset_index()\n"], "link": "https://stackoverflow.com/questions/61927861/add-column-in-dataframe-from-make-list"}
{"id": 160, "q": "mutate several columns from a dataframe based on a mapping between values from another dataframe", "d": "I have 2 dataframes: looks like this while looks like this what i want to obtain is the following so basically using the map between number and colours in rules to mutate df", "q_apis": "columns between values map between", "io": "id v1 v2 v3 v4 etc. 1 1 4 2 5 2 4 4 6 1 3 2 1 3 4 etc. <s> id v1 v2 v3 v4 etc. 1 red green blue black 2 green green gold red 3 blue red grey green etc. ", "apis": "applymap loc name", "code": ["results = df.applymap(lambda i: rules.loc[i, 'name'])\n"], "link": "https://stackoverflow.com/questions/64880268/mutate-several-columns-from-a-dataframe-based-on-a-mapping-between-values-from-a"}
{"id": 62, "q": "How to divide second column by first column in dataframe?", "d": "I've this triangle dataframe(df1) , i wanted to calculate new dataframe(df2) that contains the result:second_column(df2)/first_column(df2) and third_column(df2)/second_column(df2) and so on.. i tried like this(i know its wrong). and i wanted df2 like this: Thank You for your time..", "q_apis": "second first contains time", "io": " DP 1 DP 2 DP 3 DP 4 DP 5 DP 6 DP 7 DP 8 DP 9 DP 10 3,57,848 11,24,788 17,35,330 22,18,270 27,45,596 33,19,994 34,66,336 36,06,286 38,33,515 39,01,463 3,52,118 12,36,139 21,70,033 33,53,322 37,99,067 41,20,063 46,47,867 49,14,039 53,39,085 2,90,507 12,92,306 22,18,525 32,35,179 39,85,995 41,32,918 46,28,910 49,09,315 3,10,608 14,18,858 21,95,047 37,57,447 40,29,929 43,81,982 45,88,268 4,43,160 11,36,350 21,28,333 28,97,821 34,02,672 38,73,311 3,96,132 13,33,217 21,80,715 29,85,752 36,91,712 4,40,832 12,88,463 24,19,861 34,83,130 3,59,480 14,21,128 28,64,498 3,76,686 13,63,294 3,44,014 <s> DP 1 DP 2 DP 3 DP 4 DP 5 DP 6 DP 7 DP 8 DP 9 DP 10 3.14 1.54 1.28 1.24 1.21 1.04 1.04 1.06 1.02 - 3.51 1.76 1.55 1.13 1.08 1.13 1.06 1.09 - 4.45 1.72 1.46 1.23 1.04 1.12 1.06 - 4.57 1.55 1.71 1.07 1.09 1.05 - 2.56 1.87 1.36 1.17 1.14 - 3.37 1.64 1.37 1.24 - 2.92 1.88 1.44 - 3.95 2.02 - 3.62 - ", "apis": "replace astype shift div", "code": ["df2 = df1.replace(',', '', regex = True).astype(float)\n", "df2 = df2.shift(-1, axis = 1).div(df2)\n"], "link": "https://stackoverflow.com/questions/66939075/how-to-divide-second-column-by-first-column-in-dataframe"}
{"id": 256, "q": "Extract data from certain columns and generate new rows", "d": "I have a pandas dataframe The output I want will be: Is there any convenient way I can use?", "q_apis": "columns any", "io": "COL1 COL2 COL3 COL4 A B C [{COL5: D1, COL6: E1, COL7: F1}, {COL5: D2, COL6: E2, COL7: F2}, {COL5: D3, COL6: E3, COL7: F3}, ... {COL5: D10, COL6: E10, COL7: F10}] <s> COL1 COL2 COL3 COL5 COL6 COL7 A B C D1 E1 F1 A B C D2 E2 F2 A B C D3 E3 F3 ... A B C D10 E10 F10 ", "apis": "explode reset_index drop iloc join DataFrame", "code": ["u = df.explode('COL4').reset_index(drop=True)\nu.iloc[:, :-1].join(pd.DataFrame(u['COL4'].tolist()))\n"], "link": "https://stackoverflow.com/questions/62923547/extract-data-from-certain-columns-and-generate-new-rows"}
{"id": 414, "q": "Pandas Adding Row with All Values Zero", "d": "If I have a following dataframe: How can i add a row end of the dataframe with all values \"0 (Zero)\"? Desired Output is; Could you please help me about this?", "q_apis": "add all values", "io": " A B C D E 1 1 2 0 1 0 2 0 0 0 1 -1 3 1 1 3 -5 2 4 -3 4 2 6 0 5 2 4 1 9 -1 6 1 2 2 4 1 <s> A B C D E 1 1 2 0 1 0 2 0 0 0 1 -1 3 1 1 3 -5 2 4 -3 4 2 6 0 5 2 4 1 9 -1 6 1 2 2 4 1 7 0 0 0 0 0 ", "apis": "append Series index columns", "code": ["df = df.append(pd.Series(0, index=df.columns), ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/58434018/pandas-adding-row-with-all-values-zero"}
{"id": 515, "q": "Convert the last non-zero value to 0 for each row in a pandas DataFrame", "d": "I'm trying to modify my data frame in a way that the last variable of a label encoded feature is converted to 0. For example, I have this data frame, top row being the labels and the first column as the index: Columns 1-10 are the ones that have been encoded. What I want to convert this data frame to, without changing anything else is: So the last values occurring in each row should be converted to 0. I was thinking of using the last_valid_index method, but that would take in the other remaining columns and change that as well, which I don't want. Any help is appreciated", "q_apis": "last value DataFrame last first index last values last_valid_index take columns", "io": "df 1 2 3 4 5 6 7 8 9 10 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 <s> 1 2 3 4 5 6 7 8 9 10 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ", "apis": "iloc columns iloc cumsum le fillna", "code": ["u = df.iloc[:, :10]\ndf[u.columns] = u[~u.iloc[:, ::-1].cumsum(1).le(1)].fillna(0, downcast='infer')\n"], "link": "https://stackoverflow.com/questions/54737891/convert-the-last-non-zero-value-to-0-for-each-row-in-a-pandas-dataframe"}
{"id": 573, "q": "Calculating grid values given the distance in python", "d": "I have a cell grid of big dimensions. Each cell has an ID (), cell value () and coordinates in actual measures (, ). This is how first 10 rows/cells look like Neighbouring cells of cell in the can be determined as (, , , , , ). For example: of 5 has neighbours - 4,6,504,505,506. (these are the ID of rows in the upper table - ). What I am trying to is: For the chosen value/row in , I would like to know all neighbours in the chosen distance from and sum all their values. I tried to apply this solution (link), but I don't know how to incorporate the distance parameter. The cell value can be taken with , but the steps before this are a bit tricky for me. Can you give me any advice? EDIT: Using the solution from Thomas and having df called : I'd like to add another column and use the values from columns But it doesn't work. If I add a number instead of a reference to row it works like charm. But how can I use values from p3 column automatically in function? SOLVED: It worked with:", "q_apis": "values value first value all sum all values apply value any add values columns add values", "io": " p1 p2 p3 X Y 0 0 0.0 0.0 0 0 1 1 0.0 0.0 100 0 2 2 0.0 12.0 200 0 3 3 0.0 0.0 300 0 4 4 0.0 70.0 400 0 5 5 0.0 40.0 500 0 6 6 0.0 20.0 600 0 7 7 0.0 0.0 700 0 8 8 0.0 0.0 800 0 9 9 0.0 0.0 900 0 <s> p3 0 45 1 580 2 12000 3 12531 4 22456 ", "apis": "DataFrame filter filter iloc sum sum", "code": ["import numpy as np\nimport pandas\n\n# Generating toy data\nN = 10\ndata = pandas.DataFrame({'p3': np.random.randn(N)})\nprint(data)\n\n# Finding neighbours\nget_candidates = lambda i: [i-500+1, i-500-1, i-1, i+1, i+500+1, i+500-1]\nfilter = lambda neighbors, N: [n for n in neighbors if 0<=n<N]\nget_neighbors = lambda i, N: filter(get_candidates(i), N)\n\nprint(\"Neighbors of 5: {}\".format(get_neighbors(5, len(data))))\n\n# Summing p3 on neighbors\ndef sum_neighbors(data, i, col='p3'):\n  return data.iloc[get_neighbors(i, len(data))][col].sum()\n\nprint(\"p3 sum on neighbors of 5: {}\".format(sum_neighbors(data, 5)))\n"], "link": "https://stackoverflow.com/questions/52873547/calculating-grid-values-given-the-distance-in-python"}
{"id": 340, "q": "Pandas - Groupby or Cut multi dataframes to bins", "d": "I'm having a dataframe with starting axis points and their end points like this I'm drawing a heatmap. I need each \"zone\" of that map has a line which shows the average distance and angle of lines which have x/y from that zone and their x_end/y_end. It looks like this My bins is I've already plotted a heatmap I'm looking for something like this", "q_apis": "map", "io": "x y x_end y_end distance 14.14 30.450 31.71 41.265 20.631750 -27.02 55.650 -33.60 63.000 9.865034 -19.25 70.665 -28.98 80.115 13.563753 16.45 59.115 9.94 41.895 18.409468 <s> xbins = np.linspace(-35, 35, 11) ybins = np.linspace(0, 105, 22) ", "apis": "drop groupby cut cut mean dropna all add_prefix", "code": ["(df.drop(['x','y'], axis=1)\n  .groupby([pd.cut(df.x, xbins),\n            pd.cut(df.y, ybins)],\n          )\n   .mean()\n   .dropna(how='all')\n   .add_prefix('Average_')\n)\n"], "link": "https://stackoverflow.com/questions/60492857/pandas-groupby-or-cut-multi-dataframes-to-bins"}
{"id": 143, "q": "Efficiently relocate elements conditionally in a panda.Dataframe", "d": "I am trying to sort the values of my data.frame in the following way: It is working, however it is very slow for my +40k rows. How can I do this more efficiently and more elegantly? I would prefer a solution that directly manipulates the original df, if possible. Example data: Desired output:", "q_apis": "values", "io": "x1 p1 x2 p2 1 0.4 2 0.6 2 0.2 1 0.8 <s> x1 p1 x2 p2 1 0.4 2 0.6 1 0.8 2 0.2 ", "apis": "loc loc values", "code": ["check = df[\"x1\"] > df[\"x2\"]\ndf.loc[check, [\"x2\", \"x1\", \"p2\", \"p1\"]] = df.loc[check, [\"x1\", \"x2\", \"p1\", \"p2\"]].values\n"], "link": "https://stackoverflow.com/questions/65222196/efficiently-relocate-elements-conditionally-in-a-panda-dataframe"}
{"id": 317, "q": "Issue in applying str.contains across multiple columns in Python", "d": "Dataframe: Desired output: What I have tried: (this doesn't delete all rows with alpha-numeric vales) I want to delete all alphanumeric rows, and have only the rows containing numbers alone. Col1 and Col2 has decimal points, but Col3 has only whole numbers. I have tried few other similar threads, but it didn't work. Thanks for the help!!", "q_apis": "contains columns delete all delete all", "io": "col1 col2 col3 132jh.2ad3 34.2 65 298.487 9879.87 1kjh8kjn0 98.47 79.8 90 8763.3 7hkj7kjb.k23l 67 69.3 3765.9 3510 <s> col1 col2 col3 98.47 79.8 90 69.3 3765.9 3510 ", "apis": "apply contains flags any loc apply contains flags any", "code": ["df[~df.apply(lambda row: row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n", "df[~df.loc[:, 'col1':'col3'].apply(lambda row:\n    row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n"], "link": "https://stackoverflow.com/questions/61016701/issue-in-applying-str-contains-across-multiple-columns-in-python"}
{"id": 649, "q": "Python pandas: how to fast process the value in columns", "d": "Hi there is a dataframe like the following dataframes df1. The data type is string. I want to get the dataframe like the following dataframe. The eye data is divided into eye_x, eye_y, the other columns is the same, the data type is float. Until now I know how to get the (x, y) value together with the following code:", "q_apis": "value columns get columns now get value", "io": " eye_x eye_y nose_x nose_y mouse_x mouse_y ear_x ear_y 34 35 45 66 45 64 78 87 35 38 75 76 95 37 38 79 64 43 85 66 65 45 87 45 <s> eye nose mouse ear (34, 35) (45,66) (45,64) (78,87) (35, 38) (75,76) (95,37) (38,79) (64, 43) (85,66) (65,45) (87,45) ", "apis": "stack iloc columns unstack swaplevel columns columns map join", "code": ["v = df.stack().str.split('_', expand=True).iloc[:, :-1]\nv.columns = ['x', 'y']\n\nv = v.unstack().swaplevel(0, 1, axis=1)\nv.columns = v.columns.map('_'.join)\n"], "link": "https://stackoverflow.com/questions/49525337/python-pandas-how-to-fast-process-the-value-in-columns"}
{"id": 257, "q": "Pandas: Multiline data schema to single line", "d": "I have a big (huge) dataset that have the next schema: and for many reasons, one of them the to reduce the size, I want to transform it to the next schema: I tried grouping by ['dt', 'id'] and then iterating over each group to build the new rows but it is too slow. I'm not figuring out a way without iterating over every original row. Any idea?", "q_apis": "size transform", "io": "dt | id | val_t | val 1 | 1 | 1 | 123 1 | 1 | 2 | 145 1 | 1 | 3 | 234 1 | 2 | 1 | 234 1 | 2 | 2 | 433 1 | 2 | 3 | 453 .................. N | X | 1 | 652 N | X | 2 | 543 N | X | 3 | 324 <s> dt | id | val_1 | val_2 | val_3 1 | 1 | 123 | 145 | 234 1 | 2 | 234 | 433 | 453 .................. N | X | 652 | 543 | 324 ", "apis": "pop astype set_index unstack columns columns droplevel rename_axis columns reset_index", "code": ["df['temp'] = 'val_'\ndf['val_t'] = df.pop('temp') + df['val_t'].astype(str)\ndf = df.set_index(['dt', 'id', 'val_t']).unstack()\ndf.columns = df.columns.droplevel()\ndf = df.rename_axis(columns=None).reset_index()\n"], "link": "https://stackoverflow.com/questions/62164809/pandas-multiline-data-schema-to-single-line"}
{"id": 371, "q": "Pandas - split dataframe according to sorted sequence in columns", "d": "I have a pandas dataframe with this type of structure: Basically, I sort the data frame according to the values of val1 and val2 beforehand, so I know I'll have two ascending sequences afterwards. What I want is to split this df in two new dfs, according to the two sequences, which in my example would be this: I have checked this question and this, but I don't know the number of values/rows beforehand... I've also checked another question, so I thought about using split with a regular expression. But I only know the sequences will be ascending, there's no guarantee that the values will be continuous, so it doesn't work as expected. Is this possible to achieve? I appreciate in advance any help!", "q_apis": "columns values values values any", "io": "df Val1 Val2 Col1 Col2 1 1 0 3 1 2 2 4 2 1 2 3 3 2 2 5 1 2 3 4 2 1 3 1 3 4 2 1 <s> df1 Val1 Val2 Col1 Col2 1 1 0 3 1 2 2 4 2 1 2 3 3 2 2 5 df2 Val1 Val2 Col1 Col2 1 2 3 4 2 1 3 1 3 4 2 1 ", "apis": "shift groupby cumsum", "code": ["m = df['Val1'].shift() > df['Val1']\ndfs = [df for _, df in df.groupby(m.cumsum())]\n"], "link": "https://stackoverflow.com/questions/59645274/pandas-split-dataframe-according-to-sorted-sequence-in-columns"}
{"id": 521, "q": "Faster way to update a column in a pandas data frame based on the value of another column", "d": "I have a pandas data frame with columns = [A, B, C, D, ...I, Z]. There are around ~80000 rows in the dataframe, and columns A, B, C, D, ..., I have value 0 for all these rows. Z has a value between [0, 9]. What I am trying to do is update the value of the x'th column for all rows in the data frame, where x is the current value of Z. If value of x is 0, then ignore. The data frame looks like - This is what I have so far. This is way too slow, and causes the script to stop executing midway. Is there a faster or better way to do it? I tried looking at np.where and np.apply, but I am not able to figure out the syntax. This is what I tried using np.apply - The desired output for the above sample is -", "q_apis": "update value columns columns value all value between update value all where value value stop at where apply apply sample", "io": " A B C D ... Z 0 0 0 0 0 ... 9 1 0 0 0 0 ... 1 2 0 0 0 0 ... 2 3 0 0 0 0 ... 3 <s> A B C D ... Z 0 0 0 0 0 ... 9 1 0 1 0 0 ... 1 2 0 0 1 0 ... 2 3 0 0 0 1 ... 3 ", "apis": "array dtype array DataFrame concat get_dummies drop", "code": ["In [276]: cols[df['Z']]\nOut[276]: array(['temp', 'B', 'C', 'D', 'B', 'F', 'E'], dtype='<U3')\n", "import numpy as np\nimport pandas as pd\ncols = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'temp'])\ndf = pd.DataFrame({'Z':[9,1,2,3,1,5,4]})\n\ndf = pd.concat([pd.get_dummies(cols[df['Z']]), df['Z']], axis=1)\ndf = df.drop('temp', axis=1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/54640734/faster-way-to-update-a-column-in-a-pandas-data-frame-based-on-the-value-of-anoth"}
{"id": 354, "q": "Fill the values with each column combination with some default values in pandas data frame", "d": "I have a dataframe like this, Now CD is not present with col1 1908 and 1909 values, FR not present with 1908 and 1909 values and PR not present wth 1907. Now I want to create rows with col2 values which are not with all col1 values with col3 values as 0. So final dataframe will look like, I could do this using a for loop with every possible col2 values and comparing with every col1 group. But I am looking for shortcuts to do it most efficiently.", "q_apis": "values values values values values all values values values", "io": "df col1 col2 col3 1907 CD 49 1907 FR 33 1907 SA 34 1908 PR 1 1908 SA 37 1909 PR 16 1909 SA 38 <s> df col1 col2 col3 1907 CD 49 1907 FR 33 1907 SA 34 1907 PR 0 1908 CD 0 1908 FR 0 1908 PR 1 1908 SA 37 1908 CD 0 1908 FR 0 1909 PR 16 1909 SA 38 ", "apis": "MultiIndex from_product unique unique names set_index reindex reset_index", "code": ["mux = pd.MultiIndex.from_product([df['col1'].unique(), \n                                  df['col2'].unique()], names=['col1','col2'])\ndf = df.set_index(['col1','col2']).reindex(mux, fill_value=0).reset_index()\n"], "link": "https://stackoverflow.com/questions/60095370/fill-the-values-with-each-column-combination-with-some-default-values-in-pandas"}
{"id": 376, "q": "add column to groups on dataframe pandas", "d": "I have a dataframe: How can I add a new column to dataframe relative to the id column? for example:", "q_apis": "add groups add", "io": " id | x | y 1 | 0.3 | 0.4 1 | 0.2 | 0.5 2 | 0.1 | 0.6 2 | 0.9 | 0.1 3 | 0.8 | 0.2 3 | 0.7 | 0.3 <s> id | x | y | color 1 | 0.3 | 0.4 | 'green' 1 | 0.2 | 0.5 | 'green' 2 | 0.1 | 0.6 | 'black' 2 | 0.9 | 0.1 | 'black' 3 | 0.8 | 0.2 | 'red' 3 | 0.7 | 0.3 | 'red' ", "apis": "unique map", "code": ["d={x:random_color() for x in df.id.unique()}\ndf['color']=df['id'].map(d)\n"], "link": "https://stackoverflow.com/questions/59533480/add-column-to-groups-on-dataframe-pandas"}
{"id": 531, "q": "How to delete continuous four digits from a column value in pandas dataframe", "d": "I have a data frame like this: I want to remove where the digits occurring for exact four times The result will look like: What is the best way to do it using python", "q_apis": "delete value where", "io": "col1 col2 col3 A 12134 tea2014 2 B 2013 coffee 1 1 C green 2015 tea 4 <s> col1 col2 col3 A 12134 tea 2 B coffee 1 1 C green tea 4 ", "apis": "any", "code": ["(\n    ?<!   # negative lookbehind \n    \\d    # any single digit\n)\n\\d{4}     # match exactly 4 digits\n(\n    ?!    # negative lookahead\n    \\d\n)\n"], "link": "https://stackoverflow.com/questions/54020869/how-to-delete-continuous-four-digits-from-a-column-value-in-pandas-dataframe"}
{"id": 184, "q": "Pandas loop to numpy . Numpy count occurrences of string as nonzero in array", "d": "Suppose I have the following dataframe with element types in brackets When using pandas loops I use the following code. If : I am trying to use NumPy and array methods for efficiency. I have tried translating the method but no luck. Expected output", "q_apis": "count array array", "io": " Column1(int) Column2(str) Column3(str) 0 2 02 34 1 2 34 02 2 2 80 85 3 2 91 09 4 2 09 34 <s> Column1(int) Column2(str) Column3(str) Column4(int) 0 2 02 34 1 1 2 34 02 2 2 2 80 85 0 3 2 91 09 0 4 2 09 34 1 ", "apis": "map value_counts fillna where eq", "code": ["s = df['Column2'].map(df['Column3'].value_counts()).fillna(0)\ndf['Column4'] = np.where(df['Column1'].eq(2), s, 'F')\n"], "link": "https://stackoverflow.com/questions/64526205/pandas-loop-to-numpy-numpy-count-occurrences-of-string-as-nonzero-in-array"}
{"id": 653, "q": "Select consecultive times in a dataframe", "d": "I have a data frame where I would like to select the consecutive timestamp. I mean times that happen one after the other, in this case, that happened 15 minutes consecutively. For example, I would select only those from the above dataframe I have This is just an example but I am dealing with many timestamps. How can I do this with python commands?", "q_apis": "where select timestamp mean select", "io": "2017-07-19 17:45:00+02:00 16 2017-07-23 02:45:00+02:00 23 2017-07-25 14:15:00+02:00 23 2017-07-27 07:00:00+02:00 25 2017-07-28 09:30:00+02:00 22 2017-07-28 18:00:00+02:00 17 2017-07-29 04:00:00+02:00 28 2017-07-29 04:15:00+02:00 19 2017-07-29 11:30:00+02:00 20 2017-07-30 09:00:00+02:00 11 2017-08-03 02:45:00+02:00 22 2017-08-04 06:45:00+02:00 27 2017-08-06 01:45:00+02:00 21 2017-08-08 19:30:00+02:00 27 2017-08-08 19:45:00+02:00 27 2017-08-08 20:00:00+02:00 15 2017-08-08 21:45:00+02:00 25 <s> 2017-07-29 04:00:00+02:00 28 2017-07-29 04:15:00+02:00 19 2017-08-08 19:30:00+02:00 27 2017-08-08 19:45:00+02:00 27 2017-08-08 20:00:00+02:00 15 ", "apis": "time diff total_seconds eq time dtype bool", "code": ["In [205]: df.time.diff().dt.total_seconds().eq(900)\nOut[205]:\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7      True\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14     True\n15     True\n16    False\nName: time, dtype: bool\n"], "link": "https://stackoverflow.com/questions/49367519/select-consecultive-times-in-a-dataframe"}
{"id": 336, "q": "Transpose pandas dataframe", "d": "How do I convert a list of lists to a panda dataframe? it is not in the form of coloumns but instead in the form of rows. for example: I want it to be shown as rows and not coloumns. currently it shows somethign like this I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists? This is how I want the output to look like with other coloumns also filled in. However. won't help.", "q_apis": "all", "io": " B P F I FP BP 2 M 3 1 I L 0 64 73 76 64 61 32 36 94 81 49 94 48 1 57 58 69 46 34 66 15 24 20 49 25 98 2 99 61 73 69 21 33 78 31 16 11 77 71 3 41 1 55 34 97 64 98 9 42 77 95 41 4 36 50 54 27 74 0 8 59 27 54 6 90 5 74 72 75 30 62 42 90 26 13 49 74 9 6 41 92 11 38 24 48 34 74 50 10 42 9 7 77 9 77 63 23 5 50 66 49 5 66 98 8 90 66 97 16 39 55 38 4 33 52 64 5 9 18 14 62 87 54 38 29 10 66 18 15 86 10 60 89 57 28 18 68 11 29 94 34 37 59 11 78 67 93 18 14 28 64 11 77 79 94 66 <s> B P F I FP BP 2 M 3 1 I L 0 64 1 73 1 76 2 64 3 61 4 32 5 36 6 94 7 81 8 49 9 94 10 48 ", "apis": "DataFrame T columns", "code": ["import numpy\n\ndf = pandas.DataFrame(numpy.asarray(data[x]).T.tolist(),\n                      columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])\n"], "link": "https://stackoverflow.com/questions/24412510/transpose-pandas-dataframe"}
{"id": 514, "q": "Conditional mean and sum of previous N rows in pandas dataframe", "d": "Concerned is this exemplary pandas dataframe: Whenever is , I wish to calculate sum and mean of the last 3 (starting from current) valid measurements. Measurements are considered valid, if the column is . So let's clarify using the two examples in the above dataframe: : Indices should be used. Expected : Indices should be used. Expected I have tried and creating new, shifted columns, but was not successful. See the following excerpt from my tests (which should directly run): Any help or solution is greatly appreciated. Thanks and Cheers! EDIT: Clarification: This is the resulting dataframe I expect: EDIT2: Another clarification: I did indeed not miscalculate, but rather I did not make my intentions as clear as I could have. Here's another try using the same dataframe: Let's first look at the column: We find the first in index 3 (green rectangle). So index 3 is the point, where we start looking. There is no valid measurement at index 3 (Column is ; red rectangle). So, we start to go further back in time, until we have accumulated three lines, where is . This happens for indices 2,1 and 0. For these three indices, we calculate the sum and mean of the column (blue rectangle): SUM: 2.0 + 4.0 + 3.0 = 9.0 MEAN: (2.0 + 4.0 + 3.0) / 3 = 3.0 Now we start the next iteration of this little algorithm: Look again for the next in the column. We find it at index 7 (green rectangle). There is also a valid measuremnt at index 7, so we include it this time. For our calculation, we use indices 7,6 and 5 (green rectangle), and thus get: SUM: 1.0 + 2.0 + 3.0 = 6.0 MEAN: (1.0 + 2.0 + 3.0) / 3 = 2.0 I hope, this sheds more light on this little problem.", "q_apis": "mean sum sum mean last columns first at first index index where start at index start time where indices indices sum mean start at index at index time indices get", "io": "Sum = 9.0, Mean = 3.0 <s> Sum = 6.0, Mean = 2.0", "apis": "rolling mean rolling sum loc mean sum rolling agg mean sum loc mean sum loc mean sum shift mean sum", "code": ["df['RollM'] = df.Measurement.rolling(window=3,min_periods=0).mean()\n\ndf['RollS'] = df.Measurement.rolling(window=3,min_periods=0).sum()\n", "df.loc[df.Trigger == False,['RollS','RollM']] = np.nan\n", "   Measurement  Trigger  Valid     RollM  RollS\n0          2.0    False   True       NaN    NaN\n1          4.0    False   True       NaN    NaN\n2          3.0    False   True       NaN    NaN\n3          0.0     True  False  2.333333    7.0\n4        100.0    False   True       NaN    NaN\n5          3.0    False   True       NaN    NaN\n6          2.0    False   True       NaN    NaN\n7          1.0     True   True  2.000000    6.0\n", "df['mean'],df['sum'] = np.nan,np.nan\n\nroller = df.Measurement.rolling(window=3,min_periods=0).agg(['mean','sum'])\n\ndf.loc[(df.Trigger == True) & (df.Valid == True),['mean','sum']] = roller\n\ndf.loc[(df.Trigger == True) & (df.Valid == False),['mean','sum']] = roller.shift(1)\n", "  Measurement  Trigger  Valid  mean  sum\n0          2.0    False   True   NaN  NaN\n1          4.0    False   True   NaN  NaN\n2          3.0    False   True   NaN  NaN\n3          0.0     True  False   3.0  9.0\n4        100.0    False   True   NaN  NaN\n5          3.0    False   True   NaN  NaN\n6          2.0    False   True   NaN  NaN\n7          1.0     True   True   2.0  6.0\n"], "link": "https://stackoverflow.com/questions/46723310/conditional-mean-and-sum-of-previous-n-rows-in-pandas-dataframe"}
{"id": 470, "q": "How do you stack two Pandas Dataframe columns on top of each other?", "d": "Is there a library function or correct way of stacking two Pandas data frame columns on top of each other? For example make 4 columns into 2: to The documentation for Pandas Data Frames that I read for the most part only deal with concatenating rows and doing row manipulation, but I'm sure there has to be a way to do what I described and I am sure it's very simple. Any help would be great.", "q_apis": "stack columns columns columns", "io": "a1 b1 a2 b2 1 2 3 4 5 6 7 8 <s> c d 1 2 5 6 3 4 7 8 ", "apis": "DataFrame columns iloc iloc columns columns concat", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.arange(1, 9).reshape((2, 4)),\n        columns=[\"a1\", \"b1\", \"a2\", \"b2\"])\n\npart1 = df.iloc[:,0:2]\npart2 = df.iloc[:,2:4]\n\nnew_columns = [\"c\", \"d\"]\npart1.columns = new_columns\npart2.columns = new_columns\n\nprint pd.concat([part1, part2], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/27513890/how-do-you-stack-two-pandas-dataframe-columns-on-top-of-each-other"}
{"id": 215, "q": "How to split dataframe at headers that are in a row", "d": "I've got a page I'm scraping and most of the tables are in the format Heading --info. I can iterate through most of the tables and create separate dataframes for all the various information using pandas.read_html. However, there are some where they've combined information into one table with subheadings that I want to be separate dataframes with the text of that row as the heading (appending a list). Is there an easy way to split this dataframe - It will always be heading followed by associated rows, new heading followed by new associated rows. eg. Should be It'd be nice if people would just create web pages that made sense with the data but that's not the case here. I've tried iterrows but cannot seem to come up with a good way to create what I want. Help would be very much appreciated!", "q_apis": "at info all read_html where iterrows", "io": " Col1 Col2 0 thing 1 1 2 2 2 3 3 thing2 4 1 2 5 2 3 6 3 4 <s> thing 1 1 1 2 2 2 thing2 4 1 2 5 2 3 6 3 4 ", "apis": "reset_index drop where applymap empty rename columns iloc drop index", "code": ["import numpy as np\n\n\nres = [x.reset_index(drop=True) for x in np.split(df, np.where(df.applymap(lambda x: x == ''))[0]) if not x.empty]\nfor x in res:\n    x = x.rename(columns=x.iloc[0]).drop(x.index[0])\n    print(x)\n"], "link": "https://stackoverflow.com/questions/63799400/how-to-split-dataframe-at-headers-that-are-in-a-row"}
{"id": 381, "q": "Why doesn&#39;t pandas reindex() operate in-place?", "d": "From the reindex docs: Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Therefore, I thought that I would get a reordered by setting in place (!). It appears, however, that I do get a copy and need to assign it to the original object again. I don't want to assign it back, if I can avoid it (the reason comes from this other question). This is what I am doing: Outs: Reindex gives me the correct output, but I'd need to assign it back to the original object, which is what I wanted to avoid by using : The desired output after that line is: Why is not working in place? Is it possible to do that at all? Working with python 3.5.3, pandas 0.23.3", "q_apis": "reindex reindex DataFrame index value index index copy get get copy assign assign assign at all", "io": " a b c d e 0 0.234296 0.011235 0.664617 0.983243 0.177639 1 0.378308 0.659315 0.949093 0.872945 0.383024 2 0.976728 0.419274 0.993282 0.668539 0.970228 3 0.322936 0.555642 0.862659 0.134570 0.675897 4 0.167638 0.578831 0.141339 0.232592 0.976057 <s> e d c b a 0 0.177639 0.983243 0.664617 0.011235 0.234296 1 0.383024 0.872945 0.949093 0.659315 0.378308 2 0.970228 0.668539 0.993282 0.419274 0.976728 3 0.675897 0.134570 0.862659 0.555642 0.322936 4 0.976057 0.232592 0.141339 0.578831 0.167638 ", "apis": "reindex reindex index copy reindex index copy", "code": ["df = df.reindex(['e', 'd', 'c', 'b', 'a'], axis=1)  \n", "id(df)\n# 4839372504\n\nid(df.reindex(df.index, copy=False)) # same object returned \n# 4839372504\n\nid(df.reindex(df.index, copy=True))  # new object created - ids are different\n# 4839371608  \n"], "link": "https://stackoverflow.com/questions/56462088/why-doesnt-pandas-reindex-operate-in-place"}
{"id": 185, "q": "How to create a new column based on matching ID&#39;s and string&#39;s in names of other columns in the same data frame?", "d": "I have tried to find a solution online but I cannot. I have a dataframe with 10 separate id columns, and 10 separate corresponding value columns for each ID. A brief example is shown below Example: I want to create a new column that takes the value column from the corresponding match of ID's between the 'shooter_id' and any of the 'player_id' columns like below: I have really been struggling to make this work, I am not sure if I need to merge within itself, for loop through the dataframe, or .apply.. any insight would be very helpful! Thank you!", "q_apis": "names columns columns value columns value between any columns merge apply any", "io": "player_id_1 player_1_x player_id_2 player_2_x shooter_id 300 10 301 12 301 299 11 300 13 299 <s> player_id_1 player_1_x player_id_2 player_2_x shooter_id shooter_x 300 10 301 12 301 12 299 11 300 13 299 11 ", "apis": "filter eq idxmax replace add lookup index", "code": ["c = df.filter(like='player_id')\\\n      .eq(df['shooter_id'], axis=0)\\\n      .idxmax(1).str.replace('_id', '').add('_x')\n\ndf['shooter_x'] = df.lookup(df.index, c)\n"], "link": "https://stackoverflow.com/questions/64524795/how-to-create-a-new-column-based-on-matching-ids-and-strings-in-names-of-other"}
{"id": 110, "q": "Conditional Row shift in Pandas", "d": "I'm attempting to shift a row based on whether or not another column is not null. There's inconsistent spacing in the Description column so I can't do a .shift() Here's the original data And this is what I want my result to be Here's the code that I used from Align data in one column with another row, based on the last time some condition was true However when I run it, no errors and no changes in the dataframe.", "q_apis": "shift shift shift last time", "io": "Permit Number A Description 1234 NaN NaN NaN NaN NaN NaN NaN foo 3456 NaN NaN NaN NaN bar <s> Permit Number A Description 1234 NaN foo NaN NaN NaN NaN NaN NaN 3456 NaN bar NaN NaN NaN ", "apis": "mask notnull notnull isnull assign groupby mask cumsum transform iloc where", "code": ["mask = df['Description'].notnull()\nfmask = (df['Permit Number'].notnull() & df['Description'].isnull())\ndf = df.assign(Description=df.groupby(mask[::-1].cumsum())['Description'].transform(lambda x: x.iloc[-1]).where(fmask))\n"], "link": "https://stackoverflow.com/questions/65834585/conditional-row-shift-in-pandas"}
{"id": 138, "q": "Pandas advanced problem : For each row, get complex info from another dataframe", "d": "Problem I have a dataframe : And I have another dataframe , with products like this : What I want is to add to a column, that will sum the Prices of the last products of each type known at the current date (with ). An example is the best way to explain : For the first row of The last A-Product known at this date bought by is this one : (since the 4th one is older, and the 5th one has a > ) The last B-Product known at this date bought by is this one : So the row in , after transformation, will look like that ( being , prices of the 2 products of interest) : The full after transformation will then be : As you can see, there are multiple possibilities : Buyers didn't necessary buy all products (see , who only bought A-products) Sometimes, no product is known at (see row 3 of the new , in 2015, we don't know any product bought by ) Sometimes, only one product is known at , and the value is the one of the product (see row 3 of the new , in 2015, we only have one product for , which is a B-product bought in 2014, whose price is ) What I did : I found a solution to this problem, but it's taking too much time to be used, since my dataframe is huge. For that, I iterate using iterrows on rows of , I then select the products linked to the Buyer, having on , then get the older grouping by and getting the max date, then I finally sum all my products prices. The fact I solve the problem iterating on each row (with a for iterrows), extracting for each row of a part of that I work on to finally get my sum, makes it really long. I'm almost sure there's a better way to solve the problem, with pandas functions ( for example), but I couldn't find the way. I've been searching a lot. Thanks in advance for your help Edit after Dani's answer Thanks a lot for your answer. It looks really good, I accepted it since you spent a lot of time on it. Execution is still pretty long, since I didn't specify something. In fact, are not shared through buyers : each buyers has its own multiple products types. The true way to see this is like this : As you can understand, product types are not shared through different buyers (in fact, it can happen, but in really rare situations that we won't consider here) The problem remains the same, since you want to sum prices, you'll add the prices of last occurences of johndoe-ID2 and johndoe-ID3 to have the same final result row But as you now understand, there are actually more than , so the step \"get unique product types\" from your answer, that looked pretty fast on the initial problem, actually takes a lot of time. Sorry for being unclear on this point, I didn't think of a possibility of creating a new df based on product types.", "q_apis": "get info add sum last at date first last at date last at date all product at any product product at value product product product time iterrows select get max date sum all iterrows get sum time product sum add last now step get unique product time product", "io": "3 A 2019-01-01 johndoe 600 <s> 7 B 2016-11-15 johndoe 300 ", "apis": "get unique product unique DataFrame explode merge date product merge merge_asof sort_values sort_values merge merge fillna sum date merge groupby sum rename columns get unique product unique DataFrame explode get unique product groupby apply DataFrame map explode merge date product merge merge_asof sort_values sort_values merge merge fillna sum date merge groupby sum rename columns", "code": ["# get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\nprint(res)\n", "# get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n", "# get unique product types\nproduct_types = df_prod.groupby('Buyer')['Product-Type'].apply(lambda x: list(set(x)))\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = df['Client_ID'].map(product_types)\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\n\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65392984/pandas-advanced-problem-for-each-row-get-complex-info-from-another-dataframe"}
{"id": 97, "q": "Dataframe - Pandas - How ploting same columns of two dataframe for visualising the differences", "d": "There are two dataframes and I would like to have a plot that shows the both price columns on X axis and sum on the Y axis to see how are the difference between these two dataframes. I tried the below but does nothing. What is the best way to show the differences between the two patterns of the price in these two dataframes? I thought of something like this, but if there is a better way to show the differences please mention it.", "q_apis": "columns plot columns sum difference between between", "io": "df1 +-----+-----+-------+ | | id | price | +-----+-----+-------+ | 1 | 1 | 5 | +-----+-----+-------+ | 2 | 2 | 12 | +-----+-----+-------+ | 3 | 3 | 34 | +-----+-----+-------+ | 4 | 4 | 62 | +-----+-----+-------+ | ... | ... | ... | +-----+-----+-------+ | 125 | 125 | 90 | +-----+-----+-------+ <s> df2 +-----+-----+-------+ | | id | price | +-----+-----+-------+ | 1 | 1 | 14 | +-----+-----+-------+ | 2 | 2 | 15 | +-----+-----+-------+ | 3 | 3 | 45 | +-----+-----+-------+ | 4 | 4 | 62 | +-----+-----+-------+ | ... | ... | ... | +-----+-----+-------+ | 125 | 125 | 31 | +-----+-----+-------+ ", "apis": "DataFrame round DataFrame round plot first plot second merge abs", "code": ["import pandas as pd \nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\n\ndf1 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\ndf2 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\n\nplt.plot(df1['col2'], label='first')\nplt.plot(df2['col2'], label='second')\nplt.legend()\n", "df3 = pd.merge(df1,df2, on='col1')\nsns.lineplot(x='col2_x', y='col2_y', data=df3)\n", "df3['dif'] = abs(df3['col2_x'] - df3['col2_y'])\n\nsns.lineplot(x='col1', y='dif', data=df3)\nplt.xticks(df3['col1'])\n"], "link": "https://stackoverflow.com/questions/66194236/dataframe-pandas-how-ploting-same-columns-of-two-dataframe-for-visualising-t"}
{"id": 410, "q": "Python Pandas Get Values According to If/Else", "d": "My input dataframe; I want to count whether any difference or not among \"Order\" and Need values with below code; I want to that something like this; If (WarehouseStock-StoreStock) >= Need: Else Desired Outputs are; Count 3 Could you please help me about this?", "q_apis": "count any difference values", "io": "Order Need WarehouseStock StoreStock 1 3 74 5 0 4 44 44 0 0 44 44 6 12 44 44 0 6 644 44 6 6 44 44 <s> Order Need WarehouseStock StoreStock 1 3 74 5 6 12 44 44 0 6 644 44 ", "apis": "where ge between between", "code": ["import pandas as pd\nimport numpy as np\n\ns = df['Need'] - df['Order']\nind = np.where((df['WarehouseStock'] - df['StoreStock']).ge(df['Need']), ~s.between(-1, 1), ~s.between(-5 , 5))\n"], "link": "https://stackoverflow.com/questions/58522174/python-pandas-get-values-according-to-if-else"}
{"id": 130, "q": "How to fill multiple list by 0&#39;s in Pandas data frame?", "d": "I have Pandas data frame and I am trying to add 0's in those lists where numbers are missing. In the below data frame, the max length of the list is 4 which is in the 3rd position. accordingly, I will add 0's to the remaining lists. Input: Output:", "q_apis": "add where max length add", "io": " Lists 0 [158, 202] 1 [609, 405] 2 [544, 20] 3 [90, 346, 130, 202] 4 [6] <s> Lists 0 [158, 202, 0, 0] 1 [609, 405, 0, 0] 2 [544, 20, 0, 0] 3 [90, 346, 130, 202] 4 [6, 0, 0, 0] ", "apis": "max apply apply", "code": ["maxlen = df['Lists'].str.len().max() #as suggested by Anky, better than an apply since vectorised\nf = lambda x: x + ([0] * (maxlen - len(x)))\n\ndf['Padded'] = df['Lists'].apply(f)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/65601285/how-to-fill-multiple-list-by-0s-in-pandas-data-frame"}
{"id": 302, "q": "selecting rows on pandas dataframe based on conditions", "d": "I have the following code: that creates a dataframe stored in variable 'df'. It consists of 2 columns (column1 and column2) filled with random 0s and 1s. This is the output I got when I ran the program (If you try to run it you won't get exactly the same result because of the randomint generation). I would like to create a filter on column2, showing only the clusters of data when there are three or more 1s in a row. The output would be something like this: I have left a space between the clusters for visual clarity, but the real output would not have the empty spaces in the dataframe. I would like to do it in the following way. Thank you", "q_apis": "columns get filter left between empty", "io": " column1 column2 0 0 1 1 1 0 2 0 1 3 1 1 4 0 1 5 1 1 6 0 1 7 1 1 8 1 0 9 0 1 10 0 0 11 1 1 12 1 1 13 0 1 14 0 0 15 0 1 16 1 1 17 1 1 18 0 1 19 1 0 20 0 0 21 1 0 22 0 1 23 1 0 24 1 1 25 0 0 26 1 1 27 1 0 28 0 1 29 1 0 <s> column1 column2 2 0 1 3 1 1 4 0 1 5 1 1 6 0 1 7 1 1 11 1 1 12 1 1 13 0 1 15 0 1 16 1 1 17 1 1 18 0 1 ", "apis": "ne shift cumsum groupby transform size ge eq loc eq loc mul map value_counts ge mean std groupby transform size ge mean std", "code": ["n = 3\nblocks = df['column2'].ne(df['column2'].shift()).cumsum()\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\nm2 = df['column2'].eq(1)\ndf_filtered = df.loc[m1 & m2]\n# Alternative without df['column2'].eq(1)\n#df_filtered = df.loc[m1.mul(df['column2'])]\nprint(df_filtered)\n", "%%timeit\nm1 = blocks.map(blocks.value_counts().ge(n))\n1.41 ms \u00b1 122 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n\n%%timeit\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\n2.12 ms \u00b1 226 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/61545023/selecting-rows-on-pandas-dataframe-based-on-conditions"}
{"id": 123, "q": "Transform a pandas dataframe: need for a more efficient solution", "d": "I have a dataframe indexed by dates from a certain period. My columns are predictions about the value of a variable by the end of a given year. My original dataframe looks something like this: where NaN means that the prediction does not exist for that given year. Since I am working with 20+ years and most predictions are for the next 2-3 years, my real dataframe has 20+ columns mostly containing values. For instance, the column for the year 2005 has predictions made in 2003-2005, but in the range 2006-2020 it's all . I would like to transform my dataframe to something like this: where represents the prediction made for the . This way, I would have a dataframe with only 4 columns (Y_0, Y_1, Y_2, Y_3). I actually achieved this, but in what I think it is a very inefficient way: For a dataframe with only 1000 rows, this is taking almost 3 seconds to run. Can anyone think of a better solution?", "q_apis": "columns value year where year columns values year all transform where columns seconds", "io": " 2016 2017 2018 2016-01-01 0.0 1 NaN 2016-07-01 1.0 1 4.1 2017-01-01 NaN 5 3.0 2017-07-01 NaN 2 2.0 <s> Y_0 Y_1 Y_2 2016-01-01 0 1 NaN 2016-07-01 1 1 4.1 2017-01,01 5 3 NaN 2017-07-01 2 2 NaN ", "apis": "year to_datetime date year year pivot index date columns values dropna all add_prefix", "code": ["df['year'] = pd.to_datetime(df['date']).dt.year\ndf['dt'] = df['prediction_year'] - df['year']\ndf = df.pivot(index = 'date', columns='dt', values='prediction').dropna(axis = 1, how = 'all').add_prefix('Y_')\n"], "link": "https://stackoverflow.com/questions/65721916/transform-a-pandas-dataframe-need-for-a-more-efficient-solution"}
{"id": 651, "q": "Insert characters at multiple positions in a dataframe column", "d": "I have a table with a lot of rows, and i need to change all the first \" \" space for \"h\" the second space for \"m\" and add at last char \"s\" I have made this code, but for my use is getting a slow. With this code I change my data frame from: To This: Is there any function that i can do this? I tried using df.replace, but it replaces all \" \" in the table... thanks for now", "q_apis": "at all first second add at last any replace all now", "io": " DEC RA +26 01 09.559 11 50 10.4747 +26 01 10.770 11 50 10.2641 +26 01 11.980 11 50 10.0534 +26 01 13.191 11 50 09.8428 <s> DEC RA 0 +26d01m09.559s 11h50m10.4747s 1 +26d01m10.770s 11h50m10.2641s 2 +26d01m11.980s 11h50m10.0534s 3 +26d01m13.191s 11h50m09.8428s ", "apis": "", "code": ["def split_combine(v, letters=list('dms')):\n    v = v.str.split(expand=True)\n    return (\n       v[0] + letters[0] \n     + v[1] + letters[1] \n     + v[2] + letters[2]\n    )\n"], "link": "https://stackoverflow.com/questions/49499352/insert-characters-at-multiple-positions-in-a-dataframe-column"}
{"id": 263, "q": "Pandas: creating values in a column based on the previous value in that column", "d": "Quick example: Before: After So the formula here is . I started with adding a Value column where each cell is 0. I then have looked a shift() but that uses the value in the previous row from the start of the command/function. So it will always use 0 as the value for Value. Is there a way of doing this without using something like iterrows() or a for loop ?", "q_apis": "values value where shift value start value iterrows", "io": "In Out 1 5 10 0 2 3 <s> In Out Value 1 5 -4 10 0 6 2 3 5 ", "apis": "cumsum", "code": ["df['Value'] = (df['In'] - df['Out']).cumsum()\n"], "link": "https://stackoverflow.com/questions/62862213/pandas-creating-values-in-a-column-based-on-the-previous-value-in-that-column"}
{"id": 76, "q": "Copy the last seen non empty value of a column based on a condition in most efficient way in Pandas/Python", "d": "I need to copy and paste the previos non-empty value of a column based on a condition. I need to do it in the most efficient way because the number of rows is a couple of millions. Using for loop will be computationally costly. So it will be highly appreciated if somebody can help me in this regard. Based on the condition, whenever the Col_A will have any value (not null) 10.2.6.1 in this example, the last seen value in Col_B (51,61 respectively) will be paste on that corresponding row where the Col_A value is not null. And the dataset should look like this: I tried with this code below but it's not working:", "q_apis": "last empty value copy empty value any value last value where value", "io": "|Col_A |Col_B | |--------|--------| |10.2.6.1| NaN | | NaN | 51 | | NaN | NaN | |10.2.6.1| NaN | | NaN | 64 | | NaN | NaN | | NaN | NaN | |10.2.6.1| NaN | <s> |Col_A |Col_B | |--------|--------| |10.2.6.1| NaN | | NaN | 51 | | NaN | NaN | |10.2.6.1| 51 | | NaN | 64 | | NaN | NaN | | NaN | NaN | |10.2.6.1| 64 | ", "apis": "loc notnull loc notnull drop columns ffill dropna", "code": ["df.loc[df['Col_A'].notnull(), 'Col_B'] = df.loc[df['Col_A'].notnull(), 'Col_C']\ndf = df.drop(columns=['Col_C'])\n", "df['Col_B'] = df['Col_B'].ffill()\ndf = df.dropna()\n"], "link": "https://stackoverflow.com/questions/66523605/copy-the-last-seen-non-empty-value-of-a-column-based-on-a-condition-in-most-effi"}
{"id": 136, "q": "why having std for 1 column and others are nan?", "d": "i have DataFrame looks something like this but with shape (345,5) like this and i want to get the std for the numeric columns ONLY with my manually std function and save in dictionary, the probelm is i am getting this result for the first column only: and here is my code:", "q_apis": "std DataFrame shape get std columns std first", "io": "|something1| something2| numbers1| number2 |number3| |----------|------------|----------|---------|-------| | A | str | 45 | nan |nan | |B | str2 | 6 | nan | nan | | c | str3 | 34 | 67 | 45 | |D | str4 | 56 | 45 | 23 | <s> {'number1': 18.59267328815305, 'number2': nan, 'number3': nan, 'number4': nan} ", "apis": "std dropna values columns", "code": ["std = {column:std_func(df[column].dropna().values) for column in df.columns}\n"], "link": "https://stackoverflow.com/questions/65451097/why-having-std-for-1-column-and-others-are-nan"}
{"id": 177, "q": "Calculate similarity between rows of a dataframe (count values in common)", "d": "I want to calculate similarity between the rows of my dataframe. I have some columns with informations about some people. One row is one person. It looks like that : I want to count for each row the number of values in common with the other rows divided by the number of columns if at least 3 columns are completed. For example, between the row with the index 1 and the row with the index 2, there are 4 variables in common. So, my similarity will be 4/5 (id doesn't count) = 80% of similarity. My result has to be a similarity matrix, because after that I want to find the rows with a similarity higher than 0.6 to build a new dataframe. It could be something like that : Because the results are duplicated, half of that would be enough : I'm looking for a function that will automate that but I couldn't find. Does something like that exist? Thanks for reading, any advice or idea will be welcomed.", "q_apis": "between count values between columns count values columns at columns between index index count duplicated any", "io": " print(similarity) 0 1 2 3 4 0 1 0 0 0 0.2 1 0.2 1 0.8 0.2 0 2 0 0.8 1 0.2 0 3 0 0.2 0.2 1 0 4 0.2 0 0 0 1 <s> print(similarity) 0 1 2 3 4 0 0 0 0 0.2 1 0.8 0.2 0 2 0.2 0 3 0 4 ", "apis": "DataFrame set_index mean", "code": ["from scipy.spatial.distance import pdist, squareform\npd.DataFrame(1 - squareform(pdist(df.set_index('id'), lambda u,v: (u != v).mean())))\n"], "link": "https://stackoverflow.com/questions/64646490/calculate-similarity-between-rows-of-a-dataframe-count-values-in-common"}
{"id": 292, "q": "Reverse the order of elements by group", "d": "Say I have a DataFrame like this: which looks like this I would like to reverse its elements within each group, where column determines the group. So, the desired output would be How can I do this?", "q_apis": "DataFrame where", "io": " a b 0 1 1 1 1 2 2 1 3 3 1 4 4 2 5 5 2 6 6 2 7 7 2 8 <s> a b 0 1 4 1 1 3 2 1 2 3 1 1 4 2 8 5 2 7 6 2 6 7 2 5 ", "apis": "groupby apply iloc reset_index drop", "code": ["(\n    df.groupby('a', sort=False)\n    .apply(lambda x: x.iloc[::-1])\n    .reset_index(drop=True)\n)\n"], "link": "https://stackoverflow.com/questions/61969815/reverse-the-order-of-elements-by-group"}
{"id": 489, "q": "How to concatenate pandas dataframe date and different time formats to single timestamp?", "d": "I have two columns in a data frame as outlined below. Notice how some of the is in , some is in format. When running... ...I can get in a consumable (for my purposes) format (e.g. ). But when running... ... is added to the times and is not being applied to all rows. How do I concatenate the date and times (which include multiple time formats) in a single timestamp? Edit1: @Wen-Ben 's solution got me here: Then to concatenate EVENT_DATE and EVENT_TIME, I found this (which works): ...results in: Next I want to get this into ISO8601 format. So I found this (which works): ...results in: HERES MY NEW PROBLEM: Running still shows the concatenated versions (e.g. ) instead of the ISO version (e.g.) How do I get the ISO8601 column \"added\" to the dataframe? Ideally, I want it to take the place of . I want it as a transformation of the data, not tacked on as a new column.", "q_apis": "date time timestamp columns get all date time timestamp get get take", "io": "1 19:53:00 11 14:30:00 15 16:30:00 <s> 1 1999-07-28 19:53:00 11 2001-07-28 14:30:00 15 2002-06-07 16:30:00 ", "apis": "to_datetime to_datetime fillna", "code": ["s1=pd.to_datetime(df['EVENT_TIME'], format='%H.%M.%S', errors='coerce')\ns2=pd.to_datetime(df['EVENT_TIME'], format='%I:%M:%S %p', errors='coerce')\ndf['EVENT_TIME']=s1.fillna(s2)\n"], "link": "https://stackoverflow.com/questions/55859947/how-to-concatenate-pandas-dataframe-date-and-different-time-formats-to-single-ti"}
{"id": 513, "q": "Grouping columns by unique values in Python", "d": "I have a data set with two columns and I need to change it from this format: to this I need every unique value in the first column to be on its own row. I am a beginner with Python and beyond reading in my text file, I'm at a loss for how to proceed.", "q_apis": "columns unique values columns unique value first at", "io": "10 1 10 5 10 3 11 5 11 4 12 6 12 2 <s> 10 1 5 3 11 5 4 12 6 2 ", "apis": "DataFrame", "code": ["import pandas as pd\n\ndf = pd.DataFrame({'A':[10,10,10,11,11,12,12],'B':[1,5,3,5,4,6,2]})\nprint(df)\n"], "link": "https://stackoverflow.com/questions/44606413/grouping-columns-by-unique-values-in-python"}
{"id": 343, "q": "Pandas: How to remove non-alphanumeric columns in Series", "d": "A Pandas' Series can contain invalid values: I want to produce a clean Series keeping only the columns that contain a numeric value or a non-empty non-space-only alphanumeric string: should be dropped because it is an empty string; because ; and because space-only strings. The expected result: How can I filter the columns that contain numeric or valid alphanumeric? returns for , instead of the True I would expect. changes 's to string and later considers it a valid string. of course drops only (). I don't see so many other possibilities listed at https://pandas.pydata.org/pandas-docs/stable/reference/series.html As a workaround I can loop on the items() checking type and content, and create a new Series from the values I want to keep, but this approach is inefficient (and ugly): Is there any boolean filter that can help me to single out the good columns?", "q_apis": "columns Series Series values Series columns value empty empty filter columns at items Series values any filter columns", "io": "a b c d e f g 1 \"\" \"a3\" np.nan \"\\n\" \"6\" \" \" <s> a c f 1 \"a3\" \"6\" ", "apis": "astype notna dtype", "code": ["row = row[row.astype(str).str.isalnum() & row.notna()]\nprint (row)\na     1\nc    a3\nf     6\nName: 0, dtype: object\n"], "link": "https://stackoverflow.com/questions/60394977/pandas-how-to-remove-non-alphanumeric-columns-in-series"}
{"id": 433, "q": "Subtract previous row value from the current row value in a Pandas column", "d": "I have a pandas column with the name 'values' containing respective values . I want to subtract the each value from the next value so that I get the following format: I've tried to solve this using a for loop that loops over all the data-frame but this method was time consuming. Is there a straightforward method the dataframe functions to solve this problem quickly? The output we desire is the following:", "q_apis": "value value name values values value value get all time", "io": "10 15 36 95 99 <s> 10 5 21 59 4", "apis": "Series diff fillna", "code": ["import pandas as pd\n\ns = pd.Series([11,15,22,27,36,69,77])\ns.diff().fillna(s)\n"], "link": "https://stackoverflow.com/questions/57801048/subtract-previous-row-value-from-the-current-row-value-in-a-pandas-column"}
{"id": 368, "q": "Is there a way to replace each cell value in a dataframe with the column name, row value in the first column and the value itself?", "d": "I have a matrix in excel which I am reading in as a pandas dataframe in python I want to be able to concatenate the column name, cell values from the first column and the current value of the cell for all cells in columns greater than col1. I essentially want the following output: I could not figure out a way to do this in python.", "q_apis": "replace value name value first value name values first value all columns", "io": " col1 col2 col3 C_0 a f C_1 b g C_2 c h C_3 d i C_4 e j <s> col1 col2 col3 C_0 col2_C_0_a col3_C_0_f C_1 col2_C_1_b col3_C_1_g C_2 col2_C_2_c col3_C_2_h C_3 col2_C_3_d col3_C_3_i C_4 col2_C_4_e col3_C_4_j ", "apis": "astype set_index radd index radd columns reset_index", "code": ["m = df.astype(str).set_index('col1')\nm.radd(m.index+'_',axis=0).radd(m.columns + '_').reset_index()\n"], "link": "https://stackoverflow.com/questions/59754548/is-there-a-way-to-replace-each-cell-value-in-a-dataframe-with-the-column-name-r"}
{"id": 612, "q": "How to select specific column items as list from pandas dataframe?", "d": "I have a dataframe like this : How to convert it into this form (All zeros appearing are not considered) : And finally into a set of items like :", "q_apis": "select items items", "io": " A B C D --------------- 0 A 0 C D 1 A 0 C D 2 0 B C 0 3 A 0 0 D 4 0 B C 0 5 A 0 0 0 <s> A B C D E ---------------------- 0 A 0 C D [A,C,D] 1 A 0 C D [A,C,D] 2 0 A C 0 [A,C] 3 A 0 0 D [A,D] 4 0 A C 0 [A,C] 5 A 0 0 0 [A] ", "apis": "values", "code": ["s = [set([y for y in x if y != '0']) for x in df.values.tolist()]\nprint (s)\n[{'A', 'D', 'C'}, {'A', 'D', 'C'}, {'C', 'B'}, {'A', 'D'}, {'C', 'B'}, {'A'}]\n"], "link": "https://stackoverflow.com/questions/51259229/how-to-select-specific-column-items-as-list-from-pandas-dataframe"}
{"id": 452, "q": "Error when trying to .insert() into dataframe", "d": "I have some code that takes a csv file, that finds the min/max each day, then tells me what time that happens. I also have 2 variables to find the percentage for both max/min. This is currently the output for the dataframe Then I have 2 varables for the % of High/Lows.. (Just ph shown) I've tried to do an .insert(), but recieve this error. TypeError: insert() takes from 4 to 5 positional arguments but 6 were given This was my code I would like the output to show the % in columns 3 &4", "q_apis": "insert min max day time max min insert insert columns", "io": ">Out High Low 10:00 6.0 10.0 10:05 10.0 3.0 10:10 1.0 7.0 10:15 1.0 NaN 10:20 4.0 4.0 10:25 4.0 4.0 10:30 5.0 1.0 10:35 5.0 6.0 10:40 3.0 2.0 10:45 4.0 5.0 10:50 4.0 1.0 10:55 3.0 4.0 11:00 4.0 5.0 > <s> >Out High Low % High %Low 10:00 6.0 10.0 .015306 10:05 10.0 3.0 .025510 10:10 1.0 7.0 .002551 10:15 1.0 NaN .002551 10:20 4.0 4.0 .010204 10:25 4.0 4.0 .010204 > ", "apis": "insert insert", "code": ["#adding % to end of dataframe\nresult[\"High %\"] = ph\nresult[\"Low %\"] = pl\n", "result.insert(2, \"High %\", ph)\nresult.insert(3, \"Low %\", pl)\n"], "link": "https://stackoverflow.com/questions/56976142/error-when-trying-to-insert-into-dataframe"}
{"id": 365, "q": "Setting subset of a pandas DataFrame by a DataFrame if a value matches", "d": "I think the easiest way to explain what I am trying to do is by showing an example: Given a DataFrame and a subset of a second DataFrame : I want to replace the NaN's from the second DataFrame by the first, BUT at the place where the ID matches, as I am not sure that the selected data will always be in the same order or if all IDs will be included. I know I could do it with a for and if loop, but I am wondering if there is a faster way. If an ID form the second DataFrame is not included in the first DataFrame the values should just stay as NaN's. Any help is highly appreciated.", "q_apis": "DataFrame DataFrame value DataFrame second DataFrame replace second DataFrame first at where all second DataFrame first DataFrame values", "io": " V_set V_reset I_set I_reset HRS LRS ID 0 0.599417 -0.658417 0.000021 -0.000606 84562.252849 1097.226787 1383.0 1 0.595250 -0.684708 0.000023 -0.000617 43234.544776 1144.445368 1384.0 2 0.621229 -0.710812 0.000026 -0.000625 51719.718749 1216.609759 1385.0 3 0.625292 -0.720104 0.000029 -0.000625 40827.993527 1209.966052 1386.0 4 0.634563 -0.735937 0.000029 -0.000641 46881.785573 1219.497465 1387.0 ... ... ... ... ... ... ... 1066 0.167521 0.000000 0.000581 0.000000 720.116614 708.098519 2811.0 1067 0.167360 0.000000 0.000581 0.000000 718.165882 708.284487 2812.0 1068 0.172812 0.000000 0.000278 0.000000 715.302620 708.167571 2813.0 1069 0.167729 0.000000 0.000581 0.000000 716.096291 708.333064 2814.0 1070 0.173037 0.000000 0.000278 0.000000 715.474310 707.980273 2815.0 <s> V_set V_reset I_set I_reset HRS LRS ID 1383 NaN NaN NaN NaN NaN NaN 1383.0 1384 NaN NaN NaN NaN NaN NaN 1384.0 1385 NaN NaN NaN NaN NaN NaN 1385.0 1386 NaN NaN NaN NaN NaN NaN 1386.0 1387 NaN NaN NaN NaN NaN NaN 1387.0 ... ... ... ... ... ... ... 2811 NaN NaN NaN NaN NaN NaN 2811.0 2812 NaN NaN NaN NaN NaN NaN 2812.0 2813 NaN NaN NaN NaN NaN NaN 2813.0 2814 NaN NaN NaN NaN NaN NaN 2814.0 2815 NaN NaN NaN NaN NaN NaN 2815.0 ", "apis": "set_index columns fillna map", "code": ["df.set_index('ID',inplace=True)\nfor column in df.columns:\n    df2[column] = df2[column].fillna(df2['ID'].map(df[column]))\n"], "link": "https://stackoverflow.com/questions/59786133/setting-subset-of-a-pandas-dataframe-by-a-dataframe-if-a-value-matches"}
{"id": 529, "q": "Select columns in panda&#39;s dataframe that have an integer header", "d": "I have a dataframe in pandas that looks like this: What I want to do is select specific columns from this data frame. But when I try the following code (the df_matrix being the dataframe displayed at the top) : It does not work and from what I can tell is because it is an integer. I tried to force it with str(100) but gave the same error as before: Does anyone know how to get around this? Thanks! EDIT 1: After trying to use it worked as expecte. Btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use: and the output will be:", "q_apis": "columns select columns at get select columns at time", "io": " 100 200 300 400 0 1 1 0 1 1 1 1 1 0 <s> 100 300 0 1 0 1 1 1 ", "apis": "columns columns", "code": ["df.columns = [str(x) for x in df.columns]"], "link": "https://stackoverflow.com/questions/54333620/select-columns-in-pandas-dataframe-that-have-an-integer-header"}
{"id": 436, "q": "Identical dictionaries in DataFrame all changing at once", "d": "I'm working on a project and currently experiencing an issue with populating some dictionaries within a DataFrame. The problem is more complicated but essentially the main issue can be simplified as follows: I have a DataFrame of dictionaries, all of which initially empty, say When I attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one I'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a': I'm assuming this behaviour is caused by using in my DataFrame initialization, but I'm not familiar enough with Python to understand why. Is Python creating one dictionary and passing references to it to populate the DataFrame? If so, how could I initialise it to create individual dictionaries? I found that I can create a deep copy of each dictionary before processing them, i.e. , but I'm curious if there is a way of doing it without resorting to that.", "q_apis": "DataFrame all at DataFrame DataFrame all empty add value item at all identical add value DataFrame DataFrame copy", "io": " 0 1 0 {} {} 1 {} {} 2 {} {} <s> 0 1 0 {'char': 'a'} {'char': 'a'} 1 {'char': 'a'} {'char': 'a'} 2 {'char': 'a'} {'char': 'a'} ", "apis": "DataFrame index DataFrame", "code": ["example = pd.DataFrame([[{} for _ in range(2)] for _ in range(3)], index=range(0, 3))", "example = pd.DataFrame([[{} for _ in range(n)] for _ in range(m)])"], "link": "https://stackoverflow.com/questions/57589538/identical-dictionaries-in-dataframe-all-changing-at-once"}
{"id": 48, "q": "How to add a value to a new column by referencing the values in a column", "d": "I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this?", "q_apis": "add value values value names at first value value value first", "io": "id reason x1 x2 x3 x4 x5 1 x1 100 15 10 20 25 2 x1 15 16 14 10 10 3 x4 10 50 40 30 25 4 x3 12 15 60 5 1 5 x1 80 15 10 20 25 6 x1 15 19 84 10 10 7 x4 90 40 90 30 25 8 x4 12 85 60 50 10 <s> id reason x1 x2 x3 x4 x5 xy 1 x1 100 15 10 20 25 100 2 x1 15 16 14 10 10 15 3 x4 10 50 40 30 25 30 4 x3 12 15 60 5 1 60 5 x1 80 15 10 20 25 80 6 x1 15 19 84 10 10 15 7 x4 90 40 90 30 25 30 8 x4 12 85 60 50 10 50 ", "apis": "apply", "code": ["df[\"xy\"] = df.apply(lambda x: x[x[\"reason\"]], axis=1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column"}
{"id": 44, "q": "Python pandas dataframe apply result of function to multiple columns where NaN", "d": "I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result:", "q_apis": "apply columns where columns values value values get apply mask get", "io": " x y z 0 a 1.0 2.0 1 b 1.0 2.0 2 c 1.0 2.0 3 d NaN NaN 4 e NaN NaN 5 f NaN NaN <s> x y z 0 a 1.0 2.0 1 b 1.0 2.0 2 c 1.0 2.0 3 d a1 a2 4 e b2 b1 5 f c3 c4 ", "apis": "fillna apply set_axis", "code": ["out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n"], "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan"}
{"id": 522, "q": "How to create a join in Dataframe based on the other dataframe?", "d": "I have 2 dataframes. One containing student batch details and another one with points. I want to join 2 dataframes. Dataframe1 contains Dataframe2 contains I am trying to map the mark in the same dataset for each student. I tried below code but it is replacing the values one by one.", "q_apis": "join join contains contains map values", "io": "+-------+-------+-------+--+ | s1 | s2 | s3 | | +-------+-------+-------+--+ | Stud1 | Stud2 | Stud3 | | | Stud2 | Stud4 | Stud1 | | | Stud1 | Stud3 | Stud4 | | +-------+-------+-------+--+ <s> +-------+-------+-------+----+----+----+ | Stud1 | Stud2 | Stud3 | 90 | 80 | 95 | | Stud2 | Stud4 | Stud1 | 80 | 55 | 90 | | Stud1 | Stud3 | Stud4 | 90 | 95 | 55 | +-------+-------+-------+----+----+----+ ", "apis": "set_index join replace add_prefix join apply map add_prefix", "code": ["s = dfnamepoints.set_index('Name')['Point']\ndf = df3.join(df3.replace(s).add_prefix('new_'))\n", "df = df3.join(df3.apply(lambda x: x.map(s)).add_prefix('new_'))\n"], "link": "https://stackoverflow.com/questions/54607098/how-to-create-a-join-in-dataframe-based-on-the-other-dataframe"}
{"id": 139, "q": "How to apply a function to every element in a dataframe?", "d": "This is probably a very basic question but I can't find the answer in other questions. I have two lists that I have used to create a 2D dataframe, let's say: Which gives: I would like to go through all elements in the dataframe and use the values of and as inputs to some function, , that I have written. For example, in the 2rd row, 1st column (using zero indexing) position I have , so in this position I would like to apply and not . I think I should be able to use or somehow but I can't figure it out!", "q_apis": "apply all values apply", "io": " 10.0 15.0 20.0 25.0 0.00 NaN NaN NaN NaN 0.25 NaN NaN NaN NaN 0.50 NaN NaN NaN NaN 0.75 NaN NaN NaN NaN 1.00 NaN NaN NaN NaN 1.25 NaN NaN NaN NaN 1.50 NaN NaN NaN NaN 1.75 NaN NaN NaN NaN 2.00 NaN NaN NaN NaN <s> (X, Y) = (0.5, 15.0)", "apis": "name index name index DataFrame index columns apply name index", "code": ["import numpy as np\nimport pandas as pd \n\ndef foo(name, index):\n    return name - index\n\nx = np.arange(0, 2.01, 0.25)\ny = np.arange(10, 30, 5.0) \n\ndf = pd.DataFrame(index = x, columns = y)\n\ndf.apply(lambda x: foo(x.name, x.index))\n"], "link": "https://stackoverflow.com/questions/65356414/how-to-apply-a-function-to-every-element-in-a-dataframe"}
{"id": 386, "q": "Groupby &amp; Sum from occurance of a particular value till the occurance of another particular value or the same value", "d": "I have a dataframe as below. I want to 'user' & 'eve' and 'Ses' till 100/200 & from 100 to 200. Also, return the value of column 'Name' where 100/200 occurs. If after an hundred, there is no 100 or 200 (like last row in group a & 123 or a & 456), ignore it. The expected output for the above input df is a df below.", "q_apis": "value value value value where last", "io": "User eve Ses ID Name a 123 1 10 a a 123 2 11 a a 123 3 12 a a 123 4 13 a a 123 3 100 xyz a 123 6 10 a a 456 1 11 a a 456 2 12 a a 456 3 13 a a 456 4 40 a a 456 1 100 mno a 456 14 10 a a 456 7 20 a a 456 8 30 a a 456 12 200 pqr a 456 10 10 a b 123 1 20 a b 123 2 30 a b 123 3 40 a b 123 4 50 a b 123 1 70 a b 123 6 100 abc b 888 1 20 a b 888 1 200 jkl b 888 3 10 a b 888 4 20 a b 888 5 30 a b 888 1 100 rrr b 888 7 50 a b 888 8 70 a <s> User eve Ses Name a 123 13 xyz a 456 11 mno a 456 41 pqr b 123 17 abc b 888 2 jkl b 888 13 rrr ", "apis": "isin mask where groupby bfill notnull groupby groupby shift cumsum agg sum last", "code": ["# valid IDs\ndf['valids'] = df['ID'].isin([100,200])\n\n# mask the trailing non-hundred ids\nheads = (df['ID'].where(df['valids'])\n             .groupby([df['User'],df['eve']])\n             .bfill().notnull()\n        )\ndf = df[heads]\n\n# groupby and output:\n(df.groupby(['User','eve', df['valids'].shift(fill_value=0).cumsum()],\n           as_index=False)\n   .agg({'Ses':'sum', 'Name':'last'})\n)\n"], "link": "https://stackoverflow.com/questions/59281644/groupby-sum-from-occurance-of-a-particular-value-till-the-occurance-of-another"}
{"id": 356, "q": "multiply two columns from two different pandas dataframes", "d": "I have a pandas.DataFrame. I have another pandas.DataFrame I want to apply df2['Price_factor'] to df1['Price'] column. I tried my code but it didn't work. Thank you for your help in advance.", "q_apis": "columns DataFrame DataFrame apply", "io": " df1 Year Class Price EL 2024 PC1 $243 Base 2025 PC1 $215 Base 2024 PC1 $217 EL_1 2025 PC1 $255 EL_1 2024 PC2 $217 Base 2025 PC2 $232 Base 2024 PC2 $265 EL_1 2025 PC2 $215 EL_1 <s> df2 Year Price_factor 2024 1 2025 0.98 ", "apis": "map set_index astype", "code": ["df1['Price_factor'] = df1['Year'].map(df2.set_index('Year')['Price_factor'])\ndf1['Price_adjusted']= df1['Price'].str.strip('$').astype(int) * df1['Price_factor']\ndf1\n"], "link": "https://stackoverflow.com/questions/60065499/multiply-two-columns-from-two-different-pandas-dataframes"}
{"id": 481, "q": "What is the most efficient way to create a dictionary of two pandas Dataframe columns?", "d": "What is the most efficient way to organise the following pandas Dataframe: data = into a dictionary like ?", "q_apis": "columns", "io": "Position Letter 1 a 2 b 3 c 4 d 5 e <s> alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']", "apis": "Series values index to_dict", "code": ["In [9]: pd.Series(df.Letter.values,index=df.Position).to_dict()\nOut[9]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}\n"], "link": "https://stackoverflow.com/questions/17426292/what-is-the-most-efficient-way-to-create-a-dictionary-of-two-pandas-dataframe-co"}
{"id": 575, "q": "Creating a function to perform grouping and sorting based on columns in Pandas dataframe and Labeling", "d": "I am wanting to group the data into two groups based on the Col2 group. However the first match should be assigned one value and the rest of the matches should be assigned a different value. Rahlf helped me to get a function created and then do However, I need two modifications to the function. Instead of the val, it will take the corresponding value from the Col 4 and then return one value (like 'low' to the first match within a group (based on the sorted col1) and then say 'low_red' for the rest of the matches in the group. So my question is how can I modify the function to do that? My Input: Expected Output:", "q_apis": "columns groups first value value get take value value first", "io": " Col1 Col2 Col3 Col4 100 m1 1 4 200 m2 7 5 120 m1 4 4 240 m2 8 5 300 m3 5 4 330 m3 2 4 350 m3 11 4 200 m4 9 4 <s> Col1 Col2 Col3 Col4 Col 5 100 m1 1 4 low 200 m2 7 5 med 120 m1 4 4 low_red 240 m2 8 5 med_red 300 m3 5 4 high 330 m3 2 4 high_red 350 m3 11 4 high_red 200 m4 9 4 high ", "apis": "iloc index value iloc iloc value value shape shape any iloc sort_values groupby transform", "code": ["def my_function(group):\n\n    val = df.iloc[group.index]['Col4']\n\n    value = deeper_logic(group.iloc[0], val.iloc[0], group)\n\n    return [value if i==0 else value + '_red' for i in range(group.shape[0])]\n\ndef deeper_logic(x, val, group):\n\n    if group.shape[0]==1:\n        if x>val:\n            return 'high'\n        else:\n            return 'low'\n\n    if x>val and any(i<=val for i in group.iloc[1:]):\n        return 'high'\n    elif x>val:\n        return 'med'\n    elif x<=val:\n        return 'low'\n    else:\n        return np.nan\n\ndf['Col5'] = df.sort_values(['Col2','Col1']).groupby('Col2')['Col3'].transform(my_function)\n"], "link": "https://stackoverflow.com/questions/52744334/creating-a-function-to-perform-grouping-and-sorting-based-on-columns-in-pandas-d"}
{"id": 597, "q": "creating list from dataframe", "d": "I am a newbie to python. I am trying iterate over rows of individual columns of a dataframe in python. I am trying to create an adjacency list using the first two columns of the dataframe taken from csv data (which has 3 columns). The following is the code to iterate over the dataframe and create a dictionary for adjacency list: and the following is the output I am getting: I see that I am not getting the entire list when I use the constructor. Hence I am not able to loop over the entire data. Could anyone tell me where I am going wrong? To summarize, Here is the input data: the output that I am expecting:", "q_apis": "columns first columns columns where", "io": "A,B,C 933,4139,20100313073721718 933,6597069777240,20100920094243187 933,10995116284808,20110102064341955 933,32985348833579,20120907011130195 933,32985348838375,20120717080449463 1129,1242,20100202163844119 1129,2199023262543,20100331220757321 1129,6597069771886,20100724111548162 1129,6597069776731,20100804033836982 <s> 933: [4139,6597069777240, 10995116284808, 32985348833579, 32985348838375] 1129: [1242, 2199023262543, 6597069771886, 6597069776731] ", "apis": "columns names groupby apply to_dict columns iloc groupby iloc apply to_dict", "code": ["#selecting by columns names\nd = df1.groupby('A')['B'].apply(list).to_dict()\n\n#seelcting columns by positions\nd = df1.iloc[:, 1].groupby(df1.iloc[:, 0]).apply(list).to_dict()\n"], "link": "https://stackoverflow.com/questions/51872125/creating-list-from-dataframe"}
{"id": 621, "q": "How do I sum time series data by day in Python? resample.sum() has no effect", "d": "I am new to Python. How do I sum data based on date and plot the result? I have a Series object with data like: I have the following code: This gives me the following line(?) graph: It's a start; now I want to sum the doses by date. However, this code fails to effect any change: The resulting plot is the same. What is wrong? I have also tried , , , but there is no change in the plot. Is even the correct function? I understand resampling to be sampling from the data, e.g. randomly taking one point per day, whereas I want to sum each day's values. Namely, I'm hoping for some result (based on the above data) like:", "q_apis": "sum time day resample sum sum date plot Series start now sum date any plot plot day sum day values", "io": "2017-11-03 07:30:00 NaN 2017-11-03 09:18:00 NaN 2017-11-03 10:00:00 NaN 2017-11-03 11:08:00 NaN 2017-11-03 14:39:00 NaN 2017-11-03 14:53:00 NaN 2017-11-03 15:00:00 NaN 2017-11-03 16:00:00 NaN 2017-11-03 17:03:00 NaN 2017-11-03 17:42:00 800.0 2017-11-04 07:27:00 600.0 2017-11-04 10:10:00 NaN 2017-11-04 11:48:00 NaN 2017-11-04 12:58:00 500.0 2017-11-04 13:40:00 NaN 2017-11-04 15:15:00 NaN 2017-11-04 16:21:00 NaN 2017-11-04 17:37:00 500.0 2017-11-04 21:37:00 NaN 2017-11-05 03:00:00 NaN 2017-11-05 06:30:00 NaN 2017-11-05 07:19:00 NaN 2017-11-05 08:31:00 200.0 2017-11-05 09:31:00 500.0 2017-11-05 12:03:00 NaN 2017-11-05 12:25:00 200.0 2017-11-05 13:11:00 500.0 2017-11-05 16:31:00 NaN 2017-11-05 19:00:00 500.0 2017-11-06 08:08:00 NaN <s> 2017-11-03 800 2017-11-04 1600 2017-11-05 1900 2017-11-06 NaN ", "apis": "read_csv plot date to_datetime loc plot Series loc values index name plot combine day resample sum plot", "code": ["import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')\n# plot bar graph of date and painkiller amount\ntimes = pd.to_datetime(df.loc[:,'Time'])\n\n# raw plot of data\nts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,\n               name = 'Painkiller over Time')\nfig1 = ts.plot()\n\n# combine data by day\ntest2 = ts.resample('D').sum()\nfig2 = test2.plot()\n"], "link": "https://stackoverflow.com/questions/50983386/how-do-i-sum-time-series-data-by-day-in-python-resample-sum-has-no-effect"}
{"id": 47, "q": "How can I add a new line in pandas dataframe based in a condition?", "d": "I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame.", "q_apis": "add first value second values value add add last last add DataFrame", "io": "[1] [2] [3] 1 30 2 1 30 1 1 30 3 1 90 3 1 370 3 1 430 3 1 705 3 1 805 3 1 880 2 1 905 3 1 1005 3 1 1170 3 1 1230 3 1 1970 3 1 2030 3 1 2970 3 1 3030 3 1 3970 3 1 4030 3 1 4423 3 1 4539 3 1 4575 3 1 4630 2 1 4635 3 1 4671 3 1 4787 3 1 4957 3 1 5057 3 1 5270 3 1 5330 3 1 5970 3 1 6030 3 1 6970 3 1 7030 3 1 7970 3 1 8030 3 1 8158 3 1 8257 3 1 8332 2 1 8357 3 1 8457 3 1 8970 3 1 9030 3 1 9970 3 1 10030 3 1 10970 3 1 11030 3 1 11470 3 1 11530 3 1 11853 3 1 11953 3 <s> 1 90 3 1 190 0 1 370 3 ", "apis": "diff loc Series iloc iloc index index index explode where apply astype mask diff mask mask idxmax loc Series iloc iloc index index index explode where apply astype", "code": ["m = df[\"[2]\"].diff() > 100\n\ndf.loc[m, \"[2]\"] = pd.Series(\n    [\n        [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(\"[2]\")\ndf[\"[3]\"] = np.where(\n    df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n)\ndf[\"[2]\"] = df[\"[2]\"].astype(int)\nprint(df)\n", "mask = df[\"[2]\"].diff() > 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, \"[2]\"] = pd.Series(\n        [\n            [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(\"[2]\")\n    df[\"[3]\"] = np.where(\n        df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n    )\n    df[\"[2]\"] = df[\"[2]\"].astype(int)\n    print(df)\n"], "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition"}
{"id": 359, "q": "Python: how to add a column to a pandas dataframe between two columns?", "d": "I would like to add a column to a dataframe between two columns in number labeled columns dataframe. In the following dataframe the first column corresponds to the index while the first row to the name of the columns. I have that I want to put between the columns and , so", "q_apis": "add between columns add between columns columns first index first name columns put between columns", "io": "df 0 0 1 2 3 4 5 1 6 7 4 5 2 1 2 0 3 1 3 3 4 3 9 8 4 3 6 2 <s> df 0 0 1 2 3 4 5 6 1 6 7 4 5 2 2 1 2 0 3 1 3 3 3 4 3 9 8 4 3 6 5 2 ", "apis": "insert", "code": ["df = df.insert(4, 'new_col_name', tmp)"], "link": "https://stackoverflow.com/questions/36437849/python-how-to-add-a-column-to-a-pandas-dataframe-between-two-columns"}
{"id": 458, "q": "How to append value_counts() output to the original df?", "d": "So I have the following : when I running this line: I get the following output: I was wonder how I could append this output to the original to make it look like this: Thank you very much for your help in advance.", "q_apis": "append value_counts get append", "io": " Open High Low Close 0 0.001268 0.001277 0.001266 0.001271 1 0.001268 0.001269 0.001265 0.001266 2 0.001265 0.001265 0.001242 0.001254 3 0.001253 0.001271 0.001244 0.001251 4 0.001253 0.001259 0.001249 0.001257 5 0.001257 0.001260 0.001241 0.001248 <s> 0.001253 2 0.001268 2 0.001265 1 0.001257 1 ", "apis": "value_counts merge", "code": ["s = df[\"Open\"].value_counts()\n\ndf.merge(s, left_on='Open', right_index=True,\n         suffixes=['','_count'])\n"], "link": "https://stackoverflow.com/questions/56813691/how-to-append-value-counts-output-to-the-original-df"}
{"id": 571, "q": "Pandas variable shifting within groups", "d": "I have a dataframe: I want to create a new field val2 such that each value in val2 is the value in val2 shifted by Lag number of rows. The tricky part here is that the shift should happen within the groups defined in field c1, such that the output looks something like I have been trying with along the lines of to no avail and getting a \"The truth value of a Series is ambiguous.\" error. Appreciate any help. Thanks!", "q_apis": "groups value value shift groups value Series any", "io": "c1 Lag Val1 A 3 10 A 1 5 A 2 20 A 2 15 A 1 10 B 1 25 B 2 10 <s> c1 Lag Val1 Val2 A 3 10 15 A 1 5 20 A 2 20 10 A 2 15 NaN A 1 10 NaN B 1 25 10 B 2 10 NaN ", "apis": "columns rename columns drop columns copy index index index index reset_index merge reset_index index left drop columns index", "code": ["# Copy and keep only the columns that are relevant\ndf2 = df.rename(columns={'Val1': 'Val2'}).drop(columns='Lag').copy()\n\n# Shift the index\ndf.index = df.index+df.Lag\n\n# Merge, requiring match on shifted index and within group.\ndf.reset_index().merge(df2.reset_index(), on=['index', 'c1'], how='left').drop(columns='index')\n"], "link": "https://stackoverflow.com/questions/52940317/pandas-variable-shifting-within-groups"}
{"id": 291, "q": "How can I create a new dataframe by subtracting the first column from every other column?", "d": "df1 is my original dataframe. I want to create another dataframe by substracting column a from every other column (taking the difference between column a and all other columns). df2 is the outcome. I would appreciate your help.", "q_apis": "first difference between all columns", "io": " a b c d e 0 1 3 5 7 9 1 1 3 5 7 9 2 1 3 5 7 9 <s> b c d e 0 2 4 6 8 1 2 4 6 8 2 2 4 6 8 ", "apis": "loc apply", "code": ["df = df1.loc[:,'b':].apply(lambda x: x-df1['a'])\nprint(df)\n"], "link": "https://stackoverflow.com/questions/61969568/how-can-i-create-a-new-dataframe-by-subtracting-the-first-column-from-every-othe"}
{"id": 407, "q": "Write pandas dataframe to_csv in columns with trailing zeros", "d": "I have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns). The complicating factor is I also want each column to be rounded to different number of decimals (some need much higher accuracy). To reproduce: Current result for out.txt: Desired:", "q_apis": "to_csv columns to_csv pad columns", "io": "0 1.0 3.0 5.0 1 1.5 3.455 5.45454 <s> 0 1.0 3.000 5.00000 1 1.5 3.455 5.45454 ", "apis": "to_string", "code": ["with open('out.txt', 'w') as file:\n    file.writelines(df_rounded.to_string(header=False))\n"], "link": "https://stackoverflow.com/questions/58649009/write-pandas-dataframe-to-csv-in-columns-with-trailing-zeros"}
{"id": 190, "q": "How to find the average of each cell in multiple csv&#39;s", "d": "I have several excel files with data in them in a format similar to this There are 3 csv's total, and I need to create a new csv with the average of each cell. So would be as follows So far I have the files imported but I am not sure how to proceed.", "q_apis": "", "io": "csv1 csv1 a b c a b c x 1 2 3 x 3 2 1 y 4 5 6 y 6 5 4 <s> a b c x (3+1)/2) (2+2)/2 (3+1)/2 y (6+4)/2 etc. ", "apis": "", "code": ["import numpy as np\ncsv1 = np.genfromtxt('my_file1.csv', delimiter=',')\ncsv2 = np.genfromtxt('my_file2.csv', delimiter=',')\nnp.savetxt(\"foo.csv\", (csv1+csv2)/2, delimiter=\",\")    \n"], "link": "https://stackoverflow.com/questions/64444765/how-to-find-the-average-of-each-cell-in-multiple-csvs"}
{"id": 281, "q": "Remove quotation marks and brackets from Pandas DataFrame .csv file after performing a GroupBy with MultiIndex", "d": "I'm new to pandas so apologies if my explanations of things are wrong. I have a data frame created as follows: Then I perform a weighted mean, using the indices, using code from the second top answer here. The output on the console looks like this: where (x,y) are the indices that I have grouped by and the number at the end is the weighted mean. When I export to a .csv file, I get a file that looks like this: This is not what I want. I would like to get a .csv file that looks like this: I've tried using reset.index() but this does not work. I want to remove the brackets, quotation marks and the rogue ,0 at the start of the .csv file. How can I do this? Many thanks in advance.", "q_apis": "DataFrame MultiIndex mean indices second where indices at mean get get index at start", "io": " (1, 2) 3 (4, 5) 6 (7, 8) 9 <s> ,0 \"(1, 2)\",3 \"(4, 5)\",6 \"(7, 8)\",9 ", "apis": "groupby index names apply rename reset_index to_csv index", "code": ["df2 = df.groupby(level=df.index.names).apply(lambda x: np.average(x.name3, weights=x.name4))\n\n# save the df2 to csv file\ndf2.rename('avg').reset_index().to_csv('data.csv', index=False)\n"], "link": "https://stackoverflow.com/questions/62302113/remove-quotation-marks-and-brackets-from-pandas-dataframe-csv-file-after-perfor"}
{"id": 536, "q": "Merge order with items in columns", "d": "I have a dataset with all the order, customer and orderitem information. I wandt to expand my orderitems in new columns, but without losing the information about the customer And the result should be somehow: I tried", "q_apis": "items columns all columns", "io": "CustomerId OrderId Item 1 1 CD 1 1 DVD 2 2 CD <s> CustomerId OrderId CD DVD 1 1 1 1 2 2 1 0 ", "apis": "set_index get_dummies max reset_index", "code": ["(df.set_index(['CustomerId', 'OrderId'])\n   .Item.str.get_dummies()\n   .max(level=[0, 1])\n   .reset_index())\n"], "link": "https://stackoverflow.com/questions/54037912/merge-order-with-items-in-columns"}
{"id": 633, "q": "Python pandas: apply on separated values", "d": "How can I sum values in dataframe that a separated by semicolon? Got: Need:", "q_apis": "apply values sum values", "io": " col1 col2 2018-03-05 2.1 8 2018-03-06 8 3.1;2 2018-03-07 1;1 8;1 <s> col1 col2 2018-03-05 2.1 8 2018-03-06 8 5.1 2018-03-07 2 9 ", "apis": "apply astype sum", "code": ["df = df.apply(lambda x: x.str.split(';', expand=True).astype(float).sum(axis=1))\n"], "link": "https://stackoverflow.com/questions/50409452/python-pandas-apply-on-separated-values"}
{"id": 378, "q": "Calculate percentage of similar values in pandas dataframe", "d": "I have one dataframe , with two columns : Script (with text) and Speaker And I have the following list : With the following code, I obtain this dataframe : Which line can I add in my code to obtain, for each line of my dataframe , a percentage value of all lines spoken by speaker, in order to have the following dataframe :", "q_apis": "values columns add value all", "io": "Speaker a b c Speaker 1 2 1 1 Speaker 2 2 0 0 Speaker 3 0 1 0 <s> Speaker a b c Speaker 1 50% 25% 25% Speaker 2 100% 0 0 Speaker 3 0 100% 0 ", "apis": "set_index join join get_dummies sum", "code": ["out = (df.set_index('Speaker')['Script'].str.findall('|'.join(L))\n         .str.join('|')\n         .str.get_dummies()\n         .sum(level=0))\n"], "link": "https://stackoverflow.com/questions/59502905/calculate-percentage-of-similar-values-in-pandas-dataframe"}
{"id": 421, "q": "Filter duplicate rows of a pandas DataFrame", "d": "I'm trying to filter the rows of a pandas DataFrame based on some conditions and I'm having difficulties with it. The DataFrame is like so: The selection I would like to apply is the following: For all cus_id that appear more than once (i.e. for all duplicates cus_id), keep only the ones where cus_group is equal to 1. Caution: If a cus_id appears more than once but it only belongs to group 0, we keep all instances of this customer. Visually, the resulting DataFrame I want is like so: As you can see for cus_id = 5555, even though it does appear twice, we keep both records since it only belongs to group 0. I have tried a few things using the duplicated() method but with no success. Any additional help is would be appreciated. EDIT: The solution provided by jezrael works perfectly for the example above. I have noticed that in the real DataFrame I'm using there are cases where customers are linked to group. For example: Using the solution of jezrael those customers are dropped. Is there a quick fix to keep ALL (duplicates included) such cases in the final DataFrame? Visually (after filtering):", "q_apis": "DataFrame filter DataFrame DataFrame apply all all where all DataFrame duplicated DataFrame where DataFrame", "io": " cus_id cus_group 0 1111 1 1 2222 1 2 3333 0 3 4444 1 4 4444 1 5 5555 0 6 5555 0 <s> cus_id cus_group 0 1111 1.0 1 2222 1.0 2 3333 0.0 3 4444 1.0 4 4444 1.0 5 5555 0.0 6 5555 0.0 7 6666 NaN 8 7777 NaN 9 7777 NaN ", "apis": "eq groupby transform all eq groupby transform nunique eq eq", "code": ["df = df[df['cus_group'].eq(0).groupby(df['cus_id']).transform('all') | df['cus_group'].eq(1)]\n", "df = df[df.groupby('cus_id')['cus_group'].transform('nunique').eq(1) | df['cus_group'].eq(1)]\n"], "link": "https://stackoverflow.com/questions/58198249/filter-duplicate-rows-of-a-pandas-dataframe"}
{"id": 648, "q": "Merge 2 CSV files with mapped values in another file separated by comma", "d": "here is my problem: I have tow csv files as follows: Book1.csv Book2.csv I want merge above files and get an output file as this: The code I am using right now is: what I get from this is : All the Attributes and Products are merged correctly. But what I want is merge Attibutes into one string and separate by comma (not line by line). How do I do this? Thank you in advance!", "q_apis": "values merge get right now get merge", "io": "Id Product 0 aaaa 1 bbbb 2 cccc 3 dddd <s> Id Attribute 0 aaad 0 sssd 1 fffd 1 gggd 1 cccd 2 bbbd 3 hhhd 3 bbbd ", "apis": "groupby apply join map", "code": ["g = df2.groupby('Id')['Attribute'].apply(', '.join)\ndf1['Attributes'] = df1['Id'].map(g)\n"], "link": "https://stackoverflow.com/questions/49274575/merge-2-csv-files-with-mapped-values-in-another-file-separated-by-comma"}
{"id": 61, "q": "Splitting by indices: I want to split the train + test from the data whose indices have been given. How shall I get train/test df?", "d": "for example= df is the data with features. I want to split the train + test from the data whose indices have been given. How shall I get train/test df. where train.txt is where in this dataframe indices are given. How should I get the training data from those indices? Contents in data_train.txt(there are 10000 of data in which train indices are given in this txt file) I want these indices for training data with feature:- like final train should look like this (see the index):", "q_apis": "indices test indices get test test indices get test where where indices get indices indices indices index", "io": "df= 0 2 0.3 0.5 0.5 1 4 0.5 0.7 0.4 2 2 0.5 0.1 0.4 3 4 0.4 0.1 0.3 4 2 0.3 0.1 0.5 <s> 0 2 0.3 0.5 0.5 2 2 0.5 0.1 0.4 4 2 0.3 0.1 0.5 ", "apis": "index iloc index loc isin", "code": ["#if you're trying to match the index of the df itself\ntrain_df = df.iloc[train_indices]\n#if you're trying to match column 0, which might be important \n#if it's not aligned to the index\ntrain_df =  df.loc[df[0].isin(train_indices)]\n"], "link": "https://stackoverflow.com/questions/66977794/splitting-by-indices-i-want-to-split-the-train-test-from-the-data-whose-indic"}
{"id": 178, "q": "How to modify numercial values in a column of mixed data types in a pandas dataframe?", "d": "I have a pandas dataframe in pyhton that looks like this (my actual dataframe is MUCH bigger than this): How can I perform some operations on the numerical values of specific columns. For example, multiply the numerical values of col_2 by 10 to get something like this: Although it looks like a simple task I couldn't find a solution for it anywhere on internet. Thanks in advance.", "q_apis": "values values columns values get", "io": " col_1 col_2 0 0.8 0.1 1 nope 0.6 2 0.4 0.7 3 nope nope <s> col_1 col_2 0 0.8 1 1 nope 6 2 0.4 7 3 nope nope ", "apis": "to_numeric", "code": ["In [141]: df.col_2 = pd.to_numeric(df.col_2, errors='coerce')\n"], "link": "https://stackoverflow.com/questions/64640182/how-to-modify-numercial-values-in-a-column-of-mixed-data-types-in-a-pandas-dataf"}
{"id": 29, "q": "Python dataframe create index column based on other id column", "d": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:", "q_apis": "index", "io": "ID Price 000afb96ded6677c 1514.5 000afb96ded6677c 13.0 000afb96ded6677c 611.0 000afb96ded6677c 723.0 000afb96ded6677c 2065.0 ffea14e87a4e1269 2286.0 ffea14e87a4e1269 1150.0 ffea14e87a4e1269 80.0 fff455057ad492da 650.0 fff5fc66c1fd66c2 450.0 <s> ID Price ID 2 000afb96ded6677c 1514.5 1 000afb96ded6677c 13.0 1 000afb96ded6677c 611.0 1 000afb96ded6677c 723.0 1 000afb96ded6677c 2065.0 1 ffea14e87a4e1269 2286.0 2 ffea14e87a4e1269 1150.0 2 ffea14e87a4e1269 80.0 2 fff455057ad492da 650.0 3 fff5fc66c1fd66c2 450.0 4 ", "apis": "groupby ngroup", "code": ["df['ID_2'] = df.groupby('ID').ngroup() + 1\n"], "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column"}
{"id": 542, "q": "Dataframe summary math based on condition from another dataframe?", "d": "I have what amounts to 3D data but can't install the Pandas recommended xarray package. df_values df_condition I know I can get the average of all values in like this. Question... \ud83d\udc47 What is the simplest way to find the where ?", "q_apis": "get all values where", "io": " | a b c ----------------- 0 | 5 9 2 1 | 6 9 5 2 | 1 6 8 <s> | a b c ----------------- 0 | y y y 1 | y n y 2 | n n y ", "apis": "concat concat values eq where values eq stack mean", "code": ["# Pandas 0.23.0, NumPy 1.14.3\nn = 10**5\ndf_values = pd.concat([df_values]*n, ignore_index=True)\ndf_condition = pd.concat([df_condition]*n, ignore_index=True)\n\n%timeit np.nanmean(df_values.values[df_condition.eq('y')])       # 32 ms\n%timeit np.nanmean(df_values.where(df_condition == 'y').values)  # 88 ms\n%timeit df_values[df_condition.eq('y')].stack().mean()           # 107 ms\n"], "link": "https://stackoverflow.com/questions/53953121/dataframe-summary-math-based-on-condition-from-another-dataframe"}
{"id": 135, "q": "Replace NaN Values with the Means of other Cols based on Condition", "d": "I have the following Pandas DataFrame I am writing the following function: I want to to replace the missing values present in columns with labels in the list . The value to be replaced is computed as the mean of the non missing values of the corresponding group. Groups are formed based on the values in the columns with labels in the list . When is applied to the above dataframe with arguments, it should yield: this is because the record on line 4 belongs to the group that has a mean of (1+3)/2 = 2. I tried using but it is giving me the error", "q_apis": "DataFrame replace values columns value mean values values columns mean", "io": " Col1 Col2 Col3 0 A c 1.0 1 A c 3.0 2 B c 5.0 3 A d 6.0 4 A c NaN <s> Col1 Col2 Col3 0 A c 1.0 1 A c 3.0 2 B c 5.0 3 A d 6.0 4 A c 2.0 ", "apis": "groupby transform mean fillna to_dict", "code": ["def replace_missing_with_conditional_mean(df, condition_cols, cols):\n    s = df.groupby(condition_cols)[cols].transform('mean')\n    return df.fillna(s.to_dict('series'))\n\n\nres = replace_missing_with_conditional_mean(df, ['Col1', 'Col2'], ['Col3'])\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65483406/replace-nan-values-with-the-means-of-other-cols-based-on-condition"}
{"id": 444, "q": "Drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe", "d": "I want to remove all rows (or take all rows without) a question mark symbol in any column. I also want to change the elements to float type. Input: Output: Preferably using pandas dataframe operations.", "q_apis": "value any all take all any", "io": "X Y Z 0 1 ? 1 2 3 ? ? 4 4 4 4 ? 2 5 <s> X Y Z 1 2 3 4 4 4 ", "apis": "all any", "code": ["df = df[(df != '?').all(axis=1)]\n", "df = df[~(df == '?').any(axis=1)]\n"], "link": "https://stackoverflow.com/questions/35682719/drop-rows-with-a-question-mark-value-in-any-column-in-a-pandas-dataframe"}
{"id": 373, "q": "Replace negatives with zeros in a dataframe column of lists", "d": "I have a dataframe containing two columns. The first column is the date index. Each row of the second column is a list of 60 numbers that include negative values. I want to replace all negative values in this column with zeros. Here is the complete data for the first two rows: Currently, my solution is to convert the column of lists into a separate df of 60 columns. I can then convert the negatives into zeros in this df. Although this does the job, the .apply() operation is slow (taking 1.3 minutes for a df with 400,000 rows). Could someone please offer a more efficient (faster) alternative?", "q_apis": "columns first date index second values replace all values first columns apply", "io": " Spc 1976-10-31 15:00:00 [0.0124, 0.0096, 0.0325, 0.1562, 0.4494, 0.738...-1., -1., -1., -1.] 1976-11-01 03:00:00 [0.0254, 0.0299, 0.0273, 0.1229, 0.596, 0.9833...-1., -1., -1., -1.] 1976-11-01 15:00:00 [0.0226, 0.0236, 0.0269, 0.085, 0.4163, 0.8011...-1., -1., -1., -1.] 1976-11-02 03:00:00 [0.0132, 0.0154, 0.0172, 0.1336, 0.4743, 0.694...-1., -1., -1., -1.] 1976-11-02 15:00:00 [0.0124, 0.0169, 0.028, 0.5028, 1.4503, 1.6055...-1., -1., -1., -1.] : : : : : : : : : : 2017-05-20 04:00:00 [5.374061e-13, 1.2720002e-06, 0.00052255474, 0...2.8157034e-03, 1.4578120e-03] 2017-05-20 04:30:00 [1.2021946e-12, 3.3477074e-06, 0.0014435094, 0...5.88221522e-03, 3.44922021e-03] 2017-05-20 05:00:00 [1.2236685e-13, 5.018357e-07, 0.00023753957, 0...2.28277827e-03, 1.07194704e-03] 2017-05-20 05:30:00 [3.5527579e-13, 1.1004944e-06, 0.0005480177, 0...2.0632602e-03, 1.6171171e-03] 2017-05-20 06:00:00 [4.968573e-13, 1.4969078e-06, 0.00065009575, 0...1.21051911e-03, 1.18123344e-03] <s> 1976-10-31 15:00:00 [ 0.0013, 0.0016, 0.007, 0.03, 0.0803, 0.2318, 0.5842, 0.8401, 0.6, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -1., -1., -1., -1., -1., -1., -1., -1., -1. ] 1976-11-01 03:00:00 [ 0.0022, 0.004, 0.0104, 0.0512, 0.1112, 0.2227, 0.5263, 0.7085, 0.4, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -1., -1., -1., -1., -1., -1., -1., -1., -1. ] ", "apis": "copy apply Series to_numpy copy map copy array array clip min copy stack to_numpy clip min copy array clip min", "code": ["def func_1(df_in):\n    df_in = df_in.copy()\n    temp = df_in['col_2'].apply(pd.Series)\n    temp[temp < 0] = 0\n    df_in['col_2'] = temp.to_numpy().tolist()\n    return df_in\n", "def func_2(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = df_in['col_2'].map(lambda l: [0 if elem < 0 else elem for elem in l])\n    return df_in\n", "def func_3(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array([np.array(elem) for elem in df_in['col_2']]).clip(min=0).tolist()\n    return df_in\n", "def func_4(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.stack(df_in['col_2'].to_numpy()).clip(min=0).tolist()\n    return df_in\n", "def func_5(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array(df_in['col_2'].tolist()).clip(min=0).tolist()\n    return df_in\n"], "link": "https://stackoverflow.com/questions/59594392/replace-negatives-with-zeros-in-a-dataframe-column-of-lists"}
{"id": 560, "q": "creating a pandas dataframe based on cell content of two other dataframes", "d": "I have wo dataframes with the same number of rows and columns. I would like to create a third dataframe based on these two dataframes that has the same dimensions as the other two dataframes. Each cell in the third dataframe should be the result by a function applied to the corresponding cell values in df1 and df2 respectively. i.e. if I have then df3 should be like this I have a way to do this that I do not think is very pythonic nor appropriate for large dataframes and would like to know if there is an efficient way to do such a thing? The function I wish to apply is: It can be used to produce a single scalar value OR an array of values. In my use case above the input to the function would be two scalar values. So smape(1, 5) = 0.66.", "q_apis": "columns values apply value array values values", "io": "df1 = | 1 | 2 | | 3 | 4 | df2 = | 5 | 6 | | 7 | 8 | <s> df3 = | func(1, 5) | func(2, 6) | | func(3, 7) | func(4, 8) | ", "apis": "abs abs DataFrame where eq eq", "code": ["def smape3(df1, df2):\n    return (df2 - df1).abs() / (df2 + df1).abs()\n\ndf = pd.DataFrame(np.where(df1.eq(0) & df2.eq(0), 0, smape3(df1, df2)))\n"], "link": "https://stackoverflow.com/questions/53327932/creating-a-pandas-dataframe-based-on-cell-content-of-two-other-dataframes"}
{"id": 566, "q": "functools reduce In-Place modifies original dataframe", "d": "I currently facing the issue that \"functools.reduce(operator.iadd,...)\" alters the original input. E.g. I have a simple dataframe df = pd.DataFrame([[['A', 'B']], [['C', 'D']]]) Applying the iadd operator leads to following result: Now, the original df changed to Also copying the df using df.copy(deep=True) beforehand does not help. Has anyone an idea to overcome this issue? THX, Lazloo", "q_apis": "DataFrame copy", "io": " 0 0 [A, B] 1 [C, D] <s> 0 0 [A, B, C, D] 1 [C, D] ", "apis": "sum", "code": ["In [39]: df[0].sum()\nOut[39]: ['A', 'B', 'C', 'D']\n"], "link": "https://stackoverflow.com/questions/53149552/functools-reduce-in-place-modifies-original-dataframe"}
{"id": 146, "q": "How do I apply a function to the groupby sub-groups that depends on multiple columns?", "d": "Take the following data frame and groupby object. How would I apply to the groupby object , multiplying each element of and together and then taking the sum. So for this example, for the group and for the group. So my desired output for the groupby object is:", "q_apis": "apply groupby sub groups columns groupby apply groupby sum groupby", "io": "2*3 + 4*5 = 26 <s> a f 0 1 26 2 2 30 ", "apis": "DataFrame columns groupby sum", "code": ["df = pd.DataFrame([[1, 2, 3],[1, 4, 5],[2, 5, 6]], columns=['a', 'b', 'c'])\ndf['f'] = df['c'] * df['b']\nres = df.groupby('a', as_index=False)['f'].sum()\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65178805/how-do-i-apply-a-function-to-the-groupby-sub-groups-that-depends-on-multiple-col"}
{"id": 267, "q": "Rename Columns in a Pandas Dataframe with values form dictionary", "d": "I have a pandas data frame read from an excel file. Note: the column names remain the same but the position of the column might vary in the excel file. df I have a list of dictionaries that should be used to change the column names, which is as below field_map I could convert the column keys for each row in the DataFrame separately in this way and using the for further operations. This method is taking too long when my file is large. I want to change the column headers of the data Frame before processing the entries further, this will reduce a lot of processing time for me. Kindly help me with this. I'm expecting the data frame to be something like this Expected df Thanks in Advance", "q_apis": "values names names keys DataFrame time", "io": " colA colB colC ... 0 val11 val12 val13 ... 1 val21 val22 val23 ... ... ... ... <s> tab1 tab2 tab3 ... 0 val11 val12 val13 ... 1 val21 val22 val23 ... ... ... ... ", "apis": "rename columns rename columns", "code": ["df = df.rename(columns={\"colA\":\"tab1\", \"colB\":\"tab2\", \"colB\":\"tab3\"})\n", "col_rename_dict = {el[\"file_field\"]:el[\"table_field\"] for el in field_map}\ndf = df.rename(columns=col_rename_dict)\n"], "link": "https://stackoverflow.com/questions/62811834/rename-columns-in-a-pandas-dataframe-with-values-form-dictionary"}
{"id": 602, "q": "pandas: assign random numbers in given range to equal column values", "d": "I am working with a large dataset, and one of the columns has very long integers, like below: What is important here is not the actual number in Column_2, but when those numbers are the same while Column_1 is different. I would like to reassign the values of Column_2 randomly from a range of smaller numbers, say (1, 999). My issue is figuring a way to describe in a lambda function that each equal value in Column_2 needs the same random number.", "q_apis": "assign values columns values describe value", "io": " Column_1 Column_2 1 A 12345123451 2 B 12345123451 3 C 12345123451 4 D 23456789234 5 E 23456789234 6 F 34567893456 <s> Column_1 Column_2 1 A 120 2 B 120 3 C 120 4 D 54 5 E 54 6 F 567 ", "apis": "array", "code": [">>> nums\narray([274, 842, 860])\n"], "link": "https://stackoverflow.com/questions/51735106/pandas-assign-random-numbers-in-given-range-to-equal-column-values"}
{"id": 312, "q": "Count how many cells are between the last value in the dataframe and the end of the row", "d": "I'm using the pandas library in Python. I have a data frame: Is it possible to create a new column that is a count of the number of cells that are empty between the end of the row and the last value above zero? Example data frame below:", "q_apis": "between last value count empty between last value", "io": " 0 1 2 3 4 0 0 0 0 1 0 1 0 0 0 0 1 2 0 0 1 0 0 3 1 0 0 0 0 4 0 0 1 0 0 5 0 1 0 0 0 6 1 0 0 1 1 <s> 0 1 2 3 4 Value 0 0 0 0 1 0 1 1 0 0 0 0 1 0 2 0 0 1 0 0 2 3 1 0 0 0 0 4 4 0 0 1 0 0 2 5 0 1 0 0 0 3 6 1 0 0 1 1 0 ", "apis": "value apply iloc argmax where iloc argmax", "code": ["df['value'] = df.apply(lambda x: (x.iloc[::-1] == 1).argmax(),1)\n\n##OR\n", "df['Value'] = np.where(df.iloc[:,::-1] == 1,True,False).argmax(1)\n"], "link": "https://stackoverflow.com/questions/61138851/count-how-many-cells-are-between-the-last-value-in-the-dataframe-and-the-end-of"}
{"id": 31, "q": "Sort pandas df subset of rows (within a group) by specific column", "d": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation?", "q_apis": "sub", "io": " A B C D E z k s 7 d z k s 6 l x t r 2 e x t r 1 x u c r 8 f u c r 9 h y t s 5 l y t s 2 o <s> A B C D E z k s 6 l z k s 7 d x t r 1 x x t r 2 e u c r 8 f u c r 9 h y t s 2 o y t s 5 l ", "apis": "sort_values", "code": ["df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"], "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column"}
{"id": 78, "q": "How to manipulate data cell by cell in pandas df?", "d": "Let the sample df (df1) be, We can achieve df2 or final data-frame by manipulating the data of df1 in the following manner, Step 1: Remove all positive numbers including zeros After Step 1 the sample data should look like, Step 2: If A row is a negative number and B is blank, then remove the -ve number of A row Step 3: If A row is blank and B is a negative number, then keep the -ve number of B row After Steps 1,2 and 3 are done, Step 4: If both A and B of are negative then, For each A and B row of , check the left-side (LHS) value (for a given month) of the same A and B row of Step 4.1: If either of the LHS values of A or B is a -ve number, then delete the current row value of B and keep the current row value of A After Step 4.1, the sample data should look like this, Step 4.2: If the LHS value of A and B is blank, then keep the current row value of B and delete the current row value of A Sample data after Step 4.2 should look like, Since we see two negative numbers still, we perform Step 4.1 again and then the final data-frame or df2 will look like, How may I achieve the above using pandas? I was able to achieve till Step 1 but have no idea as to how to proceed further. Any help would be greatly appreciated. This is the approach that I took, Small Test data: df1, df2 (expected output), Test data: df1 df2 (expected output) , Note: I have implemented my code on the basis of the Test data provided. The sample data is merely to focus on the columns that are supposed to be manipulated.", "q_apis": "sample all sample left value month values delete value value sample value value delete value sample columns", "io": "{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [0, 10, 0, 0], 'Mar-21': [0, 0, 70, 70], 'Apr-21': [-10, -10, -8, 60], 'May-21': [-30, -60, -10, 40], 'Jun-21': [-20, 9, -40, -20], 'Jul-21': [30, -10, 0, -20], 'Aug-21': [-30, -20, 0, -20], 'Sep-21': [0, -15, 0, -20], 'Oct-21': [0, -15, 0, -20]} <s> {'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan], 'Apr-21': [nan, -10.0, nan, nan], 'May-21': [-30.0, nan, nan, nan], 'Jun-21': [nan, nan, nan, -20.0], 'Jul-21': [nan, -10.0, nan, -20.0], 'Aug-21': [-30.0, nan, nan, -20.0], 'Sep-21': [nan, -15.0, nan, -20.0], 'Oct-21': [nan, -15.0, nan, -20.0]} ", "apis": "DataFrame unique index reset_index drop index loc index shift loc index columns index where fillna set_index drop index T values isnull values index loc T groupby index apply reset_index drop set_index drop index T isnull values values index loc loc T groupby index apply reset_index drop set_index drop index T columns get_loc columns get_loc step values values index index get_indexer iloc notnull any index index get_indexer iloc step values values index index get_indexer iloc isnull all index index get_indexer iloc step values values index index get_indexer iloc notnull any index index get_indexer iloc T groupby index apply reset_index drop", "code": ["df = pd.DataFrame(  ...  )\n\n#Compute the unique index for each pair of rows\ndf.reset_index(drop=False, inplace=True)\nix = df.index\nix = ix[ix%2==0]\ndf.loc[ix, 'index'] = df.shift(-1).loc[ix, 'index']\n\n#step1 :\ncols = [x for x in df.columns.tolist() if not x.startswith('column') and x != \"index\"]\ndf[cols] = df[cols].where(df[cols] < 0, np.nan)\n\n\ncols_index = [\"column4\", \"column1\", \"column2\", \"column3\", \"column5\", \"column6\", \"column7\"]\ndf[cols_index] = df[cols_index].fillna(-1)\n\n#step2 :\ndef step2(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            (df.A<=0).values\n            & df.B.isnull().values\n         ].index\n    df.loc[ix, \"A\"] = np.nan\n    return df.T\ndf = df.groupby('index').apply(step2)\nprint(df)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n\n#step3 :\ndef step3(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            df.A.isnull().values \n            & (df.B>=0).values\n            ].index\n    df.loc[ix, \"B\"] = df.loc[ix, \"A\"]\n    return df.T\ndf = df.groupby('index').apply(step3)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n#step4 :\ndef step4(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    a_pos = df.columns.get_loc('A')\n    b_pos = df.columns.get_loc('B')\n    \n    #step 4.1\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n    \n    #step 4.2\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].isnull().all(axis=1)    \n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, a_pos] = np.nan\n    \n    #step 4.1 (again)\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n        \n    return df.T\n\ndf = df.groupby('index').apply(step4)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n"], "link": "https://stackoverflow.com/questions/66422275/how-to-manipulate-data-cell-by-cell-in-pandas-df"}
{"id": 432, "q": "pandas how to drop rows when all float columns are NaN", "d": "I have the following df With the following dtypes Is there a way to drop rows only when ALL float columns are NaN? output: I can't do it with df.dropna(subset=['ID1','ID2','ID3','ID4']) because my real df has several dynamic floating columns. Thanks", "q_apis": "drop all columns dtypes drop columns dropna columns", "io": " AAA BBB CCC DDD ID1 ID2 ID3 ID4 0 txt txt txt txt 10 NaN 12 NaN 1 txt txt txt txt 10 NaN 12 13 2 txt txt txt txt NaN NaN NaN NaN <s> AAA BBB CCC DDD ID1 ID2 ID3 ID4 0 txt txt txt txt 10 NaN 12 NaN 1 txt txt txt txt 10 NaN 12 13 ", "apis": "dropna select_dtypes columns all dropna select_dtypes columns all", "code": ["df1 = df.dropna(subset=df.select_dtypes(float).columns, how='all')\n#for return same dataframe \n#df.dropna(subset=df.select_dtypes(float).columns, how='all', inplace=True)\n"], "link": "https://stackoverflow.com/questions/57842073/pandas-how-to-drop-rows-when-all-float-columns-are-nan"}
{"id": 105, "q": "Create DF Columns Based on Second DDF", "d": "I have 2 dataframes with different columns: I would like to add the missing columns for the 2 dataframes - so each one will have each own columns + the other DFs columns (without column \"number\"). And the new columns will have initial number for our choice (let's say 0). So the final output: What's the best way to achieve this result? I've got messed up with getting the columns and trying to create new ones. Thank!", "q_apis": "columns add columns columns columns columns columns", "io": "DF A - DF B - number | a | b | c |||| a | c | d | e | f 1 | 12 | 13 | 15 |||| 22 | 33 | 44 | 55 | 77 <s> DF A - number | a | b | c | d | e | f 1 | 12 | 13 | 15 | 0 | 0 | 0 DF B - a | b | c | d | e | f 22 | 0 | 33 | 44 | 55 | 77 ", "apis": "columns to_list columns to_list columns columns", "code": ["all_columns = list(set(A.columns.to_list() + B.columns.to_list()))\n", "col_missing_from_A = [col for col in all_columns if col not in A.columns]\ncol_missing_from_B = [col for col in all_columns if col not in B.columns]\n"], "link": "https://stackoverflow.com/questions/66075388/create-df-columns-based-on-second-ddf"}
{"id": 613, "q": "Convert list of dictionaries to dataframe with one column for keys and one for values", "d": "Let's suppose I have the following list: Which I want to convert it to a panda dataframe that have two columns: one for the keys, and one for the values. To do so, I have tried to use and also , but, in both cases, I get a dataframe like: Is there any way to specify what I want? By doing research I could only find the way I am describing above.", "q_apis": "keys values columns keys values get any", "io": "list1 = [{'a': 1}, {'b': 2}, {'c': 3}] <s> a b c 0 1.0 NaN NaN 1 NaN 2.0 NaN 2 NaN NaN 3.0 ", "apis": "items", "code": ["print ([(i, j) for a in list1 for i, j in a.items()])\n\n[('a', 1), ('b', 2), ('c', 3)]\n"], "link": "https://stackoverflow.com/questions/51186619/convert-list-of-dictionaries-to-dataframe-with-one-column-for-keys-and-one-for-v"}
{"id": 652, "q": "Merging/Combining Dataframes in Pandas", "d": "I have a df1, example: ,and a df2, example: The column and row 'C' is common in both dataframes. I would like to combine these dataframes such that I get, Is there an easy way to do this? pd.concat and pd.append do not seem to work. Thanks! Edit: df1.combine_first(df2) works (thanks @jezarel), but can we keep the original ordering?", "q_apis": "combine get concat append combine_first", "io": " C E D C 2 3 E 1 D 2 <s> B A C D E B 1 A 1 C 2 2 3 D 1 E 2 ", "apis": "columns append columns unique index append index unique combine_first reindex index columns", "code": ["c = df1.columns.append(df2.columns).unique()\ni = df1.index.append(df2.index).unique()\n\ndf = df1.combine_first(df2).reindex(index=i, columns=c)\n"], "link": "https://stackoverflow.com/questions/49493720/merging-combining-dataframes-in-pandas"}
{"id": 93, "q": "Reverse columns of dataframe based on the column name", "d": "I have a dataframe: I would like to reverse the columns that their column names have their 1st and 2nd letters reversed and their 3rd and 4th as-is. i.e. 1st col: 1000 \u2192 2nd col: 0100 3rd col: 0010 \u2192 5th col: 1110 4th col: 0001 \u2192 6th col: 1101 7th col: 1011 \u2192 8th col: 0111 I would like to have a dataframe like this: This is what I have for the reversion:", "q_apis": "columns name columns names", "io": " '1000' '0100' '0010' '0001' '1110' '1101' '1011' '0111' 0 0 1 2 3 4 5 6 7 1 00 11 22 33 44 55 66 77 <s> '0100' '1000' '1110' '1101' '0010' '0001' '1011' '0111' 0 1 0 4 5 2 3 7 6 1 11 00 44 55 22 33 77 66 ", "apis": "columns columns columns array bool columns", "code": ["df.columns = df.columns.str.strip(\"'\")\ncols = df.columns\n\narr = np.array([[bool(int(y)) for y in x] for x in df.columns])\nprint (arr)\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n"], "link": "https://stackoverflow.com/questions/66255574/reverse-columns-of-dataframe-based-on-the-column-name"}
{"id": 641, "q": "Cross reference list of ids to index", "d": "I have grouped together a list of ids that are associated with a certain value and placed all these lists of ids into a dataframe. It looks like this: (with index = id) I want to iterate through these lists and cross reference them to the id index where phase equals either a 2 or 3, then just keep the ids that match within the original list (or if not possible, create a new column with modified lists). Something like this below: If possible I'd like to do this within the dataframe object as there are multiple features/dependencies for each row. Any tips on how to go about this? My actual data: And the dtypes: And the good_ids output:", "q_apis": "index value all index index where equals dtypes", "io": " phase list_ids id a1 1 [a1,a2,c3] a2 3 [a1,b2,c3] b1 3 [a2,b2] b2 2 [b1,b2,c1] b3 3 [b2,c1] c1 1 [a1,a2,c3] c2 1 [a1,b1,c4] c3 2 [c1,c2,c4] c4 1 [c1,c2] <s> phase ids Study_id ACP-103-006 2.0 [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... ACP-103-008 2.0 [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... ACP-103-010 2.0 [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10... ACP-103-012 3.0 [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10... ACP-103-014 3.0 [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10... ", "apis": "isin index", "code": ["good_ids = set(df[df[\"phase\"].isin([2,3])].index)\nprint(good_ids)\n#{'a2', 'b1', 'b2', 'b3', 'c3'}\n"], "link": "https://stackoverflow.com/questions/49885060/cross-reference-list-of-ids-to-index"}
{"id": 406, "q": "Convert standard date format to string splitting by point in Python", "d": "One have one column which has the following format: How can I convert it to format six digits format? I have tried with the following code to but only get : But my desired output: Thank you.", "q_apis": "date get", "io": "0 2019/5/20 22:49:29 1 2019/5/20 23:18:23 2 2019/3/8 9:11:35 3 2019/3/8 9:19:58 4 2019/5/20 22:57:12 5 2019/3/8 9:06:41 <s> 0 19.05.20 1 19.05.20 2 19.03.08 3 19.03.08 4 19.05.20 5 19.03.08 ", "apis": "date date strftime date date to_datetime date strftime", "code": ["df['date'] = df['date'].dt.strftime('%y.%m.%d')\n\n# if your date is not datetime type:\n# df['date'] = pd.to_datetime(df['date']).dt.strftime('%y.%m.%d')\n"], "link": "https://stackoverflow.com/questions/58653299/convert-standard-date-format-to-string-splitting-by-point-in-python"}
{"id": 221, "q": "pandas drop diffrent rows with differing column values", "d": "I have DataFrame that looks like Input: I would like to remove rows from \"col1\" that share a common value in \"col2\" except values that are the same i.e. letter \"e\". I would like it to be where only one value in \"col1\" can = a unique one in \"col2\" The expected output would look something like... Output: What would be the process of doing this?", "q_apis": "drop values DataFrame value values where value unique", "io": " col1 col2 col3 0 a 1 1 1 b 3 2 2 c 3 3 3 d 2 4 4 e 6 5 5 e 6 6 <s> col1 col2 col3 0 a 1 1 3 d 2 4 4 e 6 5 5 e 6 6 ", "apis": "duplicated duplicated", "code": ["df[np.logical_or(~df.duplicated('col2', keep = False),df.duplicated('col1', keep = False)) ]\n"], "link": "https://stackoverflow.com/questions/63674555/pandas-drop-diffrent-rows-with-differing-column-values"}
{"id": 382, "q": "Pandas: How to return the rows where col value is greater than &#39;x&#39; in rolling window", "d": "I have a large df and I am trying find all rows where the value in a specific column is above a given number but within a window of say 3 rows and returning only the rows with the highest value over the given number. If I wanted to do this with the above example for column D, where the value must be above 11, the output would be. What would be the best way to go about this? I've tried: but can't find a way to include the greater than condition. Any help is appreciatted. Thanks!", "q_apis": "where value rolling all where value value where value", "io": "A B C D E 1 5 9 10 15 2 4 7 12 16 3 3 5 10 18 4 2 3 15 17 5 1 1 10 14 6 5 9 17 13 7 4 7 10 14 8 3 5 19 19 9 2 3 10 18 10 4 7 5 14 11 3 5 6 19 12 2 3 7 18 <s> A B C D E 2 4 7 12 16 6 5 9 17 13 8 3 5 19 19. ", "apis": "floor index groupby apply loc max max", "code": ["threshold = 11\nwindow = 3\ndf['r'] = np.floor(df.index / window)\nprint(df.groupby('r').apply(lambda x : (x.loc[x['D'] == x['D'].max() ,:]) if x['D'].max() > threshold else None))\n"], "link": "https://stackoverflow.com/questions/59371013/pandas-how-to-return-the-rows-where-col-value-is-greater-than-x-in-rolling-wi"}
{"id": 170, "q": "Pandas remove characters from index", "d": "I have the following dataframe: I want to remove the '-' character with the upper value in the index so I end up with the following dataframe: How do I do this?", "q_apis": "index value index", "io": " A 0-1.5 1 1.5-3.3 2 3.3-5.4 3 5.4-7.9 4 <s> A 0 1 1.5 2 3.3 3 5.4 4 ", "apis": "rename", "code": ["df = df.rename(lambda x: x.split('-')[0])\n"], "link": "https://stackoverflow.com/questions/64747375/pandas-remove-characters-from-index"}
{"id": 493, "q": "How to print rows if a list of values appear in any column of pandas dataframe", "d": "How to print rows if values appear in any column of pandas dataframe I would like to print all rows of a dataframe where I find some values from a list of values in any of the columns. The dataframe follows this structure: First: I have a Series of values with size 3 that I get from a combinatory of 6 different values. Second: I have a dataframe with 2143 rows. I want to check if in any of these rows, I have those three values in any sort of order in the columns. Gave me this: I just tried the query command and this is what I've got: df_ordered.query('_1 == 2 & _2 == 12') Now, I want to expand the same thing, but I want to look at all those columns and find any of those values. I also didn't know how to plug those series into a loop to find the values into the query statement. EDIT: I tried the command, but I have no ideia how to expand it to the 6 columns I have.", "q_apis": "values any values any all where values values any columns Series values size get values any values any columns query query at all columns any values values query columns", "io": "1476 13/03/2013 4 10 26 37 47 57 1475 09/03/2013 12 13 37 44 48 51 1474 06/03/2013 1 2 3 11 28 43 1473 02/03/2013 2 12 33 57 58 60 1472 27/02/2013 12 18 23 25 45 50 1471 23/02/2013 10 25 33 36 40 58 1470 20/02/2013 2 34 36 38 51 55 1469 16/02/2013 4 13 35 54 56 58 1468 13/02/2013 1 2 10 19 20 37 1467 09/02/2013 23 24 26 41 52 53 1466 06/02/2013 4 6 13 34 37 51 1465 02/02/2013 6 11 16 26 44 53 1464 30/01/2013 2 24 32 50 54 59 1463 26/01/2013 13 22 28 29 40 48 1462 23/01/2013 5 9 25 27 38 40 1461 19/01/2013 31 36 44 47 49 54 1460 16/01/2013 4 14 27 38 50 52 1459 12/01/2013 2 6 30 34 35 52 1458 09/01/2013 2 4 16 33 44 51 1457 05/01/2013 15 16 34 42 46 59 1456 02/01/2013 6 8 14 26 36 40 1455 31/12/2012 14 32 33 36 41 52 1454 22/12/2012 4 27 29 41 48 52 1453 20/12/2012 6 13 25 32 47 57 <s> 0 [(2, 12, 35), (2, 12, 51), (2, 12, 57), (2, 12... 1 [(12, 35, 51), (12, 35, 57), (12, 35, 58), (12... 2 [(35, 51, 57), (35, 51, 58), (35, 57, 58)] 3 [(51, 57, 58)] ", "apis": "array mask array values any mask", "code": ["import numpy as np\nfrom itertools import combinations\n\ninputlist = [2,12,35,51,57,58]\ncombined = np.array(list(combinations(inputlist, 3)))\n\nmask = (np.array([set(row).issuperset(c) for row in df.values for c in combined])\n        .reshape(len(df), -1).any(1))\n\nprint(df[mask])\n"], "link": "https://stackoverflow.com/questions/55777300/how-to-print-rows-if-a-list-of-values-appear-in-any-column-of-pandas-dataframe"}
{"id": 439, "q": "In a dataframe how can I count a specific value and then select the value with the highest count to create another dataframe?", "d": "I am looking for a way to select specific rows of data from a dataframe. Here is an example of the dataframe. I am looking for an output frame like this: Note, ID 006DE4E3 is not in the output because there the counts of the value was equal. Thank You!", "q_apis": "count value select value count select value", "io": "Id \\ Value 0 002D85EF 5 1 002D85EF 1 2 002D85EF 5 3 00557D1B 1 4 00557D1B 1 5 00557D1B 5 6 0063EAFB 5 7 0063EAFB 5 8 0063EAFB 5 9 006DE4E3 1 10 006DE4E3 5 11 006DE4E3 1 12 006DE4E3 5 <s> Id \\ Value 0 002D85EF 5 1 00557D1B 1 2 0063EAFB 5 ", "apis": "size DataFrame columns value groupby count nunique iloc groupby apply value dropna", "code": ["import numpy as np\nimport pandas as pd\na = np.random.randint(5, high=10, size=(20, 1))\nb = np.random.choice(['a', 'b', 'c', 'd'], 20)[:, None]\nc = pd.DataFrame(np.hstack([b,a]), columns=['id', 'value'])\n\n\ndef first_or_none(grp, col_name):\n    cnts = grp.groupby(col_name).count()\n    if len(cnts) == len(cnts.nunique()):\n        return None\n    else:\n        return grp.iloc[0]\n\nc.groupby(['id']).apply(first_or_none, 'value').dropna()\n"], "link": "https://stackoverflow.com/questions/57481676/in-a-dataframe-how-can-i-count-a-specific-value-and-then-select-the-value-with-t"}
{"id": 57, "q": "How to replace &#39;Zero&#39; by &#39;One&#39; for particular row in data frame", "d": "I've this dataframe:df1 I would like to Find the minimum value of last two entry of Variance row. I would like to last two entries and finding minimum , like in variance last two entries are 474.0 and 1101.0 and that should be added in Nan place. Output look like I've tried this code:", "q_apis": "replace value last last last", "io": "Variance 160244.0 37745.0 42003.0 15082.0 13695.0 89.0 474.0 1101.0 NaN -0.0 <s> Variance 160244.0 37745.0 42003.0 15082.0 13695.0 89.0 474.0 1101.0 474.0 -0.0 ", "apis": "iloc iloc min index get_loc iloc iloc min loc columns loc columns min", "code": ["df1.iloc[-2, -2] = df1.iloc[-2, -4:-2].min()\n", "pos = df1.index.get_loc('Variance')\ndf1.iloc[pos, -2] = df1.iloc[pos, -4:-2].min()\n", "df1.loc['Variance', df1.columns[-2]] = df1.loc['Variance', df1.columns[-4:-2]].min()\n"], "link": "https://stackoverflow.com/questions/67053308/how-to-replace-zero-by-one-for-particular-row-in-data-frame"}
{"id": 331, "q": "Need help getting the frequency of each number in a pandas dataframe", "d": "I am trying to find a simple way of converting a pandas dataframe into another dataframe with frequency of each feature. I'll provide an example of what I'm trying to do below Current dataframe example (feature labels are just index values here): Dataframe I would like to convert this to: As you can see, the column label corresponds to the possible numbers within the dataframe and each frequency of that number per row is put into that specific feature for the row in question. Is there a simple way to do this with python? I have a large dataframe that I am trying to transform into a dataframe of frequencies for feature selection. If any more information is needed I will update my post.", "q_apis": "index values put transform any update", "io": " 0 1 2 3 4 ... n 0 2 3 1 4 2 ~ 1 4 3 4 3 2 ~ 2 2 3 2 3 2 ~ 3 1 3 0 3 2 ~ ... m ~ ~ ~ ~ ~ ~ <s> 0 1 2 3 4 ... n 0 0 1 2 1 1 ~ 1 0 0 1 2 2 ~ 2 0 0 3 2 0 ~ 3 1 1 1 2 0 ~ ... m ~ ~ ~ ~ ~ ~ ", "apis": "T melt crosstab value", "code": ["df2 = df.T.melt()\npd.crosstab(df2['variable'], df2['value'])\n"], "link": "https://stackoverflow.com/questions/60728016/need-help-getting-the-frequency-of-each-number-in-a-pandas-dataframe"}
{"id": 224, "q": "Round in pandas column scientific numbers", "d": "Hello I have a df such as : I knwo how to roun the digit in the COL2 by using but if I do the same for COL1 it only displays 0 how can I get instead: in fact the big issue is here: it displays : and I would like to keep only 3 number after the coma", "q_apis": "get", "io": "COL1 COL2 0.005554 0.35200000000000004 5.622e-11 0.267 0.006999999999999999 0.307 2.129e-14 0.469 2.604e-14 0.39 1.395e-60 0.27899999999999997 8.589999999999998e-74 0.29600000000000004 1.025e-42 0.4270000000000001 <s> COL1 COL2 0.005 0.352 5.622e-11 0.267 0.007 0.307 2.129e-14 0.469 2.604e-14 0.39 1.395e-60 0.279 8.560e-74 0.296 1.025e-42 0.427 ", "apis": "", "code": ["pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"], "link": "https://stackoverflow.com/questions/63616688/round-in-pandas-column-scientific-numbers"}
{"id": 555, "q": "Modify and flatten values from Pandas dataframe", "d": "Here is the dataframe I am working with: dtypes gives this: You can get a sample of the data by click on the link below: https://ufile.io/x534q What I would like to do now is to get rid of the header, the first column (0 to 6) and to flatten the rest of values so that the end result looks like this: Could you please help me? Thanks in advance.", "q_apis": "values dtypes get sample now get first values", "io": " 0 0 380.143752 1 379.942595 2 379.589472 3 379.816187 4 379.622086 5 379.299071 6 379.559615 <s> 380.143752 379.942595 379.589472 379.816187 379.622086 379.299071 379.559615 ", "apis": "to_csv mode index any", "code": ["newdf.to_csv(file,mode='w', sep=',',header=False,index=False) # you can use any separator here 'tabs' if you don't want commas.\n"], "link": "https://stackoverflow.com/questions/53592186/modify-and-flatten-values-from-pandas-dataframe"}
{"id": 581, "q": "Split pandas dataframe into multiple dataframes based on null columns", "d": "I have a pandas dataframe as follows: Is there a simple way to split the dataframe into multiple dataframes based on non-null values?", "q_apis": "columns values", "io": " a b c 0 1.0 NaN NaN 1 NaN 7.0 5.0 2 3.0 8.0 3.0 3 4.0 9.0 2.0 4 5.0 0.0 NaN <s> a 0 1.0 b c 1 7.0 5.0 a b c 2 3.0 8.0 3.0 3 4.0 9.0 2.0 a b 4 5.0 0.0 ", "apis": "dropna groupby isnull dot columns", "code": ["d = {y : x.dropna(1) for y, x in df.groupby(df.isnull().dot(df.columns))}\n"], "link": "https://stackoverflow.com/questions/52502179/split-pandas-dataframe-into-multiple-dataframes-based-on-null-columns"}
{"id": 434, "q": "pandas Integers to negative integer powers are not allowed", "d": "I have a , I want to create a new column based on the following calculation: but I got the following error, I am wondering how to get around this, so the result will look like,", "q_apis": "get", "io": "decimal_places amount 2 10 3 100 1 1000 <s> decimal_places amount converted_amount 2 10 10 3 100 10 1 1000 10000 ", "apis": "astype assign fillna", "code": ["df = df.astype(float)\ndf = (df.assign(\n      converted_amount=lambda x: x.amount * 10 ** (2 - x.decimal_places.fillna(2))))\n"], "link": "https://stackoverflow.com/questions/57726939/pandas-integers-to-negative-integer-powers-are-not-allowed"}
{"id": 229, "q": "How to convert a pandas dataframe column from string to an array of floats?", "d": "I have a dataframe where a column is an array of floats. When I am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows: I want to convert this long character string into an array of floats like this: Is there a way to do that?", "q_apis": "array where array array", "io": "'[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0]', '[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0]',.... <s> [4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0], [13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0],... ", "apis": "apply", "code": ["from ast import literal_eval    \ndf[\"a\"] = df[\"a\"].apply(lambda x: literal_eval(x))\n"], "link": "https://stackoverflow.com/questions/63479071/how-to-convert-a-pandas-dataframe-column-from-string-to-an-array-of-floats"}
{"id": 442, "q": "Pandas concatenate levels in multiindex", "d": "I do have following excel file: I would like to create following dataframe: What I tried: The new dataframe: This approach works but is kind of tedious: Which gives me: Is there a simpler solution available ?", "q_apis": "levels", "io": "{0: {0: nan, 1: nan, 2: nan, 3: 'A', 4: 'A', 5: 'B', 6: 'B', 7: 'C', 8: 'C'}, 1: {0: nan, 1: nan, 2: nan, 3: 1.0, 4: 2.0, 5: 1.0, 6: 2.0, 7: 1.0, 8: 2.0}, 2: {0: 'AA1', 1: 'a', 2: 'ng/mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 3: {0: 'AA2', 1: 'a', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 4: {0: 'BB1', 1: 'b', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 5: {0: 'BB2', 1: 'b', 2: 'mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 6: {0: 'CC1', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 7: {0: 'CC2', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}} <s> AA1 AA2 CB1 BB2 CC1 CC2 a a b b c c ng/mL N/A N/A mL N/A N/A 0 1 A 1 1 1 1 1 1 1 2 1 1 1 1 1 1 B 1 1 1 1 1 1 1 2 1 1 1 1 1 1 C 1 1 1 1 1 1 1 2 1 1 1 1 1 1 ", "apis": "read_excel set_index columns rename_axis read_excel set_index columns rename_axis columns get_level_values columns get_level_values columns get_level_values where columns to_series", "code": ["df = pd.read_excel('file.xlsx', header=[0,1,2])\n\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n", "df = pd.read_excel('file.xlsx', header=[0,1,2])\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n\nlv1 = df.columns.get_level_values(0)\nlv2 = df.columns.get_level_values(1)\nlv3 = df.columns.get_level_values(2)\nlv3 = lv3.where(~lv3.str.startswith('Unnamed'),'N/A')\n\ndf.columns = [lv1, lv2.to_series() + ' ' + lv3]\n"], "link": "https://stackoverflow.com/questions/57411679/pandas-concatenate-levels-in-multiindex"}
{"id": 81, "q": "Repeating single DataFrame with changing DateTimeIndex", "d": "Let's say I have very simple DataFrame like this: Output: I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this: It's still the same DataFrame, repeating again and again, and year is incrementally changed. I could do something like this (example for 3 years): I have mainly two questions: Is there a way how to do this in a single command? What is the best way how to deal with leap-year?", "q_apis": "DataFrame DataFrame take DataFrame append DataFrame year index DataFrame year year", "io": " A B C D 2010-01-31 6 0 8 10 2010-02-28 7 8 10 3 2010-03-31 10 5 8 10 2010-04-30 4 4 9 7 2010-05-31 2 3 0 11 2010-06-30 8 7 10 8 2010-07-31 11 9 0 4 2010-08-31 0 3 8 6 2010-09-30 4 6 7 9 2010-10-31 1 0 11 9 2010-11-30 5 4 8 4 2010-12-31 1 4 5 1 <s> A B C D 2010-01-31 6 0 8 10 2010-02-28 7 8 10 3 2010-03-31 10 5 8 10 2010-04-30 4 4 9 7 2010-05-31 2 3 0 11 2010-06-30 8 7 10 8 2010-07-31 11 9 0 4 2010-08-31 0 3 8 6 2010-09-30 4 6 7 9 2010-10-31 1 0 11 9 2010-11-30 5 4 8 4 2010-12-31 1 4 5 1 2011-01-31 6 0 8 10 2011-02-28 7 8 10 3 2011-03-31 10 5 8 10 2011-04-30 4 4 9 7 2011-05-31 2 3 0 11 2011-06-30 8 7 10 8 2011-07-31 11 9 0 4 2011-08-31 0 3 8 6 2011-09-30 4 6 7 9 2011-10-31 1 0 11 9 2011-11-30 5 4 8 4 2011-12-31 1 4 5 1 2012-01-31 6 0 8 10 2012-02-28 7 8 10 3 2012-03-31 10 5 8 10 2012-04-30 4 4 9 7 2012-05-31 2 3 0 11 2012-06-30 8 7 10 8 2012-07-31 11 9 0 4 2012-08-31 0 3 8 6 2012-09-30 4 6 7 9 2012-10-31 1 0 11 9 2012-11-30 5 4 8 4 2012-12-31 1 4 5 1 ", "apis": "concat set_index index year", "code": ["data_new = pd.concat([\n    df.set_index(df.index + pd.DateOffset(year=x)) for x in [2010, 2011, 2012]])\n"], "link": "https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex"}
{"id": 394, "q": "How to find the correlation between a group of values in a pandas dataframe column", "d": "I have a dataframe df: I want to find the pearson correlation coefficient value between and for every So the result should look like this: update: Must make sure all columns of variables are or", "q_apis": "between values value between update all columns", "io": "ID Var1 Var2 1 1.2 4 1 2.1 6 1 3.0 7 2 1.3 8 2 2.1 9 2 3.2 13 <s> ID Corr_Coef 1 0.98198 2 0.97073 ", "apis": "groupby corr eq reset_index drop rename reset_index", "code": ["df_out = df.groupby('ID').corr()\n(df_out[~df_out['Var1'].eq(1)]\n          .reset_index(1, drop=True)['Var1']\n          .rename('Corr_Coef')\n          .reset_index())\n"], "link": "https://stackoverflow.com/questions/45064916/how-to-find-the-correlation-between-a-group-of-values-in-a-pandas-dataframe-colu"}
{"id": 659, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe which look like the following: I would like to add a column that gives me something like a summary of Null values. So I need a command which gives me for every row which columns are NULL. Something like this: I could not find anything which satisfies my need on the internet.", "q_apis": "get columns add values columns", "io": " a b c NaN 2 165 NaN 9 NaN NaN NaN NaN 15 15 NaN 5 NaN 11 <s> a b c Summary NaN 2 165 a NaN 9 NaN a + c NaN NaN NaN a + b + c 15 15 NaN c 5 NaN 11 b ", "apis": "DataFrame columns apply join", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[np.nan, 2, 165], [np.nan, 9, np.nan], [np.nan, np.nan, np.nan],\n                   [15, 15, np.nan], [5, np.nan, 11]], columns=['a', 'b', 'c'])\n\ndf['Errors'] = df.apply(lambda row: ' + '.join(i for i in ['a', 'b', 'c'] if np.isnan(row[i])), axis=1)\n"], "link": "https://stackoverflow.com/questions/48563186/python-dataframe-get-the-nan-columns-for-each-row"}
{"id": 585, "q": "How to get the top frequency elements after grouping by columns?", "d": "I have a DataFrame named , and I want to count the top frequency elements in column , and on different . As you see, the of both and is . For , appears the most in column , and , appears the second most. So for and , the most frequency element is , and the second most is .", "q_apis": "get columns DataFrame count second second", "io": "df id app_0 app_1 app_2 sex 0 1 a b c 0 1 2 b c b 0 2 3 c d a 1 3 4 d NaN a 1 <s> df id app_0 app_1 app_2 sex top_1 top_2 0 1 a b c 0 b c 1 2 b c b 0 b c 2 3 c d a 1 a d 3 4 d NaN a 1 a d ", "apis": "stack value_counts Series index index index", "code": ["def f(x):\n    s = x.stack().value_counts()\n    return pd.Series([s.index[0], s.index[1]], index=['top_1','top_2'])\n"], "link": "https://stackoverflow.com/questions/52439222/how-to-get-the-top-frequency-elements-after-grouping-by-columns"}
{"id": 71, "q": "add a string prefix to each value in a string column using Pandas", "d": "I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly). I already figured out how to kind-of do this and I am currently using: This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)? In case this is yet unclear, I would like to turn: into:", "q_apis": "add value append start value any where", "io": " col 1 a 2 0 <s> col 1 stra 2 str0 ", "apis": "astype", "code": ["df['col'] = 'str' + df['col'].astype(str)\n"], "link": "https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas"}
{"id": 589, "q": "Rearranging python data frame index and columns", "d": "I want to convert this dataframe (note that 'ABC' is the index name): to this dataframe: What's the best way to perform this function?", "q_apis": "index columns index name", "io": " t1 t2 t3 ABC gp 7 11 26 fp 6 14 23 pm 3 -1 7 wm 2 -2 9 <s> s1 tx gp fp pm wm 0 ABC t1 7 6 3 2 1 ABC t2 11 14 -1 -2 2 ABC t3 26 23 7 9 ", "apis": "T reset_index rename columns index", "code": ["df = df.T\ndf.reset_index(inplace=True)\ndf.rename(columns={\"index\":\"tx\"}, inplace=True)\ndf[\"s1\"] = \"ABC\"\n"], "link": "https://stackoverflow.com/questions/52155125/rearranging-python-data-frame-index-and-columns"}
{"id": 323, "q": "Pandas: Reshaping dataframe", "d": "I have a panda's related question. My dataframe looks something like this: I want to transform it into something like: I thought of something like adding a sub_id column that is enumerated cyclically by a, b and c and then do an unstack of the frame. Is there an easier/smarter solution? Thanks a lot! Tim", "q_apis": "transform unstack", "io": " id val1 val2 0 1 0 1 1 1 1 0 2 1 0 0 3 2 1 1 4 2 1 1 5 2 1 0 6 3 0 0 7 3 0 1 8 3 1 1 9 4 1 0 10 4 0 1 11 4 0 0 <s> a b c id a0 a1 b0 b1 c0 c1 1 0 1 1 0 0 0 2 1 1 1 1 1 0 3 0 0 1 1 1 1 4 1 0 0 1 0 0 ", "apis": "set_index groupby apply Series values columns MultiIndex from_tuples", "code": ["res = df.set_index('id').groupby('id').apply(\n    lambda grp: pd.Series(grp.values.flatten()))\n", "res.columns = pd.MultiIndex.from_tuples(\n    [(x, x + y) for x in list('abc') for y in list('01')])\n"], "link": "https://stackoverflow.com/questions/60968943/pandas-reshaping-dataframe"}
{"id": 208, "q": "data frame to file.txt python", "d": "I have this dataframe I want to save it as a text file with this format I tried this code but is not working:", "q_apis": "", "io": " X Y Z Value 0 18 55 1 70 1 18 55 2 67 2 18 57 2 75 3 18 58 1 35 4 19 54 2 70 <s> X Y Z Value 18 55 1 70 18 55 2 67 18 57 2 75 18 58 1 35 19 54 2 70 ", "apis": "values to_csv index mode", "code": ["np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter=\"\\t\", header=\"X\\tY\\tZ\\tValue\")  \n", "a.to_csv('xgboost.txt', header=True, index=False, sep='\\t', mode='a')\n"], "link": "https://stackoverflow.com/questions/41428539/data-frame-to-file-txt-python"}
{"id": 630, "q": "Merge dataframes including extreme values", "d": "I have 2 data frames, df1 and df2: I want to merge dataframes but at the same time including the first and/or last value of the set in column A. This is an example of the desired outcome: I'm trying to use but that only slice the portion of data frames that coincides. Someone have an idea to deal with this? thanks!", "q_apis": "values merge at time first last value", "io": "df1 Out[66]: A B 0 1 11 1 1 2 2 1 32 3 1 42 4 1 54 5 1 66 6 2 16 7 2 23 8 3 13 9 3 24 10 3 35 11 3 46 12 3 51 13 4 12 14 4 28 15 4 39 16 4 49 df2 Out[80]: B 0 32 1 42 2 13 3 24 4 35 5 39 6 49 <s> df3 Out[93]: A B 0 1 2 1 1 32 2 1 42 3 1 54 4 3 13 5 3 24 6 3 35 7 3 46 8 4 28 9 4 39 10 4 49 ", "apis": "merge left eval groupby apply rolling max astype bool", "code": ["df[df.merge(df2, on='B', how='left', indicator='Ind').eval('Found=Ind == \"both\"')\n     .groupby('A')['Found']\n     .apply(lambda x: x.rolling(3, center=True, min_periods=2).max()).astype(bool)]\n"], "link": "https://stackoverflow.com/questions/50592595/merge-dataframes-including-extreme-values"}
{"id": 113, "q": "How can I use split() in a string when broadcasting a dataframe&#39;s column?", "d": "Take the following dataframe: Result: I need to create a 3rd column (broadcasting), using a condition on , and splitting the string on . This is ok to do: Result: But I need to specify dynamic indexes to split the string on , instead of (5, 8). When I try to run the following code it does not work, because is treated as a : I'm spending a huge time trying to solve this without needing to iterate the dataframe.", "q_apis": "time", "io": " col_1 col_2 0 0 here 123 1 1 here 456 <s> col_1 col_2 col_3 0 0 here 123 NaN 1 1 here 456 456 ", "apis": "values", "code": ["df['col_3']=[y.split(' ')[1] if x==1 else float('nan') for x,y in df[['col_1','col_2']].values]\n"], "link": "https://stackoverflow.com/questions/65893903/how-can-i-use-split-in-a-string-when-broadcasting-a-dataframes-column"}
{"id": 415, "q": "using a dictionary to modify the dfs values", "d": "I have a df like this: Then I have a dictionary with some keys (which correspond to the index names of the df) and values (column names): I would like to use the dictionary to check that those column names that do not appear in the dict values , are set to zero to generate this output: How could I use the dictionary to generate the desired output?", "q_apis": "values keys index names values names names values", "io": " xx yy zz A 6 5 2 B 4 4 5 B 5 6 7 C 6 6 6 C 7 7 7 <s> xx yy zz A 6 0 0 B 0 4 5 B 0 6 7 C 6 0 6 C 7 0 7 ", "apis": "mask DataFrame values index keys stack reset_index drop get_dummies groupby sum astype bool", "code": ["mask = (pd.DataFrame(d.values(), index=d.keys())\n          .stack()\n          .reset_index(level=1, drop=True)\n          .str.get_dummies()\n          .groupby(level=0).sum()\n          .astype(bool)\n        )\n"], "link": "https://stackoverflow.com/questions/58402008/using-a-dictionary-to-modify-the-dfs-values"}
{"id": 404, "q": "How can I remove columns of pandas dataframe conditional on last row values?", "d": "Given a data-frame like: I would like to remove the columns in which the value of the last row is less than (<) a constant X, say X = 25. In this example It would remove the column B only and the output would be: Thanks", "q_apis": "columns last values columns value last", "io": " A B C 2019-11-02 120 25 11 2019-11-03 119 28 15 2019-11-04 115 23 18 2019-11-05 119 30 20 2019-11-06 121 32 25 2019-11-07 117 24 30 <s> A C 2019-11-02 120 11 2019-11-03 119 15 2019-11-04 115 18 2019-11-05 119 20 2019-11-06 121 25 2019-11-07 117 30 ", "apis": "drop columns columns iloc lt select last iloc dtype columns value iloc lt dtype bool select columns iloc lt Index dtype", "code": ["df = df.drop(columns = df.columns[df.iloc[-1].lt(25)])\n", "# select last row\n\ndf.iloc[-1]\n\nA    117\nB     24\nC     30\nName: 2019-11-07, dtype: int64\n", "# check which columns have value < 25:\n\ndf.iloc[-1].lt(25)\n\nA    False\nB     True\nC    False\nName: 2019-11-07, dtype: bool\n", "# select those column(s) with boolean indexing:\n\ndf.columns[df.iloc[-1].lt(25)]\n\nIndex(['B'], dtype='object')\n"], "link": "https://stackoverflow.com/questions/58758098/how-can-i-remove-columns-of-pandas-dataframe-conditional-on-last-row-values"}
{"id": 255, "q": "Dask equivalent to pandas.DataFrame.update", "d": "I have a few functions that are using method, and I'm trying to move into using instead for the datasets, but the Dask Pandas API doesn't have the method implemented. Is there an alternative way to get the same result in ? Here are the methods I have using : Forward fills data with last known value input output Replaces values in a dataframe with values from another dataframe based on an id/index column input df1 df2 output", "q_apis": "DataFrame update get last value values values index", "io": "id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 .... 1 10 20 0 40 0 50 2 10 30 30 0 0 50 . . <s> id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 .... 1 10 20 20 40 40 50 2 10 30 30 30 30 50 . . ", "apis": "name name set_index to_dict copy map name name rename columns where notnull merge left drop name name drop rename columns merge left", "code": ["import pandas as pd\nimport dask.dataframe as dd\n\ndef replace_names(df1, # can be pandas or dask dataframe\n                  df2, # this should be pandas.\n                  idxCol='id',\n                  srcCol='name',\n                  dstCol='name'):\n    diz = df2[[idxCol, srcCol]].set_index(idxCol).to_dict()[srcCol]\n    out = df1.copy()\n    out[dstCol] = out[idxCol].map(diz)\n    return out\n", "def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    if srcCol == dstCol:\n        df2 = df2.rename(columns={srcCol:f\"{srcCol}_new\"})\n        srcCol = f\"{srcCol}_new\"\n    \n    def map_replace(x, srcCol, dstCol):\n        x[dstCol] = np.where(x[srcCol].notnull(),\n                             x[srcCol],\n                             x[dstCol])\n        return x\n    \n    df = dd.merge(df1, df2, on=idxCol, how=\"left\")\n    df = df.map_partitions(lambda x: map_replace(x, srcCol, dstCol))\n    df = df.drop(srcCol, axis=1)\n    return df\n\ndf = replace_names_dask(df1, df2)\n", "def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    df1 = df1.drop(dstCol, axis=1)\n    df2 = df2.rename(columns={srcCol: dstCol})\n    df = dd.merge(df1, df2, on=idxCol, how=\"left\")\n    return df\n\ndf = replace_names_dask(df1, df2)\n"], "link": "https://stackoverflow.com/questions/62900970/dask-equivalent-to-pandas-dataframe-update"}
{"id": 583, "q": "Pandas Dataframe data are same or new?", "d": "In Python, Pandas dataframes are used : dataframe_1 : dataframe_2 : Here, dataframe_2 contains AB20, AB10 and AB17 same as dataframe_1 in random order. How to check which elements in dataframe_2 are new and which are same as dataframe_1 ???", "q_apis": "contains", "io": " id 0 AB17 1 AB18 2 AB19 3 AB20 4 AB10 <s> id 0 AB20 1 AB10 2 AB17 3 AB21 4 AB09 ", "apis": "mask isin mask dtype bool loc mask diff loc mask unique values loc mask unique diff loc mask unique diff", "code": ["mask = dataframe_2['id'].isin(dataframe_1['id'])\nprint (mask)\n0     True\n1     True\n2     True\n3    False\n4    False\nName: id, dtype: bool\n\nsame = dataframe_2.loc[mask, 'id'].tolist()\ndiff = dataframe_2.loc[~mask, 'id'].tolist()\n\n#if want unique values\n#same = dataframe_2.loc[mask, 'id'].unique().tolist()\n#diff = dataframe_2.loc[~mask, 'id'].unique().tolist()\n\nprint (same)\n['AB20', 'AB10', 'AB17']\n\nprint (diff)\n['AB21', 'AB09']\n"], "link": "https://stackoverflow.com/questions/52491327/pandas-dataframe-data-are-same-or-new"}
{"id": 46, "q": "Pandas Replace NaN with blank/empty string", "d": "I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so:", "q_apis": "empty values empty", "io": " 1 2 3 0 a NaN read 1 b l unread 2 c NaN read <s> 1 2 3 0 a \"\" read 1 b l unread 2 c \"\" read ", "apis": "replace", "code": ["import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"], "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string"}
{"id": 22, "q": "Splitting a dataframe into separate CSV files", "d": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header.", "q_apis": "value value", "io": "+---------+---------+ | Column1 | Column2 | +---------+---------+ | 1 | 93644 | | 2 | 63246 | | 3 | 47790 | | 3 | 39644 | | 3 | 32585 | | 1 | 19593 | | 1 | 12707 | | 2 | 53480 | +---------+---------+ <s> +---+-------+----------------+ | 1 | 19593 | NewColumnValue | | 1 | 93644 | NewColumnValue | | 1 | 12707 | NewColumnValue | +---+-------+----------------+ +---+-------+-----------------+ | 2 | 63246 | NewColumnValue | | 2 | 53480 | NewColumnValue | +---+-------+-----------------+ +---+-------+-----------------+ | 3 | 47790 | NewColumnValue | | 3 | 39644 | NewColumnValue | | 3 | 32585 | NewColumnValue | +---+-------+-----------------+ ", "apis": "groupby to_csv", "code": ["for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"], "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files"}
{"id": 203, "q": "Pandas : new column with index of unique values of another column", "d": "My dataframe: Expected new dataframe:", "q_apis": "index unique values", "io": "ID Name_Identify ColumnA ColumnB ColumnC 1 POM-OPP D43 D03 D59 2 MIAN-ERP D80 D74 E34 3 POM-OPP E97 B56 A01 4 POM-OPP A66 D04 C34 5 DONP28 B55 A42 A80 6 MIAN-ERP E97 D59 C34 <s> ID Name_Identify ColumnA ColumnB ColumnC NEW_ID 1 POM-OPP D43 D03 D59 1 2 MIAN-ERP D80 D74 E34 2 3 POM-OPP E97 B56 A01 1 4 POM-OPP A66 D04 C34 1 5 DONP28 B55 A42 A80 3 6 MIAN-ERP E97 D59 C34 2 ", "apis": "unique start map unique start map", "code": ["convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\ndf[\"NEW_ID\"] = df.Name_Identify.map(convert)\n", "In[24]: convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\nIn[25]: convert\n", "In[26]: df[\"NEW_ID\"] = df.Name_Identify.map(convert)\nIn[27]: df\n"], "link": "https://stackoverflow.com/questions/64168703/pandas-new-column-with-index-of-unique-values-of-another-column"}
{"id": 450, "q": "Pandas list of tuples to MultiIndex", "d": "I have a that looks like this: I need to return a that looks like this: What is the best approach to this?", "q_apis": "MultiIndex", "io": " id t_l 0 100 [('a', 1), ('b', 2)] 1 151 [('x', 4), ('y', 3)] <s> id f g 0 100 'a' 1 1 'b' 2 2 151 'x' 4 3 'y' 3 ", "apis": "DataFrame index repeat columns reset_index", "code": ["from itertools import chain\npd.DataFrame(chain.from_iterable(df.t_l), index=np.repeat(df.id, df.t_l.str.len()), \\\n                                          columns=['f', 'g']).reset_index()\n"], "link": "https://stackoverflow.com/questions/57050068/pandas-list-of-tuples-to-multiindex"}
{"id": 500, "q": "Pandas filter rows based on condition, but always retain the first row", "d": "I would like to drop some rows that meets certain conditions but I do not want to drop the first row even if the first row meets that criteria. I tried dropping rows by using the df.drop function but it will erase the first row if the first row meets that condition. I do not want that. Data looks something like this: I want to do it in a way that if a row has a value of 3 in column2 then drop it. And I want the new data to be like this (after dropping but keeping the first one even though the first row had a value of 3 in column 2):", "q_apis": "filter first drop drop first first drop first first value drop first first value", "io": "Column1 Column2 Column3 1 3 A 2 1 B 3 3 C 4 1 D 5 1 E 6 3 F <s> Column1 Column2 Column3 1 3 A 2 1 B 4 1 D 5 1 E ", "apis": "index", "code": ["<ORIGINAL CONDITION> or <CONDITION TO KEEP FIRST ROW>", "(df['Column2'] == 3) & (df.index != 0)"], "link": "https://stackoverflow.com/questions/55388727/pandas-filter-rows-based-on-condition-but-always-retain-the-first-row"}
{"id": 393, "q": "Phyton: How to get the average of the n largest values for each column grouped by id", "d": "I'm trying to get the mean for each column while grouped by id. But I don't get it to work as I want to. The data: What I got so far: I got those two tries. But they are both just for one column and I don't know how to do it for more then just one.: What I want: Idealy, I would like to have a dataframe as follows: So that each row contains the mean values for the 100 biggest values for EACH column grouped by id.", "q_apis": "get values get mean get contains mean values values", "io": "ID Property3 Property2 Property3 1 10.2 ... ... 1 20.1 1 51.9 1 15.8 1 12.5 ... 1203 104.4 1203 11.5 1203 19.4 1203 23.1 <s> ID Property3 Property2 Property3 1 37.8 5.6 2.3 2 33.0 1.5 10.4 3 34.9 91.5 10.3 4 33.0 10.3 14.3 ", "apis": "groupby agg nlargest mean reset_index", "code": ["df = (data.groupby('ID')['Property1','Property2','Property3']\n          .agg(lambda grp: grp.nlargest(100).mean())\n          .reset_index())\n"], "link": "https://stackoverflow.com/questions/59067194/phyton-how-to-get-the-average-of-the-n-largest-values-for-each-column-grouped-b"}
{"id": 86, "q": "Changing Value of adjacent column based on value of of another column", "d": "I have following dataframe: I want to change value in column A1 to NaN whenever corresponding value in column A2 is No or NA. Same for B1. Note: NA here is a string objects not NaN.", "q_apis": "value value value", "io": " A1 A2 B1 B2 0 10 20 20 NA 1 20 40 30 No 2 50 No 50 10 3 40 NA 50 20 <s> A1 A2 B1 B2 0 10 20 NaN NA 1 20 40 NaN No 2 NaN No 50 10 3 NaN NA 50 20 ", "apis": "loc isna eq mask isna eq", "code": ["df.loc[df.A2.isna() | df.A2.eq('No'), 'A1'] = np.nan\n", "df['A1'] = df['A1'].mask(df.A2.isna() | df.A2.eq('No'))\n"], "link": "https://stackoverflow.com/questions/66383901/changing-value-of-adjacent-column-based-on-value-of-of-another-column"}
{"id": 647, "q": "Dynamically accessing subset of pandas dataf rame, perform calculation and write to new data frame", "d": "I have a very large data frame from which I would like to pull a subsample, perform some calculation and then write these results into a new data frame. For the sample, please consider: returning this: Now I would like \"extract\" always 3 rows, rolling from the beginning and calculate the averages (as an example, other calculations would work too) of each column: the result data frame is then How can I do that?", "q_apis": "sample rolling", "io": " a b c d e 0 1 9 0 3 0 1 5 4 1 0 3 2 9 3 6 3 5 3 6 2 5 9 7 4 9 0 7 9 5 <s> df_1 a b c d e 0 1 9 0 3 0 1 5 4 1 0 3 2 9 3 6 3 5 df_2 a b c d e 1 5 4 1 0 3 2 9 3 6 3 5 3 6 2 5 9 7 df_3 a b c d e 2 9 3 6 3 5 3 6 2 5 9 7 4 9 0 7 9 5 ", "apis": "rolling mean iloc", "code": ["N = 3\ndf = df.rolling(N).mean().iloc[N-1:]\n"], "link": "https://stackoverflow.com/questions/49715104/dynamically-accessing-subset-of-pandas-dataf-rame-perform-calculation-and-write"}
{"id": 364, "q": "Using np.split_array and then saving each split into dataframes", "d": "Appending data to a dataframe but changing rows after certain # of columns The above is my previous post, where I attempted to convert 1800 row x 1 column dataframe into 300 row x 6 column dataframe through: I would then would like to further split the dataframe into six chunks. I was thinking about using np split like: This line would be added right after (I know the lines won't work if split is applied). For example: The starting data table would look like: and so on (please note that the numbers are just random for this post, and for testing, you can use any floating numbers, these are essentially p-values). The rows are in groups of 50 rows and hence why I would like to separate the 300x6 df into 6 df of 50x6. Because of the data size, I wasn't able to insert all of it and had to express the table as above, but for the actual testing, you can probably generate random values with 300x6 shape df (not counting the headers). what I want is: and so on. I am not sure how I would iterate over each split from then save as separate dataframes. Any help or suggestions would be appreciated.", "q_apis": "columns where right any values groups size insert all values shape", "io": "col1 col2 col3 col4 col5 col6 1 0.658 0.1067 0.777 0.459 0.3307 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 . . . . 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 . . . <s> [df1] col1 col2 col3 col4 col5 col6 1 0.658 0.1067 0.777 0.459 0.3307 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 [df2] col1 col2 col3 col4 col5 col6 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 ", "apis": "DataFrame apply name groupby get_group set_index groups columns shape first head head head values array values last tail tail values array values", "code": ["import numpy as np\nimport pandas as pd\n\ndata = np.random.rand(300,6)\ndf = pd.DataFrame(data)\n\ndf[\"label\"] = df.apply(lambda x: x.name//50, axis=1)\ngb = df.groupby(\"label\")\ndf_list = [gb.get_group(x).set_index(\"label\") for x in gb.groups]\n", "for x in df_list: # each dataframe should have 50 rows and 6 columns\n    assert x.shape == (50, 6)\n", "# print first dataframe head (rows should be same as head printed above)\ndf_list[0].head(3) # and access the values/numpy array by df_list[0].values\n", "# print last section (rows should be same as tail printed above)\ndf_list[5].tail(3) # and access the values/numpy array by df_list[5].values\n"], "link": "https://stackoverflow.com/questions/59819257/using-np-split-array-and-then-saving-each-split-into-dataframes"}
{"id": 490, "q": "Predicting Values in Movie Recommendations", "d": "I've been trying to create a recommendation system using the movielens dataset in python. My goal is to determine the similarity between users and then output the top five recommended movies for each user in this format: The data I am using for now is this ratings dataset. Here is the code so far: I am trying to implement the prediction function. I want to predict the missing values and add them to c1. I am trying to implement this. The formula as well as an example of how it should be used is in the picture. As you can see it uses the similarity scores of the most similar users. The output of similarity looks like this: For example here is user1's similarity: I need help using these similarities in the prediction function to predict missing movie ratings. If that is solved I will then have to find the top 5 recommended movies for each user and output them in the format above. I currently need help with the prediction function. Any advice helps. Please let me know if you need any more information or clarification. Thank you for reading", "q_apis": "between now values add any", "io": "User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5 User-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5 <s> [(34, 0.19269904365720053) (196, 0.19187531680008307) (538, 0.14932027335788825) (67, 0.14093020024386654) (419, 0.11034407313683092) (319, 0.10055810007385564)] ", "apis": "index sum sum columns read_csv pivot columns index values DataFrame T fillna index columns columns columns fillna apply join mid mid fillna name nlargest iloc nlargest index apply join mid mid fillna name nlargest iloc nlargest index", "code": ["import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import scale\n\n\ndef predict(l):\n    # finds the userIds corresponding to the top 5 similarities\n    # calculate the prediction according to the formula\n    return (df[l.index] * l).sum(axis=1) / l.sum()\n\n\n# use userID as columns for convinience when interpretering the forumla\ndf = pd.read_csv('ratings.csv').pivot(columns='userId',\n                                                index='movieId',\n                                                values='rating')\n\nsimilarity = pd.DataFrame(cosine_similarity(\n    scale(df.T.fillna(-1000))),\n    index=df.columns,\n    columns=df.columns)\n# iterate each column (userID),\n# for each userID find the highest five similarities\n# and use to calculate the prediction for that user,\n# use fillna so that original ratings dont change\n\nres = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in col.fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\nprint(res)\n", "res = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in (0 * col).fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\n"], "link": "https://stackoverflow.com/questions/48821092/predicting-values-in-movie-recommendations"}
{"id": 366, "q": "Save in DataFrame unique values for every column", "d": "If I have a data (df) like this: With the next fuction: It returns something like: \u00bfHow can I save the return of the fuction in a DataFrame?, I would like to see it like this: Thanks you !", "q_apis": "DataFrame unique values DataFrame", "io": "X1 X2 X3 A A C B A C C B C <s> X1 X2 X3 A A C B B C ", "apis": "apply Series unique fillna apply drop_duplicates reset_index drop fillna", "code": ["df1 = df.apply(lambda x: pd.Series(pd.unique(x))).fillna('')\n", "df1 = df.apply(lambda x: x.drop_duplicates().reset_index(drop=True)).fillna('')\n"], "link": "https://stackoverflow.com/questions/59767001/save-in-dataframe-unique-values-for-every-column"}
{"id": 582, "q": "Compare each of the column values and return final value based on conditions", "d": "I currently have a dataframe which looks like this: What I want to do is apply some condition to the column values and return the final result in a new column. The condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4] I tried to define a function to append the final results but wasnt really getting anywhere...any thoughts? The desired outcome would look something like: where col4 is the new column created. Thanks", "q_apis": "values value apply values assign values where first append any where", "io": "col1 col2 col3 1 2 3 2 3 NaN 3 4 NaN 2 NaN NaN 0 2 NaN <s> col1 col2 col3 col4 1 2 3 2 2 3 NaN 2 3 4 NaN 3 2 NaN NaN 2 0 2 NaN 2 ", "apis": "apply", "code": ["def func(x,l=[2,1,3,0,4,5]):\n    for j in l:\n      if(j in x):\n         return j\n\ndf['new'] = df.apply(lambda x: func(list(x)),axis =1)\n"], "link": "https://stackoverflow.com/questions/52492961/compare-each-of-the-column-values-and-return-final-value-based-on-conditions"}
{"id": 609, "q": "How to find which row items are appearing most in a pandas dataframe", "d": "I have a dataframe something like this : How to find which row is appearing the most number of times and unique items count? Here this is appearing most times in rows . I tried ,but it is giving me 100+ rules if my data is big. .NB : My real data is not and . This is mock data.", "q_apis": "items unique items count", "io": " a b c d e f ------------------------ 0 0 0 1 1 0 1 1 1 0 1 1 0 0 2 0 0 1 1 0 1 3 1 0 1 0 0 0 4 0 0 1 1 0 1 5 0 1 1 0 0 0 6 1 0 1 0 1 1 7 0 0 1 1 0 1 8 1 0 1 1 1 0 9 0 0 1 1 0 1 <s> 0 0 1 1 0 1", "apis": "groupby columns columns transform size index max Int64Index dtype", "code": ["s = df.groupby(df.columns.tolist())[df.columns[0]].transform('size')\nidx = s.index[s == s.max()]\nprint (idx)\nInt64Index([0, 2, 4, 7, 9], dtype='int64')\n"], "link": "https://stackoverflow.com/questions/51357253/how-to-find-which-row-items-are-appearing-most-in-a-pandas-dataframe"}
{"id": 132, "q": "Count the number of specific values in multiple columns pandas", "d": "I have a data frame: I want to count the number of times 'BUY' appears in each row. Intended result: I have tried the following but it simply gives 0 for all the rows: Note that BUY can only appear in B, C, D, E columns. I tried to find the solution online but shockingly found none. Little help will be appreciated. THANKS!", "q_apis": "values columns count all columns", "io": "A B C D E 12 4.5 6.1 BUY NaN 12 BUY BUY 5.6 NaN BUY 4.5 6.1 BUY NaN 12 4.5 6.1 0 NaN <s> A B C D E score 12 4.5 6.1 BUY NaN 1 12 BUY BUY 5.6 NaN 2 15 4.5 6.1 BUY NaN 1 12 4.5 6.1 0 NaN 0 ", "apis": "apply count", "code": ["df['score'] = df.apply(lambda x: x.tolist().count('BUY'), axis=1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/65550028/count-the-number-of-specific-values-in-multiple-columns-pandas"}
{"id": 124, "q": "How to concat two or more data frames with different columns names in pandas", "d": "I have hundreds csv files and I need join it to one file. I have it all load as pandas dataframes. Sample dataframes: I need this output: or How can I do that? Thanks EDIT: I have cca 500 csv files, this is my code to make one file from them:", "q_apis": "concat columns names join all", "io": " a x y z 0 e1 4 7 1 e1 5 8 2 e1 6 9 3 e2 13 16 100 4 e2 14 17 101 5 e2 15 18 102 <s> a x y z 0 e1 4 7 na 1 e1 5 8 na 2 e1 6 9 na 3 e2 13 16 100 4 e2 14 17 101 5 e2 15 18 102 ", "apis": "DataFrame DataFrame append hist DataFrame read_csv append to_csv index", "code": ["df1 = pd.DataFrame({'a':['e1','e1','e1'],'x':[4,5,6],'y':[7,8,9]})\ndf2 = pd.DataFrame({'a':['e2','e2','e2'],'x':[13,14,15],'y':[16,17,18], 'z':[100,101,102]})\nnewdf = df1.append(df2, ignore_index=True)\n", "import glob\nimport pandas as pd\n\npath = r'C:/Users/Miro/data hist'\nall_files = glob.glob(path + \"/*.csv\")\n\nli = pd.DataFrame()\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, sep='delimiter', header=None)\n    li = li.append(df, ignore_index=True)\n\n\nli.to_csv( \"full.csv\", index=False, encoding='utf-8-sig')\n"], "link": "https://stackoverflow.com/questions/65694203/how-to-concat-two-or-more-data-frames-with-different-columns-names-in-pandas"}
{"id": 441, "q": "Change value of datetime in pandas Dataframe", "d": "i want to change the value of a pandas dataframe column with datetime format. Basically i want to add always 20 seconds to a row. Im new to python/pandas so i dont know any ways to solve that issue. Here is my code so far: Dataframe df_date: original: ... expected: ... Any help on this would be appreciated!", "q_apis": "value value add seconds any", "io": "0 2017-03-10 01:00:00 1 2017-03-10 01:00:00 2 2017-03-10 01:00:00 3 2017-03-10 01:00:00 4 2017-03-10 01:00:00 <s> 0 2017-03-10 01:00:20 1 2017-03-10 01:00:40 2 2017-03-10 01:01:00 3 2017-03-10 01:00:20 4 2017-03-10 01:00:40 ", "apis": "to_timedelta timedelta_range freq", "code": ["td = pd.to_timedelta(np.arange(1, len(dataframe) + 1) * 20, unit='s')\n", "td = pd.timedelta_range(0, periods=len(dataframe), freq='20s') \n"], "link": "https://stackoverflow.com/questions/57428633/change-value-of-datetime-in-pandas-dataframe"}
{"id": 216, "q": "Pandas fill in missing data from columns in other rows", "d": "I have a df like below: Output: For ac are null, get prev's value, and then look up at the id column. Fill in the null with value at ac column. Expected output: How do I achieve this? Thanks.", "q_apis": "columns get value at value at", "io": " id ac prev 0 a 123 NaN 1 b 223 NaN 2 c NaN a 3 d NaN b <s> id ac prev 0 a 123 NaN 1 b 223 NaN 2 c 123 a 3 d 223 b ", "apis": "isna loc loc map set_index", "code": ["m = df['ac'].isna()\ndf.loc[m, 'ac'] = df.loc[m, 'prev'].map(df.set_index('id')['ac'])\n"], "link": "https://stackoverflow.com/questions/63779231/pandas-fill-in-missing-data-from-columns-in-other-rows"}
{"id": 527, "q": "Add column to DataFrame in a loop", "d": "Let's say I have a very simple pandas dataframe, containing a single indexed column with \"initial values\". I want to read in a loop N other dataframes to fill a single \"comparison\" column, with matching indices. For instance, with my inital dataframe as and the following two dataframes to read in a loop I would like to produce the following result Using , or , I only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with . What's the most pandas-pythonic way of achieving this? Below an example from the proposed duplicate solution: Second edit: @W-B, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. It also requires turning off warnings, which might be dangerous...", "q_apis": "DataFrame values indices", "io": " Initial 0 a 1 b 2 c 3 d <s> Initial Comparison 0 a e 1 b f 2 c g 3 d h ", "apis": "DataFrame array columns DataFrame array columns DataFrame array columns index", "code": ["import functools\ndf1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])\ndf1['Compare']=np.nan\ndf2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])\ndf3 = pd.DataFrame(np.array(['g','h','i']), columns=['Compare'],index=[2,3,4])\n"], "link": "https://stackoverflow.com/questions/54407275/add-column-to-dataframe-in-a-loop"}
{"id": 69, "q": "python pandas give comma separated values into columns with &quot;title&quot;", "d": "I have some comma-separated data in the same column and I wish to separate each value into different columns. I have done separation using below and the output I got is but I need something like below (has to give some titles for each column) How would I do that?", "q_apis": "values columns value columns", "io": "0 13.4119837, 42.082885, 13.4119837, 42.082885 1 11.6285463, 42.4193742, 11.6285463, 42.4193742 2 -3.606772, 39.460299, -3.606772, 39.460299 3 -0.515639, 38.988847, -0.515639, 38.988847 4 -2.403309, 37.241792, -2.403309, 37.241792 <s> 0 1 2 3 0 13.4119837 42.082885 13.4119837 42.082885 1 11.6285463 42.4193742 11.6285463 42.4193742 2 -3.606772 39.460299 -3.606772 39.460299 3 -0.515639 38.988847 -0.515639 38.988847 4 -2.403309 37.241792 -2.403309 37.241792 ", "apis": "columns", "code": ["data = data['column_name'].str.split(\",\", n = 3, expand = True)\ndata.columns = ['minLat', 'maxLat', 'minLong', 'maxLong']\n"], "link": "https://stackoverflow.com/questions/66782284/python-pandas-give-comma-separated-values-into-columns-with-title"}
{"id": 324, "q": "Pandas select only the first 3 YYYYMM per group", "d": "CGood afternoon, I have a datrame like the one below I need to get only the first three *CONSECUTIVE MMMMYY per USR. Im able to get the first 3 records using head(3) but of course it dont bring back what I need, neither using it gets the consecutive when ['check'] is true but in some cases I may need to get 200001 and 200003 only and they are not consecutive between them. Any guidance will be appreciated Thanks", "q_apis": "select first get first get first head get between", "io": "+---+---+--------+ | |USR| MMMMYY | +---+---+--------+ | 1 | A | 200002 | +---+---+--------+ | 2 | A | 200003 | +---+---+--------+ | 3 | A | 200004 | +---+---+--------+ | 4 | A | 200005 | +---+---+--------+ | 5 | B | 200001 | +---+---+--------+ | 6 | B | 200003 | +---+---+--------+ | 7 | B | 200008 | +---+---+--------+ | 8 | B | 200009 | +---+---+--------+ <s> +---+---+--------+ | |USR| MMMMYY | +---+---+--------+ | 1 | A | 200002 | +---+---+--------+ | 2 | A | 200003 | +---+---+--------+ | 3 | A | 200004 | +---+---+--------+ | 5 | B | 200001 | +---+---+--------+ | 6 | B | 200003 | +---+---+--------+ ", "apis": "to_datetime groupby transform min", "code": ["df['MMMMYY'] = pd.to_datetime(df.MMMMYY, format='%Y%m')\n\ns = df.groupby('USR')['MMMMYY'].transform('min') + pd.offsets.MonthOffset(3)\n\ndf[df.MMMMYY<s]\n"], "link": "https://stackoverflow.com/questions/60959528/pandas-select-only-the-first-3-yyyymm-per-group"}
{"id": 233, "q": "Pandas Dataframe split into seperate csv by column value", "d": "Hello i have dataset containing 4 columns i want to split my dataframe into separate csv files by their \"s\" column but I couldn't figure out a proper way of doing it. \"s\" column has arbitrary numbers of labels. We don't know how many 1's or 2's dataset has. it is until 30 but not every number is contained in this dataset. My desired output is: after I get this split I can easily write it to separate csv files. The problem I am having is that I don't know how many different \"s\" values I have and how much from each of them. Thank you", "q_apis": "value columns get values", "io": "x y z s 1 42.8 157.5 1 1 43.8 13.5 1 1 44.8 152 2 . . . 4 7528 157.5 2 4 45.8 13.5 3 8 72.8 152 3 <s> df1 x y z s 1 42.8 157.5 1 . 1 43.8 13.5 1 df2 1 44.8 152 2 . 4 7528 157.5 2 df3 4 45.8 13.5 3 . 8 72.8 152 3 ", "apis": "groupby to_csv index", "code": ["for i, x in df.groupby('s'): x.to_csv(f'df{i}.csv', index=False)\n"], "link": "https://stackoverflow.com/questions/63353050/pandas-dataframe-split-into-seperate-csv-by-column-value"}
{"id": 265, "q": "Turn column levels inside-out", "d": "I have a pandas DataFrame which looks like this (code to create it is at the bottom of the question): I would like to turn the and columns \"inside out\", i.e. my expected output is: Is there an efficient (i.e. that does not involve writing a python loop that goes through each row one-by-one) way to do this in pandas? Code to generate the starting DataFrame: Code to generate expected output:", "q_apis": "levels DataFrame at columns DataFrame", "io": " col_1 col_2 foo_1 foo_2 col_3 col_4 col_3 col_4 0 1 4 2 8 5 7 1 3 1 6 3 8 9 <s> col_1 col_2 col_3 col_4 0 1 4 {'foo_1': 2, 'foo_2': 5} {'foo_1': 8, 'foo_2': 7} 1 3 1 {'foo_1': 6, 'foo_2': 8} {'foo_1': 3, 'foo_2': 9} ", "apis": "filter droplevel agg filter droplevel agg drop droplevel", "code": ["df['col_3'] = df.filter(like='col_3').droplevel(1, 1).agg(dict, axis=1)\ndf['col_4'] = df.filter(like='col_4').droplevel(1, 1).agg(dict, axis=1)\n\ndf = df.drop(['foo_1', 'foo_2'], 1).droplevel(1, 1)\n"], "link": "https://stackoverflow.com/questions/62818462/turn-column-levels-inside-out"}
{"id": 598, "q": "Pandas DataFrame filter column A depending on if column B contains x for group of values in A", "d": "I would like to filter the below DataFrame on column , based on if for the value in , column contains the value . Here, values 1, 3, and 4 contain at least one row with value in column , while 2 and 5 do not. I am trying to filter out any rows with 2 and 5 so that the final output is: How could I do this (preferably in one step)?", "q_apis": "DataFrame filter contains values filter DataFrame value contains value values at value filter any step", "io": "In [32]: df Out[32]: ref type 0 1 P 1 1 C 2 1 A 3 2 C 4 3 P 5 3 P 6 4 P 7 4 A 8 5 C 9 5 A <s> In [34]: df Out[34]: ref type 0 1 P 1 1 C 2 1 A 4 3 P 5 3 P 6 4 P 7 4 A ", "apis": "groupby filter values", "code": ["df.groupby('ref').filter(lambda x : ('P' in x['type'].values))\n"], "link": "https://stackoverflow.com/questions/51850165/pandas-dataframe-filter-column-a-depending-on-if-column-b-contains-x-for-group-o"}
{"id": 101, "q": "Pandas: Clean up String column containing Single Quotes and Brackets using Regex?", "d": "I want to clean the following Pandas dataframe column, but in a single and efficient statement than the way I am trying to achieve it in the code below. Input: Output: Code:", "q_apis": "", "io": " string 0 ['string', '#string'] 1 ['#string'] 2 [] <s> string 0 string, #string 1 #string 2 NaN ", "apis": "astype replace", "code": ["df['string'] = df['string'].astype(str).str.replace(r\"^[][\\s]*$|(^\\[+|\\]+$|')\", lambda m: '' if m.group(1) else np.nan)\n"], "link": "https://stackoverflow.com/questions/66126525/pandas-clean-up-string-column-containing-single-quotes-and-brackets-using-regex"}
{"id": 557, "q": "Pandas corr() returning NaN too often", "d": "I'm attempting to run what I think should be a simple correlation function on a dataframe but it is returning NaN in places where I don't believe it should. Code: Subject DataFrame: corr() Result: According to the (limited) documentation on the function, it should exclude \"NA/null values\". Since there are overlapping values for each column, should the result not all be non-NaN? There are good discussions here and here, but neither answered my question. I've tried the idea discussed here, but that failed as well. @hellpanderr's comment brought up a good point, I'm using 0.22.0 Bonus question - I'm no mathematician, but how is there a 1:1 correlation between B and C in this result?", "q_apis": "corr where DataFrame corr values values all between", "io": " A B C 0 NaN 500.0 NaN 1 99.0 100.0 100.0 2 NaN 100.0 NaN 3 100.0 NaN 100.0 4 100.0 NaN NaN 5 100.0 100.0 NaN 6 NaN 500.0 300.0 7 NaN 500.0 NaN 8 NaN 100.0 NaN <s> A B C A 1.0 NaN NaN B NaN 1.0 1.0 C NaN 1.0 1.0 ", "apis": "dropna std", "code": ["df[['A', 'C']].dropna().std()\n\nA    0.707107\nC    0.000000\n"], "link": "https://stackoverflow.com/questions/52466844/pandas-corr-returning-nan-too-often"}
{"id": 576, "q": "How to take first one or two digits from float number?", "d": "I have a Pandas dataframe. In a series, I have time in hour and minute represented as , as below. I want only the hours: Example of time column values from (1 to 12) : Example of time column values from (13 to 24) : I have tried this code but it takes very long time until I close the editor so I didn't see the result", "q_apis": "take first time hour minute time values time values time", "io": "1000.0 -> 10 901.0 -> 9 <s> 1850.0 -> 18 2301.0 -> 23 ", "apis": "hour astype", "code": ["df['hour'] = (df['dep_time'] // 100).astype(int)\n"], "link": "https://stackoverflow.com/questions/52745166/how-to-take-first-one-or-two-digits-from-float-number"}
{"id": 358, "q": "Apply qcut for complete dataframe", "d": "I want to replace the column values with bin numbers based on quantiles but for each and every column present in the dataframe. I know how to do this with qcut and labels as its parameter for a single column, but do not know whether it can be applied for complete dataframe or not. say the dataframe looks like below.. The ID column should remain unchanged but the other columns should be replaced by bin numbers, like below.. the bin numbers I have provided here for CC, DD, EE are not exact and for understanding purpose only. And in the real dataset, there are more than 100 columns and 1000 rows, and I do not want to replace the 'ID' column, but all the other columns. How to do this?", "q_apis": "qcut replace values qcut columns columns replace all columns", "io": " ID CC DD EE 0 Q1 0 23 18 1 Q2 2 32 19 2 Q3 3 45 20 3 Q4 4 54 21 4 Q5 5 67 22 5 Q6 6 76 23 <s> ID CC DD EE 0 Q1 1 1 1 1 Q2 2 2 1 2 Q3 3 2 2 3 Q4 4 3 3 4 Q5 5 4 4 5 Q6 5 5 5 ", "apis": "cut", "code": ["import pandas as pd\n\ndf['CC'] = pd.cut(df['CC'], [0, 5, 10,20])\n"], "link": "https://stackoverflow.com/questions/59915944/apply-qcut-for-complete-dataframe"}
{"id": 352, "q": "Pandas.DataFrame: efficient way to add a column &quot;seconds since last event&quot;", "d": "I have a Pandas.DataFrame with a standard index representing seconds, and I want to add a column \"seconds elapsed since last event\" where the events are given in a list. Specifically, say and Then I want to obtain I tried so apparently to make it work I need to write . More importantly, when I then do I obtain That is: the previous has been overwritten. Is there a way to assign new values without overwriting existing values by nan ? Then, I can do something like Or is there a more efficient way ? For the record, here is my naive code. It works, but looks inefficient and of poor design:", "q_apis": "DataFrame add seconds last DataFrame index seconds add seconds last where assign values values", "io": "| | 0 | x | |---:|----:|-----:| | 0 | 0 | <NA> | | 1 | 0 | <NA> | | 2 | 0 | 0 | | 3 | 0 | 1 | | 4 | 0 | 2 | | 5 | 0 | 0 | | 6 | 0 | 1 | <s> | | 0 | x | |---:|----:|----:| | 0 | 0 | nan | | 1 | 0 | nan | | 2 | 0 | nan | | 3 | 0 | nan | | 4 | 0 | nan | | 5 | 0 | 0 | | 6 | 0 | 1 | ", "apis": "index isin cumsum loc reindex index isna cumsum where groupby cumcount", "code": ["s = df.index.isin(event).cumsum()\n# or equivalently\n# s = df.loc[event, 0].reindex(df.index).isna().cumsum()\n\ndf['x'] = np.where(s>0,df.groupby(s).cumcount(), np.nan)\n"], "link": "https://stackoverflow.com/questions/60157250/pandas-dataframe-efficient-way-to-add-a-column-seconds-since-last-event"}
{"id": 431, "q": "Print dataframe without float precision", "d": "I would like to print a dataframe in my console without displaying any float decimal precision. The output I got after printing is: Whereas what I expected is: There seems to be an issue to display the tuple without float precision. Any idea why ? Cheers", "q_apis": "any", "io": "0 (a, 2.01212121212) 1 1.12 2 8.12 <s> 0 (a, 2.01) 1 1.12 2 8.12 ", "apis": "DataFrame append round append append append round", "code": ["pd_tmp = pd.DataFrame()\npd_tmp[\"new_column\"] = [(\"a\", 2.01212121212), 1.123123, 8.1212]\n\n\ndef foo(pd_col):\n    pd_new_col = []\n    for i in pd_col:\n        if not isinstance(i, float):\n            lst = []\n            for j in i:\n                if not isinstance(j, float):\n                    k = j\n                    lst.append(k)\n                else:\n                    k = round(j, 2)\n                    lst.append(k)\n            pd_new_col.append(tuple(lst))\n\n        else:\n            pd_new_col.append(round(i, 2))\n\n   return pd_new_col\n\n\npd_tmp[\"new_column\"] = foo(pd_tmp[\"new_column\"])\nprint(pd_tmp)\n"], "link": "https://stackoverflow.com/questions/57864979/print-dataframe-without-float-precision"}
{"id": 25, "q": "move column above and delete rows in pandas python dataframe", "d": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be", "q_apis": "delete sample DataFrame start", "io": "A B C D E F G H a.1 b.1 c.1 d.1 c.2 d.2 e.1 f.1 g.1 h.1 <s> A B C D E F G H a.1 b.1 c.1 d.1 e.1 f.1 g.1 h.1 c.2 d.2 ", "apis": "apply shift first_valid_index dropna all fillna index index reset_index drop apply shift first_valid_index dropna all fillna index index name name columns get_indexer name shift shift first_valid_index apply", "code": ["out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n", "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n", "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"], "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe"}
{"id": 610, "q": "pandas apply and applymap functions are taking long time to run on large dataset", "d": "I have two functions applied on a dataframe {{Update}} Dataframe has got almost 700 000 rows. This is taking much time to run. How to reduce the running time? Sample data : output: This line of code takes items from a list and fill one by one to each column as shown above. There will be almost 38 columns.", "q_apis": "apply applymap time time time items columns", "io": " A ---------- 0 [1,4,3,c] 1 [t,g,h,j] 2 [d,g,e,w] 3 [f,i,j,h] 4 [m,z,s,e] 5 [q,f,d,s] <s> A B C D E ------------------------- 0 [1,4,3,c] 1 4 3 c 1 [t,g,h,j] t g h j 2 [d,g,e,w] d g e w 3 [f,i,j,h] f i j h 4 [m,z,s,e] m z s e 5 [q,f,d,s] q f d s ", "apis": "select_dtypes columns apply", "code": ["cols = res.select_dtypes(object).columns\nres[cols] = res[cols].apply(lambda x: x.str.strip('\"'))\n"], "link": "https://stackoverflow.com/questions/51279903/pandas-apply-and-applymap-functions-are-taking-long-time-to-run-on-large-dataset"}
{"id": 186, "q": "Drop rows and sort one dataframe according to another", "d": "I have two pandas data frames ( and ): My goal is to append the corresponding from to each in . However, the relationship is not one-to-one (this is my client's fault and there's nothing I can do about this). To solve this problem, I want to sort by such that is identical to . So basically, for any row in 0 to : if then keep row in . if then drop row from and repeat. The desired result is: This way, I can use to assign to . Is there a standardized way to do this? Does have a method for doing this? Before this gets voted as a duplicate, please realize that , so threads like this are not quite what I'm looking for.", "q_apis": "append identical any drop repeat assign", "io": "# df1 ID COL 1 A 2 F 2 A 3 A 3 S 3 D 4 D # df2 ID VAL 1 1 2 0 3 0 3 1 4 0 <s> ID COL 1 A 2 F 3 A 3 S 4 D ", "apis": "assign groupby cumcount merge assign groupby cumcount columns", "code": ["(df1.assign(idx=df1.groupby('ID').cumcount())\n    .merge(df2.assign(idx=df2.groupby('ID').cumcount()),\n           on=['ID','idx'],\n           suffixes=['','_drop'])\n    [df1.columns]\n)\n"], "link": "https://stackoverflow.com/questions/62778713/drop-rows-and-sort-one-dataframe-according-to-another"}
{"id": 239, "q": "Pandas merge and keep only non-matching records", "d": "How can I merge/join these two dataframes ONLY on \"id\". Produce 3 new dataframes: 1)R1 = Merged records 2)R2 = (DF1 - Merged records) 3)R3 = (DF2 - Merged records) Using pandas in Python. First dataframe (DF1) Second dataframe (DF2) My solution for I am unsure that is the easiest way to get R2 and R3 R2 should look like R3 should look like:", "q_apis": "merge merge join get", "io": "| id | salary | |-----------|--------| | 1 | 20 | | 2 | 30 | | 3 | 40 | | 4 | 50 | | 6 | 33 | | 7 | 23 | | 8 | 24 | | 9 | 28 | <s> | id | salary | |-----------|--------| | 6 | 33 | | 7 | 23 | | 8 | 24 | | 9 | 28 | ", "apis": "merge groupby", "code": ["total_merge = df1.merge(df2, on='id', how='outer', indicator=True)\n\nR1 = total_merge[total_merge['_merge']=='both']\nR2 = total_merge[total_merge['_merge']=='left_only']\nR3 = total_merge[total_merge['_merge']=='right_only']\n", "dfs = {k:v for k,v in total_merge.groupby('_merge')}\n"], "link": "https://stackoverflow.com/questions/63184392/pandas-merge-and-keep-only-non-matching-records"}
{"id": 72, "q": "How to convert data frame column list value to element", "d": "Hi I have a dataframe like this: I want to change it into: How can I do that? I tries with", "q_apis": "value", "io": " A 0 [] 1 [1234] 2 [] <s> A 0 0 1 1234 2 0 ", "apis": "where", "code": ["df['A'] = np.where(df['A'].str.len() == 0, 0, df['A'].str[0]) \n"], "link": "https://stackoverflow.com/questions/59225169/how-to-convert-data-frame-column-list-value-to-element"}
{"id": 614, "q": "Create new column depending on values from other column", "d": "I have a DataFrame that looks something like this: df I want to give indexes the values that are between and in column and create a new columns as :", "q_apis": "values DataFrame values between columns", "io": " A B C 0 vt 40462 5 6 1 5 6 6 2 5 5 8 3 4 3 1 4 vl 6450 5 6 5 5 6 7 6 1 2 3 7 vt 40462 5 6 8 5 5 8 9 vl 658 6 7 10 5 5 8 11 4 3 1 12 vt 40461 5 6 13 5 5 8 14 7 8 5 <s> A B C D 0 vt 40462 5 6 vt 40462 1 5 6 6 vt 40462 2 5 5 8 vt 40462 3 4 3 1 vt 40462 4 vl 6450 5 6 vl 6450 5 5 6 7 vl 6450 6 1 2 3 vl 6450 7 vt 40462 5 6 vt 40462 8 5 5 8 vt 40462 9 vl 658 6 7 vl 658 10 5 5 8 vl 658 11 4 3 1 vl 658 12 vt 40461 5 6 vt 40461 13 5 5 8 vt 40461 14 7 8 5 vt 40461 ", "apis": "loc astype contains ffill", "code": ["df.loc[df.A.astype(str).str.contains('^[A-Za-z]'), 'D'] = df.A\n\ndf.ffill()\n"], "link": "https://stackoverflow.com/questions/51143187/create-new-column-depending-on-values-from-other-column"}
{"id": 226, "q": "Applying a function to chunks of the Dataframe", "d": "I have a (for instance - simplified version) and generated 20 bootstrap resamples that are all now in the same df but differ in the Resample Nr. Now I want to apply a certain function on each Reample Nr. Say: The outlook would look like this: So there are 20 different new values. I know there is a df.iloc command where I can specify my row selection but I would like to find a command where I don't have to repeat the code for the 20 samples. My goal is to find a command that identifies the Resample Nr. automatically and then calculates the function for each Resample Nr. How can I do this? Thank you!", "q_apis": "all now apply values iloc where where repeat", "io": " A B 0 2.0 3.0 1 3.0 4.0 <s> A B 0 1 0 2.0 3.0 1 1 1 3.0 4.0 2 2 1 3.0 4.0 3 2 1 3.0 4.0 .. .. .. .. 39 20 0 2.0 3.0 40 20 0 2.0 3.0 ", "apis": "assign mul groupby transform sum div", "code": ["s = df.assign(x=df['A'].mul(df['B']), y=df['B']**2)\\\n      .groupby(level=1)[['x', 'y']].transform('sum')\ndf['C'] = s['x'].div(s['y'])\n"], "link": "https://stackoverflow.com/questions/63598638/applying-a-function-to-chunks-of-the-dataframe"}
{"id": 149, "q": "Find the latest occurrence of an class item and store how many values are between these two in a pandas DataFrame", "d": "I have a pandas DataFrame with some labels for classes. Now I want to add a column and store how many items are between two elements of the same class. and I want to get this: This is the code I used: This seems circuitous to me. Is there a more elegant way of producing this output?", "q_apis": "item values between DataFrame DataFrame add items between get", "io": " Class 0 0 1 1 2 1 3 1 4 0 <s> Class Shift 0 0 NaN 1 1 NaN 2 1 1.0 3 1 1.0 4 0 4.0 ", "apis": "shift shift groupby shift diff", "code": ["df['shift'] = np.arange(len(df))\ndf['shift'] = df.groupby('Class')['shift'].diff()\nprint(df)\n"], "link": "https://stackoverflow.com/questions/65087280/find-the-latest-occurrence-of-an-class-item-and-store-how-many-values-are-betwee"}
{"id": 471, "q": "How to create a json from pandas data frame where columns are the key", "d": "I have a data frame df The json I am looking for is: I have tries df.to_json but its not working This is not the output i was looking for How to do it in most effective way using pandas/python ?", "q_apis": "where columns to_json", "io": "df: col1 col2 col3 1 2 3 4 5 6 7 8 9 <s> { \"col1\": 1, \"col1\": 4, \"col1\": 7, }, { \"col2\": 2, \"col2\": 5, \"col2\": 8 }, { \"col3\": 3, \"col3\": 6, \"col3\": 9, } ", "apis": "DataFrame columns to_json columns join join index columns", "code": ["import json\nfrom io import StringIO\n\ndf = pd.DataFrame(np.arange(1,10).reshape((3,3)), columns=['col1','col2','col3'])\nio = StringIO()\ndf.to_json(io, orient='columns')\nparsed = json.loads(io.getvalue())\nwith open(\"pretty.json\", '+w') as of:\n    json.dump(parsed, of, indent=4)\n", "with open(\"exact.json\", \"w+\") as of:\n    of.write('[\\n\\t{\\n' + '\\t},\\n\\t{\\n'.join([\"\".join([\"\\t\\t\\\"%s\\\": %s,\\n\"%(c, df[c][i]) for i in df.index]) for c in df.columns])+'\\t}\\n]')\n"], "link": "https://stackoverflow.com/questions/56592574/how-to-create-a-json-from-pandas-data-frame-where-columns-are-the-key"}
{"id": 494, "q": "Choose a value from a set of columns based on value and create new column with the value?", "d": "so if I have a pandas Dataframe like: and want to insert row 'E' by choosing from columns 'A', 'B', or 'C' based on conditions in column 'D', how would I go about doing this? For example: if D == a, choose 'A', else choose 'B', outputting: Thanks in advance!", "q_apis": "value columns value value insert columns", "io": " A B C D 0 1 2 3 a 1 2 4 6 a 2 4 8 8 b 3 2 3 5 c <s> A B C D E 0 1 2 3 a 1 1 2 4 6 a 2 2 4 8 8 b 8 3 2 3 5 c 3 ", "apis": "lookup index array dtype lookup index", "code": ["df.lookup(df.index,df.D.str.upper())\nOut[749]: array([1, 2, 8, 5], dtype=int64)\n\ndf['E']=df.lookup(df.index,df.D.str.upper())\n"], "link": "https://stackoverflow.com/questions/55756126/choose-a-value-from-a-set-of-columns-based-on-value-and-create-new-column-with-t"}
{"id": 525, "q": "Matching dictionaries with columns and indices in DataFrame | python", "d": "I have a DataFrame with column names as on example and the indices from 0 to 1000. The dataframe is filled with zeros. Then, I have dictionary, e.g.: Dictionary name edited in order not to confuse anyone later. My goal is to: for every dict key match it with the column for every number in the list as dictionary value to match with the index if there is a match of index and column, then change the cell to 1 else: leave zero How would you do that?", "q_apis": "columns indices DataFrame DataFrame names indices name value index index", "io": "House 1 | House 2 | House 5 | House 8 | ... 0 1 2 3 4... <s> dict_of_houses = {'House 1':[100,201,306,387,500,900],'House 2':[31,87,254,675,987],'House 5':[23,45,67,123,345,654,789,808,864,987,999],'House 8':[23,675,786,858,868,912,934]} ", "apis": "indices items loc indices", "code": ["for house, indices in dict_.items():\n    df.loc[indices, house] = 1\n"], "link": "https://stackoverflow.com/questions/54460092/matching-dictionaries-with-columns-and-indices-in-dataframe-python"}
{"id": 327, "q": "Pandas groupby and change/reassign one element", "d": "I want to given dataframe, and then, for each group, for given column overwrite the value of its last element (of each group) to (with being the sum of all the elements apart from the last one). Note that after performing the operation, the sum of all values in for each group is equal to . For example, for this input dataframe (grouping by and ): the expected output would be: I managed to perform the operation using loop: but I am looking for more elegant way of doing this, without explicitly looping through each group. I tried this: But I don't really know how to assemble those outputs given that their indices differ.", "q_apis": "groupby value last sum all last sum all values indices", "io": " c1 c2 p 0 x a 0.4 1 y a 0.2 2 x a 0.3 3 y b 0.6 <s> c1 c2 p 0 x a 0.4 1 y a 1.0 2 x a 0.6 3 y b 1.0 ", "apis": "groupby apply assign iloc sum reset_index drop iloc sum iloc groupby apply", "code": ["df.groupby(['c1', 'c2']).apply(\n        lambda x: x.assign(p=x['p'][:-1].tolist()+[1 - x.iloc[:-1].sum()['p']])\n).reset_index(level=[0,1], drop=True)\n", "def func(row):\n    result = 1 - row.iloc[:-1].sum()['p']\n    row['p'].iloc[-1] = result\n    return row\n\ndf.groupby(['c1', 'c2']).apply(func)\n"], "link": "https://stackoverflow.com/questions/60875750/pandas-groupby-and-change-reassign-one-element"}
{"id": 337, "q": "Pandas dataframe compare columns with one value and get this row and previous row into another dataframe", "d": "I have a dataframe like this: I want to create another dataframe with and also store its previous row. It should look like this: What is the best way. Thanks in advance.", "q_apis": "compare columns value get", "io": ">>> df A B 0 1 56 1 2 75 2 3 102 3 4 15 4 5 19 5 6 116 <s> >>> df1 A B 1 2 75 2 3 102 4 5 19 5 6 116 ", "apis": "gt shift gt shift", "code": ["df1 = df[df.B.gt(100) | df.B.shift(-1).gt(100)]\n", "df1 = df[(df.B>100) | (df.B.shift(-1)>100)]\n"], "link": "https://stackoverflow.com/questions/60486231/pandas-dataframe-compare-columns-with-one-value-and-get-this-row-and-previous-ro"}
{"id": 657, "q": "pandas sum the differences between two columns in each group", "d": "I have a looks like, the of and are , and are of ; I like to and and get the differences between and , but I don't know how to sum such differences in each group and assign the values to a new column say , possibly in a new ,", "q_apis": "sum between columns get between sum assign values", "io": "A B C D 2017-10-01 2017-10-11 M 2017-10 2017-10-02 2017-10-03 M 2017-10 2017-11-01 2017-11-04 B 2017-11 2017-11-08 2017-11-09 B 2017-11 2018-01-01 2018-01-03 A 2018-01 <s> C D E M 2017-10 11 M 2017-10 11 B 2017-11 4 B 2017-11 4 A 2018-01 2 ", "apis": "merge groupby apply sum reset_index days days days days days", "code": ["df.merge(df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A']).sum(level=[0,1]).reset_index())\nOut[292]: \n           A          B  C        D       0\n0 2017-10-01 2017-10-11  M  2017-10 11 days\n1 2017-10-02 2017-10-03  M  2017-10 11 days\n2 2017-11-01 2017-11-04  B  2017-11  4 days\n3 2017-11-08 2017-11-09  B  2017-11  4 days\n4 2018-01-01 2018-01-03  A  2018-01  2 days\n"], "link": "https://stackoverflow.com/questions/49324988/pandas-sum-the-differences-between-two-columns-in-each-group"}
{"id": 156, "q": "Choose the next number from columns in Pandas", "d": "I want to make a new column that shows the next biggest value after within the columns. Following is my intended result: I have been researching it a little but no luck. Some help will be appreciated, Thanks!", "q_apis": "columns value columns", "io": "ID Op Cl V C R0 R1 R2 R3 R4 R5 UN 22.85 22.86 8830500 0.21 25 34 12 87 105 102 SS 55.01 52.67 6500 5.45 84 122 147 124 644 788 PN 90.00 90.99 1000 102 89 55 100 156 44 87 PI 184.99 182.38 15000 84 56 77 97 45 44 33 <s> ID Op Cl V C R0 R1 R2 R3 R4 R5 X UN 22.85 22.86 8830500 0.21 25 34 12 87 105 102 25 SS 55.01 52.67 6500 5.45 84 122 147 124 644 788 84 PN 90.00 90.99 1000 102 89 55 100 156 44 87 100 PI 184.99 182.38 15000 84 56 77 97 45 44 33 NaN ", "apis": "columns filter where columns mask gt values idxmax lookup where mask any lookup index mask idxmax filter values now mask values assign where mask any shape mask argmax", "code": ["# extract the `R` columns\ns = df.filter(like='R')\n\n# find out where these columns are larger than `Cl`:\nmask = s.gt(df['Cl'], axis='rows')\n\n# extract the values with `idxmax` and `lookup`:\ndf['X'] = np.where(mask.any(1), s.lookup(s.index,mask.idxmax(1)), np.nan)\n", "# extract and sort by rows\ns = np.sort(df.filter(like='R').values, axis=1)\n\n# now we work with numpy data:\nmask = s > df['Cl'].values[:,None]\n\n# check and assign\ndf['X'] = np.where(mask.any(1), s[np.arange(s.shape[0]),mask.argmax(1)], np.nan)\n"], "link": "https://stackoverflow.com/questions/64972959/choose-the-next-number-from-columns-in-pandas"}
{"id": 252, "q": "Pandas Conditional Rolling Sum of Two Columns", "d": "I have four columns in a data frame like so: And I want to conditionally sum by: D has a value, then D Else, take value from previous row for D plus C So for above, D would be . I've been able to do this successfully with a for loop But the for loop is obviously really expensive, anti-pattern and basically doesn't run over my whole data frame. I'm trying to copy an excel function here that has basically been written over a panel of data, and for the life of me I cannot figure out how to do this with: Simply calculating different column values I was attempting to do it with for a while, but I realized that I kept having to make a separate column for each row, and that's why I went with a for loop. Generalized to Groups Thanks to Scott Boston for the help!", "q_apis": "columns sum value take value copy values", "io": " A B C D 75472 d1 x -36.0 0.0 75555 d2 x -38.0 0.0 75638 d3 x -18.0 0.0 75721 d4 x -18.0 1836.0 75804 d5 x 1151.0 0.0 75887 d6 x 734.0 0.0 75970 d7 x -723.0 0.0 <s> [-36, -74, -92, 1836, 2987, 3721, 2998]", "apis": "cumsum mask combine_first groupby cumsum cumsum mask combine_first", "code": ["grp = (df['D'] != 0).cumsum()\ndf['D_new'] = df['D'].mask(df['D'] == 0).combine_first(df['C']).groupby(grp).cumsum()\ndf\n", "grp = (df['D'] != 0).cumsum()\n", "df['newCD'] = df['D'].mask(df['D'] == 0).combine_first(df['C'])\n"], "link": "https://stackoverflow.com/questions/62935300/pandas-conditional-rolling-sum-of-two-columns"}
{"id": 179, "q": "Subtract from every value in a DataFrame", "d": "I have a dataframe that looks like this: I want to subtract the movie scores from each movie so the output would look like this: The actual dataframe has thousands of movies and the movies are referenced by name so im trying to find a solution to comply with that. I should have also mention that the movies are not listed in order like [\"movie1\", \"movie2\", \"movie3\"], they are listed by their titles instead like [\"Star Wars\", \"Harry Potter\", \"Lord of the Rings\"]. The dataset could be changed so I wont know what the last movie in the list is.", "q_apis": "value DataFrame name last", "io": "userId movie1 movie2 movie3 movie4 score 0 4.1 2.1 1.0 NaN 2 1 3.1 1.1 3.4 1.4 1 2 2.8 NaN 1.7 NaN 3 3 NaN 5.0 NaN 2.3 4 4 NaN NaN NaN NaN 1 5 2.3 NaN 2.0 4.0 1 <s> userId movie1 movie2 movie3 movie4 score 0 2.1 0.1 -1.0 NaN 2 1 2.1 0.1 2.4 0.4 1 2 -0.2 NaN -2.3 NaN 3 3 NaN 1.0 NaN -1.7 4 4 NaN NaN NaN NaN 1 5 1.3 NaN 1.0 3.0 1 ", "apis": "columns columns isin values", "code": ["x = df.columns[~df.columns.isin(['userId', 'score'])]\ndf[x] = df[x] - df.score.values[:, None]\n"], "link": "https://stackoverflow.com/questions/64625342/subtract-from-every-value-in-a-dataframe"}
{"id": 372, "q": "Replace comma-separated values in a dataframe with values from another dataframe", "d": "this is my first question on StackOverflow, so please pardon if I am not clear enough. I usually find my answers here but this time I had no luck. Maybe I am being dense, but here we go. I have two pandas dataframes formatted as follows df1 df2 Basically, Ref_ID in df2 contains IDs that form the string contained in the field References in df1 What I would like to do is to replace values in the References field in df1 so it looks like this: So far, I had to deal with columns and IDs with a 1-1 relationship, and this works perfectly Pandas - Replacing Values by Looking Up in an Another Dataframe But I cannot get my mind around this slightly different problem. The only solution I could think of is to re-iterate a for and if cycles that compare every string of df1 to df2 and make the substitution. This would be, I am afraid, very slow as I have ca. 2000 unique Ref_IDs and I have to repeat this operation in several columns similar to the References one. Anyone is willing to point me in the right direction? Many thanks in advance.", "q_apis": "values values first time contains replace values columns get compare unique repeat columns right", "io": "+------------+-------------+ | References | Description | +------------+-------------+ | 1,2 | Descr 1 | | 3 | Descr 2 | | 2,3,5 | Descr 3 | +------------+-------------+ <s> +-------------------------------------+-------------+ | References | Description | +-------------------------------------+-------------+ | Smith (2006); Mike (2009) | Descr 1 | | John (2014) | Descr 2 | | Mike (2009);John (2014);Jill (2019) | Descr 3 | +-------------------------------------+-------------+ ", "apis": "DataFrame DataFrame explode map assign astype set_index groupby agg explode map assign astype set_index groupby agg join", "code": ["df1 = pd.DataFrame({'Reference':['1,2','3','1,3,5'], 'Description':['Descr 1', 'Descr 2', 'Descr 3']})\ndf2 = pd.DataFrame({'Ref_ID':[1,2,3,4,5,6], 'ShortRef':['Smith (2006)',\n                                                       'Mike (2009)',\n                                                       'John (2014)',\n                                                       'Cole (2007)',\n                                                       'Jill (2019)',\n                                                       'Tom (2007)']})\n\ndf1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(list))\n", "df1['Reference2'] = (df1['Reference'].str.split(',')\n                                     .explode()\n                                     .map(df2.assign(Ref_ID=df2.Ref_ID.astype(str))\n                                             .set_index('Ref_ID')['ShortRef'])\n                                     .groupby(level=0).agg(';'.join))\n"], "link": "https://stackoverflow.com/questions/59617019/replace-comma-separated-values-in-a-dataframe-with-values-from-another-dataframe"}
{"id": 245, "q": "How to find first occurrence of a significant difference in values of a pandas dataframe?", "d": "In a Pandas DataFrame, how would I find the first occurrence of a large difference between two values at two adjacent indices? As an example, if I have a DataFrame column A with data , I would want index holding 1.5, which would be 5. In my code below, it would give me the index holding 7.2, because . How should I fix this problem, so I get the index of the first 'large difference' occurrence?", "q_apis": "first difference values DataFrame first difference between values at indices DataFrame index index get index first difference", "io": "[1, 1.1, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1] <s> 15 - 7.2 > 7 - 1.5", "apis": "DataFrame diff abs index quantile index index mean values Series index index index eq squeeze argmax", "code": ["df = pd.DataFrame({'A': [1, 1.05, 1.2, 1.3, 1.4, 1.5, 7, 7.1, 7.2, 15, 15.1]})\ndifferences = df['A'].diff(-1).abs()\nidx = differences.index[differences >= differences.quantile(.75)][0]\nprint(idx, differences[idx])\n", "idx = differences.index[differences >= 1.5][0]\n", "idx = differences.index[differences >= differences.mean()][0]\n", "from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2).fit(differences.values[:-1].reshape(-1, 1))\nclusters = pd.Series(kmeans.labels_, index=differences.index[:-1])\nidx = clusters.index[clusters.eq(np.squeeze(kmeans.cluster_centers_).argmax())][0]\n"], "link": "https://stackoverflow.com/questions/62989401/how-to-find-first-occurrence-of-a-significant-difference-in-values-of-a-pandas-d"}
{"id": 299, "q": "How to find and replace values of even-positioned elements in sequence", "d": "I have a list as follows: There are sequences of elements whose value's distances are equal. In this list, that distance is , for example, . How can I replace the value of the even-positioned elements for each of these sequences / sublists with a new value, say -1. The output will be:", "q_apis": "replace values value replace value value", "io": "list_1 = [12, 15, 18, 21, 6, 9, 7, 21, 38, 62, 65, 68, 81, 21, 25, 96, 101, 8, 11] <s> list_2 = [12, -1, 18, -1, 6, -1, 7, 21, 38, 62, -1, 68, 81, 21, 25, 96, 101, 8, -1] ", "apis": "Series loc diff index loc groupby diff ne cumsum cumcount", "code": ["s = pd.Series(list_1)\ns.loc[(s.diff() == 3) & (s.index % 2 == 1)] = -1\n", "s.loc[s.groupby(s.diff().ne(3).cumsum()).cumcount() % 2 == 1] = -1\n"], "link": "https://stackoverflow.com/questions/61615757/how-to-find-and-replace-values-of-even-positioned-elements-in-sequence"}
{"id": 632, "q": "Lengthening a DataFrame based on stacking columns within it in Pandas", "d": "I am looking for a function that achieves the following. It is best shown in an example. Consider: which looks like: I would like to collapase the and columns, lengthening the DataFame where necessary, so that the output is: That is, one row for each combination between either and , or and . I am looking for a function that does this relatively efficiently, as I have multiple s and many rows.", "q_apis": "DataFrame columns columns where between", "io": " x y1 y2 0 1 2 3 1 4 5 NaN <s> x y 0 1 2 1 1 3 2 4 5 ", "apis": "columns iloc values repeat values DataFrame iloc values repeat values DataFrame", "code": ["def gather_columns(df):\n    col_mask = [i.startswith('y') for i in df.columns]\n    ally_vals = df.iloc[:,col_mask].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n", "def gather_columns_v2(df):\n    ally_vals = df.iloc[:,1:].values\n    y_valid_mask = ~np.isnan(ally_vals)\n\n    reps = np.count_nonzero(y_valid_mask, axis=1)\n    x_vals = np.repeat(df.x.values, reps)\n    y_vals = ally_vals[y_valid_mask]\n    return pd.DataFrame({'x':x_vals, 'y':y_vals})\n"], "link": "https://stackoverflow.com/questions/50481372/lengthening-a-dataframe-based-on-stacking-columns-within-it-in-pandas"}
{"id": 579, "q": "Convert a Dense Vector to a Dataframe using Pyspark", "d": "Firstly I tried everything in the link below to fix my error but none of them worked. How to convert RDD of dense vector into DataFrame in pyspark? I am trying to convert a dense vector into a dataframe (Spark preferably) along with column names and running into issues. My column in spark dataframe is a vector that was created using Vector Assembler and I now want to convert it back to a dataframe as I would like to create plots on some of the variables in the vector. Approach 1: Below is the Error Approach 2: Error: I also tried to convert the dataframe into a Pandas dataframe and after that I am not able to split the values into separate columns Approach 3: Above code runs fine but I still have only one column in my dataframe with all the values separated by commas as a list. Any help is greatly appreciated! EDIT: Here is how my temp dataframe looks like. It just has one column all_features. I am trying to create a dataframe that splits all of these values into separate columns (all_features is a vector that was created using 200 columns) Expected output is a dataframe with all 200 columns separated out in a dataframe Here is how my Pandas DF output looks like", "q_apis": "DataFrame names now values columns all values all values columns columns all columns", "io": "+----------------------------+ | col1| col2| col3|... +----------------------------+ |0.01193689934723|0.0|0.5049431301173817... |0.04774759738895|0.0|0.1657316216149636... |0.0|0.0|7.213126372469... |0.02387379869447|0.0|0.1866693496827619|... |1.89796699621085|0.0|0.3192169213385746|... +----------------------------+ only showing top 5 rows <s> 0 0 [0.011936899347238104, 0.0, 0.5049431301173817... 1 [0.047747597388952415, 0.0, 0.1657316216149636... 2 [0.0, 0.0, 0.19441761495525278, 7.213126372469... 3 [0.023873798694476207, 0.0, 0.1866693496827619... 4 [1.8979669962108585, 0.0, 0.3192169213385746, ... ", "apis": "map item select", "code": ["#column_names\ntemp = temp.rdd.map(lambda x:[float(y) for y in x['all_features']]).toDF(column_names)\n", "import pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\nsplits = [F.udf(lambda val: float(val[i].item()),FloatType()) for i in range(200)]\ntemp = temp.select(*[s(F.col('all_features')).alias(c) for c,s in zip(column_names,splits)])\ntemp.show()\n"], "link": "https://stackoverflow.com/questions/52545438/convert-a-dense-vector-to-a-dataframe-using-pyspark"}
{"id": 244, "q": "Filter dataframe rows which contribute to X% of values in one column", "d": "I have a dataframe: I want to see only those rows which contribute to 90% of Col3. In this case the expected output will be : I tried the below but is doesnt work as expected: Is there any solution for the same?", "q_apis": "values any", "io": "df Col1 Col2 Col3 A B 5 C D 4 E F 1 <s> Col1 Col2 Col3 A B 5 C D 4 ", "apis": "sort_values reset_index drop cumsum sum idxmin", "code": ["df = df[df.Col3 > 0] # optionally remove 0 valued rows\ndf = df.sort_values(by='Col3', ascending=False).reset_index(drop=True)\ntotals = df.Col3.cumsum()\ncutoff = totals[totals >= df.Col3.sum() * .7].idxmin()\nprint(df[:cutoff + 1])\n"], "link": "https://stackoverflow.com/questions/63059265/filter-dataframe-rows-which-contribute-to-x-of-values-in-one-column"}
{"id": 512, "q": "Pandas: Complex merge of DataFrames with date basis", "d": "I have 2 pandas 's that I need to merge in a bit of a complex manner so I am in need of some help. DataFrame to be inserted: DataFrame to receive insertion 1) The needs to establish a common basis for (notice column is different), thus the needs to be organized into , , , where the dates of , , and are used for assembling to correctly reflect the values in , , at a certain date. Sample output from step 1: 2) will need to be sorted by date according to and the columns , , will be added as columns in the . I imagine this will follow similar methods as in 1). Sample output from step 2: 3) The new columns , , will need to be filled forward with cumsum but I think I got that down: ~ Sample output from step 3: So the end goal would result in the values and shares held according to the index. For 's that will not have a column value (since column is \"missing\" some dates) in the resulting , it would be best to make that value but 0 would suffice. Happy to clarify anything, I appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! EDIT: Trying to merge in a loop now since number of - pairs may vary. I now have a list of separate for the - pairs: . Since number of pairs may vary, it seems best not to based on column labels hence .", "q_apis": "merge date merge DataFrame DataFrame where values at date step date columns columns step columns cumsum step values index value value any all at merge now now", "io": " 0 1 2 3 4 5 0 1998-01-02 16.25 2014-03-27 558.46 1998-01-02 131.13 1 1998-01-05 15.88 2014-03-28 559.99 1998-01-05 130.38 2 1998-01-06 18.94 2014-03-31 556.97 1998-01-06 131.13 3 1998-01-07 17.50 2014-04-01 567.16 1998-01-07 129.56 4 1998-01-08 18.19 2014-04-02 567.00 1998-01-08 130.50 5 1998-01-09 18.19 2014-04-03 569.74 1998-01-09 127.00 6 1998-01-12 18.25 2014-04-04 543.14 1998-01-12 129.50 7 1998-01-13 19.50 2014-04-07 538.15 1998-01-13 132.13 8 1998-01-14 19.75 2014-04-08 554.90 1998-01-14 131.13 9 1998-01-15 19.19 2014-04-09 564.14 1998-01-15 132.31 10 1998-01-16 18.81 2014-04-10 540.95 1998-01-16 135.25 11 1998-01-20 19.06 2014-04-11 530.60 1998-01-20 137.81 12 1998-01-21 18.91 2014-04-14 532.52 1998-01-21 137.00 13 1998-01-22 19.25 2014-04-15 536.44 1998-01-22 138.63 14 1998-01-23 19.50 2014-04-16 556.54 1998-01-23 138.25 15 1998-01-26 19.44 2014-04-17 536.10 1998-01-26 141.75 <s> 0 1 3 5 0 1998-01-02 16.25 NA 131.13 1 1998-01-05 15.88 NA 130.38 2 1998-01-06 18.94 NA 131.13 3 1998-01-07 17.50 NA 129.56 4 1998-01-08 18.19 NA 130.50 5 1998-01-09 18.19 NA 127.00 6 1998-01-12 18.25 NA 129.50 7 1998-01-13 19.50 NA 132.13 8 1998-01-14 19.75 NA 131.13 ... 10 2014-04-10 18.81 558.46 135.25 11 2014-04-11 19.06 559.99 137.81 12 2014-04-14 18.91 556.97 137.00 13 2014-04-15 19.25 567.16 138.63 14 2014-04-16 19.50 567.00 138.25 15 2014-04-17 19.44 569.74 141.75 ... ", "apis": "merge left right dropna columns", "code": ["final_df=pd.merge(left=rec_df, right=insert_df, left_index=True, right_index=True, dropna=False)\n", "df=df[[list of columns in order]]"], "link": "https://stackoverflow.com/questions/54752268/pandas-complex-merge-of-dataframes-with-date-basis"}
{"id": 279, "q": "extract a value from a pandas dataframe dict into another dataframe", "d": "df.head(10) How to convert the above dataframe into a new dataframe by selecting X: df.info() shows", "q_apis": "value head info", "io": " XYZVal 0 {\"X\":\"56.68\",\"Y\":\"51.56\",\"Z\":\"100\"} 1 {\"X\":\"58.05\",\"Y\":\"52.37\",\"Z\":\"62.6\"} 2 {\"X\":\"59.32\",\"Y\":\"54.48\",\"Z\":\"69.59\"} 3 {\"X\":\"58.51\",\"Y\":\"36.36\",\"Z\":\"82.76\"} 4 {\"X\":\"65.21\",\"Y\":\"60.26\",\"Z\":\"71.06\"} 5 {\"X\":\"57.64\",\"Y\":\"52.07\",\"Z\":\"67.89\"} 6 {\"X\":\"58.24\",\"Y\":\"50\",\"Z\":\"75\"} 7 {\"X\":\"57.69\",\"Y\":\"52.13\",\"Z\":\"68.64\"} 8 {\"X\":\"57.83\",\"Y\":\"53.05\",\"Z\":\"65.92\"} 9 {\"X\":\"60.87\",\"Y\":\"51.73\",\"Z\":\"71.35\"} <s> { 56.68 ,58.05 ,59.32 ,58.51 ,65.21 ,57.64 ,58.24 ,57.69 ,57.83 ,60.87 } ", "apis": "apply", "code": ["import json\ndf['XYZVal'].apply(lambda x: json.loads(x)[\"X\"])\n"], "link": "https://stackoverflow.com/questions/62351000/extract-a-value-from-a-pandas-dataframe-dict-into-another-dataframe"}
{"id": 618, "q": "Pandas count values greater than current row in the last n rows", "d": "How to get count of values greater than current row in the last n rows? Imagine we have a dataframe as following: I am trying to get a table such as following where n=3. Thanks in advance.", "q_apis": "count values last get count values last get where", "io": " col_a 0 8.4 1 11.3 2 7.2 3 6.5 4 4.5 5 8.9 <s> col_a col_b 0 8.4 0 1 11.3 0 2 7.2 2 3 6.5 3 4 4.5 3 5 8.9 0 ", "apis": "shape shape shape shape shape rolling apply sum astype count iloc max sum items values sum DataFrame size columns", "code": ["np.random.seed(1256)\nn = 3\n\ndef rolling_window(a, window):\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n\ndef roll(df):\n    df['new'] = (df['col_a'].rolling(n+1, min_periods=1).apply(lambda x: (x[-1] < x[:-1]).sum(), raw=True).astype(int))\n    return df\n\ndef list_comp(df):\n    df['count'] = [(j < df['col_a'].iloc[max(0, i-3):i]).sum() for i, j in df['col_a'].items()]\n    return df\n\ndef strides(df):\n    x = np.concatenate([[np.nan] * (n), df['col_a'].values])\n    arr = rolling_window(x, n + 1)\n    df['new1'] = (arr[:, :-1] > arr[:, [-1]]).sum(axis=1)\n    return df\n\n\ndef make_df(n):\n    df = pd.DataFrame(np.random.randint(20, size=n), columns=['col_a'])\n    return df\n\nperfplot.show(\n    setup=make_df,\n    kernels=[list_comp, roll, strides],\n    n_range=[2**k for k in range(2, 15)],\n    logx=True,\n    logy=True,\n    xlabel='len(df)')\n"], "link": "https://stackoverflow.com/questions/51039857/pandas-count-values-greater-than-current-row-in-the-last-n-rows"}
{"id": 360, "q": "Multidimensional numpy.ndarray from multi-indexed pandas.DataFrame", "d": "I want to produce a 3-dimensional numpy.ndarray from a multi-indexed pandas.DataFrame. More precisely, say I have: which gives me and I want to write a function which returns, with the above argument, the numpy.ndarray Pandas multi-index looks like a substitute for multidimensional arrays, but it does not provide (or at least does not document) ways to go back and forth... Thanks.", "q_apis": "DataFrame DataFrame index at", "io": " z t x y 1 1 1 10 2 2 20 2 1 5 30 <s> [[[1, 10], [2, 20]], [[5, 30], [NaN, NaN]]] ", "apis": "to_xarray values transpose", "code": ["df.to_xarray().to_array().values.transpose(1,2,0)\n\n>>[[[ 1. 10.]\n  [ 2. 20.]]\n\n [[ 5. 30.]\n  [nan nan]]]\n"], "link": "https://stackoverflow.com/questions/59935954/multidimensional-numpy-ndarray-from-multi-indexed-pandas-dataframe"}
{"id": 187, "q": "Checking for NaNs in many columns in Pandas", "d": "I want to add a binary column to my dataframe based on whether given columns contain NaN or not. I have tried to do it with the below code. but I got a ValueError at the line before last. Sample input: Expected output: I want to check NaNs only for A, B, C columns.", "q_apis": "columns add columns at last columns", "io": "A B C D 10 NaN 40 NaN NaN NaN 80 90 20 45 NaN 89 NaN NaN NaN 46 <s> A B C D E 10 NaN 40 NaN 0 NaN NaN 80 90 0 20 45 NaN 89 0 NaN NaN NaN 46 1 ", "apis": "isna all mean std index isin drop dropna index mean std count eq view mean std where isnull all mean std", "code": ["In [1720]: %timeit df['ismissing'] = df[['A','B','C']].isna().all(axis=1)\n989 \u00b5s \u00b1 70 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "In [1719]: %timeit df['New']=~df.index.isin(df.drop('D',1).dropna(thresh=1).index)\n2.05 ms \u00b1 113 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "In [1724]: %timeit df['all_nan'] = df[['A','B','C']].count(axis=1).eq(0).view('i1')\n1.48 ms \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "In [1723]: %timeit dat['E'] = np.where(dat[['A','B','C']].isnull().all(1), 1, 0)\n914 \u00b5s \u00b1 18.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/62006098/checking-for-nans-in-many-columns-in-pandas"}
{"id": 155, "q": "Explode pandas dataframe singe row into multiple rows across multiple columns simultaneously", "d": "I have a dataframe as I want to break each record in such a way that values in column and explode into multiple rows but such that the first value in after splitting upon corresponds to the first value in after splitting upon . So my should look like this: NOTE: this is not the same as Split (explode) pandas dataframe string entry to separate rows as here the exploding/splitting of one record is not just across one column but the need is to split or explode one row into multiple rows, in two columns simultaneously. Any help is appreciated. Thanks", "q_apis": "columns values explode first value first value explode explode columns", "io": "df col1 act_id col2 -------------------- 0 40;30;30 act1 A;B;C 1 25;50;25 act2 D;E;F 2 70;30 act3 G;H <s> desired_df col1 act_id col2 --------------- 0 40 act1 A 1 30 act1 B 2 30 act1 C 3 25 act2 D 4 50 act2 E 5 25 act2 F 6 70 act3 G 7 30 act3 H ", "apis": "set_index apply Series stack dropna reset_index columns", "code": ["df2.set_index('act_id').apply(lambda x: pd.Series(x.col1.split(';'),x.col2.split(';')), axis=1).stack().dropna().reset_index()\n\ndf2.columns = ['col1','act_id','col2']\n"], "link": "https://stackoverflow.com/questions/56437405/explode-pandas-dataframe-singe-row-into-multiple-rows-across-multiple-columns-si"}
{"id": 230, "q": "How to combine rows into seperate dataframe python pandas", "d": "i have the following dataset: i want to combine x y z into another dataframe like this: and i want these dataframes for each x y z value like first, second third and so on. how can i select and combine them? desired output:", "q_apis": "combine combine value first second select combine", "io": "A B C D E F 154.6175111 148.0112337 155.7859835 1 1 x 255 253.960131 242.5382584 1 1 x 251.9665958 235.1105659 185.9121703 1 1 x 137.9974994 225.3985177 254.4420772 1 1 x 85.74722877 116.7060415 158.4608395 1 1 x 123.6969939 140.0524405 132.6798037 1 1 x 133.3251695 80.08976196 38.81201612 1 1 y 118.0718812 243.5927927 255 1 1 y 189.5557302 139.9046713 91.90519519 1 1 y 172.3117291 188.000268 129.8155501 1 1 y 48.07634611 21.9183119 25.99669279 1 1 y 23.40525987 8.395857933 25.62371342 1 1 y 228.753009 164.0697727 172.6624107 1 1 z 203.3405006 173.9368303 189.8103708 1 1 z 184.9801932 117.1591341 87.94739034 1 1 z 29.55251224 46.03945452 70.7433477 1 1 z 143.6159623 120.6170926 155.0736604 1 1 z 142.5421179 128.8916843 169.6013111 1 1 z <s> A B C D E F 154.6175111 148.0112337 155.7859835 1 1 x 133.3251695 80.08976196 38.81201612 1 1 y 228.753009 164.0697727 172.6624107 1 1 z A B C D E F 255 253.960131 242.5382584 1 1 x 118.0718812 243.5927927 255 1 1 y 203.3405006 173.9368303 189.8103708 1 1 z A B C D E F 251.9665958 235.1105659 185.9121703 1 1 x 189.5557302 139.9046713 91.90519519 1 1 y 184.9801932 117.1591341 87.94739034 1 1 z A B C D E F 137.9974994 225.3985177 254.4420772 1 1 x 172.3117291 188.000268 129.8155501 1 1 y 29.55251224 46.03945452 70.7433477 1 1 z A B C D E F 85.74722877 116.7060415 158.4608395 1 1 x 48.07634611 21.9183119 25.99669279 1 1 y 143.6159623 120.6170926 155.0736604 1 1 z A B C D E F 123.6969939 140.0524405 132.6798037 1 1 x 23.40525987 8.395857933 25.62371342 1 1 y 142.5421179 128.8916843 169.6013111 1 1 z ", "apis": "groupby cumcount groupby", "code": ["g = df.groupby('F').cumcount()\n\nfor i, g in df.groupby(g):\n    print (g)\n"], "link": "https://stackoverflow.com/questions/63451784/how-to-combine-rows-into-seperate-dataframe-python-pandas"}
{"id": 413, "q": "How to make a deepcopy of a dataframe with dataframes within it? (python)", "d": "I want a copy of a dataframe which contains a dataframe. When I change something in the nested dataframe, it shouldn't change in the original dataframe. I have a dataframe like this: Generated with the next code: When I make a deepcopy of the hole dataframe and the nested dataframe and change something in the nested dataframe in the copy, the value also changes in the original. output: but I want: What is the fix for this problem?", "q_apis": "copy contains copy value", "io": " 0 1 2 0 1 2 doei 1 4 5 6 0 1 2 0 1 2 doei 1 4 5 6 <s> 0 1 2 0 1 2 doei 1 4 5 6 0 1 2 0 1 2 3 1 4 5 6 ", "apis": "iloc iloc iloc iloc", "code": ["import pickle\ndeepCopy = pickle.loads(pickle.dumps(df))\n\ndeepCopy.iloc[0,2].dfCombinations.iloc[0,2] = \"doei\"\n\nprint(deepCopy.iloc[0,2].dfCombinations)\nprint(\" \")\nprint(df.iloc[0,2].dfCombinations)\n"], "link": "https://stackoverflow.com/questions/58431148/how-to-make-a-deepcopy-of-a-dataframe-with-dataframes-within-it-python"}
{"id": 318, "q": "How to break/pop a nested Dictionary inside a list, inside a pandas dataframe?", "d": "I have a dataframe which has a dictionary inside a nested list and i want to split the column 'C' : expected output :", "q_apis": "pop", "io": "A B C 1 a [ {\"id\":2,\"Col\":{\"x\":3,\"y\":4}}] 2 b [ {\"id\":5,\"Col\":{\"x\":6,\"y\":7}}] <s> A B C_id Col_x Col_y 1 a 2 3 4 2 b 5 6 7 ", "apis": "apply Series concat", "code": ["df[[\"Col\", \"id\"]] = df[\"C\"].apply(lambda x: pd.Series(x[0]))\n", "df = pd.concat([df, json_normalize(df.Col)], axis=1)\n"], "link": "https://stackoverflow.com/questions/60862810/how-to-break-pop-a-nested-dictionary-inside-a-list-inside-a-pandas-dataframe"}
{"id": 326, "q": "Sort pandas dataframe by index", "d": "I have the following pandas dataframe: I would like to order it from highest to lowest based on the index no matter if it is repeated, what I say would look like this: The maximum value corresponds to Cmpd6 = 0.79, followed by Cmpd4 = 0.69 ... at some point Cmpd6 = 0.56 which I would like to leave the list like this: This with each value of the array, no matter how many times the indexes are repeated, I was trying with but it does not produce any of this, I also tried but it does not give me the indexes. How can i fix this? Thanks! My solution:", "q_apis": "index index value at value array any", "io": " Cmpd1 Cmpd2 Cmpd3 Cmpd4 Cmpd5 Cmpd6 Cmpd1 1 Cmpd2 0.4 1 Cmpd3 0.6 0.3 1 Cmpd4 0.46 0.69 0.32 1 Cmpd5 0.57 0.44 0.41 0.51 1 Cmpd6 0.41 0.79 0.33 0.56 0.43 1 <s> Cmpd6 = 0.79 Cmpd4 = 0.69 Cmpdx = x Cmpd6 = 0.56 ", "apis": "stack apply reset_index drop sort_values astype replace stack reset_index drop sort_values", "code": ["s = df.stack()\ns[s.apply(lambda x: type(x) is not str and x < 1)]\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\\\n    .astype(float)\n", "s = df.replace(r'^\\s*$', np.nan, regex=True).stack()\\\n    .reset_index(level=1, drop=True).sort_values(ascending=False)\ns[s < 1]\n"], "link": "https://stackoverflow.com/questions/60897173/sort-pandas-dataframe-by-index"}
{"id": 592, "q": "easy tool to filtering columns with specific conditions using pandas", "d": "I'm wondering if exist a tool in python to filter data between columns that follow an specific condition. I need to generate a clean dataframe where all the data in column 'A' must have the same consecutive number in column 'E'(and this number is repeated at least twice). Here an example: The output will be:", "q_apis": "columns filter between columns where all at", "io": "df Out[30]: A B C D E 6 1 2.366 8.621 10.835 1 7 1 2.489 8.586 10.890 2 8 1 2.279 8.460 10.945 2 9 1 2.296 8.559 11.000 2 10 2 2.275 8.620 11.055 2 11 2 2.539 8.528 11.110 2 50 2 3.346 5.979 10.175 5 51 3 3.359 5.910 10.230 1 52 3 3.416 5.936 10.285 1 <s> df Out[31]: A B C D E 7 1 2.489 8.586 10.890 2 8 1 2.279 8.460 10.945 2 9 1 2.296 8.559 11.000 2 10 2 2.275 8.620 11.055 2 11 2 2.539 8.528 11.110 2 51 3 3.359 5.910 10.230 1 52 3 3.416 5.936 10.285 1 ", "apis": "groupby shift cumsum filter size groupby shift cumsum transform size", "code": ["import numpy as np\n\ndf.groupby((df.E != df.E.shift(1)).cumsum()).filter(lambda x: np.size(x.E) >= 2)\n# or\ndf[df.groupby((df.E != df.E.shift(1)).cumsum()).E.transform('size') >= 2]\n"], "link": "https://stackoverflow.com/questions/52118251/easy-tool-to-filtering-columns-with-specific-conditions-using-pandas"}
{"id": 634, "q": "split dataframe entries at midnight", "d": "I have a , with and datatime. and can be expected to be sorted interally, but gaps/overlaps may occur between consecutive rows. I would like to create a new dataframe with the difference that if row contains midnight (e.g. midnight is contained in [,]), the row is then split in two parts before and after midnight ex: should be", "q_apis": "at overlaps between difference contains", "io": " Start End 0 2010-02-01 00:00:00 2010-02-01 04:00:00 1 2010-02-01 05:03:00 2010-02-01 09:03:00 2 2010-02-01 10:06:00 2010-02-01 14:06:00 3 2010-02-01 15:09:00 2010-02-01 19:09:00 4 2010-02-01 20:12:00 2010-02-02 00:12:00 5 2010-02-02 01:15:00 2010-02-02 05:15:00 <s> Start End 0 2010-02-01 00:00:00 2010-02-01 04:00:00 1 2010-02-01 05:03:00 2010-02-01 09:03:00 2 2010-02-01 10:06:00 2010-02-01 14:06:00 3 2010-02-01 15:09:00 2010-02-01 19:09:00 ----------------------------------------- 4 2010-02-01 20:12:00 2010-02-01 23:59:00 5 2010-02-02 00:00:00 2010-02-02 00:12:00 ----------------------------------------- 6 2010-02-02 01:15:00 2010-02-02 05:15:00 ", "apis": "date date copy", "code": ["splits = df[df.End.dt.date > df.Start.dt.date].copy()\n"], "link": "https://stackoverflow.com/questions/50336251/split-dataframe-entries-at-midnight"}
{"id": 50, "q": "Best way to change column data for all rows over multiple dataframes in pandas?", "d": "Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example :", "q_apis": "all all replace unique update where all apply", "io": "+---+----+ | | id | +---+----+ | 1 | a1 | +---+----+ | 2 | c1 | +---+----+ <s> +---+----+ | | id | +---+----+ | 1 | b1 | +---+----+ ", "apis": "replace map replace replace", "code": ["import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n"], "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas"}
{"id": 200, "q": "Using Python&#39;s max to return two equally large values across columns of a data frame", "d": "I would like to find the column of a data frame with the maximum value per row and if there are multiple equally large values, then return all the column names where those values are. I would like to store all of these values in the last column of the data frame. I have been referencing the following post, and am unsure of how to modify it to handle data frames: Using Python's max to return two equally large values So if my data looked like this My goal is an output that looks like this: I know how to use idxmax(axis=1,skipna = True) to return the first max and know that if I change 0 to Nan in the dataframe it will populate the last row correctly, just not sure how to do this when there are multiple max values. Any help is greatly appreciated ! I am an R programmer and this is my first time in Python.", "q_apis": "max values columns value values all names where values all values last max values idxmax first max last max values first time", "io": "Key Column_1 Column_2 Column_3 0 1 2 3 1 1 1 0 2 0 0 0 <s> Key Column_1 Column_2 Column_3 Column_4 0 1 2 3 Column_3 1 1 1 0 Column_1,Column_2 2 0 0 0 NA ", "apis": "set_index select_dtypes eq max dot columns mask eq all", "code": ["d = df.set_index('Key').select_dtypes('number')\nv = d.eq(d.max(axis=1), axis=0).dot(d.columns + ',').str.rstrip(',')\ndf['Column_4'] = v.mask(d.eq(0).all(axis=1)))\n"], "link": "https://stackoverflow.com/questions/64229332/using-pythons-max-to-return-two-equally-large-values-across-columns-of-a-data-f"}
{"id": 85, "q": "How to set new columns in a multi-column index from a dict with partially specified tuple keys?", "d": "I have a pandas dataframe initialized in the following way: which gives: Now I'd like to add a new column to this dataframe using partial key slicing BUT not in code, I'd like to do this from configuration i.e. a dictionary with partial tuple keys: which gives: notice that setting 'x' doesn't depend on the second component of the key and setting 'y1' and 'y2' do depend on the second component of the key. A possible solution is to fully specify the mapping but this is also not desirable if I have a 100 whose assignment doesn't depend on the second component. I wish to reach the above result but not hard-coding the sliced assignments, instead I'd like to do it from an externalized dictionary: My configuration dictionary would look like this: Is there a pythonic and pandas-tonic way to apply this dictionary with partially specified keys to reach the sliced assignment produced before?", "q_apis": "columns index keys add keys second second second apply keys", "io": "# col1 col2 # key1 key2 # a a1 1 2 # a2 3 4 # b b1 5 6 # b2 7 8 <s> # key1 key2 # a a1 1 2 x # a2 3 4 x # b b1 5 6 y1 # b2 7 8 y2 ", "apis": "value items loc value", "code": ["my_dict = {\n    ('a',): 'x',\n    ('b', 'b1'): 'y1',\n    ('b', 'b2'): 'y2'\n}\n\nfor idx, value in my_dict.items():\n    df.loc[idx, 'desc1'] = value\n"], "link": "https://stackoverflow.com/questions/66398540/how-to-set-new-columns-in-a-multi-column-index-from-a-dict-with-partially-specif"}
{"id": 197, "q": "Rename column in dataframe that contains digits in the middle", "d": "Say I have a dataframe columns as such : To: This need to be done dynamically. My plan is: Use regex to find digits in the MIDDLE of string. Replace to the back of the column name, iteratively. My current code :", "q_apis": "contains columns name", "io": " # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Action_3.@source 1 non-null object 1 Description_3.#text 1 non-null object 2 Code_3.@source 1 non-null object 3 Others 1 non-null object 4 Animal_1 1 non-null object <s> # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Action.@source_3 1 non-null object 1 Description.#text_3 1 non-null object 2 Code.@source_3 1 non-null object 3 Others 1 non-null object 4 Animal_1 1 non-null object ", "apis": "DataFrame columns first last first last columns replace Index dtype", "code": ["df = pd.DataFrame(\n    [],\n    columns=[\n        \"Action_3.@source\",\n        \"Description_3.#text\",\n        \"Code_3.@source\",\n        \"Others\",\n        \"Animal_1\",\n    ],\n)\n\npat = r\"(?P<first>.+)(?P<middle>_\\d)(?P<last>.+)\"\nrepl = lambda m: f\"{m.group('first')}{m.group('last')}{m.group('middle')}\"\n\ndf.columns.str.replace(pat, repl)\n\nIndex(['Action.@source_3', 'Description.#text_3', 'Code.@source_3', 'Others',\n       'Animal_1'],\n      dtype='object')\n"], "link": "https://stackoverflow.com/questions/64275557/rename-column-in-dataframe-that-contains-digits-in-the-middle"}
{"id": 375, "q": "Create a list in list comprehension and then create another list from that list inside the same list comprehension", "d": "That title is a mouthful, so it may be easier to show what I am trying to achieve via code. The above is not where I am having trouble with, but I wanted to provide as much context as possible. I am trying to iterate through the dataframe and retrieve the first element of the column name where the dataframe's element equals 1. I can do this using the following: This generates the first list I need to create: I can assign that output to a variable and perform another list comprehension to get my final output: This outputs my final list of: Is it possible to perform both of these inside the same list comprehension while only receiving that final list as the output? This is somewhat inelegant, but this is what it would look like in non-list comprehension: Thank you for taking the time to look this over!", "q_apis": "where first name where equals first assign get time", "io": "[1, 1, 1, 2, 3, 4] <s> [(1, 1), (1, 1), (1, 2), (2, 3), (3, 4)] ", "apis": "eq dot columns eq dot columns iloc eq dot columns eq dot columns iloc", "code": ["[(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(df.columns.str[0])[1:])]\n#same with .iloc -> [(a,b) for a,b in zip(df.eq(1).dot(df.columns.str[0]),df.eq(1).dot(\n#                                          df.columns.str[0]).iloc[1:])]\n"], "link": "https://stackoverflow.com/questions/59548017/create-a-list-in-list-comprehension-and-then-create-another-list-from-that-list"}
{"id": 248, "q": "How to shuffle a dataframe while maintaining the order of a specific column", "d": "I have a pandas dataframe which I want to shuffle, but keep the order of 1 column. So imagine I have the following df: I want to shuffle the rows but keep the order of the ID column of the first df. My wanted result would be something like this: How do I do this?", "q_apis": "first", "io": "| i | val | val2| ID | | 0 | 2 | 2 |a | | 1 | 3 | 3 |b | | 2 | 4 | 4 |a | | 3 | 6 | 5 |b | | 4 | 5 | 6 |b | <s> | i | val | val2| ID | | 2 | 4 | 4 |a | | 4 | 5 | 6 |b | | 0 | 2 | 2 |a | | 3 | 6 | 5 |b | | 1 | 3 | 3 |b | ", "apis": "DataFrame groupby transform sample", "code": ["df = pd.DataFrame({\"val\": [1, 2, 3, 4, 5, 6, 7], \"ID\": [\"a\", \"b\", \"a\", \"b\", \"a\", \"a\", \"b\"]})\ndf[\"val\"] = df.groupby(\"ID\").transform(lambda x: x.sample(frac=1))\nprint(df)\n"], "link": "https://stackoverflow.com/questions/62972913/how-to-shuffle-a-dataframe-while-maintaining-the-order-of-a-specific-column"}
{"id": 150, "q": "How to filter rows in a dataframe to get only 3 most popular and delete others unused data?", "d": "Theory I have some data on car brands in the US. I have to arrange them on the map of individual states and after hovering over with the mouse, I have to display the 3 most popular brands for a given state. Question I have the following dataframe I need to achieve something like that (structure is probably wrong - I am not sure what kind of structure would be best - I just need data about name of column and its value): Current situation I was able to create something like this: So the situation is identical to the example I gave at the top. Now I have to somehow \"filter this data and keep information about the brand name and its percentage in a given state\". It is a bit difficult for me, can someone please help me?", "q_apis": "filter get delete map name value identical at filter name", "io": " A B C D E 1 20 0 40 10 30 2 0 60 15 5 20 3 50 30 20 0 0 <s> 1 (C: 40) (E: 30) (A: 20) 2 (B: 60) (E: 20) (C: 15) 3 (A: 50) (B: 30) (C: 20) ", "apis": "apply Series nlargest to_dict apply Series nlargest items", "code": ["res = df.apply(lambda x: pd.Series(x).nlargest(3).to_dict(), axis=1)\nprint(res)\n", "res = df.apply(lambda x: list(pd.Series(x).nlargest(3).items()), axis=1)\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65077340/how-to-filter-rows-in-a-dataframe-to-get-only-3-most-popular-and-delete-others-u"}
{"id": 99, "q": "Pandas regex comprehension - isolate single result", "d": "I have a dataframe that's been extracted from an SQL server, and I used regex to extract a string of three dimensions. I need all three dimensions in three new columns so I have used a regular expression for a number optionally separated by a period, and created a column from this findall result. But the result shows as a list and I am unable to index the second dimension. Due to the urgency I have been able to temporarily solve this with a lookaround. But how can I directly extract dimension column extract - not all are in this format code extract for finding dims sample output using the findall result I would need a column for 1493.4 and a second column for 204.2 - I can do the first one but how would I create a column for specific indexes in the regex results. I have tried lambda, list comprehension, and everything else I can think of. So far I cannot find a similar question online and I know it should be simple - but its taken me 2 days! Many thanks for all your help EDIT: To confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern. Additionally, my focus is on obtaining only list item n to a new column and not every item in the list", "q_apis": "all columns index second all sample second first days all where item item", "io": "[1103.5 x 308.8 x 25.4 mm] [33.3 x 13 x 9.5mm] [136.5 x 15 x 12.7 mm] <s> [1493.4, 204.2, 25.4, 0013, 900, 4] [136.5, 15, 12.7, 001, 900, 2] ", "apis": "DataFrame apply Series", "code": ["import pandas as pd\ndf = pd.DataFrame({'dim':['10x10','12x10','10x12']})\ndf['dim_list'] = df.dim.str.findall(r'\\d+')\ndf[['width','height']] = df.dim_list.apply(pd.Series)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/66138245/pandas-regex-comprehension-isolate-single-result"}
{"id": 306, "q": "Match on multiple columns using array", "d": "I'm working on a project where my original dataframe is: But, I have an array with new labels for certain points (for that I only used columns A and B) in the original dataframe. Something like this: My goal is to add the new labels to the original dataframe. I know that the combination of A and B unique is. What is the fastest way to assign the new label to the correct row? This is my try: Wanted output (rows with index 1 and 2 are changed): For small datasets may this be okay with I'm currently using it for datasets with more than 25000 labels. Is there a way that is faster? Also, in some cases I used all columns expect the column 'label'. That dataframe exists out of 64 columns so my method can not be used here. Has someone an idea to improve this? Thanks in advance", "q_apis": "columns array where array columns add unique assign index all columns columns", "io": " A B C label 0 1 2 2 Nan 1 2 4 5 7 2 3 6 5 Nan 3 4 8 7 Nan 4 5 10 3 8 5 6 12 4 8 <s> A B C label 0 1 2 2 Nan 1 2 4 5 5 2 3 6 5 9 3 4 8 7 Nan 4 5 10 3 8 5 6 12 4 8 ", "apis": "values astype view dtype dtype shape loc", "code": ["import numpy as np\n\nX_labeled = [[2, 4], [3,6]]\ny_labeled = [5,9]\n\na = df.values[:,:2].astype(int) #indexing on A and B\n\ndef view_as_1d(a):\n    a = np.ascontiguousarray(a)\n    return a.view(np.dtype((np.void, a.dtype.itemsize * a.shape[-1])))\n\nix = np.in1d(view_as_1d(a), view_as_1d(X_labeled))\ndf.loc[ix, 'label'] = y_labeled\n"], "link": "https://stackoverflow.com/questions/61293428/match-on-multiple-columns-using-array"}
{"id": 351, "q": "Renaming columns on slice of dataframe not performing as expected", "d": "I was trying to clean up column names in a dataframe but only a part of the columns. It doesn't work when trying to replace column names on a slice of the dataframe somehow, why is that? Lets say we have the following dataframe: Note, on the bottom is copy-able code to reproduce the data: I want to clean up the column names (expected output): Approach 1: I can get the clean column names like this: Or Approach 2: But when I try to overwrite the column names, nothing happens: Same for the second approach: This does work, but you have to manually concat the name of the first column, which is not ideal: Is there an easier way to achieve this? Am I missing something? Dataframe for reproduction:", "q_apis": "columns names columns replace names copy names get names names second concat name first", "io": " Value ColAfjkj ColBhuqwa ColCouiqw 0 1 a e i 1 2 b f j 2 3 c g k 3 4 d h l <s> Value ColA ColB ColC 0 1 a e i 1 2 b f j 2 3 c g k 3 4 d h l ", "apis": "columns columns iloc columns", "code": ["df.columns = [df.columns[0]] + list(df.iloc[:, 1:].columns.str[:4])\n"], "link": "https://stackoverflow.com/questions/56293392/renaming-columns-on-slice-of-dataframe-not-performing-as-expected"}
{"id": 43, "q": "Convert dataframe objects to float by iterating over columns", "d": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance", "q_apis": "columns Series Series DataFrame values", "io": " c1 c2 0 - 75.0% 1 -5.5% 65.8% . n - 6.9% <s> c1 c2 0 0.0 75.0 1 -5.5 65.8 . n 0.0 6.9 ", "apis": "replace", "code": ["# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"], "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns"}
{"id": 107, "q": "Join two pandas dataframes based on lists columns", "d": "I have 2 dataframes containing columns of lists. I would like to join them based on 2+ shared values on the lists. Example: In this case we can see that id1 matches id3 because there are 2+ shared values on the lists. So the output will be (columns name are not important and just for example): How can I achieve this result? I've tried to iterate each row in dataframe #1 but it doesn't seem a good idea. Thank you!", "q_apis": "columns columns join values values columns name", "io": "ColumnA ColumnB | ColumnA ColumnB id1 ['a','b','c'] | id3 ['a','b','c','x','y', 'z'] id2 ['a','d,'e'] | <s> ColumnA1 ColumnB1 ColumnA2 ColumnB2 id1 ['a','b','c'] id3 ['a','b','c','x','y', 'z'] ", "apis": "merge apply intersection mean std map intersection mean std DataFrame DataFrame merge map intersection", "code": ["    df = df1.merge(df2, how='cross')         # simplified cross joint for pandas >= 1.2.0\n", "%%timeit\ndf['overlap'] = df.apply(lambda x: \n                         len(set(x['ColumnB1']).intersection(\n                             set(x['ColumnB2']))), axis=1)\n\n\n800 \u00b5s \u00b1 59.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "%%timeit\ndf['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n217 \u00b5s \u00b1 25.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "    data = {'ColumnA1': ['id1', 'id2'], 'ColumnB1': [['a', 'b', 'c'], ['a', 'd', 'e']]}\n    df1 = pd.DataFrame(data)\n\n    data = {'ColumnA2': ['id3', 'id4'], 'ColumnB2': [['a','b','c','x','y', 'z'], ['d','e','f','p','q', 'r']]}\n    df2 = pd.DataFrame(data)\n\n    df = df1.merge(df2, how='cross')             # for pandas version >= 1.2.0\n\n    df['overlap'] = list(map(lambda x, y: len(set(x).intersection(set(y))), df['ColumnB1'], df['ColumnB2']))\n\n    df = df[df['overlap'] >= 2]\n    print (df)\n"], "link": "https://stackoverflow.com/questions/66060591/join-two-pandas-dataframes-based-on-lists-columns"}
{"id": 485, "q": "Is there a way to get the mean value of previous two columns in pandas?", "d": "I want to calculate the mean value of previous two rows and fill the NAN's in my dataframe. There are only few rows with missing values in the column. I tried using and but it only captures the previous or next row/column value and fill NAN. My example data set has 7 columns as below: The output I want:", "q_apis": "get mean value columns mean value values value columns", "io": "X 1990-2000 2000-2010 2010-19 1990-2000 2000-2010 2010-19 Hyderabad 10 20 NAN 1 3 NAN <s> X 1990-2000 2000-2010 2010-19 1990-2000 2000-2010 2010-19 Hyderabad 10 20 15 1 3 2 ", "apis": "iloc mean iloc fillna", "code": ["col_indices = [3, 6]\n\nfor i in col_indices:\n    means = df.iloc[:, [i-1, i-2]].mean(axis=1)\n    df.iloc[:, i].fillna(means, inplace=True)\n"], "link": "https://stackoverflow.com/questions/55986653/is-there-a-way-to-get-the-mean-value-of-previous-two-columns-in-pandas"}
{"id": 223, "q": "Replace NaN with values in a row from previous matching values in column", "d": "I have following data frame (df). And I want to get to this state: I want to go through both columns and replace NaN with appropriate zip_code or city. Here is what I have done but as you can see it didn't fully work. If columns 'zip_mapped' and 'city_mapped' were properly populated I would have just replaced them with original cols. Can anyone help me here?", "q_apis": "values values get columns replace columns", "io": "df city zip_code 0 city1 90287 1 city2 90288 2 city3 80023 3 city4 90210 4 city1 NaN 5 city4 NaN 6 city7 NaN 7 NaN 90210 8 NaN 80023 <s> city zip_code 0 city1 90287 1 city2 90288 2 city3 80023 3 city4 90210 4 city1 90287 5 city4 90210 6 city7 NaN 7 city4 90210 8 city3 80023 ", "apis": "fillna groupby transform first fillna groupby transform first", "code": ["df.zip_code = df.zip_code.fillna(df.zip_code.groupby(df.city).transform('first'))\n\ndf.city = df.city.fillna(df.city.groupby(df.zip_code).transform('first'))\n"], "link": "https://stackoverflow.com/questions/63620593/replace-nan-with-values-in-a-row-from-previous-matching-values-in-column"}
{"id": 606, "q": "How to fill column&#39; value with another column and keep existing?", "d": "I have this dataframe with two column, and I want to fill with its corresponding if exist, also if already have a value keep it don't change it with . Here is an example: input output It's look easy, but I'm beginner in python :( Any help please.", "q_apis": "value value", "io": "c1 c2 HP_0003470 HP_8362789 HP_0093723 MP_0000231 MP_0000231 <s> c1 c2 HP_0003470 HP_8362789 HP_0093723 MP_0000231 MP_0000231 MP_0000231 MP_0000231 ", "apis": "replace fillna replace fillna", "code": ["df['c1'] = df['c1'].replace('',np.NaN).fillna(df['c2'])\ndf['c2'] = df['c2'].replace('',np.NaN).fillna(df['c1'])\n"], "link": "https://stackoverflow.com/questions/51408990/how-to-fill-column-value-with-another-column-and-keep-existing"}
{"id": 477, "q": "pandas create multiple dataframes based on duplicate index dataframe", "d": "If I have a dataframe with duplicates in the index, how would I create a set of dataframes with no duplicates in the index? More precisely, given the dataframe: I would want as output, a list of dataframes: This needs to be scalable to as many dataframes as needed based on the number of duplicates.", "q_apis": "index index index", "io": " a b 1 1 6 1 2 7 2 3 8 2 4 9 2 5 0 <s> a b 1 1 6 2 3 8 a b 1 2 7 2 4 9 a b 2 5 0 ", "apis": "groupby index index max nth", "code": ["import numpy as np\n\ng = df.groupby(df.index)\ncnt = np.bincount(df.index).max()\ndfs = [g.nth(i) for i in range(cnt)]\n"], "link": "https://stackoverflow.com/questions/56271520/pandas-create-multiple-dataframes-based-on-duplicate-index-dataframe"}
{"id": 258, "q": "Replace last non NaN value in row", "d": "I'd like to replace all the last non NaNs in rows in data frame with NaN value. I have 300 rows and 1068 columns in my data frame. and each row have different number of valid values in them padded with NaNs. Here is an example of a row: a row in dataframe = output = How to replace last non NaN value in rows in CSV file?", "q_apis": "last value replace all last value columns values replace last value", "io": "[1 2 3 NaN NaN NaN] <s> [1 2 NaN NaN NaN NaN]", "apis": "DataFrame to_numpy shape argmax array argmax array dtype shape argmax array dtype", "code": ["import numpy as np\ndf = pd.DataFrame([[1, 2, 3, np.nan, np.nan, np.nan], [1, 2, 3, np.nan, np.nan, 2]])\n", "a = df.to_numpy()\nm = a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\nnp.put_along_axis(a, m[:,None], np.nan, axis=1)\ndf[:] = a\n", "np.isnan(a[:,::-1])\narray([[ True,  True,  True, False, False, False],\n       [False,  True,  True, False, False, False]])\n", "np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([3, 0], dtype=int64)\n", "a.shape[1]-1 - np.argmax(~np.isnan(a[:,::-1]), axis=1)\n# array([2, 5], dtype=int64)\n"], "link": "https://stackoverflow.com/questions/62913446/replace-last-non-nan-value-in-row"}
{"id": 273, "q": "A better way to map data in multiple datasets, with multiple data mapping rules", "d": "I have three datasets (, , ), and I wish to add a new column called in dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes. final_NN: ppt_code: herd_id: Expected output: The rules is: if in final_NN is not , = in ; if in final_NN is but in is not Null, then search the ppt_code dataframe, and use the and its corresponding \"number\" to map and fill in the \"MapValue\" in ; if both and in are and null respectively, but in is not Null, then search dataframe, and use the and its corresponding to fill in the in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result?", "q_apis": "map add value codes map first", "io": " code number 0 AA 11 1 AA 11 2 BB 22 3 BB 22 4 CC 33 <s> ID number 0 799 678 1 813 789 ", "apis": "drop_duplicates set_index drop_duplicates set_index replace fillna map fillna map", "code": ["ppt_map = ppt_code.drop_duplicates(subset=['code']).set_index('code')['number']\nhrd_map = herd_id.drop_duplicates(subset=['ID']).set_index('ID')['number']\n\nfinal_NN['MapNumber'] = final_NN['number'].replace({'Unknown': np.nan})\nfinal_NN['MapNumber'] = (\n    final_NN['MapNumber']\n    .fillna(final_NN['code'].map(ppt_map))\n    .fillna(final_NN['ID'].map(hrd_map))\n)\n"], "link": "https://stackoverflow.com/questions/62527486/a-better-way-to-map-data-in-multiple-datasets-with-multiple-data-mapping-rules"}
{"id": 540, "q": "Create a dataframe from arrays python", "d": "I'm try to construct a dataframe (I'm using Pandas library) from some arrays and one matrix. in particular, if I have two array like this: And one matrix like this : Can i create a dataset like this? Maybe is a stupid question, but i m very new with Python and Pandas. I seen this : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html but specify only 'colums'. I should read the matrix row for row and paste in my dataset, but I m think that exist a more easy solution with Pandas.", "q_apis": "array DataFrame", "io": "1 2 2 3 3 3 4 4 4 <s> A B C D 1 2 2 E 3 3 3 F 4 4 4 ", "apis": "columns array DataFrame index columns columns", "code": ["columns = [\"A\", \"B\", \"C\"]\nrows = [\"D\", \"E\", \"F\"]\ndata = np.array([[1, 2, 2], [3, 3, 3],[4, 4, 4]])\ndf = pd.DataFrame(data=data, index=rows, columns=columns)\n"], "link": "https://stackoverflow.com/questions/53961914/create-a-dataframe-from-arrays-python"}
{"id": 453, "q": "Can not make desired pandas dataframe", "d": "I am trying to make a pandas dataframe using 2 paramters as columns. But it makes a dataframe transpose of what I need. I have and as column parameters as follows: This gives the following dataframe: However, I want the dataframe as:", "q_apis": "columns transpose", "io": " 0 1 2 3 4 0 1 2 3 4 5 1 11 22 33 44 55 <s> 0 1 0 1 11 1 2 22 2 3 33 3 4 44 4 5 55 ", "apis": "DataFrame DataFrame", "code": ["df = pd.DataFrame(list(zip(a, b)))\n#pandas 0.24+\n#df = pd.DataFrame(zip(a, b))\n"], "link": "https://stackoverflow.com/questions/56929277/can-not-make-desired-pandas-dataframe"}
{"id": 556, "q": "How to handle missing data with respect to type of dataset?", "d": "I have a dataset where has column types that has type like , . df I want to replace missing value with for each type. Such as- result_df How can do it with Python?", "q_apis": "where replace value", "io": " ID types C D 0 101 Primary 2 3 1 103 Primary 6 3 2 108 Primary 10 ? 3 109 Primary 3 12 4 118 Secondary 5 2 5 122 Secondary ? 6 6 123 Secondary 5 6 7 125 Secondary 2 5 <s> ID types C D 0 101 Primary 2 3 1 103 Primary 6 3 2 108 Primary 10 3 3 109 Primary 3 12 4 118 Secondary 5 2 5 122 Secondary 5 6 6 123 Secondary 5 6 7 125 Secondary 2 5 ", "apis": "dtypes dtype apply to_numeric apply to_numeric dtypes dtype", "code": ["In [1274]: df.dtypes\nOut[1274]: \nID        int64\ntypes    object\nC        object\nD        object\ndtype: object\n", "In [1256]: df.C = df.C.apply(pd.to_numeric)\nIn [1258]: df.D = df.D.apply(pd.to_numeric)\n\nIn [1279]: df.dtypes\nOut[1279]: \nID         int64\ntypes     object\nC        float64\nD        float64\ndtype: object\n"], "link": "https://stackoverflow.com/questions/53589894/how-to-handle-missing-data-with-respect-to-type-of-dataset"}
{"id": 294, "q": "Python: Convert matrices to permutations table", "d": "Given a set of ids, I need to get the values from a matrix (time A & B) for each id combination, and create a dataframe appending the values for all the permutations. I have been able to do it by creating the permutations dataframe and then iterating through it while looking & filling the values. However I need to do this for ~3000 ids, not 3, and I don't know how to do it efficiently. Can I generate a Time A/B dataframe as my example without having to iterate through 9000000* rows? I know I shouldn't be iterating though a dataframe however I haven't found an alternative yet. Ids (3): Time A matrix (3x3): Time B matrix (3x3): Time A/B dataframe (6):", "q_apis": "get values time values all values", "io": "id 15 24 38 15 0 1.8 1.7 24 1.2 0 1.9 38 1.5 1.3 0 <s> id 15 24 38 15 0 88.7 87.3 24 42.2 0 32.7 38 65.6 13.5 0 ", "apis": "drop set_index index concat set_index stack to_frame name set_index stack to_frame name rename_axis index query reset_index", "code": ["# drop set_index('id') if `id` is already index\n(pd.concat([timeA.set_index('id').stack().to_frame(name='A'),\n           timeB.set_index('id').stack().to_frame(name='B')], axis=1)\n   .rename_axis(index=['id_start','id_end'])\n   .query('id_start != id_end')\n   .reset_index()\n)\n"], "link": "https://stackoverflow.com/questions/61926146/python-convert-matrices-to-permutations-table"}
{"id": 137, "q": "python/pandas: update a column based on a series holding sums of that same column", "d": "I have a dataframe with a non-unique col1 like the following Some of the values of col1 repeat lots of times and others not so. I'd like to take the bottom (80%/50%/10%) and change the value to 'other' ahead of plotting. I've got a series which contains the codes in col1 (as the index) and the amount of times that they appear in the df in descending order by doing the following: I've also got my cut-off point (bottom 80%) I'd like to update col1 in df with the value 'others' when col1 appears after the cutOff in the index of the series df2. I don't know how to go about checking and updating. I figured that the best way would be to do a groupby on col1 and then loop through, but it starts to fall apart, should I create a new groupby object? Or do I call this as an .apply() for each row? Can you update a column that is being used as the index for a dataframe? I could do with some help about how to start. edit to add: So if the 'b's in col1 were not in the top 20% most populous values in col1 then I'd expect to see:", "q_apis": "update unique values repeat take value contains codes index cut update value index groupby groupby apply update index start add values", "io": " col1 col2 0 a 1 1 a 1 2 a 2 3 b 3 4 b 3 5 c 2 6 c 2 <s> col1 col2 0 a 1 1 a 1 2 a 2 3 others 3 4 others 3 5 c 2 6 c 2 ", "apis": "DataFrame columns groupby size sort_values round iloc copy loc isin index", "code": ["data = [[\"a \", 1],\n        [\"a \", 1],\n        [\"a \", 2],\n        [\"b \", 3],\n        [\"b \", 3],\n        [\"c \", 2],\n        [\"c \", 2], ]\ndf = pd.DataFrame(data, columns=[\"col1\", \"col2\"])\nprint(df)\n\ndf2 = df.groupby(['col1']).size().sort_values(ascending=False)\nprint(df2)\n\ncutOff = round(len(df2) / 5)\nothers = df2.iloc[cutOff + 1:]\nprint(others)\n\nresult = df.copy()\nresult.loc[result[\"col1\"].isin(others.index), \"col1\"] = \"others\"\nprint(result)\n"], "link": "https://stackoverflow.com/questions/65385779/python-pandas-update-a-column-based-on-a-series-holding-sums-of-that-same-colum"}
{"id": 66, "q": "Assign unique ID to Pandas group but add one if repeated", "d": "I couldn't find a solution and want something faster than what I already have. So, the idea is to assign a unique ID for 'fruit' column, e.g. However, if repeated, add 1 to the last result, so that instead of: I will end up with: So it adds up until the end, even if there may only be 4 fruits changing their positions. Here is my solution but it's really slow and I bet there is something that Pandas can do, inherently: Any ideas?", "q_apis": "unique add assign unique add last", "io": "df['id'] = [0, 0, 1, 1, 2, 0, 0, 2, 2] <s> df['id'] = [0, 0, 1, 1, 2, 3, 3, 4, 4] ", "apis": "groupby shift cumsum ngroup groupby groupby sum", "code": ["df[\"id\"] = df.groupby((df[\"fruit\"] != df[\"fruit\"].shift(1)).cumsum()).ngroup()\nprint(df)\n", "    fruit  id\n0   apple   0\n1   apple   0\n2  orange   1\n3  orange   1\n4   lemon   2\n5   apple   3\n6   apple   3\n7   lemon   4\n8   lemon   4\n", "from itertools import groupby\n\ndata, i = [], 0\nfor _, g in groupby(df[\"fruit\"]):\n    data.extend([i] * sum(1 for _ in g))\n    i += 1\n\ndf[\"id\"] = data\nprint(df)\n"], "link": "https://stackoverflow.com/questions/66846548/assign-unique-id-to-pandas-group-but-add-one-if-repeated"}
{"id": 544, "q": "How to append ndarray values into dataframe rows of particular columns?", "d": "I have a function that returns an like this Now, I have a data frame with columns A,B,C,...,Z ; but the array we are getting has only 20 values. Hence I want to find a way such that for every array I get as output, I am able to store it in like this (A,B,W,X,Y,Z are to be left blank):", "q_apis": "append values columns columns array values array get left", "io": "[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0] <s> __| A | B | C | D | E | F | ... 0 |nan|nan| 0 | 1 | 0 | 0 | ... 1 |nan|nan| 1 | 1 | 0 | 1 | ... . . . ", "apis": "DataFrame columns DataFrame T Index columns array append T append Series index value", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(columns=[chr(i) for i in range(ord('A'),ord('Z')+1)])\n\nprint(df)\n", "Empty DataFrame\nColumns: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z]\nIndex: []\n\n[0 rows x 26 columns]\n", "list1 = [i for i in range(101,121)]\narr1d = np.array(list1)\n\narr1d\n", "# Create alphabet list of uppercase letters\nalphabet = []\nfor letter in range(ord('C'),ord('W')):\n    alphabet.append(chr(letter))\nalphabet\n", "['C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'K',\n 'L',\n 'M',\n 'N',\n 'O',\n 'P',\n 'Q',\n 'R',\n 'S',\n 'T',\n 'U',\n 'V']\n", "df = df.append(pd.Series(arr1d, index=alphabet), ignore_index=True)\n#This line of code can be used for every new value of arr1d \n"], "link": "https://stackoverflow.com/questions/53932155/how-to-append-ndarray-values-into-dataframe-rows-of-particular-columns"}
{"id": 509, "q": "Pandas:Calculate mean of a group of n values of each columns of a dataframe", "d": "I have a dataframe of the following type: I want to calculate the mean of the first 3 element of each column and then next 3 elements and so on and then store in a dataframe. Desired Output- Using Group By was one of the approach I thought of but I am unable to figure out how to use Group by in this case.", "q_apis": "mean values columns mean first", "io": " A B 0 1 2 1 4 5 2 7 8 3 10 11 4 13 14 5 16 17 <s> A B 0 4 5 1 12 14 ", "apis": "index Int64Index dtype groupby mean", "code": ["print (df.index // 3)\nInt64Index([0, 0, 0, 1, 1, 1], dtype='int64')\n", "df = df.groupby(np.arange(len(df)) // 3).mean()\n"], "link": "https://stackoverflow.com/questions/54806672/pandascalculate-mean-of-a-group-of-n-values-of-each-columns-of-a-dataframe"}
{"id": 39, "q": "how to extract each numbers from pandas string column to list?", "d": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list:", "q_apis": "", "io": "Column_A 11.2 some text 17 some text 21 some text 25.2 4.1 some text 53 17 78 121.1 bla bla bla 14 some text 12 some text <s> listA[0] = 11.2 listA[1] = 17 listA[2] = 21 listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78 listC[0] = 121.1 listC[1] = 14 listD[0] = 12 ", "apis": "apply map", "code": ["df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n", "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"], "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list"}
{"id": 307, "q": "How to fill elements between intervals of a list", "d": "I have a list like this: So there are intervals that begin with and end with . How can I replace the values in those intervals, say with 1? The outcome will look like this: I use in this example, but a generalized solution that can apply to any value will also be great", "q_apis": "between replace values apply any value", "io": "list_1 = [np.NaN, np.NaN, 1, np.NaN, np.NaN, np.NaN, 0, np.NaN, 1, np.NaN, 0, 1, np.NaN, 0, np.NaN, 1, np.NaN] <s> list_2 = [np.NaN, np.NaN, 1, 1, 1, 1, 0, np.NaN, 1, 1, 0, 1, 1, 0, np.NaN, 1, np.NaN] ", "apis": "Series fillna ffill where eq", "code": ["s = pd.Series(list_1)\ns.fillna(s.ffill().where(lambda x: x.eq(1))).tolist()\n"], "link": "https://stackoverflow.com/questions/61292759/how-to-fill-elements-between-intervals-of-a-list"}
{"id": 111, "q": "Creating new columns within a dataframe, based on the latest value from previous columns", "d": "I've just completed a beginner's course in python, so please bear with me if the code below doesn't make sense or my issue is because of some rookie mistake. I've been trying to put the learning to use by working with college production of NFL players, with a view to understanding which statistics can be predictive or at least correlate to NFL production. It turns out that there's a lot of data out there so I have about 200 columns of data for 600 odd prospects from the last 20 years (just for running backs so far). However, one of the problems with this data is that each stat is only provided by the age the prospect was in that season giving me something like this: What I want to do at the moment is to be able to take the last year of college production and put it into a new column (for 17 different statistics). I've therefore defined the following function: Which I think should go backwards through the columns until I find a value which isn't NaN, and then take that value as the output. I've then defined the columns via a list: and have then run the function through a for loop based on this list: The result I'm getting back is a slightly bizarre one - the for loop appears to work, as all the new columns I'm expecting are created, however they are only populated with data where the player had an age 23 season. The remainder of indexes are filled with 'NaN': This suggests to me that the first 'if' statement in my function is working fine, but that all of the 'elif' statements aren't triggering and I can't work out why. I'm wondering whether it's because I need to be more explicit about why they would trigger, rather than just relying on a logical test of 'if the column is not, not equal to NaN, go to the next one', or if I'm misunderstanding the elif aspect all together. I've put the whole segment of code in also, just because when I've run into issues so far the problem has often not been where I originally thought. By all means tell me if you think I've gone about this in a weird way - this just seemed like a logical approach to the problem but open to other ways of getting the desired result. Thanks in advance!", "q_apis": "columns value columns put view at columns last at take last year put columns value take value columns all columns where first all test all put where all", "io": " GP 18 GP 19 GP 20 GP 21 GP 22 GP 23 50 14.0 13.0 14.0 NaN NaN NaN 51 14.0 14.0 14.0 NaN NaN NaN 53 13.0 12.0 11.0 NaN NaN NaN 56 10.0 13.0 9.0 13.0 NaN NaN 59 10.0 13.0 15.0 NaN NaN NaN 61 NaN NaN 11.0 11.0 NaN NaN 66 NaN 12.0 13.0 12.0 2.0 13.0 <s> GP Last 50 NaN 51 NaN 53 NaN 56 NaN 59 NaN 61 NaN 66 13.0 ", "apis": "mask columns loc mask ffill iloc", "code": ["stats_list = ['GP']\nfor column in stats_list:\n    mask = df.columns.str.startswith(column)\n    df[f'{column} Last'] = df.loc[:, mask].ffill(axis=1).iloc[:, -1]\n"], "link": "https://stackoverflow.com/questions/65917323/creating-new-columns-within-a-dataframe-based-on-the-latest-value-from-previous"}
{"id": 5, "q": "Extract part of a 3 D dataframe", "d": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this:", "q_apis": "take", "io": " d1 d2 d3 A B C D... A B C D... A B C D.. 0 1 2 <s> d1 d2 d3 A B A B A B 0 1 2 ", "apis": "loc columns isin DataFrame columns MultiIndex from_product", "code": ["filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n", "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"], "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe"}
{"id": 380, "q": "Iterative comparison with pandas", "d": "I don't know to approach this issue. I have a data frame that looks like this What I need to do is to iterate between each row per usuario_id and check if there's a difference between each row, and create a new data set with the row changed and the usuario_web in charge of this change, to generate a data frame that looks like this: Is there any way to do this? I'm working with pandas on python and this dataset could be a little big, let's say around 10000 rows, sorted by usuario_id. Thanks for any advice.", "q_apis": "between difference between any any", "io": "cuenta_bancaria nombre_empresa perfil_cobranza usuario_id usuario_web 5545 a 123 500199 5012 5551 a 123 500199 3321 5551 a 55 500199 5541 5551 b 55 500199 5246 <s> usuario_id cambio usuario_web 500199 cuenta_bancaria 3321 500199 perfil_cobranza 5541 500199 nombre_empresa 5246 ", "apis": "columns intersection ne shift sum eq", "code": ["c = df.columns.intersection(\n        ['nombre_empresa', 'perfil_cobranza', 'cuenta_bancaria']\n)\n\ni = df[c].ne(df[c].shift())\nj = i.sum(1).eq(1)\n"], "link": "https://stackoverflow.com/questions/47872271/iterative-comparison-with-pandas"}
{"id": 524, "q": "Check if group contains same value in Pandas", "d": "I am curious if there is a pre-built function in Pandas to check if all members of a group (factors in a column) contain the same value in another column. i.e. if my dataframe was similar to below it would return an empty list. However, if my dataframe appeared as such (notice the 1 in Col1): Then the output would be a list containing the object \"B\" since the group B has different values in Col1.", "q_apis": "contains value all value empty values", "io": "Col1 Col2 2 A 2 A 0 B 0 B <s> Col1 Col2 2 A 2 A 0 B 1 B ", "apis": "groupby nunique index", "code": ["a = df.groupby('Col2').Col1.nunique() > 1\na[a].index.tolist()\n"], "link": "https://stackoverflow.com/questions/54518504/check-if-group-contains-same-value-in-pandas"}
{"id": 467, "q": "Re-arranging a single column of strings based on text containing different dates, by date", "d": "I am looking to arrange a dataframe by dates, however, the dates are a part of a string within each row. The rows must be rearranged in order by day. Other solutions from stack overflow show how to sort based on a column of dates alone, this example is different because other information is a part of each string and is mixed with the dates. The dataframe is one column with an index, but the rows are not arranged in order from the dates contained on the far right side of each string. The score numbers are random and do not require any attention. The expected dataframe should look like this (repeated dates have no preference for order between each other and index doesn't matter). The respective scores must stay with their associated dates. What is a way to do this?", "q_apis": "date day stack index right any between index", "io": " 0 __________________________ 0 score17 6-20-19.xlsx 1 score23 6-7-19.xlsx 2 score4 6-17-19.xlsx 3 score34 6-8-19.xlsx 4 score10 6-7-19.xlsx <s> 0 __________________________ 1 score23 6-7-19.xlsx 4 score10 6-7-19.xlsx 3 score34 6-8-19.xlsx 2 score4 6-17-19.xlsx 0 score17 6-20-19.xlsx ", "apis": "date date date", "code": ["df[['score', 'date']] = df['column_name'].str.split(' ', n=1, expand=True)\n", "df['date'] = df['date'].str.split('.', expand = True)\n"], "link": "https://stackoverflow.com/questions/56693262/re-arranging-a-single-column-of-strings-based-on-text-containing-different-dates"}
{"id": 36, "q": "Convert dictionary with sub-list of dictionaries into pandas dataframe", "d": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance!", "q_apis": "sub", "io": " 0 2000 {'team': 'Manchester United', 'points': '91'} 2001 {'team': 'Manchester United', 'points': '80'} 2002 {'team': 'Arsenal', 'points': '87'} <s> team points 2000 Manchester United 91 2001 Manchester United 80 2002 Arsenal 87 ", "apis": "items DataFrame T", "code": ["\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"], "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe"}
{"id": 637, "q": "Pandas: Create dataframe from data and column order", "d": "what i'm asking must be something very easy, but i honestly can't see it.... :( I have an array, lets say and i want to put it in a dataframe. I do aiming for: but i am getting: (notice the discrepancy between column names and data) I know i can re-arrange the column names order in the dataframe creation, but i'm trying to understand how it works. Am i doing something wrong, or it's normal behaviour? (why though?)", "q_apis": "array put between names names", "io": "col1 col2 col3 1 2 3 4 5 6 7 8 9 10 11 12 <s> col3 col1 col2 1 2 3 4 5 6 7 8 9 10 11 12 ", "apis": "columns", "code": ["df = pd.Dataframe(data, columns=['col1', 'col2', 'col3'])\n"], "link": "https://stackoverflow.com/questions/50150090/pandas-create-dataframe-from-data-and-column-order"}
{"id": 214, "q": "How to split dataframe made from objects?", "d": "I want to split one column pandas dataframe that look like this: into two columns: So it can look like this: But its showing type as:", "q_apis": "columns", "io": " 0 0 38 A 1 35 B 2 14 B <s> Number Letter 0 38 A 1 35 B 2 14 B ", "apis": "set_axis", "code": ["df['0'].str.split(' ', expand=True).set_axis(['Number', 'Letter'], axis=1)\n"], "link": "https://stackoverflow.com/questions/63849171/how-to-split-dataframe-made-from-objects"}
{"id": 144, "q": "How to read list of json objects from Pandas DataFrame?", "d": "I want just want to loop through the array of json objects, and get the values of 'box'..... I have a DataFrame which looks like this and the column 'facesJson' (dstype = object) contain array of json objects which look like this: when i run this code i get this error:", "q_apis": "DataFrame array get values DataFrame array get", "io": " img facesJson 0 2b26mn4.jpg [{'box': [57, 255, 91, 103], 'confidence': 0.7... 1 cd7ntf.jpg [{'box': [510, 85, 58, 87], 'confidence': 0.99... 2 m9kf3e.jpg [{'box': [328, 78, 93, 123], 'confidence': 0.9... 3 b4hx0n.jpg [{'box': [129, 30, 38, 54], 'confidence': 0.99... 4 afx0fm.jpg [{'box': [86, 126, 221, 298], 'confidence': 0.... <s> [ { \"box\":[ 158,115,84,112 ], \"confidence\":0.9998929500579834, }, { \"box\":[ 404,105, 86,114 ], \"confidence\":0.9996863603591919, } ] ", "apis": "keys", "code": ["import ast\n\nline='[ { \"box\":[ 158,115,84,112 ], \"confidence\":0.9998929500579834, }, { \"box\":[ 404,105, 86,114 ], \"confidence\":0.9996863603591919, } ]'\n\n\nparsed=ast.literal_eval(line)\n\nprint(parsed[0].keys())\n"], "link": "https://stackoverflow.com/questions/63282322/how-to-read-list-of-json-objects-from-pandas-dataframe"}
{"id": 213, "q": "Drop a pandas DataFrame row that comes after a row that contains a particular value", "d": "I am trying to drop all rows that come after a row which has inside the column df: Required output df: Look at the following code: Returns a error message I have tried many different variations of this using different methods like and but I can't seem to figure it out anyway. I have also tried truncate: This returns: IndexError: index 1 is out of bounds for axis 0 with size 1", "q_apis": "DataFrame contains value drop all at truncate index size", "io": " Ammend 0 no 1 yes 2 no 3 no 4 yes 5 no <s> Ammend 0 no 1 yes 3 no 4 yes ", "apis": "DataFrame", "code": ["import pandas as pd\nimport numpy as np\n\nnp.random.seed(525)\ndf = pd.DataFrame({'Other': np.random.rand(10), 'Ammend': np.random.choice(['yes', 'no'], 10)})\ndf\n"], "link": "https://stackoverflow.com/questions/63855152/drop-a-pandas-dataframe-row-that-comes-after-a-row-that-contains-a-particular-va"}
{"id": 425, "q": "Replicate multiple rows of events for specific IDs multiple times", "d": "I have a call log data made on customers. Which looks something like below, where ID is customer ID and A and B are log attributes: I want to replicate each set of event for each ID based on some slots. For e.g. if slot value is 2 then all events for ID \"A\" should be replicated slot-1 times. and a new Index should be created indicating which slot does replicated values belong to: I have tried following solution: it gives me the expected output but is not scalable when slots are increased and number of customers increases in order of 10k. I think its taking a long time because of the loop. Any solution which is vectorized will be really helpful.", "q_apis": "where value all Index values time", "io": " ID A B A A 46 31 A A 99 54 A A 34 9 B B 46 48 B B 7 75 C C 1 25 C C 71 40 C C 74 53 D D 57 17 D D 19 78 <s> ID A B A A 46 31 A A 99 54 A A 34 9 A A 46 31 A A 99 54 A A 34 9 ", "apis": "concat assign Index Index Index", "code": ["slots = 2\nnew_df = pd.concat(df.assign(Index=f'_{i}') for i in range(1, slots+1))\n\nnew_df['Index'] = new_df['ID'] + new_df['Index']\n"], "link": "https://stackoverflow.com/questions/58086144/replicate-multiple-rows-of-events-for-specific-ids-multiple-times"}
{"id": 275, "q": "from two arrays to one dataframe python", "d": "I am trying to put my values into two arrays and then to make them a dataframe. I am using python, numpy and pandas to do so. my arrays are: and I would like to put them into a pandas dataframe. When I print my dataframe, I would like to see this: How can I do that? I read some related questions, but I can't get it right. One of the errors says that indexes must not be tuples, but, as you can see, I don't have tuples", "q_apis": "put values put get right", "io": "k = [7.0, 8.0, 6.55, 7.0000001, 10.12] p = [6.94, 9.0, 4.44444, 13.0, 9.0876] <s> a b c d e k 7.0 8.0 6.6 7.0 10.1 p 6.9 9.0 4.4 13.0 9.1 ", "apis": "DataFrame columns index round", "code": ["from string import ascii_lowercase\npd.DataFrame([k,p],columns=list(ascii_lowercase[:len(k)]),index=['k','p']).round()\n"], "link": "https://stackoverflow.com/questions/62455905/from-two-arrays-to-one-dataframe-python"}
{"id": 199, "q": "Shifting and reverting multiple rows in pandas dataframe", "d": "I have the following dataframe and wish to shift over the 0 values to the right and then revert each row: This is the result I would like to get: I've tried varius shift and apply combinations without any success. Is there a simple way of achieving this?", "q_apis": "shift values right get shift apply any", "io": " H00 H01 H02 H03 H04 H05 H06 NR 1 33 28 98 97 0 0 0 2 29 24 22 98 97 0 0 3 78 76 98 97 0 0 0 4 16 15 98 97 0 0 0 5 81 72 70 98 97 0 0 <s> H00 H01 H02 H03 H04 H05 H06 NR 1 97 98 28 33 0 0 0 2 97 98 22 24 29 0 0 3 97 98 76 78 0 0 0 4 97 98 15 16 0 0 0 5 97 98 70 72 81 0 0 ", "apis": "apply", "code": ["def reverse_part(series):\n  series[series > 0] = series[series > 0][::-1]\n  return series\n\ndf.apply(reverse_part, axis=1, raw=True)\n"], "link": "https://stackoverflow.com/questions/64230159/shifting-and-reverting-multiple-rows-in-pandas-dataframe"}
{"id": 68, "q": "Merge pandas dataframes by timestamps", "d": "I've got a few pandas dataframes indexed with timestamps and I would like to merge them into one dataframe, matching nearest timestamp. So I would like to have for example: What exact timestamp there is going to be in final DataFrame is not important to me. BTW. Is there an easy way to leter convert \"absolute\" timestamps into time from start (either in seconds or miliseconds)? So for this example:", "q_apis": "merge timestamp timestamp DataFrame time start seconds", "io": "a = CPU 2021-03-25 13:40:44.208 70.571797 2021-03-25 13:40:44.723 14.126870 2021-03-25 13:40:45.228 17.182844 b = X Y 2021-03-25 13:40:44.193 45 1 2021-03-25 13:40:44.707 46 1 2021-03-25 13:40:45.216 50 2 a + b = CPU X Y 2021-03-25 13:40:44.208 70.571797 45 1 2021-03-25 13:40:44.723 14.126870 46 1 2021-03-25 13:40:45.228 17.182844 50 2 <s> CPU X Y 0.0 70.571797 45 1 0.5 14.126870 46 1 1.0 17.182844 50 2 ", "apis": "merge_asof", "code": ["pd.merge_asof(df1, df2, left_index=True, right_index=True, direction='nearest')\n"], "link": "https://stackoverflow.com/questions/66801447/merge-pandas-dataframes-by-timestamps"}
{"id": 523, "q": "Pandas DataFrame from Dictionary", "d": "Let's assume that I have a JSON file like below, and I want to convert this file into a data frame with 2 columns: This is what I already tried, but I am unable to create DF and probably there is a more efficient way of doing this task: Expecting output like this for all elements in a Json file Also I want to create a directed graph for each parent node with child nodes as clusters because parent nodes has the same child nodes which are also present in other parent nodes. Thanks in Advance", "q_apis": "DataFrame columns all", "io": "{\"1087\": [4,5,6,7,8,9,10,12,13,21,22,23,24,25,26,27,28,34,35 ,37,39,40,42,44,45,46,47,48,51,52,54,55,56,59,60,61,63,64,65,66,67,68,72,73,74,75,78,80,81,82,83,84,85,87,88,92,94,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,149,180,181,196,198,200,202,206,222,223,226,227,230,231,232,233,234,235,242,255,257,258,259,261,263,264,265,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,302,303,304,305,306,307,308,309,310,311,313,314,316,318,319,320,323,325,326,327,328,330,334,336,337,339,340,342,343,350,351,354,355,362,363,365,366,367,368,369,370,371,372,374,375,376,377,378,379,380,383,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,427,428,429,430,431,432,433,434,435,437,438,444,446,449,451,455,457,461,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,494,496,498,499,500,502,504,506,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,559,560,561,565,567,569,571,573,574,576,579,580,581,583,586,587,588,590,593,594,597,598,600,601,602,604,605,606,607,608,609,611,612,613,614,615,616,617,620,621,622,624,625,626,629,631,633,634,636,638,639,640,641,643,644,647,649,650,651,652,653,654,657,658,664,665,666,667,669,671,674,675,676,677,678,682,683,685,686,687,688,692,694,695,702,703,705,708,712,713,714,715,716,717,718,720,728,732,734,735,739,740,742,743,745,746,751,752,759,769,770,772,778,779,780,783,784,786,792,805,815,823,831,832,834,835,836,837,838,839,852,854,855,856,867,875,877,879,888,890,891,896,900,908,909,910,911,912,913,914,915,916,917,918,919,934,935,936,937,938,939,944,945,946,950,951,952,953,957,958,959,960,964,965,966,967,971,975,977,978,980,981,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1027,1028,1033,1034,1035,1036,1037,1038,1039,1049,1061,1063,1065,1067,1069,1070,1071,1072,1073,1074,1076,1077,1078,1080,1081,1084,1088,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1127,1128,1129,1130,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1151,1155,1156,1201,1202,1203,1204,1207,1208,1209,1213,1214,1215,1216,1217,1220,1221,1222,1223,1224,1232,1233,1235,1237,1238,1241,1243,1244,1245,1248,1249,1251,1254,1269,1271,1273,1274,1275,1284,1289,1298,1301,1302,1303,1470,1495,1500,1501,1508,1509,1517,1518,1572,1573,1574,1575,1614,1619,1620,1625,1633,1639,1661,1669,1670,1671,1692,1693,1694,1695,1696,1698,1699,1700,1701,1706,1707,1708,1709,1711,1712,1713,1715,1720,1726,1728,1729,1730,1731,1732,1734,1755,1771,1780,1781,1785,1788,1794,1795,1797,1801,1802,1803,1805,1827,1829,1830,1836,1838,1843,1845,1847,1849,1851,1852,1853,1854,1855,1897,1899,1901,1920,1922,1923,1974,1987,1988,1989,1990,1991,1993,1994,2013,2014,2038,2039,2040,2044,2057,2086,2108,2144,2150,2215,2216,2218,2219,2220,2227,2228,2229,2230,2250,2258,2271,2279,2283,2285,2286,2287,2295,2302,2327,2390,2397,2406,2407,2411,2413,2414,2415,2419,2421,2429,2441,2471,2472,2490,2493,2507,2514,2519,2524,2525,2531,2532,2535,2538,2541,2551,2552,2553,2555,2560,2561,2562,2564,2570,2577,2578,2579,2580,2581,2586,2587,2588,2589,2591,2592,2594,2595,2596,2597,2599,2600,2601,2602,2603,2604,2605,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2625,2626,2627,2628,2629,2630,2631,2634,2635,2665,2668,2669,2671,2673,2681,2682,2683,2684,2705,2706,2707,2708,2709,2710,2711,2712,2713,2750,2766,2768,2769,2770,2798,2804,2817,2821,2822,2823,2824,2825,2826,2844,2847,2853,2855,2858,2860,2861,2862,2863,2864,2865,2880,2900,2901,2902,2903,2906,2911,2912,2913,2916,2918,2922,2925,2926,2932,2935,2941,2943,2945,2947,2948,2949,2950,2958,2959,2966,2967,2972,2976,2977,2978,2979,2980,2981,2982,2987,2988,2991,2992,2993,2994,2995,2996,2999,3001,3003,3007,3008,3011,3012,3015,3016,3017,3018,3024,3030,3031,3033,3034,3045,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3093,3099,3105,3112,3113,3114,3115,3116,3117,3127,3128,3154,3155,3156,3157,3297,3299,3300,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3336,3339,3416,3417,3420,3424,3550,3587,3588,3589,3590,3591,3592,3593,3598,3599,3600,3602,3603,3604,3605,3606,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3625,3655,3656,3657,3718,3721,3724,3725,3726,3730,3732,3733,3736,3738,3741,3743,3744,3747,3748,3750,3752,3754,3756,3762,3763,3764,3765,3766,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3795,3797,3798,3800,3806,3811,3864,3866,3867,3871,3881,3883,3884,3885,3886,3925,3926,3929,3930,3935,3936,3940,4018,4030,4045,4049,4050,4051,4054,4058,4059,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4104,4109,4110,4113,4116,4117,4118,4119,4161,4267,4285,4310,4317,4335,4358,4359,4365,4366,4467,4471,4475,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4638,4639,4764,4765,4766,4782,4803,4824,4827,4828,4830,4888,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4926,4990,6998,7026,7027,7028],\"1096\": [25,26,27,28,45,46,63,64,65,66,67,80,81,82,83,84,85,128,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,330,366,367,376,385,386,387,388,391,417,418,419,420,437,449,451,553,555,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,641,643,644,779,780,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1273,1274,1275,1278,1280,1289,1292,1670,1671,1713,1730,1731,1847,1849,1993,2086,2218,2219,2220,2258,2421,2586,2587,2608,2610,2611,2629,2631,2673,2708,2709,2710,2711,2712,2713,2821,2822,2823,2825,2844,2847,2858,2860,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2959,2976,2977,2978,2979,2980,2981,2982,2991,2992,2993,2994,2995,3001,3003,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3112,3113,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3416,3417,3589,3590,3591,3592,3593,3598,3608,3609,3610,3612,3613,3614,3615,3616,3617,3618,3656,3657,3732,3738,3743,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3797,3798,3871,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4091,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4267,4358,4359,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4616,4764,4765,4766,7026,7027,7028],\"1144\": [25,26,27,28,144,372,374,422,768,1005,1051,1052,1053,1054,1057,1058,1060,1062,1064,1066,1068,1098,1101,1146,1703,1704,1705,1713,1716,1994,2086,2382,3095,3096,3097,3114,3115,3116,3339,3619,3620,3621,3732,3738,3743,3881,3883,3884,3885,3886,4113,4116,4117,4118,4119,4267,4285,4365,4370,4371,4372,4373,4374,4375,4471,4764,4765,4766,4803,4824,4828,4830,4990],\"-1\": [40,63,64,65,66,67,68,80,81,82,83,84,85,87,130,131,132,133,134,135,136,137,138,139,140,141,234,261,263,264,267,268,269,271,272,293,308,314,318,319,337,366,367,375,376,385,386,387,388,391,407,416,417,418,419,420,435,461,489,559,561,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,623,624,625,626,632,634,636,644,666,682,683,685,686,687,688,694,695,696,720,737,854,855,870,882,883,888,896,916,917,918,919,930,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,971,978,982,986,987,988,993,994,995,996,1000,1001,1002,1036,1037,1038,1039,1081,1132,1133,1134,1135,1136,1210,1317,1321,1341,1347,1377,1378,1380,1383,1384,1386,1396,1398,1408,1410,1432,1456,1458,1473,1500,1501,1525,1614,1670,1730,1808,1838,1982,1983,1984,1985,1993,2033,2034,2038,2039,2040,2069,2150,2151,2258,2355,2356,2571,2596,2692,2729,2737,2821,2822,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2976,2977,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3150,3151,3316,3416,3417,3594,3744,3769,3770,3773,3776,3785,3786,3787,3871,4064,4066,4068,4070,4072,4074,4076,4078,4080,4082,4084,4086,4088,4092,4094,4096,4098,4109,4288,4321,4330,4466,4991,5567,6913],\"2060\": [47,65,67,80,81,148,155,156,166,167,168,226,227,267,268,269,580,594,597,600,601,602,604,607,608,609,611,614,634,636,682,683,685,686,687,688,694,695,696,728,733,738,744,944,945,946,993,994,995,1317,1321,1341,1347,1377,1378,1380,1383,1384,1385,1386,1387,1396,1398,1408,1410,1432,1456,1458,1473,1525,1696,1736,1737,1738,1739,1754,1808,1859,1865,1873,1879,1885,1892,1922,1982,1983,1984,1985,1993,1994,2038,2039,2040,2150,2151,2254,2300,2355,2356,2377,2391,2448,2478,2530,2564,2723,2742,2745,2746,2747,2882,2922,2925,2947,2948,2949,2950,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3341,3383,3482,3493,3494,3506,3530,3672,3675,3821,4332,4439,4440,4459,4908,4913,4914,4915,4916,4917,4919,4920,4921,4922,4923,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5177,5178,5179,5180,5181,5182,5183,5184,5188,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5217,5218,5219,5220,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5486,5487,5488,5489,5490,5491,5492,5493,5494,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5559,5566,5567,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,7037],\"1742\": [47,60,61,63,64,65,66,67,80,81,82,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,226,227,232,233,242,267,268,269,271,314,319,323,366,367,376,385,388,417,418,419,420,554,559,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,633,634,636,643,700,701,702,703,705,717,728,745,746,834,835,836,837,838,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1003,1088,1119,1120,1121,1122,1123,1207,1208,1209,1213,1214,1215,1216,1269,1386,1508,1509,1692,1693,1694,1695,1698,1699,1700,1701,1720,1726,1727,1729,1780,1781,1830,1851,1920,1993,2127,2216,2258,2295,2390,2564,2621,2821,2823,2844,2847,2862,2863,2864,2865,2911,2912,2913,2916,2922,2925,2935,2943,2945,2947,2948,2949,2950,2978,2979,2980,2981,2982,2987,2988,2996,3007,3008,3011,3012,3015,3016,3017,3018,3072,3099,3112,3113,3154,3155,3156,3157,3311,3312,3315,3318,3353,3355,3420,3422,3423,3590,3591,3592,3625,3732,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3794,3801,3871,3940,4049,4050,4058,4063,4169,4170,4476,4477,4482,4616,4906,6864,6865],\"1125\": [47,53,65,67,80,81,172,174,187,190,196,198,200,202,246,267,268,269,309,313,316,319,320,323,324,325,326,370,372,374,421,448,594,597,600,634,636,657,658,673,679,692,708,735,860,944,945,946,993,994,995,1061,1063,1065,1067,1220,1222,1223,1277,1502,1517,1518,1572,1573,1574,1575,1621,1622,1623,1632,1635,1637,1661,1696,1849,1897,1899,1901,1968,1993,2032,2033,2034,2069,2283,2421,2422,2423,2445,2471,2472,2490,2493,2527,2529,2609,2623,2627,2669,2671,2729,2738,2739,2804,2825,2826,2853,2854,2855,2856,2894,2895,2901,2902,2903,2904,2918,2926,2932,3024,3107,3114,3115,3116,3299,3353,3354,3355,3356,3364,3365,3373,3390,3391,3392,3393,3415,3422,3423,3425,3541,3550,3715,3719,3720,3721,3792,3793,3794,3795,3800,3835,3836,3844,3845,3846,3847,3864,3866,3867,3920,3925,3926,3929,3930,3935,3936,4030,4059,4090,4111,4112,4138,4143,4145,4161,4162,4165,4306,4311,4351,4361,4368,4397,4457,4467,4471,4480,4638,4639,4754,4764,4765,4766,4770,4803,4806,4807,4808,4824,4830,4888,4904,4905,4911,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,6892,6893],\"1095\": [64,65,66,67,80,81,187,190,196,198,200,202,219,267,268,269,319,320,324,385,388,559,573,576,580,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,615,616,624,625,626,634,636,639,640,644,679,689,690,691,751,756,842,843,844,845,846,847,848,937,938,939,944,945,946,951,952,953,958,959,960,964,965,966,967,986,987,988,993,994,995,1000,1001,1002,1993,2098,2250,2258,2354,2421,2495,2496,2821,2823,2839,2844,2847,2854,2856,2862,2863,2864,2865,2922,2925,2947,2948,2949,2950,2978,2980,3007,3011,3015,3016,3017,3018,3113,3299,3306,3310,3315,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3368,3422,3423,3590,3591,3592,3748,3750,3752,3754,3756,3762,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3792,3793,3794,3795,3801,3871],\"1145\": [64,65,66,67,80,81,82,84,85,125,129,130,131,132,133,134,135,140,141,263,264,267,268,269,327,334,351,367,388,446,553,594,597,600,601,602,604,607,608,609,611,614,620,622,629,631,634,643,692,735,786,867,896,937,938,939,944,945,946,950,951,952,953,958,959,960,975,977,982,986,987,988,993,994,995,996,1000,1001,1002,1076,1077,1088,1119,1120,1121,1122,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1204,1224,1232,1233,1244,1245,1275,1517,1518,1572,1573,1574,1575,1661,1669,1670,1671,1709,1711,1729,1730,1731,1843,1845,1897,1899,1901,1993,1994,2013,2057,2218,2219,2220,2227,2228,2229,2230,2250,2258,2283,2327,2471,2472,2490,2493,2560,2562,2587,2596,2608,2609,2610,2611,2630,2631,2673,2682,2683,2684,2705,2706,2708,2709,2710,2711,2712,2713,2817,2821,2825,2844,2862,2863,2880,2900,2901,2902,2916,2918,2922,2925,2926,2941,2947,2948,2949,2950,2977,2978,2982,2991,2992,2993,3007,3011,3015,3016,3017,3018,3033,3034,3072,3297,3306,3310,3311,3312,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3417,3588,3721,3744,3864,3866,3867,4030,4034,4059,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4092,4093,4094,4095,4096,4097,4098,4099,4109,4110,4161,4358,4359,4366,4638,4639],\"1966\": [64,65,66,67,80,81,82,83,84,85,129,130,131,132,133,134,135,136,137,138,139,140,141,263,264,267,268,269,271,272,314,366,367,376,385,386,387,388,391,417,418,419,420,559,569,573,574,576,579,580,581,583,586,587,588,590,593,594,597,600,601,602,604,607,608,609,611,614,616,624,625,626,634,636,639,640,641,644,778,936,937,938,939,944,945,946,950,951,952,953,958,959,960,964,965,966,967,982,986,987,988,993,994,995,996,1000,1001,1002,1075,1114,1115,1116,1117,1118,1728,1993,2258,2596,2673,2821,2823,2844,2847,2862,2863,2864,2865,2916,2922,2925,2947,2948,2949,2950,2978,2979,2980,2981,2982,3007,3011,3015,3016,3017,3018,3112,3113,3590,3591,3592,3744,3748,3750,3752,3754,3756,3762,3763,3764,3770,3773,3776,3779,3780,3781,3782,3783,3784,3785,3786,3787,3789,3800,3871,4062,4764,4765,4766],\"2148\": [159,220,696,1050,1277,1278,1280,1696,1994,2032,2033,2034,2151,2319,2429,2432,2433,2434,2435,2436,2437,2441,2445,3299,3310,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3622,4913,6945,6946,6947,6948,6949,6950,6951,6952,6953,6990,6991],\"2387\": [159,1050,1994,2708,2709,2710,2711,2712,2713,2976,2977,3416,3417,3622,4358,4359],\"865\": [216,217,851,860,2053,2054,2055,2056,2131,2132,2422,2423],\"2442\": [220,1277,1621,1622,1623,1632,1635,1637,1859,1865,1873,1879,1885,1892,1994,2032,2033,2034,2432,2433,2434,2435,2436,2437,2445,2780,2789,3299],\"2370\": [321,692,1245,1517,1518,1572,1573,1574,1575,1661,1897,1899,1901,2068,2094,2095,2096,2106,2109,2263,2264,2270,2283,2284,2303,2327,2366,2367,2390,2428,2490,2493,2719,2722,2726,2735,2736,2738,2739,2740,2741,2765,2827,2894,2895,2901,2902,2903,2926,3019,3024,3031,3077,3078,3079,3081,3083,3084,3085,3349,3590,3591,3592,3605,3606,3715,3716,3853,3854,3855,3856,3857,3861,4112,4120,4284,4306,4398,4620,4621,6902,6903],\"1950\": [684,816,1285,1286,1656,1657,2405,2512,2527,3651],\"3852\": [779,780,1213,1214,1289,1847,1849,3339,3732,3738,3743,3790,3797,3800,4054,4113,4765,4766],\"2381\": [781,782,810,2023,2024,2632,2633,4365],\"1108\": [920,921],\"1105\": [1276,1502,1994,2032,2269,2319,2342,2343,2344,2348,2349,2350,2420,2421,2429,2441,3310,3321,3322,3323,3326,3327,3328],\"2725\": [2723,4161],\"2727\": [2729],\"2728\": [2730],\"2820\": [2858,2860]} <s> Parent Child 1087 4 1087 5 1087 6 ..... ...... 1096 25 1096 26 1096 27 1096 28 ...... ...... 1144 25 1144 26 1144 27 ..... ..... ", "apis": "items items", "code": ["[[k, i] for k, v in data.items() for i in v]", "In [14]: [[k, i] for k, v in data.items() for i in v]                                                                                                                                                                                                        \nOut[14]:\n[['1108', 920],\n ['1108', 921],\n ['2381', 781],\n ['2381', 782],\n ['2381', 810],\n ['2381', 2023],\n ['2381', 2024],\n ['2381', 2632],\n ['2381', 2633],\n ['2381', 4365],\n ['2728', 2730]]\n"], "link": "https://stackoverflow.com/questions/54577752/pandas-dataframe-from-dictionary"}
{"id": 455, "q": "How to manipulate column entries using only one specific output of a function that returns several values?", "d": "I have a dataframe like this: and I have a function that returns several values. Here I just use a dummy function that returns the minimum and maximum for a certain input iterable: Now I want to e.g. add the maximum of each column to each value in the respective column. So gives and then yields the desired outcome I am wondering whether there is a more straightforward way that avoids the two chained 's. Just to make sure: I am NOT interested in a type solution. I highlighted the to illustrate that this not my actual function but just serves as a minimal example function that has several outputs.", "q_apis": "values values add value", "io": "a (0, 3) b (2, 5) <s> a b 0 3 7 1 4 8 2 5 9 3 6 10 ", "apis": "min max add apply Series min max index min max add apply loc max max add apply", "code": ["def return_min_max(x):\n    return (np.min(x), np.max(x))\n\ndf.add(df.apply(return_min_max).str[1])\n", "def return_min_max(x):\n    return pd.Series([np.min(x), np.max(x)], index=['min', 'max'])\n\ndf.add(df.apply(return_min_max).loc['max'])\n", "def return_max(x):\n    return np.max(x)\n\ndf.add(df.apply(return_max))\n"], "link": "https://stackoverflow.com/questions/56891370/how-to-manipulate-column-entries-using-only-one-specific-output-of-a-function-th"}
{"id": 295, "q": "pandas groupby expanding df based on unique values", "d": "I have below: I want to achieve the following. For each unique , the bottom row is (this is ). I want to count how many times each unique value of occurs where . This part would be achieved by something like: However, I also want to add, for each unique value of , a column indicating how many times that value occurred after the point where for the value indicated by the row. I understand this might sound confusing; here's how the output woud look like in this example: It is important that the solution is general enough to be applicable on a similar case with more unique values than just , and . UPDATE As a bonus, I am also interested in how, instead of the count, one can instead return the sum of some value column, under the same conditions, divided by the corresponding in the rows. Example: suppose we now depart from below instead: The output would need to sum for the cases indicated by the counts in the solution by @jezrael, and divide that number by . The output would instead look like:", "q_apis": "groupby expanding unique values unique count unique value where add unique value value where value unique values count sum value now sum", "io": "df V2 V1 A B C 0 A 0 0 0 0 1 B 1 0 0 0 2 C 2 1 2 0 <s> df V2 V1 A B C 0 A 0 0 0 0 1 B 1 0 0 0 2 C 2 1 3.5 0 ", "apis": "pivot_table index columns mean reindex columns index", "code": ["df = df.pivot_table(\n    index='n',\n    columns='V2',\n    aggfunc=({\n        'V3': 'mean'\n    })\n).V3.reindex(columns=v, index=v, fill_value=0)\n"], "link": "https://stackoverflow.com/questions/61868871/pandas-groupby-expanding-df-based-on-unique-values"}
{"id": 38, "q": "Working with list inside a Pandas dataframe", "d": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here?", "q_apis": "length", "io": "ID | Column1 | 0 | [] | 1 | [1,2] | 2 | [] | <s> ID | Column1 | Column2 | 0 | [] | 0 | 1 | [1,2] | 2 | 2 | [] | 0 | ", "apis": "", "code": ["df['Column2'] = [len(x) for x in df['Column1']]\n"], "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe"}
{"id": 322, "q": "Add ID found in list to new column in pandas dataframe", "d": "Say I have the following dataframe (a column of integers and a column with a list of integers)... And also a separate list of IDs... Given that, and ignoring the column and any index, I want to see if any of the IDs in the list are mentioned in the column. The code I have so far is: This works but only if the list is longer than the dataframe and for the real dataset the list is going to be a lot shorter than the dataframe. If I set the list to only two elements... I get a very popular error (I have read many questions with the same error)... I have tried converting the list to a series (no change in the error). I have also tried adding the new column and setting all values to before doing the comprehension line (again no change in the error). Two questions: How do I get my code (below) to work for a list that is shorter than a dataframe? How would I get the code to write the actual ID found back to the column (more useful than True/False)? Expected output for : Ideal output for (ID(s) are written to a new column or columns): Code:", "q_apis": "any index any get all values get get columns", "io": " ID Found_IDs 0 12345 [15443, 15533, 3433] 1 15533 [2234, 16608, 12002, 7654] 2 6789 [43322, 876544, 36789] <s> bad_ids = [15533, 876544, 36789, 11111] ", "apis": "apply", "code": ["bad_ids_set = set(bad_ids)\ndf['Found_IDs'].apply(lambda x: list(set(x) & bad_ids_set))\n"], "link": "https://stackoverflow.com/questions/60989914/add-id-found-in-list-to-new-column-in-pandas-dataframe"}
{"id": 426, "q": "How to change values of rows based on conditions in dataframe?", "d": "I have dataframe that is shown below, (Need some magic to be done) The change should be made in type column such that, in a row if is and is then next row of should be assigned . Full dataframe should look like this:", "q_apis": "values", "io": " type label 0 0 0 1 0 0 2 0 0 3 0 0 4 2 1 5 2 1 6 2 1 7 2 1 8 2 1 9 2 1 10 0 0 11 0 0 12 0 0 13 0 0 14 0 0 15 0 0 16 0 0 17 0 0 18 0 0 19 0 0 <s> type label 0 0 0 1 2 0 2 2 0 3 0 0 4 2 1 5 2 1 6 2 1 7 2 1 8 2 1 9 2 1 10 0 0 11 2 0 12 2 0 13 2 0 14 2 0 15 2 0 16 2 0 17 2 0 18 2 0 19 2 0 ", "apis": "eq eq loc shift", "code": ["m = df['type'].eq(0) & df['label'].eq(0)\ndf.loc[m == m.shift(1),'type'] = 2\nprint(df)\n"], "link": "https://stackoverflow.com/questions/58060206/how-to-change-values-of-rows-based-on-conditions-in-dataframe"}
{"id": 119, "q": "Adding value from one pandas dataframe to another dataframe by matching a variable", "d": "Suppose I have a pandas dataframe with 2 columns A second dataframe, contains c1, c2 and a few other columns. My goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. What is the best way to do this?", "q_apis": "value columns second contains columns replace empty values values first values", "io": " c1 c2 0 v1 b1 1 v2 b2 2 v3 b3 3 v4 b4 4 v5 b5 <s> c1 c2 c3 c4 0 \"\" b5 500 3 1 \"\" b2 420 7 2 \"\" b1 380 5 3 \"\" b2 470 9 4 \"\" b3 290 2 ", "apis": "merge left", "code": ["main_df = pd.merge(df2, df, on=\"c2\", how=\"left\")"], "link": "https://stackoverflow.com/questions/65750044/adding-value-from-one-pandas-dataframe-to-another-dataframe-by-matching-a-variab"}
{"id": 626, "q": "Copying columns within pandas dataframe", "d": "I want to slice and copy columns in a Python Dataframe. My data frame looks like the following: I want to make it of the form Which basically means that I want to shift the values in Columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1' For the same, I wrote the code as No copying takes places within the columns. How shall I modify my code??", "q_apis": "columns copy columns shift values columns", "io": " 1928 1928.1 1929 1929.1 1930 1930.1 0 0 0 0 0 0 0 1 1 3 3 2 2 2 2 4 1 3 0 1 2 <s> 1928 1928.1 1929 1929.1 1930 1930.1 0 0 0 1 1 3 2 4 1 3 0 0 4 3 2 5 3 0 6 0 0 7 2 2 8 1 2 ", "apis": "values", "code": ["cols = ['1929', '1929.1', '1930', '1930.1']\nvals = df[cols].values.reshape(-1, 2)\n"], "link": "https://stackoverflow.com/questions/50533522/copying-columns-within-pandas-dataframe"}
{"id": 478, "q": "Append rows to groups in pandas", "d": "I'm trying to append a number of NaN rows to each group in a pandas dataframe. Essentially I want to pad each group to be 5 rows long. Ordering is important. I have: I want:", "q_apis": "groups append pad", "io": " Rank id 0 1 a 1 2 a 2 3 a 3 4 a 4 5 a 5 1 c 6 2 c 7 1 e 8 2 e 9 3 e <s> Rank id 0 1 a 1 2 a 2 3 a 3 4 a 4 5 a 5 1 c 6 2 c 7 NaN c 8 NaN c 9 NaN c 10 1 e 11 2 e 12 3 e 13 NaN e 14 NaN e ", "apis": "crosstab iloc unstack reset_index loc", "code": ["df = pd.crosstab(df.Rank, df.ID).iloc[:5].unstack().reset_index()\ndf.loc[(df[0]==0),'Rank'] = np.nan\ndel df[0]\n"], "link": "https://stackoverflow.com/questions/50529459/append-rows-to-groups-in-pandas"}
{"id": 24, "q": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas", "d": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance!", "q_apis": "replace replace all value any last value", "io": " L1 D1 L2 D2 L3 1.0 ABC 1.1 4.1 NaN NaN NaN 1.7 NaN NaN NaN 4.1 NaN NaN NaN NaN 1.8 3.2 PQR NaN NaN NaN 1.6 NaN NaN <s> L1 D1 L2 D2 L3 1.0 ABC 1.1 4.1 - NaN NaN 1.7 - - NaN 4.1 - - - NaN 1.8 3.2 PQR - NaN NaN 1.6 - - ", "apis": "loc notna cumsum eq mask", "code": ["minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"], "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas"}
{"id": 88, "q": "transform a pandas dataframe in a pandas with multicolumns", "d": "I have the following pandas dataframe, where the columna is the dataframe index And i want to convert this datframe in to a multi column data frame, that looks like this I've tried transforming my old pandas dataframe in to a dict this way: But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome.", "q_apis": "transform where index", "io": "+----+-----------+------------+-----------+------------+ | | price_A | amount_A | price_B | amount_b | |----+-----------+------------+-----------+------------| | 0 | 0.652826 | 0.941421 | 0.823048 | 0.728427 | | 1 | 0.400078 | 0.600585 | 0.194912 | 0.269842 | | 2 | 0.223524 | 0.146675 | 0.375459 | 0.177165 | | 3 | 0.330626 | 0.214981 | 0.389855 | 0.541666 | | 4 | 0.578132 | 0.30478 | 0.789573 | 0.268851 | | 5 | 0.0943601 | 0.514878 | 0.419333 | 0.0170096 | | 6 | 0.279122 | 0.401132 | 0.722363 | 0.337094 | | 7 | 0.444977 | 0.333254 | 0.643878 | 0.371528 | | 8 | 0.724673 | 0.0632807 | 0.345225 | 0.935403 | | 9 | 0.905482 | 0.8465 | 0.585653 | 0.364495 | +----+-----------+------------+-----------+------------+ <s> +----+-----------+------------+-----------+------------+ | | A | B | +----+-----------+------------+-----------+------------+ | id | price | amount | price | amount | |----+-----------+------------+-----------+------------| | 0 | 0.652826 | 0.941421 | 0.823048 | 0.728427 | | 1 | 0.400078 | 0.600585 | 0.194912 | 0.269842 | | 2 | 0.223524 | 0.146675 | 0.375459 | 0.177165 | | 3 | 0.330626 | 0.214981 | 0.389855 | 0.541666 | | 4 | 0.578132 | 0.30478 | 0.789573 | 0.268851 | | 5 | 0.0943601 | 0.514878 | 0.419333 | 0.0170096 | | 6 | 0.279122 | 0.401132 | 0.722363 | 0.337094 | | 7 | 0.444977 | 0.333254 | 0.643878 | 0.371528 | | 8 | 0.724673 | 0.0632807 | 0.345225 | 0.935403 | | 9 | 0.905482 | 0.8465 | 0.585653 | 0.364495 | +----+-----------+------------+-----------+------------+ ", "apis": "columns MultiIndex from_tuples columns index name", "code": ["df.columns=pd.MultiIndex.from_tuples([x.split('_')[::-1] for x in df.columns])\ndf.index.name='id'\n"], "link": "https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns"}
{"id": 339, "q": "How to drop pandas consecutive column by column name simultaneously?", "d": "Here's my data The Output I expected, What I did But this is not efficient, how to do this effectively?", "q_apis": "drop name", "io": "Id Column1 Column2 Column3 Column4 .... Column112 Column113 ... Column143 1 67 89 86 43 56 72 67 <s> Id Column1 Column113 ... Column143 1 67 72 67 ", "apis": "drop loc columns", "code": ["df1 = df.drop(df.loc[:, 'Column2':'Column112'].columns, axis=1)\n"], "link": "https://stackoverflow.com/questions/60502832/how-to-drop-pandas-consecutive-column-by-column-name-simultaneously"}
{"id": 411, "q": "Pandas: Drop duplicates based on row value", "d": "I have a dataframe and I want to drop duplicates based on different conditions.... I want to drop all the duplicates from column A except rows with \"-\". After this, I want to drop duplicates from column A with \"-\" as a value based on their column B value. Given the input dataframe, this should return the following:- I have the following code but it's not very efficient for very large amounts of data, how can I improve this....", "q_apis": "value drop drop all drop value value", "io": " A B 0 1 1.0 1 1 1.0 2 2 2.0 3 2 2.0 4 3 3.0 5 4 4.0 6 5 5.0 7 - 5.1 8 - 5.1 9 - 5.3 <s> A B 0 1 1.0 2 2 2.0 4 3 3.0 5 4 4.0 6 5 5.0 7 - 5.1 9 - 5.3 ", "apis": "duplicated eq duplicated", "code": ["df[~df.duplicated('A')            # keep those not duplicates in A\n   | (df['A'].eq('-')             # or those '-' in A\n      & ~df['B'].duplicated())]   # which are not duplicates in B\n"], "link": "https://stackoverflow.com/questions/58490071/pandas-drop-duplicates-based-on-row-value"}
{"id": 290, "q": "How can I find and store how many columns it takes to reach a value greater than the first value in each row?", "d": "Original dataframe is df1. For each row, I want to find the first time a value is bigger than the value in the first column and store it in a new dataframe. df2 is the resulting dataframe. For example, for df1 row 1; the first value is 3 and the first value bigger than 3 is 4 (column c). Hence in df2 row 1, we store 2 (there are two columns from column a to c). For df1 row 2, the first value is 4 and the first value bigger than 4 is 5 (column d). Hence in df2 row 2, we store 3 (there are three columns from column a to d). For df1 row 3, the first value is 5 and the first value bigger than 5 is 6 (column e). Hence in df2 row 3, we store 4 (there are four columns from column a to e). I would appreciate the help.", "q_apis": "columns value first value first time value value first first value first value columns first value first value columns first value first value columns", "io": "df1 a b c d e 0 3 1 4 1 9 1 4 2 3 5 4 2 5 3 3 4 6 <s> df2 b 0 2 1 3 2 4 ", "apis": "columns get_indexer drop sub ge idxmax array", "code": ["s=df1.columns.get_indexer(df1.drop('a',1).sub(df1.a,0).ge(0).idxmax(1))\narray([1, 1, 3])\ndf['New']=s\n"], "link": "https://stackoverflow.com/questions/61979568/how-can-i-find-and-store-how-many-columns-it-takes-to-reach-a-value-greater-than"}
{"id": 459, "q": "Pandas: Keep values in a set of columns if they exist in another set of columns in the same row, otherwise set it to NaN", "d": "I have a couple of columns in my dataframe that have values in them. I want to only keep those values in those columns if they exist in another set of columns in the same row. Otherwise, I want to set the value to . Here's an example dataframe: In this case, I want and to be changed based on and : It's been difficult to form a query to google this, and the closest I've gotten is to use like this: Which gives me this: Which appears to be somewhat useful, but I'm not sure if this is the right path or what to do with it from here.", "q_apis": "values columns columns columns values values columns columns value query right", "io": " A B C D 0 1 30 1 29 1 5 42 99 5 2 64 67 12 22 3 2 22 22 0 4 43 6 9 43 <s> A B C D 0 1 30 1.0 NaN 1 5 42 NaN 5.0 2 64 67 NaN NaN 3 2 22 22.0 NaN 4 43 6 NaN 43.0 ", "apis": "map get", "code": ["df[['C', 'D']] = [\n    (c if c in ab else np.nan, d if d in ab else np.nan)\n    for *ab, c, d in zip(*map(df.get, ['A', 'B', 'C', 'D']))\n]\n"], "link": "https://stackoverflow.com/questions/56810353/pandas-keep-values-in-a-set-of-columns-if-they-exist-in-another-set-of-columns"}
{"id": 129, "q": "put only elements into a list with a certian number", "d": "is the sales ID and is the sold itemid. I would like to use all unique i_ids to find all purchases that have interacted with i_id. I also implemented this in the loop. What I would like that I only want to add something to the list when the has more of a 1 item. How do I do that so that I only add the purchases to the list if it contains more than one item? Output But what I want means that the element does not exist, I only wrote for a better understanding", "q_apis": "put all unique all add item add contains item", "io": "[[1], [1, 2, 3], [1], [4, 1, 2]] [[1, 2, 3], [4, 1, 2]] [[1, 2, 3], [3, 5]] [[4, 1, 2]] [[3, 5]] <s> [[REMOVED], [1, 2, 3], [REMOVED], [4, 1, 2]] [[1, 2, 3], [4, 1, 2]] [[1, 2, 3], [3, 5]] [[4, 1, 2]] [[3, 5]] ", "apis": "mean std mean std", "code": ["6.71 ms \u00b1 91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)", "12.3 ms \u00b1 201 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)"], "link": "https://stackoverflow.com/questions/65642545/put-only-elements-into-a-list-with-a-certian-number"}
{"id": 357, "q": "Multi-column to single column in Pandas", "d": "I have the following data frame : And I need this : This is just a basic example, the real deal can have over 60 children.", "q_apis": "", "io": " parent 0 1 2 3 0 14026529 14062504 0 0 0 1 14103793 14036094 0 0 0 2 14025454 14036094 0 0 0 3 14030252 14030253 14062647 0 0 4 14034704 14086964 0 0 0 <s> parent_id child_id 0 14026529 14062504 1 14025454 14036094 2 14030252 14030253 3 14030252 14062647 4 14103793 14036094 5 14034704 14086964 ", "apis": "astype where ne set_index stack reset_index name", "code": ["(df.astype('Int64').where(df.ne(0))\n .set_index('parent')\n .stack()\n .reset_index(level=0, name='child'))\n"], "link": "https://stackoverflow.com/questions/60042289/multi-column-to-single-column-in-pandas"}
{"id": 405, "q": "Groupby and perform row-wise calculation using a custom function", "d": "Following on from this question: python - Group by and add new row which is calculation of other rows I have a pandas dataframe as follows: And I want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to X and Z from col_2 and create a new row with these values. So the output would be as below: Where are the outputs of the function. Original question (which only requires a simple addition) was answered with: I'm now looking for a way to use a custom function, such as or , rather than . How can I modify this code to work with my new requirements?", "q_apis": "add value apply values columns values now", "io": "col_1 col_2 col_3 col_4 a X 5 1 a Y 3 2 a Z 6 4 b X 7 8 b Y 4 3 b Z 6 5 <s> col_1 col_2 col_3 col_4 a X 5 1 a Y 3 2 a Z 6 4 a NEW * * b X 7 8 b Y 4 3 b Z 6 5 b NEW * * ", "apis": "values replace", "code": ["def f(x):\n    y = x.values\n    return y[0] / y[1] # replace with your function\n"], "link": "https://stackoverflow.com/questions/46451284/groupby-and-perform-row-wise-calculation-using-a-custom-function"}
{"id": 249, "q": "How can I remove a certain type of values in a group in pandas?", "d": "I have the following dataframe which is a small part of a bigger one: I'd like to delete all rows where the last items are \"d\". So my desired dataframe would look like this: So the point is, that a group shouldn't have \"d\" as the last item. There is a code that deletes the last row in the groups where the last item is \"d\". But in this case, I have to run the code twice to delete all last \"d\"-s in group 3 for example. Is there a better solution to this problem?", "q_apis": "values delete all where last items last item last groups where last item delete all last", "io": " acc_num trans_cdi 0 1 c 1 1 d 3 3 d 4 3 c 5 3 d 6 3 d <s> acc_num trans_cdi 0 1 c 3 3 d 4 3 c ", "apis": "shift iloc iloc", "code": ["df = df[~((df['trans_cdi'] == 'd') & (df.shift(1)['trans_cdi'] == 'd'))]\nif df['trans_cdi'].iloc[-1] == 'd': df = df.iloc[0:-1]\ndf\n"], "link": "https://stackoverflow.com/questions/62967408/how-can-i-remove-a-certain-type-of-values-in-a-group-in-pandas"}
{"id": 595, "q": "Transform a large dataframe - takes too long", "d": "I have a dataframe loaded from a CSV in the following format: I want to transform it to the following format: This is my function ( is the original data frame) that does the transformation, but it takes 7 minutes for 547500 row dataframe. Is there a way to speed it up?", "q_apis": "transform", "io": " stock_code price 20180827 001 10 20180827 002 11 20180827 003 12 20180827 004 13 20180826 001 14 20180826 002 15 20180826 003 11 20180826 004 10 20180826 005 19 <s> 001 002 003 004 005 20180827 10 11 12 13 nan 20180826 14 15 11 10 19 ", "apis": "pivot index", "code": ["df = pd.pivot(df.index, df['stock_code'], df['price'])\n"], "link": "https://stackoverflow.com/questions/52033359/transform-a-large-dataframe-takes-too-long"}
{"id": 261, "q": "Drop last n rows within pandas dataframe groupby", "d": "I have a dataframe where I want to drop last rows within a group of columns. For example, say is defined as below the group is of columns and : Desired output for is as follows: Desired output for is as follows:", "q_apis": "last groupby where drop last columns columns", "io": ">>> df a b c d 0 abd john 0 1000 1 abd john 1 1001 4 pqr john 4 1004 9 xyz doe 9 1009 10 xyz doe 10 1010 11 xyz doe 11 1011 12 xyz doe 12 1012 13 xyz doe 13 1013 >>> <s> >>> df a b c d 0 abd john 0 1000 9 xyz doe 9 1009 10 xyz doe 10 1010 11 xyz doe 11 1011 12 xyz doe 12 1012 >>> ", "apis": "drop groupby tail index", "code": ["n = 2\ndf.drop(df.groupby(['a','b']).tail(n).index, axis=0)\n"], "link": "https://stackoverflow.com/questions/62882426/drop-last-n-rows-within-pandas-dataframe-groupby"}
{"id": 83, "q": "Python transposing multiple dataframes in a list", "d": "I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you.", "q_apis": "columns item get transpose get any", "io": "0 email factor1_final factor2_final factor3_final 1 john@abc.com 85% 90% 50% 2 peter@abc.com 80% 60% 60% 3 shelby@abc.com 50% 70% 60% 4 jess@abc.com 60% 65% 50% 5 mark@abc.com 98% 50% 60% <s> email john@abc.com peter@abc.com shelby@abc.com jess@abc.com mark@abc.com factor1 85% 80% 50% 60% 98% factor2 90% 60% 70% 65% 50% factor3 50% 60% 60% 50% 60% ", "apis": "set_index T", "code": ["dfs = [df.set_index('email').T for df in df_list]\n"], "link": "https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list"}
{"id": 451, "q": "pandas - downsample a more frequent DataFrame to the frequency of a less frequent DataFrame", "d": "I have two DataFrames that have different data measured at different frequencies, as in those csv examples: df1: df2: I would like to obtain a single df that have all the measures of both dfs at the times of the first one (which get data less frequently). I tried to do that with a for loop averaging over the df2 measures between two timestamps of df1 but it was extremely slow.", "q_apis": "DataFrame DataFrame at all at first get between", "io": "i,m1,m2,t 0,0.556529,6.863255,43564.844 1,0.5565576199999884,6.86327749999999,43564.863999999994 2,0.5565559400000003,6.8632764,43564.884 3,0.5565699799999941,6.863286799999996,43564.903999999995 4,0.5565570200000007,6.863277200000001,43564.924 5,0.5565316400000097,6.863257100000007,43564.944 ... <s> i,m3,m4,t 0,306.81162500000596,-1.2126870045404683,43564.878125 1,306.86175000000725,-1.1705838272666433,43564.928250000004 2,306.77552454544787,-1.1240195386446195,43564.97837499999 3,306.85900545454086,-1.0210345363692084,43565.0285 4,306.8354250000052,-1.0052431772666657,43565.078625 5,306.88397499999286,-0.9468344809917896,43565.12875 ... ", "apis": "groups cut right copy columns groupby get groupby groups mean reset_index", "code": ["groups =pd.cut(df2.t, bins= list(df1.t) + [np.inf],\n               right=False,\n               labels=df1['t'])\n\n# cols to copy\ncols = [col for col in df2.columns if col != 't']\n\n# groupby and get the average\nnew_df = (df2[cols].groupby(groups)\n                   .mean()\n                   .reset_index()\n         )\n"], "link": "https://stackoverflow.com/questions/57043695/pandas-downsample-a-more-frequent-dataframe-to-the-frequency-of-a-less-frequen"}
{"id": 259, "q": "Merge two columns of a dataframe into an already existing column of dictionaries as a key value pair", "d": "If we have 3 columns of a dataframe as : I want the column3 to be something like : I have tried a few things from using lambda functions with apply to iterating over rows but all were unsuccessful.", "q_apis": "columns value columns apply all", "io": "column1 : ['A','A','B','C'] column2 : [12,13,14,15] column3 : [{\"key1\":\"val1\"},{\"key2\":\"val2\"},{\"key3\":\"val3\"},{\"key4\":\"val4\"}] <s> column3 : [{\"key1\":\"val1\", \"A\":12},{\"key2\":\"val2\", \"A\":13},{\"key3\":\"val3\", \"B\":14},{\"key4\":\"val4\", \"C\":15}] ", "apis": "values", "code": ["df['col3'] = [{**d, k:v} for k,v,d in df.values.tolist()]\n"], "link": "https://stackoverflow.com/questions/62912823/merge-two-columns-of-a-dataframe-into-an-already-existing-column-of-dictionaries"}
{"id": 567, "q": "Swap contents of columns inside dataframe", "d": "I have a pandas dataframe with this contents; I would like to swap the contents between Column 1 and Column 2. The output dataframe should look like this; I am using python v3.6", "q_apis": "columns between", "io": "Column1 Column2 Column3 C11 C21 C31 C12 C22 C32 C13 C23 C33 <s> Column1 Column2 Column3 C21 C11 C31 C22 C12 C32 C23 C13 C33 ", "apis": "DataFrame columns", "code": ["df = pd.DataFrame({\"Column1\": [\"C11\", \"C12\", \"C13\"],\n                   \"Column2\": [\"C21\", \"C22\", \"C23\"],\n                   \"Column3\": [\"C31\", \"C32\", \"C33\"]})\ndf.columns = [\"Column2\", \"Column1\", \"Column3\"]\ndf[[\"Column1\", \"Column2\", \"Column3\"]]\n"], "link": "https://stackoverflow.com/questions/52107572/swap-contents-of-columns-inside-dataframe"}
{"id": 480, "q": "How does pandas convert one column of data into another?", "d": "I have a dataframe generated by pandas, as follows\uff1a I want to convert the CODE column data to get the NUM column. The encoding rules are as follows: thank you\uff01", "q_apis": "get", "io": "NO CODE 1 a 2 a 3 a 4 a 5 a 6 a 7 b 8 b 9 a 10 a 11 a 12 a 13 b 14 a 15 a 16 a <s> NO CODE NUM 1 a 1 2 a 2 3 a 3 4 a 4 5 a 5 6 a 6 7 b b 8 b b 9 a 1 10 a 2 11 a 3 12 a 4 13 b b 14 a 1 15 a 2 16 a 3 ", "apis": "eq where groupby ne shift cumsum cumcount", "code": ["a_group = df.CODE.eq('a')\n\ndf['NUM'] = np.where(a_group, \n                     df.groupby(a_group.ne(a_group.shift()).cumsum())\n                       .CODE.cumcount()+1, \n                     df.CODE)\n"], "link": "https://stackoverflow.com/questions/56086666/how-does-pandas-convert-one-column-of-data-into-another"}
{"id": 594, "q": "Add a different item from a list to each cell in a dataframe with Pandas", "d": "Given a dataframe I want to make a list of sequential that has as many elements as there are rows in the dataframe And then add each element of the list (as a string) onto the end of a single column in the dataframe. Does not work, as it adds the whole list as a string to each value as opposed to one value of the list per value in the dataframe. Essentially, I want to transform to So anyway to do that would be fine. Thanks for the help!", "q_apis": "item add value value value transform", "io": "A B C 23 16 85 9 74 12 99 24 83 <s> A B C 231 16 85 92 74 12 993 24 83 ", "apis": "astype reset_index index astype astype RangeIndex astype", "code": ["df['A'] = df.A.astype(str) + (df.reset_index().index + 1).astype(str)\n", "df['A'] = df.A.astype(str) + (pd.RangeIndex(len(df)) + 1).astype(str)\n"], "link": "https://stackoverflow.com/questions/52064324/add-a-different-item-from-a-list-to-each-cell-in-a-dataframe-with-pandas"}
{"id": 377, "q": "Pandas: Swap rows between columns", "d": "Some rows were input in the wrong columns so now I need to swap them. My current approach Expected output It works but this is only possible because it was 4 columns. Is there a better way to do this? Note the dtypes can cause issues with sorting is Maybe there is something like", "q_apis": "between columns columns now columns dtypes", "io": " a b c d 0 0 10 22:58:00 23:27:00 1 10 17 23:03:00 23:39:00 2 22:58:00 23:27:00 0 10 3 23:03:00 23:39:00 10 17 <s> a b c d 0 0 10 22:58:00 23:27:00 1 10 17 23:03:00 23:39:00 2 0 10 22:58:00 23:27:00 3 10 17 23:03:00 23:39:00 ", "apis": "DataFrame to_numpy", "code": ["AttributeError: 'DataFrame' object has no attribute 'to_numpy'"], "link": "https://stackoverflow.com/questions/59505475/pandas-swap-rows-between-columns"}
{"id": 121, "q": "Turn columns&#39; values to headers of columns with values 1 and 0 ( accordingly) [python]", "d": "I got a column of the form : The column represents the answers of users to a question of 5 choices (1-5). I want to turn this into a matrix of 5 columns where the indexes are the 5 possible answers and the values are 1 or 0 according to the user's given answer. Visualy i want a matrix of the form:", "q_apis": "columns values columns values columns where values", "io": "0 q4 1 4 2 3 3 1 4 2 5 1 6 5 7 1 8 3 <s> 0 q4_1 q4_2 q4_3 q4_4 q4_5 1 Nan Nan Nan 1 Nan 2 Nan Nan 1 Nan Nan 3 1 Nan Nan Nan Nan 4 Nan 1 Nan Nan Nan 5 1 Nan Nan Nan Nan ", "apis": "where", "code": ["for i in range(1,6):\n    df['q4_'+str(i)]=np.where(df.q4==i, 1, 0)\n\ndef df['q4']\n"], "link": "https://stackoverflow.com/questions/65739584/turn-columns-values-to-headers-of-columns-with-values-1-and-0-accordingly-p"}
{"id": 345, "q": "Use duplicated values to increment column", "d": "I have a Pandas dataframe and I want to increment a column based on the amount of duplicated values. So when a duplicate is found, all other occurrences is incremented. So given this input dataframe return I tried this line of code but I don't know how to increment", "q_apis": "duplicated values duplicated values all", "io": " SM 0 AB 1 AC 2 AD 3 AB 4 AB 5 AC 6 AE 7 AD <s> SM DM 0 AB AB 1 AC AC 2 AD AD 3 AB AB_1 4 AB AB_2 5 AC AC_1 6 AE AE 7 AD AD_1 ", "apis": "groupby cumcount where eq astype", "code": ["s = df.groupby('SM').cumcount()\n\ndf['DM'] = df['SM'].where(s.eq(0), df['SM'] + '_' + s.astype(str))\n"], "link": "https://stackoverflow.com/questions/60336550/use-duplicated-values-to-increment-column"}
{"id": 270, "q": "Append list to list in specific cell in pandas", "d": "I have a pandas dataframe, which looks like this: And I want to append a new list c = [5,6] to the row, where So the result would be: Right now my attempt looks like this: Any help is appreciated!", "q_apis": "append where now", "io": " key arr a 't1' [1,2] b 't2' [3,4] <s> key arr a 't1' [1,2,5,6] b 't2' [3,4] ", "apis": "loc Series repeat sum index index", "code": ["m = df['key'] == 't1'\n\ndf.loc[m, 'arr'] += pd.Series(np.repeat([c], m.sum(), axis=0).tolist(), index=df.index[m])\n"], "link": "https://stackoverflow.com/questions/62696143/append-list-to-list-in-specific-cell-in-pandas"}
{"id": 70, "q": "Group a dataframe on one column and take max from one column and its corresponding value from the other col", "d": "I have a large dataframe which has a similar pattern as below: And can be constructed as: Now I want to group this dataframe by the first column i.e., and take from the column and its corresponding value from . And if there are two max values in , then I would like to take alphabetically first value from . So my expected result would look like: I have tried but this selects max from and first from both at the same time. Additionally I know there is a approach, but this would take a lot of time for my dataset. Any suggestions on how could I proceed would be appreciated. Thanks in advance:)", "q_apis": "take max value first take value max values take first value max first at time take time", "io": " X Y Z 0 a p 2 1 a q 5 2 a r 6 3 a s 3 4 b w 10 5 b z 20 6 b y 9 7 b x 20 <s> df = { 'X': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], 'Y': ['p', 'q', 'r', 's', 'w', 'x', 'y', 'z'], 'Z': [2, 5, 6, 3, 10, 20, 9, 5] } ", "apis": "sort_values drop_duplicates", "code": ["df.sort_values(['X', 'Z', 'Y'], ascending=[True, False, True]).drop_duplicates('X')\n"], "link": "https://stackoverflow.com/questions/66638218/group-a-dataframe-on-one-column-and-take-max-from-one-column-and-its-correspondi"}
{"id": 19, "q": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?", "d": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result:", "q_apis": "name name columns values value concat value all values columns", "io": "(a) (b) (c) (d) a1 b1 c1 d1 a2 b2 c2 d2 <s> (a) (b) (e) (f) a3 b3 e1 f1 a4 b4 e1 f1 a5 b5 e2 f2 a6 b6 e4 f4 a7 a8 e4 f5 ", "apis": "ExcelWriter ExcelWriter read_excel items drop concat apply join groupby concat apply drop_duplicates last concat to_excel index", "code": ["import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"], "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da"}
{"id": 596, "q": "python subplot plot.bar from one dataframe and legend from a different dataframe", "d": "I have two data sets below Df1: I draw a bar diagram. (This is not an exact code of mine, but it will give you an idea) Now I want a legend based on a second data set of centroids. Df2: HPE subplot should have HPE column values (19.282091,1790.500000,1500.000000) as below. How can I do that?", "q_apis": "plot second values", "io": " Cluster HPE FRE UNE 0 0 176617 255282 55881 1 1 126130 7752 252045 2 2 12613 52326 7434 <s> Cluster HPE FRE UNE 0 0 19.282091 106.470162 1620.005037 1 1 1790.500000 367.625000 537.856177 2 2 1500.000000 180.148148 4729.275913 ", "apis": "", "code": ["subp.legend([str(a) + ' - ' + str(b) for a, b in zip(df2['Cluster'].tolist(), df2['HPE'].tolist())])\n"], "link": "https://stackoverflow.com/questions/51902887/python-subplot-plot-bar-from-one-dataframe-and-legend-from-a-different-dataframe"}
{"id": 289, "q": "How to pick some values of a column and make another one with them?", "d": "This is a table similar to the one I'm working with And what I'm trying to do is take some values of the column A that follow a certain pattern and create another column with such values. For example, the column C would have only the values from A that are bigger than 12, and column D the ones smaller or equal: I've tried making a list for each group of values, but I can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. I think there's an esasier way to do that, but I can't seem to find it. How can I do that?", "q_apis": "values take values values values values merge repeat columns", "io": " A B 0 12.2 43 1 10.1 32 2 3.4 34 3 12.0 55 4 40.6 31 <s> A B C D 0 12.2 43 12.2 NaN 1 10.1 32 NaN 10.1 2 3.4 34 NaN 3.4 3 12.0 55 NaN 12.0 4 40.6 31 40.6 NaN ", "apis": "loc loc", "code": ["df['C'] = df.loc[df.A > 12, 'A']\ndf['D'] = df.loc[df.A <= 12, 'A']\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/62011520/how-to-pick-some-values-of-a-column-and-make-another-one-with-them"}
{"id": 448, "q": "Using pandas.interpolate()", "d": "Suppose I would like to apply following command: which returns Question: How can i apply a restriction on the minimum number of valid numbers (i.e not NaN) before AND after a group of NaNs, so as to apply the interpolation In this example, i would like to fill first group of NaNs because there are minimum 3 valid numbers before AND after, but NOT interpolate the second group of NaNs, as there are only two valid numbers after the NaNs (and not 3 as i would prefer) Expected result:", "q_apis": "interpolate apply apply apply first interpolate second", "io": " 0 0 1.000000 1 2.000000 2 3.000000 3 3.166667 4 3.333333 5 NaN 6 3.666667 7 3.833333 8 4.000000 9 5.000000 10 6.000000 11 5.500000 12 5.000000 13 NaN 14 4.000000 15 3.500000 16 3.000000 17 4.000000 18 NaN <s> 0 0 1.000000 1 2.000000 2 3.000000 3 3.166667 4 3.333333 5 NaN 6 3.666667 7 3.833333 8 4.000000 9 5.000000 10 6.000000 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 3.000000 17 4.000000 18 NaN ", "apis": "copy array DataFrame values size notnull mask mask mask start stop indices mask mask mask mask size size DataFrame interpolate update", "code": ["    import numpy as np\n    import pandas as pd\n    from copy import deepcopy\n\n    a = np.array([1,2,3,np.nan,np.nan,np.nan,np.nan,np.nan,4,5,6,np.nan,np.nan,np.nan,np.nan,np.nan,3,4,np.nan,1])\n\n    df = pd.DataFrame(a)\n    # store values for later, to keep information from blocks that are below size limit:\n    temp = deepcopy(df[df[0].notnull()]) \n\n    mask = np.concatenate(([False],np.isfinite(a),[False]))\n    idx = np.nonzero(mask[1:] != mask[:-1])[0] # start and stop indices of your blocks of finite numbers\n    counts = (np.flatnonzero(mask[1:] < mask[:-1]) - np.flatnonzero(mask[1:] > mask[:-1])) # n finite numbers per block\n\n    sz_limit = 2 # set limit, exclusive in this case\n    for i, size in enumerate(counts):\n        if size <= sz_limit:\n            a[idx[i*2]:idx[i*2+1]] = np.nan\n\n", "\n    a_inter = pd.DataFrame(a).interpolate(limit = 2, limit_direction = 'both', limit_area = 'inside') \n    a_inter.update(other = temp)  \n\n"], "link": "https://stackoverflow.com/questions/57143033/using-pandas-interpolate"}
{"id": 4, "q": "Compare two columns that contains timestamps in pandas", "d": "Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0", "q_apis": "columns contains compare timestamp columns timestamp timestamp columns", "io": " Col0 Col1 Col2 Col3 Col4 1.txt 2021-06-23 15:04:30 2021-06-23 14:10:30 2021-06-23 14:15:30 2021-06-23 14:20:30 2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30 2021-06-23 14:35:30 2021-06-23 14:40:30 <s> Col0 Col1 Col2 Col3 Col4 1.txt 2021-06-23 15:04:30 NaN NaN NaN 2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30 NaN NaN ", "apis": "select_dtypes mask lt shift cumsum astype bool loc columns", "code": ["dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n"], "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas"}
{"id": 342, "q": "remove unnamed colums pandas dataframe", "d": "i'm a student and have a problem that i cant figure it out how to solve it.i have csv data like this : code for reading csv like this : SMT print out : expected output : i already trying but it will be like this : i already trying to put or //is not working, and the last time I tried it like this : and i got i just want to get rid the Unnamed: 5 ~ Unnamed: 8, how the correct way to get rid of this Unnamed thing ?", "q_apis": "put last time get get", "io": " 1 2 3 4 6 7 8 9 11 12 13 14 <s> 1 2 3 4 0 6 7 8 9 1 11 12 13 14 2 0 0 0 0 ", "apis": "read_csv test names", "code": ["pd.read_csv(\"test.csv\", usecols=(5,6,7,8), skiprows=3, nrows=3, header=0, names=[\"c1\", \"c2\", \"c3\", \"c4\"])\n"], "link": "https://stackoverflow.com/questions/60440882/remove-unnamed-colums-pandas-dataframe"}
{"id": 335, "q": "Using values from dataframe for calculation", "d": "I am selecting dedicated data from a dataframe and would like to make a linear interpolation based on my defined formula: I would like to interpolate e.g. between rank 2.0 and 3.0, where the needed rank is 2.5. The calculation looks like the following: y = -9.080002 + (-9.039993 - (-9.080002))*[(2.5-2)/(3-2)] = -9.059997500000 where the values are defined in the code as the following: The code looks like the following: The result looks like the following: The Excel file looks like the following:", "q_apis": "values interpolate between rank where rank where values", "io": "y = y0 + (y1 - y0) * [(x-x0)/(x1-x0)] <s> -9.080002000000007 2.0 -9.360001000000011 1.0 -9.22000150000001 ", "apis": "index index", "code": ["def Calc(data):\n\n    Rank = 2.5\n    Rank_Up = 3.0\n    Rank_Down = 2.0\n\n    Calculation = 0.0\n\n    check_int = isinstance(Rank, int)\n    if not check_int:\n        for ind in data.Rank:\n            if (ind in (Rank_Down, Rank_Up)):\n                index0 = data.Rank.index[ind-1]\n                index1 = data.Rank.index[ind]\n                Calculation = data['DataPoint'][index0] + (data['DataPoint'][index1]- data['DataPoint'][index0]) *((Rank-Rank_Down)/(Rank_Up-Rank_Down))\n                print(data['DataPoint'][index0], ind)\n                print(data['DataPoint'][index1], ind+1)\n                break\n\n    return Calculation\n"], "link": "https://stackoverflow.com/questions/60578553/using-values-from-dataframe-for-calculation"}
{"id": 222, "q": "How to average DataFrame row with another row only if the first row is a substring of other next row", "d": "I have a dataframe called 'data': I want to combine 'ABC-1' with 'ABC-1B' into a single row using the first USER name and then averaging the two values to arrive here: The dataframe may not be in order and there are other values in there as well that are unrelated that don't need averaging. I only want to average the two rows where 'XXX-X' is in 'XXX-XB'", "q_apis": "DataFrame first combine first name values values where", "io": "USER VALUE XOXO 21 ABC-1 2 ABC-1B 4 ABC-2 4 ABC-2B 6 PEPE 12 <s> USER VALUE XOXO 21 ABC-1 3 ABC-2 5 PEPE 12 ", "apis": "replace groupby mean", "code": ["df.USER = df.USER.str.replace('(-\\d)B', r\"\\1\")\ndf = df.groupby(\"USER\", as_index=False, sort=False).VALUE.mean()\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/63621901/how-to-average-dataframe-row-with-another-row-only-if-the-first-row-is-a-substri"}
{"id": 643, "q": "Calculating the duration an event in a time series python", "d": "I have a dataframe as show below: The index is datetime and have column record the rainfall value(unit:mm) in each hour,I would like to calculate the \"Average wet spell duration\", which means the average of continuous hours that exist values (not zero) in a day, so the calculation is and the \"average wet spell amount\", which means the average of sum of the values in continuous hours in a day. The datafame above is just a example, the dataframe which I have have more longer time series (more than one year for example), how can I write a function so it could calculate the two value mentioned above in a better way? thanks in advance! P.S. the values may be NaN, and I would like to just ignore it.", "q_apis": "time index value hour values day sum values day time year value values", "io": "2 + 4 + 1 + 1 + 2 + 5 / 6 (events) = 2.5 (hr) <s> { (14.5 + 15.8) + ( 13.6 + 4.3 + 13.7 + 14.4 ) + (17.2) + (5.3) + (2 + 4)+ (3.9 + 7.2 + 1 + 1 + 10) } / 6 (events) = 21.32 (mm) ", "apis": "columns day value astype bool shift value astype bool cumsum day index normalize day get unique count value count value astype bool groupby day nunique value astype bool groupby day value count map day map day map groupby day value sum value", "code": ["# create helper columns defining contiguous blocks and day\ndf['block'] = (df['value'].astype(bool).shift() != df['value'].astype(bool)).cumsum()\ndf['day'] = df['index'].dt.normalize()\n\n# group by day to get unique block count and value count\nsession_map = df[df['value'].astype(bool)].groupby('day')['block'].nunique()\nhour_map = df[df['value'].astype(bool)].groupby('day')['value'].count()\n\n# map to original dataframe\ndf['sessions'] = df['day'].map(session_map)\ndf['hours'] = df['day'].map(hour_map)\n\n# calculate result\nres = df.groupby(['day', 'hours', 'sessions'], as_index=False)['value'].sum()\nres['duration'] = res['hours'] / res['sessions']\nres['amount'] = res['value'] / res['sessions']\n"], "link": "https://stackoverflow.com/questions/49352279/calculating-the-duration-an-event-in-a-time-series-python"}
{"id": 114, "q": "Create new rows in Pandas, by adding to previous row, looping until x number of rows are made", "d": "Input: Output: Desired Output: Starting from an initial reference row of list \"tw\" I need to add 1 to the starting value and keep going. How do I loop and keep creating rows so that the next row is the previous row +1, I need to do this for 3000 rows. A lot of solutions I've seen require me to create lists and add to the pandas dataframe however I cannot manually create 3000 lists and then manually add them to my dataframe. There must be a way to loop over this, please help!", "q_apis": "add value add add", "io": " 1 2 3 4 0 4 7 3 5 <s> 1 2 3 4 0 4 7 3 5 1 5 8 4 6 2 6 9 5 7 ... ... ... ... ... 3000 3003 3006 3002 3004 ", "apis": "DataFrame columns reindex fillna cumsum", "code": ["df = (pd.DataFrame([tw], columns=[1,2,3,4])\n        .reindex(range(N))\n        .fillna(1, downcast='infer')\n        .cumsum())\n"], "link": "https://stackoverflow.com/questions/65863722/create-new-rows-in-pandas-by-adding-to-previous-row-looping-until-x-number-of"}
{"id": 108, "q": "Assigning column to larger DataFrame in specific positions", "d": "I have two lists. I want to create C: Basically, where in there is a I want to have a value of , where there is a , a . In reality, contains around 10k elements, around 40k and I have many of them. I am working with a pandas.DataFrame (each \"array\" is a column of two different dataframes, I have to \"fit\" in by transforming it into ). I have done it with a for loop, but I am wondering how to do it in a better way. Thank you.", "q_apis": "DataFrame where value where contains DataFrame array", "io": "A = [0, 1, 0, 0, 1, 1, 0, 1] B = [2, 3, 4, 5] <s> C = [NaN, 2, NaN, NaN, 3, 4, NaN, 5] ", "apis": "pop", "code": ["[None if i==0 else B.pop(0) for i in A]\n", "[None, 2, None, None, 3, 4, None, 5]\n"], "link": "https://stackoverflow.com/questions/66034318/assigning-column-to-larger-dataframe-in-specific-positions"}
{"id": 537, "q": "Change values in columns of dataframe depending on values of other columns (values come from lists)", "d": "I have a Data Frame in python, for example this one: The code for the creation of the dataframe: Let the list1 be ['A','C','E'] and list2 be ['B','D','F']. What I want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). I have tried which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. Could someone help please? This is my expected output:", "q_apis": "values columns values columns values last replace right", "io": " col1 col2 col3 col4 0 A C B D 1 C E E A 2 E A E A 3 A D D D 4 B B B B 5 D D D D 6 F F A F 7 E E E E 8 B B B B <s> col1 col2 col3 col4 0 A B D 1 C E E A 2 E A E A 3 A D D 4 B B B B 5 D D D D 6 F F A F 7 E E E E 8 B B B B ", "apis": "isin isin any loc", "code": ["m1 = df['col1'].isin(list1)\nm2 = df[['col2', 'col3', 'col4']].isin(list2).any(1)\n\ndf.loc[m1 & m2, 'col2'] = ''\n"], "link": "https://stackoverflow.com/questions/54005974/change-values-in-columns-of-dataframe-depending-on-values-of-other-columns-valu"}
{"id": 645, "q": "Shift NaNs to the end of their respective rows", "d": "I have a DataFrame like : What I want to get is This is my approach as of now. Is there any efficient way to achieve this ? Here is way to slow . Thank you for your assistant!:) My real data size", "q_apis": "DataFrame get now any size", "io": " 0 1 2 0 0.0 1.0 2.0 1 NaN 1.0 2.0 2 NaN NaN 2.0 <s> Out[116]: 0 1 2 0 0.0 1.0 2.0 1 1.0 2.0 NaN 2 2.0 NaN NaN ", "apis": "values left", "code": ["df[:] = justify(df.values, invalid_val=np.nan, axis=1, side='left')\n"], "link": "https://stackoverflow.com/questions/45970751/shift-nans-to-the-end-of-their-respective-rows"}
{"id": 501, "q": "Add row values of all columns when a particular column value is null until it gets the not null values?", "d": "I have a data frame like this: If col1 value is null I want to add all the values of other columns untill it finds the notnull element in col1 The data frame i am looking for should look like: How to do in in most efficient way using python/pandas", "q_apis": "values all columns value values value add all values columns notnull", "io": "df col1 col2 col3 col4 A 12 34 XX B 20 25 PP B nan nan nan nan P 54 nan nan R nan nan nan nan nan PQ C D 32 SS R S 32 RS <s> col1 col2 col3 col4 A 12 34 XX B 20 25 PP B PR 54 PQ C D 32 SS R S 32 RS ", "apis": "notna cumsum ffill select_dtypes columns fillna mean dtype join groupby agg reset_index drop reset_index", "code": ["df['new'] = df['col1'].notna().cumsum()\ndf['col1'] = df['col1'].ffill()\nc = df.select_dtypes(object).columns\ndf[c] = df[c].fillna('')\n\nf = lambda x: x.mean() if np.issubdtype(x.dtype, np.number) else ''.join(x)\ndf = df.groupby(['col1', 'new']).agg(f).reset_index(level=1, drop=True).reset_index()\n"], "link": "https://stackoverflow.com/questions/55356991/add-row-values-of-all-columns-when-a-particular-column-value-is-null-until-it-ge"}
{"id": 181, "q": "Pandas - shifting a rolling sum after grouping spills over to following groups", "d": "I might be doing something wrong, but I was trying to calculate a rolling average (let's use sum instead in this example for simplicity) after grouping the dataframe. Until here it all works well, but when I apply a shift I'm finding the values spill over to the group below. See example below: Expected result: Result I actually get: You can see the result of A2 gets passed to B3 and the result of B5 to C6. I'm not sure this is the intended behaviour and I'm doing something wrong or there is some bug in pandas? Thanks", "q_apis": "rolling sum groups rolling sum all apply shift values get", "io": "X A 0 NaN 1 NaN 2 3.0 B 3 NaN 4 NaN 5 3.0 C 6 NaN 7 NaN 8 3.0 <s> X A 0 NaN 1 NaN 2 3.0 B 3 5.0 4 NaN 5 3.0 C 6 5.0 7 NaN 8 3.0 ", "apis": "groupby rolling sum groupby shift groupby transform rolling sum shift", "code": ["grouped_df = (df.groupby(by='X')['Y'].rolling(window=2, min_periods=2).sum()\n                .groupby(level=0).shift(periods=1)\n             )\n", "grouped_df = (df.groupby('X')['Y']\n                .transform(lambda x: x.rolling(window=2, min_periods=2)\n                                      .sum().shift(periods=1))\n             )\n"], "link": "https://stackoverflow.com/questions/64592950/pandas-shifting-a-rolling-sum-after-grouping-spills-over-to-following-groups"}
{"id": 183, "q": "merge pandas dataframes under new index level", "d": "I have 2 s and that I want to combine into a single dataframe : Dataframe should contain one more column index level than and , and contain each under its own level-0 identifier, like so: Any ideas as to how to do this? It's a bit like ing the two frames: ...but using an additional level, instead of a suffix, to prevent name collisions. I tried: I could use loops to build up the series-by-series, but that doesn't seem right. Many thanks.", "q_apis": "merge index combine index name right", "io": "act #have a b 0 0.853910 0.405463 1 0.822641 0.255832 2 0.673718 0.313768 exp #have a c 0 0.464781 0.325553 1 0.565531 0.269678 2 0.363693 0.775927 <s> df #want act exp a b a c 0 0.853910 0.405463 0.464781 0.325553 1 0.822641 0.255832 0.565531 0.269678 2 0.673718 0.313768 0.363693 0.775927 ", "apis": "concat keys", "code": ["pd.concat([act, exp], axis=1, keys=['act', 'exp'])\n"], "link": "https://stackoverflow.com/questions/64577375/merge-pandas-dataframes-under-new-index-level"}
{"id": 623, "q": "Python - Add a column to a dataframe containing a value from another row based on condition", "d": "My dataframe looks like this: Now I need to add another column which conatains the difference between the value of column from the current row and the value of column from the row with the same number () and the group () that is written in . Exeption: If equals , and are the same. So the result should be: Explanation for the first two rows: First row: equals -> = Second row: search for the row with the same (123) and as (A1) and calculate of the current row minus of the referenced row (7.3 - 5.0 = 2.3). I thought I might need to use groupby() and apply(), but how? Hope my example is detailed enough, if you need any further information, please ask :)", "q_apis": "value add difference between value value equals first equals groupby apply any", "io": "+-----+-------+----------+-------+ | No | Group | refGroup | Value | +-----+-------+----------+-------+ | 123 | A1 | A1 | 5.0 | | 123 | B1 | A1 | 7.3 | | 123 | B2 | A1 | 8.9 | | 123 | B3 | B1 | 7.9 | | 465 | A1 | A1 | 1.4 | | 465 | B1 | A1 | 4.5 | | 465 | B2 | B1 | 7.3 | +-----+-------+----------+-------+ <s> +-----+-------+----------+-------+----------+ | No | Group | refGroup | Value | refValue | +-----+-------+----------+-------+----------+ | 123 | A1 | A1 | 5.0 | 5.0 | | 123 | B1 | A1 | 7.3 | 2.3 | | 123 | B2 | A1 | 8.9 | 3.9 | | 123 | B3 | B1 | 7.9 | 0.6 | | 465 | A1 | A1 | 1.4 | 1.4 | | 465 | B1 | A1 | 4.5 | 3.1 | | 465 | B2 | B1 | 7.3 | 2.8 | +-----+-------+----------+-------+----------+ ", "apis": "merge where value value", "code": ["df_out = df.merge(df, \n                  left_on=['No','refGroup'], \n                  right_on=['No','Group'], \n                  suffixes=('','_ref'))\n\ndf['refValue'] = np.where(df_out['Group'] == df_out['refGroup'],\n                          df_out['value'],\n                          df_out['value'] - df_out['value_ref'])\n\ndf\n"], "link": "https://stackoverflow.com/questions/50860328/python-add-a-column-to-a-dataframe-containing-a-value-from-another-row-based-o"}
{"id": 390, "q": "New dataframe which contais a certain value within each group", "d": "I have a dataframe as below I want to group by & and get a new dataframe with all the groups that contains 6 or 14 When I use the code below I accurately get the groups which have either 6 or 14 as below I am just not able to use this information to get the new dataframe which has the groups that are . The expected output is the new dataframe as below. Can anyone guide?", "q_apis": "value get all groups contains get groups get groups", "io": "User eve Ses a 123 1 a 123 2 a 123 3 a 123 4 a 123 5 a 123 6 a 456 1 a 456 2 a 456 3 a 456 4 a 456 5 a 456 14 a 456 7 a 456 8 a 456 9 a 456 10 a 888 1 a 888 2 a 888 3 a 888 4 a 888 5 a 888 5 a 888 7 a 888 8 b 123 1 b 123 2 b 123 3 b 123 4 b 123 5 b 123 6 b 456 1 b 456 2 b 456 3 b 456 4 b 456 5 b 456 9 b 456 7 b 456 8 b 456 9 b 456 10 b 888 1 b 888 2 b 888 3 b 888 4 b 888 5 b 888 6 b 888 7 b 888 8 <s> User eve Ses a 123 1 a 123 2 a 123 3 a 123 4 a 123 5 a 123 6 a 456 1 a 456 2 a 456 3 a 456 4 a 456 5 a 456 14 a 456 7 a 456 8 a 456 9 a 456 10 b 123 1 b 123 2 b 123 3 b 123 4 b 123 5 b 123 6 b 888 1 b 888 2 b 888 3 b 888 4 b 888 5 b 888 6 b 888 7 b 888 8 ", "apis": "groupby filter any any", "code": ["df = df.groupby(['User','eve']).filter(lambda x: (x['Ses']==6).any()|(x['Ses']==14).any())\n"], "link": "https://stackoverflow.com/questions/59189297/new-dataframe-which-contais-a-certain-value-within-each-group"}
{"id": 588, "q": "Repeating a series to create a df in python", "d": "I have a series, How do I repeat this column 9 times to create a dataframe. Expected output: is I can use but this is a bit messy. I tried but it wouldn't work along , and isn't what I need Thanks", "q_apis": "repeat", "io": " A 0 1.5 1 2.5 2 1.3 <s> A A ... A 0 1.5 1.5 ... 1.5 1 2.5 2.5 ... 2.5 2 1.3 1.3 ... 1.3 ", "apis": "concat keys", "code": ["pd.concat([s for i in range(9)], axis=1)\n", "keys=[f'A{i}' for i in range(9)]\n"], "link": "https://stackoverflow.com/questions/52151831/repeating-a-series-to-create-a-df-in-python"}
{"id": 94, "q": "Create pandas duplicate rows based on the number of items in a list type column", "d": "I have a data frame like this, Now I want to create new rows based on the number of values in the col2 list where the col1 values will be same so the final data frame would look like, I am looking for some pandas short cuts to do it more efficiently", "q_apis": "items values where values", "io": " df col1 col2 A [1] B [1,2] A [2,3,4] C [1,2] B [4] <s> df col1 col2 A [1] B [1] B [2] A [2] A [3] A [4] C [1] C [2] B [4] ", "apis": "explode apply", "code": ["df2 = df.explode('col2')\n\ndf2['col2'] = df2['col2'].apply(lambda x: [x])\n"], "link": "https://stackoverflow.com/questions/66255188/create-pandas-duplicate-rows-based-on-the-number-of-items-in-a-list-type-column"}
{"id": 578, "q": "Pandas DataFrame filter", "d": "My question is about the pandas.DataFrame.filter command. It seems that pandas creates a copy of the data frame to write any changes. How am I able to write on the data frame itself? In other words: Output: Desired Output:", "q_apis": "DataFrame filter DataFrame filter copy any", "io": " col1 col2 0 1 3 1 2 4 <s> col1 col2 0 10 3 1 2 4 ", "apis": "filter columns loc", "code": ["cols = df.filter(regex='col1').columns \ndf.loc[0, cols]=10\n"], "link": "https://stackoverflow.com/questions/52591303/pandas-dataframe-filter"}
{"id": 370, "q": "Pandas conditionally creating a new dataframe using another", "d": "I have a list; I want to create another where entries corresponding to positive values above are sum of positives, and those corresponding to negative values above are sum negatives. So the desired output is: I am doing this: So basically i was trying to create an empty dataframe like raw, and then conditionally fill it. However, the above method is failing. Even when i try to create a new column instead of a new df, it fails: The best solution I've found so far is this: However, I dont understand what is wrong with the other approaches. Is there something I am missing here?", "q_apis": "where values sum values sum empty", "io": "orig= [2, 3, 4, -5, -6, -7] <s> final = [9, 9, 9, 18, 18, 18] ", "apis": "loc values", "code": ["final.loc[raw['raw'] > 0, 'final'] = sum_pos.values\n\nprint(final)\n"], "link": "https://stackoverflow.com/questions/50121789/pandas-conditionally-creating-a-new-dataframe-using-another"}
{"id": 225, "q": "Merge two dataframes with uncertainty on keys", "d": "I am trying to use 'merge' to combine two data frames with shape: Those looks as follows: What I am looking for is to merge columns 'A' and 'B' with a defined uncertainty. For example + - 0.5. I don't have clear how to handle this. What I was trying to do is to manually add an uncertainty: After this, I do the merge: But here I got stuck because I can not figure it out how to use a conditional merge. The idea is to merge all those rows were columns 'A' and 'B' be the same with a definite certainty The expected output would be:", "q_apis": "keys merge combine shape merge columns add merge merge merge all columns", "io": "In [29]: df1 Out[29]: ID1 A B 0 10 1 3 1 32 4 5 2 53 2 2 3 65 5 9 4 3 4 3 In [32]: df2 Out[32]: ID2 A B 0 68 9 6 1 35 5 5 2 93 3 1 3 5 7 9 4 23 4 3 <s> df3: ID1 A B ID2 A B 1 32 4 5 35 5 5 2 53 2 2 93 3 1 4 3 4 3 23 4 3 ", "apis": "assign merge assign assign assign query drop columns to_string index DataFrame DataFrame columns merge_asof merge_asof sort_values sort_values sort_values sort_values value value merge_asof assign sort_values assign sort_values to_string index to_string index", "code": ["dfs = (df1.assign(foo=1).merge(df2.assign(foo=1), on=\"foo\", suffixes=(\"\",\"_df2\"))\n .assign(adiff=lambda x: x[\"A\"]-x[\"A_df2\"])\n .assign(bdiff=lambda x: x[\"B\"]-x[\"B_df2\"])\n .query(\"adiff>=-1 and adiff<=1 and bdiff>=-1 and bdiff<=1\")\n .drop(columns=[\"adiff\",\"bdiff\",\"foo\"])\n)\n\nprint(dfs.to_string(index=False))\n", "df1 = pd.DataFrame({'ID1':[10,32,53,65,3],'A':[1,4,2,5,4],'B':[3,5,2,9,3]})\ndf2 = pd.DataFrame({'ID2':[68,35,93,5,23],'A':[9,5,3,7,4],'B':[6,5,1,9,3]})\n\n\n# find nearest row in df2 when matching on columns A or B\ndfmab = pd.merge_asof(\n    (pd.merge_asof(df1.sort_values(\"A\"), df2.sort_values(\"A\"), \n                   on=\"A\", direction=\"nearest\", suffixes=(\"\",\"_Adf2\"))\n    ).sort_values(\"B\") \n    ,df2.sort_values(\"B\"),\n    on=\"B\", direction=\"nearest\", suffixes=(\"\",\"_Bdf2\")\n)\n\n# calculate a value that represents value of row\ndfmd = pd.merge_asof(\n    df1.assign(d=lambda x: x[\"A\"]*x[\"B\"]).sort_values(\"d\"),\n    df2.assign(d=lambda x: x[\"A\"]*x[\"B\"]).sort_values(\"d\"),\n    on=\"d\", direction=\"nearest\", suffixes=(\"\",\"_Ddf2\")\n)\n\nprint(dfmab.to_string(index=False))\nprint(dfmd.to_string(index=False))\n"], "link": "https://stackoverflow.com/questions/63609893/merge-two-dataframes-with-uncertainty-on-keys"}
{"id": 640, "q": "Pandas : Get the least number of records so all columns have at least one non null value", "d": "I have a dataframe with 62 columns that are largely null. Some records have multiple columns with non-null values, others just a single non-null. I'm wondering if there's a way to use .dropna or other strategy to return the least number of rows with each column having at least one non-null value. For a simplified example Would return ...", "q_apis": "all columns at value columns columns values dropna at value", "io": " a b c NaN 1 NaN 1 NaN NaN NaN NaN NaN NaN 1 1 <s> a b c 1 NaN NaN NaN 1 1 ", "apis": "sample DataFrame columns empty notnull sum idxmax append columns drop columns columns loc notnull any loc", "code": ["import pandas as pd\nimport numpy as np\n\n# sample dataframe\ndf = pd.DataFrame({'a':[np.nan,1,np.nan,np.nan],'b':[1, np.nan, np.nan, 1],'c':[np.nan, np.nan, np.nan, 1]})\ndf_orig = df\ncols = df.columns.tolist()\n\nrows = []\nwhile not df.empty:\n    ## Find the row with most non-null column entries\n    x = df.notnull().sum(axis=1).idxmax() # edit - fix for null/nonnull\n    ## Add the row to our list and continue\n    rows.append(x)\n    ## Remove the columns from our dataframe\n    df = df.drop(columns=df.columns[df.loc[[x]].notnull().any()].tolist())\n\n## Access the dataframe with only 'essential' rows\ndf_orig.loc[rows]\n"], "link": "https://stackoverflow.com/questions/49925888/pandas-get-the-least-number-of-records-so-all-columns-have-at-least-one-non-nu"}
{"id": 457, "q": "Python[pandas]: Select certain rows by index of another dataframe", "d": "I have a dataframe and I would select only rows that contain index value into df1.index. for Example: and these indexes I would like this output: thanks", "q_apis": "index select index value index", "io": "In [96]: df Out[96]: A B C D 1 1 4 9 1 2 4 5 0 2 3 5 5 1 0 22 1 3 9 6 <s> In [96]: df Out[96]: A B C D 1 1 4 9 1 3 5 5 1 0 22 1 3 9 6 ", "apis": "loc index index loc index index loc index intersection index", "code": ["df = df.loc[df.index & df1.index]\ndf = df.loc[np.intersect1d(df.index, df1.index)]\ndf = df.loc[df.index.intersection(df1.index)]\n"], "link": "https://stackoverflow.com/questions/48864923/pythonpandas-select-certain-rows-by-index-of-another-dataframe"}
{"id": 316, "q": "Pandas assignment and copy", "d": "If we run the following code, We get Whereas, if we run the following, We get I know there's a concept of pass by object reference in python. Why don't the in the second code gets copied? Thanks", "q_apis": "copy get get second", "io": " 0 0 1.298967 1 -0.887922 2 1.913559 3 -0.082032 4 -0.466594 .. ... 95 -0.845137 96 0.628542 97 -0.588897 98 0.464374 99 0.267946 <s> 0 a 0 -0.510875 1 1 0.401580 1 2 -0.037484 1 3 -0.935115 1 4 -1.108471 1 .. ... .. 95 0.362075 1 96 -1.017991 1 97 1.881081 1 98 0.376828 1 99 0.771661 1 ", "apis": "assign DataFrame columns assign DataFrame DataFrame copy DataFrame copy DataFrame", "code": ["def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) #doesnot return the changed columns\n", "def f(df):\n    df = df.assign(b = 1)\n    df[\"a\"] = 1\n    return df\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df))\n", "def f(df):\n    df = df\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df)\n", "def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n\ndf = pd.DataFrame(np.random.randn(100, 1))\nf(df)\nprint(df) # doesnot return the a column\n", "def f(df):\n    df = df.copy()\n    df[\"a\"] = 1\n    return df\ndf = pd.DataFrame(np.random.randn(100, 1))\nprint(f(df)) #returns the column a\n"], "link": "https://stackoverflow.com/questions/61024520/pandas-assignment-and-copy"}
{"id": 409, "q": "Replace repetitive number with NAN values except the first, in pandas column", "d": "I have a data frame like this, Now we can see that there is continuous occurrence of A, B and C. I want only the rows where the occurrence is starting. And the other values of the same occurrence will be nan. The final data frame I am looking for will look like, I can do it using for loop and comparing, But the execution time will be more. I am looking for pythonic way to do it. Some panda shortcuts may be.", "q_apis": "values first where values time", "io": "df col1 col2 1 A 2 A 3 B 4 C 5 C 6 C 7 B 8 B 9 A <s> df col1 col2 1 A 2 NA 3 B 4 C 5 NA 6 NA 7 B 8 NA 9 A ", "apis": "where ne shift where ne shift ne shift dtype bool", "code": ["df['col2'] = df['col2'].where(df['col2'].ne(df['col2'].shift()))\n#alternative\n#df['col2'] = np.where(df['col2'].ne(df['col2'].shift()), df['col2'], np.nan)\n", "print (df['col2'].ne(df['col2'].shift()))\n0     True\n1    False\n2     True\n3     True\n4    False\n5    False\n6     True\n7    False\n8     True\nName: col2, dtype: bool\n"], "link": "https://stackoverflow.com/questions/58538845/replace-repetitive-number-with-nan-values-except-the-first-in-pandas-column"}
{"id": 329, "q": "How to get absolute difference amongst all the values of a column with each other?", "d": "I am trying to find difference among-st all values in key column keeping these 4 digit code as my index value. I tried using pivot for this operation but failed. It would be really helpful if I can get the approach for this presentation. df1 Expected df", "q_apis": "get difference all values difference all values index value pivot get", "io": "Name | Key | 1001 | 1002 | 1003 | Abb AB 5 8 10 Baa BA 10 11 33 Cbb CB 12 40 90 <s> Code | Key | AB | BA | CB | 1001 AB 0 5 7 1001 BA 5 0 2 1001 CB 7 2 0 1002 . . . 1003 ", "apis": "loc repeat index values columns columns columns droplevel", "code": ["df2 = df1.loc[np.repeat(df1.index.values,len(df1.columns))] \ndf2.columns = df2.columns.droplevel(0)\n"], "link": "https://stackoverflow.com/questions/60763203/how-to-get-absolute-difference-amongst-all-the-values-of-a-column-with-each-othe"}
{"id": 353, "q": "Group together matched pairs across multiple columns Python", "d": "Thank you for reading. I have a dataframe which looks like this: Each row consists of a match between two IDs (e.g. ID1 from Col_A matches to ID2 from Col_B on the first row). In the example above, all 5 IDs are connected (1 is connected to 2, 2 to 3, 2 to 4, 1 to 5). I therefore want to create a new column which clusters all of these rows together so that I can easily access each group of matched pairs: I have not yet been able to find a similar question, but apologies if this is a duplicate. Many thanks in advance for any advice.", "q_apis": "columns between first all all any", "io": "Col_A Col_B Col_C Col_D Col_E 1 2 null null null 1 null 3 null null null 2 3 null null null 2 null 4 null 1 null null null 5 <s> Col_A Col_B Col_C Col_D Col_E Group ID 1 2 null null null 1 1 null 3 null null 1 null 2 3 null null 1 null 2 null 4 null 1 1 null null null 5 1 ", "apis": "apply dropna to_numpy to_numpy apply", "code": ["import networkx as nx\nd_edge = df.apply(lambda x: x.dropna().to_numpy(), axis=1)\nG = nx.from_edgelist(d_edge.to_numpy().tolist())\ncc_list = list(nx.connected_components(G))\ndf['groupid'] = d_edge.apply(lambda  x: [n for n, i in enumerate(cc_list) if x[0] in i][0] + 1)\ndf\n"], "link": "https://stackoverflow.com/questions/60153802/group-together-matched-pairs-across-multiple-columns-python"}
{"id": 242, "q": "Easy way to find find itinerant max value in grouped pandas list", "d": "I have a dataset where I have multiple value entries per year and some properties per entry. I want to find the maximum value per year and return that as a new data frame (to keep the other properties in the data frame), but only if the value in a year is greater than what it was in the years before (something like the \"All-time record value per year\"). So far I can find the max value per year, e.g. where the output then is This is almost what I want, expect for 2014 where I would like the value of 2013 with its according properties to go (since the value was greater in 2013 than it was in 2014). So the desired outcome would be Is there a good way to achieve this with pandas?", "q_apis": "max value where value year value year value year time value year max value year where where value value", "io": " Year Value Property 0 2012 35 Property B 1 2013 43 Property D 2 2014 37 Property C 3 2015 41 Property F <s> Year Value Property 0 2012 35 Property B 1 2013 43 Property D 2 2014 43 Property D 3 2015 43 Property D ", "apis": "cummax merge groupby first", "code": ["df_sorted_max['cummax_value'] = df_sorted_max.Value.cummax()\n\nprint(\n    df_sorted_max.merge(\n        df_sorted_max.groupby('cummax_value')[['Property']].first(),\n        on=\"cummax_value\"\n    )\n)\n"], "link": "https://stackoverflow.com/questions/63091593/easy-way-to-find-find-itinerant-max-value-in-grouped-pandas-list"}
{"id": 399, "q": "python pandas time series year extraction", "d": "I have a DF containing timestamps: I would like to extract the year from each timestamp, creating additional column in the DF that would look like: Obviously I can go over all DF entries stripping off the first 4 characters of the date. Which is very slow. I wonder if there is a fast python-way to do this. I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime(DF,'%Y-%m-%d %H:%M:%S') but when I try to then apply datetime.datetime.year(DF) it doesn't work. I will also need to parse the timestamps to months and combinations of years-months and so on... Help please. Thanks.", "q_apis": "time year year timestamp all first date to_datetime apply year parse", "io": "0 2005-08-31 16:39:40 1 2005-12-28 16:00:34 2 2005-10-21 17:52:10 3 2014-01-28 12:23:15 4 2014-01-28 12:23:15 5 2011-02-04 18:32:34 6 2011-02-04 18:32:34 7 2011-02-04 18:32:34 <s> 0 2005-08-31 16:39:40 2005 1 2005-12-28 16:00:34 2005 2 2005-10-21 17:52:10 2005 3 2014-01-28 12:23:15 2014 4 2014-01-28 12:23:15 2014 5 2011-02-04 18:32:34 2011 6 2011-02-04 18:32:34 2011 7 2011-02-04 18:32:34 2011 ", "apis": "year timestamp apply year", "code": ["df1['year'] = df1['timestamp'].apply(lambda x: x.year)\n"], "link": "https://stackoverflow.com/questions/28990256/python-pandas-time-series-year-extraction"}
{"id": 220, "q": "Python: Selecting rows matching Feb till current month in pandas dataframe", "d": "I have below dataframe: Considering the current month is June, the expected output is as follows: Here i have done filtering of rows by months from Feb:Current Month (in this case June) and then group by to find all the names once per mode. (Example: F will be only once for actual and once for plan) I previously tried taking a transpose of columns and then using the below to summarize the data till current month: where : But this is getting very complicated since the actual data has lots and lots of modes and also many years of data. Is there any easier solution for the same where this complication can be avoided and the similar solution can be achieved? In the end i am expecting to have the below dataframe: I guess i can have this format using pivot_table in pandas:", "q_apis": "month month all names mode transpose columns month where any where pivot_table", "io": "Name1 Name2 Mode Value1 Value2 C N Actual 4 4 D N Plan 2 2 E N Actual 10 10 F N Actual 7 7 F N Plan 3 3 <s> Name1 Name2 Actual_Value1 Actual_Value2 Plan_Value1 Plan_Value1 C N 4 4 D N 2 2 E N 10 10 F N 7 7 3 3 ", "apis": "today month isin groupby agg sum sum pivot_table index columns values sum reset_index rename_axis columns replace", "code": ["lstAllMonths=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\ncurMth = datetime.today().month  # 7=July\ncurMth = 6  # Jun for testing\nlstMth = lstAllMonths[1:curMth]\n\ndf = df[df['Month'].isin(lstMth)][['Name1','Name2','Mode','Value1','Value2']]\ngb = df.groupby(['Name1','Name2','Mode'])\ndfagg = gb.agg({'Value1':sum, 'Value2':sum})\n\ndfpvt = pd.pivot_table(dfagg,index=['Name1', 'Name2'], \n                      columns=['Mode'],\n                      values=['Value1', 'Value2'], \n                      aggfunc=np.sum, fill_value=0).reset_index().rename_axis(1)\n                      \ndfpvt.columns=['Name1','Name2','Actual_Value1','Plan_Value1','Actual_Value2','Plan_Value2']\ndfpvt.replace(0,'', inplace=True)\ndfpvt = dfpvt[['Name1','Name2','Actual_Value1','Actual_Value2','Plan_Value1','Plan_Value2']]    \nprint(dfpvt)\n"], "link": "https://stackoverflow.com/questions/63000962/python-selecting-rows-matching-feb-till-current-month-in-pandas-dataframe"}
{"id": 125, "q": "How to spread a key-value pair across multiple columns and flatten the matrix based on another column?", "d": "Using Pandas 1.2.0, I want to transform this dataframe where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively: into: My attempt: What should I do next to flatten the diagonals of these sub-matrices and group by 'a'?", "q_apis": "value columns transform where contains groups value sub", "io": " a b c 0 x_1 1 6.00 1 x_1 2 3.00 2 x_1 3 0.00 3 x_1 4 1.00 4 x_1 5 3.40 5 j_2 1 4.50 6 j_2 2 0.10 7 j_2 3 0.20 8 j_2 5 0.88 <s> a 1 2 3 4 5 0 x_1 6.0 3.0 0.0 1.0 3.40 1 j_2 4.5 0.1 0.2 NaN 0.88 ", "apis": "pivot index columns values pivot", "code": ["df = df.pivot(index='a', columns='b', values='c')", "df = df.pivot('a', 'b', 'c')"], "link": "https://stackoverflow.com/questions/65692969/how-to-spread-a-key-value-pair-across-multiple-columns-and-flatten-the-matrix-ba"}
{"id": 10, "q": "Concatenate values and column names in a data frame to create a new data frame", "d": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used", "q_apis": "values names values names value name sample", "io": " Value col1 col2 col3 0 a aa ab ac 1 b ba bb bc 2 c ca cb cc 3 d da db dc 4 e ea eb ec <s> Value Col 1 0 a_Col 1 aa 1 a_Col 2 ab 2 a_Col 3 ac 3 b_Col 1 ba 4 b_Col 2 bb 5 b_Col 3 bc 6 c_Col 1 ca 7 c_Col 2 cb 8 c_Col 3 cc 9 d_Col 1 da 10 d_Col 2 db 11 d_Col 3 dc 12 e_Col 1 ea 13 e_Col 2 eb 14 e_Col 3 ec ", "apis": "melt drop columns", "code": ["x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n"], "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame"}
{"id": 658, "q": "Pass Different Columns in Pandas DataFrame in a Custom Function in df.apply()", "d": "Say I have a dataframe : I wanna have two new columns that are x * y and x * z: So I define a function (just for example) that takes either the string or the string as an argument to indicate which column I want to multiply with the column x: And apply the function to the dataframe : Apparently it is wrong here because I didn't specify the , or . Question is, just takes function name, how do I tell it to take the two arguments?", "q_apis": "DataFrame apply columns apply name take", "io": " x y z 0 1 2 3 1 4 5 6 2 7 8 9 <s> x y z xy xz 0 1 2 3 2 3 1 4 5 6 20 24 2 7 8 9 56 63 ", "apis": "apply apply", "code": ["def func(row, colName):\n    return row * colName\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x['x'], x[c]), axis=1)\n", "def func(row, colName):\n    return row['x'] * row[colName]\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x, c), axis=1)\n"], "link": "https://stackoverflow.com/questions/49301344/pass-different-columns-in-pandas-dataframe-in-a-custom-function-in-df-apply"}
{"id": 232, "q": "Python Dataframe rowwise min and max with NaN values", "d": "My dataframe has nans. But I am trying to find row wise min and max. How do I find it. I am tring to find min and max in each row and difference between them in column and . My code: Present output: Expected output:", "q_apis": "min max values min max min max difference between", "io": "df['dif'] = 0 66 1 66 2 66 <s> df['dif'] = 0 51 1 66 2 52 ", "apis": "array", "code": ["np.nanmax(df[poacols],1)-np.nanmin(df[poacols],1)\nOut[81]: array([51., 66., 52.])\n"], "link": "https://stackoverflow.com/questions/63387459/python-dataframe-rowwise-min-and-max-with-nan-values"}
{"id": 89, "q": "Convert a Pandas DataFrame to a dictionary", "d": "I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be and the elements of other columns in same row be . DataFrame: Output should be like this: Dictionary:", "q_apis": "DataFrame DataFrame columns DataFrame first columns DataFrame", "io": " ID A B C 0 p 1 3 2 1 q 4 3 2 2 r 4 0 9 <s> {'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]} ", "apis": "to_dict columns index to_dict to_dict index", "code": [">>> df.to_dict('split')\n{'columns': ['a', 'b'],\n 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],\n 'index': [0, 1, 2]}\n", ">>> df.to_dict('records')\n[{'a': 'red', 'b': 0.5}, \n {'a': 'yellow', 'b': 0.25}, \n {'a': 'blue', 'b': 0.125}]\n", ">>> df.to_dict('index')\n{0: {'a': 'red', 'b': 0.5},\n 1: {'a': 'yellow', 'b': 0.25},\n 2: {'a': 'blue', 'b': 0.125}}\n"], "link": "https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary"}
{"id": 297, "q": "Filter Series/DataFrame by another DataFrame", "d": "Let's suppose I have a Series (or DataFrame) , for example list of all Universities and Colleges in the USA: And another Series (od DataFrame) , for example list of all cities in the USA: And my desired output (bascially an intersection of and ): The thing is: I'd like to create a Series that consists of cities but only these, that have a university/college. My very first thought was to remove \"University\" or \"College\" parts from the , but it turns out that it is not enough, as in case of . Then I thought of leaving only the first word, but that excludes . Finally, I got a series of all the cities and now I'm trying to use it as a filter (something similiar to or ), so if a string (Uni name) contains any of the elements of (city name), then return only the city name. My question is: how to do it in a neat way?", "q_apis": "Series DataFrame DataFrame Series DataFrame all Series DataFrame all intersection Series first first all now filter name contains any name name", "io": " City 0 Searcy 1 Angwin 2 New York 3 Ann Arbor <s> Uni City 0 Searcy 1 Angwin 2 Fairbanks 3 Ann Arbor ", "apis": "Series contains any name", "code": ["pd.Series([i for i in s2 if s1.str.contains(i).any()], name='Uni City')\n"], "link": "https://stackoverflow.com/questions/61678886/filter-series-dataframe-by-another-dataframe"}
{"id": 620, "q": "Fill all values in a group with the first non-null value in that group", "d": "The following is the pandas dataframe I have: If we look into the data, cluster 1 has Value 'A' for one row and remain all are NA values. I want to fill 'A' value for all the rows of cluster 1. Similarly for all the clusters. Based on one of the values of the cluster, I want to fill the remaining rows of the cluster. The output should be like, I am new to python and not sure how to proceed with this. Can anybody help with this ?", "q_apis": "all values first value all values value all all values", "io": "cluster Value 1 A 1 NaN 1 NaN 1 NaN 1 NaN 2 NaN 2 NaN 2 B 2 NaN 3 NaN 3 NaN 3 C 3 NaN 4 NaN 4 S 4 NaN 5 NaN 5 A 5 NaN 5 NaN <s> cluster Value 1 A 1 A 1 A 1 A 1 A 2 B 2 B 2 B 2 B 3 C 3 C 3 C 3 C 4 S 4 S 4 S 5 A 5 A 5 A 5 A ", "apis": "dropna set_index to_dict map map unique values dropna set_index to_dict iterrows at iterrows at at at", "code": ["nan_map = df.dropna().set_index('cluster').to_dict()['Value']\ndf['Value'] = df['cluster'].map(nan_map)\n\nprint(df)\n", "# Create a dict to map clusters to unique values\nnan_map = df.dropna().set_index('cluster').to_dict()['Value']\n# nan_map: {1: 'A', 2: 'B', 3: 'C', 4: 'S', 5: 'A'}\n\n# Apply\nfor i, row in df.iterrows():\n    df.at[i,'Value'] = nan_map[row['cluster']]\n\nprint(df)\n", "# Apply\nfor i, row in df.iterrows():\n    if isinstance(df.at[i,'Value'], float) and math.isnan(df.at[i,'Value']):\n        df.at[i,'Value'] = nan_map[row['cluster']]\n"], "link": "https://stackoverflow.com/questions/50957993/fill-all-values-in-a-group-with-the-first-non-null-value-in-that-group"}
{"id": 282, "q": "Python Pandas Iterate over columns and also update columns based on apply conditions", "d": "I am trying to update dataframe columns based on consecutive columns values. If columns say col1 and col2 has >0 and <0 values, then same columns should get updated as col2=col1 + col2 and col1=0 and also counter +1 (gives how many fixes has been done throughout the column). Dataframe look like: Required Dataframe after applying logic: I tried: It failed I also looked into but I couldn't figure out the way. DDL to generate DataFrame: Thanks!", "q_apis": "columns update columns apply update columns columns values columns values columns get DataFrame", "io": "id col0 col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 1 0 5 -5 5 -5 0 0 1 4 3 -3 2 0 0 0 0 0 0 0 4 -4 0 0 3 0 0 1 2 3 0 0 0 5 6 0 <s> id col0 col1 col2 col3 col4 col6 col7 col8 col9 col10 fix 1 0 0 0 0 0 0 0 1 4 0 0 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0 0 1 2 3 0 0 0 5 6 0 9 0 ", "apis": "gt shift lt mask shift fillna shift mask assign sum", "code": ["c = df.gt(0) & df.shift(-1,axis=1).lt(0)\n\nout = (df.mask(c.shift(axis=1).fillna(False),df+df.shift(axis=1))\n      .mask(c,0).assign(Fix=c.sum(1)))\n\nprint(out)\n"], "link": "https://stackoverflow.com/questions/62215453/python-pandas-iterate-over-columns-and-also-update-columns-based-on-apply-condit"}
{"id": 545, "q": "Python Setting Values Without Loop", "d": "I have a time series dataframe where there is 1 or 0 in it (true/false). I wrote a function that loops through all rows with values 1 in them. Given user defined integer parameter called , I will set values 1 to n rows forward from the initial row. For example, in the dataframe below I will be loop to row . If , then I will set both and to 1 too.: The resulting will then is The problem I have is this is being run 10s of thousands of times and my current solution where I am looping over rows where there are ones and subsetting is way too slow. I was wondering if there are any solutions to the above problem that is really fast. Here is my (slow) solution, is the initial signal dataframe:", "q_apis": "time where all values values where where any", "io": "2016-08-03 0 2016-08-04 0 2016-08-05 1 2016-08-08 0 2016-08-09 0 2016-08-10 0 <s> 2016-08-03 0 2016-08-04 0 2016-08-05 1 2016-08-08 1 2016-08-09 1 2016-08-10 0 ", "apis": "where ne shift ffill fillna Series size copy diff index index get_loc mean std where ne shift ffill fillna mean std", "code": ["n_hold = 2\ns = x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n", "np.random.seed(123)\nx = pd.Series(np.random.choice([0,1], p=(.8,.2), size=1000))\nx1 = x.copy()\n#print (x)\n\n\ndef orig(x):\n    n_hold = 2\n    entry_sig_diff = x.diff()\n    entry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index\n    final_signal = x * 0\n    for i in range(0, len(entry_sig_dt)):\n        row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])\n\n        if (row_idx + n_hold) >= len(x):\n            break\n\n        final_signal[row_idx:(row_idx + n_hold + 1)] = 1\n    return final_signal\n\n#print (orig(x))\n", "%timeit (orig(x))\n24.8 ms \u00b1 653 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit x.where(x.ne(x.shift()) & (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')\n1.36 ms \u00b1 12.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/53885404/python-setting-values-without-loop"}
{"id": 219, "q": "How to re order this DataFrame in Python wthout hard coding column values?", "d": "I have a df ('COL3 SUM' is the full name with a space): How can I re order this df so that 'COL3 SUM' always comes at the end of the dataframe like so without re ordering any of the rest of the df?", "q_apis": "DataFrame values name at any", "io": "COL1 COL2 COL3 SUM COL4 COL5 1 2 3 4 5 <s> COL1 COL2 COL4 COL5 COL3 SUM 1 2 4 5 3 ", "apis": "columns columns", "code": ["new_cols = [col for col in df.columns if \"SUM\" not in col]\nmoved_cols = list(set(df.columns) - set(new_cols)) \nnew_cols.extend(moved_cols)\ndf = df[new_cols]\n"], "link": "https://stackoverflow.com/questions/63712894/how-to-re-order-this-dataframe-in-python-wthout-hard-coding-column-values"}
{"id": 374, "q": "pandas group by the column values with all values less than certain numbers and assign the group numbers as new columns", "d": "I have a data frame like this, Now I want to create another column col3 with grouping all the col2 values which are under below 5 and keep col3 values as 1 to number of groups, so the final data frame would look like, I could do this comparing the the prev values with the current values and store into a list and make it the col3. But the execution time will be huge in this case, so looking for some shortcuts/pythonic way to do it most efficiently.", "q_apis": "values all values assign columns all values values groups values values time", "io": "df col1 col2 A 2 B 3 C 1 D 4 E 6 F 1 G 2 H 8 I 1 J 10 <s> col1 col2 col3 A 2 1 B 3 1 C 1 1 D 4 1 E 6 2 F 1 2 G 2 2 H 8 3 I 1 3 J 10 4 ", "apis": "gt cumsum loc", "code": ["N = 5\ndf['col3'] = df['col2'].gt(N).cumsum() + int(df.loc[0, 'col2'] < 5)\n"], "link": "https://stackoverflow.com/questions/59589861/pandas-group-by-the-column-values-with-all-values-less-than-certain-numbers-and"}
{"id": 253, "q": "Trying to append the sum of an existing dataframe into a new excel sheet", "d": "So I have been trying to append the of an existing dataframe into a new dataframe for a new the below example: I want this to be appended to a new excel as: Following is the code where I am merging the files in a particular location I need help with the summing part.", "q_apis": "append sum append where", "io": "A B C 10 10 10 10 10 10 10 10 10 <s> A B C 30 30 30 ", "apis": "sum dtype sum to_frame", "code": ["df.sum()\nA    30\nB    30\nC    30\ndtype: int64\n", "df.sum().to_frame()\n\n0 \nA 30\nB 30\nC 30\n \n"], "link": "https://stackoverflow.com/questions/62935487/trying-to-append-the-sum-of-an-existing-dataframe-into-a-new-excel-sheet"}
{"id": 145, "q": "Python: How do I merge rows that shares the same name of &quot;Ocode&quot; or &quot;Ccode in dataframe", "d": "I'm thinking of using pandas to merge several repetitive rows of \"Ocode\" and Ccode\". Ideally I want only one \"Ocode\" or \"Ccode\" per row. When there are repetitive dates under c## (I.E. c21), only the latest date is kept. Separate dates under different column with the same \"Ocode\"/\"Ccode\" should also be merged. (For reference purpose: O and C code correspondingly represents Organization Code and Corporation code.) This is the heading of the dataframe. Which should become ----> Attempt: and perform the merge one by one. However, this method tends to delete information under other column when dealing with one of the c##(I.E. c21) column df.to_excel(ic + '.xlsx', index=False)", "q_apis": "merge name merge date merge delete to_excel index", "io": "Num Ocode Ccode c21 c57 c58 c59 c70 c71 c74 c75 0 BK0001000 NaN NaN NaN NaN NaN NaN NaN NaN 2021-02-09 1 CU0030000 NaN NaN NaN NaN NaN NaN 2021-12-31 NaN NaN 2 CU0030000 NaN NaN NaN NaN NaN NaN 2021-12-31 NaN NaN 3 CU0048000 NaN NaN NaN 2018-06-19 NaN NaN NaN NaN NaN 4 CU0056000 NaN NaN NaN NaN NaN NaN NaN 2020-06-04 NaN ... ... ... ... ... ... ... ... ... ... ... 2384 NaN CU0280002 2017-12-31 NaN NaN NaN NaN NaN NaN NaN 2385 NaN CU0280002 2016-12-31 NaN NaN NaN NaN NaN NaN NaN 2386 NaN CU0280002 NaN NaN NaN NaN NaN NaN 2017-12-31 NaN 2387 NaN CU0659001 NaN NaN NaN NaN NaN NaN 2022-05-31 NaN <s> Num Ocode Ccode c21 c57 c58 c59 c70 c71 c74 c75 0 BK0001000 NaN NaN NaN NaN NaN NaN NaN NaN 2021-02-09 1 CU0030000 NaN NaN NaN NaN NaN NaN 2021-12-31 NaN NaN 3 CU0048000 NaN NaN NaN 2018-06-19 NaN NaN NaN NaN NaN 4 CU0056000 NaN NaN NaN NaN NaN NaN NaN 2020-06-04 NaN ... ... ... ... ... ... ... ... ... ... ... 2384 NaN CU0280002 2017-12-31 NaN NaN NaN NaN NaN 2017-12-31 NaN 2387 NaN CU0659001 NaN NaN NaN NaN NaN NaN 2022-05-31 NaN ", "apis": "assign fillna fillna groupby last reset_index replace groupby last reset_index loc notna notna assign fillna drop groupby last reset_index assign fillna drop groupby last reset_index concat replace", "code": ["df = (df.assign(Ocode = df['Ocode'].fillna('nan'),Ccode = df['Ccode'].fillna('nan'))\n        .groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index()\n        .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n", "df = (df.groupby(['Ocode','Ccode'])\n        .last()\n        .reset_index())\n", "df.loc[df['Ocode'].notna() & df['Ocode'].notna(), 'Ccode'] = np.nan\n", "df1 = (df.assign(Ocode = df['Ocode'].fillna('nan'))\n         .drop('Ccode', axis=1)\n         .groupby('Ocode')\n         .last()\n         .reset_index())\n\ndf2 = (df.assign(Ccode = df['Ccode'].fillna('nan'))\n         .drop('Ocode', axis=1)\n         .groupby('Ccode')\n         .last()\n         .reset_index())\n\nboth = (pd.concat([df1, df2], sort=False)\n          .replace({'Ocode': {'nan':np.nan}, 'Ccode':{'nan':np.nan}}))\n"], "link": "https://stackoverflow.com/questions/65194434/python-how-do-i-merge-rows-that-shares-the-same-name-of-ocode-or-ccode-in-da"}
{"id": 27, "q": "Dictionary making for a transportation model from a Dataframe", "d": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.", "q_apis": "name any value", "io": " d = {c1:80, c2:270, c3:250, c4:160, c5:180} # customer demand M = {p1:500, p2:500, p3:500} # factory capacity I = [c1,c2,c3,c4,c5] # Customers J = [p1,p2,p3] # Factories cost = {(p1,c1):4, (p1,c2):5, (p1,c3):6, (p1,c4):8, (p1,c5):10, ...... } <s> {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan} ", "apis": "set_index loc loc T squeeze dropna iteritems iteritems drop columns columns index drop columns stack iteritems", "code": ["df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"], "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe"}
{"id": 96, "q": "Retrieve rows with highest value with condition", "d": "I have a dataframe that looks like this: I want to write a function that takes the rows with same id and label A and filter it based on the highest width so the after applying the function the dataframe would be:", "q_apis": "value filter", "io": "| Id | Label | Width | |----|-------| ------| | 0 | A | 5 | | 0 | A | 3 | | 0 | B | 4 | | 1 | A | 7 | | 1 | A | 9 | <s> | Id | Label | Width | |----|-------| ------| | 0 | A | 5 | | 0 | B | 4 | | 1 | A | 9 | ", "apis": "eq loc groupby idxmax concat sort_index dtype bool", "code": ["m = df['Label'].eq('A')\ndf_a = df.loc[df[m].groupby(['Id', 'Label'])['Width'].idxmax()]\n\ndf_out = pd.concat([df[~m], df_a]).sort_index()\n", ">>> m\n\n0     True\n1     True\n2    False\n3     True\n4     True\nName: Label, dtype: bool\n"], "link": "https://stackoverflow.com/questions/66194784/retrieve-rows-with-highest-value-with-condition"}
{"id": 492, "q": "Add values to existing rows -DataFrame", "d": "I'm appending some weather data (from json- dict) - in Japanese to DataFrame. I would like to have something like this But I have this How Could I change the Codes to make it like that? Here is the code", "q_apis": "values DataFrame DataFrame", "io": " \u5929\u6c17 \u98a8 0 \u72b6\u614b: Clouds \u98a8\u901f: 2.1m 1 NaN \u5411\u304d: 230 <s> \u5929\u6c17 \u98a8 0 \u72b6\u614b: Clouds NaN 1 NaN \u98a8\u901f: 2.1m 2 NaN \u5411\u304d: 230 ", "apis": "DataFrame columns append append", "code": ["df = pd.DataFrame(columns=['\u5929\u6c17','\u98a8'])\ndf = df.append({'\u5929\u6c17': weather_status, '\u98a8': wind_speed}, ignore_index=True)\ndf = df.append({'\u98a8': wind_deg}, ignore_index=True)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/55803354/add-values-to-existing-rows-dataframe"}
{"id": 310, "q": "Error: None of [Index([&#39;...&#39;], dtype=&#39;object&#39;)] are in the [index]", "d": "I am trying to delete a grouped set of rows in pandas according to the following condition: If a group (grouped by col1) has more than 2 values 'c' in col2, then remove the whole group. What I have looks like this And I am trying to get here: Typically I do this for other very similar dataframes (and it works): But for this one is not working and I receive this error: Does someone have an idea on how I could do this? Thanks!", "q_apis": "Index dtype index delete values get", "io": " col1 col2 0 A 10:10 1 A 20:05 2 A c 3 A 00:10 4 B 04:15 2 B c 3 B c 4 B 13:40 <s> col1 col2 0 A 10:10 1 A 20:05 2 A c 3 A 00:10 ", "apis": "eq dtype bool eq groupby transform sum lt dtype bool", "code": ["print (df['col2'].eq('c'))\n0    False\n1    False\n2     True\n3    False\n4    False\n2     True\n3     True\n4    False\nName: col2, dtype: bool\n", "print (df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2))\n0     True\n1     True\n2     True\n3     True\n4    False\n2    False\n3    False\n4    False\nName: col2, dtype: bool\n"], "link": "https://stackoverflow.com/questions/61170036/error-none-of-index-dtype-object-are-in-the-index"}
{"id": 384, "q": "How to Concat 2 columns in single column with column value check", "d": "I want to concat two column from data frame where Column1 not equals to ANY: DataFrame : as a result I want dataframe as follows ANY is variable, could represent Null, EmptyString, String, Number. Thanks.", "q_apis": "columns value concat where equals DataFrame", "io": " COLUMN1 | COLUMN2 0 A | FOO 1 B | BAR 2 ANY | FOO 3 ANY | BAR 4 C | FOO <s> COLUMN1 | COLUMN2 0 A | FOO_A 1 B | BAR_B 2 ANY | FOO 3 ANY | BAR 4 C | FOO_C ", "apis": "apply", "code": ["df['COLUMN2']=df.apply(lambda row:row['COLUMN2']+'_'+row['COLUMN1'] if row['COLUMN1']!='ANY' else row['COLUMN2'],axis=1)\n"], "link": "https://stackoverflow.com/questions/59284049/how-to-concat-2-columns-in-single-column-with-column-value-check"}
{"id": 505, "q": "Top 3 Values Per Row in Pandas", "d": "I have a large Pandas dataframe that is in the vein of: and I would like to generate output that looks like this, taking the column names of the 3 highest values for each row: I have tried using df.idmax(axis=1) which returns the 1st maximum column name but am unsure how to compute the other two? Any help on this would be truly appreciated, thanks!", "q_apis": "names values name", "io": "| ID | Var1 | Var2 | Var3 | Var4 | Var5 | |----|------|------|------|------|------| | 1 | 1 | 2 | 3 | 4 | 5 | | 2 | 10 | 9 | 8 | 7 | 6 | | 3 | 25 | 37 | 41 | 24 | 21 | | 4 | 102 | 11 | 72 | 56 | 151 | ... <s> | ID | 1st Max | 2nd Max | 3rd Max | |----|---------|---------|---------| | 1 | Var5 | Var4 | Var3 | | 2 | Var1 | Var2 | Var3 | | 3 | Var3 | Var2 | Var1 | | 4 | Var5 | Var1 | Var3 | ... ", "apis": "set_index apply Series nlargest index index reset_index", "code": ["c = ['1st Max','2nd Max','3rd Max']\ndf = (df.set_index('ID')\n        .apply(lambda x: pd.Series(x.nlargest(3).index, index=c), axis=1)\n        .reset_index())\n"], "link": "https://stackoverflow.com/questions/54923349/top-3-values-per-row-in-pandas"}
{"id": 260, "q": "Find out intersection of 2 pandas DataFrame according to 2 columns", "d": "I would to find out intersection of 2 pandas DataFrame according to 2 columns 'x' and 'y' and combine them into 1 DataFrame. The data are: The expected output is something like (can ignore index): Thank you very much!", "q_apis": "intersection DataFrame columns intersection DataFrame columns combine DataFrame index", "io": "df[1]: x y id fa 0 4 5 9283222 3.1 1 4 5 9283222 3.1 2 10 12 9224221 3.2 3 4 5 9284332 1.2 4 6 1 51249 11.2 df[2]: x y id fa 0 4 5 19283222 1.1 1 9 3 39224221 5.2 2 10 12 29284332 6.2 3 6 1 51242 5.2 4 6 2 51241 9.2 5 1 1 51241 9.2 <s> x y id fa 0 4 5 9283222 3.1 1 4 5 9283222 3.1 2 10 12 9224221 3.2 3 4 5 9284332 1.2 4 6 1 51249 11.2 0 4 5 19283222 1.1 2 10 12 29284332 6.2 3 6 1 51242 5.2 ", "apis": "intersection merge drop_duplicates concat merge intersection merge intersection", "code": ["intersection = df1[['x', 'y']].merge(df2[['x', 'y']]).drop_duplicates()\npd.concat([df1.merge(intersection), df2.merge(intersection)])\n"], "link": "https://stackoverflow.com/questions/41529340/find-out-intersection-of-2-pandas-dataframe-according-to-2-columns"}
{"id": 580, "q": "How to select, and replace specific values with NaN in pandas dataframe. How to remove a column from each level 1 multiindex", "d": "I have a csv file, which I read into a pandas frame: This is the view of the csv file (print(csv_file)): The resulting dataframe is MultiIndexed with two levels: print(df.column()): If a coordinate has a lower likelihood, I wan't the coordinates to be replaced by NaN. The new dataframe shan't have a likelihood column(s). An example with the first row from 'nose': After function should be like this: Note that outstanding values remain unchanged during this process!", "q_apis": "select replace values view levels first values", "io": "coords x y likelihood 0 197.486369 4.545954 3.890000e-07 <s> coords x y 0 NaN NaN ", "apis": "columns levels loc", "code": ["for col in df.columns.levels[0]:\n    df.loc[df[(col, 'likelihood')] < threshold, [(col, 'x'), (col, 'y')]] = np.nan\n"], "link": "https://stackoverflow.com/questions/52506702/how-to-select-and-replace-specific-values-with-nan-in-pandas-dataframe-how-to"}
{"id": 389, "q": "Python: Sort subsection of columns", "d": "Suppose we have the following dataframe: Which can be computed as follows I was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. I would want the end result to look as", "q_apis": "columns last columns", "io": "Label1 2016-03-31 2016-05-31 2016-04-30 0 A A1 1 6 1 B B1 3 4 2 C C2 5 7 3 D D1 7 2 4 E E4 9 4 5 F F1 11 6 <s> Label1 2016-03-31 2016-04-30 2016-05-31 0 A A1 6 1 1 B B1 4 3 2 C C2 7 5 3 D D1 2 7 4 E E4 4 9 5 F F1 6 11 ", "apis": "columns columns sort_values", "code": ["new_cols = list(df.columns[:-3]) + list(df.columns[-3:].sort_values())\n\ndf[new_cols]\n"], "link": "https://stackoverflow.com/questions/59250863/python-sort-subsection-of-columns"}
{"id": 205, "q": "Add character to column based on ascending order of another column if condition met pandas", "d": "Stuck on a data problem in pandas. See data below: The rules are: Only one Cost for each unique (Product, Level) combination. If multiple Cost for each unique (Product, Level) combination, add a letter to the Level value (L1 A, L1 B, etc) based on the Cost value (L1 A being the smallest Cost). If (Product, Level) combination has a unique Cost then do nothing. Desired output:", "q_apis": "unique unique add value value unique", "io": "| Product | Level | Cost | --------- ------- ------ | Prod_A | L1 | 100 | | Prod_A | L1 | 100 | | Prod_A | L1 | 200 | | Prod_A | L2 | 100 | | Prod_A | L3 | 100 | | Prod_B | L1 | 150 | | Prod_B | L1 | 150 | | Prod_B | L2 | 200 | | Prod_B | L2 | 300 | | Prod_C | L3 | 100 | <s> | Product | Level | Cost | --------- ------- ------ | Prod_A | L1 A | 100 | | Prod_A | L1 A | 100 | | Prod_A | L1 B | 200 | | Prod_A | L2 | 100 | | Prod_A | L3 | 100 | | Prod_B | L1 | 150 | | Prod_B | L1 | 150 | | Prod_B | L2 A | 200 | | Prod_B | L2 B | 300 | | Prod_C | L3 | 100 | ", "apis": "groupby transform factorize nunique map fillna", "code": ["charlist='ABCDEFG'\ndd = {k:' '+v for k, v in enumerate(charlist)}\ndf['Level'] += df.groupby(['Product', 'Level'])['Cost']\\\n                 .transform(lambda x: x.factorize()[0] if x.nunique()>1 else -1)\\\n                 .map(dd).fillna('')\n"], "link": "https://stackoverflow.com/questions/64165393/add-character-to-column-based-on-ascending-order-of-another-column-if-condition"}
{"id": 532, "q": "Dropping duplicate rows but keeping certain values Pandas", "d": "I have 2 similar dataframes that I concatenated that have a lot of repeated values because they are basically the same data set but for different years. The problem is that one of the sets has some values missing whereas the other sometimes has these values. For example: I want to drop duplicates on the since some repetitions don't have years. However, I'm left with the data that has no and I'd like to keep the data with these values: How do I keep these values rather than the blanks?", "q_apis": "values values values values drop left values values", "io": "Name Unit Year Level Nik 1 2000 12 Nik 1 12 John 2 2001 11 John 2 2001 11 Stacy 1 8 Stacy 1 1999 8 . . <s> Name Unit Year Level Nik 1 2000 12 John 2 2001 11 Stacy 1 1999 8 . . ", "apis": "sort_values drop_duplicates", "code": ["df = df.sort_values(subset + ['Year']).drop_duplicates(subset)\n"], "link": "https://stackoverflow.com/questions/54219106/dropping-duplicate-rows-but-keeping-certain-values-pandas"}
{"id": 82, "q": "specify number of spaces between pandas DataFrame columns when printing", "d": "When you print a pandas DataFrame, which calls DataFrame.to_string, it normally inserts a minimum of 2 spaces between the columns. For example, this code outputs which has a minimum of 2 spaces between each column. I am copying DataFarames printed on the console and pasting it into documents, and I have received feedback that it is hard to read: people would like more spaces between the columns. Is there a standard way to do that? I see no option in either DataFrame.to_string or pandas.set_option. I have done a web search, and not found an answer. This question asks how to remove those 2 spaces, while this question asks why sometimes only 1 space is between columns instead of 2 (I also have seen this bug, hope someone answers that question). My hack solution is to define a function that converts a DataFrame's columns to type str, and then prepends each element with a string of the specified number of spaces. This code (added to the code above) outputs which is the desired effect. But I think that pandas surely must have some builtin simple standard way to do this. Did I miss how? Also, the solution needs to handle a DataFrame whose columns are a MultiIndex. To continue the code example, consider this modification:", "q_apis": "between DataFrame columns DataFrame DataFrame to_string between columns between between columns DataFrame to_string between columns DataFrame columns DataFrame columns MultiIndex", "io": " c1 c2 a3235235235 0 a 11 1 1 bb 22 2 2 ccc 33 3 3 dddd 44 4 4 eeeeee 55 5 <s> c1 c2 a3235235235 0 a 11 1 1 bb 22 2 2 ccc 33 3 3 dddd 44 4 4 eeeeee 55 5 ", "apis": "length astype agg max pad between iteritems max max pad", "code": ["from functools import partial\n\n# Formatting string \ndef get_fmt_str(x, fill):\n    return '{message: >{fill}}'.format(message=x, fill=fill)\n\n# Max character length per column\ns = df.astype(str).agg(lambda x: x.str.len()).max() \n\npad = 6  # How many spaces between \nfmts = {}\nfor idx, c_len in s.iteritems():\n    # Deal with MultIndex tuples or simple string labels. \n    if isinstance(idx, tuple):\n        lab_len = max([len(str(x)) for x in idx])\n    else:\n        lab_len = len(str(idx))\n\n    fill = max(lab_len, c_len) + pad - 1\n    fmts[idx] = partial(get_fmt_str, fill=fill)\n"], "link": "https://stackoverflow.com/questions/66375262/specify-number-of-spaces-between-pandas-dataframe-columns-when-printing"}
{"id": 338, "q": "Pandas Drop Columns with Only One Unique Value for a Group", "d": "I have a df that consists of duplicate : I want to remove columns where all values for an are the same. This could mean that all values in the column are the same () or all the values are the same for each (). Desired result: I used this to identify the counts of unique values in each column: If I drop all columns where this count is equal to 1, this would take care of the scenario. However, how should I take care of the scenario? The df has already been grouped by id to find duplicate, but do I need to use again? As a \"bonus\", I wouldn't mind knowing how to identify where even one has text that is all the same (i.e. ). I'm essentially trying to find which columns cause there to be duplicates. Thank you for any and all insight you all might have!", "q_apis": "columns where all values mean all values all values unique values drop all columns where count take take where all columns any all all", "io": "id text text2 text3 1 hello hello hello 1 hello hello hello 2 hello hello goodbye 2 goodbye hello goodbye 2 hello hello goodbye <s> id text 1 hello 1 hello 2 hello 2 goodbye 2 hello ", "apis": "nunique groupby agg nunique all", "code": ["In [1227]: u = df.nunique()\n", "In [1228]: gu = gu = (df.groupby('id').agg('nunique') == 1).all()\n"], "link": "https://stackoverflow.com/questions/45336415/pandas-drop-columns-with-only-one-unique-value-for-a-group"}
{"id": 547, "q": "How to create a dataframe from numpy arrays?", "d": "I am trying to create a matrix / DataFrame with the numbers stored in 2 variables and I would like them to look like this: I would like it to be in a so I can work with it with the library. Thanks in advance", "q_apis": "DataFrame", "io": "x = np.linspace(0,50) y = np.exp(x) <s> x | y ___________________ 0 | 1.0... 1 | 2.77... 2 | 7.6... ... | ... 50 | 5.18e+21... ", "apis": "xs xs xs xs", "code": [">>> xs = np.arange(51)                                                                                                 \n>>> ys = np.exp(xs) \n", ">>> xs = np.arange(51)                                                                                                 \n>>> ys = np.exp(xs)\n"], "link": "https://stackoverflow.com/questions/53820131/how-to-create-a-dataframe-from-numpy-arrays"}
{"id": 63, "q": "Python: How to pass Dataframe Columns as parameters to a function?", "d": "I have a dataframe with 2 columns of text embeddings namely and . I want to create a third column in named which should contain the cosine_similarity between every row of and . But when I try to implement this using the following code I get a . How to fix it? Dataframe Code to Calculate Cosine Similarity Error Required Dataframe", "q_apis": "columns between get", "io": " embedding_1 | embedding_2 [[-0.28876397, -0.6367827, ...]] | [[-0.49163356, -0.4877703,...]] [[-0.28876397, -0.6367827, ...]] | [[-0.06686627, -0.75147504...]] [[-0.28876397, -0.6367827, ...]] | [[-0.42776933, -0.88310856,...]] [[-0.28876397, -0.6367827, ...]] | [[-0.6520882, -1.049325,...]] [[-0.28876397, -0.6367827, ...]] | [[-1.4216679, -0.8930428,...]] <s> embedding_1 | embedding_2 | distances [[-0.28876397, -0.6367827, ...]] | [[-0.49163356, -0.4877703,...]] | 0.427 [[-0.28876397, -0.6367827, ...]] | [[-0.06686627, -0.75147504...]] | 0.673 [[-0.28876397, -0.6367827, ...]] | [[-0.42776933, -0.88310856,...]] | 0.882 [[-0.28876397, -0.6367827, ...]] | [[-0.6520882, -1.049325,...]] | 0.665 [[-0.28876397, -0.6367827, ...]] | [[-1.4216679, -0.8930428,...]] | 0.312 ", "apis": "apply apply", "code": ["def cal_cosine_similarity(row):\n    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])\n\ndf['distances'] = df.apply(cal_cosine_similarity, axis=1)\n", "df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)\n"], "link": "https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function"}
{"id": 272, "q": "Pandas: multiply column value by sum of group", "d": "I have a dataframe which looks like this: I want to add a column 'c' which multiplies the value of 'b' by the sum of the group it belongs to in column 'a'. So, first row should be 0.15 * 0.5 (sum of group A) = 0.075. This would be the excel formula for column 'c' =B1*SUMIF($A$1:$A$9,A1,$B$1:$B$9) Resulting dataframe should look like this:", "q_apis": "value sum add value sum first sum", "io": " a b 0 A 0.15 1 A 0.25 2 A 0.10 3 B 0.20 4 B 0.10 5 B 0.25 6 B 0.60 7 C 0.50 8 C 0.70 <s> a b c 0 A 0.15 0.075 1 A 0.25 0.125 2 A 0.10 0.05 3 B 0.20 0.23 4 B 0.10 0.115 5 B 0.25 0.2875 6 B 0.60 0.69 7 C 0.50 0.6 8 C 0.70 0.84 ", "apis": "groupby transform sum groupby transform sum", "code": ["df['b'] * df.groupby('a')['b'].transform('sum')\n#df['c'] = df['b'] * df.groupby('a')['b'].transform('sum')\n"], "link": "https://stackoverflow.com/questions/62611339/pandas-multiply-column-value-by-sum-of-group"}
{"id": 349, "q": "Efficiently replace values from a column to another column Pandas DataFrame", "d": "I have a Pandas DataFrame like this: I want to replace the values with the values in the second column () only if values are equal to 0, and after (for the zero values remaining), do it again but with the third column (). The Desired Result is the next one: I did it using the function, but it seems too slow.. I think must be a faster way to accomplish that. is there a faster way to do that?, using some other function instead of the function?", "q_apis": "replace values DataFrame DataFrame replace values values second values values", "io": " col1 col2 col3 1 0.2 0.3 0.3 2 0.2 0.3 0.3 3 0 0.4 0.4 4 0 0 0.3 5 0 0 0 6 0.1 0.4 0.4 <s> col1 col2 col3 1 0.2 0.3 0.3 2 0.2 0.3 0.3 3 0.4 0.4 0.4 4 0.3 0 0.3 5 0 0 0 6 0.1 0.4 0.4 ", "apis": "where where where where", "code": ["df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\ndf['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n", "df['col1'] = np.where(df['col1'] == 0, \n                      np.where(df['col2'] == 0, df['col3'], df['col2']),\n                      df['col1'])\n"], "link": "https://stackoverflow.com/questions/39903090/efficiently-replace-values-from-a-column-to-another-column-pandas-dataframe"}
{"id": 112, "q": "pandas.MultiIndex: assign all elements in first level", "d": "I have a dataframe with a multiindex, as per the following example: This gives me a dataframe like this: I have another 2-dimensional dataframe, as follows: Resulting in the following: I would like to assign to the cell, across all dates, and the cell, across all dates etc. Something akin to the following: Of course this doesn't work, because I am attempting to set a value on a copy of a slice : A value is trying to be set on a copy of a slice from a I tried several variations: How can I select index level 1 (eg: row 'a'), column 'a', across all index level 0 - and be able to set the values?", "q_apis": "MultiIndex assign all first assign all all value copy value copy select index all index values", "io": " a b c 2020-01-01 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN 2020-01-02 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN 2020-01-03 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN 2020-01-04 a NaN NaN NaN b NaN NaN NaN c NaN NaN NaN <s> a b c 2020-01-01 0.540867 0.426181 0.220182 2020-01-02 0.864340 0.432873 0.487878 2020-01-03 0.017099 0.181050 0.373139 2020-01-04 0.764557 0.097839 0.499788 ", "apis": "to_numpy DataFrame shape index index columns columns", "code": ["a = df.to_numpy()\n\npanel = pd.DataFrame((a[...,None] * a[:,None,:]).reshape(-1, df.shape[1]), \n                     index=panel.index, columns=panel.columns)\n"], "link": "https://stackoverflow.com/questions/65905694/pandas-multiindex-assign-all-elements-in-first-level"}
{"id": 250, "q": "Slicing each dataframe row into 3 windows with different slicing ranges", "d": "I want to slice each row of my dataframe into 3 windows with slice indices that are stored in another dataframe and change for each row of the dataframe. Afterwards i want to return a single dataframe containing the windows in form of a MultiIndex. The rows in each windows that are shorter than the longest row in the window should be filled with NaN values. Since my actual dataframe has around 100.000 rows and 600 columns, i am concerned about an efficient solution. Consider the following example: This is my dataframe which i want to slice into 3 windows And the second dataframe containing my slicing indices having the same count of rows as : I've tried slicing the windows, like so: Which gives me the following error: My expected output is something like this: Is there an efficient solution for my problem without iterating over each row of my dataframe?", "q_apis": "indices MultiIndex values columns second indices count", "io": ">>> df 0 1 2 3 4 5 6 7 0 0 1 2 3 4 5 6 7 1 8 9 10 11 12 13 14 15 2 16 17 18 19 20 21 22 23 <s> >>> df_slice 0 1 0 3 5 1 2 6 2 4 7 ", "apis": "reset_index melt index merge index to_numeric loc loc loc shift values left groupby index transform min groupby transform min groups index columns pivot_table index index columns values value columns MultiIndex from_product columns pivot_table index index columns values value columns MultiIndex from_product columns pivot_table index index columns values value columns MultiIndex from_product columns concat columns MultiIndex from_tuples columns", "code": ["    t = df.reset_index().melt(id_vars=\"index\")\n    t = pd.merge(t, df_slice, left_on=\"index\", right_index=True)\n    t.variable = pd.to_numeric(t.variable)\n    \n    t.loc[t.variable < t.c_0,\"group\"] = \"A\"\n    t.loc[(t.variable >= t.c_0) & (t.variable < t.c_1), \"group\"] = \"B\"\n    t.loc[t.variable >= t.c_1, \"group\"] = \"C\"\n\n    # shift relevant values to the left\n    shift_val = t.groupby([\"group\", \"index\"]).variable.transform(\"min\") - t.groupby([\"group\"]).variable.transform(\"min\")\n    t.variable = t.variable - shift_val\n    \n    # extract a, b, and c groups, and create a multi-level index for their\n    # columns\n    df_a = pd.pivot_table(t[t.group == \"A\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_a.columns = pd.MultiIndex.from_product([[\"a\"], df_a.columns])\n    \n    df_b = pd.pivot_table(t[t.group == \"B\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_b.columns = pd.MultiIndex.from_product([[\"b\"], df_b.columns])\n    \n    df_c = pd.pivot_table(t[t.group == \"C\"], index= \"index\", columns=\"variable\", values=\"value\")\n    df_c.columns = pd.MultiIndex.from_product([[\"c\"], df_c.columns])\n    \n    res = pd.concat([df_a, df_b, df_c], axis=1)\n    \n    res.columns = pd.MultiIndex.from_tuples([(c[0], i) for i, c in enumerate(res.columns)])\n    \n    print(res)\n"], "link": "https://stackoverflow.com/questions/62962437/slicing-each-dataframe-row-into-3-windows-with-different-slicing-ranges"}
{"id": 591, "q": "Conditional removal of duplicates entries in Pandas", "d": "How can I remove the duplicate entries in the Pandas DataFrame given below. Desired result: The condition is: remove if and .", "q_apis": "DataFrame", "io": "a b c d 11216 08-08-2018 2000 SIP 40277 28-08-2018 1000 SIP 44165 02-08-2018 8000 Lump 44165 03-08-2018 5000 Lump 45845 16-08-2018 25000 Lump 45845 18-08-2018 50000 Lump 52730 13-08-2018 10000 Lump 52730 27-08-2018 10000 Lump 53390 20-08-2018 400000 Lump 56180 02-08-2018 1000 Lump 58537 11-07-2018 5000 Lump 58537 22-08-2018 2000 SIP 912813 15-08-2018 160001 Lump 912813 15-08-2018 6000 SIP 85606 16-08-2018 3500 SIP 88327 06-08-2018 5000 SIP 90240 07-08-2018 2000 SIP <s> a b c d 11216 08-08-2018 2000 SIP 40277 28-08-2018 1000 SIP 44165 02-08-2018 8000 Lump 45845 16-08-2018 25000 Lump 52730 13-08-2018 10000 Lump 53390 20-08-2018 400000 Lump 58537 11-07-2018 5000 Lump 912813 15-08-2018 160001 Lump 912813 15-08-2018 6000 SIP 85606 16-08-2018 3500 SIP 88327 06-08-2018 5000 SIP 90240 07-08-2018 2000 SIP ", "apis": "", "code": ["i = 0 \nwhile i < len(a)-1 :\n    if a[i] == a[i+1] and if b[i] != b[s] :\n        del a[i]\n        del b[i]\n        del c[i]\n        del d[i]\n        i -= 1 \n    i += 1 \n"], "link": "https://stackoverflow.com/questions/52113834/conditional-removal-of-duplicates-entries-in-pandas"}
{"id": 603, "q": "Pandas dataframe: Replace multiple rows based on values in another column", "d": "I'm trying to replace some values in one dataframe's column with values from another data frame's column. Here's what the data frames look like. has a lot of rows and columns. The final df should look like this So basically what needs to happen is that and need to be matched and then needs to have values replaced by the corresponding row in for the rows that matched. I don't want to lose any values in which are not in I believe the module in python can do that? This is what I have so far: But it definitely doesn't work. I could also use merge as in but that doesn't replace the values inline.", "q_apis": "values replace values values columns values any values merge replace values", "io": "df1 0 1029 0 aaaaa Green 1 bbbbb Green 2 fffff Blue 3 xxxxx Blue 4 zzzzz Green df2 0 1 2 3 .... 1029 0 aaaaa 1 NaN 14 NaN 1 bbbbb 1 NaN 14 NaN 2 ccccc 1 NaN 14 Blue 3 ddddd 1 NaN 14 Blue ... 25 yyyyy 1 NaN 14 Blue 26 zzzzz 1 NaN 14 Blue <s> 0 1 2 3 .... 1029 0 aaaaa 1 NaN 14 Green 1 bbbbb 1 NaN 14 Green 2 ccccc 1 NaN 14 Blue 3 ddddd 1 NaN 14 Blue ... 25 yyyyy 1 NaN 14 Blue 26 zzzzz 1 NaN 14 Green ", "apis": "DataFrame DataFrame fillna merge left where isna drop", "code": ["df1 = pd.DataFrame({'0':['aaaaa','bbbbb','fffff','xxxxx','zzzzz'], '1029':['Green','Green','Blue','Blue','Green']})\n\ndf2 = pd.DataFrame({'0':['aaaa','bbbb','ccccc','ddddd','yyyyy','zzzzz',], '1029':[None,None,'Blue','Blue','Blue','Blue']})\n\n\n# Fill NaNs\ndf2['1029'] = df2['1029'].fillna(df1['1029'])\n\n# Merge the dataframes \ndf_ = df2.merge(df1, how='left', on=['0'])\n\ndf_['1029'] = np.where(df_['1029_y'].isna(), df_['1029_x'], df_['1029_y'])\n\ndf_.drop(['1029_y','1029_x'],1,inplace=True)\nprint(df_)\n"], "link": "https://stackoverflow.com/questions/51587685/pandas-dataframe-replace-multiple-rows-based-on-values-in-another-column"}
{"id": 638, "q": "Saving a Pandas dataframe in fixed format with different column widths", "d": "I have a pandas dataframe (df) that looks like this: I want to save this dataframe in a fixed format. The fixed format I have in mind has different column width and is as follows: \"one space for column A's value then a comma then four spaces for column B's values and a comma and then five spaces for column C's values\" Or symbolically: My dataframe above (df) would look like the following in my desired fixed format: How can I write a command in Python that saves my dataframe into this format?", "q_apis": "value values values", "io": " A B C 0 1 10 1234 1 2 20 0 <s> 1, 10, 1234 2, 20, 0 ", "apis": "apply apply to_csv index", "code": ["df['B'] = df['B'].apply(lambda t: (' '*(4-len(str(t)))+str(t)))\ndf['C'] = df['C'].apply(lambda t: (' '*(5-len(str(t)))+str(t)))\ndf.to_csv('path_to_file.csv', index=False)\n"], "link": "https://stackoverflow.com/questions/50067957/saving-a-pandas-dataframe-in-fixed-format-with-different-column-widths"}
{"id": 607, "q": "Pulling Multiple Ranges from Dataframe Pandas", "d": "Lets say I have the following data set: I have a the following list of ranges. How do I efficiently pull rows that lie in those ranges? Wanted Output Edit: Needs to work for floating points", "q_apis": "", "io": "A B 10.1 53 12.5 42 16.0 37 20.7 03 25.6 16 30.1 01 40.9 19 60.5 99 <s> A B 10.1 53 12.5 42 20.7 03 40.9 19 ", "apis": "isin", "code": ["from itertools import chain\n\nvals = set(chain.from_iterable(range(i, j) for i, j in L))\n\nres = df[df['A'].isin(vals)]\n"], "link": "https://stackoverflow.com/questions/51371507/pulling-multiple-ranges-from-dataframe-pandas"}
{"id": 201, "q": "Dropping rows from pandas dataframe based on value in column(s)", "d": "Suppose I have a dataframe which has Column 'A' and Column 'B' How do I drop rows where Column 'A' and 'B' are equal , but not in same row. I only wanto to drop rows where column 'B' is equal to column 'A' For example Column 'B' from Rows 4, 8 & 9 is equal to Rows 2,3&5 Column 'A'. I want to drop Rows 4, 8 & 9 Drop Rows 4, 8 & 9 since Column B from rows is equal to column A from row 2,3&5 Expected output Rows 4, 8 & 9 needs to be deleted Adding additional details: Column A and B will never be equal in same row. Multiple rows in Column B may have matching values in Column A. To illustrate I have expanded the dataframe Sorry if my originial row numbers are not matching. To summarize the requirement. Multiple rows will have column B matching with Column A and expectation is to delete all rows where column B is matching with Column A in any row. To reiterate Column A and Column B will not be equal in same row", "q_apis": "value drop where drop where drop values delete all where any", "io": " Column A Column B 1 10 62 2 10 72 3 20 75 4 20 10 5 30 35 6 30 45 7 40 55 8 40 20 9 40 30 <s> Column A Column B 1 10 62 2 10 72 3 20 75 5 30 35 6 30 45 7 40 55 ", "apis": "isin", "code": ["df[~(df['Column B'].isin(df['Column A']) & (df['Column B'] != df['Column A']))]\n"], "link": "https://stackoverflow.com/questions/64198934/dropping-rows-from-pandas-dataframe-based-on-value-in-columns"}
{"id": 590, "q": "How to calculate dictionaries of lists using pandas DataFrame?", "d": "I have two strings in Python3.x, which are defined to be the same length: I am also given an integer which is meant to represent the \"starting index\" of . In this case, . The goal is to create a dictionary based on the indices. So, begins at , begins at . The dictionary \"converting\" these coordinates is as follows: which can be constructed (give the variables above) with: I currently have this data in the form of a pandas DataFrame: There are multiple entries of the same string in column . In this case, the dictionary for the coordinates with should be: I would like to take this DataFrame and calculate similar dictionaries of the coordinates. Such a statement looks like one should somehow use ? I'm not sure how to populate dictionary lists like this... Here is the correct output (keeping the DataFrame structure). Here the DataFrame has the column such that it looks like the following:", "q_apis": "DataFrame length index indices at at DataFrame take DataFrame DataFrame DataFrame", "io": "{0: 51, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61} <s> {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54, 86], 3: [34, 55, 87], 4: [35, 56, 88], 5: [36, 57, 89], 6: [37, 58, 90], 7: [38, 59, 91]} ", "apis": "dtype", "code": ["column1\nLJNVTJOY      {0: [31, 52, 84], 1: [32, 53, 85], 2: [33, 54,...\nMXRBMVQDHF    {0: [79], 1: [80], 2: [81], 3: [82], 4: [83], ...\nWHLAOECVQR    {0: [18], 1: [19], 2: [20], 3: [21], 4: [22], ...\nName: val, dtype: object\n"], "link": "https://stackoverflow.com/questions/52143288/how-to-calculate-dictionaries-of-lists-using-pandas-dataframe"}
{"id": 569, "q": "Create a column that has the same length of the longest column in the data at the same time", "d": "I have the following data: Output: Is it possible to create a 4th column AT THE SAME TIME the others columns are created in data, which has the same length as the longest column of this dataframe (3rd one)? The data of this column doesn't matter. Assume it's 8. So this is the desired output can be: In my script the dataframe keeps changing every time. This means the longest columns keeps changing with it. Thanks for reading", "q_apis": "length at time columns length time columns", "io": " 0 1 2 0 1.0 1.0 1.0 1 2.0 2.0 2.0 2 3.0 3.0 3.0 3 NaN 4.0 4.0 4 NaN 5.0 5.0 5 NaN NaN 6.0 6 NaN NaN 7.0 <s> 0 1 2 3 0 1.0 1.0 1.0 8.0 1 2.0 2.0 2.0 8.0 2 3.0 3.0 3.0 8.0 3 NaN 4.0 4.0 8.0 4 NaN 5.0 5.0 8.0 5 NaN NaN 6.0 8.0 6 NaN NaN 7.0 8.0 ", "apis": "", "code": ["data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]] + [[]]\n"], "link": "https://stackoverflow.com/questions/52676653/create-a-column-that-has-the-same-length-of-the-longest-column-in-the-data-at-th"}
{"id": 315, "q": "pandas data change based on condition", "d": "I have data which has special characters, I want to change the conditional cell values. Data is below first few lines df_orig: I want to change cell values where $ in D, A = 0 and B = C THE OUTPUT SHOULD BE change: I tried at my end with but it didn't work.", "q_apis": "values first values where at", "io": "idx A B C D 0 0.5 2 5 # 1 3 5 8 % 2 6 8 10 $ 3 9 10 15 $ 4 11 15 18 # <s> idx A B C D 0 0.5 2 5 # 1 3 5 8 % 2 0 10 10 $ 3 0 15 15 $ 4 11 15 18 # ", "apis": "eq loc assign loc values", "code": ["m = df['D'].eq('$')\ndf.loc[m, ['A','B']] = df.assign(E=0).loc[m, ['E','C']].values\n"], "link": "https://stackoverflow.com/questions/61075047/pandas-data-change-based-on-condition"}
{"id": 464, "q": "How should I construct this json return from a pandas dataframe", "d": "I have some data, organised by date, as a datetime index. I then subset it so it is effectively irregular: In my service (this is not for interactive use) I am given a string which I pass to to aggregate my time data to any level. The string is supplied directly to the argument, and can be values like , , , I would like to use the same string to create an json which is similar to the following structure: The array will be 'ragged' i.e. not all records will be present, and not all keys in the json will have the same length of array. The goals for a good solution are: is the level set by the same string parameter as given to the array in the aggregate level will always be the raw, datetime hourly values Ideally the string parameter is not associated with a bunch of 'translatation rules' such as \"If then use like this, but if use this and if use this would just return a single array, of the raw values So in practice if a is supplied: Note that there are two keys at daily level, with the values in the array split to the correct day. If is supplied: Note that this means the contents of the value array will be 3 in this example, as the 3 datetimes are all in the same month Things I've tried/looked at that I haven't made work well: , they look like they aggregate only, based on some rules. I specifically need to return the actual records Parseing a new column based on the argument would technically work, but it seems wrong as I would have to start converting each to a or similar. I have not yet found a function that accepts the same character string and does not also perform aggregations Is setting a multi-index a solution to this? It might be but I'm not certain how to populate it in regards to the point above about the , , etc. custom resampler: Which is not working. I understand, it may be that this is not solvable with the rules above, but I am probably not good enough at yet to realise it.", "q_apis": "date index aggregate time any values array all all keys length array array aggregate values array values keys at values array day value array all month at aggregate start index at", "io": "{'2018-01-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00'}, '2018-01-08':{'2018-01-08 03:00:00'}} <s> {'2018-01': {'2018-01-01 03:00:00', '2018-01-01 07:00:00', '2018-01-08 03:00:00'}} ", "apis": "to_json resample apply", "code": ["def custom_resampler(array_like):\n    return array_like.to_json()\n\ndf.resample('D').apply(custom_resampler)\n"], "link": "https://stackoverflow.com/questions/56681714/how-should-i-construct-this-json-return-from-a-pandas-dataframe"}
{"id": 419, "q": "merging two rows of data in a single row with Python/Pandas", "d": "I have a dataframe like this: I need the rows belonging to the same ID to be merged in a single row, the merged dataframe will be like this I tried using np.random.permutation, np.roll etc but unable to get the desired result. The count of rows in my original data set is in thousands so loops and creating individual data sets and then merging is not helping", "q_apis": "get count", "io": " ID A1 A2 A3 A4 0 01 100 101 103 104 1 01 501 502 503 504 2 01 701 702 703 704 3 02 1001 1002 1003 1004 4 03 2001 2002 2003 2004 5 03 5001 5002 5003 5004 <s> ID A1 A2 A3 A4 B1 B2 B3 B4 C1 C2 C3 C4 0 01 101 102 103 104 501 502 503 504 701 702 703 704 1 02 1001 2001 1003 1004 2 03 2001 2002 2003 2004 5001 5002 5003 5004 ", "apis": "columns Series loc unstack values index groupby apply unstack", "code": ["import pandas as pd\n\n\ndef widen(x):\n    num_rows = len(x)\n    num_cols = len(x.columns)\n\n    new_index = [\n        chr(ord('A') + row_number) + str(col_number + 1)\n        for row_number in range(num_rows)\n        for col_number in range(num_cols)\n    ]\n\n    return pd.Series(x.loc[:, 'A1':].unstack().values, index=new_index)\n\nres = df.groupby('ID').apply(widen).unstack()\n"], "link": "https://stackoverflow.com/questions/58254668/merging-two-rows-of-data-in-a-single-row-with-python-pandas"}
{"id": 530, "q": "How can I remove the &#39;NaN&#39; not removing the data?", "d": "I'm trying to remove the 'NaN'. In detail, there is data on one line and 'NaN'. My data looks like the one below. I want to eliminate the NAN between the data and make one data for every 18 lines. I tried option 'dropna()' (using 'how = 'all'' or 'thread = '10''). But these are not what I want. How can I remove NaN and merge data? Add This is the code that I using(python2). The is the data that have NaN. If you look at the data, there are data in the 0th line from 1 to 10, and data in the 1st line from 11th to 21st. That is, there are two lines of data. I want to wrap this in a single line without NaN. Like this result. I tried to re-index the row to time to using resampling. And I save the start and end index. And I save the index_time to use resampling time. The result of 'df_time_merge' is like this. enter image description here It's working!! But if I have data(starting with Nan) like this, the code didn't working. enter image description here If I run same code, the and . Where did I miss?", "q_apis": "between dropna all merge at index time start index time", "io": " 01 02 03 04 05 06 07 08 09 10 ... 12 13 \\ 0 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 14 15 16 17 18 19 20 21 0 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 1 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 <s> 01 02 03 04 05 06 07 08 09 10 ... 12 13 \\ 0 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 132.0 321.0 0.0 31.0 ... 0.936 0.0 14 15 16 17 18 19 20 21 0 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 1 8.984375 15.234375 646.25 0.0 0.0 9.765625 0.0 0.0 ", "apis": "test test test test test test test test array test all array test array array", "code": ["def make_sample():\n    test=np.full((8,12), np.nan)\n    test[0,:6]=np.arange(6)\n    test[1,6:]=np.arange(6,18,2)\n    test[4:6,:]=2*test[:2,:]\n    return test\n\ntest=make_sample()\n\nIn [74]: test\nOut[74]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n", "filt=1^np.isnan(test).all(axis=1)\n\nIn [78]: filt\nOut[78]: array([1, 1, 0, 0, 1, 1, 0, 0])\n", "compress=np.compress(filt, test, axis=0)\n\nIn [80]: compress\nOut[80]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10., nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan, 12., 16., 20., 24., 28., 32.]])\n", "compress[np.isnan(compress)]=0\n\nIn [83]: compress\nOut[83]: \narray([[ 0.,  1.,  2.,  3.,  4.,  5.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  6.,  8., 10., 12., 14., 16.],\n       [ 0.,  2.,  4.,  6.,  8., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0., 12., 16., 20., 24., 28., 32.]])\n"], "link": "https://stackoverflow.com/questions/54273695/how-can-i-remove-the-nan-not-removing-the-data"}
{"id": 332, "q": "Removing Duplicate values from a Cell of DataFramein python", "d": "DataFrame Output I want Any Help will be Appreciated", "q_apis": "values DataFrame", "io": "ID Source 1 [192.168.1.121, 10.1.161.10, 192.168.1.121, 192.168.1.121] 2 [192.168.1.121, 10.1.161.10, 10.1.161.10, 10.1.161.10, 192.168.1.121] 3 [192.168.1.121, 192.168.1.121, 192.168.1.121] 4 [10.1.161.10, 192.168.1.121, 10.1.161.10, 10.1.161.10] <s> ID Source 1 192.168.1.121, 10.1.161.10 2 192.168.1.121, 10.1.161.10 3 192.168.1.121 4 10.1.161.10, 192.168.1.121 ", "apis": "apply join unique", "code": ["df['source'] = df['source'].apply(\nlambda x: ', '.join(pd.unique(x)),\n)\n"], "link": "https://stackoverflow.com/questions/60709527/removing-duplicate-values-from-a-cell-of-dataframein-python"}
{"id": 274, "q": "Vectorizing for-loop", "d": "I have a very large dataframe (~10^8 rows) where I need to change some values. The algorithm I use is complex so I tried to break down the issue into a simple example below. I mostly programmed in C++, so I keep thinking in for-loops. I know I should vectorize but I am new to python and very new to pandas and cannot come up with a better solution. Any solutions which increase performance are welcome. Any ideas? EDIT: I was ask to explain what I do with my for-loops. For every eventID I want to know if all corresponding types contain a 1 or a 0 or both. If they contain a 1, all values which are equal to -1 should be changed to 1. If the values are 0, all values equal to -1 should be changed to 0. My problem is to do this efficiently for each eventID independently. There can be one or multiple entries per eventID. Input of example: Output of example:", "q_apis": "where values all all values values all values", "io": " eventID types 0 1 0 1 1 -1 2 1 -1 3 2 -1 4 2 1 5 3 0 6 4 0 7 5 0 8 6 -1 9 6 -1 10 6 -1 11 6 1 12 7 -1 13 8 -1 <s> eventID types 0 1 0 1 1 0 2 1 0 3 2 1 4 2 1 5 3 0 6 4 0 7 5 0 8 6 1 9 6 1 10 6 1 11 6 1 12 7 -1 13 8 -1 ", "apis": "eq groupby transform any eq groupby transform any select", "code": ["m1 = mydf['types'].eq(1).groupby(mydf['eventID']).transform('any')\nm2 = mydf['types'].eq(0).groupby(mydf['eventID']).transform('any')\nmydf['types'] = np.select([m1 , m2], [1, 0], mydf['types'])\n"], "link": "https://stackoverflow.com/questions/62536086/vectorizing-for-loop"}
{"id": 100, "q": "Python find closest neighbors to a value in a dataframe", "d": "I have a dataframe or list. I want to find the closest values and their index to a given value. My code: Present output (val_idx): Expected output (val_idx):", "q_apis": "value values index value", "io": " num 1 24 0 20 <s> num 2 35 1 24 ", "apis": "DataFrame get test get index argmin test where", "code": ["import numpy as np\nimport pandas as pd\n\n# setup\ndf = pd.DataFrame({'num':[20,24,35,38]})\nval = 26 \n\n# subtract to get differences\ntest = np.absolute(np.subtract(val, df[\"num\"]))\n\n# get index\nidx = np.argmin(test)\n# Condition\nidx = np.where(df[\"num\"][idx] > val, [idx-1, idx], [idx, idx+1])\n\nprint(idx)\n"], "link": "https://stackoverflow.com/questions/66128637/python-find-closest-neighbors-to-a-value-in-a-dataframe"}
{"id": 52, "q": "How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?", "d": "I have two dataframes: df1 df2 df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be: ABD123 = 1 (because it has 1 different character from ABC123) DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one) GHI789 = 3 (because it has an equivalent at df1) I know how to check for the differences of each code individually and save the \"length\" of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way?", "q_apis": "compare difference between values get item codes codes at length apply compare all", "io": " Code Number 0 ABC123 1 1 DEF456 2 2 GHI789 3 3 DEA456 4 <s> Code 0 ABD123 1 DEA458 2 GHI789 ", "apis": "iterrows iterrows difference compare difference", "code": ["for index2, row2 in df2.iterrows():\n    for index1, row1 in df1.iterrows():\n        difference = compare(row2,row1)\n        #do something with the difference.\n"], "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf"}
{"id": 543, "q": "Column dupe renaming in pandas", "d": "I have the following csv file of data: Pandas currently renames this to: Is there a way to customize how this is renamed? For example, I would prefer:", "q_apis": "", "io": " id number id.1 0 132605 1 1 1 132750 2 1 <s> id number id2 0 132605 1 1 1 132750 2 1 ", "apis": "replace count count count count columns", "code": ["from collections import defaultdict\nimport csv\n\n# replace StringIO(file) with open('file.csv', 'r')\nwith StringIO(file) as fin:\n    headers = next(csv.reader(fin))\n\ndef rename_duplicates(original_cols):\n    count = defaultdict(int)\n    for x in original_cols:\n        count[x] += 1\n        yield f'{x}{count[x]}' if count[x] > 1 else x\n\ndf.columns = rename_duplicates(headers)\n"], "link": "https://stackoverflow.com/questions/53939072/column-dupe-renaming-in-pandas"}
{"id": 167, "q": "Pandas resample column based on other column", "d": "I have a similar dataframe: And I want to resample this dataframe such that x values with the same y value is averaged. In other words: I've looked into the pandas.DataFrame.resample function, but not sure how to do this without timestamps.", "q_apis": "resample resample values value DataFrame resample", "io": "x | y 1 | 1 3 | 1 3 | 1 4 | 1 5 | 2 5 | 2 9 | 2 8 | 2 <s> x | y (1+3+3+4)/4 | 1 (5+5+9+8)/4 | 2 ", "apis": "DataFrame groupby mean reset_index", "code": ["import pandas\ndf = pandas.DataFrame({\"x\":[1,3,3,4,5,5,9,8],\"y\":[1,1,1,1,2,2,2,2]})\ndf.groupby([\"y\"]).mean().reset_index()\n"], "link": "https://stackoverflow.com/questions/64766331/pandas-resample-column-based-on-other-column"}
{"id": 465, "q": "Negating column values and adding particular values in only some columns in a Pandas Dataframe", "d": "Taking a Pandas dataframe df I would like to be able to both take away the value in the particular column for all rows/entries and also add another value. This value to be added is a fixed additive for each of the columns. I believe I could reproduce df, say dfcopy=df, set all cell values in dfcopy to the particular numbers and then subtract df from dfcopy but am hoping for a simpler way. I am thinking that I need to somehow modify So for example of how this should look: Then negating only those values in columns (0,3,4) and then adding 10 (for example) we would have: Thanks.", "q_apis": "values values columns take value all add value value columns all values values columns", "io": " A B C D E 0 1.0 3.0 1.0 2.0 7.0 1 2.0 1.0 8.0 5.0 3.0 2 1.0 1.0 1.0 1.0 6.0 <s> A B C D E 0 9.0 3.0 1.0 8.0 3.0 1 8.0 1.0 8.0 5.0 7.0 2 9.0 1.0 1.0 9.0 4.0 ", "apis": "iloc iloc iloc iloc", "code": ["df.iloc[:, [0,2,7,10,11] = -df.iloc[:, [0,2,7,10,11]\n", "df.iloc[:, [0,2,7,10,11] = df.iloc[:, [0,2,7,10,11]+c\n"], "link": "https://stackoverflow.com/questions/56723207/negating-column-values-and-adding-particular-values-in-only-some-columns-in-a-pa"}
{"id": 321, "q": "choosing rows by values in DataFrame", "d": "A post gives a way to choose rows by column value Here is a DataFrame with this code , I got when I run this , I got error this code gives This is close, what I am trying to get is a new DataFrame consists of rows at [2,4,6,8,9]. How to do that? Thanks to anyone who gives some inspiration.", "q_apis": "values DataFrame value DataFrame get DataFrame at", "io": " 0 1 0 877.443401 808.520962 1 826.300620 848.761594 2 824.403359 861.395174 3 866.732033 804.494156 4 853.461260 874.307851 5 822.906499 830.102249 6 852.605652 863.602725 7 893.421600 825.032893 8 863.768363 862.298227 9 899.976622 864.111539 <s> 0 1 0 NaN NaN 1 NaN NaN 2 NaN 861.395174 3 NaN NaN 4 NaN 874.307851 5 NaN NaN 6 NaN 863.602725 7 NaN NaN 8 NaN 862.298227 9 NaN 864.111539 ", "apis": "loc copy query", "code": ["new_df = df.loc[df[1] > 850].copy()\n", "new_df = df.query('a > 850')\n"], "link": "https://stackoverflow.com/questions/60994122/choosing-rows-by-values-in-dataframe"}
{"id": 628, "q": "Creating single row pandas dataframe", "d": "I have a dataframe like this- I want to create a new dataframe which looks like this-", "q_apis": "", "io": " 0 0 a 43 1 b 630 2 r 587 3 i 462 4 g 153 5 t 266 <s> a b r i g t 0 43 630 587 462 153 266 ", "apis": "columns DataFrame columns T reset_index drop rename columns iloc drop index", "code": ["df.columns = ['col']\ndf = pd.DataFrame(df.col.str.split(' ',1).tolist(), columns = ['col1','col2']).T.reset_index(drop=True)\ndf = df.rename(columns=df.iloc[0]).drop(df.index[0])\n"], "link": "https://stackoverflow.com/questions/50642233/creating-single-row-pandas-dataframe"}
{"id": 13, "q": "Getting first/second/third... value in row of numpy array after nan using vectorization", "d": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks.", "q_apis": "first second value array values left where first value left array left values array first value array second value", "io": "f(offset=0) | 0 | 1 | | -- | -- | | 1 | 25 | | 2 | 29 | | 3 | 33 | | 4 | 31 | | 5 | 30 | | 6 | 35 | | 7 | 31 | | 8 | 33 | | 9 | 26 | | 10 | 27 | | 11 | 35 | | 12 | 33 | | 13 | 28 | | 14 | 25 | | 15 | 25 | | 16 | 26 | | 17 | 34 | | 18 | 28 | | 19 | 34 | | 20 | 28 | <s> f(offset=1) | 0 | 1 | | -- | --- | | 1 | nan | | 2 | nan | | 3 | nan | | 4 | 35 | | 5 | 34 | | 6 | 34 | | 7 | 26 | | 8 | 25 | | 9 | 31 | | 10 | 26 | | 11 | 25 | | 12 | 35 | | 13 | 25 | | 14 | 25 | | 15 | 26 | | 16 | 31 | | 17 | 29 | | 18 | 29 | | 19 | 26 | | 20 | 30 | ", "apis": "argmax clip shape shape any shape stack groupby nth reindex index concat to_numpy mean std mean std mean std", "code": ["def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n", "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n", "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"], "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector"}
{"id": 456, "q": "Pandas: Remove index entry (and all it&#39;s rows) from multilevel index when all data in a column is NaN", "d": "I'd like to clean up some data I have in a dataframe with a multilevel index. I'd like to loose the complete group indexed by bar, because all of the data in column A is NaN. I'd like to keep foo, because only some of the data in column A is NaN (column B is not important here, even if it's all NaN). I'd like to keep baz, because not all of column Ais NaN. So my result should look like this: What's the best way to do this with pandas and python? I suppose there is a better way than looping through the data...", "q_apis": "index all index all index all all all", "io": " | A | B | ----------------+-----+-----+ foo 2019-01-01 | x | NaN | 2019-01-02 | x | NaN | 2019-01-03 | NaN | NaN | ................+.....+.....+ bar 2019-01-01 | NaN | x | 2019-01-02 | NaN | y | 2019-01-03 | NaN | z | ................+.....+.....+ baz 2019-01-01 | x | x | 2019-01-02 | x | x | 2019-01-03 | x | x | <s> | A | B | ----------------+-----+-----+ foo 2019-01-01 | x | NaN | 2019-01-02 | x | NaN | 2019-01-03 | NaN | NaN | ................+.....+.....+ baz 2019-01-01 | x | x | 2019-01-02 | x | x | 2019-01-03 | x | x | ", "apis": "notna groupby transform any dtype bool", "code": ["m = df['A'].notna().groupby(level=0).transform('any')\nprint(m)\n\nidx  idx2      \nfoo  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nbar  2019-01-01    False\n     2019-01-02    False\n     2019-01-03    False\nbaz  2019-01-01     True\n     2019-01-02     True\n     2019-01-03     True\nName: A, dtype: bool\n"], "link": "https://stackoverflow.com/questions/56869643/pandas-remove-index-entry-and-all-its-rows-from-multilevel-index-when-all-da"}
{"id": 328, "q": "Merge columns with have \\n", "d": "ex) I'm merging columns, but I want to give '\\n\\n' in the merging process. so output what I want I want 'nan' to drop. I tried However, this includes all nan values. thank you for reading.", "q_apis": "columns columns drop all values", "io": " C1 C2 C3 C4 C5 C6 0 A B nan C A nan 1 B C D nan B nan 2 D E F nan C nan 3 nan nan A nan nan B <s> C 0 A B C A 1 B C D B 2 D E F C 3. A B ", "apis": "merge stack groupby agg join filter columns merge filter stack groupby agg join merge apply join dropna filter columns merge filter apply join dropna", "code": ["df['merge'] = df.stack().groupby(level=0).agg('\\n\\n'.join)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').stack().groupby(level=0).agg('\\n\\n'.join)\n", "df['merge'] = df.apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n#for filter only C columns\ndf['merge'] = df.filter(like='C').apply(lambda x: '\\n\\n'.join(x.dropna()), axis=1)\n"], "link": "https://stackoverflow.com/questions/60825821/merge-columns-with-have-n"}
{"id": 198, "q": "how to create new dataframe by combining some columns together of existing one?", "d": "I am having a dataframe df like shown: where the explanation of the columns as the following: the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3. I would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups. The new dataframe I need is like the shown:", "q_apis": "columns where columns first second groups groups max any groups", "io": "1-1 1-2 1-3 2-1 2-2 3-1 3-2 4-1 5-1 10 3 9 1 3 9 33 10 11 21 31 3 22 21 13 11 7 13 33 22 61 31 35 34 8 10 16 6 9 32 5 4 8 9 6 8 <s> 1 2 3 4 5 10 3 33 10 11 31 22 13 7 13 61 35 34 10 16 32 5 9 6 8 ", "apis": "groupby max", "code": ["df1 = df.groupby(lambda x: x.split('-')[0], axis=1).max()\n"], "link": "https://stackoverflow.com/questions/64242508/how-to-create-new-dataframe-by-combining-some-columns-together-of-existing-one"}
{"id": 472, "q": "Combine pandas dataframes eliminating common columns with python", "d": "I have 3 dataframes: I want to combine them together to get the following results: When I try to combine them, I keep getting: The common column (A) is duplicated once for each dataframe used in the concat call. I have tried various combinations on: Some variations have been disastrous while some keep giving the undesired result. Any suggestions would be much appreciated. Thanks.", "q_apis": "columns combine get combine duplicated concat", "io": " A B C D E F 0 A0 B0 C0 D0 E0 F0 1 A1 B1 C1 D1 E1 F1 2 A2 B2 C2 D2 E2 F2 3 A3 B3 C3 D3 E3 F3 <s> A B C D A E A F 0 A0 B0 C0 D0 A0 E0 A0 F0 1 A1 B1 C1 D1 A1 E1 A1 F1 2 A2 B2 C2 D2 A2 E2 A2 F2 3 A3 B3 C3 D3 A3 E3 A3 F3 ", "apis": "concat set_index reset_index", "code": ["df4 = (pd.concat((df.set_index('A') for df in (df1,df2,df3)), axis=1)\n         .reset_index()\n      )\n"], "link": "https://stackoverflow.com/questions/56572543/combine-pandas-dataframes-eliminating-common-columns-with-python"}
{"id": 168, "q": "pandas dataframe select list value from another column", "d": "Everyone! I have a pandas dataframe like this: as we can see, the A column is a list and the B column is an index value. I want to get a C column which is index by B from A: Is there any elegant method to solve this? Thank you!", "q_apis": "select value index value get index any", "io": " A B 0 [1,2,3] 0 1 [2,3,4] 1 <s> A B C 0 [1,2,3] 0 1 1 [2,3,4] 1 3 ", "apis": "to_numpy apply", "code": ["df['C'] = [x[y] for x, y in df[['A','B']].to_numpy()]\n", "df['C'] = df.apply(lambda x: x.A[x.B], axis=1)\n"], "link": "https://stackoverflow.com/questions/64765350/pandas-dataframe-select-list-value-from-another-column"}
{"id": 207, "q": "How to create bins for a dataframe column if the range is given", "d": "This is an example data frame that I want to play with If I do this, I get the output as: Here's the twist: Let's say the age columns can take values between 18 to 58(the range of the column) and I want the bins(or the output) as: How can I do that? because 'cut' takes the values which are in the column. I got the desired result by doing it manually but if the values of bins were say 100 - how can I do it?", "q_apis": "get columns take values between cut values values", "io": "0 (17.964, 25.2] 1 (17.964, 25.2] 2 (25.2, 32.4] 3 (25.2, 32.4] 4 (32.4, 39.6] 5 (32.4, 39.6] 6 (39.6, 46.8] 7 (46.8, 54.0] 8 (46.8, 54.0] <s> 0 (18.0, 26.0] 1 (18.0, 26.0] 2 (26.0, 34.0] 3 (26.0, 34.0] 4 (34.0, 42.0] 5 (34.0, 42.0] 6 (34.0, 42.0] 7 (50.0, 58.0] 8 (42.0, 50.0] ", "apis": "cut", "code": ["pd.cut(df['Age'], bins=np.linspace(18, 58, 100), include_lowest=True)\n"], "link": "https://stackoverflow.com/questions/64110166/how-to-create-bins-for-a-dataframe-column-if-the-range-is-given"}
{"id": 231, "q": "Sort Pandas dataframe column index by date", "d": "I want to sort dataframe by column index. The issue is my columns are 'dates' dd/mm/yyyy directly imported from my excel. For ex: The output I want is: I am using It is giving me following error: TypeError: '<' not supported between instances of 'datetime.datetime' and 'str' I want to do it in panda dataframe. Any help will be appreciated. Thanks", "q_apis": "index date index columns between", "io": " 10/08/20 12/08/20 11/08/20 0 2.0 6.0 15.0 1 6.0 11.0 8.0 2 4.0 7.0 3.0 3 7.0 12.0 2.0 4 12.0 5.0 7.0 <s> 10/08/20 11/08/20 12/08/20 0 2.0 15.0 6.0 1 6.0 8.0 11.0 2 4.0 3.0 7.0 3 7.0 2.0 12.0 4 12.0 7.0 5.0 ", "apis": "columns Series columns apply strptime sort_index", "code": ["import datetime as dt\ndf.columns=pd.Series(df.columns).apply(lambda d: dt.datetime(d, dt.datetime.strptime(d, '%d/%m/%Y')))\ndf.sort_index(axis = 1)\n"], "link": "https://stackoverflow.com/questions/63440751/sort-pandas-dataframe-column-index-by-date"}
{"id": 408, "q": "Python Round Dataframe Columns with Specific Value If Exists", "d": "My input dataframe; I want to round my dataframe columns according to a specifi value if exists. My code is like below; Output is; The issue is if there is no \"rounding\" variable, it should be run automatically as default (0.5). I need a code that can run for both together. Something like this or different; I saw many topics about rounding with specific value but i couldn' t see for this. Could you please help me about this?", "q_apis": "round columns value value", "io": "A B 0.3 0.6 0.4 3.05 1.6 4.35 0.15 5.47 4.19 9.99 <s> A B 1 1 1 3 2 5 0 6 4 10 ", "apis": "isna", "code": ["rounding = np.nan\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0\n", "rounding = 0 if pd.isna(rounding) else rounding\nprint (rounding)\n0\n", "rounding = 0.25\n\nrounding = 0 if rounding != rounding else rounding\nprint (rounding)\n0.25\n"], "link": "https://stackoverflow.com/questions/58539757/python-round-dataframe-columns-with-specific-value-if-exists"}
{"id": 443, "q": "Identify increasing features in a data frame", "d": "I have a data frame that present some features with cumulative values. I need to identify those features in order to revert the cumulative values. This is how my dataset looks (plus about 50 variables): What I wish to achieve is: I've seem this answer, but it first revert the values and then try to identify the columns. Can't I do the other way around? First identify the features and then revert the values? Finding cumulative features in dataframe? What I do at the moment is run the following code in order to give me the feature's names with cumulative values: Afterwards, I save these features names manually in a list called cum_features and revert the values, creating the desired dataset: Is there a better way to solve my problem?", "q_apis": "values values first values columns values at names values names values", "io": "a b 346 17 76 52 459 70 680 96 679 167 246 180 <s> a b 346 17 76 35 459 18 680 26 679 71 246 13 ", "apis": "diff dropna all dtype bool", "code": ["out = (df.diff().dropna()>0).all()\n#Output:\na     True\nb    False\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/57376287/identify-increasing-features-in-a-data-frame"}
{"id": 300, "q": "Python DataFrame Data Analysis of Large Amount of Data from a Text File", "d": "I have the following code: I am using a text file (that is not formatted) to pull chunks of data from. When the text file is opened, it looks something like this, except on a way bigger scale: Here are the things I'm having trouble doing with this data: I only need the second, third, sixth, and seventh columns of data. The issue with this one, I believe I've solved with my code above by reading the individual lines and creating a dataframe with the columns necessary. I am open to suggestions if anyone has a better way of doing this. I need to skip the first row of data. This one, the open feature doesn't have a skiprows attribute, so when I drop the first row, I also lose my index starting at 0. Is there any way around this? I need the resulting dataframe to look like a nice clean dataframe. As of right now, it looks something like this: Everything is right-aligned under the column and it looks strange. Any ideas how to solve this? I also need to be able to perform Statistic Analysis on the columns of data, and to be able to find the Name with the highest data and the lowest data, but for some reason, I always get errors because I think that, even though I've got all the data set up as a dataframe, the values inside the dataframe are reading as objects instead of integers, strings, floats, etc. So, if my data is not analyzable using Python functions, does anyone know how I can fix this to make the data be able to run correctly? Any help would be greatly appreciated. I hope I've laid out all of my needs clearly. I am new to Python, and I'm not sure if I'm using all the proper terminology.", "q_apis": "DataFrame second columns columns first drop first index at any right now right columns get all values all all", "io": "00 2381 1.3 3.4 1.8 265879 Name 34 7879 7.6 4.2 2.1 254789 Name 45 65824 2.3 3.4 1.8 265879 Name 58 3450 1.3 3.4 1.8 183713 Name 69 37495 1.3 3.4 1.8 137632 Name 73 458913 1.3 3.4 1.8 138024 Name <s> Col1 Col2 Col3 Col4 2381 3.4 265879 Name 7879 4.2 254789 Name 65824 3.4 265879 Name 3450 3.4 183713 Name 37495 3.4 137632 Name 458913 3.4 138024 Name ", "apis": "read_csv names", "code": ["keep = ['col1', 'col3', 'col5', 'col6']\ndf = pd.read_csv('txt2pd.txt', \n                 sep='\\s+', \n                 names=['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6'], \n                 skiprows=1)\ndf = df[keep]\n"], "link": "https://stackoverflow.com/questions/61580297/python-dataframe-data-analysis-of-large-amount-of-data-from-a-text-file"}
{"id": 496, "q": "Sort DataFrame column with given input list", "d": "Hi I want to sort DataFrame column with given input list values. My list looks like : And DataFrame is : Here I want to sort DataFrame column 'val' on basis of given 'inputlist'. I am expecting following output :", "q_apis": "DataFrame DataFrame values DataFrame DataFrame", "io": " val kaywords 195 keyword3 221 keyword5 307 keyword8 309 keyword9 354 keyword0 426 keyword1 585 keyword2 698 keyword4 789 keyword33 <s> val kaywords 309 keyword9 585 keyword2 221 keyword5 789 keyword33 195 keyword3 354 keyword0 307 keyword8 698 keyword4 426 keyword1 ", "apis": "set_index reindex reset_index", "code": ["inputlist = [int(x) for x in inputlist]\ndf = df.set_index('val').reindex(inputlist).reset_index()\n"], "link": "https://stackoverflow.com/questions/55672247/sort-dataframe-column-with-given-input-list"}
{"id": 367, "q": "Replace outliers with median exept NaN", "d": "I would like to replace outliers with median in a dataframe but only outliers and not NaN. First : I would like to replace the -60 which is an outlier with the median using : It works fine but it also delete all rows containing a NaN how can I avoid that ? Output : As you can see, 3 rows have been deleted which is not very convenient. Any ideas ? Thanks !", "q_apis": "median replace median replace median delete all", "io": " January February 0 -5.0 -7.0 1 -6.0 -6.0 2 -5.0 -5.0 3 -3.0 -6.0 4 -6.0 -8.0 5 -11.0 -9.0 6 -6.0 5.0 7 -8.0 -11.0 8 -11.0 -12.0 9 -8.0 -9.0 10 -8.0 -6.0 11 -8.0 -5.0 12 -8.0 -4.0 13 -10.0 1.0 14 -10.0 3.0 15 -9.0 -9.0 16 -6.0 -6.0 17 -6.0 -6.0 18 -4.0 -4.0 19 -8.0 2.0 20 -9.0 3.0 21 -14.0 1.0 22 -15.0 -3.0 23 -17.0 -4.0 24 -19.0 -6.0 25 -60.0 -8.0 26 -8.0 -8.0 27 -9.0 -11.0 28 -5.0 NaN 29 -6.0 NaN 30 -7.0 NaN <s> January February 0 -5.0 -7.0 1 -6.0 -6.0 2 -5.0 -5.0 3 -3.0 -6.0 4 -6.0 -8.0 5 -11.0 -9.0 6 -6.0 5.0 7 -8.0 -11.0 8 -11.0 -12.0 9 -8.0 -9.0 10 -8.0 -6.0 11 -8.0 -5.0 12 -8.0 -4.0 13 -10.0 1.0 14 -10.0 3.0 15 -9.0 -9.0 16 -6.0 -6.0 17 -6.0 -6.0 18 -4.0 -4.0 19 -8.0 2.0 20 -9.0 3.0 21 -14.0 1.0 22 -15.0 -3.0 23 -17.0 -4.0 24 -19.0 -6.0 25 -10.0 -8.0 26 -8.0 -8.0 27 -9.0 -11.0 ", "apis": "apply abs mean std isna all", "code": ["df = df[df.apply(lambda x: (np.abs(x - x.mean()) / x.std() < 4) | x.isna()).all(axis=1)]\nprint(df)\n"], "link": "https://stackoverflow.com/questions/59760868/replace-outliers-with-median-exept-nan"}
{"id": 385, "q": "Jupyter Pandas - dropping items which have average over a threshold", "d": "I have a data frame with items and their prices, something like this: I want to exclude all rows from this df where the item has an average price over 200. So filtered df should look like this: I'm new to python and pandas but as a first step was thinking something like this to get a new df for avg prices: avg_prices_df = df.groupby('ItemID').Price.mean().reset_index and then not sure how to proceed from there. Not sure even that first step is correct. To further complicate the matter, I am using vaex to read the data in ndf5 form as I have over 400 million rows. Many thanks in advance for any advice. EDIT: So I got the following code working, though I am sure it is not optimised.. ` create dataframe of ItemIDs and their average prices df_item_avg_price = df.groupby(df.ItemID, agg=[vaex.agg.count('ItemID'), vaex.agg.mean('Price')]) filter this new dataframe by average price threshold df_item_avg_price = (df_item_avg_price[df_item_avg_price[\"P_r_i_c_e_mean\"] <= 50000000]) create list of ItemIDs which have average price under the threshold items_in_price_range = df_item_avg_price['ItemID'].tolist() filter the original dataframe to include rows only with the items in price range filtered_df = df[df.ItemID.isin(items_in_price_range)] ` Any better way to do this?", "q_apis": "items items all where item first step get groupby mean reset_index first step any groupby agg agg count agg mean filter filter items isin", "io": " \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Item \u2551 Day \u2551 Price \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 A \u2551 1 \u2551 10 \u2551 \u2551 B \u2551 1 \u2551 20 \u2551 \u2551 C \u2551 1 \u2551 30 \u2551 \u2551 D \u2551 1 \u2551 40 \u2551 \u2551 A \u2551 2 \u2551 100 \u2551 \u2551 B \u2551 2 \u2551 20 \u2551 \u2551 C \u2551 2 \u2551 30 \u2551 \u2551 D \u2551 2 \u2551 40 \u2551 \u2551 A \u2551 3 \u2551 500 \u2551 \u2551 B \u2551 3 \u2551 25 \u2551 \u2551 C \u2551 3 \u2551 35 \u2551 \u2551 D \u2551 3 \u2551 1000 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d <s> \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Item \u2551 Day \u2551 Price \u2551 \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563 \u2551 B \u2551 1 \u2551 20 \u2551 \u2551 C \u2551 1 \u2551 30 \u2551 \u2551 B \u2551 2 \u2551 20 \u2551 \u2551 C \u2551 2 \u2551 30 \u2551 \u2551 B \u2551 3 \u2551 25 \u2551 \u2551 C \u2551 3 \u2551 35 \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d", "apis": "groupby transform mean groupby filter mean", "code": ["avg_prices_df = df[df.groupby('Item')['Price'].transform('mean') < 200]\n", "avg_prices_df = df.groupby('Item').filter(lambda x: x['Price'].mean() < 200)\n"], "link": "https://stackoverflow.com/questions/59233282/jupyter-pandas-dropping-items-which-have-average-over-a-threshold"}
{"id": 346, "q": "How to replace the value of a dataframe column with the value of another column using groupby.first()?", "d": "I have a df like this: I want to check the first of every Year-Month. If it's < 0, I want value2 to replace with value1. How can I do that? In this example, the result should be: Because only first are negative, first are positive, just leave it. I used: it doesn't seem to work. Thanks.", "q_apis": "replace value value groupby first first replace first first", "io": " Value1 Value2 2008-01-01 -1 4 2008-01-01 -1 5 2008-01-03 -1 6 2008-02-25 0 7 2008-02-26 -1 8 2008-02-27 0 9 2008-03-02 5 10 2008-03-16 -1 11 2008-03-17 -1 12 2009-04-04 -1 13 2009-04-07 0 14 <s> Value1 Value2 2008-01-01 -1 -1 2008-01-01 -1 5 2008-01-03 -1 6 2008-02-25 0 7 2008-02-26 -1 8 2008-02-27 0 9 2008-03-02 5 10 2008-03-16 -1 11 2008-03-17 -1 12 2009-04-04 -1 -1 2009-04-07 0 14 ", "apis": "groupby index to_period head loc lt index", "code": ["s = df.groupby(df.index.to_period('M'), as_index=False).head(1)\ndf.loc[s[s['Value1'].lt(0)].index, 'Value1'] = df['Value2']\n"], "link": "https://stackoverflow.com/questions/60305830/how-to-replace-the-value-of-a-dataframe-column-with-the-value-of-another-column"}
{"id": 642, "q": "Sum values in third column while putting together corespondinng values in first and second columns", "d": "I have 3 columns of data. I have data stored in three columns (k, v, t) in csv. For instance, Data: I want to get as the following data. Basically, sum all the values of t that has the same k and v. this is the code I have so far: and it keeps going until the end. I use \"for loop\" and \"if\" but it is too long. Can I use numpy in a short and clean way? or any other better way?", "q_apis": "values values first second columns columns columns get sum all values any", "io": "k v t a 1 2 b 2 3 c 3 4 a 2 3 b 3 2 b 3 4 c 3 5 b 2 3 <s> a 1 5 b 2 6 b 3 6 c 3 9 ", "apis": "read_csv groupby sum", "code": ["df = pd.read_csv('file.csv')\n\ng = df.groupby(['k', 'v'], as_index=False)['t'].sum()\n"], "link": "https://stackoverflow.com/questions/49073547/sum-values-in-third-column-while-putting-together-corespondinng-values-in-first"}
{"id": 106, "q": "Transpose dataframe based on column list", "d": "I have a dataframe in the following structure: I would like to transpose - create columns from the names in cNames. But I can't manage to achieve this with transpose because I want a column for each value in the list. The needed output: How can I achieve this result? Thanks! The code to create the DF:", "q_apis": "transpose columns names transpose value", "io": "cNames | cValues | number [a,b,c] | [1,2,3] | 10 [a,b,d] | [55,66,77]| 20 <s> a | b | c | d | number 1 | 2 | 3 | NaN | 10 55 | 66 | NaN | 77 | 20 ", "apis": "concat Series name iterrows T join iloc DataFrame iterrows T join iloc DataFrame iterrows T join iloc mean std concat Series name iterrows T join iloc mean std apply concat apply Series mean std apply Series explode set_index append unstack reset_index drop mean std set_index apply explode reset_index astype pivot_table index values columns droplevel reset_index mean std explode explode to_numeric pivot_table columns index values mean std", "code": ["pd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n", "pd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n", "%%timeit\npd.DataFrame({idx: dict(zip(x['cNames'], x['cValues']) )\n              for idx, x in df.iterrows()\n            }).T.join(df.iloc[:,2:])\n1.29 ms \u00b1 36.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "%%timeit\npd.concat([pd.Series(x['cValues'], x['cNames'], name=idx) \n               for idx, x in df.iterrows()], \n          axis=1\n         ).T.join(df.iloc[:,2:])\n2.03 ms \u00b1 86.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) \n", "%%timeit\ndf['series'] = df.apply(lambda x: dict(zip(x['cNames'], x['cValues'])), axis=1)\npd.concat([df['number'], df['series'].apply(pd.Series)], axis=1)\n\n2.09 ms \u00b1 65.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "%%timeit\ndf.apply(pd.Series.explode)\\\n  .set_index(['number', 'cNames'], append=True)['cValues']\\\n  .unstack()\\\n  .reset_index()\\\n  .drop('level_0', axis=1)\n\n4.9 ms \u00b1 135 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "%%timeit\ng=df.set_index('number').apply(lambda x: x.explode()).reset_index()\ng['cValues']=g['cValues'].astype(int)\npd.pivot_table(g, index=[\"number\"],values=[\"cValues\"],columns=[\"cNames\"]).droplevel(0, axis=1).reset_index()\n\n7.27 ms \u00b1 162 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "%%timeit\ndf1 = df.explode('cNames').explode('cValues')\ndf1['cValues'] = pd.to_numeric(df1['cValues'])\ndf1.pivot_table(columns='cNames',index='number',values='cValues')\n\n9.42 ms \u00b1 189 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/66070517/transpose-dataframe-based-on-column-list"}
{"id": 98, "q": "Is there a way to apply a condition while using apply and lambda in a DataFrame?", "d": "I have a Pandas dataframe that looks like this: And I'm looking for a way to iter trough the Dyn column, generating another one that sums only the numbers that are bigger than a cutoff, i.e.: 0.150, assigning all the values that pass it a value of one. This is what the expected result should look like: I thought I could use apply, while ittering trough all of the rows: But I'm lost on how to apply the condition (only sum it if it's greater than 0.150) to all the values inside 'Dyn' and how to assign the value of 1 to them. All advice is accepted. Thanks!", "q_apis": "apply apply DataFrame all values value apply all apply sum all values assign value", "io": " ID Dyn 0 AA01 0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1 1 BG54 0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200 <s> ID Dyn Sum 0 AA01 0.084, 0.049, 0.016, -0.003, 0, 0.025, 0.954, 1 2 1 BG54 0.216, 0.201, 0.174, 0.175, 0.179, 0.191, 0.200 7 ", "apis": "assign sum DataFrame explode sum sum sum astype values groupby sum sum gt groupby sum sum reset_index", "code": ["#Create temp column to hold Dyn convereted into list\ndf=df.assign(sum=df['Dyn'].str.split(','))\n\n#Explode DataFrame\ndf=df.explode('sum')\n#Convert to float\ndf['sum']=df['sum'].astype(float)\n#Filter out values greater that 0.015, groupby and sum\ndf[df['sum'].gt(0.150)].groupby(['ID','Dyn'])['sum'].sum().reset_index()\n"], "link": "https://stackoverflow.com/questions/66146761/is-there-a-way-to-apply-a-condition-while-using-apply-and-lambda-in-a-dataframe"}
{"id": 159, "q": "Process pandas group efficiently", "d": "I have a dataframe df with columns a,b,c,d and e. What I want is, group by df on the basis of a,b and c. And tthen for each group I want to remove NULL value of column d and e with most frequent value of that column in that group. And then finally drop duplicates for each group. I am doing the following procesing: But the iteration is making my processing really very slow. Can someone suggest me better way to do it? Sample input: Sample output:", "q_apis": "columns value value drop", "io": "a b c d e a1 b1 c1 NULL e2 a2 b2 c2 NULL NULL a2 b2 c2 NULL NULL a1 b1 c3 d4 e4 a1 b1 c1 NULL e2 a1 b1 c1 d1 e2 a1 b1 c1 d1 NULL <s> a b c d e a1 b1 c1 d1 e2 a2 b2 c2 NULL NULL a1 b1 c3 d4 e4 ", "apis": "mode iloc groupby agg", "code": ["def get_mode(series):\n    out = series.mode()\n    return out.iloc[0] if len(out) else np.nan\n\ndf.groupby(['a','b','c'], as_index=False, sort=False).agg(get_mode)\n"], "link": "https://stackoverflow.com/questions/64904669/process-pandas-group-efficiently"}
{"id": 227, "q": "Change the value of column based on quantity of equals rows", "d": "I have a dataframe like this: I need to change the value of column to 1 if value of row equals the actual quantity of rows, where columns and are equals (row0 and row1 in this example). Desired output:", "q_apis": "value equals value value equals where columns equals", "io": " id desc quantity 0 B668441DE83B Car 2 1 B668441DE83B Car 2 2 B668441DE83B Bus 1 3 89C26DEE41E2 Bus 3 4 89C26DEE41E2 Bus 3 <s> id desc quantity 0 B668441DE83B Car 1 1 B668441DE83B Car 1 2 B668441DE83B Bus 1 3 89C26DEE41E2 Bus 3 4 89C26DEE41E2 Bus 3 ", "apis": "mask groupby transform size eq loc mask", "code": ["mask = df.groupby(['id','desc'])['id'].transform('size').eq(df['quantity'])\n\ndf.loc[mask, 'quantity'] = 1\n"], "link": "https://stackoverflow.com/questions/63595559/change-the-value-of-column-based-on-quantity-of-equals-rows"}
{"id": 440, "q": "How to rearrange/reorder the rows and columns in python dataframe?", "d": "SCREEN SHOT OF ACTUAL DATA FRAMEDataframe of 5000 rows and 192 columns I want to change the size of my data frame of m rows and n columns (m= 5000 and n = 192) into a size of n/3 rows(64 rows) and m*5000 columns(15000 columns)?? existing data frame DESIRED data frame", "q_apis": "columns columns size columns size columns columns", "io": "0 A1 A2 A3 A4 A5 A6 A7 A8 A9.....A192 1 B1 B2 B3 B4 B5 B6 B7 B8 B9.....B192 . . . 5000 192 X1 X2 X3 X4 X5 X6 X7 X8 X9.....X192 <s> 0 A1 A2 A3 B1 B2 B3.....X1 X2 X3 1 A4 A5 A6 B4 B5 B6.....X4 X5 X6 2 A7 A8 A9 B7 B8 B9.....X7 X8 X9 . . 64 A190 A191 A192 B190 B191 B192.....X190 X191 X192 ", "apis": "DataFrame iloc values", "code": ["pd.DataFrame([df.iloc[:, e:e+3].values.flatten() for e in range(0, 192, 3)])\n"], "link": "https://stackoverflow.com/questions/57479385/how-to-rearrange-reorder-the-rows-and-columns-in-python-dataframe"}
{"id": 347, "q": "python dataframe merge columns according to other column values", "d": "What I want to do is merge columns according to values in another column It is better illustrated with a simple example: I have a dataframe with 5 columns: I want to get the following table: where the columns are filled with values from team_1.x and team_1.y for rows of players with number less than 5 and values from team_2.x and team_2.y for rows of players with number bigger than 5", "q_apis": "merge columns values merge columns values columns get where columns values values", "io": "| player_num | team_1.x | team_1.y | team_2.x | team_2.y | |------------ |---------- |---------- |---------- |---------- | | 1 | x_1 | y_1 | x_2 | y_2 | | 4 | x_3 | y_3 | x_4 | y_4 | | 8 | x_5 | y_5 | x_6 | y_6 | <s> | x | y | |----- |----- | | x_1 | y_1 | | x_3 | y_3 | | x_6 | y_6 | ", "apis": "where where", "code": ["import numpy as np\n...\ndf['x'] = np.where(df['player_num'] < 5, df['team_1.x'], df['team_2.x'])\ndf['y'] = np.where(df['player_num'] < 5, df['team_1.y'], df['team_2.y'])\n"], "link": "https://stackoverflow.com/questions/60279369/python-dataframe-merge-columns-according-to-other-column-values"}
{"id": 424, "q": "pandas create a column and assign values to it from a dictionary", "d": "I have a dictionary looks like this, I have a df looks like this, I like to create a column in whose values will be based on the values in , so the result will look like, I am wondering whats the best way to do this.", "q_apis": "assign values values values", "io": "id code 1 SA01 2 SA02 3 SA03 4 AP01 5 AP02 6 AP03 <s> id code region 1 SA01 South America 2 SA02 South America 3 SA03 South America 4 AP01 Asia Pacific 5 AP02 Asia Pacific 6 AP03 Asia Pacific ", "apis": "name map", "code": ["d_ = {code:sd['name'] for sd in d['regions'] for code in sd['code'].split(',')} \n# {'SA01': 'South America', 'SA02': 'South America', 'SA03': 'South America',...\ndf['region'] = df.code.map(d_)\n"], "link": "https://stackoverflow.com/questions/58085046/pandas-create-a-column-and-assign-values-to-it-from-a-dictionary"}
{"id": 30, "q": "Element wise numeric comparison in Pandas dataframe column value with list", "d": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code:", "q_apis": "value value value compare min max value", "io": " | A | B | C | | Val | Val | Val | |---------------------|-----------------------|--------------------| 0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] | 1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] | 2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] | <s> | A | B | C | | Max | Max | Max | |-------|-------|------| 0 | 28.68 | 18.42 | 1.37 | 1 | 29.50 | 17.31 | 1.47 | 2 | 29.87 | 20.45 | 1.39 | ", "apis": "name IndexSlice to_numpy IndexSlice to_numpy array name IndexSlice to_numpy IndexSlice to_numpy array apply rename columns", "code": ["def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n", "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"], "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list"}
{"id": 601, "q": "Combination of two dataframes without duplicate and reversion in efficient way | python", "d": "I have two dataframes with thousands of rows, I need to combine both into one dataframe without duplicate and reversion. for example: Dataframe 1 Dataframe 2 So, the output dataframe will be: output-dataframe I don't want the output combination containing something like: I actually try it using but it return duplicate and reversion and also took long time because I have thousands in Dataframes 1 and 2 Any help please ?", "q_apis": "combine time", "io": "drug1 disease1 drug1 disease2 drug1 disease3 drug2 disease1 drug2 disease2 drug2 disease3 drug3 disease1 drug3 disease2 drug3 disease3 <s> disease1 drug1 drug1 drug1 disease1 disease1 ", "apis": "DataFrame merge merge drop", "code": ["from pandas import DataFrame, merge\n\ndf1['key'] = 1\ndf2['key'] = 1\n\nresult = df1.merge(df2, on='key').drop('key', axis=1)\n"], "link": "https://stackoverflow.com/questions/51734814/combination-of-two-dataframes-without-duplicate-and-reversion-in-efficient-way"}
{"id": 313, "q": "how to split a nested dictionary inside a column of a dataframe into new rows?", "d": "I have a dataframe : I need to split col3 into new rows: expected output dataframe : This doesnt seem to work :", "q_apis": "", "io": "Col1 Col2 Col3 01 ABC {'link':'http://smthing1} 02 DEF {'link':'http://smthing2} <s> Col1 Col2 Col3 01 ABC 'http://smthing1' 02 DEF 'http://smthing2' ", "apis": "apply get", "code": ["#converting to dicts\n#import ast\n#df['Col3'] = df['Col3'].apply(ast.literal_eval)\ndf['Col3'] = df['Col3'].str.get('link')\n"], "link": "https://stackoverflow.com/questions/61117957/how-to-split-a-nested-dictionary-inside-a-column-of-a-dataframe-into-new-rows"}
{"id": 56, "q": "Match multiple columns on Python to a single value", "d": "I hope you are doing well. I am trying to perform a match based on multiple columns where my values of Column B of df1 is scattered in three to four columns in df2. The goal here is the the return the values of Column A of df2 if values of Column B matches any values in the columns C,D,E. What I did until now was actually to do multiple left merges (and changing the name of Column B to match the name of columns C,D,E of df2). I am trying to simplify the process but I am unsure how I am supposed to do this? My dataset looks like that: Df1: DF2: My goal is to have in df1: Thank you very much !", "q_apis": "columns value columns where values columns values values any values columns now left name name columns", "io": " ID 0 77 1 4859 2 LSP <s> ID X 0 77 AAAAA_XX 1 4859 BBBBB_XX 2 LSP CCCC_YY ", "apis": "concat reset_index merge left iloc rename columns", "code": ["df3 = pd.concat([df2.id1, df2.id2]).reset_index()\ndf1 = df2.merge(df3, how=\"left\", left_on = df1.ID, right_on = df3[0])\ndf1 = df1.iloc[:, :2]\ndf1 = df1.rename(columns={\"key_0\": \"ID\"})\n"], "link": "https://stackoverflow.com/questions/67087432/match-multiple-columns-on-python-to-a-single-value"}
{"id": 482, "q": "Function on dataframe rows to reduce duplicate pairs Python", "d": "I've got a dataframe that looks like: Each 'layer'/row has pairs that are duplicates that I want to reduce. The one problem is that there are repeating 0s as well so I cannot just simply remove duplicates per row or it will leave an uneven number of rows. My desired output would be a lambda function that I could apply to all rows of this dataframe to get this: Is there a simple function I could write to do this?", "q_apis": "apply all get", "io": "0 1 2 3 4 5 6 7 8 9 10 11 12 13 13 13.4 13.4 12.4 12.4 16 0 0 0 0 14 12.2 12.2 13.4 13.4 12.6 12.6 19 5 5 6.7 6.7 . . . <s> 0 1 2 3 4 5 6 12 13 13.4 12.4 16 0 0 14 12.2 13.4 12.6 19 5 6.7 . . . ", "apis": "columns iloc columns columns", "code": ["idxcols = [x-1 for x in range(len(df.columns)) if x % 2]\n\ndf = df.iloc[:, idxcols]\n\ndf.columns = range(len(df.columns))\n"], "link": "https://stackoverflow.com/questions/56044354/function-on-dataframe-rows-to-reduce-duplicate-pairs-python"}
{"id": 608, "q": "How to map new variable in pandas in effective way", "d": "Here's my data What I need, is to map : if is more than , is . But,if is less than , is What I did It works, but not highly configurable and not effective.", "q_apis": "map map", "io": "Id Amount 1 6 2 2 3 0 4 6 <s> Id Amount Map 1 6 1 2 2 0 3 0 0 4 5 1 ", "apis": "columns concat values astype mean std astype mean std", "code": ["#[400000 rows x 3 columns]\ndf = pd.concat([df] * 100000, ignore_index=True)\n\nIn [133]: %timeit df['Map'] = (df['Amount'].values >= 3).astype(int)\n2.44 ms \u00b1 97.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [134]: %timeit df['Map'] = (df['Amount'] >= 3).astype(int)\n2.6 ms \u00b1 66.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/51374862/how-to-map-new-variable-in-pandas-in-effective-way"}
{"id": 383, "q": "Symmetrical column values in pandas data frame", "d": "I have one set of variable as in below data frame: Another set of variable in below data frame: 1st columns are index columns. I want to add each row (v1+v2) to get v3. How do I make the index column values (0 to 4) and (41 to 45) symmetrical ( either 0-4) or (42-45) in both data fame? I am working on pandas (python) jupyter notebook.", "q_apis": "values columns index columns add get index values", "io": " v1 ---------- 0 0.036286 1 -0.018490 2 0.011699 3 0.028955 4 -0.000373 <s> v2 ---------- 41 12.31 42 12.20 43 12.12 44 12.31 45 12.47 ", "apis": "concat sum sum values values", "code": ["df = pd.concat(df1, df2, axis=1, ignore_index=True)\ndf['v3'] = df.sum(axis=1) # or df[['v1','v2']].sum(axis=1)\n", "v3 = df1['v1'].values + df2['v2'].values\n"], "link": "https://stackoverflow.com/questions/59345178/symmetrical-column-values-in-pandas-data-frame"}
{"id": 45, "q": "Checking if column headers match PYTHON", "d": "I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python?", "q_apis": "columns get any", "io": " ID Open High Low 1 64 66 52 <s> ID Open High Volume 1 33 45 30043 ", "apis": "DataFrame DataFrame columns columns", "code": ["import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [64],\n        \"High\": [66],\n        \"Low\": [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [33],\n        \"High\": [45],\n        \"Volume\": [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns & df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(\"Columns only available in df1\", df1_columns_only)\nprint(\"Columns only available in df2\", df2_columns_only)\n", "Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n"], "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python"}
{"id": 266, "q": "position or move pandas column to a specific column index", "d": "I have a DF and it has multiple columns (over 75 columns) with default numeric index: I need to arrange/change position to as follows: I can get the index of using: but I don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list.", "q_apis": "index columns columns index get index all columns", "io": "Col1 Col2 Col3 ... Coln <s> Col1 Col3 Col2 ... Coln ", "apis": "index index", "code": ["l = list(df)\n\ni1, i2 = l.index('Col2'), l.index('Col3')\nl[i2], l[i1] = l[i1], l[i2]\n\ndf = df[l]\n"], "link": "https://stackoverflow.com/questions/56044461/position-or-move-pandas-column-to-a-specific-column-index"}
{"id": 60, "q": "How to convert this DataFrame into Json", "d": "I have this with 2 columns when I try to convert it into it goes wrong: I don't even know ehere the numbers come from. My desired :", "q_apis": "DataFrame columns", "io": "print(df) a b 10 {'A': 'foo', ...} 20 {'B': 'faa', ...} 30 {'C': 'fee', ...} 40 {'D': 'fii', ...} 50 {'E': 'foo', ...} <s> [{ 'a': 10, 'b': { 'A': 'foo', ... }, ... 'a': 50, 'b': { 'E': 'foo', ... } } ] ", "apis": "append", "code": ["data = []\nfor i in df:\n    data.append({'a': df[i[0]], 'b': df(i[1])})\n", "with open(\"myjson.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n"], "link": "https://stackoverflow.com/questions/66991966/how-to-convert-this-dataframe-into-json"}
{"id": 475, "q": "How to apply a method to a Pandas Dataframe", "d": "I have this dataframe I would like to convert it to I know how to create a dataframe (with indexes) for 1 column, but not for multiple columns This code produces this result how can I amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally I wouln't want to have to enter the same code for every column)", "q_apis": "apply columns add", "io": " Col1 Col2 0 A (1000 EUR) C ( 3000 USD) 1 B (2000 CHF) D ( 4000 GBP) <s> Col1 Col2 0 1000 3000 1 2000 4000 ", "apis": "DataFrame apply apply all columns apply apply to_numeric", "code": ["# creates your dataframe\ndf = pd.DataFrame({'Col1':['A (1000 EUR)','B (2000 CHF)'], 'Col2':['C (3000 USD)', 'D (4000 GBP)']})\n\n# use the apply function to  apply your code to all elements of both columns\ndf = df.apply(lambda x: x.str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce'))\n"], "link": "https://stackoverflow.com/questions/56328404/how-to-apply-a-method-to-a-pandas-dataframe"}
{"id": 619, "q": "How to group phone number with and without country code", "d": "I am trying to detect phone number, my country code is but some phone manufacturer or operator use and , after query and pivoting I get pivoted data. But, the pivoted data is out of context Here's the pivoted data Here's what I need to group, but I don't want to group manually ( and is same, etc)", "q_apis": "query get", "io": "Id +623684682 03684682 +623684684 03684684 1 1 0 1 1 2 1 1 2 1 <s> Id 03684682 03684684 1 1 2 2 2 3 ", "apis": "groupby replace sum columns columns replace sum", "code": ["df = df.groupby(lambda x: x.replace('+62','0'), axis=1).sum()\n", "df.columns = df.columns.str.replace('\\+62','0')\ndf = df.sum(level=0, axis=1)\n"], "link": "https://stackoverflow.com/questions/51018104/how-to-group-phone-number-with-and-without-country-code"}
{"id": 235, "q": "Python: Append 2 columns of a dataframe together", "d": "I am loading a csv file into a data frame using pandas. My dataframe looks something like this: I wish to append 2 of the columns into a new column: col4 needs to be created by appending the contents of col1 and col2 together. How can I do this in pandas/python? EDIT", "q_apis": "columns append columns", "io": "col1 col2 col3 1 4 1 2 5 2 3 6 3 <s> col1 col2 col3 col4 1 4 1 1 2 5 2 2 3 6 3 3 4 5 6 ", "apis": "append rename concat rename join concat reset_index drop append rename append rename concat", "code": ["s = df['col1'].append(df['col2'], ignore_index=True).rename('col4')\n#alternative\n#s = pd.concat([df['col1'], df['col2']], ignore_index=True).rename('col4')\n\ndf1 = df.join(s, how='outer')\n#alternative\n#df1 = pd.concat([df, s], axis=1)\n", "df = df.reset_index(drop=True)\n\ns1 = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ns2 = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\n\ndf1 = pd.concat([df, s1, s2], axis=1)\n"], "link": "https://stackoverflow.com/questions/63264777/python-append-2-columns-of-a-dataframe-together"}
{"id": 26, "q": "Replicating the DataFrame row in a special manner", "d": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output:", "q_apis": "DataFrame", "io": "col1 mob_no col3 a 9382949201/3245622535 45 b 8383459345/4325562678 67 c 8976247543/1827472398 89 d 7844329432 09 <s> col1 mob_no col3 a 9382949201 45 a 3245622535 45 b 8383459345 67 b 4325562678 67 c 8976247543 89 c 1827472398 89 d 7844329432 09 ", "apis": "explode", "code": ["df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"], "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner"}
{"id": 209, "q": "Get &quot;Last Purchase Year&quot; from Sales Data Pivot in Pandas", "d": "I have pivoted the Customer ID against their year of purchase, so that I know how many times each customer purchased in different years: My desired result is to append the column names with the latest year of purchase, and thus the number of years since their last purchase: Here is what I tried: However what I got is \"TypeError: cannot convert the series to <class 'float'>\" Could anyone help me to get the result I need? Thanks a lot! Dennis", "q_apis": "year append names year last get", "io": "Customer ID 1996 1997 ... 2019 2020 100000000000001 7 7 ... NaN NaN 100000000000002 8 8 ... NaN NaN 100000000000003 7 4 ... NaN NaN 100000000000004 NaN NaN ... 21 24 100000000000005 17 11 ... 18 NaN <s> Customer ID 1996 1997 ... 2019 2020 Last Recency 100000000000001 7 7 ... NaN NaN 1997 23 100000000000002 8 8 ... NaN NaN 1997 23 100000000000003 7 4 ... NaN NaN 1997 23 100000000000004 NaN NaN ... 21 24 2020 0 100000000000005 17 11 ... 18 NaN 2019 1 ", "apis": "filter columns notna cumsum idxmax max", "code": ["c = df.filter(regex=r'\\d+').columns\ndf['Last'] = df[c].notna().cumsum(1).idxmax(1)\ndf['Recency'] = c.max() - df['Last']\n"], "link": "https://stackoverflow.com/questions/64032701/get-last-purchase-year-from-sales-data-pivot-in-pandas"}
{"id": 417, "q": "How to split a string in a column within a pandas dataframe?", "d": "This is an example of the file I have, So, in the column 'Name', where '_EN' is present, I want to remove the '_EN' part. The output should be as follows: This is what I was trying: However, this is not working. What is a good way to do this?", "q_apis": "where", "io": "Name Att1 Att2 Att3 AB_EN 1 2 3 CD 5 6 7 FG_EN 7 8 9 <s> Name Att1 Att2 Att3 AB 1 2 3 CD 5 6 7 FG 7 8 9 ", "apis": "DataFrame", "code": ["df = pd.DataFrame({\"Name\": [\"AB_EN\", \"CD\", \"FG_EN\"]})\ndf['Name'] = df['Name'].str.split(\"_\").str[0]\nprint(df)\n"], "link": "https://stackoverflow.com/questions/58321988/how-to-split-a-string-in-a-column-within-a-pandas-dataframe"}
{"id": 362, "q": "Sort a pandas DataFrame by a column in another dataframe - pandas", "d": "Let's say I have a Pandas DataFrame with two columns, like: And let's say I also have a Pandas Series, like: How can I sort the column to become the same order as the series, with the corresponding row values sorted together? My desired output would be: Is there any way to achieve this? Please check self-answer below.", "q_apis": "DataFrame DataFrame columns Series values any", "io": " a b 0 1 100 1 2 200 2 3 300 3 4 400 <s> a b 0 1 100 1 3 300 2 2 200 3 4 400 ", "apis": "DataFrame values index columns columns DataFrame Series set_index reindex rename_axis reset_index set_index loc reset_index iloc map index DataFrame values index columns columns", "code": ["print(pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns))\n", "import pandas as pd\nfrom timeit import timeit\ndf = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [100, 200, 300, 400]})\ns = pd.Series([1, 3, 2, 4])\ndef u10_1():\n    return df.set_index('a').reindex(s).rename_axis('a').reset_index('a')\ndef u10_2():\n    return df.set_index('a').loc[s].reset_index()\ndef u10_3():\n    return df.iloc[list(map(df['a'].tolist().index, s))]\ndef u10_4():\n    return pd.DataFrame(sorted(df.values.tolist(), key=lambda x: s.tolist().index(x[0])), columns=df.columns)\nprint('u10_1:', timeit(u10_1, number=1000))\nprint('u10_2:', timeit(u10_2, number=1000))\nprint('u10_3:', timeit(u10_3, number=1000))\nprint('u10_4:', timeit(u10_4, number=1000))\n"], "link": "https://stackoverflow.com/questions/59925197/sort-a-pandas-dataframe-by-a-column-in-another-dataframe-pandas"}
{"id": 7, "q": "How to combine rows in a dataframe in a pairwise fashion while applying some function", "d": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code:", "q_apis": "combine keys values combine left", "io": "ID Val1 Val2 id0 10 20 id0 11 19 id1 5 5 id1 1 1 id1 2 4 <s> ID Val1 Val2 id0_1 10.5 19.5 id1_1 3 3 id1_2 1.5 2.5 ", "apis": "groupby rolling mean dropna all reset_index drop index groupby cumcount add astype", "code": ["out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"], "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun"}
{"id": 254, "q": "How to split a DataFrame on each different value in a column?", "d": "Below is an example DataFrame. I want to split this into new dataframes when the row in column 0 changes. I've tried adapting the following solutions without any luck so far. Split array at value in numpy Split a large pandas dataframe", "q_apis": "DataFrame value DataFrame any array at value", "io": " 0 1 2 3 4 0 0.0 13.00 4.50 30.0 0.0,13.0 1 0.0 13.00 4.75 30.0 0.0,13.0 2 0.0 13.00 5.00 30.0 0.0,13.0 3 0.0 13.00 5.25 30.0 0.0,13.0 4 0.0 13.00 5.50 30.0 0.0,13.0 5 0.0 13.00 5.75 0.0 0.0,13.0 6 0.0 13.00 6.00 30.0 0.0,13.0 7 1.0 13.25 0.00 30.0 0.0,13.25 8 1.0 13.25 0.25 0.0 0.0,13.25 9 1.0 13.25 0.50 30.0 0.0,13.25 10 1.0 13.25 0.75 30.0 0.0,13.25 11 2.0 13.25 1.00 30.0 0.0,13.25 12 2.0 13.25 1.25 30.0 0.0,13.25 13 2.0 13.25 1.50 30.0 0.0,13.25 14 2.0 13.25 1.75 30.0 0.0,13.25 15 2.0 13.25 2.00 30.0 0.0,13.25 16 2.0 13.25 2.25 30.0 0.0,13.25 <s> 0 1 2 3 4 0 0.0 13.00 4.50 30.0 0.0,13.0 1 0.0 13.00 4.75 30.0 0.0,13.0 2 0.0 13.00 5.00 30.0 0.0,13.0 3 0.0 13.00 5.25 30.0 0.0,13.0 4 0.0 13.00 5.50 30.0 0.0,13.0 5 0.0 13.00 5.75 0.0 0.0,13.0 6 0.0 13.00 6.00 30.0 0.0,13.0 7 1.0 13.25 0.00 30.0 0.0,13.25 8 1.0 13.25 0.25 0.0 0.0,13.25 9 1.0 13.25 0.50 30.0 0.0,13.25 10 1.0 13.25 0.75 30.0 0.0,13.25 11 2.0 13.25 1.00 30.0 0.0,13.25 12 2.0 13.25 1.25 30.0 0.0,13.25 13 2.0 13.25 1.50 30.0 0.0,13.25 14 2.0 13.25 1.75 30.0 0.0,13.25 15 2.0 13.25 2.00 30.0 0.0,13.25 16 2.0 13.25 2.25 30.0 0.0,13.25 ", "apis": "groupby", "code": ["out = [sub_df for _, sub_df in df.groupby(0)]\n"], "link": "https://stackoverflow.com/questions/59649084/how-to-split-a-dataframe-on-each-different-value-in-a-column"}
{"id": 463, "q": "Is there a way to use previous row value in pandas&#39; apply function when previous value is iteratively summed ?or an efficient way?", "d": "I have a dataframe with some columns and I would like to apply the following transformation in an efficient manner. Given the Dataframe below: It should be transformed in such a way I can get the following output: Note that: C[i] = C[i] + C[i - 1] + ... + C[0] and D[i] = D[i] + C[i - 1] NaN values should be filtered. Thx!", "q_apis": "value apply value columns apply get values", "io": " C D =========== Nan 10 0 22 2 280 4 250 6 270 <s> C D =========== Nan 10 0 22 2 280 6 252 12 276 ", "apis": "cumsum add shift fillna", "code": ["df['C'] = df['C'].cumsum()\ndf['D'] = df['D'].add(df['C'].shift(1).fillna(0))\n"], "link": "https://stackoverflow.com/questions/56738691/is-there-a-way-to-use-previous-row-value-in-pandas-apply-function-when-previous"}
{"id": 75, "q": "How to transform rows of other columns to columns on the basis of unique values of a column?", "d": "Suppose I have a df in the following structure, relation between column1 to column2 - one to many relation between column2 to column1 - one to many Expected Output: Also, while transforming, for every column7 can I create an empty column right beside column6_yyyymm? Final Output, How can I achieve Final Output using a python function and/or pandas library? If there is anything unclear please let me know. UPDATE: For all empty_yyyymm columns I want to implement the following function, How can achieve this too? Note: yyyymm is generic way of referring column7. It is not actually a column.", "q_apis": "transform columns columns unique values between between empty right all columns", "io": "column1 | column2 | column3 | column4 | column5 | column6 | column7 A | B | C | 10 | 78 | 12 | 202001 A | B | D | 21 | 64 | 87 | 202001 A | B | E | 21 | 64 | 87 | 202001 X | K | C | 54 | 23 | 23 | 202001 X | K | D | 21 | 55 | 87 | 202001 X | K | E | 21 | 43 | 22 | 202001 A | B | C | 10 | 78 | 12 | 202002 A | B | D | 23 | 64 | 87 | 202002 A | B | E | 21 | 11 | 34 | 202002 Z | K | C | 10 | 78 | 12 | 202002 Z | K | D | 21 | 13 | 56 | 202002 Z | K | E | 12 | 77 | 34 | 202002 <s> column1 | column2 | column3 | column4_202001 | column5_202001 | column6_202001 | column4_202002 | column5_202002 | column6_202002 | A | B | C | 10 | 78 | 12 | 10 | 78 | 12 | A | B | D | 21 | 64 | 87 | 23 | 64 | 87 | A | B | E | 21 | 64 | 87 | 21 | 11 | 34 | X | K | C | 54 | 23 | 23 | 0 | 0 | 0 | X | K | D | 21 | 55 | 87 | 0 | 0 | 0 | X | K | E | 21 | 43 | 22 | 0 | 0 | 0 | Z | K | C | 0 | 0 | 0 | 10 | 78 | 12 | Z | K | D | 0 | 0 | 0 | 21 | 13 | 56 | Z | K | E | 0 | 0 | 0 | 12 | 77 | 34 | ", "apis": "assign empty set_index unstack sort_index", "code": ["df = (df.assign(empty = np.nan)\n        .set_index(['column1','column2','column3','column7'])\n        .unstack(fill_value=0)\n        .sort_index(level=1, axis=1))\n"], "link": "https://stackoverflow.com/questions/66545100/how-to-transform-rows-of-other-columns-to-columns-on-the-basis-of-unique-values"}
{"id": 147, "q": "Parsing a txt file into data frame, filling columns based on the multiple separators", "d": "Having a .txt file structure as below trying to parse into dataframe of the following structure describing the rule: # i - 'i' is the row number n:data - 'n' is the column number to fill, 'data' is the value to fill into i'th row if the number of columns would be small enough it could be done manually, but txt considered has roughly 2000-3000 column values and some of them are missing. gives the following result I tried to remove the odd rows in data1 even in data2, then will hopefully figure out how to split the odd and merge the 2 df's, but there might be a faster and more beautiful method to do it, that's why asking here update, spent 3 hours figuring out how to work with dataframes, as I was not that familiar with them. now from that using It became this any suggestions on how to add unknown number of phantom columnsnd fill them using \"n:value\" from the list to fill the \"n\" column with the \"value\"?", "q_apis": "columns parse value columns values merge update now any add value value", "io": "#n 1 a 1:0.0002 3:0.0003... #n 2 b 2:0.0002 3:0.0003... #n 3 a 1:0.0002 2:0.0003... ... <s> # type 1 2 3 1 a 0.0002 null 0.0003 .... 2 b null 0.0002 0.0003 .... 3 a 0.0002 0.0003 null .... ... ", "apis": "all at time take second first value add DataFrame from_dict index reindex columns reindex index", "code": ["# read all lines\nlines = file.readlines()\n\n# here we will store the results, dictionary of dictionaries\nparsing_res = {}\n\n# a fancy way of processing two lines, odd and even, at the same time\nfor line1,line2 in zip(lines[::2],lines[1::2]):\n    # line1 has the form '#n  1', we split on whitespace and take the second tokem\n    row_index = line1.split()[1]\n    # line2 is the other type of lines, split into tokens by whitespace\n    tokens = line2.split()\n    # first one is 'type'\n    t = tokens[0]\n\n    # the others are pairs 'x:y', split them into x,y and stick into a dictionary with label x and value y\n    row_dict = {token.split(':')[0]:token.split(':')[1] for token in tokens[1:]}\n\n    # add type\n    row_dict['type'] = t\n   \n    # store the result for these two lines into the main dictionary\n    parsing_res[row_index] = row_dict\nparsing_res\n", "df = pd.DataFrame.from_dict(parsing_res, orient='index')\ndf.reindex(sorted(df.columns), axis=1).reindex(sorted(df.index), axis=0)\n"], "link": "https://stackoverflow.com/questions/65169011/parsing-a-txt-file-into-data-frame-filling-columns-based-on-the-multiple-separa"}
{"id": 398, "q": "pandas documentation example for append does not work (pandas.DataFrame.append)", "d": "I copied the example from the pandas documentation for the append method, but it isn't working for me. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html outputs: and not: What are possible reasons for this? Where is my mistake? Thanks for your help.", "q_apis": "append DataFrame append append DataFrame append", "io": " A B 0 1 2 1 3 4 <s> A B 0 1 2 1 3 4 0 5 6 1 7 8 ", "apis": "DataFrame columns DataFrame columns append", "code": ["df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n\ndf = df.append(df2)\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/58942218/pandas-documentation-example-for-append-does-not-work-pandas-dataframe-append"}
{"id": 564, "q": "How to drop rows with respect to a column values in Python?", "d": "I wantto remove rows with respect column values. df Here the list of value those I want to remove. I want to output like following: df How can I do this?", "q_apis": "drop values values value", "io": " ID B C D 0 101 1 2 3 1 103 5 6 7 2 108 9 10 11 3 109 5 3 12 4 118 11 15 2 5 121 2 5 6 <s> ID B C D 0 101 1 2 3 3 109 5 3 12 4 118 11 15 2 ", "apis": "isin dtype bool isin dtype bool", "code": [">>> df['ID'].isin(remove_id)\n>>> \n0    False\n1     True\n2     True\n3    False\n4    False\n5     True\nName: ID, dtype: bool\n>>> ~df['ID'].isin(remove_id)\n>>> \n0     True\n1    False\n2    False\n3     True\n4     True\n5    False\nName: ID, dtype: bool\n"], "link": "https://stackoverflow.com/questions/53252056/how-to-drop-rows-with-respect-to-a-column-values-in-python"}
{"id": 534, "q": "Report difference/change in values between two dataFrames of identical shape", "d": "The context is I want to compare two df's and find the difference. Here's df and df2 with a small difference: Comparing them yields a 2D boolean df of the same shape: I tried to extract the elements corresponding to the True's, but other elements (that I don't want) still occurs as NaN How to extract only the elements corresponding to the True's and the indices (so I know where in the df): update: the above example has only one True. In a general situation with multiple True's, I think are two cases: df is small and one may want to see: df is large and one may want to see: @U9-Forward's answer works nicely for case 1, and when there's only one True. @coldspeed provided a comprehensive solution. Thanks!", "q_apis": "difference values between identical shape compare difference difference shape indices where update", "io": "df[df != df2] Out[29]: a b 0 NaN NaN 1 NaN 1.0 2 NaN NaN <s> df[df != df2] # somehow? Out[30]: b 1 1.0 ", "apis": "values array agg dropna dtype", "code": ["df.values[df != df2]\n# array([1])\n", "df[df != df2].agg(lambda x: x.dropna().tolist())\n\na    [0.0]\nb    [1.0]\ndtype: object\n"], "link": "https://stackoverflow.com/questions/54059466/report-difference-change-in-values-between-two-dataframes-of-identical-shape"}
{"id": 251, "q": "python - pandas ffill with groupby", "d": "I am trying to forward fill the missing rows to complete the missing time-series rows in the dataset. The size of the dataset is huge. More than 100 million rows. The original source dataset is as shown below. desired output is as below I need to group on and to fill the missing time-series rows in for each of the combinations. Currently, I have the below code which is working but its extremely slow due to the for-loop. Is there any way I can avoid the for-loop and send the whole dataset for creating missing rows and ffill()? Thanks and Appreciate the help. Update: The above code is working but it's too slow. It takes more than 30 minutes for just 300k rows. Hence, I'm looking for help to make it faster and avoid the for-loop.", "q_apis": "ffill groupby time size time any ffill", "io": " col1 col2 col3 col4 col5 col6 0 2020-01-01 b1 c1 1 9 17 1 2020-01-05 b1 c1 2 10 18 2 2020-01-02 b2 c2 3 11 19 3 2020-01-04 b2 c2 4 12 20 4 2020-01-10 b3 c3 5 13 21 5 2020-01-15 b3 c3 6 14 22 6 2020-01-16 b4 c4 7 15 23 7 2020-01-30 b4 c4 8 16 24 <s> col1 col2 col3 col4 col5 col6 0 2020-01-01 b1 c1 1.0 9.0 17.0 1 2020-01-02 b1 c1 1.0 9.0 17.0 2 2020-01-03 b1 c1 1.0 9.0 17.0 3 2020-01-04 b1 c1 1.0 9.0 17.0 4 2020-01-05 b1 c1 2.0 10.0 18.0 5 2020-01-02 b2 c2 3.0 11.0 19.0 6 2020-01-03 b2 c2 3.0 11.0 19.0 7 2020-01-04 b2 c2 4.0 12.0 20.0 8 2020-01-10 b3 c3 5.0 13.0 21.0 9 2020-01-11 b3 c3 5.0 13.0 21.0 10 2020-01-12 b3 c3 5.0 13.0 21.0 11 2020-01-13 b3 c3 5.0 13.0 21.0 12 2020-01-14 b3 c3 5.0 13.0 21.0 13 2020-01-15 b3 c3 6.0 14.0 22.0 14 2020-01-16 b4 c4 7.0 15.0 23.0 15 2020-01-17 b4 c4 7.0 15.0 23.0 16 2020-01-18 b4 c4 7.0 15.0 23.0 17 2020-01-19 b4 c4 7.0 15.0 23.0 18 2020-01-20 b4 c4 7.0 15.0 23.0 19 2020-01-21 b4 c4 7.0 15.0 23.0 20 2020-01-22 b4 c4 7.0 15.0 23.0 21 2020-01-23 b4 c4 7.0 15.0 23.0 22 2020-01-24 b4 c4 7.0 15.0 23.0 23 2020-01-25 b4 c4 7.0 15.0 23.0 24 2020-01-26 b4 c4 7.0 15.0 23.0 25 2020-01-27 b4 c4 7.0 15.0 23.0 26 2020-01-28 b4 c4 7.0 15.0 23.0 27 2020-01-29 b4 c4 7.0 15.0 23.0 28 2020-01-30 b4 c4 8.0 16.0 24.0 ", "apis": "set_index groupby resample ffill reset_index drop reset_index", "code": ["(df.set_index('col1').groupby(['col2', 'col3'])\n   .resample('D').ffill()\n   .reset_index(['col2','col3'], drop=True)\n   .reset_index()\n)\n"], "link": "https://stackoverflow.com/questions/62941861/python-pandas-ffill-with-groupby"}
{"id": 165, "q": "How to replace duplicate dataframe column values with certain conditions in python", "d": "I have a dataframe with shape (10x401) having duplicate columns with same column names and values. Some of them have nulls while other have numeric values. The columns names are not in sorted order. A short example of dataframe is given below: By ignoring the null values, i need to replace each first occurrence of the numeric value (from 0 to 10) with 1 and the rest of the values with -1 for all 10 rows and 400 columns ignoring the ID column. The resulting dataframe will look like: I will be thankful for some help here.", "q_apis": "replace values shape columns names values values columns names values replace first value values all columns", "io": " ID#, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,.........,100, 100, 100, 100 1, , , , , 3, 3, 3, 3, , , , ,........., 0, 0, 0, 0 2, 0, 0, 0, 0, , , , , 10, 10, 10, 10,........., , , , 3, 9, 9, 9, 9, 1, 1, 1, 1, 4, 4, 4, 4,........., 1, 1, 1, 1 . . . 10, , , , , , , , , , , , ,........., 6, 6, 6, 6 <s> ID#, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,.........,100, 100, 100, 100 1, , , , , 1, -1, -1, -1, , , , ,........., 1, -1, -1, -1 2, 1, -1, -1, -1, , , , , 1, -1, -1, -1,........., , , , 3, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1,........., 1, -1, -1, -1 . . . 10, , , , , , , , , , , , ,........., 1, -1, -1, -1 ", "apis": "read_csv columns columns shift", "code": ["import pandas as pd\n\nfrom io import StringIO\n\ndf_string = '''\nID;1;1;1;1;2;2;2;2;3;3;3;3\n1;;;;;3;3;3;3;;;;\n2;0;0;0;0;;;;;10;10;10;10\n3;9;9;9;9;1;1;1;1;4;4;4;4\n4;;;;;;;;;6;6;6;6\n'''\n\ndf = pd.read_csv(StringIO(df_string), sep = \";\", index_col=\"ID\")\n\n# Removing the automatically added .1/.2/... suffixes. You don't need that for your data.\ndf.columns = df.columns.str[0]\n", "StartMask = (df.shift() != df) & ValueMask\n"], "link": "https://stackoverflow.com/questions/64789227/how-to-replace-duplicate-dataframe-column-values-with-certain-conditions-in-pyth"}
{"id": 403, "q": "Group identical consecutive values in pandas DataFrame", "d": "I have the following pandas dataframe : I want to store the values in another dataframe such as every group of consecutive indentical values make a labeled group like this : The column A represent the value of the group and B represents the number of occurences. this is what i've done so far: It works but it's a bit messy. Do you think of a shortest/better way of doing this ?", "q_apis": "identical values DataFrame values values value", "io": " a 0 0 1 0 2 1 3 2 4 2 5 2 6 3 7 2 8 2 9 1 <s> A B 0 0 2 1 1 1 2 2 3 3 3 1 4 2 2 5 1 1 ", "apis": "groupby ne shift cumsum agg first count groupby ne shift cumsum agg first count", "code": ["new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .agg(A=('a','first'),B=('a','count')) )\n\nprint(new_df)\n", "new_df= ( df.groupby(df['a'].ne(df['a'].shift()).cumsum(),as_index=False)\n            .a\n            .agg({'A':'first','B':'count'}) )\n"], "link": "https://stackoverflow.com/questions/58771645/group-identical-consecutive-values-in-pandas-dataframe"}
{"id": 341, "q": "Rolling over values from one column to other based on another dataframe", "d": "I have two dataframes: Now I have another dataframe which only have unique IDs from first dataframe, and dates that represent months: So based on first dataframe I need to fill the values based on the value that is in the first dataframe that is within the corresponding month ( so for example I take the last value for the from and put it in the column in . IF there are no other values for that ID just fill all the remaining columns in with the value in the most right filled column(roll over to the right). So the end result is exactly like this", "q_apis": "values unique first first values value first month take last value put values all columns value right right", "io": "ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31 A1 A2 A3 A4 A5 <s> ID 2018-01-31 2018-02-28 2018-03-31 2018-04-30 2018-05-31 2018-06-30 2018-07-31 A1 8500 8500 8500 8500 8500 8500 8500 A2 NA 1900 1900 1900 1900 1900 1900 A3 NA NA NA 3000 110 0 0 A4 NA NA NA NA NA 10 10 A5 NA NA NA NA NA NA 500 ", "apis": "to_datetime to_period to_datetime add groupby last unstack ffill reset_index rename_axis columns", "code": ["month_ends = pd.to_datetime(df1.DatePaid).dt.to_period('M')\n# also\n# month_ends = pd.to_datetime(df1.DatePaid).add(pd.offsets.MonthEnd(0))\n\n(df1.groupby(['ID', month_ends])\n    ['Remaining'].last()\n    .unstack(-1)\n    .ffill(1)\n    .reset_index()\n    .rename_axis(columns=None)\n)\n"], "link": "https://stackoverflow.com/questions/60455539/rolling-over-values-from-one-column-to-other-based-on-another-dataframe"}
{"id": 42, "q": "Reformat Dataframe / Add rows when condition is met", "d": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated!", "q_apis": "add value add length date any", "io": " ColA ColB 2021-03-09 1 2021-03-09 3 2021-03-10 2 2021-03-10 1 2021-03-10 2 2021-03-11 2 <s> ColA ColB 2021-03-09 1 2021-03-09 1 2021-03-09 1 2021-03-09 1 2021-03-10 1 2021-03-10 1 2021-03-10 1 2021-03-10 1 2021-03-10 1 2021-03-11 1 2021-03-11 1 ", "apis": "DataFrame repeat assign", "code": ["import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"], "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met"}
{"id": 152, "q": "Filtering DataFrame rows which have overlapping values cross-columns", "d": "I have a dataframe that reflects rows with at least one conflict inside that row. Rows 0-3 and rows 4-5 have overlapping values with other rows, but the overlap occurs across various columns. How can I: drop all but the first row of each overlap group, in a table-wise or series-wise manner, ie without using down the rows This would be the output (though don't care about index): Below snippet for easy repro", "q_apis": "DataFrame values columns at values columns drop all first index", "io": " email id1 id2 id3 0 de@l Z7 Q4 Q4 1 sco@g Q4 Z7 Q4 2 alpha@n Q4 Z7 Z7 3 numer@o Z7 Z7 Q4 4 endo@c D8 D8 L1 5 chrono@k L1 L1 D8 <s> email id1 id2 id3 0 de@l Z7 Q4 Q4 4 endo@c D8 D8 L1 ", "apis": "filter to_numpy Series index index duplicated", "code": ["L = [frozenset(x) for x in df.filter(like='id').to_numpy()]\ndf = df[~pd.Series(L, index=df.index).duplicated()]\n"], "link": "https://stackoverflow.com/questions/65035031/filtering-dataframe-rows-which-have-overlapping-values-cross-columns"}
{"id": 479, "q": "how to slice a dataframe and reassemble it into a new dataframe", "d": "I get a dataframe like this: Slice every two columns and then reorganize to form a new dataframe, as follows: I have tried but somethings wrong happened! Thank you.", "q_apis": "get columns", "io": "A YEAR2000 B YEAR2001 C YEAR2002 a 1 b 3 a 7 b 3 c 5 e 6 c 6 d 2 f 3 e 1 g 0 <s> type YEAR2000 YEAR2001 YEAR2002 a 1 7 b 3 3 c 6 5 d 2 e 1 6 f 3 g 0 ", "apis": "DataFrame columns DataFrame columns DataFrame columns merge merge merge merge", "code": ["df1 = pd.DataFrame([['a', 1], ['b', 3], ['c', 6]],columns=['letter', 'number'])\ndf2 = pd.DataFrame([['b', 3], ['c', 5], ['d', 2], ['e', 1]],columns=['letter', 'number'])\ndf3 = pd.DataFrame([['a', 7], ['e', 6], ['f', 3], ['g', 0]],columns=['letter', 'number'])\npd.merge(pd.merge(df1, df2, how='outer', on='letter'), df3, how='outer', on='letter')\n", "df1.merge(df2, how='outer', on='letter').merge(df3, how='outer', on='letter')\n"], "link": "https://stackoverflow.com/questions/56122209/how-to-slice-a-dataframe-and-reassemble-it-into-a-new-dataframe"}
{"id": 151, "q": "Organize data based on a weird column distribution in pandas", "d": "Is there an elegant way of segment data in a dataframe in which the first row includes the name of the data owner, and the second row includes headers, with all the data organized below? I have this: I need to order that so that I can analyze it in something like: I though about making different dataframes, but that would be a waste of resources. Is there a more elegant way of doing this? Thanks.", "q_apis": "first name second all", "io": "0 n_1 NaN NaN NaN NaN n_2 NaN NaN NaN NaN ... n_3 NaN NaN NaN NaN n_4 NaN NaN NaN NaN 1 V1 V2 V3 V4 V5 V1 V2 V3 V4 V5 ... V1 V2 V3 V4 V5 V1 V2 V3 V4 V5 2 45 43 30 32 NaN 45 52 47 47 NaN ... 45 57 51 50 NaN 45 51 47 50 NaN 3 50 53 38 38 NaN 50 55 50 41 NaN ... 50 51 48 49 NaN 50 53 52 52 1 4 50 54 37 41 NaN 50 53 49 49 1 ... 50 54 50 47 NaN 50 54 48 41 1 5 50 51 40 39 NaN 50 53 50 48 NaN ... 50 53 50 49 NaN 50 51 49 50 NaN 6 50 53 47 50 NaN 50 50 47 35 NaN ... 50 55 44 34 NaN 50 50 47 47 NaN 7 50 51 47 45 NaN 50 52 48 48 1 ... 50 51 48 46 NaN 50 51 47 50 NaN 8 50 52 50 50 NaN 50 50 47 50 NaN ... 50 51 47 48 NaN NaN NaN NaN NaN NaN 9 NaN NaN NaN NaN NaN 50 54 51 53 NaN ... 50 52 48 51 NaN NaN NaN NaN NaN NaN <s> 0 Own V1 V2 V3 V4 V5 1 n_1 45 43 30 32 NaN 2 n_1 50 53 38 38 NaN 3 n_1 50 54 37 41 NaN 4 n_1 50 51 40 39 NaN 5 n_1 50 53 47 50 NaN 6 n_1 50 51 47 45 NaN 7 n_1 50 52 50 50 NaN 8 n_2 45 52 47 47 NaN 9 n_2 50 55 50 41 NaN 10 n_2 50 53 49 49 1 11 n_2 50 53 50 48 NaN 12 n_2 50 50 47 35 NaN 13 n_2 50 52 48 48 1 14 n_2 50 50 47 50 NaN 15 n_2 50 54 51 53 NaN 16 n_3 45 57 51 50 NaN 17 n_3 50 51 48 49 NaN 18 n_3 50 54 50 47 NaN 19 n_3 50 53 50 49 NaN 20 n_3 50 55 44 34 NaN 21 n_3 50 51 48 46 NaN 22 n_3 50 51 47 48 NaN 23 n_3 50 52 48 51 NaN 24 n_4 45 51 47 50 NaN 25 n_4 50 53 52 52 1 26 n_4 50 54 48 41 1 27 n_4 50 51 49 50 NaN 28 n_4 50 50 47 47 NaN 29 n_4 50 50 51 47 NaN ", "apis": "read_csv iloc T replace ffill replace columns MultiIndex from_frame stack reset_index rename columns iloc", "code": ["df = pd.read_csv('your_file.csv',headers=None)\n\ns = df.iloc[:2].T.replace('NaN',np.nan).ffill() # you may need to be smart with your replace here. \ndf.columns = pd.MultiIndex.from_frame(s)\n\ndf1 = df.stack(0).reset_index(1).rename(columns={0 : 'own'}).iloc[2:]\n"], "link": "https://stackoverflow.com/questions/65043847/organize-data-based-on-a-weird-column-distribution-in-pandas"}
{"id": 554, "q": "How to identify string repetition throughout rows of a column in a Pandas DataFrame?", "d": "I'm trying to think of a way to best handle this. If I have a data frame like this: How would I go about setting up a search and find to locate and identify repetition in the middle or on edges or complete strings? Sorry the formatting looks bad Basically I have the module, line item, and formula columns filled in, but I need to figure out some sort of search function that I can apply to each of the last 3 columns. I'm not sure where to start with this. I want to match any repetition that occurs between 3 or more words, including if for example a formula was and that occurred 4 times in the Formula column, I'd want to give a yes to the boolean column \"repetition\" return on the \"Where repeated\" column and a list of every module/line item combination where it occurred on the last column. I'm sure I can tweak it more to fit my needs once I get started.", "q_apis": "DataFrame item columns apply last columns where start any between item where last get", "io": "1 + 2 + 3 + 4 <s> 1 + 2 + 3 + 4", "apis": "value value item values where now value value value value value value update value value loc contains value index update value value keys values value append value value append append DataFrame where where unique where append loc where append loc append merge DataFrame index where where where item", "code": ["#the dictionary will have a key containing row number and the value we searched for\n#the value will contain the module and line item values\nresult = {}\n#create a rownumber variable so we know where in the dataset we are\nrownumber = -1\n#now we just iterate over every row of the Formula series\nfor row in df['Formula']:\n    rownumber +=1\n    #and also every relevant value within that cell\n    for value in row.split('+'):\n        #we clean the value from trailing/preceding whitespace\n        value = value.strip()\n        #and then we return our key and value and update our dictionary\n        key = 'row:|:'+str(rownumber)+':|:'+value\n        value = (df.loc[((df.Formula.str.contains(value,regex=False))) & (df.index!=rownumber),['Module','Line Item']])\n        result.update({key:value})\n", "where_raw = []\nwhat_raw = []\nrows_raw = []\nfor key,value in zip(result.keys(),result.values()):\n    if 'Empty' in str(value):\n        continue\n    else:\n        where_raw.append(list(value['Module']+' '+value['Line Item']))\n        what_raw.append(key.split(':|:')[2])\n        rows_raw.append(int(key.split(':|:')[1]))\n\ntempdf = pd.DataFrame({'row':rows_raw,'where':where_raw,'what':what_raw})\n", "where = []\nwhat = []\nrows = []        \n\nfor row in tempdf.row.unique():\n    where.append(list(tempdf.loc[tempdf.row==row,'where']))\n    what.append(list(tempdf.loc[tempdf.row==row,'what']))\n    rows.append(row)\nresult = df.merge(pd.DataFrame({'index':rows,'where':where,'what':what}))\n", " print(result)\n Module     Line Item   Formula                                         what                                                 where\n Module 1   Line Item 1 hello[SUM: hello2]                              ['hello[SUM: hello2]']                               [['Module 1 Line Item 2']]\n Module 1   Line Item 2 goodbye[LOOKUP: blue123] + hello[SUM: hello2]   ['goodbye[LOOKUP: blue123]', 'hello[SUM: hello2]']   [['Module 2 Line Item 1'], ['Module 1 Line Item 1']]\n Module 2   Line Item 1 goodbye[LOOKUP: blue123] + some other line item ['goodbye[LOOKUP: blue123]']                         [['Module 1 Line Item 2']]\n"], "link": "https://stackoverflow.com/questions/53602854/how-to-identify-string-repetition-throughout-rows-of-a-column-in-a-pandas-datafr"}
{"id": 12, "q": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries", "d": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category?", "q_apis": "groups value cut values get all value copy DataFrame loc value", "io": "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]} <s> ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]} ", "apis": "name name name cut groupby value apply name", "code": ["def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"], "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction"}
{"id": 9, "q": "pandas group many columns to one column where every cell is a list of values", "d": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this?", "q_apis": "columns where values all columns columns get", "io": "df = c1 c2 c3 c4 c5 1. 2. 3. 1. 5 8. 2. 1. 3. 8 4. 9. 1 2. 3 <s> df = l [1,2,3,1,5] [8,2,1,3,8] [4,9,1,2,3] ", "apis": "values to_numpy agg apply", "code": ["#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"], "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values"}
{"id": 473, "q": "How to compress dataframe by removing columns that contains &#39;NaN&#39; value in between columns that has a value?", "d": "I am currently following the answer here. It mostly worked but when I viewed the whole dataframe, I saw that there are columns that contains 'NaN' values in between columns that do contain a value. For example I keep getting a result of something like this: Is there a way to remove those cells that contains NaN such that the output would be like this:", "q_apis": "columns contains value between columns value columns contains values between columns value contains", "io": " ID | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 300 1001|1001|1002| NaN | NaN | NaN |1001|1002| NaN | NaN | NaN 301 1010|1010|NaN | NaN | 1000 | 2000|1234| NaN| NaN | 1213 | 1415 302 1100|1234|5678| 9101 | 1121 | 3141|2345|6789| 1011 | 1617 | 1819 303 1000|2001|9876| NaN | NaN | NaN |1001|1002| NaN | NaN | NaN <s> ID | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 300 1001|1001|1002| 1001| 1002 | NaN |NaN | NaN| NaN | NaN | NaN 301 1010|1010|1000| 2000| 1234 | 1213|1415| NaN| NaN | NaN | NaN 302 1100|1234|5678| 9101| 1121 | 3141|2345|6789| 1011 | 1617 | 1819 303 1000|2001|9876| 1001| 1002 | NaN |NaN |NaN | NaN | NaN | NaN ", "apis": "columns concat dropna reset_index drop iterrows T", "code": ["import pandas as pd\n\ndf[df.columns] = pd.concat([s.dropna().reset_index(drop=True) for i,s in df.iterrows()], 1).T\n"], "link": "https://stackoverflow.com/questions/56488071/how-to-compress-dataframe-by-removing-columns-that-contains-nan-value-in-betwe"}
{"id": 466, "q": "How to find the last non zero element in every column throughout dataframe?", "d": "How can one go about finding the last occurring non zero element in every column of a dataframe? Input Output", "q_apis": "last last", "io": " A B 0 0 1 1 0 2 2 9 0 3 10 0 4 0 0 5 0 0 <s> A B 0 10 2 ", "apis": "values ne values argmax shape DataFrame columns columns shape ne values argmax values shape DataFrame columns columns", "code": ["first_max = df.values[df.ne(0).values.argmax(0), range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n", "row_ix = df.shape[0]-df.ne(0).values[::-1].argmax(0)-1\nfirst_max = df.values[row_ix, range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n"], "link": "https://stackoverflow.com/questions/56666271/how-to-find-the-last-non-zero-element-in-every-column-throughout-dataframe"}
{"id": 445, "q": "Dataframe to dictionary, values came out scrambled", "d": "I have a dataframe that contains two columns that I would like to convert into a dictionary to use as a map. I have tried multiple ways of converting, but my dictionary values always comes up in the wrong order. My python version is 3 and Pandas version is 0.24.2. This is what the first few rows of my dataframe looks like: I would like my dictionary to look like this: But instead my outputs came up with the wrong order for the values. I tried this first but the dictionary came out unordered: Then I tried coverting the two columns into list first then convert to dictionary, but same problem occurred: I tried converting to OrderedDict and that didn't work either. Then I've tried: I've also tried: I've also tried the answers suggested: panda dataframe to ordered dictionary None of these work. Really not sure what is going on?", "q_apis": "values contains columns map values first values first columns first ordered", "io": "{100100: 36276, 100124: 36310, 100460: 35005, 100460: 35062, 100460: 35214,...} <s> {100100: 98520, 100124: 36310, 100460: 57520, 100484: 35540, 100676: 19018, 100820: 57311, 100988: 15483, 101132: 36861,...} ", "apis": "", "code": ["defaultdict(<class 'list'>, {100100: [36276], 100124: [36310], 100460: [35005, 35062, 35214]})\n"], "link": "https://stackoverflow.com/questions/57264388/dataframe-to-dictionary-values-came-out-scrambled"}
{"id": 334, "q": "Show records contain multiple key words using OR or if elif (filter rows)", "d": "I am trying to show the rows that contain set of key words. The table look like this What I want is to filter this table where the row contain the tow strings (LD and AB) OR (LD and AD) OR (AC) So I get this result I tried This obviously didn't work , so I tried using the if function: and using this They didn't work So can someone help with what I made wrong", "q_apis": "filter filter where get", "io": "Col0 col1 col2 col3 1 LD AN CC 2 AB LD SS BB 1 AA LD AD CC 3 LD AC NN 2 FF UH BB <s> Col0 col1 col2 col3 2 AB LD SS BB 1 AA LD AD CC 3 LD AC NN ", "apis": "count isin contains contains contains contains contains contains contains contains contains contains contains contains", "code": ["df = df[count.isin([1,2]) (s.str.contains('LD') & s.str.contains('AB'))\n        | (s.str.contains('LD') & s.str.contains('AD'))\n        | (s.str.contains('LD') & s.str.contains('AC'))]\n", "df = df[(s.str.contains('LD') & s.str.contains('AB'))\n        | (s.str.contains('LD') & s.str.contains('AD'))\n        | (s.str.contains('LD') & s.str.contains('AC'))]\n"], "link": "https://stackoverflow.com/questions/60585417/show-records-contain-multiple-key-words-using-or-or-if-elif-filter-rows"}
{"id": 163, "q": "Pandas: Fill gaps in a series with mean", "d": "Given df I want to replace the nans with the inbetween mean Expected output: I have seen this_answer but it's for a grouping which isn't my case and I couldn't find anything else.", "q_apis": "mean replace mean", "io": " distance 0 0.0 1 1.0 2 2.0 3 NaN 4 3.0 5 4.0 6 5.0 7 NaN 8 NaN 9 6.0 <s> distance 0 0.0 1 1.0 2 2.0 3 2.5 4 3.0 5 4.0 6 5.0 7 5.5 8 5.5 9 6.0 ", "apis": "ffill bfill", "code": ["(df.ffill() + df.bfill()) / 2\n"], "link": "https://stackoverflow.com/questions/64828120/pandas-fill-gaps-in-a-series-with-mean"}
{"id": 280, "q": "create a nested dictionary from dataframe, where first column is the key for parent dictionary", "d": "I am trying to create a nested dictionary from a pandas dataframe. The first column-values are supposed to be the key for the upper dictionary, which will contaion the other columns as dictionary, where the column header is the key. I would like to avoid loops. the dataframe: what I would like to have: what I have tried: which unfortunately returns: Any help is highly appreciated. Thanks", "q_apis": "where first first values columns where", "io": "dict_expt = {'11': {'B': [1, 2, 3], 'C': [1.0, 0.7, 0.3]}, '12': {'B': [4, 5], 'C': [1.0]}} <s> {'B': {11: [1, 2, 3], 12: [4, 5]}, 'C': {11: [1.0, 0.7, 0.3], 12: [1.0]}} ", "apis": "groupby agg unique unique to_dict index", "code": ["df.groupby(['A']).agg({'B':lambda x: list(x.unique()),\n                      'C':lambda x: list(x.unique())}).to_dict(\"index\")\n"], "link": "https://stackoverflow.com/questions/62328295/create-a-nested-dictionary-from-dataframe-where-first-column-is-the-key-for-par"}
{"id": 305, "q": "How to replace values among blocks of consecutive values", "d": "I have a list like this: So in this list there are blocks of consecutive values, separated by . How can I replace the values before the maximum of each block, for example with -1. The result looks like:", "q_apis": "replace values values values replace values", "io": "list_tmp = [np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN] <s> list_tmp = [np.NaN, np.NaN, -1, -1, 3, np.NaN, -1, 2, np.NaN, np.NaN, -1, -1, -1, 4, np.NaN] ", "apis": "array array", "code": ["a = np.array([np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN])\n\na[~np.isnan(a) & ~np.isnan(np.r_[a[1:],np.nan])] = -1\n", "print(a)\narray([nan, nan, -1., -1.,  3., nan, -1.,  2., nan, nan, -1., -1., -1., 4., nan])\n"], "link": "https://stackoverflow.com/questions/61348221/how-to-replace-values-among-blocks-of-consecutive-values"}
{"id": 568, "q": "How can check the duplication on the group level?", "d": "How can I check for duplicated groups and remove them? Here is my data frame: In this data frame group A and B are duplicate where as C is not because its forth element is different and thus it is deeper to be unique not duplicate, the resultant data frame should look like this: I tried to groupby and check for duplicates, but this will check the values on the observational level. How can check the duplication on the group level?", "q_apis": "duplicated groups where unique groupby values", "io": "Group Value_1 Value_2 A 17 0.1 A 20 0.8 A 22 0.9 A 24 0.13 B 17 0.1 B 20 0.8 B 22 0.9 B 24 0.13 C 17 0.1 C 20 0.8 C 22 0.9 C 26 0.11 <s> Group Value_1 Value_2 A 17 0.1 A 20 0.8 A 22 0.9 A 24 0.13 C 17 0.1 C 20 0.8 C 22 0.9 C 26 0.11 ", "apis": "groupby agg drop_duplicates index groupby agg drop_duplicates index groupby cumcount set_index unstack drop_duplicates index", "code": ["idx = df.groupby('Group').agg(frozenset).drop_duplicates().index\n#alternative solution\nidx = df.groupby('Group').agg(tuple).drop_duplicates().index\n", "g = df.groupby('Group').cumcount()\nidx = df.set_index(['Group',g]).unstack().drop_duplicates().index\n"], "link": "https://stackoverflow.com/questions/53062225/how-can-check-the-duplication-on-the-group-level"}
{"id": 23, "q": "Find unique column values out of two different Dataframes", "d": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read", "q_apis": "unique values unique values first", "io": "67 Hij 14 Xyz 87 Pqr <s> 43 Def 67 Lmn 14 Xyz ", "apis": "concat drop_duplicates", "code": ["unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"], "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes"}
{"id": 15, "q": "How to Create a Correlation Dataframe from already related data", "d": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this?", "q_apis": "first at corr between columns between value columns", "io": " 0 1 2 0 English Spanish 0.50 1 English Russian 0.15 <s> English Spanish Russian English 1 0.5 0.15 Spanish 0.5 1 - Russian 0.15 - 1 ", "apis": "concat unique crosstab update set_index squeeze unstack update set_index squeeze unstack T", "code": ["lg = pd.concat([df[0], df[1]]).unique()  # ['English', 'Spanish', 'Russian']\ncx = pd.crosstab(lg, lg)\n\ncx.update(df.set_index([0, 1]).squeeze().unstack())\ncx.update(df.set_index([0, 1]).squeeze().unstack().T)\n"], "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data"}
{"id": 577, "q": "Match a value in the column and return another column in pandas | python", "d": "I have an of two columns(tab-separted): And which have one column: what I want is to search an element from in , and return the corresponding value in . If there is more than one matched value, then return all together in one column. Here is what should the output file be like: would be left empty because it does not exist. Any suggestion will be helpful.", "q_apis": "value columns value value all left empty", "io": "c1\\tc2 aaa\\t232 65 19 32 bbew\\t32 22 20 jhsi\\t986 1 32 463 221 <s> 19 aaa 1 jhsi 32 aaa bbew jhsi 277 ", "apis": "lookup keys itertuples index map append DataFrame keys map join values reindex set_index reindex reset_index", "code": ["# use set for O(1) lookup\nscope_set = set(df2['c1'])\n\n# initialise defualtdict of lists\ndd = defaultdict(list)\n\n# iterate and create dictionary mapping numbers to keys\nfor row in df1.itertuples(index=False):\n    for num in map(int, row.c2.split()):\n        if num in scope_set:\n            dd[num].append(row.c1)\n\n# construct dataframe from defaultdict\ndf = pd.DataFrame({'num': list(dd), 'keys': list(map(' '.join, dd.values()))})\n\n# reindex to include blanks\ndf = df.set_index('num').reindex(sorted(scope_set)).reset_index()\n"], "link": "https://stackoverflow.com/questions/52705423/match-a-value-in-the-column-and-return-another-column-in-pandas-python"}
{"id": 296, "q": "Read large .json file with index format into Pandas dataframe", "d": "I was following this answer but after some discussion with it's writer, it seems it only gives a solution to data format. This is the difference: I have the index format because my data is from an SQL database read into a dataframe and the index field is needed to specify every records. My json file is 2.5 GB, had been exported from the dataframe with format. This means that the whole file is actually one huge string and not a list like collection of records: This means I can't use any line or chunck based iterative solution like this: According to the documentation, can only be used if the records are in a list like format, this is why does not even accept this argument unless the orient is not . The restriction for comes from this as well, it says: And exactly this is the reason of the question, trying to read such a huge .json file gives back: I was thinking about adding the index values as a first column as well, this case it wouldn't be lost with the records format; or maybe even store an index list separately. Only I fear it would decrease the search performance later on. Is there any solution to handle the situation strictly using the .json file and no other database or big-data based technology? Update #1 For request here is the actual structure of my data. The SQL table: The pandas pivot table is almost the same as in the example, but with a 50,000 rows and 4,000 columns: And this is how it is saved with an index formatted json: Only I could not give the arg, so it is actually cramped into one huge string making it a one-liner json:", "q_apis": "index difference index index any index values first index any pivot columns index", "io": "{ \"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, \"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...}, \"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, ... \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...} } <s> {\"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, ... \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}} ", "apis": "transpose read_json test T", "code": ["# read and transpose!\ndf = pd.read_json(\"test.json\").T\n"], "link": "https://stackoverflow.com/questions/61800463/read-large-json-file-with-index-format-into-pandas-dataframe"}
{"id": 217, "q": "Python: Combine two Pandas Dataframes, extend index if needed", "d": "How can I combine two Dataframes into one with keping all rows and all index values of both Dataframes? Let's say, I have two dataframes, with partly different index values: I want to create a new dataframe, which contains both columns with a combined index. I tried: which results in: where I would like to have, all rows & index values preserved, with NaN, if no value is available for this index value:", "q_apis": "index combine all all index values index values contains columns index where all index values value index value", "io": " a b 0 -1.089084 NaN 2 -0.552297 1.704591 3 -0.242239 -0.803438 4 0.247463 -1.511515 5 -0.139740 NaN <s> a b 0 -1.089084 NaN 1 NaN -0.407245 2 -0.552297 1.704591 3 -0.242239 -0.803438 4 0.247463 -1.511515 5 -0.139740 NaN 6 NaN 0.303360 ", "apis": "concat", "code": ["dd = pd.concat([df1, df2], axis=1)\nprint(dd)\n"], "link": "https://stackoverflow.com/questions/48475687/python-combine-two-pandas-dataframes-extend-index-if-needed"}
{"id": 234, "q": "Pandas - Drop lines from dataframe if column value is in list (.csv)", "d": "I have a pandas dataframe imported from SQL, and I would like to drop lines for which a column value is in a list, which I get from a csv file. It seems pretty straighforward, I looked it up and I tried several things using but this is not working as I expect. For example the dataframe imported from SQL looks like this, let's call it df : I import this list this way : Let's assume I print the list, this is what I see : Then I use the following : I would expect to get this (initial df with lines 1 and 2 dropped because they are in the list) However this is not what happens. I get the exact same df as initially with no lines dropped, and also no error message of any kind. What am I doing wrong ? I thought the data in the list and in the df column might not be the same type and I tried fiddling with , but without much success. Perhaps i'm using it wrong. Would appreciate any help. Thanks !", "q_apis": "value drop value get get get any any", "io": " SKU Brand 0 AD31KL-A1 BrandA 1 BC31KL-B3 BrandB 2 DE31KL-D4 BrandC 3 FG31KL-F5 BrandD <s> SKU Brand 0 AD31KL-A1 BrandA 3 FG31KL-F5 BrandD ", "apis": "apply DataFrame columns DataFrame iloc first", "code": ["list = df2.apply(lambda x: x.tolist(), axis=1)\n", "# Well, I don't have list.csv, so let me just create a dataframe\ndf = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ], columns = ['SKU'] )\nprint(df)\nlist =  df['SKU'].tolist() \nprint( list ) \n", "df = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ] )\nprint(df)\nlist =  df.iloc[:, 0].tolist()  # first column of dataframe\nprint( list ) \n"], "link": "https://stackoverflow.com/questions/63286249/pandas-drop-lines-from-dataframe-if-column-value-is-in-list-csv"}
{"id": 387, "q": "How to append a list in Pandas?", "d": "I'm reading a dataframe and trying to insert a list inside another list and then converting it to json file. I'm using python 3 and 0.25.3 version of pandas for it. ============================ Data that I'm reading: ============================ Here is my code: ============================= What is expected: ============================ What I'm getting: ====================== I tried to do the same thing using function (and posted a question here 'Dataframe and conversion to JSON using Pandas'), but some people recommend me to try another way using another function. I know that is a stupid thing add object inside my , but I already tried of others way. Could you help me?", "q_apis": "append insert add", "io": "[{ \"id\": 6, \"label\": \"Sao Paulo\", \"Customer\": [{ \"id\": \"CUS-99992\", \"label\": \"Brazil\", \"number\": [{ \"part\": \"7897\", \"client\": \"892\" }, { \"part\": \"888\", \"client\": \"12\" }] }] }, { \"id\": 92, \"label\": \"Hong Kong\", \"Customer\": [{ \"id\": \"CUS-88888\", \"label\": \"China\", \"number\": [{ \"part\": \"147\", \"client\": \"288\" }] }] }] <s> [{ \"id\": 6, \"label\": \"Sao Paulo\", \"Customer\": [{ \"id\": \"CUS-99992\", \"label\": \"Brazil\" }], \"number\": [{ \"part\": \"7897\", \"client\": \"892\" }], \"number\": [{ \"part\": \"888\", \"client\": \"12\" }] }, { \"id\": 92, \"label\": \"Hong Kong\", \"Customer\": [{ \"id\": \"CUS-88888\", \"label\": \"China\" }], \"number\": [{ \"part\": \"147\", \"client\": \"288\" }] }] ", "apis": "eval to_json iloc iloc astype astype rename columns groupby apply to_json values", "code": ["def getNum(grp):\n    return eval(grp[['part', 'client']].to_json(orient='records'))\n", "def getCust(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id_customer, 'label': r0.label_customer, 'number': getNum(grp) }\n", "def getGrp(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id, 'label': r0.label, 'Customer': getCust(grp) }\n", "df.part_number = df.part_number.astype('str')\ndf.number_client = df.number_client.astype('str')\n", "df.rename(columns={'part_number': 'part', 'number_client': 'client'})\\\n    .groupby(['id', 'label', 'id_customer', 'label_customer'])\\\n    .apply(getGrp).to_json(orient='values')\n"], "link": "https://stackoverflow.com/questions/59254205/how-to-append-a-list-in-pandas"}
{"id": 395, "q": "In python is there a way to delete parts of a column?", "d": "I want to trim the values of a pandas data frame. For example, I have the following: And I would like the result to be: If anyone could help it would be very appreciated.", "q_apis": "delete values", "io": " A B C 33344-10 5555-78 999902 3444441 5555679 2334 2334 5555 3344 <s> A B C 3334 5555 9999 3444 5555 2334 2334 5555 3344 ", "apis": "stop", "code": ["for c in ['A', 'B', 'C']:\n    df[c] = df[c].str.slice(stop=4)\n"], "link": "https://stackoverflow.com/questions/59036924/in-python-is-there-a-way-to-delete-parts-of-a-column"}
{"id": 131, "q": "How to use pandas dataframe as condition for other dataframe", "d": "Say I have dataframe A: and dataframe B: How can I use dataframe A as a condition for dataframe B, so that df B's cells are within the and values of df A. The ideal output is this: (first cell is explanatory)", "q_apis": "values first", "io": " A B C lower 1 0 -5 upper 2 2 0 <s> A B C sa 5 1 -2 sb 3 0 2 sc 1 -5 1 ", "apis": "ge loc le loc loc loc", "code": ["df = B.ge(A.loc['lower']) & B.le(A.loc['upper'])\n#alternative\n#df = (B>= A.loc['lower']) & (B <= A.loc['upper'])\nprint (df)\n        A      B      C\nsa  False   True   True\nsb  False   True  False\nsc   True  False  False\n"], "link": "https://stackoverflow.com/questions/65563873/how-to-use-pandas-dataframe-as-condition-for-other-dataframe"}
{"id": 140, "q": "Pandas assigning values to dataframe, conditional on values in another with the same dimensions issue/question", "d": "I'm trying to better understand Pandas/Python so I've been playing around with some stuff. I ran into an issue, I know some workarounds, but I'm wondering why it happened in the first place. Here's my full code, followed by an explanation: I create, 2 dataframes. The first with random numbers, the second dataframe is empty but has the same dimensions as the first. Based on the values in the first dataframe, I'd like to modify the values in the second. My first datframe I create looks like this: I create a second dataframe based on the dimensions of the second: What I would like to do now, is say that for values that are greater 0.6 in df1, I would like the corresponding value in df2 to be 1. And for values less than 0.6 I would like the values to be 0. I did that in the following way, by slicing df1 and then using that slice on df2, and then assigning the values. I thought this would work, but instead, the first row and first column are still NANs Now the reason this didn't work, I think, is because the column names and the row names don't align between the two indices, but what I'm trying to understand is why that's happening. I thought when I sliced df1 based on the conditional it created an array of trues/falses, that I could use on any other dataframe with the same dimension: r I thought that mapping of trues and falses above could be used anywhere, it seems like it can't. Is there a way around this doesn't involve renaming the columns/rows to match between the 2 dataframes?", "q_apis": "values values first first second empty first values first values second first second second now values value values values values first first names names align between indices array any columns between", "io": "df1 1 2 3 4 5 6 7 8 9 10 1 0.24 0.03 0.93 0.38 0.03 0.83 0.47 0.85 0.79 0.65 2 0.66 0.25 0.01 0.28 0.19 0.26 0.25 0.48 0.33 0.92 3 0.53 0.33 0.78 0.04 0.36 0.63 0.16 0.16 0.21 0.96 4 0.76 0.03 0.89 0.15 0.24 0.90 0.59 0.41 0.92 0.98 5 0.72 0.45 0.95 0.44 0.79 0.93 0.90 0.48 0.61 0.02 <s> df2 0 1 2 3 4 5 6 7 8 9 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 NaN 0 0 1 0 0 1 0 1 1 2 NaN 1 0 0 0 0 0 0 0 0 3 NaN 0 0 1 0 0 1 0 0 0 4 NaN 1 0 1 0 0 1 0 0 1 ", "apis": "astype", "code": ["df2 = (df1 > 0.6).astype(int)\nprint(df2)\n"], "link": "https://stackoverflow.com/questions/65294879/pandas-assigning-values-to-dataframe-conditional-on-values-in-another-with-the"}
{"id": 503, "q": "Aggregate values of same name pandas dataframe columns to single column", "d": "I have multiple csv files that were produced by tokenizing code. These files contain keywords in uppercase and lowercase. I would like to merge all those files in one single dataframe which contains all the unique values (summed) in lowercase. What would you suggest to get the result below? Initial DF: Result I don't have access to the raw data from which the csv files where created so I cannot correct this at an earlier step. At the moment I have tried mapping .lower() to the dataframe headers that I create, but it returns seperate columns with the same name like so: Using pandas is not essential. I have thought of converting the csv files to dictionaries and then trying the above procedure (turns out it is much more complicated than I thought), or using lists. Also, group by does not do the job as it will remove non duplicate column names. Any approach is welcome.", "q_apis": "values name columns merge all contains all unique values get where at step columns name names", "io": "+---+---+----+-----+ | a | b | A | B | +---+---+----+-----+ | 1 | 2 | 3 | 1 | | 2 | 1 | 3 | 1 | +---+---+----+-----+ <s> +---+---+ | a | b | +---+---+ | 4 | 3 | | 5 | 2 | +---+---+ ", "apis": "columns unique columns map columns DataFrame columns columns columns columns columns DataFrame columns sum DataFrame columns size", "code": ["def sumDupeColumns(df):\n    \"\"\"Return dataframe with columns with the same lowercase spelling summed.\"\"\"\n\n    # Get list of unique lowercase column headers\n    columns = set(map(str.lower, df.columns))\n    # Create new (zero-initialised) dataframe for output\n    df1 = pd.DataFrame(data=np.zeros((len(df), len(columns))), columns=columns)\n\n    # Sum matching columns\n    for col in df.columns:\n        df1[col.lower()] += df[col]\n\n    return df1\n", "import pandas as pd\nimport numpy as np\n\nnp.random.seed(seed=42)\n\n# Generate DataFrame with random int input and 'duplicate' columns to sum\ndf = pd.DataFrame(columns = ['a','A','b','B','Cc','cC','d','eEe','eeE','Eee'], \n                  data = np.random.randint(9, size=(5,10))\n\ndf = sumDupeColumns(df)\n"], "link": "https://stackoverflow.com/questions/55132744/aggregate-values-of-same-name-pandas-dataframe-columns-to-single-column"}
{"id": 148, "q": "Pandas, mapping one Dataframe onto another?", "d": "I'm not sure how to tackle this problem. I have 3 data frames; one is a true/false table [3532x622], the other is a single series of integers[662x1], the other is my main dataframe[3532x8]. The true/false table was create by comparing a series of points to find which ones where inside a polygon, that is why is has the shape it does. I have outlined a diagram below as to what I am trying to accomplish. Convert to: Then map this onto the main dataframe This is what I have started I don't know where to go from here.", "q_apis": "where shape map where", "io": "df_2 0 1 2 8 9 0 56489 np.nan np.nan ... np.nan 89641 1 np.nan 86932 np.nan ... 45871 np.nan 2 np.nan 86932 np.nan ... np.nan np.nan <s> df_3 0 1 0 poly_a 56489 1 ploy_a 89641 2 poly_b 86932 3 poly_b 45871 4 poly_c 86932 ", "apis": "DataFrame DataFrame DataFrame Series Series copy first columns T copy iloc values index replace loc apply concat", "code": ["import numpy\nimport pandas\n\n# Creating Example Dataframes\ndf_1 = pandas.DataFrame([56489, 45872, 89657, 56895, 87456])\ndf_2 = pandas.DataFrame(\n    [\n        [True, False, False, False, True],\n        [False, True, True, False, False],\n        [False, True, False, True, True],\n    ]\n)\ndf_3 = pandas.DataFrame([\"poly_a\", \"poly_b\", \"poly_c\"])\n\n\ndef replace_values(row: pandas.Series) -> pandas.Series:\n\n    # Make a copy of df_1 (first row) but flip it to become columns\n    c = df_1.T.copy().iloc[0]\n\n    # Use the boolean values from the row as index, replace False with NaN\n    c.loc[~row] = numpy.nan\n    return c\n\n\n# Combine 2 and 1\ncombined = df_2.apply(replace_values, axis=1)\n\n# Add 3\nresult = pandas.concat([df_3, combined], axis=1, ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/65083476/pandas-mapping-one-dataframe-onto-another"}
{"id": 518, "q": "Efficient python pandas equivalent/implementation of R sweep with multiple arguments", "d": "Other questions attempting to provide the equivalent to 's function (like here) do not really address the case of multiple arguments where it is most useful. Say I wish to apply a 2 argument function to each row of a Dataframe with the matching element from a column of another DataFrame: In I got the equivalent using on what is basically a loop through the row counts. I highly doubt this is efficient in , what is a better way of doing this? Both bits of code should result in a Dataframe/matrix of 6 numbers when applying : I should state clearly that the aim is to insert one's own function into this like behavior say: resulting in: What is a good way of doing that in python pandas?", "q_apis": "where apply DataFrame insert", "io": " A B 1 10 110 2 22 132 3 36 156 <s> A B [1,] 3 4 [2,] 3 4 [3,] 3 5 ", "apis": "dtypes DataFrame DataFrame apply mean std Series shape apply iloc iloc columns get_loc mean std all", "code": ["import pandas as pd\nfrom pandas.core.dtypes.generic import ABCSeries\n\ndef sweep(df, series, FUN):\n    assert isinstance(series, ABCSeries)\n\n    # row-wise application\n    assert len(df) == len(series)\n    return df._combine_match_index(series, FUN)\n\n\n# define your binary operator\ndef f(x, y):\n    return x*y    \n\n# the input data frames\ndf = pd.DataFrame( { \"A\" : range(1,4),\"B\" : range(11,14) } )\ndf2 = pd.DataFrame( { \"X\" : range(10,13),\"Y\" : range(10000,10003) } )\n\n# apply\ntest1 = sweep(df, df2.X, f)\n\n# performance\n# %timeit sweep(df, df2.X, f)\n# 155 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)#\n\n# another method\nimport numpy as np\ntest2 = pd.Series(range(df.shape[0])).apply(lambda row_count: np.multiply(df.iloc[row_count,:],df2.iloc[row_count,df2.columns.get_loc('X')]))\n\n# %timeit performance\n# 1.54 ms \u00b1 56.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nassert all(test1 == test2)\n"], "link": "https://stackoverflow.com/questions/54621203/efficient-python-pandas-equivalent-implementation-of-r-sweep-with-multiple-argum"}
{"id": 166, "q": "Split two columns in a pandas dataframe into two and name them", "d": "I have this pandas dataframe I would like to split the x and y columns and get an output with these given names on the columns. Is there a straight forward way to do this in python?", "q_apis": "columns name columns get names columns", "io": " x y Values 0 A B C D 4.7 1 A B C D 10.9 2 A B C D 1.8 3 A B C D 6.5 4 A B C D 3.4 <s> x f y g Values 0 A B C D 4.7 1 A B C D 10.9 2 A B C D 1.8 3 A B C D 6.5 4 A B C D 3.4 ", "apis": "columns names sum values columns", "code": ["df[['x', 'f']] = df.x.str.split(\" \", expand=True)\ndf[['y', 'g']] = df.y.str.split(\" \", expand=True)\ndf[['x','f','y','g', 'Values']]\n", "# Define the target columns to split, and their new column names\ncols={\n    'x': ['x','f'],\n    'y': ['y','g']\n}\n# Apply the function to each target-column\nfor k in cols:\n    df[cols[k]] = df[k].str.split(\" \", expand=True)\n\n# Reorder the dataframe as you wish\nnew_columns = sum(cols.values(),[])\nold_columns = set(df.columns) - set(new_columns)\ndf[new_columns + list(old_columns)]\n"], "link": "https://stackoverflow.com/questions/64773001/split-two-columns-in-a-pandas-dataframe-into-two-and-name-them"}
{"id": 104, "q": "Create a pandas column of numbers from 1 to 3 and repeat again", "d": "I have a dataframe: I want to add a column so that my final dataframe looks like: I don't want the new row to depend upon StoreNumber which looks ovious in example. I want to start numbering with 1 and when I reach 3, then again start with 1. How do I do it?", "q_apis": "repeat add start start", "io": " StoreNumber Year 1000 2000 1000 2001 1000 2002 1001 2000 1001 2001 1001 2002 <s> StoreNumber Year New 1000 2000 1 1000 2001 2 1000 2002 3 1001 2000 1 1001 2001 2 1001 2002 3 ", "apis": "", "code": ["from itertools import cycle\n\nnum_cycle = cycle([1, 2, 3])\ndf['New'] = [next(num_cycle) for num in range(len(df))]\n"], "link": "https://stackoverflow.com/questions/66075712/create-a-pandas-column-of-numbers-from-1-to-3-and-repeat-again"}
{"id": 506, "q": "How can I change a specific row label in a Pandas dataframe?", "d": "I have a dataframe such as: Where the final row contains averages. I would like to rename the final row label to so that the dataframe will look like this: I understand columns can be done with . But how can I do this with a specific row label?", "q_apis": "contains rename columns", "io": " 0 1 2 3 4 5 0 41.0 22.0 9.0 4.0 2.0 1.0 1 6.0 1.0 2.0 1.0 1.0 1.0 2 4.0 2.0 4.0 1.0 0.0 1.0 3 1.0 2.0 1.0 1.0 1.0 1.0 4 5.0 1.0 0.0 1.0 0.0 1.0 5 11.4 5.6 3.2 1.6 0.8 1.0 <s> 0 1 2 3 4 5 0 41.0 22.0 9.0 4.0 2.0 1.0 1 6.0 1.0 2.0 1.0 1.0 1.0 2 4.0 2.0 4.0 1.0 0.0 1.0 3 1.0 2.0 1.0 1.0 1.0 1.0 4 5.0 1.0 0.0 1.0 0.0 1.0 A 11.4 5.6 3.2 1.6 0.8 1.0 ", "apis": "index index", "code": ["df.index = df.index[:-1].tolist() + ['a']\n"], "link": "https://stackoverflow.com/questions/42142756/how-can-i-change-a-specific-row-label-in-a-pandas-dataframe"}
{"id": 636, "q": "how to extract a 2D array encoded in a list of strings in a pandas dataframe?", "d": "I have messed up a dataframe. I have a columns which contain strings which encode a list of numbers e.g. EDIT: actually, the commas are missing as well Each of the strings encodes a list with a fixed number of elements. I would like to convert this into 3 (in general N, where each of them numeric, containing one element from the original list in mycol I have tried the following, without success", "q_apis": "array columns where", "io": "df= mycol 0 '[ 0.5497076, 0.59722222, 0.42361111]' 1 '[ 0.8030303, 0.69090909, 0.52727273]' 2 '[ 0.51461988, 0.38194444, 0.66666667]' <s> df= mycol 0 '[ 0.5497076 0.59722222 0.42361111]' 1 '[ 0.8030303 0.69090909 0.52727273]' 2 '[ 0.51461988 0.38194444 0.66666667]' ", "apis": "DataFrame apply replace replace apply Series", "code": ["import pandas as pd\ndf = pd.DataFrame({\"mycol\": ['[ 0.5497076   0.59722222  0.42361111]', '[ 0.8030303   0.69090909  0.52727273]']})\ndf[['mycol1','mycol2','mycol3']]  = df[\"mycol\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").split()).apply(pd.Series)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/50152137/how-to-extract-a-2d-array-encoded-in-a-list-of-strings-in-a-pandas-dataframe"}
{"id": 561, "q": "Python - pandas explode rows by turns", "d": "I have a dataframe as below. Now I extracted letters from B1-B3 and add to new columns U1-U3 get: and I want to let the row to explode like this: Thanks in advance", "q_apis": "explode add columns get explode", "io": " B1 B2 B3 U1 U2 U3 0 1C C 1 3A 1A A A 2 41A 28A 3A A A A <s> B1 B2 B3 U1 U2 U3 0 1C C 1 3A 1A A 2 3A 1A A 3 41A 28A 3A A 4 41A 28A 3A A 5 41A 28A 3A A ", "apis": "merge apply add_prefix notnull sum loc repeat index values groupby index apply dropna mask", "code": ["df = df.merge(df.apply(lambda x: x.str.extract('([A-Za-z])')).add_prefix('U_'), left_index=True,right_index=True,how='outer')\n", "# Duplicating the rows of dataframe\nval = df[['U_B1','U_B2','U_B3']].notnull().sum(axis=1)\ndf1 = df.loc[np.repeat(val.index,val)]\n", "masked values of identity matrix", "df1[['U_B1','U_B2','U_B3']] = df1.groupby(df1.index)['U_B1','U_B2','U_B3'].apply(lambda x: x.dropna(axis=1).mask(np.identity(len(x))==0))\n"], "link": "https://stackoverflow.com/questions/53314441/python-pandas-explode-rows-by-turns"}
{"id": 243, "q": "Dropping dataframe rows in time series dataframe using pandas", "d": "I have the below sequence of data as a pandas dataframe It should always be the case that id 404 gets repeated after another different id. For example if the above is motion sensors in a house e.g. 404:hallway, 202:bedroom, 303:kitchen, 201:studyroom, where the hallway is in the middle, then moving from bedroom to kitchen to studyroom and back to bedroom should trigger 202, 404, 303, 404, 201, 404, 202 in that order because one always passes through the hallway (404) to any room. My output has cases that violate this sequence and I want to drop such rows. For example from the snippet dataframe above the below rows violate this: and therefore the rows below should be droped (but of course I have a much larger dataset). I have tried shift and drop but the result still has some inconsistencies. How best can I approach this?", "q_apis": "time where any drop shift drop", "io": "303,2012-06-25 18:01:56,2012-06-25 18:02:06,10 303,2012-06-25 18:02:23,2012-06-25 18:02:44,21 303,2012-06-25 18:03:43,2012-06-25 18:05:51,128 101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104 <s> 303,2012-06-25 18:02:23,2012-06-25 18:02:44,21 101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104 ", "apis": "mask ne shift ne mask", "code": ["mask = df['id'].ne(404) & df['id'].shift(fill_value=404).ne(404)\ndf = df[~mask]\n"], "link": "https://stackoverflow.com/questions/63090827/dropping-dataframe-rows-in-time-series-dataframe-using-pandas"}
{"id": 558, "q": "drop group by number of occurrence", "d": "Hi I want to delete the rows with the entries whose number of occurrence is smaller than a number, for example: Here I want to delete all the rows if the number of occurrence in column 'a' is less than twice. Wanted output: What I know: we can find the number of occurrence by , and it will give me something like: But I don't know how I should approach from here to delete the rows. Thanks in advance!", "q_apis": "drop delete delete all delete", "io": " a b c 0 1 4 0 1 2 5 1 2 3 6 3 3 2 7 2 <s> a b c 1 2 5 1 3 2 7 2 ", "apis": "groupby transform size", "code": ["res = df[df.groupby('a')['b'].transform('size') >= 2]\n"], "link": "https://stackoverflow.com/questions/53471422/drop-group-by-number-of-occurrence"}
{"id": 40, "q": "Groupby, counts in ranges and spread in Pandas", "d": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output:", "q_apis": "count items groupby index transpose columns", "io": " a b a (0, 10] 2 BBB (10, 20] 3 BBB (20, 30] 1 AAA <s> (0, 10] (10, 20] (20, 30] AAA 0 0 1 BBB 2 3 0 ", "apis": "assign cut pivot_table index columns values count", "code": ["df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"], "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas"}
{"id": 469, "q": "how to find the last value of consecutive values in pandas dataframe?", "d": "I have a data frame like this I want to group by this data frame on col1 where there is consecutive values, and take the last value for each consecutive groups, The final data frame should look like: I have tried something like: But its missing the consecutive condition. How to implement it in most effective way using pandas/ python", "q_apis": "last value values where values take last value groups", "io": "df: col1 col2 1 10 1 20 2 11 3 33 1 20 1 10 2 24 3 21 3 28 <s> df col1 col2 1 20 2 11 3 33 1 10 2 24 3 28 ", "apis": "ne shift dtype bool", "code": ["print (df['col1'].ne(df['col1'].shift(-1)))\n0    False\n1     True\n2     True\n3     True\n4    False\n5     True\n6     True\n7    False\n8     True\nName: col1, dtype: bool\n"], "link": "https://stackoverflow.com/questions/56662690/how-to-find-the-last-value-of-consecutive-values-in-pandas-dataframe"}
{"id": 388, "q": "How to drop the rows if two columns cells are empty?", "d": "This is my DF i want to compare the columns B and C then i have to check both are null after that i want to remove that rows from DF. Output looks like,this Then i need to check again both columns of B and C like whether the values or same or not , if same i need to create one column say validation_results and print Y and if not same print N. I am new to python so anybody here tell me how can i do this with minimum lines of code.", "q_apis": "drop columns empty compare columns columns values", "io": "A B C 1 10 10 2 3 12 12 4 5 21 22 <s> A B C 1 10 10 3 12 12 5 21 22 ", "apis": "ne ne all dtype bool", "code": ["print (df[['B','C']].ne(''))\n       B      C\n0   True   True\n1  False  False\n2   True   True\n3  False  False\n4   True   True\n", "print (df[['B','C']].ne('').all(axis=1))\n0     True\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/59266300/how-to-drop-the-rows-if-two-columns-cells-are-empty"}
{"id": 572, "q": "Series calculation based on shifted values / recursive algorithm", "d": "I have the following: This lines basically only take in df['Alpha'] but not the df['PositionLong'].shift(1).. It cannot recognize it but I dont understand why? It produces this: However what I wanted the code to do is this: I believe the solution is to loop each row, but this will take very long. Can you help me please?", "q_apis": "Series values take shift take", "io": "df['Alpha'] df['Bravo'] df['PositionLong'] 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 <s> df['Alpha'] df['Bravo'] df['PositionLong'] 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 ", "apis": "empty shape values values astype", "code": ["from numba import njit\n\n@njit\ndef rec_algo(alpha, bravo):\n    res = np.empty(alpha.shape)\n    res[0] = 1 if alpha[0] == 1 else 0\n    for i in range(1, len(res)):\n        if (alpha[i] == 1) or ((res[i-1] == 1) and bravo[i] == 1):\n            res[i] = 1\n        else:\n            res[i] = 0\n    return res\n\ndf['PositionLong'] = rec_algo(df['Alpha'].values, df['Bravo'].values).astype(int)\n"], "link": "https://stackoverflow.com/questions/52902568/series-calculation-based-on-shifted-values-recursive-algorithm"}
{"id": 484, "q": "Convert Rows per Unique Id into all comma separated possibilities", "d": "i have some data in the following format, at the moment this is in a Pandas Dataframe. What i require is all of the possible combinations of the Lenders columns for each Uid so the output would be something like this And the same for Uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. Thanks,", "q_apis": "all at all columns", "io": "Row Uid Lender 1 1 HSBC 2 1 Lloyds 3 1 Barclays 4 2 Lloyds 5 2 Barclays 6 2 Santander 7 2 RBS 8 2 HSBC <s> Row Uid LenderCombo 1 1 Barclays 2 1 Lloyds 3 1 HSBC 4 1 Barclays, HSBC 5 1 Barclays, Lloyds 6 1 HSBC, Lloyds 7 1 Barclays, HSBC, Lloyds ", "apis": "map groupby apply Series join reset_index rename columns", "code": ["from itertools import chain, combinations\n\n#https://stackoverflow.com/a/5898031\ndef all_subsets(ss):\n    return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))\n\ndf = (df.groupby('Uid')['Lender']\n       .apply(lambda x: pd.Series([', '.join(y) for y in all_subsets(x)]))\n       .reset_index()\n       .rename(columns={'level_1':'Row'}))\n"], "link": "https://stackoverflow.com/questions/56020272/convert-rows-per-unique-id-into-all-comma-separated-possibilities"}
{"id": 546, "q": "Using .iterrows() with series.nlargest() to get the highest number in a row in a Dataframe", "d": "I am trying to create a function that uses and . I want to iterate over each row and find the largest number and then mark it as a . This is the data frame: Here is the output I wish to have: This is the function I wish to use here: I get the following error: AttributeError: 'tuple' object has no attribute 'nlargest' Help would be appreciated on how to re-write my function in a neater way and to actually work! Thanks in advance", "q_apis": "iterrows nlargest get get nlargest", "io": "A B C 9 6 5 3 7 2 <s> A B C 1 0 0 0 1 0 ", "apis": "iterrows nlargest sum", "code": ["for i, row in df.iterrows():\n    top_numbers = row.nlargest(top_n).sum()\n"], "link": "https://stackoverflow.com/questions/51645934/using-iterrows-with-series-nlargest-to-get-the-highest-number-in-a-row-in-a"}
{"id": 319, "q": "How to freeze first numbers in sequences between NaNs in Python pandas dataframe", "d": "Is there a Pythonic way to, in a timeseries dataframe, by column, go down and pick the first number in a sequence, and then push it forward until the next NaN, and then take the next non-NaN number and push that one down until the next NaN, and so on (retaining the indices and NaNs). For example, I would like to convert this dataframe: To this dataframe: I know I can use a loop to iterate down the columns to do this, but would appreciate some help on how to do it in a more efficient Pythonic way on a very large dataframe. Thank you.", "q_apis": "first between first take indices columns", "io": " A B C 0 NaN 8.0 NaN 1 1.0 6.0 NaN 2 3.0 4.0 4.0 3 5.0 NaN 2.0 4 7.0 NaN 6.0 5 NaN 9.0 NaN 6 2.0 7.0 1.0 7 4.0 3.0 5.0 8 6.0 NaN 2.0 9 NaN 3.0 8.0 <s> A B C 0 NaN 8.0 NaN 1 1.0 8.0 NaN 2 1.0 8.0 4.0 3 1.0 NaN 4.0 4 1.0 NaN 4.0 5 NaN 9.0 NaN 6 2.0 9.0 1.0 7 2.0 9.0 1.0 8 2.0 NaN 1.0 9 NaN 3.0 1.0 ", "apis": "where mask notna shift value mask mask replace all ffill fillna iloc columns where mask replace", "code": ["# where DF is not NaN\nmask = DF.notna()\nResult = (DF.shift(-1)           # fill the original NaN's with their next value\n            .mask(mask)          # replace all the original non-NaN with NaN\n            .ffill()             # forward fill \n            .fillna(DF.iloc[0])  # starting of the the columns with a non-NaN\n            .where(mask)         # replace the original NaN's back\n         )\n"], "link": "https://stackoverflow.com/questions/61011933/how-to-freeze-first-numbers-in-sequences-between-nans-in-python-pandas-dataframe"}
{"id": 171, "q": "Concatenation of two dataframe after onehotencoding", "d": "Let us consider following code result of this code is following ( i am writing final dataframe) all others works fine, they are so my point is to remove commas in header part of the final dataframe, please help me", "q_apis": "all", "io": " Alphabet (A,) (B,) (C,) 0 A 1.0 0.0 0.0 1 B 0.0 1.0 0.0 2 C 0.0 0.0 1.0 3 A 1.0 0.0 0.0 4 B 0.0 1.0 0.0 <s> A B C 0 1.0 0.0 0.0 1 0.0 1.0 0.0 2 0.0 0.0 1.0 3 1.0 0.0 0.0 4 0.0 1.0 0.0 ", "apis": "columns join columns", "code": ["Encoded_Dataframe.columns = [''.join(col) for col in Encoded_Dataframe.columns]\n"], "link": "https://stackoverflow.com/questions/64722958/concatenation-of-two-dataframe-after-onehotencoding"}
{"id": 498, "q": "Pandas: get the min value between 2 dataframe columns", "d": "I have 2 columns and I want a 3rd column to be the minimum value between them. My data looks like this: And I want to get a column C in the following way: Some helping code: Thanks!", "q_apis": "get min value between columns columns value between get", "io": " A B 0 2 1 1 2 1 2 2 4 3 2 4 4 3 5 5 3 5 6 3 6 7 3 6 <s> A B C 0 2 1 1 1 2 1 1 2 2 4 2 3 2 4 2 4 3 5 3 5 3 5 3 6 3 6 3 7 3 6 3 ", "apis": "min min values mean std mean std min min values min to_numpy mean std mean std mean std", "code": ["%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n314 \u00b5s \u00b1 3.63 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n34.4 \u00b5s \u00b1 161 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n", "%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n%timeit np.min(df.to_numpy(),axis=1)\n314 \u00b5s \u00b1 3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n35.2 \u00b5s \u00b1 680 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n35.5 \u00b5s \u00b1 262 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n"], "link": "https://stackoverflow.com/questions/55654105/pandas-get-the-min-value-between-2-dataframe-columns"}
{"id": 115, "q": "Select value from a list of columns that is close to another value in pandas", "d": "I have the following data frame: I want to create another column which stores one value lower than the . Intended result: Here's what I have been trying: If the difference is greater than 3% it doesn't do the job right. Note that the s columns may vary so I would want to keep the Little help will be appreciated. THANKS!", "q_apis": "value columns value value difference right columns", "io": "S0 S1 S2 S3 S4 S5... Price 10 15 18 12 18 19 16 55 45 44 66 58 45 64 77 84 62 11 61 44 20 <s> S0 S1 S2 S3 S4 S5... Price Sup 10 15 18 12 18 19 16 15 55 45 44 66 58 45 64 58 77 84 62 11 61 44 20 11 ", "apis": "filter compare mask lt where mask where mask max all max fillna where mask max fillna", "code": ["s = df.filter(like='S')\n\n# compare to the price\nmask = s.lt(df['Price'], axis=0)\n\n# `where(mask)` places `NaN` where mask==False\n# `max(1)` takes maximum along rows, ignoring `NaN`\n# rows with all `NaN` returns `NaN` after `max`, \n# `fillna(0)` fills those with 0\ndf['Price'] = s.where(mask).max(1).fillna(0)\n"], "link": "https://stackoverflow.com/questions/65833007/select-value-from-a-list-of-columns-that-is-close-to-another-value-in-pandas"}
{"id": 611, "q": "python: how to sum unique elements respectively of a dataframe column based on another column", "d": "For example, I have a df with two columns. Input Output I want to count the element in group by user_id respectively. The expected output is shown as follow. Expected Briefly, in column , I count the number of in column based on column . Hopefully for help!", "q_apis": "sum unique columns count count", "io": "df label user_id 0 0 a 1 0 a 2 1 a 3 0 b 4 0 b 5 2 b 6 0 c 7 1 c 8 2 c <s> df label user_id label_0 label_1 label_2 0 0 a 2 1 0 1 0 a 2 1 0 2 1 a 2 1 0 3 0 b 2 0 1 4 0 b 2 0 1 5 2 b 2 0 1 6 0 c 1 1 1 7 1 c 1 1 1 8 2 c 1 1 1 ", "apis": "join groupby size unstack add_prefix join groupby value_counts unstack add_prefix merge crosstab add_prefix left", "code": ["df = (df.join(df.groupby(['user_id', 'label'])\n                .size()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n", "df = (df.join(df.groupby('user_id')['label']\n                .value_counts()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n", "df = (df.merge(pd.crosstab(df['user_id'], df['label'])\n                 .add_prefix('label_'), on='user_id', how='left'))\n"], "link": "https://stackoverflow.com/questions/51265888/python-how-to-sum-unique-elements-respectively-of-a-dataframe-column-based-on-a"}
{"id": 516, "q": "Pandas: Remove all NaN values in all columns", "d": "I have a data frame with many null records: I want to remove all NaN values in all rows of columns . As you could see, each column has different number of rows. So, I want to get something like this: I tried But it removes all records in the dataframe. How may I do that ?", "q_apis": "all values all columns all values all columns get all", "io": "Col_1 Col_2 Col_3 10 5 2 22 7 7 3 9 5 4 NaN NaN 5 NaN NaN 6 4 NaN 7 6 7 8 10 NaN 12 NaN 1 <s> Col_1 Col_2 Col_3 10 5 2 22 7 7 3 9 5 4 4 7 6 6 1 7 10 8 12 ", "apis": "dropna values columns columns", "code": ["{c: df[c].dropna().values for c in df.columns}\n", "{c: list(df[c]) for c in df.columns}\n"], "link": "https://stackoverflow.com/questions/50418202/pandas-remove-all-nan-values-in-all-columns"}
{"id": 195, "q": "Shuffling Pandas Dataframe Columns", "d": "I need to shuffle dataframe columns. Currently I do it this way: Before: After: So it does the job, but there must be a better way to do this. Any ideas?", "q_apis": "columns", "io": " 0 1 2 3 4 0 0.472918 0.261734 0.987053 0.921826 0.144114 <s> 0 1 2 3 4 0 0.472918 0.921826 0.987053 0.144114 0.261734 ", "apis": "apply DataFrame", "code": ["def shuffle(df, n=1):\n    for _ in range(n):\n        df.apply(np.random.shuffle)\n        return df\ndf = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nshuffle(df)\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/64368604/shuffling-pandas-dataframe-columns"}
{"id": 435, "q": "Pandas dataframe: uniformly scale down values when column sum exceeds treshold", "d": "Initial Situation Consider the following example dataframe: which in printed form looks like: Desired Result I would now like to do the following for each column of this dataframe: Calculate the sum of the column's values (ignoring any NaN values). If the sum exceeds 10.0, then I want to uniformly scale down all values in the column such that the new sum is exactly 10.0 (again ignoring any NaN values). Basically I'd like to obtain a result dataframe that looks like this: Tried thus far The following code obtains the desired result. However this code feels a bit verbose and inefficient to me. Based on my experience with pandas thus far I'd suspect that a more vectorized solution is still possible. Would anyone be able to help me find this?", "q_apis": "values sum now sum values any values sum all values sum any values", "io": " A B C D 0 3.0 7.0 4.0 1.0 1 2.0 NaN 5.0 0.0 2 1.0 1.0 1.0 2.0 3 NaN 3.0 2.0 3.0 <s> A B C D 0 3.0 6.363636 3.333333 1.0 1 2.0 NaN 4.166667 0.0 2 1.0 0.909091 0.833333 2.0 3 NaN 2.727273 1.666667 3.0 ", "apis": "sum clip", "code": ["thres = 10\nresult = df * thres / df.sum().clip(lower=thres)\n"], "link": "https://stackoverflow.com/questions/57650114/pandas-dataframe-uniformly-scale-down-values-when-column-sum-exceeds-treshold"}
{"id": 474, "q": "Leading zero issues with pandas read_csv function", "d": "I have a column of values such as this: When I do or they both produce values like I was searching through stackexchange, and people say that you should use , but it doesn't work for me..", "q_apis": "read_csv values values", "io": "123, 234, 345, 456, 567 <s> 00123, 00234, 00345, 00456, 00567. ", "apis": "read_csv", "code": ["df = pd.read_csv('data.csv', converters={'ColName': str})\n"], "link": "https://stackoverflow.com/questions/45239357/leading-zero-issues-with-pandas-read-csv-function"}
{"id": 308, "q": "How to replace pandas dataframe values based on lookup values in another dataframe?", "d": "I have a large pandas dataframe with numerical values structured like this: I need to replace all of the the above cell values with a 'description' that maps to the field name and cell value as referenced in another dataframe structured like this: The desired output would be like: I could figure out a way to do this on a small scale using something like .map or .replace - however the actual datasets contain thousands of records with hundreds of different combinations to replace. Any help would be really appreciated. Thanks.", "q_apis": "replace values lookup values values replace all values name value map replace replace", "io": ">>> df1 A B C 0 2 1 2 1 1 2 3 2 2 3 1 <s> >>> df3 A B C 0 YES x BAD 1 NO y FINE 2 YES z GOOD ", "apis": "replace pivot columns index values to_dict replace pivot columns index values to_dict", "code": ["df1 = df1.replace(df2.pivot(columns='field_name', index='code', values='description')\n                     .to_dict())\n", "df1[cols] = df1[cols].replace(df2.pivot(columns='field_name',\n                                        index='code', values='description')\n                                 .to_dict())\n"], "link": "https://stackoverflow.com/questions/61233112/how-to-replace-pandas-dataframe-values-based-on-lookup-values-in-another-datafra"}
{"id": 517, "q": "How to get the Rank of current row compared to previous rows", "d": "How to get the Rank of current row compared to previous rows I have a dataframe like: I want to get the rank of current row compared to all previous rows for Volume Column. Desired Dataframe Data: pandas.DataFrame.rank Function doesnot serve my purpose.", "q_apis": "get get get rank all DataFrame rank", "io": "Instru Price Volume ABCD 1000 100258 ABCD 1000 100252 ABCD 1000 100168 ABCD 1000 100390 ABCD 1000 100470 ABCD 1000 100420 <s> Instru Price Volume Rank ABCD 1000 100258 1 => 1st Row so Rank 1 ABCD 1000 100252 2 => Rank 2 (Compare 100258,100252) ABCD 1000 100168 3 => Rank 3 (Compare 100258,100252,100168) ABCD 1000 100390 1 => Rank 1 (Compare 100390,100258,100252,100168) ABCD 1000 100470 1 => Rank 1 (Compare 100470,100390,100258,100252,100168) ABCD 1000 100420 2 => Rank 2 (Compare 100470,100420,100390,100258,100252,100168) ", "apis": "array searchsorted", "code": ["df['Rank'] = np.array([i - np.searchsorted(sorted(df.Volume[:i]), v) for i, v in enumerate(df.Volume)]) + 1\nprint(df)\n"], "link": "https://stackoverflow.com/questions/54733869/how-to-get-the-rank-of-current-row-compared-to-previous-rows"}
{"id": 604, "q": "indexing with multidimensional key pandas error", "d": "I have a dataframe as the following: I want to filter out g2 & g4 where all of 'B', 'C', 'D' values are 0 I tried returns output same as input, also returns output same as input, returns 'Cannot index with multi-dimensional key. The expected output is, What is the error here? Thanks in Advance.", "q_apis": "filter where all values index", "io": "A B C D E g1 1 -10 20 text1 g2 0 0 0 text2 g3 0 1 0 text3 g4 0 0 0 text4 <s> A B C D E g1 1 -10 20 text1 g3 0 1 0 text3 ", "apis": "", "code": ["print(df[['B', 'C', 'D']] == 0)\n\n       B      C      D\n0  False  False  False\n2   True  False   True\n"], "link": "https://stackoverflow.com/questions/51578845/indexing-with-multidimensional-key-pandas-error"}
{"id": 286, "q": "Remove duplicates from dataframe, based on two columns A,B, keeping row with max value in another column C", "d": "I have a pandas dataframe which contains duplicates values according to two columns (A and B): I want to remove duplicates keeping the row with max value in column C. This would lead to: I cannot figure out how to do that. Should I use , something else?", "q_apis": "columns max value contains values columns max value", "io": "A B C 1 2 1 1 2 4 2 7 1 3 4 0 3 4 8 <s> A B C 1 2 4 2 7 1 3 4 8 ", "apis": "groupby transform max loc", "code": ["c_maxes = df.groupby(['A', 'B']).C.transform(max)\ndf = df.loc[df.C == c_maxes]\n"], "link": "https://stackoverflow.com/questions/32093829/remove-duplicates-from-dataframe-based-on-two-columns-a-b-keeping-row-with-max"}
{"id": 28, "q": "How to get list of previous n values of a column conditionally in DataFrame?", "d": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result?", "q_apis": "get values DataFrame get all get", "io": "Subject Score 1 15 2 0 3 18 2 30 3 17 1 5 4 9 2 7 1 20 1 8 2 9 1 12 <s> Subject Score Previous 1 15 [] 2 0 [] 3 18 [] 2 30 [0] 3 17 [18] 1 5 [15] 4 9 [] 2 7 [30,0] 1 20 [5,15] 1 8 [20,5,15] 2 9 [7,30,0] 1 12 [8,20,5] ", "apis": "sort_values agg groupby rolling sort_values agg groupby rolling sort_index", "code": ["window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n", "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"], "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe"}
{"id": 437, "q": "Ignore Nulls in pandas map dictionary", "d": "My Dataframe looks like this : I am trying to label encode with nulls as such. My result should look like: The code i tried : Achieved Result : Expected Result :", "q_apis": "map", "io": "COL1 COL2 COL3 A M X B F Y NaN M Y A nan Y <s> COL1_ COL2_ COL3_ 0 0 0 1 1 1 NaN 0 1 0 nan 1 ", "apis": "join apply Series map dropna index dropna index dropna index add_suffix", "code": ["print(df.join(df.apply(lambda x: pd.Series(map(x.dropna().tolist().index, x.dropna()), index=x.dropna().index)).add_suffix('_')))\n"], "link": "https://stackoverflow.com/questions/57533654/ignore-nulls-in-pandas-map-dictionary"}
{"id": 593, "q": "In dataframe of strings AND floats, cast floats to integer then string", "d": "I have the following dataframe of dtype : I would like to cast all the floats to integers then convert everything to strings, so the output would be of dtype : Is there something that allows me to cast a dataframe to an int but ignore errors? Or achieve this in a different way? (using the , seems to ignore the entire thing)", "q_apis": "dtype all dtype", "io": " col1 col2 col3 0 1.1 3.3 spam 1 2.2 foo eggs 2 bar 4.4 5.5 <s> col1 col2 col3 0 1 3 spam 1 2 foo eggs 2 bar 4 5 ", "apis": "", "code": ["def try_to_int(obj):\n    try:\n        return str(int(float(obj)))\n    except (ValueError, TypeError):\n        return obj\n"], "link": "https://stackoverflow.com/questions/52085200/in-dataframe-of-strings-and-floats-cast-floats-to-integer-then-string"}
{"id": 476, "q": "Iterate over columns in python dataframe to do calculations and insert new columns between existing columns", "d": "I'm new to python and programming in general and can't seem to find a solution to my problem. I have a dataframe imported from an excel sheet with 15 rows of species and their number and 3 columns which are locations where they are found. That is a species by station matrix: I want to calculate for each column the top-10 species (index), their value, percentage of total in column, cumulative percentage and insert the new columns after each exististing column and return in one dataframe. This is the result I'm looking for (example with two first columns): I have managed to do this by calculating each column and make new data frames and using concat to merge the data frames together in the end using the following code: This code works, but my datasets are much larger with often more then 50 columns so I'm wondering if it possible to an iteration for each column that results in the same dataframe as shown above. Sorry for the long read.", "q_apis": "columns insert columns between columns columns where index value insert columns first columns concat merge columns", "io": " A1 A2 A3 Species 1 1259 600 151 Species 2 912 1820 899 Species 3 1288 1491 631 Species 4 36 609 1946 Species 5 1639 819 1864 Species 6 1989 748 843 Species 7 688 271 1206 Species 8 1031 341 756 Species 9 1517 1164 138 Species 10 1290 669 811 Species 11 16 409 1686 Species 12 329 521 954 Species 13 1782 958 1727 Species 14 464 1804 1105 Species 15 1002 1483 109 <s> Species A1 pct cum_pct Species A2 pct cum_pct 0 Species 6 1989 13 13 Species 2 1820 13 13 1 Species 13 1782 11 24 Species 14 1804 13 26 2 Species 5 1639 10 35 Species 3 1491 10 37 3 Species 9 1517 9 45 Species 15 1483 10 48 4 Species 10 1290 8 53 Species 9 1164 8 56 5 Species 3 1288 8 62 Species 13 958 6 63 6 Species 1 1259 8 70 Species 5 819 5 69 7 Species 8 1031 6 77 Species 6 748 5 75 8 Species 15 1002 6 83 Species 10 669 4 79 9 Species 2 912 5 89 Species 4 609 4 84 ", "apis": "append nlargest to_frame assign sum cumsum rename_axis reset_index concat append nlargest to_frame assign sum cumsum assign mul astype mul astype rename_axis reset_index concat", "code": ["frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n", "frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .assign(pct=lambda x: x['pct'].mul(100).astype(int),\n                          cum_pct=lambda x: x['cum_pct'].mul(100).astype(int))\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n"], "link": "https://stackoverflow.com/questions/56289734/iterate-over-columns-in-python-dataframe-to-do-calculations-and-insert-new-colum"}
{"id": 153, "q": "Splitting Dataframe based on duplicate values into multiple csv files", "d": "I have a dataset with multiple columns but only focusing on one column called 'VAL'. Every value in this column ranges from 0 to 4 so I would like to split this into 5 separate data frames based on those duplicate values and then export each of these data frames into individual csv files. I have been able to sort the numbers using pandas but now I need to divide up the values into smaller datasets keeping in mind that I have multiple files I would like to do this to so possibly a for loop? this is what I currently have as an output this is what I would like it to relatively look like", "q_apis": "values columns value values now values", "io": " A B C D E F G VAL FILE 954 380 158 166 431 201 769 0 001.csv 1142 348 203 962 0 878 1023 0 001.csv 1688 279 229 0 488 1007 0 0 001.csv 4792 371 420 29 372 0 745 0 001.csv 2106 352 76 196 388 0 695 0 001.csv ... ... ... ... ... ... ... ... 5634 441 283 277 788 45 585 4 001.csv 827 672 606 24 1023 463 742 4 001.csv 6703 324 203 0 623 214 726 4 001.csv 9056 604 398 0 981 0 633 4 001.csv 0 574 338 144 942 608 793 4 001.csv <s> A B C D E F G VAL FILE 954 380 158 166 431 201 769 0 val_0.csv 1142 348 203 962 0 878 1023 0 val_0.csv 1688 279 229 0 488 1007 0 0 val_0.csv 4792 371 420 29 372 0 745 0 val_0.csv 2106 352 76 196 388 0 695 0 val_0.csv A B C D E F G VAL FILE 5634 441 283 277 788 45 585 4 val_4.csv 827 672 606 24 1023 463 742 4 val_4.csv 6703 324 203 0 623 214 726 4 val_4.csv 9056 604 398 0 981 0 633 4 val_4.csv 0 574 338 144 942 608 793 4 val_4.csv ", "apis": "groupby to_csv index", "code": ["for group,data in df.groupby('VAL'):\n    data.to_csv(f\"val_{group}.csv\",index=False)\n"], "link": "https://stackoverflow.com/questions/59340489/splitting-dataframe-based-on-duplicate-values-into-multiple-csv-files"}
{"id": 563, "q": "Estimate the mean of a DataFrameGroupBy by only considering values in a percentile range", "d": "I need to estimate the mean of a pandas DataFrameGroupBy by only considering the values between a given percentile range. For instance, given the snippet the result is However, if a percentile range is picked to exclude the maximum and minimum values the result should be How can I filter, for each group, the values between an arbitrary percentile range before estimating the mean? For instance, only considering the values between the 20th and 80th percentiles.", "q_apis": "mean values mean values between values filter values between mean values between", "io": "m1 = 1 0 1 2.333333 2 2.333333 <s> m1 = 1 0 1 2 2 2 ", "apis": "DataFrame mean groupby apply reset_index quantile values between mean groupby apply reset_index concat", "code": ["import pandas as pd\nimport numpy as np\n\na = np.matrix('1 1; 1 2; 1 4; 2 1; 2 2; 2 4')\ndata = pd.DataFrame(a)\n\ndef jpp_np(df):\n    def meaner(x, lowperc, highperc):\n        low, high = np.percentile(x, [lowperc, highperc])\n        return x[(x > low) & (x < high)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndef jpp_pd(df):\n    def meaner(x, lowperc, highperc):\n        low, high = x.quantile([lowperc/100, highperc/100]).values\n        return x[x.between(low, high, inclusive=False)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndata = pd.concat([data]*10000)\n\nassert np.array_equal(jpp_np(data), jpp_pd(data))\n\n%timeit jpp_np(data)  # 11.2 ms per loop\n%timeit jpp_pd(data)  # 12.5 ms per loop\n"], "link": "https://stackoverflow.com/questions/53277378/estimate-the-mean-of-a-dataframegroupby-by-only-considering-values-in-a-percenti"}
{"id": 486, "q": "Operation on Pandas Dataframe columns using its Index", "d": "This should be relatively easy. I have a pandas dataframe (Dates): I would like to take the difference between Dates.index and Dates. The output would be like so: Naturally, I tried this: But I receive this lovely TypeError: Instead, I've written a loop to go column by column, but that just seems silly. Can anyone suggest a pythonic way to do this? EDIT", "q_apis": "columns Index take difference between index", "io": " A B C 1/8/2017 1/11/2017 1/20/2017 1/25/2017 1/9/2017 1/11/2017 1/20/2017 1/25/2017 1/10/2017 1/11/2017 1/20/2017 1/25/2017 1/11/2017 1/20/2017 1/25/2017 1/31/2017 1/12/2017 1/20/2017 1/25/2017 1/31/2017 1/13/2017 1/20/2017 1/25/2017 1/31/2017 <s> A B C 1/8/2017 3 12 17 1/9/2017 2 11 16 1/10/2017 1 10 15 1/11/2017 9 14 20 1/12/2017 8 13 19 1/13/2017 7 12 18 ", "apis": "apply to_datetime columns values apply index to_series", "code": ["df = df.apply(pd.to_datetime, axis=\"columns\")  # just to make sure values are datetime\ndf.apply(lambda x: x - df.index.to_series(), axis=\"rows)"], "link": "https://stackoverflow.com/questions/43096821/operation-on-pandas-dataframe-columns-using-its-index"}
{"id": 118, "q": "Insert complete repeated row under condition pandas", "d": "Basically, I'm trying to consider the third column (df1[3]) if the value is higher or equal to 2 I want to repeat i.e insert the whole row to a new row, not to replace. Here is the dataframe: desired output: code for the DataFrame and attempt to solve it: Obviously, the above-stated approach doesn't create a new row with the same values from each column but replaces it. Append() wouldn't solve it either because I do have to preserve the exact same order of the data frame. Is there anything similar to insert/extend/add or slicing approach in list when it comes to pandas dataframe?", "q_apis": "value repeat insert replace DataFrame values insert add", "io": " 1 2 3 0 5614 banana 1 1 4564 kiwi 1 2 3314 salsa 2 3 3144 avocado 1 4 1214 mix 3 5 4314 juice 1 <s> 1 2 3 1 5614 banana 1 2 4564 kiwi 1 3 3314 salsa 2 4 3314 salsa 2 5 3144 avocado 1 6 1214 mix 3 7 1214 mix 3 8 1214 mix 3 7 4314 juice 1 ", "apis": "count to_numeric fillna astype replace name loc index repeat count", "code": ["count = pd.to_numeric(df['3'], errors='coerce').fillna(0).astype(int)\n\n# replace '3' with actual column name\ndf.loc[df.index.repeat(count)]\n"], "link": "https://stackoverflow.com/questions/65797595/insert-complete-repeated-row-under-condition-pandas"}
{"id": 526, "q": "How to sum N columns in python?", "d": "I've a pandas df and I'd like to sum N of the columns. The df might look like this: I'd like to get a df like this: The A variable is not an index, but a variable.", "q_apis": "sum columns sum columns get index", "io": "A B C D ... X 1 4 2 6 3 2 3 1 2 2 3 1 1 2 4 4 2 3 5 ... 1 <s> A Z 1 15 2 8 3 8 4 11 ", "apis": "iloc join iloc sum rename", "code": ["df = df.iloc[:, [0]].join(df.iloc[:, 1:].sum(axis=1).rename('Z'))\n"], "link": "https://stackoverflow.com/questions/48749201/how-to-sum-n-columns-in-python"}
{"id": 276, "q": "group rows in a pandas data frame when the difference of consecutive rows are less than a value", "d": "I have a data frame like this, Now I want to group those rows where there difference between two consecutive col1 rows is less than 3. and sum other column values, create another column(col4) with the last value of the group, So the final data frame will look like, using for loop to do this is tedious, looking for some pandas shortcuts to do it most efficiently.", "q_apis": "difference value where difference between sum values last value", "io": "col1 col2 col3 1 2 3 2 3 4 4 2 3 7 2 8 8 3 4 9 3 3 15 1 12 <s> col1 col2 col3 col4 1 7 10 4 7 8 15 9 ", "apis": "groupby diff ge cumsum agg first sum sum last groups groupby diff ge cumsum groups agg first sum sum groups last", "code": ["(df.groupby(df.col1.diff().ge(3).cumsum(), as_index=False)\n   .agg(col1=('col1','first'),\n        col2=('col2','sum'),\n        col3=('col3','sum'),\n        col4=('col1','last'))\n)\n", "groups = df.groupby(df.col1.diff().ge(3).cumsum())\nnew_df = groups.agg({'col1':'first', 'col2':'sum','col3':'sum'})\nnew_df['col4'] = groups['col1'].last()\n"], "link": "https://stackoverflow.com/questions/62436677/group-rows-in-a-pandas-data-frame-when-the-difference-of-consecutive-rows-are-le"}
{"id": 348, "q": "How can I convert columns of a pandas DataFrame into a list of lists?", "d": "I have a pandas DataFrame with multiple columns. What I want to do is to convert this into a list like following 2u 2s 4r 4n 4m 7h 7v are column headings. It will change in different situations, so don't bother about it.", "q_apis": "columns DataFrame DataFrame columns", "io": "2u 2s 4r 4n 4m 7h 7v 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 <s> X = [ [0, 0, 1, 1, 1, 0], [1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1], [0, 1, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 1, 1, 1, 0], [1, 1, 0, 0, 0, 1] ] ", "apis": "values", "code": ["[list(l) for l in zip(*df.values)]\n"], "link": "https://stackoverflow.com/questions/15112234/how-can-i-convert-columns-of-a-pandas-dataframe-into-a-list-of-lists"}
{"id": 483, "q": "How can I sort numbers in a string in pandas?", "d": "I have a dataframe like this: I want to sort it in desending order like this I tried this: The above function do some kind of sorting but not the way I want it.", "q_apis": "", "io": "id String 1 345 -456 -13 879 2 158 -926 -81 249 35 -4 -53 9 3 945 -506 -103 <s> id String 1 879 345 -13 -457 2 249 158 35 9 -4 -53 -81 -926 3 945 -103 -506 ", "apis": "join map map map", "code": ["f = lambda x: ' '.join(map(str, sorted(map(int, x.split()), reverse=True)))\ndf['string'] = df['string'].map(f)\n"], "link": "https://stackoverflow.com/questions/56036178/how-can-i-sort-numbers-in-a-string-in-pandas"}
{"id": 191, "q": "How to add to dataframe column a dict?", "d": "Input dataframe: Following dataframe want as output: It is giving wrong output!", "q_apis": "add", "io": " Id Score Score1 0 19138359 0.5347029367015973 0.832428474443 1 12134001 0.9347094453553113 0.632535428479 <s> Id Scores 0 19138359 {'Score': 0.5347029367015973, 'Score1': 0.832428474443} 1 12134001 {'Score': 0.9347094453553113, 'Score1': 0.632535428479} ", "apis": "columns filter columns to_dict columns drop columns", "code": ["# Get score columns\nscore_columns = df.filter(like='Score').columns\n\n# Create dict of scores column\ndf['Scores'] = df[score_columns].to_dict(orient='records')\n\n# Drop original score columns\ndf.drop(columns=score_columns, inplace=True)\n"], "link": "https://stackoverflow.com/questions/64446862/how-to-add-to-dataframe-column-a-dict"}
{"id": 430, "q": "identify common elements between df rows to create a new column", "d": "My df is shown below. I want to create a new column called common which contains which other key has the same value as my current key. The final dataframe would look like: The only way I can think of is to create a column with empty dictionaries and then have two loops to get the result. I wanted to know if there is an easy way to do this. Thanks", "q_apis": "between contains value empty get", "io": " key val 0 A1 [1, 2, 3, 4] 1 A2 [1, 2, 7, 9] 2 A3 [1, 3, 5] 3 A4 [6, 9] 4 A5 [8] <s> key val common 0 A1 [1, 2, 3, 4] {'A2':[1, 2], 'A3':[1, 3]} 1 A2 [1, 2, 7, 9] {'A1':[1, 2], 'A3':[1], 'A4':[9], 'A5':[7]} 2 A3 [1, 3, 5] {'A1':[1, 3], 'A2':[1]} 3 A4 [6, 9] {'A2':[9]} 4 A5 [8] {} ", "apis": "reset_index drop to_dict groupby Series reindex values", "code": ["l={x: y.reset_index(level=0,drop=True).to_dict()for x , y in s.groupby(level=0)}\n\ndf['common']=pd.Series(l).reindex(df.Key).values\n"], "link": "https://stackoverflow.com/questions/57873651/identify-common-elements-between-df-rows-to-create-a-new-column"}
{"id": 2, "q": "pd.read_html changed number formatting", "d": "Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result", "q_apis": "read_html get", "io": " [ BBBBBB CCCCCCC AAAAAAA 0 DDDDDD 123456 1234.56 1 EEEEEEEEE 123456 1234.56 2 EEEEEEEEE 123456 1234.56 3 EEEEEEEEE 123456 1234.56 4 FFFFFFFFF 123456 1234.56 5 GGGGGGGGG 123456 1234.56 6 HHHHHHHHH 123456 1234.56 7 IIIIIIIIII 123456 1234.56 8 JJJJJJJJ 123456 1234.56 9 KKKKKKKK 1/2/3/4/5/6 1234.56 10 KKKKKKKK 1/2/3/4/5/6 1234.56] <s> [ BBBBBB CCCCCCC AAAAAAA 0 DDDDDD 1,2,3,4,5,6 1234.56 1 EEEEEEEEE 1,2,3,4,5,6 1234.56 2 EEEEEEEEE 1,2,3,4,5,6 1234.56 3 EEEEEEEEE 1,2,3,4,5,6 1234.56 4 FFFFFFFFF 1,2,3,4,5,6 1234.56 5 GGGGGGGGG 1,2,3,4,5,6 1234.56 6 HHHHHHHHH 1,2,3,4,5,6 1234.56 7 IIIIIIIIII 1,2,3,4,5,6 1234.56 8 JJJJJJJJ 1,2,3,4,5,6 1234.56 9 KKKKKKKK 1/2/3/4/5/6 1234.56 10 KKKKKKKK 1/2/3/4/5/6 1234.56] ", "apis": "div attrs read_html", "code": ["from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n"], "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting"}
{"id": 487, "q": "how to set Pandas to extract certain rows of certain columns and stack them on top of each other?", "d": "How can I extract certain columns and rows to stack them together? I created a simple exemplary dataframe with this data: This is what I would like to get: Another Format I would like to get is in two columns: The headers in the desired results are just for explanation", "q_apis": "columns stack columns stack get get columns", "io": " d1_d2_d3-t1-t2 1 101 2 201 3 102 4 202 5 103 6 203 <s> d1_d2_d3-t1-t2 d1_d2_d3-t3-t4 1 101 301 2 201 401 3 102 302 4 202 402 5 103 303 6 203 403 ", "apis": "loc melt DataFrame loc values ravel DataFrame loc values ravel loc values ravel", "code": ["idx = ['t1','t2']\ncols = ['d1','d2', 'd3']\ndf = dfa.loc[idx, cols].melt(value_name='data')[['data']]\n", "df = pd.DataFrame({'data': dfa.loc[idx, cols].values.ravel()})\n", "df = pd.DataFrame({'data1': dfa.loc[idx1, cols].values.ravel(),\n                   'data2': dfa.loc[idx2, cols].values.ravel()})\n"], "link": "https://stackoverflow.com/questions/55920437/how-to-set-pandas-to-extract-certain-rows-of-certain-columns-and-stack-them-on-t"}
{"id": 427, "q": "Count of rows where given columns of a DataFrame are non-zero", "d": "I have a that looks like this: I would like to have another matrix which gives me the number of non-zero elements for the intersection of every column except for . For example, the intersection of columns and would be 2 (because 1 and 3 have non-zero values for and ), intersection of and would be 2 as well (because 1 and 3 have non-zero values for and ). The final matrix would look like this: As we can see, it should be a symmetric matrix, similar to a correlation matrix, but not the correlation matrix. Intersection of any 2 columns = # of having non-zero values in both columns. I would show some initial code here but I feel like there would be a simple function to do this task that I don't know of. Here's the code to create the : Any pointers would be appreciated. TIA.", "q_apis": "where columns DataFrame intersection intersection columns values intersection values any columns values columns", "io": "MemberID A B C D 1 0.3 0.5 0.1 0 2 0 0.2 0.9 0.3 3 0.4 0.2 0.5 0.3 4 0.1 0 0 0.7 <s> A B C D A 3 2 2 2 B 2 3 3 2 C 2 3 3 2 D 2 2 2 3 ", "apis": "T dot", "code": ["z = (df != 0) * 1\nz.T.dot(z)\n"], "link": "https://stackoverflow.com/questions/38445974/count-of-rows-where-given-columns-of-a-dataframe-are-non-zero"}
{"id": 164, "q": "How to reorder rows by a condition in pandas?", "d": "I have two dataframes and one of their orders is correct for me. I want to make the other's order the same as the correct one. Here is the point, it's not about index numbers, order depends on a variable. Like this df1 df2 I want the order of df2 to be same as df1, I put them in a for loop but it took long time (my real data is much greater than reproducible example) Is there any easier way to make my wish real ? Thanks in advice.", "q_apis": "index put time any", "io": "A B 13 2 20 5 15 3 . . . . <s> A B 15 3 13 2 20 5 . . . . ", "apis": "DataFrame DataFrame set_index loc", "code": ["df1 = pd.DataFrame({\"A\": [13,20,15], \"B\": [2,5,3]})\ndf2 = pd.DataFrame({\"A\": [15,13,20], \"B\": [3,2,5]})\ndf2 = df2.set_index(\"A\")\nprint(df2.loc[df1[\"A\"]])\n"], "link": "https://stackoverflow.com/questions/64826052/how-to-reorder-rows-by-a-condition-in-pandas"}
{"id": 574, "q": "How to remove unwanted data from python panda data frame?", "d": "After reading my txt file: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale The panda datadframe like as below: But I need the data as below( Space and extra letter should be removed)", "q_apis": "", "io": "1 2 3 4 -0.4302012 2 -0.3233208 3 0.576837 4 0.426791 5 <s> 1 2 3 4 -0.4302012 -0.3233208 0.576837 0.426791 ", "apis": "read_csv drop map replace sample", "code": ["data = pd.read_csv(filepath,delimiter=':',header= None)\ndata = data.drop([0],axis=1)\nfor i in range(1,9):\n   search_string = str(i+1)\n   data[i] = data[i].map(lambda x: x.replace(search_string.rjust(1),''))\nprint(data)\ndata.sample(n=5)\n"], "link": "https://stackoverflow.com/questions/52800453/how-to-remove-unwanted-data-from-python-panda-data-frame"}
{"id": 55, "q": "How to edit Excel file using DataFrame and save it back as Excel file?", "d": "I have this Excel file. I also put the screenshot of my the file below. I want to edit the data on column with this 2 criteria: removing mark between the text. removing values. removing mark. So, for example, from this text: I want to make it look like this: Of course, I can do this manually one by one, but unfortunately because I have about 20 similar files that I have to edit, I can't do it manually, so I think I might need help from Python. My idea to do it on Python is to load the Excel file to a DataFrame, edit the data row by row (maybe using and method), and put the edit result back to original Excel file, or maybe generate a new one consisting an edited data column. But, I kinda have no idea on how to do code it. So far, what I've tried to do is this: read the Excel files to Python. read column in that Excel file. load it to a dataframe. Below is my current code. My question is how can I edit the data per row and put the edit result back again to original or a new Excel file? I have difficulties accessing the data because I can't get the string value. Is there any way in Python to achieve it?", "q_apis": "DataFrame put between values DataFrame put put get value any", "io": "['0', 'E3', 'F3', 'F#3 / Gb3', 'G3', 'G#3 / Ab3', 'A3', 'A#3 / Bb3', 'B3', 'C4', 'C#4 / Db4', 'D4'] <s> [E3, F3, F#3 / Gb3, G3, G#3 / Ab3, A3, A#3 / Bb3, B3, C4, C#4 / Db4, D4] ", "apis": "read_excel astype replace astype replace to_excel", "code": ["df = pd.read_excel(\"014_twinkle_twinkle 300 0.0001 dataframe.xlsx\")\ndf[\"pitch-class\"] = df[\"pitch-class\"].astype(str).str.replace(\"'0', \", \"\")\ndf[\"pitch-class\"] = df[\"pitch-class\"].astype(str).str.replace(\"'\", \"\")\ndf.to_excel(\"results.xlsx\")\n"], "link": "https://stackoverflow.com/questions/67105986/how-to-edit-excel-file-using-dataframe-and-save-it-back-as-excel-file"}
{"id": 570, "q": "Get minimum value from index in data frame", "d": "I have a dataframe like this. I would like to get minimum values for each value in column1. So my output would be When I try the code It gives me an empty dataframe and if I try it deletes some values, for reasons I don't understand. I use python 2.7", "q_apis": "value index get values value empty values", "io": "column1 column2 1 2 1 3 1 4 2 3 2 1 2 4 <s> column1 column2 1 2 2 1 ", "apis": "sort_values drop_duplicates last", "code": ["df = df.sort_values('column2', ascending=False).drop_duplicates('column1', keep='last')\n"], "link": "https://stackoverflow.com/questions/52985798/get-minimum-value-from-index-in-data-frame"}
{"id": 541, "q": "Ranking groups based on size", "d": "Sample Data: What I would like to do is replace the largest cluster id with and the second largest with and so on and so forth. Output would be as shown below. I'm not quite sure where to start with this. Any help would be much appreciated.", "q_apis": "groups size replace second where start", "io": "id cluster 1 3 2 3 3 3 4 3 5 1 6 1 7 2 8 2 9 2 10 4 11 4 12 5 13 6 <s> id cluster 1 0 2 0 3 0 4 0 5 2 6 2 7 1 8 1 9 1 10 3 11 3 12 4 13 5 ", "apis": "unique values argsort unique values array array array", "code": ["u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_counts=True\n)\n(-c).argsort()[i]\n", "u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_couns=True\n)\n", "array([2, 2, 2, 2, 0, 0, 1, 1, 1, 3, 3, 4, 5])\n", "array([3, 3, 3, 3, 1, 1, 2, 2, 2, 4, 4, 5, 6])\n", "array([2, 3, 4, 2, 1, 1])\n"], "link": "https://stackoverflow.com/questions/47402346/ranking-groups-based-on-size"}
{"id": 54, "q": "Python, Pandas: check each element in list values of column to exist in other dataframe", "d": "I have dataframe column with values in lists, want to add new column with filtered values from list if they are in other dataframe. df: df2: I need to add new column with filtered column in so that it contains lists with only elements which are in column . Result: Speed is crucial, as there is a huge amount of records. What I did for now: created a set of possible values Try to use with comprehensive lists, but it's not quite working and too slow. Appreciate any help. UPD In lists and df2 not always integer values, sometimes it's strings.", "q_apis": "values values add values add contains now values any values", "io": "**a**|**b** :-----:|:-----: 1|[10, 1, 'xxx'] 2|[] 5|[1, 2, 3] 7|[5] 9|[25, 27] <s> **a**|**b**|**c** :-----:|:-----:|:-----: 1|[10, 1, 'xxx']|[1,'xxx'] 2|[]|[] 5|[1, 2, 3]|[1] 7|[5]|[5] ", "apis": "map unique astype join map unique astype join", "code": ["l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall('|'.join([fr\"\\b{i}\\b\" for i in l]))\n", "l = map(str,df2['e'].unique())\ndf['c'] = df['b'].astype(str).str.findall(fr\"\\b({'|'.join(l)})\\b\")\n"], "link": "https://stackoverflow.com/questions/67111775/python-pandas-check-each-element-in-list-values-of-column-to-exist-in-other-da"}
{"id": 309, "q": "Pivot Table in Pandas with two column(Index and Value)", "d": "I have a CSV file with and column. I need to sum values for each and have output like below Input: Output: I have tried below code,As I have just two column to apply I added column with unique value to have pivot(pivot table need Index,Column and Value).Then column is just to help. However out put is sum thing weird!!! output of my code:", "q_apis": "Index sum values apply unique value pivot pivot Index put sum", "io": "+-----+------+ | obj | \u00a0VS \u00a0| +-----+------+ | B \u00a0 | 2048 | | A \u00a0 | 1024 | | B \u00a0 | \u00a0 10 | | A \u00a0 | 1024 | | B \u00a0 | 1025 | | A \u00a0 | 1026 | | B \u00a0 | 1027 | +-----+------+ <s> +---+------+ | A | 3074 | +---+------+ | B | 4110 | +---+------+ ", "apis": "read_csv dtype value dtypes to_numeric dtypes pivot_table values index columns value sum", "code": ["import pandas as pd \nimport numpy as np \n\nfilename='1test.csv'\ndf = pd.read_csv(filename, dtype='str')\ndf[\"value\"]=1\nprint(df.dtypes)\ndf['VS']=pd.to_numeric(df['VS'])\nprint(df.dtypes)\npd.pivot_table(df, values=\"VS\", index=\"obj\", columns=\"value\", aggfunc=np.sum)\n"], "link": "https://stackoverflow.com/questions/61195022/pivot-table-in-pandas-with-two-columnindex-and-value"}
{"id": 303, "q": "Pandas interpolate NaNs from zero to next valid value", "d": "I am looking for a way to linear interpolate missing values (NaN) from zero to the next valid value. E.g.: Given this table, i want the output to look like this: I've tried using fillna to fill only the next NaN to a valid value to 0 and to then linear interpolate the whole dataframe. The problem I'm facing here is that specifying a value and a limit with fillna won't affect consecutive NaNs, but limit the total amount of columns to be filled. If possible please only suggest solutions without iterating over each row manually since I'm working with large dataframes. Thanks in advance.", "q_apis": "interpolate value interpolate values value fillna value interpolate value fillna columns", "io": " A B C D E 0 NaN 2.0 NaN NaN 0 1 3.0 4.0 NaN NaN 1 2 NaN NaN NaN NaN 5 3 NaN 3.0 NaN NaN 4 <s> A B C D E 0 NaN 2.0 0 0 0 1 3.0 4.0 0 0.5 1 2 NaN NaN NaN NaN 5 3 NaN 3.0 0 2 4 ", "apis": "notnull cummax isnull astype diff fillna update where eq loc cummin eq replace update update interpolate", "code": ["m = (df.notnull().cummax(axis=1) & df.isnull()).astype(int).diff(axis=1).fillna(0)\nupdate = m.where(m.eq(1) & m.loc[:, ::-1].cummin(axis=1).eq(-1)).replace(1, 0)\n\ndf.update(update)  # Add in 0s\n\ndf = df.interpolate(axis=1, limit_area='inside')\n"], "link": "https://stackoverflow.com/questions/61530817/pandas-interpolate-nans-from-zero-to-next-valid-value"}
{"id": 412, "q": "Python List to Pandas DataFrame with Number &amp; Strings", "d": "If I have a following list (this list need the separator for each comma); And also another list; How can i get this desire output with python? Could you please help me about this?", "q_apis": "DataFrame get", "io": "[(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)] <s> A B C D E F G H 5461 1.20 A BR SK-EL 7 EP 146 E 52 0 ", "apis": "DataFrame columns DataFrame columns", "code": ["df = pd.DataFrame([list(x) for x in data], columns=cols)\n", "df = pd.DataFrame([[x for x in data]], columns=cols)\n"], "link": "https://stackoverflow.com/questions/58448671/python-list-to-pandas-dataframe-with-number-strings"}
{"id": 624, "q": "Pandas delete all rows which contains &quot;required value&quot; in all column", "d": "I have the following dataframe I want to delete all rows which contains all column a V or N Output data frame will be :-", "q_apis": "delete all contains value all delete all contains all", "io": " A B C D BUY 150 Q 2018 SELL 63 Q 2018 N N N N V v v v SELL 53 Q 2018 <s> A B C D BUY 150 Q 2018 SELL 63 Q 2018 SELL 53 Q 2018 ", "apis": "isin isin all dtype bool isin all dtype bool", "code": ["print (df.isin(['V', 'v', 'N', 'n']))\n       A      B      C      D\n0  False  False  False  False\n1  False  False  False  False\n2   True   True   True   True\n3   True   True   True   True\n4  False  False  False  False\n", "print (df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0    False\n1    False\n2     True\n3     True\n4    False\ndtype: bool\n", "print (~df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/50717904/pandas-delete-all-rows-which-contains-required-value-in-all-column"}
{"id": 361, "q": "Change value of only 1 cell based on criteria DataFrame", "d": "Based on a condition, I want to change the value of the first row on a certain column, so far this is what I have So I want to change only the first value of the column recibos by the value on a where (despesas['despesas']==a) & (despesas['recibos']=='') Edit 1 Example: And the result should be:", "q_apis": "value DataFrame value first first value value where", "io": "despesas['despesas'] = [11.95, 2.5, 1.2 , 0.6 , 2.66, 2.66, 3. , 47.5 , 16.95,17.56] recibos['recibos'] = [11.95, 1.2 , 1.2 , 0.2 , 2.66, 2.66, 3. , 47.5 , 16.95, 17.56] <s> [[11.95, 11.95], [2.5, null] , [1.2, 1.2] , [0.6, null] , [2.66, 2.66], [2.66, 2.66], [3., 3] , [47.5, 45.5 ], [16.95, 16.95], [17.56, 17.56]] ", "apis": "count index iterrows loc iloc index drop loc index", "code": ["from itertools import count, filterfalse\n\ndespesas['recibos'] =''\nfor index, a in despesas.iterrows():\n    if len(recibos.loc[recibos['recibos']==a['despesas']])>0:\n        despesas.iloc[index,1]=True\n        recibos.drop(recibos.loc[recibos['recibos']==a['despesas']][:1].index, inplace=True)\n"], "link": "https://stackoverflow.com/questions/59900864/change-value-of-only-1-cell-based-on-criteria-dataframe"}
{"id": 51, "q": "How to convert rows into columns and filter using the ID", "d": "I have a CSV file that looks like this: and I would like to use simple python or pandas to: Make each unique customer id in a separate row convert key_id to the columns titles and the values are the quantity The output table should look like this: I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips?", "q_apis": "columns filter unique columns values filter", "io": "customer_id | key_id. | quantity | 1 | 777 | 3 | 1 | 888 | 2 | 1 | 999 | 3 | 2 | 777 | 6 | 2 | 888 | 1 | <s> | 777 | 888 | 999 | 1 | 3 | 2 | 3 | 2 | 6 | 1 | 0 | ", "apis": "pivot_table index columns values max fillna", "code": ["df.pivot_table(\n    index='customer_id',\n    columns='key_id',\n    values='quantity',\n    aggfunc='max',\n).fillna(0)\n"], "link": "https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id"}
{"id": 553, "q": "Swapping of elements in a PANDAS dataframe", "d": "Given below is a table : This was originally an excel sheet which has been converted into a dataframe. I wish to swap some of the elements such that the A number column has only 9999574081. Therefore the output should look like : This is the code I have used : However, I am not getting the desired result. Please help me out. Thanks:)", "q_apis": "", "io": " A NUMBER B NUMBER 7042967611 9999574081 12320 9999574081 9999574081 9810256463 9999574081 9716551924 9716551924 9999574081 9999574081 8130945859 <s> A NUMBER B NUMBER 9999574081 7042967611 9999574081 12320 9999574081 9810256463 9999574081 9716551924 9999574081 9716551924 9999574081 8130945859 ", "apis": "loc loc values where", "code": ["m = df['A NUMBER'] != 9999574081\n\ndf.loc[m, ['A NUMBER','B NUMBER']] = df.loc[m, ['B NUMBER','A NUMBER']].values\n", "df['B NUMBER'] = np.where(df['A NUMBER'] != 9999574081, df['A NUMBER'], df['B NUMBER'])\ndf['A NUMBER'] = 9999574081\n"], "link": "https://stackoverflow.com/questions/53611917/swapping-of-elements-in-a-pandas-dataframe"}
{"id": 212, "q": "Interpolates one series, and outputs constant for the second (constant) series", "d": "I'm trying to create a function that fills in missing numbers in multiple series, with different numerical scales, and at the same time generates a constant column for each of the series. Is it possible to create the following function with Pandas? Sample dataframe: Expected output:", "q_apis": "second at time", "io": "1029 400 1035 400 1031 340 1039 340 1020 503 1025 503 <s> 1029 400 1030 400 1031 400 1032 400 1033 400 1034 400 1035 400 1031 340 1032 340 1033 340 1034 340 1035 340 1036 340 1037 340 1038 340 1039 340 1020 503 1021 503 1022 503 1023 503 1024 503 1025 503 ", "apis": "min max set_index reindex ffill reset_index lt shift cumsum concat groupby", "code": ["def fill(d):\n    idx = range(d['col1'].min(), d['col1'].max() + 1)\n    return d.set_index('col1').reindex(idx, method='ffill').reset_index()\n\n\ng = df['col1'].lt(df['col1'].shift()).cumsum()\ndf = pd.concat([fill(g) for k, g in df.groupby(g)], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/63867276/interpolates-one-series-and-outputs-constant-for-the-second-constant-series"}
{"id": 237, "q": "How to to fuzzy merge items from a list that repeat many times python pandas", "d": "I have a one column df called ```logos''' consisting of the following list: (note I have searched for similar questions on stackoverflow to no avail I would like to merge with the following df that consists of each item, minus the .png filename I would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df I am wondering how I should go about this considering the and aren't identical, and item in the df I would like to merge with is listed multiple times. Is there such thing as a fuzzy join in python like in R? Thanks in advance for any help.", "q_apis": "merge items repeat merge item merge item time identical item merge join any", "io": "0 ARI 1 ARI 2 ARI 3 DEN 4 DEN 5 DEN <s> 0 ARI ARI.png 1 ARI ARI.png 2 ARI ARI.png 3 DEN DEN.png 4 DEN DEN.png 5 DEN DEN.png ", "apis": "merge left", "code": ["df = df.merge(logos, how='left', left_on='column_name', right_on='no_ext')\n"], "link": "https://stackoverflow.com/questions/63241112/how-to-to-fuzzy-merge-items-from-a-list-that-repeat-many-times-python-pandas"}
{"id": 116, "q": "Pandas: Replace missing dataframe values / conditional calculation: fillna", "d": "I want to calculate a pandas dataframe, but some rows contain missing values. For those missing values, i want to use a diffent algorithm. Lets say: If column B contains a value, then substract A from B If column B does not contain a value, then subtract A from C results in: Approach 1: fill the NaN rows using : which results in SyntaxError: cannot assign to function call. Approach 2: fill the NaN rows using : is executed without errors and calculation is correct, these values are printed to the console: but the values are not written into , the datafram remains as is: What is the correct way of overwriting the values?", "q_apis": "values fillna values values contains value value assign values values values", "io": "print(df) a b c calc 0 1 1.0 2 0.0 1 2 1.0 2 -1.0 2 3 NaN 2 NaN 3 4 1.0 2 -3.0 <s> print(df['calc']) 0 0.0 1 -1.0 2 NaN 3 -3.0 ", "apis": "index iterrows iloc index isnull loc index where value where isnull", "code": ["for index, row in df.iterrows():\n    i = df['calc'].iloc[index]\n\n    if pd.isnull(row['b']):\n        i = row['c']-row['a']\n        print(i)\n    else:\n        i = row['b']-row['a']\n        print(i)\n    df.loc[index,'calc'] = i #<------------- here\n", "Pandas where() method is used to check a data frame for one or more condition and return the result accordingly. By default, The rows not satisfying the condition are filled with NaN value.", "df['calc'] = np.where(df['b'].isnull(), df['c']-df['a'], df['calc'])\n"], "link": "https://stackoverflow.com/questions/65811195/pandas-replace-missing-dataframe-values-conditional-calculation-fillna"}
{"id": 495, "q": "Python: Count combinations of values within two columns and find max frequency of each combination", "d": "My pandas dataframe looks like this: First, I need to add another column containing the frequency of each combination of Section and Group. It is important to keep all rows. Desired output: The second step would be marking the highest value within Count for each Section. For example, with a column like this: The original data frame has lots of rows. That is why I'm asking for an efficient way because I cannot think of one. Thank you very much!", "q_apis": "values columns max add all second step value", "io": "+-----+---------+-------+ | No. | Section | Group | +-----+---------+-------+ | 123 | 222 | 1 | | 234 | 222 | 1 | | 345 | 222 | 1 | | 456 | 222 | 3 | | 567 | 241 | 1 | | 678 | 241 | 2 | | 789 | 241 | 2 | | 890 | 241 | 3 | +-----+---------+-------+ <s> +-----+---------+-------+-------+ | No. | Section | Group | Count | +-----+---------+-------+-------+ | 123 | 222 | 1 | 3 | | 234 | 222 | 1 | 3 | | 345 | 222 | 1 | 3 | | 456 | 222 | 3 | 1 | | 567 | 241 | 1 | 1 | | 678 | 241 | 2 | 2 | | 789 | 241 | 2 | 2 | | 890 | 241 | 3 | 1 | +-----+---------+-------+-------+ ", "apis": "groupby transform size groupby transform max", "code": ["df['Count']=df.groupby(['Section','Group'])['Group'].transform('size')\ndf['Max']=df.groupby(['Section'])['Count'].transform('max')==df['Count']\ndf\nOut[508]: \n    No  Section  Group  Count    Max\n0  123      222      1      3   True\n1  234      222      1      3   True\n2  345      222      1      3   True\n3  456      222      3      1  False\n4  567      241      1      1  False\n5  678      241      2      2   True\n6  789      241      2      2   True\n7  890      241      3      1  False\n"], "link": "https://stackoverflow.com/questions/49266390/python-count-combinations-of-values-within-two-columns-and-find-max-frequency-o"}
{"id": 37, "q": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?", "d": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this?", "q_apis": "concat iterrows DataFrame columns add get where names", "io": "A B C 4 c 12 5 d 19 2 b 43 <s> 0 A B C A 2 NaN NaN NaN B b NaN NaN NaN C 43 NaN NaN NaN 0 NaN 4.0 c 12.0 1 NaN 5.0 d 19.0 ", "apis": "concat", "code": ["pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa"}
{"id": 519, "q": "Transfer pandas dataframe column names to dictionary", "d": "I'm trying to convert a pandas dataframe column names into a dictionary. Not so worried about the actual data in the dataframe. Say I have an example dataframe like this and I'm not too worried about index just now: I'd like to get an output of a dictionary like: Not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order.", "q_apis": "names names index now get get keys name", "io": "Col1 Col2 Col3 Col4 -------------------- a b c a b d e c <s> {'Col1': 0, 'Col2': 1, 'Col3': 2, 'Col4': 3} ", "apis": "columns", "code": ["{c: i for i, c in enumerate(df.columns)}\n"], "link": "https://stackoverflow.com/questions/54674956/transfer-pandas-dataframe-column-names-to-dictionary"}
{"id": 304, "q": "How to delete the first and last rows with NaN of a dataframe and replace the remaining NaN with the average of the values below and above?", "d": "Let's take this dataframe as a simple example: I would like first to remove first and last rows until there is no longer NaN in the first and last row. Intermediate expected output : Then, I would like to replace the remaining NaN by the mean of the nearest value below which is not a NaN, and the one above. Final expected output : I know I can have the positions of NaN in my dataframe through But I can't solve my problem. How please could I do ?", "q_apis": "delete first last replace values take first first last first last replace mean value", "io": " Col1 Col2 Col3 1 1.0 1.0 1.0 2 1.0 NaN NaN 3 2.0 NaN 5.0 4 3.0 3.0 1.0 <s> Col1 Col2 Col3 0 1.0 1.0 1.0 1 1.0 2.0 3.0 2 2.0 2.0 5.0 3 3.0 3.0 1.0 ", "apis": "notnull all at at loc idxmax idxmax ffill bfill", "code": ["# identify the rows with some NaN\ns = df.notnull().all(1)\n\n# remove those with NaN at beginning and at the end:\nnew_df = df.loc[s.idxmax():s[::-1].idxmax()]\n\n# average:\nnew_df = (new_df.ffill()+ new_df.bfill())/2\n"], "link": "https://stackoverflow.com/questions/61351795/how-to-delete-the-first-and-last-rows-with-nan-of-a-dataframe-and-replace-the-re"}
{"id": 454, "q": "python: populating tuples in tuples over dataframe range", "d": "I have 4 portfolios a,b,c,d which can take on values either \"no\" or \"own\" over a period of time. (code included below to facilitate replication) Summary of schedule: What I have tried: create a holding dataframe and filling in values based on the schedule. Unfortunately the first portfolio 'a' gets overridden desired output: I am sure there's an easier way of achieving this but probably this is an example I haven't encountered before. Many thanks in advance!", "q_apis": "take values time values first", "io": " portf base st end 0 a no 2018-01-01 2018-01-02 1 a own 2018-01-03 2018-01-04 2 b no 2018-01-01 2018-01-05 3 b own 2018-01-06 2018-01-07 4 c own 2018-01-09 2018-01-10 5 d own 2018-01-09 2018-01-09 <s> 2018-01-01 (('a','no'), ('b','no')) 2018-01-02 (('a','no'), ('b','no')) 2018-01-03 (('a','own'), ('b','no')) 2018-01-04 (('a','own'), ('b','no')) 2018-01-05 ('b','no') ... ", "apis": "reset_index melt index date set_index date groupby index resample ffill drop columns index reset_index pivot index date columns resample first between", "code": ["cols = ['portf', 'base']\ns = (df.reset_index()\n       .melt(cols+['index'], value_name='date')\n       .set_index('date')\n       .groupby(cols+['index'], group_keys=False)\n       .resample('D').ffill()\n       .drop(columns=['variable', 'index'])\n       .reset_index())\n\nres = s.pivot(index='date', columns='portf')\nres = res.resample('D').first()  # Recover missing dates between\n"], "link": "https://stackoverflow.com/questions/56915574/python-populating-tuples-in-tuples-over-dataframe-range"}
{"id": 325, "q": "Fastest way to calculate in Pandas?", "d": "Given these two dataframes: has no column names, but you can assume column 0 is an offset of and column 1 is an offset of . I would like to transpose onto to get the Start and End differences. The final dataframe should look like this: I have a solution that works, but I'm not satisfied with it because it takes too long to run when processing a dataframe that has millions of rows. Below is a sample test case to simulate processing 30,000 rows. As you can imagine, running the original solution (method_1) on a 1GB dataframe is going to be a problem. Is there a faster way to do this using Pandas, Numpy, or maybe another package? UPDATE: I've added the provided solutions to the benchmarks. Output:", "q_apis": "names transpose get sample test", "io": "df1 = Name Start End 0 A 10 20 1 B 20 30 2 C 30 40 df2 = 0 1 0 5 10 1 15 20 2 25 30 <s> Name Start End Start_Diff_0 End_Diff_0 Start_Diff_1 End_Diff_1 Start_Diff_2 End_Diff_2 0 A 10 20 5 10 -5 0 -15 -10 1 B 20 30 15 20 5 10 -5 0 2 C 30 40 25 30 15 20 5 10 ", "apis": "to_numpy to_numpy", "code": ["a = df1[['Start','End']].to_numpy()\nb = df2[[0,1]].to_numpy()\n"], "link": "https://stackoverflow.com/questions/60843541/fastest-way-to-calculate-in-pandas"}
{"id": 533, "q": "Add numbers with duplicate values for columns in pandas", "d": "I have a data frame like this: I found that there is duplicate value and its pqr. I want to add 1,2,3 where pqr occurs. The final data frame I want to achieve is: How to do it in efficient way", "q_apis": "values columns value add where", "io": "df: col1 col2 1 pqr 3 abc 2 pqr 4 xyz 1 pqr <s> df1 col1 col2 1 pqr1 3 abc 2 pqr2 4 xyz 1 pqr3 ", "apis": "mask duplicated loc mask groupby cumcount add astype", "code": ["mask = df['col2'].duplicated(keep=False)\ndf.loc[mask, 'col2'] += df.groupby('col2').cumcount().add(1).astype(str)\n"], "link": "https://stackoverflow.com/questions/54105419/add-numbers-with-duplicate-values-for-columns-in-pandas"}
{"id": 1, "q": "Pandas: Loop through rows to update column value", "d": "Here is sample dataframe look like: From this dataFrame I want to update value. Condition is when or is not immediate next value of will be replaced by previous value afterthat next point value should be reindexed(cycle .1 to .6). eg. in row index(2) when So, the next value should be also 0.3 instead of 0.4, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively) OUTPUT I want: Code I tried:", "q_apis": "update value sample update value value value value index value index", "io": ">>> df point x y 0 0.1 NaN NaN 1 0.2 NaN NaN 2 0.3 5.0 NaN 3 0.4 NaN NaN 4 0.5 NaN 1.0 5 0.6 NaN NaN 6 0.7 1.0 1.0 7 0.8 NaN NaN 8 0.9 NaN NaN 9 1.1 NaN NaN 10 1.2 NaN NaN 11 1.3 NaN NaN 12 1.4 NaN 2.0 13 1.5 NaN NaN 14 1.6 NaN NaN 15 1.7 NaN NaN 16 0.1 NaN NaN 17 0.2 NaN NaN 18 0.3 NaN NaN 19 0.4 NaN NaN 20 0.5 NaN NaN 21 0.6 2.0 NaN 22 0.7 NaN NaN 23 1.1 NaN NaN <s> point x y 0 0.1 NaN NaN 1 0.2 NaN NaN 2 0.3 5.0 NaN 3 0.3 NaN NaN 4 0.4 NaN 1.0 5 0.4 NaN NaN 6 0.5 1.0 1.0 7 0.5 NaN NaN 8 0.6 NaN NaN 9 1.1 NaN NaN 10 1.2 NaN NaN 11 1.3 NaN NaN 12 1.4 NaN 2.0 13 1.4 NaN NaN 14 1.5 NaN NaN 15 1.6 NaN NaN 16 0.1 NaN NaN 17 0.2 NaN NaN 18 0.3 NaN NaN 19 0.4 NaN NaN 20 0.5 NaN NaN 21 0.6 2.0 NaN 22 0.6 NaN NaN 23 1.1 NaN NaN ", "apis": "mask any shift astype sub shift ne cumsum sub mask groupby cumsum div", "code": ["mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)\npoint = df['point'].astype(int)\ngroup = point.sub(point.shift(1)).ne(0).cumsum()\n\ndf['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))\n"], "link": "https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value"}
{"id": 639, "q": "Removing random rows from a data frame until count is equal some criteria", "d": "I have a dataframe with data that I feed to a ML library in python. The data I have is categorized into 5 different tasks, t1,t2,t3,t4,t5. The data I have right now for every task is uneven, to simplify things here is an example. In the case above, I want to remove random rows with the task label of \"t1\" until there is an equal amount of \"t1\" as there is \"t2\" So after the code is run, it should look like this: What is the most clean way to do this? I could of course just do for loops and if conditions and use random numbers and count the occurances for each iteration, but that solution would not be very elegant. Surely there must be a way using functions of dataframe? So far, this is what I got:", "q_apis": "count right now count", "io": "task, someValue t1, XXX t1, XXX t1, XXX t1, XXX t2, XXX t2, XXX <s> task, someValue t1, XXX t1, XXX t2, XXX t2, XXX ", "apis": "value_counts min groupby head", "code": ["v = df['task'].value_counts().min()\ndf = df.groupby('task', as_index=False).head(v)\n"], "link": "https://stackoverflow.com/questions/50003791/removing-random-rows-from-a-data-frame-until-count-is-equal-some-criteria"}
{"id": 35, "q": "How to create columns from a string in a dataframe?", "d": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.", "q_apis": "columns get get", "io": " long string 0 ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho... 1 hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho... 2 ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha... <s> ha-tra ha-la hi-tra hi-la ho-tra ho-la 0 1 2 1 2 1 2 1 1 2 1 2 1 2 2 1 2 1 2 1 2 ", "apis": "droplevel set_index append set_axis unstack swaplevel columns columns map join", "code": ["ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"], "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe"}
{"id": 550, "q": "Check one-on-one relationship between two columns", "d": "I have two columns A and B in a pandas dataframe, where values are repeated multiple times. For a unique value in A, B is expected to have \"another\" unique value too. And each unique value of A has a corresponding unique value in B (See example below in the form of two lists). But since each value in each column is repeated multiple times, I would like to check if any one-to-one relationship exists between two columns or not. Is there any inbuilt function in pandas to check that? If not, is there an efficient way of achieving that task? Example: Here, for each 1 in A, the corresponding value in B is always 5, and nothing else. Similarly, for 2-->10, and for 3-->12. Hence, each number in A has only one/unique corresponding number in B (and no other number). I have called this one-on-one relationship. Now I want to check if such relationship exists between two columns in pandas dataframe or not. An example where this relationship is not satisfied: Here, 1 in A doesn't have a unique corresponding value in B. It has two corresponding values - 5 and 7. Hence, the relationship is not satisfied.", "q_apis": "between columns columns where values unique value unique value unique value unique value value any between columns any value unique between columns where unique value values", "io": "A = [1, 3, 3, 2, 1, 2, 1, 1] B = [5, 12, 12, 10, 5, 10, 5, 5] <s> A = [1, 3, 3, 2, 1, 2, 1, 1] B = [5, 12, 12, 10, 5, 10, 7, 5] ", "apis": "groupby", "code": [" gb = d.groupby('A')\n grouped_b_column = gb['B']\n"], "link": "https://stackoverflow.com/questions/53687204/check-one-on-one-relationship-between-two-columns"}
{"id": 64, "q": "How to convert pandas dataframe into the numpy array with column names?", "d": "How can I convert pandas into the following Numpy array with column names? This is my pandas DataFrame : I tried to convert it as follows: But it gives me the output as follows: For some reason, the rows of data are grouped instead of .", "q_apis": "array names array names DataFrame", "io": "col1 col2 3 5 3 1 4 5 1 5 2 2 <s> [(3, 5), (3, 1), (4, 5), (1, 5), (1, 2)]", "apis": "to_records index array dtype", "code": [">>> df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '<i8'), ('B', '<f8')])\n"], "link": "https://stackoverflow.com/questions/66934423/how-to-convert-pandas-dataframe-into-the-numpy-array-with-column-names"}
{"id": 447, "q": "group rows (dates) and summarize serveral columns (several measured values for eacht date) in Python Pandas", "d": "I use Python Pandas and load a table like this from Postgres: I want to group the Date rows using Pandas and summarize the values. The result should look like this I can group the dates and summarize the values separately, but not in one view. My results are And Thats the Code:", "q_apis": "columns values date values values view", "io": "2001-01-01 00:00:00 500 2001-02-01 00:00:00 160 <s> 1 220 2 280 3 160 ", "apis": "sum sum reset_index name sum sum sum reset_index name sum", "code": ["df1 = df.sum(axis=1).sum(level=0).reset_index(name='sum')\n\ndf1 = df.sum(level=0).sum(axis=1).reset_index(name='sum')\n"], "link": "https://stackoverflow.com/questions/57234137/group-rows-dates-and-summarize-serveral-columns-several-measured-values-for-e"}
{"id": 18, "q": "python pandas - creating a column which keeps a running count of consecutive values", "d": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.", "q_apis": "count values count values", "io": ". binary consec 1 0 0 2 1 1 3 1 2 4 1 3 5 1 4 5 0 0 6 1 1 7 1 2 8 0 0 <s> . binary consec 0 1 NaN 1 1 1 2 1 1 3 0 0 4 1 1 5 0 0 6 1 1 7 1 1 8 1 1 9 0 0 ", "apis": "dtype bool", "code": [">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"], "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val"}
{"id": 126, "q": "Python - Pandas: get row indices for a particular value in a column", "d": "Given a pandas dataframe, is there a way to get the indices of rows where a column has particular values? Consider the following toy example: CSV (save as test1.csv) What I currently have is this: Is there an option/functionality that can give me something like the following? (I want to be able to do this for large value lists, fast!) Desired output:", "q_apis": "get indices value get indices where values value", "io": "id,val1,val2 1,20,A 1,19,A 1,23,B 2,10,B 2,10,A 2,14,A <s> id val1 val2 0 1 20 A 1 1 19 A 2 1 23 B 3 2 10 B 4 2 10 A 5 2 14 A [0, 1, 2] [3, 4, 5] ", "apis": "index groupby", "code": ["{k: list(d.index) for k, d in df.groupby('id')}\n"], "link": "https://stackoverflow.com/questions/65669445/python-pandas-get-row-indices-for-a-particular-value-in-a-column"}
{"id": 264, "q": "Pandas merging rows / Dataframe Transformation", "d": "I have this example DataFrame: How would I be able to convert it to this: I am thinking maybe I should pivot by counting with lens or assigning a index that could be multiple of 3, but I really am not sure what would be the most efficient way.", "q_apis": "DataFrame pivot index", "io": "e col1 col2 col3 1 238.4 238.7 238.2 2 238.45 238.75 238.2 3 238.2 238.25 237.95 4 238.1 238.15 238.05 5 238.1 238.1 238 6 229.1 229.05 229.05 7 229.35 229.35 229.1 8 229.1 229.15 229 9 229.05 229.05 229 <s> 1 2 3 col1 col2 col3 col1 col2 col3 col1 col2 col3 1 238.4 238.7 238.2 238.45 238.75 238.2 238.2 238.25 237.95 2 238.1 238.15 238.05 238.1 238.1 238 229.1 229.05 229.05 3 229.35 229.35 229.1 229.1 229.15 229 229.05 229.05 229 ", "apis": "pop sub unique concat set_index groupby keys", "code": ["g, k = df.pop('e').sub(1) % 3 + 1, np.unique(g)\ndf1 = pd.concat([g.set_index(k) for _, g in df.groupby(g)], keys=k, axis=1)\n"], "link": "https://stackoverflow.com/questions/62848641/pandas-merging-rows-dataframe-transformation"}
{"id": 418, "q": "calculating percentile values for each columns group by another column values - Pandas dataframe", "d": "I have a dataframe that looks like below - Python script to get the dataframe below - I want to calculate certain percentile values for all the columns grouped by 'Year'. Desired output should look like - I am running below python script to perform the calculations to calculate certain percentile values- Output - How can I get the output in the required format without having to do extra data manipulation/formatting or in fewer lines of code?", "q_apis": "values columns values get values all columns values get", "io": " Year Salary Amount 0 2019 1200 53 1 2020 3443 455 2 2021 6777 123 3 2019 5466 313 4 2020 4656 545 5 2021 4565 775 6 2019 4654 567 7 2020 7867 657 8 2021 6766 567 <s> Name Value 0 Salary_0.05 1208.9720 1 Salary_0.1 1217.9440 2 Salary_0.25 1244.8600 3 Salary_0.5 1289.7200 4 Salary_0.75 1334.5800 5 Salary_0.95 1370.4680 6 Salary_0.99 1377.6456 7 Amount_0.05 53.2800 8 Amount_0.1 53.5600 9 Amount_0.25 54.4000 10 Amount_0.5 55.8000 11 Amount_0.75 57.2000 12 Amount_0.95 58.3200 13 Amount_0.99 58.5440 ", "apis": "pivot columns quantile stack", "code": ["(df.pivot(columns='Year')\n   .quantile([0.01,0.05,0.75, 0.95, 0.99])\n   .stack('Year')\n)\n"], "link": "https://stackoverflow.com/questions/58274940/calculating-percentile-values-for-each-columns-group-by-another-column-values"}
{"id": 174, "q": "Generate unique key from multiple dataframes based on name", "d": "I have two data frames. As you can see, the function merges it correctly, but it is wrong. Because the carid must be unique and must not be assigned twice. How can I solve this problem? It can appear several times in a data frame, but it must remain unique over two data records. So across all data records and not What I want", "q_apis": "unique name unique unique all", "io": "Carid = 1 = Mercedes-benz <s> Cardid = 1 = Mercedes-Benz & Citroen", "apis": "name any unique groupby columns groupby name first to_dict unique get name delta items values get max max keys delta name name delta", "code": ["import pandas as pd\nimport numpy as np\nfrom collections import ChainMap\n\n\ndef generate_new_keys(*args,key='Carid',name='Carname'):\n    \"\"\"\n    Takes in a number of dataframes and returns any duplicates with a new unique id. \n    groupby columns fixed to CarID and CarName.\n    \"\"\"\n    # adds dictionaries into a single list.\n    dicts_ = [arg.groupby(key)[name].first().to_dict() for arg in args]\n    #merges dicts on unique key, this will exclude duplicates.\n    merged_dicts = dict(ChainMap(*dicts_))\n    #get the duplicate and pass the name into a list.\n    delta = [v for each_dict in dicts_ for k,v in each_dict.items() if v not in merged_dicts.values()]\n    # get the max sequence key\n    start_key =  max(merged_dicts.keys()) + 1\n    # create a new sequence\n    sequence = range(start_key, start_key + len(delta) + 1)\n    # return a dictionary.\n    return {name : number for name,number in zip(delta,sequence)}\n    \n"], "link": "https://stackoverflow.com/questions/64663456/generate-unique-key-from-multiple-dataframes-based-on-name"}
{"id": 549, "q": "Rearrange rows of Dataframe alternatively", "d": "I have a dataframe looks like this: and I want to make it look like this: My own way to do it seems to take quite a few lines, aka not pythonic. My code: Is there any more pythonic way to accomplish the same result? Thank you in advance. EDIT: I'd like a more general method to deal with dataframe like this", "q_apis": "take any", "io": " col1 col2 0 1 random string 1 -1 random string 2 2 random string 3 -2 random string 4 3 random string 5 -3 random string 6 4 random string 7 -4 random string 8 5 random string 9 -5 random string 10 6 random string 11 -6 random string 12 7 random string 13 -7 random string 14 8 random string 15 -8 random string 16 9 random string 17 -9 random string 18 10 random string 19 -10 random string <s> col1 col2 0 1 random string 1 2 random string 2 3 random string 3 4 random string 4 5 random string 5 1x random string 6 2x random string 7 3x random string 8 4x random string 9 5x random string 10 1y random string 11 2y random string 12 3y random string 13 4y random string 14 5y random string ", "apis": "index index index sort_index reset_index drop", "code": ["n=5\n\ndf.index = df.index%n + (df.index//n)/(len(df)/n)\ndf = df.sort_index().reset_index(drop=True)\n"], "link": "https://stackoverflow.com/questions/53769189/rearrange-rows-of-dataframe-alternatively"}
{"id": 142, "q": "Unprecise values when using pd.Dataframe.values.tolist", "d": "When converting a pd.DataFrame to a nested list, some values are unprecise. pd.DataFrame examplary row: pd.DataFrame.values.tolist() of this row: How can this be explained and avoided?", "q_apis": "values values DataFrame values DataFrame DataFrame values", "io": "1.0 -3.0 -3.0 0.01 -3.0 -1.0 <s> [1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0] ", "apis": "values", "code": ["df.values.tolist()\n# [[1.0], [-3.0], [-3.0], [0.010000000000000009], [-3.0], [-1.0]]\n"], "link": "https://stackoverflow.com/questions/65223498/unprecise-values-when-using-pd-dataframe-values-tolist"}
{"id": 287, "q": "Read a Text file having row in parenthesis and values separated by comma using pandas", "d": "I want to read a text file which contains data in parenthesis as row and values in it as column.The format of txt file is below : I want the data in this format : When i am reading text file as csv file it reads all the data in one row only. It shows 1 row and all the columns. Please help me with this problem.", "q_apis": "values contains values all all columns", "io": "(a, b, c, d) (a1, b1, (c1,c12,c13), d1) (a2, b2, (c2,c22,c23), d2) (a3, b3, (c3,c32,c33), d3) (a4, b4, (c4,c42,c43), d4) <s> a b c d a1 b1 c1 d1 a2 b2 c2 d2 a3 b3 c3 d3 a4 b4 c4 d4 ", "apis": "read_csv read_csv rename columns values columns values columns iloc iloc replace last", "code": ["# Use the standard `read_csv` function of pandas.\n# Note the lineterminator option.\ndf = pd.read_csv('data.dat', sep=\",\", lineterminator=\")\")\n# rename the 1st column (remove 1st char)\ndf.columns.values[0] = df.columns.values[0][1:]\n# remove the opening parenthesis for the 1st columns:\ndf.iloc[:, 0] = df.iloc[:, 0].str.replace('^\\ ?\\(', '')\n# remove the last line:\ndf = df[:-1]  \nprint(df)\n"], "link": "https://stackoverflow.com/questions/62112525/read-a-text-file-having-row-in-parenthesis-and-values-separated-by-comma-using-p"}
{"id": 615, "q": "Map, Filter and Reduce procedures in Python", "d": "I am working through understanding the concepts of map, filter and reduce in Python. I am working in Spyder IDE with Python v3.6. I have a data frame: I want to select Cap records in increments of .005. Please see below: In this case, wouldn't a map function work in this case? Any other option would be great. I ideally need it to work in a way where I can incrementally select the records based on a certain value.", "q_apis": "map filter select map where select value", "io": "Cap OC_y GMWB PE Acc 0.01 0.0065 0.560840708 0.646683673 0.515243902 0.0105 0.0068 0.586725664 0.676530612 0.53902439 0.011 0.0071 0.612610619 0.706377551 0.562804878 0.0115 0.0073 0.629867257 0.72627551 0.578658537 0.012 0.0076 0.655752212 0.756122449 0.602439024 0.0125 0.0079 0.681637168 0.785969388 0.626219512 0.013 0.0082 0.707522124 0.815816327 0.65 0.0135 0.0085 0.73340708 0.845663265 0.673780488 0.014 0.0087 0.750663717 0.865561224 0.689634146 0.0145 0.009 0.776548673 0.895408163 0.713414634 0.015 0.0093 0.802433628 0.925255102 0.737195122 <s> Cap OC_y GMWB PE Acc 0.01 0.0065 0.560840708 0.646683673 0.515243902 0.015 0.0093 0.802433628 0.925255102 0.737195122 ", "apis": "astype iat astype", "code": ["v = df.Cap / 0.005\nout = df[np.isclose(v, v.astype(int))]\n", "v = (df.Cap - (df.Cap % 0.005).iat[0]) / 0.005\nout = df[np.isclose(v, v.astype(int))]\n"], "link": "https://stackoverflow.com/questions/51141459/map-filter-and-reduce-procedures-in-python"}
{"id": 65, "q": "Duplicate rows and rename DataFrame indexes using a list of suffixes", "d": "I have a pandas DataFrame object as follow: for which I'd like to duplicate each using a list of suffixes: The list of suffixes is: and the list of indexes that must be changed is: . Notice , and are left untouched from the original DataFrame. From this answer: https://stackoverflow.com/a/50490890/6630397 I was able to duplicate the desired rows of my initial DataFrame to the right number according to the length of the list : But now all my DataFrame indices are an array of 10 of each , , and (except for , and where there are only 1 row) where I'd like them to follow the pattern of suffixes from as shown here above. How could I elegantly achieve that with good performances? (note: if it's much better to work from a column containing the indexes it's also fine, because my objects , , , , , and in the index column previously come from a standalone column named ).", "q_apis": "rename DataFrame DataFrame left DataFrame DataFrame right length now all DataFrame indices array where where index", "io": " P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 object A NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 C NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 D NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 E NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 F NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 G NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 <s> P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 object A_XS NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_S NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_M NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_L NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A_XL NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 A NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_XS NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_S NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_M NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_L NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B_XL NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 B NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 1.0 C_XS NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_S NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_M NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_L NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C_XL NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 C NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 1.0 D_XS NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_S NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_M NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_L NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D_XL NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 D NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 1.0 E NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 1.0 F NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 1.0 G NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 1.0 ", "apis": "index reset_index assign explode set_index", "code": ["newvals = [['{}{}'.format(i,j) for j in ('_' + k if k != '' else k\n                                         for k in list_of_suffixes)]\n           if i in list_init else [i] for i in df.index]\ndf.reset_index().assign(object=newvals).explode('object').set_index('object')\n"], "link": "https://stackoverflow.com/questions/66929674/duplicate-rows-and-rename-dataframe-indexes-using-a-list-of-suffixes"}
{"id": 34, "q": "Python - Delete lines from dataframe (pandas)", "d": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should..", "q_apis": "delete delete drop drop", "io": " 0 1 2 0 9783630876672 12,35 2.62 1 9783423282789 11,67 6.07 2 9783833879500 17,25 12.40 3 9783898798822 6,91 1.16 4 9783453281417 12,93 2.84 5 9783630876672 12,35 4.08 6 9783423282789 11,67 6.07 7 9783833879500 17,25 9.94 8 9783898798822 6,91 2.96 9 9783453281417 12,93 2.68 10 3927905909 /// /// 11 3872948210 /// 0.15 12 9783293003781 /// 0.15 13 9783423246842 /// /// 14 9783423247146 /// /// 15 9783423246934 /// /// 16 387294116x /// /// 17 9783935597456 0,16 0.15 18 9783423204545 /// /// <s> 0 1 2 0 9783630876672 12,35 2.62 1 9783423282789 11,67 6.07 2 9783833879500 17,25 12.40 3 9783898798822 6,91 1.16 4 9783453281417 12,93 2.84 5 9783630876672 12,35 4.08 6 9783423282789 11,67 6.07 7 9783833879500 17,25 9.94 8 9783898798822 6,91 2.96 9 9783453281417 12,93 2.68 ", "apis": "replace astype loc ge all columns", "code": ["data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"], "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas"}
{"id": 462, "q": "Converting DataFrame to dictionary with header as key and column as array with values", "d": "I have a dataframe as follows: I want a dictionary like this: I have tried using the normal and it is no where close. If I use the transposed dataframe, hence it gets close, but I have something like this: The questions in stack overflow are limited to the dictionary having one value per key, not an array. It would be very valuable for me to use and avoid any for loop, since the database I am using is quite big and I want the computational complexity to be as low as possible.", "q_apis": "DataFrame array values where stack value array any", "io": "A B C 1 6 1 2 5 7 3 4 9 4 2 2 <s> {0: {A: 1, B: 6, C:1} , ... , 4:{A: 4, B: 2, C: 2 } } ", "apis": "to_numpy items", "code": ["{k: v.to_numpy() for k, v in df.items()}\n"], "link": "https://stackoverflow.com/questions/56762170/converting-dataframe-to-dictionary-with-header-as-key-and-column-as-array-with-v"}
{"id": 644, "q": "Replace value in Pandas Dataframe based on condition", "d": "I have a dataframe column with some numeric values. I want that these values get replaced by 1 and 0 based on a given condition. The condition is that if the value is above the mean of the column, then change the numeric value to 1, else set it to 0. Here is the code I have now: The target is the dataframe y. y is like so: and so on. mean_y is equal to 3.55. Therefore, I need that all values greater than 3.55 to become ones, and the rest 0. I applied this loop, but without success: The output is the following: What am I doing wrong? Can someone please explain me the mistake? Thank you!", "q_apis": "value values values get value mean value now all values", "io": " 0 0 16 1 13 2 12.5 3 12 <s> 0 0 16 1 13 2 0 3 12 ", "apis": "where mean", "code": ["dataset.myCol = np.where(dataset.myCol > dataset.myCol.mean(), 1, 0)\n"], "link": "https://stackoverflow.com/questions/49857470/replace-value-in-pandas-dataframe-based-on-condition"}
{"id": 288, "q": "How to sum single row to multiple rows in pandas dataframe using multiindex?", "d": "My dataframe with Quarter and Week as MultiIndex: I am trying to add the last row in Q1 (Q1-W04) to all the rows in Q2 (Q2-W15 through Q2-W18). This is what I would like the dataframe to look like: When I try to only specify the level 0 index and sumthe specific row, all Q2 values go to NaN. I have figured out that if I specify both the level 0 and level 1 index, there is no problem. Is there a way to sum the specific row to all the rows within the Q2 Level 0 index without having to call out each row individually by its level 1 index? Any insight/guidance would be greatly appreciated. Thank you.", "q_apis": "sum MultiIndex add last all index all values index sum all index index", "io": "Quarter Week X Y Z Q1 Q1-W01 1 1 1 Q1-W02 2 2 2 Q1-W03 3 3 3 Q1-W04 4 4 4 Q2 Q2-W15 15 15 15 Q2-W16 16 16 16 Q2-W17 17 17 17 Q2-W18 18 18 18 <s> Quarter Week X Y Z Q1 Q1-W01 1 1 1 Q1-W02 2 2 2 Q1-W03 3 3 3 Q1-W04 4 4 4 Q2 Q2-W15 19 19 19 Q2-W16 20 20 20 Q2-W17 21 21 21 Q2-W18 22 22 22 ", "apis": "loc loc loc values", "code": ["df.loc['Q2'] = (df.loc['Q2'] + df.loc['Q1', 'Q1-W04']).values.tolist()\n"], "link": "https://stackoverflow.com/questions/62053788/how-to-sum-single-row-to-multiple-rows-in-pandas-dataframe-using-multiindex"}
{"id": 236, "q": "pandas dataframe delete groups with more than n rows in groupby", "d": "I have a dataframe: I want to apply groupby based on columns type1, type2 and delete from the dataframe the groups with more than 2 rows. So the new dataframe will be: What is the best way to do so?", "q_apis": "delete groups groupby apply groupby columns delete groups", "io": "df = [type1 , type2 , type3 , val1, val2, val3 a b q 1 2 3 a c w 3 5 2 b c t 2 9 0 a b p 4 6 7 a c m 2 1 8 a b h 8 6 3 a b e 4 2 7] <s> df = [type1 , type2 , type3 , val1, val2, val3 a c w 3 5 2 b c t 2 9 0 a c m 2 1 8 ] ", "apis": "groupby filter", "code": ["df =df.groupby(['type1','type2']).filter(lambda x: len(x) <= 2) \n"], "link": "https://stackoverflow.com/questions/63259726/pandas-dataframe-delete-groups-with-more-than-n-rows-in-groupby"}
{"id": 392, "q": "How to create rows for unique values in columns in pandas?", "d": "I have a pandas dataframe with thousands of rows like so: I need all unique values in \"IntentName\" to have the same IntentID value like so: What is the easiest way to do this?", "q_apis": "unique values columns all unique values value", "io": "IntentID IntentName Query Response 1 Intent Name 1 Query 1 Response1 2 Intent Name 1 Query 1 Response2 3 Intent Name 2 Query 2 Response3 4 Intent Name 2 Query 2 Response4 5 Intent Name 3 Query 3 Response5 <s> IntentID IntentName Query Response 1 Intent Name 1 Query 1 Response1 1 Intent Name 1 Query 1 Response2 2 Intent Name 2 Query 2 Response3 2 Intent Name 2 Query 2 Response4 3 Intent Name 3 Query 3 Response5 ", "apis": "groupby transform first rank astype", "code": ["df['IntentID'] = df.groupby('IntentName') \\\n                    ['IntentID'].transform('first') \\\n                    .rank(method='dense') \\\n                    .astype('int')\n"], "link": "https://stackoverflow.com/questions/59149281/how-to-create-rows-for-unique-values-in-columns-in-pandas"}
{"id": 95, "q": "In panda python how do I &quot;blacklist&quot; or &quot;whitelist&quot; numbers in a DataFrame", "d": "I have two DataFrames, and I want to add a new column to the df with the first allowed numbers in df1, but only as many as are in each group, when it starts again at 1 it needs to look at the first number in Allowed_numbers. and want this: I have tried a few things and get this I was also considering some kind of mapping numbers against eachother, since 1 in order_out will always be 3 in y_goals. the order_out might also have different lengths of number row, not always up to 9.", "q_apis": "DataFrame add first at at first get", "io": " order_y y_goal 0 1 3 1 2 4 2 3 5 3 4 6 4 5 8 5 6 9 6 7 12 7 8 15 8 9 17 9 1 3 10 2 4 11 3 5 12 4 6 13 5 8 14 6 9 15 7 12 16 8 15 17 9 17 ... <s> order_out y_goal 0 1 3.0 1 2 4.0 2 3 5.0 3 4 6.0 4 5 8.0 5 6 9.0 6 7 12.0 7 8 15.0 8 9 17.0 9 1 24.0 10 2 28.0 11 3 29.0 12 4 30.0 13 5 NaN 14 6 NaN 15 7 NaN 16 8 NaN 17 9 NaN ... ", "apis": "reset_index drop", "code": ["df['y_goal'] = y_goal.reset_index(drop = True)\n"], "link": "https://stackoverflow.com/questions/66229269/in-panda-python-how-do-i-blacklist-or-whitelist-numbers-in-a-dataframe"}
{"id": 157, "q": "Merge multiple columns into one by placing one below the other based on column value pandas dataframe", "d": "I have the following dataframe df: Where the row is the row with the names of the columns in the dataframe (i.e. the row with bold font that states the names of each column). What I want is to rearrange this dataframe so that the output is this: I have tried searching different ways to append, concatenate, merge etc. the columns in the way that I want but I can't figure out how since there are multiple instances of each Video, i.e. multiple . So, for each of these multiple instances, I want to make one column of these with the Video number as the column name, and the rows be all the confidence values, as shown above. Is that possible?", "q_apis": "columns value names columns names append merge columns name all values", "io": "Video 1 1 1 1 1 1 1 1 1 1 ... 36 36 36 36 36 36 36 36 36 36 Confidence Value 3 3 4 4 4 5 5 3 5 3 ... 3 3 3 2 4 2 3 3 3 3 <s> Video 1 2 3 ... 36 0 3 5 4 ... 3 1 1 2 3 ... 2 2 2 4 4 ... 5 3 4 5 4 ... 3 ... ", "apis": "transpose groupby cumcount values transpose set_index pivot columns values transpose set_index pivot columns values", "code": ["idx = df.transpose().groupby(\"Video\").cumcount().values\nans = df.transpose().set_index(idx).pivot(columns=\"Video\", values=\"Confidence Value\")\n", "ans = df.transpose().set_index(np.tile(range(5), 4)).pivot(columns=\"Video\", values=\"Confidence Value\")\n"], "link": "https://stackoverflow.com/questions/64969344/merge-multiple-columns-into-one-by-placing-one-below-the-other-based-on-column-v"}
{"id": 188, "q": "Select highest member of close coordinates saved in pandas dataframe", "d": "I have a dataframe that has following columns: X and Y are Cartesian coordinates and Value is the value of element at these coordinates. What I want to achieve is to select only one coordinates out of that are close to other, lets say coordinates are close if distance is lower than some value , so the initial DF looks like this (example): distance is count with following function: lets say if we want to , the output dataframe would look like this: What is to be done: So I need to go through dataframe row by row, check the rest, select best match and then continue. I can't think about any simple method how to achieve this, this cant be use case of , since they are not duplicates, but looping over the whole DF will be very inefficient. One method I could think about was to loop just once, for each of rows finds close ones (probably apply countdistance()), select the best fitting row and replace rest with its values, in the end use . The other idea was to create a recursive function that would create a new DF, then while original df will have rows select first, find close ones, best match append to new DF, remove first row and all close from original DF and continue until empty, then return same function with new DF as to remove possible uncaught close points. These ideas are all kind of inefficient, is there a nice and efficient pythonic way to achieve this?", "q_apis": "columns value at select value count select any apply select replace values select first append first all empty all", "io": " X Y Value 0 0 0 6 1 0 1 7 2 0 4 4 3 1 2 5 4 1 6 6 5 5 5 5 6 6 6 6 7 7 4 4 8 8 8 8 <s> X Y Value 1 0 1 7 4 1 6 6 8 8 8 8 ", "apis": "columns copy copy empty all reset_index drop loc first loc add select select index select select idxmax select select loc select idxmax get append DataFrame select iloc add drop now overlaps all", "code": ["def recModif(self,df):\n  #columns=['','X','Y','Value']\n  new_df = df.copy()\n  new_df = new_df[new_df['Value']<0] #create copy to work with\n  changed = False\n  while not df.empty: #for all the data\n    df = df.reset_index(drop=True) #need to reset so 0 is always accessible\n    x = df.loc[0,'X'] #first row x and y\n    y = df.loc[0,'Y']\n    df['dist'] = self.countDistance(x,y,df['X'],df['Y']) #add column with distances\n    select = df[df['dist']<10] #number of meters that two elements cant be next to other \n    if(len(select.index)>1): #if there is more than one elem close\n      changed = True\n      #print(select,select['Value'].idxmax())\n    select = select.loc[[select['Value'].idxmax()]] #get the highest one\n    new_df = new_df.append(pd.DataFrame(select.iloc[:,:3]),ignore_index=True) #add it to new df\n    df = df[df['dist'] >= 10] #drop the elements now\n\n  if changed:\n    return self.recModif(new_df) #use recursion if possible overlaps\n  else: \n    return new_df #return new df if all was OK\n"], "link": "https://stackoverflow.com/questions/64464184/select-highest-member-of-close-coordinates-saved-in-pandas-dataframe"}
{"id": 8, "q": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column", "d": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.", "q_apis": "take names time get all names time all names at all all", "io": "NAME VAL1 VAL2 VAL3 player1 3 5 7 player2 2 6 8 player3 3 6 7 NAME VAL1 VAL2 VAL3 player2 5 7 7 player3 2 6 8 player5 3 6 7 <s> NAME VAL1 VAL2 VAL3 player1 3 5 7 NAME VAL1 VAL2 VAL3 player2 2 6 8 player2 5 7 7 NAME VAL1 VAL2 VAL3 player3 3 6 7 player3 2 6 8 NAME VAL1 VAL2 VAL3 player5 3 6 7 ", "apis": "concat groupby concat groupby", "code": ["dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n", "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"], "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base"}
{"id": 586, "q": "pandas dataframe take rows before certain indexes", "d": "I have a dataframe and a list of indexes, and I want to get a new dataframe such that for each index (from the given last), I will take the all the preceding rows that matches in the value of the given column at the index. The column c3 the indexes (row numbers) 2, 4 , 5 my new dataframe will be: Explanation: For index 2, rows 0,1,2 were selected because C3 equals in all of them. For index 4, no preceding row is valid. And for index 5 also no preceding row is valid, and row 6 is irrelevant because it is not preceding. What is the best way to do so?", "q_apis": "take get index last take all value at index index equals all index index", "io": " C1 C2 C3 0 1 2 A 1 3 4 A 2 5 4 A 3 7 5 B 4 9 7 C 5 2 3 D 6 1 1 D <s> C1 C2 C3 0 1 2 A 1 3 4 A 2 5 4 A 4 9 7 C 5 2 3 D ", "apis": "loc loc loc loc dropna loc loc loc dropna", "code": ["ind= 2\ncol ='C3'\n# \".loc[np.arange(ind+1)]\" creates indexes till preceding row, so rest of matching conditions can be ignored \ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n", "ind= 2\ncol ='C2'\ndf.loc[df.loc[ind][col] == df[col]].loc[np.arange(ind+1)].dropna()\n"], "link": "https://stackoverflow.com/questions/52354096/pandas-dataframe-take-rows-before-certain-indexes"}
{"id": 241, "q": "Way to show multiple spaces in Pandas Dataframe on Jupyter Notebook", "d": "When displaying Pandas Dataframe object on notebook, multiple spaces are shown as single space. And I cannot copy multiple spaces. I would like to show all spaces as they are and copy a string of them. Any way to do so? Actual My expectation", "q_apis": "copy all copy", "io": " 0 1 0 ab c ab c 1 ab c ab c <s> 0 1 0 ab c ab c 1 ab c ab c ", "apis": "style", "code": ["def print_df(df):\n    return df.style.set_properties(**{'white-space': 'pre'})\n\nprint_df(df)\n"], "link": "https://stackoverflow.com/questions/62283545/way-to-show-multiple-spaces-in-pandas-dataframe-on-jupyter-notebook"}
{"id": 90, "q": "Merge padas rows if the difference between consecutive rows are less than two", "d": "I have a data frame like this, the df is sorted by col1. Now for each col1 values of the previous or/and next row if difference between consecutive col3 values are less than 2 then merge col2 values in a single row. So the data frame would look like, This could be done using for loop by filtering col1 values each time but it will take more time to execute, looking for some pandas shortcuts to do it most efficiently.", "q_apis": "difference between values difference between values merge values values time take time", "io": "df col1 col2 col3 A [p,s] 2 A [q] 3 A [r,t] 4 A [p,x] 7 B [x,y] 8 C [s] 4 C [t,v] 6 C [u,x] 7 <s> df col1 col2 A [p,s,q,r,t] A [p,x] B [x,y] C [s] C [t,v,u,x] ", "apis": "groupby apply diff ge cumsum", "code": ["df['g'] = df.groupby('col1')['col3'].apply(lambda x: x.diff().ge(2).cumsum())\n"], "link": "https://stackoverflow.com/questions/66277212/merge-padas-rows-if-the-difference-between-consecutive-rows-are-less-than-two"}
{"id": 491, "q": "Split list element in dataframe over multiple rows", "d": "I have a df. I would like to get to a second dataframe, with only scalar values in . If and only if the original values was a list, I would like to spread it over multiple new rows, with the other values duplicated. e.g from: to: In Split set values from Pandas dataframe cell over multiple rows, I have learned how to do this for one columns, but how to handle the case where df has multiple columns which need to be duplicated as ?", "q_apis": "get second values values values duplicated values columns where columns duplicated", "io": " id foo 0 301 [a] 1 301 [b, c] 2 302 [e, f,33,'Z'] 3 303 42 <s> id foo 0 301 a 1 301 b 1 301 c 2 302 e 2 302 f 2 302 33 2 302 Z 3 303 42 ", "apis": "DataFrame values repeat pop apply Series stack reset_index drop rename join reset_index drop apply Series stack reset_index drop rename drop join reset_index drop DataFrame index T concat apply Series stack reset_index drop rename drop join reset_index drop mean std DataFrame values repeat mean std", "code": ["df['foo']  = [x if isinstance(x, list) else [x] for x in df['foo']]\n\nfrom itertools import chain\n\ndf = pd.DataFrame({\n    'id' : df['id'].values.repeat(df['foo'].str.len()),\n    'foo' : list(chain.from_iterable(df['foo'].tolist()))\n\n})\n", "s = df.pop('foo').apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.join(s).reset_index(drop=True)\n", "s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\ndf = df.drop('foo', axis=1).join(s).reset_index(drop=True)\n", "df=pd.DataFrame(data=[[301,301,302,303],[['a'],['b','c'],['e','f',33,'Z'],42]],index=['id','foo']).T\n\ndf = pd.concat([df] * 1000, ignore_index=True)\n\ndef f(df):\n    s = df['foo'].apply(pd.Series).stack().reset_index(level=1, drop=True).rename('foo')\n    return df.drop('foo', axis=1).join(s).reset_index(drop=True)\n\n\nIn [241]: %timeit (f(df))\n814 ms \u00b1 11.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [242]: %%timeit\n     ...: L  = [x if isinstance(x, list) else [x] for x in df['foo']]\n     ...: \n     ...: from itertools import chain\n     ...: \n     ...: pd.DataFrame({\n     ...:     'id' : df['id'].values.repeat([len(x) for x in L]),\n     ...:     'foo' : list(chain.from_iterable(L))\n     ...: \n     ...: })\n     ...: \n2.6 ms \u00b1 15.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/55829605/split-list-element-in-dataframe-over-multiple-rows"}
{"id": 298, "q": "Get column index nr from value in other column", "d": "I'm relativly new to python and pandas, so I might not have the full understanding of all possibilities and would appreciate a hint how to solve the following problem: I have a like this one: I want to construct a column which takes the column with the index according to and then multiplies the value in the selected column with the value in . I want to bulid a table like this ( beeing the column constructed): (row (), in row () and in row ()) The value in will always be an integer value, so i would love to use the position of a column according to the value in .", "q_apis": "index value all index value value value value value", "io": " Jan Feb Mar Apr i j a 100 200 250 100 1 0.3 b 120 130 90 100 3 0.7 c 10 30 10 20 2 0.25 <s> Jan Feb Mar Apr i j k a 100 200 250 100 1 0.3 60 b 120 130 90 100 3 0.7 70 c 10 30 10 20 2 0.25 2.5 ", "apis": "mul rename columns columns columns lookup index", "code": ["df['k'] = df['j'].mul(df.rename(columns = dict(zip(df.columns,\n                                                   range(len(df.columns)))))\n                        .lookup(df.index, df['i']))\nprint(df)\n"], "link": "https://stackoverflow.com/questions/60720542/get-column-index-nr-from-value-in-other-column"}
{"id": 600, "q": "How to slice rows from two pandas dataframes then merge them with some other value", "d": "I got two pandas dataframes and two indexes, and one datetime variable. What I would like to do is: slice the dataframes with the indexes, then I got two rows. combine the two rows to one row. add the variable to the row. then I can get new indexes and datetime values to form more rows, and assemble the rows to a new dataframe. Example: df1: df2: index: 3, 5, datetime: Output:", "q_apis": "merge value combine add get values index", "io": " A B 0 0 10 1 1 11 2 2 12 3 3 13 4 4 14 5 5 15 6 6 16 7 7 17 8 8 18 9 9 19 <s> C D 0 10 110 1 11 111 2 12 112 3 13 113 4 14 114 5 15 115 6 16 116 7 17 117 8 18 118 9 19 119 ", "apis": "index iloc index values iloc index values DataFrame columns date", "code": ["index = [3,5]\ndata = np.r_[df1.iloc[index[0]].values,df2.iloc[index[1]].values]\ndf = pd.DataFrame([data],columns = list('ABCD'))\ndt = datetime.datetime(2018, 8, 10, 16, 53, 52, 760014)\ndf['date'] = dt\n"], "link": "https://stackoverflow.com/questions/51780710/how-to-slice-rows-from-two-pandas-dataframes-then-merge-them-with-some-other-val"}
{"id": 133, "q": "Append loop output in column pandas python", "d": "I am working with the code below to append output to empty dataframe i am getting output as below but i want What i want the output to be How can i make 3 rows to 3 columns every time the loop repeats.", "q_apis": "append empty columns time", "io": " 0 0 30708 1 15 2 1800 0 19200 1 50 2 1180 <s> 0 1 2 0 30708 15 1800 1 19200 50 1180 ", "apis": "append DataFrame DataFrame", "code": ["# image_data starts as a list\nimage_data = []\n\nfor i in words:\n    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)\n    image_data.append(y)\n\n# And it ends as a DataFrame\nimage_data = pd.DataFrame(image_data)\n"], "link": "https://stackoverflow.com/questions/65530626/append-loop-output-in-column-pandas-python"}
{"id": 53, "q": "Moving data from rows to columns based on another column", "d": "I have a huge dataset with contents such as given below: HHID can be present in the file at a maximum of three times. If HHID is found once, then the VAL_CD64/VAL_CD32 should be moved to VAL1_CD64/VAL1_CD32 columns, if found 2nd time, second value should be moved to VAL2_CD64/VAL2_CD32 columns, and if found 3rd time, third value should be moved to VAL3_CD64/VAL3_CD32 columns. If value is not found, then these columns should be left blank. Output should look something like this: I tried using pivot/melt in pandas but unable to get an idea to implement it. Can anyone help in giving me a lead? Thanks", "q_apis": "columns at columns time second value columns time value columns value columns left pivot melt get", "io": "+------+------------------------------------------------------------------+----------------------------------+--+ | HHID | VAL_CD64 | VAL_CD32 | | +------+------------------------------------------------------------------+----------------------------------+--+ | 203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 373aeda34c0b4ab91a02ecf55af58e15 | | | 203 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | 6f3606577eadacef1b956307558a1efd | | | 203 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 332321ab150879e930869c15b1d10c83 | | | 720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | 2c4f97a04f02db5a36a85f48dab39b5b | | | 720 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | 2111293e946703652070968b224875c9 | | | 348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 5c80a555fcda02d028fc60afa29c4a40 | | | 348 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 6c10cd11b805fa57d2ca36df91654576 | | | 348 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 6040b29107adf1a41c4f5964e0ff6dcb | | | 403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | 71a91c4768bd314f3c9dc74e9c7937e8 | | +------+------------------------------------------------------------------+----------------------------------+--+ <s> +------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+ | HHID | VAL1_CD64 | VAL2_CD64 | VAL3_CD64 | VAL1_CD32 | VAL2_CD32 | VAL3_CD32 | | +------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+ | 203 | 8c5bfd9b6755ffcdb85dc52a701120e0876640b69b2df0a314dc9e7c2f8f58a5 | 0511dc19cb09f8f4ba3d140754dafb1471dacdbb6747cdb5a2bc38e278d229c8 | a18adc1bcae1b570a610b13565b82e5647f05fef8a4680bd6ccdd717cdd34af7 | 373aeda34c0b4ab91a02ecf55af58e15 | 6f3606577eadacef1b956307558a1efd | 332321ab150879e930869c15b1d10c83 | | | 720 | f6c581becbac4ec1291dc4b9ce566334b1cb2c85e234e489e7fd5e1393bd8751 | abad845107a699f5f99575f8ed43e0440d87a8fc7229c1a1db67793561f0f1c3 | | 2c4f97a04f02db5a36a85f48dab39b5b | 2111293e946703652070968b224875c9 | | | | 348 | 25c7cf022e6651394fa5876814a05b8e593d8c7f29846117b8718c3dd951e496 | 67d9c0a4bb98900809bcfab1f50bef72b30886a7b48ff0e9eccf951ef06542f9 | 05f1e412e7765c4b54a9acfd70741af545564f6fdfe48b073bfd3114640f5e37 | 5c80a555fcda02d028fc60afa29c4a40 | 6c10cd11b805fa57d2ca36df91654576 | 6040b29107adf1a41c4f5964e0ff6dcb | | | 403 | 3e8da3d63c51434bcd368d6829c7cee490170afc32b5137be8e93e7d02315636 | | | 71a91c4768bd314f3c9dc74e9c7937e8 | | | | +------+------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------+----------------------------------+----------------------------------+--+ ", "apis": "groupby agg columns DataFrame values index index columns map columns append concat", "code": ["df_ = df.groupby('HHID').agg({'VAL_CD32': list, 'VAL_CD64': list})\n\ndata = []\nfor col in df_.columns:\n    d = pd.DataFrame(df_[col].values.tolist(), index=df_.index)\n    d.columns = [f'{col}_{i}' for i in map(str, range(1, len(d.columns)+1))]\n    data.append(d)\n\nres = pd.concat(data, axis=1)\n"], "link": "https://stackoverflow.com/questions/67179293/moving-data-from-rows-to-columns-based-on-another-column"}
{"id": 154, "q": "Recovering DataFrame MultiIndex (from both row and column) after groupby", "d": "I have a dataframe that is multi indexed in that manner. I want to apply a function to each of its columns, for each date (so the 89.583458 and 49.828466 go together, 9.328360 and 10.058943 go together, and so forth) This gives me But now I need to recover the lost indices (to get back the same structure as the original), but failed at setting , unstacking or using pd.MultiIndex.from_frame. Any idea? Perhaps there's a better to get exactly that from the call?", "q_apis": "DataFrame MultiIndex groupby apply columns date now indices get at MultiIndex from_frame get", "io": " Value Size A B Market Cap 2019-07-01 AAPL 89.583458 9.328360 2.116356e+06 AMGN 49.828466 10.058943 1.395518e+05 2019-10-01 AAPL 74.297570 11.237253 2.116356e+06 AMGN 56.841946 10.237481 1.395518e+05 2019-12-31 AAPL 97.435257 14.736749 2.116356e+06 AMGN 71.400903 12.859612 1.395518e+05 <s> Market Cap ... B 2019-07-01 [[139551.76568603513], [139551.76568603513]] ... [[49.828465616227064], [49.828465616227064]] 2019-10-01 [[139551.76568603513], [139551.76568603513]] ... [[56.84194615992103], [56.84194615992103]] 2019-12-31 [[139551.76568603513], [139551.76568603513]] ... [[71.40090272484755], [71.40090272484755]] ", "apis": "date date date date date date index to_datetime get date get columns DataFrame get get get index index columns columns sub array sub sub values sub sub sub groupby apply groups groupby apply", "code": ["import pandas\nfrom scipy.stats.mstats import winsorize\n\n# Recreating your dataframe\ndata = [\n    {\"date\": \"2019-07-01\", \"group\": \"AAPL\", \"A\": 89.583458, \"B\": 9.328360, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-07-01\", \"group\": \"AMGN\", \"A\": 49.828466, \"B\": 10.058943, \"Market Cap\": 1.395518e+05},\n    {\"date\": \"2019-10-01\", \"group\": \"AAPL\", \"A\": 74.297570, \"B\": 11.237253, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-10-01\", \"group\": \"AMGN\", \"A\": 56.841946, \"B\": 10.237481, \"Market Cap\": 1.395518e+05},\n    {\"date\": \"2019-12-31\", \"group\": \"AAPL\", \"A\": 97.435257, \"B\": 14.736749, \"Market Cap\": 2.116356e+06},\n    {\"date\": \"2019-12-31\", \"group\": \"AMGN\", \"A\": 71.400903, \"B\": 12.859612, \"Market Cap\": 1.395518e+05},\n]\nindex = [\n    [pandas.to_datetime(line.get(\"date\")) for line in data],\n    [line.get(\"group\") for line in data],\n]\ncolumns = [\n    [\"Value\", \"Value\", \"Size\"],\n    [\"A\", \"B\", \"Market Cap\"]\n]\ndf = pandas.DataFrame(data=[[line.get(\"A\"), line.get(\"B\"), line.get(\"Market Cap\")] for line in data], index=index, columns=columns)\n\n\n# Your lambda function in a separate definition\ndef process_group(group):\n\n    # Nested\n    def _sub(sub):\n        # winsorize returns an numpy array, sub is a dataframe; sub[:] replaces the \"values\" of the dataframe, not the dataframe itself\n        sub[:] = winsorize(a=sub, limits=[0.4, 0.6])  # I didn't know your limits so I've guessed...\n        return sub\n\n    # Return the result of the processing on the nested group\n    return group.groupby(level=1, axis=1).apply(_sub)\n\n# Process the groups\ndf = df.groupby(level=0, axis=0).apply(process_group)\n"], "link": "https://stackoverflow.com/questions/64936694/recovering-dataframe-multiindex-from-both-row-and-column-after-groupby"}
{"id": 301, "q": "Parsing a list of lists to a data frame in pandas", "d": "I have list of lists. Below is how my list looks like, I want to parse it into a data frame with continuation of values with columns = A,B,C The expected data frame is as below Really appreciate the help.", "q_apis": "parse values columns", "io": "[ A B C 0 1 2 3 1 1 2 3 2 1 2 3 3 1 2 3 A B C 0 4 5 6 1 4 5 6 2 4 5 6 3 4 5 6 ] <s> A B C 0 1 2 3 1 1 2 3 2 1 2 3 3 1 2 3 4 4 5 6 5 4 5 6 6 4 5 6 7 4 5 6 ", "apis": "count count count append count concat", "code": ["import pandas as pd\nlist1=[]\ncount=0\nwhile count<len(myList):\n    list2= myList[count]\n    list1.append(list2)\n    #print(list2)\n    count+=1\ndf = pd.concat(list1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/61556827/parsing-a-list-of-lists-to-a-data-frame-in-pandas"}
{"id": 622, "q": "select individual rows from multiindex pandas dataframe", "d": "I am trying to select individual rows from a multiindex dataframe using a list of multiindices. For example. I have got the following dataframe: I would like to select all 'C' with (A,B) = [(1,1), (2,2)] My flawed code for this is as follows:", "q_apis": "select select select all", "io": " Col1 A B C 1 1 1 -0.148593 2 2.043589 2 3 -1.696572 4 -0.249049 2 1 5 2.012294 6 -1.756410 2 7 0.476035 8 -0.531612 <s> Col1 A B C 1 1 1 -0.148593 2 2.043589 2 2 7 0.476035 8 -0.531612 ", "apis": "join query", "code": ["filters = [(1,1), (2,2)]\nfilterstr = '|'.join(f'(A=={i})&(B=={j})' for i, j in filters)\nres = df.query(filterstr)\n\nprint(filterstr)\n\n(A==1)&(B==1)|(A==2)&(B==2)\n"], "link": "https://stackoverflow.com/questions/50890844/select-individual-rows-from-multiindex-pandas-dataframe"}
{"id": 584, "q": "Convert list of dict in dataframe to CSV", "d": "I have a dataframe that looks like this (df1): The detail column contains a list of dictionaries and each dictionary looks like this: I need to extract this information into a CSV with this format: In addition, the x2 and y2 in the extracted data should be like this: Expected output (Assuming the dict provided is in the first row of df1): I have tried this code to create a new df based off the detail column but I got an error: Error:", "q_apis": "contains first", "io": "{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5} <s> 1054, 78, 602, 549, 1232, 1113, 1, 5, 0.0 ", "apis": "DataFrame index columns loc loc", "code": ["a = pd.DataFrame(index=[78],columns=[\"detail\"])\na.loc[78,\"detail\"] = [{'y1': 549, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\na.loc[188,\"detail\"] = [{'y1': 649, 'score': 1, 'x2': 630, 'frame': 1054, 'y2': 564, 'x1': 602, 'visibility': 0.0, 'class': 5}]\n"], "link": "https://stackoverflow.com/questions/52489565/convert-list-of-dict-in-dataframe-to-csv"}
{"id": 428, "q": "Conditional pairwise calculations in pandas", "d": "For example, I have 2 dfs: df1 and another df is df2 I want to calculate first pairwise subtraction from df2 to df1. I am using using a function Now, I want to check, these new columns name like and . I am checking if there is any values in this new less than 5. If there is, then I want to do further calculations. Like this. For example, here for columns name , less than 5 value is 4 which is at . Now in this case, I want to subtract columns name of but at row 3, in this case it would be value . I want to subtract this value 2 with but at row 1 (because column name ) was from value at row 1 in . My is so complex for this. It would be great, if there would be some easier way in pandas. Any help, suggestions would be great. The expected new dataframe is this", "q_apis": "first columns name any values columns name value at columns name at value value at name value at", "io": "ID,col1,col2 1,5,9 2,6,3 3,7,2 4,8,5 <s> 0,1,2 Nan,Nan,Nan Nan,Nan,Nan (2-9)=-7,Nan,Nan (5-9)=-4,(5-7)=-2,Nan ", "apis": "DataFrame where values values index index columns columns", "code": ["pd.DataFrame(np.where(dist_df<5, df1.col2.values[:,None] - df2.col2.values, np.nan),\n             index=dist_df.index,\n             columns=dist_df.columns)\n"], "link": "https://stackoverflow.com/questions/57926657/conditional-pairwise-calculations-in-pandas"}
{"id": 379, "q": "Python: Convert two columns of dataframe into one interposed list", "d": "How can I convert two columns in a dataframe into an interposed list? ex: I want to do something like Closest I've found is but that returns a bunch of tuples in the list like this:", "q_apis": "columns columns", "io": ">>> [1, 2, 3, 4, 5, 6, 0, -1] <s> [(1, 2), (3, 4), (5, 6), (0, -1)]", "apis": "values array", "code": ["df.values.flatten()                                                                                                                                                                  \n# array([ 1,  2,  3,  4,  5,  6,  0, -1])\n"], "link": "https://stackoverflow.com/questions/59482469/python-convert-two-columns-of-dataframe-into-one-interposed-list"}
{"id": 193, "q": "Find the number of mutual friends in Python", "d": "I have a dataframe of users and their friends that looks like: I want to write a function in to compute the number of mutual friends for each pair: Currently I have: It works fine for small datasets, but I'm running it on a dataset with millions of rows. It takes forever to run everything. I know it's not the ideal way to find the count. Is there a better algorithm in Python? Thanks in advance!", "q_apis": "count", "io": "user_id | friend_id 1 3 1 4 2 3 2 5 3 4 <s> user_id | friend_id | num_mutual 1 3 1 1 4 1 2 3 0 2 5 0 3 4 1 ", "apis": "", "code": ["import networkx as nx\ng = nx.from_pandas_edgelist(df, \"user_id\",\"friend_id\")\nnx.draw_networkx(g)\n"], "link": "https://stackoverflow.com/questions/64405516/find-the-number-of-mutual-friends-in-python"}
{"id": 180, "q": "Is there a way to combine 9,12 or 15 columns from a single df into just 3?", "d": "I'm trying to convert a df that has the data divided every 3 columns into just three. An example is from this: To this:", "q_apis": "combine columns columns", "io": "C1 C2 C3 C4 C5 C6 C7 C8 C9 1 6 9 A D G 1A 6A 9A 2 7 10 B E H 2A 7A 10A 3 8 11 C F I 3A 8A 11A <s> C1 C2 C3 1 6 9 2 7 10 3 8 11 C4 C5 C6 A D G B E H C F I C7 C8 C9 1A 6A 9A 2A 7A 10A 3A 8A 11A ", "apis": "columns values values DataFrame", "code": ["arr = np.hsplit(np.vstack([df.columns.values, df.values]), 3)\npd.DataFrame(np.vstack(arr))\n"], "link": "https://stackoverflow.com/questions/64599227/is-there-a-way-to-combine-9-12-or-15-columns-from-a-single-df-into-just-3"}
{"id": 284, "q": "How to merge or join a stacked dataframe in pandas?", "d": "I cannot find this question answered elsewhere; I would like to do a SQL-like join in pandas but with the slight twist that one dataframe is stacked. I have created a dataframe A with a stacked column index from a csv file in pandas that looks as follows: The original csv had repeated what is in the 1st column for every entry like so: The original csv was the transposed version of this. Pandas chose to stack that when converting to dataframe. (I used this code: pd.read_csv(file, header = [0,1], index_col=0).T) In another csv/dataframe B I have for all of those so-called ticker symbols another ID that I would rather like to use: CIK. Desired output: I would like to have the CIK instead of the ticker in a new dataframe otherwise identical to A. Now in SQL I could easily join on A.name_of_2nd_column = b.Ticker since the table would just have the entry in the 1st column repeated in every line (like the original csv) and the column would have a name but in pandas I cannot. I tried this code: How do I tell pandas to use the 2nd column as the key and/or interpret the first column just as repeated values?", "q_apis": "merge join join index stack read_csv T all identical join name first values", "io": "| | | 2013-01-04 | 2013-01-07 | |----------:|-----:|-----------:|-----------:| | Adj Close | OWW | NaN | NaN | | Close | OXLC | 4.155157 | 4.147217 | | | OXM | 40.318089 | 42.988800 | | | OXY | 50.416079 | 62.934800 | <s> | | | 2013-01-04 | 2013-01-07 | |----------:|-----:|-----------:|-----------:| | Adj Close | OWW | NaN | NaN | | Close | OXLC | 4.155157 | 4.147217 | | Close | OXM | 40.318089 | 42.988800 | | Close | OXY | 50.416079 | 62.934800 | ", "apis": "reset_index assign map value dropna astype", "code": ["ticker_2_CIK = dict(zip(tix.Ticker,tix.CIK))  # create a dict\n\ntmp = df.reset_index().assign(CIK=lambda x: x['ticker'].map(ticker_2_CIK)) # use dict to find the correct value for colum \n\n# data was unclean, some ticker symbols were created after the period my data is from \n# and data was incomplete with some tickers missing\nsolution = tmp.dropna(subset=['CIK']).astype({'CIK':int})\n"], "link": "https://stackoverflow.com/questions/62011392/how-to-merge-or-join-a-stacked-dataframe-in-pandas"}
