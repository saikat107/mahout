{"id": 12, "q": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries", "d": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category?", "q_apis": "groups value cut values get all value copy DataFrame loc value", "io": "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]} <s> ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]} ", "apis": "name name name cut groupby value apply name", "code": ["def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"], "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction"}
{"id": 9, "q": "pandas group many columns to one column where every cell is a list of values", "d": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this?", "q_apis": "columns where values all columns columns get", "io": "df = c1 c2 c3 c4 c5 1. 2. 3. 1. 5 8. 2. 1. 3. 8 4. 9. 1 2. 3 <s> df = l [1,2,3,1,5] [8,2,1,3,8] [4,9,1,2,3] ", "apis": "values to_numpy agg apply", "code": ["#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"], "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values"}
{"id": 473, "q": "How to compress dataframe by removing columns that contains &#39;NaN&#39; value in between columns that has a value?", "d": "I am currently following the answer here. It mostly worked but when I viewed the whole dataframe, I saw that there are columns that contains 'NaN' values in between columns that do contain a value. For example I keep getting a result of something like this: Is there a way to remove those cells that contains NaN such that the output would be like this:", "q_apis": "columns contains value between columns value columns contains values between columns value contains", "io": " ID | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 300 1001|1001|1002| NaN | NaN | NaN |1001|1002| NaN | NaN | NaN 301 1010|1010|NaN | NaN | 1000 | 2000|1234| NaN| NaN | 1213 | 1415 302 1100|1234|5678| 9101 | 1121 | 3141|2345|6789| 1011 | 1617 | 1819 303 1000|2001|9876| NaN | NaN | NaN |1001|1002| NaN | NaN | NaN <s> ID | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 300 1001|1001|1002| 1001| 1002 | NaN |NaN | NaN| NaN | NaN | NaN 301 1010|1010|1000| 2000| 1234 | 1213|1415| NaN| NaN | NaN | NaN 302 1100|1234|5678| 9101| 1121 | 3141|2345|6789| 1011 | 1617 | 1819 303 1000|2001|9876| 1001| 1002 | NaN |NaN |NaN | NaN | NaN | NaN ", "apis": "columns concat dropna reset_index drop iterrows T", "code": ["import pandas as pd\n\ndf[df.columns] = pd.concat([s.dropna().reset_index(drop=True) for i,s in df.iterrows()], 1).T\n"], "link": "https://stackoverflow.com/questions/56488071/how-to-compress-dataframe-by-removing-columns-that-contains-nan-value-in-betwe"}
{"id": 466, "q": "How to find the last non zero element in every column throughout dataframe?", "d": "How can one go about finding the last occurring non zero element in every column of a dataframe? Input Output", "q_apis": "last last", "io": " A B 0 0 1 1 0 2 2 9 0 3 10 0 4 0 0 5 0 0 <s> A B 0 10 2 ", "apis": "values ne values argmax shape DataFrame columns columns shape ne values argmax values shape DataFrame columns columns", "code": ["first_max = df.values[df.ne(0).values.argmax(0), range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n", "row_ix = df.shape[0]-df.ne(0).values[::-1].argmax(0)-1\nfirst_max = df.values[row_ix, range(df.shape[1])]\nout = pd.DataFrame([first_max], columns=df.columns)\n"], "link": "https://stackoverflow.com/questions/56666271/how-to-find-the-last-non-zero-element-in-every-column-throughout-dataframe"}
{"id": 445, "q": "Dataframe to dictionary, values came out scrambled", "d": "I have a dataframe that contains two columns that I would like to convert into a dictionary to use as a map. I have tried multiple ways of converting, but my dictionary values always comes up in the wrong order. My python version is 3 and Pandas version is 0.24.2. This is what the first few rows of my dataframe looks like: I would like my dictionary to look like this: But instead my outputs came up with the wrong order for the values. I tried this first but the dictionary came out unordered: Then I tried coverting the two columns into list first then convert to dictionary, but same problem occurred: I tried converting to OrderedDict and that didn't work either. Then I've tried: I've also tried: I've also tried the answers suggested: panda dataframe to ordered dictionary None of these work. Really not sure what is going on?", "q_apis": "values contains columns map values first values first columns first ordered", "io": "{100100: 36276, 100124: 36310, 100460: 35005, 100460: 35062, 100460: 35214,...} <s> {100100: 98520, 100124: 36310, 100460: 57520, 100484: 35540, 100676: 19018, 100820: 57311, 100988: 15483, 101132: 36861,...} ", "apis": "", "code": ["defaultdict(<class 'list'>, {100100: [36276], 100124: [36310], 100460: [35005, 35062, 35214]})\n"], "link": "https://stackoverflow.com/questions/57264388/dataframe-to-dictionary-values-came-out-scrambled"}
{"id": 334, "q": "Show records contain multiple key words using OR or if elif (filter rows)", "d": "I am trying to show the rows that contain set of key words. The table look like this What I want is to filter this table where the row contain the tow strings (LD and AB) OR (LD and AD) OR (AC) So I get this result I tried This obviously didn't work , so I tried using the if function: and using this They didn't work So can someone help with what I made wrong", "q_apis": "filter filter where get", "io": "Col0 col1 col2 col3 1 LD AN CC 2 AB LD SS BB 1 AA LD AD CC 3 LD AC NN 2 FF UH BB <s> Col0 col1 col2 col3 2 AB LD SS BB 1 AA LD AD CC 3 LD AC NN ", "apis": "count isin contains contains contains contains contains contains contains contains contains contains contains contains", "code": ["df = df[count.isin([1,2]) (s.str.contains('LD') & s.str.contains('AB'))\n        | (s.str.contains('LD') & s.str.contains('AD'))\n        | (s.str.contains('LD') & s.str.contains('AC'))]\n", "df = df[(s.str.contains('LD') & s.str.contains('AB'))\n        | (s.str.contains('LD') & s.str.contains('AD'))\n        | (s.str.contains('LD') & s.str.contains('AC'))]\n"], "link": "https://stackoverflow.com/questions/60585417/show-records-contain-multiple-key-words-using-or-or-if-elif-filter-rows"}
{"id": 163, "q": "Pandas: Fill gaps in a series with mean", "d": "Given df I want to replace the nans with the inbetween mean Expected output: I have seen this_answer but it's for a grouping which isn't my case and I couldn't find anything else.", "q_apis": "mean replace mean", "io": " distance 0 0.0 1 1.0 2 2.0 3 NaN 4 3.0 5 4.0 6 5.0 7 NaN 8 NaN 9 6.0 <s> distance 0 0.0 1 1.0 2 2.0 3 2.5 4 3.0 5 4.0 6 5.0 7 5.5 8 5.5 9 6.0 ", "apis": "ffill bfill", "code": ["(df.ffill() + df.bfill()) / 2\n"], "link": "https://stackoverflow.com/questions/64828120/pandas-fill-gaps-in-a-series-with-mean"}
{"id": 280, "q": "create a nested dictionary from dataframe, where first column is the key for parent dictionary", "d": "I am trying to create a nested dictionary from a pandas dataframe. The first column-values are supposed to be the key for the upper dictionary, which will contaion the other columns as dictionary, where the column header is the key. I would like to avoid loops. the dataframe: what I would like to have: what I have tried: which unfortunately returns: Any help is highly appreciated. Thanks", "q_apis": "where first first values columns where", "io": "dict_expt = {'11': {'B': [1, 2, 3], 'C': [1.0, 0.7, 0.3]}, '12': {'B': [4, 5], 'C': [1.0]}} <s> {'B': {11: [1, 2, 3], 12: [4, 5]}, 'C': {11: [1.0, 0.7, 0.3], 12: [1.0]}} ", "apis": "groupby agg unique unique to_dict index", "code": ["df.groupby(['A']).agg({'B':lambda x: list(x.unique()),\n                      'C':lambda x: list(x.unique())}).to_dict(\"index\")\n"], "link": "https://stackoverflow.com/questions/62328295/create-a-nested-dictionary-from-dataframe-where-first-column-is-the-key-for-par"}
{"id": 305, "q": "How to replace values among blocks of consecutive values", "d": "I have a list like this: So in this list there are blocks of consecutive values, separated by . How can I replace the values before the maximum of each block, for example with -1. The result looks like:", "q_apis": "replace values values values replace values", "io": "list_tmp = [np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN] <s> list_tmp = [np.NaN, np.NaN, -1, -1, 3, np.NaN, -1, 2, np.NaN, np.NaN, -1, -1, -1, 4, np.NaN] ", "apis": "array array", "code": ["a = np.array([np.NaN, np.NaN, 1, 2, 3, np.NaN, 1, 2, np.NaN, np.NaN, 1, 2, 3, 4, np.NaN])\n\na[~np.isnan(a) & ~np.isnan(np.r_[a[1:],np.nan])] = -1\n", "print(a)\narray([nan, nan, -1., -1.,  3., nan, -1.,  2., nan, nan, -1., -1., -1., 4., nan])\n"], "link": "https://stackoverflow.com/questions/61348221/how-to-replace-values-among-blocks-of-consecutive-values"}
{"id": 568, "q": "How can check the duplication on the group level?", "d": "How can I check for duplicated groups and remove them? Here is my data frame: In this data frame group A and B are duplicate where as C is not because its forth element is different and thus it is deeper to be unique not duplicate, the resultant data frame should look like this: I tried to groupby and check for duplicates, but this will check the values on the observational level. How can check the duplication on the group level?", "q_apis": "duplicated groups where unique groupby values", "io": "Group Value_1 Value_2 A 17 0.1 A 20 0.8 A 22 0.9 A 24 0.13 B 17 0.1 B 20 0.8 B 22 0.9 B 24 0.13 C 17 0.1 C 20 0.8 C 22 0.9 C 26 0.11 <s> Group Value_1 Value_2 A 17 0.1 A 20 0.8 A 22 0.9 A 24 0.13 C 17 0.1 C 20 0.8 C 22 0.9 C 26 0.11 ", "apis": "groupby agg drop_duplicates index groupby agg drop_duplicates index groupby cumcount set_index unstack drop_duplicates index", "code": ["idx = df.groupby('Group').agg(frozenset).drop_duplicates().index\n#alternative solution\nidx = df.groupby('Group').agg(tuple).drop_duplicates().index\n", "g = df.groupby('Group').cumcount()\nidx = df.set_index(['Group',g]).unstack().drop_duplicates().index\n"], "link": "https://stackoverflow.com/questions/53062225/how-can-check-the-duplication-on-the-group-level"}
{"id": 23, "q": "Find unique column values out of two different Dataframes", "d": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read", "q_apis": "unique values unique values first", "io": "67 Hij 14 Xyz 87 Pqr <s> 43 Def 67 Lmn 14 Xyz ", "apis": "concat drop_duplicates", "code": ["unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"], "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes"}
{"id": 15, "q": "How to Create a Correlation Dataframe from already related data", "d": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this?", "q_apis": "first at corr between columns between value columns", "io": " 0 1 2 0 English Spanish 0.50 1 English Russian 0.15 <s> English Spanish Russian English 1 0.5 0.15 Spanish 0.5 1 - Russian 0.15 - 1 ", "apis": "concat unique crosstab update set_index squeeze unstack update set_index squeeze unstack T", "code": ["lg = pd.concat([df[0], df[1]]).unique()  # ['English', 'Spanish', 'Russian']\ncx = pd.crosstab(lg, lg)\n\ncx.update(df.set_index([0, 1]).squeeze().unstack())\ncx.update(df.set_index([0, 1]).squeeze().unstack().T)\n"], "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data"}
{"id": 577, "q": "Match a value in the column and return another column in pandas | python", "d": "I have an of two columns(tab-separted): And which have one column: what I want is to search an element from in , and return the corresponding value in . If there is more than one matched value, then return all together in one column. Here is what should the output file be like: would be left empty because it does not exist. Any suggestion will be helpful.", "q_apis": "value columns value value all left empty", "io": "c1\\tc2 aaa\\t232 65 19 32 bbew\\t32 22 20 jhsi\\t986 1 32 463 221 <s> 19 aaa 1 jhsi 32 aaa bbew jhsi 277 ", "apis": "lookup keys itertuples index map append DataFrame keys map join values reindex set_index reindex reset_index", "code": ["# use set for O(1) lookup\nscope_set = set(df2['c1'])\n\n# initialise defualtdict of lists\ndd = defaultdict(list)\n\n# iterate and create dictionary mapping numbers to keys\nfor row in df1.itertuples(index=False):\n    for num in map(int, row.c2.split()):\n        if num in scope_set:\n            dd[num].append(row.c1)\n\n# construct dataframe from defaultdict\ndf = pd.DataFrame({'num': list(dd), 'keys': list(map(' '.join, dd.values()))})\n\n# reindex to include blanks\ndf = df.set_index('num').reindex(sorted(scope_set)).reset_index()\n"], "link": "https://stackoverflow.com/questions/52705423/match-a-value-in-the-column-and-return-another-column-in-pandas-python"}
{"id": 296, "q": "Read large .json file with index format into Pandas dataframe", "d": "I was following this answer but after some discussion with it's writer, it seems it only gives a solution to data format. This is the difference: I have the index format because my data is from an SQL database read into a dataframe and the index field is needed to specify every records. My json file is 2.5 GB, had been exported from the dataframe with format. This means that the whole file is actually one huge string and not a list like collection of records: This means I can't use any line or chunck based iterative solution like this: According to the documentation, can only be used if the records are in a list like format, this is why does not even accept this argument unless the orient is not . The restriction for comes from this as well, it says: And exactly this is the reason of the question, trying to read such a huge .json file gives back: I was thinking about adding the index values as a first column as well, this case it wouldn't be lost with the records format; or maybe even store an index list separately. Only I fear it would decrease the search performance later on. Is there any solution to handle the situation strictly using the .json file and no other database or big-data based technology? Update #1 For request here is the actual structure of my data. The SQL table: The pandas pivot table is almost the same as in the example, but with a 50,000 rows and 4,000 columns: And this is how it is saved with an index formatted json: Only I could not give the arg, so it is actually cramped into one huge string making it a one-liner json:", "q_apis": "index difference index index any index values first index any pivot columns index", "io": "{ \"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, \"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...}, \"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, ... \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...} } <s> {\"0\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...},\"455\":{\"1969-w01\":1,\"1969-w02\":0,\"1969-w03\":3,\"1969-w04\":0, ...},\"40036\":{\"1969-w01\":0,\"1969-w02\":0,\"1969-w03\":0,\"1969-w04\":0, ...}, ... \"217568\":{\"1969-w01\":0,\"1969-w02\":1,\"1969-w03\":0,\"1969-w04\":2, ...}} ", "apis": "transpose read_json test T", "code": ["# read and transpose!\ndf = pd.read_json(\"test.json\").T\n"], "link": "https://stackoverflow.com/questions/61800463/read-large-json-file-with-index-format-into-pandas-dataframe"}
{"id": 217, "q": "Python: Combine two Pandas Dataframes, extend index if needed", "d": "How can I combine two Dataframes into one with keping all rows and all index values of both Dataframes? Let's say, I have two dataframes, with partly different index values: I want to create a new dataframe, which contains both columns with a combined index. I tried: which results in: where I would like to have, all rows & index values preserved, with NaN, if no value is available for this index value:", "q_apis": "index combine all all index values index values contains columns index where all index values value index value", "io": " a b 0 -1.089084 NaN 2 -0.552297 1.704591 3 -0.242239 -0.803438 4 0.247463 -1.511515 5 -0.139740 NaN <s> a b 0 -1.089084 NaN 1 NaN -0.407245 2 -0.552297 1.704591 3 -0.242239 -0.803438 4 0.247463 -1.511515 5 -0.139740 NaN 6 NaN 0.303360 ", "apis": "concat", "code": ["dd = pd.concat([df1, df2], axis=1)\nprint(dd)\n"], "link": "https://stackoverflow.com/questions/48475687/python-combine-two-pandas-dataframes-extend-index-if-needed"}
{"id": 234, "q": "Pandas - Drop lines from dataframe if column value is in list (.csv)", "d": "I have a pandas dataframe imported from SQL, and I would like to drop lines for which a column value is in a list, which I get from a csv file. It seems pretty straighforward, I looked it up and I tried several things using but this is not working as I expect. For example the dataframe imported from SQL looks like this, let's call it df : I import this list this way : Let's assume I print the list, this is what I see : Then I use the following : I would expect to get this (initial df with lines 1 and 2 dropped because they are in the list) However this is not what happens. I get the exact same df as initially with no lines dropped, and also no error message of any kind. What am I doing wrong ? I thought the data in the list and in the df column might not be the same type and I tried fiddling with , but without much success. Perhaps i'm using it wrong. Would appreciate any help. Thanks !", "q_apis": "value drop value get get get any any", "io": " SKU Brand 0 AD31KL-A1 BrandA 1 BC31KL-B3 BrandB 2 DE31KL-D4 BrandC 3 FG31KL-F5 BrandD <s> SKU Brand 0 AD31KL-A1 BrandA 3 FG31KL-F5 BrandD ", "apis": "apply DataFrame columns DataFrame iloc first", "code": ["list = df2.apply(lambda x: x.tolist(), axis=1)\n", "# Well, I don't have list.csv, so let me just create a dataframe\ndf = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ], columns = ['SKU'] )\nprint(df)\nlist =  df['SKU'].tolist() \nprint( list ) \n", "df = pd.DataFrame( ['AD31KL-A1','BC31KL-B3','DE31KL-D4','FG31KL-F5' ] )\nprint(df)\nlist =  df.iloc[:, 0].tolist()  # first column of dataframe\nprint( list ) \n"], "link": "https://stackoverflow.com/questions/63286249/pandas-drop-lines-from-dataframe-if-column-value-is-in-list-csv"}
{"id": 387, "q": "How to append a list in Pandas?", "d": "I'm reading a dataframe and trying to insert a list inside another list and then converting it to json file. I'm using python 3 and 0.25.3 version of pandas for it. ============================ Data that I'm reading: ============================ Here is my code: ============================= What is expected: ============================ What I'm getting: ====================== I tried to do the same thing using function (and posted a question here 'Dataframe and conversion to JSON using Pandas'), but some people recommend me to try another way using another function. I know that is a stupid thing add object inside my , but I already tried of others way. Could you help me?", "q_apis": "append insert add", "io": "[{ \"id\": 6, \"label\": \"Sao Paulo\", \"Customer\": [{ \"id\": \"CUS-99992\", \"label\": \"Brazil\", \"number\": [{ \"part\": \"7897\", \"client\": \"892\" }, { \"part\": \"888\", \"client\": \"12\" }] }] }, { \"id\": 92, \"label\": \"Hong Kong\", \"Customer\": [{ \"id\": \"CUS-88888\", \"label\": \"China\", \"number\": [{ \"part\": \"147\", \"client\": \"288\" }] }] }] <s> [{ \"id\": 6, \"label\": \"Sao Paulo\", \"Customer\": [{ \"id\": \"CUS-99992\", \"label\": \"Brazil\" }], \"number\": [{ \"part\": \"7897\", \"client\": \"892\" }], \"number\": [{ \"part\": \"888\", \"client\": \"12\" }] }, { \"id\": 92, \"label\": \"Hong Kong\", \"Customer\": [{ \"id\": \"CUS-88888\", \"label\": \"China\" }], \"number\": [{ \"part\": \"147\", \"client\": \"288\" }] }] ", "apis": "eval to_json iloc iloc astype astype rename columns groupby apply to_json values", "code": ["def getNum(grp):\n    return eval(grp[['part', 'client']].to_json(orient='records'))\n", "def getCust(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id_customer, 'label': r0.label_customer, 'number': getNum(grp) }\n", "def getGrp(grp):\n    r0 = grp.iloc[0]\n    return { 'id': r0.id, 'label': r0.label, 'Customer': getCust(grp) }\n", "df.part_number = df.part_number.astype('str')\ndf.number_client = df.number_client.astype('str')\n", "df.rename(columns={'part_number': 'part', 'number_client': 'client'})\\\n    .groupby(['id', 'label', 'id_customer', 'label_customer'])\\\n    .apply(getGrp).to_json(orient='values')\n"], "link": "https://stackoverflow.com/questions/59254205/how-to-append-a-list-in-pandas"}
{"id": 395, "q": "In python is there a way to delete parts of a column?", "d": "I want to trim the values of a pandas data frame. For example, I have the following: And I would like the result to be: If anyone could help it would be very appreciated.", "q_apis": "delete values", "io": " A B C 33344-10 5555-78 999902 3444441 5555679 2334 2334 5555 3344 <s> A B C 3334 5555 9999 3444 5555 2334 2334 5555 3344 ", "apis": "stop", "code": ["for c in ['A', 'B', 'C']:\n    df[c] = df[c].str.slice(stop=4)\n"], "link": "https://stackoverflow.com/questions/59036924/in-python-is-there-a-way-to-delete-parts-of-a-column"}
{"id": 131, "q": "How to use pandas dataframe as condition for other dataframe", "d": "Say I have dataframe A: and dataframe B: How can I use dataframe A as a condition for dataframe B, so that df B's cells are within the and values of df A. The ideal output is this: (first cell is explanatory)", "q_apis": "values first", "io": " A B C lower 1 0 -5 upper 2 2 0 <s> A B C sa 5 1 -2 sb 3 0 2 sc 1 -5 1 ", "apis": "ge loc le loc loc loc", "code": ["df = B.ge(A.loc['lower']) & B.le(A.loc['upper'])\n#alternative\n#df = (B>= A.loc['lower']) & (B <= A.loc['upper'])\nprint (df)\n        A      B      C\nsa  False   True   True\nsb  False   True  False\nsc   True  False  False\n"], "link": "https://stackoverflow.com/questions/65563873/how-to-use-pandas-dataframe-as-condition-for-other-dataframe"}
{"id": 140, "q": "Pandas assigning values to dataframe, conditional on values in another with the same dimensions issue/question", "d": "I'm trying to better understand Pandas/Python so I've been playing around with some stuff. I ran into an issue, I know some workarounds, but I'm wondering why it happened in the first place. Here's my full code, followed by an explanation: I create, 2 dataframes. The first with random numbers, the second dataframe is empty but has the same dimensions as the first. Based on the values in the first dataframe, I'd like to modify the values in the second. My first datframe I create looks like this: I create a second dataframe based on the dimensions of the second: What I would like to do now, is say that for values that are greater 0.6 in df1, I would like the corresponding value in df2 to be 1. And for values less than 0.6 I would like the values to be 0. I did that in the following way, by slicing df1 and then using that slice on df2, and then assigning the values. I thought this would work, but instead, the first row and first column are still NANs Now the reason this didn't work, I think, is because the column names and the row names don't align between the two indices, but what I'm trying to understand is why that's happening. I thought when I sliced df1 based on the conditional it created an array of trues/falses, that I could use on any other dataframe with the same dimension: r I thought that mapping of trues and falses above could be used anywhere, it seems like it can't. Is there a way around this doesn't involve renaming the columns/rows to match between the 2 dataframes?", "q_apis": "values values first first second empty first values first values second first second second now values value values values values first first names names align between indices array any columns between", "io": "df1 1 2 3 4 5 6 7 8 9 10 1 0.24 0.03 0.93 0.38 0.03 0.83 0.47 0.85 0.79 0.65 2 0.66 0.25 0.01 0.28 0.19 0.26 0.25 0.48 0.33 0.92 3 0.53 0.33 0.78 0.04 0.36 0.63 0.16 0.16 0.21 0.96 4 0.76 0.03 0.89 0.15 0.24 0.90 0.59 0.41 0.92 0.98 5 0.72 0.45 0.95 0.44 0.79 0.93 0.90 0.48 0.61 0.02 <s> df2 0 1 2 3 4 5 6 7 8 9 0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 NaN 0 0 1 0 0 1 0 1 1 2 NaN 1 0 0 0 0 0 0 0 0 3 NaN 0 0 1 0 0 1 0 0 0 4 NaN 1 0 1 0 0 1 0 0 1 ", "apis": "astype", "code": ["df2 = (df1 > 0.6).astype(int)\nprint(df2)\n"], "link": "https://stackoverflow.com/questions/65294879/pandas-assigning-values-to-dataframe-conditional-on-values-in-another-with-the"}
{"id": 503, "q": "Aggregate values of same name pandas dataframe columns to single column", "d": "I have multiple csv files that were produced by tokenizing code. These files contain keywords in uppercase and lowercase. I would like to merge all those files in one single dataframe which contains all the unique values (summed) in lowercase. What would you suggest to get the result below? Initial DF: Result I don't have access to the raw data from which the csv files where created so I cannot correct this at an earlier step. At the moment I have tried mapping .lower() to the dataframe headers that I create, but it returns seperate columns with the same name like so: Using pandas is not essential. I have thought of converting the csv files to dictionaries and then trying the above procedure (turns out it is much more complicated than I thought), or using lists. Also, group by does not do the job as it will remove non duplicate column names. Any approach is welcome.", "q_apis": "values name columns merge all contains all unique values get where at step columns name names", "io": "+---+---+----+-----+ | a | b | A | B | +---+---+----+-----+ | 1 | 2 | 3 | 1 | | 2 | 1 | 3 | 1 | +---+---+----+-----+ <s> +---+---+ | a | b | +---+---+ | 4 | 3 | | 5 | 2 | +---+---+ ", "apis": "columns unique columns map columns DataFrame columns columns columns columns columns DataFrame columns sum DataFrame columns size", "code": ["def sumDupeColumns(df):\n    \"\"\"Return dataframe with columns with the same lowercase spelling summed.\"\"\"\n\n    # Get list of unique lowercase column headers\n    columns = set(map(str.lower, df.columns))\n    # Create new (zero-initialised) dataframe for output\n    df1 = pd.DataFrame(data=np.zeros((len(df), len(columns))), columns=columns)\n\n    # Sum matching columns\n    for col in df.columns:\n        df1[col.lower()] += df[col]\n\n    return df1\n", "import pandas as pd\nimport numpy as np\n\nnp.random.seed(seed=42)\n\n# Generate DataFrame with random int input and 'duplicate' columns to sum\ndf = pd.DataFrame(columns = ['a','A','b','B','Cc','cC','d','eEe','eeE','Eee'], \n                  data = np.random.randint(9, size=(5,10))\n\ndf = sumDupeColumns(df)\n"], "link": "https://stackoverflow.com/questions/55132744/aggregate-values-of-same-name-pandas-dataframe-columns-to-single-column"}
{"id": 148, "q": "Pandas, mapping one Dataframe onto another?", "d": "I'm not sure how to tackle this problem. I have 3 data frames; one is a true/false table [3532x622], the other is a single series of integers[662x1], the other is my main dataframe[3532x8]. The true/false table was create by comparing a series of points to find which ones where inside a polygon, that is why is has the shape it does. I have outlined a diagram below as to what I am trying to accomplish. Convert to: Then map this onto the main dataframe This is what I have started I don't know where to go from here.", "q_apis": "where shape map where", "io": "df_2 0 1 2 8 9 0 56489 np.nan np.nan ... np.nan 89641 1 np.nan 86932 np.nan ... 45871 np.nan 2 np.nan 86932 np.nan ... np.nan np.nan <s> df_3 0 1 0 poly_a 56489 1 ploy_a 89641 2 poly_b 86932 3 poly_b 45871 4 poly_c 86932 ", "apis": "DataFrame DataFrame DataFrame Series Series copy first columns T copy iloc values index replace loc apply concat", "code": ["import numpy\nimport pandas\n\n# Creating Example Dataframes\ndf_1 = pandas.DataFrame([56489, 45872, 89657, 56895, 87456])\ndf_2 = pandas.DataFrame(\n    [\n        [True, False, False, False, True],\n        [False, True, True, False, False],\n        [False, True, False, True, True],\n    ]\n)\ndf_3 = pandas.DataFrame([\"poly_a\", \"poly_b\", \"poly_c\"])\n\n\ndef replace_values(row: pandas.Series) -> pandas.Series:\n\n    # Make a copy of df_1 (first row) but flip it to become columns\n    c = df_1.T.copy().iloc[0]\n\n    # Use the boolean values from the row as index, replace False with NaN\n    c.loc[~row] = numpy.nan\n    return c\n\n\n# Combine 2 and 1\ncombined = df_2.apply(replace_values, axis=1)\n\n# Add 3\nresult = pandas.concat([df_3, combined], axis=1, ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/65083476/pandas-mapping-one-dataframe-onto-another"}
{"id": 518, "q": "Efficient python pandas equivalent/implementation of R sweep with multiple arguments", "d": "Other questions attempting to provide the equivalent to 's function (like here) do not really address the case of multiple arguments where it is most useful. Say I wish to apply a 2 argument function to each row of a Dataframe with the matching element from a column of another DataFrame: In I got the equivalent using on what is basically a loop through the row counts. I highly doubt this is efficient in , what is a better way of doing this? Both bits of code should result in a Dataframe/matrix of 6 numbers when applying : I should state clearly that the aim is to insert one's own function into this like behavior say: resulting in: What is a good way of doing that in python pandas?", "q_apis": "where apply DataFrame insert", "io": " A B 1 10 110 2 22 132 3 36 156 <s> A B [1,] 3 4 [2,] 3 4 [3,] 3 5 ", "apis": "dtypes DataFrame DataFrame apply mean std Series shape apply iloc iloc columns get_loc mean std all", "code": ["import pandas as pd\nfrom pandas.core.dtypes.generic import ABCSeries\n\ndef sweep(df, series, FUN):\n    assert isinstance(series, ABCSeries)\n\n    # row-wise application\n    assert len(df) == len(series)\n    return df._combine_match_index(series, FUN)\n\n\n# define your binary operator\ndef f(x, y):\n    return x*y    \n\n# the input data frames\ndf = pd.DataFrame( { \"A\" : range(1,4),\"B\" : range(11,14) } )\ndf2 = pd.DataFrame( { \"X\" : range(10,13),\"Y\" : range(10000,10003) } )\n\n# apply\ntest1 = sweep(df, df2.X, f)\n\n# performance\n# %timeit sweep(df, df2.X, f)\n# 155 \u00b5s \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)#\n\n# another method\nimport numpy as np\ntest2 = pd.Series(range(df.shape[0])).apply(lambda row_count: np.multiply(df.iloc[row_count,:],df2.iloc[row_count,df2.columns.get_loc('X')]))\n\n# %timeit performance\n# 1.54 ms \u00b1 56.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nassert all(test1 == test2)\n"], "link": "https://stackoverflow.com/questions/54621203/efficient-python-pandas-equivalent-implementation-of-r-sweep-with-multiple-argum"}
{"id": 166, "q": "Split two columns in a pandas dataframe into two and name them", "d": "I have this pandas dataframe I would like to split the x and y columns and get an output with these given names on the columns. Is there a straight forward way to do this in python?", "q_apis": "columns name columns get names columns", "io": " x y Values 0 A B C D 4.7 1 A B C D 10.9 2 A B C D 1.8 3 A B C D 6.5 4 A B C D 3.4 <s> x f y g Values 0 A B C D 4.7 1 A B C D 10.9 2 A B C D 1.8 3 A B C D 6.5 4 A B C D 3.4 ", "apis": "columns names sum values columns", "code": ["df[['x', 'f']] = df.x.str.split(\" \", expand=True)\ndf[['y', 'g']] = df.y.str.split(\" \", expand=True)\ndf[['x','f','y','g', 'Values']]\n", "# Define the target columns to split, and their new column names\ncols={\n    'x': ['x','f'],\n    'y': ['y','g']\n}\n# Apply the function to each target-column\nfor k in cols:\n    df[cols[k]] = df[k].str.split(\" \", expand=True)\n\n# Reorder the dataframe as you wish\nnew_columns = sum(cols.values(),[])\nold_columns = set(df.columns) - set(new_columns)\ndf[new_columns + list(old_columns)]\n"], "link": "https://stackoverflow.com/questions/64773001/split-two-columns-in-a-pandas-dataframe-into-two-and-name-them"}
{"id": 104, "q": "Create a pandas column of numbers from 1 to 3 and repeat again", "d": "I have a dataframe: I want to add a column so that my final dataframe looks like: I don't want the new row to depend upon StoreNumber which looks ovious in example. I want to start numbering with 1 and when I reach 3, then again start with 1. How do I do it?", "q_apis": "repeat add start start", "io": " StoreNumber Year 1000 2000 1000 2001 1000 2002 1001 2000 1001 2001 1001 2002 <s> StoreNumber Year New 1000 2000 1 1000 2001 2 1000 2002 3 1001 2000 1 1001 2001 2 1001 2002 3 ", "apis": "", "code": ["from itertools import cycle\n\nnum_cycle = cycle([1, 2, 3])\ndf['New'] = [next(num_cycle) for num in range(len(df))]\n"], "link": "https://stackoverflow.com/questions/66075712/create-a-pandas-column-of-numbers-from-1-to-3-and-repeat-again"}
{"id": 506, "q": "How can I change a specific row label in a Pandas dataframe?", "d": "I have a dataframe such as: Where the final row contains averages. I would like to rename the final row label to so that the dataframe will look like this: I understand columns can be done with . But how can I do this with a specific row label?", "q_apis": "contains rename columns", "io": " 0 1 2 3 4 5 0 41.0 22.0 9.0 4.0 2.0 1.0 1 6.0 1.0 2.0 1.0 1.0 1.0 2 4.0 2.0 4.0 1.0 0.0 1.0 3 1.0 2.0 1.0 1.0 1.0 1.0 4 5.0 1.0 0.0 1.0 0.0 1.0 5 11.4 5.6 3.2 1.6 0.8 1.0 <s> 0 1 2 3 4 5 0 41.0 22.0 9.0 4.0 2.0 1.0 1 6.0 1.0 2.0 1.0 1.0 1.0 2 4.0 2.0 4.0 1.0 0.0 1.0 3 1.0 2.0 1.0 1.0 1.0 1.0 4 5.0 1.0 0.0 1.0 0.0 1.0 A 11.4 5.6 3.2 1.6 0.8 1.0 ", "apis": "index index", "code": ["df.index = df.index[:-1].tolist() + ['a']\n"], "link": "https://stackoverflow.com/questions/42142756/how-can-i-change-a-specific-row-label-in-a-pandas-dataframe"}
{"id": 636, "q": "how to extract a 2D array encoded in a list of strings in a pandas dataframe?", "d": "I have messed up a dataframe. I have a columns which contain strings which encode a list of numbers e.g. EDIT: actually, the commas are missing as well Each of the strings encodes a list with a fixed number of elements. I would like to convert this into 3 (in general N, where each of them numeric, containing one element from the original list in mycol I have tried the following, without success", "q_apis": "array columns where", "io": "df= mycol 0 '[ 0.5497076, 0.59722222, 0.42361111]' 1 '[ 0.8030303, 0.69090909, 0.52727273]' 2 '[ 0.51461988, 0.38194444, 0.66666667]' <s> df= mycol 0 '[ 0.5497076 0.59722222 0.42361111]' 1 '[ 0.8030303 0.69090909 0.52727273]' 2 '[ 0.51461988 0.38194444 0.66666667]' ", "apis": "DataFrame apply replace replace apply Series", "code": ["import pandas as pd\ndf = pd.DataFrame({\"mycol\": ['[ 0.5497076   0.59722222  0.42361111]', '[ 0.8030303   0.69090909  0.52727273]']})\ndf[['mycol1','mycol2','mycol3']]  = df[\"mycol\"].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").split()).apply(pd.Series)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/50152137/how-to-extract-a-2d-array-encoded-in-a-list-of-strings-in-a-pandas-dataframe"}
{"id": 561, "q": "Python - pandas explode rows by turns", "d": "I have a dataframe as below. Now I extracted letters from B1-B3 and add to new columns U1-U3 get: and I want to let the row to explode like this: Thanks in advance", "q_apis": "explode add columns get explode", "io": " B1 B2 B3 U1 U2 U3 0 1C C 1 3A 1A A A 2 41A 28A 3A A A A <s> B1 B2 B3 U1 U2 U3 0 1C C 1 3A 1A A 2 3A 1A A 3 41A 28A 3A A 4 41A 28A 3A A 5 41A 28A 3A A ", "apis": "merge apply add_prefix notnull sum loc repeat index values groupby index apply dropna mask", "code": ["df = df.merge(df.apply(lambda x: x.str.extract('([A-Za-z])')).add_prefix('U_'), left_index=True,right_index=True,how='outer')\n", "# Duplicating the rows of dataframe\nval = df[['U_B1','U_B2','U_B3']].notnull().sum(axis=1)\ndf1 = df.loc[np.repeat(val.index,val)]\n", "masked values of identity matrix", "df1[['U_B1','U_B2','U_B3']] = df1.groupby(df1.index)['U_B1','U_B2','U_B3'].apply(lambda x: x.dropna(axis=1).mask(np.identity(len(x))==0))\n"], "link": "https://stackoverflow.com/questions/53314441/python-pandas-explode-rows-by-turns"}
{"id": 243, "q": "Dropping dataframe rows in time series dataframe using pandas", "d": "I have the below sequence of data as a pandas dataframe It should always be the case that id 404 gets repeated after another different id. For example if the above is motion sensors in a house e.g. 404:hallway, 202:bedroom, 303:kitchen, 201:studyroom, where the hallway is in the middle, then moving from bedroom to kitchen to studyroom and back to bedroom should trigger 202, 404, 303, 404, 201, 404, 202 in that order because one always passes through the hallway (404) to any room. My output has cases that violate this sequence and I want to drop such rows. For example from the snippet dataframe above the below rows violate this: and therefore the rows below should be droped (but of course I have a much larger dataset). I have tried shift and drop but the result still has some inconsistencies. How best can I approach this?", "q_apis": "time where any drop shift drop", "io": "303,2012-06-25 18:01:56,2012-06-25 18:02:06,10 303,2012-06-25 18:02:23,2012-06-25 18:02:44,21 303,2012-06-25 18:03:43,2012-06-25 18:05:51,128 101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104 <s> 303,2012-06-25 18:02:23,2012-06-25 18:02:44,21 101,2012-06-25 18:05:58,2012-06-25 18:24:22,1104 ", "apis": "mask ne shift ne mask", "code": ["mask = df['id'].ne(404) & df['id'].shift(fill_value=404).ne(404)\ndf = df[~mask]\n"], "link": "https://stackoverflow.com/questions/63090827/dropping-dataframe-rows-in-time-series-dataframe-using-pandas"}
{"id": 558, "q": "drop group by number of occurrence", "d": "Hi I want to delete the rows with the entries whose number of occurrence is smaller than a number, for example: Here I want to delete all the rows if the number of occurrence in column 'a' is less than twice. Wanted output: What I know: we can find the number of occurrence by , and it will give me something like: But I don't know how I should approach from here to delete the rows. Thanks in advance!", "q_apis": "drop delete delete all delete", "io": " a b c 0 1 4 0 1 2 5 1 2 3 6 3 3 2 7 2 <s> a b c 1 2 5 1 3 2 7 2 ", "apis": "groupby transform size", "code": ["res = df[df.groupby('a')['b'].transform('size') >= 2]\n"], "link": "https://stackoverflow.com/questions/53471422/drop-group-by-number-of-occurrence"}
{"id": 40, "q": "Groupby, counts in ranges and spread in Pandas", "d": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output:", "q_apis": "count items groupby index transpose columns", "io": " a b a (0, 10] 2 BBB (10, 20] 3 BBB (20, 30] 1 AAA <s> (0, 10] (10, 20] (20, 30] AAA 0 0 1 BBB 2 3 0 ", "apis": "assign cut pivot_table index columns values count", "code": ["df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"], "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas"}
{"id": 469, "q": "how to find the last value of consecutive values in pandas dataframe?", "d": "I have a data frame like this I want to group by this data frame on col1 where there is consecutive values, and take the last value for each consecutive groups, The final data frame should look like: I have tried something like: But its missing the consecutive condition. How to implement it in most effective way using pandas/ python", "q_apis": "last value values where values take last value groups", "io": "df: col1 col2 1 10 1 20 2 11 3 33 1 20 1 10 2 24 3 21 3 28 <s> df col1 col2 1 20 2 11 3 33 1 10 2 24 3 28 ", "apis": "ne shift dtype bool", "code": ["print (df['col1'].ne(df['col1'].shift(-1)))\n0    False\n1     True\n2     True\n3     True\n4    False\n5     True\n6     True\n7    False\n8     True\nName: col1, dtype: bool\n"], "link": "https://stackoverflow.com/questions/56662690/how-to-find-the-last-value-of-consecutive-values-in-pandas-dataframe"}
{"id": 388, "q": "How to drop the rows if two columns cells are empty?", "d": "This is my DF i want to compare the columns B and C then i have to check both are null after that i want to remove that rows from DF. Output looks like,this Then i need to check again both columns of B and C like whether the values or same or not , if same i need to create one column say validation_results and print Y and if not same print N. I am new to python so anybody here tell me how can i do this with minimum lines of code.", "q_apis": "drop columns empty compare columns columns values", "io": "A B C 1 10 10 2 3 12 12 4 5 21 22 <s> A B C 1 10 10 3 12 12 5 21 22 ", "apis": "ne ne all dtype bool", "code": ["print (df[['B','C']].ne(''))\n       B      C\n0   True   True\n1  False  False\n2   True   True\n3  False  False\n4   True   True\n", "print (df[['B','C']].ne('').all(axis=1))\n0     True\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/59266300/how-to-drop-the-rows-if-two-columns-cells-are-empty"}
{"id": 572, "q": "Series calculation based on shifted values / recursive algorithm", "d": "I have the following: This lines basically only take in df['Alpha'] but not the df['PositionLong'].shift(1).. It cannot recognize it but I dont understand why? It produces this: However what I wanted the code to do is this: I believe the solution is to loop each row, but this will take very long. Can you help me please?", "q_apis": "Series values take shift take", "io": "df['Alpha'] df['Bravo'] df['PositionLong'] 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 <s> df['Alpha'] df['Bravo'] df['PositionLong'] 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 ", "apis": "empty shape values values astype", "code": ["from numba import njit\n\n@njit\ndef rec_algo(alpha, bravo):\n    res = np.empty(alpha.shape)\n    res[0] = 1 if alpha[0] == 1 else 0\n    for i in range(1, len(res)):\n        if (alpha[i] == 1) or ((res[i-1] == 1) and bravo[i] == 1):\n            res[i] = 1\n        else:\n            res[i] = 0\n    return res\n\ndf['PositionLong'] = rec_algo(df['Alpha'].values, df['Bravo'].values).astype(int)\n"], "link": "https://stackoverflow.com/questions/52902568/series-calculation-based-on-shifted-values-recursive-algorithm"}
{"id": 484, "q": "Convert Rows per Unique Id into all comma separated possibilities", "d": "i have some data in the following format, at the moment this is in a Pandas Dataframe. What i require is all of the possible combinations of the Lenders columns for each Uid so the output would be something like this And the same for Uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. Thanks,", "q_apis": "all at all columns", "io": "Row Uid Lender 1 1 HSBC 2 1 Lloyds 3 1 Barclays 4 2 Lloyds 5 2 Barclays 6 2 Santander 7 2 RBS 8 2 HSBC <s> Row Uid LenderCombo 1 1 Barclays 2 1 Lloyds 3 1 HSBC 4 1 Barclays, HSBC 5 1 Barclays, Lloyds 6 1 HSBC, Lloyds 7 1 Barclays, HSBC, Lloyds ", "apis": "map groupby apply Series join reset_index rename columns", "code": ["from itertools import chain, combinations\n\n#https://stackoverflow.com/a/5898031\ndef all_subsets(ss):\n    return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))\n\ndf = (df.groupby('Uid')['Lender']\n       .apply(lambda x: pd.Series([', '.join(y) for y in all_subsets(x)]))\n       .reset_index()\n       .rename(columns={'level_1':'Row'}))\n"], "link": "https://stackoverflow.com/questions/56020272/convert-rows-per-unique-id-into-all-comma-separated-possibilities"}
{"id": 546, "q": "Using .iterrows() with series.nlargest() to get the highest number in a row in a Dataframe", "d": "I am trying to create a function that uses and . I want to iterate over each row and find the largest number and then mark it as a . This is the data frame: Here is the output I wish to have: This is the function I wish to use here: I get the following error: AttributeError: 'tuple' object has no attribute 'nlargest' Help would be appreciated on how to re-write my function in a neater way and to actually work! Thanks in advance", "q_apis": "iterrows nlargest get get nlargest", "io": "A B C 9 6 5 3 7 2 <s> A B C 1 0 0 0 1 0 ", "apis": "iterrows nlargest sum", "code": ["for i, row in df.iterrows():\n    top_numbers = row.nlargest(top_n).sum()\n"], "link": "https://stackoverflow.com/questions/51645934/using-iterrows-with-series-nlargest-to-get-the-highest-number-in-a-row-in-a"}
{"id": 319, "q": "How to freeze first numbers in sequences between NaNs in Python pandas dataframe", "d": "Is there a Pythonic way to, in a timeseries dataframe, by column, go down and pick the first number in a sequence, and then push it forward until the next NaN, and then take the next non-NaN number and push that one down until the next NaN, and so on (retaining the indices and NaNs). For example, I would like to convert this dataframe: To this dataframe: I know I can use a loop to iterate down the columns to do this, but would appreciate some help on how to do it in a more efficient Pythonic way on a very large dataframe. Thank you.", "q_apis": "first between first take indices columns", "io": " A B C 0 NaN 8.0 NaN 1 1.0 6.0 NaN 2 3.0 4.0 4.0 3 5.0 NaN 2.0 4 7.0 NaN 6.0 5 NaN 9.0 NaN 6 2.0 7.0 1.0 7 4.0 3.0 5.0 8 6.0 NaN 2.0 9 NaN 3.0 8.0 <s> A B C 0 NaN 8.0 NaN 1 1.0 8.0 NaN 2 1.0 8.0 4.0 3 1.0 NaN 4.0 4 1.0 NaN 4.0 5 NaN 9.0 NaN 6 2.0 9.0 1.0 7 2.0 9.0 1.0 8 2.0 NaN 1.0 9 NaN 3.0 1.0 ", "apis": "where mask notna shift value mask mask replace all ffill fillna iloc columns where mask replace", "code": ["# where DF is not NaN\nmask = DF.notna()\nResult = (DF.shift(-1)           # fill the original NaN's with their next value\n            .mask(mask)          # replace all the original non-NaN with NaN\n            .ffill()             # forward fill \n            .fillna(DF.iloc[0])  # starting of the the columns with a non-NaN\n            .where(mask)         # replace the original NaN's back\n         )\n"], "link": "https://stackoverflow.com/questions/61011933/how-to-freeze-first-numbers-in-sequences-between-nans-in-python-pandas-dataframe"}
{"id": 171, "q": "Concatenation of two dataframe after onehotencoding", "d": "Let us consider following code result of this code is following ( i am writing final dataframe) all others works fine, they are so my point is to remove commas in header part of the final dataframe, please help me", "q_apis": "all", "io": " Alphabet (A,) (B,) (C,) 0 A 1.0 0.0 0.0 1 B 0.0 1.0 0.0 2 C 0.0 0.0 1.0 3 A 1.0 0.0 0.0 4 B 0.0 1.0 0.0 <s> A B C 0 1.0 0.0 0.0 1 0.0 1.0 0.0 2 0.0 0.0 1.0 3 1.0 0.0 0.0 4 0.0 1.0 0.0 ", "apis": "columns join columns", "code": ["Encoded_Dataframe.columns = [''.join(col) for col in Encoded_Dataframe.columns]\n"], "link": "https://stackoverflow.com/questions/64722958/concatenation-of-two-dataframe-after-onehotencoding"}
{"id": 498, "q": "Pandas: get the min value between 2 dataframe columns", "d": "I have 2 columns and I want a 3rd column to be the minimum value between them. My data looks like this: And I want to get a column C in the following way: Some helping code: Thanks!", "q_apis": "get min value between columns columns value between get", "io": " A B 0 2 1 1 2 1 2 2 4 3 2 4 4 3 5 5 3 5 6 3 6 7 3 6 <s> A B C 0 2 1 1 1 2 1 1 2 2 4 2 3 2 4 2 4 3 5 3 5 3 5 3 6 3 6 3 7 3 6 3 ", "apis": "min min values mean std mean std min min values min to_numpy mean std mean std mean std", "code": ["%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n314 \u00b5s \u00b1 3.63 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n34.4 \u00b5s \u00b1 161 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n", "%timeit df.min(axis=1)\n%timeit np.min(df.values,axis=1)\n%timeit np.min(df.to_numpy(),axis=1)\n314 \u00b5s \u00b1 3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n35.2 \u00b5s \u00b1 680 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n35.5 \u00b5s \u00b1 262 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n"], "link": "https://stackoverflow.com/questions/55654105/pandas-get-the-min-value-between-2-dataframe-columns"}
{"id": 115, "q": "Select value from a list of columns that is close to another value in pandas", "d": "I have the following data frame: I want to create another column which stores one value lower than the . Intended result: Here's what I have been trying: If the difference is greater than 3% it doesn't do the job right. Note that the s columns may vary so I would want to keep the Little help will be appreciated. THANKS!", "q_apis": "value columns value value difference right columns", "io": "S0 S1 S2 S3 S4 S5... Price 10 15 18 12 18 19 16 55 45 44 66 58 45 64 77 84 62 11 61 44 20 <s> S0 S1 S2 S3 S4 S5... Price Sup 10 15 18 12 18 19 16 15 55 45 44 66 58 45 64 58 77 84 62 11 61 44 20 11 ", "apis": "filter compare mask lt where mask where mask max all max fillna where mask max fillna", "code": ["s = df.filter(like='S')\n\n# compare to the price\nmask = s.lt(df['Price'], axis=0)\n\n# `where(mask)` places `NaN` where mask==False\n# `max(1)` takes maximum along rows, ignoring `NaN`\n# rows with all `NaN` returns `NaN` after `max`, \n# `fillna(0)` fills those with 0\ndf['Price'] = s.where(mask).max(1).fillna(0)\n"], "link": "https://stackoverflow.com/questions/65833007/select-value-from-a-list-of-columns-that-is-close-to-another-value-in-pandas"}
{"id": 611, "q": "python: how to sum unique elements respectively of a dataframe column based on another column", "d": "For example, I have a df with two columns. Input Output I want to count the element in group by user_id respectively. The expected output is shown as follow. Expected Briefly, in column , I count the number of in column based on column . Hopefully for help!", "q_apis": "sum unique columns count count", "io": "df label user_id 0 0 a 1 0 a 2 1 a 3 0 b 4 0 b 5 2 b 6 0 c 7 1 c 8 2 c <s> df label user_id label_0 label_1 label_2 0 0 a 2 1 0 1 0 a 2 1 0 2 1 a 2 1 0 3 0 b 2 0 1 4 0 b 2 0 1 5 2 b 2 0 1 6 0 c 1 1 1 7 1 c 1 1 1 8 2 c 1 1 1 ", "apis": "join groupby size unstack add_prefix join groupby value_counts unstack add_prefix merge crosstab add_prefix left", "code": ["df = (df.join(df.groupby(['user_id', 'label'])\n                .size()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n", "df = (df.join(df.groupby('user_id')['label']\n                .value_counts()\n                .unstack(fill_value=0)\n                .add_prefix('label_'), 'user_id'))\n", "df = (df.merge(pd.crosstab(df['user_id'], df['label'])\n                 .add_prefix('label_'), on='user_id', how='left'))\n"], "link": "https://stackoverflow.com/questions/51265888/python-how-to-sum-unique-elements-respectively-of-a-dataframe-column-based-on-a"}
{"id": 516, "q": "Pandas: Remove all NaN values in all columns", "d": "I have a data frame with many null records: I want to remove all NaN values in all rows of columns . As you could see, each column has different number of rows. So, I want to get something like this: I tried But it removes all records in the dataframe. How may I do that ?", "q_apis": "all values all columns all values all columns get all", "io": "Col_1 Col_2 Col_3 10 5 2 22 7 7 3 9 5 4 NaN NaN 5 NaN NaN 6 4 NaN 7 6 7 8 10 NaN 12 NaN 1 <s> Col_1 Col_2 Col_3 10 5 2 22 7 7 3 9 5 4 4 7 6 6 1 7 10 8 12 ", "apis": "dropna values columns columns", "code": ["{c: df[c].dropna().values for c in df.columns}\n", "{c: list(df[c]) for c in df.columns}\n"], "link": "https://stackoverflow.com/questions/50418202/pandas-remove-all-nan-values-in-all-columns"}
{"id": 195, "q": "Shuffling Pandas Dataframe Columns", "d": "I need to shuffle dataframe columns. Currently I do it this way: Before: After: So it does the job, but there must be a better way to do this. Any ideas?", "q_apis": "columns", "io": " 0 1 2 3 4 0 0.472918 0.261734 0.987053 0.921826 0.144114 <s> 0 1 2 3 4 0 0.472918 0.921826 0.987053 0.144114 0.261734 ", "apis": "apply DataFrame", "code": ["def shuffle(df, n=1):\n    for _ in range(n):\n        df.apply(np.random.shuffle)\n        return df\ndf = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nshuffle(df)\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/64368604/shuffling-pandas-dataframe-columns"}
{"id": 435, "q": "Pandas dataframe: uniformly scale down values when column sum exceeds treshold", "d": "Initial Situation Consider the following example dataframe: which in printed form looks like: Desired Result I would now like to do the following for each column of this dataframe: Calculate the sum of the column's values (ignoring any NaN values). If the sum exceeds 10.0, then I want to uniformly scale down all values in the column such that the new sum is exactly 10.0 (again ignoring any NaN values). Basically I'd like to obtain a result dataframe that looks like this: Tried thus far The following code obtains the desired result. However this code feels a bit verbose and inefficient to me. Based on my experience with pandas thus far I'd suspect that a more vectorized solution is still possible. Would anyone be able to help me find this?", "q_apis": "values sum now sum values any values sum all values sum any values", "io": " A B C D 0 3.0 7.0 4.0 1.0 1 2.0 NaN 5.0 0.0 2 1.0 1.0 1.0 2.0 3 NaN 3.0 2.0 3.0 <s> A B C D 0 3.0 6.363636 3.333333 1.0 1 2.0 NaN 4.166667 0.0 2 1.0 0.909091 0.833333 2.0 3 NaN 2.727273 1.666667 3.0 ", "apis": "sum clip", "code": ["thres = 10\nresult = df * thres / df.sum().clip(lower=thres)\n"], "link": "https://stackoverflow.com/questions/57650114/pandas-dataframe-uniformly-scale-down-values-when-column-sum-exceeds-treshold"}
{"id": 474, "q": "Leading zero issues with pandas read_csv function", "d": "I have a column of values such as this: When I do or they both produce values like I was searching through stackexchange, and people say that you should use , but it doesn't work for me..", "q_apis": "read_csv values values", "io": "123, 234, 345, 456, 567 <s> 00123, 00234, 00345, 00456, 00567. ", "apis": "read_csv", "code": ["df = pd.read_csv('data.csv', converters={'ColName': str})\n"], "link": "https://stackoverflow.com/questions/45239357/leading-zero-issues-with-pandas-read-csv-function"}
{"id": 308, "q": "How to replace pandas dataframe values based on lookup values in another dataframe?", "d": "I have a large pandas dataframe with numerical values structured like this: I need to replace all of the the above cell values with a 'description' that maps to the field name and cell value as referenced in another dataframe structured like this: The desired output would be like: I could figure out a way to do this on a small scale using something like .map or .replace - however the actual datasets contain thousands of records with hundreds of different combinations to replace. Any help would be really appreciated. Thanks.", "q_apis": "replace values lookup values values replace all values name value map replace replace", "io": ">>> df1 A B C 0 2 1 2 1 1 2 3 2 2 3 1 <s> >>> df3 A B C 0 YES x BAD 1 NO y FINE 2 YES z GOOD ", "apis": "replace pivot columns index values to_dict replace pivot columns index values to_dict", "code": ["df1 = df1.replace(df2.pivot(columns='field_name', index='code', values='description')\n                     .to_dict())\n", "df1[cols] = df1[cols].replace(df2.pivot(columns='field_name',\n                                        index='code', values='description')\n                                 .to_dict())\n"], "link": "https://stackoverflow.com/questions/61233112/how-to-replace-pandas-dataframe-values-based-on-lookup-values-in-another-datafra"}
{"id": 517, "q": "How to get the Rank of current row compared to previous rows", "d": "How to get the Rank of current row compared to previous rows I have a dataframe like: I want to get the rank of current row compared to all previous rows for Volume Column. Desired Dataframe Data: pandas.DataFrame.rank Function doesnot serve my purpose.", "q_apis": "get get get rank all DataFrame rank", "io": "Instru Price Volume ABCD 1000 100258 ABCD 1000 100252 ABCD 1000 100168 ABCD 1000 100390 ABCD 1000 100470 ABCD 1000 100420 <s> Instru Price Volume Rank ABCD 1000 100258 1 => 1st Row so Rank 1 ABCD 1000 100252 2 => Rank 2 (Compare 100258,100252) ABCD 1000 100168 3 => Rank 3 (Compare 100258,100252,100168) ABCD 1000 100390 1 => Rank 1 (Compare 100390,100258,100252,100168) ABCD 1000 100470 1 => Rank 1 (Compare 100470,100390,100258,100252,100168) ABCD 1000 100420 2 => Rank 2 (Compare 100470,100420,100390,100258,100252,100168) ", "apis": "array searchsorted", "code": ["df['Rank'] = np.array([i - np.searchsorted(sorted(df.Volume[:i]), v) for i, v in enumerate(df.Volume)]) + 1\nprint(df)\n"], "link": "https://stackoverflow.com/questions/54733869/how-to-get-the-rank-of-current-row-compared-to-previous-rows"}
{"id": 604, "q": "indexing with multidimensional key pandas error", "d": "I have a dataframe as the following: I want to filter out g2 & g4 where all of 'B', 'C', 'D' values are 0 I tried returns output same as input, also returns output same as input, returns 'Cannot index with multi-dimensional key. The expected output is, What is the error here? Thanks in Advance.", "q_apis": "filter where all values index", "io": "A B C D E g1 1 -10 20 text1 g2 0 0 0 text2 g3 0 1 0 text3 g4 0 0 0 text4 <s> A B C D E g1 1 -10 20 text1 g3 0 1 0 text3 ", "apis": "", "code": ["print(df[['B', 'C', 'D']] == 0)\n\n       B      C      D\n0  False  False  False\n2   True  False   True\n"], "link": "https://stackoverflow.com/questions/51578845/indexing-with-multidimensional-key-pandas-error"}
{"id": 286, "q": "Remove duplicates from dataframe, based on two columns A,B, keeping row with max value in another column C", "d": "I have a pandas dataframe which contains duplicates values according to two columns (A and B): I want to remove duplicates keeping the row with max value in column C. This would lead to: I cannot figure out how to do that. Should I use , something else?", "q_apis": "columns max value contains values columns max value", "io": "A B C 1 2 1 1 2 4 2 7 1 3 4 0 3 4 8 <s> A B C 1 2 4 2 7 1 3 4 8 ", "apis": "groupby transform max loc", "code": ["c_maxes = df.groupby(['A', 'B']).C.transform(max)\ndf = df.loc[df.C == c_maxes]\n"], "link": "https://stackoverflow.com/questions/32093829/remove-duplicates-from-dataframe-based-on-two-columns-a-b-keeping-row-with-max"}
{"id": 28, "q": "How to get list of previous n values of a column conditionally in DataFrame?", "d": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result?", "q_apis": "get values DataFrame get all get", "io": "Subject Score 1 15 2 0 3 18 2 30 3 17 1 5 4 9 2 7 1 20 1 8 2 9 1 12 <s> Subject Score Previous 1 15 [] 2 0 [] 3 18 [] 2 30 [0] 3 17 [18] 1 5 [15] 4 9 [] 2 7 [30,0] 1 20 [5,15] 1 8 [20,5,15] 2 9 [7,30,0] 1 12 [8,20,5] ", "apis": "sort_values agg groupby rolling sort_values agg groupby rolling sort_index", "code": ["window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n", "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"], "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe"}
{"id": 437, "q": "Ignore Nulls in pandas map dictionary", "d": "My Dataframe looks like this : I am trying to label encode with nulls as such. My result should look like: The code i tried : Achieved Result : Expected Result :", "q_apis": "map", "io": "COL1 COL2 COL3 A M X B F Y NaN M Y A nan Y <s> COL1_ COL2_ COL3_ 0 0 0 1 1 1 NaN 0 1 0 nan 1 ", "apis": "join apply Series map dropna index dropna index dropna index add_suffix", "code": ["print(df.join(df.apply(lambda x: pd.Series(map(x.dropna().tolist().index, x.dropna()), index=x.dropna().index)).add_suffix('_')))\n"], "link": "https://stackoverflow.com/questions/57533654/ignore-nulls-in-pandas-map-dictionary"}
{"id": 593, "q": "In dataframe of strings AND floats, cast floats to integer then string", "d": "I have the following dataframe of dtype : I would like to cast all the floats to integers then convert everything to strings, so the output would be of dtype : Is there something that allows me to cast a dataframe to an int but ignore errors? Or achieve this in a different way? (using the , seems to ignore the entire thing)", "q_apis": "dtype all dtype", "io": " col1 col2 col3 0 1.1 3.3 spam 1 2.2 foo eggs 2 bar 4.4 5.5 <s> col1 col2 col3 0 1 3 spam 1 2 foo eggs 2 bar 4 5 ", "apis": "", "code": ["def try_to_int(obj):\n    try:\n        return str(int(float(obj)))\n    except (ValueError, TypeError):\n        return obj\n"], "link": "https://stackoverflow.com/questions/52085200/in-dataframe-of-strings-and-floats-cast-floats-to-integer-then-string"}
{"id": 476, "q": "Iterate over columns in python dataframe to do calculations and insert new columns between existing columns", "d": "I'm new to python and programming in general and can't seem to find a solution to my problem. I have a dataframe imported from an excel sheet with 15 rows of species and their number and 3 columns which are locations where they are found. That is a species by station matrix: I want to calculate for each column the top-10 species (index), their value, percentage of total in column, cumulative percentage and insert the new columns after each exististing column and return in one dataframe. This is the result I'm looking for (example with two first columns): I have managed to do this by calculating each column and make new data frames and using concat to merge the data frames together in the end using the following code: This code works, but my datasets are much larger with often more then 50 columns so I'm wondering if it possible to an iteration for each column that results in the same dataframe as shown above. Sorry for the long read.", "q_apis": "columns insert columns between columns columns where index value insert columns first columns concat merge columns", "io": " A1 A2 A3 Species 1 1259 600 151 Species 2 912 1820 899 Species 3 1288 1491 631 Species 4 36 609 1946 Species 5 1639 819 1864 Species 6 1989 748 843 Species 7 688 271 1206 Species 8 1031 341 756 Species 9 1517 1164 138 Species 10 1290 669 811 Species 11 16 409 1686 Species 12 329 521 954 Species 13 1782 958 1727 Species 14 464 1804 1105 Species 15 1002 1483 109 <s> Species A1 pct cum_pct Species A2 pct cum_pct 0 Species 6 1989 13 13 Species 2 1820 13 13 1 Species 13 1782 11 24 Species 14 1804 13 26 2 Species 5 1639 10 35 Species 3 1491 10 37 3 Species 9 1517 9 45 Species 15 1483 10 48 4 Species 10 1290 8 53 Species 9 1164 8 56 5 Species 3 1288 8 62 Species 13 958 6 63 6 Species 1 1259 8 70 Species 5 819 5 69 7 Species 8 1031 6 77 Species 6 748 5 75 8 Species 15 1002 6 83 Species 10 669 4 79 9 Species 2 912 5 89 Species 4 609 4 84 ", "apis": "append nlargest to_frame assign sum cumsum rename_axis reset_index concat append nlargest to_frame assign sum cumsum assign mul astype mul astype rename_axis reset_index concat", "code": ["frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n", "frames = []\nfor col in df:\n    frames.append(df[col].nlargest(10).to_frame()\n                  .assign(pct=lambda x: x[col] / df[col].sum(),\n                          cum_pct=lambda x: x['pct'].cumsum())\n                  .assign(pct=lambda x: x['pct'].mul(100).astype(int),\n                          cum_pct=lambda x: x['cum_pct'].mul(100).astype(int))\n                  .rename_axis('Species').reset_index())\n\n\ndf_new = pd.concat(frames, axis=1)\n"], "link": "https://stackoverflow.com/questions/56289734/iterate-over-columns-in-python-dataframe-to-do-calculations-and-insert-new-colum"}
{"id": 153, "q": "Splitting Dataframe based on duplicate values into multiple csv files", "d": "I have a dataset with multiple columns but only focusing on one column called 'VAL'. Every value in this column ranges from 0 to 4 so I would like to split this into 5 separate data frames based on those duplicate values and then export each of these data frames into individual csv files. I have been able to sort the numbers using pandas but now I need to divide up the values into smaller datasets keeping in mind that I have multiple files I would like to do this to so possibly a for loop? this is what I currently have as an output this is what I would like it to relatively look like", "q_apis": "values columns value values now values", "io": " A B C D E F G VAL FILE 954 380 158 166 431 201 769 0 001.csv 1142 348 203 962 0 878 1023 0 001.csv 1688 279 229 0 488 1007 0 0 001.csv 4792 371 420 29 372 0 745 0 001.csv 2106 352 76 196 388 0 695 0 001.csv ... ... ... ... ... ... ... ... 5634 441 283 277 788 45 585 4 001.csv 827 672 606 24 1023 463 742 4 001.csv 6703 324 203 0 623 214 726 4 001.csv 9056 604 398 0 981 0 633 4 001.csv 0 574 338 144 942 608 793 4 001.csv <s> A B C D E F G VAL FILE 954 380 158 166 431 201 769 0 val_0.csv 1142 348 203 962 0 878 1023 0 val_0.csv 1688 279 229 0 488 1007 0 0 val_0.csv 4792 371 420 29 372 0 745 0 val_0.csv 2106 352 76 196 388 0 695 0 val_0.csv A B C D E F G VAL FILE 5634 441 283 277 788 45 585 4 val_4.csv 827 672 606 24 1023 463 742 4 val_4.csv 6703 324 203 0 623 214 726 4 val_4.csv 9056 604 398 0 981 0 633 4 val_4.csv 0 574 338 144 942 608 793 4 val_4.csv ", "apis": "groupby to_csv index", "code": ["for group,data in df.groupby('VAL'):\n    data.to_csv(f\"val_{group}.csv\",index=False)\n"], "link": "https://stackoverflow.com/questions/59340489/splitting-dataframe-based-on-duplicate-values-into-multiple-csv-files"}
{"id": 563, "q": "Estimate the mean of a DataFrameGroupBy by only considering values in a percentile range", "d": "I need to estimate the mean of a pandas DataFrameGroupBy by only considering the values between a given percentile range. For instance, given the snippet the result is However, if a percentile range is picked to exclude the maximum and minimum values the result should be How can I filter, for each group, the values between an arbitrary percentile range before estimating the mean? For instance, only considering the values between the 20th and 80th percentiles.", "q_apis": "mean values mean values between values filter values between mean values between", "io": "m1 = 1 0 1 2.333333 2 2.333333 <s> m1 = 1 0 1 2 2 2 ", "apis": "DataFrame mean groupby apply reset_index quantile values between mean groupby apply reset_index concat", "code": ["import pandas as pd\nimport numpy as np\n\na = np.matrix('1 1; 1 2; 1 4; 2 1; 2 2; 2 4')\ndata = pd.DataFrame(a)\n\ndef jpp_np(df):\n    def meaner(x, lowperc, highperc):\n        low, high = np.percentile(x, [lowperc, highperc])\n        return x[(x > low) & (x < high)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndef jpp_pd(df):\n    def meaner(x, lowperc, highperc):\n        low, high = x.quantile([lowperc/100, highperc/100]).values\n        return x[x.between(low, high, inclusive=False)].mean()\n    return df.groupby(0)[1].apply(meaner, 20, 80).reset_index()\n\ndata = pd.concat([data]*10000)\n\nassert np.array_equal(jpp_np(data), jpp_pd(data))\n\n%timeit jpp_np(data)  # 11.2 ms per loop\n%timeit jpp_pd(data)  # 12.5 ms per loop\n"], "link": "https://stackoverflow.com/questions/53277378/estimate-the-mean-of-a-dataframegroupby-by-only-considering-values-in-a-percenti"}
{"id": 486, "q": "Operation on Pandas Dataframe columns using its Index", "d": "This should be relatively easy. I have a pandas dataframe (Dates): I would like to take the difference between Dates.index and Dates. The output would be like so: Naturally, I tried this: But I receive this lovely TypeError: Instead, I've written a loop to go column by column, but that just seems silly. Can anyone suggest a pythonic way to do this? EDIT", "q_apis": "columns Index take difference between index", "io": " A B C 1/8/2017 1/11/2017 1/20/2017 1/25/2017 1/9/2017 1/11/2017 1/20/2017 1/25/2017 1/10/2017 1/11/2017 1/20/2017 1/25/2017 1/11/2017 1/20/2017 1/25/2017 1/31/2017 1/12/2017 1/20/2017 1/25/2017 1/31/2017 1/13/2017 1/20/2017 1/25/2017 1/31/2017 <s> A B C 1/8/2017 3 12 17 1/9/2017 2 11 16 1/10/2017 1 10 15 1/11/2017 9 14 20 1/12/2017 8 13 19 1/13/2017 7 12 18 ", "apis": "apply to_datetime columns values apply index to_series", "code": ["df = df.apply(pd.to_datetime, axis=\"columns\")  # just to make sure values are datetime\ndf.apply(lambda x: x - df.index.to_series(), axis=\"rows)"], "link": "https://stackoverflow.com/questions/43096821/operation-on-pandas-dataframe-columns-using-its-index"}
{"id": 118, "q": "Insert complete repeated row under condition pandas", "d": "Basically, I'm trying to consider the third column (df1[3]) if the value is higher or equal to 2 I want to repeat i.e insert the whole row to a new row, not to replace. Here is the dataframe: desired output: code for the DataFrame and attempt to solve it: Obviously, the above-stated approach doesn't create a new row with the same values from each column but replaces it. Append() wouldn't solve it either because I do have to preserve the exact same order of the data frame. Is there anything similar to insert/extend/add or slicing approach in list when it comes to pandas dataframe?", "q_apis": "value repeat insert replace DataFrame values insert add", "io": " 1 2 3 0 5614 banana 1 1 4564 kiwi 1 2 3314 salsa 2 3 3144 avocado 1 4 1214 mix 3 5 4314 juice 1 <s> 1 2 3 1 5614 banana 1 2 4564 kiwi 1 3 3314 salsa 2 4 3314 salsa 2 5 3144 avocado 1 6 1214 mix 3 7 1214 mix 3 8 1214 mix 3 7 4314 juice 1 ", "apis": "count to_numeric fillna astype replace name loc index repeat count", "code": ["count = pd.to_numeric(df['3'], errors='coerce').fillna(0).astype(int)\n\n# replace '3' with actual column name\ndf.loc[df.index.repeat(count)]\n"], "link": "https://stackoverflow.com/questions/65797595/insert-complete-repeated-row-under-condition-pandas"}
{"id": 526, "q": "How to sum N columns in python?", "d": "I've a pandas df and I'd like to sum N of the columns. The df might look like this: I'd like to get a df like this: The A variable is not an index, but a variable.", "q_apis": "sum columns sum columns get index", "io": "A B C D ... X 1 4 2 6 3 2 3 1 2 2 3 1 1 2 4 4 2 3 5 ... 1 <s> A Z 1 15 2 8 3 8 4 11 ", "apis": "iloc join iloc sum rename", "code": ["df = df.iloc[:, [0]].join(df.iloc[:, 1:].sum(axis=1).rename('Z'))\n"], "link": "https://stackoverflow.com/questions/48749201/how-to-sum-n-columns-in-python"}
{"id": 276, "q": "group rows in a pandas data frame when the difference of consecutive rows are less than a value", "d": "I have a data frame like this, Now I want to group those rows where there difference between two consecutive col1 rows is less than 3. and sum other column values, create another column(col4) with the last value of the group, So the final data frame will look like, using for loop to do this is tedious, looking for some pandas shortcuts to do it most efficiently.", "q_apis": "difference value where difference between sum values last value", "io": "col1 col2 col3 1 2 3 2 3 4 4 2 3 7 2 8 8 3 4 9 3 3 15 1 12 <s> col1 col2 col3 col4 1 7 10 4 7 8 15 9 ", "apis": "groupby diff ge cumsum agg first sum sum last groups groupby diff ge cumsum groups agg first sum sum groups last", "code": ["(df.groupby(df.col1.diff().ge(3).cumsum(), as_index=False)\n   .agg(col1=('col1','first'),\n        col2=('col2','sum'),\n        col3=('col3','sum'),\n        col4=('col1','last'))\n)\n", "groups = df.groupby(df.col1.diff().ge(3).cumsum())\nnew_df = groups.agg({'col1':'first', 'col2':'sum','col3':'sum'})\nnew_df['col4'] = groups['col1'].last()\n"], "link": "https://stackoverflow.com/questions/62436677/group-rows-in-a-pandas-data-frame-when-the-difference-of-consecutive-rows-are-le"}
{"id": 348, "q": "How can I convert columns of a pandas DataFrame into a list of lists?", "d": "I have a pandas DataFrame with multiple columns. What I want to do is to convert this into a list like following 2u 2s 4r 4n 4m 7h 7v are column headings. It will change in different situations, so don't bother about it.", "q_apis": "columns DataFrame DataFrame columns", "io": "2u 2s 4r 4n 4m 7h 7v 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 <s> X = [ [0, 0, 1, 1, 1, 0], [1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1], [0, 1, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 1, 1, 1, 0], [1, 1, 0, 0, 0, 1] ] ", "apis": "values", "code": ["[list(l) for l in zip(*df.values)]\n"], "link": "https://stackoverflow.com/questions/15112234/how-can-i-convert-columns-of-a-pandas-dataframe-into-a-list-of-lists"}
{"id": 483, "q": "How can I sort numbers in a string in pandas?", "d": "I have a dataframe like this: I want to sort it in desending order like this I tried this: The above function do some kind of sorting but not the way I want it.", "q_apis": "", "io": "id String 1 345 -456 -13 879 2 158 -926 -81 249 35 -4 -53 9 3 945 -506 -103 <s> id String 1 879 345 -13 -457 2 249 158 35 9 -4 -53 -81 -926 3 945 -103 -506 ", "apis": "join map map map", "code": ["f = lambda x: ' '.join(map(str, sorted(map(int, x.split()), reverse=True)))\ndf['string'] = df['string'].map(f)\n"], "link": "https://stackoverflow.com/questions/56036178/how-can-i-sort-numbers-in-a-string-in-pandas"}
{"id": 191, "q": "How to add to dataframe column a dict?", "d": "Input dataframe: Following dataframe want as output: It is giving wrong output!", "q_apis": "add", "io": " Id Score Score1 0 19138359 0.5347029367015973 0.832428474443 1 12134001 0.9347094453553113 0.632535428479 <s> Id Scores 0 19138359 {'Score': 0.5347029367015973, 'Score1': 0.832428474443} 1 12134001 {'Score': 0.9347094453553113, 'Score1': 0.632535428479} ", "apis": "columns filter columns to_dict columns drop columns", "code": ["# Get score columns\nscore_columns = df.filter(like='Score').columns\n\n# Create dict of scores column\ndf['Scores'] = df[score_columns].to_dict(orient='records')\n\n# Drop original score columns\ndf.drop(columns=score_columns, inplace=True)\n"], "link": "https://stackoverflow.com/questions/64446862/how-to-add-to-dataframe-column-a-dict"}
{"id": 430, "q": "identify common elements between df rows to create a new column", "d": "My df is shown below. I want to create a new column called common which contains which other key has the same value as my current key. The final dataframe would look like: The only way I can think of is to create a column with empty dictionaries and then have two loops to get the result. I wanted to know if there is an easy way to do this. Thanks", "q_apis": "between contains value empty get", "io": " key val 0 A1 [1, 2, 3, 4] 1 A2 [1, 2, 7, 9] 2 A3 [1, 3, 5] 3 A4 [6, 9] 4 A5 [8] <s> key val common 0 A1 [1, 2, 3, 4] {'A2':[1, 2], 'A3':[1, 3]} 1 A2 [1, 2, 7, 9] {'A1':[1, 2], 'A3':[1], 'A4':[9], 'A5':[7]} 2 A3 [1, 3, 5] {'A1':[1, 3], 'A2':[1]} 3 A4 [6, 9] {'A2':[9]} 4 A5 [8] {} ", "apis": "reset_index drop to_dict groupby Series reindex values", "code": ["l={x: y.reset_index(level=0,drop=True).to_dict()for x , y in s.groupby(level=0)}\n\ndf['common']=pd.Series(l).reindex(df.Key).values\n"], "link": "https://stackoverflow.com/questions/57873651/identify-common-elements-between-df-rows-to-create-a-new-column"}
{"id": 2, "q": "pd.read_html changed number formatting", "d": "Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result", "q_apis": "read_html get", "io": " [ BBBBBB CCCCCCC AAAAAAA 0 DDDDDD 123456 1234.56 1 EEEEEEEEE 123456 1234.56 2 EEEEEEEEE 123456 1234.56 3 EEEEEEEEE 123456 1234.56 4 FFFFFFFFF 123456 1234.56 5 GGGGGGGGG 123456 1234.56 6 HHHHHHHHH 123456 1234.56 7 IIIIIIIIII 123456 1234.56 8 JJJJJJJJ 123456 1234.56 9 KKKKKKKK 1/2/3/4/5/6 1234.56 10 KKKKKKKK 1/2/3/4/5/6 1234.56] <s> [ BBBBBB CCCCCCC AAAAAAA 0 DDDDDD 1,2,3,4,5,6 1234.56 1 EEEEEEEEE 1,2,3,4,5,6 1234.56 2 EEEEEEEEE 1,2,3,4,5,6 1234.56 3 EEEEEEEEE 1,2,3,4,5,6 1234.56 4 FFFFFFFFF 1,2,3,4,5,6 1234.56 5 GGGGGGGGG 1,2,3,4,5,6 1234.56 6 HHHHHHHHH 1,2,3,4,5,6 1234.56 7 IIIIIIIIII 1,2,3,4,5,6 1234.56 8 JJJJJJJJ 1,2,3,4,5,6 1234.56 9 KKKKKKKK 1/2/3/4/5/6 1234.56 10 KKKKKKKK 1/2/3/4/5/6 1234.56] ", "apis": "div attrs read_html", "code": ["from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n"], "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting"}
{"id": 487, "q": "how to set Pandas to extract certain rows of certain columns and stack them on top of each other?", "d": "How can I extract certain columns and rows to stack them together? I created a simple exemplary dataframe with this data: This is what I would like to get: Another Format I would like to get is in two columns: The headers in the desired results are just for explanation", "q_apis": "columns stack columns stack get get columns", "io": " d1_d2_d3-t1-t2 1 101 2 201 3 102 4 202 5 103 6 203 <s> d1_d2_d3-t1-t2 d1_d2_d3-t3-t4 1 101 301 2 201 401 3 102 302 4 202 402 5 103 303 6 203 403 ", "apis": "loc melt DataFrame loc values ravel DataFrame loc values ravel loc values ravel", "code": ["idx = ['t1','t2']\ncols = ['d1','d2', 'd3']\ndf = dfa.loc[idx, cols].melt(value_name='data')[['data']]\n", "df = pd.DataFrame({'data': dfa.loc[idx, cols].values.ravel()})\n", "df = pd.DataFrame({'data1': dfa.loc[idx1, cols].values.ravel(),\n                   'data2': dfa.loc[idx2, cols].values.ravel()})\n"], "link": "https://stackoverflow.com/questions/55920437/how-to-set-pandas-to-extract-certain-rows-of-certain-columns-and-stack-them-on-t"}
{"id": 427, "q": "Count of rows where given columns of a DataFrame are non-zero", "d": "I have a that looks like this: I would like to have another matrix which gives me the number of non-zero elements for the intersection of every column except for . For example, the intersection of columns and would be 2 (because 1 and 3 have non-zero values for and ), intersection of and would be 2 as well (because 1 and 3 have non-zero values for and ). The final matrix would look like this: As we can see, it should be a symmetric matrix, similar to a correlation matrix, but not the correlation matrix. Intersection of any 2 columns = # of having non-zero values in both columns. I would show some initial code here but I feel like there would be a simple function to do this task that I don't know of. Here's the code to create the : Any pointers would be appreciated. TIA.", "q_apis": "where columns DataFrame intersection intersection columns values intersection values any columns values columns", "io": "MemberID A B C D 1 0.3 0.5 0.1 0 2 0 0.2 0.9 0.3 3 0.4 0.2 0.5 0.3 4 0.1 0 0 0.7 <s> A B C D A 3 2 2 2 B 2 3 3 2 C 2 3 3 2 D 2 2 2 3 ", "apis": "T dot", "code": ["z = (df != 0) * 1\nz.T.dot(z)\n"], "link": "https://stackoverflow.com/questions/38445974/count-of-rows-where-given-columns-of-a-dataframe-are-non-zero"}
