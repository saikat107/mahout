{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this and an array like that I would now like to add an additional column to which contains the word based on whether the values of and are contained as a subset in . A straightforward implementation would be which gives the desired outcome Is there a solution which wold avoid the loop?", "q_apis": "get columns array now add contains values", "apis": "merge DataFrame columns where eq", "code": ["out = df.merge(pd.DataFrame(a,columns=['A','B']),how='left',indicator=\"D\")\nout['D'] = np.where(out['D'].eq(\"both\"),\"Found\",\"Not Found\")\n", "print(out)\n\n   A  B  C          D\n0  1  5  a      Found\n1  2  2  b  Not Found\n2  3  4  c      Found\n3  2  1  d  Not Found\n4  3  4  e      Found\n5  1  5  f      Found\n"], "link": "https://stackoverflow.com/questions/66088436/fill-column-based-on-subsets-of-array"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've a dataframe with row and column multiindex like this I'd like to reindex to this in a immutable manner (copying the dataframe, not changing it in place). How can I achieve this?", "q_apis": "get columns reindex", "apis": "reset_index reset_index drop set_index append rename_axis sample index sample set_index", "code": ["df = (df.reset_index(level=1)\n        .reset_index(drop=True)\n        .set_index('level_1', append=True)\n        .rename_axis(['sample', None]))\n", "mux = pd.MultiIndex.from_arrays([np.arange(len(df)), \n                                 df.index.get_level_values(1)], \n                                names=['sample', None])\ndf = df.set_index(mux)\n"], "link": "https://stackoverflow.com/questions/62448062/reindex-dataframe-multiindex"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's say I have the following dataframe: How can I delete just the nth instance of a row matching a condition (for example the second instance of )? In this case the result should be:", "q_apis": "get columns delete nth second", "apis": "bool index", "code": ["nth = 2\ndf.drop(df.index[(df['a'] == 4)][nth-1])\n\n   a  b  c\n0  1  2  3\n1  4  5  6\n3  0  1  0\n", "df['a'] == 4\n\n0    False\n1     True\n2     True\n3    False\nName: a, dtype: bool\n", "df.index[(df['a'] == 4)]\n# Int64Index([1, 2], dtype='int64')\n"], "link": "https://stackoverflow.com/questions/54489696/find-the-nth-row-that-matches-a-condition-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a python code that loops through multiple location and pulls data from a third part API. Below is the code are location id coming from a directory. As you can see from the code the data gets converted to a data frame and then saved to a Excel file. The current issue I am facing is if the API does not returns data for for certain location the loop stops and does not proceeds and I get error as shown below the code. How do I avoid this and skip to another loop if no data is returned by the API? Key Error: publication_timestamp", "q_apis": "get columns get", "apis": "columns apply to_excel", "code": ["df = articles_list_normalized\nif 'publication_timestamp' in df.columns:\n    df['publication_timestamp'] = pd.to_datetime(df['publication_timestamp'])\n    df['publication_timestamp'] = df['publication_timestamp'].apply(lambda x: x.now().strftime('%Y-%m-%d'))\n    df.to_excel(writer, sheet_name = city_name)\nelse:\n    continue\n", "if articles_list:\n    df = json_normalize(articles_list)\n    # ... rest of code ...\nelse:\n    continue\n"], "link": "https://stackoverflow.com/questions/61063980/how-skip-to-another-loop-in-python-if-no-data-returned-by-the-api"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like (except mine is very large): ...then suppose I get the following groupby and aggregation (by , and ): where the day should go from 0-364 (365 days). What I want is the interquartile range (and median) of the counts for each user for all the days -- except that the zeroes aren't counted. Life would have been easier if I had explicit zeroes for all excluded days: ... because then I could do but I'm working with a very large dataframe (the example above is a dummy one), and reindexing with zeroes is just not possible. I have an idea how to do it: since I know there are 365 days, then I should just pad the rest of the numbers by zeroes: and get the (and median) of that. However, this would involve iterating over all pairs. From experience, that takes a lot of time. Is there any vectorized solution to this? I also have to get the median, too, and I think the same approach should hold.", "q_apis": "get columns get groupby where day days median all days all days days pad get median all time any get median", "apis": "groupby mean reset_index groupby agg reset_index values median apply", "code": ["test = df.groupby(['user1', 'user2', 'day'])['quantity'].mean().reset_index()\\\n         .groupby(['user1', 'user2'])\\\n         .agg({'day': lambda x: tuple(x), 'quantity': lambda x: tuple(x)})\\\n         .reset_index()\n\ndef med_from_tuple(row):\n    # starts with everything zero, and replaces some with the nonzero values in the dataframe\n    z = np.zeros(365)\n    np.put(z, row['day'], row['quantity'])\n    return np.median(z)\n\ntest['example'] = test.apply(lambda x: med_from_tuple(x), axis=1)\n", "test\n#   user1  user2     day    quantity   example\n#0  Alice    Bob  (1, 4)  (250, 600)       0.0\n#1    Bob  Carol    (1,)       (20,)       0.0\n"], "link": "https://stackoverflow.com/questions/50528833/getting-interquartile-range-and-median-from-pandas-groupby-zero-padding-for-all"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am creating an initial pandas dataframe to store results generated from other codes: e.g. with a predefined list. Then other codes will output some number for and for each , which I will store in the dataframe. So I want the first column to be , second and third . However, pandas will automatically reorder it alphabetically to , , at creation. While I can manually reorder this again afterwards, I wonder if there is an easier way to achieve this in one step. I figured I can also do but it somehow also looks tedious. Any other suggestions?", "q_apis": "get columns codes codes first second at step", "apis": "DataFrame columns DataFrame", "code": ["# option 1:\nresult = pd.DataFrame({'date': datelist, 'total': [0]*len(datelist), \n                   'TT': [0]*len(datelist)}, columns=['date', 'total', 'TT'])\n\n# option 2:\nod = collections.OrderedDict()\nod['date'] = datelist\nod['total'] = [0]*len(datelist)\nod['TT'] = [0]*len(datelist)\nresult = pd.DataFrame(od)\n"], "link": "https://stackoverflow.com/questions/39862053/pandas-create-dataframe-without-auto-ordering-column-names-alphabetically"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am currently using Python and Pandas to form a stock price \"database\". I managed to find some codes to download the stock prices. df1 is my existing database. Each time I download the share price, it will look like df2 and df3. Then, i need to combine df1, df2 and df3 data to look like df4. Each stock has its own column. Each date has its own row. df1: Existing database df2: New data (2/1/2018 and 4/1/2018) and updated data (3/1/2018) for Google. df3: New data for Amazon df4 Final output: Basically, it merges and updates all the data into the database. (df1 + df2 + df3) --> this will be the updated database of df1 I do not know how to combine and . And I do not know how to combine and (add new row: 4/1/2018) while at the same time updating the data (2/1/2018 -> Original data: NaN; amended data: 500 | 3/1/2018 -> Original data: 100; amended data: 300) and leaving the existing intact data (1/1/2018). Can anyone help me to get df4? =) Thank you. EDIT: Based on Sociopath suggestion, I amended the code to:", "q_apis": "get columns codes time combine date all combine combine add at time get", "apis": "DataFrame DataFrame DataFrame merge fillna drop columns merge fillna drop", "code": ["df1 = pd.DataFrame({'date':['2/1/2018','3/1/2018','4/1/2018'], 'Google':[500,300,200]})\ndf2 = pd.DataFrame({'date':['1/1/2018','2/1/2018','3/1/2018','4/1/2018'], 'Amazon':[1000,1500,2000,3000]})\ndf3 = pd.DataFrame({'date':['1/1/2018','2/1/2018','3/1/2018'], 'Apple':[161,171,181], 'Google':[1000,None,100], 'Facebook':[58,75,65]})\n", "df_new = df_new.merge(df1, how='outer', on='date')\n#print(df_new)\ndf_new['Google'] = df_new['Google_y'].fillna(df_new['Google_x'])\ndf_new.drop(['Google_x','Google_y'], 1, inplace=True)\n", "    date       Apple    Facebook    Amazon  Google\n0   1/1/2018    161.0   58.0        1000    1000.0\n1   2/1/2018    171.0   75.0        1500    500.0\n2   3/1/2018    181.0   65.0        2000    300.0\n3   4/1/2018    NaN     NaN         3000    200.0\n", "dataframes = [df2, df3, df4]  \n\nfor i in dataframes:\n    stock_name = list(i.columns.difference(['date']))[0]\n    df_new = df_new.merge(i, how='outer', on='date')\n    x = stock_name+\"_x\"\n    y = stock_name+\"_y\"\n\n    df_new[stock_name] = df_new[y].fillna(df_new[x])\n    df_new.drop([x,y], 1, inplace=True)\n"], "link": "https://stackoverflow.com/questions/54074837/python-pandas-merge-and-update-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset which requires missing value treatment. Now the problem is, when I try to replace the missing with mode of other using : Code: I get the following error: Stacktrace I have checked the of the and all of its columns and it is same: 43266. I have also found a question similar to this but does not have correct answer: Click here Please help resolve the error. IndexError: ('index out of bounds', 'occurred at index Consumer_disputes') Here is a snapshot of the dataset if it helps in any way: Dataset Snapshot I am using the below code successfully. But it does not serve my purpose exactly. Helps to fill the missing values though. Edit1: (Attaching Sample) Input Given: Expected Output: You can see that the missing values for company-response of Tr-1 and Tr-3 are filled by taking mode of Complaint-Reason. And similarly for the Consumer-Disputes by taking mode of transaction-type, for Tr-5. The below snippet consists of the dataframe and the code for those who want to replicate and give it a try. Replication Code", "q_apis": "get columns value replace mode get all columns index at index any values values mode mode", "apis": "groupby transform fillna mode groupby transform fillna mode", "code": ["data11[\"Company_response\"] = data11.groupby(\"Complaint_reason\")['Company_response'].transform(lambda x: x.fillna(x.mode()[0]))\n\ndata11[\"Consumer_disputes\"] = data11.groupby(\"Transaction_Type\")['Consumer_disputes'].transform(lambda x: x.fillna(x.mode()[0]))  \n"], "link": "https://stackoverflow.com/questions/53994621/indexerror-when-replacing-missing-values-with-mode-using-groupby-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 25 variables DXCODE1 to DXCODE25, which I want to scan across to see if any of these values for each row matches the icd_list. For example, in each row, I want to scan across from DXCODE1 to DXCODE25 and see if any of these contains any one of the following three values: 'F32', 'F33', 'F34', if it does, then I want to return 1. I tried the following: But I got this error: Also I would like to make it flexible so I can somehow specify the icd code as a list in the parameter. But I don't know how to apply syntax-wise: =================== Edit: The columns are labeled DXCODE1,DXCODE2, ... DXCODE25", "q_apis": "get columns any values any contains any values apply columns", "apis": "apply isin any apply apply isin any", "code": ["icd_list = ['F32', 'F33', 'F34']\ndf['ICD_DX'] = df.apply(lambda row: 1 if row.isin(icd_list).any() else 0, axis=1)\n", "def scan_icd (row, icd_list):\n    for i in range(1, 26):\n        dx_code_loc = 'DXCODE' + str(i)\n        for j in range(0, len(icd_list)):\n            if icd_list[j] in row[dx_code_loc]:\n                return 1\n    return 0 # return 0 if none match\nicd_list = ['F32', 'F33', 'F34']\ndf['ICD_DX'] = df.apply(scan_icd, args=([icd_list]), axis=1) \n# note the list of the list icd_list in args\n", "list_col = ['DXCODE' + str(i) for i in range(1,26)]\ndf['ICD_DX'] = df.apply(lambda row: 1 if row[list_col].isin(icd_list).any() else 0, axis=1)\n# see the difference is with row[list_col]\n"], "link": "https://stackoverflow.com/questions/50007357/how-to-create-a-new-pandas-column-by-scanning-cross-multiple-columns-using-for-l"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to add \"raw\" to the end of each value in a string column if it doesn't contain the word raw.", "q_apis": "get columns add value", "apis": "loc astype loc astype", "code": ["df.loc[~df[\"Title\"].str.contains(\"raw\"), 'Title'] = \"raw \" + df[\"Title\"].astype(str)\n", "df.loc[~df[\"Title\"].str.contains(\"raw\"), 'Title'] = df[\"Title\"].astype(str) + \" raw\"\n"], "link": "https://stackoverflow.com/questions/56558897/add-a-string-suffx-to-each-value-in-a-string-column-using-pandas-if-it-doesnt-c"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to convert data in a Pandas column (containing IP address in each row) to latitude and longitude. Below are my codes: After running these codes I got an error AttributeError: 'Series' object has no attribute 'latlng' I am not sure why it does that. Could anyone provide me some feedbacks here? Really appreciate it.", "q_apis": "get columns codes codes Series", "apis": "DataFrame apply", "code": ["import geocoder\n\nd = pd.DataFrame({'ip_address' : ['49.206.217.180', '200.8.245.246', '186.188.35.217']})\nd['coordinate'] = d['ip_address'].apply(lambda x: geocoder.ip(x).latlng) \n"], "link": "https://stackoverflow.com/questions/49703873/unable-to-convert-ip-address-in-pandas-column-to-latlng-latitude-longitude"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two pandas dataframes with identical shape, index and column. Each element of is a with shape , and each element of is a float value. Now I want to efficiently append elementwise to . A minimal example: Is there something similar to that can operate elementwise between two instead of just one pandas dataframe?", "q_apis": "get columns identical shape index shape value append between", "apis": "index DataFrame index DataFrame index applymap apply groupby apply values columns DataFrame append loc loc columns index columns columns index index", "code": ["index = ['fst', 'scd']\n\ncolumn = ['a','b']\n\n\nA = pd.DataFrame([[[1, 2],[1, 4]],[[3, 4],[3, 2]]],index,column)\nB = pd.DataFrame([[0.392414,0.264117],[ 0.641136 , 1.644251]],index,column)\n", "n =  B.applymap(lambda y: [y])\nndf = A.apply(lambda x : x+n[x.name])\n", "pd.concat([A,B]).groupby(level=0).apply(lambda g: pd.Series({i: np.hstack(g[i].values) for i in A.columns}))\n", "pd.DataFrame([[np.append(A.loc[i,j], B.loc[i,j]) for j in A.columns] for i in A.index], columns=A.columns, index=A.index)\n"], "link": "https://stackoverflow.com/questions/46580327/pandas-dataframe-how-to-do-elementwise-concatenation"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "After creating the three-rows DataFrame: I check if there is any cell equal to '3-4': Since command results to object I can use it to create a \"filtered\" version of the original DataFrame like so: In Python I can check for the occurrence of the string character in another string using: What would be a way to accomplish the same while working with DataFrames? So, I could create the filtered version of the original DataFrame by checking if '-' character in every row's cell, like: But this syntax above is invalid and throws error message.", "q_apis": "get columns DataFrame any DataFrame DataFrame", "apis": "bool", "code": ["In [5]: df['a'].str.contains('-')\nOut[5]: \n0    True\n1    True\n2    True\nName: a, dtype: bool\n"], "link": "https://stackoverflow.com/questions/39299703/how-to-check-if-character-exists-in-dataframe-cell"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Loops in python taking alot time to give result.This contains around 100k records. It is taking lot of time. How time can be reduced", "q_apis": "get columns time contains time time", "apis": "mask loc mask loc mask loc mask loc mask mask", "code": ["mask = df['order_mt'] == df['enr_mt']\ndf.loc[mask, ['new_N_Loan', 'exist_N_Loan', 'exist_V_Loan']] = [1, 0, 0]\ndf.loc[mask, ['new_V_Loan']] = df['loan_agr']\n\ndf.loc[~mask, ['new_N_Loan', 'exist_N_Loan', 'new_V_Loan']] = [0, 1, 0]\ndf.loc[~mask, ['exist_V_Loan']] = df['loan_agr']\n", "mask = df['order_mt'] == df['enr_mt']\nelse_mask = df['order_mt'] != df['enr_mt']\n", "   order_mt  enr_mt new_N_Loan exist_N_Loan exist_V_Loan new_V_Loan  loan_agr\n0         1       1       None         None         None       None       100\n1         2       2       None         None         None       None       200\n2         3      30       None         None         None       None       300\n3         4      40       None         None         None       None       400\n", "   order_mt  enr_mt  new_N_Loan  exist_N_Loan  exist_V_Loan  new_V_Loan  loan_agr\n0         1       1           1             0             0         100       100\n1         2       2           1             0             0         200       200\n2         3      30           0             1           300           0       300\n3         4      40           0             1           400           0       400\n"], "link": "https://stackoverflow.com/questions/53905926/reduce-loop-time-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset of football matches as a Pandas dataframe in the form below: I'd like to track results by team in a dictionary of lists: etc. My approach naturally is to iterate over all the rows with conditionals (below is psuedo-code): I believe I can do this using but I know this isn't the desired approach in general with Pandas. Is there a way I can do this kind of operation while taking advantage of the power of Pandas DataFrames? I have seen that returns a series of True/False values but I don't know how to utilise this or extend it to the multiple conditions described above. I have also seen and from this answer, but I don't see it being applicable to this situation where I want to do something based off the outcome of a conditional on each row, using an entry in the row as a key. It feels as though iteration is the only solution here, I'm sure Pandas would support something like this but I have no idea how to search for it.", "q_apis": "get columns all values where", "apis": "DataFrame DataFrame reset_index melt index sort_values index agg groupby apply to_dict", "code": ["#Setup\ndf = pd.DataFrame({'Home': ['Liverpool', 'West Ham', 'AFC Bournemouth', 'Burnley', 'Crystal Palace', 'Norwich City'], 'Away': ['Norwich City', 'Man City', 'Sheffield United', 'Southampton', 'Everton', 'Liverpool'], 'Result': ['Liverpool', 'Man City', 'Draw', 'Burnley', 'Draw', 'Norwich City'], 'HG': [4, 0, 1, 3, 0, 4], 'AG': [1, 5, 1, 0, 0, 1]})\n\n# Restructure the DataFrame into a long-form with \"Melt\"\ndf_melted = (df.reset_index()\n             .melt(id_vars=['Result', 'index'],\n                   value_vars=['Home', 'Away'])\n             .sort_values('index')) # This maintains the match order of original df\n\n# Use numpy.select to create your conditions and choices ('W', 'L' or 'D')\ndf_melted['outcome'] = np.select(\n    condlist=[df_melted['Result'] == 'Draw',\n              df_melted['Result'] == df_melted['value'],\n              df_melted['Result'] != df_melted['value']],\n    choicelist=['D', 'W', 'L'])\n\n\n# Groupby team agg with list and return output as dict\ndf_melted.groupby('value', sort=False)['outcome'].apply(list).to_dict()\n", "{'Liverpool': ['W', 'L'],\n 'Norwich City': ['L', 'W'],\n 'West Ham': ['L'],\n 'Man City': ['W'],\n 'AFC Bournemouth': ['D'],\n 'Sheffield United': ['D'],\n 'Burnley': ['W'],\n 'Southampton': ['L'],\n 'Crystal Palace': ['D'],\n 'Everton': ['D']}\n"], "link": "https://stackoverflow.com/questions/59893471/correct-way-to-iterate-over-a-dataframe-using-multiple-conditionals"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to apply for loops inside a Pandas dataframe to access two columns at a time. My piece of code works perfectly for a single column. But when applying to multiple columns, it is throwing : \"ValueError : too many values to unpack (expected 2)\" My code snippet is as follows - The small problem is the column names are too large and not under control, because this dataframe has multiheader columns, so after merging they are creating some random filling names. Hence the \".startswith\". The column names are much larger. I am trying to perform a groupby of column 3 based on columns 1 and 2, if column 2 is not null, else a groupby using column1 when column 2 is null. Can anyone tell me where am I wrong here, or what am I missing here?", "q_apis": "get columns apply columns at time columns values names columns names names groupby columns groupby where", "apis": "columns", "code": ["import itertools as it\n\nfor col1, col2 in it.zip_longest(*[iter(df.columns)]*2):\n    ...\n"], "link": "https://stackoverflow.com/questions/67961026/looping-over-dataframe-using-multiple-columns-throwing-valueerror"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to drop rows where any column contains one of the keywords df before: df after: How can i achieve this?", "q_apis": "get columns drop where any contains", "apis": "apply join join apply join join", "code": ["df = df[~df.apply(' '.join, 1).str.contains('|'.join(keywords))]\nprint (df)\n    Brand     ID            Description\n1  iPhone  DF747                battery\n2    Acer  KH298  exchanged for a nokia\n", "df = df[~df.apply(' '.join, 1).str.contains('|'.join(keywords), case=False)]\nprint (df)\n    Brand     ID Description\n1  iPhone  DF747     battery\n"], "link": "https://stackoverflow.com/questions/57933759/delete-row-if-any-column-contains-one-of-the-keywords"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I have a dataframe containing a float64 type column and an object type column containing a string. If object column contains substring 'abc' I want to subtract 12 from the float column. If object column contains substring 'def' I want to subtract 24 from the float column. If object column contains neither 'abc' or 'def', I want to leave float column as is. Example: Expected output: I have tried the following but keep getting an error: The error I'm getting is as follows: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Note:Line 917 is the one that's highlighted as the error.", "q_apis": "get columns contains contains contains value Series empty bool item any all", "apis": "loc loc", "code": ["df.loc[df['Strng'].str.contains('abc', regex=False, na=False), 'Nmbr'] -= 12\ndf.loc[df['Strng'].str.contains('def', regex=False, na=False), 'Nmbr'] -= 24\n", "   Nmbr   Strng\n0    40  abcghi\n1    56  defghi\n2    10  ghijkl\n"], "link": "https://stackoverflow.com/questions/49223863/how-to-update-a-python-dataframe-column-dependent-on-the-presence-of-a-substring"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am grouping a dataframe by 2 columns and i aggregate by the sum of the other columns. How I can have a total by the first grouped column in the same data frame? for example my data frame is: The result of: is: I what to get: how it can be done? UPDATE: I found a similar question at Pandas groupby and sum total of group It has 2 more answer for this question.", "q_apis": "get columns columns aggregate sum columns first get at groupby sum", "apis": "append groupby sum loc groupby sum values append groupby agg sum count reindex loc groupby sum values", "code": ["df.B = pd.Categorical(\n         df.B, categories=np.append(df.B.unique(), 'total'))\nv = df.groupby(by=['A', 'B']).sum()\nv.loc(axis=0)[pd.IndexSlice[:,'total']] = v.groupby(level=0).sum().values\n", "print(v)\n                  C         D\nA   B                        \nbar one    0.400157  0.410599\n    two   -0.977278  0.121675\n    three  2.240893  1.454274\n    total  1.663773  1.986547\nfoo one    2.714141  0.340644\n    two    2.846296  0.905081\n    three -0.151357  0.333674\n    total  5.409080  1.579400\n", "df.B = pd.Categorical(\n         df.B, categories=np.append(df.B.unique(), 'total'))\nidx = pd.MultiIndex.from_product([df.A.unique(), df.B.cat.categories]) \n\nv = df.groupby(by=['A', 'B']).agg(['sum', 'count']).reindex(idx)\nv.loc(axis=0)[pd.IndexSlice[:,'total']] = v.groupby(level=0, sort=False).sum().values\n", "print(v)\n                  C               D      \n                sum count       sum count\nfoo one    2.714141   2.0  0.340644   2.0\n    two    2.846296   2.0  0.905081   2.0\n    three -0.151357   1.0  0.333674   1.0\n    total  5.409080   5.0  1.579400   5.0\nbar one    0.400157   1.0  0.410599   1.0\n    two   -0.977278   1.0  0.121675   1.0\n    three  2.240893   1.0  1.454274   1.0\n    total  1.663773   3.0  1.986547   3.0\n", "df.pivot_table(index=['A', 'B'], \n               values=['C', 'D'], \n               aggfunc=['sum', 'count'], \n               margins=True)\n\n                sum           count     \n                  C         D     C    D\nA   B                                   \nbar one    0.400157  0.410599   1.0  1.0\n    two   -0.977278  0.121675   1.0  1.0\n    three  2.240893  1.454274   1.0  1.0\nfoo one    2.714141  0.340644   2.0  2.0\n    two    2.846296  0.905081   2.0  2.0\n    three -0.151357  0.333674   1.0  1.0\nAll        7.072852  3.565947   8.0  8.0\n"], "link": "https://stackoverflow.com/questions/53719607/groupby-two-columns-with-margins-for-first-level"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How can I create a first-differenced dataframe for each item in the list below? Code needs to be general. I tried:", "q_apis": "get columns first item", "apis": "diff fillna update", "code": ["listxx  = [\"price\"]\nfor x in listxx:\n      globals().update({f'{x}_first_diff':globals()[x].diff().fillna('')})\n\nprice_first_diff\n#              price\n#date               \n#2010-01-04         \n#2010-01-04  89.3272\n#2010-01-04  229.779\n#2010-01-04 -340.599\n#2010-01-04    -0.63\n", "listxx  = [\"price\"]\ndc={f'{x}_first_diff' :globals()[x].diff().fillna('') for x in listxx}\nglobals().update(dc)\n"], "link": "https://stackoverflow.com/questions/63381880/create-new-dataframe-using-globals"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have dataframe with time series where one column has strings: and . I would like to find all rows which are just between rows with and assign them 0 to new column. Rows which have and are not between rows with should have value 1. column represents high peaks in time series. Sample dataframe: Expected output:", "q_apis": "get columns time where all between assign between value time", "apis": "values append append append append values append", "code": ["a = []\nis_wrong = 0\nfor value in df['String'].values:\n    if is_wrong == 0:\n        if value == 'Normal Value':\n            a.append(1)\n        else:\n            a.append(0)\n            is_wrong = 1\n    else:\n        if value == 'Normal Value':\n            a.append(0)\n        else:\n            a.append(0)\n            is_wrong = 0\ndf['Expected Value'] = a\n", "a = []\nis_wrong = False # store the current state\nfor value in df['String'].map({'Normal Value':True,'Wrong Value':False}).values:\n    a.append(value and not is_wrong) # check the current state and output value\n    is_wrong = is_wrong if value else not is_wrong # change the state if needed\ndf['Expected Value'] = [int(x) for x in a]\n", "df['Expected Value'] = [1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n"], "link": "https://stackoverflow.com/questions/58391709/how-to-assign-value-to-rows-which-are-between-two-rows-with-specific-string-in-c"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Is there any way to improve the performance of when the data is already sorted by the columns that are used for the index? On a dataset with 40 mio records, takes 3.25 mins for me, independent on whether the data is sorted already or not. If there is no intended way high level way to do it, is there maybe a low level way of changing the state to sorted by index without actually sorting it?", "q_apis": "get columns any columns index index", "apis": "index sort_index", "code": ["if not df.index.is_monotonic_increasing:\n    df = df.sort_index()\n"], "link": "https://stackoverflow.com/questions/62150533/can-pandas-dataframe-sort-index-performance-be-improved-on-sorted-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am rying to use the label encoder in orrder to convert categorical data into numeric values. I needed a LabelEncoder that keeps my missing values as 'NaN' to use an Imputer afterwards. So I would like to use a mask to replace form the original data frame after labelling like this So I get a dataframe with True/false value Then , in create the encoder : How can I proceed then, in orfer to encoder these values? thanks", "q_apis": "get columns values values mask replace get value values", "apis": "dtypes", "code": ["That case you can do the following:\ndf = df.apply(lambda series: pd.Series(\n    LabelEncoder().fit_transform(series[series.notnull()]),\n    index=series[series.notnull()].index\n))\nprint(df)\nOut:\n     A  B    C\n0  0.0  0  1.0\n1  NaN  1  0.0\n2  1.0  2  NaN\n", "df.dtypes\nA    float64\nB      int64\nC    float64\ndtype: object\n", "encoders = dict()\n\nfor col_name in df.columns:\n    series = df[col_name]\n    label_encoder = LabelEncoder()\n    df[col_name] = pd.Series(\n        label_encoder.fit_transform(series[series.notnull()]),\n        index=series[series.notnull()].index\n    )\n    encoders[col_name] = label_encoder\n\nprint(df)\nOut:\n     A  B    C\n0  0.0  0  1.0\n1  NaN  1  0.0\n2  1.0  2  NaN\n", "print(encoders)\nOut\n{'A': LabelEncoder(), 'B': LabelEncoder(), 'C': LabelEncoder()}\n", "encoders['B'].inverse_transform(df['B'])\nOut:\narray([1, 6, 9])\n", "print(df)\nOut:\n     A  B    C\n0    x  1  2.0\n1  NaN  6  1.0\n2    z  9  NaN\ndf = df.apply(lambda series: pd.Series(\n    LabelEncoder().fit_transform(series[series.notnull()]),\n))\nprint(df)\nOut:\n     A  B    C\n0  0.0  0  1.0\n1  1.0  1  0.0\n2  NaN  2  NaN\n", "series_stack = df.stack().astype(str)\nlabel_encoder = LabelEncoder()\ndf = pd.Series(\n    label_encoder.fit_transform(series_stack),\n    index=series_stack.index\n).unstack()\nprint(df)\nOut:\n     A    B    C\n0  5.0  0.0  2.0\n1  NaN  3.0  1.0\n2  6.0  4.0  NaN\n"], "link": "https://stackoverflow.com/questions/54444260/labelencoder-that-keeps-missing-values-as-nan"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a huge .csv file with date as one of the column and I'm trying to plot it on a graph but I'm getting this error \"time data '01-Sept-20' does not match format '%d-%b-%y' (match)\" I'm using this line of code to convert it into datetime format I think this error is because 'Sept' should be 'Sep' What can I do to make Sept to Sep? I'm using this dataset: covid19 api", "q_apis": "get columns date plot time", "apis": "replace", "code": ["import pandas as pd\n\ndf = pd.read_csv('covid.csv')\ndf['Date_YMD'] = pd.to_datetime(df['Date_YMD'])\ndf['Date'] = pd.to_datetime(df['Date'].str.replace('Sept', 'Sep'), format='%d-%b-%y')\n"], "link": "https://stackoverflow.com/questions/65747951/change-date-01-sept-20-to-01-sep-20-using-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here's my starting dataframe: I need to create a list of individual dataframes based on duplicate values in columns A and B, so it should look like this: I've seen a lot of answers that explain how to DROP duplicates, but I need to keep the duplicate values because the information in column C will usually be different between rows regardless of duplicates in columns A and B. All of the row data needs to be preserved in the new dataframes. Additional note, the starting dataframe (StartDF) will change in length, so each time this is run, the number of individual dataframes created will be variable. Ultimately, I need to print the newly created dataframes to their own csv files (I know how to do this part). Just need to know how to break out the data from the original dataframe in an elegant way.", "q_apis": "get columns values columns values between columns length time", "apis": "groupby", "code": ["df_list = [g for _, g in df.groupby(['A', 'B'])]\n", "print(*df_list, sep='\\n\\n')\n\n   A  B     C\n0  1  2    10\n1  1  2  1000\n\n   A  B    C\n2  2  4  250\n\n   A  B    C\n3  4  2  100\n\n   A  B    C\n4  5  2  550\n\n   A  B     C\n5  5  4   100\n6  5  4  3000\n\n   A  B    C\n7  5  5  250\n"], "link": "https://stackoverflow.com/questions/48448647/splitting-a-dataframe-based-on-duplicate-values-in-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame I also have a dictionary, where in signifies that I'm only modifying rows where . How do I populate the DataFrame with values from the dictionary to get", "q_apis": "get columns DataFrame where where DataFrame values get", "apis": "apply apply", "code": ["test['C'] = test['B'].map(input_b).apply(lambda x: x[0] if type(x)==list else x)\ntest['D'] = test['B'].map(input_b).apply(lambda x: x[1] if type(x)==list else x)\n", "   A  B     C         D\n0  a  1   NaN       NaN\n1  b  2  Moon  Elephant\n2  b  3   NaN       NaN\n3  b  4   Sun     Mouse\n"], "link": "https://stackoverflow.com/questions/54014687/populate-pandas-dataframe-using-a-dictionary-based-on-a-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have an empty . for some reason I want to generate df2, another empty dataframe, with two columns 'a' and 'b'. If I do it does not work (I get the columns renamed to 'ab') and neither does the following How to add a separate column 'b' to df, and keep on being ? Using .loc is also not possible as it returns", "q_apis": "get columns empty empty columns get columns add loc", "apis": "DataFrame columns assign assign index index DataFrame columns DataFrame", "code": ["df=pd.DataFrame(columns=['a'])\ndf['b'] = None\ndf = df.assign(c=None)\ndf = df.assign(d=df['a'])\ndf['e'] = pd.Series(index=df.index)   \ndf = pd.concat([df,pd.DataFrame(columns=list('f'))])\nprint(df)\n", "Empty DataFrame\nColumns: [a, b, c, d, e, f]\nIndex: []\n"], "link": "https://stackoverflow.com/questions/50372272/how-to-add-columns-to-an-empty-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hi I want to create a dataframe that stores a unique variable and its average in every column. Currently I have a dataframe that has 2 columns. One has a list of names while the other has a single value. I want to associate that value with all of the names in the list and eventually find the average value for all the names This is the data I have: This is what I want: I thought about doing an apply over all the rows somehow or using set() to remove duplicates out of every list but I am not sure. Any help would be appreicated", "q_apis": "get columns unique columns names value value all names value all names apply all", "apis": "DataFrame values values groupby mean", "code": ["ndf = pd.DataFrame({'a':np.repeat(df.cost_col.values, df.names_col.str.len()),\n                    'b':np.concatenate(df.names_col.values)})\n", "    a   b\n0   3   milk\n1   3   eggs\n2   3   cookies\n3   5   water\n4   5   milk\n5   5   yogurt\n6   7   cookies\n7   7   diaper\n8   7   yogurt\n", "ndf.groupby('b').mean()\n\n        a\nb   \ncookies 5\ndiaper  7\neggs    3\nmilk    4\nwater   5\nyogurt  6\n"], "link": "https://stackoverflow.com/questions/51664482/python-pandas-create-unique-dataframe-out-of-many-lists"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've got a dataframe structured in the following way: timestamp participant id upstream downstream 1.1.2020 person 1 1 0 1.1.2020 person 1 1 1 1.2.2020 person 1 1 0 1.2.2020 person 1 1 1 1.1.2020 person 2 1 0 1.1.2020 person 2 1 1 1.2.2020 person 2 1 0 I'm looking to create a function which creates a cumulative count of sessions per person. A new session starts when upstream = 1 and downstream = 0. The ideal output is this: timestamp participant id session 1.1.2020 person 1 1 1.1.2020 person 1 1 1.2.2020 person 1 2 1.2.2020 person 1 2 1.1.2020 person 2 1 1.1.2020 person 2 1 1.1.2020 person 2 2 What I've attempted so far is; But i'm unsure how to get this to give a cumulative count specific to each person. So when there is a new person identifier, the count restarts to 0. Any help much appreciated, thanks a lot.", "q_apis": "get columns timestamp count timestamp get count count", "apis": "eq eq groupby cumsum", "code": ["df[\"session\"] = df[\"upstream\"].eq(1) & df[\"downstream\"].eq(0)\ndf[\"session\"] = df.groupby(\"participant id\", as_index=False)[\"session\"].cumsum()\nprint(df)\n", "  timestamp participant id  upstream  downstream  session\n0  1.1.2020       person 1         1           0        1\n1  1.1.2020       person 1         1           1        1\n2  1.2.2020       person 1         1           0        2\n3  1.2.2020       person 1         1           1        2\n4  1.1.2020       person 2         1           0        1\n5  1.1.2020       person 2         1           1        1\n6  1.2.2020       person 2         1           0        2\n"], "link": "https://stackoverflow.com/questions/68322968/create-function-to-count-number-of-web-sessions-in-dataframe-per-unique-id"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i am currently learning pandas and have an issue cleaning my Dataframe: I do not understand why pandas recognizes parts as float64 and others as object. Do you guys have any clue? Because of this, i started trying to convert the columns on my own: But i get an error: Why does pandas cant read the .dat file correct from the start and what is my fault converting it. In the next stemp i want to interpolate via df.interpolate() to clear the nan's thanks for any help!", "q_apis": "get columns any columns get start interpolate interpolate any", "apis": "drop apply", "code": ["df = pd.read_csv(FilePath, parse_dates=['TIMESTAMP'], index_col=['TIMESTAMP'])\n\ndf = df.drop(['RECORD'],axis=1)\n", "df = df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n", "import pandas as pd\n\npd.options.display.max_columns = 20\n\ntemp=u\"\"\"\"TIMESTAMP\",\"RECORD\",\"WM1_u_ms\",\"WM1_v_ms\",\"WM1_w_ms\",\"WM2_u_ms\",\"WM2_v_ms\",\"WM2_w_ms\",\"WS1_u_ms\",\"WS1_v_ms\"\n\"2018-04-06;14:31:11.5\",29699805,2.628a,4.629a,0.599s,3.908,7.971,0.47,2;;51,7.18\n\"2018-04-06;14:31:11.75\",29699806,3.264,4.755,-0.095,2.961,6.094,-0.504,2.47,7.18\n\"2018-04-06;14:31:12\",29699807,1.542,5.793,0.698,4.95,4.91,0.845,2.18,7.5\n\"2018-04-06;14:31:12.25\",29699808,2.527,5.207,0.012,4.843,6.285,0.924,2.15,7.4\n\"2018-04-06;14:31:12.5\",29699809,3.511,4.528,1.059,2.986,5.636,0.949,3.29,5.54\n\"2018-04-06;14:31:12.75\",29699810,3.445,3.957,-0.075,3.127,6.561,0.259,3.85,5.45\n\"2018-04-06;14:31:13\",29699811,2.624,5.238,-0.166,3.451,7.199,0.242,3.94,a\"\"\"\n#after testing replace 'pd.compat.StringIO(temp)' to 'filename.csv'\ndf = pd.read_csv(pd.compat.StringIO(temp), parse_dates=['TIMESTAMP'], index_col=['TIMESTAMP'])\n", "print (df)\n                           RECORD WM1_u_ms WM1_v_ms WM1_w_ms  WM2_u_ms  \\\nTIMESTAMP                                                                \n2018-04-06 14:31:11.500  29699805   2.628a   4.629a   0.599s     3.908   \n2018-04-06 14:31:11.750  29699806    3.264    4.755   -0.095     2.961   \n2018-04-06 14:31:12.000  29699807    1.542    5.793    0.698     4.950   \n2018-04-06 14:31:12.250  29699808    2.527    5.207    0.012     4.843   \n2018-04-06 14:31:12.500  29699809    3.511    4.528    1.059     2.986   \n2018-04-06 14:31:12.750  29699810    3.445    3.957   -0.075     3.127   \n2018-04-06 14:31:13.000  29699811    2.624    5.238   -0.166     3.451   \n\n                         WM2_v_ms  WM2_w_ms WS1_u_ms WS1_v_ms  \nTIMESTAMP                                                      \n2018-04-06 14:31:11.500     7.971     0.470    2;;51     7.18  \n2018-04-06 14:31:11.750     6.094    -0.504     2.47     7.18  \n2018-04-06 14:31:12.000     4.910     0.845     2.18      7.5  \n2018-04-06 14:31:12.250     6.285     0.924     2.15      7.4  \n2018-04-06 14:31:12.500     5.636     0.949     3.29     5.54  \n2018-04-06 14:31:12.750     6.561     0.259     3.85     5.45  \n2018-04-06 14:31:13.000     7.199     0.242     3.94        a  \n", "print (df.dtypes)\nRECORD        int64\nWM1_u_ms     object\nWM1_v_ms     object\nWM1_w_ms     object\nWM2_u_ms    float64\nWM2_v_ms    float64\nWM2_w_ms    float64\nWS1_u_ms     object\nWS1_v_ms     object\ndtype: object\n\nprint (df.index)\nDatetimeIndex(['2018-04-06 14:31:11.500000', '2018-04-06 14:31:11.750000',\n                      '2018-04-06 14:31:12', '2018-04-06 14:31:12.250000',\n               '2018-04-06 14:31:12.500000', '2018-04-06 14:31:12.750000',\n                      '2018-04-06 14:31:13'],\n              dtype='datetime64[ns]', name='TIMESTAMP', freq=None)\n\n\ndf = df.drop(['RECORD'],axis=1)\ndf = df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n", "print (df)\n                         WM1_u_ms  WM1_v_ms  WM1_w_ms  WM2_u_ms  WM2_v_ms  \\\nTIMESTAMP                                                                   \n2018-04-06 14:31:11.500       NaN       NaN       NaN     3.908     7.971   \n2018-04-06 14:31:11.750     3.264     4.755    -0.095     2.961     6.094   \n2018-04-06 14:31:12.000     1.542     5.793     0.698     4.950     4.910   \n2018-04-06 14:31:12.250     2.527     5.207     0.012     4.843     6.285   \n2018-04-06 14:31:12.500     3.511     4.528     1.059     2.986     5.636   \n2018-04-06 14:31:12.750     3.445     3.957    -0.075     3.127     6.561   \n2018-04-06 14:31:13.000     2.624     5.238    -0.166     3.451     7.199   \n\n                         WM2_w_ms  WS1_u_ms  WS1_v_ms  \nTIMESTAMP                                              \n2018-04-06 14:31:11.500     0.470       NaN      7.18  \n2018-04-06 14:31:11.750    -0.504      2.47      7.18  \n2018-04-06 14:31:12.000     0.845      2.18      7.50  \n2018-04-06 14:31:12.250     0.924      2.15      7.40  \n2018-04-06 14:31:12.500     0.949      3.29      5.54  \n2018-04-06 14:31:12.750     0.259      3.85      5.45  \n2018-04-06 14:31:13.000     0.242      3.94       NaN  \n\nprint (df.dtypes)\nWM1_u_ms    float64\nWM1_v_ms    float64\nWM1_w_ms    float64\nWM2_u_ms    float64\nWM2_v_ms    float64\nWM2_w_ms    float64\nWS1_u_ms    float64\nWS1_v_ms    float64\ndtype: object\n"], "link": "https://stackoverflow.com/questions/53744456/pandas-cleaning-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Assume that I have the following dataframe: How can I keep only the columns where , and including the column as well? The desired output is:", "q_apis": "get columns columns where", "apis": "set_index T count mean min loc T reset_index count mean min max", "code": ["df2 = df.set_index('summary').T\nm1 = df2['count'] > 5\nm2 = df2['mean'] > 4\nm3 = df2['min'] > 0\ndf2.loc[m1 & m2 & m3].T.reset_index()\n", "    summary col3\n0   count   10\n1   mean    5\n2   stddev  3\n3   min     5\n4   max     47\n"], "link": "https://stackoverflow.com/questions/57464432/drop-columns-in-pandas-dataframe-based-on-conditions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Problem I've got a column in a pandas DataFrame that contains timestamps with timezones. There are two different timezones present in this column, and I need to ensure that there's only one. Here's the output of the end of the column: For what it's worth, the timestamps vary between and , and have the following output: for for What I've Done I've been trying to use tz.localize and tz.convert, which have worked fine in the past, but I suppose that data has only ever had one timezone. E.g., if I do: I get: Question Is there a way to convert these to MST? Or any timezone, really? I guess I could break up the DataFrame by timezone (not 100% sure how, but I imagine it's possible) and act on chunks of it, but I figured I'd ask to see if there's a smarter solution out there. Thank you!", "q_apis": "get columns DataFrame contains between tz tz get any DataFrame", "apis": "DataFrame tz_localize tz_localize", "code": ["df = pd.DataFrame({'timestamp':['2019-05-21 12:00:00-06:00',\n                                '2019-05-21 12:15:00-07:00']})\ndf['timestamp'] = pd.to_datetime(df.timestamp)\n\ndf.timestamp.dt.tz_localize('MST')\n", "0   2019-05-21 18:00:00-07:00\n1   2019-05-21 19:15:00-07:00\nName: timestamp, dtype: datetime64[ns, MST]\n", "df = pd.DataFrame({'timestamp':[pd.to_datetime('2019-05-21 12:00:00').tz_localize('MST'),\n                         pd.to_datetime('2019-05-21 12:15:00').tz_localize('EST')]})\n", "df.timestamp = pd.to_datetime(df.timestamp, utc=True)\n\n# df.timestamp\n# 0   2019-05-21 19:00:00+00:00\n# 1   2019-05-21 17:15:00+00:00\n# Name: timestamp, dtype: datetime64[ns, UTC]\n\ndf.timestamp.dt.tz_convert('MST')\n", "0   2019-05-21 12:00:00-07:00\n1   2019-05-21 10:15:00-07:00\nName: timestamp, dtype: datetime64[ns, MST]\n"], "link": "https://stackoverflow.com/questions/56245036/convert-pandas-column-with-multiple-timezones-to-single-timezone"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset where each row is a sample, and a column (name \"Sample_ID\") names each sample (df1 below). Some samples are repeated multiple times (i.e. have identical values for \"Sample_ID\"). I would like to generate a new column with different names for each sample (I'll call it \"Sample_code\") based on a simple ascending pattern (e.g. SAMP001, SAMP002, SAMP003 etc) from the first row to the last row in the table. But rows with identical Sample_IDs need to have identical Sample_code values as well (so I can't simply generate an ascending set of sample names for the new column). In the example data below, df1 represents my starting data. df2 is what I want to end up with: the Sample_code column values ascend as you go down each row, but with the same value for the rows where Sample_ID is duplicated. I'm quite puzzled where to start so any help would be much appreciated, thank you. EDIT Ideally I would like to have the ascending Sample_code names be in the original order of the rows, as the rows in the starting dataset are ordered by date of collection. I'd like the Sample_code names to be based on the first time a particular sample appears as you go down the rows. A new illustrative df3 has the date column to give a sense of what I mean. The solution suggested below works, but it creates Sample_code names based on the final row in which the repeated Sample_ID values appear, e.g. Sample_ID \"123123\" is labelled \"SAMP006\" (for the final row this value appears), but I'd like this one to be \"SAMP001\" (the first row in which it appears).", "q_apis": "get columns where sample name names sample identical values names sample first last identical identical values sample names values value where duplicated where start any names ordered date names first time sample date mean names values value first", "apis": "lookup lookup apply lookup", "code": ["lookup = {}\nfor i, sample_name in enumerate(df.Sample_ID.unique()):\n    lookup[sample_name] = f'SAMP{i:03}'\n\ndf['Sample_code'] = df.Sample_ID.apply(lambda x: lookup[x])\n"], "link": "https://stackoverflow.com/questions/60877331/how-to-create-column-of-ascending-values-based-on-unique-values-in-another-colum"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's say I have the below: I want to add a column with values if the corresponding number in is odd, otherwise . I would like to do it in this way if possible: I don't know how to check if the value is odd.", "q_apis": "get columns add values value", "apis": "where", "code": ["my_df['b'] = np.where(my_df['A'] % 2 != 0, 'X', 'Y')\n", "value % 2 != 0", "value % 2 == 0", "   A  b\n0  1  X\n1  2  Y\n2  3  X\n"], "link": "https://stackoverflow.com/questions/60178560/add-new-column-based-on-odd-even-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm a beginner in Python. I have two dataframes, each with 5 columns but only the first two columns from each dataframe have matching data. Each dataframe have different number of records. I would like to compare column A from df1 against column A from df2 and if they match, then output column D (ownerEmail) from df2. If columns A don't match, column D should be null. df1 df2 Desired output I have tried something like this but it didn't work. Any help would be much appreciated. Thank you.", "q_apis": "get columns columns first columns compare columns", "apis": "merge astype astype", "code": [">>> pd.merge(df1.astype({'subscriptionId': str}),\n             df2[['subscriptionId', 'ownerEmail']].astype({'subscriptionId': str}),\n             on='subscriptionId', how='left')\n\n   subscriptionId displayName    state authorization  tenantId          ownerEmail\n0           12345     DEV_SPS  Enabled     RoleBased  938c49a8  john.doe@gmail.com\n1           67890  PROD_LINUX  Enabled     RoleBased  0a9cb9ee  alex.bre@gmail.com\n2           11900     TST_WIN  Enabled     RoleBased  e1513511                 NaN\n"], "link": "https://stackoverflow.com/questions/68281776/compare-two-dataframes-and-output-new-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have been strugling with an optimization problem with Pandas. I had developed a script to apply computation on every line of a relatively small DataFrame (~a few 1000s lines, a few dozen columns). I relied heavily on the apply() function which was obviously a poor choice in most cases. After a round of optimization I only have a method which takes time and I haven't found an easy solution for : Basically my dataframe contains a list of video viewing statistics with the number of people who watched the video for every quartile (how many have watched 0%, 25%, 50%, etc) such as : video_name video_length video_0 video_25 video_50 video_75 video_100 video_1 6 1000 500 300 250 5 video_2 30 1000 500 300 250 5 I am trying to interpolate the statistics to be able to answer \"how many people would have watched each quartile of the video if it lasted X seconds\" Right now my function takes the dataframe and a \"new_length\" parameter, and calls apply() on each line. The function which handles each line computes the time marks for each quartile (so 0, 7.5, 15, 22.5 and 30 for the 30s video), and time marks for each quartile given the new length (so to reduce the 30s video to 6s, the new time marks would be 0, 1.5, 3, 4.5 and 6). I build a dataframe containing the time marks as index, and the stats as values in the first column: index (time marks) view_stats 0 1000 7.5 500 15 300 22.5 250 30 5 1.5 NaN 3 NaN 4.5 NaN I then call DataFrame.interpolate(method=\"index\") to fill the NaN values. It works and gives me the result I expect, but it is taking a whopping 11s for a 3k lines dataframe and I believe it has to do with the use of the apply() method combined with the creation of a new dataframe to interpolate the data for each line. Is there an obvious way achieve the same result \"in place\", e.g by avoiding the apply / new dataframe method, directly on the original dataframe ? EDIT: The expected output when calling the function with 6 as the new length parameter would be : video_name video_length video_0 video_25 video_50 video_75 video_100 new_video_0 new_video_25 new_video_50 new_video_75 new_video_100 video_1 6 1000 500 300 250 5 1000 500 300 250 5 video_2 6 1000 500 300 250 5 1000 900 800 700 600 The first line would be untouched because the video is already 6s long. In the second line, the video would be cut from 30s to 6s so the new quartiles would be at 0, 1.5, 3, 4.5, 6s and the stats would be interpolated between 1000 and 500, which were the values at the old 0% and 25% time marks EDIT2: I do not care if I need to add temporary columns, time is an issue, memory is not. As a reference, this is my code :", "q_apis": "get columns apply DataFrame columns apply round time contains interpolate seconds now apply time time length time time index values first index time DataFrame interpolate index values apply interpolate apply length first second cut at between values at time add columns time", "apis": "get values xs values values iloc values DataFrame xs columns columns", "code": ["# get the old x values\nxs = df['video_length'].values[:, None] * [0, 0.25, 0.50, 0.75, 1]\n\n# the corresponding y values\nys = df.iloc[:, 2:].values\n\n# note that 6 is the new value\nnxs = np.repeat(np.array(6), 2)[:, None] * [0, 0.25, 0.50, 0.75, 1]\n\nres = pd.DataFrame(data=np.array([np.interp(nxi, xi, yi) for nxi, xi, yi in zip(nxs, xs, ys)]), columns=\"new_\" + df.columns[2:] )\n\nprint(res)\n", "   new_video_0  new_video_25  new_video_50  new_video_75  new_video_100\n0       1000.0         500.0         300.0         250.0            5.0\n1       1000.0         900.0         800.0         700.0          600.0\n", "output = pd.concat((df, res), axis=1)\nprint(output)\n", "  video_name  video_length  video_0  ...  new_video_50  new_video_75  new_video_100\n0    video_1             6     1000  ...         300.0         250.0            5.0\n1    video_2            30     1000  ...         800.0         700.0          600.0\n\n[2 rows x 12 columns]\n"], "link": "https://stackoverflow.com/questions/65291793/optimizing-interpolation-of-values-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two Dataframes df1: df2: how would I remove the rows that are in df2 so that: output: I have looked at other examples and most of them join based off one column, how do you do this with multiple columns?", "q_apis": "get columns at join columns", "apis": "merge loc", "code": ["out = df1.merge(df2,how='left',indicator=True).loc[lambda x : x['_merge']=='left_only']\nOut[128]: \n   A  B  C  D  E  F  G     _merge\n0  1  2  3  4  5  6  7  left_only\n1  8  9  0  1  2  3  4  left_only\n"], "link": "https://stackoverflow.com/questions/65851458/remove-rows-that-are-in-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "can anyone please help me I have created a model, like so and I can insert data into the database by uploading a file in the admin panel using import-export, like so Screenshot of admin panel Here are the admin.py content How can I import data to the database from a .py file? I have tried this Here are my folder structure Screenshot of folder structure I am new to Django and I don't even know if my approach is possible My goal is to be able to run a script periodically that inserts into the database Any kind of help is greatly appreciated, thank you", "q_apis": "get columns insert", "apis": "iloc iloc iloc iloc iloc iloc iterrows", "code": ["from django.core.management.base import BaseCommand\nfrom tasks.models import TickerOHLC\n\nclass Command(BaseCommand):\n    def handle(self, **options):\n        dataframe = generateDataframe()\n        \n        datas = [\n            TickerOHLC(\n                date = dataframe.iloc[row]['Date'],\n                open = dataframe.iloc[row]['Open'],\n                close = dataframe.iloc[row]['Close'],\n                low = dataframe.iloc[row]['Low'],\n                high = dataframe.iloc[row]['High'],\n                volume = dataframe.iloc[row]['Volume'],\n            )\n            for row in dataframe.iterrows()\n        ]\n        TickerOHLC.objects.bulk_create(datas)\n"], "link": "https://stackoverflow.com/questions/64924326/how-to-insert-data-to-django-database-from-python-file-periodically"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to find the median of a list of timeDelta objects generated from a PANDAS dataframe. I've tried using the statistics library as such: st is what I imported the statistics library as. But I get the error: I tried to make a function to calculate it: ` And I use it like this: And I get this error: Is there an easier way to do this and if not then what I am doing wrong? Sample Data:", "q_apis": "get columns median get get", "apis": "DataFrame mean median median", "code": ["import pandas as pd\n\nDF = pd.DataFrame({'TimeDelta': pd.to_timedelta(['0 days 00:00:36.35700000', \n                                                 '0 days 00:47:11.213000000'])})\n\nDF['TimeDelta'].mean()\n# Timedelta('0 days 00:23:53.785000')\nDF['TimeDelta'].median()\n# Timedelta('0 days 00:23:53.785000')\n", "pd.to_timedelta(['0 days 00:00:36.35700000', '0 days 00:47:11.213000000']).median()\n"], "link": "https://stackoverflow.com/questions/62349385/calculating-the-median-or-mean-of-a-timedelta-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have a dataframe containing a column of probability. Now I create a map function which returns 1 if the probability is >a threshold value other wise returns 0. Now the catch is that I want to specify the threshold by giving it as an argument to function, and then mapping it on the pandas dataframe. Take the code example below: I.e. how do I pass the threshold value inside my map function now?", "q_apis": "get columns map value value map now", "apis": "applymap", "code": ["df2 = df.applymap(lambda x: partition(x, threshold=0.5))\n"], "link": "https://stackoverflow.com/questions/62682024/can-we-apply-pandas-map-where-the-function-mapping-takes-more-than-1-argument"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas data frame, of which the contents of the columns are integers. However, these integers were read from a .txt file and actually represent binary digits. So for example, let's say one of the entries is Decimal = 3 In my table, the \"0011\" is stored as an 'int64' type. I want to convert this into the decimal of the binary that it represents, and I need to do this for all entries within certain columns, how do I go about this?", "q_apis": "get columns columns all columns", "apis": "astype", "code": ["int(11, 2)\nTypeError: int() can't convert non-string with explicit base\n", "df['val'] = df['val'].astype(str).map(lambda x: int(x, 2))\n", "data = { 'val': [11, 101, 1001, 1101]}\ndf = pd.DataFrame(data)\nprint(df)\n                \nOutput:  (before conversion)    \n\n                 val\n            0     11\n            1    101\n            2   1001\n            3   1101\n        \ndf['val'] = df['val'].astype(str).map(lambda x: int(x, 2))\nprint(df)\n        \nOutput:  (after conversion)\n        \n            val\n        0     3\n        1     5\n        2     9\n        3    13\n"], "link": "https://stackoverflow.com/questions/66282293/convert-int-to-binary"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to remove case-sensitive duplicates keeping the first occurrence and maintaining the order of the sentence. This need to be done on each row of a column. Is this possible to be done with python? I've created a function which works and removes duplicates but only by converting in lower and changing the order witch, is not feasible in my case.", "q_apis": "get columns first", "apis": "replace apply join", "code": ["def uniqueList(row):\n    words = row.split(\" \")\n    unique = words[0]\n    for w in words:\n        if w.lower() not in unique.lower():\n            unique = unique + \" \" + w\n    return unique\n\ndata[\"col_sentence\"].str.replace(\"-\", \" \").apply(uniqueList)\n", "def uniqueList_full(row):\n    words = row.split(\" \")\n    unique = [words[0]]\n    for w in words:\n        if w.lower() not in [u.lower() for u in unique]:\n            unique = unique + [w]\n    return \" \".join(unique)\n"], "link": "https://stackoverflow.com/questions/67992953/keep-first-occurrence-while-removing-duplicates-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've been doing this in VBA but would really appreciate advice on how to do it pythonically. I've got an Excel sheet with 8000+ rows and >25 columns. I need to mark certain rows as requiring close review by a human. Basically, for each individual in the 'Name' column, we need to randomly flag 10% of that person's rows as needing review. We don't want to lose/delete/suppress the other rows; they need to remain available--that's why I'm thinking wouldn't work. In the actual data, there will be 20-30 unique Names, each with 300-400 rows. So far I've used pandas to read the data into a dataframe, done some stuff which isn't germane to this question, and written the dataframe back into an Excel file. As part of this, I've also created a 'Random_No.' column in the dataframe, thinking this would be a useful step towards my goal (''based on how I was doing it in VBA)...maybe invoking something like this for each Name. There's probably a million ways to do this, and I've been fiddling with various approaches, but I'd really appreciate some advice on what you think the most efficient way would be. I seem to be creating a lot of 'helper' dataframes and series and ArrayOfObjects...and in general making everything more complicated than it probably needs to be. Is there a simple way to do this within the dataframe as opposed to making a newbie-Python mess? Here's a simplified schema of the data; the 'Needs_review' represents the kind of column I'm trying to create--again, 10% for each Name. As always thanks for any advice/direction.", "q_apis": "get columns columns delete unique step any", "apis": "DataFrame groupby agg index index replace unstack loc groupby count", "code": ["df = pd.DataFrame({\n    'Name': [f\"Name_{i}\" for i in np.random.randint(0,10,10000)],\n    'Col1': np.random.randn(10000)})\n\nneed_review = []\ndf.groupby(['Name']).agg(\n    lambda x: need_review.extend(\n        np.random.choice(\n            x.index, int(0.1*len(x.index)), replace=False).tolist())).unstack()\ndf['Needs_Review'] = False\ndf.loc[need_review, 'Needs_Review'] = True\n\nprint (df.groupby(['Name', 'Needs_Review'])['Needs_Review'].count())\n", "Name    Needs_Review\nName_0  False           871\n        True             96\nName_1  False           925\n        True            102\nName_2  False           895\n        True             99\nName_3  False           890\n        True             98\nName_4  False           842\n        True             93\nName_5  False           932\n        True            103\nName_6  False           911\n        True            101\nName_7  False           932\n        True            103\nName_8  False           909\n        True            101\nName_9  False           898\n        True             99\n"], "link": "https://stackoverflow.com/questions/63070336/randomly-sample-from-dataframe-based-on-condition-without-losing-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Greeting everyone. I have an excel file that I need to clean and fill NaN values according to column data types, like if column data type is object I need to fill \"NULL\" in that column and if data types is integer or float 0 needs to be filled in those columns. So far I have tried 2 method to do the job but no luck, here is the first using bulit method for selecting columns by data types and the output that I get is not an error but a warning and there is no change in data frame as the first one was slice error so I thought doing it one column at a time, here is the code Both of my methods doesn't work. Many thanks for any help or suggestions. Regards -Manish", "q_apis": "get columns values columns first columns get first at time any", "apis": "fillna dtypes replace fillna dtypes replace", "code": ["df.fillna(df.dtypes.replace({'float64': 0.0, 'O': 'NULL'}), inplace=True)\n", "df = pd.DataFrame({\n    'a': [1.0, 2, np.nan, 4],\n    'b': [np.nan, 'hello', np.nan, 'blah'],\n    'c': [1.1, 1.2, 1.3, np.nan]\n})\n", "df.fillna(df.dtypes.replace({'float64': 0.0, 'O': 'NULL'}), downcast='infer', inplace=True)\n", "   a      b    c\n0  1   NULL  1.1\n1  2  hello  1.2\n2  0   NULL  1.3\n3  4   blah  0.0\n"], "link": "https://stackoverflow.com/questions/60706548/how-to-fill-nan-values-according-to-the-data-type-in-pandas-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the dataframe as below and I want to extract the rows for each client where Flag = 1 and the previous row (if it exists) for the same client. For example, row number 1 (I start from 1, not 0), 2,3,6,7,8,10,11,12", "q_apis": "get columns where start", "apis": "groupby shift eq eq groupby shift eq eq", "code": ["df['prev'] = df.groupby('customer_id')['Flag'].shift(-1).eq(1) | df['Flag'].eq(1)\ndf['next'] = df.groupby('customer_id')['Flag'].shift().eq(1) | df['Flag'].eq(1)\nprint (df)\n   report_date  customer_id  Flag   prev   next\n0     01/01/20            1     0   True  False\n1     02/01/20            1     1   True   True\n2     03/01/20            1     1   True   True\n3     04/01/20            1     0  False   True\n4     05/01/20            1     0  False  False\n5     01/01/20            2     0   True  False\n6     02/01/20            2     1   True   True\n7     03/01/20            2     1   True   True\n8     04/01/20            2     0  False   True\n9     01/01/20            3     0   True  False\n10    02/01/20            3     1   True   True\n11    03/01/20            3     1   True   True\n12    04/01/20            3     0  False   True\n13    05/01/20            3     0  False  False\n14    06/01/20            3     0  False  False\n15    07/01/20            3     0  False  False\n"], "link": "https://stackoverflow.com/questions/66242181/select-the-current-and-previous-rows-per-client-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to use Pandas but only for certain rows As an example, I want to do something like this, but my actual issue is a little more complicated: What I want in this example is the value in 'a' divided by the log of the value in 'b' for each row, and for rows where 'b' is 0, I simply want to return 0.", "q_apis": "get columns value value where", "apis": "mask mask loc mask", "code": ["import numpy as np\n\nmask = (z['b'] != 0)\nz_valid = z[mask]\n\nz['c'] = 0\nz.loc[mask, 'c'] = z_valid['a'] / np.log(z_valid['b'])\n"], "link": "https://stackoverflow.com/questions/33769860/pandas-apply-but-only-for-rows-where-a-condition-is-met"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two pandas dataframes. The first dataframe contains the location of different circles over time. Example: df1 = The second dataframe is organised exactly like the first, but contains the locations of squares at different times. Example: df2 = I would like to figure out how to loop through the dataframe df1 to access all the present squares in df2 at each time point, to find out which is closest. Example output: I'm guessing I have to use either iterrows() or itertuples() in a for loop to get at this but I'm not sure, and scipy cdist to get the distance.", "q_apis": "get columns first contains time second first contains at all at time iterrows itertuples get at get", "apis": "DataFrame apply apply DataFrame get apply loc get at DataFrame assign DataFrame take iloc index loc assign sort_values reset_index iloc", "code": ["import pandas as pd\nimport numpy as np\ncircles = pd.DataFrame({'circle': {0: 1.0, 1: 2.0, 2: 3.0, 3: 4.0, 4: 5.0, 5: 6.0},\n         'time': {0: 1.0, 1: 1.0, 2: 1.0, 3: 2.0, 4: 4.0, 5: 5.0},\n         'x': {0: 235, 1: 236, 2: 245, 3: 215, 4: 287, 5: 394},\n         'y': {0: 133, 1: 133, 2: 425, 3: 325, 4: 203, 5: 394}})\ncircles\n-------------------------------\n        circle  time    x   y\n    0   1.0     1.0    235  133\n    1   2.0     1.0    236  133\n    2   3.0     1.0    245  425\n    3   4.0     2.0    215  325\n    4   5.0     4.0    287  203\n    5   6.0     5.0    394  394\n", "squares = pd.DataFrame({'square': {0: 1.0, 1: 1.0, 2: 2.0, 3: 2.0, 4: 3.0, 5: 3.0},\n                        'time': {0: 1.0, 1: 2.0, 2: 3.0, 3: 4.0, 4: 4.0, 5: 5.0},\n                        'x': {0: 243, 1: 293, 2: 189, 3: 189, 4: 176, 5: 374},\n                        'y': {0: 233, 1: 436, 2: 230, 3: 233, 4: 203, 5: 394}})\\\n                        .set_index('time')\n\nsquares\n------------------\n    square  x   y\ntime            \n1.0  1.0    243 233\n2.0  1.0    293 436\n3.0  2.0    189 230\n4.0  2.0    189 233\n4.0  3.0    176 203\n5.0  3.0    374 394\n", "def closest_square(circleseries):\n\"\"\"\nThis function takes a pd.Series which is what pd.DataFrame.apply gives \nthe function that you apply, in this case to the DataFrame circles. So \nthe pd.Series it will get from apply will represent a single circle. \nWe use loc to get the squares at the correct time point which gives us\na DataFrame that is a subset of squares. We then use assign to give\nthat DataFrame an extra column with the distance and the ID of the\ncircle. Finally we sort this whole thing in ascending order by the \ndistance to our circle, take the square with the lowest distance to\nour circle (iloc[0]). We also reset the index because we don't want\nthe time column from squares to be our but a regular column.\n\"\"\"\nreturn squares.loc[[circleseries.time], :]\\\n           .assign(distance = lambda s: np.sqrt( (s.x- circleseries.x)**2 + (s.y- circleseries.y)**2),\n circle=circleseries.circle)\\\n           .sort_values('distance', ascending=True)\\\n           .reset_index()\\\n           .iloc[0]\n", "circles.apply(closest_square, axis=1)\n--------------------------------------------------------\n        time    square  x       y       circle  distance\n   0    1.0     1.0    243.0    233.0   1.0    100.319490\n   1    1.0     1.0    243.0    233.0   2.0    100.244701\n   2    1.0     1.0    243.0    233.0   3.0    192.010416\n   3    2.0     1.0    293.0    436.0   4.0    135.665029\n   4    4.0     2.0    189.0    233.0   5.0    102.489024\n   5    5.0     3.0    374.0    394.0   6.0    20.000000\n", "circles.merge(circles.apply(closest_square, axis=1), on='circle')\n----------------------------------------------------------------------\n    circle  time_x  x_x y_x time_y  square  x_y     y_y     distance\n0   1.0     1.0     235 133 1.0     1.0     243.0   233.0   100.319490\n1   2.0     1.0     236 133 1.0     1.0     243.0   233.0   100.244701\n2   3.0     1.0     245 425 1.0     1.0     243.0   233.0   192.010416\n3   4.0     2.0     215 325 2.0     1.0     293.0   436.0   135.665029\n4   5.0     4.0     287 203 4.0     2.0     189.0   233.0   102.489024\n5   6.0     5.0     394 394 5.0     3.0     374.0   394.0   20.000000\n"], "link": "https://stackoverflow.com/questions/49279387/looping-through-a-pandas-dataframe-to-get-values-from-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In a Pandas.DataFrame, I would like to find the index of the row whose value in a given column is closest to (but below) a specified value. Specifically, say I am given the number 40 and the DataFrame df: I want to find the index of the row such that df[\"x\"] is lower but as close as possible to 40. Here, the answer would be 3 because df[3,'x']=25 is smaller than the given number 40 but closest to it. My dataframe has other columns, but I can assume that the column \"x\" is increasing. For an exact match, I did (correct me if there is a better method): But for the general case, I do not know how to do it in a \"vectorized\" way.", "q_apis": "get columns DataFrame index value value DataFrame index columns", "apis": "loc lt idxmax max where lt DataFrame index max where lt index", "code": ["a = df.loc[df['x'].lt(40), 'x'].idxmax()\nprint (a)\n3\n", "a = np.max(np.where(df['x'].lt(40))[0])\nprint (a)\n3\n", "df = pd.DataFrame({'x':[11,15,17,25,54]}, index=list('abcde'))\n\na = np.max(np.where(df['x'].lt(40))[0])\nprint (a)\n3\n\nprint (df.index[a])\nd\n"], "link": "https://stackoverflow.com/questions/60149936/pandas-dataframe-find-the-index-of-the-row-whose-value-in-a-given-column-is-clo"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to populate a dataframe in pandas. I've tried to create a dictionary and then put that in a dataframe, but it hasn't worked. Here's my current code: And my desired output is something like this: Does anyone have any ideas?", "q_apis": "get columns put any", "apis": "items items DataFrame columns set_index", "code": ["import pandas as pd\n\nholidays_dic = {\n    'Half_Summer17': {'26-05-2017': '01-06-2017'}\n    , 'Summer17': {'21-07-2017': '31-08-2017'}\n    , 'Half_Fall17': {'20-10-2017': '26-10-2017'}\n    , 'Xmas17': {'20-12-2017': '02-01-2018'}\n    , 'Half_Spring18': {'12-02-2018': '16-02-2018'}\n    , 'Easter18': {'30-03-2018': '13-04-2018'}\n    , 'Half_Summer18': {'28-05-2018': '01-06-2018'}\n    , 'Summer18': {'25-07-2018': '04-09-2018'}\n    , 'Half_Fall18': {'22-10-2018': '25-10-2018'}\n    , 'Xmas18': {'20-12-2018': '03-01-2018'}\n}\n\ndata = [[holidays, start, end] for holidays, date_range in holidays_dic.items() for start, end in date_range.items()]\ndf = pd.DataFrame(data=data, columns=['holiday', 'sDate', 'eDate']).set_index(['holiday'])\nprint(df)\n", "                    sDate       eDate\nholiday                              \nHalf_Summer18  28-05-2018  01-06-2018\nEaster18       30-03-2018  13-04-2018\nXmas18         20-12-2018  03-01-2018\nXmas17         20-12-2017  02-01-2018\nHalf_Fall17    20-10-2017  26-10-2017\nHalf_Summer17  26-05-2017  01-06-2017\nSummer18       25-07-2018  04-09-2018\nHalf_Fall18    22-10-2018  25-10-2018\nSummer17       21-07-2017  31-08-2017\nHalf_Spring18  12-02-2018  16-02-2018\n"], "link": "https://stackoverflow.com/questions/53152701/populating-a-dataframe-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset example shown here: I want to get all rows where the product = 'a' and the 'unit' = 'ng/l' and convert the value and the unit to be value/1000 and unit = 'ng/ml' I nearly have it working but I don't know how to get the value and divide by 1000 in the code below In place of 'value'/1000 what do I put? If I just used a constant in the square brackets then it works, but I want to grab the value it already is and divide.", "q_apis": "get columns get all where product value value get value value put value", "apis": "mask product eq eq update loc mask update loc mask", "code": ["mask = df['product'].eq('a') & df['unit'].eq('ng/L') \n\n# update value\ndf.loc[mask, 'value'] /= 1000\n\n# update unit\ndf.loc[mask,'unit']='ng/mL'\n", "  product        unit     value  market_penetration_rate\n0       a       ng/mL   0.00020                     0.82\n1       a        k/uL   1.00000                     0.64\n2       c  x10(3)/mcL  67.00000                    77.50\n3       c  x10(3)/mcL  71.50000                    12.50\n4       d        k/uL  23.20000                    22.50\n5       b        ng/L  71.00000                    88.00\n6       a       ng/mL   0.00044                     0.34\n7       b         sss  59.30000                    98.20\n8       c         sss  12.70000                    87.40\n"], "link": "https://stackoverflow.com/questions/64901689/pandas-change-multiple-columns-based-on-multiple-conditions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How do I drop a limited number of rows? So far my code drops every instance I give. So in the example below, every instance of 'dog' is dropped. However, I would like to drop a specified number of instances, so for example only drop 2 instances of dog, it would also be a benefit if the instances to drop were sampled at random.", "q_apis": "get columns drop drop drop drop at", "apis": "groupby append sample max get replace sort_index sample groupby lt", "code": ["to_drop = {'dog': 2, 'raccoon': 1}\n\nl = []\nfor animal, gp in df.groupby('Animal'):\n    l.append(gp.sample(n=max(0, len(gp)-to_drop.get(animal, 0)), replace=False))\n\npd.concat(l).sort_index()\n", "   Number   Color   Animal\n1      20   white      cat\n3      10   green    gecko\n4      40   white     bear\n5      50  orange  raccoon\n7      60   black     goat\n8      70    blue     goat\n9      20     red      dog\n", "to_drop = {'dog': 2, 'raccoon': 1}\n\nm = (df.sample(frac=1).groupby('Animal').cumcount()\n       .lt(df['Animal'].map(to_drop)))\ndf = df[~m]\n"], "link": "https://stackoverflow.com/questions/66956423/drop-specified-rows-from-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So i've this sample dataframe: Now i want to apply these function and to column to get new columns. Like the function is using column & the range as the input to create column , similarly column & the range are used for the column : I can create each column one by one, by using these one liners, but i want to apply this to both column columns in one go with a one liner. The resultant dataframe should look like this: I want to create these columns in one go, instead of creating one column ata time. How can i do this? Any suggestions? or something like this could work?", "q_apis": "get columns sample apply get columns apply columns columns time", "apis": "replace mean replace mean add apply", "code": ["range_dict = {}\nrange_dict['x_range'] = x_range\nrange_dict['y_range'] = y_range\n", "def min_max_calculator(df, range_dictionary, mean_columns_list):\n    for i in range(len(mean_cols_list)):\n        # this returns 'x_mean'\n        current_column = mean_cols_list[i]\n        # this returns 'x_min_max_value'\n        output_col_name = current_column.replace('mean','min_max_value')\n        # this returns 'x_range'\n        range_name = current_column.replace('mean','range')\n        # this returns the list of ranges for x_range\n        range_list = range_dict[range_name]\n        # This add the calculated column to the dataframe\n        df[output_col_name] = df[current_column].apply(lambda x: min_max_range(x,range_list))\n    return(df)\n\ndf_output = min_max_calculator(df, range_dict, mean_cols_list)\n"], "link": "https://stackoverflow.com/questions/59699910/how-to-apply-the-same-function-with-different-input-arguments-to-create-new-colu"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here is a snippet of data-frame which looks like this (original data frame contains 8k rows): I'm trying to find out how long each unique User spends in an 'Active' state till they change into a different state other than 'Active'. There is an 'endstate' column which contains an 'Active' value, So I wanted to calculate the total time difference from when the 'State' column starts as 'Active' until the 'endstate' column contains 'Active' Originally, I used the following code: The returned results are: For User 100234, 19 days and 00:40:35 is calculated for Line 2 and 3 however it should be 19 days and 00:41:36 (using Line 4) as it takes the User 1 minute and 1 second to transition from 'Active' to 'Live'. I was hoping to use the 'endstate' column in this code so that the time duration of the User being 'Active' is run using the 'State' column until the next line of code has 'Active' as the value in 'end_state' and a different value other than 'Active' for 'State'. Here is an example of how i'm hoping to calculate the time duration: Is there a way to do this? Here is how i'm trying to calculate the duration:", "q_apis": "get columns contains unique contains value time difference contains days days minute second time value value time", "apis": "eq eq groupby cumsum agg droplevel eq eq groupby first groupby first sub", "code": ["m = df['State'].eq('Active') | df['endstate'].eq('Active')\ns = df[m].groupby(['User', (~m).cumsum()])['change_datetime'].agg(np.ptp).droplevel(1)\n", "m1 = df['State'].eq('Active')\nm2 = ~m1 & df['endstate'].eq('Active')\n\ns1 = df[m1].groupby('User')['change_datetime'].first()\ns2 = df[m2].groupby('User')['change_datetime'].first()\n\ns = s2.sub(s1)\n", "print(s)\nUser\n100234   19 days 00:41:36\n213421   30 days 19:37:00\nName: change_datetime, dtype: timedelta64[ns]\n"], "link": "https://stackoverflow.com/questions/63559473/how-to-calculate-total-length-of-process-until-it-changes-using-two-columns-in-p"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Stata files include labels/descriptions for each column, which can be viewed in Stata using the command. For example, the and variables in this online dataset, have descriptions and , respectively: Those descriptions do not show up in Pandas, for example with : Is there a way to view this information after loading it to a Pandas DataFrame using ?", "q_apis": "get columns view DataFrame", "apis": "describe size", "code": ["sysuse auto, clear\n\ndescribe\n\nContains data from auto.dta\n  obs:            74                          1978 Automobile Data\n vars:            12                          13 Apr 2014 17:45\n size:         3,182                          (_dta has notes)\n-------------------------------------------------------------------------------------------------------------------------------------\n              storage   display    value\nvariable name   type    format     label      variable label\n-------------------------------------------------------------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and Model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair Record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn Circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear Ratio\nforeign         byte    %8.0g      origin     Car type\n-------------------------------------------------------------------------------------------------------------------------------------\nSorted by: foreign\n", "import pandas as pd\ndata = pd.read_stata('auto.dta', iterator = True)\nlabels = data.variable_labels()\nlabels\n\nOut[5]: \n{'make': 'Make and Model',\n 'price': 'Price',\n 'mpg': 'Mileage (mpg)',\n 'rep78': 'Repair Record 1978',\n 'headroom': 'Headroom (in.)',\n 'trunk': 'Trunk space (cu. ft.)',\n 'weight': 'Weight (lbs.)',\n 'length': 'Length (in.)',\n 'turn': 'Turn Circle (ft.) ',\n 'displacement': 'Displacement (cu. in.)',\n 'gear_ratio': 'Gear Ratio',\n 'foreign': 'Car type'}\n"], "link": "https://stackoverflow.com/questions/60498830/view-stata-variable-labels-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i have two dataframes; df and df2. I need to place them into an excel, with df being in one sheet and df2 being in another sheet. What would be the easiest way to do this in python?", "q_apis": "get columns", "apis": "to_excel to_excel", "code": ["with pd.ExcelWriter('output.xlsx') as writer:  \n    df.to_excel(writer, sheet_name='Sheet_name_1')\n    df2.to_excel(writer, sheet_name='Sheet_name_2')\n"], "link": "https://stackoverflow.com/questions/63298842/placing-dataframes-into-excel-sheets"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How I can count if a country that is in more rows , has failed or passed, enter image description here Like is and the result should be like this enter image description here Because Netherlands is in 4 rows , and has 3 passed and one failed.", "q_apis": "get columns count", "apis": "assign explode", "code": ["df1 = df.assign(Countries = df.Countries.str.split(', ')).explode('Countries')\ndf2 = pd.crosstab(df1['Countries'],df1['Test'])\n"], "link": "https://stackoverflow.com/questions/68014379/python-dataframe-in-one-column-strings-delimit-by-comma-and-in-another-column-i"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to define a function which returns an index into a DataFrame. For example, I have I can then slice into the DataFrame via or which will return the values in matching the conditions on the index and . In the first case, I would get one value, in the second case two. Instead of explicitly writing the index each time, I would like to define a function which generates this automatically. My pseudo function (which does not work) could be, With this, I would like to be able to do something like and to get the same as if directly calling or respectively. As I have multiple levels in my index, it would be very useful to have a way of defining the operator if the argument passed to the function is . Update: I have quite a few levels in the index, i.e. that I would like to avoid writing an statement for every possible combination of 's which I could have. For example, assume I have four levels in the index. It would be great to do something like, instead of writing an if statement for every combination of in the arguments (7 in this case).", "q_apis": "get columns index DataFrame DataFrame values index first get value second index time get levels index levels index levels index", "apis": "loc", "code": ["def func(df, a, b=None):\n    if b is None:\n        b = slice(None)\n    return df.loc[a, b]\n"], "link": "https://stackoverflow.com/questions/49574080/using-colon-operator-to-create-dataframe-slices-via-a-function-call"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am importing data using sklearn: sklearn on the fly converts categorical data into numbers. Now I want to convert this dataset into Pandas DataFrame: but this command drops all categorical data - they are numbers now. What I want is dataframe which will have after conversion original text labels in place of numbers. So, after conversion from sklearn dataframe into pandas dataframe data should look identically as if I simply downloaded this data with command: Is it possible?", "q_apis": "get columns DataFrame all now", "apis": "values", "code": ["[...]\ndata : np.array or scipy.sparse.csr_matrix of floats\n    The feature matrix. Categorical features are encoded as ordinals.\n[...]\ncategories : dict\n    Maps each categorical feature name to a list of values, such that the value\n    encoded as i is ith in the list.\n[...]\n", "from sklearn import datasets\nimport pandas as pd\nimport numpy as np\n\ndef main():\n    dataset = datasets.fetch_openml('credit-g', version = 'active')\n\n    raws = [\n        np.take(dataset['categories'][feature], dataset['data'][:,i].astype(int))  # Take string value for categorical features\n        if feature in dataset['categories'] else dataset['data'][:,i]  # Else use the floats as is\n        for i, feature in enumerate(dataset['feature_names'])\n    ]\n\n    data = pd.DataFrame(np.stack(raws, axis=1), columns=dataset.feature_names)\n    data['class'] = pd.Series(dataset.target)\n    print(\"Initial dtypes:\")\n    print(data.dtypes)\n\n    dtypes = {\n        f: 'category' if f in dataset['categories'] else 'float'\n        for f in dataset['feature_names']\n    }\n    dtypes['class'] = 'category'\n    data = data.astype(dtypes)\n    print(\"\\nFirst cast:\")\n    print(data.dtypes)\n\n    int_cols = [1, 4, 12]\n    data.iloc[:, int_cols] = data.iloc[:, int_cols].astype('int64')\n    print(\"\\nInt cast:\")\n    print(data.dtypes)\n\nif __name__ == '__main__':\n    main()\n"], "link": "https://stackoverflow.com/questions/58916408/transform-sklearn-dataframe-into-pandas-dataframe-preserving-categorical-labels"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I'm having the following problem: I have a dataframe like the one bellow where is the time difference between each row and the row above in minutes. So, for example, I had 20 minutes after . First I have to check if the time difference between two rows is < 60 (one hour) and create a column using the formula . My lambda is a constant with the value of 0.97. And then, if the time difference between each row and 2 rows above is still inferior to 60, I have to re-do the same thing comparing each row with 2 rows above. And then I have to do the same thing comparing 3 rows above and etc. To do that I wrote the following code: My question is: since I have to re-do this at least 10 times (even more) with the real values I have, is there a way to create the \"rem columns\" dynamically? Thanks in advance!", "q_apis": "get columns where time difference between time difference between hour value time difference between at values columns", "apis": "mask loc mask shift mask mask shift loc mask shift mask", "code": ["n = 3\nfor i in range(1, n):\n    if (i==1):\n        mask = df['time_diff_float']\n        df.loc[mask, 'rem_' +str(i)] = df['value'].shift() * (lambda_ ** (mask - 1))\n\n    else:\n        mask += df['time_diff_float'].shift(i-1)\n        df.loc[mask < 60, 'rem_'+str(i)] = df['value'].shift(i) * (lambda_ ** (mask - 1))\n"], "link": "https://stackoverflow.com/questions/61068685/how-to-dynamically-create-columns-based-on-multiple-conditions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am working with this csv https://drive.google.com/file/d/1o3Nna6CTdCRvRhszA01xB9chawhngGV7/view?usp=sharing I am trying to sort by the 'Taxes' column, but when I use I get This is baffling to me and I cannot find a similar problem online. How can I sort the data by the \"Taxes\" column? EDIT: I should explain that my real problem is that when I use df.sort_values('Taxes') I get this output: Therefore, I assume the commas are getting in the way of my chart sorting properly. How do I get over this?", "q_apis": "get columns view get sort_values get get", "apis": "DataFrame", "code": ["import pandas as pd\ndf = pd.DataFrame({\"Taxes\": [\"1,000\", \"100\", \"100,000\"]})\n", ">>> df.sort_values(by=\"Taxes\")\n     Taxes\n0    1,000\n1      100\n2  100,000\n", ">>> df['Taxes'] = df['Taxes'].str.replace(\",\", \"\").astype(int)\n\n>>> df.sort_values(by=\"Taxes\")\n    Taxes\n1     100\n0    1000\n2  100000\n"], "link": "https://stackoverflow.com/questions/64948865/cant-sort-dataframe-column-numpy-ndarray-object-has-no-attribute-sort-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the table like this: one two three four 20 15 10 5 20 15 10 5 20 15 10 5 20 15 10 5 I want to move every rows values left according their rows index. Row values with index 0 stays the same, Row values with index 1 moves left in one point, Row values with index 2 moves left in two points, etc... Desired table should looks like this: one two three four 20 15 10 5 15 10 5 0 10 5 0 0 5 0 0 0 Thanks for helping me!", "q_apis": "get columns values left index values index values index left values index left", "apis": "iloc iloc shift fillna", "code": ["for i in range(len(df)):\n    df.iloc[i,:] = df.iloc[i,:].shift(-i)\n\ndf.fillna(0, inplace=True)\n", ">>> df\n   one   two  three  four\n0   20  15.0   10.0   5.0\n1   15  10.0    5.0   0.0\n2   10   5.0    0.0   0.0\n3    5   0.0    0.0   0.0\n"], "link": "https://stackoverflow.com/questions/67859331/python-dataframe-move-rows-values-left-according-index-of-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This is my DataFrame: I'm trying to print matching values but i get this error. Explain the output.", "q_apis": "get columns DataFrame values get", "apis": "where", "code": ["data.where(data[\"Gender\"] == \"Male\" and data[\"Age\"] == 19)\n"], "link": "https://stackoverflow.com/questions/60457276/why-do-i-get-this-error-when-i-try-to-perform-some-logical-operation-on-datafram"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In the following code: ...want to assign a new column called which is based on its respective dictionary. Missing values will be NaN. The code: gives Where am I going wrong?", "q_apis": "get columns assign values", "apis": "groupby apply", "code": ["mapper = {'A': dictforA, \n          'B': dictforB}\n\ndf['Value'] = df.groupby('Group').Key.apply(lambda s: s.map(mapper[s.name]))\n", ">>> print(df.head(10))\n\n  Group  Key  Value\n0     B    3    0.9\n1     A    2    0.1\n2     B    3    0.9\n3     A    3    0.8\n4     A    3    0.8\n5     A    2    0.1\n6     B    2    NaN\n7     B    2    NaN\n8     A    4    0.2\n9     A    2    0.1\n"], "link": "https://stackoverflow.com/questions/58271479/running-through-groupby-with-different-keyvalue-dictionaries"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the follow dataframe: I'm trying to return an output that would tell me all of the tasks & IDs where 'Coolant' is in the 'Type'. But, only the tasks & IDs where coolant isn't mixed with any other type. I think my expected output would be a dataframe like this: Could somebody have a go at it? Also, could they refer me to the correct reading material in oprder to help me approach a similar problem in the future? Thank you,", "q_apis": "get columns all where where any at", "apis": "apply", "code": ["coolant_only = df['Type'].apply(lambda x: set(x)=={'Coolant'})\n\ndf[coolant_only]\n"], "link": "https://stackoverflow.com/questions/63940523/is-there-a-function-to-search-a-value-in-a-pandas-cell-list-and-return-the-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe and its rows are basically logs of record temperatures from a given station at a given time (each day, each station has one record high and one record low). The column tells me if it's a record high or a record low. It looks like this: I would like to group these temperatures by date, and for each date, extract the maximal record high temperature and minimal record low one and their associated stations where the observation has been done. An expected result would be something of the likes of: (Note that I don't care that much if there are several stations ID which have a same record high or record low. Having the ID of just one of these stations is good enough) I have tried doing this by creating a custom function that is applied on the grouped dataframe (by date) and iterates over the groups of \"sub\"-dataframes constructed from a second groupby on to know if it should use min or max. This also returns the index of the passed by dataframe so that I can go back into it and look for the ID of the maximal or minimal entry. Truth is, this works. But it is quite long. And, even though I've just started to use Python a month ago, I somehow feel like this is just a very weird way to do it... So I was wondering if there is a faster/clever way to do that.", "q_apis": "get columns at time day date date where date groups sub second groupby min max index month", "apis": "eq set_index groupby T agg idxmax T max set_index groupby T agg idxmin T min reset_index drop sort_values T drop_duplicates set_index add_suffix drop sort_values T drop_duplicates set_index add_suffix reset_index", "code": ["m = df['MinMax'].eq('MAX')\ndf1 = (df[m].set_index('ID')\n            .groupby('Date')['T']\n            .agg([('ID MAX', 'idxmax'),('T MAX','max')]))\ndf2 = (df[~m].set_index('ID')\n             .groupby('Date')['T']\n             .agg([('ID MIN', 'idxmin'),('T MIN','min')]))\n\ndf_min_max = pd.concat([df1, df2], axis=1).reset_index()\n", "df1 = (df[m].drop('MinMax', axis=1)\n            .sort_values(['Date', 'T'], ascending=[True, False])\n            .drop_duplicates('Date')\n            .set_index('Date')\n            .add_suffix(' MAX'))\ndf2 = (df[~m].drop('MinMax', axis=1)\n             .sort_values(['Date', 'T'])\n             .drop_duplicates('Date')\n             .set_index('Date')\n             .add_suffix(' MIN'))\n\ndf_min_max = pd.concat([df1, df2], axis=1).reset_index()\n"], "link": "https://stackoverflow.com/questions/65683244/applying-a-function-on-a-groupby-table-that-depends-on-the-value-of-another-colu"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My df: Output: now i would like to set to the lowest value of of the group() whenever the first entry of this at the position is <0. Expected Output:", "q_apis": "get columns now value first at", "apis": "lt groupby transform first loc groupby transform min", "code": ["m = df['numb_diff'].lt(0).groupby(df['id']).transform('first')\ndf.loc[m, 'time1'] = df.groupby('id')['time'].transform('min')\n", "   id                time               time1  numb  numb_diff\n0   1 2020-01-01 12:00:15 2020-01-01 12:00:00     1        NaN\n1   1 2020-01-01 12:00:30 2020-01-01 12:00:00     5        4.0\n2   1 2020-01-01 12:00:45 2020-01-01 12:00:00     8        3.0\n3   2 2020-01-03 08:00:00 2020-01-03 08:00:00     0       -8.0\n4   2 2020-01-03 08:00:15 2020-01-03 08:00:00     4        4.0\n"], "link": "https://stackoverflow.com/questions/64304073/pandas-changing-values-of-a-group-under-a-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm using Pandas to populate 6 new variables with values that are conditional to other data variables. The entire dataset consists of about 700,000 rows and 14 variables (columns) including my newly added ones. My first approach was to use , mainly down to experience being minimal here. This clocked around 9600 seconds. I've managed to get this more efficient (~3500 seconds) by using apply(). Here is an example of one of the new variables. Each new variable follows the same pattern and needs to do the following: Here is a small example of the output data: In case it's relevant, here is my method for creating a dataframe from the stata file in the first line: Here is an example of the first 50 rows of data", "q_apis": "get columns values columns first seconds get seconds apply first first", "apis": "loc index apply groupby transform count", "code": ["def hh_n(row):\n    # BOOLEAN INDEXING + LEN()\n    df = housing_df.loc[\n                (housing_df[\"pidp\"] == row[\"pidp\"]) & (housing_df[\"wave\"] == row[\"wave\"])\n            ]\n    return str(len(df.index) + 1)\n\nhousing_df[\"hh_n\"] = housing_df.apply(hh_n, axis=1)\n", "housing_df[\"hh_n\"] = housing_df.groupby(['pidp', 'wave']).transform('count') + 1\n"], "link": "https://stackoverflow.com/questions/56823357/what-is-a-more-efficent-way-to-populate-new-variables-based-on-other-variable-re"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have multiple list of different lengths such as the following: List 1 List 2 I want to store all the entries in each of these list as a single entry into a columns row as these list represent a category and all the elements in the list are grouped under a specific category. Like this: I have tried but this treats each item in the list as an entry for a row.", "q_apis": "get columns all columns all item", "apis": "empty columns DataFrame columns columns loc loc", "code": ["#Create an empty dataframe with the columns you want\ndf = pd.DataFrame(columns = [\"Description\",\"Organizations\"])\n\n#Fill the columns\ndf.loc[0,\"Description\"] = list1\ndf.loc[0,\"Organizations\"] = list2\n", "Description                                         | Organizations\n____________________________________________________|_______________\n[In addition to new financing, a coalition of ...   |[African Union Development Agency (AUDA-NEPAD)...\n"], "link": "https://stackoverflow.com/questions/64557951/how-to-store-an-entire-list-in-a-single-row-of-a-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here is my dataset: And want to convert my dataset between two dates. I have tried the solution mentioned here. But it didn't work. Here is a screenshot using: Boolean Mask and DatetimeIndex", "q_apis": "get columns between DatetimeIndex", "apis": "sort_values", "code": ["data[(data['Date'] >= '2020-03-25') & (data['Date'] <= '2020-05-22')].sort_values('Date')"], "link": "https://stackoverflow.com/questions/62221921/select-dataset-rows-between-two-dates"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm scraping a website. I want all the data I've collected to be used to create a dataframe . Not knowing the best method to do so, I've used to fill in the values to an empty list. Then I use the same list to create a dataframe. The Problem Some values I scrape are empty, but doesn't take them into the list. Thus lists are not equal in length to create a df from. I need to know which parts are empty because everything goes straight to the g sheets. In the past, I've used , but that just puts non-empty values first and fills up empty spots with Nan values. But that doesn't solve the problem, because it messes up the data table. Here's the code:", "q_apis": "get columns all values empty values empty take length empty empty values first empty values", "apis": "get div append append append append append append append append append DataFrame", "code": ["i = 0\nwhile i >= 0:\n    url = 'https://www.website.com/page=' + \\\n        str(i)\n    page = requests.get(url)\n    soup = BeautifulSoup(page.content, 'html.parser')\n    if page.status_code == 200:\n        job_elems = soup.find_all(\n            'div', class_='cvo_module_offer_box offer_content')\n\n        for job_elem in job_elems:\n            try:\n                title = job_elem.h2.text\n                job_title.append(title)\n\n                link = job_elem.h2.find('a')['href']\n                link = link[2:]\n                job_link.append(link)\n\n                employer = job_elem.find(\n                    'ul', class_='cvo_module_offer_meta').findChild().text\n                job_employer.append(employer)\n\n                salary = job_elem.find('span', class_='salary-blue').text\n                job_salary.append(salary)\n\n                posted = job_elem.find(\n                    'ul', class_='cvo_module_offer_meta offer_dates').findChild().text\n                job_posted.append(posted)\n\n                deadline = job_elem.find(\n                    'ul', class_='cvo_module_offer_meta offer_dates').contents[3].text\n                job_deadline.append(deadline)\n            except Exception as e:\n                salary = None\n                job_salary.append(salary)\n                posted = None\n                job_posted.append(posted)\n                deadline = None\n                job_deadline.append(deadline)\n            # print(title)\n            # print(link)\n            # print(employer)\n            # print(salary)\n            # print(posted)\n            # print(deadline)\n            # print()\n        i += 1\n    else:\n        break\n\ndf = pandas.DataFrame(\n    {\n        'Job Title': pandas.Series(job_title),\n        'Link': pandas.Series(job_link),\n        'Employer': pandas.Series(job_employer),\n        'Salary': pandas.Series(job_salary),\n        'When Posted': pandas.Series(job_posted),\n        'Deadline': pandas.Series(job_deadline)\n    })\n"], "link": "https://stackoverflow.com/questions/59791891/how-to-include-empty-values-to-list-that-can-be-used-not-losing-its-length-fo"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm aiming to places to people in a . Specifically, using the below I determine how many places are currently on. I want to use these values and them in groups of 3. For instance, total places occurring that are less than 3 should be assigned to . Places that are 3-6 should be assigned to etc. Note: The total number of places occurring at once can go up to 20 so the number of assigned groups need to accomodate for this. Here is my attempt. Output: Intended Output:", "q_apis": "get columns values groups at groups", "apis": "DataFrame groupby iloc append add drop groupby apply shape", "code": ["import pandas as pd\nimport numpy as np\n\nd = ({\n    'Time' : ['8:03:00','8:07:00','8:10:00','8:23:00','8:27:00','8:30:00','8:37:00','8:40:00','8:48:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 4','House 5','House 1','House 2','House 3','House 4'],                                     \n     })\n\ndf = pd.DataFrame(data=d)\n\ndf['u'] = df[::-1].groupby('Place').Place.cumcount()\nids = [1]\nseen = set([df.iloc[0].Place])\ndec = False\nfor val, u in zip(df.Place[1:], df.u[1:]):\n    ids.append(ids[-1] + (val not in seen) - dec)\n    seen.add(val)\n    dec = u == 0\ndf['Places On'] = ids\n\ndf = df.drop(df[['u']], axis=1)\n\ndef g(gps):\n        s = gps['Place'].unique()\n        d = dict(zip(s, np.arange(len(s)) // 3 + 1))\n        gps['P'] = gps['Place'].map(d)\n        return gps\n\ndf = df.groupby('Place', sort=False).apply(g)\n\nfor i in range(df.shape[0]):\n    if(df['Places On'][i]<=3):\n        df['P'][i]=1\n    else:\n        df['P'][i]=2\nprint(df)\n"], "link": "https://stackoverflow.com/questions/54797112/assign-column-values-in-groups-of-3-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the dataframe: So, i try to run but i get an error 'bool' object has no attribute 'shift' How do i can to run it without getting an error? Can anyone see the problem Output dataframe", "q_apis": "get columns get bool shift", "apis": "groupby agg eq shift reset_index apply agg", "code": ["out=(df.groupby('in_id')['name']\n   .agg(lambda x: x[x.eq('four').shift(-1,fill_value=False)])\n   .reset_index())\n#you can also use apply() in place of agg() method\n"], "link": "https://stackoverflow.com/questions/68209684/error-bool-has-no-attribute-shift-shift-doesnt-work"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to rotate my pandas dataframe so that my data that looks like... to look like... I've been looking at pivoting the data frame, but I can't figure out what the \"index\" would be. When I tried to add an index value to each row I tried this pivot: but it gave me an error saying . I'm clearly out of my depth here. Can anyone point me in the right direction?", "q_apis": "get columns at index add index value pivot right", "apis": "set_index T", "code": ["res = df.set_index('date').T\n\nprint(res)\n\ndate    '2017-04-01'  '2017-05-01'\ndata_1           100           500\ndata_2           200           222\ndata_3           300           333\ndata_4           400           444\n"], "link": "https://stackoverflow.com/questions/50918180/how-do-i-make-pandas-treat-each-row-as-a-column-based-on-a-certain-date-variable"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that contains the columns, \"quarter\" and \"resale-price\". I used the dataframe to plot a boxplot, using seaborn. The boxplot displays the quarterly value such as (2007-Q2, 2007-Q3, 2007-Q4, 2008-Q2). However, I want it to display the yearly value such as (2007, 2008, 2009). How can I achieve that?", "q_apis": "get columns contains columns quarter plot boxplot boxplot value value", "apis": "boxplot", "code": ["for item in ax.get_xticklabels():\n   item.set_rotation(90)\n", "df['year'] = df['quarter'].str[:4]\nax = sns.boxplot(data = df, x='year', y='resale_price')\n"], "link": "https://stackoverflow.com/questions/54605768/boxplot-how-to-display-yearly-value-instead-of-quarterly-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Is there a difference in relation to time execution between this two commands : Thank you for your help", "q_apis": "get columns difference time between", "apis": "columns columns columns columns", "code": ["In [398]: pd.read_sql_query??\nSignature: pd.read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None)\nSource:\ndef read_sql_query(sql, con, index_col=None, coerce_float=True, params=None,\n                   parse_dates=None, chunksize=None):\n    pandas_sql = pandasSQL_builder(con)\n    return pandas_sql.read_query(\n        sql, index_col=index_col, params=params, coerce_float=coerce_float,\n        parse_dates=parse_dates, chunksize=chunksize)\n", "In [399]: pd.read_sql_table??\nSignature: pd.read_sql_table(table_name, con, schema=None, index_col=None, coerce_float=True, parse_dates=None, columns=None, chunksize=None\n)\nSource:\ndef read_sql_table(table_name, con, schema=None, index_col=None,\n                   coerce_float=True, parse_dates=None, columns=None,\n                   chunksize=None):\n    con = _engine_builder(con)\n    if not _is_sqlalchemy_connectable(con):\n        raise NotImplementedError(\"read_sql_table only supported for \"\n                                  \"SQLAlchemy connectable.\")\n    import sqlalchemy\n    from sqlalchemy.schema import MetaData\n    meta = MetaData(con, schema=schema)\n    try:\n        meta.reflect(only=[table_name], views=True)\n    except sqlalchemy.exc.InvalidRequestError:\n        raise ValueError(\"Table %s not found\" % table_name)\n\n    pandas_sql = SQLDatabase(con, meta=meta)\n    table = pandas_sql.read_table(\n        table_name, index_col=index_col, coerce_float=coerce_float,\n        parse_dates=parse_dates, columns=columns, chunksize=chunksize)\n\n    if table is not None:\n        return table\n    else:\n        raise ValueError(\"Table %s not found\" % table_name, con)\n"], "link": "https://stackoverflow.com/questions/48171611/difference-between-pandas-read-sql-query-and-read-sql-table"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to set up my code to create one dataframe from each url in a list and then combine these dataframes into a single dataframe. I am very close to being done; however, as it is right now, my code shows this error message I need help getting the code to create a df from each url and then combine them and write to csv. Here is the complete error message", "q_apis": "get columns combine right now combine", "apis": "append", "code": [" df = tables[0]\n appended_data.append(df)\n driver.close()\n"], "link": "https://stackoverflow.com/questions/55979113/how-to-create-a-data-frame-from-each-url-and-then-merge-them"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have really strange situation. I try to merge two DataFrames using one common column like below: Nevertheless, no columns from df2 join df1 even though they share customer_id. In both DataFrames customer_id is same type \"int\". How it is possible? What can I do ?", "q_apis": "get columns merge columns join", "apis": "DataFrame DataFrame merge merge merge", "code": ["df1 = pd.DataFrame({'customer_id':[1,1,1,2,2,2], 'data1':['a','b','c','d','e','f']})\ndf2 = pd.DataFrame({'customer_id':[2,5,3,4,3,1], 'data2':['aa','bb','cc','dd','ee','ff']})\n", "df1.merge(df2, on = \"customer_id\", how = \"left\") \n", "pd.merge(df1, df2, how='left', on= 'customer_id')\nof \nout = pd.merge(df1, df2, how='left', left_on= 'customer_id', right_on = 'customer_id')\n"], "link": "https://stackoverflow.com/questions/68032061/can-not-merge-df2-to-df1-despite-shared-values-in-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to do a calculation over each row from csv-file in Python. This is the loaddataa File: These were my first steps: In the for - loop I don't know how to go on. The csv - file consists of 9 columns and 150 row. For the calculation I just need column 2, 4 and 5. The calculation with the formula in shall be executed over each row, so that in the end I have 150 values. I hope it's clear what I want to do. Thanks for helping me.", "q_apis": "get columns first columns values", "apis": "append", "code": ["resultsHorizontal = list()\n\nfor i in range (len(strideData)):\n    strideData_yAngle = strideData.to_numpy()[i, 2]\n    strideData_xAcceleration = strideData.to_numpy()[i, 4]\n    strideData_yAcceleration = strideData.to_numpy()[i, 5]\n    resultsHorizontal.append(horizontal(strideData_yAngle, strideData_yAcceleration, strideData_xAcceleration))\n"], "link": "https://stackoverflow.com/questions/62156779/calculation-over-each-row-from-a-csv-file-with-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe where I am trying to highlight duplicates using: I want users of this script to input the column_subset using: I want users to enter the column names as follows: Is there any way to do this?", "q_apis": "get columns where names any", "apis": "append columns", "code": ["column_subset = []\n\n# number of elements as input \nn = int(input(\"Enter number of elements : \")) \n\n# iterating till the range \nfor i in range(0, n): \n    ele = int(input()) \n    column_subset.append(ele) # adding the element \n", "column_subset = input(\"Enter list of columns for group by:\")\ncolumn_subset = column_subset.split(',')\n"], "link": "https://stackoverflow.com/questions/61254469/is-there-a-way-to-input-column-names-without-quotes-and-use-that-as-a-subset-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Is it possible to covert a df into a matrix like the following? Given : The matrix would be: Here's the logic behind it: Let me know if this makes sense. I know I have to use sort, group, and count but couldn't figure out how to set up the matrix. Thank you!!!", "q_apis": "get columns count", "apis": "max values bool", "code": ["i = np.arange(1, df.Value.max() + 1)\nj = df.Value.values[:, None] >= i\n\ndf = pd.DataFrame(j, columns=i, index=df.Name).sum(level=0)\n\n        1    2    3    4    5\nName                         \nx     4.0  4.0  3.0  1.0  1.0\ny     2.0  2.0  1.0  0.0  0.0\nz     1.0  1.0  1.0  1.0  0.0\n", "i = np.arange(1, df.Value.max() + 1)\ni\narray([1, 2, 3, 4, 5])\n", "j = df.Value.values[:, None] >= i\nj\n\narray([[ True,  True,  True,  True,  True],\n       [ True,  True, False, False, False],\n       [ True,  True,  True, False, False],\n       [ True,  True,  True, False, False],\n       [ True,  True,  True, False, False],\n       [ True,  True, False, False, False],\n       [ True,  True,  True,  True, False]], dtype=bool)\n", "k = pd.DataFrame(j, columns=i, index=df.Name)\nk\n         1     2      3      4      5\nName                                 \nx     True  True   True   True   True\nx     True  True  False  False  False\nx     True  True   True  False  False\nx     True  True   True  False  False\ny     True  True   True  False  False\ny     True  True  False  False  False\nz     True  True   True   True  False\n", "k.sum(level=0)\n\n        1    2    3    4    5\nName                         \nx     4.0  4.0  3.0  1.0  1.0\ny     2.0  2.0  1.0  0.0  0.0\nz     1.0  1.0  1.0  1.0  0.0\n", "k.sum(level=0).astype(int)\n\n      1  2  3  4  5\nName               \nx     4  4  3  1  1\ny     2  2  1  0  0\nz     1  1  1  1  0\n"], "link": "https://stackoverflow.com/questions/47897491/pandas-converting-df-into-matrix-by-conditions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I faced with an issue to merge two DF into one and save all duplicate rows by value from the second DF. Example: I'm. expecting the output to be: As you can see we added a new column and all rows of df1 a null there now and row with is replaced with all values from (amount of updated columns can be different, so it's not about updating specific columns values, but about replacing the whole row by ) I dont care about indexes and sort Looking for efficient solution as I have huge amount of files, which I should merge this way into main DF", "q_apis": "get columns merge all value second all now all values columns columns values merge", "apis": "drop_duplicates last", "code": ["df = pd.concat([df1, df2]).drop_duplicates('id', keep='last')\n"], "link": "https://stackoverflow.com/questions/65806053/pandas-merge-two-df-with-rows-replacement"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Im trying to create function which will create a new column in a pandas dataframe, where it figures out which substring is in a column of strings and takes the substring and uses that for the new column. The problem being that the text to find does not appear at the same location in variable which of is in a given row I've made a function that works, but is terribly slow for large datasets", "q_apis": "get columns where at", "apis": "apply join join join", "code": ["df['t'] = df['x'].apply(lambda x: ''.join([i for i in finds if i in x]))\n", "                            x  x1       t\n0      var_m500_0_somevartext   4  m500_0\n1     var_m500_0_vartextagain   5  m500_0\n2  varwithsomeothertext_0_500   6   0_500\n3   varwithsomext_m150_0_text   8  m150_0\n", "df[\"t\"] = df[\"x\"].str.extract(\"(%s)\" % '|'.join(finds))\n", "df[\"t\"] = df[\"x\"].str.extract(\"(\" + '|'.join(finds) + \")\")\n"], "link": "https://stackoverflow.com/questions/55742928/pandas-check-which-substring-is-in-column-of-strings"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have an awkward CSV file which has multiple delimiters: the delimiter for the non-numeric part is , for the numeric part . I want to construct a dataframe only out of the numeric part as efficiently as possible. I have made 5 attempts: among them, utilising the argument of , using regex with , using . They are all more than 2x slower than reading the entire CSV file with no conversions. This is prohibitively slow for my use case. I understand the comparison isn't like-for-like, but it does demonstrate the overall poor performance is not driven by I/O. Is there a more efficient way to read in the data into a numeric Pandas dataframe? Or the equivalent NumPy array? The below string can be used for benchmarking purposes. Checks: Benchmarking results: Update I'm open to using command-line tools as a last resort. To that extent, I have included such an answer. My hope is there is a pure-Python or Pandas solution with comparable efficiency.", "q_apis": "get columns all array last", "apis": "replace", "code": ["import os\n\nos.chdir(r'C:\\temp')                       # change directory location\nos.system('fart.exe -c file.csv \";\" \",\"')  # run FART with character to replace\n\ndf = pd.read_csv('file.csv', usecols=[3, 4, 5], header=None)  # read file into Pandas\n"], "link": "https://stackoverflow.com/questions/54044022/reading-data-from-csv-into-dataframe-with-multiple-delimiters-efficiently"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm constructing a pandas Dataframe for some ML. The X Dataframe has a date index composed of all existing dates from my various data files: Then I construct my datavariable where I want to consolidate every data I have: And, of course, now I want to fill it with the data (data is 1 DF, later on, it will be a list of DF): The error is : What I tried : This error is weird since set_value is Deprecated. And the doc page says to use .at. And .at uses set_value... I also tried to see the type of the variables type(data['Column Name'][i]) -> it's float64 I also tried to convert with pd.is_numeric. Same error I try to print out data['Column Name'][i] in the loop, no error. If I try to print out X, also no error. If I try without loop : X.at['2018-11-24', 0] = data['Column Name'][0] It works... I expect to get: A DataFrame with as index all dates in my multiple csv files, as column the values (if available) from my csv files. If not available, just nan.", "q_apis": "get columns date index all where now at at is_numeric at get DataFrame index all values", "apis": "index values add DataFrame sort_values reset_index drop insert set_index join", "code": ["all_dates=set()\nfor table in data:\n    for ind in table.index.values:\n        all_dates.add(table['Date'][ind])\n\ndates_list=list(all_dates)\nData={'Date': dates_list}\ntemp=pd.DataFrame(Data)\ntemp.sort_values(by=['Date'], inplace=True, ascending=True)\ntemp=temp.reset_index(drop=True)\n", "data.insert(0,temp)\ndfs = [df.set_index('Date') for df in data]\ndf_final=dfs[0].join(dfs[1:])\n"], "link": "https://stackoverflow.com/questions/54168819/setting-values-in-pandas-dataframe-with-at-incomprehensible-type-error"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here's my data Here's the result what I want, the logic is is more than , wins, if not , basically it just sorting algorithm", "q_apis": "get columns", "apis": "iloc idxmax first index idxmax", "code": ["df['Res'] = df.iloc[:, 1:].idxmax(axis=1)\n#if first column is index\n#df['Res'] = df.idxmax(axis=1)\nprint (df)\n   No  Candidate_A  Candidate_B  Candidate_C          Res\n0   1            8            9           12  Candidate_C\n1   2           18            9           12  Candidate_A\n2   3            9            9            7  Candidate_A\n"], "link": "https://stackoverflow.com/questions/50618066/how-do-i-get-winner-voter-on-pandas-based-on-sorting-algorithm"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe (snippet below) with index in format YYYYMM and several columns of values, including one called \"month\" in which I've extracted the MM data from the index column. What I want to do is make a new column called 'stavg' which takes the 5-year average of the 'st' column for the given month. For example, since the top row refers to 202001, the stavg for that row should be the average of the January values from 2019, 2018, 2017, 2016, and 2015. Going back in time by each additional year should pull the moving average back as well, such that stavg for the row for, say, 201205 should show the average of the May values from 2011, 2010, 2009, 2008, and 2007. I know how to generate new columns of data based on operations on other columns on the same row (such as dividing 'st' by 'us' to get 'stu' and extracting digits from index to get 'month') but this notion of creating a column of data based on previous values is really stumping me. Any clues on how to approach this would be greatly appreciated!! I know that for the first five years of data, I won't be able to populate the 'stavg' column with anything, which is fine--I could use NaN there.", "q_apis": "get columns index columns values month index year month values time year values columns columns get index get month values first", "apis": "index astype astype get index query mean apply", "code": ["df['year'] = (df['index'].astype(int)/100).astype(int)\n\ndef get_stavg(df, year, month):\n    # get year from index\n\n    df_year_month = df.query('@year - 5 <= year < @year and month == @month')\n    return df_year_month.st.mean()\n\n\ndf['stavg'] = df.apply(lambda x: get_stavg(df, x['year'], x['month']), axis=1)\n"], "link": "https://stackoverflow.com/questions/61166546/finding-historical-seasonal-average-for-given-month-in-a-monthly-series-in-a-dat"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've following data: I want to sort values and ignore the zero. The expected output should be:", "q_apis": "get columns values", "apis": "sort_values loc sort_values mask mask sort_values", "code": ["df = df[df['profit'] != 0].sort_values('profit', ascending=False)\n", "df = df.loc[df['profit'] != 0]\\\n       .sort_values('profit', ascending=False)\n", "mask = df['profit'] != 0\ndf = df[mask].sort_values('profit', ascending=False)\n"], "link": "https://stackoverflow.com/questions/50593621/pandas-sort-value-and-ignore-0"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a , now I want to and assign to rows in the group(s), whose length is 1 or < 2; I am wondering what is the best way to do it. so the result ,", "q_apis": "get columns now assign length", "apis": "loc groupby transform size where duplicated", "code": ["df.loc[df.groupby('name_id')['name_id'].transform('size') == 1, 'name_id'] = -1\n", "df['name_id'] = np.where(df.groupby('name_id')['name_id'].transform('size') == 1, \n                         -1, df['name_id'])\n\nprint (df)\n   name_id name\n0       -1    a\n1        2    b\n2        2    b\n3        3    c\n4        3    c\n5        3    c\n", "df['name_id'] = np.where(df.duplicated('name_id', keep=False), df['name_id'], -1)\n"], "link": "https://stackoverflow.com/questions/51026581/pandas-finds-indices-of-rows-in-each-group-which-meets-certain-condition-and-ass"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe from which I want to create a new dataframe by applying a filter based on the count function such that only those columns should be selected whose count is equal to a specified number. For example in the dataframe below: If my_variable = 4, then df1 should only contain Col B and Col D alongwith the index month_end. How do I do this?", "q_apis": "get columns filter count columns count index", "apis": "notna sum loc", "code": ["s = df.notna().sum(0) == 4     \ndf = df.loc[:, s]\n"], "link": "https://stackoverflow.com/questions/63780304/pandas-dataframe-using-the-count-function-to-filter-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Using Python Pandas I am trying to find the & with the maximum value. This returns the maximum value: But how do I get the corresponding and name?", "q_apis": "get columns value value get name", "apis": "loc idxmax", "code": ["In [34]: df.loc[df['Value'].idxmax()]\nOut[34]: \nCountry        US\nPlace      Kansas\nValue         894\nName: 7\n"], "link": "https://stackoverflow.com/questions/15741759/find-maximum-value-of-a-column-and-return-the-corresponding-row-values-using-pan"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "After preprocessing i have a final dataframe with columns 'timestamp', 'group', 'person1', 'person2'. I am trying to figure out how to code my requirement or want to know is it possible using python. What I am trying to extract is groups within each group. for example: in group G0, A is meeting with B, B meets with C, A meets with D. It means ABCD forms a group within the group. There can be multiple groups within each group (for example in group G1). How can I do this? what logic or code can I apply to extract this? I searched a lot, but it was not of any help.. The pic of dataframe sample and expected output is: sample data:", "q_apis": "get columns columns timestamp groups groups apply any sample sample", "apis": "DataFrame apply join groupby apply groupby agg min max", "code": ["import networkx as nx\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"timestamp\": ['25-06-2020 09:29','25-06-2020 09:29','25-06-2020 09:31','25-06-2020 09:32','25-06-2020 09:33','25-06-2020 09:33','25-06-2020 11:17','25-06-2020 11:17','25-06-2020 11:17','25-06-2020 11:17','25-06-2020 12:29','25-06-2020 12:29','25-06-2020 12:30','25-06-2020 12:30'],\n        \"group\": ['G0','G0','G0','G0','G0','G0','G1','G1','G1','G1','G1','G2','G2','G2'],\n        \"person1\": ['A','A','B','A','X','Z','A','B','L','X','Y','L','N','O'],\n        \"person2\": ['B','B','C','D','Y','N','B','C','M','Y','Z','M','O','P']\n    }\n)\n\ndef f(x):\n    G = nx.from_pandas_edgelist(x, 'person1', 'person2')\n    l = x.apply(lambda n: ''.join(nx.node_connected_component(G, n['person1'])), axis=1)\n    return l\n\ndf['subgroup'] = df.groupby('group').apply(f).to_numpy()\ndf\n", "           timestamp group person1 person2 subgroup\n0   25-06-2020 09:29    G0       A       B     DACB\n1   25-06-2020 09:29    G0       A       B     DACB\n2   25-06-2020 09:31    G0       B       C     DACB\n3   25-06-2020 09:32    G0       A       D     DACB\n4   25-06-2020 09:33    G0       X       Y       YX\n5   25-06-2020 09:33    G0       Z       N       NZ\n6   25-06-2020 11:17    G1       A       B      ACB\n7   25-06-2020 11:17    G1       B       C      ACB\n8   25-06-2020 11:17    G1       L       M       ML\n9   25-06-2020 11:17    G1       X       Y      ZYX\n10  25-06-2020 12:29    G1       Y       Z      ZYX\n11  25-06-2020 12:29    G2       L       M       ML\n12  25-06-2020 12:30    G2       N       O      ONP\n13  25-06-2020 12:30    G2       O       P      ONP\n", "df['timestamp'] = pd.to_datetime(df['timestamp'])\ndf.groupby('subgroup')['timestamp'].agg(['min', 'max'])\n", "                         min                 max\nsubgroup                                        \nACB      2020-06-25 11:17:00 2020-06-25 11:17:00\nDACB     2020-06-25 09:29:00 2020-06-25 09:32:00\nML       2020-06-25 11:17:00 2020-06-25 12:29:00\nNZ       2020-06-25 09:33:00 2020-06-25 09:33:00\nONP      2020-06-25 12:30:00 2020-06-25 12:30:00\nYX       2020-06-25 09:33:00 2020-06-25 09:33:00\nZYX      2020-06-25 11:17:00 2020-06-25 12:29:00\n"], "link": "https://stackoverflow.com/questions/62606537/associative-group-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Consider the two dataframes: Then the code: I can easily insert the color in df1 The problem is if column 'fruits' has alternative values, like: How to keep this code working? What I've tried for last was to create a new column with separated values for fruits: And .apply(tuple) here: But it doesn't match.", "q_apis": "get columns insert values last values apply", "apis": "apply explode values values items", "code": ["print(df1)\n\n    fruits_names\n0   green apple and banana are sweet fruits\n1   how fresh is the banana\n2   cherry and opal apple from japan\n", "df2[\"fruits\"] = df2[\"fruits\"].apply(lambda x: x.split(\"|\"))\n\ndf2 = df2.explode(\"fruits\")\n\nprint(df2)\n", "   fruits              colors\n0   green apple        red\n0   opal apple         red\n1   banana             yellow\n1   cavendish banana   yellow\n2   cherry             black\n", "d = {i:j for i,j in zip(df2[\"fruits\"].values, df2[\"colors\"].values)}\n", "df1[\"colors\"] = [[v for k,v in d.items() if k in x] for x in df1[\"fruits_names\"]]\n\nprint(df1)\n", "    fruits_names                            colors\n0   green apple and banana are sweet fruits [red, yellow]\n1   how fresh is the banana                 [yellow]\n2   cherry and opal apple from japan        [red, black]\n"], "link": "https://stackoverflow.com/questions/59086374/alternative-values-in-a-column-to-test-with-isin-pandas-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a large for loop calculating many variables that ultimately I would like to store in a a Pandas multiindex dataframe. Each step of the loop, I need to write to a slice of the dataframe. The datafram has two row indices and many columns. Each operation needs to write to a slice defined as: all top index, one particular second index, and one particular column. An example of the operation is below. I'm finding this very slow, which make my total runtime very slow as the operation needs to be performed hundreds of times. Is there a faster way to do this using Pandas? The only alternative I am aware of is storing the values in a separate Numpy array, and writing the Numpy array to the dataframe after the loop. But this means I have to create a large number of these temporary arrays", "q_apis": "get columns step indices columns all index second index values array array", "apis": "values groupby loc values T T DataFrame index columns T T DataFrame index columns loc", "code": ["import numpy as np\nimport pandas as pd\n\n\ndef assignValue(df, timestep, values):\n    for path, new_df in df.groupby(level=0):\n        new_df.loc[(path, timestep)] = values[path-1]\n\n\nn_paths = 1000\npathgrid=np.arange(1, n_paths + 1)\ndt = 0.25\nT = 40\n\ntimegrid = np.arange(0, T + dt, dt)\n\nmultiindex = pd.MultiIndex.from_product(\n    [pathgrid, timegrid], names=['Path', 'Timestep'])\n\ndf = pd.DataFrame(index=multiindex, columns=['Values'])\n\ninput = np.random.random_sample(n_paths)\n\ni = 0\n%timeit assignValue(df, 0.0, input)\n", "import numpy as np\nimport pandas as pd\n\n\nn_paths = 1000\ndt = 0.25\nT = 40\n\npathgrid=np.arange(1, n_paths + 1)\ntimegrid = np.arange(0, T + dt, dt)\n\nmultiindex = pd.MultiIndex.from_product(\n    [pathgrid, timegrid],\n    names=['Path', 'Timestep']\n)\n\ndf = pd.DataFrame(index=multiindex, columns=['Values'])\n\ninputs = np.random.random_sample(n_paths)\n%timeit for i in range(1, n_paths+1): df.loc[(i, 0.0)] = inputs[i-1]\n"], "link": "https://stackoverflow.com/questions/65945882/writing-to-slice-of-pandas-multindex-dataframe-is-slow"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: [1]: https://i.stack.imgur.com/wBi6V.png I used the following code to highlight the max values of every cell: But I just want to apply this formatting to a certain row, for example the row 'Book Value Per Share (mrq)'. I can't find any code that helps me with this!!! Anyone knows?", "q_apis": "get columns stack max values apply any", "apis": "style apply columns", "code": ["s = df.style.apply(highlight_max, axis=1, subset=(0, df.columns))\n"], "link": "https://stackoverflow.com/questions/66618135/how-to-apply-different-conditional-format-to-different-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My data is STRAVA activities in a dataframe with the index set as the date of the activity. I want to insert rows that are indexed with the date that is missing i.e. my dataframe would be indexed at a frequency of days from oldest to newest in the original data. I have tried the following methods from two other posts here Add missing dates to pandas dataframe and here pandas fill missing dates in time series However the issue I run into is as follows. Because on some dates two activities occur the index label for the row no longer is unique and is duplicated. So when I run df.reindex(index=pd.date_range(df.index.min(),df.index.max(), fill=0, I get the dates inserted but lose the duplicates. Similarly with df.index.asfreq(\"D\"). I get the same issue. The only solution I have found is using df.align() as parsing in a series with index labels at frequencies of days as below. Then I return the first dataframe in the tuple producing the following result. Is this the only solution? Is there a way to ignore duplicate indexes but continue to insert rows with dates that are MISSING and therefore not end up with even more duplicate date indexs?", "q_apis": "get columns index date insert date at days time index unique duplicated reindex index date_range index min index max get index asfreq get align index at days first insert date", "apis": "DataFrame merge", "code": ["import pandas as pd\n\ndates = pd.date_range('2020-12-20', '2020-12-24', freq = \"D\").to_frame(name='date')\n\nts = pd.DataFrame({'date': {0: '2020-12-20',\n  1: '2020-12-20',\n  2: '2020-12-22',\n  3: '2020-12-22',\n  4: '2020-12-23',\n  5: '2020-12-24'},\n 'value': {0: 8.0, 1: 7.0, 2: 6.5, 3: 9.0, 4: 4.0, 5: 3.0}})\n\n\nts['date'] = pd.to_datetime(ts['date'])\n\n\ndates.merge(ts, on='date', how='left')\n", "        date  value\n0 2020-12-20    8.0\n1 2020-12-20    7.0\n2 2020-12-21    NaN\n3 2020-12-22    6.5\n4 2020-12-22    9.0\n5 2020-12-23    4.0\n6 2020-12-24    3.0\n"], "link": "https://stackoverflow.com/questions/66124010/is-df-align-in-pandas-the-optimal-solution-for-inserting-missing-date-rows-wh"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following code in Python: As result, I have the following dataframe: What should I do to ensure in my dataset that every is a datetime prior to the of its next row? SOLUTION THAT HAS BEEN IMPLEMENTED I basically transformed the dataset into an array, put it in ascending order and transformed it back into a dataframe. It may not be the most elegant of the solutions, but it worked correctly and generated consistent data for what I intend to use.", "q_apis": "get columns array put", "apis": "min min max DataFrame columns size sort_values get last", "code": ["\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n# date_rng = pd.date_range(start='5/18/2019', end='7/22/2020', freq='S')\ndate_rng = pd.date_range(start='5/18/2019', end='5/19/2019', freq='min')\n\nsec = [(date_rng.min() - datetime(1970, 1, 1)).total_seconds(),\n       (date_rng.max() - datetime(1970, 1, 1)).total_seconds() ]\ndf = pd.DataFrame(date_rng, columns=['start_timestamp'])\ndf['start_timestamp'] = np.random.randint(sec[0],sec[1],size=(len(date_rng)))\ndf = df.sort_values(by=\"start_timestamp\")\nl = df[\"start_timestamp\"].tolist()  # get randomised start times\nl[-1] = sec[1] # set last time to end of range\n# randomise end time between two start times\ndf['end_timestamp'] = [np.random.randint(l[i], l[i+1]) if i<len(l)-1  and l[i]<l[i+1] else l[i] for i, s in enumerate(l)]\ndf['start_timestamp'] = pd.to_datetime(df['start_timestamp'],unit='s')\ndf['end_timestamp'] = pd.to_datetime(df['end_timestamp'],unit='s')\n\n\n"], "link": "https://stackoverflow.com/questions/63026370/check-if-a-date-on-a-row-is-earlier-than-another-date-on-the-next-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a csv file with 15 columns and around 17000 rows. My problem is to search in a specific column (for example: column 'name') for an input string, if it matches, print the the row [i] that contains the string, the previous row [i-1] and the next row [i+1], in order i-1, i, i+1. Repeat the process till the last element of the column (my data file is formated so that it contains no duplicate). I use this reference to find the rows and the program runs well. Below is my python code: I would like to ask how to export the filtered data above to a new dataframe and output it to a new csv file? I follow this reference: The output file is ok but it doesn't include the columns' names and I also think that it is not so formal and optimal acording to the author of the topic. Thank you very much.", "q_apis": "get columns columns name contains last contains columns names", "apis": "index append iloc max min index iloc max min index to_csv index index to_csv", "code": ["df = pd.DataFrame({\n        'A':list('abcdef'),\n         'B':[4,5,4,5,5,4],\n         'C':[7,8,9,4,2,3],\n         'D':[1,3,5,7,1,0],\n         'E':[5,3,6,9,2,4],\n         'name':list('aaabbb')\n})\n\nprint (df)\n\n#tested matching first row\nx = 'a'\n#tested matching last row\n#x = 'b'\n\nidx = df[df.name.str.contains(x, na=False)].index.tolist()\n\npd.DataFrame(columns=df.columns).to_csv('result.csv')\nfor i in idx:\n    df1 = df.iloc[[max(0, i-1), i, min(df.index[-1], i+1)]]\n\n    df1.to_csv('result.csv', index=False, mode='a', header=None)\n    #if need index values\n    #df1.to_csv('result.csv', mode='a', header=None)\n", "x = 'a'\nidx = df[df.name.str.contains(x, na=False)].index.tolist()\n\ndfs = []\nfor i in idx:\n    dfs.append(df.iloc[[max(0, i-1), i, min(df.index[-1], i+1)]])\n\n#list comprehension alternative\n#dfs = [df.iloc[[max(0, i-1), i, min(df.index[-1], i+1)]] for i in idx]\n", "pd.concat(dfs).to_csv('result.csv', index=False)\n#if need index\n#pd.concat(dfs).to_csv('result.csv')\n"], "link": "https://stackoverflow.com/questions/53167851/pandas-find-some-specific-rows-and-output-the-filtered-dataframe-into-a-new-cs"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am newly in pandas and I just start my code learning. Please, it would be great if you could help me. I have a simple XML like this and I wanna convert it in a dataframe with pandas I use some code but anyway it does not help me:", "q_apis": "get columns start", "apis": "count product product product product product product count product product product product product product count product product product product append DataFrame", "code": ["import xml.etree.ElementTree as ET\n\nimport pandas as pd\n\nxml = '''<products_availability date=\"2020-01-24 06:32\" >\n    <region id=\"122\">\n        <products count=\"45453242\">\n            <product id=\"1000001\">0</product>\n            <product id=\"1000002\">5</product>\n            <product id=\"1000003\">3</product>\n        </products>\n    </region>\n    <region id=\"133\">\n        <products count=\"45453277\">\n            <product id=\"1000004\">7</product>\n            <product id=\"1000005\">3</product>\n            <product id=\"1000006\">1</product>\n        </products>\n    </region>\n</products_availability>'''\n\ndata = []\nroot = ET.fromstring(xml)\nregions = root.findall('.//region')\nfor region in regions:\n    region_id = region.attrib['id']\n    products_count = region.find('./products').attrib['count']\n    for product in region.findall('.//product'):\n        entry = {'region_id': region_id, 'products_count': products_count,\n                 'product_id': product.attrib['id'], 'number': product.text}\n        data.append(entry)\ndf = pd.DataFrame(data)\nprint(df)\n", "  region_id products_count product_id number\n0       122       45453242    1000001      0\n1       122       45453242    1000002      5\n2       122       45453242    1000003      3\n3       133       45453277    1000004      7\n4       133       45453277    1000005      3\n5       133       45453277    1000006      1\n"], "link": "https://stackoverflow.com/questions/59919333/how-to-convert-xml-to-dataframe-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am building a somewhat automated query builder for a process I have that needs to query data from influx for a time range. Towards that end, I need to filter by time, which means I need epoch millisecond strings in my query. This code throws the following error. and the stack trace:", "q_apis": "get columns query query time filter time query stack", "apis": "query", "code": ["import pandas as pd\n\nts = pd.Timestamp(pd.Timestamp('now', tz='US/Pacific').date(), tz='US/Pacific')\nts_str = unicode(int(ts.value / (10 ** 6)))\nquery_str = 'SELECT * FROM \"table\".\"measurement\" WHERE time <= ' + \\\n            ts_str + 'ms LIMIT 10'\njson_payload = client.query(query_str)\nprint(json_payload)\n"], "link": "https://stackoverflow.com/questions/43082504/influxdb-dataframeclient-keyerror-83"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to add two columns to an existing dataframe based on the values of a few other columns. My dataframe looks like this: I want to add two columns 'Score_A' and 'Score_B' such that 'Score_A' will be the mean of scores for cases where Type is 'A' (for each row). Likewise for 'Score_B'. A catch is that wherever the Type is blank, the score should not be used to calculate the mean. The result of a successful function, in this case, will be: I have run nested loops at a row level to do this, but is there a better way?", "q_apis": "get columns add columns values columns add columns mean where mean at", "apis": "DataFrame where mean fillna DataFrame where mean fillna", "code": ["m1 = (df[['Type', 'Type1', 'Type2']] == 'A')\nm2 = (df[['Type', 'Type1', 'Type2']] == 'B')\nscores = df[['Score', 'Score1', 'Score2']]\n\ndf['Score_A'] = pd.DataFrame(np.where(m1, scores, np.nan)).mean(skipna=True, axis=1).fillna(0)\ndf['Score_B'] = pd.DataFrame(np.where(m2, scores, np.nan)).mean(skipna=True, axis=1).fillna(0)\n\nprint(df)\n", "  Type Type1 Type2  Score  Score1  Score2   Score_A  Score_B\n0    A     A     A      1       2       1  1.333333      0.0\n1    A     A     B      2       1       3  1.500000      3.0\n2    A           B      3       0       2  3.000000      2.0\n3    B     B     B      1       1       1  0.000000      1.0\n4                A      0       0       2  2.000000      0.0\n5                       0       0       0  0.000000      0.0\n"], "link": "https://stackoverflow.com/questions/61924147/how-do-i-add-two-new-columns-on-the-basis-of-the-values-of-multiple-other-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have dataframe with the following structure: I would like to modify the values in column with a condition that if the exists in , then suffix with '_1stParty' else '_3rdParty'. The dataframe should eventually look like:", "q_apis": "get columns values", "apis": "apply", "code": ["m = df.apply(lambda x: x['source'] in x['website'], axis=1)\n", "m = [a in b for a, b in zip(df['source'], df['website'])]\n", "df['type'] += np.where(m, '_1stParty',  '_3rdParty')\n#'long' alternative\n#df['type'] = df['type'] + np.where(m, '_1stParty',  '_3rdParty')\nprint (df)\n        website            type         source\n0       bbc.com  image_1stParty            bbc\n1       cnn.com  audio_3rdParty         google\n2    google.com  image_3rdParty  stackoverflow\n3  facebook.com  video_1stParty       facebook\n"], "link": "https://stackoverflow.com/questions/58152627/pandas-how-to-modify-values-in-a-column-in-dataframe-by-comparing-other-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes. df1 : df2: I wanted to merge them but the column_a and column_b would become a list.", "q_apis": "get columns merge", "apis": "groupby agg dropna where astype bool", "code": ["df = pd.concat([df1, df2]).groupby('key').agg(lambda x: x.dropna().tolist())\nprint (df)\n       column_a    column_b\nkey                        \nAA   [AAB, AAC]       [AAA]\nAB   [ABA, ABK]  [ABA, ABK]\nAC           []       [ACC]\n", "df = df.where(df.astype(bool))\nprint (df)\n       column_a    column_b\nkey                        \nAA   [AAB, AAC]       [AAA]\nAB   [ABA, ABK]  [ABA, ABK]\nAC          NaN       [ACC]\n"], "link": "https://stackoverflow.com/questions/52327345/python-pandas-how-to-merge-two-dataframes-that-have-two-different-columns-to-t"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "how to fill rows with help of index pandas? I have dataframe like I have another dataframe with same no of columns and different index values like How to combine the above two rows sequentially in pandas? Like the below dataframe", "q_apis": "get columns index columns index values combine", "apis": "sort_values", "code": ["df = pd.concat([df1, df2]).sort_values('Alert number', ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/67264876/how-to-fill-rows-with-help-of-index-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am requesting a data loop, I want to get the id of a bunch of coins, to run an indicator through them to detect which one is trending and filter the rest out. However, the following code returns an error: How to progress? EDIT: FULL ERROR MESSAGE: UPDATE: after following the advice of Trenton, It works if I keep the input as however if I add another column I would like to have again I get the error that the TypeError: Unhashable Type: list", "q_apis": "get columns get filter add get", "apis": "get DataFrame head first get DataFrame columns set_index DataFrame columns set_index DataFrame columns set_index combine index index index add head items plot loc", "code": ["import pandas as pd\nimport requests\nimport matplotlib.pyplot as plt\n\n# make the API call\nAPI_URL = 'https://api.coingecko.com/api/v3'\n\nr_coins_d = requests.get(API_URL + '/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=10&page=1&sparkline=false')\nd_coins = r_coins_d.json()\n\n# create the markets dataframe\ndf_coins_markets = pd.DataFrame(d_coins)\n\n# display(df_coins_markets.head())\n         id symbol      name                                                                           image  current_price    market_cap  market_cap_rank  fully_diluted_valuation  total_volume  high_24h       low_24h  price_change_24h  price_change_percentage_24h  market_cap_change_24h  market_cap_change_percentage_24h  circulating_supply  total_supply  max_supply       ath  ath_change_percentage                  ath_date        atl  atl_change_percentage                  atl_date                                                                               roi              last_updated\n0   bitcoin    btc   Bitcoin        https://assets.coingecko.com/coins/images/1/large/bitcoin.png?1547033579       37958.00  704703722512                1             7.947549e+11   47467298406  38237.00  36673.000000        723.720000                      1.94367           1.168963e+10                           1.68678        1.862056e+07  2.100000e+07  21000000.0  41940.00               -9.76302  2021-01-08T15:05:37.863Z  67.810000            55711.87093  2013-07-06T00:00:00.000Z                                                                              None  2021-02-05T23:14:54.266Z\n1  ethereum    eth  Ethereum     https://assets.coingecko.com/coins/images/279/large/ethereum.png?1595348880        1713.19  195808148792                2                      NaN   38923140914   1752.05   1587.800000         92.100000                      5.68139           9.863290e+09                           5.30442        1.145515e+08           NaN         NaN   1752.05               -2.72207  2021-02-05T16:15:12.089Z   0.432979           393534.55297  2015-10-20T00:00:00.000Z  {'times': 59.35104667718437, 'currency': 'btc', 'percentage': 5935.104667718438}  2021-02-05T23:13:13.252Z\n2    tether   usdt    Tether  https://assets.coingecko.com/coins/images/325/large/Tether-logo.png?1598003707           1.00   27851925281                3                      NaN   88244095697      1.00      0.994689          0.005368                      0.53863           1.790294e+07                           0.06432        2.779522e+10  2.779522e+10         NaN      1.32              -24.36226  2018-07-24T00:00:00.000Z   0.572521               74.79873  2015-03-02T00:00:00.000Z                                                                              None  2021-02-05T23:10:05.466Z\n", "                                prices                         market_caps                        total_volumes\n0  [1609978421091, 10.051749696667137]  [1609978421091, 9599471028.799435]  [1609978421091, 1433598906.2376666]\n1  [1609981399992, 10.404913579249927]  [1609981399992, 9800416299.991219]   [1609981399992, 1484399005.370463]\n", "df_market_dict = dict()  # create a dict of dataframes\n\n# iterate through the unique coin market ids\nfor coin_id in df_coins_markets.id.unique()[:5]:  # first 5\n    print(f'Coin ID: {coin_id}')\n\n    # call the API\n    r_market_d = requests.get(API_URL + f'/coins/{coin_id}/market_chart?vs_currency=usd&days=30&interval=hourly')\n\n    # extract the JSON from the response\n    d_market = r_market_d.json()\n    \n    # separately extract each key from the json\n    prices = pd.DataFrame(d_market['prices'], columns=['date', 'prices']).set_index('date')\n    market_caps = pd.DataFrame(d_market['market_caps'], columns=['date', 'market_caps']).set_index('date')\n    total_volumes = pd.DataFrame(d_market['total_volumes'], columns=['date', 'total_volumes']).set_index('date')\n\n    # combine the separate dataframes\n    df_market = pd.concat([prices, market_caps, total_volumes], axis=1)\n    \n    # convert the index to a datetime dtype\n    df_market.index = pd.to_datetime(df_market.index, unit='ms')\n    \n    # add the market_data dataframe to the dict, with the coin_id as the key\n    df_market_dict[coin_id] = df_market  \n    \n    display(df_market.head())  # use print if not in a notebook or spyder\n", "df_market_dict['bitcoin'].head()\n\n                               prices   market_caps  total_volumes\ndate                                                              \n2021-01-07 01:05:05.955  37215.781816  6.929761e+11   7.262550e+10\n2021-01-07 02:08:34.785  36981.672166  6.871691e+11   7.031503e+10\n2021-01-07 03:02:56.008  37059.042450  6.890434e+11   7.341887e+10\n2021-01-07 04:14:05.845  37493.615768  6.971249e+11   7.513032e+10\n2021-01-07 05:21:11.600  37473.956686  6.967610e+11   6.715542e+10\n", "fig, ax = plt.subplots(figsize=(9, 7))\nfor k, v in df_market_dict.items():\n    v.plot(y='prices', label=f'{k}', ax=ax)\n\nax.set_title('Prices')\nax.set_yscale('log')\nax.legend(title='Coin ID', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n"], "link": "https://stackoverflow.com/questions/66071243/how-to-parse-out-lists-returned-by-an-api-into-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I have a a set of csv files of the following general format: And I would like to reformat the data frame to be of the format: Im moving to python from so I have a very very limited understanding of what I'm doing do far in term of manipulating these dataframes in python and I cant seem to find any examples of others attempting to do anything like this. Another way of phrasing what I'm doing is attempting to overlay each row of the same type into one row that contains all of the times each corresponding with their original columns. All columns are predefined in the original csv so I do not need to, nor want to create any more columns.", "q_apis": "get columns any contains all columns columns any columns", "apis": "replace groupby first replace", "code": ["# Original Dataframe\n>>> df\n  Post_Type Time1 Time2 TimeN\n0     Type1  1:12            \n1     Type1        2:34      \n2     Type1              0:35\n3     Type2  1:11            \n4     Type3  5:34            \n5     Type3              2:45\n\n# Processed:\n>>> df.replace('', np.nan).groupby('Post_Type').first().replace(np.nan, '')\n          Time1 Time2 TimeN\nPost_Type                  \nType1      1:12  2:34  0:35\nType2      1:11            \nType3      5:34        2:45\n"], "link": "https://stackoverflow.com/questions/50959078/merging-multiple-rows-into-one-based-on-row-name-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I am using plt.subplots() to plot multiple graphs in a single output whilst using matplotlib magic function. Anyways, I noticed that I was doing the same calculation for each \"variable\", or column, for my dataframe. Basically, it looks like this So, I am clearly getting the counts of billcounts for each column in my dataframe starting with the variable 'paperless'. Now, I want to count the accounts for each variable, find the percentage of them out of the number of cases I have, and plot it. How do I automate this using a function or some type of for loop? I have about 15 variables so doing it manually is not really efficient... Thanks in advance, I will update with what I come up with as this question is open", "q_apis": "get columns plot count plot update", "apis": "axes count sum get all columns axes groupby agg plot plot plot", "code": ["import pandas as pd\n%matplotlib inline\n\nfig, axes =plt.subplots(nrows=1, ncols=3, sharex=False, sharey=True, figsize=(5,5))\n\naggregators = {'ACCOUNT':'count', 'cases': 'sum'}\nvariables = ['PAPERLESS', 'More PAPERLESS', 'PAPEERLESS NOT'] #For example\n'''\n# One way to get all the variables\nvariables = list(df.columns)\nvariables.remove('ACCOUNT')\nvariables.remove('cases')\n'''\n\nfor variable, ax in zip(variables, axes):\n    mid = df.groupby(variable)['ACCOUNT', 'cases'].agg(aggregators) #Map a function to each column\n    percts = (mid.ACCOUNT / mid.cases) * 100 #Return a pd.Series with the percentages since you only plot that anyways\n    percts.plot(kind='bar', ax=ax) #Only plot percentage\n    ax.set_title(variable)\n"], "link": "https://stackoverflow.com/questions/49699247/how-to-go-through-each-column-in-dataframe-and-perform-a-calculation"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This is probably a pretty basic question. Suppose I have two dataframes: I want to join the dataframes on to look like: So far, I have been doing left merges, e.g. but this results in duplicated columns (see below) that I have to correct by filling the nans, renaming the columns, and then dropping the duplicate. This becomes particularly tedious if I have more than 2 dataframes to merge. What's a better way to do this?", "q_apis": "get columns join left duplicated columns columns merge", "apis": "combine_first set_index set_index reset_index", "code": ["df1.combine_first(df2)\n\n   id     a\n0   0   5.0\n1   1  10.0\n2   2  15.0\n3   3  20.0\n4   4  25.0\n5   5  30.0\n", "my_list_df = [df1, df2]\n\nfrom functools import reduce\nreduce(lambda new_df,  df_to_combine: new_df.combine_first(df_to_combine), \n       map(lambda df: df.set_index('id'), my_list_df)).reset_index()\n", "   id     a\n0   0   5.0\n1   1  10.0\n2   2  15.0\n3   3  20.0\n4   4  25.0\n5   5  30.0\n", "reduce(lambda new_df, df_to_combine: new_df.combine_first(df_to_combine.set_index('id')),\n       my_list_df[1:], my_list_df[0].set_index('id')).reset_index()\n"], "link": "https://stackoverflow.com/questions/61621816/most-efficient-way-to-combine-several-dataframes-with-complementary-missing-valu"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame like this: Each row with col1 == 'B' has a comment which will be a string. I need to aggregate the comments and fill the preceding row (where col1 != 'B') with the resulting aggregated string. Any given row where col1 != 'B' could have none, one or many corresponding rows of comments (col1 == 'B') which seems to be the crux of the problem. I can't just use fillna('bfill') etc. I have looked into iterrows(), groupby(), while loops and tried to build my own function. But, I don't think I'm fully understanding how all of those are working. Finished product should look like this: Eventually I will be dropping all rows where col1 == 'B', but for now I'd like to keep them for verification.", "q_apis": "get columns DataFrame aggregate where where fillna bfill iterrows groupby all product all where now", "apis": "index loc loc groupby apply reset_index merge", "code": ["df['col_group'] = -1\ncol_group = 0\nfor i in df.index:\n    if df.loc[i, 'col1'] != 'B':\n        col_group += 1\n    df.loc[i, 'col_group'] = col_group\n\ncomments = df[df['col1'] == 'B']\ntransactions = df[df['col1'] != 'B']\nagg_comments = comments.groupby('col_group')['col2'].apply(lambda x: reduce(lambda i,j: i+\"&$#\"+j,x)).reset_index()\ndf = pd.merge(transactions, agg_comments, on='col_group', how='outer')\n"], "link": "https://stackoverflow.com/questions/56153666/aggregating-string-data-from-multiple-sequential-rows-of-a-column-based-on-condi"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a nested for loop that first searches for a songs name based on DataFrame values using the Spotipy API. This for loop is intended to add values from the json output to a list. Here is the DataFrame I'm calling: pandas dataframe The for loop pulls each artists name and song, searches for the song in spotipy, and appends those values to a list. Heres my code: Output: My plan is append these lists to the DataFrame above, capture each songs popularity, and drop any unneeded columns afterwards. However, I can not get the nested for loop to work correctly: . Ive attempted using i in range, t to enumerate, and other solutions to add each searched value to the list, produce output, and append to the list with no success. I either get back multiple lines of the same song values or an error about non scriptable (like above) or uncallable values. Any advice on how to get this to work?", "q_apis": "get columns first name DataFrame values add values DataFrame name values append DataFrame drop any columns get add value append get values values get", "apis": "items append append append", "code": ["tracks = track_id[\"tracks\"][\"items\"]\nfor i in range (searchtrack):\n        track_name.append(tracks[i]['name'])\n        ids.append(tracks[i]['id'])\n        popularity.append(tracks[i]['popularity'])\n"], "link": "https://stackoverflow.com/questions/67851894/using-spotipy-api-to-loop-through-a-dataframe-help-needed"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to use Pandas to store values about my cars. I can view the information in a coordinate system like setup with the Pivot command, but only one kind of info per Pivot. I also want to be able to get the values fast, like with a class (cars.[x][y].x_goal). How do I do this? Or is there a better way to store coordinat system information like this? This is what I have tried. The print(result) at the end does not work.", "q_apis": "get columns values view info get values at", "apis": "loc pivot pivot_table index columns values pivot loc query", "code": ["result = cars[(cars['x']==2) & ((cars['y']==3))]['x_goal']\nprint(result)\n", "result = cars.loc[((cars['x']==2) & (cars['y']==3)), 'x_goal']\n", "pivot = cars.pivot_table(index='x', columns='y', values='x_goal')\nresult2 = pivot.loc[2,3]\nprint(result2)\n", "result3 = cars.query('x == 2 & y == 3')['x_goal']\nprint(result3)\n"], "link": "https://stackoverflow.com/questions/66301030/python-panda-get-value-of-cell-with-x-y-coordinates"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given the dataframe: I want to drop duplicates based on col1,col2. eg.(row(0): A type1, row(2): A type1) keeping only the row that has the latest hour eg.(18:13:46). I tried using groupby to return subset based on col1, and drop_duplicates to drop the duplicate in col2. I need to find a way to pass the condition (latest hour) example code: Expected outcome: EDIT adding context my original dataframe is larger, the solution needs to work for also: it would still need to drop the column based on the hour", "q_apis": "get columns drop hour groupby drop_duplicates drop hour drop hour", "apis": "sort_values drop_duplicates last", "code": ["df.sort_values('hour').drop_duplicates(['col1','col2'] , keep = 'last')\n\n"], "link": "https://stackoverflow.com/questions/57807305/pandas-drop-duplicates-in-cola-keeping-row-based-on-condition-on-colb"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How could I change all columns that have in the column name? For these columns I want to then conditionally replace rows. Want in general code for a larger dataset Expected Output:", "q_apis": "get columns all columns name columns replace", "apis": "filter columns replace", "code": ["In [555]: cols = df.filter(like='change_').columns\n\nIn [556]: df[cols] = df[cols].replace('onee', 'one')\n\nIn [557]: df\nOut[557]: \n  change_1 comment change_2\n0      one  number      two\n1    three  larger      one\n"], "link": "https://stackoverflow.com/questions/65583340/change-columns-that-contain-string-and-replace-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hi I am trying to two inside but getting error. How to achive , What am i doing wrong ? Code:", "q_apis": "get columns", "apis": "DataFrame DataFrame where merge", "code": ["df1 = pd.DataFrame(\n    {\"id\": range(300), \"name\": np.random.choice(list(\"abcdefghijkjlmnopqrstuvwxyz\"), 300)}\n)\ndf3 = pd.DataFrame(\n    {\n        \"id\": range(35),\n        \"name\": np.random.choice(list(\"abcdefghijkjlmnopqrstuvwxyz\"), 35),\n        \"present_in_old\": np.random.choice([\"yes\", \"no\", \"maybe\"], 35),\n    }\n)\n\n\ndf3[\"old_result\"] = np.where(\n    (df3[\"present_in_old\"] == \"yes\"),\n    df3.merge(df1, left_on=\"id\", right_on=\"id\", suffixes=(\"_left\", \"\"), how=\"left\")[\"name\"],\n    None,\n)\n\n"], "link": "https://stackoverflow.com/questions/68048606/pandas-merge-valueerror-operands-could-not-be-broadcast-together-with-shapes-3"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How could I generate the \"important\" column? It's Yes if per fruit: (1) There is a sale that year and (2) there was no sale the year before and (3) there is at least a 3-year gap between that year and the previous \"important\" year.", "q_apis": "get columns year year at year between year year", "apis": "shift diff diff fillna loc index isin index", "code": ["for i in df['fruit'].unique():\n    df1 = df[(df['sale'] == 'True') & (df['sale'].shift() != 'True') & (df['fruit'] == i)]\n    df1 = df1[(df1['year'].diff() >=3) | (df1['year'].diff().fillna(0) == 0)]\n    df.loc[df.index.isin(df1.index), 'important'] = 'Yes'\n", "    fruit   year    sale    important\n0   apple   2010    None    NaN\n1   apple   2011    None    NaN\n2   apple   2012    None    NaN\n3   apple   2013    None    NaN\n4   apple   2014    True    Yes\n5   apple   2015    True    NaN\n6   apple   2017    True    NaN\n7   apple   2018    True    NaN\n8   apple   2019    None    NaN\n9   apple   2020    True    Yes\n10  banana  2010    True    NaN\n"], "link": "https://stackoverflow.com/questions/65037222/groupby-with-multiple-conditions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to left join multiple pandas dataframes on a single column, but when I attempt the merge I get warning: KeyError: 'Id'. I think it might be because my dataframes have offset columns resulting from a statement, but I could very well be wrong. Either way I can't figure out how to \"unstack\" my dataframe column headers. None of the answers at this question seem to work. My code: Returns: How to get those offset headers into the top level?", "q_apis": "get columns left join merge get columns unstack at get", "apis": "groupby sum", "code": ["In [11]: df = pd.DataFrame([[2, 3], [5, 6]], pd.Index([1, 4], name=\"A\"), columns=[\"B\", \"C\"])\n\nIn [12]: df\nOut[12]:\n   B  C\nA\n1  2  3\n4  5  6\n\nIn [13]: df.reset_index()\nOut[13]:\n   A  B  C\n0  1  2  3\n1  4  5  6\n", "step1 = step3.groupby(['Id', 'interestingtabsplittest2__grp'], as_index=False)['applications'].sum()\n"], "link": "https://stackoverflow.com/questions/33004573/after-groupby-how-to-flatten-column-headers"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Output I want to count repeating, sequential \"obs\" values within a \"site\" and \"parm\". I have this code which is close: Output It creates the new column with the count. The gap is when the parm changes from 8 to 9 it includes the parm 9 in the parm 8 count. The expected output is:", "q_apis": "get columns count values count count", "apis": "groupby ne shift cumsum transform size", "code": ["df['consecutive'] = (df.groupby([df.obs.ne(df.obs.shift()).cumsum(),\n                                 'site', 'parm']\n                                )\n                     ['obs'].transform('size')\n                     )\n", "   site  parm  date  obs  consecutive\n0     1     8     1    1            2\n1     1     8     2    1            2\n2     1     8     3    2            1\n3     1     8     4    3            2\n4     1     8     5    3            2\n5     1     9     1    3            1\n6     1     9     2    5            2\n7     1     9     3    5            2\n8     1     9     4    6            2\n9     1     9     5    6            2\n"], "link": "https://stackoverflow.com/questions/65258458/need-to-count-repeating-consecutive-values-in-python-dataframe-within-a-groupby"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How do i split a single column in a DataFrame that has a string without creating more columns. And get rid of the brackets. For example two rows looks like this: And I would like for the output dataframe to look like this, where the information column is a string: Within the same column for Information", "q_apis": "get columns DataFrame columns get where", "apis": "merge apply replace", "code": ["def combine_with_nan(x, cols):\n    combined=''\n    for column in cols:\n        try:\n            np.isnan(x[column])\n            Temp = ''\n        except:\n            Temp = x[column]\n        combined= combined + ' || ' + Temp\n\n    return combined \ncols=['Columns you want to merge']\npracticedf = practicedf.apply(combine_with_nan, axis=1,args=(cols,)).to_frame().replace(r\"\\\\n\",\" || \", regex=True)\n\n"], "link": "https://stackoverflow.com/questions/59161659/how-to-split-column-values-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: I would like to derive a new dataframe from the current which looks like the following: How would this be possible?", "q_apis": "get columns", "apis": "pivot index columns values count columns reset_index", "code": ["df1 = df1.pivot(index='foo', columns='bar', values='count')\ndf1.columns = ['Count1', 'Count2']\ndf1 = df1.reset_index()\n", "  foo  Count1  Count2\n0   a     110      42\n1   b      90      87\n2   c     165      23\n"], "link": "https://stackoverflow.com/questions/63844339/derive-dataframe-using-count-of-data-and-splitting-the-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am running Pandas 0.20.3 with Python 3.5.3 on macOS. I have a multiindexed dataframe similar to the following : I want to modify the dataframe and set some values to 0. Say where is equal to 'A' and where date is before 2018-01-15. I am using the following: I do not get any and the dataframe is modified correctly on my mac. However when I run this code on a Windows environment with the same pandas version, the dataframe is not modified. Hence my question: Is the above code incorrect? If not, how to properly make the assignment I need?", "q_apis": "get columns values where where date get any", "apis": "index index loc", "code": ["m1 = df.index.get_level_values(0) == 'A'\nm2 = df.index.get_level_values(1) < '2018-01-15'\n\ndf.loc[m1 & m2, 'value'] = 0\n", "print (df.head(20))\n                   value\nref date                \nA   2018-01-01  0.000000\n    2018-01-02  0.000000\n    2018-01-03  0.000000\n    2018-01-04  0.000000\n    2018-01-05  0.000000\n    2018-01-06  0.000000\n    2018-01-07  0.000000\n    2018-01-08  0.000000\n    2018-01-09  0.000000\n    2018-01-10  0.000000\n    2018-01-11  0.000000\n    2018-01-12  0.000000\n    2018-01-13  0.000000\n    2018-01-14  0.000000\n    2018-01-15 -0.701757\n    2018-01-16 -0.160638\n    2018-01-17 -0.226917\n    2018-01-18 -0.431952\n    2018-01-19 -0.339794\n    2018-01-20 -0.050133\n"], "link": "https://stackoverflow.com/questions/51264932/how-to-properly-make-an-assignment-to-a-slice-of-a-multiindexed-dataframe-in-pan"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am attempting to roll-up rows from a data set with similar measures into a consolidated row. There are two conditions that must be met for the roll-up: The measures (ranging from 1-5) should remain the same across the rows for them to be rolled up to a single row. The dates should be continuous (no gaps in dates). If these conditions are not met, the code should generate a separate row. This is the sample data that I am using: The expected result should be: I have implemented the code below which seems to address condition # 1, but I am looking for ideas on how to incorporate condition # 2 into the solution. Any help is appreciated.", "q_apis": "get columns sample", "apis": "iloc cumsum astype cumsum all all astype astype groupby agg first first first first first first last", "code": ["import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Convert begin_date and end_time to datetime\ndf['begin_date'] = pd.to_datetime(df['begin_date'], format='%m/%d/%Y')\ndf['end_date']= pd.to_datetime(df['end_date'], format='%m/%d/%Y')\n\n# We create a new column which contains the end_date+1 from the previous row\ndf['end_date_prev'] = df['end_date'].iloc[:-1] + timedelta(days=1)\ndf['end_date_prev'] = np.roll(df['end_date_prev'], 1)\n\n# Create a cumsum that resets when begin_date and end_date_prev doesn't match\ndf['cont'] = (~(df['begin_date'] == df['end_date_prev'])).astype(int).cumsum()\n\n# Since we need all measures to match we create a string column containing all measurements\ndf['comb_measure'] = df['measure1'].astype(str).str.cat(df[['measure{}'.format(i) for i in range(2,6)]].astype(str))\n\n# Get the final df\nnew_df = df.groupby(['id', 'comb_measure', 'cont']).agg(\n    {'measure1':'first', 'measure2':'first', 'measure3':'first', 'measure4':'first', 'measure5':'first', \n     'begin_date':'first', 'end_date':'last'})\n"], "link": "https://stackoverflow.com/questions/59490321/gaps-in-dates-while-rolling-up-quarters-into-a-single-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 different Dataframes. One is a list of \"Values\" that have a date, and a value. The second Dataframe is a dataframe of dates, that lie in between the dates of Dataframe 1. I want to merge the 2 Dataframes and interpolate the values from dataframe 1 to the values in dataframe2. I have attempted the below: but it doesnt work! im going around in a loop! Any help appreciated! Thanks Maturity DF 2019-04-11 1.0 2019-04-12 0.999933 2019-04-15 0.999732 2019-04-16 0.9996649999999999 2019-04-23 0.9991969999999999 2019-04-29 0.9987959999999999 2019-05-06 0.9983290000000001 2019-05-15 0.9977290000000001 2019-06-17 0.995532 2019-07-15 0.99369 2019-08-15 0.991663 2019-09-16 0.989582 2019-10-15 0.987725 2019-11-15 0.985764 2019-12-16 0.98383 2020-01-15 0.9819899999999999 2020-02-18 0.979935 2020-03-16 0.9783209999999999 2020-04-15 0.97655 2020-07-15 0.971353 2020-10-15 0.96631 2021-04-15 0.9568610000000001 2022-04-19 0.938263 2023-04-17 0.9198 2024-04-15 0.9006190000000001 Dataframe2: Maturity 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-17 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-18 2019-04-23 2019-04-23 2019-04-23 2019-04-23 2019-04-23 2019-04-24 2019-04-24 2019-04-24 2019-04-24 2019-04-24 2019-04-24 2019-04-24 2019-04-24 2019-04-25 2019-04-25 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-26 2019-04-29 2019-04-29 2019-04-30 2019-04-30 2019-04-30 2019-04-30 2019-04-30 2019-05-02 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-03 2019-05-07 2019-05-07 2019-05-07 2019-05-07 2019-05-07 2019-05-07 2019-05-08 2019-05-08 2019-05-08 2019-05-08 2019-05-09 2019-05-09 2019-05-09 2019-05-10 2019-05-10 2019-05-10 2019-05-10 2019-05-13 2019-05-13 2019-05-14 2019-05-14 2019-05-14 2019-05-14 2019-05-14 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-15 2019-05-16 2019-05-16 2019-05-16 2019-05-16 2019-05-16 2019-05-17 2019-05-17 2019-05-17 2019-05-20 2019-05-21 2019-05-21 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-22 2019-05-23 2019-05-23 2019-05-23 2019-05-23 2019-05-24 2019-05-24 2019-05-24 2019-05-28 2019-05-28 2019-05-29 2019-06-07 2019-06-07 2019-06-11 2019-06-13 2019-06-14 2019-06-18 2019-06-18 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-19 2019-06-20 2019-06-20 2019-06-20 2019-06-21 2019-06-25 2019-06-28 2019-06-28 2019-06-28 2019-06-28 2019-07-03 2019-07-11 2019-07-12 2019-07-12 2019-07-15 2019-07-15 2019-07-15 2019-07-15 2019-07-15 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-16 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-17 2019-07-19 2019-07-22 2019-07-22 2019-07-22 2019-07-24 2019-07-25 2019-08-21 2019-08-21 2019-09-06 2019-09-17 2019-09-18 2019-09-30 2019-10-02 2019-10-07 2019-10-16", "q_apis": "get columns date value second between merge interpolate values values", "apis": "get merge drop_duplicates set_index interpolate resample interpolate reset_index merge merge", "code": ["df.Maturity = pd.to_datetime(df.Maturity)\ndf2.Maturity = pd.to_datetime(df2.Maturity)\n\n# get the common date in the two data frames\ncommon = df.merge(df2.drop_duplicates(), how='outer').set_index('Maturity')\n\n# interpolate the value:\ncommon = common.resample('d').interpolate().reset_index()\n\n# merge\ncommon.merge(df2, how='right')\n"], "link": "https://stackoverflow.com/questions/55709234/merging-two-dataframes-in-python-linear-interpolating-dates-from-one-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with two columns and I have a word list. I want to filter one column of the dataframe by this list and save the matching rows of that column in a new list and at the same time get the value of the same index from the other column to save that value in a second list. So I basically want to filter my dataframe by one column except that I have strings that I want to compare to words so I have to loop through them. My code: My desired outcome: the problem is this line I don't know how to get the index from the one column that's currently looked at in the for loop and get the value of the same index of the other column. The current error I get is this: So I get the right value of the first iteration of the for loop but how do I get the index of that to apply to the other column? Or is there maybe another way to filter the dataframe by my word list maybe?", "q_apis": "get columns columns filter at time get value index value second filter compare get index at get value index get get right value first get index apply filter", "apis": "apply any", "code": ["word_list = [\"elit\", \"tempor\", \"aliqua\"]\n\ndf = df[df[\"sentence\"].apply(lambda x: any(w in x for w in word_list))]\nprint(df)\n", "  letter                     sentence\n1      C  consectetur adipiscing elit\n2      S        sed do eiusmod tempor\n4      D         dolore magna aliqua.\n"], "link": "https://stackoverflow.com/questions/66715094/python-pandas-get-index-of-value-from-one-column-to-apply-to-another-in-datafram"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Dataframe that looks like that: I want to set a specific cell value, for example open bid for date 2010-01-04 so I tried this: but nothing has happened to the dataframe. When I remove at the end values for both bid and ask change but I don't know how to change only one value at a time.", "q_apis": "get columns value date at values value at time", "apis": "at", "code": ["df.at['2010-01-04', ('SPY', 'Open', 'Bid')] = 10\n"], "link": "https://stackoverflow.com/questions/51266795/how-to-set-a-cell-value-in-a-multi-header-multi-index-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Not winning here. Need to use a free text field passed into a dataframe to lookup a different column in a second data frame: imagine lots of data above ^ I've tried a number of methods to arrive at this: This is what I've tried: TRY1 TRY2 TRY3 Needless to say, I am struggling...... apologies for not exactly correct formatting, this is a made up examples and my caffeine levels are dipping", "q_apis": "get columns lookup second at levels", "apis": "DataFrame columns get apply apply", "code": ["import pandas as pd\n\ndf = pd.DataFrame([['Julie', 'Sheets were dirty'],\n                   ['Samantha', 'Meal arrived cold'],\n                   ['Rachel', 'Cocktails were delicious']],\n                  columns=['User', 'Review'])\n\nd = {'Keyword': ['Sheets','Cocktails','Meal'],\n     'Department' : ['Bedrooms','Restaurant','Restaurant'],\n     'Issue Type': ['Beds','Drinks','Food']}\n\nd2 = {key: (dep, iss) for key, dep, iss in \\\n           zip(d['Keyword'], d['Department'], d['Issue Type'])}\n\ndef mapper(x):\n    return d2.get(next((i for i in d2 if i in x), None))\n\ndf[['Department', 'IssueType']] = df['Review'].apply(mapper).apply(pd.Series)\n", "       User                    Review  Department IssueType\n0     Julie         Sheets were dirty    Bedrooms      Beds\n1  Samantha         Meal arrived cold  Restaurant      Food\n2    Rachel  Cocktails were delicious  Restaurant    Drinks\n"], "link": "https://stackoverflow.com/questions/49573396/pandas-use-dataframe-as-dictionary-or-lookup"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am having difficulty mastering pandas special merge functions like . I have two dataframes: - pings from an EV gps, and - other EV attributes such as navigation destination and battery level. My objective is to merge them such that the output dataframe row number equals the sum of both dataframes' number of rows. For example: and should be merged so that the timestamps are in consecutive order, and so that rows should be matched to rows in with the nearest timestamp that comes \"before\". Lastly, , , and should should be filled forward. The output from the examples above would be: I have tried with but this does not produce the correct result, it fills backwards and only keeps records from . What are the correct commands to produce the desired result in ?", "q_apis": "get columns merge merge equals sum timestamp", "apis": "sort_values sort_values", "code": ["(pd.concat([pd.merge_asof(df1, df2, on='ts'), df2])\n   .sort_values('ts')\n)\n", "                          ts      lat       lng     nav_to  battery\n0 2021-01-02 16:08:24.067971  58.3019 -134.4197        NaN      NaN\n1 2021-01-06 12:54:18.535681  58.3021 -134.4195        NaN      NaN\n2 2021-01-08 22:15:35.036423  58.3025 -134.4195        NaN      NaN\n3 2021-01-16 01:10:39.610540  58.3029 -134.4193        NaN      NaN\n0 2021-01-26 12:47:52.972586      NaN       NaN     Juneau     90.0\n4 2021-01-27 12:28:45.202376  58.3030 -134.4197     Juneau     90.0\n5 2021-01-30 05:32:09.404525  58.3031 -134.4190     Juneau     90.0\n6 2021-02-08 10:39:19.686159  58.3033 -134.4187     Juneau     90.0\n1 2021-02-14 23:23:18.186058      NaN       NaN  Anchorage     50.0\n7 2021-02-15 01:30:16.733921  58.3039 -134.4187  Anchorage     50.0\n8 2021-02-16 12:49:55.366025  58.3040 -134.4185  Anchorage     50.0\n2 2021-02-19 07:26:35.357977      NaN       NaN  Fairbanks     30.0\n9 2021-02-19 23:57:57.369978  58.3041 -134.4181  Fairbanks     30.0\n", "(pd.concat([pd.merge_asof(df1, df2, on='ts'), \n            pd.merge_asof(df2, df1, on='ts')\n           ])\n   .sort_values('ts')\n)\n", "                          ts      lat       lng     nav_to  battery\n0 2021-01-02 16:08:24.067971  58.3019 -134.4197        NaN      NaN\n1 2021-01-06 12:54:18.535681  58.3021 -134.4195        NaN      NaN\n2 2021-01-08 22:15:35.036423  58.3025 -134.4195        NaN      NaN\n3 2021-01-16 01:10:39.610540  58.3029 -134.4193        NaN      NaN\n0 2021-01-26 12:47:52.972586  58.3029 -134.4193     Juneau     90.0\n4 2021-01-27 12:28:45.202376  58.3030 -134.4197     Juneau     90.0\n5 2021-01-30 05:32:09.404525  58.3031 -134.4190     Juneau     90.0\n6 2021-02-08 10:39:19.686159  58.3033 -134.4187     Juneau     90.0\n1 2021-02-14 23:23:18.186058  58.3033 -134.4187  Anchorage     50.0\n7 2021-02-15 01:30:16.733921  58.3039 -134.4187  Anchorage     50.0\n8 2021-02-16 12:49:55.366025  58.3040 -134.4185  Anchorage     50.0\n2 2021-02-19 07:26:35.357977  58.3040 -134.4185  Fairbanks     30.0\n9 2021-02-19 23:57:57.369978  58.3041 -134.4181  Fairbanks     30.0\n"], "link": "https://stackoverflow.com/questions/66648356/merge-on-nearest-retrospective-timestamp-and-fill-forward-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe which looks like the following: What I want to do is the following: if I get 2 as input my code is supposed to search for 2 in the dataframe and when it finds it returns the value of the other column. In the above example my code would return 0 and 3. I know that I can simply look at each row and check if any of the elements is equal to 2 but I was wondering if there is one-liner for such a problem. UPDATE: None of the columns are index columns. Thanks", "q_apis": "get columns get value at any columns index columns", "apis": "any stack", "code": ["n_input = 2\n\ndf[(df == n_input).any(1)].stack()[lambda x: x != n_input].unique()\n# array([0, 3])\n"], "link": "https://stackoverflow.com/questions/42796085/finding-elements-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe : I want to analyze each unique value in separate dataframes. To create separate dataframes (=) I use the code: This gives me a subdf like this for every unique value: subdf_1 subdf_2 subdf_3 Instead of returning subdataframes for all my unique values, I'd like subdataframes to be created for only the 3 most common values (i.e. 50, 30, 20 for the example above). How can I adjust my code above to get to this result? Thank you.", "q_apis": "get columns unique value unique value all unique values values get", "apis": "value_counts index eq", "code": ["idx = df['value'].value_counts().index[:3]\nresults = {f'subdf_{i}': df[df['value'].eq(v)] for i, v in enumerate(idx, 1)}\n", "print(results['subdf_1'])\n\n   index  value  value_count\n5      5     30            2\n6      6     30            2\n"], "link": "https://stackoverflow.com/questions/63192897/adjust-for-loop-to-only-return-values-based-on-frequency-of-occurence-in-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So, I have indexes in range data frame. I want to use them to find values in test dataframe and extract values from into new data frame. My current code is:", "q_apis": "get columns values test values", "apis": "columns join", "code": ["cols = __test__.columns\n\ndf = __range__.join(__test__, how='inner')\n\ndf=df[cols]\n"], "link": "https://stackoverflow.com/questions/61404150/searching-data-frame-by-indexes-and-exporting-information"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am performing an outer join on two DataFrames: This gives the result Is it possible to perform the outer join such that the -columns are concatenated? In other words, how to perform the join such that we get the DataFrame where all have been set to .", "q_apis": "get columns join join columns join get DataFrame where all", "apis": "merge assign fillna fillna drop merge assign filter fillna values sum drop", "code": ["df = (pd.merge(df1, df2, on=[\"id\",\"date\"], how=\"outer\", suffixes=('','_'))\n        .assign(str=lambda x: x['str'].fillna('') + x['str_'].fillna(''))\n        .drop('str_', 1))\n", "df = (pd.merge(df1, df2, on=[\"id\",\"date\"], how=\"outer\", suffixes=('','_'))\n        .assign(str=lambda x: x.filter(like='str').fillna('').values.sum(axis=1))\n        .drop('str_', 1))\n", "print (df)\n   date  id str\n0     4   1  aA\n1     5   2  bB\n2     6   3  cC\n3     7   4  dD\n4     8   5   e\n5     8   6   Q\n"], "link": "https://stackoverflow.com/questions/49206879/performing-outer-join-that-merges-joined-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with paths of images. I am performing object detection and want to save the names of the detected objects in a new column (named ) of the dataframe. However the code works fine when only 1 object is detected, when there is a list of multiple objects the code gives an error. I think there is some problem with my way of inserting values in the dataframe. I have actually created an empty column first and then using loop inserting vaues in the column. Code Output of df.info()", "q_apis": "get columns names values empty first info", "apis": "all empty append", "code": ["train_image_paths = 'data/train_images/' + df['image']    # train_images is the folder containing images and \n                                                          #  df['image'] contains all image paths\n\nobjects = []     # Creating an empty column for storing detected objects\n\nfor idx, img in enumerate(train_image_paths):\n    ..\n    #object detection code    # Performs object detection and \n                              # stores the detected objects in a list named detected_objects\n    ..\n    \n    objects.append(detected_objects)   # Adding the detected objects to the dataframe\n\n\ndf['objects'] = objects\nreturn df\n"], "link": "https://stackoverflow.com/questions/67053463/python-valueerror-inserting-a-list-of-values-using-df-loc"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm doing a calculation on a DataFrame and then want to scale the results. I keep getting errors about expecting a 2D array and to \"Reshape your data either using array.reshape(-1, 1) if your data has a single feature\" I'm expecting a resulting DataFrame or Series with the original index intact and a single column of scaled results.", "q_apis": "get columns DataFrame array array DataFrame Series index", "apis": "DataFrame values values values values values", "code": ["series = pd.DataFrame({'values':series})\n\n    values\na   \naaa 0.625\nbbb 0.700\nccc 0.750\n\nseries['values'] = scaler.fit_transform(series[['values']])\n\n    values\na   \naaa -1.297771\nbbb 0.162221\nccc 1.135550\n"], "link": "https://stackoverflow.com/questions/64884874/scaling-pandas-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have my data as say - In e code column, it is the employee code and there is some code which I want to delete the row for these codes only like - Gs01, Fireman9801 because it contains string and some characters which is not a genuine employee code, and deleted row must be stored in any other dataframe say so my OP should be - - - Please help me to achieve the above result using python, thanks in advance.", "q_apis": "get columns delete codes contains any", "apis": "notnull", "code": [">>> df1.to_dict()\n\n{'e code': {0: 'Gs01', 1: 44325, 2: 889, 3: 'Fireman\\\\9801'},\n 'training': {0: 'safety',\n  1: 'eLearning_since_2001',\n  2: 'air exhaust_open_chamber',\n  3: 'internal_fireman'}}\n\n>>> df1\n\n         e code                  training\n0          Gs01                    safety\n1         44325      eLearning_since_2001\n2           889  air exhaust_open_chamber\n3  Fireman\\9801          internal_fireman\n", "new = df1[pd.to_numeric(df1['e code'], errors='coerce').notnull()]\n", "  e code                  training\n1  44325      eLearning_since_2001\n2    889  air exhaust_open_chamber\n"], "link": "https://stackoverflow.com/questions/67211071/delete-rows-if-found-string-or-special-character-in-a-particular-column-in-pytho"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 65 K records such as below snippet in my dataframe: Within the Same Scrip And the Same Date (from the field Timestamp1), I would like to query all the records and return records which Satisfy 2 complex conditions. These conditions are: a)The NSEPr value should be at least 3.5 % Higher than the First value of NSEPr for That DAY (Day can be extracted from Timetamp1 here) b)The Sum of Values for SellQ1 + SellQ2.. (tillSell 5) should be 3 times (or Higher than the Sum of Values for BuyQ1 + BuyQ2.. (tillBuyQ5). I managed to extract the Date from timestamp1 using df['mydt'] = df.Timestamp1.dt.date.. I tried achieving the above task using for loop with df.iterrows(), i.e. iterating across the Df. This failed due to an endless loop.. I remember the above is achievable using df.groupby['Scrip','mydt'].apply Or perhaps by using df.groupby['scrip','mydt'].apply(lambda x However I am not able to find the solution to this. I will really appreciate some help on the above. TIA.", "q_apis": "get columns query all value at value date iterrows groupby apply groupby apply", "apis": "get first values groupby agg first sum columns sum columns copy merge apply merge columns", "code": ["# get the first values per scrip and day\ndf_a_first_vals= df.groupby([df['Timestamp1'].dt.date, df['Scrip']]).agg({'NSEPr': 'first'})\n\n# create an indexer for condition b and extract the\n# corresponding data with the date stored in a separate\n# column\ndf_b_indexer= df[['Sellq1', 'Sellq2', 'Sellq3', 'Sellq4', 'Sellq5']].sum(axis='columns') >= df[['Buyq1', 'Buyq2', 'Buyq3', 'Buyq4', 'Buyq5']].sum(axis='columns')*3\ndf_b_data= df[df_b_indexer].copy(deep=True)\ndf_b_data['Timestamp1_date']= df_b_data['Timestamp1'].dt.date\n\n# merge a and b to apply condition a\ndf_ab_merged= df_b_data.merge(df_a_first_vals, left_on=['Timestamp1_date', 'Scrip'], right_index=True, suffixes=['', '_first'])\n\n# output the result\ndf_ab_merged[df_ab_merged['NSEPr']>=df_ab_merged['NSEPr_first']*1.035]\n", "Out[148]: \n  Scrip          Timestamp1               NSETS  NSEPr  ...  TotalBuyQty  TotalSellQty  Timestamp1_date  NSEPr_first\n7  APEX 2018-01-31 11:56:14 2018-01-31 11:54:48   38.0  ...        23399         624.0       2018-01-31         20.0\n\n[1 rows x 29 columns]\n", "raw=\"\"\"Scrip   Timestamp1          NSETS               NSEPr Buyq1 Buyq2   Buyq3   Buyq4   Buyq5   Sellq1  Sellq2  Sellq3  Sellq4  Sellq5  Sellp1  Sellp2  Sellp3  Sellp4  Sellp5  buyp1   buyp2   buyp3   buyp4   buyp5   ActPr   TotalBuyQty TotalSellQty    \nALANKIT 2018-01-12T13:02:06 2018-01-12T13:00:50 78.10   759.00  100.00  996.00  1287.00 200 15.00   300.00  100.00  1787.00 5614.00 78.25   78.35   78.40   78.45   78.50   78.10   78.05   78.00   77.80   77.75   78.25   63928   194206  \nALANKIT 2018-01-12T13:32:29 2018-01-12T13:22:21 79.50   28.00   100.00  200.00  1288.00 248 50.00   178.00  898.00  100.00  487.00  79.50   79.55   79.60   79.65   79.75   79.30   79.15   79.10   79.05   78.80   79.20   61927   175983  \nALANKIT 2018-01-12T13:36:26 2018-01-12T13:34:51 79.20   39.00   3649.00 1287.00 7.00    11  1500.00 1024.00 1000.00 220.00  65.00   79.20   79.25   79.50   79.55   79.60   79.15   79.00   78.85   78.65   78.55   79.00   65503   176990  \nALANKIT 2018-01-12T14:32:29 2018-01-12T14:31:23 78.80   810.00  1000.00 1287.00 1342.00 555 58.00   20.00   100.00  10.00   1250.00 78.80   78.85   78.90   78.95   79.00   78.70   78.60   78.55   78.50   78.30   78.70   84405   184759  \nALANKIT 2018-01-12T14:12:58 2018-01-12T14:11:22 78.50   1.00    5.00    100.00  25.00   510 2542.00 25.00   95.00   50.00   500.00  78.50   78.55   78.60   78.85   78.90   78.30   78.25   78.20   78.15   78.10   78.85   74505   189866  \nAPEX    2018-03-05T14:14:30 2018-03-05T14:13:23 72.00   51.00   71.00   20.00   150 1.00    1.00    14.00   20.00   1108.00 690.00  690.15  690.80  690.95  691.00  689.60  689.55  689.45  689.15  689.00  0   35535   61963   690.00\nAPEX    2018-01-31T11:52:11 2018-01-31T11:50:48 20.00   10.00   10.00   15.00   50  50.00   50.00   10.00   16.00   67.00   621.15  621.20  621.40  621.80  621.95  619.50  619.00  618.00  617.00  616.50  0   8083    25609   619.50\nAPEX    2018-01-31T11:56:14 2018-01-31T11:54:48 38.00   29.00   67.00   174.00  124 53.00   50.00   50.00   16.00   25.00   625.00  625.40  625.45  626.00  626.90  623.95  623.90  623.50  623.45  623.00  0   12587   23399   624.00\nAPEX    2018-01-18T09:36:03 2018-01-18T09:35:14 38.00   46.00   67.00   226.00  6   5.00    50.00   36.00   20.00   30.00   781.00  781.80  781.85  781.95  782.00  780.20  780.15  780.05  780.00  779.95  782.70  17023   21946   780.75\nAPEX    2018-01-18T09:44:16 2018-01-18T09:42:15 47.00   50.00   25.00   67.00   2887    25.00   8.00    58.00   5.00    50.00   791.60  791.65  791.95  792.30  792.65  790.20  790.15  790.00  789.05  789.00  791.45  22314   26007   790.05\nSTRTECH 2018-01-19T14:57:51 2018-01-19T14:56:24 20.50   1.00    5.00    2.00    3   3.00    20.00   3.00    5.00    10.00   2484.95 2485.00 2489.00 2489.90 2490.00 2477.55 2477.50 2477.20 2477.05 2476.70 2480.60 32408   8565    2485.00\nSTRTECH 2018-01-19T15:50:10 2018-01-25T10:47:46 32.65   1.00    511.00  1.00    12  9.00    5.00    100.00  23.00   20.00   2484.60 2484.70 2484.80 2485.00 2486.00 2480.15 2480.10 2480.00 2475.00 2471.15 2534.60 28306   18002   2484.70\"\"\"\n\ndf= pd.read_csv(io.StringIO(raw), sep='\\s+', parse_dates=['Timestamp1', 'NSETS'], index_col=None)\n", "Out[212]: \n      Scrip          Timestamp1               NSETS  NSEPr  ...  TotalBuyQty  TotalSellQty  Timestamp1_date  NSEPr_first\n11  STRTECH 2018-01-19 15:50:10 2018-01-25 10:47:46  32.65  ...        18002        2484.7       2018-01-19         20.5\n\n[1 rows x 29 columns]\n"], "link": "https://stackoverflow.com/questions/57542174/pandasall-results-which-fulfill-multiple-conditions-within-a-categories"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames SF and OF. SF: OF: What I want to do is to add an extra row before each duplicated row present in SF and append it to the OF data frame. For example, if there are duplicates in a parent like 2,2,3,3 the first row of 2 and the second row of 2 needs to be copied, and in addition to all rows, there have to be an extra one added before them with info as in the description. So the end result should look like Result (SF rows appended in OF): What I have done so far is taken the unique values and stored them in an array, then I have used a loop to create a row and append the needed information, my code looks like this However, I am getting a result like I know what is wrong just have no idea how to fix it? How can I get the results mentioned above. Thanks", "q_apis": "get columns add duplicated append first second all info unique values array append get", "apis": "DataFrame all duplicated duplicated loc copy rename columns columns columns columns copy drop first assign drop_duplicates first copy append first index append sort_index append append", "code": ["# create a DataFrame keep SF all duplicated rows\ncond = SF.duplicated(keep=False)\ndf = SF.loc[cond, ['ParentPartNumber', 'Webname']].copy()\ndf['Published'] = 0\n\n# rename columns, and sorted columns\ndf.columns = ['SKU', 'Name', 'Published']\ndf = df[OF.columns].copy()\n\n# drop duplicates(keep the first duplicate row), and assign 1 to Published column\ndfn = df.drop_duplicates(keep='first').copy()\ndfn['Published'] = 1\n\n# append first duplicate row to df, and sort index to keep row order\ndfn = dfn.append(df).sort_index()\n\n# append the duplicates rows to OF\nprint(OF.append(dfn))\n", "  SKU  Published   Name\n0    2          1   Sidi\n1    4          1    Bec\n2    8          1    Lin\n0    2          1   Sidi\n0    2          0   Sidi\n1    2          0   Sidi\n2    2          0   Sidi\n3    3          1  Shoei\n3    3          0  Shoei\n4    3          0  Shoei\n"], "link": "https://stackoverflow.com/questions/65486589/insert-new-row-before-each-duplicated-id"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two tables in Python pandas df1 and df2. DataFrame -df1(100 records). df2(2000 records) df1 having having unique entries of customer and credit limit and df2 having multiple transaction records against customers. now I have to check in df2 against each customers and amount, In df2 if any Customer's transaction Amount is greater than limit of df1 than replace it with Limit value of df1. In nutshell-- if Customer A1 and Amount in df2 is greater than Limit of customer A1 in df1 than replace value of Amount in df2 with value of limit from df1. desired output is : df3:", "q_apis": "get columns DataFrame unique now any replace value replace value value", "apis": "merge loc drop", "code": ["df3 = df2.merge(df1[['Customer','Limit']], how='left', on='Customer')\ndf3.loc[df3['Amount']>df3['Limit'], 'Amount'] = df3['Limit']\ndf3.drop('Limit', axis=1, inplace=True)\n", "   No: Customer      Month       Type     Amount\n0    1       A1  12-Jan-04  JEWELLERY  500000.00\n1    2       A1   3-Jan-04      PETRO  410556.13\n2    3       A1  15-Jan-04    CLOTHES   23740.46\n3    4       A3  25-Jan-04       FOOD    8000.47\n4    5       A3  17-Jan-05     CAMERA   10000.00\n"], "link": "https://stackoverflow.com/questions/59594404/search-one-value-of-dataframe-and-replace-with-reference-value-in-another-datafr"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "When I try to read each element in a column of Dataframe, I cannot use replace to change a old substring into a new substring, the text will have the same value after using replace.", "q_apis": "get columns replace value replace", "apis": "DataFrame replace apply", "code": ["import pandas as pd\n\ndf = pd.DataFrame({\n    'r.name': ['ABCD', 'DEFG', 'OMNP'],\n    'main_ingreds': ['A', 'FG', 'NP']\n})\n\ndef func(row):\n    for token in row['main_ingreds'].split(','):\n        row['r.name'] = row['r.name'].replace(token, \" \")\n    return row\n\ndf = df.apply(func, axis=1)\n\nprint( df['r.name'] )\n"], "link": "https://stackoverflow.com/questions/62909621/cannot-replace-string-when-reading-from-a-dataframe-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Assuming I have the following DataFrame: I want to remove the duplicate rows with respect to column A, and I want to retain the row with value 'PhD' in column B as the original, if I don't find a 'PhD', I want to retain the row with 'Bs' in column B. I am trying to use with a condition", "q_apis": "get columns DataFrame value", "apis": "index", "code": [">>> df\n    A   B\n0   1   Ms\n1   1   Ms\n2   1   Ms\n3   1   Ms\n4   1   PhD\n5   2   Ms\n6   2   Ms\n7   2   Bs\n8   2   PhD\n", "def sort_df(df, column_idx, key):\n    '''Takes a dataframe, a column index and a custom function for sorting, \n    returns a dataframe sorted by that column using that function'''\n    \n    col = df.ix[:,column_idx]\n    df = df.ix[[i[1] for i in sorted(zip(col,range(len(col))), key=key)]]\n    return df\n", "cmp = lambda x:2 if 'PhD' in x else 1 if 'Bs' in x else 0", "    A   B\n4   1   PhD\n8   2   PhD\n"], "link": "https://stackoverflow.com/questions/33042777/removing-duplicates-from-pandas-dataframe-with-condition-for-retaining-original"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe : I have a line of code which allows me to extract up until a certain maturity ( is an input I give): However the problem is that say if I want to get up to the 20Y and put as 20Y, it will only get up to 15Y. Is there a way where it will extract all the rows up to and including the 20Y row?", "q_apis": "get columns get put get where all", "apis": "index shift iloc", "code": ["max_maturity = '20Y'\n#if need extract 20\nmax_maturity = int(''.join(filter(str.isdigit, max_maturity)))\n\nmax_maturity = 20\n#remove Y\ndf = df[df['maturity'].str.replace('Y','').astype(int) <= max_maturity]\n#get numbers only\n#df = df[df['maturity'].str.extract('(\\d+)', expand=False).astype(int) <= max_maturity]\n\nprint (df)\n   maturity  spot rate\n0        1Y      0.182\n1        2Y      0.202\n2        3Y      0.284\n3        4Y      0.426\n4        5Y      0.585\n5        6Y      0.745\n6        7Y      0.892\n7        8Y      1.021\n8        9Y      1.130\n9       10Y      1.224\n10      12Y      1.375\n11      15Y      1.522\n12      20Y      1.653\n", "idx = data.index[data.maturity.str.contains(max_maturity,na=False).shift(fill_value=False)]\n\ndata = data.iloc[: idx[0]]\n"], "link": "https://stackoverflow.com/questions/66673789/getting-subset-of-dataframe-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a table with scores for each product that needed to be sold for 10 days and availability of each product (totally number of products = 10) Product availability First I had to rank each product and prioritize what I need to sell for each day. I was able to do that by That gives me now comes the hard part how do i create a table that contains the product to be sold for each day looking at the Max1 column but also keeping track of the availability. If the product is not available then chose the next maximum. The final df should look like this. Breaking my head over this. Any help is appreciated. Thanks.", "q_apis": "get columns product days product rank product day now contains product day at product head", "apis": "product product values append product product values product values append product product values product values append product product values product values append product product values DataFrame product values append product product values DataFrame", "code": ["import pandas as pd\ndf1=pd.read_csv('file1',sep='\\s+',header=None,names=['product','available'])\nprint df1\ndf2=pd.read_csv('file2',sep='\\s+')\nprint df2\n\nmaxy=[]\nfor i in range(len(df2)):\n    if df1['available'][df1['product']==df2['Max1'][i]].values[0]>0:\n        maxy.append(df2['Max1'][i])\n        df1['available'][df1['product']==df2['Max1'][i]]=df1['available'][df1['product']==df2['Max1'][i]].values[0]-1\n    elif df1['available'][df1['product']==df2['Max2'][i]].values[0]>0:\n        maxy.append(df2['Max2'][i])\n        df1['available'][df1['product']==df2['Max2'][i]]=df1['available'][df1['product']==df2['Max2'][i]].values[0]-1\n    elif df1['available'][df1['product']==df2['Max3'][i]].values[0]>0:\n        maxy.append(df2['Max3'][i])\n        df1['available'][df1['product']==df2['Max3'][i]]=df1['available'][df1['product']==df2['Max3'][i]].values[0]-1\n    elif df1['available'][df1['product']==df2['Max4'][i]].values[0]>0:\n        maxy.append(df2['Max4'][i])\n        df1['available'][df1['product']==df2['Max4'][i]]=df1['available'][df1['product']==df2['Max4'][i]].values[0]-1\n    else:\n        print (\"Check\")\n\npd.DataFrame(maxy)    \n", " product  available\n0       A          3\n1       B          2\n2       C          3\n3       D          2\n  Max1 Max2 Max3 Max4\n0    D    B    A    C\n1    A    C    D    B\n2    D    B    C    A\n3    D    A    C    B\n4    C    D    A    B\n5    D    A    B    C\n6    A    C    C    B\n7    D    B    C    A\n8    B    C    A    D\n9    D    A    B    C\n\n\n\n    0\n0   D\n1   A\n2   D\n3   A\n4   C\n5   A\n6   C\n7   B\n8   B\n9   C\n", "cols = list(df2)\nmaxy=[]\n\nfor i in range(len(df2)):\n    for x in cols:\n        if df1['available'][df1['product']==df2[x][i]].values[0]>0:\n            maxy.append(df2[x][i])\n            df1['available'][df1['product']==df2[x][i]]=df1['available'][df1['product']==df2[x][i]].values[0]-1\n            break\n\nfinal=pd.DataFrame(maxy)\nprint(final)\n"], "link": "https://stackoverflow.com/questions/60244154/how-to-create-a-multi-conditional-1d-dataframe-from-a-multi-dimensional-datafram"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 Dataframes and I want to remove the rows from dataframe who have the value of greater than the respective field from dataframe. The problem is that both dataframes have multiple values for and the total number of rows of both dataframes are unequal. The code that I used works only if both dataframes have equal number of rows, otherwise it produces an error. Part of badges Part of test_df Code Error Required Output", "q_apis": "get columns value values", "apis": "product", "code": ["badges[\"Date\"] = pd.to_datetime(badges[\"Date\"])\ntest_df[\"date\"] = pd.to_datetime(test_df[\"date\"])\n", "valid_dates = {\n    badge_date\n    for badge_date, test_df_date in product(badges.Date.unique(), test_df.date.unique())\n    if badge_date < test_df_date\n}\n", "badges.loc[badges[\"Date\"].isin(valid_dates)]\n\n    UserId  Date\n0   101     2009-09-01 15:17:50.660\n1   101     2009-09-01 15:17:50.660\n2   101     2009-09-02 15:17:50.660\n3   102     2009-09-03 15:17:50.660\n6   104     2009-09-30 15:17:50.660\n"], "link": "https://stackoverflow.com/questions/63844846/how-to-compare-2-dataframes-of-unequal-length-based-on-datetime-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "By default, columns are all set to zero. Make entry as 1 at (row,column) where column name string present on URL column L # list that contains column names used to check if found on URL Dataframe Image I am a beginner, it throws and error: /usr/local/lib/python3.6/dist-packages/pandas/core/series.py in f(x) 4195 4196 def f(x): -> 4197 return func(x, *args, **kwds) 4198 4199 else: TypeError: generate() takes 2 positional arguments but 9 were given Any suggestions would be helpful Edit 1: after, got error: Edit 2: \"I missed to emphasize on URL column in for loop code will rectify that\" Edit 3: Updated and fixed to, Thanks for all the support!", "q_apis": "get columns columns all at where name contains names all", "apis": "DataFrame columns apply", "code": ["df = pd.DataFrame([['xyz', 'US'],['abc', 'MX'],['xyz', 'CA']], columns = [\"Name\", \"Country\"])\n\nprint(df)\n\nName    Country\nxyz     US\nabc     MX\nxyz     CA\n", "def generate(statement,col):\n    if statement.find(col) == -1:\n        return 0\n    else:\n        return 1\n", "for col in L:\n    print(df[col].apply(generate, args=(col)))\n\n\nTypeError: generate() takes 2 positional arguments but 5 were given\n\n"], "link": "https://stackoverflow.com/questions/64930073/type-error-pandas-dataframe-apply-function-argument-passing"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe with roughly 150,000,000 rows in the following format: I want to aggregate it by ID & TERM, and count the number of rows. Currently I do the following: But this takes roughly two minutes. The same operation using R data.tables takes less than 22 seconds. Is there a more efficient way to do this in python? For comparison, R data.table:", "q_apis": "get columns aggregate count seconds", "apis": "values T any diff DataFrame columns", "code": ["def groupby_count(df):\n    unq, t = np.unique(df.TERM, return_inverse=1)\n    ids = df.ID.values\n    sidx = np.lexsort([t,ids])\n\n    ts = t[sidx]\n    idss = ids[sidx]\n\n    m0 = (idss[1:] != idss[:-1]) | (ts[1:] != ts[:-1])\n    m = np.concatenate(([True], m0, [True]))\n    ids_out = idss[m[:-1]]\n    t_out = unq[ts[m[:-1]]]\n    x_out = np.diff(np.flatnonzero(m)+1)\n\n    out_ar = np.column_stack((ids_out, t_out, x_out))\n    return pd.DataFrame(out_ar, columns = [['ID','TERM','X']])\n", "def groupby_count_v2(df):    \n    a = df.values\n    sidx = np.lexsort(a[:,:2].T)\n    b = a[sidx,:2]\n    m = np.concatenate(([True],(b[1:] != b[:-1]).any(1),[True]))\n    out_ar = np.column_stack((b[m[:-1],:2], np.diff(np.flatnonzero(m)+1)))\n    return pd.DataFrame(out_ar, columns = [['ID','TERM','X']])\n", "In [332]: df\nOut[332]: \n   ID TERM   X\n0   1    A   0\n1   1    A   4\n2   1    A   6\n3   1    B   0\n4   1    B  10\n5   2    A   1\n6   2    B   1\n7   2    F   1\n\nIn [333]: groupby_count(df)\nOut[333]: \n  ID TERM  X\n0  1    A  3\n1  1    B  2\n2  2    A  1\n3  2    B  1\n4  2    F  1\n", "In [339]: df1 = df.iloc[np.random.permutation(len(df))]\n\nIn [340]: df1\nOut[340]: \n   ID TERM   X\n7   2    F   1\n6   2    B   1\n0   1    A   0\n3   1    B   0\n5   2    A   1\n2   1    A   6\n1   1    A   4\n4   1    B  10\n\nIn [341]: groupby_count(df1)\nOut[341]: \n  ID TERM  X\n0  1    A  3\n1  1    B  2\n2  2    A  1\n3  2    B  1\n4  2    F  1\n"], "link": "https://stackoverflow.com/questions/47098571/most-efficient-way-to-groupby-aggregate-for-large-dataframe-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've been trying to debug my code for the last 30 minutes to no avail, maybe you can help? The error lies withing line 17, but I'm truly clueless to why it's happening. So, I'm using a dictionary in order to store the data that my function returns, which is a type. I iterate through it and insert all of the values from to . I then insert my dictionary into a with the library. Then I iterate through my dataFrame in order to retrieve the count of the values that are set to true. But the problem is that doesn't seem to be working, even though I used the same function on a different dictionary and dataframe set. Expected result : what I get:", "q_apis": "get columns last insert all values insert count values get", "apis": "iterrows columns", "code": ["for row in df2.iterrows():\n    if (row[1][df.columns.get_loc('nonTrivial') == True):\n        n = n+1\n"], "link": "https://stackoverflow.com/questions/54186567/dataframe-cant-be-iterated-through-getting-following-error-tuple-indices-must"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I have a DataFrame with 180000+ values and I need to (1) replace duplicate and certain values in cells by row and (2) rearrange. Here is my DataFrame, df: nan values represent np.nan. And forbidden string is 'not'. So what I need to do is check columns item1~6 replace strings that are contained in the makrc column with nan. As well, I also want to replace 'not's' with nan's. After replacing strings to np.nan, I need to rearrange the item1~6 to left justify non-nan data to the leftmost empty cell, as shown below, (expected output): So as you can see in a first index, I have removed apt string in item2 and changed to np.nan because same string is in makrc column. In index 1, I have removed rye and replace with np.nan. But this time, I rearranged the 'app' string from item2 to item1 because np.nan values should come after the values. In index 2, I have replaced pro and not since I need to replace every 'not'string in the item columns to np.nan. Also I have rearranged the items. I've tried combining all item columns as a list and replacing it, but there are a few rows with only np.nan items. Can you guys recommend an ideal process to solve my problem? Thank you so much.", "q_apis": "get columns DataFrame values replace values DataFrame values columns replace replace left empty first index index replace time values values index replace item columns items all item columns items", "apis": "columns iloc isin mask notnull mask mask shape mask T T T mask T", "code": ["m = df.columns.str.contains('item')\ni = df.iloc[:, m]\n", "j = i[~i.isin(df.makrc.tolist() + ['not'])]\n", "df.loc[:, m] = j.apply(sorted, key=pd.isnull, axis=1)\ndf\n\n    key  sellyr brand makrc item1 item2  item3  item4  item5  item6\n0  da12    2013   imp   apt  furi   NaN    NaN    NaN    NaN    NaN\n1  da32    2013    sa   rye   app   NaN    NaN    NaN    NaN    NaN\n2  da14    2013    sa   pro   pan   fan    NaN    NaN    NaN    NaN\n", "i\n\n  item1 item2 item3 item4  item5  item6\n0  furi   apt   NaN   NaN    NaN    NaN\n1   rye   app   NaN   NaN    NaN    NaN\n2   not   pro   pan   fan    NaN    NaN\n", "j\n\n  item1 item2 item3 item4  item5  item6\n0  furi   NaN   NaN   NaN    NaN    NaN\n1   NaN   app   NaN   NaN    NaN    NaN\n2   NaN   NaN   pan   fan    NaN    NaN\n", "def justify(a, invalid_val=0, axis=1, side='left'):    \n    \"\"\"\n    Justifies a 2D array\n\n    Parameters\n    ----------\n    A : ndarray\n        Input array to be justified\n    axis : int\n        Axis along which justification is to be made\n    side : str\n        Direction of justification. It could be 'left', 'right', 'up', 'down'\n        It should be 'left' or 'right' for axis=1 and 'up' or 'down' for axis=0.\n\n    \"\"\"\n\n    if invalid_val is np.nan:\n        mask = pd.notnull(a)\n    else:\n        mask = a!=invalid_val\n    justified_mask = np.sort(mask,axis=axis)\n    if (side=='up') | (side=='left'):\n        justified_mask = np.flip(justified_mask,axis=axis)\n    out = np.full(a.shape, invalid_val, dtype=object) \n    if axis==1:\n        out[justified_mask] = a[mask]\n    else:\n        out.T[justified_mask.T] = a.T[mask.T]\n    return out\n", "df.loc[:, m] = justify(j.values, invalid_val=np.nan, axis=1, side='left')\ndf\n\n    key  sellyr brand makrc item1 item2  item3  item4  item5  item6\n0  da12    2013   imp   apt  furi   NaN    NaN    NaN    NaN    NaN\n1  da32    2013    sa   rye   app   NaN    NaN    NaN    NaN    NaN\n2  da14    2013    sa   pro   pan   fan    NaN    NaN    NaN    NaN\n"], "link": "https://stackoverflow.com/questions/47897166/left-justify-string-values-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame like this: df1 another dataframe as : I want to filter df2 based on combination of 'Names' and 'subject' in df1. If a particular combination of 'Name' and 'subject' in df1 appears more than once and then it is matched in df2. If it matches then we get those rows from df2 as output. Desired output: can anyone help without using 'merge' option?", "q_apis": "get columns DataFrame filter get merge", "apis": "duplicated duplicated merge columns columns loc duplicated duplicated merge", "code": ["df11 = df1[df1.duplicated(subset=['Name','subject'], keep=False) & \n           df1.duplicated(subset=['Name','subject'])]\n\ndf3 = df11.merge(df2, suffixes=('_',''))[df2.columns]\nprint (df3)\n   Name  subject\n0  nick      geo\n1  juli    maths\n2   Tom  science\n", "cols = df2.columns\ndf11 = df1.loc[df1[cols].duplicated(keep=False) & df1[cols].duplicated(), cols]\ndf3 = df11.merge(df2)\nprint (df3)\n   Name  subject\n0  nick      geo\n1  juli    maths\n2   Tom  science\n    \n"], "link": "https://stackoverflow.com/questions/64204505/filter-dataframes-based-on-2-columns-of-another-dataframe-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have several pandas Dataframes Each of them contains column of timestamps and column of respective values. For example: df1: df2: I'm not sure if I put these into words correctly, so I need to combine these datafiles into one where there will be only one timestamp column and other columns will contain the measurement values. I'm thinking about something that would take the timestamp with the smallest increment and insert the other rows to their respective places. For the timestamp value that contains one measurement but not the other it will put Nan or keep it empty. The expected output would be something like this: So how can I do this? Any suggestions or comments are highly appreciated.", "q_apis": "get columns contains values put combine where timestamp columns values take timestamp insert timestamp value contains put empty", "apis": "sort_values sort_index", "code": ["                    timestamp            value1\n0    2015-01-01T15:41:10.500Z    9239.337890625\n1    2015-01-01T15:41:50.543Z    9539.337890625\n2    2015-01-01T15:42:30.600Z    8957.0458984375\n3    2015-01-01T15:43:00.606Z    8237.0458984375\n", "df_all = pd.concat([df1, df2, df3, ...], ignore_index = True)\ndf_all.sort_values(by='timestamp', inplace = True)\n", "                                     value1\n2015-01-01T15:41:10.500Z    9239.337890625\n2015-01-01T15:41:50.543Z    9539.337890625\n2015-01-01T15:42:30.600Z    8957.0458984375\n2015-01-01T15:43:00.606Z    8237.0458984375\n", "df_all = pd.concat([df1, df2, df3, ...])\ndf_all.sort_index(inplace = True)\n"], "link": "https://stackoverflow.com/questions/54832400/how-to-combine-two-pandas-dataframes-with-different-timestamp-densities"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with values for Turkish provinces: I want to write an if loop or a function that can create a new column that would categorize each of these provinces by regions. Therefore, I create 7 lists which contain the provinces that are included in each of the 7 regions: After having done that, I loop through the dataset with a for and if loop: But all I end up getting is all my columns with values \"something\" for some reason. I tried some examples which suggest using a function instead: But the result is similarly off for me: I have the feeling that I am doing some seriously stupid mistake which can be easily fixed but can't figure it out. Would be very grateful to anyone who could help!", "q_apis": "get columns values all all columns values", "apis": "items", "code": ["In [2511]: medite=['Adana', 'Antalya', 'Mersin']\nIn [2508]: blacksea = ['Amasya', 'G\u00fcm\u00fc\u015fhane', 'Bart\u0131n','Zonguldak']\n\nIn [2512]: province_map = {'medite': medite, 'blacksea':blacksea}\n\nIn [2513]: print(province_map)\nOut[2513]: \n{'medite': ['Adana', 'Antalya', 'Mersin'],\n 'blacksea': ['Amasya', 'G\u00fcm\u00fc\u015fhane', 'Bart\u0131n', 'Zonguldak']}\n", "In [2514]: d = {i: k for k,v in province_map.items() for i in v}\n\nIn [2515]: print(d)\nOut[2515]: \n{'Adana': 'medite',\n 'Antalya': 'medite',\n 'Mersin': 'medite',\n 'Amasya': 'blacksea',\n 'G\u00fcm\u00fc\u015fhane': 'blacksea',\n 'Bart\u0131n': 'blacksea',\n 'Zonguldak': 'blacksea'}\n", "In [2518]: df['Region'] = df.province.map(d)\n\nIn [2519]: df\nOut[2519]: \n          province    Region\n2078982      Adana    medite\n2078983      Adana    medite\n2078984      Adana    medite\n2078985      Adana    medite\n2078986      Adana    medite\n2210113  Zonguldak  blacksea\n2210114  Zonguldak  blacksea\n2210115  Zonguldak  blacksea\n2210116  Zonguldak  blacksea\n2210117  Zonguldak  blacksea\n"], "link": "https://stackoverflow.com/questions/64885164/how-to-create-a-new-column-in-pandas-and-set-its-values-according-to-whether-a-s"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following table : FIRST_NAME LAST_NAME id PAUL OLSAN 54 LAURA HINKLE 32 I have to create a documentation for each case so I need the details something like this: (eventually I wanna copy each case details and then write the analysis of it , writing each detail one at a time will take a lot of time ) FIRST_NAME : PAUL LAST_NAME : OLSAN ID :54 FIRST_NAME :LAURA LAST_NAME : HINKLE ID :32", "q_apis": "get columns copy at time take time", "apis": "to_string iterrows", "code": ["_ = [print(f'{i.to_string()}\\n') for idx, i in df.iterrows()]\n", "FIRST_NAME     PAUL\nLAST_NAME     OLSAN\nid               54\n\nFIRST_NAME     LAURA\nLAST_NAME     HINKLE\nid                32\n"], "link": "https://stackoverflow.com/questions/66665933/get-row-values-with-column-names-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to take data that contains time gaps as well as time repeats and basically create a time series using the first occurrence of any given time and filling forwards. Consider the following example. Lets say this is the time range we are interested in: Time 1:00 1:01 1:02 1:03 1:04 1:05 And this is the data, dataframe X, we would like to put into our time series: Occurance Value 1:00 \"R\" 1:03 \"G\" 1:03 \"L\" 1:03 \"P\" 1:03 \"T\" 1:05 \"S\" And this is the Final Dataframe: Occurance Value 1:00 \"R\" 1:01 \"R\" 1:02 \"R\" 1:03 \"G\" 1:04 \"G\" 1:05 \"S\" As you can see, in the Final Dataframe, 1:00 has the value \"R\" because that is the value in the first occurrence of 1:00 in dataframe X. 1:01 and 1:02 also have the value \"R\" because there is no data for those time instances in dataframe X and will therefore use the last valid value (which is the value for 1:00). 1:03 has the value \"G\" because, similar to the case with 1:00, \"G\" is the first value for 1:03 that we have in dataframe X. Since there is no value for 1:04 in dataframe X, 1:04 gets the last valid value, \"G\", in our resulting dataframe. Lastly, 1:05 will have the value \"S\" in our resulting dataframe as that is the value for the first occurrence of 1:05 in dataframe X. What is the quickest way to accomplish this?", "q_apis": "get columns take contains time time time first any time time put time T value value first value time last value value value first value value last value value value first", "apis": "T T set_axis T T T set_axis drop_duplicates", "code": ["df1['Time'] = pd.to_timedelta(df1['Time'] + ':00')\ndf2['Occurance'] = pd.to_timedelta(df2['Occurance'] + ':00')\n", "pd.merge_asof(df1, df2, left_on='Time', right_on='Occurance')\n\n             Time       Occurance Value\n0 0 days 01:00:00 0 days 01:00:00     R\n1 0 days 01:01:00 0 days 01:00:00     R\n2 0 days 01:02:00 0 days 01:00:00     R\n3 0 days 01:03:00 0 days 01:03:00     T\n4 0 days 01:04:00 0 days 01:03:00     T\n5 0 days 01:05:00 0 days 01:05:00     S\n", "pd.merge_asof(df1.set_axis(['Occurance'], axis=1), df2)\n\n        Occurance Value\n0 0 days 01:00:00     R\n1 0 days 01:01:00     R\n2 0 days 01:02:00     R\n3 0 days 01:03:00     T\n4 0 days 01:04:00     T\n5 0 days 01:05:00     S\n", "        Occurance Value\n0 0 days 01:00:00     R\n1 0 days 01:03:00     G  # same Occurance\n2 0 days 01:03:00     L  # same Occurance\n3 0 days 01:03:00     P  # same Occurance\n4 0 days 01:03:00     T  # same Occurance\n5 0 days 01:05:00     S\n", "pd.merge_asof(df1.set_axis(['Occurance'], axis=1),\n              df2.drop_duplicates('Occurance'))\n\n        Occurance Value\n0 0 days 01:00:00     R\n1 0 days 01:01:00     R\n2 0 days 01:02:00     R\n3 0 days 01:03:00     G\n4 0 days 01:04:00     G\n5 0 days 01:05:00     S\n"], "link": "https://stackoverflow.com/questions/66819864/fastest-way-to-create-time-series-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Sample Input DataFrame: Desired DataFrame: As implied above, I have an input DataFrame with n columns; the first one has unique values () and the other n-1 () are some attributes of the rows. I want to generate a new DataFrame with two columns as follows: For each it will have 0 or more rows Starting from the leftmost column, it takes every adjacent pair of not null values e.g. ; the pair can only exist if is null Every pair will be a new row Each column's value is modified like this: value_YYYY where the value remains the same and the YYYY is taken from the column name (e.g. ) Thanks in advance", "q_apis": "get columns DataFrame DataFrame DataFrame columns first unique values DataFrame columns values value where value name", "apis": "columns melt columns columns melt apply values sort_values empty values dropna values shift where values mask eq shift bfill loc mask columns reindex columns", "code": ["# A list of columns to melt.\nvalue_cols = list(df.columns)[1:]\n\n# Melt said columns while leaving the others (in this case only 'Full Name') intact.\ndf = pd.melt(df, id_vars=['Full Name'], value_vars=value_cols)\n\n# Get the year from 'variable'\ndf['variable'] = df['variable'].str.split(' ').apply(lambda x:x[-1])\n\n# Sort the values by 'Full Name' and then year (required).\ndf = df.sort_values(by=['Full Name', 'variable'])\n\n# Drop rows with empty values.\ndf = df.dropna()\n\ndf['Source'] = df['value'] + '_' + df['variable']\n\n# Pair the values (This is why the previous sort is required).\ndf['Target'] = df['Source'].shift(-1)\n\n# Remove rows where the values don't belong to the same name.\nmask = df['Full Name'].eq(df['Full Name'].shift(-1).bfill())\ndf = df.loc[mask]\n\n# Keep only relevant columns.\ndf = df.reindex(columns=['Source', 'Target'])\n"], "link": "https://stackoverflow.com/questions/59469781/return-multiple-rows-per-row-for-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "returns is the dataframe with dates as index, where I want to regress each column individually with the last column (which is ^BSESN) of the dataframe. Since there are more than 700 columns, I want to iterate through the columns. I also want to store the residuals, after each regression. I used various versions of the following but I'm getting the same error constantly. I need to remove the rows which have NaN values during each individual regression (rather than removing rows with any column having nan value). But I keep getting the following error: How can I remove the error? Any guidance on the best way to do this would be much appreciated.", "q_apis": "get columns index where last columns columns values any value", "apis": "columns dropna shape", "code": ["residuals = {}\nfor column in returns.columns[:-1]:\n  selected = returns[['^BSESN', column]].dropna()\n  if selected.shape[0] > 0:\n    reg = sm.OLS(selected[column], selected['^BSESN']).fit()\n    residuals[column] = reg.residuals\n"], "link": "https://stackoverflow.com/questions/62841004/regress-each-column-individually-with-the-last-column-of-a-dataframe-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe in which I wish to perform a time difference operation on a shifted column within a group for work times. For example see data below: I wish to add a column that subtracts the next starttime for a driver in the same vehicle from the current stoptime so as to identify breaks between tasks. But I want to keep the operation within a group of my choosing, in this case driver_id and vehicle. output should look like this: In R this is simple as shown below using : How do I accomplish this in python? I can produce a shifted difference, I just need the addition of the group. See below:", "q_apis": "get columns time difference add between difference", "apis": "groupby shift groupby shift", "code": ["df['break_from_last'] = df['stoptime'] - df.groupby('driver_id')['starttime'].shift()\ndf\n", "  driver_id    veh           starttime            stoptime break_from_last\n0     kg123  10010 2018-12-21 15:17:29 2018-12-21 15:18:57             NaT\n1     kg124  10012 2019-01-01 00:10:16 2019-01-01 00:16:32             NaT\n2     kg124  10012 2019-01-01 00:27:11 2019-01-01 00:31:38 0 days 00:21:22\n3     kg124  10012 2019-01-01 00:46:20 2019-01-01 01:04:54 0 days 00:37:43\n4     kg125  10013 2019-01-01 00:19:06 2019-01-01 00:39:43             NaT\n", "df['break_from_last'] = df['stoptime'] - df.groupby(['driver_id', 'veh'])['starttime'].shift()\n", "  driver_id    veh           starttime            stoptime break_from_last\n0     kg123  10010 2018-12-21 15:17:29 2018-12-21 15:18:57             NaT\n1     kg124  10012 2019-01-01 00:10:16 2019-01-01 00:16:32             NaT\n2     kg124  10012 2019-01-01 00:27:11 2019-01-01 00:31:38 0 days 00:21:22\n3     kg214  10012 2019-01-01 00:46:20 2019-01-01 01:04:54             NaT\n4     kg125  10013 2019-01-01 00:19:06 2019-01-01 00:39:43             NaT\n"], "link": "https://stackoverflow.com/questions/68276371/python-pandas-how-do-i-perform-an-operation-on-a-shifted-column-within-a-group"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe I would like to iterate over. A simplified example of my dataframe: I would like to iterate over each unique gene and create a new file named: For the above example I should get three iterations with 3 outfiles and 3 dataframes: The resulting data frame contents split up by chunks will be sent to another function that will perform the analysis and return the contents to be written to file.", "q_apis": "get columns unique get", "apis": "to_csv", "code": ["genes = df['Gene'].unique()\nfor gene in genes:\n    outfilename = gene + '.pdf'\n    print(outfilename)\n    df[df['Gene'] == gene].to_csv(outfilename)\nHAPPY.pdf\nSAD.pdf\nLEG.pdf\n", "gp = df.groupby('Gene')\n# groups() returns a dict with 'Gene':indices as k:v pair\nfor g in gp.groups.items():\n    print(df.loc[g[1]])   \n    \n    chr  start  end   Gene  Value  MoreData\n0  chr1    123  123  HAPPY   41.1       3.4\n1  chr1    125  129  HAPPY   45.9       4.5\n2  chr1    140  145  HAPPY   39.3       4.1\n    chr  start  end Gene  Value  MoreData\n3  chr1    342  355  SAD   34.2       9.0\n4  chr1    360  361  SAD   44.3       8.1\n5  chr1    390  399  SAD   29.0       7.2\n6  chr1    400  411  SAD   35.6       6.5\n    chr  start  end Gene  Value  MoreData\n7  chr1    462  470  LEG     20       2.7\n"], "link": "https://stackoverflow.com/questions/26103676/how-to-split-a-dataframe-by-unique-groups-and-save-to-a-csv"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to convert all columns in a dataframe (3700 rows x 22 columns) which including Boolean operators: true and false (t and f) to Zeros and ones with ignoring the missing values. Please note that I read the data from csv file. For Example: After running this code: I got this dataframe: and this is the output which I am looking for: I need your help in what should I change in the code above to get the dataframe above.", "q_apis": "get columns all columns columns values get", "apis": "select_dtypes columns", "code": ["b_cols = [col for col in df if set([\"f\", \"t\"]).intersection(df[col].unique())]\nd_cols = df.select_dtypes(\"object\").columns.difference(b_cols).tolist()\n", "df[b_cols] = df[b_cols].replace({\"f\": False, \"t\": True}).astype(\"boolean\")\nprint(df)\n\n     X1     X2     X3 X4\n0  3030  False   True  a\n1  3456   True    NaN  b\n2  3567  False  False  a\n3  4568  False   True  b\n\n\ndf = pd.get_dummies(df, columns=d_cols)\nprint(df)\n\n     X1     X2     X3  X4_a  X4_b\n0  3030  False   True     1     0\n1  3456   True    NaN     0     1\n2  3567  False  False     1     0\n3  4568  False   True     0     1\n"], "link": "https://stackoverflow.com/questions/67451313/how-to-change-the-boolean-operators-t-f-to-zeros-and-ones-without-any-changing"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'd like to filter a dataframe to get sub-datasets in a nested for-loop, then apply to each sub-datasets, pick one row from each sub-dataset based on time duration column called , then concatenate all the individual rows into one dataframe. Here's the code: is a dataframe looks as below: I want to pick the row with longest , and also in the range of to in each sub-dataset to combine them into one dataframe. However it caught error: I realised that it may be due to the fact that the dataframe passed as arguments should be of form based on this question. I tried which returned: Expected Output, still in this format but more rows: Update: Tried: which only returned one row:", "q_apis": "get columns filter get sub apply sub sub time all sub combine", "apis": "loc idxmax append DataFrame loc loc idxmax append DataFrame", "code": ["            long_event = res_df.loc[res_df['TimeDiff'].idxmax()]\n            total_t.append(long_event)\n\n    total_t = pd.DataFrame(total_t)\n    return total_t\n", "df = pd.DataFrame({'TimeDiff': ['00:01:13.124000', '00:00:03.124000', '00:12:03.126000', '00:04:54.124000'], \n                   'Other': [1, 2, 3, 4]})\ndf['TimeDiff'] = pd.to_timedelta(df.TimeDiff)\n", "df = df.loc[(df['TimeDiff']> datetime.timedelta(seconds=60)) & (df['TimeDiff']<datetime.timedelta(minutes=5))]\ntotal_t = []\nlong_event = df.loc[df['TimeDiff'].idxmax()]\ntotal_t.append(long_event)\n\n# after the for loops are completed in your case\ntotal_t = pd.DataFrame(total_t)\ntotal_t\n    TimeDiff                    Other\n3   0 days 00:04:54.124000000   4\n\n"], "link": "https://stackoverflow.com/questions/62589270/filter-sub-dataframe-in-nested-for-loop"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes: df_data which contain and ID column which link it to another dataframe (df_data_req) and other columns there contain some data. Not all columns would have data, which is ok in some cases. df_data_req contains the same ID as in df_data, and in this dataframe it is specified which columns there should have data. I would then like to validate all rows in df_data, and check (based in the ID column), if all columns there are specified in df_data_req have a value. What I expected is addition column to df_data which indicate \"Missing data\" or \"OK\" which are depended on what there are specified in df_data_req. The output for the sample data would then be: ID col1 col2 col3 validation x 1 5 MissingData y 5 1 OK x 2 5 3 OK z f 5 5 OK", "q_apis": "get columns columns all columns contains columns all all columns value sample", "apis": "set_index replace set_index replace fillna reindex_like eq any where", "code": ["a = df_data.set_index(\"ID\").replace(\"\",np.nan)\nb = df_data_req.set_index(\"ID\").replace(\"\",np.nan)\nc = a.fillna(b.reindex_like(a)).eq(\"X\").any(1)\ndf_data['validation'] = np.where(c,\"MissingData\",\"OK\")\n", "print(df_data)\n\n  ID col1 col2 col3   validation\n0  x    1    5       MissingData\n1  y         5    1           OK\n2  x    2    5    3           OK\n3  z    f    5    5           OK\n"], "link": "https://stackoverflow.com/questions/68024482/pandas-check-if-required-columns-have-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a series of data in a pandas dataframe which I import from a CSV, but this file (sampled per minute) has missing data for entire days. I would like to replace them with the same data of the previous DAY or DAY OF WEEK (7 days before), or, also, with the average values per minute o the previous DAYS or DAYS OF THE WEEK (7 x n days before). This should be done with pandas or python but I frankly don't know how to do it, maybe try to resample or groupby and then resample again with minute frequency? I don't want all the data to be changed though.. The file looks like this one: and in the output I would like to obtain the full complete dataset, replacing the missing values with the same WEEKDAY (or DAY BEFORE) values, or the averages. So in case I replace the value with the values of the day before the dataset will look like: Thanks to whoever can help.", "q_apis": "get columns minute days replace days values minute days resample groupby resample minute all values values replace value values day", "apis": "set_index reindex min reset_index rename columns index groupby ffill", "code": ["    date        val\n0   2018-01-05 00:00:00 2900.41\n1   2018-01-05 00:01:00 2919.24\n2   2018-01-05 00:02:00 2938.07\n3   2018-01-08 00:00:00 3118.00\n4   2018-01-08 00:01:00 3118.00\n5   2018-01-08 00:02:00 3125.54\n", "df['date'] = pd.to_datetime(df.date)\ndf = df.set_index('date')\ndf = df.reindex(pd.date_range('2018-01-05 00:00:00', \n                         '2018-01-08 00:02:00', freq='1 min')\n          ).reset_index().rename(columns={'index':'date'})\ndf = df.groupby(df.date.dt.time).ffill()\n", "        date                val\n0       2018-01-05 00:00:00 2900.41\n1       2018-01-05 00:01:00 2919.24\n2       2018-01-05 00:02:00 2938.07\n1440    2018-01-06 00:00:00 2900.41\n1441    2018-01-06 00:01:00 2919.24\n1442    2018-01-06 00:02:00 2938.07\n2880    2018-01-07 00:00:00 2900.41\n2881    2018-01-07 00:01:00 2919.24\n2882    2018-01-07 00:02:00 2938.07\n"], "link": "https://stackoverflow.com/questions/67275219/fill-missing-data-in-pandas-time-series-with-data-from-the-previous-weekday-or"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to add an index column to a pandas data frame, with the index restarting from 0 every time the string content of a specific column changes. This is the data frame and every time that in the column description Take #1: changes to Take #2: I would like to restart the index to 0 and the same thing for when it changes to Take #3:", "q_apis": "get columns add index index time time index", "apis": "index", "code": ["df = pd.DataFrame({'Description' : ['Take #1 : aezr', 'Take #2 : qsdkf','Take #2 : kfqdskl'],\n                  'A' : [12, 46, 57], 'B' : [13, 46,57]})\n\n# First we find where the takes # change looking in the description field\ndf['inter'] = [int(x[6]) for x in df['Description'] ]\n\n# Then for each different take we fill the index column using the range function\nfor x in df['inter'].unique():\n    mask = df['inter'] == x\n    df.loc[mask, 'index'] = [x for x in range(len(df[mask]))]\n\n# You can now drop the inter column which is no longer needed :\ndf.drop(columns='inter', inplace = True)\n", "         Description   A   B  index\n0     Take #1 : aezr  12  13    0.0\n1    Take #2 : qsdkf  46  46    0.0\n2  Take #2 : kfqdskl  57  57    1.0\n"], "link": "https://stackoverflow.com/questions/57216806/restart-index-from-0-every-time-the-string-content-of-a-specific-column-changes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Good evening, I have a question about detecting certain pattern. I don't know whether my question has specific terminology. I have a pandas dataframe like this: How can I detect the pattern which is [-1 following after 1] in IF statement. For example: Grabbing price column from index 3 and 4 because pattern column at index 3 is 1 and index 4 is -1 which match my condition. Next would be index 7 and 8 then index 10 and 11. I probably convey my question pretty vague, however I don't really know how to describe it.", "q_apis": "get columns index at index index index index describe", "apis": "shift loc apply loc shift", "code": ["df['pattern2'] = df.pattern.shift()\n\n# Selecting just prices of meeting condition\nprices = df.loc[df.apply(lambda x: True if ((x['pattern'] == -1) & (x['pattern2'] == 1)) else False, axis=1), 'price']\n", "prices = df.loc[(df.pattern - df.pattern.shift() == -2), 'price']\n"], "link": "https://stackoverflow.com/questions/66433311/python-pandas-pattern-serching"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to change the headers of these groups of dataframes simultaneuosly with the name of the dataframes like I have a list of all the dataframes like . Can someone help?", "q_apis": "get columns groups name all", "apis": "add_prefix", "code": ["allDf = [df1, df2, df3]\n    \nfor i,j in enumerate(allDf):\n   allDf[I] = j.add_prefix(\"df\"+str(i+1))\n", "df1:\n\n    df1a    df1b\n0   0        0\n1   1        1\n2   2        2\n3   3        3\n\n\ndf2:\n\n    df2a    df2b\n0   0        0\n1   1        1\n2   2        2\n3   3        3\n\n\ndf3:\n\n    df3a    df3b\n0   0        0\n1   1        1\n2   2        2\n3   3        3\n"], "link": "https://stackoverflow.com/questions/67762551/how-to-affect-multiple-dataframes-simultaneously-using-loop"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to query a dataframe and store the results into another dataframe. The dataframe sample I have is: I have split the dataframe into two - one for passenger_type normal and one for tatkal. Then for each dataframe corresponding to passenger_type I want the result dataframe to be like: I am using groupby with value_counts and sorting the results, but the result is not what I want. I am getting something like this: The code I am using is : Can someone please help me with the code to obtain the desired results.", "q_apis": "get columns query sample groupby value_counts", "apis": "value_counts unstack add_prefix sum sort_values reset_index", "code": ["z = (df[['start', 'end', 'train']]\n         .value_counts()\n         .unstack(fill_value=0)\n         .add_prefix('freq by '))\n\nz['frequency'] = z.sum(1)\n\ndf_out = z.sort_values('frequency', ascending=False).reset_index()\ndf_out\n", "train      start        end  freq by hwh indb spl  freq by rajdhani  frequency\n0         howrah  allahabad                     1                 1          2\n1      allahabad     howrah                     1                 0          1\n", "train      start        end  frequency\n0         howrah  allahabad          2\n1      allahabad     howrah          1\n"], "link": "https://stackoverflow.com/questions/67744265/how-to-get-desired-dataframe-as-output-from-groupby-and-value-counts-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this 3 lists on a Pandas DF: What I want to do is to get a new field on my DF with a list consisting of the flat_values list and the tmp_values inserted at the position reflected in the indexes list numbers. So that: Thank you very much", "q_apis": "get columns get at", "apis": "copy apply", "code": ["def replace_val(row):\n    l = row['flat_values'].copy()\n\n    for i, m in zip(row['indexes'], row['tmp_values']):\n        l[i] = m\n\n    return l\n\ndf['new_values'] = df.apply(replace_val, axis=1)\n", "a = [20, None, None, None, None, 30, 40]\nb = [1, 2, 3, 4]\nc = [40, 30, 10, 10]\n\nd = [20, None, None, 30, 40]\ne = [1, 2]\nf = [100, 120]\n\ndf = pd.DataFrame({'flat_values': [a, d], 'indexes': [b, e], 'tmp_values': [c, f]})\n\ndef replace_val(row):\n    l = row['flat_values'].copy()\n\n    for i, m in zip(row['indexes'], row['tmp_values']):\n        l[i] = m\n\n    return l\n\ndf['new_values'] = df.apply(replace_val, axis=1)\nprint(df)\n\n                            flat_values       indexes        tmp_values  \\\n0  [20, None, None, None, None, 30, 40]  [1, 2, 3, 4]  [40, 30, 10, 10]\n1              [20, None, None, 30, 40]        [1, 2]        [100, 120]\n\n                     new_values\n0  [20, 40, 30, 10, 10, 30, 40]\n1        [20, 100, 120, 30, 40]\n"], "link": "https://stackoverflow.com/questions/59662715/replace-values-of-list-knowing-its-index-and-value-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What am I doing wrong here? I am trying to change my data frame but nothing is happening?", "q_apis": "get columns", "apis": "iloc isna all", "code": ["df.iloc[(df[['var1', 'var2']].isna().all(1)) & (df.var3 >= 0), 'var2'] = 0\n"], "link": "https://stackoverflow.com/questions/65152269/df-loc-with-nan-is-not-changing-anything"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm receiving a when applying transformations to a DataFrame using a scikit-learn , and I'm not sure why that is. This is my code. This is what the original DataFrame looks like... Then I create the steps in the ColumnTransformer and I specify the index of the column rather than the column name. Then I create the ColumnTransformer... Finally, I print the result of the applying the ColumnTransformer to my DataFrame. This is the output of the transformations. The ColumnTransformer returns a numpy array as expected (with columns 'C' and 'E' first and second respectively), but I don't understand why I'm receiving the SettingWithCopy warning. I have managed to fix it by changing the filling_nan() function slightly, but I don't understand why that fixes it. I've been unable to reproduce the result outside of using a ColumnTransformer so was wondering if it was something to do with that?", "q_apis": "get columns DataFrame DataFrame index name DataFrame array columns first second", "apis": "columns values fillna columns values fillna", "code": ["def filling_nan(frame):\n    \"\"\"Fills columns that have null values with zeros.\"\"\"\n    frame = frame.fillna(0)\n    return frame\n\ndef filling_nan_inplace(frame):\n    \"\"\"Fills columns that have null values with zeros.\"\"\"\n    frame.fillna(0, inplace=True)\n    return frame\n\nprint(id(df))\nprint(id(filling_nan_inplace(df)))\nprint(id(filling_nan(df)))\n"], "link": "https://stackoverflow.com/questions/59577674/python-columntransformer-settingwithcopywarning"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I get my data into pandas dataframes, and normally clean up my column headers with However I recently encountered a dataframe that had integer type column names, not strings. When I tried and do a .str.strip() it throws an error. How would I write some python code that strips whitespace from the column names if they are strings. I'm new to python, so the more hand holding the better. Thanks", "q_apis": "get columns get names names", "apis": "DataFrame columns columns columns columns", "code": ["df = pd.DataFrame(columns=[1, 2, 'A '])\ndf.columns = [col.strip() if isinstance(col, str) else col for col in df.columns]\n", "In [75]: df.columns\nOut[75]: Index([1, 2, 'A'], dtype='object')\n"], "link": "https://stackoverflow.com/questions/53510121/wanted-function-to-remove-whitespace-from-column-headers-that-is-robust-to-colu"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have some data that looks like this in a dataframe: Using Pandas, I am looking for a way to return this in a new column So far I have been able to use to get but when I run I get I think I'm supposed to use str.extract instead of replace, but i also get an error message in that case.", "q_apis": "get columns get get replace get", "apis": "join index join index", "code": ["s = df.Japanese.str.findall('(?i)[a-z]+')\npd.Series([', '.join({*x}) for x in s], s.index)\n\n0    Adverb, Weather, Case\ndtype: object\n", "s = df.Japanese.str.findall('(?i)[a-z]+')\npd.Series([', '.join(sorted({*x})) for x in s], s.index)\n\n0    Adverb, Case, Weather\ndtype: object\n"], "link": "https://stackoverflow.com/questions/52824407/extract-alpha-letters-between-a-pipe-and-japanese-character-and-replace-with-sp"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas data frame with a unique ID, a stage, and a date associated with that stage. I would like to create a new column showing the time spent in that particular stage. This means subtracting the date for that unique ID minus the date of the next stage associated with that same ID. Since the data is sorted on ID and stage, the last stage of that ID record should error out and read 'current' or n/a. It helps to see the data below. I'm thinking there is a way to do multiple for loops, but am unsure how to do this. I have tried creating new columns before melting the data frame, but there are many cases in which the next stage is n/a. It is not always stage 1=>2=>3 etc, it can skip directly from stage 1 to stage 3.", "q_apis": "get columns unique date time date unique date last columns", "apis": "groupby diff", "code": ["     Opportunity_ID  stage      value difference\n0   0061R00000l43xP    1.0 2018-11-07        NaT\n1   0061R00000lUT5r    1.0 2019-05-02   -20 days\n2   0061R00000lUT5r    2.0 2019-05-22   -12 days\n3   0061R00000lUT5r    3.0 2019-06-03   -99 days\n5   0061R00000lUT5r    5.5 2019-09-10        NaT\n6   0061R00000lXwZL    1.0 2018-12-05  -125 days\n7   0061R00000lXwZL    4.0 2019-04-09   -10 days\n8   0061R00000lXwZL    5.0 2019-04-19     0 days\n9   0061R00000lXwZL    5.5 2019-04-19   -14 days\n10  0061R00000lXwZL    8.0 2019-05-03   -67 days\n11  0061R00000lXwZL    9.0 2019-07-09   -24 days\n12  0061R00000lXwZL   11.0 2019-08-02        NaT\n13  0061R00000lY4Vm    1.0 2018-12-06  -294 days\n14  0061R00000lY4Vm    2.0 2019-09-26        NaT\n15  0061R00000lrOGm    3.0 2019-02-15  -215 days\n16  0061R00000lrOGm    4.0 2019-09-18        NaT\n4   80061R0000lUT5r    5.0 2019-06-20        NaT\n", "df['value']=pd.to_datetime(df['value'])\ndf['difference']=df.groupby('Opportunity_ID')['value'].diff(-1)\nprint(df)\n\n    Opportunity_ID  stage      value difference\n0   0061R00000l43xP    1.0 2018-11-07        NaT\n1   0061R00000lUT5r    1.0 2019-05-02   -20 days\n2   0061R00000lUT5r    2.0 2019-05-22   -12 days\n3   0061R00000lUT5r    3.0 2019-06-03   -99 days\n4   80061R0000lUT5r    5.0 2019-06-20        NaT\n5   0061R00000lUT5r    5.5 2019-09-10        NaT\n6   0061R00000lXwZL    1.0 2018-12-05  -125 days\n7   0061R00000lXwZL    4.0 2019-04-09   -10 days\n8   0061R00000lXwZL    5.0 2019-04-19     0 days\n9   0061R00000lXwZL    5.5 2019-04-19   -14 days\n10  0061R00000lXwZL    8.0 2019-05-03   -67 days\n11  0061R00000lXwZL    9.0 2019-07-09   -24 days\n12  0061R00000lXwZL   11.0 2019-08-02        NaT\n13  0061R00000lY4Vm    1.0 2018-12-06  -294 days\n14  0061R00000lY4Vm    2.0 2019-09-26        NaT\n15  0061R00000lrOGm    3.0 2019-02-15  -215 days\n16  0061R00000lrOGm    4.0 2019-09-18        NaT\n"], "link": "https://stackoverflow.com/questions/58384297/how-to-iterate-through-unique-id-and-calculate-difference-between-date-values-an"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a from pandas: If a change in a row's string value occurs comparing to previous row, I want to identify it in a separate row \"Cng-Address\", and if row's numeric value changes identify it in \"Cng-Year\" column. If there is no change identify it as zero. The index is \u201cName\u201d meaning that the above calculations should be done for all rows associated to person name. If a \u201cName\u201d changes (i.e. John to Steve) then calculations for \"Cng-Address\" and \"Cng-Year\" should reset. Column year sorted ascending. As a final report I want to get: John changed years \u201c1\u201d time and changed locations \u201c2\u201d times Steve changed years \u201c2\u201d times and change locations \u201c2\u201d times Total changed addresses for Year 2019 is \u201c2\u201d times Current Output: Ideal Output:", "q_apis": "get columns value value index all name year get time", "apis": "groupby shift fillna ne astype", "code": ["groups = df.groupby('Name')\n\nfor col in ['Year', 'Address']:\n    df[f'cng-{col}'] = groups[col].shift().fillna(df[col]).ne(df[col]).astype(int)\n", "    Name  Year        Address  cng-Year  cng-Address\n0   John  2018  Beverly hills         0            0\n1   John  2018  Beverly hills         0            0\n2   John  2019  Beverly hills         1            0\n3   John  2019  Orange county         0            1\n4   John  2019       New York         0            1\n5  Steve  2018         Canada         0            0\n6  Steve  2019         Canada         1            0\n7  Steve  2019         Canada         0            0\n8  Steve  2020     California         1            1\n9  Steve  2020         Canada         0            1\n"], "link": "https://stackoverflow.com/questions/61089138/how-to-identify-string-change-in-a-row-comparing-to-previous-row-in-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes, first is: df1 df2 Desired output df2: I have tried multiple ways but the closest I get is: And I thought this will be good, but I got exactly the same values for all rows. Any help is welcomed. Thanks", "q_apis": "get columns first get values all", "apis": "join merge", "code": ["pat = '|'.join(r\"\\b{}\\b\".format(x) for x in df1['col_one'].unique())\ndf2['col_one'] = df2['col_1'].str.extract('(' + pat + ')')\nprint (df2)\n                 col_1 col_one\n0  operate ABBC1, to 2   ABBC1\n1   operate JJKS3, FOM   JJKS3\n\ndf = df1.merge(df2, on='col_one')\nprint (df)\n  col_one    col_two                col_1\n0   ABBC1  (1, 2, 3)  operate ABBC1, to 2\n1   JJKS3  (5, 2, 5)   operate JJKS3, FOM\n", "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in df1['col_one'].unique())\ns = df2['col_1'].str.findall('(' + pat + ')')\nprint (s)\n0    [ABBC1, JJKS3]\n1           [JJKS3]\nName: col_1, dtype: object\n\nlens = s.str.len()\na = np.repeat(df2['col_1'], lens)\nb = np.concatenate(s)\ndf2 = pd.DataFrame({'col_1':a, 'col_one':b})\nprint (df2)\n                               col_1 col_one\n0  operate ABBC1, to 2  JJKS3 3 to 5   ABBC1\n0  operate ABBC1, to 2  JJKS3 3 to 5   JJKS3\n1                 operate JJKS3, FOM   JJKS3\n\ndf = df1.merge(df2, on='col_one')\nprint (df)\n  col_one    col_two                              col_1\n0   ABBC1  (1, 2, 3)  operate ABBC1, to 2  JJKS3 3 to 5\n1   JJKS3  (5, 2, 5)  operate ABBC1, to 2  JJKS3 3 to 5\n2   JJKS3  (5, 2, 5)                 operate JJKS3, FOM\n"], "link": "https://stackoverflow.com/questions/50796559/map-column-if-string-contain-values-from-other-column-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i have a dataframe with multiple columns, i want to know if there is a function that will allow me to check if i have at least one identical values in two columns. here is a small example of my data and the desired result. thank you for your help. input file : output file :", "q_apis": "get columns columns at identical values columns", "apis": "notna notna", "code": ["df[\"column3\"] = [\"ok\"\n                 if pd.notna(col1) and pd.notna(col2) and (col1 in col2)\n                 else \"nok\"\n                 for col1, col2 in zip(df.column1, df.column2)]\n", ">>> df\n\n  column1                          column2 column3\n0  France  Canada |Sweden | France | Italy      ok\n1   Spain     Switzerland | Canada |Sweden     nok\n"], "link": "https://stackoverflow.com/questions/68289720/check-if-my-two-columns-contain-at-least-one-seminar-value-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this dataframe. I have converted this dataframe to different chunks. After every 6 rows new chunk is created. What i want to do is this: 1. Take absolute difference between two users for same movie (each pair of user have watched three movies). 2. Take the average of difference for first two movies then compare it with 3rd movie difference. if the third movie difference is less than avg of first two movies take it positive otherwise negative. For example, in first chunk diff b/w first movie is 1 and for next is 3 (avg diff is 2). For the 3rd pair diff is 2.5 (diff > avg) it should give negative result. 3. Do it for every chunk in dataframe.", "q_apis": "get columns difference between difference first compare difference difference first take first diff first diff diff diff", "apis": "append append", "code": ["dfchunk                                                                                                              \n   userId  movieId  ratings\n0      10      500      3.5\n1      20      500      4.5\n2      10      800      2.0\n3      20      800      5.0\n4      10      700      4.0\n5      20      700      1.5\n\nr=dfchunk.pivot(index=\"movieId\",columns=\"userId\")                                                                    \n\n        ratings     \nuserId       10   20\nmovieId             \n500         3.5  4.5\n700         4.0  1.5\n800         2.0  5.0\n\nr.columns=[\"u1\",\"u2\"]                                                                                                \n\nr[\"drate\"]=r.u1.sub(r.u2).abs()                                                                                      \n\n          u1   u2  drate\nmovieId                 \n500      3.5  4.5    1.0\n700      4.0  1.5    2.5\n800      2.0  5.0    3.0\n\nv= r.drate.iloc[:-1].mean()-r.drate.iloc[-1]                                                                            \n-1.25\n", "if v<0:\n   negative_counter.append(v)\nelse:\n   positive_counter.append(v)\n"], "link": "https://stackoverflow.com/questions/59541515/finding-absolute-difference-between-rows-and-comparing-the-difference-with-other"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Selecting specific excel rows using python. So in excel I would do For obtaining those columns in a data frame that is neither Closed or blank. Tried using The problem is python is removing columns which are for example: Which I don't want As suggested in one of the links also tried Got the same result as when I tried .isin", "q_apis": "get columns columns columns isin", "apis": "isin isin isin isin isin isin loc loc isin isin loc isin isin", "code": ["           A         B  ~df.A.isin  ~df.B.isin  ~A & ~B  ~A | ~B\n0     Closed    Closed       False       False    False    False\n1     Closed   No Data       False       False    False    False\n2   Approved    Closed        True       False    False     True\n3    No Data   No Data       False       False    False    False\n4     Closed  Approved       False        True    False     True\n5    No Data  Restrict       False        True    False     True\n6   Approved   No Data        True       False    False     True\n7     Closed  Restrict       False        True    False     True\n8   Approved  Approved        True        True     True     True\n9    No Data  Approved       False        True    False     True\n10  Restrict   No Data        True       False    False     True\n11  Restrict  Approved        True        True     True     True\n", "           A         B  ~df.A.isin  ~df.B.isin  ~A & ~B  ~A | ~B\n8   Approved  Approved        True        True     True     True\n11  Restrict  Approved        True        True     True     True\n", "           A         B  ~df.A.isin  ~df.B.isin  ~A & ~B  ~A | ~B\n2   Approved    Closed        True       False    False     True\n4     Closed  Approved       False        True    False     True\n5    No Data  Restrict       False        True    False     True\n6   Approved   No Data        True       False    False     True\n7     Closed  Restrict       False        True    False     True\n8   Approved  Approved        True        True     True     True\n9    No Data  Approved       False        True    False     True\n10  Restrict   No Data        True       False    False     True\n11  Restrict  Approved        True        True     True     True\n", "       A         B\nApproved    Closed\n No Data  Restrict\nRestrict   No Data\n", "     A         B\nClosed    Closed\nClosed   No Data\n", "df.loc[(df[A] != \"Closed\") & (df[B] != \"Closed\") &\n       (df[A] != \"No data\") & (df[B] != \"No data\")\n", "df.loc[(df['A'] != \"Closed\") & (df['B'] != \"Closed\") &\n       (df['A'] != \"No Data\") & (df['B'] != \"No Data\")]\n", "           A         B  ~df.A.isin  ~df.B.isin  ~A & ~B  ~A | ~B\n8   Approved  Approved  True  True  True  True  True  True  True\n11  Restrict  Approved  True  True  True  True  True  True  True\n", "df.loc[((df['A'] != \"Closed\") & (df['A'] != \"No Data\")) | \n       ((df['B'] != \"Closed\") & (df['B'] != \"No Data\"))]\n", "           A         B  ~df.A.isin  ~df.B.isin  ~A & ~B  ~A | ~B\n2   Approved    Closed        True       False    False     True\n4     Closed  Approved       False        True    False     True\n5    No Data  Restrict       False        True    False     True\n6   Approved   No Data        True       False    False     True\n7     Closed  Restrict       False        True    False     True\n8   Approved  Approved        True        True     True     True\n9    No Data  Approved       False        True    False     True\n10  Restrict   No Data        True       False    False     True\n11  Restrict  Approved        True        True     True     True\n"], "link": "https://stackoverflow.com/questions/55284261/python-equivalent-of-excel-nested-if-condition-for-filtering-pandas-dataframe-ro"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've got a pandas DataFrame where I want to replace certain values in a selection of columns with the value from another in the same row. I did the following: is a list with column names. 99 is considered a missing value which I want to replace with the (already calculated) Mean for the given class (i.e., col1 or col2 depending on the selection) It works, but time it takes to replace all those values seems to take longer than would be necessary. I figured there must be a quicker (computationally) way of achieving the same. Any suggestions?", "q_apis": "get columns DataFrame where replace values columns value names value replace time replace all values take", "apis": "where where", "code": ["import numpy as np\n\ndf[cols[23:30]] = np.where(df[cols[23:30]] == 99, df[['col1'] * (30-23)], df[cols[23:30]])\n\ndf[cols[30:36]] = np.where(df[cols[30:36]] == 99, df[['col2'] * (36-30)], df[cols[30:36]])\n"], "link": "https://stackoverflow.com/questions/58799940/pandas-dataframe-replace-values-in-multiple-columns-with-the-value-from-another"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to develop a Python Script for my Data Engineering Project and I want to loop over 47 URLS stored in a dataframe, which downloads a CSV File and stores in my local machine. Below is the example of top 5 URLS: I have this for a single file, but instead of the opening a CSV File and writing the Data in it, I want to directly download all the files and save it in local machine.", "q_apis": "get columns all", "apis": "index items index index items", "code": ["for index, url in url_df.items():\n    urllib.urlretrieve(url, \"cdcData\" + index + \".csv\")\n", "for index, url in url_df.items():\n    name = url.split(\"/\")[-1]\n    urllib.urlretrieve(url, name)\n"], "link": "https://stackoverflow.com/questions/65163867/download-csv-data-by-looping-over-a-pandas-data-frame-which-consists-of-47-url"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataFrame with 2 columns a A and B. I have to separate out subset of dataFrames using pandas to delete all the duplicate values. My dataFrame looks like this Then the output should be How do I do that? Thanks in advance.", "q_apis": "get columns columns delete all values", "apis": "stack duplicated unstack DataFrame where columns columns index index style apply columns style apply stack duplicated bool stack duplicated unstack", "code": ["def color_dupes(x):\n    c1='background-color:red'\n    c2=''\n    cond=x.stack().duplicated(keep=False).unstack()\n    df1 = pd.DataFrame(np.where(cond,c1,c2),columns=x.columns,index=x.index)\n    return df1\ndf.style.apply(color_dupes,axis=None)\n# if df has many columns: df.style.apply(color_dupes,axis=None,subset=['A','B'])\n", "df.stack().duplicated(keep=False)\n\n0  A     True\n   B     True\n1  A    False\n   B    False\n2  A     True\n   B     True\n3  A     True\n   B     True\n4  A    False\n   B    False\n5  A     True\n   B    False\ndtype: bool\n", "df.stack().duplicated(keep=False).unstack()\n       A      B\n0   True   True\n1  False  False\n2   True   True\n3   True   True\n4  False  False\n5   True  False\n"], "link": "https://stackoverflow.com/questions/59694301/conditional-formatting-on-duplicate-values-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am looking to combine row 2 with row 0, so that the unit of the row is part of the column header and not following the data point. The data comes from a .csv file in this format, but it would be much easier to work with if the unit was in the header. The unit is not always \"mi/h\" and can change in any given reference file. Sections like these have been extracted from larger .csv files containing many differently formatted tables. My end goal is to export each section individually into a its own .csv file. This is already working but I'm hoping to adjust the data, as described above, and then continue with exporting it to its own .csv file. Ideally, the output would look like this:", "q_apis": "get columns combine any", "apis": "columns loc columns loc loc loc columns", "code": ["df.columns=[c+\"(\"+str(df.loc[2,c])+\")\" for c in df.columns]\n", "df.loc[0,:]=[str(df.loc[0,c])+str(df.loc[2,c]) for c in df.columns ]\n"], "link": "https://stackoverflow.com/questions/63079112/is-there-a-way-to-combine-rows-in-a-pandas-dataframe-given-the-row-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Setting keep=False should remove all duplicates but if I run my function is still returns a duplicate of the previous row If my csv file is empty with only the column headers 'Date' and 'Price' and I run my function 3 times it returns this in csv: When I expect it to return something like this:", "q_apis": "get columns all empty", "apis": "append to_csv index", "code": ["import pandas as pd\nfrom datetime import datetime\n\n\ndef date_to_csv(): \n     df = pd.read_csv('test.csv') \n     df = df.append({'Date': str(datetime.now().date()), 'Price': randint(1, 100)}, ignore_index=True) \n     df.to_csv('test.csv', index=False) \n"], "link": "https://stackoverflow.com/questions/68141498/pandas-dropping-duplicates-doesnt-drop-last-duplicate"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a problem with a huge DataFrame. I need to clean it, but just the rows, where from -1 to 2 the Amount and the Bill a zero (see picture) Maybe it ist possible to connect the rows and after that delete it, but I do not know how? Thank you Hanna", "q_apis": "get columns DataFrame where delete", "apis": "index values drop", "code": ["temp_df = df[(df.Delivery > -2) & (df.Delivery < 3) & (df.Bill == 0)]\nindex_lst = temp_df.index.values.tolist()\ndf = df.drop(index_lst)\n"], "link": "https://stackoverflow.com/questions/61749572/python-pandas-big-dataframe-deleting-empty-columns-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am putting my data into a bokeh layout of a heat map, but am getting a KeyError: '1'. It occurs right at the line does anybody know why this would be? The pivot table I am using is below: Below is the section of code leading up to the error: and upon request: and Also the bokeh code is here: http://docs.bokeh.org/en/latest/docs/gallery/unemployment.html", "q_apis": "get columns map right at pivot", "apis": "DataFrame set_index sort_index columns groupby agg columns sum reset_index columns pivot_table pivot index columns values pivot_table index pivot_table columns append append pivot_table loc append append min", "code": ["import pandas as pd\nimport numpy as np\n\n# just following your previous post to simulate your data\nnp.random.seed(0)\ndates = np.random.choice(pd.date_range('2015-01-01 00:00:00', '2015-06-30 00:00:00', freq='1h'), 10000)\ncompany = np.random.choice(['company' + x for x in '1 2 3 4 5'.split()], 10000)\ndf = pd.DataFrame(dict(recvd_dttm=dates, CompanyName=company)).set_index('recvd_dttm').sort_index()\ndf['C'] = 1\ndf.columns = ['CompanyName', '']\nresult = df.groupby([lambda idx: idx.month, 'CompanyName']).agg({df.columns[1]: sum}).reset_index()\nresult.columns = ['Month', 'CompanyName', 'counts']\npivot_table = result.pivot(index='CompanyName', columns='Month', values='counts')\n\n\n\n\ncolors = [\"#75968f\", \"#a5bab7\", \"#c9d9d3\", \"#e2e2e2\", \"#dfccce\",\n    \"#ddb7b1\", \"#cc7878\", \"#933b41\", \"#550b1d\" ]\n\nmonth = []\ncompany = []\ncolor = []\nrate = []\nfor y in pivot_table.index:\n    for m in pivot_table.columns:\n        month.append(m)\n        company.append(y)\n        num_calls = pivot_table.loc[y, m]\n        rate.append(num_calls)\n        color.append(colors[min(int(num_calls)-2, 8)])\n"], "link": "https://stackoverflow.com/questions/31170593/keyerror-in-for-loop-of-dataframe-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe similar to the below mentioned database: I'm grouping by all the columns mentioned above and i'm able to get the count in a different column named successfully using the below command: But I want the count in the above dataframe only in those rows with and rest should be Desired Output: I tried to count for with the below code: which obviously gives me those rows with and discarded the rest. I want the discarded ones with Is there any way to get the result? Thanks in advance!", "q_apis": "get columns all columns get count count count any get", "apis": "groupby apply sum reset_index count assign groupby sum astype reset_index count", "code": ["df1 = (df.groupby(['time','id','status'])\n         .apply(lambda x: (x['status']== 'Yes').sum())\n         .reset_index(name='count'))\n", "df1 = (df.assign(A=df['status']=='Yes')\n         .groupby(['time','id','status'])['A']\n         .sum()\n         .astype(int)\n         .reset_index(name='count'))\n", "df1 = ((df['status']=='Yes')\n        .groupby([df['time'],df['id'],df['status']])\n        .sum()\n        .astype(int)\n        .reset_index(name='count'))\n\nprint (df)\n         time   id status  count\n0  1451606400  id1    Yes      2\n1  1456790400  id2     No      0\n2  1456790400  id2    Yes      1\n"], "link": "https://stackoverflow.com/questions/53153703/groupby-count-only-when-a-certain-value-is-present-in-one-of-the-column-in-panda"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 sets of Dataframe, both with an unique identifier and a datetime data in the format as such \"2020-01-01 00:00:01\"-datetime and \"12345\" - unique identifier and Type 1st Question, DF1: I would like to based on the ID's 1st time stamp with the same \"Type\", and remove the rows 10mins after as such: I've tried to explore timerange/daterange but could not find any similar concept of coding. Would hope that if anyone can point out what kind of ways i can look into to explore and not trying to get a full solution. Have not touch python for a few years and not familiar with it previously. Thank you Updated with additional data row for more accurate example", "q_apis": "get columns unique unique time any get", "apis": "columns copy round values diff diff copy sort_values diff groupby diff first diff diff diff isnull where cumsum fillna ffill duplicated sum first reset_index set_index groupby first drop diff all diff copy index reset_index set_index index sort_index get copy index", "code": ["Timestamp = pd.to_datetime\ndata = [{'DatetimeX': Timestamp('2020-01-01 02:00:01'), 'ID': 12345, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 02:00:03'), 'ID': 12345, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 05:00:03'), 'ID': 12345, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 05:03:05'), 'ID': 12345, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 03:00:09'), 'ID': 13333, 'Type': 'D'},\n {'DatetimeX': Timestamp('2020-01-01 02:00:09'), 'ID': 12345, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 02:01:35'), 'ID': 12345, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 02:10:35'), 'ID': 12345, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 02:00:01'), 'ID': 13333, 'Type': 'D'},\n {'DatetimeX': Timestamp('2020-01-01 02:05:35'), 'ID': 13333, 'Type': 'D'},\n {'DatetimeX': Timestamp('2020-01-01 02:00:50'), 'ID': 13333, 'Type': 'E'},\n {'DatetimeX': Timestamp('2020-01-01 02:00:01'), 'ID': 12211, 'Type': 'C'},\n {'DatetimeX': Timestamp('2020-01-01 02:09:50'), 'ID': 13333, 'Type': 'E'},\n {'DatetimeX': Timestamp('2020-01-01 02:11:50'), 'ID': 13333, 'Type': 'E'}]\ndf1 = pd.DataFrame(data)\n\n\ncol_raw = df1.columns\nwhile True:\n    df1.sort_values(['ID', 'Type', 'DatetimeX'], inplace=True)\n    df1['diff1_lt10min'] = df1.groupby(['ID', 'Type'])['DatetimeX'].diff().dt.seconds < 10 * 60\n    df1['tag_group'] = (~df1['diff1_lt10min']).cumsum()\n    if df1.duplicated('tag_group').sum()==0:\n        break\n    df1 = df1.merge((df1.groupby('tag_group')['DatetimeX'].first()\n               .reset_index()\n               .rename(columns={'DatetimeX':'DatetimeX_1st'})),\n              on='tag_group')\n    df1['diff2_lt10min'] = (df1.DatetimeX - df1.DatetimeX_1st).dt.seconds < 10 * 60\n    cond = df1['diff1_lt10min'] & df1['diff2_lt10min']\n    df1 = df1.loc[~cond, col_raw]\ndf1 = df1[col_raw]\n", "# repeat\ncol_raw = df1.columns\ndf4 = df1.copy()\nn_round = 1\nwhile True:\n    print('#'*20, f'round {n_round}', '#'*20)\n    # step 1 sort the values & group by ['Type', 'ID'] calculate the DatetimeX's time diff\n    # notice: the time-diff is not the actual wanted\n    df = df4[col_raw].copy()\n    df.sort_values(['ID', 'Type', 'DatetimeX'], inplace=True)\n    df['diff'] = df.groupby(['Type', 'ID'])['DatetimeX'].diff()\n    print('#'*10, 'step1', '#'*10)\n    print(df)\n\n    # step 2, create a tag column to store the first 10min gap from 'diff' column\n    cond = False \n    cond |= df['diff'].dt.seconds > 10 * 60\n    cond |= df['diff'].isnull()\n    df['tag'] = np.where(cond, 1, 0)\n    df['tag'] = df['tag'].cumsum().fillna(method = 'ffill')\n    print('#'*10, 'step2', '#'*10)\n    print(df)\n\n    # step 3, use 'tag' to judge to stop the while loop or not\n    # tag should be unique\n    break_sign = df.tag.duplicated().sum()\n    if break_sign == 0:\n        break\n    print('#'*10, 'step3', '#'*10)\n    print(break_sign)\n    \n    # step 4:\n        # create a 'DatetimeX_1st' with the 'tag' group's first DatetimeX\n        # create a 'diff2' = 'DatetimeX' - 'DatetimeX_1st'\n    df2 = df.reset_index().set_index('tag')\n    df2['DatetimeX_1st'] = df.groupby('tag').first()['DatetimeX']\n    df2['diff2'] = df2['DatetimeX'] - df2['DatetimeX_1st']\n    print('#'*10, 'step4', '#'*10)\n    print(df2)\n    \n    # step 5:\n        # drop the True < 10min gaps records\n        # 'diff' and 'diff2' should all < 10min\n    cond = (df2['diff2'].dt.seconds < 10 * 60) & (df2['diff'].dt.seconds < 10 * 60)\n    df3 = df2[~cond].copy()\n    print('#'*10, 'step5', '#'*10)\n    print(df3)\n    \n    \n    # step 6:\n        # reset index\n    cols = 'tag DatetimeX   ID  Type'.split()\n    df4 = df3.reset_index().set_index('index').sort_index()[cols]\n    print('#'*10, 'step6', '#'*10)\n    print(df4)\n    \n    n_round += 1\n    print()\n    \n# get result\nresult = df[['DatetimeX', 'ID', 'Type']].copy()\nresult.index.name = None\nprint()\nprint('#'*10, 'result', '#'*10)\nprint(result)\n", "#################### round 1 ####################\n########## step1 ##########\n             DatetimeX     ID Type            diff\n11 2020-01-01 02:00:01  12211    C             NaT\n0  2020-01-01 02:00:01  12345    C             NaT\n1  2020-01-01 02:00:03  12345    C 0 days 00:00:02\n5  2020-01-01 02:00:09  12345    C 0 days 00:00:06\n6  2020-01-01 02:01:35  12345    C 0 days 00:01:26\n7  2020-01-01 02:10:35  12345    C 0 days 00:09:00\n2  2020-01-01 05:00:03  12345    C 0 days 02:49:28\n3  2020-01-01 05:03:05  12345    C 0 days 00:03:02\n8  2020-01-01 02:00:01  13333    D             NaT\n9  2020-01-01 02:05:35  13333    D 0 days 00:05:34\n4  2020-01-01 03:00:09  13333    D 0 days 00:54:34\n10 2020-01-01 02:00:50  13333    E             NaT\n12 2020-01-01 02:09:50  13333    E 0 days 00:09:00\n13 2020-01-01 02:11:50  13333    E 0 days 00:02:00\n########## step2 ##########\n             DatetimeX     ID Type            diff  tag\n11 2020-01-01 02:00:01  12211    C             NaT    1\n0  2020-01-01 02:00:01  12345    C             NaT    2\n1  2020-01-01 02:00:03  12345    C 0 days 00:00:02    2\n5  2020-01-01 02:00:09  12345    C 0 days 00:00:06    2\n6  2020-01-01 02:01:35  12345    C 0 days 00:01:26    2\n7  2020-01-01 02:10:35  12345    C 0 days 00:09:00    2\n2  2020-01-01 05:00:03  12345    C 0 days 02:49:28    3\n3  2020-01-01 05:03:05  12345    C 0 days 00:03:02    3\n8  2020-01-01 02:00:01  13333    D             NaT    4\n9  2020-01-01 02:05:35  13333    D 0 days 00:05:34    4\n4  2020-01-01 03:00:09  13333    D 0 days 00:54:34    5\n10 2020-01-01 02:00:50  13333    E             NaT    6\n12 2020-01-01 02:09:50  13333    E 0 days 00:09:00    6\n13 2020-01-01 02:11:50  13333    E 0 days 00:02:00    6\n########## step3 ##########\n8\n########## step4 ##########\n     index           DatetimeX     ID Type            diff  \\\ntag                                                          \n1       11 2020-01-01 02:00:01  12211    C             NaT   \n2        0 2020-01-01 02:00:01  12345    C             NaT   \n2        1 2020-01-01 02:00:03  12345    C 0 days 00:00:02   \n2        5 2020-01-01 02:00:09  12345    C 0 days 00:00:06   \n2        6 2020-01-01 02:01:35  12345    C 0 days 00:01:26   \n2        7 2020-01-01 02:10:35  12345    C 0 days 00:09:00   \n3        2 2020-01-01 05:00:03  12345    C 0 days 02:49:28   \n3        3 2020-01-01 05:03:05  12345    C 0 days 00:03:02   \n4        8 2020-01-01 02:00:01  13333    D             NaT   \n4        9 2020-01-01 02:05:35  13333    D 0 days 00:05:34   \n5        4 2020-01-01 03:00:09  13333    D 0 days 00:54:34   \n6       10 2020-01-01 02:00:50  13333    E             NaT   \n6       12 2020-01-01 02:09:50  13333    E 0 days 00:09:00   \n6       13 2020-01-01 02:11:50  13333    E 0 days 00:02:00   \n\n          DatetimeX_1st           diff2  \ntag                                      \n1   2020-01-01 02:00:01 0 days 00:00:00  \n2   2020-01-01 02:00:01 0 days 00:00:00  \n2   2020-01-01 02:00:01 0 days 00:00:02  \n2   2020-01-01 02:00:01 0 days 00:00:08  \n2   2020-01-01 02:00:01 0 days 00:01:34  \n2   2020-01-01 02:00:01 0 days 00:10:34  \n3   2020-01-01 05:00:03 0 days 00:00:00  \n3   2020-01-01 05:00:03 0 days 00:03:02  \n4   2020-01-01 02:00:01 0 days 00:00:00  \n4   2020-01-01 02:00:01 0 days 00:05:34  \n5   2020-01-01 03:00:09 0 days 00:00:00  \n6   2020-01-01 02:00:50 0 days 00:00:00  \n6   2020-01-01 02:00:50 0 days 00:09:00  \n6   2020-01-01 02:00:50 0 days 00:11:00  \n########## step5 ##########\n     index           DatetimeX     ID Type            diff  \\\ntag                                                          \n1       11 2020-01-01 02:00:01  12211    C             NaT   \n2        0 2020-01-01 02:00:01  12345    C             NaT   \n2        7 2020-01-01 02:10:35  12345    C 0 days 00:09:00   \n3        2 2020-01-01 05:00:03  12345    C 0 days 02:49:28   \n4        8 2020-01-01 02:00:01  13333    D             NaT   \n5        4 2020-01-01 03:00:09  13333    D 0 days 00:54:34   \n6       10 2020-01-01 02:00:50  13333    E             NaT   \n6       13 2020-01-01 02:11:50  13333    E 0 days 00:02:00   \n\n          DatetimeX_1st           diff2  \ntag                                      \n1   2020-01-01 02:00:01 0 days 00:00:00  \n2   2020-01-01 02:00:01 0 days 00:00:00  \n2   2020-01-01 02:00:01 0 days 00:10:34  \n3   2020-01-01 05:00:03 0 days 00:00:00  \n4   2020-01-01 02:00:01 0 days 00:00:00  \n5   2020-01-01 03:00:09 0 days 00:00:00  \n6   2020-01-01 02:00:50 0 days 00:00:00  \n6   2020-01-01 02:00:50 0 days 00:11:00  \n########## step6 ##########\n       tag           DatetimeX     ID Type\nindex                                     \n0        2 2020-01-01 02:00:01  12345    C\n2        3 2020-01-01 05:00:03  12345    C\n4        5 2020-01-01 03:00:09  13333    D\n7        2 2020-01-01 02:10:35  12345    C\n8        4 2020-01-01 02:00:01  13333    D\n10       6 2020-01-01 02:00:50  13333    E\n11       1 2020-01-01 02:00:01  12211    C\n13       6 2020-01-01 02:11:50  13333    E\n\n#################### round 2 ####################\n########## step1 ##########\n                DatetimeX     ID Type            diff\nindex                                                \n11    2020-01-01 02:00:01  12211    C             NaT\n0     2020-01-01 02:00:01  12345    C             NaT\n7     2020-01-01 02:10:35  12345    C 0 days 00:10:34\n2     2020-01-01 05:00:03  12345    C 0 days 02:49:28\n8     2020-01-01 02:00:01  13333    D             NaT\n4     2020-01-01 03:00:09  13333    D 0 days 01:00:08\n10    2020-01-01 02:00:50  13333    E             NaT\n13    2020-01-01 02:11:50  13333    E 0 days 00:11:00\n########## step2 ##########\n                DatetimeX     ID Type            diff  tag\nindex                                                     \n11    2020-01-01 02:00:01  12211    C             NaT    1\n0     2020-01-01 02:00:01  12345    C             NaT    2\n7     2020-01-01 02:10:35  12345    C 0 days 00:10:34    3\n2     2020-01-01 05:00:03  12345    C 0 days 02:49:28    4\n8     2020-01-01 02:00:01  13333    D             NaT    5\n4     2020-01-01 03:00:09  13333    D 0 days 01:00:08    6\n10    2020-01-01 02:00:50  13333    E             NaT    7\n13    2020-01-01 02:11:50  13333    E 0 days 00:11:00    8\n\n########## result ##########\n             DatetimeX     ID Type\n11 2020-01-01 02:00:01  12211    C\n0  2020-01-01 02:00:01  12345    C\n7  2020-01-01 02:10:35  12345    C\n2  2020-01-01 05:00:03  12345    C\n8  2020-01-01 02:00:01  13333    D\n4  2020-01-01 03:00:09  13333    D\n10 2020-01-01 02:00:50  13333    E\n13 2020-01-01 02:11:50  13333    E\n"], "link": "https://stackoverflow.com/questions/65821366/python-removing-rows-with-time-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Summary I am using Python 2.7. I have a data frame with all categorical variables i.e. data type is string. I would like to transform unique row values of one column into multiple columns. Additionally, the values of those resulting columns must have the corresponding values from another column. To describe in detail, I have provided a reproducible data frame and expected output for your reference. Dataframe that needs transposing can be created as follows: The data frame that needs transposing looks like this: The expected final output should like this: Note: The objective is transposition. I am not overly concerned whether the blank spaces are NULL values or zeroes.", "q_apis": "get columns all transform unique values columns values columns values describe values", "apis": "DataFrame columns columns where astype bool where astype bool columns columns", "code": ["import pandas as pd\ncodes = ['codeA','codeB', 'codeC']\nvariables = ['textA','textA','textB']\ndataset = list(zip(codes,variables))\ndf = pd.DataFrame(data = dataset, columns=['codes','variables'])\ndf['string'] = 'string1'\n\ndf = pd.get_dummies(df, columns=['variables'])\ndf.variables_textA = df.codes.where(df.variables_textA.astype(bool),0)\ndf.variables_textB = df.codes.where(df.variables_textB.astype(bool),0)\ncolumns = ['variables_textA', 'variables_textB','string']\ndf = df[columns]\n"], "link": "https://stackoverflow.com/questions/61038220/how-to-transform-python-data-frame-such-that-unique-row-values-are-transposed-to"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe as shown here. There are many more columns in that frame that are not important concerning the task. I would like to make a list out of certain colums so the first sentence (sente=21) and every other looks something like that. Meaing that every sentence has an unique entry for itself. I already have a function to do this for one sentence but I can not figure out how to do it for all sentences (sentences that have the same sente value) in the frame. ` As my frame is very large there is no way to use constructions like this: Any ideas?", "q_apis": "get columns columns first unique all value", "apis": "itertuples append", "code": ["from collections import defaultdict\n\nd = defaultdict(list)\n\nfor _, num, *data in df[['sente', 'value', 'pos', 'id']].itertuples():\n    d[num].append(data)\n", "defaultdict(list,\n            {21: [('I', 'a', 1),\n                  ('have', 'b', 2),\n                  ('a', 'b', 3),\n                  ('cat', 'a', 4),\n                  ('!', 'd', 5)],\n             22: [('My', 'a', 1),\n                  ('cat', 'a', 2),\n                  ('is', 'b', 3),\n                  ('cute', 'a', 4),\n                  ('.', 'd', 5)]})\n"], "link": "https://stackoverflow.com/questions/50089580/making-a-list-ouf-of-values-in-a-dataframe-depending-on-values-in-another-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "We can assign an empty dataframe with pandas. The dataframe is empty containing nothing in it,create a empty data file containing nothing in it. The file is empty. How to make pandas read an empty file and assign it as a empty dataframe such as shows ? Is there a way to add some argument to make the below commands running without error info.", "q_apis": "get columns assign empty empty empty empty empty assign empty add info", "apis": "DataFrame DataFrame", "code": ["import pandas as pd\nzero = pd.DataFrame()\nfh = open('/tmp/test.txt','r')\nfh.close()\ntry:\n    newzero = pd.read_csv('test.txt',header=None)\nexcept pd.errors.EmptyDataError:\n    newzero = pd.DataFrame()\n        \n"], "link": "https://stackoverflow.com/questions/62610286/how-to-make-pandas-read-empty-data-file-as-an-empty-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The dataframe I am working with looks like this: I am trying to calculate a similarity score between the two columns cap1 and cap2. However, I want to create a new column FSim that stores this similarity score for each row. The code I have implemented till now is: But I am getting the same similarity scored stored for each row like this: Same value for all the rows. How to get properly stored the score for each row?", "q_apis": "get columns between columns now value all get", "apis": "iterrows append append transform iloc", "code": ["def preproc_and_lemmatize(x):\n  v1 = pre_process(x)\n  return lemmatize_sentence(v1)\n\ndef calc_sim(x, y):\n  x2 = preproc_and_lemmatize(x)\n  y2 = preproc_and_lemmatize(y)\n  feature_vectors = tfidf_vectorize.transform([x2, y2])\n  return get_cosine_similarity(feature_vectors[0], feature_vectors[1])\n\nmerged['fsim'] = [\n  calc_sim(x, y) for x, y in zip(merged['cap1'], merged['cap2'])\n]\n", "merged[\"fsim\"] = 0\nfor i, row in merged.iterrows():\n    captions = []\n    captions.append(row['cap1'])\n    captions.append(row['cap2'])\n\n    for c in range(len(captions)):\n        captions[c] = pre_process(captions[c])\n        captions[c] = lemmatize_sentence(captions[c])\n\n    feature_vectors = tfidf_vectorizer.transform(captions)\n\n    fsims = get_cosine_similarity(feature_vectors[0], feature_vectors[1])\n    merged['fsim'].iloc[i] = fsims\n"], "link": "https://stackoverflow.com/questions/61692451/how-to-store-a-calculated-value-in-new-column-by-iterating-through-each-row-in-a"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes, one with the input info and one with the output: The output its a array of arrays. Variable in quantity. What I need is map the index and append every vector in a row, like this: the df its very large so im trying to avoid a loop if its possible. thank you in advance estimates.", "q_apis": "get columns info array map index append", "apis": "DataFrame DataFrame index itertuples join", "code": ["input_vectors = pd.DataFrame({'vectors':[[['D', .5],['E',.3]],\n                                         [['A',.3]],\n                                         [['B',.8],['C',.5],['H',.2]]]})\ninput_vectors\n", "                          vectors\n0            [[D, 0.5], [E, 0.3]]\n1                      [[A, 0.3]]\n2  [[B, 0.8], [C, 0.5], [H, 0.2]]\n", "   index col1 col2\n0      0    A    B\n1      1    B    H\n2      2    C    D\n", "pd.concat([pd.DataFrame(x, index=[i]*len(x)) \n            for i, x in input_vectors.itertuples()])\\\n  .join(df_input)\n", "   0    1  index col1 col2\n0  D  0.5      0    A    B\n0  E  0.3      0    A    B\n1  A  0.3      1    B    H\n2  B  0.8      2    C    D\n2  C  0.5      2    C    D\n2  H  0.2      2    C    D\n"], "link": "https://stackoverflow.com/questions/56433248/create-dataframe-mapping-a-list-of-arrays"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am a bit new to Python and I have the following requirement. From this data frame I wanted to generate the below dataframe- The calculation involved here is of weighted average- Source1 - For a given Date and for a given Source, we need to obtain the total number of tickets. For Source1, there are 3 records (tickets) on a given date(3/1/2018). The \"Value\" column for these tickets have to be sorted in ascending order. Then based on the count of tickets, the highest weightage has to be given to the least \"Value\" Overall Result column is calculated as for a given date, how many were 1s divided by overall count of tickets for that date Date - 3/1/2018 => 1+1+0+1+0+1+1+0+1+1/10=0.66 I have huge amount of data for which these calculations has to be done. The number of Source column values can also be immense. In modified dataframe, I want it as a column. One way to do is to write the logic in a function and call upon every record. Any suggestions or help are welcome. Thanks in advance.", "q_apis": "get columns date count date count date values", "apis": "sort_values pivot_table index columns values sum cumsum fillna groupby apply sum size", "code": ["import numpy as np\nimport pandas as pd\n\ndf2 = df.sort_values('Value').pivot_table(\n        index='Date', \n        columns='Source',\n        values='Value',\n        aggfunc = lambda x: (x*np.arange(len(x), 0, -1)).sum()/np.arange(len(x), 0, -1).cumsum()[-1]).fillna(0)\n\ndf2['Result'] = df.groupby('Date').Result.apply(lambda x: x.sum()/np.size(x))\n", "Source     Source1   Source2   Source3  Source4   Source5    Result\nDate                                                               \n3/1/2018  0.945753  0.958896  0.963069  0.96348  0.000000  0.714286\n3/2/2018  0.000000  0.000000  0.000000  0.00000  0.955507  0.857143\n"], "link": "https://stackoverflow.com/questions/52933145/obtain-data-in-the-desired-format"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to practice using the read_html function from pandas by crawling a table but I got an error. My code as follow: The above code returned errors and didn't work out, so I tried the one following, but it still doesn't work. I don't exactly know what's wrong. Can anyone enlighten me on that? Thanks!", "q_apis": "get columns read_html", "apis": "all get", "code": ["import requests\nimport pandas as pd\n\nurl = \"https://www.pokemondb.net/pokedex/all\"\nhtml = requests.get(url)\n\ndfs = pd.read_html(html.text)\n\nprint(dfs)\n"], "link": "https://stackoverflow.com/questions/62788296/read-html-function-failed-to-crawl-pokemongo-data-tables"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that looks like this: The df contains the max value of temperature for a specific location every 12 hours from May - July 11 during 2000-2020. I want to count the number of times that the value is >90 and then store that value in a column where the row is the year. Should I use groupby to accomplish this? Expected output:", "q_apis": "get columns contains max value count value value where year groupby", "apis": "compare gt sum groupby sum reset_index", "code": ["# extract the years from dates\nyears = df['Date'].dt.year\n\n# compare `DFW` with `90`\n# gt90 will be just True or False\ngt90 = df['DFW'].gt(90)\n\n# sum the `True` by years\noutput = gt90.groupby(years).sum()\n\n# set the years as normal column:\noutput = output.reset_index()\n"], "link": "https://stackoverflow.com/questions/62881665/count-values-greater-than-threshold-and-assign-to-appropriate-year-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to convert all the .bin files in a folder to .txt file in Python, This is what I tried I want to save the each text file as the same name of the bin file. How can i do that. I am new to coding, Please help me to learn.", "q_apis": "get columns all name", "apis": "replace values", "code": ["            np.savetxt( name.replace(\"bytes.bin\", \".txt\"), df.values, fmt='%d')\n"], "link": "https://stackoverflow.com/questions/51479146/i-am-trying-to-convert-all-the-bin-files-in-a-folder-to-txt-files-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe which looks as follows: The ColB (any component out of two shown) & ColC combination make a unique combination. I want to create a dataframe from this dataframe which will look like the following I have tried creating new columns if there is a single element in colB, but cannot get how to do using the semicolon splitter and then how to create columns of tuples. If I do not have any semicolon in ColB, then I can use However the presence of semicolon in ColB I am getting issues, as how to split that string into multiple ones and assign the tuples. Any help will be very much appreciated.", "q_apis": "get columns any unique columns get columns any assign", "apis": "merge explode apply merge reset_index drop merge", "code": ["def merge(row):\n    return pd.Series({\n            \"colAA\": (row.colB, row.colC),\n            \"colBB\": (row.colC, row.colA),\n        })\n\ndf['colB'] = df['colB'].str.split(';')\ndf = df.explode('colB')\nnewDf = df.apply(merge, axis=1).reset_index(drop=True)\n", "df['colB'] = df['colB'].str.split(';')\ndf = df.explode('colB')\n\n# output\n    colA    colB    colC\n0   rqp 129 a\n1   pot 217 u\n1   pot 345 u\n2   ghay    716 b\n3   rbba    217 d\n", "def merge(row):\n    for b in row.colB.split(\";\"):\n         return pd.Series({\n            \"colAA\": (b, row.colC),\n            \"colBB\": (row.colC, row.colA),\n\n        })\n", "newDf = df.apply(merge, axis=1).reset_index(drop=True)\n\n# output\n    colAA        colBB\n0   (129, a)    (a, rqp)\n1   (217, u)    (u, pot)\n2   (345, u)    (u, pot)\n3   (716, b)    (b, ghay)\n4   (217, d)    (d, rbba)\n5   (345, d)    (d, rbba)\n6   (612, a)    (a, tary)\n7   (811, a)    (a, tary)\n8   (760, a)    (a, tary)\n9   (716, t)    (t, kals)\n"], "link": "https://stackoverflow.com/questions/62165220/pandas-splitting-columns-and-creating-columns-of-tuples"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I was trying to return only those rows when any value in column director has a separator '|'. But it is not filtering on the separator basis and instead shows all the rows. Please let me know the possible issue with this. I tried the following: But it shows following It should only show rows that with id 135397 and 766341", "q_apis": "get columns any value all", "apis": "bool bool", "code": ["df1 = df[df.director.str.contains(\"\\|\")]\nprint (df1)\n      id  popularity     budget                      Cast  \\\n2  76757    6.189369  176000003  Mila Kunis|Channing Lana   \n\n                    director  \n2  Wachowski|Lilly Wachowski  \n", "df2 = df[~df.director.str.contains(\"\\|\")]\nprint (df2)\n       id  popularity     budget                       Cast         director\n0  135397   32.985763  150000000    Chris Pratt|Irrfan Khan  Colin Trevorrow\n1   76341   28.419936  150000000  Tom Hardy|Charlize Theron    George Miller\n", "print (df.director.str.contains(\"\\|\"))\n0    False\n1    False\n2     True\nName: director, dtype: bool\n\nprint (~df.director.str.contains(\"\\|\"))\n0     True\n1     True\n2    False\nName: director, dtype: bool\n"], "link": "https://stackoverflow.com/questions/56681228/pandas-filtering-rows-with-data-in-particular-column-on-the-separator-basis"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In a df I have some columns with answers on a 4-point Likert scale which I need to invert meaning I need to flip the values in each row of this column: 4->1, 2->3, 3->2, 4->1 I've tried this: The problem is that since it's not in the same command, the lines are of course ran seperately. If for example 3 was changed to 2 in the next line it will be changed to 3 again. Any ideas how to prevent this problem/ how to write a command which does all of these conditional changes at once? Thank you very much in advance!", "q_apis": "get columns columns values all at", "apis": "abs", "code": ["questDay1Df['STAI_State_01'] = np.abs(questDay1Df['STAI_State_01'] - 5)\n", "questDay1Df['STAI_State_01'] = questDay1Df['STAI_State_01'].replace({\n    1: 4, \n    2: 3, \n    3: 2, \n    4: 1,\n})\n"], "link": "https://stackoverflow.com/questions/64716913/change-column-values-in-pandas-df-under-different-conditions-invert-answers-on"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Ok so I have a dataframe which contains timeseries data that has a multiline index for each columns. Here is a sample of what the data looks like and it is in csv format. Loading the data is not an issue here. What I want to do is to be able to create a boxplot with this data grouped according to different catagories in a specific line of the multiindex. For example if I were to group by 'SPECIES' I would have the groups, 'aq', 'gr', 'mix', 'sed' and a box for each group at a specific time in the timeseries. I've tried this: but it gives me a boxplot (flat line) for each point in the group rather than for the grouped set. Is there an easy way to do this? I don't have any problems grouping as I can aggregate the groups any which way I want, but I can't get them to boxplot.", "q_apis": "get columns contains index columns sample boxplot groups at time boxplot any aggregate groups any get boxplot", "apis": "groupby T boxplot", "code": ["grouped = data['2013-08-17'].groupby(axis=1, level='SPECIES').T\ngrouped.boxplot()\n"], "link": "https://stackoverflow.com/questions/18498690/boxplot-with-pandas-groupby-multiindex-for-specified-sublevels-from-multiindex"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm looking to select all columns that contain a string value in any of the rows and add that column to a list for manipulation later on. Could you please help find the way? extract from my df: I'd like to see a list of columns that contain value of 'Q1', 'Q2', 'Q3', 'Q4' anywhere in the column. In this case columns 1 and 3.", "q_apis": "get columns select all columns value any add columns value columns", "apis": "applymap any bool", "code": ["df.applymap(lambda x : 'Q' in str(x)).any()\nOut[268]: \n1     True\n2    False\n3     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/56193410/pandas-dataframe-select-all-columns-that-cointain-specific-string-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame that looks like this: I want to iterate through the data frame and using make another column that will categorize those jobs. There are 8 categories. The output will be : here's what I've tried so far: of course , I'm a noob at programming and this throws the error:", "q_apis": "get columns categories at", "apis": "join keys", "code": ["cat_dict = {\"Support\":\"Support\", \"designer\":\"Designer\", \"Manager\": \"Management\"}\n\ndf['category'] = (df['Title'].str.extract(fr\"\\b({'|'.join(cat_dict.keys())})\\b\")[0]\n                    .map(cat_dict)\n                 )\n"], "link": "https://stackoverflow.com/questions/62579298/create-new-category-column-using-pandas-string-contains-and-loops"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Below is some dummy data that reflects the data I am working with. df1: df2: DataFrame details: First off, Loc1 will correspond with P Change_1 and Loc2 corresponds to P Change_2, etc. Looking at Loc1 first, I want to either fill up the DataFrame containing Loc1 and Loc2 with the relevant values or compute a new dataframe that has columns Calc1 and Calc2. The calculation: I want to start with the 1994 value of Loc1 and calculate a new value for 1993 by taking Loc1 1993 = Loc1 1994 + (Loc1 1994 * P Change_1 1993). With the values filled in it would be 2.5415 +(-0.313341 * 2.5415) which equals about 1.74514. This 1.74514 value will replace the NaN value in 1993, and then I want to use that calculated value to get a value for 1992. This means we now compute Loc1 1992 = Loc1 1993 + (Loc1 1993 * P Change_1 1992). I want to carry out this operation row-wise until it gets the earliest value in the timeseries. What is the best way to go about implementing this row-wise equation? I hope this makes some sense and any help is greatly appreciated!", "q_apis": "get columns DataFrame at first DataFrame values columns start value value values equals value replace value value get value now value any", "apis": "columns columns columns columns", "code": ["df = pd.merge(df1, df2, how='inner', right_index=True, left_index=True)   # merging dataframes on date index\ndf['count'] = range(len(df))    # creating a column, count for easy operation\n\n\n# divides dataframe in two part, one part above the not NaN row and one below\nda1 = df[df['count']<=df.dropna().iloc[0]['count']]  \nda2 = df[df['count']>=df.dropna().iloc[0]['count']]\n\n\nda1.sort_values(by=['count'],ascending=False, inplace=True)\ng=[da1,da2]\nnum_col=len(df1.columns)\n\nfor w in range(len(g)):\n    list_of_col=[]\n    count = 0\n    list_of_col=[list() for i in range(len(g[w]))]\n    for item, rows in g[w].iterrows():\n        n=[]\n        if count==0:\n            for p in range(1,num_col+1):\n                n.append(rows[f'Loc{p}'])\n        else:\n            for p in range(1,num_col+1):\n                n.append(list_of_col[count-1][p-1]+  list_of_col[count-1][p-1]* rows[f'P Change_{p}'])\n        list_of_col[count].extend(n)\n        count+=1\n    tmp=[list() for i in range(num_col)]\n    for d_ in range(num_col):\n        for x_ in range(len(list_of_col)):\n            tmp[d_].append(list_of_col[x_][d_])\n    z1=[]\n    z1.extend(tmp)\n    for i in range(num_col):\n        g[w][f'Loc{i+1}']=z1[i]\n\nda1.sort_values(by=['count'] ,inplace=True)\nfinal_df = pd.concat([da1, da2[1:]])\n\ncalc_df = pd.DataFrame()\nfor i in range(num_col):\n    calc_df[f'Calc{i+1}']=final_df[f'Loc{i+1}']\nprint(calc_df)\n", "df1.columns = [f'P Change_{i+1}' for i in range(len(df1.columns))]\ndf2.columns = [f'Loc{i+1}' for i in range(len(df2.columns))]\n"], "link": "https://stackoverflow.com/questions/62156626/pandas-calculate-row-values-based-on-prior-row-value-update-the-result-to-be"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have very similar question to the one on this link , with only difference being to have multiple conditions met. Let us assume that we have (sorted) dataframe, similar to the one in the example, with extra column: And I would like to have extra column called PREV_TIME next to each of the rows, which will contain the previous value of TIME column where both conditions to have the column VALUE equals to 1, and column EXTRA_FILTER equals to A are met, something like this:", "q_apis": "get columns difference value where equals equals", "apis": "eq eq shift fillna", "code": ["df[\"PREV_TIME\"] = df[df[\"VALUE\"].eq(1) & df[\"EXTRA_FILTER\"].eq(\"A\")][\"TIME\"].shift()\ndf[\"PREV_TIME\"].fillna(\"\", inplace=True)\nprint(df)\n", "     TIME  VALUE EXTRA_FILTER PREV_TIME\n0   23:01      0            A          \n1   23:02      0            A          \n2   23:03      1            A          \n3   23:04      0            B          \n4   23:05      0            B          \n5   23:06      1            B          \n6   23:07      0            A          \n7   23:08      0            A          \n8   23:09      0            A          \n9   23:10      0            A          \n10  23:11      1            A     23:03\n11  23:12      0            A          \n12  23:13      0            A          \n13  23:14      0            A          \n14  23:15      0            A          \n15  23:16      1            A     23:11\n"], "link": "https://stackoverflow.com/questions/64147365/how-can-i-get-a-previous-row-from-where-the-conditions-are-met-in-data-frame-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I used array that is data to calculate entropy like I made for loop to calculate entropy and make it to list Output is I want to make the output to pandas dataframe like plz help...", "q_apis": "get columns array", "apis": "DataFrame", "code": ["import pandas as pd\ndf = pd.DataFrame(result[:,1])\n", "    0\n0   1.193770\n1   1.212889\n2   1.423144\n3   1.047516\n4   1.006879\n5   1.376627\n"], "link": "https://stackoverflow.com/questions/58127321/how-to-make-list-to-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a CSV file containing multiple columns(almost 100). How can I filter multiple columns at once using certain criteria in Python? To be more precise, many of the columns are of no use to me. How can the file be filtered? PS: I am a beginner user.", "q_apis": "get columns columns filter columns at columns", "apis": "replace replace dropna all", "code": ["import numpy as np\nimport pandas as pd\n\ndf = df.replace(0, np.nan)\n\n", "df = df.replace(0, np.nan).dropna(axis=1, how=all)\n"], "link": "https://stackoverflow.com/questions/61156308/deleting-multiple-columns-of-csv-files-based-on-certain-conditions-on-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Objective: For every cell in Vendor: I want to check if EVERY word in the vendor's name exists in the Names Database. i.e. Adam AND Smith must BOTH exist to make IsPerson = TRUE I know that I can do this using lambda.apply() and other ways but all of them are loops based. I'd like to make this as fast and as efficient as possible because I have 1.2 million rows. I've heard about Numpy Vectorization but not sure how to use it when I need to run some routine on the individual contents of each cell. Thanks", "q_apis": "get columns name apply all", "apis": "ffill isin all DataFrame join DataFrame join mean std ffill isin all mean std", "code": ["df1['IsPerson'] = (df1['Vendor'].str.split(expand=True)\n                                .ffill(axis=1)\n                                .isin(df2['Persons'].tolist())\n                                .all(axis=1))\n", "s = set(df2['Persons'])\ndf1['IsPerson'] = ~df1['Vendor'].map(lambda x: s.isdisjoint(x.split()))\n", "np.random.seed(123)\n\nN = 100000\nL = list('abcdefghijklmno ')\n\ndf1 = pd.DataFrame({'Vendor': [''.join(x) for x in np.random.choice(L, (N, 5))]})\ndf2 = pd.DataFrame({'Persons': [''.join(x) for x in np.random.choice(L, (N * 10, 5))]})\n", "In [133]: %%timeit\n     ...: s = set(df2['Persons'])\n     ...: df1['IsPerson1'] = ~df1['Vendor'].map(lambda x: s.isdisjoint(x.split()))\n     ...: \n470 ms \u00b1 7.02 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [134]: %%timeit\n     ...: df1['IsPerson2'] = (df1['Vendor'].str.split(expand=True)\n     ...:                                 .ffill(axis=1)\n     ...:                                 .isin(df2['Persons'].tolist())\n     ...:                                 .all(axis=1))\n     ...:                                 \n858 ms \u00b1 18.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"], "link": "https://stackoverflow.com/questions/59318438/pandas-utilize-numpy-vectorization-in-a-user-defined-function-instead-of-using-l"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a below data frame file with two columns \"Message\" and \"name\". I have to do two things using pandas. In \"name\" column if there is only \"Urgent\" or \"My Filings\" or both but not anything else in list then add \"Uncategorized\" in the list. Like below image. After that if there is \"['Urgent']\" in name column, move it in the next column called \"urgent_flag\" else write \"['Unflag_urgent']\". Or if there is \"['My Filings']\", move it in the next column called \"myfilings_flag\" else write \"['Unflag_myfilings']\". Or if there is both \"['Urgent', 'My Filings']\", then move \"['Urgent']\" in \"urgent_flag\" column and \"['My Filings']\" in \"myfilings_flag\" column. Like below image. How can I do this in python?", "q_apis": "get columns columns name name add name", "apis": "apply all apply apply any apply any iloc first", "code": ["df['new_name'] = df.name.apply(lambda x: ['Unauthorized'] if all(y in ('Urgent', 'My Filings') for y in x) else x)\ndf['new_name'] = df.new_name.apply(lambda x: [y for y in x if y not in ('Urgent', 'My Filings')])\ndf['urgent_flag'] = df.name.apply(lambda x: ['Urgent'] if any(y in 'Urgent' for y in x) else ['Unflag_urgent'])\ndf['myfilings_flag'] = df.name.apply(lambda x: ['My Filings'] if any(y in 'My Filings' for y in x) else ['Unflag_myfilings'])\n\nprint (df.iloc[:,1:]) #ignoring first column as its irrelevant for this problem statement\n", "                      name        new_name      urgent_flag      myfilings_flag\n0                  [Order]         [Order]  [Unflag_urgent]  [Unflag_myfilings]\n1        [Hearing, Urgent]       [Hearing]         [Urgent]  [Unflag_myfilings]\n2   [Pleading, My Filings]      [Pleading]  [Unflag_urgent]        [My Filings]\n3  [Discovery, My Filings]     [Discovery]  [Unflag_urgent]        [My Filings]\n4             [My Filings]  [Unauthorized]  [Unflag_urgent]        [My Filings]\n5                 [Urgent]  [Unauthorized]         [Urgent]  [Unflag_myfilings]\n6     [Urgent, My Filings]  [Unauthorized]         [Urgent]        [My Filings]\n"], "link": "https://stackoverflow.com/questions/66932466/how-to-add-a-string-in-list-in-column-and-then-preprocess-it"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following DataFrame: I have used following line to groupby I am trying to group by this DataFrame in the following format. But values of High must not change. Is it possible to do this or shall i create a new DataFrame? EDIT: Here is the expected output", "q_apis": "get columns DataFrame groupby DataFrame values DataFrame", "apis": "values astype columns truncate mean std values mean std", "code": ["N = 3\ndf['New'] = np.floor(df['High'].values * 10 ** N) / 10 ** N\nprint (df)\n      High  counts   Total    New\n11  1.2492       2  2.4984  1.249\n1   1.2466       2  2.4932  1.246\n20  1.2574       1  1.2574  1.257\n19  1.2547       1  1.2547  1.254\n18  1.2523       1  1.2523  1.252\n", "df['New'] = (df['High'].values * 10**N).astype(int) / 10**N\n", "#[5000 rows x 4 columns]\ndf = pd.concat([df] * 1000, ignore_index=True)\n\nIn [213]: %timeit df['New1'] = df['High'].map(truncate)\n8.27 ms \u00b1 667 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [214]: %timeit df['New1'] = np.floor(df['High'].values * 10 ** N) / 10 ** N\n220 \u00b5s \u00b1 2.98 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/49702440/how-can-i-apply-a-format-to-a-groupby-function"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "We have a dataframe with three different columns, like shown in the example above (df). The goal of this task is to replace the first element of the column 2 by a np.nan, everytime the letter in the column 1 changes. Since the database under study is very big, it cannot be used a for loop. Also every solution that involves a shift is excluded because it is too slow. I believe the easiest way is to use the groupby and the head method, however I don't know how to replace in the original dataframe. Examples: to select the elements that we want to change, we can do the following: However in the original dataframe nothing changes. The goal is to obtain the following: Edit: Based on comments, we won't returning to a group already seen, e.g. is not possible.", "q_apis": "get columns columns replace first shift groupby head replace select", "apis": "ne shift values loc", "code": ["df[2] = np.ma.masked_array(df[2], df[1].ne(df[1].shift(1))).filled(np.nan)\n# array([nan, 2.1, 3.1, nan, 4.3, 2.1, 4.3])\n", "a = df[1].values\ndf.loc[np.roll(a, 1)!=a, 2] = np.nan\n", "   0  1    2\n0  A  Z  NaN\n1  B  Z  2.1\n2  C  Z  3.1\n3  D  X  NaN\n4  E  X  4.3\n5  E  X  2.1\n6  F  X  4.3\n"], "link": "https://stackoverflow.com/questions/52247112/change-column-in-pre-selected-elements-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Trying to create new column with values that meet specific conditions. Below I have set out code which goes some way in explaining the logic but does not produce the correct output: Incorrect output: Explanation why output is incorrect: Value in index row 1 for column is incorrect because this value is from index row 6 which has has a datetime of which is greater than the datetime contained in index row 1. Value in index row 3 for column is incorrect because this value is from index row 7 which has a datetime of which is greater than the datetime contained in index row . Correct output: Explanation why output is correct: Value in index row 1 for column is correct because we are looking for the minimum value in column which has a datetime in column less than the datetime in index row 1 for column and greater or equal to the datetime in index row 1 for column Value in index row 3 for column is correct because we are looking for the maximum value in column which has a datetime in column less than the datetime in index row 3 for column and greater or equal to the datetime in index row 3 for column Considerations: Maybe is not best suited for this task and some sort of function might serve the task better? Also, maybe I need to create some sort of dynamic to use as a starting point for each row? Request Please can anyone out there help me amend my code to achieve the correct output", "q_apis": "get columns values index value index index index value index index index value index index index value index index", "apis": "eq eq reset_index merge assign merge assign groupby index max rename eq eq reset_index merge all DataFrame merge reset_index assign merge reset_index assign min sort_values groupby index head set_index index filter rename columns eq eq reset_index merge reset_index sort_values groupby index tail set_index index filter rename columns", "code": ["import pandas as pd\n\ndf['date'] = pd.to_datetime(df['date'])\ndf['colourDate'] = pd.to_datetime(df['colourDate'])\n", "# Subset of rows we need to do this for\ndfmin = df[df.type.eq(1) & df.colour.eq('red')].reset_index()\n\n# To each row merge all rows from the original DataFrame\ndfmin = dfmin.merge(df[['date', 'minPixel']], how='cross')\n# If pd.version < 1.2 instead use: \n#dfmin = dfmin.assign(t=1).merge(df[['date', 'minPixel']].assign(t=1), on='t')\n\n# Only keep rows between the dates, then among those find the min minPixel\nsmin = (dfmin[dfmin.date_y.between(dfmin.date_x, dfmin.colourDate)]\n            .groupby('index')['minPixel_y'].min()\n            .rename('pixel_limit'))\n#index\n#1    14\n#Name: pixel_limit, dtype: int64\n", "# Max is basically a mirror\ndfmax = df[df.type.eq(1) & df.colour.eq('blue')].reset_index()\n\ndfmax = dfmax.merge(df[['date', 'maxPixel']], how='cross')\n#dfmax = dfmax.assign(t=1).merge(df[['date', 'maxPixel']].assign(t=1), on='t')\n\nsmax = (dfmax[dfmax.date_y.between(dfmax.date_x, dfmax.colourDate)]\n           .groupby('index')['maxPixel_y'].max()\n           .rename('pixel_limit'))\n", "df['pixel_limit'] = pd.concat([smin, smax])\n\n                 date  type colour  maxPixel  minPixel          colourDate  pixel_limit\n0 2019-08-06 09:00:00   0.0   blue       255        86 2019-08-06 12:00:00          NaN\n1 2019-08-06 12:00:00   1.0    red      7346        96 2019-08-08 16:00:00         14.0\n2 2019-08-06 18:00:00   NaN    NaN        32        14 2019-08-06 23:00:00          NaN\n3 2019-08-06 21:00:00   1.0   blue      5184      3540 2019-08-06 22:00:00       5184.0\n4 2019-08-07 09:00:00   NaN    NaN       600       528 2019-08-08 09:00:00          NaN\n5 2019-08-07 16:00:00   NaN    NaN       322       300 2019-08-09 16:00:00          NaN\n6 2019-08-08 17:00:00   0.0   blue        72        12 2019-08-08 23:00:00          NaN\n7 2019-08-09 16:00:00   0.0    red      6000      4009 2019-08-11 16:00:00          NaN\n", "# Subset of rows we need to do this for\ndfmin = df[df.type.eq(1) & df.colour.eq('red')].reset_index()\n\n# To each row merge all rows from the original DataFrame\ndfmin = dfmin.merge(df[['date', 'minPixel']].reset_index(), how='cross', \n                    suffixes=['', '_match'])\n# For older pandas < 1.2\n#dfmin = (dfmin.assign(t=1)\n#              .merge(df[['date', 'minPixel']].reset_index().assign(t=1), \n#                     on='t', suffixes=['', '_match'])) \n\n# Only keep rows between the dates, then among those find the min minPixel row. \n# A bunch of renaming. \nsmin = (dfmin[dfmin.date_match.between(dfmin.date, dfmin.colourDate)]\n            .sort_values('minPixel_match', ascending=True)\n            .groupby('index').head(1)\n            .set_index('index')\n            .filter(like='_match')\n            .rename(columns={'minPixel_match': 'pixel_limit'}))\n", "dfmax = df[df.type.eq(1) & df.colour.eq('blue')].reset_index()\ndfmax = dfmax.merge(df[['date', 'maxPixel']].reset_index(), how='cross', \n                    suffixes=['', '_match'])\n\nsmax = (dfmax[dfmax.date_match.between(dfmax.date, dfmin.colourDate)]\n            .sort_values('maxPixel_match', ascending=True)\n            .groupby('index').tail(1)\n            .set_index('index')\n            .filter(like='_match')\n            .rename(columns={'maxPixel_match': 'pixel_limit'}))\n", "result = pd.concat([df, pd.concat([smin, smax])], axis=1)\n", "                  date  type colour  maxPixel  minPixel           colourDate  index_match           date_match  pixel_limit\n0  2019-08-06 09:00:00   0.0   blue       255        86  2019-08-06 12:00:00          NaN                  NaN          NaN\n1  2019-08-06 12:00:00   1.0    red      7346        96  2019-08-08 16:00:00          2.0  2019-08-06 18:00:00         14.0\n2  2019-08-06 18:00:00   NaN    NaN        32        14  2019-08-06 23:00:00          NaN                  NaN          NaN\n3  2019-08-06 21:00:00   1.0   blue      5184      3540  2019-08-06 22:00:00          3.0  2019-08-06 21:00:00       5184.0\n4  2019-08-07 09:00:00   NaN    NaN       600       528  2019-08-08 09:00:00          NaN                  NaN          NaN\n5  2019-08-07 16:00:00   NaN    NaN       322       300  2019-08-09 16:00:00          NaN                  NaN          NaN\n6  2019-08-08 17:00:00   0.0   blue        72        12  2019-08-08 23:00:00          NaN                  NaN          NaN\n7  2019-08-09 16:00:00   0.0    red      6000      4009  2019-08-11 16:00:00          NaN                  NaN          NaN\n"], "link": "https://stackoverflow.com/questions/66165400/find-specific-value-that-meets-conditions-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've a dataframe like this and I'd like to get (note: I'm using a dataframe cause I manage other data for each column in addition.). How can I get a dataframe like this e.g. using with ?", "q_apis": "get columns get get", "apis": "loc values columns", "code": ["from scipy.signal import argrelextrema\nidx=argrelextrema(df.loc[0].values,np.greater)\ndf.columns[idx[0]]\nOut[227]: Float64Index([1.3, 2.0], dtype='float64')\n", "df[:]=df.columns.isin(df.columns[idx[0]])\ndf\nOut[234]: \n     1.0    1.1    1.2   1.3    1.4  ...    1.9   2.0    2.1    2.2    2.3\n0  False  False  False  True  False  ...  False  True  False  False  False\n[1 rows x 14 columns]\n"], "link": "https://stackoverflow.com/questions/58630974/how-to-get-the-local-maximum-in-row-ranges-in-a-1d-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How do I turn this dataframe: into this series: Python 3, Pandas 1.1.1.", "q_apis": "get columns", "apis": "unstack stack reorder_levels sort_index", "code": ["s = df.unstack([0,1])\nprint (s)\nC  FULL      count_x    1\n             count_y    2\n   CALIB     count_x    3\n             count_y    4\nP  FULL      count_x    5\n             count_y    6\n   CALIB     count_x    7\n             count_y    8\ndtype: int64\n", "s = df.stack().reorder_levels([2,0,1]).sort_index()\nprint (s)\n   col_name         \nC  CALIB     count_x    3\n             count_y    4\n   FULL      count_x    1\n             count_y    2\nP  CALIB     count_x    7\n             count_y    8\n   FULL      count_x    5\n             count_y    6\ndtype: int64\n"], "link": "https://stackoverflow.com/questions/64390040/convert-a-two-column-dataframe-into-a-multi-indexed-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given the following data which looks like I would like to create a column which classifies based on its prefix, which would be as follows: The solution should apply to a dataframe, the following for example Could be processed as: edit This approach is nice : But it depends on the values in the current data, I would like for the solution to be independent of the current values, for example; the solution might be in the form", "q_apis": "get columns apply values values", "apis": "DataFrame loc loc loc fillna items mask loc mask fillna", "code": ["# sys.version\n# '3.7.6 (default, Dec 30 2019, 19:38:28) \\n[Clang 11.0.0 (clang-1100.0.33.16)]'\nimport pandas as pd\nimport random\nrandom.seed(1)\n\nstarts = ['a. ', 'bc. ', '']\ndf = pd.DataFrame( {\n    'A' : [f\"{s}foo{i}\" for i,s in enumerate([random.choice(starts) for _ in range(5)])],\n    'B' : [random.randint(10,20) for _ in range(5)] })\n", "df.loc[df['A'].str.startswith(\"a.\"), \"class\"] = \"type_a\"\ndf.loc[df['A'].str.startswith(\"bc.\"), \"class\"] = \"type_bc\"\ndf.loc[:, 'class'].fillna(\"neither\", inplace=True)\n", "# df is the same data as originally created\nmapping = {\n    \"a.\": \"type_a\",\n    \"bc.\": \"type_bc\",\n}\nfor k, v in mapping.items():\n    mask = df[\"A\"].str.startswith(k)\n    df.loc[mask, \"class\"] = v\ndf[\"class\"].fillna(\"neither\", inplace=True)\n"], "link": "https://stackoverflow.com/questions/60104219/create-a-column-of-classifications-based-on-the-contents-of-a-column-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I can successfully drop duplicates and update rows in an existing dataframe. When I write this dataframe too a csv that already has data in it, how do I do the same commands in the dataframe to the csv to drop duplicates and update rows. I need the csv to look like this: This is my code for pandas to drop and update in the dataframe: Not sure how to do the pandas line for the csv. Thank you in advance for your help.", "q_apis": "get columns drop update drop update drop update", "apis": "drop sort_values drop_duplicates last drop sort_values drop_duplicates last to_csv index", "code": ["# filename containing data\nfilename = 'file.csv'\n\n# drop duplicates from existing dataframe\npermanent = permanent.sort_values('ID')\\\n                     .drop_duplicates('ID', keep='last')\n\n# read file into dataframe\ndf = pd.read_csv(filename)\n\n# concatenate the above dataframes and drop duplicates\nnew_df = pd.concat([permanent, df], ignore_index=True)\\\n           .sort_values('ID')\\\n           .drop_duplicates('ID', keep='last')\n\n# write to file\nnew_df.to_csv(filename, index=False)\n"], "link": "https://stackoverflow.com/questions/48914161/how-to-write-dataframe-to-csv-while-updating-and-dropping-duplicates-in-csv"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have below dataframe I want to generate dictionary like below", "q_apis": "get columns", "apis": "groupby agg to_dict", "code": ["{1: [10], 2: [20, 30], 3: [40], 5: [50]}\n", "d = df.groupby('a')['b'].agg(lambda s: list(s) if len(s) > 1 else s).to_dict()\n", "{1: 10, 2: [20, 30], 3: 40, 5: 50}\n"], "link": "https://stackoverflow.com/questions/68259621/creating-dictionary-from-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I load some machine learning data from a CSV file. The first 2 columns are observations and the remaining columns are features. Currently, I do the following: which gives something like: I'd like to slice this dataframe in two dataframes: one containing the columns and and one containing the columns , and . It is not possible to write something like I'm not sure what the best method is. Do I need a ? By the way, I find dataframe indexing pretty inconsistent: is permitted, but is not. On the other side, is not permitted but is. Is there a practical reason for this? This is really confusing if columns are indexed by Int, given that", "q_apis": "get columns first columns columns columns columns columns", "apis": "all all columns at loc loc loc loc loc columns loc loc columns columns loc loc", "code": ["# selects all rows and all columns beginning at 'foo' up to and including 'sat'\ndf.loc[:, 'foo':'sat']\n# foo bar quz ant cat sat\n", "# slice from 'foo' to 'cat' by every 2nd column\ndf.loc[:, 'foo':'cat':2]\n# foo quz cat\n\n# slice from the beginning to 'bar'\ndf.loc[:, :'bar']\n# foo bar\n\n# slice from 'quz' to the end by 3\ndf.loc[:, 'quz'::3]\n# quz sat\n\n# attempt from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar']\n# no columns returned\n\n# slice from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar':-1]\nsat cat ant quz bar\n\n# slice notation is syntatic sugar for the slice function\n# slice from 'quz' to the end by 2 with slice function\ndf.loc[:, slice('quz',None, 2)]\n# quz cat dat\n\n# select specific columns with a list\n# select columns foo, bar and dat\ndf.loc[:, ['foo','bar','dat']]\n# foo bar dat\n", "# slice from 'w' to 'y' and 'foo' to 'ant' by 3\ndf.loc['w':'y', 'foo':'ant':3]\n#    foo ant\n# w\n# x\n# y\n"], "link": "https://stackoverflow.com/questions/10665889/how-to-take-column-slices-of-dataframe-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "With this two simplified dataframes df1 df2 How can I multiply by matching and resulting in", "q_apis": "get columns", "apis": "set_index set_index mul rename reset_index", "code": ["df1i = df1.set_index(['COUNTRY','YEAR'])\ndf2i = df2.set_index(['COUNTRY','YEAR'])\n\ndf2i['PROPORTION'].mul(df1i['VALUE'], fill_value=1).rename('PROPORTION').reset_index()\n", "  COUNTRY  YEAR  PROPORTION\n0       A     1        50.0\n1       A     2       100.0\n2       A     3        10.0\n3       B     1        50.0\n4       B     2        20.0\n5       C     1       100.0\n6       C     2       100.0\n7       C     3        10.0\n"], "link": "https://stackoverflow.com/questions/57487317/conditional-operations-on-a-set-of-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Assume to have a 2 layers MultiIndex dataframe: which looks like this one My goal is to highlight (with Pandas's styling) the minimum (or equivalently the maximum) between the elements of for all the columns and for each element in Do you have any suggestion? I already tried this one, but it works column-wise and not based on the index. The results if the following", "q_apis": "get columns MultiIndex between all columns any index", "apis": "ndim apply min groupby transform min DataFrame where index index columns columns", "code": ["def highlight_min(data):\n    color= 'red'\n    attr = 'background-color: {}'.format(color)\n\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_min = data == data.min()\n        return [attr if v else '' for v in is_min]\n    else: \n        is_min = data.groupby(level=0).transform('min') == data\n        return pd.DataFrame(np.where(is_min, attr, ''),\n                            index=data.index, columns=data.columns)\n"], "link": "https://stackoverflow.com/questions/55688221/highlight-max-min-on-multiindex-dataframe-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to replace by for each dataframe which is inside a python dictionary, for this task I am using a for-loop, however it runs without error and doesn't perform the desired task Code: Output: Expected Output: What could I adjust in the for-loop to accomplish the task?", "q_apis": "get columns replace", "apis": "items replace", "code": ["for k, v in all_dataframes.items():\n     v.replace({'-': 'NaN'}, regex=True, inplace=True)\n     #print(v)\n", "{'df1':   col1 col2\n 0    1    3\n 1    2  NaN\n 2  NaN    4,\n 'df2':   col3 col4      col5\n 0    1    1      John\n 1    2    2      Mary\n 2    3   NA    Gordon\n 3  NaN  NaN   Cynthia\n 4    5   NA  Marianne,\n 'df3':   col6\n 0   19\n 1  NaN\n 2   20\n 3   23}\n"], "link": "https://stackoverflow.com/questions/65893741/replacing-dataframe-row-element-string-inside-a-python-dictionary"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to iteratively append pandas DataFrames to a csv file. This is usually not a problem. However, the DataFrames may not have all columns. So simply appending appends the DataFrame to the wrong columns. I start with Then for example I add the DataFrame df using the command However, the next DataFrame that I want to append may look like When I append it again, I do not ant to write the header because it already exists. Doing the same as before (as expected) does not work: It yields Of course I could import the existing csv into a DataFrame then append and the overwrite the old file: This does exactly what I want But always reading the entire csv file and appending in pandas and overwriting the csv is definitely bad concerning performance when I repeat the process many times. I want to write my intermediate results to a csv because all the aggregated data is lost if I only append in a pandas DataFrame and then an error occurs. Any better solutions to my problem? I also tried to add new empty columns but they get added at the end which doesnt help but may help to find a better performing solution.", "q_apis": "get columns append all columns DataFrame columns start add DataFrame DataFrame append append DataFrame append repeat all append DataFrame add empty columns get at", "apis": "to_csv index mode", "code": ["In [959]: df = df[['a','b','c']]\n\nIn [960]: df\nOut[960]: \n   a b  c\n0  1    1\n1  1    1\n", "In [961]: df.to_csv('test.csv', index = False, header = False, mode = 'a')\n"], "link": "https://stackoverflow.com/questions/53131205/append-pandas-dataframe-to-csv-with-fixed-header"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Edit I no longer need to do that (smh) but I'm keeping the post up for other's sorry for trouble. So, I have two dataframes one has 1000 rows and second has 129. As you can see I have duplicates of ID's. I need to merge them together that data from second dataframe will duplicate accordingly to IDs of first one df1 df2 So after merging, I can get something like this", "q_apis": "get columns second merge second first get", "apis": "ffill", "code": ["pd.merge_ordered(df1, df2, on='id', fill_method='ffill', how=\"inner\")\n"], "link": "https://stackoverflow.com/questions/66108185/merge-two-datarames-with-different-length-on-id-and-fill-empty-values-with-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with 5 minute time granularity. By now I group the df by cutting it down to the entire day and read the min / max values from two columns: Now, instead of getting the whole day, I need to boil the dataframe down to a split day, with unequal intervals. Let's say 7:00 to 15:00 and 15:00 to 22:00. How could I do it? allows only equal intervals. I also have a column with value 'A' for the first part of the day, and 'B' for the second part of the day, in case it's easier to group.", "q_apis": "get columns minute time now day min max values columns day day value first day second day", "apis": "groupby agg min max groupby agg min max", "code": ["df = df.groupby([df.Date.dt.date, 'Session']).agg({'Low':'min', 'High':'max'})\n", "df = (\n    df.groupby([df.Date.dt.date,\n        pd.cut(df.Date.dt.hour, bins=[7, 15, 22], labels=['7-15', '15-22'])])\n    .agg({'Low':'min', 'High':'max'})\n)\n"], "link": "https://stackoverflow.com/questions/63454751/how-to-group-pandas-dataframe-by-unequal-time-interval-or-string-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "It how do i separate adam and steve into a different row? I want it to line up like the table below. Table", "q_apis": "get columns", "apis": "DataFrame DataFrame DataFrame", "code": ["import pandas as pd\n\ndata = pd.DataFrame(['adam',22,'steve',25,'emily',18])\n\nprint(data)\n#print(data[0].to_list()[0::2])\n#print(data[0].to_list()[1::2])\n\ndf = pd.DataFrame({\n    'Name': data[0].to_list()[0::2],\n    'Age': data[0].to_list()[1::2],\n})\n\nprint(df)    \n", "       0\n0   adam\n1     22\n2  steve\n3     25\n4  emily\n5     18\n", "    Name  Age\n0   adam   22\n1  steve   25\n2  emily   18\n", "import pandas as pd\n\ndata = ['adam',22,'steve',25,'emily',18]\n\nprint(data)\n\ndf = pd.DataFrame({\n    'Name': data[0::2],\n    'Age': data[1::2],\n})\n\nprint(df)    \n"], "link": "https://stackoverflow.com/questions/60499739/having-trouble-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In a pandas dataframe, I need to find columns that contain a zero in any row, and drop that whole column. For example, if my dataframe looks like this: I need to drop columns A, B, D, and F. I know how to drop the columns, but identifying the ones with zeros programmatically is eluding me.", "q_apis": "get columns columns any drop drop columns drop columns", "apis": "DataFrame columns drop columns", "code": ["import pandas as pd\n\ndf = pd.DataFrame({'A': [1, 1, 1], 'B': [1, 0, 1]})\nfor col in df.columns:\n    if 0 in df[col].tolist():\n        df = df.drop(columns=col)\ndf\n"], "link": "https://stackoverflow.com/questions/53936862/how-to-drop-columns-from-a-dataframe-that-contain-specific-values-in-any-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a , I am wondering how to assign values of to only if and ; the result look like,", "q_apis": "get columns assign values", "apis": "where gt le", "code": ["df['delta1'] = np.where(df.delta1.gt(0) & df.delta2.le(0), df.delta2, df.delta1)\n", "   delta1  delta2\n0       0      -1\n1       0       0\n2      -1       0\n3       0       0\n"], "link": "https://stackoverflow.com/questions/55168663/how-to-assign-values-from-column-1-to-column-2-when-column-1-is-0-and-column-2"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Having some troubles figuring out pandas str.split. Where the occurrence comes from a column value instead of placing one static value for the string to be split on. I have looked around on this sight for similar type questions but most seem to just have a static approach to the split. Below I have dataframe. The .str.split('|',1).str[-1] will remove the left part of the string at the first occurrence of the pipe('|'). This static approach will perform the same down through the series. Because the occurrence argument does not change. What I would like to happen: The .str.split('|',df['occurrence']).str[-1] could be dynamic and utilize the value within the occurrence column and be used as the str.split occurrence argument. And if value is zero or less then no action is taken on string. The lambda statement actually works and performs correctly however, it starts from the right side of string, splits and joins based on the value between the pipes. But the final outcome is good. Different approach. I just can't make it do the same thing from the left side of the string. Last point to make: The removal needs to start from the left of string. I would appreciate any light that you can shine on this subject for me to work through this problem. As always your time is valuable and I Thank You for it.", "q_apis": "get columns value value left at first pipe value value right value between left start left any time", "apis": "join", "code": ["df['string'] = ['|'.join(s[i:]) for i, s in zip(df['occurrence'], df['string'].str.split('|'))]\n", "print(df)\n   occurrence          string\n0           7    8|9|10|11|12\n1           2  12.2|13.6|14.7\n2           0           1|2|3\n3           3       4|5|6|7|8\n4           4     5|6.2|7|8.1\n5           0       1|2|3|4|5\n", "df.shape\n(60000, 2)\n\n%%timeit -n10\n_ = ['|'.join(s[i:]) for i, s in zip(df['occurrence'], df['string'].str.split('|'))]\n67.9 ms \u00b1 2.05 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%%timeit -n10 (using 'apply')\n_ = df.apply(lambda x: '|'.join(x['string'].split('|')[x.occurrence:]), axis=1)\n1.93 s \u00b1 34.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n"], "link": "https://stackoverflow.com/questions/63564296/using-pandas-str-split-where-the-occurrence-needs-to-be-value-of-column-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have DataFrames and with different number of rows but same columns: What I want is: To get the minimum, absolute of all possible subtractions between the values of and but only when the condition is True. To get the value for each one of the above values. My code so far is: which gives the following error: Any ideas? Thank you in advance for your input.", "q_apis": "get columns columns get all between values get value values", "apis": "min abs abs idxmin append append", "code": ["temp_operator = []\ntemp_op_defect = []\nk = -1\nfor i in dfSide0['distoperator']:\n    k = k + 1\n    c = min(abs(i - dfSide1['distoperator'][dfSide1['camera_row'] == dfSide0['camera_row'][k]]))\n    h = abs(i - dfSide1['distoperator'][dfSide1['camera_row'] == dfSide0['camera_row'][k]]).idxmin()\n    f = dfSide1['Name'][h]\n    temp_operator.append(c)\n    temp_op_defect.append(f)\n"], "link": "https://stackoverflow.com/questions/54144955/getting-the-index-of-the-minimum-value-of-a-list-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm doing a personal project to practice pandas, and Beautiful soup, I scraped this info and have it in a pandas df like this: So in order to analyse this info I want to clean and transform this data into integers, what I could come up with was this: I replaced first the money sign, then check for dots so I can replace the M for 5 zeros, then the remaining M's for 6 zeros, then the K's for 3 zeros and then I do the type change into int. But I feel like this is not a good way of doing this, What do you think? What would be a better way of doing this? I tried creating a function but couldn't do it.", "q_apis": "get columns info info transform first replace", "apis": "replace astype astype replace replace replace eval astype replace eval astype", "code": ["df['Value'] = df['Value'].replace({'\u20ac': '', ' ': '', 'M': 'e+06', 'K': 'e+03'}, regex=True).astype(float).astype(int)\n", "         Value\n0        \u20ac8.5M\n1           \u20ac0\n2        \u20ac9.5M\n3          \u20ac2M\n4         \u20ac21M\n16534    \u20ac1.8M\n16535    \u20ac1.1M\n16536    \u20ac550K\n16537    \u20ac650K\n16538    \u20ac1.1M\n", "print(df)\n\n          Value\n0       8500000\n1             0\n2       9500000\n3       2000000\n4      21000000\n16534   1800000\n16535   1100000\n16536    550000\n16537    650000\n16538   1100000\n", "df['Value'] = df['Value'].str.replace('\u20ac', '')\ndf['Value'] = df['Value'].str.replace('M', ' * 1000000')\ndf['Value'] = df['Value'].str.replace('K', ' * 1000')\ndf['Value'] = df['Value'].map(pd.eval).astype(int)\n", "df['Value'] = df['Value'].replace({\"\u20ac\": \"\", \"M\": \"*1E6\", \"K\": \"*1E3\"}, regex=True).map(pd.eval).astype(int)\n", "print(df)\n\n\n          Value\n0       8500000\n1             0\n2       9500000\n3       2000000\n4      21000000\n16534   1800000\n16535   1100000\n16536    550000\n16537    650000\n16538   1100000\n", "         Value\n0        \u20ac8.5M\n1           \u20ac0\n2        \u20ac9.5M\n3          \u20ac2M\n4         \u20ac21M\n16534    \u20ac1.8M\n16535    \u20ac1.1M\n16536    \u20ac550K\n16537    \u20ac650K\n16538    \u20ac1.1M\n", "               Value\n0      8.5 * 1000000\n1                  0\n2      9.5 * 1000000\n3        2 * 1000000\n4       21 * 1000000\n16534  1.8 * 1000000\n16535  1.1 * 1000000\n16536     550 * 1000\n16537     650 * 1000\n16538  1.1 * 1000000\n"], "link": "https://stackoverflow.com/questions/67891653/python-pandas-df-best-way-to-replace-m-and-k-in-currency-amount-to-change-to"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I get an AttributeError from pandas.DataFrame.apply(). Despite that, the function appears to work, but I'd like to understand apply() better, so I'm posting this question anyway... I have two dataframes, like these: I merge them like so: And I get the following: Now I need to drop records in c where TIME_START <= TIME <= TIME_END is FALSE. This way, I wind up with only records that have the relevant CORR_KEY. (For example, the correct CORR_KEY for ID 123 is abc, NOT jkl.) I use the following function, and apply it along the dataframe: The result is: AttributeError: 'ID' is not a valid function for 'Series' object HOWEVER, checking c again, I get the expected output: So, what causes the AttributeError? Thanks!", "q_apis": "get columns get DataFrame apply apply merge get drop where apply Series get", "apis": "mask mask", "code": ["mask = (c.TIME_START <= c.TIME) & (c.TIME <= c.TIME_END)\nc = c[mask]\n"], "link": "https://stackoverflow.com/questions/61415382/dataframe-apply-error-series-is-not-a-valid-function-for-series-object"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I found this piece of code online for comparing two csv files and print the difference, I'm not quite sure about the logic behind the first two lines, can someone explain many thanks.", "q_apis": "get columns difference first", "apis": "stack", "code": ["import pandas as pd\n\n# Creating the first dataframe\ndf1 = pd.DataFrame({\"A\":[1,5,7,8], \n                  \"B\":[5,8,4,3], \n                  \"C\":[10,4,9,3]}) \n\n# Creating the second dataframe \ndf2 = pd.DataFrame({\"A\":[5,3,6,4], \n                  \"B\":[11,2,4,3], \n                  \"C\":[4,3,8,5]})\n", "resultBool = (df1 != df2).stack() \n"], "link": "https://stackoverflow.com/questions/58610427/can-someone-explain-this-piece-of-code-for-identifying-difference-between-two-da"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to append a row (df_row) with every iteration to a parent dataframe (df_all). The parent dataframe has all the possible column values and every iteration produces a row with a unique set of columns which are a subset of the all possible columns. It looks something like this: initially has all the possible column names: Iteration 1: Now looks as below: Iteration 2: : Now looks as below: And so on... However, I have >20000 rows to add and the time taken to add every row is increasing with every new iteration. Is there a way to add this more efficiently within a reasonable amount of time?", "q_apis": "get columns append all values unique columns all columns all names add time add add time", "apis": "append DataFrame", "code": ["In [46]: pd.DataFrame([pd.Series({'A':1,'B':2}), pd.Series({'A':2,'C':3})])\nOut[186]: \n     A    B    C\n0  1.0  2.0  NaN\n1  2.0  NaN  3.0\n\nIn [187]: pd.DataFrame([{'A':1,'B':2}, {'A':2,'C':3}])\nOut[187]: \n   A    B    C\n0  1  2.0  NaN\n1  2  NaN  3.0\n", "data = []\nfor n in range(20000):\n    df_row = pd.Series(...)\n    data.append(df_row)\n\ndf = pd.DataFrame(data)\n"], "link": "https://stackoverflow.com/questions/57631668/most-efficient-way-to-append-rows-to-a-dataframe-with-unequal-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like: And the expected output should look like So, for each record where ID is same, the same consecutive 'Task' has to be merged and the 'Value' to be added. My approach is listed below but with that, it is not considering the cases where the same 'Task' occurs more than twice (the 'D' task in the example). Also, not able to drop the rows after merging, don't know why.", "q_apis": "get columns where where drop", "apis": "ne shift any cumsum sum join astype sum join groupby agg droplevel reset_index", "code": ["u = df[['Task','ID']]\ng = u.ne(u.shift()).any(1).cumsum()\n\nd = {\"Value\":\"sum\",\"Sno\":lambda x: ','.join(x.astype(str))}\n#d = {\"Value\":\"sum\",\"Sno\":lambda x: ','.join([str(i) for i in x])}\n\nout = df.groupby(['Task','ID',g]).agg(d).droplevel(-1).reset_index()\n", "print(out)\n\n  Task  ID  Value    Sno\n0    A  A1      7    1,2\n1    A  A1      1      4\n2    A  B1     10      5\n3    B  A1      4      3\n4    C  B1      3      6\n5    D  B1     19  7,8,9\n6    E  C1     25     10\n"], "link": "https://stackoverflow.com/questions/67557429/merge-and-drop-multiple-rows-based-on-columns-value-using-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I feel kinda silly being stuck on something that seems so simple for so long but since Im about to put my head through the wall, I figured I'd ask for some help. I have a loop that splits my data into smaller subsets and loops through each one. For each loop, it produces a and a array. It'll be variable size but the shape is (X,). In order to plot the the two arrays vs each other, i just assigned the arrays to an empty dataframe and just used to plot. Now, i'd like to just be able to also keep a running total of the and so I can see a plot of the entire data set. What I've tried: Initially, I just tried creating another empty data frame outside my loop and thought to just append the arrays to the end of my columns of the dataframe but i found appending arrays to dataframe was not possible. Then i thought I'll just append to an empty array for each time through the loop and convert to a dataframe at the end to plot but Im not having much luck there either and if I understand correctly - is creating a new array of the appended data every time I append? Wasn't sure if this would get memory intensive. I was wondering what is the best way to do this? Here is my code (I tried to remove a lot of the lines that weren't necessary to the problem to make it easier to follow):", "q_apis": "get columns put head array size shape plot empty plot plot empty append columns append empty array time at plot array time append get", "apis": "DataFrame append DataFrame values DataFrame DataFrame append DataFrame first DataFrame append DataFrame DataFrame append", "code": ["# If you already have data as an array\ndata = np.random.random((10,5))\n# Create a dataframe from a numpy array\ndf = pd.DataFrame(data)\n# Create a numpy array from a dataframe\nas_array = df.to_numpy()\n", "# Looping - arrays can handle n dimensions\ndata = []\nfor i in range(10):\n    row = np.random.random((1,1,1,1,1))\n    # Add a second dimension\n    row = row[:,np.newaxis]\n    # Remove the second dimension\n    row = row[:,-1]\n    # A list can hold anything\n    data.append(row)\n# Construct an array from a list of arrays\narray = np.array(data)\n", "# looping - dataframes can work with only one dimension per row\ndata = []\nfor i in range(10):\n    data.append(np.random.random(5))\n# Construct a DataFrame from a list of values\ndf = pd.DataFrame(data)\n", "df = pd.DataFrame()\nfor i in range(10):\n    n = np.random.random(1)\n    # To append to a DataFrame, first create a Series (a row or a column) or a DataFrame\n    row = pd.Series(n, name=i)\n    # append a Series (or a DataFrame) to the \"bottom\" of another DataFrame\n    df = df.append(row)\n"], "link": "https://stackoverflow.com/questions/56566154/having-trouble-with-array-appending"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dictionary as follows: I would like to convert it into a pandas data frame which would look like this: I tried to do it like this: But I get this error: I appreciate that it would be possible by looping through each item and each element of the tuple, but is there a cleaner way to do it?", "q_apis": "get columns get item", "apis": "count count", "code": ["import pandas as pd\n\nd = {(\"Sam\", \"Scotland\", \"23\"): 25,\n     (\"Oli\", \"England\", \"23\"): 28,\n     (\"Ethan\", \"Wales\", \"18\"): 19}\n\ndf = pd.DataFrame([k + (v,) for k, v in d.items()], columns=['name', 'country', 'age', 'count'])\nprint(df)\n", "    name   country age  count\n0  Ethan     Wales  18     19\n1    Sam  Scotland  23     25\n2    Oli   England  23     28\n", "import pandas as pd\n\nd = {(\"Sam\", \"Scotland\", \"23\"): 25,\n     (\"Oli\", \"England\", \"23\"): 28,\n     (\"Ethan\", \"Wales\", \"18\"): 19}\n\ndf = pd.DataFrame(\n    [{\"name\": name, \"country\": country, \"age\": age, \"count\": value} for (name, country, age), value in d.items()])\n\nprint(df)\n", "  age  count   country   name\n0  23     28   England    Oli\n1  23     25  Scotland    Sam\n2  18     19     Wales  Ethan\n"], "link": "https://stackoverflow.com/questions/53063664/convert-dictionary-items-to-rows-of-pandas-data-frame-where-keys-are-tuples-and"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to find the frequently repeated element in a column and save the results as Dataframe then pull out the related information of those elements from the original Dataframe the above code gives me but what I want is How can I get the result that I want? Thank you", "q_apis": "get columns get", "apis": "count", "code": ["          B      D  count\n0  company0   test      2\n1  company1  train      2\n2  company2    cup      3\n3  company5    bib      3\n"], "link": "https://stackoverflow.com/questions/66468475/how-to-save-the-value-counts-in-dataframe-and-pull-out-the-related-data-from-ori"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Goal: Id like to still show who the person is so that I can display the NANs associated with them so that I can quickly find who is missing info. Consider this dataset: I'd like to clean the data up and show something like this (similar to an excel view when filtering for blanks): Then maybe populate the empty data with something that says \"Data exists\" or just leave it blank. Im open to suggestions. And also drop the rows that have all data populated. I've tried: That works great but I have a big data source and I see a lot of unnecessary info that already has data. I only care about seeing the person's name and what their missing. Anyone have any ideas?", "q_apis": "get columns info view empty drop all info name any", "apis": "mask columns notnull dropna drop index", "code": ["df.mask((df.columns != 'Name') & (df.notnull()), \"\", inplace=True)\ndf2 = df.dropna()\ndf.drop(df2.index, inplace=True)\n", "Name        Phone            Address\nJohn Doe     NAN               \nJenny Gump                     NAN\nLarry Bean    NAN \n"], "link": "https://stackoverflow.com/questions/68137444/keep-one-columns-data-in-pandas-and-show-all-nans-from-other-columns-only"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "df1: df2: Output df: I need to append two dataframe, but rows from df2 should append at specific location in df1. For Example in df2 the first two rows \"Market\" Column belongs to Portugal and in my df1 Country Portugal first row Id is 102, it should append after 1st row of portugal with same Id. Same follows for other countries.", "q_apis": "get columns append append at first first append", "apis": "duplicated set_axis columns duplicated replace assign sort_values astype groupby transform first drop", "code": ["df1['sortkey'] = df1['Country'].duplicated()\ndf2 = df2.set_axis(df1.columns[:-1], axis=1)\n\ndf1['sortkey'] = df1['Country'].duplicated().replace({True:2, False:0})\ndf_sorted = (pd.concat([df1, df2.assign(sortkey=1)])\n               .sort_values(['Country', 'sortkey'], \n                            key=lambda x: x.astype(str).str.split(' ').str[0]))\n\ndf_sorted['Id'] = df_sorted.groupby(df_sorted['Country'].str.split(' ').str[0])['Id'].transform('first')\nprint(df_sorted.drop('sortkey', axis=1))\n", "     Id       Country      P_Type  Sales\n8   105       Germany  Industries   1451\n4   105   Germany all      Sports   4587\n5   105   Germany sts      Sports   4756\n9   105       Germany      Office   1635\n10  105       Germany       Clubs   1520\n11  105       Germany        cars   1265\n4   109         India  House_hold   1651\n2   109      India QA     Housing   4529\n3   109     India stm     Housing   1576\n5   109         India      Office   1125\n6   109         India      Bakery   1752\n7   109         India  House_hold   1259\n0   102      Portugal  Industries   1265\n0   102  Portugal ALL        Wine   5642\n1   102   Portugal St        Wine   4568\n1   102      Portugal      Office   1455\n2   102      Portugal       Clubs   1265\n3   102      Portugal        cars   1751\n"], "link": "https://stackoverflow.com/questions/63265648/how-to-append-two-dataframe-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to find a \"pandas\" solution for this: I have a dataframe with two columns, one for datetime and one for numeric values. Assume this for the dataframe: The output is something like this: I need to create a new dataframe with new columns: new column A: sum of diffs for the same date from hour 5:00 to hour 21:00 if diffs.value>0 new column B: sum of diffs for the same date from hour 5:00 to hour 21:00 if diffs.value<0 new column C: sum of diffs if diffs.value>0 for the group 'y-m-d 22:00:00' to 'y-m-d+1 4:00:00' new column D: sum of diffs if diffs.value<0 for the group 'y-m-d 22:00:00' to 'y-m-d+1 4:00:00' So practically, 5 new columns: 1) date 2) to accommodate the sum of the positive diffs per day from hours 5 to 21 3) to accommodate the sum of the negative diffs per day from hours 5 to 21 4) to accommodate the sum of the positive diffs from 22:00 of one day to 4:00 of the next day 5) to accommodate the sum of the negative diffs from 22:00 of one day to 4:00 of the next day I could start iterating over lists to create new lists and then bring them back together into a new dataframe. But I am trying to figure out if I could somehow groupby and apply criteria in different columns and aggregate. Note: the sum as described in (4) and (5) should fall under the date of the day 1. I would welcome your input. I am not a developer and definitely not experienced in pandas, but the library seems to offer huge possibilities, that I try to explore. Hoping my description is clear, thanks in advance.", "q_apis": "get columns columns values columns sum date hour hour value sum date hour hour value sum value sum value columns date sum day sum day sum day day sum day day start groupby apply columns aggregate sum date day", "apis": "size astype index DataFrame columns index groupby agg sum sum sum groupby agg sum sum sum", "code": ["dates=pd.date_range('01-05-2018 13:00:00', periods=12000, freq=\"1H\")\nrange_series=pd.Series(np.random.randint(-2,2,size=12000).astype(float), index=dates)\ndf=pd.DataFrame(range_series, columns=['diffs'])\ndf.index.name='datetime'\n", "day_change=day_df.groupby(day_df['date'])['diffs'].\\\n        agg([('daytime change' , lambda x : x.sum()) ,\\\n             ('daytime negative change' , lambda x : x[x < 0].sum()) , \\\n             ('daytime positive change' , lambda x : x[x > 0].sum())])\nnight_change=night_df.groupby(night_df['date'])['diffs'].\\\n        agg([('nighttime change' , lambda x : x.sum()) ,\\\n             ('nighttime negative change' , lambda x : x[x < 0].sum()) , \\\n             ('nighttime positive change' , lambda x : x[x > 0].sum())])\n", "print(change.head(1))\n            daytime change  daytime negative change  daytime positive change  \\\ndate                                                                           \n2018-01-05            -7.0                    -10.0                      3.0   \n\n            nighttime change  nighttime negative change  \\\ndate                                                      \n2018-01-05               0.0                        0.0   \n\n            nighttime positive change  \ndate                                   \n2018-01-05                        0.0  \n"], "link": "https://stackoverflow.com/questions/57479743/new-dataframe-group-by-date-condition-on-time-sum-values-with-if-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe: I want to count along with group by like: But I want to perform nunique with a condition only when Desired Output: How can I modify the code in that case. Thanks!!", "q_apis": "get columns count nunique", "apis": "groupby transform sum", "code": ["df['myvals'] = (df['myvals'] == 3).groupby(df['dates']).transform('sum')\n", "        dates  myvals\n0  2015-01-01     1.0\n1  2015-01-02     0.0\n2  2015-01-03     2.0\n3  2015-01-03     2.0\n4  2015-01-02     0.0\n5  2015-01-02     0.0\n6  2015-01-01     1.0\n"], "link": "https://stackoverflow.com/questions/61804196/conditional-groupby-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe (~8k rows) that contains a a few columns that correspond to a fixed values to be looked up. Dataframe looks like this-(ish): class_name size colour First L green First L blue Second XL red Lookup table looks like this (a df or a nested python dict): class_name size blue red green First L 90 95 90 First XL 100 105 100 Second XL 100 105 100 What I'm looking for is an efficient way to add a new column value to my first df. Something like this: class_name size colour value First L green 90 First L blue 90 Second XL red 105 Right now I'm solving this with This is quite slow (after my 8k df is processed, I have plenty of new 8k df's to do). How can I do this faster and more elegant?", "q_apis": "get columns contains columns values size size add value first size value now", "apis": "lookup melt size", "code": ["melted = lookup.melt(id_vars=['class_name', 'size'], value_vars=['blue', 'red', 'green'], var_name='colour')\n"], "link": "https://stackoverflow.com/questions/66475150/lookup-from-multiple-columns-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I saw this code in someone's iPython notebook, and I'm very confused as to how this code works. As far as I understood, pd.loc[] is used as a location based indexer where the format is: However, in this case, the first index seems to be a series of boolean values. Could someone please explain to me how this selection works. I tried to read through the documentation but I couldn't figure out an explanation. Thanks!", "q_apis": "get columns loc where first index values", "apis": "DataFrame index columns loc loc loc loc loc", "code": ["    df = pd.DataFrame([[1, 2], [3, 4]], index=['A', 'B'], columns=['X', 'Y'])\n", "df.loc[['B', 'A'], 'X']\n\nB    3\nA    1\nName: X, dtype: int64\n", "df.loc[['B', 'A'], ['X']]\n\n   X\nB  3\nA  1\n", "df.loc[[True, False], ['X']]\n\n   X\nA  1\n", "iris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'Iris-versicolor'\n", "iris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'Iris-versicolor'\n"], "link": "https://stackoverflow.com/questions/44890713/selection-with-loc-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Result", "q_apis": "get columns", "apis": "assign merge assign query groupby mean", "code": ["mapper = df_A.assign(key=1).merge(df_B.assign(key=1))\\\n             .query('start_date <= event_date <= end_date')\\\n             .groupby('start_date')['price'].mean()\ndf_A['avg.price'] = df_A['start_date'].map(mapper)\nprint(df_A)\n", "  start_date   end_date   avg.price\n0 2017-03-01 2017-04-20  100.000000\n1 2017-03-20 2017-04-27         NaN\n2 2017-04-10 2017-05-25  133.333333\n3 2017-04-17 2017-05-22  125.000000\n"], "link": "https://stackoverflow.com/questions/61220012/merging-two-df-based-on-dates-if-between-some-range-and-average-the-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame called resource Resource Xy usage was ended at 06-12-2018 11:00 .Till the usage of the resource Xy by Per1 for P1, Per8 for P2 cannot use the resource. I want to copy end value resource Xy used in Project P1 to start column for Project P2 used by Per8 replacing the default value. end column will be (start time + allocated time for that person). I want to iterate though out the rows again and again till all default value (1970-01-01 00:00:00) are replaced I have used this above code to create a column I am not able to get how to proceed further Project Resource Person Allocation Time start end P1 Xy Per 1 04:00 06-12-2018 08:00 06-12-2018 11:00 P1 Z Per 2 05:00 06-12-2018 08:00 06-12-2018 12:00 P1 Y Per 3 07:00 06-12-2018 08:00 06-12-2018 18:00 P1 X Per 1 03:50 06-12-2018 08:00 06-12-2018 12:00 P2 X Per 6 02:20 01-01-1970 00:00 01-01-1970 00:00 P2 Y Per 7 01:25 01-01-1970 00:00 01-01-1970 00:00 P2 Xy Per 8 02:30 01-01-1970 00:00 01-01-1970 00:00 P2 Xy Per 9 14:00 01-01-1970 00:00 01-01-1970 00:00 P2 X Per 7 12:35 01-01-1970 00:00 01-01-1970 00:00 P2 Y Per 6 11:10 01-01-1970 00:00 01-01-1970 00:00 P2 Z Per 11 13:45 01-01-1970 00:00 01-01-1970 00:00 P2 Z Per 12 10:00 01-01-1970 00:00 01-01-1970 00:00 P3 X Per 5 07:30 01-01-1970 00:00 01-01-1970 00:00", "q_apis": "get columns at copy value start value start time time all value get start", "apis": "copy groupby iloc iterrows loc loc drop", "code": ["import pandas as pd\nfrom io import StringIO\n\ntxt = \"\"\"\nProject Resource    Person  Allocation Time start   end\nP1  Xy  Per 1   4:00    06-12-2018 8:00 06-12-2018 11:00\nP1  Z   Per 2   5:00    06-12-2018 8:00 06-12-2018 12:00\nP1  Y   Per 3   7:00    06-12-2018 8:00 06-12-2018 18:00\nP1  X   Per 1   3:50    06-12-2018 8:00 06-12-2018 12:00\nP2  X   Per 6   2:20    01-01-1970 0:00 01-01-1970 0:00\nP2  Y   Per 7   1:25    01-01-1970 0:00 01-01-1970 0:00\nP2  Xy  Per 8   2:30    01-01-1970 0:00 01-01-1970 0:00\nP2  Xy  Per 9   14:00   01-01-1970 0:00 01-01-1970 0:00\nP2  X   Per 7   12:35   01-01-1970 0:00 01-01-1970 0:00\nP2  Y   Per 6   11:10   01-01-1970 0:00 01-01-1970 0:00\nP2  Z   Per 11  13:45   01-01-1970 0:00 01-01-1970 0:00\nP2  Z   Per 12  10:00   01-01-1970 0:00 01-01-1970 0:00\nP3  X   Per 5   7:30    01-01-1970 0:00 01-01-1970 0:00\n\"\"\"\n\nt_m = pd.read_csv(StringIO(txt), sep='\\t')\n\nt_m = t_m.assign(\n    start=lambda f: pd.to_datetime(f.start),\n    end=lambda f: pd.to_datetime(f.end),\n    alloc_time=lambda f: pd.to_timedelta(f[\"Allocation Time\"].apply(lambda x: x+':00')))\n", "out = t_m.copy()\nfor resource, group in t_m.groupby('Resource'):\n    start = group.start.iloc[0]\n    for ix, row in group.iterrows():\n        out.loc[ix, 'start'] = start\n        start = start + row['alloc_time']\n        out.loc[ix, 'end'] = start\n\nout = out.drop('Allocation Time', axis=1)\nprint(out)\n", "   Project Resource  Person               start                 end alloc_time\n0       P1       Xy   Per 1 2018-06-12 08:00:00 2018-06-12 12:00:00   04:00:00\n1       P1        Z   Per 2 2018-06-12 08:00:00 2018-06-12 13:00:00   05:00:00\n2       P1        Y   Per 3 2018-06-12 08:00:00 2018-06-12 15:00:00   07:00:00\n3       P1        X   Per 1 2018-06-12 08:00:00 2018-06-12 11:50:00   03:50:00\n4       P2        X   Per 6 2018-06-12 11:50:00 2018-06-12 14:10:00   02:20:00\n5       P2        Y   Per 7 2018-06-12 15:00:00 2018-06-12 16:25:00   01:25:00\n6       P2       Xy   Per 8 2018-06-12 12:00:00 2018-06-12 14:30:00   02:30:00\n7       P2       Xy   Per 9 2018-06-12 14:30:00 2018-06-13 04:30:00   14:00:00\n8       P2        X   Per 7 2018-06-12 14:10:00 2018-06-13 02:45:00   12:35:00\n9       P2        Y   Per 6 2018-06-12 16:25:00 2018-06-13 03:35:00   11:10:00\n10      P2        Z  Per 11 2018-06-12 13:00:00 2018-06-13 02:45:00   13:45:00\n11      P2        Z  Per 12 2018-06-13 02:45:00 2018-06-13 12:45:00   10:00:00\n12      P3        X   Per 5 2018-06-13 02:45:00 2018-06-13 10:15:00   07:30:00\n"], "link": "https://stackoverflow.com/questions/58952267/how-to-iterate-through-rows-in-one-dataframe-and-assgin-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Similar to my last question, I'm having an iteration issue. I'm using the code to get the list of names from one web page. However, when I try this for all pages, it creates new tables for each page instead of appending the output from each page together. So for page 1 I'd get Page 2 : etc. But I want my output to be: Below is my full code (with the url omitted) for iterating through all the pages How can I fix this? Thank you.", "q_apis": "get columns last get names all get all", "apis": "get DataFrame", "code": ["  username_list = []\n  for pageno in range(0,99):\n      page=requests.get('full url'+ str(pageno))\n      soup=BeautifulSoup(page.text, 'html.parser')\n      username_list += [name.text for name in (soup.findAll('p',{'class':'profile-name'}))]\n\n  df1 = pd.DataFrame({'Username':  username_list})\n"], "link": "https://stackoverflow.com/questions/64383144/output-from-webscraped-page-not-appended-to-output-from-previous-page"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes. The first dataframe contains an ID column, and the second a name column. An ID can have more than one name, but a name always belongs to a single ID. I have a second dataframe, which have the name column as well. What I would like to have is a new column in the second dataframe, with the matching IDs to the names. How do I do that? I understand I create an empty column, than take the name as an input, search for that in the first dataframe's name column, and if I have a match, grab the ID from the next cell, and return that, but I cannot figure out how to that.", "q_apis": "get columns first contains second name name name second name second names empty take name first name", "apis": "merge", "code": ["+-----+-----------+\n| id  | name      |\n+-----+-----------+\n| 1   | Arthur    |\n| 1   | Brigit    |\n| 2   | Christoph |\n+-----+-----------+\n", "+-------------+-----------+\n| Other_data  | name      |\n+-------------+-----------+\n| 203423      | Arthur    |\n| 125323      | Brigit    |\n| 125323      | Christoph |\n+-------------+-----------+\n", "df_combined = pd.merge(df_data, df_name_id, on='name', how='left')\n", "+-------------+-----------+-------+\n| Other_data  | name      |  id   |\n+-------------+-----------+-------+\n| 203423      | Arthur    |  1    |\n| 125323      | Brigit    |  1    |\n| 125323      | Christoph |  2    |\n+-------------+-----------+-------+\n"], "link": "https://stackoverflow.com/questions/64369369/how-to-get-the-value-from-the-next-cell-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How can I compare two columns in a dataframe and create a new column based on the difference of those two columns efficiently? I have a feature in my table that has a lot of missing values and I need to backfill those information by using other tables in the database that contain that same feature. I have used to compare the feature in my original table with the same feature in other table, but I feel like there should be an easy method. Eg: I expect the new column to contain values . Any help will be appreciated!", "q_apis": "get columns compare columns difference columns values backfill compare values", "apis": "DataFrame combine_first mask isna isna mask", "code": ["df = pd.DataFrame({'A': [1,2,3,4,np.nan], 'B':[1,np.nan,30,4,np.nan]})\nC = df.A.combine_first(df.B)\n", "0    1.0\n1    2.0\n2    3.0\n3    4.0\n4    NaN\n", "mask = ~df.A.isna() & ~df.B.isna() & (df.A != df.B)\nC[mask] = 'different'\n", "0            1\n1            2\n2    different\n3            4\n4          NaN\n"], "link": "https://stackoverflow.com/questions/56419885/comparing-each-value-in-two-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to create a which contains the same in every row: input: output: How could I do this properly?", "q_apis": "get columns contains", "apis": "DataFrame", "code": ["import pandas as pd\n\nlength = 5\na = {'this':1, 'is':2, 'an':3, 'example':4}\n\nout = pd.DataFrame({0:[a] * length})\nprint(out)\n", "                                             0\n0  {'this': 1, 'is': 2, 'an': 3, 'example': 4}\n1  {'this': 1, 'is': 2, 'an': 3, 'example': 4}\n2  {'this': 1, 'is': 2, 'an': 3, 'example': 4}\n3  {'this': 1, 'is': 2, 'an': 3, 'example': 4}\n4  {'this': 1, 'is': 2, 'an': 3, 'example': 4}\n"], "link": "https://stackoverflow.com/questions/63632718/create-a-pandas-dataframe-of-specified-shape-containing-the-same-dict-in-every-r"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a variable, playlists, which I want to assign a categorical variable which takes values of \"sleep\" or \"no_sleep\", for each playlist. I have both lists with sleep / no sleep already assign as follows: Sleep: \"Peaceful piano\", \"sleep\", \"Baby Sleep Aid\", \"White Noise\" No_sleep: \"Dance Hits\", \"Dance Classics\", \"Massive Dance Hits\", \"Dance Party\" : (\"Spotify\", \"37i9dQZF1DXaXB8fQg7xif\") How do I add the new variable to the pd.dataframe and assign the values? I imagine an if function for sleep and everything else is No_sleep, but not sure how to get started.", "q_apis": "get columns assign values assign add assign values get", "apis": "isin index iloc", "code": ["df['sleep'] = \"no_sleep\"\n\nsleep_idxs = np.array(df[df['playlist'].isin(sleep_playlists)].index)\n\ndf.iloc[sleep_idxs, 'sleep'] = \"sleep\"\n"], "link": "https://stackoverflow.com/questions/62501500/how-do-i-assign-values-to-a-new-category-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have dataframe consists of 3 columns : sitename, circle, time and count. I need to group dataframe based on multiple columns within interval of every 10 min. When I am doing this by below command, it working fine: Output: But, we I am taking input from user with multiple fields it is not working: when I am using below command it is giving error: ValueError: Grouper and axis must be same length I have also tried to add input and in list and then use that list in groupby, but it didn't work.", "q_apis": "get columns columns time count columns min Grouper length add groupby", "apis": "groupby", "code": ["df.groupby(inp.split(\",\") + [pd.Grouper(key='Time', freq='10T', axis =0)])\n"], "link": "https://stackoverflow.com/questions/64042482/grouping-dataframe-on-the-basis-of-fields-entered-by-user-on-runtime-and-time-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Never mind the below--I see the cause of the problem. The shift of course produces a N/A. I want to prevent a type conversion that occurs when concatenating a dataframe to itself horizontally. I have a dataframe where all columns are int64 (and the index is a datetime64[ns]): I concatenate to have the next row's columns (suffixed with \"_next\") appear on the same line as the current row: But then the types on the concatenated columns change to float64: Is there a way to prevent that type conversion? Thanks.", "q_apis": "get columns shift where all columns index columns columns", "apis": "shift fillna astype add_suffix shift add_suffix fillna astype", "code": ["df = pd.DataFrame({'op': [1, 2, 3]})\ndf = pd.concat([df, df.shift(-1).add_suffix('_next')], axis=1)\n\nprint(df)\n\n   op  op_next\n0   1      2.0\n1   2      3.0\n2   3      NaN\n", "df = pd.concat([df, df.shift(-1).fillna(0).astype(int).add_suffix('_next')], axis=1)\n", "df = pd.concat([df, df.shift(-1).add_suffix('_next')], axis=1)\ndf = df.fillna(0).astype(int)\n"], "link": "https://stackoverflow.com/questions/50300983/involuntary-conversion-of-int64-to-float64-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Below is the dataframe with column name 'Address'. I want to create a separate column 'City' with specific string using filter from Address column. Below is the script that I am using Below is the intended output", "q_apis": "get columns name filter", "apis": "DataFrame columns apply", "code": ["import pandas as pd\ndf1=pd.DataFrame([[1,'India Gate Delhi'],[2,'Delhi Redcross Hospital'],[3,'Tolleyganj Bus Stand Kolkata'],[4,'Kolkata Howrah'],[5,'Katra Jammu']],columns=['Serial_No','Address'])\nprint(df1)\n\ndef f(df1):\n    if 'Delhi' in df1['Address']:\n        val = 'Delhi'\n    elif 'Kolkata' in df1['Address']:\n        val = 'Kolkata'\n    else:\n        val = 'None'\n    return val\ndf1['City'] = df1.apply(f, axis=1)\nprint(df1)\n"], "link": "https://stackoverflow.com/questions/67497271/creating-new-dataframe-column-using-string-filter-of-other-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two lists to be merged as a pandas dataframe. The columns would be the header of the CSV and the data contains the rows of data as a single list. Can someone help me merging these two lists as a pandas dataframe? Please let me know if I am missing on any other details. Thank you!", "q_apis": "get columns columns contains any", "apis": "DataFrame columns columns", "code": ["df = pandas.DataFrame([tuple(t) for t in cursor.fetchall()], columns=columns)\n"], "link": "https://stackoverflow.com/questions/52855568/how-to-merge-two-lists-of-different-lengths-as-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to implement the - in : According to dash: my parameter needs to follow this pattern: I am interested in using a with company names in my as the So I realised that I could leverage on the : results into the what I expect but with only the first in the company name column. It repeated this all through. What is wrong with my operation?", "q_apis": "get columns names first name all", "apis": "append", "code": ["option = []\n\nfor comp in df.company:\n    mydict = {}\n    mydict[\"label\"] = comp \n    mydict[\"value\"] = comp \n    option.append(mydict)\n"], "link": "https://stackoverflow.com/questions/62021718/updating-pandas-dictionary-python-while-looping"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe shown below. It has only one column (the 0'th column), whose values are all string: I am trying to use the values in column 0 as a formula in pandas (mentioned below) I have another empty data frame where I try the following to insert data: ...this comes to I need this to be so that I can save data available in \"Order No\" column of dataframe in \"columns1\" column of \"DF2\" dataframe [Note: p_input is another dataframe, due to some issues these assumptions can not change]", "q_apis": "get columns values all values empty where insert", "apis": "eval", "code": ["DF2 = eval(p_input[Line Of Business])"], "link": "https://stackoverflow.com/questions/64011134/pythonstring-using-a-string-saved-in-a-dataframe-cell-as-a-pandas-formula"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a training set and test set for machine learning, however the training set contains too many rows of data and the test set contains too little. I calculated I need to move 245 rows from the training set to the test set to produce a better split. How can I do this? I have 5116 total rows in training set. First I randomized the rows of the training set using this And then I wanted to grab the last 245 rows and move them to I found these two solutions here Pandas dataframe - move rows from one dataframe to another and Pandas move rows from 1 DF to another DF However they are selecting the rows based on a condition which I don't have. I kind of want to do it like you would in python using slice on arrays if that's possible. Maybe like (rows 0-5116 - 245 and all columns starting from 0) Then append that to the test set like I'm not sure if this is the correct way or not.", "q_apis": "get columns test contains test contains test last all columns append test", "apis": "iloc append drop index", "code": ["transferdata_df = train_df.iloc[- 245:, 0:]\n\ntest_df = test_df.append(transferdata_df)\n\ntrain_df =train_df.drop(transferdata_df.index)\n"], "link": "https://stackoverflow.com/questions/65433114/pandas-dataframe-move-n-number-of-rows-from-one-dataframe-to-another"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to use point reference data and turn it into a type that is time-series analysable. My data has start_date and end_date, which I used to calculate the duration of the event (if start and end_date on same day, return 1. Duration values range between 0 to 326). Furthermore, I have another pd dataframe which is a flattened series of images sized x*y, which has (among others) three columns: time, lon(x), lat(y). Now, I would like to append to the latter dataset a True boolean if the date falls inside (start_date + duration). Else, return False. Below I have created some example data out the required output to visualise what I would like to achieve. Does someone know how to approach this issue? Next, I would like to add true labels to the rows that fall within that duration. See the example of my data with the required output. Does somebody know how I should tackle this issue? Required output would look like: Edit: Below is the an example of how my data is really structured: Each day has 42x46 combinations of , which are passed before going to the next day. In the newly added table you see a forest fire occured on with coordinates x and y has a duration of 2. I would like to see that if I would now go 1932 (42x46) rows down to to see the column 'fire' (which is a label) updated to . Say we group by lon and lat, the data would look the data in the 'required output' example dataframe.", "q_apis": "get columns time start day values between columns time append date add day day now", "apis": "rename columns merge eq", "code": ["df['time'] = pd.to_datetime(df['time'])\ndf['end_time'] = pd.to_datetime(df['end_time']) + pd.Timedelta(1, unit='d')\n\ns = df.pop('end_time').sub(df['time']).dt.days\ndf = df.loc[df.index.repeat(s)].copy()\ncounter = df.groupby(level=0).cumcount()\ndf['time'] = df['time'].add(pd.to_timedelta(counter, unit='d'))\ndf = df.reset_index(drop=True)\nprint (df)\n   lat  long       time  duration\n0   50  -120 2020-01-01         4\n1   50  -120 2020-01-02         4\n2   50  -120 2020-01-03         4\n3   50  -120 2020-01-04         4\n4   60  -110 2020-05-06         1\n", "df1 = pd.DataFrame({'time': [np.datetime64('2020-01-03'),np.datetime64('2002-05-18'),\n                             np.datetime64('2002-05-18'),np.datetime64('2002-05-18'),\n                             np.datetime64('2002-05-18')],\n                   'lon': [-120, -115.875, -115.625, -115.375, -115.125],\n                   'lat': [50, 55.125, 55.125, 55.125, 55.125],\n                   'out_date': [np.datetime64('2002-05-19'), None, None, None, None],\n                   'duration_days': [2, None, None, None, None],\n                   'Fire': [True, None, None, None, None],\n                   'size_ha': [0.1, None, None, None, None],\n                   'fire_count': [1, None, None, None, None]})\n", "df = (df1.rename(columns={'lon':'long'})\n         .merge(df, \n                on=['lat','long','time'], \n                indicator='label', \n                how='left'))\ndf['label'] = df['label'].eq('both')\nprint (df)\n        time     long     lat   out_date  duration_days  Fire  size_ha  \\\n0 2020-01-03 -120.000  50.000 2002-05-19            2.0  True      0.1   \n1 2002-05-18 -115.875  55.125        NaT            NaN  None      NaN   \n2 2002-05-18 -115.625  55.125        NaT            NaN  None      NaN   \n3 2002-05-18 -115.375  55.125        NaT            NaN  None      NaN   \n4 2002-05-18 -115.125  55.125        NaT            NaN  None      NaN   \n\n   fire_count  duration  label  \n0         1.0       4.0   True  \n1         NaN       NaN  False  \n2         NaN       NaN  False  \n3         NaN       NaN  False  \n4         NaN       NaN  False  \n", "df = pd.DataFrame({'lat':[50,60], \n          'long':[-120,-110], \n          'time':[np.datetime64('2020-01-01'),np.datetime64('2020-05-06')],\n          'end_time':[np.datetime64('2020-01-04'),np.datetime64('2020-05-04')],\n          'duration':[4,1]})\n\ndf['time'] = pd.to_datetime(df['time'])\ndf['end_time'] = pd.to_datetime(df['end_time']) \nprint (df)\n   lat  long       time   end_time  duration\n0   50  -120 2020-01-01 2020-01-04         4\n1   60  -110 2020-05-06 2020-05-04         1 end_time  < time\n", "m = df['time'].gt(df['end_time'])\nd = {'end_time':'time','time':'end_time'}\ndf.loc[m, ['time','end_time']] = df.loc[m, ['end_time','time']].rename(columns=d)\n\ndf['end_time'] = df['end_time']  + pd.Timedelta(1, unit='d')\nprint (df)\n   lat  long       time   end_time  duration\n0   50  -120 2020-01-01 2020-01-05         4\n1   60  -110 2020-05-04 2020-05-07         1\n", "s = df.pop('end_time').sub(df['time']).dt.days\ndf = df.loc[df.index.repeat(s)].copy()\ncounter = df.groupby(level=0).cumcount()\ndf['time'] = df['time'].add(pd.to_timedelta(counter, unit='d'))\ndf = df.reset_index(drop=True)\nprint (df)\n   lat  long       time  duration\n0   50  -120 2020-01-01         4\n1   50  -120 2020-01-02         4\n2   50  -120 2020-01-03         4\n3   50  -120 2020-01-04         4\n4   60  -110 2020-05-04         1\n5   60  -110 2020-05-05         1\n6   60  -110 2020-05-06         1\n"], "link": "https://stackoverflow.com/questions/66901316/time-series-automate-labeling-based-on-value-per-day"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've got some data that looks like this: How can I get to a count or pivot that looks like below where the instance that Ashley and Jake both teach a class is properly added to the counts? The instance of one instructor is trivial, but two or more for a single class in the same cell is tripping me up. I'd like to get to something like this:", "q_apis": "get columns get count pivot where get", "apis": "groupby sum loc sum", "code": ["ret = (df['Instructor'].str.get_dummies('/')\n     .groupby(df['Class']).sum()\n)\nret.loc['Total'] = ret.sum()\n", "                     Ashley  Jake\nClass                            \nAlgorithms                1     1\nIntro to Philosophy       0     2\nSpanish I                 1     0\nVector Calculus           0     1\nTotal                     2     4\n"], "link": "https://stackoverflow.com/questions/64806044/counting-pivot-of-table-with-multiple-values-in-cell"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here is sample output: Basically each seller have multiple products. I tried but it gives output like: I don't understand where is the link1 then?", "q_apis": "get columns sample where", "apis": "DataFrame", "code": ["import pandas as pd\n\ndf_data = pd.DataFrame({\n  \"S_URL\": \"link1\",\n  \"NAME\": \"Name1\",\n  \"PHONE\": \"Phone1\",\n  \"Product\": [\"prod1\", \"prod2\", \"prod3\", \"prod4\"]\n})\n", "df = pd.concat([df2, df], sort=False)\nprint(df)\n\n  seller_link   name   phone Product   Price\n0       link2  Name2  Phone2   prod1  price1\n1       link2  Name2  Phone2   prod2  price2\n2       link2  Name2  Phone2   prod3  price3\n3       link2  Name2  Phone2   prod4  price4\n0       link1  Name1  Phone1   prod1  price1\n1       link1  Name1  Phone1   prod2  price2\n2       link1  Name1  Phone1   prod3  price3\n3       link1  Name1  Phone1   prod4  price4\n"], "link": "https://stackoverflow.com/questions/65758948/how-to-create-dataframe-where-some-fields-changes-and-remaining-repeats"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need some help in converting the following code to a more efficient one without using iterrows(). The above code basically reads a string under 'index_vec' column, parses and converts to integers, and then increments the associated columns by one for each integer. An example of the output is shown below: Take the 0th row as an example. Its string value is \"[370, 370, -1]\". So the above code increments column \"370\" by 2 and column \"-1\" by 1. The output display is truncated so that only \"-10\" to \"17\" columns are shown. The use of iterrows() is very slow to process a large dataframe. I'd like to get some help in speeding it up. Thank you.", "q_apis": "get columns iterrows columns value columns iterrows get", "apis": "DataFrame value_counts index index apply", "code": ["      index_vec  1201  370  -1\n0  [370, -1, -1]     0    0   1\n1   [1201, 1201]     0    1   1\n", "import pandas as pd \n\ndf = pd.DataFrame({'index_vec': [\"[370, -1, -1]\", \"[1201, 1201]\"], '1201': [0, 0], '370': [0, 1], '-1': [1, 1]})\n\ndef add_counts(x):\n  counts = pd.Series(x['index_vec'].strip(\"[]\").split(\", \")).value_counts()\n  x[counts.index] = x[counts.index] + counts\n  return x\n\ndf.apply(add_counts, axis = 1)\n\nprint(df)\n", "      index_vec  1201  370  -1\n0  [370, -1, -1]     0    1   3\n1   [1201, 1201]     2    1   1\n"], "link": "https://stackoverflow.com/questions/61993448/how-to-avoid-iterrows-for-this-pandas-dataframe-processing"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a hard time merging and updating Pandas dataframes right now. I have a bunch of CSV files that I'm parsing with pandas (which is not a problem). In very few cases I have multiple files that contains some columns present in both files. So, for example, let's say I have: What I want is this dataframe: Pandas has this nice guide: Merge, join, concatenate and compare. But I fail to find a solution to what I want to achieve. For example raises . Passing is not an option, because the end result is: Not quite what I want. looks promising, but it is not quite right either, because the indices are ignored: Passing and yields a dataframe similar to , so not what I want. Using is almost what I want, would modify to but does nothing (I assume because the indices of and are disjunct). So, is what I want even possible with a single line of code? EDIT I came up with this one: This is what I want, the question is: is this correct or just a coincidence that this yields the same result as I wanted? How are you determining which 'A' column has priority? In the order I'm reading the files. The files are generated by a device (which is kind of a \"black box\" to me) and generates files with a date in them. So I do: And I would like to do (no error checking as this is an example):", "q_apis": "get columns time right now contains columns join compare right indices indices date", "apis": "DataFrame combine_first", "code": ["from functools import reduce\n\nto_merge = [c, b, a]\n\nresult = reduce(pd.DataFrame.combine_first, to_merge)\n", ">>> result\n\n        A    B\n0   110.0  4.0\n1   111.0  5.0\n2     2.0  6.0\n3   113.0  7.0\n41   11.0  NaN\n51   12.0  NaN\n61   13.0  NaN\n71   14.0  NaN\n"], "link": "https://stackoverflow.com/questions/68025366/merging-and-updating-multiple-pandas-dataframes-with-overlapping-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In my Python application, I find it handy to use a dictionary of dictionaries as the source data for constructing a sparse pandas DataFrame, which I then use to train a model in sklearn. The structure of the dictionary is like this: Ideally, I'd like turn it into a dataframe like this: Which generates this: Now, here's my problem. My data has a number of rows in the hundreds of thousands (ie, the number of keys in the outer dictionary). Each one of these has only a handful of columns associated with it (ie, the number of keys in each inner dictionary), but the total number of columns numbers in the thousands. I've found DataFrame generation using from_dict to be very slow, on the order of 2.5-3 minutes for 200,000 rows and 6,000 columns. Furthermore, in the case when the row index is a MultiIndex (ie, instead of X, Y and Z the keys of the outer directionary are tuples), from_dict is even slower, on the order of 7+ minutes for 200,000 rows. I've found that this overhead can be avoided if instead of a dictionary of dictionaries, one uses a list of dictionaries and then adds the MultiIndex back to the resulting DataFrame using set_index. In summary, how would you suggest I deal with this? Performance with the MultiIndex can clearly be improved by the library developers, but am I using the wrong tool for the job here? If written to disk, the DataFrame is around 2.5GB in size. Reading a 2.5GB file from disk in around 2 or so minutes seems about right, but the sparsity of my data in memory should theoretically allow this to be much faster.", "q_apis": "get columns DataFrame keys columns keys columns DataFrame from_dict columns index MultiIndex keys from_dict MultiIndex DataFrame set_index MultiIndex DataFrame size right", "apis": "DataFrame index columns", "code": ["vectorizer = sklearn.feature_extraction.DictVectorizer(dtype=numpy.uint8, \nsparse=False)\n\nmatrix = vectorizer.fit_transform(data)\ncolumn_labels = vectorizer.get_feature_names()\n\ndf = pandas.DataFrame(matrix, index=row_labels, columns=column_labels)\n"], "link": "https://stackoverflow.com/questions/49491511/pandas-dataframe-from-dict-poor-performance-when-generating-from-a-lengthy-dic"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to parse the below xml into a Pandas dataframe that would like something like the below: xml_data The below code works for another xml I have but for the above it only returns: I guess I need to start iterating from the Header level? any thoughts much appreciated thanks", "q_apis": "get columns parse start any", "apis": "product get values combine values product merge sub at append append DataFrame values product values append DataFrame", "code": ["import xml.etree.ElementTree as ET\nfrom itertools import product, chain\n\nroot = ET.parse('test.xml').getroot()\n\nelements = ['TimeserieId', 'ObjectID', 'Unit', 'Granularity','Name','LastUpdate','Value']\nbox = []\nfor entry in root.findall(\".//Timeseries\"):\n    #initialize dictionary within for loop\n    #to create new dict everytime we hit another Timeseries element\n    d = {}\n    for key in elements:\n        for ent in entry.findall(f\".//{key}\"):\n            d[key] = ent.text\n        #get values from the Values element\n        content = [ent.attrib for ent in entry.findall(f\".//Values/Value\")]\n    #combine the dictionary with the list of values[FromUTC, UntilUTC]\n    #this will be a generator expression containing two dictionaries\n    #this way, we pair the dictionary with each FromUTC and UntilUTC\n    merger = product([d],content)\n    #merge the dictionaries in each sub generator expression\n    #I would suggest you run the code step by step to understand what is \n    #going on here\n    #you could even spot some opportunity for optimization\n    merger = [{**dict1, **dict2} for dict1, dict2 in merger]\n    \n    #at the end of every iteration within the for loop\n    #append the result from merger to the box list\n    #this will hold the results of the iteration \n    #from which our dataframe will be created\n    box.append(merger)\n\nprint(box)\n\n[[{'TimeserieId': '509',\n   'ObjectID': 'EntryArnoldstein_GebuchteKapazitaetJaehrlich',\n   'Unit': 'kWh/h',\n   'Granularity': 'HOUR',\n   'Name': 'BCY En Arnoldstein',\n   'LastUpdate': '2020-03-02T14:40:00.000Z',\n   'Value': '521331.0',\n   'FromUTC': '2020-06-17T11:00:00.000Z',\n   'UntilUTC': '2020-06-17T12:00:00.000Z'},\n  {'TimeserieId': '509',\n   'ObjectID': 'EntryArnoldstein_GebuchteKapazitaetJaehrlich',\n   'Unit': 'kWh/h',\n   'Granularity': 'HOUR',\n   'Name': 'BCY En Arnoldstein',\n   'LastUpdate': '2020-03-02T14:40:00.000Z',\n   'Value': '521331.0',\n   'FromUTC': '2020-06-17T12:00:00.000Z',\n   'UntilUTC': '2020-06-17T13:00:00.000Z'}],\n [{'TimeserieId': '530',\n   'ObjectID': 'EntryArnoldstein_TechnischeKapazitaetJaehrlich',\n   'Unit': 'kWh/h',\n   'Granularity': 'HOUR',\n   'Name': 'TCY En Arnoldstein',\n   'LastUpdate': '2020-03-02T19:00:08.000Z',\n   'Value': '17377622',\n   'FromUTC': '2020-06-17T11:00:00.000Z',\n   'UntilUTC': '2020-06-17T12:00:00.000Z'},\n  {'TimeserieId': '530',\n   'ObjectID': 'EntryArnoldstein_TechnischeKapazitaetJaehrlich',\n   'Unit': 'kWh/h',\n   'Granularity': 'HOUR',\n   'Name': 'TCY En Arnoldstein',\n   'LastUpdate': '2020-03-02T19:00:08.000Z',\n   'Value': '17377622',\n   'FromUTC': '2020-06-17T12:00:00.000Z',\n   'UntilUTC': '2020-06-17T13:00:00.000Z'}]]\n\n#flatten the box list\n#to eliminate the sublists\n#and have only dictionaries within\n\nflatten = chain.from_iterable\nbox = flatten(box)\n\n#create dataframe\npd.DataFrame(box)\n", "box = []\nfor entry in root.findall(\".//Timeseries\"):\n    d = {ent.tag: ent.text for ent in entry.findall(\".//*\") if not ent.text.isspace()}    \n    values = [ent.attrib for ent in entry.findall(\".//Values/Value\")]\n    merger = product([d],values)\n    merger = [{**dict1, **dict2} for dict1, dict2 in merger]\n    box.append(merger)\n", "#flatten box to remove sublists, and keep only the dictionaries : \n   flatten = chain.from_iterable\n   pd.DataFrame(flatten(box))\n"], "link": "https://stackoverflow.com/questions/62428514/parse-xml-file-into-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have weekly data for various stores in the following form: As you can see the data is at a store week level and I want to calculate euclidean distance between each store for the same week and then take an average of the calculated distance. So for example the calculation for Store S1 and S2 would look as follows: Finally the output should be as follows: I have some idea about group by function for grouping columns in data frame and scipy.spatial.distance.cdist for calculating euclidean distances, but I am unable to tie up these concepts and come up with a solution.", "q_apis": "get columns at week between week take columns", "apis": "pivot index columns values fillna DataFrame index index columns index", "code": ["df1  = (df.pivot(index='Store', columns='Week', values=['Sales', 'Cust_count'])\n       #  .fillna(0)  # Uncomment if you want to treat missing store-weeks as 0s\n       )\narr1 = df1['Sales'].to_numpy()\narr2 = df1['Cust_count'].to_numpy()\n\ndata = np.nanmean(np.sqrt(((arr1[None, :] - arr1[:, None])**2 \n                         + (arr2[None, :] - arr2[:, None])**2)), \n                  axis=2)\n\npd.DataFrame(data, index=df1.index, columns = df1.index)\n", "Store        S1        S2        S3\nStore                              \nS1     0.000000  1.414214  2.828427\nS2     1.414214  0.000000  1.414214\nS3     2.828427  1.414214  0.000000\n"], "link": "https://stackoverflow.com/questions/58065741/calculate-euclidean-distance-between-groups-in-a-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list of dataframes with differing no. of rows: I want to transpose each dataframe in the list and concatenate it to one dataframe. Since there are over 600 dataframes in my list, I wanted to use a loop... I was only able to apply this to single dataframes. Code for one single data frame: My try: How can I do it more efficiently for all the dataframes in my list??", "q_apis": "get columns transpose apply all", "apis": "T rename index", "code": ["df_final = [x.T.rename(index={'Score':0}) for x in L]\n"], "link": "https://stackoverflow.com/questions/58995953/loop-to-transpose-and-concatenate-list-of-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm new to python and very new to Pandas. I've looked through the Pandas documentation and tried multiple ways to solve this problem unsuccessfully. I have a DateFrame with timestamps in one column and prices in another, such as: In addition to the two columns of interest, this DataFrame also has additional columns with data not particularly relevant to the question (represented with OtherData Cols). I want to create a new column 'Fut2Min' (Price Two Minutes into the Future). There may be missing data, so this problem can't be solved by simply getting the data from 2 rows below. I'm trying to find a way to make the value for Fut2Min Col in each row == the Price at the row with the timestamp + 120000 (2 minutes into the future) or null (or NAN or w/e) if the corresponding timestamp doesn't exist. For the example data, the DF should be updated to: (Code used to mimic desired result)", "q_apis": "get columns columns DataFrame columns value at timestamp timestamp", "apis": "values loc iloc apply", "code": ["       TimeStamp  Price  OtherData1  OtherData2  Fut2Min\n0  1603822620000    101           1           8        0\n1  1603822680000    105           2           7        0\n2  1603822740000    102           3           6        0\n3  1603822800000    108           4           5        0\n4  1603823040000    105           5           4        0\n5  1603823100000    101           6           3        0\n6  1603823160000    106           7           2        0\n7  1603823220000    111           8           1        0\n", "import pandas as pd\n\ndef Fut2MinFunc(row):\n    futTimeStamp = row.TimeStamp + 120000\n    if (futTimeStamp in df.TimeStamp.values):\n        return df.loc[df['TimeStamp'] == futTimeStamp, 'Price'].iloc[0]\n    else:\n        return None\n\ndf['Fut2Min'] = df.apply(Fut2MinFunc, axis = 1)\n", "       TimeStamp  Price  OtherData1  OtherData2  Fut2Min\n0  1603822620000    101           1           8    102.0\n1  1603822680000    105           2           7    108.0\n2  1603822740000    102           3           6      NaN\n3  1603822800000    108           4           5      NaN\n4  1603823040000    105           5           4    106.0\n5  1603823100000    101           6           3    111.0\n6  1603823160000    106           7           2      NaN\n7  1603823220000    111           8           1      NaN\n"], "link": "https://stackoverflow.com/questions/64591225/create-new-pandas-dataframe-column-equaling-values-from-other-row-in-same-datafr"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a series of dataframes which contain rainfall data from a selection of raingauges that were operational at overlapping times over the last twenty years. For example the first worked between 2001 and 2004, then second worked between 2003 and 2008, the third between 2007 and 2015. They all have date as their index, but I can't figure out how to merge them while keeping all indexes even when I use the following which I thought would work: I had expected this to produce a dataframe with an index from 2001 and 2008, with two columns containing the recorded data. Instead, it returns from 2003 to 2008, i.e. the indexes from the second dataframe... any ideas? Many thanks in advance!", "q_apis": "get columns at last first between second between between all date index merge all index columns second any", "apis": "merge", "code": ["RG1_2 = RG1.merge(RG2, left_index=True, right_index=True,how='outer')\n"], "link": "https://stackoverflow.com/questions/53192562/python-join-merge-dataframes-with-overlapping-but-different-date-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe. I am grouping it by columns A, B and applying a function to each group. The function returns a dictionary which is appended as a row to a new dataframe. This is what I have done: is having only NaN values after the operation. Is there a more efficient way to perform this?", "q_apis": "get columns columns values", "apis": "append groupby", "code": ["df.append([foo(x) for _, x in df.groupby(['A', 'B'])], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/67253028/pandas-groupby-and-apply-function-that-returns-a-dictionary"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I Have read a CSV file (that have name and addresses of customers) and assign the data into DataFrame table. Description of the csv file (or the DataFrame table) DataFrame contains several rows and 7 columns Database example So far I have written this code to generate the aformentioned table : The code is I am looking to remove the entire row if there are repetitive values in the Client id, Client name , and Full_Address columns, so far code doesnt show any error but at the same time, I havnt got the expected out ( i do beleive the modification would be in the last line of the attached code) The expected output is", "q_apis": "get columns name assign DataFrame DataFrame DataFrame contains columns values name columns any at time last", "apis": "drop_duplicates first first last", "code": ["  <your_dataframe>.drop_duplicates(subset=[\"Client_id\", \"Client_name\", \"Full_Address\"], keep=\"first\") # \"first\" or \"last\"\n"], "link": "https://stackoverflow.com/questions/58538400/removing-entire-row-if-there-are-repetitive-values-in-specific-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hi I have three lists \"A\",\"B\",\"C\" which contains 10 ,20 30 values respectively.I need to create a dataframe as shown below The columns here are Values and Type. Sorry Couldnt draw a better figure. .", "q_apis": "get columns contains values columns", "apis": "DataFrame DataFrame DataFrame append append reset_index drop loc loc loc DataFrame index DataFrame index DataFrame index append append index reset_index drop", "code": ["df1 = pd.DataFrame({'Type': ['A']*100})\ndf2 = pd.DataFrame({'Type': ['B']*75})\ndf3 = pd.DataFrame({'Type': ['C']*80})\n\n#Concatenate the DataFrames\ndf=df1.append(df2).append(df3).reset_index(drop=True)\n\ndf.loc[df['Type']=='A','Values']='ListA_values'\ndf.loc[df['Type']=='B','Values']='ListB_values'\ndf.loc[df['Type']=='C','Values']='ListC_values'\n\ndf=df[['Values','Type']]\n", "df1 = pd.DataFrame({'Type': ['A']*100})\ndf1.index = ['ListA_values'] * len(df1)\ndf2 = pd.DataFrame({'Type': ['B']*75})\ndf2.index = ['ListB_values'] * len(df2)\ndf3 = pd.DataFrame({'Type': ['C']*80})\ndf3.index = ['ListC_values'] * len(df3)\n\n#Concatenate the DataFrames\ndf=df1.append(df2).append(df3)\ndf['Values'] = df.index\ndf=df.reset_index(drop=True)\ndf=df[['Values','Type']]\n\ndf\nOut[31]: \n         Values Type\n0  ListA_values    A\n1  ListA_values    A\n2  ListA_values    A\n3  ListA_values    A\n4  ListA_values    A\n5  ListA_values    A\n6  ListA_values    A\n..            ...  ...\n252  ListC_values    C\n253  ListC_values    C\n254  ListC_values    C\n"], "link": "https://stackoverflow.com/questions/53393049/how-to-add-a-column-which-has-three-different-values-at-varying-lengths"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list that looks like this: and I want to convert it to a pandas dataframe like this: The first columns of characters in the list has a fixed lenght.", "q_apis": "get columns first columns", "apis": "DataFrame columns", "code": ["df = pd.DataFrame([i.split(maxsplit=2)[1:] for i in Sum],columns=['col1','col2'])\n", "                  col1           col2\n0          Report_type          Leach\n1          Result_text  Concentration\n2               Run_Id            179\n3             Location       MUENSTER\n4        Meteo_station         KREM-M\n5            Soil_type           KREM\n6        Crop_calendar      SUGARBEET\n7            Substance            ABC\n8   Application_scheme            DRY\n9    Deposition_scheme             No\n10             Results         0.0001\n"], "link": "https://stackoverflow.com/questions/63179502/convert-list-of-strings-to-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have a simple pandas dataframe as so: I would like to find the row indices of a specific pattern that spans both columns. In my real application, the above dataframe has a few thousand rows and I have a few thousand dataframes so performance is not important. The pattern, say, that I am interested in is: Hence, using the above example, my desired output would be: Typically of course, for one pattern, one would do something like but I am not sure how to do it when I am looking for a specific and multivariate pattern over multiple columns. I.e. I am looking for (and I am pretty the following is not correct): . Help much appreciated. Thx!", "q_apis": "get columns indices columns columns", "apis": "index isin isin index", "code": ["df1 = df[(df.name == 'bob') & df.car.isin(['b','c'])]\nprint (df1)\n   name car\n0   bob   b\n1   bob   c\n8   bob   b\n9   bob   c\n10  bob   b\n11  bob   c\n", "out_idx = df.index[(df.name == 'bob') & df.car.isin(['b','c'])]\n", "out_idx = df[(df.name == 'bob') & df.car.isin(['b','c'])].index\n", "df1 = df[(df.name == 'bob') & ((df.car == 'b') | (df.car == 'c'))]\n"], "link": "https://stackoverflow.com/questions/58195572/find-all-indices-instances-of-all-repeating-patterns-across-columns-and-rows-of"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like - I have to remove those rows from the datframe in which EntityName column's first letter is lowercase. i.e I have to retain values that start with a upper case. I have used to methods till now - but it is giving NaN values. another thing that i tried was regex. but it is showing no effect. another one was - but it is showing error - 'Series' object has no attribute 'isupper' Can someone suggest me the correct code snippet or any kind of hint?", "q_apis": "get columns first values start now values Series any", "apis": "fillna fillna mask mask", "code": ["df = df[df['EntityName'].str[0].str.isupper()]\n#for working with NaN and None\n#df = df[df['EntityName'].str[0].str.isupper().fillna(False)]\n", "df = df[~df['EntityName'].str[0].str.islower()]\n#for working with NaN and None\ndf = df[~df['EntityName'].str[0].str.islower().fillna(False)]\n", "df = df[[x[0].isupper() for x in df['EntityName']]]\n", "mask = [x[0].isupper() if isinstance(x,str) and len(x)>0 else False for x in df['EntityName']]\ndf = df[mask]\n", "print (df)\n              FileName          ...                       EntityName\n1 17743633 -         1          ...            Ambuja Cement Limited\n2 17743633 -         1          ...                      Vessel Name\n5 17743633 -         1          ...            Binani Cement Limited\n"], "link": "https://stackoverflow.com/questions/50599934/removing-rows-from-dataframe-whose-first-letter-is-in-lowercase"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have below dataframe and I want to calcultae cumulative average for each date by considering current date and previous all date data. but below script does not help: I need result like below: In MS excel we can find average for latest date -2018 by selecting all previous rows like below: like above I want to calculate for all dates", "q_apis": "get columns date date all date date all all", "apis": "sort_values reset_index groupby index cumsum index values where duplicated last reset_index drop where duplicated last reset_index drop", "code": ["df = df.sort_values(['Items', 'Date', 'Variable'], ascending=[True, True, True])\n\nx = df.reset_index().groupby(['Items', 'Variable'])['Value']\nindex = x.cumcount()+1\ndf['Value'] = x.cumsum()/(index.values)\n\ndf1 = df[np.where(df[['Items', 'Variable', 'Date']].duplicated(keep='last'), False, True)].reset_index(drop=True)\n", "    Items   Variable    Date    Value\n0   Item1   V1       2020-12-16 5.000000\n1   Item1   V1       2020-12-17 3.000000\n2   Item1   V1       2020-12-18 2.333333\n3   Item2   V2       2020-12-16 1.500000\n4   Item2   V2       2020-12-17 1.500000\n5   Item2   V2       2020-12-18 1.400000\n", "df[np.where(df[['Items', 'Variable', 'Date']].duplicated(keep='last'), False, True)].reset_index(drop=True)"], "link": "https://stackoverflow.com/questions/65410379/how-to-get-average-for-each-date-by-considering-current-date-and-previous-all-da"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to create a dataframe that contains all possible start times for a scheduler for some machines. My initial dataframe (msDF) contains three simple columns: MachID - the ID of each machine Start - the starting datetime that the machine is available for scheduling slots - the number of slots available starting from that time msDF is copied from a master dataframe, but for illustration, it may look like this: MachID Start slots 0 1 02/04/2021 9:00 2 1 2 06/04/2021 12:30 3 2 3 09/04/2021 10:00 4 3 1 12/04/2021 11:00 3 4 1 15/04/2021 08:00 1 I need to explode this dataframe so that each row is duplicated \"slots\" times with a slotIndex. The desired output is: MachID Start slots SlotIndex 0 1 02/04/2021 9:00 2 0 0 1 02/04/2021 9:00 2 1 1 2 06/04/2021 12:30 3 0 1 2 06/04/2021 12:30 3 1 1 2 06/04/2021 12:30 3 2 My approach is problematic. I am creating variable length lists into the SlotIndex and exploding them, but this creates warnings. To do this, I use: It works but with warnings : SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame I later explode msDF to get the result I want: How can this be improved?", "q_apis": "get columns contains all start contains columns time explode duplicated length value copy DataFrame explode get", "apis": "groupby index transform cumsum", "code": ["df['slot_id'] = 1\ndf['slot_id'] = df.groupby(df.index)['slot_id'].transform('cumsum')\n"], "link": "https://stackoverflow.com/questions/66621981/create-a-pandas-dataframe-column-of-variable-sized-lists"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have multiple dataframes like below. DF1: DF2 DF3 I want to store them to a file like below.So the data should be in sequential order from each dataframe. Can any one help me to find a way to store in the below format in a file.", "q_apis": "get columns any", "apis": "to_csv", "code": ["df_list = [DF1,DF2,DF3] ## create a list with your dataframes\ndf = pd.concat(df_list)\n\ndf.to_csv('test.csv')\n"], "link": "https://stackoverflow.com/questions/62121540/how-to-export-data-to-csv-sequentially-from-multiple-data-frames-in-a-sequential"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here is my code snippet and results After I get a results from query, I want to show in bar graph but there is an error here. After I revised this code shown below. The error is defined below.", "q_apis": "get columns get query", "apis": "query value_counts reindex fillna", "code": ["rating_order =  ['G', 'TV-Y', 'TV-G', 'PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG', 'PG-13', 'TV-14', 'R', 'NC-17', 'TV-MA']\ntv_show_rating = netflix_df.query(\"type == 'TV Show'\")[\"rating\"].value_counts().reindex(rating_order).fillna(0)\n"], "link": "https://stackoverflow.com/questions/61783576/dataframe-throwing-passing-list-likes-to-loc-or-with-any-missing-labels-is"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Lets consider two data frames I want to compare the two dataframes on the column G, but we should do it only if each row in each dataframe as same value., So from A to F, if each row is same in df1 and df2 generate a column called result which shows column G from df1 - column G from df2 to yield a dataframe like this. The row number 5 in df2 should be discarded. I tried", "q_apis": "get columns compare value", "apis": "columns merge assign drop columns columns set_index merge set_index apply reset_index", "code": ["cols = [col for col in df2.columns if col != 'G']\ndf2 = df2.merge(df1, on=cols)\ndf2.assign(Result=df2['G_y'] - df2['G_x']).drop(columns=['G_x', 'G_y'])\n", "    A   B   C   D   E   F  Result\n0  a1  b1  c1  d1  e1  f1     0.0\n1  a2  b2  c2  d2  e2  f2     0.0\n2  a3  b3  c3  d3  e3  f2     1.0\n3  a4  b4  c4  d4  e4  f4     NaN\n", "cols = [col for col in df2.columns if col != 'G']\n\ndf2.set_index(cols).merge(df1.set_index(cols), \n                          left_index=True,\n                          right_index=True).apply(lambda x: x['G_x'] - x['G_y'], axis=1)\\\n                                           .reset_index(name=\"Result\")\n"], "link": "https://stackoverflow.com/questions/57517132/how-to-do-pandas-data-frame-comparison-on-one-column-row-by-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Assume, I have a df with some integers: And condition for them: What it the most efficient way to count the number of rows above current since the condition was true? It should be like so: UPD. The df should look so:", "q_apis": "get columns count", "apis": "cumsum gt groupby", "code": ["s = mdf['CM'].cumsum()\nmdf[\"SINCE\"] = mdf[s.gt(0)].groupby(s).cumcount()\n\nprint (mdf)\n      A     CM  SINCE Formula\nIdx                          \n0    23  False    NaN     NaN\n1    55  False    NaN     NaN\n2    48  False    NaN     NaN\n3    17  False    NaN     NaN\n4    24   True    0.0     4-4\n5    30  False    1.0     5-4\n6    99   True    0.0     6-6\n7    11  False    1.0     7-6\n8    47   True    0.0     8-8\n9    25  False    1.0     9-8\n10   78  False    2.0    10-8\n11   40   True    0.0   11-11\n12   99   True    0.0   12-12\n13    7  False    1.0   13-12\n14    6  False    2.0   14-12\n15   64   True    0.0   15-15\n16   62  False    1.0   16-15\n17   39   True    0.0   17-17\n18   41   True    0.0   18-18\n19   28  False    1.0   19-18\n"], "link": "https://stackoverflow.com/questions/67364920/count-the-number-of-rows-above-since-the-condition-was-true"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have all these dataframes: I have to do this if I wanted to join two of them: Then to keep joining more I do this: How do I do this in just one go?", "q_apis": "get columns all join", "apis": "append DataFrame merge drop_duplicates", "code": ["file_list = [\n  \"Demographic Data Part 1\",\n  \"Demographic Data Part 2\",\n  ...\n]\n\ndf_list = []\nfor file_name in file_list:\n  df = pd.read_csv(file_name,index_col=0,dtype={'Year':object})\n  df_list.append(df)\n\ncombined_df = pd.DataFrame()\nfor df in df_list:\n  combined_df = combined_df.merge(df.drop_duplicates(subset=['Location+Type']), how='left')\n"], "link": "https://stackoverflow.com/questions/59884835/merging-multiple-dataframes-together-all-at-once"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a (test3) which looks like this ( is format) where the first column is an index of dataframe. I'm rendering new data ( var for for the first column) and ( var for third column) Another columns are not important. makes bottom of result looks like which is not what I was looking for.. I just want to append data at the end of dataframe(or both acceptable), preserving shape and indexes. How could I make it the same format? Like this", "q_apis": "get columns where first index var first var columns append at shape", "apis": "DataFrame set_index DataFrame set_index loc loc loc", "code": ["data = {'date': ['1890-07-01 00:00:00', '1890-07-08 00:00:00', '1890-07-15 00:00:00', '1890-07-22 00:00:00', '1890-07-29 00:00:00'],\n    'date.1': ['1890-07-07', '1890-07-14', '1890-07-21', '1890-07-28', '1890-08-04'],\n    'mean_temp': [23.3, 23.9, 28.3, 26.1, 26.8],\n    'max_temp': [32.3, 33.2, 35.8, 33.3, 34.6],\n    'min_temp': [18.9, 17.0, 22.5, 22.0, 22.3]}\ndf = pd.DataFrame(data)\ndf.date = pd.to_datetime(df.date)\ndf.set_index('date', inplace=True)\n\nrendered_date = pd.to_datetime('2020-08-07')\nnext_value_ = 28.330473\nd = {'date': [rendered_date], 'mean_temp': [next_value_]}\ndf = pd.concat([df, pd.DataFrame(d).set_index('date')])\n", "                date.1  mean_temp  max_temp  min_temp\ndate\n1890-07-01  1890-07-07  23.300000      32.3      18.9\n1890-07-08  1890-07-14  23.900000      33.2      17.0\n1890-07-15  1890-07-21  28.300000      35.8      22.5\n1890-07-22  1890-07-28  26.100000      33.3      22.0\n1890-07-29  1890-08-04  26.800000      34.6      22.3\n2020-08-07         NaN  28.330473       NaN       NaN\n", "df.loc[rendered_date] = {'mean_temp': next_value_}\n# # or\n# df.loc[rendered_date] = [np.nan, next_value_, np.nan, np.nan]\n# # or even\n# df.loc[rendered_date, 'mean_temp'] = next_value_\n"], "link": "https://stackoverflow.com/questions/63385974/add-values-to-dataframe-preserving-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm attempting to use a lambda and to extract the latitude and longitude coordinates from a data frame. The dataframe is as follows I'm attempting to grab the coordinates from the state column with the following. Here's an example of one of the state column values: But each time I do, I get the following error: I did a search around and found a similar error reported here on github for Pandas but couldn't quite understand the conclusion, other than the inferred type of float is incorrect. Any suggestions? Thanks in advance.", "q_apis": "get columns values time get", "apis": "apply notnull", "code": ["df['State'].apply(lambda x: x[x.find(\"(\")+1:x.find(\")\")] if pd.notnull(x) else x)\n"], "link": "https://stackoverflow.com/questions/50952907/attribute-error-when-using-find-within-an-lambda"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given two dataframes (that can contain multiple rows with same id): I'd like to get a list (or better: a set) of all email addresses for each user: (I tried various things with , , but without success, I don't have a clear view of a pythonic solution.) How to merge 2 dataframes and create a list/set of values relative to a few columns (here , , )?", "q_apis": "get columns get all view merge values columns", "apis": "set_index set_index filter stack reset_index drop", "code": ["s1 = df1.set_index('userid')['email_work']\ns2 = df2.set_index('id').filter(like='email').stack().reset_index(level=1, drop=True)\n", "df = (pd.concat([s1, s2])\n        .groupby(level=0)\n        .agg(set)\n        .rename_axis('userid')\n        .reset_index(name='emails'))\nprint (df)\n   userid                       emails\n0       1                    {a@a.com}\n1       2           {b@b.com, f@f.com}\n2       3                    {c@c.com}\n3       4  {g@g.com, d@d.com, e@e.com}\n"], "link": "https://stackoverflow.com/questions/61156889/merge-2-dataframes-and-create-a-list-of-values-relative-to-a-few-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two Dataframes one with set of dates (df1) and another with set of emp_ids (df2). I am trying to create a new Dataframe such that every emp_id in df2 is tagged to every date in df1. Given below is how my Dataframe look like df1 df2 Expected output: I converted the date column to a string and tried doing the below but it returned an empty Dataframe I tried doing", "q_apis": "get columns date date empty", "apis": "merge drop DataFrame columns DataFrame columns merge drop sort_values", "code": ["df1['key'] = 0\ndf2['key'] = 0\n\nresult = df1.merge(df2, how='outer').drop('key',axis= 1)\n", "df1 = pd.DataFrame(['2018-01-01','2018-01-02','2018-01-03','2018-01-04'],columns=['date'])\ndf2 = pd.DataFrame(['emp_1','emp_2','emp_3'],columns=['id'])\n\n# res\ndf1['key'] = 0\ndf2['key'] = 0\n\nres = df1.merge(df2, how='outer').drop('key',axis= 1)\n\n# print\nprint(res.sort_values('id'))\n", "    date        id\n0   2018-01-01  emp_1\n3   2018-01-02  emp_1\n6   2018-01-03  emp_1\n9   2018-01-04  emp_1\n1   2018-01-01  emp_2\n4   2018-01-02  emp_2\n7   2018-01-03  emp_2\n10  2018-01-04  emp_2\n2   2018-01-01  emp_3\n5   2018-01-02  emp_3\n8   2018-01-03  emp_3\n11  2018-01-04  emp_3\n"], "link": "https://stackoverflow.com/questions/54179894/pandas-filling-each-rows-of-one-dataframe-with-value-from-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to print summary statistics from a for loop into a dataframe. Currently I am printing it as a long string. I would really like to iteratively fill a dataframe and print that but I am not sure how. What I would like the output to look like is:", "q_apis": "get columns", "apis": "items sort_values items groupby append sum count DataFrame columns items sort_values DataFrame columns items groupby sum count DataFrame columns append", "code": ["items = [\"a\", \"b\", \"c\", \"d\", \"e\",\"f\"]\nyear = df.sort_values().unique()\n\nresult = []\nfor j, item in enumerate(items):\n    groups = df.groupby(\"year\")\n    i = 0\n    for label, binned_data in groups:\n        result.append[year[i], item, binned_data['var1'].sum(), binned_data['var2'].count()]\n        i += 1\ndf = pd.DataFrame(result, columns=[\"Year\", \"Label\", \"Var1_Sum\", \"Var2_Count\"])\nprint (df)\n", "items = [\"a\", \"b\", \"c\", \"d\", \"e\",\"f\"]\nyear = df.sort_values().unique()\n\ndf = pd.DataFrame(columns=[\"Year\", \"Label\", \"Var1_Sum\", \"Var2_Count\"])\nfor j, item in enumerate(items):\n    groups = df.groupby(\"year\")\n    i = 0\n    for label, binned_data in groups:\n        result = [year[i], item, binned_data['var1'].sum(), binned_data['var2'].count()]\n        df = pd.DataFrame(result, columns=[\"Year\", \"Label\", \"Var1_Sum\", \"Var2_Count\"]).append(df, ignore_index=True)\n        print (df)\n        i += 1\n"], "link": "https://stackoverflow.com/questions/64654298/printing-data-from-a-for-loop-into-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 dataframe one with a shape of df1 = (1000, 2) and another with shape df2 = (2500, 4) now I am trying to merge them on single common column in both but getting different error on each try. Note: in df2 account_id can be repeated so thats the reason of (2500, 2) Here I want df2['label'] to be merged in df1 on account_id", "q_apis": "get columns shape shape now merge", "apis": "drop_duplicates set_index", "code": ["s = df2.drop_duplicates(subset=['account_id']).set_index('account_id')['label']\ndf1['label'] = df1['account_id'].map(s)\n"], "link": "https://stackoverflow.com/questions/51258736/merge-dataframe-column-with-another-dataframe-on-single-column-with-different-sh"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list of columns in a dataframe that shouldn't be empty. I want to remove any rows that are empty in any of these columns. My solution would be to iterate through the required columns and set the column 'excluded' to the error message that the user will be shown before excluding them (I will present these to the user in the form of a report at the end of the process) I'm currently trying something like this: but no luck - the columns aren't updated. The filter by itself (to get only the empty rows) works, the update part doesn't seem to be working. I'm used to SQL:", "q_apis": "get columns columns empty any empty any columns columns at columns filter get empty update", "apis": "any values", "code": ["import pandas as pd, numpy as np\n\ndf = pd.DataFrame({'A': [1, np.nan, 2, 3, 4, 5],\n                   'B': [2, 3, np.nan, 5, 1, 9],\n                   'C': [5, 8, 1, 9, np.nan, 7]})\n\n     A    B    C\n0  1.0  2.0  5.0\n1  NaN  3.0  8.0\n2  2.0  NaN  1.0\n3  3.0  5.0  9.0\n4  4.0  1.0  NaN\n5  5.0  9.0  7.0\n", "df['test'] = np.any(np.isnan(df.values), axis=1)\n\n     A    B    C   test\n0  1.0  2.0  5.0  False\n1  NaN  3.0  8.0   True\n2  2.0  NaN  1.0   True\n3  3.0  5.0  9.0  False\n4  4.0  1.0  NaN   True\n5  5.0  9.0  7.0  False\n"], "link": "https://stackoverflow.com/questions/49458219/update-where-equivalent-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pd.DatetimeIndex named \"raw_Ix\" which contains all the indices i am working with and two pandas (Time)series(\"t1\" and \"nextloc_ixS\") (both with the same timeindex). the values in \"nextloc_ixS\" are the same indices of t1.index and nextloc_ixS.index Shifted by one in raw_Ix. to better understand what \"nextloc_ixS\" is: All three get passed into a function, where i need them in the following form: I need to drop the t1-rows where t1.index is not in raw_Ix (to avoid errors, since raw_Ix could have been manipulated) After that I now copy t1 deeply (lets call it t1_copy). because I need the Values of nextloc_ixS as the new DatetimeIndex of t1_copy. (sounds simple, but here i got difficulties) But before I replace the index of i might need to save the old index of t1_copy as a column in t1_copy, for the last step (== step 5). The actual function selects some indices of t1_copy in a specific procedure and returns \"result\", which is a pd.DatetimeIndex that containes some indices of t1_copy with duplicates i need to shift result back by 1, but not via np.searchsorted. (note: \"result\" is still artificially shifted forward, so we can set it backwards by getting the indices location in t1_copy.index and then in the backup column from step 3 getting the \"old\"-indices. I know it sounds a bit complicated, therefore here is the inefficient code which i worked on: So in a nutshell: I try to do an index shift back and later again forth while avoiding np.searchsorted() and instead using the two pd.Series (or better call it columns because they get passed seperately from a DataFrame) Is there any way to do that efficiently in terms of codelines and timeuse? (very large amount of rows)", "q_apis": "get columns DatetimeIndex contains all indices values indices index index get where drop where index now copy DatetimeIndex replace index index last step step indices DatetimeIndex indices shift searchsorted indices index step indices index shift searchsorted Series columns get DataFrame any", "apis": "sample where DataFrame set_index size assign dropna where index isin dropna drop columns assign shift index dropna shift size head to_string", "code": ["import time\nd = [d for d in pd.date_range(dt.datetime(2015,5,1,2), \n                          dt.datetime(2020,5,1,4), freq=\"128s\") \n     if random.randint(0,3) < 2 ] # miss some sample times... \n\n# random manipulation of rawIdx so there are some rows where ts is not in rawIdx\ndf = pd.DataFrame({\"ts\":d, \"rawIdx\":[x if random.randint(0,3)<=2 \n                                     else x + pd.Timedelta(1, unit=\"s\") for x in d], \n                   \"val\":[random.randint(0,50) for x in d]}).set_index(\"ts\")\nstart = time.time()\nprint(f\"size before: {len(df)}\")\ndfc = df.assign(\n    # make it float64 so can have nan, map False to nan so can dropna() rows that are not in rawIdx\n    issue=lambda dfa: np.array(np.where(dfa.index.isin(dfa[\"rawIdx\"]),True, np.nan), dtype=\"float64\"),\n).dropna().drop(columns=\"issue\").assign(\n    # this should be just a straight forward shift.  rawIdx will be same as index due to dropna()\n    nextloc_ixS=df.rawIdx.shift(-1)\n)\n\nprint(f\"size after: {len(dfc)}\\ntime: {time.time()-start:.2f}s\\n\\n{dfc.head().to_string()}\")\n", "size before: 616264\nsize after: 462207\ntime: 0.13s\n\n                                 rawIdx  val         nextloc_ixS\nts                                                              \n2015-05-01 02:02:08 2015-05-01 02:02:08   33 2015-05-01 02:06:24\n2015-05-01 02:06:24 2015-05-01 02:06:24   40 2015-05-01 02:08:33\n2015-05-01 02:10:40 2015-05-01 02:10:40   15 2015-05-01 02:12:48\n2015-05-01 02:12:48 2015-05-01 02:12:48   45 2015-05-01 02:17:04\n2015-05-01 02:17:04 2015-05-01 02:17:04   14 2015-05-01 02:21:21\n\n"], "link": "https://stackoverflow.com/questions/63312570/how-to-shift-index-of-series-by-1-row-in-another-pandas-timeindex"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here is the dataframe. I want the dates here in format. I tried, but, it gives: Am I missing something? Is there any other way to achieve the dates in required format? I even tried: but gives a", "q_apis": "get columns any", "apis": "DataFrame iloc iloc replace iloc iloc apply", "code": ["import pandas as pd\ndf2 = pd.DataFrame([['2017-18','','','','','','','','','','','',''], ['COMPANIES', '01-APR-2017', '01-MAY-2017', '01-JUN-2017', \n                    '01-JULY-2017', '01-AUG-2017', '01-SEP-2017', '01-OCT-2017', '01-NOV-2017', '01-DEC-2017', '01-JAN-2018', '01-FEB-2018', '01-MAR-2018']])\n\n\ndf2.iloc[1, 1:] = df2.iloc[1, 1:].str.replace(\"JULY\", \"JUL\")\ndf2.iloc[1, 1:] = df2.iloc[1, 1:].apply(pd.to_datetime, format = '%d-%b-%Y').dt.strftime('%Y-%m-%d %H:%M:%S')\n\nprint(df2)\n", "          0                    1                    2                    3   \\\n0    2017-18                                                                  \n1  COMPANIES  2017-04-01 00:00:00  2017-05-01 00:00:00  2017-06-01 00:00:00   \n\n                    4                    5                    6   \\\n0                                                                  \n1  2017-07-01 00:00:00  2017-08-01 00:00:00  2017-09-01 00:00:00   \n\n                    7                    8                    9   \\\n0                                                                  \n1  2017-10-01 00:00:00  2017-11-01 00:00:00  2017-12-01 00:00:00   \n\n                    10                   11                   12  \n0                                                                 \n1  2018-01-01 00:00:00  2018-02-01 00:00:00  2018-03-01 00:00:00  \n"], "link": "https://stackoverflow.com/questions/50347089/issue-while-converting-date-to-datetime-format-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I concatenate series objects, with existing column names together to a DataFrame in Pandas. The result looks like this: Now I want to insert another column name A above the column names X, Y, Z, for the whole DataFrame. This should look like this at the end: So far I did not find a solution how to insert a column name A above the existing columns names X, Y, Z for the complete DataFrame. I would be grateful for any help. :)", "q_apis": "get columns names DataFrame insert name names DataFrame at insert name columns names DataFrame any", "apis": "columns columns", "code": ["df = pd.concat([x, y, z], axis=1)\ndf.columns = pd.MultiIndex.from_product([['A'], df.columns])\n", "A            \nX     Y     Z\ndata  data  data\n"], "link": "https://stackoverflow.com/questions/64511457/add-a-column-name-to-a-panda-dataframe-multi-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "If I have the following list of lists: Is there a way to create a list between the second and third one (preferably at any position) that has the same length as the longest list in this list of lists? For example, in my case create a list between the second and third/last list that has the same length as the last one (since this is the longest list with length 7): I'm using this data in a dataframe with pandas. Maybe pandas can help me accomplish my goal?", "q_apis": "get columns between second at any length between second last length last length", "apis": "max copy insert", "code": ["data = [[1,2,3], [1,2,3,4,5], [1,2,3,4,5,6,7]]\n\nlongest_sublist = max(data, key=len)\nnew_list = longest_sublist.copy()\n\ndesired_position = 2\ndata.insert(desired_position, new_list)\n", "[[1, 2, 3], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7]]\n"], "link": "https://stackoverflow.com/questions/52684290/create-a-new-list-in-a-list-of-lists-that-has-the-same-length-as-the-longest-one"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a (shortened for this question). The column names are weird because in my actual code, this result is derived from a with the following grouping operation. I want to rename the columns of , the desired result looks like this: I cannot simply use or the answers from this similar question because after using the columns are in arbitrary order. (Applying also seems tricky because of the duplicate name .) Currently, I am working around the problem by using an for , thus having a deterministic column oder. But assuming the creation of cannot be fixed upstream, how would I arrive at my desired result elegantly?", "q_apis": "get columns names rename columns columns name at", "apis": "columns columns columns columns columns join columns sum count sum items columns get columns", "code": ["overview.columns = [f'{i}_{j}' for i, j in overview.columns]\n", "overview.columns = ['{i}_{j}'.format(i, j) for i, j in overview.columns]\n\noverview.columns = list(map('_'.join, overview.columns))\n", "d = {('total', 'sum'): 'gross', ('total', 'count'): 'checkouts',\n     ('quantity', 'sum'): 'items'}\n\noverview.columns = np.vectorize(d.get)(overview.columns)\n"], "link": "https://stackoverflow.com/questions/51816479/renaming-multiindex-columns-with-arbitrary-order-and-duplicate-column-names"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe, I want to have different color for different value of block (2 and 4. second column) My code for this plot is How can I have different color for each variables ?", "q_apis": "get columns value second plot", "apis": "groupby plot nunique groupby plot", "code": ["import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,4))\n\nfor idx, gp in df.groupby('block'):\n    gp.plot(x='array_size', y='time', ax=ax, label=idx)\nplt.show()\n", "df = pd.DataFrame({'x': np.tile(np.arange(1,11,1),20),\n                   'y': np.random.randint(1,25,200),\n                   'block': np.repeat(np.arange(1,21,1),10)})\n", "fig, ax= plt.subplots(figsize=(8,4))\n\ncolors = sns.color_palette(\"coolwarm\", df.block.nunique())\nax.set_prop_cycle('color', colors)\n\nfor idx, gp in df.groupby('block'):\n    gp.plot(x='x', y='y', ax=ax, legend=False)\nplt.show()\n"], "link": "https://stackoverflow.com/questions/52287321/pandas-how-to-have-different-color-line-graph"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes of a format similar to below: df1: df2: I want to merge finding matches based on fname and lname and then update the note column in the first DataFrame with the note column in the second DataFrame The result I am trying to achieve would be like this: This is the code I was working with so far: but it just creates a new column with _y appended to it. How can I get it to just update that column? Any help would be greatly appreciate, thanks!", "q_apis": "get columns merge update first DataFrame second DataFrame get update", "apis": "merge loc isna drop", "code": ["df_3 = pd.merge(df1, df2, on=['fname', 'lname'], how='outer') \n\ndf_3['note'] = df_3['note_x']\ndf_3.loc[df_3['note'].isna(), 'note'] = df_3['note_y']\ndf_3 = df_3.drop(['note_x', 'note_y'], axis=1)\n"], "link": "https://stackoverflow.com/questions/67115972/python-pandas-merge-different-sized-dataframes-and-replace-value-in-row-with-m"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to plot three dataframe based on their Total Activity Hours Per Month so I used the groupby function. Download link for the dataset https://drive.google.com/file/d/1YdSsP8BM4PVNh8m2kW7244NOo75lNUT7/view?usp=sharing Here is some sample data from the dataframe: Please take a look at the attached picture. What I really wanted is actually a stacked barchart like this:", "q_apis": "get columns plot groupby view sample take at", "apis": "loc isin groupby sum reset_index", "code": ["import seaborn as sns\n\ndf['hue'] = df[\"Attendance Abs Type Code\"]\ndf.loc[~df['hue'].isin(['110', '120']), 'hue'] = 'Other'\ndf = df.groupby(['Activity Month', 'hue'])['Activity Hours'].sum().reset_index()\n\nsns.barplot(x='Activity Month', y='Activity Hours', hue='hue', data=df)\n"], "link": "https://stackoverflow.com/questions/58479198/plot-multiple-pandas-dataframes-groupby-plot"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe as below : I need to derive a new column with the below logic: For a particular : If for a particular , both and is then , if any one is , then , else . For a particular : If a is repeated more than 2 times then , if 2 times then , else . For a particular : If a belongs to which is the Top Category the , else Eg : , ( as its frequency count is 4 out 5 ), all in category for this user will get Score of . Finally for we need the cumulative score for each - . Eg : , . 1) Liked / Shared = Any one is - 2) of that belongs to Top Category (Music) for that User_ID so 3) is repeated more than twice for this - Cumulative Score : Below is the expected output : NOTE: In the expected output, only User_ID, Game_ID and Rating is vital. Remaining columns are just for details. Can you friends help me ?", "q_apis": "get columns any count all get columns", "apis": "astype astype groupby transform size sub clip groupby apply value_counts head index value_counts head values astype groupby max reset_index", "code": ["df['Cond1'] = (df['Liked'] == 'Y').astype(int) + (df['Shared'] == 'Y').astype(int)\n\ndf['Cond2'] = df.groupby(['User_ID','Game_ID'])['Game_ID'].transform('size').sub(1).clip(0,2)\n\ndf['Cond3'] = df.groupby('User_ID')['Category'].apply(lambda x: ((x.value_counts().head(1).index[0] == x) & (x.value_counts().head(1).values[0] > 1).astype(int)))\n\ndf['Score'] = df['Cond1'] + df['Cond2'] + df['Cond3']\n\ndf_out = df.groupby(['User_ID','Game_ID'])['Score'].max().reset_index()\n\ndf_out\n", "   User_ID  Game_ID  Score\n0        1       11      4\n1        1       22      3\n2        1       33      0\n3        2       11      0\n4        2       33      1\n5        2       44      2\n"], "link": "https://stackoverflow.com/questions/55501676/how-to-derive-a-score-ranking-based-on-multiple-columns-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a python dataframe with close to 1M rows. There is a string column with some numbers in it like I need to extract from this column and save it as a new column. I can iterate over each cell and do a string transform but that consumes a lot of time for a large dataset. Any ideas are appreciated.", "q_apis": "get columns transform time", "apis": "apply", "code": ["import re\n\ndef func(x):\n    result = re.findall(r\"\\d+(?=FT)\",x)\n    if not result:\n        try:\n            return int(x[:2])\n        except:\n            return None\n    return result[0]\n\ndf[\"num_col\"] = df[\"String_Col\"].apply(func)\n"], "link": "https://stackoverflow.com/questions/59763120/how-to-extract-numbers-from-a-complex-string-in-a-large-python-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a numpy array, and for simplicity sake it is empty. The dimensions are 8x12 . I have my row and column headers defined. It looks like this: I want to know if I can put the column at the bottom instead of the top?", "q_apis": "get columns array empty put at", "apis": "keys columns values values columns values keys values append any append", "code": ["    import numpy as np\n    import pandas as pd\n\n    column = [_ for _ in 'ABCDEFGH']\n    row = range(12, 0, -1)\n    # board = np.full((12, 8), 7) # to test dtype changes\n    board = np.full((12, 8), '||')\n    df0 = pd.DataFrame(board, index=row, \n    columns=column).sort_index(ascending=False)\n", "    footer = {'A':'A', 'B':'B', 'C':'C', 'D':'D', 'E':'E', 'F':'F', 'G':'G', 'H':'H'}\n", "    keys = list(df0.columns.values)\n    values = list(df0.columns.values)\n    footer = dict(zip(keys, values))\n", "    df2 = df0.append(pd.Series((pd.Series(footer)), name='Footer'))  # Change 'Footer' to any text\n", "    df3 = df0.append(pd.Series((pd.Series(footer)), name=''))  # Same as df2 but blank name\n", "    df4 = df3\n    df4.columns = [''] * len(df4.columns)\n\n    print(df4)  # OP's solution\n"], "link": "https://stackoverflow.com/questions/54565981/is-it-possible-to-put-the-pandas-dataframe-column-header-as-a-footer-instead"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: I want the output to be: I can do it looping over the original dataframe and adding data to a new dataframe but I have the impression there must be a simpler approach. I have tried pivot_table, grouping,query and dictionaries but with no results. Any ideas? Thanks in advance.", "q_apis": "get columns pivot_table query", "apis": "DataFrame", "code": ["df = pd.DataFrame({'Owner':['John Doe','Woody Woodpecker','Bart Simpson','Ringo Star','Woody Woodpecker',\n                           'John Doe','Donald Duck','Woody Woodpecker','John Doe','Bart Simpson'],\n                   'Lot':['A%02d' % n for n in np.arange(1,11)]})\ndf['Owner'] = pd.Categorical(df['Owner'],categories=pd.unique(df['Owner']))\n", "df['N'] = df.groupby('Owner').cumcount()+1\n\n    Owner              Lot  N\n0   John Doe           A01  1\n1   Woody Woodpecker   A02  1\n2   Bart Simpson       A03  1\n3   Ringo Star         A04  1\n4   Woody Woodpecker   A05  2\n5   John Doe           A06  2\n6   Donald Duck        A07  1\n7   Woody Woodpecker   A08  3\n8   John Doe           A09  3\n9   Bart Simpson       A10  2\n", "df['N'] = ['Lot'+str(i) for i in df['N']]\ndf.pivot(index='Owner',columns='N',values='Lot').reset_index()\n\n\nN   Owner               Lot1 Lot2   Lot3\n0   John Doe            A01  A06    A09\n1   Woody Woodpecker    A02  A05    A08\n2   Bart Simpson        A03  A10    NaN\n3   Ringo Star          A04  NaN    NaN\n4   Donald Duck         A07  NaN    NaN\n"], "link": "https://stackoverflow.com/questions/62602512/extracting-values-from-dataframe-columns-and-inserting-them-into-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am a beginner (stepped into programming a few days ago during covid lockdown lol) and self-taught, i'm trying to improve myself every day but... i'm stuck on this problem! (it's a project i'm doing for hobby) Is there a way to tell the program to read the lines and change the equal values \u200b\u200bto another random value? let me explain! that's my result: each row should have only one \"7\", one \"14\" and one \"14R\". each column cannot have \"7\" or \"8\" or \"9\" after \"14\" or \"14R\" thanks in advance for any reply! (and sorry for my bad english)", "q_apis": "get columns days day values value any", "apis": "apply duplicated apply isin shift isin", "code": ["row_condition = df.apply(pd.Series.duplicated, axis=1)\n\ncol_condition = df.apply(lambda x: np.logical_and(x.isin(['14','14R']).shift(1, fill_value=False), x.isin(['7','8', '9'])))\n\nto_replace = np.logical_or(row_condition, col_condition)\n"], "link": "https://stackoverflow.com/questions/64631038/how-to-change-the-values-of-a-rows-if-the-values-are-the-same"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The training data looks like below : The first column is the label about whether this mushroom is edible.(e:edible, p:poisonous) And I want to split this data into two part by edible or not. My code is below : The problem is, when I delete in line 2 to line 6, this code works. However, when I do , the terminal return error message. Same method with another column is fine. For example, is running correctly. I have no idea about this.", "q_apis": "get columns first delete", "apis": "shape size shape", "code": ["mushdf = pd.read_csv('agaricus-lepiota.data', header=None, names=[\n                'edible?','cap-shape','cap-surface','cap-color','bruises?','odor',\n                'gill-attachment','gill-spacing','gill-size','gill-color',\n                'stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring',\n                'stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color',\n                'ring-number','ring-type','spore-print-color','population','habitat'])\n"], "link": "https://stackoverflow.com/questions/58476374/error-about-using-dict-to-split-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the below JSON string in . I want it to look like the Expected Result Below Expected Result: Category_matchType Category_expression Action_matchType Action_expression Label_matchType Label_expression 0 EXACT ABC EXACT DEF REGEXP GHI|JKL What I've Tried: This question is similar, but I'm not using the index the way the OP is. Following this example, I've tried using and then using various forms of , , , , etc. But there has to be an easier way! type matchType expression 0 CATEGORY EXACT ABC 1 ACTION EXACT DEF 2 LABEL REGEXP GHI|JKL", "q_apis": "get columns index", "apis": "set_index groupby stack unstack columns columns join", "code": ["df = pd.json_normalize(data, 'eventConditions')\ndf = df.set_index([df.groupby('type').cumcount(), 'type']).stack().unstack([1, 2])\ndf.columns = df.columns.map('_'.join)\n", "  CATEGORY_matchType CATEGORY_expression ACTION_matchType ACTION_expression LABEL_matchType LABEL_expression\n0              EXACT                 ABC            EXACT               DEF          REGEXP          GHI|JKL\n"], "link": "https://stackoverflow.com/questions/67300838/flatten-and-shape-json-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame Description as mentioned below I am trying to do a keyword search on the description column and I have list of keywords as a list . My current code checks only exact matches not partial matches.If there are multiple keywords present in the row these will be separated by a delimiter and populated new column. My code How can this be done?", "q_apis": "get columns", "apis": "join groupby agg join", "code": ["...\nkeywords_lower = [item.lower() for item in keywords]\npattern = '(' + '|'.join('(?:' + i + ')' for i in keywords_lower) + ')'\ndf['Keyword'] = df['Description'].str.extractall(pattern, re.I).groupby(level=0).agg('/'.join)\n", "                       Description           Keyword\n0  Government entertainment people  Govern/entertain\n1                  Dinner with CFO            Dinner\n2  Commission to Agents government      Agent/govern\n"], "link": "https://stackoverflow.com/questions/60602630/partial-keyword-match-not-working-when-i-am-trying-to-create-a-new-column-from-a"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "`` a_dict = {'Bristol': '25005','Plymouth': '25023','Worcester': '25027','Hillsborough' :'33011', 'Rockingham':'33015'} n_dict = {'Br': dataBristol,'Pl' : dataPlymouth,'W': dataWorcester,'Hillsborough' :'H', 'Rockingham':'R'}", "q_apis": "get columns", "apis": "rename columns pop items", "code": ["d = ({key: data[data[\"fips\"] == str(value)].rename(columns={'cases':\"case\"+str(key), \n                                                           'deaths': \"deaths\"+str(key), \n                                                           'population': 'pop'+str(key)}) \n                 for key, value in a_dict.items()})\n"], "link": "https://stackoverflow.com/questions/63700439/iterating-through-a-dictionary-of-pandas-dataframes-to-rename-columns-using-the"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with the column 'Department' with 6 different variables. However, when I have created a new df, using: The df contains two entries for 'R&D Operations': This must be some type of format difference from my original df. How can I combine these two entries from the 'Department' Column. Many thanks", "q_apis": "get columns contains difference combine", "apis": "groupby groupby count", "code": ["# To remove the white spaces from both ends (left and right)\n\ndept.Department = dept.Department.str.strip() \n\n# Then perform the groupby operation\n\nindicator = dept.groupby('Department')[['Assignment Status']].count()\n\n"], "link": "https://stackoverflow.com/questions/64942858/pandas-combine-concatenate-two-values-from-same-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There's probably an easy way to do this, but I hit a wall on this one. I have a dataframe with text as the row data. I'm trying to add new columns to the dataframe based on existing column names. What I'd like it to do is the below. If it matters, a string can only appear once per row, and can also not show up. I found a method using argsort, but that doesn't help with strings. Thanks very much.", "q_apis": "get columns add columns names argsort", "apis": "stack reset_index var pivot var rename_axis index columns", "code": ["s = df.stack().reset_index(name='var')\ns.pivot('level_0', 'var', 'level_1').rename_axis(index=None, columns=None)\n", "    level_0 level_1       var\n0         0    Col1       Boy\n1         0    Col2      Girl\n2         0    Col3  Baseball\n3         0    Col4    Lizard\n4         1    Col1       Boy\n5         1    Col2      Girl\n6         1    Col3  Baseball\n7         1    Col4    Hockey\n8         2    Col1       Boy\n9         2    Col2      Girl\n10        2    Col3  Baseball\n11        2    Col4    Lizard\n12        3    Col1       Boy\n13        3    Col2  Baseball\n14        3    Col3    Lizard\n15        3    Col4      Girl\n16        4    Col1       Boy\n17        4    Col2      Girl\n18        4    Col3  Baseball\n19        4    Col4    Hockey\n", "  Baseball   Boy  Girl Hockey Lizard\n0     Col3  Col1  Col2    NaN   Col4\n1     Col3  Col1  Col2   Col4    NaN\n2     Col3  Col1  Col2    NaN   Col4\n3     Col2  Col1  Col4    NaN   Col3\n4     Col3  Col1  Col2   Col4    NaN\n"], "link": "https://stackoverflow.com/questions/65764024/find-value-in-pandas-df-row-and-return-the-column-name"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe (df) with the column structure : this dataframe has data for say Jan, Feb, Mar, Apr. A,B,C,D are numeric columns. For the month of Feb , I want to recalculate column A and update it in the dataframe i.e. for month = Feb, A = B + C + D Code I used : This ran without errors but did not change the values in column A for the month Feb. In the console, it gave a message that : A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead I tried to use .loc but right now the dataframe I am working on, I had used on it and I am not sure how to set index and use .loc. I followed documentation but not clear. Could you please help me out here? This is an example dataframe : I want to update say one date : 2000-01-03. I am unable to give the snippet of my data as it is real time data.", "q_apis": "get columns columns month update month values month value copy DataFrame loc value loc right now index loc update date time", "apis": "loc loc loc loc", "code": ["df.loc[df['month'] == 'Feb', 'A'] = df.loc[df['month'] == 'Feb', 'B'] + df.loc[df['month'] == 'Feb', 'C'] + df.loc[df['month'] == 'Feb', 'D'] \n"], "link": "https://stackoverflow.com/questions/34499584/use-of-loc-to-update-a-dataframe-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The situation: I have a pandas dataframe where I have some data about the production of a product. The product is produced in 3 phases. The phases are not fixed meaning that their cycles (the time till last) is changing. During the production phases, at each cycle the temperature of the product is measured. Please see the table below: The problem: I need to calculate the slope for each cycle of each phase for each product. I also need to add it to the dataframe in a new column called \"Slope\". The one you can see, highlighted in yellow was added by me manually in an excel file. The real dataset contains hundreds of parameters (not only temperatures) so in reality I need to calculate the slope for many, many columns, therefore I tried to define a function. My solution is not working at all: This is the code I tried, but it does not work. I am trying to catch the first and last row for the given product, for the given phase. And then get the temperature data and the difference of these two rows. And this way I could calculate the slope. This is all I could come up with so far (I created another column called: \"Max_cylce_no\", this stores the maximum amount of the cycle for each phase): And the way I would like to apply it: Unfortunatelly I get a NameError right away saying that: name 'row' is not defined. Could you please help me and show me the right direction on how to solve this problem. It gives me a really hard time. :( Thank you in advance!", "q_apis": "get columns where product product time last at product product add contains columns at all first last product get difference all apply get right name right time", "apis": "iloc iloc groupby transform", "code": ["f = lambda x: (x.iloc[-1] - x.iloc[0]) / len(x)\ndf['new'] = df.groupby(['Product_no','Phase_no'])['Temperature'].transform(f)\n"], "link": "https://stackoverflow.com/questions/53242570/pandas-dataframe-find-first-and-last-element-given-condition-and-calculate-slope"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to read yearly data into a Pandas dataframe from a CSV file, but it's not reading the years correctly. I think the problem is that I have to transpose the rows and columns. Here's a simplified example that shows the problem. That generates this pair of plots: I want to plot the price of each fruit over the years, but the data I'm reading has a row for each fruit, and a column for each year. The first plot shows what happens when I plot the data I wish I had. The second plot shows what happens when I plot the data I have after doing the transpose. Why are the years not shown on the second plot's x axis? The data is evenly spaced, so is it not even reading the year data?", "q_apis": "get columns transpose columns plot year first plot plot second plot plot transpose second plot year", "apis": "columns columns astype T", "code": ["df2 = pd.read_csv(csv_source2, index_col=0)\ndf2.columns = df2.columns.astype(int)\ndf2 = df2.T\n", "from io import StringIO\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sn\n\n# This is what I wish I had.\ncsv_source1 = StringIO(\"\"\"\\\nyear,Apples,Bananas,Cherries\n1990,1,2,3\n1997,1,4,9\n1999,1,8,27\n\"\"\")\ndf1 = pd.read_csv(csv_source1, index_col=0)\ndf1.index.names = ['Year']\ndf1.columns.names = ['fruit']\n\n# This is what I actually have.\ncsv_source2 = StringIO(\"\"\"\\\nfruit,1990,1997,1999\nApples,1,1,1\nBananas,2,4,8\nCherries,3,9,27\n\"\"\")\n# So I convert the years to integers and transpose it.\ndf2 = pd.read_csv(csv_source2, index_col=0)\ndf2.columns = df2.columns.astype(int)\ndf2 = df2.T\ndf2.index.names = ['Year']\n\nsn.set()\nax = plt.subplot(211)\ndf1.plot(ax=ax)\n\nax = plt.subplot(212)\ndf2.plot(ax=ax)\n\nplt.tight_layout()\nplt.show()\n"], "link": "https://stackoverflow.com/questions/50993804/pandas-dataframe-cant-plot-x-values-from-csv"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe called bank and \u0131 would like to create a new column explaining percentage change but when I try \u0131 encounter with this error:unsupported operand type(s) for -: 'str' and 'str' How can I solve this problem or is there other ways for me to create this column?", "q_apis": "get columns", "apis": "apply replace astype", "code": ["# Replace commas with points\nbank = bank.apply(lambda x: x.str.replace(',', '.'))\n\n# Convert to float\nbank = bank.astype('float64')\n"], "link": "https://stackoverflow.com/questions/67256847/pandas-creating-a-new-column-based-on-other-other-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames, one named 'foo' and one named 'bar'. My dataframe 'foo' has some unique columns and my dataframe 'bar' also has some unique columns. However, they both share one column, the column 'google'. I am trying to see if there is a way to keep all columns in data frame 1, 'foo', and add one additional column, the column 'CLRS', which will be 1 if the content in the column 'google' in that row of 'foo' appears somewhere in the column 'google' in 'bar'. More specifically, let's assume that the dataframes I have are structured like this: 'foo' contains columns: 'foo_1','foo_2', ..., 'google' and bar contains columns: 'bar_1', 'bar_2, ..., 'google'. I want to join/merge 'foo' and 'bar' in a way such that 'foo' has an additional column, 'CLRS', such that 'CLRS' has a 1 if the contents of 'google' in that row of 'foo' appear at some point in the 'google' column of 'bar'. I have tried the following code: Unfortunately, after running the previous join code, foo_merged contains all columns of foo and one additional column which contains always the contents of the column 'google' from 'bar'. My desired result instead would be a df such that the additional column 'CLRS' contains 1 if the content of 'google' in that row of 'foo' appears somewhere as a content of the column 'google' in 'bar' and 0 otherwise.", "q_apis": "get columns unique columns unique columns all columns add contains columns contains columns join merge at join contains all columns contains contains", "apis": "merge astype where", "code": ["df = pd.merge(foo, bar, how='left', on = 'google', indicator = True)\ndf['CLRS'] = (df['_merge'] == 'both').astype(int)    \n#or df['CLRS'] = np.where(df['_merge'] == 'both', 1, 0)\n"], "link": "https://stackoverflow.com/questions/56762008/merge-join-pandas-command-to-mark-all-shared-instances-of-a-column-in-df-with-th"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe: I want to: Groupby (file_name & iteration) Filter the data Run some fucntions on the resulting grp2 dataframe Return a final dataframe with a summary of the results. Below is a working example code. However it is hard to read and gets long very fast as soon as you add more groupby operations (and lists where i store values). My real code uses 3 grouby, and the functions all take a combination of columns and output a single value. Out: Is there a more concise way of creating a dataframe that is composed of the result from funcitons applied to a groupby from a previous dataframe?", "q_apis": "get columns add groupby where values all take columns value groupby", "apis": "get size groupby size transform size filter last size filter groupby apply rename apply rename reset_index", "code": ["# get running number and group size\ndfgp = df.groupby([\"file_name\", \"iteration\"])\ndf[\"cumcount\"] = dfgp.cumcount()\ndf[\"size\"] = dfgp[\"cumcount\"].transform(\"size\")\n\n# (a) filter out the last 50% per group\ndf = df[df[\"cumcount\"] < df[\"size\"] // 2]\n\n# (b) filter out everyting smaller than 0.5\ndf = df[df[\"x\"] >= 0.5]\n\n# (c) keep only every 2 valuees\ndf = df[df[\"cumcount\"] % 2 == 0]\n", "dfgp = df.groupby([\"file_name\", \"iteration\"])  # regroup on the new df\ndf_ans = pd.concat([\n    dfgp[[\"x\", \"y\"]].apply(lambda df_: fun_a(df_[\"x\"], df_[\"y\"])).rename(\"fun_a_res\"),\n    dfgp[[\"x\", \"z\"]].apply(lambda df_: fun_b(df_[\"x\"], df_[\"z\"])).rename(\"fun_b_res\")\n], axis=1).reset_index()\n", "print(df_ans)\n  file_name iteration  fun_a_res  fun_b_res\n0    File_1         A   0.285714  -1.857143\n1    File_1         B   2.571428  -2.714286\n2    File_2         C   0.285714   0.142857\n3    File_2         D   2.571428  -1.714286\n", "  file_name iteration  fun_a_res  fun_b_res\n0    File_1         A   1.428572  -4.285714\n1    File_1         B   8.285714  -2.928571\n2    File_2         C   1.428572  -6.785714\n3    File_2         D   6.285714  -2.928571\n", "  file_name iteration  fun_a_res  fun_b_res\n0    File_1         A   0.571429  -3.714285\n1    File_1         B   2.857143  -3.071429\n2    File_2         C   0.571429  -4.214285\n3    File_2         D   2.857143  -3.071429\n"], "link": "https://stackoverflow.com/questions/65021074/python-groupby-multiple-pandas-col-perform-operation-and-output-new-summary-d"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Say I have a pandas dataframe like Here Gender has 3 levels, M,F,missing, Color has 2 levels, red,blue, Fruit has 4 levels apple, orange, banana, missing, numerics doesn't matter So total 9 levels in entire dataframe, I wish to subset One row for each Level, For example My resultant dataframe will be like So I got One row for each category, M,F,missing_Gender,red,blue,apple,orange,banana,missing_Fruit My Dropping Criteria is All categories in the 3 rows that is dropped were available for us in result_data", "q_apis": "get columns levels levels levels levels categories", "apis": "DataFrame columns drop_duplicates first drop_duplicates last empty drop_duplicates", "code": ["data=[['M', 14, 'blue','apple'],\n      ['M', 20, 'red','apple'], \n      ['F', 14, 'blue','apple'], \n      ['F', 14, 'red','apple'],\n      ['F', 20, 'blue','apple']]\ndf = pd.DataFrame(data, columns = ['Gender', 'Age', 'Color','Fruit']) \n\n# The code below will result in \n# 'M', 14, 'blue','apple'\n# 'F', 14, 'blue','apple' \ndf.drop_duplicates(subset=['Gender','Fruit'], keep='first')\n\n# The code below will result in \n# 'M', 20, 'red','apple'\n# 'F', 20, 'blue','apple'\ndf.drop_duplicates(subset=['Gender','Fruit'], keep='last')\n\n# The code below will result in empty, since no duplicates\n# are kept\ndf.drop_duplicates(subset=['Gender','Fruit'], keep=False)\n"], "link": "https://stackoverflow.com/questions/56896901/how-to-subset-pandas-dataframe-with-entire-rows-of-each-unique-values-of-each-co"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to group by , then filter out rows which are in between the rows where column is and column is (I've marked them in the below). Note that the letters are case-sensitive(i.e., has to be and not ). looks like this: Expected output:", "q_apis": "get columns filter between where", "apis": "DataFrame columns loc loc cumsum", "code": ["df = pd.DataFrame(x, columns=cols)\ndf['ind'] = 0\ndf.loc[df['A'] == 'STATION', 'ind'] = 1\ndf.loc[df['A'] == 'END', 'ind'] = -1\ndf['ind'] = df['ind'].cumsum()\ndf[(df['ind']==1) & (df['A'] != 'STATION')]\n"], "link": "https://stackoverflow.com/questions/68191341/groupby-filter-rows-between-specific-strings-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes and . contains the information of the age of people, while contains the information of the sex of people. Not all the people are in nor in I want to have the information of the sex of the people in and setting if I do not have this information in . I tried to do but I keep the information of some people in that I don't want.", "q_apis": "get columns contains contains all", "apis": "DataFrame copy DataFrame", "code": ["df1 = pd.DataFrame({'Name': ['Tom', 'Sara', 'Eva', 'Jack', 'Laura'], \n                    'Age': [34, 18, 44, 27, 30]})\n\n#print (df1)\ndf3 = df1.copy()\n\ndf2 = pd.DataFrame({'Name': ['Tom', 'Paul', 'Eva', 'Jack', 'Michelle'], \n                    'Sex': ['M', 'M', 'F', 'M', 'F']})\n#print (df2)\n", "df1['Sex'] = df1['Name'].map(df2.set_index('Name')['Sex'])\nprint (df1)\n    Name  Age  Sex\n0    Tom   34    M\n1   Sara   18  NaN\n2    Eva   44    F\n3   Jack   27    M\n4  Laura   30  NaN\n", "df = df3.merge(df2[['Name','Sex']], on='Name', how='left')\nprint (df)\n    Name  Age  Sex\n0    Tom   34    M\n1   Sara   18  NaN\n2    Eva   44    F\n3   Jack   27    M\n4  Laura   30  NaN\n", "df1 = pd.DataFrame({'Name': ['Tom', 'Sara', 'Eva', 'Jack', 'Laura'], \n                    'Year':[2000,2003,2003,2004,2007],\n                    'Code':[1,2,3,4,4],\n                    'Age': [34, 18, 44, 27, 30]})\n\nprint (df1)\n    Name  Year  Code  Age\n0    Tom  2000     1   34\n1   Sara  2003     2   18\n2    Eva  2003     3   44\n3   Jack  2004     4   27\n4  Laura  2007     4   30\n\ndf2 = pd.DataFrame({'Name': ['Tom', 'Paul', 'Eva', 'Jack', 'Michelle'], \n                    'Sex': ['M', 'M', 'F', 'M', 'F'],\n                    'Year':[2001,2003,2003,2004,2007],\n                    'Code':[1,2,3,5,3],\n                    'Val':[21,34,23,44,67]})\nprint (df2)\n       Name Sex  Year  Code  Val\n0       Tom   M  2001     1   21\n1      Paul   M  2003     2   34\n2       Eva   F  2003     3   23\n3      Jack   M  2004     5   44\n4  Michelle   F  2007     3   67\n", "#merge by all columns\ndf = df1.merge(df2, on=['Year','Code'], how='left')\nprint (df)\n  Name_x  Year  Code  Age Name_y  Sex   Val\n0    Tom  2000     1   34    NaN  NaN   NaN\n1   Sara  2003     2   18   Paul    M  34.0\n2    Eva  2003     3   44    Eva    F  23.0\n3   Jack  2004     4   27    NaN  NaN   NaN\n4  Laura  2007     4   30    NaN  NaN   NaN\n\n#specified columns - columns for join (Year, Code) need always + appended columns (Val)\ndf = df1.merge(df2[['Year','Code', 'Val']], on=['Year','Code'], how='left')\nprint (df)\n    Name  Year  Code  Age   Val\n0    Tom  2000     1   34   NaN\n1   Sara  2003     2   18  34.0\n2    Eva  2003     3   44  23.0\n3   Jack  2004     4   27   NaN\n4  Laura  2007     4   30   NaN\n", "df1 = pd.DataFrame({'Name': ['Tom', 'Sara', 'Eva', 'Jack', 'Laura'], \n                    'Age': [34, 18, 44, 27, 30]})\n\nprint (df1)\n    Name  Age\n0    Tom   34\n1   Sara   18\n2    Eva   44\n3   Jack   27\n4  Laura   30\n\ndf3, df4 = df1.copy(), df1.copy()\n\ndf2 = pd.DataFrame({'Name': ['Tom', 'Tom', 'Eva', 'Jack', 'Michelle'], \n                    'Val': [1,2,3,4,5]})\nprint (df2)\n       Name  Val\n0       Tom    1 <-duplicated name Tom\n1       Tom    2 <-duplicated name Tom\n2       Eva    3\n3      Jack    4\n4  Michelle    5\n\ns = df2.set_index('Name')['Val']\ndf1['New'] = df1['Name'].map(s)\nprint (df1)\n", "#default keep first value\ns = df2.drop_duplicates('Name').set_index('Name')['Val']\nprint (s)\nName\nTom         1\nEva         3\nJack        4\nMichelle    5\nName: Val, dtype: int64\n\ndf1['New'] = df1['Name'].map(s)\nprint (df1)\n    Name  Age  New\n0    Tom   34  1.0\n1   Sara   18  NaN\n2    Eva   44  3.0\n3   Jack   27  4.0\n4  Laura   30  NaN\n", "#add parameter for keep last value \ns = df2.drop_duplicates('Name', keep='last').set_index('Name')['Val']\nprint (s)\nName\nTom         2\nEva         3\nJack        4\nMichelle    5\nName: Val, dtype: int64\n\ndf3['New'] = df3['Name'].map(s)\nprint (df3)\n    Name  Age  New\n0    Tom   34  2.0\n1   Sara   18  NaN\n2    Eva   44  3.0\n3   Jack   27  4.0\n4  Laura   30  NaN\n", "#map by dictionary\nd = dict(zip(df2['Name'], df2['Val']))\nprint (d)\n{'Tom': 2, 'Eva': 3, 'Jack': 4, 'Michelle': 5}\n\ndf4['New'] = df4['Name'].map(d)\nprint (df4)\n    Name  Age  New\n0    Tom   34  2.0\n1   Sara   18  NaN\n2    Eva   44  3.0\n3   Jack   27  4.0\n4  Laura   30  NaN\n"], "link": "https://stackoverflow.com/questions/53010406/pandas-how-to-merge-two-dataframes-on-a-column-by-keeping-the-information-of-th"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What I would like to do is to parse an expression such this one: Where A and B are columns of a dataframe. So I would have to parse the expresion like this in order to get the result: Where is the dataframe. I have tried with but it would be good only to replace the column variables (not the functions) like this: So, my questions are: Is there a python library to do this? If not, how can I achieve this in a simple way? Creating a recursive function could be the solution? If I use the \"reverse polish notation\" could simplify the parsing? Would I have to use the module?", "q_apis": "get columns parse columns parse get replace", "apis": "DataFrame DataFrame eq eval eq eq eval eq", "code": ["import pandas as pd\n# create an example DataFrame to work with\ndf = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n# define equation\neq = 'A + 3 / B'\n# actual computation\ndf.eval(eq)\n\n# more complicated equation\neq = \"A + B + sqrt(B + 4)\"\ndf.eval(eq)\n"], "link": "https://stackoverflow.com/questions/47135742/how-to-parse-and-evaluate-a-math-expression-with-pandas-dataframe-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In have a dataframe from AMAZON DATASET The dataset has a 'helpful' column which looks like this: 'helpful' :[0, 0] where the first element is a 'yes' vote and the second is a 'total' vote. I\u00b4d like to Split this columns into two columns USING PANDAS (PYTHON). The first column must contains only the first element. The last with the second element", "q_apis": "get columns where first second columns columns first contains first last second", "apis": "eval values first DataFrame columns first first replace replace values replace replace values", "code": ["  helpful  first  second\n0  [0, 0]      0       0\n1  [1, 3]      1       3\n2  [0, 0]      0       0\n", "df['helpful'] = [eval(h) for h in df['helpful'].values]\ndf[['first','second']]=pd.DataFrame(df['helpful'].tolist(),columns=['first','second'])\n", "df['first'] = [str(h).replace('[','').replace(']','').split(',')[0] for h in df['helpful'].values]\ndf['second'] = [str(h).replace('[','').replace(']','').split(',')[1] for h in df['helpful'].values]\n"], "link": "https://stackoverflow.com/questions/57642082/python-dataframe-split-one-column-of-numbers-0-0-dataframe-into-two-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm new with Pandas, looking for a method to add missing value in the dataframe. any idea how to produce from the input to output? Thanks", "q_apis": "get columns add value any", "apis": "set_index reindex min max fillna index reset_index", "code": ["df = df.set_index('Module') \\\n    .reindex(np.arange(df[\"Module\"].min(), df[\"Module\"].max() + 1)) \\\n    .fillna(pd.Series(['N', 'www.cat.com'], index=['Status', 'Domain'])) \\\n    .reset_index()\nprint(df)\n", "   Module Status       Domain\n0       1      N  www.cat.com\n1       2      N  www.cat.com\n2       3      N  www.cat.com\n3       4      N  www.cat.com\n4       5      Y  www.cat.com\n5       6      N  www.cat.com\n6       7      Y  www.cat.com\n7       8      Y  www.cat.com\n8       9      N  www.cat.com\n"], "link": "https://stackoverflow.com/questions/67495603/pandas-add-missing-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: And I wish to index the dataframe by column and row values, using the following indexing: And modify the values at these positions. So essentially all cases in the dataframe where the index is more than 5 and the value is between -1 and 1. This can be done in the following manner: with no issues. However, I now want to do this on a dataframe that contains another 3 columns called . See the following: And using the exact same mask I used for the previous values, modify all of the columns in the dataframe. So using this same example, if the original dataframe was modified in the following manner for the numerical columns: Then the string columns would look like: The issue is I don't know how to do this on the original dataframe, I can only do it on a copy: Full code to reproduce this found below: Edit: Wanted to add that this can technically be solved with the following: However, this would mean doubling the memory as a copy of the original dataframe would need to be taken, which ideally I would like to avoid. I'm pretty sure the memory is already doubled because of the mask (), so I want to avoid exploding the memory as much as possible", "q_apis": "get columns index values values at all where index value between now contains columns mask values all columns columns columns copy add mean copy mask", "apis": "mask mask mask values", "code": ["mask = row_indexer[:, None] & col_indexer\ndf[str_cols] = df[str_cols].mask(mask.values, 'new string')\n", "           a         b         c       a_str       b_str       c_str\n0  -1.810802 -0.776590 -0.495147       blank       blank       blank\n1   1.381038  0.235168  2.334671       blank       blank       blank\n2   0.406279 -1.571401  1.011139       blank       blank       blank\n3  -1.200217 -1.013983 -0.040659       blank       blank       blank\n4   1.261759  0.863896  0.228914       blank       blank       blank\n5   0.696952 -1.384910  1.204492       blank       blank       blank\n6        NaN  1.180030       NaN  new string       blank  new string\n7  -2.027946       NaN       NaN       blank  new string  new string\n8        NaN       NaN       NaN  new string  new string  new string\n9        NaN       NaN       NaN  new string  new string  new string\n10 -1.389175  2.263662       NaN       blank       blank  new string\n11       NaN -1.077414       NaN  new string       blank  new string\n12       NaN -1.696859 -1.049889  new string       blank       blank\n13 -1.057308       NaN       NaN       blank  new string  new string\n14       NaN       NaN -1.206815  new string  new string       blank\n15       NaN       NaN       NaN  new string  new string  new string\n16  2.063715 -1.981503       NaN       blank       blank  new string\n17       NaN -1.022833  1.957646  new string       blank       blank\n18  1.315031       NaN  1.425088       blank  new string       blank\n19 -1.860641       NaN       NaN       blank  new string  new string\n"], "link": "https://stackoverflow.com/questions/67528807/pandas-indexing-by-columns-and-mask"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a series that looks like the following: I want to use Pandas to perform a conditional rolling count of each block of time that contains step = 2 and output the count to a new column. I have found answers on how to do conditional rolling counts (Pandas: conditional rolling count), but I cannot figure out how to count the sequential runs of each step as a single block. The output should look like this:", "q_apis": "get columns rolling count time contains step count rolling rolling count count step", "apis": "where eq dropna groupby isna cumsum", "code": ["s = df.Step.where(df.Step.eq(2))\ndf['Run_count'] = s.dropna().groupby(s.isna().cumsum()).ngroup()+1\n", "    Time  Step  Run_count\n0      0     0        NaN\n1      1     1        NaN\n2      2     2        1.0\n3      3     2        1.0\n4      4     2        1.0\n5      5     3        NaN\n6      6     0        NaN\n7      7     1        NaN\n8      8     2        2.0\n9      9     2        2.0\n10    10     2        2.0\n11    11     3        NaN\n"], "link": "https://stackoverflow.com/questions/63640710/pandas-conditional-rolling-block-count"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "when i go to write my for loop generated dataframes to an excel file, only the last line is written into the excel file. i have tried concatenating the dataframes, as each iteration creates a new data frame and then write it to the excel file. so essentially what i am looking to do is, to successfully concatenate the data frames into one dataframe before i write them to the excel file. i cant write them in individually using pd.ExcelWriter, as I may have 100's of feature names", "q_apis": "get columns last ExcelWriter names", "apis": "to_excel iloc iloc to_excel", "code": ["def CCC_90(df_1,x):\n    writer = pd.ExcelWriter('No3_dVSa.xlsx', engine='xlsxwriter')\n    startrow = 1\n\n    for i in x:\n        # ...\n        appended_data.to_excel(writer, sheet_name='Sheet1', startrow=startrow)\n        startrow = len(appended_data)\n    writer.save()\n", "def CCC_90(df_1,x):\n    writer = pd.ExcelWriter('No3_dVSa.xlsx', engine='xlsxwriter')\n    # the next line should result in a dataframe with\n    # a column containing the feature numbers say fc\n    feature_nums = df_1.iloc[x]\n\n    # getting the feature names is more direct\n    feature_names = dfFeaturename.iloc[feature_nums.fc.tolist()]\n    feature_names.to_excel(writer, sheet_name='Sheet1', startrow=1)\n    write.save()\n"], "link": "https://stackoverflow.com/questions/56811571/append-data-frames-which-are-created-in-a-for-loop"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame similar to this: I need to change the values of some cells depending on what is: needs + 1 needs + 1 needs + 1 My issue is that my current solutions are extremely verbose. What I've tried ( is the original data source): As I have to do this for more columns than the example (similar concept, though) this becomes very long and verbose, and possibly error prone (in fact, I've made several mistakes just typing down the example) due to all the duplicated lines. This question is similar to mine, but there the values are predefined, while I need to get them from the data themselves.", "q_apis": "get columns DataFrame values columns all duplicated values get", "apis": "loc isin loc eq", "code": ["df.loc[df['Type'].isin(['SNP','INS']), 'Start_Position'] += 1\ndf.loc[df['Type'].eq('INS'), 'End_Position'] += 1\n"], "link": "https://stackoverflow.com/questions/66047584/replacing-specific-values-in-a-pandas-dataframe-basing-on-the-values-of-another"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Pandas DataFrame which contains sentences and their corresponding tokens such as the following: I want to remove duplicated sentences from this dataframe, that is, based on the sentence id, remove all rows if tokens matched previously. For example, if there is another sentence with the tokens (with the same order), I want to remove all the rows of that sentence. How can I achieve this? Thank you.", "q_apis": "get columns DataFrame contains duplicated all all", "apis": "groupby apply join duplicated index isin", "code": ["dup_ix = df.groupby('sent_id')['token'].apply(' '.join).duplicated()\ngood_id = dup_ix[~dup_ix].index\ndf[df['sent_id'].isin(good_id)]\n"], "link": "https://stackoverflow.com/questions/61443445/python-pandas-remove-duplicate-rows-based-on-grouping"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i am wondering how to create a data frame that create from list of list and also transpose it i have a data frame below My Expected Result", "q_apis": "get columns transpose", "apis": "values isin values isin values loc sum apply", "code": ["def summer(x):\n    values = x.split(',')\n    m1 = df2['ColA'].isin(values)\n    m2 = df2['ColB'].isin(values)\n    return df2.loc[m1 | m2, 'Sales'].sum()\n\ndf['TotalSales'] = df['ColA'].apply(summer)\n"], "link": "https://stackoverflow.com/questions/53541529/how-to-create-a-dataframe-from-a-column-in-another-dataframe-that-concat-by-comm"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following table with fictive data: I want to remove any duplicate rows and keep only the row which contains a positive value in \"Won Turnover\". Hence, the two rows marked with red should be removed in this case Moreover, if there are duplicate rows with only Lost Turnover, then the row with the highest turnover should be kept (The bottom two rows).", "q_apis": "get columns any contains value", "apis": "sort_values drop_duplicates", "code": ["df.sort_values(['Won Turnover', 'Lost Turnover'], ascending=False).drop_duplicates('Supplier')\n"], "link": "https://stackoverflow.com/questions/58265366/python-remove-duplicates-from-dataframe-based-on-another-column-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe of minute stock returns and I would like to create a new column that is conditional on whether a return was exceeded (pos or negative), and if so that row is equal to the limit (pos or negative), otherwise equal to the last column that was checked. The example below illustrates this: The target would be this: So in the first row, the returns never exceeded the limit, so the value becomes equal to the value in returns2 (0.03). In row 2, the returns were exceeded on the upside, so the value should be the positive limit. In row 3 the returns where exceeded on the downside first, so the value should be the negative limit. My actual dataframe has a couple thousand columns, so I am not quite sure how to do this (maybe a loop?). I appreciate any suggestions. The idea is to test a stop loss or limit trading algorithm. Whenever, the lower limit is triggered, it should replace the final column with the lower limit, same for the upper limit, whichever comes first for that row. So once either one is triggered, the next row should be tested. I am adding a different example with one more column here to make this a bit clearer (the limit is +/- 0.1) In the first row, the limit was never triggered to the final return is from returns3 (0.02). In row 2 the limit was triggered on the upside in returns 1 so the fin_return is equal to the upper limit (anything that happens in returns2 and returns 3 is irrelevant for this row). In row 3, the limited was exceeded on the downside in returns 2, so the fin_return becomes -0.1, and anything in returns3 is irrelevant.", "q_apis": "get columns minute last first value value value where first value columns any test stop replace first first", "apis": "ge DataFrame set_index drop", "code": ["dict = [\n        {'ticker':'jpm','date': '2016-11-28','returns1': 0.02,'returns2': 0.03,'limit': 0.1,'returns3':0.02},\n{ 'ticker':'ge','date': '2016-11-28','returns1': 0.2,'returns2': -0.3,'limit': 0.1,'returns3':0.6},\n{'ticker':'fb', 'date': '2016-11-28','returns1': -0.02,'returns2': -0.2,'limit': 0.1,'returns3':0.7},\n]\ndf = pd.DataFrame(dict)\ndf['date']      = pd.to_datetime(df['date'])\ndf=df.set_index(['date','ticker'], drop=True)  \n", "#select all columns without first (here limit column)\ndf1 = df.iloc[:, 1:]\n\n#comapre if all columns under +-limit\nmask = df1.lt(df['limit'], axis=0) & df1.gt(-df['limit'], axis=0) \nm1 = mask.all(axis=1)\nprint (m1)\ndate        ticker\n2016-11-28  jpm        True\n            ge        False\n            fb        False\ndtype: bool\n\n#replace first columns in limit with NaNs and back filling missing values, seelct first col\nm2 = df1.mask(mask).bfill(axis=1).iloc[:, 0].gt(df['limit'])\nprint (m2)\ndate        ticker\n2016-11-28  jpm       False\n            ge         True\n            fb        False\ndtype: bool\n\narr = np.select([m1,m2, ~m2], [df1.iloc[:, -1], df['limit'], -df['limit']])\n#set first column in DataFrame by insert\ndf.insert(0, 'fin_return', arr)\nprint (df)\n                   fin_return  limit  returns1  returns2  returns3\ndate       ticker                                                 \n2016-11-28 jpm           0.02    0.1      0.02      0.03      0.02\n           ge            0.10    0.1      0.20     -0.30      0.60\n           fb           -0.10    0.1     -0.02     -0.20      0.70\n"], "link": "https://stackoverflow.com/questions/56765781/conditional-if-statement-applied-to-multiple-columns-of-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to find the way change name of specific column in a multilevel dataframe. With this data: This code not working: Result: And also not: Result: But with combination of above codes working!!! Result: Is this bug of Pandas?", "q_apis": "get columns name codes", "apis": "columns last columns columns values rename columns columns columns values columns", "code": ["dataDF.columns[0] = ('Z', '100', 'Z')\n", "---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-32-2c0b76762235> in <module>()\n----> 1 dataDF.columns[0] = ('Z', '100', 'Z')\n\n//anaconda/envs/3.5/lib/python3.5/site-packages/pandas/indexes/base.py in __setitem__(self, key, value)\n   1372 \n   1373     def __setitem__(self, key, value):\n-> 1374         raise TypeError(\"Index does not support mutable operations\")\n   1375 \n   1376     def __getitem__(self, key):\n\nTypeError: Index does not support mutable operations\n", "dataDF.columns.values[0] = ('Z', '100', 'Z')\n", "dataDF.rename(columns={('A', '1', 'I'): ('Z', '100', 'Z')}, inplace=True)\n", "from_col = ('A', '1', 'I')\nto_col = ('Z', '100', 'Z')\ncolloc = dataDF.columns.get_loc(from_col)\ncvals = dataDF.columns.values\ncvals[colloc] = to_col\n\ndataDF.columns = pd.MultiIndex.from_tuples(cvals.tolist())\n\ndataDF\n\n[![enter code here][1]][1]\n"], "link": "https://stackoverflow.com/questions/40459254/pandas-change-a-specific-column-name-in-dataframe-having-multilevel-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame (Main) as below. The columns have a group classification as described in the Group Dict. There is a second DataFrame with Group values. I need to subtract from each column in Main the value from the corresponding group in Group Value DataFrame. The resulting table is shown below as well. (Exp: , etc.) Is there a matrix form of doing this or will I need to use a for-loop? Code: '", "q_apis": "get columns DataFrame columns second DataFrame values value DataFrame", "apis": "DataFrame index columns DataFrame index columns reindex values set_axis keys reset_index DataFrame DataFrame DataFrame set_index set_index reindex set_axis columns reindex set_axis reset_index", "code": ["df = pd.DataFrame(index=pd.date_range(\"1/1/2018\",\"1/10/2018\"), columns= \n[\"AAA\",\"BBB\",\"CCC\",\"DDD\"])\ndf[\"AAA\"]=range(100,110)\ndf[\"BBB\"]=range(200,210)\ndf[\"CCC\"]=range(300,310)\ndf[\"DDD\"]=range(400,410)\n\nGroup_Dict = dict({\"AAA\":\"Group1\",  \"BBB\":\"Group2\", \"CCC\":\"Group1\", \"DDD\":\"Group2\"})\n\ngroup_value = pd.DataFrame(index=pd.date_range(\"1/1/2018\",\"1/10/2018\"), columns=[\"Group1\",\"Group2\"])\ngroup_value[\"Group1\"]=range(10,29)[::2]\ngroup_value[\"Group2\"]=range(100,600)[::50]\n\nsub_group = group_value.reindex(Group_Dict.values(), axis=1)\\\n                       .set_axis(Group_Dict.keys(), axis=1, inplace=False)\n\ndf_out = (df - sub_group).reset_index()\nprint(df_out)\n", "       index  AAA  BBB  CCC  DDD\n0 2018-01-01   90  100  290  300\n1 2018-01-02   89   51  289  251\n2 2018-01-03   88    2  288  202\n3 2018-01-04   87  -47  287  153\n4 2018-01-05   86  -96  286  104\n5 2018-01-06   85 -145  285   55\n6 2018-01-07   84 -194  284    6\n7 2018-01-08   83 -243  283  -43\n8 2018-01-09   82 -292  282  -92\n9 2018-01-10   81 -341  281 -141\n", "    main = pd.DataFrame({'Date':pd.date_range('01-01-2018',periods=10,freq='D'),\n                         'AAA':np.arange(100,110),'BBB':np.arange(200,210),\n                         'CCC':np.arange(300,310),'DDD':np.arange(400,410)})\n    groupdict=pd.DataFrame({'Key':['AAA','BBB','CCC','DDD'],\n                            'Group':['Group1','Group1','Group2','Group2']})\n    groupvalue=pd.DataFrame({'Date':pd.date_range('01-01-2018',periods=10,freq='D'),\n                             'Group1':np.arange(10,29,2),'Group2':np.arange(100,575,50)})\n    \n    groupvalue=groupvalue.set_index('Date')\n\nmain = main.set_index('Date')\n\n#Use reindex and set_axis to expand and match your main dataframe columns\nsub_group = groupvalue.reindex(groupdict.Group,axis=1)\\\n                      .set_axis(groupdict.Key, axis=1, inplace=False)\n\n#Subtract letting pandas handle data alighnment with indexes.\ndf_out = (main - sub_group).reset_index()\nprint(df_out)\n", "        Date  AAA  BBB  CCC  DDD\n0 2018-01-01   90  190  200  300\n1 2018-01-02   89  189  151  251\n2 2018-01-03   88  188  102  202\n3 2018-01-04   87  187   53  153\n4 2018-01-05   86  186    4  104\n5 2018-01-06   85  185  -45   55\n6 2018-01-07   84  184  -94    6\n7 2018-01-08   83  183 -143  -43\n8 2018-01-09   82  182 -192  -92\n9 2018-01-10   81  181 -241 -141\n"], "link": "https://stackoverflow.com/questions/50358140/pandas-dataframe-subtract-a-series-using-groupby-classification"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe looks like : The output desired is : When I want to remove all id which have a difference time between the first and last taking time one year minimum. I tried with But I'm not sure if it's the right way to do this and if yes what is meaning of ? Days, Months, Years ... ?", "q_apis": "get columns all difference time between first last time year right", "apis": "groupby transform diff sum astype bool groupby transform max min groupby transform max min", "code": ["idx = df.groupby('id').TakingTime.transform(lambda x: x.dt.year.diff().sum().astype(bool))\ndf[idx]\n", "    id TakingTime\n0    1 2015-03-01\n1    1 2015-07-18\n2    1 2015-10-22\n3    1 2016-01-14\n8    3 2015-11-02\n9    3 2015-11-16\n10   3 2016-02-19\n11   3 2016-04-21\n", "days = 865\ndf.groupby('id').TakingTime.transform(lambda x: (x.max() - x.min()).days >= days)\n", "from datetime import timedelta\ndays = timedelta(865)\ndf.groupby('id').TakingTime.transform(lambda x: (x.max() - x.min()) >= days)\n"], "link": "https://stackoverflow.com/questions/65270999/how-to-modify-a-data-to-remove-ids-within-certain-bounds-of-another-time-veriabl"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have been looking this site and google for an answer to my question, but they all apply to columns. In my data set there are a couple of cells which only contain a space, instead of NaN. So I would like to drop all the rows there this is the case. I know I can use the code below to do so per column. But how do I apply this to the entire dataframe?", "q_apis": "get columns all apply columns drop all apply", "apis": "all", "code": ["df= df[(df != ' ').all(axis=1)]\n"], "link": "https://stackoverflow.com/questions/65917369/drop-all-rows-from-a-dataframe-based-on-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have following data frame. The data frame is constructed by reading a csv file. Its a large data set but for this question purpose I have used 15 rows from the data set as an example. Expected Results After pivot what I want is following data frame: Reason I am doing this is once I do this I can use this for a prediction task where as label data. So far I have done following to to build the above matrix but I am unable to get the column as shown after the pivot. This is what I have done: By doing this I get: As you can see I am missing the column at the end. So the question is How can I get above data frame with the expert_level as I shown in my \"Expected Results\"?", "q_apis": "get columns pivot where get pivot get at get", "apis": "pivot lookup drop_duplicates lookup set_index pivot join lookup join", "code": ["pivoted = df.pivot('user_id', 'group_space', 'contrib_count')\n", "lookup = df.drop_duplicates('user_id')[['user_id', 'expert_level']]\nlookup.set_index(['user_id'], inplace=True)\n", "pivoted2 = df.pivot('user_id', 'group_space', 'total_min_length')\n", "result = pivoted.join(lookup).join(pivoted2, lsuffix=\"_contrib_count\", rsuffix=\"_total_min_length\")\n", "0, 1, 2, 3, 4"], "link": "https://stackoverflow.com/questions/26431157/python-pandas-pivot-table-missing-column-after-pivot"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to convert a dataframe that has list of various size for example something like this: to something like this: Thank you for the help", "q_apis": "get columns size", "apis": "pop explode sum add_prefix", "code": ["dfB = pd.get_dummies(df['B'].explode()).sum(level=0).add_prefix('B-')\n#dfB = pd.get_dummies(df['B'].explode()).max(level=0).add_prefix('B-')\n\ndf = pd.concat([df['A'], dfB], axis=1)\n\n#   A  B-1  B-2  B-3  B-4  B-5\n#0  1    1    1    1    0    0\n#1  2    0    0    1    0    1\n#2  3    0    0    0    1    0\n", "df = pd.concat([df, pd.get_dummies(df.pop('B').explode()).sum(level=0).add_prefix('B-')],\n                axis=1)\n"], "link": "https://stackoverflow.com/questions/61263759/pandas-dataframe-convert-column-of-lists-to-multiple-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame where data is located in another column and I want to take those dates from that column and create a date column and store them. Here is my sample data. And I want to get the following output. Here is my attempt. And this code is giving me the output below. But the problem is that this code will only merge the rows. Not sure how to duplicate the dates across the respective rows as shown in the desired output and put it in a different column. Can anyone please help me on how to achieve this?", "q_apis": "get columns where take date sample get merge put", "apis": "assign where shift replace dropna all ffill groupby agg join reset_index", "code": ["task_mask = df.Task.str.match(\"Task\\s+\\d\")\ndf.assign(Task = df.Task[task_mask],\n          Date = pd.Series(np.where(~task_mask, df[\"Task\"], np.NaN)).shift()) \\\n        .replace(\"\", np.NaN) \\\n        .dropna(how='all') \\\n        .ffill() \\\n        .groupby([\"Task\", \"ID\", \"Date\"]).agg({\"Supervisor\": lambda x: \" \".join(x)}) \\\n        .reset_index()\n", "#      Task     ID                      Date                Supervisor\n# 0  Task 1  13588   Monday, 13 January 2020     Jack Address 1 City 1\n# 1  Task 2  13589   Monday, 13 January 2020      Ammie Address 2 City\n# 2  Task 3  13589   Monday, 13 January 2020   Amanda Address 3 City 3\n# 3  Task 4  13587  Tuesday, 14 January 2020  Chelsea Address 4 City 4\n# 4  Task 5  13586  Tuesday, 14 January 2020  Ibrahim Address 5 City 5\n# 5  Task 6  13585  Tuesday, 14 January 2020     Kate Address 6 City 6\n", "# Create dataframe\ndata = [['Monday, 13 January 2020', '', ''], ['Task 1', 13588, 'Jack'], ['', '', 'Address 1'], ['', '', 'City 1'], ['Task 2', 13589, 'Ammie'], ['', '', 'Address 2'], ['', '', 'City'], ['Task 3', 13589, 'Amanda'], ['', '', 'Address 3'], ['', '', 'City 3'], [\n    'Tuesday, 14 January 2020', '', ''], ['Task 4', 13587, 'Chelsea'], ['', '', 'Address 4'], ['', '', 'City 4'], ['Task 5', '13586', 'Ibrahim'], ['', '', 'Address 5'], ['', '', 'City 5'], ['Task 6', 13585, 'Kate'], ['', '', 'Address 6'], ['', '', 'City 6']]\ndf = pd.DataFrame(data)\ndf.columns = ['Task', 'ID', 'Supervisor']\nprint(df)\n\n# Step 1\ntask_mask = df.Task.str.match(\"Task\\s+\\d\")\nprint(task_mask)\n# 0     False\n# 1      True\n# 2     False\n# 3     False\n# 4      True\n# 5     False\n# 6     False\n# 7      True\n# 8     False\n# 9     False\n# 10    False\n# 11     True\n# 12    False\n# 13    False\n# 14     True\n# 15    False\n# 16    False\n# 17     True\n# 18    False\n# 19    False\n# Name: Task, dtype: bool\n\n# Step 2\nprint(df.Task[task_mask])\n# 1     Task 1\n# 4     Task 2\n# 7     Task 3\n# 11    Task 4\n# 14    Task 5\n# 17    Task 6\n# Name: Task, dtype: object\n\n# Step 3\nprint(pd.Series(np.where(~task_mask, df[\"Task\"], np.NaN)).shift())\n# 0                          NaN\n# 1      Monday, 13 January 2020\n# 2                          NaN\n# 3\n# 4\n# 5                          NaN\n# 6\n# 7\n# 8                          NaN\n# 9\n# 10\n# 11    Tuesday, 14 January 2020\n# 12                         NaN\n# 13\n# 14\n# 15                         NaN\n# 16\n# 17\n# 18                         NaN\n# 19\n# dtype: object\n\n# Step 4\nprint(df.assign(Task=df.Task[task_mask],\n                Date=pd.Series(np.where(~task_mask, df[\"Task\"], np.NaN)).shift())\n        .replace(\"\", np.NaN))\n#       Task     ID Supervisor                      Date\n# 0      NaN    NaN        NaN                       NaN\n# 1   Task 1  13588       Jack   Monday, 13 January 2020\n# 2      NaN    NaN  Address 1                       NaN\n# 3      NaN    NaN     City 1                       NaN\n# 4   Task 2  13589      Ammie                       NaN\n# 5      NaN    NaN  Address 2                       NaN\n# 6      NaN    NaN       City                       NaN\n# 7   Task 3  13589     Amanda                       NaN\n# 8      NaN    NaN  Address 3                       NaN\n# 9      NaN    NaN     City 3                       NaN\n# 10     NaN    NaN        NaN                       NaN\n# 11  Task 4  13587    Chelsea  Tuesday, 14 January 2020\n# 12     NaN    NaN  Address 4                       NaN\n# 13     NaN    NaN     City 4                       NaN\n# 14  Task 5  13586    Ibrahim                       NaN\n# 15     NaN    NaN  Address 5                       NaN\n# 16     NaN    NaN     City 5                       NaN\n# 17  Task 6  13585       Kate                       NaN\n# 18     NaN    NaN  Address 6                       NaN\n# 19     NaN    NaN     City 6                       NaN\n\n# Step 5:\nprint(df.assign(Task = df.Task[task_mask],\n                Date = pd.Series(np.where(~task_mask, df[\"Task\"], np.NaN)).shift()) \\\n        .replace(\"\", np.NaN) \\\n        .dropna(how='all'))\n#       Task     ID Supervisor                      Date\n# 1   Task 1  13588       Jack   Monday, 13 January 2020\n# 2      NaN    NaN  Address 1                       NaN\n# 3      NaN    NaN     City 1                       NaN\n# 4   Task 2  13589      Ammie                       NaN\n# 5      NaN    NaN  Address 2                       NaN\n# 6      NaN    NaN       City                       NaN\n# 7   Task 3  13589     Amanda                       NaN\n# 8      NaN    NaN  Address 3                       NaN\n# 9      NaN    NaN     City 3                       NaN\n# 11  Task 4  13587    Chelsea  Tuesday, 14 January 2020\n# 12     NaN    NaN  Address 4                       NaN\n# 13     NaN    NaN     City 4                       NaN\n# 14  Task 5  13586    Ibrahim                       NaN\n# 15     NaN    NaN  Address 5                       NaN\n# 16     NaN    NaN     City 5                       NaN\n# 17  Task 6  13585       Kate                       NaN\n# 18     NaN    NaN  Address 6                       NaN\n# 19     NaN    NaN     City 6                       NaN\n\n# Step 6:\nprint(df.assign(Task = df.Task[task_mask],\n                 Date = pd.Series(np.where(~task_mask, df[\"Task\"], np.NaN)).shift()) \\\n            .replace(\"\", np.NaN) \\\n            .dropna(how='all') \\\n            .ffill())\n#       Task     ID Supervisor                      Date\n# 1   Task 1  13588       Jack   Monday, 13 January 2020\n# 2   Task 1  13588  Address 1   Monday, 13 January 2020\n# 3   Task 1  13588     City 1   Monday, 13 January 2020\n# 4   Task 2  13589      Ammie   Monday, 13 January 2020\n# 5   Task 2  13589  Address 2   Monday, 13 January 2020\n# 6   Task 2  13589       City   Monday, 13 January 2020\n# 7   Task 3  13589     Amanda   Monday, 13 January 2020\n# 8   Task 3  13589  Address 3   Monday, 13 January 2020\n# 9   Task 3  13589     City 3   Monday, 13 January 2020\n# 11  Task 4  13587    Chelsea  Tuesday, 14 January 2020\n# 12  Task 4  13587  Address 4  Tuesday, 14 January 2020\n# 13  Task 4  13587     City 4  Tuesday, 14 January 2020\n# 14  Task 5  13586    Ibrahim  Tuesday, 14 January 2020\n# 15  Task 5  13586  Address 5  Tuesday, 14 January 2020\n# 16  Task 5  13586     City 5  Tuesday, 14 January 2020\n# 17  Task 6  13585       Kate  Tuesday, 14 January 2020\n# 18  Task 6  13585  Address 6  Tuesday, 14 January 2020\n# 19  Task 6  13585     City 6  Tuesday, 14 January 2020\n\n# Step 7\nprint(df.assign(Task = df.Task[task_mask],\n                Date = pd.Series(np.where(~task_mask, df[\"Task\"], np.NaN)).shift()) \\\n        .replace(\"\", np.NaN) \\\n        .dropna(how='all') \\\n        .ffill() \\\n        .groupby([\"Task\", \"ID\", \"Date\"]).agg({\"Supervisor\": lambda x: \" \".join(x)}))\n#                                                      Supervisor\n# Task   ID    Date\n# Task 1 13588 Monday, 13 January 2020      Jack Address 1 City 1\n# Task 2 13589 Monday, 13 January 2020       Ammie Address 2 City\n# Task 3 13589 Monday, 13 January 2020    Amanda Address 3 City 3\n# Task 4 13587 Tuesday, 14 January 2020  Chelsea Address 4 City 4\n# Task 5 13586 Tuesday, 14 January 2020  Ibrahim Address 5 City 5\n# Task 6 13585 Tuesday, 14 January 2020     Kate Address 6 City 6\n\n# Step 8\ndf = df.assign(Task = df.Task[task_mask],\n               Date = pd.Series(np.where(~task_mask, df[\"Task\"], np.NaN)).shift()) \\\n        .replace(\"\", np.NaN) \\\n        .dropna(how='all') \\\n        .ffill() \\\n        .groupby([\"Task\", \"ID\", \"Date\"]).agg({\"Supervisor\": lambda x: \" \".join(x)}) \\\n        .reset_index()\nprint(df)\n\n#      Task     ID                      Date                Supervisor\n# 0  Task 1  13588   Monday, 13 January 2020     Jack Address 1 City 1\n# 1  Task 2  13589   Monday, 13 January 2020      Ammie Address 2 City\n# 2  Task 3  13589   Monday, 13 January 2020   Amanda Address 3 City 3\n# 3  Task 4  13587  Tuesday, 14 January 2020  Chelsea Address 4 City 4\n# 4  Task 5  13586  Tuesday, 14 January 2020  Ibrahim Address 5 City 5\n# 5  Task 6  13585  Tuesday, 14 January 2020     Kate Address 6 City 6\n"], "link": "https://stackoverflow.com/questions/61556939/how-to-rearrange-row-of-data-frame-using-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There are three columns in a dataFrame Ticker, Attribute, and Value. The original dataFrame can be seen here I want to set the Attribute values as a column which can easily be done by setting it as an index and then taking the transpose but the problem is that I want to keep the Ticker as a column when I take the transpose it become the row. When I take Attribute as an index and take its transpose When I set both as an index and then take transpose it looks something like this which I don't want When both the Attribute and Ticker taken as index and transposed What I want is this The required output", "q_apis": "get columns columns values index transpose take transpose take index take transpose index take transpose index", "apis": "pivot_table index columns values", "code": ["df = df.pivot_table(index= 'Ticker', columns='Attribute', values = 'Value')\n"], "link": "https://stackoverflow.com/questions/67637508/keep-the-existing-columns-and-convert-the-row-to-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe (the sample looks like this) What I want to do is sort the values based on Size so the final data frame should look like this: I am able to successfully get the results using this code The issue is that the given code dosen't work on dataframe it gives the results '' and '' whereas I want the results as also in case of more sizes, the order should be '' and in case of second example, '' This is what I have done so far but it's not working and showing the wrong order", "q_apis": "get columns sample values get second", "apis": "values DataFrame columns add first get size size size join join size size cumsum sort_values sort_values", "code": ["import pandas as pd\n\n#----------------------#\n# Recreate the dataset #\n#----------------------#\n# raw input data_1 = \"\"\" Variable|2|Boots|Shoes on sale|XL,S,M|  \n                         Variation|2.5|Boots XL||XL|330 Variation|2.6|Boots S||S|330 \n                         Variation|2.7|Boots M||M|330 Variable|3|Helmet|Helmet Sizes|E42,E41| \n                         Variation|3.8|Helmet E42||E42|89 \n                         Variation|3.2|Helmet E41||E41|89\"\"\"\n\ndata_2 = \"\"\" Variable|2|Boots|Shoes on sale|XL,S,3XL| \n             Variation|2.5|Boots XL||XL|330 \n             Variation|2.6|Boots 3XL||3XL|330 \n             Variation|2.7|Boots S||S|330 \n             Variable|3|Helmet|Helmet Sizes|S19, S9| \n             Variation|3.8|Helmet E42||S19|89 \n             Variation|3.2|Helmet E41||S9|89\"\"\"\n\n# Construct 1 data set\ndata = 'Type|SKU|Description|FullDescription|Size|Price'\ndata += data_2 # this can also be data_1  or data_1 + data_2\n\n# pre-process: split the lines and values into a list of lists.\ndata = [row.split('|') for row in data.split('\\n')]\n\n#-------------#\n# create a df #\n#-------------#\ndf = pd.DataFrame(data[1:], columns=data[0]) df\n", "Type    SKU     Description     FullDescription          Size   Price\n0   Variable    2               Boots   Shoes on sale   XL,S,3XL    \n1   Variation   2.5             Boots XL                XL          330\n2   Variation   2.6             Boots 3XL               3XL         330\n3   Variation   2.7             Boots S                 S           330\n4   Variable    3               Helmet  Helmet Sizes    S19, S9     \n5   Variation   3.8             Helmet E42              S19         89\n6   Variation   3.2             Helmet E41              S9          89\n", "# Prioritize the sizes\n# ps, i don't know the order :) \npriority_dict = {k : e for e, k in enumerate([ 'XXS','XS','S','M','L','XL','XXL','3XL','4XL','5XL', 'E41', 'E42', 'S9', 'S19' ])}\npriority_dict\n", "{'XXS': 0,\n 'XS': 1,\n 'S': 2,\n 'M': 3,\n 'L': 4,\n 'XL': 5,\n 'XXL': 6,\n '3XL': 7,\n '4XL': 8,\n '5XL': 9,\n 'E41': 10,\n 'E42': 11,\n 'S9': 12,\n 'S19': 13}\n", "# Split the string  \"SIZE\" into a list    \"XL,S,M\" --> [\"XL\", \"S\", \"M\"]\n# And, add the value from our priority dict to it  --> [(5, \"XL\"), (2, \"S\"), (3, \"M\")]\n# Last but not least, sort list (by the first value) --> [(2, \"S\"), (3, \"M\"), (5, \"XL\")]\ndf[\"TMP_SIZE\"] = [ sorted([(priority_dict.get(size.strip()), size.strip())  for size in sizes.split(',')]) for sizes in df.Size]\ndf\n", "Type    SKU     Description     FullDescription          Size       Price  TMP_SIZE\n0   Variable    2               Boots   Shoes on sale   XL,S,3XL           [(2, S), (5, XL), (7, 3XL)]\n1   Variation   2.5             Boots XL                XL          330    [(5, XL)]\n2   Variation   2.6             Boots 3XL               3XL         330    [(7, 3XL)]\n3   Variation   2.7             Boots S                 S           330    [(2, S)]\n4   Variable    3               Helmet  Helmet Sizes    S19, S9            [(12, S9), (13, S19)]\n5   Variation   3.8             Helmet E42              S19         89     [(13, S19)]\n6   Variation   3.2             Helmet E41              S9          89     [(12, S9)]\n", "# Create a new SIZE\n# loop over the TMPS_SIZE and create a string from the second value of the tuplelist --> ', '.join( my_list )\n\ndf['NEW_SIZE'] = [', '.join([ size[1]for size in sizes ]) for sizes in df[\"TMP_SIZE\"] ]\n", "Type    SKU     Description     ...     Size        Price  TMP_SIZE                       NEW_SIZE\n0   Variable    2               ...     XL,S,3XL           [(2, S), (5, XL), (7, 3XL)]  S, XL, 3XL\n1   Variation   2.5             ...     XL          330    [(5, XL)]                    XL\n2   Variation   2.6             ...     3XL         330    [(7, 3XL)]                   3XL\n3   Variation   2.7             ...     S           330    [(2, S)]                     S\n4   Variable    3               ...     S19, S9            [(12, S9), (13, S19)]        S9, S19\n5   Variation   3.8             ...     S19         89     [(13, S19)]                  S19\n6   Variation   3.2             ...     S9          89     [(12, S9)]                   S9\n", "#grp\ndf['grp']= (df['Type'] == 'Variable').cumsum()\ndf\n", "# sort the dataset\ndf = df.sort_values('TMP_SIZE') # notice that we sort on the list of tuples\ndf.sort_values(by=['grp', 'Type'])\n"], "link": "https://stackoverflow.com/questions/65616914/sorting-rows-in-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like as shown below but my real dataframe has millions of rows I would like to find the number of times a test is conducted (identify ) every 24hours (using , , ) I tried the below Instead of repeating the same line thrice, is there any other efficient and elegant way to find this? Because my real dataframe has million of rows", "q_apis": "get columns test any", "apis": "groupby median drop columns groupby median", "code": ["df_out=te_df.groupby('test').median().drop(columns=['subject_id'])\n\n#output\n\n       0-24hrs  24-48hrs    48-72hrs\ntest            \ntest1   1.5     1.5         2.5\ntest2   1.0     1.0         3.0\ntest3   1.0     1.0         1.0\ntest4   NaN     1.0         NaN\n", "out = df.groupby('test')[['0-24hrs', '24-48hrs', '48-72hrs']].median()\n"], "link": "https://stackoverflow.com/questions/66907067/elegant-and-efficient-way-to-find-the-median-value-based-on-different-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this: How to get this result: The way calculate val_nm as follows: val_11 is equal to the column mean value of the column value of A1 divided by the column value of B1, Note that the column A1 divided by the column B1, the corresponding number is divided by the result, if it is greater than 1, take the reciprocal, and then find the average of the result So whether A1 is divided by B1 or B1 is divided by A1, the result value must be the same. In order to calculate val, it may be necessary to define a function, val is greater than 0, there will be no division by 0 I take val_11 as example A1[5,8,7,4,2,2] B1[2,9,17,14,32,3] val_11 =avg (A1/B1) =avg(5/2 take 2/5 + 8/9 +7/17 + 4/15 +2/32 +2/3) = 0.4525 so no matter A1/B1 or B1/A1, result will be the same please help me caculate result", "q_apis": "get columns get mean value value value take value take take", "apis": "loc columns values loc columns values where mean pivot product columns merge pivot index columns values reset_index assign pivot index columns values reset_index assign assign loc T loc columns astype values loc columns astype values where mean pivot product columns merge pivot index columns values reset_index assign pivot index columns values reset_index assign assign apply", "code": ["def meanofdiv(dfa):\n    a = dfa.loc[:,[c for c in dfa.columns if \"_A\" in c]].values \n    b = dfa.loc[:,[c for c in dfa.columns if \"_B\" in c]].values\n    return np.where((a/b)>1, b/a, a/b).mean(axis=1)\n\n# pivot key/val pair data to tables\n# caretesian product of tables\n# simple calculation of columns from A and a column from B\ndfr = pd.merge(\n    data1.pivot(index=\"An\", columns=\"colA\", values=\"Val\").reset_index().assign(foo=1),\n    data2.pivot(index=\"Bm\", columns=\"colA\", values=\"Val\").reset_index().assign(foo=1),\n    on=\"foo\",\n    suffixes=(\"_A\",\"_B\")\n).assign(resname=lambda dfa: dfa[\"An\"]+dfa[\"Bm\"],\n        res=meanofdiv)\n\ndfr.loc[:,[\"An\",\"Bm\",\"res\"]]\n\n", "def meanofdiv(dfa):\n    dfa = dfa.to_frame().T\n    a = dfa.loc[:,[c for c in dfa.columns if \"_A\" in c]].astype(float).values[0] \n    b = dfa.loc[:,[c for c in dfa.columns if \"_B\" in c]].astype(float).values[0]\n    a = a[~np.isnan(b)]\n    b = b[~np.isnan(b)]\n    return np.where((a/b)>1, b/a, a/b).mean()\n\n# pivot key/val pair data to tables\n# caretesian product of tables\n# simple calculation of columns from A and a column from B\ndfr = pd.merge(\n    data1.pivot(index=\"An\", columns=\"colA\", values=\"Val\").reset_index().assign(foo=1),\n    data2.pivot(index=\"Bm\", columns=\"colA\", values=\"Val\").reset_index().assign(foo=1),\n    on=\"foo\",\n    suffixes=(\"_A\",\"_B\")\n).assign(resname=lambda dfa: dfa[\"An\"]+dfa[\"Bm\"],\n        res=lambda dfa: dfa.apply(meanofdiv, axis=1))\n\n"], "link": "https://stackoverflow.com/questions/66255412/pandas-has-two-dataframes-want-the-average-of-the-divisions-between-each-group"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Looks ugly: Does not work: Is there an elegant and working solution of the above \"problem\"?", "q_apis": "get columns", "apis": "isin", "code": ["df_new[df_new['l_ext'].isin([31, 22, 30, 25, 64])]\n"], "link": "https://stackoverflow.com/questions/18250298/how-to-check-if-a-value-is-in-the-list-in-selection-from-pandas-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to forward fill a column and I want to specify a limit, but I want the limit to be based on the index---not a simple number of rows like limit allows. For example, say I have the dataframe given by: which looks like If I group by the column and forward fill in that group with , then my resulting data frame will be What I actually want to do here however is only forward fill onto rows whose indexes are within say 2 from the first index of each group, as opposed to the next 2 rows of each group. For example, if we just look at the groups on the dataframe: I would want the second group here to only be forward filled to index 4---not 8 and 9. The first group's NaN values are all within 2 indexes from the last non-NaN values, so they would be filled completely. The resulting dataframe would look like: FWIW in my actual use case, my index is a DateTimeIndex (and it is sorted). I currently have a solution which sort of works, requiring looping through the dataframe filtered on the group indexes, creating a time range for every single event with a non-NaN value based on the index, and then combining those. But this is far too slow to be practical.", "q_apis": "get columns index first index at groups second index first values all last values index time value index", "apis": "reset_index index mask index loc mask groupby ffill", "code": ["import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({\n    'data': [0.0, 1.0, 1, 3.0, np.nan, 22, np.nan, 5, np.nan, np.nan],\n    'group': [0, 0, 1, 0, 1, 0, 1, 0, 1, 1]})\n\ndf = df.reset_index()\ndf['stop_index'] = df['index'] + 2\ndf['stop_index'] = df['stop_index'].where(pd.notnull(df['data']))\ndf['stop_index'] = df.groupby('group')['stop_index'].ffill()\ndf['mask'] = df['index'] <= df['stop_index']\ndf.loc[df['mask'], 'data'] = df.groupby('group')['data'].ffill()\nprint(df)\n#    index  data  group  stop_index   mask\n# 0      0   0.0      0         2.0   True\n# 1      1   1.0      0         3.0   True\n# 2      2   1.0      1         4.0   True\n# 3      3   3.0      0         5.0   True\n# 4      4   1.0      1         4.0   True\n# 5      5  22.0      0         7.0   True\n# 6      6   NaN      1         4.0  False\n# 7      7   5.0      0         9.0   True\n# 8      8   NaN      1         4.0  False\n# 9      9   NaN      1         4.0  False\n\n# clean up df\ndf = df[['data', 'group']]\nprint(df)\n", "   data  group\n0   0.0      0\n1   1.0      0\n2   1.0      1\n3   3.0      0\n4   1.0      1\n5  22.0      0\n6   NaN      1\n7   5.0      0\n8   NaN      1\n9   NaN      1\n", "df = df.reset_index()\ndf['stop_index'] = df['index'] + 2\n", "df['mask'] = df['index'] <= df['stop_index']\ndf.loc[df['mask'], 'data'] = df.groupby('group')['data'].ffill()\n"], "link": "https://stackoverflow.com/questions/54357758/forward-fill-column-with-an-index-based-limit"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this: and a dictionary that looks like this I now want to the values of to columns that match the . I can do however, then the there are all the columns missing that don't match the regex. So, I can therefore do: which gives the desired output Is there a smarter solution that avoids the tempory dataframe?", "q_apis": "get columns now values columns all columns", "apis": "columns loc columns apply columns loc loc apply", "code": ["df.columns.str.contains(r'^c\\d+$') # use raw strings, it's good hygene\n# array([ True,  True, False,  True])\n", "df.loc[:, df.columns.str.contains(r'^c\\d+$')] = df.apply(lambda x: x.map(d))\n", "m = df.columns.str.contains(r'^c\\d+$')\ndf.loc[:, m] = df.loc[:, m].apply(lambda x: x.map(d))\n", "df\n\n    c1   c2 ignore_me   c3\n0  foo  foo  b         bar\n1  bar  foo  b         foo\n2  foo  foo  b         foo\n"], "link": "https://stackoverflow.com/questions/56419136/how-to-map-values-in-place"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have been trying to make a comparison of two dataframes, creating new dataframes for the ones which have the same entries in two columns. I thought I had cracked it but the code I have now just looks at the two columns of interest and if the string is found anywhere in that column it considers it a match. I need the two strings to be common on the same row across the columns. A sample of the code follows. So this code comes out with a table which has a list of the rows which has both source and target strings, but not the source and target strings necessarily having to appear in the same row. Can anyone help me look specifically row by row?", "q_apis": "get columns columns now at columns columns sample", "apis": "merge", "code": ["result = pd.merge(df1, df2, on='key')\n"], "link": "https://stackoverflow.com/questions/56022355/how-to-compare-two-dataframes-and-create-a-new-one-for-those-entries-which-are-t"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Pandas dataframe like this: And I wanted to extract the value from the column and add that extraction to a new column and obtain this output:", "q_apis": "get columns value add", "apis": "astype", "code": ["In [1168]: df \nOut[1168]: \n   id     Month\n0   1  Month 01\n1   1  Month 05\n2   2  Month 12\n\nIn [1169]: df['Month_no'] = df['Month'].str.split().str[1].astype(int)    \n\nIn [1170]: df                                     \nOut[1170]: \n   id     Month  Month_no\n0   1  Month 01         1\n1   1  Month 05         5\n2   2  Month 12        12\n"], "link": "https://stackoverflow.com/questions/62140187/how-to-extract-a-portion-of-a-datframe-column-and-create-another-column-with-tha"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am able to print/get to CSV dataframe from one URL using the code down below I would like is to concatenate multiple dataframes in one. dataframe1 from http://www.url1.com dataframe2 from http://www.url2.com ... dataframeN from http://www.urlN.com Combine all the dataframes into a single one and export it to 'file.csv'.", "q_apis": "get columns get all", "apis": "to_csv", "code": ["import pandas as pd\n\nurls = ['http://www.url1.com',\n        'http://www.url2.com',\n        'http://www.url3.com']\n\ndf = pd.concat([pd.concat(pd.read_html(url, header=0), axis=0) for url in urls], axis=0)\n\ndf.to_csv('file.csv')\n"], "link": "https://stackoverflow.com/questions/49834916/how-to-concatenate-list-of-results-from-pandas-read-html"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This might be considered as a duplicate of a thorough explanation of various approaches, however I can't seem to find a solution to my problem there due to a higher number of Data Frames. I have multiple Data Frames (more than 10), each differing in one column . This is just a quick and oversimplified example: Each has same or different depths for the same profiles, so I need to create a new DataFrame which would merge all separate ones, where the key columns for the operation are and , with all appearing depth values for each profile. The value should be therefore where there is no depth measurement of that variable for that profile. The result should be a thus a new, compressed DataFrame with all as additional columns to the and ones, something like this: Note that the actual number of profiles is much, much bigger. Any ideas?", "q_apis": "get columns DataFrame merge all where columns all values value where DataFrame all columns", "apis": "set_index reset_index", "code": ["dfs = [df.set_index(['profile', 'depth']) for df in [df1, df2, df3]]\n\nprint(pd.concat(dfs, axis=1).reset_index())\n#      profile  depth       VAR1     VAR2    VAR3\n# 0  profile_1    0.5  38.198002      NaN     NaN\n# 1  profile_1    0.6  38.198002  0.20440     NaN\n# 2  profile_1    1.1        NaN  0.20442     NaN\n# 3  profile_1    1.2        NaN  0.20446  15.188\n# 4  profile_1    1.3  38.200001      NaN  15.182\n# 5  profile_1    1.4        NaN      NaN  15.182\n"], "link": "https://stackoverflow.com/questions/55652704/merge-multiple-dataframes-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Edit: the rookie mistake I made in string having pointed out by @coldspeed, @wen-ben, @ALollz. Answers are quite good, so I don't delete this question to keep those answers. Original: I have read this question/answer What's the difference between groupby.first() and groupby.head(1)? That answer explained that the differences are on handling value. However, when I call with , they both pick fine. Furthermore, Pandas has with similar functionality to , and What are difference of with ? Example below: I saw that `firs()' resets index while the other 2 doesn't. Besides that, is there any differences?", "q_apis": "get columns delete difference between groupby first groupby head value difference index any", "apis": "DataFrame", "code": ["import pandas as pd\ndf = pd.DataFrame({'A': [1,1,2,2,3,3], 'B': [None, '1', np.NaN, '2', 3, 4]})\n", "df.groupby('A', as_index=False).first()\n#   A     B\n#0  1  None\n#1  2     2\n#2  3     3\n\ndf.groupby('A', as_index=False).first(dropna=True)\n#   A  B\n#0  1  1\n#1  2  2\n#2  3  3\n", "df.groupby('A', as_index=False).head(1)\n#   A     B\n#0  1  None\n#2  2   NaN\n#4  3     3\n\ndf.groupby('A', as_index=False).head(200)\n#   A     B\n#0  1  None\n#1  1     1\n#2  2   NaN\n#3  2     2\n#4  3     3\n#5  3     4\n", "df.groupby('A', as_index=False).nth(0)\n#   A     B\n#0  1  None\n#2  2   NaN\n#4  3     3\n", "df.groupby('A', as_index=False).nth(0, dropna='any')\n#   A  B\n#A      \n#1  1  1\n#2  2  2\n#3  3  3\n"], "link": "https://stackoverflow.com/questions/55583246/what-is-different-between-groupby-first-groupby-nth-groupby-head-when-as-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two pandas DataFrames of daily data: I would like to take the correlation of the values in each dataframe in non-overlapping monthly segments. The return value should be a DataFrame indexed by month, with columns ['a','b','c'], where each value is the correlation of daily values in df1 and df2 for that calendar month. I can perform this calculation looping over columns and months, but that does not sound like how a panda would do it. Is there a way to split two dataframes based on calendar month, apply a correlation between them, and combine into a single dataframe?", "q_apis": "get columns take values value DataFrame month columns where value values month columns month apply between combine", "apis": "DataFrame from_dict groupby apply corr values columns", "code": ["pd.DataFrame.from_dict({col:pd.concat([df1[[col]],df2[[col]]],axis=1).groupby(pd.Grouper(freq='M')).apply(lambda x: x.corr().values[0,1]) for col in df1.columns})\n"], "link": "https://stackoverflow.com/questions/50530496/monthly-correlations-of-daily-data-between-two-pandas-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My dataframe looks like this: I want to choose n rows at random while maintaining diversity in the strain values. For example, I want a group of 6, so I'd expect my final rows to include at least one of every type of strain with two strains appearing twice. I've tried converting the Strain column into a numpy array and using the method random.choice but that didn't seem to run. I've also tried using .sample but it does not maximize strain diversity. This is my latest attempt which outputs a sample of size 7 in order (identifiers 0-7) and the Strains are all the same.", "q_apis": "get columns at values at array sample sample size all", "apis": "sample groupby iloc", "code": ["n = 6\n\ndf = df.sample(frac=1)                      # step 1 \nenums = df.groupby('Strain').cumcount()     # step 2 \n        \norders = np.argsort(enums)                  # step 3\nsamples = df.iloc[orders[:n]]               # step 4\n", "   Identifier Strain  Other columns, etc.\n2           3      D                  NaN\n7           8      B                  NaN\n0           1      A                  NaN\n5           6      C                  NaN\n4           5      A                  NaN\n8           9      D                  NaN\n"], "link": "https://stackoverflow.com/questions/66716352/how-can-i-choose-a-random-sample-of-size-n-from-values-from-a-single-pandas-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "part of my data frame is as following: and I want to check if, in any row, the \"ActivityType\" column is equal to \"home_1\" and if \"Time\" column is NaN then replace \"Act_delay\" column to 10800. I have the following code\" but it does not work. the result is the same as before. what should I do?", "q_apis": "get columns any replace", "apis": "mask eq isna loc mask", "code": ["mask = df['ActivityType'].eq('home_1') & df['Time'].isna()\n\ndf.loc[mask, 'Act_delay'] = 10800\n", "     personId   ActivityType     Time  Act_delay\n0  1473237100  remote_work_4  57651.0    57651.0\n1  1473237100         home_2  59185.0    59185.0\n2  1473237100  remote_work_5  65849.0    65849.0\n3  1473237100         home_1      NaN    10800.0\n"], "link": "https://stackoverflow.com/questions/62936111/check-on-element-in-a-row-in-a-data-frame-in-pandas-is-nan-then-replace-it"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two Dask dataframes, df1 of length 5000 and df2 of length 100000, both with start_time and end_time columns. I am trying to find the df1 rows where df2's start_time-end_time interval is smaller or equal with df1's start_time-end_time interval (df1.start_time <= df2.start_time <= df2.end_time <= df2.end_time). I have tried the following, but to no avail: The first snippet has been running for forever, whilst the second returns Example inputs (df1, df2) and output (df3): Is there a way of achieving this?", "q_apis": "get columns length length start_time end_time columns where start_time end_time start_time end_time start_time start_time end_time end_time first second", "apis": "ge le any", "code": ["dd1 = dd.from_pandas(df1, npartitions=2)\ndd2 = dd.from_pandas(df2, npartitions=2)\n", "def enclosesAny(row, other):\n    return (other.start_time.ge(row.start_time) &\n        other.end_time.le(row.end_time)).any()\n"], "link": "https://stackoverflow.com/questions/59649174/create-subset-dask-dataframe-based-on-another-dask-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "After downloading Facebook data, they provide json files with your post information. I read the json and dataframe with pandas. Now I want to count the characters of every post I made. The posts are in: df['data'] like: [{'post': 'Happy bday Raul'}]. I want the output to be the count of characters of: \"Happy bday Raul\" which will be 15 in this case or 7 in the case of \"Morning\" from [{'post': 'Morning'}]. The columns are Date and Data with this format: I tried to count the characters of this [{'post': 'Morning'}] by doing this But it's not working as result in \"1\". I need to extract the value of the dictionary and do the len to count the characters. The output will be: EDITED: Used to_dict() Output", "q_apis": "get columns count count columns count value count to_dict", "apis": "apply", "code": ["df[1] = pd.Series([x['post'] for x in df[0]]).str.len()\n", "df[1] = df[0].apply(lambda x: x['post']).str.len()\n", "df = pd.DataFrame({0: [{'post': 'Feliz cumplea\u00c3\u00b1os Raul'}],\n 1: [{'post': 'Muchas felicidades Tere!!! Espero que todo vaya genial y siga a\u00c3\u00ban mejor! Un beso desde la Escandinavia profunda'}],\n 2: [{'post': 'Hola!\\nUna investigadora vendr\u00c3\u00a1 a finales de mayo, \u00c2\u00bfAlguien tiene una habitaci\u00c3\u00b3n libre en su piso para ella? Many Thanks!'}],\n 3: [{'post': '\u00c2\u00bfC\u00c3\u00b3mo va todo? Se que muchos est\u00c3\u00a1is o est\u00c3\u00a1bais por Galicia :D\\n\\nOs recuerdo, el proceso de Matriculaci\u00c3\u00b3n tiene unos plazos concretos: desde el lunes 13 febrero hasta el viernes 24 de febrero.'}]\n})\ndf = df.T\ndf[1] = [x['post'] for x in df[0]]\ndf[2] = df[1].str.len()\ndf\nOut[1]: \n                                                   0  \\\n0                 {'post': 'Feliz cumplea\u00c3\u00b1os Raul'}   \n1  {'post': 'Muchas felicidades Tere!!! Espero qu...   \n2  {'post': 'Hola!\nUna investigadora vendr\u00c3\u00a1 a fi...   \n3  {'post': '\u00c2\u00bfC\u00c3\u00b3mo va todo? Se que muchos est\u00c3\u00a1...   \n\n                                                   1    2  \n0                             Feliz cumplea\u00c3\u00b1os Raul   22  \n1  Muchas felicidades Tere!!! Espero que todo vay...  112  \n2  Hola!\\nUna investigadora vendr\u00c3\u00a1 a finales de ...  123  \n3  \u00c2\u00bfC\u00c3\u00b3mo va todo? Se que muchos est\u00c3\u00a1is o est\u00c3\u00a1...  195  \n"], "link": "https://stackoverflow.com/questions/64795714/counting-the-characters-of-dictionary-values-inside-a-dataframe-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Currently trying to change some market values in a dataframe if some value is present in another column. I'm only familiar with list comprehension for creating new columns, but im not sure its possible to do it this way. I tried the following function, though it doesn't work: I tried applying this on the market value column, but no luck. Assume there are two columns a and b in a pd.dataframe, and i would like to create c. Also assume 2 lists, and . For every observation, if is present in then multiply by 0.5 and return this in column . If is present in b then multiply by 0.333 and return in . If not present either return in . I need the following:", "q_apis": "get columns values value columns value columns", "apis": "isin isin values values values", "code": ["df['values_adj'] = np.select([df['country'].isin(list_a), df['country'].isin(list_b)], [df['values']*0.5, df['values']*0.333], df['values'])\n\nprint(df)\n", "values                country  values_adj\n0     200                Denmark       200.0\n1     300         Denmark Sweden       150.0\n2     500  Denmark Sweden Norway       166.5\n3    1000                 Sweden      1000.0\n4     200                 Norway       200.0\n"], "link": "https://stackoverflow.com/questions/55314073/new-column-based-on-conditional"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hi I'm trying to perform a forloop This code works for apple. But how could I do a forloop: The loop needs to: first only keep rows if that respective fruit's column =True perform the 2 calculations, naming the columns and graph with respective `fruit' as the prefix.", "q_apis": "get columns first columns", "apis": "copy drop drop drop drop drop", "code": ["all_fruits = ['apple', 'banana', 'orange']\n\nfor fruit in all_fruits:\n    drop_fruits = all_fruits.copy()\n    drop_fruits.remove(fruit)\n\n    print('to drop:', drop_fruits)\n", "to drop: ['banana', 'orange']\nto drop: ['apple', 'orange']\nto drop: ['apple', 'banana']\n", "all_fruits = ['apple', 'banana', 'orange']\n\nfor fruit in all_fruits:\n    drop_fruits = list(set(all_fruits) - set( [fruit] ))\n\n    print('to drop:', drop_fruits)\n", "new_df = df[ ['concatted', 'score', 'date', 'status', fruit] ]\n", "df[ fruit + '_count' ] = ...\ndf[ fruit + '_high' ] = ...\n", "df[ '{}_count'.format(fruit) ] = ...\ndf[ '{}_high'.format(fruit) ] = ...\n", "df[ f'{fruit}_count' ] = ...\ndf[ f'{fruit}_high' ] = ...\n"], "link": "https://stackoverflow.com/questions/63031051/forloop-python-with-list-naming"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I was wondering if there is an easy way to get only the first row of each grouped object (subject id for example) in a dataframe. Doing this: gives us each one of the rows, but I am interested in doing something like this: Is there a pythonic way to do the above?", "q_apis": "get columns get first", "apis": "groupby apply iloc DataFrame groupby shape append reset_index iloc DataFrame groupby shape append reset_index iloc", "code": ["df.groupby('Subject id').apply(lambda df: df.iloc[0, :])\n", "n = 3\ncol = 'Subject id'\nres_df = pd.DataFrame()\nfor name, df in df.groupby(col):\n    if n < (df.shape[0]):\n        res_df = res_df.append(df.reset_index().iloc[n, :])\n", "def group_by_select_nth_row(df, col, n):\n    res_df = pd.DataFrame()\n    for name, df in df.groupby(col):\n        if n < df.shape[0]:\n            res_df = res_df.append(df.reset_index().iloc[n, :])\n    return res_df\n"], "link": "https://stackoverflow.com/questions/56525178/get-only-first-row-per-subject-in-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i have created a dataframe from python pandas using a numpy array but i want to know how do i add values in specific columns horizontally not vertically let's assume i have this dataframe: how can i add [1.2,3.5,2.2] to the second row of (-1,label) (-2,label) (0,label)?", "q_apis": "get columns array add values columns add second", "apis": "last columns index loc columns last columns index iloc", "code": ["#if need set last 3 columns and index 1\ndf.loc[1, df.columns[-3:]] = [1.2,3.5,2.2]\n", "#if need set last 3 columns and second index\ndf.iloc[1, -3:] = [1.2,3.5,2.2]\n", "#if need set columns by names\ncols = [col1, col3, col5]\ndf.loc[1, cols] = [1.2,3.5,2.2]\n"], "link": "https://stackoverflow.com/questions/66100266/add-values-in-specific-columns-horizontally-in-python-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: As you can see numbers are listed twice from 1-12 / 1-12, instead, I would like to change the index to 1-24. The problem is that when plotting it I see the following: I would like to see a continuous line from 1 to 24.", "q_apis": "get columns index", "apis": "index", "code": ["esta2.index = np.arange(1, len(esta2) + 1)"], "link": "https://stackoverflow.com/questions/59593321/reassign-index-of-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How do you better structure the code in your class so that your class returns the that you want, but you don't have a main method which calls a lot of other methods in sequential order. I find that in a lot of situations I arrive at this structure and it seems bad. I have a that I just overwrite it with the result of other base functions (that I unit test) until I get what I want.", "q_apis": "get columns at test get", "apis": "pipe pipe pipe pipe", "code": ["class A:\n    def main(self):\n        df = self.load_file_into_df()\n\n        df = df.pipe(self.add_x_columns)\\\n               .pipe(self.calculate_y)\\\n               .pipe(self.calculate_consequence)\n\n    return df\n", "from toolz import compose\n\nclass A:\n    def main(self)\n        df = self.load_file_into_df()\n\n        transformer = compose(self.calculate_consequence,\n                              self.calculate_y,\n                              self.add_x_columns)\n\n        df = df.pipe(transformer)\n\n    return df\n"], "link": "https://stackoverflow.com/questions/50680275/better-way-to-structure-a-series-of-df-manipulations-in-your-class"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 1 DataFrame contain 2 columns of string data. i need to compare columns 'NameTest'and'Name'. and i want each name in columns'NameTest' compare too all name in columns 'Name'. and if they matching more than 80% print closest match name. *My dataframe NameTest Name 0 john carry john carrt 1 alex midlane john crat 2 robert patt alex mid 3 david baker alex 4 NaN patt 5 NaN robert 6 NaN david baker My Code Any suggestions? Thank you for your help", "q_apis": "get columns DataFrame columns compare columns name columns compare all name columns name mid", "apis": "DataFrame filter", "code": ["import numpy as np\nimport pandas as pd\nfrom rapidfuzz import process, fuzz\n\ndf = pd.DataFrame({\n\"NameTest\": [\"john carry\", \"alex midlane\", \"robert patt\", \"david baker\", np.nan, np.nan, np.nan],\n\"Name\": [\"john carrt\", \"john crat\", \"alex mid\", \"alex\", \"patt\", \"robert\", \"david baker\"]\n})\n\n# filter out non strings, since they are notsupported by rapidfuzz/fuzzywuzzy/difflib\nNames = [name for name in df[\"Name\"] if isinstance(name, str)]\n\nfor NameTest in df[\"NameTest\"]:\n  if isinstance(NameTest, str):\n    match = process.extractOne(\n      NameTest, Names,\n      scorer=fuzz.ratio,\n      processor=None,\n      score_cutoff=80)\n\n    if match:\n      print(match[0], match[1])\n", "john carrt 90.0\nalex mid 80.0\ndavid baker 100.0\n"], "link": "https://stackoverflow.com/questions/66237662/fuzzy-duplicated-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given the following DataFrame how can I retrieve only the values where IS_TESTED has both True and False values. In the following example, my desired result should be: since 701 & 703 occurs only once.", "q_apis": "get columns DataFrame values where values", "apis": "groupby transform nunique", "code": ["d[d.groupby('ID').IS_TESTED.transform('nunique') > 1]\n\n\n    ID.         IS_TESTED   TEST_NAME\n0   700         True         A\n1   700         False        B\n3   702         False        A\n4   702         True         B\n"], "link": "https://stackoverflow.com/questions/49307001/select-data-frame-result-based-on-row-occurrences"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The top table is what I have and the bottom is what I want. I'm doing this in a Pandas dataframe. Any help would be appreciated. Thanks!", "q_apis": "get columns", "apis": "apply apply", "code": ["df['label'] = df['sentiment'].apply(lambda x: x[0]['label'])\ndf['score'] = df['sentiment'].apply(lambda x: x[0]['score'])\n"], "link": "https://stackoverflow.com/questions/66037049/how-can-i-split-out-this-list-containing-a-dictionary-into-separate-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have an excel file which I imported as a dataframe. I want to loop through the columns of the dataframe. For eg, I want to compare 2nd column with first , then 3rd column with second.I have converted rule_id column into index. This is the data: And this is the code I am using. With this code , I am only able to compare second column with first , I have set the value of n to 0 and also applied the logic , n=n+1 which increments the value of n every time the conditions fulfills. Your help would be much appreciated. I have created these two functions: and I have stored these functions in daframe2 and dataframe3 respectively. I want the result for first comparison between 2nd and 1st column as: And the result for the comparison between column 3rd and 2nd should look like this:", "q_apis": "get columns columns compare first second index compare second first value value time first between between", "apis": "any all loc loc diff astype astype loc diff loc diff", "code": ["In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'rule_id': [53139,51181,50412,50356,50239,50238,50014], 'reqid1':[0,1,0,0,0,1,1],'reqid2':[0,1,1,0,1,1,0],'reqid3':[1,0,1,1,0,0,1]})\n\nIn [3]: df\nOut[3]: \n   rule_id  reqid1  reqid2  reqid3\n0    53139       0       0       1\n1    51181       1       1       0\n2    50412       0       1       1\n3    50356       0       0       1\n4    50239       0       1       0\n5    50238       1       1       0\n6    50014       1       0       1\n\nIn [4]: df['compare_1_and_2'] = df.reqid1 == df.reqid2\n\nIn [5]: df\nOut[5]: \n   rule_id  reqid1  reqid2  reqid3  compare_1_and_2\n0    53139       0       0       1             True\n1    51181       1       1       0             True\n2    50412       0       1       1            False\n3    50356       0       0       1             True\n4    50239       0       1       0            False\n5    50238       1       1       0             True\n6    50014       1       0       1            False\n\nIn [6]: df['compare_2_and_3'] = df.reqid2 == df.reqid3\n\nIn [7]: df\nOut[7]: \n   rule_id  reqid1  reqid2  reqid3  compare_1_and_2  compare_2_and_3\n0    53139       0       0       1             True            False\n1    51181       1       1       0             True            False\n2    50412       0       1       1            False             True\n3    50356       0       0       1             True            False\n4    50239       0       1       0            False            False\n5    50238       1       1       0             True            False\n6    50014       1       0       1            False            False\n", "In [8]: df.compare_1_and_2.any()\nOut[8]: True\n", "In [9]: df.compare_1_and_2.all()\nOut[9]: False\n", "df['solved_prior_1_vs_2'] = np.NaN\ndf['repeated_prior_1_vs_2'] = np.NaN\ndf.loc[(df.compare_1_and_2 == False) & (df.reqid1 == 1),'solved_prior_1_vs_2'] = 100\ndf.loc[(df.compare_1_and_2 == True) & (df.reqid1 == 1),'repeated_prior_1_vs_2'] = 1\n", "In [27]: df[['rule_id','reqid1','reqid2','solved_prior_1_vs_2','repeated_prior_1_vs_2']]\nOut[27]: \n   rule_id  reqid1  reqid2  solved_prior_1_vs_2  repeated_prior_1_vs_2\n0    53139       0       0                  NaN                    NaN\n1    51181       1       1                  NaN                    1.0\n2    50412       0       1                  NaN                    NaN\n3    50356       0       0                  NaN                    NaN\n4    50239       0       1                  NaN                    NaN\n5    50238       1       1                  NaN                    1.0\n6    50014       1       0                100.0                    NaN\n", "def compare_columns(df, col1, col2):\n    repeated_name = \"{}_{}_repeated\".format(col1, col2)\n    solved_name = \"{}_{}_solved\".format(col1, col2)\n    diff = df[col1] == df[col2]\n    col1_is_1 = df[col1] == 1\n    df[repeated_name] = 100\n    df[solved_name] = 1\n    df[repeated_name] = df[repeated_name].astype(int)\n    df[solved_name] = df[sovled_name].astype(int)\n    df.loc[~(diff & col1_is_1), solved_name] = np.NaN\n    df.loc[~(~diff & col1_is_1), repeated_name] = np.NaN\n    return df\n", "In [42]: df1 = compare_columns(df, 'reqid1', 'reqid2')\nIn [43]: df1\nOut[43]: \n   rule_id  reqid1  reqid2  reqid3  reqid1_reqid2_repeated  reqid1_reqid2_solved\n0    53139       0       0       1                     NaN                   NaN\n1    51181       1       1       0                     NaN                   1\n2    50412       0       1       1                     NaN                   NaN\n3    50356       0       0       1                     NaN                   NaN\n4    50239       0       1       0                     NaN                   NaN\n5    50238       1       1       0                     NaN                   1\n6    50014       1       0       1                     100                   NaN\n"], "link": "https://stackoverflow.com/questions/54629699/how-to-loop-through-comparing-one-column-with-its-corresponding-column-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a lisit of DataFrames that come from the census api, i had stored each year pull into a list. So at the end of my for loop i have a list with dataframes per year and a list of years to go along side the for loop. The problem i am having is merging all the DataFrames in the list while also taging them with a list of years. So i have tried using the reduce function, but it looks like it only taking 2 of the 6 Dataframes i have. concat just adds them to the dataframe with out tagging or changing anything so this generates the two seperate list one with the year and other with dataframe. So my output came out to either one Dataframe with missing Dataframe entries or it just concatinated all without changing columns. what im looking for is how to merge all within a list, but datalst[0] to be tagged with yearlst[0] when merging if at all possible", "q_apis": "get columns year at year all concat year all columns merge all at all", "apis": "get DataFrame rename columns astype query dropna assign append", "code": ["for year in range(2012, 2019):\n    c = Census(api_key, year) \n\n    data = c.acs5.get(('NAME', \"B25077_001E\",\"B25064_001E\", \"B15003_022E\",\"B19013_001E\"), \n                      {'for': 'zip code tabulation area:*'}) \n\n    cleandata = (pd.DataFrame(data) \n                   .rename(columns={\"NAME\": \"Name\", \n                                    \"zip code tabulation area\": \"Zipcode\", \n                                    \"B25077_001E\": \"Median_Home_Value\", \n                                    \"B25064_001E\": \"Median_Rent\", \n                                    \"B15003_022E\": \"Bachelor_Degrees\", \n                                    \"B19013_001E\": \"Median_Income\"}) \n                   .astype({'Zipcode':'int64'}) \n                   .query('(Median_Home_Value > 0) & (Median_Rent > 0) & (Median_Income > 0)')\n                   .dropna()\n                   .assign(year_column = year)\n                 )\n\n    datalst.append(cleandata)\n\n\nfinal_data = pd.concat(datalst, ignore_index = True)\n"], "link": "https://stackoverflow.com/questions/57780336/how-to-merge-a-list-of-multiple-dataframes-and-tag-each-column-with-a-another-li"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm looking to apply a function that looks for changes in values pairs between two columns. However i'm unsure how to compare the data in a way that I return a specific value. e.g ( sally gee and vanilla are entered into the database first and therefore they are paired, but now the sally gee's flavor in row three has been changed to chocolate <--- I want to return the row that this occurs ) Test DataFrame Custom Function Pseudo Code Edits after answer I believe we are making progress, I revised the df to show the change in flavor as an error. I essentially would like to pull out row 2 when that change occurs and that's it. In a automated format using a custom function. As of now I display all the rows and have to manually check I just want to get that row where changes occur.", "q_apis": "get columns apply values between columns compare value first now DataFrame now all get where", "apis": "groupby transform count groupby apply assign shift reset_index drop groupby transform first", "code": ["    idNum   name        flavor\n0   id 1    sally gee   vanilla\n1   id 2    sally gee   vanilla\n", "def find_matched_rows(df, find_n_matches = 2):\n    return df[df.groupby(['name','flavor']).transform('count')['idNum']>=find_n_matches]\n", "df2 = df.groupby(['name']).apply(lambda d:d.assign(change = (d['flavor'] != d['flavor'].shift()) )).reset_index(drop = True)\ndf2\n\n", "    idNum    name       flavor     change\n--  -------  ---------  ---------  --------\n 0  id 1     sally gee  vanilla    True\n 1  id 2     sally gee  vanilla    False\n 2  id 3     sally gee  Chocolate  True\n", "\n    idNum   name        flavor      change\n0   id 1    sally gee   vanilla     True\n2   id 3    sally gee   Chocolate   True\n", "df['original flavor'] = df.groupby(['name'])['flavor'].transform('first')\ndf[df['original flavor'] != df['flavor']]\n"], "link": "https://stackoverflow.com/questions/65154294/comparing-columns-for-changes-using-a-custom-function-on-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have array = ['T4U measured','FTI measured', 'lithium'] and need to do value_counts on all these columns. something like df[array].value_counts(), which show histogram of values in columns", "q_apis": "get columns array value_counts all columns array value_counts values columns", "apis": "hist value_counts", "code": ["for i in range(0, len(array)):\n    df[array[i]].hist(bins=df[array[i]].value_counts())\n"], "link": "https://stackoverflow.com/questions/52965887/value-counts-on-array-of-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Dataframe: Here there is id and count columns, how many id's are there where count is 1. If one id has 1,1,1 then i need that id in the result list. if the id has 0,1,1 then never mind. if the id has 0,3,0 then never mind. Expected result: also can i please expect the answer as the function in python.", "q_apis": "get columns count columns where count", "apis": "loc groupby count transform nunique loc groupby count transform nunique", "code": ["df.loc[df.groupby('id')['count'].transform(lambda x: (x.nunique() == 1) & (x == 1)), 'id'].unique()\n", "def unique_list_ids(df, valuecount):\n    resultlist = (df.loc[df.groupby('id')['count']\n                           .transform(lambda x: (x.nunique() == 1) & \n                                                (x == valuecount)), \n                         'id'].unique())\n    return resultlist\n\nunique_list_ids(df, 1)\n"], "link": "https://stackoverflow.com/questions/61605339/get-the-unique-values-of-column-in-dataframe-on-the-other-column-condition-in-py"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to create 3 new columns in a dataframe, which are based on previous pairs information. You can think of the dataframe as the results of comptetion ('xx' column) within diffrerent types ('type' column) at different dates ('date column). The idea is to create the following new columns: (i) numb_comp_past: sum of the number of times every type faced the competitors in the past. (ii) win_comp_past: sum of the win (+1), ties (+0), and loss (-1) of the previous competitions that all the types competing with each other had in the past. (iii) win_comp_past_difs: sum of difference of the results of the previous competitions that all the types competing with each other had in the past. The original dataframe (df) is the following: And it looks like this: The 3 new columns I need to add to the dataframe are shown below (expected output of the Pandas code): Note that: (i) for numb_comp_past if there are no previous competitions I assign a value of 0. On 2018-06-01, for example, the type A has a value of 3 given that he previously competed with type B on 2018-01-01 and 2018-03-01 and with type C on 2018-03-01. (ii) for win_comp_past if there are no previous competitions I assign a value of 0. On 2018-06-01, for example, the type A has a value of -3 given that he previously lost with type B on 2018-01-01 (-1) and 2018-03-01 (-1) and with type C on 2018-03-01 (-1). Thus adding -1-1-1=-3. (iii) for win_comp_past_value if there are no previous competitions I assign a value of 0. On 2018-06-01, for example, the type A has a value of -10 given that he previously lost with type B on 2018-01-01 by a difference of -4 (=1-5) and on 2018-03-01 by a diffrence of -5 (=2-7) and with type C on 2018-03-01 by -1 (=2-3). Thus adding -4-5-1=-10. I really don't know how to start solving this problem. Any ideas of how to solve the new columns decribed in (i), (ii) and (ii) are very welcome.", "q_apis": "get columns columns at date columns sum sum all sum difference all columns add assign value value assign value value assign value value difference start columns", "apis": "get index DataFrame columns values index values stack index index groupby apply ge astype le astype groupby sum cumsum shift assign values sum apply sum apply sum", "code": ["# get differences of pairs, useful for win counts and win_difs\ndef get_diff(x):\n    teams = x.index.get_level_values(1)\n    tmp = pd.DataFrame(x[:,None]-x[None,:],\n                       columns = teams.values,\n                       index=teams.values).stack()\n    return tmp[tmp.index.get_level_values(0)!=tmp.index.get_level_values(1)]\n\nnew_df = df.groupby('date').xx.apply(get_diff).to_frame()\n\n# win matches\nnew_df['win'] = new_df.xx.ge(0).astype(int) - new_df.xx.le(0).astype(int)\n\n# group by players\ngroups = new_df.groupby(level=[1,2])\n\n# sum function\ndef cumsum_shift(x):\n    return x.cumsum().shift()\n\n# assign new values\ndf['num_comp_past'] = groups.xx.cumcount().sum(level=[0,1])\ndf['win_comp_past'] = groups.win.apply(cumsum_shift).sum(level=[0,1])\ndf['win_comp_past_difs'] = groups.xx.apply(cumsum_shift).sum(level=[0,1])\n", "+------------+------+-----+---------------+---------------+--------------------+\n|            |      | xx  | num_comp_past | win_comp_past | win_comp_past_difs |\n+------------+------+-----+---------------+---------------+--------------------+\n| date       | type |     |               |               |                    |\n+------------+------+-----+---------------+---------------+--------------------+\n| 2018-01-01 | A    | 1.0 | 0.0           | 0.0           | 0.0                |\n|            | B    | 5.0 | 0.0           | 0.0           | 0.0                |\n| 2018-02-01 | B    | 3.0 | NaN           | NaN           | NaN                |\n| 2018-03-01 | A    | 2.0 | 1.0           | -1.0          | -4.0               |\n|            | B    | 7.0 | 1.0           | 1.0           | 4.0                |\n|            | C    | 3.0 | 0.0           | 0.0           | 0.0                |\n|            | D    | 1.0 | 0.0           | 0.0           | 0.0                |\n|            | E    | 6.0 | 0.0           | 0.0           | 0.0                |\n| 2018-05-01 | B    | 3.0 | NaN           | NaN           | NaN                |\n| 2018-06-01 | A    | 5.0 | 3.0           | -3.0          | -10.0              |\n|            | B    | 2.0 | 3.0           | 3.0           | 13.0               |\n|            | C    | 3.0 | 2.0           | 0.0           | -3.0               |\n| 2018-07-01 | A    | 1.0 | NaN           | NaN           | NaN                |\n| 2018-08-01 | B    | 9.0 | 2.0           | 0.0           | 3.0                |\n|            | C    | 3.0 | 2.0           | 0.0           | -3.0               |\n| 2018-09-01 | A    | 2.0 | 3.0           | -1.0          | -6.0               |\n|            | B    | 7.0 | 3.0           | 1.0           | 6.0                |\n| 2018-10-01 | C    | 3.0 | 5.0           | -1.0          | -10.0              |\n|            | A    | 6.0 | 6.0           | -2.0          | -10.0              |\n|            | B    | 8.0 | 7.0           | 3.0           | 20.0               |\n| 2018-11-01 | A    | 2.0 | NaN           | NaN           | NaN                |\n| 2018-12-01 | B    | 7.0 | 4.0           | 2.0           | 14.0               |\n|            | C    | 9.0 | 4.0           | -2.0          | -14.0              |\n| 2018-01-01 | A    | 1.0 | 0.0           | 0.0           | 0.0                |\n|            | B    | 5.0 | 0.0           | 0.0           | 0.0                |\n| 2018-02-01 | B    | 3.0 | NaN           | NaN           | NaN                |\n| 2018-03-01 | A    | 2.0 | 1.0           | -1.0          | -4.0               |\n|            | B    | 7.0 | 1.0           | 1.0           | 4.0                |\n|            | C    | 3.0 | 0.0           | 0.0           | 0.0                |\n|            | D    | 1.0 | 0.0           | 0.0           | 0.0                |\n|            | E    | 6.0 | 0.0           | 0.0           | 0.0                |\n| 2018-05-01 | B    | 3.0 | NaN           | NaN           | NaN                |\n| 2018-06-01 | A    | 5.0 | 3.0           | -3.0          | -10.0              |\n|            | B    | 2.0 | 3.0           | 3.0           | 13.0               |\n|            | C    | 3.0 | 2.0           | 0.0           | -3.0               |\n| 2018-07-01 | A    | 1.0 | NaN           | NaN           | NaN                |\n| 2018-08-01 | B    | 9.0 | 2.0           | 0.0           | 3.0                |\n|            | C    | 3.0 | 2.0           | 0.0           | -3.0               |\n| 2018-09-01 | A    | 2.0 | 3.0           | -1.0          | -6.0               |\n|            | B    | 7.0 | 3.0           | 1.0           | 6.0                |\n| 2018-10-01 | C    | 3.0 | 5.0           | -1.0          | -10.0              |\n|            | A    | 6.0 | 6.0           | -2.0          | -10.0              |\n|            | B    | 8.0 | 7.0           | 3.0           | 20.0               |\n| 2018-11-01 | A    | 2.0 | NaN           | NaN           | NaN                |\n| 2018-12-01 | B    | 7.0 | 4.0           | 2.0           | 14.0               |\n|            | C    | 9.0 | 4.0           | -2.0          | -14.0              |\n+------------+------+-----+---------------+---------------+--------------------+\n"], "link": "https://stackoverflow.com/questions/56135088/construct-new-columns-based-on-all-previous-pairs-information-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe that represents a shift schedule for an entire year, given as: Where 1 represents Day shift (06:00 - 18:00), 2 represents Night shift (18:00 - 06:00) and 0 can be ignored. Only a single shift team will be working for a given period. I need the data in a format where the data is indexed by the DateTime stamp with the current working shift, e.g. : What would be the most efficient Pandas method to re-index the data to achieve this, i.e. avoiding for loops?", "q_apis": "get columns shift year shift shift shift where shift index", "apis": "get first first iloc values values first mask mask isna values replace first first index ffill fillna first filter first add first index mask values set_index first append columns first columns columns astype values mask stack columns mask stack rename_axis reset_index add add pop pop index sort_values reset_index drop", "code": ["#get first column by position\nfirst = df.iloc[:, 0]\n#convert column to datetimes with missing values for no datetimes values\ndates = pd.to_datetime(first, errors='coerce')\n#mask for data row\nmask = dates.isna()\n#forward filling missing values and replace first NaNs by first column name\ndf.index = dates.ffill().fillna(pd.to_datetime(first.name))\n#filter out rows with datetimes in first column, add first column to index\ndf = df[mask.values].set_index(first.name, append=True)\n#convert columns names to timedeltas in days, first is 0 days\ndf.columns = pd.to_timedelta(df.columns.astype(int) - 1, unit='D')\n#dictionary for map 1, 2 values\nmapp = {1: pd.Timedelta('06:00:00'), 2:pd.Timedelta('18:00:00')}\n#remove 0 rows with convert to NaN by mask and reshape by stack\n#map by dict and convert MultiIndex to columns\ndf = (df.mask(df == 0)\n       .stack()\n       .map(mapp)\n       .rename_axis(('Datetime','Shift', 'day'))\n       .reset_index(name='td')\n       )\n#add days to hours and add to Datetime\ndf['Datetime'] += (df.pop('td') + df.pop('day'))\n#sorting ans create default index\ndf = df.sort_values(['Datetime','Shift']).reset_index(drop=True)\n", "print (df)\n               Datetime    Shift\n0   2019-01-01 06:00:00  Shift A\n1   2019-01-01 18:00:00  Shift D\n2   2019-01-02 06:00:00  Shift A\n3   2019-01-02 18:00:00  Shift B\n4   2019-01-03 06:00:00  Shift A\n..                  ...      ...\n113 2019-02-26 18:00:00  Shift D\n114 2019-02-27 06:00:00  Shift A\n115 2019-02-27 18:00:00  Shift B\n116 2019-02-28 06:00:00  Shift A\n117 2019-02-28 18:00:00  Shift B\n\n[118 rows x 2 columns]\n"], "link": "https://stackoverflow.com/questions/58340866/what-is-an-efficient-pandas-method-to-reindex-this-shift-schedule"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that has 3 columns and looks like this: The other dataframe looks like this: I need to match the data types of one df to another. Because I have one additional column in df_1 I got an error. My code looks like this: I got an error: What would be a workaround here? I need the dtypes of df_2 to be exactly the same as df_1. Thanks!", "q_apis": "get columns columns dtypes", "apis": "columns", "code": ["df1->that has 3 columns\ndf2->other dataframe\n"], "link": "https://stackoverflow.com/questions/66873335/match-dtypes-of-one-df-to-another-with-different-number-of-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to calculate running difference on the date column depending on \"event column\". So, to add another column with date difference between in event column (there only and ). Spo far I came to this half-working crappy solution Dataframe: Code: But I'm sure there is a more elegant solution. Thanks for any advice. Output format:", "q_apis": "get columns difference date add date difference between any", "apis": "reset_index rename columns index loc fillna bfill DataFrame copy shift fillna merge fillna bfill", "code": ["df = df.reset_index().rename(columns={'index':'idx'})\ndf.loc[df['event']==0, 'idx'] = np.nan\ndf['idx'] = df['idx'].fillna(method='bfill')\n", "df['duration'] = df.groupby('idx')['event'].count() \ndf['duration'] = df['duration'].fillna(method='bfill') \n\n# Alternatively, the previous two lines can be combined as pointed out by OP\n# df['duration'] = df.groupby('idx')['event'].transform('count')\n\ndf = df.drop(columns='idx')\n\nprint(df)\n    date  event  duration\n0      1      0       2.0\n1      2      1       2.0\n2      3      0       3.0\n3      4      0       3.0\n4      5      1       3.0\n5      6      0       5.0\n6      7      0       5.0\n7      8      0       5.0\n8      9      0       5.0\n9     10      1       5.0\n10    11      0       6.0\n11    12      0       6.0\n12    13      0       6.0\n13    14      0       6.0\n14    15      0       6.0\n15    16      1       6.0\n16    17      0       NaN\n", "df = pd.DataFrame({'date':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],'event':[0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0]})\n\ntmp = df[df['event']==1].copy()\ntmp['duration'] = (tmp['date'] - tmp['date'].shift(1)).fillna(tmp['date'])\ndf = pd.merge(df, tmp[['date','duration']], on='date', how='left').fillna(method='bfill')\n"], "link": "https://stackoverflow.com/questions/66009914/dataframe-calculate-difference-in-dates-column-by-another-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframe df1 and df2, where df1 has columns p1,p2 and p3. While dataframe df2 has values assigned for p1, p2 and p3. I would like to create a new dataframe (newdf1) where values in df1 becomes columns and p1,p2 and p3 are new values based on dataframe df2.. Query for df1 and df2 are My df1 looks like this And df2 look like this While my desired newdf1 should look like this (yellow highlight is to show transformation of p1 in the new dataframe)", "q_apis": "get columns where columns values where values columns values", "apis": "melt set_index pivot fillna", "code": ["m = df1.melt('user')\nm['variable'] = m['variable'].map(df2.set_index('N1')['N2'])\nm.pivot('user', 'value', 'variable').fillna(0)\n", "print(m)\n    user variable value\n0  user1       p1     A\n1  user2       p1     C\n2  user3       p1     B\n3  user1       p2     C\n4  user2       p2     E\n5  user3       p2     A\n6  user1       p3     D\n7  user2       p3     A\n8  user3       p3     B\n", "print(m)\n\n    user variable value\n0  user1        2     A\n1  user2        2     C\n2  user3        2     B\n3  user1        3     C\n4  user2        3     E\n5  user3        3     A\n6  user1        4     D\n7  user2        4     A\n8  user3        4     C\n", "value  A  B  C  D  E\nuser                \nuser1  2  0  3  4  0\nuser2  4  0  2  0  3\nuser3  3  2  0  0  0\n"], "link": "https://stackoverflow.com/questions/65860704/how-to-modify-columns-of-a-dataframe-into-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list of string values I read this from a text document with . which yields something like this I have tried this I want to make a dataframe out of this", "q_apis": "get columns values", "apis": "pop first DataFrame columns", "code": ["data = [row.split('|') for row in X]\nheaders = data.pop(0) # Pop the first element since it's header\ndf = pd.DataFrame(data, columns=headers)\n"], "link": "https://stackoverflow.com/questions/58007929/convert-list-of-multiple-strings-into-a-python-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a master dataframe df looks like below: I have another dataframe df looks like below: Expected result: I tried But not working as expected. Any help please?", "q_apis": "get columns", "apis": "set_index join groupby agg reset_index", "code": ["my_df.set_index(['Student', 'Subject']).join(master_df.groupby(['Student', 'Subject']).agg(lambda x: list(x)[-3:])).reset_index()\n", "   Student Subject                                              Score\n0     1000     ENG          [0.35104403633346903, 0.1978421698809576]\n1     1001     ENG                               [0.4159411753678969]\n2     1000    MATH                                                NaN\n3     1002    MATH                                                NaN\n4     1001    MATH  [0.4051361256846634, 0.23072043308688617, 0.67...\n"], "link": "https://stackoverflow.com/questions/68161955/pandas-get-column-values-of-last-n-records-as-a-list-from-another-dataframe-base"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with one column in datetime format and the other columns in integers and floats. I would like to group the dataframe by the weekday of the first column. The other columns would be added. Basically the outcome would be sometime alike: I am flexible if it says exactly Monday, or MO or 01 for the first day of the week, as long it is visible which consumption was done on Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday.", "q_apis": "get columns columns weekday first columns first day week", "apis": "groupby sum", "code": ["import pandas as pd\ndf['Day'] = pd.to_datetime(df['Day'])\ndf.groupby(df['Day'].dt.day_name()).sum()\n"], "link": "https://stackoverflow.com/questions/56992292/group-dataframe-with-datetime-column-by-weekday"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas DataFrame with multiple offset columns: Is it possible to efficiently group/sort the columns by the values of the cells while inserting NaN, -1, or some other value in the missing locations? Output: The columns don't need to be sorted in any particular manner, I'm just looking to create timeline plots for each of the above values. e.g.", "q_apis": "get columns DataFrame columns columns values value columns any values", "apis": "mul shape replace plot shape columns", "code": ["df1 = pd.get_dummies(df.stack().astype('int64')).max(level=0)\n\n   532201   577834   577837   649835   649839   649845   839785   839786   1003273\n0        1        1        1        0        0        0        0        1        1\n1        0        1        1        1        0        0        0        1        1\n2        0        1        1        1        0        0        0        1        1\n3        0        1        1        0        1        1        1        1        1\n4        0        1        1        0        1        0        0        1        1\n5        0        1        1        0        1        0        0        1        1\n6        0        1        1        0        1        0        0        1        1\n", "import matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(4, 4))\n\n(df1.mul(np.arange(1, df1.shape[1]+1)).replace(0, np.NaN)\n    .plot(ax=ax, marker='o', legend=False))\n\nplt.yticks(np.arange(1, df1.shape[1]+1, 1))\nax.set_yticklabels(df1.columns)\nplt.show()\n"], "link": "https://stackoverflow.com/questions/66164134/pandas-grouping-cell-values-by-value-into-individual-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm working with a model, and after splitting into train and test, I want to apply StandardScaler(). However, this transformation converts my data into an array and I want to keep the format I had before. How can I do this? Basically, I have: How can I get back to the format that had? Update: I don't want to get to reverse back to before being scaled. I just want to be a dataframe in the easiest possible way.", "q_apis": "get columns test apply array get get", "apis": "columns DataFrame columns DataFrame transform columns", "code": ["import pandas as pd\n\ncols = X_train.columns\nsc = StandardScaler()\nX_train_sc = pd.DataFrame(sc.fit_transform(X_train), columns=cols)\nX_test_sc = pd.DataFrame(sc.transform(X_test), columns=cols)\n"], "link": "https://stackoverflow.com/questions/64161419/how-can-i-convert-the-standardscaler-transformation-back-to-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have to combine several files (roughly 40), that have several variations of headers (ie, headers in different columns depending on the file, some files with a few column names that don't show up in others, etc.). I have a python script that works to combined the files, but it simply puts them in the same order found in the original file. I want this script to be able to add a new column when a new column name shows up, and map all future occurrences of that column name with the respective row. An example of my desired output is below, where the 'Gross Commission' and 'Payouts' columns only show up in the 2019 July file, and the '%' and '$' columns only show up in the 2018 June file (and all of the other columns showing up in both): *Each file is for a different 'Period'. Current Code: Thank you for the help, and let me know if I can provide any more detail.", "q_apis": "get columns combine columns names add name map all name where columns columns all columns Period any", "apis": "DataFrame append to_excel index", "code": ["import pandas as pd\nimport os\nimport tkinter as tk\nfrom tkinter import filedialog\n\nroot = tk.Tk()\nroot.withdraw()\n\nin_path = filedialog.askdirectory()\nlisting = os.listdir(in_path)\n\nfiles_xlsx = [f for f in listing if f[-4:] == 'xlsx']\n\ndf = pd.DataFrame()\n\nfor infile in listing:\n    file_data = pd.read_excel(in_path + '/' + infile,\n                              header=0,\n                              encoding = \"ANSI\")\n    df = df.append(file_data, sort=False)\n\n\nout_path = in_path + ' Combined.xlsx'\n\nwriter = pd.ExcelWriter(out_path, engine='xlsxwriter')\n\ndf.to_excel(writer,\n            sheet_name='Combined',\n            index=False,\n            header=True)\n\nwriter.save()\n"], "link": "https://stackoverflow.com/questions/64625521/how-can-i-combined-multiple-excel-files-with-different-headers-into-one-sheet-us"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am plotting a dataframe that looks like this. The code for plotting is here. This plots a line plot for each of the years listed in the dataframe over each day in the year. However, I would like to make the line for 2020 much thicker than the others so it stands out more clearly. Is there a way to do that using this one line of code? Or do I need to manually plot all of the years such that I can control the thickness of each line separately? A current picture is attached, where the line thicknesses are all the same.", "q_apis": "get columns plot day year plot all where all", "apis": "plot", "code": ["fig, ax = plt.subplots()\ndf.plot(figsize=(20,12), title='Arctic Sea Ice Extent', \n        lw=3, fontsize=16, ax=ax, grid=True)\n\nfor line in ax.get_lines():\n    if line.get_label() == '2020':\n        line.set_linewidth(15)\nplt.show()\n"], "link": "https://stackoverflow.com/questions/62470743/change-line-width-of-specific-line-in-line-plot-pandas-matplotlib"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How do i add a boolean if the value is lower than the value for the row above, per fruit? And if so the average of the previous 5 years", "q_apis": "get columns add value value", "apis": "groupby diff lt apply rolling mean shift where cummax", "code": ["g = df.groupby('fruit')\ndf['lower_than_before'] = g['value'].diff().lt(0)\ndf['5 year avg'] = g['value'].apply(lambda x: x.rolling(5).mean().shift().where(df['lower_than_before'].cummax()))\ndf\n", "    fruit  year  value  lower_than_before  5 year avg\n0   apple  1950      2              False         NaN\n1   apple  1951      3              False         NaN\n2   apple  1952      3              False         NaN\n3   apple  1953      4              False         NaN\n4   apple  1954      5              False         NaN\n5   apple  1955      4               True         3.4\n6  banana  1950    333              False         NaN\n7  banana  1951    335              False         NaN\n"], "link": "https://stackoverflow.com/questions/64105108/groupby-for-value-before-previous-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I currently have a dataframe, where an uniqueID has multiple dates in another column. I want extract the hours between each date, but ignore the weekend if the next date is after the weekend. For example, if today is friday at 12 pm, and the following date is tuesday at 12 pm then the difference in hours between these two dates would be 48 hours. Here is my dataset with the expected output: This is what I have so far, but it includes the weekends: Thanks!", "q_apis": "get columns where between date date today at date at difference between", "apis": "sort_values groupby shift mask notnull loc mask mask apply where where fillna DataFrame size size sort_values groupby shift T sum round mask notnull loc mask mask apply sort_values groupby shift mask notnull loc mask mask apply where where fillna mean std mean std", "code": ["df[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf = df.sort_values(['UniqueID','Date'])\n\ndf[\"shifted\"] = df.groupby([\"UniqueID\"])[\"Date\"].shift(-1)\ndf[\"hours1\"] = df[\"Date\"].dt.floor('d') \ndf[\"hours2\"] = df[\"shifted\"].dt.floor('d') \n\nmask = df['shifted'].notnull()\nf = lambda x: np.busday_count(x['hours1'] + pd.Timedelta(1, unit='d'), x['hours2'])\ndf.loc[mask, 'hours3'] = df[mask].apply(f, axis=1) * 24\n\nmask1 = df['hours1'].dt.dayofweek < 5\nhours1 = df['hours1'] + pd.Timedelta(1, unit='d') - df['Date']\ndf['hours1'] = np.where(mask1, hours1, np.nan) / np.timedelta64(1 ,'h')\n\nmask1 = df['hours2'].dt.dayofweek < 5\ndf['hours2'] = np.where(mask1, df['shifted']-df['hours2'], np.nan) / np.timedelta64(1 ,'h')\n\ndf['date_diff'] = df['hours1'].fillna(0) + df['hours2'] + df['hours3']\n", "print (df)\n  UniqueID                Date ExpectedOutput             shifted  hours1  \\\n0        A 2018-12-07 10:30:00           28.0 2018-12-10 14:30:00    13.5   \n1        A 2018-12-10 14:30:00           27.0 2018-12-11 17:30:00     9.5   \n2        A 2018-12-11 17:30:00            Nan                 NaT     6.5   \n3        B 2018-12-14 09:00:00           48.0 2018-12-18 09:00:00    15.0   \n4        B 2018-12-18 09:00:00           74.0 2018-12-21 11:00:00    15.0   \n5        B 2018-12-21 11:00:00            NaN                 NaT    13.0   \n6        C 2019-01-01 15:00:00           96.0 2019-01-07 15:00:00     9.0   \n7        C 2019-01-07 15:00:00            NaN                 NaT     9.0   \n\n   hours2  hours3  date_diff  \n0    14.5     0.0       28.0  \n1    17.5     0.0       27.0  \n2     NaN     NaN        NaN  \n3     9.0    24.0       48.0  \n4    11.0    48.0       74.0  \n5     NaN     NaN        NaN  \n6    15.0    72.0       96.0  \n7     NaN     NaN        NaN  \n", "np.random.seed(2019)\n\ndates = pd.date_range('2015-01-01','2018-01-01', freq='H')\ndf = pd.DataFrame({\"UniqueID\": np.random.choice(list('ABCDEFGHIJ'), size=100),\n                   \"Date\": np.random.choice(dates, size=100)})\nprint (df)\n", "def old(df):\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    df = df.sort_values(['UniqueID','Date'])\n\n    df[\"shifted\"] = df.groupby([\"UniqueID\"])[\"Date\"].shift(-1)\n\n    def f(x):\n        a = pd.date_range(x['Date'],  x['shifted'], freq='T')\n        return ((a.dayofweek < 5).sum() / 60).round()\n\n\n    mask = df['shifted'].notnull()\n    df.loc[mask, 'date_diff'] = df[mask].apply(f, axis=1)  \n    return df\n", "def new(df):\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    df = df.sort_values(['UniqueID','Date'])\n\n    df[\"shifted\"] = df.groupby([\"UniqueID\"])[\"Date\"].shift(-1)\n    df[\"hours1\"] = df[\"Date\"].dt.floor('d') \n    df[\"hours2\"] = df[\"shifted\"].dt.floor('d') \n\n    mask = df['shifted'].notnull()\n    f = lambda x: np.busday_count(x['hours1'] + pd.Timedelta(1, unit='d'), x['hours2'])\n    df.loc[mask, 'hours3'] = df[mask].apply(f, axis=1) * 24\n\n    mask1 = df['hours1'].dt.dayofweek < 5\n    hours1 = df['hours1'] + pd.Timedelta(1, unit='d') - df['Date']\n    df['hours1'] = np.where(mask1, hours1, np.nan) / np.timedelta64(1 ,'h')\n\n    mask1 = df['hours2'].dt.dayofweek < 5\n    df['hours2'] = np.where(mask1, df['shifted'] - df['hours2'], np.nan) / np.timedelta64(1 ,'h')\n\n    df['date_diff'] = df['hours1'].fillna(0) + df['hours2'] + df['hours3']\n    return df\nprint (new(df))\nprint (old(df))\n", "In [44]: %timeit (new(df))\n22.7 ms \u00b1 115 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [45]: %timeit (old(df))\n1.01 s \u00b1 8.03 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"], "link": "https://stackoverflow.com/questions/54721914/difference-of-datetimes-in-hours-excluding-the-weekend"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a database from max mind.which is giving me location information from IP. I have written the below function to retrieve city and country from the ip :- I am processing this every minute and applying to a column in pandas:- The problem is it's taking more than 3 minutes to execute in every iteration I am getting around 150,000 rows and i am applying the function on each of them. I want to complete this operation in less than a minute. Any advice.", "q_apis": "get columns max minute minute", "apis": "apply apply", "code": [">>> # This creates a Reader object. You should use the same object\n>>> # across multiple requests as creation of it is expensive.\n", "def country(ipa, reader):\n    try:\n        response = reader.city(ipa)\n        response = response.country.iso_code\n        return response\n    except:\n        return 'NA'\n\ndef city(ipa, reader):\n    try:\n        response = reader.city(ipa)\n        response = response.city.name\n        return response\n    except:\n        return 'NA'\n", "with geoip2.database.Reader('/home/jupyter/GeoIP2-City.mmdb') as reader:\n    df['country'] = df['raddr'].apply(country, reader=reader)\n    df['city'] = df['raddr'].apply(city, reader=reader)\n"], "link": "https://stackoverflow.com/questions/64112136/making-apply-fast-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In the pandas package of python I'd like to group by, so that I keep a specific order. The below code seems to do this, but is there a faster / simpler way? Or is it guaranteed, that the group by function of pandas keeps the order of records in the source dataframe?", "q_apis": "get columns", "apis": "groupby product apply join reset_index product", "code": ["df2 = df1.groupby('id', sort=False)['product'].apply(sep.join).reset_index()\nprint (df2)\n   id                 product\n0   1   apple - pear - banana\n1   2  orange - apple - lemon\n"], "link": "https://stackoverflow.com/questions/52493360/how-to-group-by-in-pandas-keeping-a-specific-order"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There are two tables like: , , , ... I would like to create a new table with only the people listed that are present in both tables. If I try to lookup by looping an \"isin\"-Method, I can look for a match for one column, but I would like to match both columns at the same time.", "q_apis": "get columns lookup isin columns at time", "apis": "DataFrame columns DataFrame columns merge", "code": ["df1 = pd.DataFrame(np.array([['Jack', 'Brown', '1980-01-01'], ['Joe', 'Doe', '1990-02-02']\n                            , ['John', 'Jones', '2000-03-03']])\n                   , columns=['first_name', 'last_name', 'birth_date'])\n", "first_name last_name birth_date\nJack       Brown     1980-01-01\nJoe        Doe       1990-02-02\nJohn       Jones     2000-03-03\n", "df2 = pd.DataFrame(np.array([['Jack', 'Brown', '2020-01-29'], ['Joe', 'Smith', '1999-09-09']\n                            , ['Sarah', 'Morphy', '2011-11-11']])\n                   , columns=['first_name', 'last_name', 'birth_date'])\n", "first_name last_name birth_date\nJack       Brown     2010-10-10\nJoe        Smith     1999-09-09\nSarah      Morphy    2011-11-11\n", "result = pd.merge(df1, df2, how=\"inner\", on=[\"first_name\", \"last_name\"])\n", "first_name last_name birth_date_x birth_date_y\nJack       Brown     1980-01-01   2010-10-10\n"], "link": "https://stackoverflow.com/questions/65534148/merge-dataframes-on-multiple-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list of list of dictionary that I would like to be transformed into a dataframe but I can't seem to make my code work. Currently, this is my example list The larger list has around 9000 lists of dictionary. And I would like it to be transformed into Any help would be great! I am a beginner and hence a little unsure on how to proceed. This is a repost of a deleted post as I posted unrelated code in that post.", "q_apis": "get columns", "apis": "aggregate append DataFrame columns items columns columns columns", "code": ["from collections import defaultdict\n\n# aggregate data by order\nd = defaultdict(list)\nfor L in eglist:\n    for row in L:\n        d[row['order']].append([row['var1'], row['name_id'], row['team']])\n\n# convert to dataframe\ndf = pd.concat([pd.DataFrame(v, columns=[f'{k}_var1', f'{k}_name_id', f'{k}_team'])\n                for k, v in d.items()], axis=1)\n\n# convert columns to MultiIndex\nheaders = [('Order '+col.split('_')[0], col.split('_')[-1]) for col in df.columns]\ndf.columns = pd.MultiIndex.from_tuples(headers, names=['Team', 'Data'])\n\nprint(df)\n\nTeam Order 0          Order 1          Order 2        \nData    var1  id team    var1  id team    var1 id team\n0      False  23    0   False  24    0   False  1    0\n1      False  23    0   False  24    0   False  1    0\n"], "link": "https://stackoverflow.com/questions/51379302/list-of-list-of-dictionary-to-multiindex-dataframe-with-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hi I have the following two pandas dataframes: df1 and df2. I want to create a new dataframe, df3 such that it is the same as df1 but with one extra column called \"new price\". The way I want new price to be populated is to return the first price with the same code from df2 that is greater than or equal to the price in df1. Here are the dataframes: df1: df2: So as an example let is consider the first entry in df1 So the column new price should look at all prices with code X in df2 and return the smallest price from df2 that is greater than or equal to 4.3. In this case it is 4.5. Repeat this for each line to get df3: Does anyone know how to achieve this, I have tried pandas merge but that didn't work.", "q_apis": "get columns first first at all get merge", "apis": "assign merge assign drop query groupby first reset_index", "code": ["m=(df1.assign(key=1).merge(df2.assign(key=1),on='key',suffixes=('','_y')).drop('key', 1)\n                                            .query(\"(Code==Code_y)&(Price<=Price_y)\"))\nm.groupby(['Code','Price'],sort=False)['Price_y'].first().reset_index(name='New Price')\n", "  Code  Price  New Price\n0    X   4.30        4.5\n1    X   2.50        2.5\n2    X   4.00        4.0\n3    X   1.50        1.5\n4    X   0.24        0.5\n5    X   1.00        1.0\n6    X   1.30        1.5\n7    Y   3.90        4.0\n8    Y   2.60        3.0\n"], "link": "https://stackoverflow.com/questions/57257033/how-to-add-a-new-column-to-a-pandas-df-that-returns-the-smallest-value-that-is-g"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I don't understand why this renaming operation affects the original dataframe when the copy command is used. Why is df_copy a view of df and not really a copy? I would expect the print statement to output 'x' not 'y'.", "q_apis": "get columns copy view copy", "apis": "columns columns", "code": ["df_copy.columns = ['y']\nprint(df.columns)\n#Index([u'x'], dtype='object')\n"], "link": "https://stackoverflow.com/questions/50562606/renaming-pandas-dataframe-column-on-copy-affects-the-original-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The program below imports thousands of stock tickers from a .CSV file to a list and passes the tickers as a parameter to a function which pulls the 'Adjusted Close' column of that particular stock and sets the ticket as the column name. This was I have one dataframe containing the thousands of stocks and can obtain the adjusted close just by using df['EnterTickerNameHere']. The data from yahoo contains a 'Volume' field which I would like to only add a stock to my df if the volume is greater than 100,000. I am not sure how to do this while keeping my dataframe in the same format it is now. Thank you for reading!", "q_apis": "get columns name contains add now", "apis": "first apply T iloc index", "code": ["def getTickers(min_volume=100000):\n    today = str(datetime.date.today())\n    fourty_days_prior = str(datetime.date.today() - datetime.timedelta(days=40))\n\n    # import the tickers from csv to a python list\n    tickers = pd.read_csv('tickers.csv', sep='\\n',dtype={'Tickers' : str})\n    tickers = tickers.Tickers.tolist()\n\n    # first import the latest volumes and apply your condition\n    volumes = pdr.get_data_yahoo(tickers, start=today, end=today)['Volume'].T\n    filtered_tickers = volumes[volumes.iloc[:,0] > min_volume].index.tolist()\n\n    data = pdr.get_data_yahoo(filtered_tickers, start= fourty_days_prior, end=today)['Adj Close']\n    return data\n"], "link": "https://stackoverflow.com/questions/52008668/pandas-dataframe-yahoo-finance-checking-if-volume-meets-criteria"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Assume that I have a pandas dataframe called similar to: I'm currently able to output a JSON file that iterates through the various sources, creating an object for each, with the code below: This yields the following output: Essentially, what I want to do is also iterate through those list of sources and add the table objects corresponding to each source respectively. My desired output would look similar to as follows: Any assistance on how I can modify my code to also iterate through the tables column and append that to the respective source values would be greatly appreciated. Thanks in advance.", "q_apis": "get columns add append values", "apis": "groupby agg items groupby apply items", "code": ["data = [\n    {k: v} \n    for k, v in df.groupby('source')['tables'].agg(\n        lambda x: {v: {} for v in x}).items()\n]\n\nwith open('data.json', 'w') as f:\n    json.dump(data, f, indent=2)  \n", "[\n  {\n    \"src1\": {\n      \"table1\": {},\n      \"table2\": {},\n      \"table3\": {}\n    }\n  },\n  {\n    \"src2\": {\n      \"table1\": {},\n      \"table2\": {}\n    }\n  }\n]\n", "df['tables2'] = 'abc'\n\ndef func(g): \n    return {x: y for x, y in zip(g['tables'], g['tables2'])}\n\ndata = [{k: v} for k, v in df.groupby('source').apply(func).items()]\ndata\n# [{'src1': {'table1': 'abc', 'table2': 'abc', 'table3': 'abc'}},\n#  {'src2': {'table1': 'abc', 'table2': 'abc'}}]\n"], "link": "https://stackoverflow.com/questions/60478009/convert-pandas-dataframe-to-2-layer-nested-json-using-groupby"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to format the result of a data frame to look more readable. The df looks like this: The lack_of_minutes df is type timedelta64[ns]. To avoid the error TypeError: Timedelta('0 days 01:08:12') is not JSON serializable i returned the json like this: Basically it just converts everything it doesn't know to strings But the format that it's returning the time it's like this: I would like to exclude the 0 days and return only the time. to be more readable. My first idea was to slice the df but didn't work. What is the correct approach to do this. I am using python 2.x", "q_apis": "get columns Timedelta days time days time first", "apis": "apply", "code": ["import pandas as pd\n\n# create a dummy Timedelta\nd = datetime.datetime(2017,11,21,12,30) - datetime.datetime(2017,11,20,23,59)\n\nprint str(pd.to_timedelta(d))[-8:]\n", "# new column is a string HH:MM:SS\nlack_of_minutes['NewPeriod'] = lack_of_minutes['Period'].apply(lambda x: str(x)[-8:])\n"], "link": "https://stackoverflow.com/questions/47416164/extract-specific-strings-from-the-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The above code outputs: And at the end of the logs (after all other processes), it also outputs: Why does it print when I try to log df.info()? How can I get df.info() at the intended location in my logs?", "q_apis": "get columns at all info get info at", "apis": "info info info", "code": ["from io import StringIO\nbuf = StringIO()\ndf.info(buf=buf)\n\nlogger.info(type(df))    \nlogger.info(buf.getvalue())\n"], "link": "https://stackoverflow.com/questions/61024263/python-logging-does-not-log-pd-info"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to decrease the processing time of the function below. Is there a module I could import that could decrease the processing time for this function? It calculates the standard deviation each of the the iterating 10000 values one by one. Although the function is fast I am looking to perhaps decrease the processing time by half. The function turns the calculations to numpy arrays at the end. Variables: Function Performance: Sample of the data frame:", "q_apis": "get columns time time values time at", "apis": "std var sum var sum var var var std rolling std dropna std mean std mean std", "code": ["from numba import njit\n\n@njit\ndef std(a, k):\n    n = len(a)\n    m = n - k + 1\n    k_ = k\n    mu  = np.zeros(m, np.float64)\n    var = np.zeros(m, np.float64)\n    mu[0]  = a[:k].sum() / k\n    var[0] = ((a[:k] - mu[0]) ** 2).sum() / k_\n\n    for i in range(1, m):\n        old = a[i-1]\n        new = a[i+k-1]\n        d = (new - old)\n        mu[i] = mu[i-1] + d / k\n        old_ = mu[i-1]\n        new_ = mu[i]\n        var[i] = var[i-1] + d * (new + old + new_ + old_) / k_\n    return mu, var ** 0.5\n", "np.random.seed([3, 14])\ns = pd.Series(np.random.randn(1_000_000))\n", "mu, sig = std(s.to_numpy(), 1000)\n", "%timeit s.rolling(1000).std().dropna()\n%timeit pd.Series(std(s.to_numpy(), 1000)[1])\n\n28 ms \u00b1 189 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n10.3 ms \u00b1 42.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/66757752/optimizing-pandas-function-for-faster-results-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to read a text file with two columns in Pandas. One of the column datatypes is JSON. I want to convert this column into a list of lists or just a list. Input: Expected Output: I tried the following code but getting an error:", "q_apis": "get columns columns", "apis": "items apply", "code": ["def convert_json(row):\n    return [[k] + v[0] for k,v in json.loads(row).items()]\n    \ndf['time'] = df['time'].apply(convert_json)\n", "    bank                                               time\n0    ABC    [[Monday, 9:00, 18:00], [Tuesday, 9:00, 18:00]]\n"], "link": "https://stackoverflow.com/questions/65419467/how-to-read-a-column-of-datatype-json-and-convert-into-list-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe and it contains some values like my desired output is I have tried to use but nothing worked, can anyone help? Thanks", "q_apis": "get columns contains values", "apis": "apply replace apply replace", "code": ["              col1           col2\n0           value1  the change ()\n1    the change ()         value3\n2           value2         value4\n3  () other change            NaN\n", "df = df.apply(lambda x: x.str.replace(r\"\\s*\\(\\)\\s*\", \"\", regex=True))\nprint(df)\n", "           col1        col2\n0        value1  the change\n1    the change      value3\n2        value2      value4\n3  other change         NaN\n", "              col1            col2\n0           value1  the change (\u2660)\n1   the change (\u29bb)          value3\n2           value2          value4\n3  () other change             NaN\n", "df = df.apply(lambda x: x.str.replace(r\"\\s*\\(.*?\\)\\s*\", \"\", regex=True))\nprint(df)\n", "           col1        col2\n0        value1  the change\n1    the change      value3\n2        value2      value4\n3  other change         NaN\n"], "link": "https://stackoverflow.com/questions/67890804/replace-from-a-dataframe-pandas-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataset: I have used ngrams to analyse text and words/sentences frequency. Since I am interested in finding in which text a particular word/words sequence was used, I would need to create an association between text (i.e. ) and text with particular ngrams. For example: I am interested in finding texts which contains , i.e. and . To find the n-grams for one single word, I have been using this: However, I do not know how to find the associated to the text and how to select more words in the function above. How could I do this? Any idea? Please feel free to change tags, if you require. Thanks", "q_apis": "get columns between contains select", "apis": "loc assign replace apply explode loc isin", "code": ["import re\ntxt = 'save the date'\nprint (f'The ngrams \"{txt}\" is in IDs: ',\n       df.loc[df['Text'].str.contains(txt, flags=re.IGNORECASE), 'ID'].tolist())\nThe ngrams \"save the date\" is in IDs:  [31, 451]\n", "txt = 'save the date'\n\ndf_ = df.assign(ngrams=df.Text.str.replace(r'[^\\w\\s]+', '') #remove punctuation\n                              .str.lower() # lower case\n                              .str.split() #split over a whitespace\n                              .apply(lambda x: list(ngrams(x, 3))))\\\n        .explode('ngrams') #create a row per ngram\n\nprint (df_.loc[df_['ngrams'].isin(ngrams(txt.lower().split(), 3))])\n    ID                                               Text             ngrams\n2   31       Time for a Royal Celebration! Save the date.  (save, the, date)\n5  451  Save the date, we\u2019re hosting a fabulous & fun ...  (save, the, date)\n"], "link": "https://stackoverflow.com/questions/62123967/selecting-ids-rows-within-a-databrame-based-on-n-grams"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let us say I have this data frame. df Based on the line and priority column values (when the are the same or duplicate as shown above), I want to combine to_line values. The proposed result should look like the following. I tried something like this but I couldn't get what I want. Could you please help to figure out this? I appreciate your time and effort.", "q_apis": "get columns values combine values get time", "apis": "groupby agg", "code": ["df = pd.DataFrame({\n    'line': [10,10,50,60,50],\n    'to_line': [20,30,40,70,80],\n    'priority': [1,1,3,2,3] \n})\n\narray_agg = lambda x: '/'.join(x.astype(str))\n\ngrp_df = df.groupby(['line', 'priority']).agg({'to_line': array_agg})\n", "grp_df = df.groupby(['line', 'priority'], as_index=False).agg({'to_line': array_agg})\n", "              to_line\nline priority        \n10   1          20/30\n50   3          40/80\n60   2             70\n"], "link": "https://stackoverflow.com/questions/58435058/combine-column-values-based-on-the-other-column-values-in-pandas-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying generate strings or atleast a different dataframe from a dataframe that I have. The one that I have is: I am trying to format the above snippet to strings such that it looks like this What it should do is that It should take the MM/DD/YY data where the value of TEST is and combine all the values in TEST upto each and make a string for each occurrence of . The raw data that I used to get till this Dataframe was different and was lot of work. But now I am kinda stuck on how to get this format. Any ideas/suggestions will be appreciated. Thanks :)", "q_apis": "get columns take where value combine all values get now get", "apis": "groupby shift eq cumsum agg last join reset_index drop", "code": ["(df.groupby(df.TEST.shift().eq('<LF>').cumsum())\n   .agg({'MM/DD/YYhh:mm:ss.ms.us':'last',\n         'TEST':''.join})\n   .reset_index(drop=True) \n)\n", "     MM/DD/YYhh:mm:ss.ms.us              TEST\n0  04/17/2013:44:18.220.187  SUPPLYPR<CR><LF>\n1  04/17/2013:44:18.722.812        UL<CR><LF>\n2  04/17/2013:44:19.238.100       ERR<CR><LF>\n3  04/17/2013:44:19.652.156     MODE?<CR><LF>\n4  04/17/2013:44:20.161.604        TP<CR><LF>\n5  04/17/2013:44:20.643.865        PR<CR><LF>\n"], "link": "https://stackoverflow.com/questions/61375202/segment-dataframe-every-time-a-value-repeats-in-python-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am reading excel file (specific one sheet), it looks very much like this. I would like to remove all the numbers, underscore and hyphens under 'Org' columns. Output under 'Org' should be and so on. I tried below to remove numbers but it's not working .. Any help will be appreciated.!", "q_apis": "get columns all columns", "apis": "DataFrame replace", "code": ["import pandas as pd\n\ndf = pd.DataFrame({\"Org\": [\"14_ddc_-_systems\", \"14_ddc_-_systems\", \"23_kbf_org\"]})\ndf[\"New\"] = df[\"Org\"].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\nprint(df)\n", "                Org          New\n0  14_ddc_-_systems  ddc systems\n1  14_ddc_-_systems  ddc systems\n2        23_kbf_org      kbf org\n"], "link": "https://stackoverflow.com/questions/51833930/how-should-i-remove-special-characters-from-data-frame-except-space"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am reading a xls file with pandas, then did some transformation to change the column names and drop some values, and now I need to transform this dataframe to a dictionary or json in the format required by Zendesk and send it via rest Here is what I already done: This is the dataFrame And this is the format that i need to build the dict/json", "q_apis": "get columns names drop values now transform", "apis": "to_dict columns columns columns columns columns columns", "code": ["di = df.to_dict('split')\n\n\nticket['ticket'] = {di['columns'][0]:di['data'][0][0], \n                    di['columns'][1]:di['data'][0][1],\n                    di['columns'][2]:di['data'][0][2],\n                    di['columns'][3]:di['data'][0][3],\n                    di['columns'][4]:di['data'][0][4],\n                    di['columns'][5]:di['data'][0][5]}\n", "{'ticket': {'subject': 'Atendimento - J\u00e9ssica - Erro no C\u00f3digo de Barras dos Boletos',\n  'created_at': '2018-12-14T16:14:00Z',\n  'type': 'question',\n  'priority': 'low',\n  'status': 'closed',\n  'description': 'PROBLEMA:A cliente entrou com problemas na gera\u00e7\u00e3o dos boletos, o c\u00f3digo de barras estava desconfigurado.\\xa0SOLU\u00c7\u00c3O:Acessamos a maquina da cliente e instalamos as fontes do windows AZALEIA, ap\u00f3s isso ficou correto.',\n  'custom_fields': [{'id': 360018333334,\n    'value': 'adm__compras_e_licita\u00e7\u00f5es'}]}}\n", "{'tickets': [{'subject': 'Atendimento - J\u00e9ssica - Erro no C\u00f3digo de Barras dos Boletos',\n  'created_at': '2018-12-14T16:14:00Z',\n  'type': 'question',\n  'priority': 'low',\n  'status': 'closed',\n  'description': 'PROBLEMA:A cliente entrou com problemas na gera\u00e7\u00e3o dos boletos, o c\u00f3digo de barras estava desconfigurado.\\xa0SOLU\u00c7\u00c3O:Acessamos a maquina da cliente e instalamos as fontes do windows AZALEIA, ap\u00f3s isso ficou correto.',\n  'custom_fields': [{'id': 360018333334,\n    'value': 'adm__compras_e_licita\u00e7\u00f5es'}]}\n   {'subject': 'Atendimento - J\u00e9ssica - Erro no C\u00f3digo de Barras dos Boletos',\n  'created_at': '2018-12-14T16:14:00Z',\n  'type': 'question',\n  'priority': 'low',\n  'status': 'closed',\n  'description': 'PROBLEMA:A cliente entrou com problemas na gera\u00e7\u00e3o dos boletos, o c\u00f3digo de barras estava desconfigurado.\\xa0SOLU\u00c7\u00c3O:Acessamos a maquina da cliente e instalamos as fontes do windows AZALEIA, ap\u00f3s isso ficou correto.',\n  'custom_fields': [{'id': 360018333334,\n    'value': 'adm__compras_e_licita\u00e7\u00f5es'}]\n   }\n  ]\n}\n"], "link": "https://stackoverflow.com/questions/55828317/get-xls-data-with-dataframe-and-send-it-to-a-api-post"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe column on which I would like to perform binning, for example: I want one column for the bin range and one column for the label, as follows: Apparently, setting the parameter as follows would just result in a column for bin labels, but not for the range anymore. Is there a more elegant solution to this instead of running 2 times for the 2 columns? Thanks", "q_apis": "get columns columns", "apis": "bool drop drop groupby apply", "code": ["retbins: bool, default False\n    Whether to return the bins or not. Useful when bins is provided as a scalar.\n", "bins: numpy.ndarray or IntervalIndex.\n    The computed or specified bins. Only returned when\n    retbins=True. For scalar or sequence bins, this is\n    an ndarray with the computed bins. If set\n    duplicates=drop, bins will drop non-unique bin. For\n    an IntervalIndex bins, this is equal to bins.\n", "assignments, edges = pd.cut(df.X, bins=5, labels=False, retbins=True)\ndf['label'] = assignments\ndf['bin_floor'] = edges[assignments]\ndf['bin_ceil'] = edges[assignments + 1]\n", "def assign_dynamic_bin_ids_and_labels(\n    df,\n    value_col,\n    nbins,\n    label_col='label',\n    bin_floor_col='bin_floor',\n    bin_ceil_col='bin_ceil',\n):\n    assignments, edges = pd.cut(\n        df[value_col], bins=5, labels=False, retbins=True\n    )\n\n    df[label_col] = assignments\n    df[bin_floor_col] = edges[assignments]\n    df[bin_ceil_col] = edges[assignments + 1]\n\n    return df\n\ndf.groupby('id').apply(assign_dynamic_bin_ids_and_labels, 'X', 5)\n"], "link": "https://stackoverflow.com/questions/61778610/output-both-bins-and-labels-column-in-pandas-binning"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i have the following problem and had an idea to solve it, but it didn't worked: I have the data on DAX Call and Put Options for every trading day in a month. After transforming and some calculations I have the following DataFrame: DaxOpt. The goal is now to get rid of every row (either Call or Put Option) which does not have the respective pair. With pair I mean a Call and Put Option with the same 'EXERCISE_PRICE' and 'TAU', where 'TAU' = the time to maturity in years. The red boxes in the picture are examples for a pair. So either having a DataFrame with only the pairs or having two DataFrames with Call and Put Options where the rows are the respective pairs. My idea was creating two new DataFrames one which contains only the Call Options and the other the Put Options, sort them after 'TAU' and 'EXERCISE_PRICE' and working my way through with pandas isin function, in order to get rid of the Call or Put Options which do not have the respective pair. DaxOptCall = DaxOpt[DaxOpt.CALL_PUT_FLAG == 'C'] DaxOptPut = DaxOpt[DaxOpt.CALL_PUT_FLAG == 'P'] The problem is that the DaxOptCall and DaxOptPut have different dimensions, so isin function is not applicable. I am trying to find the most efficient way, since the data I am using now is just a fraction from the real data. Would appreciate any help or idea.", "q_apis": "get columns day month DataFrame now get mean where time DataFrame where contains isin get isin now any", "apis": "columns set_index set_index", "code": ["# Assuming your unique columns are TAU and EXERCISE_PRICE\ndf_call = df_call.set_index([\"EXERCISE_PRICE\", \"TAU\"])\ndf_put = df_put.set_index([\"EXERCISE_PRICE\", \"TAU\"])\n"], "link": "https://stackoverflow.com/questions/59127233/splitting-dataframe-into-two-dataframes-and-filter-these-two-dataframes-in-order"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have list of pandas dataframes with two columns, basically class and value: df1: Name Count Bob 10 John 20 df2: Name Count Mike 30 Bob 40 There might be same \"Names\" in different dataframes, there might be no same \"Names\", and list contains over 100 dataframes. But in each dataframe all \"Names\" are unique. What I need is to iterate over all dataframes and create one big one, where presented all values from \"Names\" and their total sums of \"Count\" from all the dataframes, so like: result: Name Count Bob 50 John 20 Mike 30 Bob's data is summed, others are not, as they are only present once. Is there efficient way once there are many dataframes?", "q_apis": "get columns columns value contains all unique all where all values all", "apis": "where", "code": ["df = pd.concat(dfs) # where dfs is a list of dataframes \n"], "link": "https://stackoverflow.com/questions/65551089/how-to-sum-list-of-pandas-dataframes-by-with-respect-to-given-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My pandas dataframe has this: I want to split by endTime (which is in secs) and count number of times the name changes within that block of 1 minute. So my output should be Any help is appreaciated Thanks", "q_apis": "get columns count name minute", "apis": "first ne shift groupby sum max ne shift groupby sum", "code": ["# substract a very small amount so that 60 is first minute\nminutes = (df['endTime']-1e-9)//60\ndf['name'].ne(df['name'].shift()).groupby(minutes).sum()\n", "endTime\n0.0    4\n1.0    5\nName: name, dtype: int64\n", "minutes = pd.cut(df['endTime'], bins=np.arange(0,df.endTime.max()+60, 60))\ndf['name'].ne(df['name'].shift()).groupby(minutes).sum()\n", "endTime\n(0.0, 60.0]      4\n(60.0, 120.0]    5\nName: name, dtype: int64\n"], "link": "https://stackoverflow.com/questions/64285518/identify-number-of-times-the-values-changes-in-a-pandas-column-within-time-in-ot"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe and I'd like to calculate the percentage difference compared to the beginning value. Is there a way to use pct_change() to complete this task? e.g my data I'd like the change to appear like", "q_apis": "get columns difference value pct_change", "apis": "iloc", "code": ["df['change'] = (df.close/df.close.iloc[0] - 1) * 100\n", "         date  close  change\n0  2020-05-08    100     0.0\n1  2020-05-11    102     2.0\n2  2020-05-12    108     8.0\n"], "link": "https://stackoverflow.com/questions/61775495/calculating-percentage-change-in-pandas-dataframe-compared-to-baseline-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Dataframe as below: I need to find the result color in column group of (Col_1, Col_2, Col_3, Col_4, Col_5, Col_6) where the color is not Zero. Two possible condition can exist in above dataframe: Only one out of 6 columns will be Non Zero. If all columns have Zero Value then result will be Zero. I want the Output as below:", "q_apis": "get columns where columns all columns", "apis": "columns apply join values astype replace replace", "code": ["df['result'] = df[[c for c in df.columns if 'Col_' in c]].apply(lambda row: ''.join(row.values.astype(str)), axis=1).str.replace('[^a-zA-Z]', '').replace('',0)\n", "        SYS  Date_Time Col_1 Col_2  Col_3  Col_4 Col_5 Col_6 result\n0   SYSTEM1 2021-01-07     0     0      0      0     Y     0      Y\n1   SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n2   SYSTEM1 2021-01-07     R     0      0      0     0     0      R\n3   SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n4   SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n5   SYSTEM1 2021-01-07     0     R      0      0     0     0      R\n6   SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n7   SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n8   SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n9   SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n10  SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n11  SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n12  SYSTEM1 2021-01-07     0     0      0      0     0     G      G\n13  SYSTEM1 2021-01-07     0     0      0      0     R     0      R\n14  SYSTEM1 2021-01-07     0     0      0      0     0     0      0\n"], "link": "https://stackoverflow.com/questions/65654227/find-result-as-non-zero-value-in-group-of-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have found questions about replacing values in a column, but I don't know how to specifically replace each value with its column mean. For example, in the code provided, how would I replace each -1 with the mean of the column the -1 was found in? I'm pretty new to python and don't know where to go", "q_apis": "get columns values replace value mean replace mean where", "apis": "mean", "code": ["df = df.mask(df.eq(-1), df.mean(), axis=1)\nprint (df)\n          A          B          C\n0  1.000000   2.000000  20.000000\n1  2.000000   4.000000   8.285714\n2  4.000000   5.142857   4.000000\n3  3.857143   8.000000   8.000000\n4  6.000000  10.000000  19.000000\n5  7.000000   6.000000   1.000000\n6  8.000000   7.000000   7.000000\n", "print (df.mean())\nA    3.857143\nB    5.142857\nC    8.285714\ndtype: float64\n"], "link": "https://stackoverflow.com/questions/58558363/in-python-how-can-i-replace-a-specific-value-in-a-data-frame-with-its-column-me"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "As is now, I have rows from one dataframe (dataset) extracted into my variable (train). I would also like some rows from the dataset2. How would I do something similar to append certain rows from dataset2 onto train?", "q_apis": "get columns now append", "apis": "loc append loc", "code": ["dataset = pd.read_csv('winequality-red.csv')\ndataset2 = pd.read_csv('winequality-white.csv')\ntrain = dataset.loc[0:1450,:]\n\ntrain = train.append(dataset2.loc[0:1000,:],ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/67675800/how-to-extract-rows-from-multiple-pandas-dataframes-into-one-variable"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes with identifiers for physical facilities. I then have a list of facilities. I would like to return only the locations used in both dataframes from my master list. I am attempting to get the following to work, and it appears I have something in the syntax incorrect, or am not using isin properly. I have tried a few variations of this - while there are many examples in the documents that specifically show how to scan two lists using dictionaries etc. I am having trouble sourcing the quickest/most direct way to return only values that are in the two specified columns in the other dataframes.", "q_apis": "get columns get isin values columns", "apis": "isin", "code": ["ids = set(filtered_departments.buildingid.tolist() +\\\nfiltered_stores.facilityid.tolist())\n\nmy_locations = filtered_locations[filtered_locations['id'].isin(ids)]\n"], "link": "https://stackoverflow.com/questions/58273178/using-isin-to-return-values-from-two-specific-columns-from-two-pandas-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to organize a big dataset by \"correct\" and \"incorrect\" answers. The condition for correct answer is this: If the rows meet those conditions, I want to append them to a df_correct: If the conditions are not met, I need the rows to be appended to another dataframe. I thought of looping through the rows of the dataframe but this doesn't seem to be working: This is how part of the df looks: Since all of them match the condition, they should go to the df_correct. What's the correct way to do this? Thank you.", "q_apis": "get columns append all", "apis": "loc loc loc loc DataFrame", "code": ["c1 = df['color'].eq('green') & df['correct'].eq('v')\nc2 = df['color'].eq('blue') & df['correct'].eq('a')\nc3 = df['color'].eq('red') & df['correct'].eq('r')\nm = c1 | c2 | c3\n", "correct_df = df.loc[m]\nincorrect_df = df.loc[~m]\n", "correct_df = df.loc[m, ['word', 'rt']]\nincorrect_df = df.loc[~m, ['word', 'rt']]\n", "df = pd.DataFrame({'color': ['green', 'blue', 'red'] * 2,\n                   'correct': ['v', 'r', 'v', 'a', 'a', 'r'],\n                   'word': list('abcdef'),\n                   'rt': range(1, 7)})\n", "   color correct word  rt\n0  green       v    a   1\n1   blue       r    b   2\n2    red       v    c   3\n3  green       a    d   4\n4   blue       a    e   5\n5    red       r    f   6\n", "correct_df = df.loc[m]\n\n   color correct word  rt\n0  green       v    a   1\n4   blue       a    e   5\n5    red       r    f   6\n\ncorrect_df = df.loc[m, ['word', 'rt']]\n\n  word  rt\n0    a   1\n4    e   5\n5    f   6\n", "incorrect_df = df.loc[~m]\n\n   color correct word  rt\n1   blue       r    b   2\n2    red       v    c   3\n3  green       a    d   4\n\nincorrect_df = df.loc[~m, ['word', 'rt']]\n\n  word  rt\n1    b   2\n2    c   3\n3    d   4\n"], "link": "https://stackoverflow.com/questions/68198298/appending-to-new-df-if-items-in-rows-meet-conditions-between-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a .csv file with some data. There is only one column of in this file, which includes timestamps. I need to organize that data into bins of 30 minutes. This is what my data looks like: So in this case, the last two data points would be grouped together in the bin that includes all the data from 13:30 to 14:00. This is what I have already tried I am getting around 7000 rows showing all hours for all days with the count next to them, like this: I want to create bins for only the hours that I have in my dataset. I want to see something like this: Thanks in advance!", "q_apis": "get columns last all all all days count", "apis": "groupby size reset_index", "code": ["df['Timestamp'] = pd.to_datetime(df['Timestamp'])\ndf = df.Timestamp.dt.floor('30min').dt.time.to_frame()\\\n                 .groupby('Timestamp').size()\\\n                 .reset_index(name='Count')\n", "df = df.Timestamp.dt.floor('30min').dt.time.value_counts().reset_index(name='Count')\n\n\nprint(df)\n  Timestamp  Count\n0  11:00:00      1\n1  13:00:00      1\n2  13:30:00      2\n3  16:30:00      1\n"], "link": "https://stackoverflow.com/questions/54309436/group-data-into-bins-of-30-minutes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list with name of organizations like this: and another list of cu values like this: When i'm trying to convert it to dataframe it's giving me error message saying, \"ValueError: 2 columns passed, passed data had 1 columns\" My code to convert list ot dataframe: Where am i going wrong? Thanks in advance!", "q_apis": "get columns name values columns columns", "apis": "DataFrame DataFrame columns", "code": ["df = pd.DataFrame({'name of issuer': name, 'cusip':cu})  \n", "df = pd.DataFrame(list(zip(name, cu)), columns=['name of issuer', 'cusip'])\n", "print (df)\n   name of issuer      cusip\n0    ALPHABET INC  02079K305\n1  AMAZON COM INC  023135106\n2       APPLE INC  037833100\n"], "link": "https://stackoverflow.com/questions/60751819/valueerror-2-columns-passed-passed-data-had-1-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a daraframe as below: I want to calculate the sum of continious non zero values of Column (Fn) I want my result dataframe as below:", "q_apis": "get columns sum values", "apis": "eq cumsum ne groupby cumsum", "code": ["groups = df.Fn.eq(0).cumsum()\ndf['Sum'] = df.Fn.ne(0).groupby(groups).cumsum()\n", "groups = df.Fn.eq(0).cumsum()\n\n#    groups  Fn (Fn added just for comparison)\n# 0       1   0\n# 1       1   1\n# 2       2   0\n# 3       2   1\n# 4       2   1\n# 5       3   0\n# 6       4   0\n# 7       4   1\n# 8       4   1\n# 9       4   1\n", "df['Sum'] = df.Fn.ne(0).groupby(groups).cumsum()\n\n#        Datetime     Data  Fn  Sum\n# 0  18747.385417  11275.0   0    0\n# 1  18747.388889   8872.0   1    1\n# 2  18747.392361   7050.0   0    0\n# 3  18747.395833   8240.0   1    1\n# 4  18747.399306   5158.0   1    2\n# 5  18747.402778   3926.0   0    0\n# 6  18747.406250   4043.0   0    0\n# 7  18747.409722   2752.0   1    1\n# 8  18747.420139   3502.0   1    2\n# 9  18747.423611   4026.0   1    3\n"], "link": "https://stackoverflow.com/questions/67342807/conditional-sum-of-non-zero-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to iterate through a row in dataframe and on every iteration I want to change some values in the dictionary. I did the same with a column and the output was fine but when i use a row it only iterates once. what should I do. here is a part of my code- this is a loop inside another loop ie an inner loop. count is being incremented on every iteration of the outer loop. Now this row has 15 values. but it is only iterating once. and at once all the values in it are showing. I have done same thing with a column and it iterated as a list should. what should I do iterate it, like a column?", "q_apis": "get columns values count values at all values", "apis": "iloc count values count", "code": ["for c in df.iloc[count,4:16].values: # the count+1 upper bound is not necessary\n    if(c < 40):\n       no_of_failed_assessment = no_of_failed_assessment +1\n"], "link": "https://stackoverflow.com/questions/51610742/when-i-am-making-a-list-out-of-a-row-in-a-dataframe-it-is-iterating-only-once-i"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Still a newbie with Python just trying to learn this stuff. Appreciate any help. Right now when I connect to Alpha Vantage I get the full range of data for all the dates and it looks like this I found some good sources for guides, but I keep getting empty dataframes or errors This is how the code looks so far", "q_apis": "get columns any now get all empty", "apis": "index index", "code": ["stock_ticker = 'SPY'\napi_key = 'apikeyddddd'\n\nts = TimeSeries (key=api_key, output_format = \"pandas\")\ndata_daily, meta_data = ts.get_daily_adjusted(symbol=stock_ticker, outputsize ='full')\n\ntest = data_daily[(data_daily.index > '2014-01-01') & (data_daily.index <= '2017-08-15')]\n\nprint(data_daily)\nprint(test)\n\n"], "link": "https://stackoverflow.com/questions/63966086/how-to-filter-for-dates-range-in-timeseries-or-dataframe-using-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame containing strings. I would like to create another DataFrame that indicates whether the string contains a specific month through one-hot encoding. Using the below as an example: I am looking to produce the following sort of DataFrame: I have tried the following but I get a value error and it also would not produce the desired DataFrame: Thanks in advance!", "q_apis": "get columns DataFrame DataFrame contains month DataFrame get value DataFrame", "apis": "explode", "code": ["df = pd.concat([df[\"User\"], df.Months.str.split(r\"[,;]\")], axis=1).explode(\n    \"Months\"\n)\nprint(pd.crosstab(df[\"User\"], df[\"Months\"]))\n", "Months   August   December   February   January  August  January  March  October\nUser                                                                            \n1             0          0          1         0       0        1      0        0\n2             1          0          0         0       0        0      1        0\n3             0          0          0         1       0        0      0        1\n4             0          1          0         0       1        0      0        0\n"], "link": "https://stackoverflow.com/questions/67270393/python-dataframe-one-hot-encode-rows-containing-a-specific-substring"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My code looks like: I was expecting that this would simply change the value for the column 'Country Name', but it is changing the values of all the columns. For instance all column in the row with 'Korea, Rep.' have been changed to the value 'South Korea' in columns 49-59. The resulting df looks something like:", "q_apis": "get columns value values all columns all value columns", "apis": "loc loc loc set_index iloc replace", "code": ["GDP = pd.read_excel(\"GDP_in.xls\", skiprows=4)\nGDP.loc[GDP['Country Name'] == 'Korea, Rep.', 'Country Name'] = 'South Korea'\nGDP.loc[GDP['Country Name'] == 'Iran, Islamic Rep.', 'Country Name'] = 'Iran'\nGDP.loc[GDP['Country Name'] == 'Hong Kong SAR, China', 'Country Name'] = 'Hong Kong'\nGDP = GDP.set_index(['Country Name'])\nGDP = GDP.iloc[:, 49:59]\n", "df['Country Name'].replace(\n                         ['Korea, Rep.', 'Iran, Islamic Rep.', 'Hong Kong SAR, China'], \n                         ['South Korea', 'Iran', 'Hong Kong'],\n                         inplace = True)\n"], "link": "https://stackoverflow.com/questions/50178072/loc-replaces-all-columns-values-not-just-the-indexed-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am using sanpy to gather crypto market data, compute alpha, beta and rsquared with statsmodels, and then create a crypto = input(\"Cryptocurrency: \") function with a while loop that allows me to ask the user for an specific crypto and output its respective statistics, followed by showing the input again. With the following code I receive the error: ValueError: If using all scalar values, you must pass an index The error is in the following line: I tried solving the issue by changing it to: But with that, it gave me a different error: AttributeError: 'dict' object has no attribute 'mean'. The goal is to create a single DataFrame with the datatime column, columns for the cryptos and their pct.change data, an additional column for MKT Return with the daily mean from all cryptos' pct.change. Then, use all this data to calculate each crypto's statistics and finally create the input function mentioned at the beginning. I hope I made myself clear and that someone is able to help me with this matter.", "q_apis": "get columns all values index mean DataFrame columns mean all all at", "apis": "get get query pct_change empty get all DataFrame get add at", "code": ["import san\nimport pandas as pd\n\n# List of data we are interested in    \ncryptos = [\"bitcoin\", \"ethereum\", \"ripple\", \"bitcoin-cash\", \"tether\",\n\"bitcoin-sv\", \"litecoin\", \"binance-coin\", \"eos\", \"chainlink\",\n\"monero\", \"bitcoin-gold\"]\n\n# function to get the data from san into a dataframe and turn in into\n# a daily percentage change\ndef get_and_process_data(c):\n    raw_data = san.get(\"daily_closing_price_usd/\" + c, from_date=\"2014-12-31\", to_date=\"2019-12-31\", interval=\"1d\") # \"query/slug\"\n    return raw_data.pct_change()[1:]\n\n# now set up an empty dataframe to get all the data put into\ndf = pd.DataFrame()\n# cycle through your list\nfor c in cryptos:\n    # get the data as percentage changes\n    dftemp = get_and_process_data(c)\n    # then add it to the output dataframe df\n    df[c] = dftemp['value']\n\n# have a look at what you have\nprint(df)\n"], "link": "https://stackoverflow.com/questions/65500769/error-using-santiment-sanpy-library-for-cryptocurrency-data-analysis"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames F1 and F2 containing both the column id1, id2. F1 contains two columns . F2 contains three column [id1,id2,Description] I wante to test if exists in OR F2['id2']F1['id2'] then i must addd a colmun in F1 with Description of this id1 or id2 in F2` . The contens of F1 and F2 are are HERE. The Output That im attending on F1 is also HERE I created F1 and F2 like This Actually i tried several solutions . But nothing helps me to do it . Help please", "q_apis": "get columns contains columns contains test", "apis": "replace values replace replace drop_duplicates dropna set_index drop_duplicates dropna set_index", "code": ["#if necessary replace string NaN to missing values\nF1 = F1.replace('NaN', np.nan)\nF2 = F2.replace('NaN', np.nan)\n", "s1 = F2.drop_duplicates('id1').dropna(subset=['id1']).set_index('id1')['Description']\ns2 = F2.drop_duplicates('id2').dropna(subset=['id2']).set_index('id2')['Description']\n", "F1['Description'] = F1['id1'].map(s1).combine_first(F1['id2'].map(s2))\nprint (F1)\n    id1  id2 Description\n0   x22  NaN  California\n1   x13  223          LA\n2   NaN  788          NY\n3  x421  NaN      Munich\n"], "link": "https://stackoverflow.com/questions/55435919/check-the-existence-of-value-on-other-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm new and learning of machine learning, kindly bear with me if the way of asking is not fine and question is so simple. The issue is my developed model is returning loss as nan, please advice me if anything wrong I made. below are the details. Program Logic I'm getting the output", "q_apis": "get columns", "apis": "add add", "code": ["model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(1, activation='relu'))\n\nmodel.add(tf.keras.layers.BatchNormalization(input_shape = (13,)))\n\nmodel.compile(optimizer='sgd', loss='mse')\n"], "link": "https://stackoverflow.com/questions/54966616/loss-nan-when-build-a-model-for-bike-sharing"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset with several columns. But for this question only two of them are important. The column Body and the column Valid, the first is a comment in twitter and the second is the output of a ML algorithm that determines if it is valid or not for the project that I am working on. The problem is that I have a list of tweets from the Body column that have being predicted wrongly. What I want to do is to change that value on Valid column if the body column coincides with any of the values inside wrong_one(which is a list). So taking into account that wrong_one is a list and that raw_data is my dataframe. I have tried this: OUT: TypeError: 'Series' objects are mutable, thus they cannot be hashed", "q_apis": "get columns columns first second value any values Series", "apis": "loc isin", "code": ["wrong_one = ['LOL1', 'LOL']\n\nraw_data.loc[raw_data['BODY'].isin(wrong_one), 'VALID'] = False\n"], "link": "https://stackoverflow.com/questions/53006031/change-a-value-of-a-column-when-it-another-column-value-is-equal-to-one-of-the-v"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to somehow hash the strings of the dataframe's fields. I have this df: And it looks like this: I am trying to create a dictionary or another form of data structure that would be useful in this case, in this way: so that I would finally have the cars and all the associated colors matched, like in this example: I tried this code but I don't know how to continue...: It does not have to be a dictionary, it could be anything, just to have all these key -values matched. I just thought that this data structure would fit. How to make this work? Thanks a lot!!!!", "q_apis": "get columns all all values", "apis": "groupby agg", "code": ["x = df.groupby(\"Cars\", as_index=False).agg(list)\nout = dict(zip(x.Cars, x[\"Included Colors\"]))\nprint(out)\n", "{'Audi': ['yellow'], 'Fiat': [nan], 'Mercedes': [nan, 'orange'], 'Renault': ['green'], 'Tesla': ['red', nan, 'black']}\n"], "link": "https://stackoverflow.com/questions/67115484/how-to-hash-the-strings-of-a-dataframe-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's say I have the following dataframe, for each month separately I have a bunch of data, stores in arrays for three variables : What I ultimately want to do is to make a scatterplot between Y and X1 for month 01 with markers in darkblue, for month two in lightblue, and so on. Maybe I also want the scatterplot for Y and X2 in different shades of red as well in the same plot.. I tried this one: but get the message that there are no numeric objects to plot... Are the Nan values a problem??? Any ideas?! Thanks a lot for helping!", "q_apis": "get columns month between month month plot get plot values", "apis": "plot loc loc", "code": ["import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\ndata =  {\"ID\":[0,1,2,3,4],\n         \"Y\":[np.array([2,4,6,8]), \n              np.array([np.nan,4,6,8]),\n              np.array([3,4,5,6]), \n              np.array([1,2,3,4]), \n              np.array([2,4,6,8])],\n        \"X1\":[np.array([2,4,6,8]), \n              np.array([1,2,5,4]),\n              np.array([1,9,7,7]), \n              np.array([5,6,7,8]), \n              np.array([2,4,6,8])],\n        \"X2\":[np.array([2,4,6,8]), \n              np.array([4,3,3,3]),\n              np.array([2,2,6,np.nan]), \n              np.array([9,9,np.nan,6]), \n              np.array([2,4,6,8])],\n        \"month\":[1,2,3,4,5]\n}\n\n\ndf = pd.DataFrame(data)\n\ncheck = 0\nfor v in range(len(df[\"Y\"])):\n    val_y = df[\"Y\"][v]\n    val_x1 = df[\"X1\"][v]\n    val_x2 = df[\"X2\"][v]\n    ID = df[\"ID\"][v]\n    month = df[\"month\"][v]\n\n    if check == 0:\n        helper_dat = {\"ID\":ID,\"Y\":list(val_y),\"X1\":list(val_x1),\"X2\":list(val_x2),\"month\":month}\n        new_df = pd.DataFrame(helper_dat)\n    else:\n        helper_dat = {\"ID\":ID,\"Y\":list(val_y),\"X1\":list(val_x1),\"X2\":list(val_x2),\"month\":month}\n        helper = pd.DataFrame(helper_dat)\n        new_df = new_df.append(helper,ignore_index=True)   \n    check += 1\n", "    ID    Y  X1   X2  month\n0    0  2.0   2  2.0      1\n1    0  4.0   4  4.0      1\n2    0  6.0   6  6.0      1\n3    0  8.0   8  8.0      1\n4    1  NaN   1  4.0      2\n5    1  4.0   2  3.0      2\n6    1  6.0   5  3.0      2\n7    1  8.0   4  3.0      2\n8    2  3.0   1  2.0      3\n9    2  4.0   9  2.0      3\n10   2  5.0   7  6.0      3\n11   2  6.0   7  NaN      3\n12   3  1.0   5  9.0      4\n13   3  2.0   6  9.0      4\n14   3  3.0   7  NaN      4\n15   3  4.0   8  6.0      4\n16   4  2.0   2  2.0      5\n17   4  4.0   4  4.0      5\n18   4  6.0   6  6.0      5\n19   4  8.0   8  8.0      5\n", "plt.scatter(new_df[\"X1\"],new_df[\"Y\"],c=new_df[\"month\"], marker='^',label=\"X1\")\nplt.scatter(new_df[\"X2\"],new_df[\"Y\"],c=new_df[\"month\"], marker='o',label=\"X2\")\nplt.legend()\n", "sc = plt.scatter(new_df[\"X1\"],new_df[\"Y\"],c=new_df[\"month\"], marker='^',label=\"X1\")\nplt.scatter(new_df[\"X2\"],new_df[\"Y\"],c=new_df[\"month\"], marker='o',label=\"X2\")\nlp = lambda i: plt.plot([],color=sc.cmap(sc.norm(i)),\n                        label=\"Month {:g}\".format(i))[0]\nhandles = [lp(i) for i in np.unique(new_df[\"month\"])]\nplt.legend(handles=handles,bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n"], "link": "https://stackoverflow.com/questions/53079200/make-scatterplot-from-arrays-within-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have recently started using dataframes in Python and I don't know how can I do the following exercise. I have two dataframes, both with the same columns ( column and column) like this: : Index Type Count 0 Album 12 1 Book 4 2 Person 3 : Index Type Count 0 Album 9 1 Person 4 2 Film 4 Same value can have different value, as you can see with ( in and in ). I want to have all data in . In this case result will be: : Index Type Count 0 Album 21 1 Book 4 2 Person 7 3 Film 4 If column value in is already in , simply sum the corresponding value of . If column value in is not in , add that row ( and value) at the end of . Hope you can help me with this. Thanks in advance.", "q_apis": "get columns columns Index Index value value all Index value sum value value add value at", "apis": "groupby agg sum reset_index", "code": ["main_df = pd.concat([main_df,df2]).groupby('Type', sort=False).agg({'Count': sum}).reset_index()\n", "     Type  Count\n0   Album     21\n1    Book      4\n2  Person      7\n3    Film      4\n"], "link": "https://stackoverflow.com/questions/67859547/merge-two-dataframes-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2-dimensional data (Column-Cell1,Cell2.., Row-Gene1,Gene2..) in which I want to delete rows with 99% zeroes and with the resultant matrix drop columns with 99% zeroes in them. I have written the following code to do the same, however since the matrix is very large, it is taking a long time to run. Is there a better way to approach this issue?", "q_apis": "get columns delete drop columns time", "apis": "columns sum columns sum index values size size columns size size columns DataFrame columns size sum max sum columns max columns", "code": ["def drop_almost_zero(df, percentage):\n    row_cut_off = int(percentage/100*len(df.columns))\n    df = df[(df==0).sum(axis='columns') <= row_cut_off]\n\n    column_cut_off = int(percentage/100*len(df)) \n    b = (df == 0).sum(axis='rows')\n    df = df[ b[ b <= column_cut_off].index.values ]\n\n    return df\n\n\n#test\nsize = 50\npercentage = 90\n\nrows = size//2\ncolumns = size\n\na = np.random.choice(2, size=(rows, columns), p=[(1-0.1), 0.1]) \ndf = pd.DataFrame(a, columns=[f'c{i}' for i in range(size)])\n\ndf = drop_almost_zero(df,percentage)\n\nassert (df == 0).sum(axis='rows').max() <= percentage/100*rows\nassert (df == 0).sum(axis='columns').max() <=  percentage/100*columns\n"], "link": "https://stackoverflow.com/questions/55352193/drop-columns-and-rows-with-certain-percentage-of-0s-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Dataframe which has a column for Minutes and correlated value, the frequency is about 79 seconds but sometimes there is missing data for a period (no rows at all). I want to detect if there is a gap of 25 or more Minutes and delete the dataset if so. How do I test if there is a gap which is? The dataframe looks like this: So there is a irregular but short gap and one that exceeds 25 Minutes. In this case I want the dataset to be empty: I am quite new to Python, especially to Pandas so an explanation would be helpful to learn.", "q_apis": "get columns value seconds at all delete test empty", "apis": "DataFrame drop index diff diff any", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'minutes': [23.000, 24.185, 27.250, 55.700, 56.790]})\n\nnp.roll(df['minutes'], 1)                                                   \n# output: array([56.79 , 23.   , 24.185, 27.25 , 55.7  ])\n", "df['rolled_minutes'] = np.roll(df['minutes'], 1)\ndropped_df = df.drop(index=0)\ndiff = dropped_df['minutes'] - dropped_df['rolled_minutes']\n(diff > 25).any()\n# output: True\n"], "link": "https://stackoverflow.com/questions/61124875/find-gaps-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to covert \"Quantity\" column to int. The quantity column has a string(,) divider or separator for the numerical values using code I am getting this error: Data It's a pandas dataframe with 124964 rows. I added the head and tail of the data What can I do to fix this problem?", "q_apis": "get columns values head tail", "apis": "apply replace astype", "code": ["# Replace string \",\" with \"\"\n\ndata[\"Quantity\"] = data[\"Quantity\"].apply(lambda x: str(x.replace(',','')))\n\ndata['Quantity'] = data['Quantity'].astype('float')\n"], "link": "https://stackoverflow.com/questions/57969740/how-to-replace-string-separator-in-numerical-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe in pandas, with a column which is a vector: and I wish to split and divide it into elements which would look like this: I have tried but with no luck. any help would be appreciated.", "q_apis": "get columns any", "apis": "DataFrame values index index", "code": ["df[['A','B','C']] = pd.DataFrame(df.averages.values.tolist(), index= df.index)\n"], "link": "https://stackoverflow.com/questions/52650041/python-split-pandas-numeric-vector-column-into-multiple-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: I want to convert this dataframe to have data each half hour, and inpute each new position with the mean between the previous and the following value (or any similar interpolation), that is, for example: Is there any pandas/datetime function to assist in this operation? Thank you", "q_apis": "get columns hour mean between value any any", "apis": "set_index", "code": ["df['datetime'] = pd.to_datetime(df['datetime'])\ndf.set_index('datetime', inplace=True)\n", "                       temp\ndatetime                   \n2015-01-01 00:00:00  11.220\n2015-01-01 00:30:00  11.270\n2015-01-01 01:00:00  11.320\n2015-01-01 01:30:00  11.310\n2015-01-01 02:00:00  11.300\n2015-01-01 02:30:00  11.275\n2015-01-01 03:00:00  11.250\n2015-01-01 03:30:00  11.285\n2015-01-01 04:00:00  11.320\n"], "link": "https://stackoverflow.com/questions/66016698/how-to-convert-hourly-data-to-half-hourly"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My pandas dataframe consists of a categorical column JOB_TITLE, a numeric column BASE_SALARY and a datetime index JOIN_DATE. I'd like to perform an aggregation over the groups of the categorical and downsampled datetimes as follows: Unfortunately, as the groupby operation is occurring before the resample, the resample operation is performed independently for each JOB_TITLE group. This results in following Series: As you can see the indexes at JOIN_DATE level for Data Scientist group and Software Engineer are not aligned. This creates a problem when you apply unstack for level JOB_TITLE as follows: This results in the following dataframe: How can I avoid this sequential operation of groupby and resample and instead perform a simultaneous operation? Thanks!", "q_apis": "get columns index groups groupby resample resample Series at apply unstack groupby resample", "apis": "groupby mean unstack groupby mean unstack", "code": ["mean_agg = (df.groupby(['JOB_TITLE',pd.Grouper(freq='5AS')])['BASE_SALARY']\n              .mean())\n\nmean_agg.unstack('JOB_TITLE')\n", "mean_agg = (df\n      .groupby(['JOB_TITLE',pd.TimeGrouper(freq='5AS')])['BASE_SALARY']\n      .mean())\n\nmean_agg.unstack('JOB_TITLE')\n"], "link": "https://stackoverflow.com/questions/42870703/simultaneous-operation-of-groupby-and-resample-on-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe All users should have 4 rows (4 Months) but period can be different (01.02.2018 -01.05.2018 or 01.02.2019 - 01.05.2019 ...) and I would like to transform like this and the last thing I should transform dataframe based on STATUS ID That is a kind of boolean operation between ID's in different Months. Any idea of how to transform this? Or could this be solved more efficiently?", "q_apis": "get columns transform last transform between transform", "apis": "eq", "code": ["print (df)\n     USER_ID       MONTH  STATUS_ID\n0      23026  2019-09-01          2\n1      23026  2019-10-01          2\n2      23026  2019-11-01          2\n3      23026  2019-12-01          2\n123    16546  2018-09-01          2\n123    16546  2018-10-01          2\n123    16546  2018-11-01          1\n123    16546  2018-12-01          1\n124    16622  2018-09-01          1\n125    16622  2018-10-01          1\n126    16622  2018-11-01          1\n127    16622  2018-12-01          1\n\ndf['MONTH1'] = 'MONTH_' + df.groupby('USER_ID').cumcount().add(1).astype(str)\ndf = df.pivot('USER_ID','MONTH1','STATUS_ID')\nprint (df)\nMONTH1   MONTH_1  MONTH_2  MONTH_3  MONTH_4\nUSER_ID                                    \n16546          2        2        1        1\n16622          1        1        1        1\n23026          2        2        2        2\n", "df1 = df1.eq(1).any(axis=1).map({True:1, False:2}).reset_index(name='ID')\nprint (df1)\n   USER_ID  ID\n0    16546   1\n1    16622   1\n2    23026   2\n", "print (df1.eq(1))\nMONTH1   MONTH_1  MONTH_2  MONTH_3  MONTH_4\nUSER_ID                                    \n16546      False    False     True     True\n16622       True     True     True     True\n23026      False    False    False    False\n"], "link": "https://stackoverflow.com/questions/60613649/transform-dataframe-based-on-id-status"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: And I want to fill all the names with the mode of the id of the row (if there are more than one element that are mode, fill with anyone), the final dataframe would look like this: I thought on groupby id and get the mode and then merge the dataframes by I can't seem to find the mode aggregating function.", "q_apis": "get columns all names mode mode groupby get mode merge mode", "apis": "mode groupby transform mode mode", "code": ["df['mode'] = df.groupby('id')['name'].transform(lambda x: x.mode()[0])\n", "    id   name   mode\n0    1  name1  name1\n1    2  name2  name2\n2    3  name2  name2\n3    4  name1  name1\n4    1  name2  name1\n5    1  name1  name1\n6    4  name1  name1\n7    4  name3  name1\n8    3  name3  name2\n9    1  name2  name1\n10   3  name2  name2\n"], "link": "https://stackoverflow.com/questions/61011485/pandas-dataframe-fill-with-mode"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "If I have the following pandas df and I want to add a new column to be 1, 2 or 3 depending on, whats the best way to do this?", "q_apis": "get columns add", "apis": "values values", "code": ["conditions = [(df.A > df.B) & (df.B > df.C),\n              (df.A < df.B) & (df.B < df.C)]\n\nvalues = [1, 2]\n\ndf['E'] = np.select(conditions, values, 3)\n"], "link": "https://stackoverflow.com/questions/50140131/multiple-logical-comparisons-in-pandas-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this code to clean LinkedIn job titles: After execute previous code, I still have some job titles incorrect. After applying clean_title function, I want to split the titles that still have the hifen (-) char. https://i.imgur.com/r0tbbN8.png How can I proceed?", "q_apis": "get columns", "apis": "stack", "code": ["def clean_title(position):\n    if 'back-end' in position.lower():\n        return 'Backend Developer'\n\n    if 'front-end' in position.lower():\n        return 'Frontend Developer'\n\n    if 'full-stack' in position.lower():\n        return 'Fullstack Developer'\n\n    if  '-' in position:\n        return clean_title(position.split('-')[0].strip())\n\n    if  '|' in position:\n        return clean_title(position.split('|')[0].strip())\n\n    if  '(' in position:\n        return clean_title(position.split('(')[0].strip())\n\n    if ':' in  position:\n        return clean_title(position.split(':')[0].strip())\n\n    return position\n"], "link": "https://stackoverflow.com/questions/68006753/split-dataframe-column-after-apply-method"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Details about the goal I am trying to use pd.get_dummies in pandas to convert the categorical features to data frames with dummy/indicator variables for each of three different genres, demographics, and prices separately. Additional details Two have a separator one a \",\" and another a \"| \" and the third there is only one choice it has a comma but that is part of the price not a separator. Overall goal - beyond this fix After I am done I would like to run a scaling function returns a numpy array containing the features KNN model from scikit-learn to the data and calculate the nearest neighbors for each distances. import and load dataset This is the current dataframe I simplified as the real data frame is massive thousand of names, genres, prices points and demographics. Dataframe: pandas.get_dummies I read here and tried a few different things to no luck. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html What i tried I also tried this errors I got goal Ideally I would like to use pd.get_dummies, a pandas method for converting categorical features to data frames with dummy/indicator variables for each genres, demographics, and prices separately. Genres has a separator like this \"| \" basically - ex: Country Music| Hip Hop & Rap| Pop Music demographics has a separator like this \"| \" basically - ex: Under 18,18-25,25-35 prices does not need a separator but has a comma - ex: Under $200,000 I am applying ideas from a few different movie database recommender systems tutorials into a real project. which should look like the following once done. expected results What I am trying to do: Genre: Demographics: Price: After I am done I would like to run a scaling function returns a numpy array containing the features KNN model from scikit-learn to the data and calculate the nearest neighbors for each distances.", "q_apis": "get columns get_dummies array names get_dummies get_dummies get_dummies array", "apis": "index", "code": ["artist1_features = pd.concat([artist1['genre'].str.get_dummies(sep=\"| \"), \n                              artist1['demo'].str.get_dummies(sep=\",\"),\n                              pd.crosstab(artist1.index, artist1['price']),axis = 1)\n"], "link": "https://stackoverflow.com/questions/62291989/convert-categorical-features-with-and-without-unique-seperators-using-pd-get-dum"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given a dataframe, how to groupby with value of (as instead of date string) while keeping date string format in result dataframe ? Expected Output What I have tried If I put in the , it would simply return the max value in alphabetical order If I apply it gives me a closer result but the date string format is distorted Is it possible to apply only during ? The challenge here is datetime format is not guaranteed to be '%b %d, %Y %I:%M %p' while I want to keep the date string as is in the result.", "q_apis": "get columns groupby value date date put max value apply date apply date", "apis": "assign groupby agg sum idxmax loc", "code": ["g = (df.assign(date=pd.to_datetime(df['Updated Date']))\n    .groupby('Item')\n    .agg({'Quantity': 'sum', 'date': 'idxmax'}))\n\ng['Updated Date'] = df.loc[g.date, 'Updated Date'].tolist()\n", "      Quantity           Updated Date\nItem                                 \nA           30  Jul 26, 2019 10:56 AM\nB           70  May 20, 2019 05:54 PM\n"], "link": "https://stackoverflow.com/questions/58405414/how-to-convert-date-string-to-datetime-in-agg-function-during-groupby"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have multiple DataFrames that I want to do the same thing to. First I create a list of the DataFrames. All of them have the same column called 'result'. I want to keep only the rows in all the DataFrames with value 'passed' so I use a for loop on my list: ...this does not work, the values are not filtered out of each DataFrame. If I filter each one separately then it does work.", "q_apis": "get columns all value values DataFrame filter", "apis": "loc loc dropna DataFrame drop index DataFrame", "code": [">>> list1 = [1,2,3,4]\n>>> list2 = [11,12,13,14]\n>>> for lyst in list1,list2:\n...   lyst = lyst[1:-1]\n... \n>>> list1, list2\n([1, 2, 3, 4], [11, 12, 13, 14])\n>>> lyst\n[12, 13]\n", "In [11]: df1\nOut[11]: \n          0         1         2         3\n0  0.957288 -0.170286  0.406841 -3.058443\n1  1.762343 -1.837631 -0.867520  1.666193\n2  0.618665  0.660312 -1.319740 -0.024854\n3 -2.008017 -0.445997 -0.028739 -0.227665\n4  0.638419 -0.271300 -0.918894  1.524009\n5  0.957006  1.181246  0.513298  0.370174\n6  0.613378 -0.852546 -1.778761 -1.386848\n7 -1.891993 -0.304533 -1.427700  0.099904\n\nIn [12]: df2\nOut[12]: \n          0         1         2         3\n0 -0.521018  0.407258 -1.167445 -0.363503\n1 -0.879489  0.008560  0.224466 -0.165863\n2  0.550845 -0.102224 -0.575909 -0.404770\n3 -1.171828 -0.912451 -1.197273  0.719489\n4 -0.887862  1.073306  0.351835  0.313953\n5 -0.517824 -0.096929 -0.300282  0.716020\n6 -1.121527  0.183219  0.938509  0.842882\n7  0.003498 -2.241854 -1.146984 -0.751192\n\nIn [13]: df3\nOut[13]: \n          0         1         2         3\n0  0.240411  0.795132 -0.305770 -0.332253\n1 -1.162097  0.055346  0.094363 -1.254859\n2 -0.493466 -0.717872  1.090417 -0.591872\n3  1.021246 -0.060453 -0.013952  0.304933\n4 -0.859882 -0.947950  0.562609  1.313632\n5  0.917199  1.186865  0.354839 -1.771787\n6 -0.694799 -0.695505 -1.077890 -0.880563\n7  1.088068 -0.893466 -0.188419 -0.451623\n\nIn [14]: for df in df1, df2, df3:\n   ....:     df.loc[:,:] = df.loc[df[1] > 0,:]\n   ....:     df.dropna(inplace = True,axis =0)\n   ....:     \n\nIn [15]: df1\ndfOut[15]: \n          0         1         2         3\n2  0.618665  0.660312 -1.319740 -0.024854\n5  0.957006  1.181246  0.513298  0.370174\n\nIn [16]: df2\nOut[16]: \n          0         1         2         3\n0 -0.521018  0.407258 -1.167445 -0.363503\n1 -0.879489  0.008560  0.224466 -0.165863\n4 -0.887862  1.073306  0.351835  0.313953\n6 -1.121527  0.183219  0.938509  0.842882\n\nIn [17]: df3\nOut[17]: \n          0         1         2         3\n0  0.240411  0.795132 -0.305770 -0.332253\n1 -1.162097  0.055346  0.094363 -1.254859\n5  0.917199  1.186865  0.354839 -1.771787\n", "In [21]: df1\nOut[21]: \n          0         1         2         3\n0 -0.804913 -0.481498  0.076843  1.136567\n1 -0.457197 -0.903681 -0.474828  1.289443\n2 -0.820710  1.610072  0.175455  0.712052\n3  0.715610 -0.178728 -0.664992  1.261465\n4 -0.297114 -0.591935  0.487698  0.760450\n5  1.035231 -0.108825 -1.058996  0.056320\n6  1.579931  0.958331 -0.653261 -0.171245\n7  0.685427  1.447411  0.001002  0.241999\n\nIn [22]: df2\nOut[22]: \n          0         1         2         3\n0  1.660864  0.110002  0.366881  1.765541\n1 -0.627716  1.341457 -0.552313  0.578854\n2  0.277738  0.128419 -0.279720 -1.197483\n3 -1.294724  1.396698  0.108767  1.353454\n4 -0.379995  0.215192  1.446584  0.530020\n5  0.557042  0.339192 -0.105808 -0.693267\n6  1.293941  0.203973 -3.051011  1.638143\n7 -0.909982  1.998656 -0.057350  2.279443\n\nIn [23]: df3\nOut[23]: \n          0         1         2         3\n0 -0.002327 -2.054557 -1.752107 -0.911178\n1 -0.998328 -1.119856  1.468124 -0.961131\n2 -0.048568  0.373192 -0.666330  0.867719\n3  0.533597 -1.222963  0.119789 -0.037949\n4  1.203075 -0.773511  0.475809  1.352943\n5 -0.984069 -0.352267 -0.313516  0.138259\n6  0.114596  0.354404  2.119963 -0.452462\n7 -1.033029 -0.787237  0.479321 -0.818260\n\n\nIn [25]: for df in df1,df2,df3:\n   ....:     df.drop(df.index[df[1] < 0],axis=0,inplace=True)\n   ....:     \n\nIn [26]: df1\nOut[26]: \n          0         1         2         3\n2 -0.820710  1.610072  0.175455  0.712052\n6  1.579931  0.958331 -0.653261 -0.171245\n7  0.685427  1.447411  0.001002  0.241999\n\nIn [27]: df2\nOut[27]: \n          0         1         2         3\n0  1.660864  0.110002  0.366881  1.765541\n1 -0.627716  1.341457 -0.552313  0.578854\n2  0.277738  0.128419 -0.279720 -1.197483\n3 -1.294724  1.396698  0.108767  1.353454\n4 -0.379995  0.215192  1.446584  0.530020\n5  0.557042  0.339192 -0.105808 -0.693267\n6  1.293941  0.203973 -3.051011  1.638143\n7 -0.909982  1.998656 -0.057350  2.279443\n\nIn [28]: df3\nOut[28]: \n          0         1         2         3\n2 -0.048568  0.373192 -0.666330  0.867719\n6  0.114596  0.354404  2.119963 -0.452462\n", "In [8]: timeit.Timer(stmt=\"df.loc[:,:] = df.loc[df[1] > 0, :];df.dropna(inplace = True,axis =0)\", setup=\"import pandas as pd,numpy as np; df = pd.DataFrame(np.random.random((8,4)))\").timeit(10000)\nOut[8]: 23.69621358400036\n\nIn [9]: timeit.Timer(stmt=\"df.drop(df.index[df[1] < 0],axis=0,inplace=True)\", setup=\"import pandas as pd,numpy as np; df = pd.DataFrame(np.random.random((8,4)))\").timeit(10000)\nOut[9]: 11.476448250003159\n"], "link": "https://stackoverflow.com/questions/38297292/apply-a-for-loop-to-multiple-dataframes-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to classify the String Values of a feature of my dataset, so that I can further use it for other things, let's say predicting or plotting. How do I convert it? I found this solution, but here I've to manually type in the code for every unique value of the feature. For 2-3 unique values, it's alright, but I've got a feature with more than 50 unique values of countries, I can't write code for every country. This changes the male values to 1 and female values to 0 in the feature - sex.", "q_apis": "get columns unique value unique values unique values values values", "apis": "rank astype", "code": ["df['ID_int'] = df['id'].rank(method='dense').astype(int)\ndf['ID_int2'] = pd.factorize(df['id'])[0]\n", "  id  ID_int  ID_int2\n0  a       2        0\n1  b       3        1\n2  c       4        2\n3  a       2        0\n4  b       3        1\n5  c       4        2\n6  A       1        3\n7  b       3        1\n"], "link": "https://stackoverflow.com/questions/56046713/how-to-classify-string-data-into-integers"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like as shown below. Thanks to SO community for helping with the below What I would like to do is a) create a new column called col based on rand value from or cols b) rand value for column is based on 2 conditions below c) would like to mention that the selection of column should be random (ex: some random subjects should have rand_value based on while others based on ) I was trying something like below I expect my output to be like as shown below", "q_apis": "get columns value value", "apis": "astype", "code": ["np.random.seed(10)\nrand = np.random.random(len(df_offset))\n\ndf_offset['rand_number'] = (rand * (df_offset.min_days_to_next_year + df_offset.min_days_to_prev_year) \n                                 -  df_offset.min_days_to_prev_year\n                           ).astype(int)\n", "      person_id  dates                  min_days_to_prev_year    min_days_to_next_year    rand_number\n--  -----------  -------------------  -----------------------  -----------------------  -------------\n 0           11  1961-12-30 00:00:00                      363                        1            -82\n 1           12  1967-05-29 00:00:00                      148                      216           -140\n 2           13  1957-01-01 00:00:00                        0                      364            230\n 3           14  1959-07-27 00:00:00                      207                      157             65\n"], "link": "https://stackoverflow.com/questions/62535573/generate-random-no-of-days-based-on-random-selection-of-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have Dataframe like this I want DataFrame like this First Balance value is 25.00 because openingbalance is 10.00 and first value is credit 15.00 so 10.00+ 15.00 ,if first value is debit then 10.00 - First Debit value please help", "q_apis": "get columns DataFrame value first value first value value", "apis": "cumsum", "code": ["df['Balance'] = openingbalance + (df['Credit'] - df['Debit']).cumsum()\n", "         Date  Credit  Debit  Balance\n0  01/09/2020    15.0    0.0     25.0\n1  02/09/2020     0.0    5.0     20.0\n2  03/09/2020     0.0    5.0     15.0\n"], "link": "https://stackoverflow.com/questions/63830631/calculation-of-balance-using-credit-debit-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have an SQL query which I want to implement it on the pandas dataframe data. This SQL is actually filtering the t_id if the e_count is null for more than 90 percent of the cases for type HIST. I have dataframe with all the columns but need to implement this logic in Python Pandas dataframe Below sql query:- Dataframe : I have a pandas dataframe like final result: i.e. drop T2 and T3 as 90% records for type HIST for that t_id for any s_id, act_dt is null. I have written the below code to identify the t_id's that has got 90% of data for act_dt as null so i can drop it from main dataframe raw_data but it is giving error from 3rd line. How can I get the list of t_id's that meet the criteria? I have written the below code to find out the t_id's that", "q_apis": "get columns query all columns query drop any drop get", "apis": "groupby filter count values shape groupby filter count values shape", "code": ["df.groupby('t_id').filter(lambda x: (x[x['type'] == \"HIST\"].act_dt.count() / x[x['type'] == \"HIST\"].act_dt.values.shape[0]) > .9)\n", "  t_id    s_id   t_name  type act_dt\n0   T1   china  android  HIST    jan\n1   T1  mumbai  android  HIST    feb\n2   T1   dubai    apple   EXT    NaN\n", "print(df.groupby('t_id').filter(lambda x: (x[x['type'] == \"HIST\"].act_dt.count() / x[x['type'] == \"HIST\"].act_dt.values.shape[0]) > .9)['t_id'].unique())\n"], "link": "https://stackoverflow.com/questions/44675861/how-to-implement-sql-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a a csv-file that has as separator the comma sign and at the same time values are separated by \". The first line is text, the second line is empty and the third line consists of column headings. If I try to import the file into a dataframe using pandas with using the code I get an error like How can I read the file into a dataframe in Pandas? I copied and pasted the sample.csv file which looks as follows:", "q_apis": "get columns at time values first second empty get sample", "apis": "sample sample sample", "code": ["IE00B0M62Q58 = pd.read_csv('ETF/sample.csv', sep=',', decimal=',')", "IE00B0M62Q58 = pd.read_csv('ETF/sample.csv', sep=',', decimal=',' thousands='.')", "IE00B0M62Q58 = pd.read_csv('ETF/sample.csv', sep=',',skiprows=2, thousands='.', decimal=',') \n"], "link": "https://stackoverflow.com/questions/67862507/reading-a-csv-file-into-a-pandads-dataframe-with-more-than-one-separator-for-the"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to rename columns in a dataframe based on a number in the column name. I created a dict and try to rename the columns by but that only works if I remove the SLL-Number before. Since they are different lengths, I would have to remove them manually, which I don't want to do. I would like to have a solution where I can rename the whole column based on the substring and the dict so that it looks like this: Could someone please help me? I have searched a lot but could not find a proper solution yet.", "q_apis": "get columns rename columns name rename columns where rename", "apis": "DataFrame columns get columns replace columns", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.uniform(0,1,(5,4)),\ncolumns=['SLL20100_CH32','SLL201301_CH04','SLL201812_CH02','SLL20123_CH03'])\n\nnew_names = [key.get(i) for i in df.columns.str.replace(\"[^ ]*_\",\"\")]\n\nnew_names\n['Batterie', 'Sal', 'LF', 'WS']\n\ndf.columns = new_names\n"], "link": "https://stackoverflow.com/questions/65966008/rename-columns-based-on-substring-and-dict-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that looks like this I want to groupby config_name and apply cumcount on each unique config_version so that I get an additional column like But I can't seem to understand how to do it. I tried using Which gives the following output I also tried But this gives the same output as the first try. Any idea how I could do this?", "q_apis": "get columns groupby apply cumcount unique get first", "apis": "drop_duplicates groupby ffill astype", "code": ["df['config_version_count'] = (df.groupby('config_name')['config_version']\n                                .transform(lambda x: pd.CategoricalIndex(x).codes))\n\nprint (df)\n   ID config_name  config_version  config_version_count\n0  aa           A               0                     0\n1  ab           A               7                     1\n2  ad           A               7                     1\n3  ad           A              27                     2\n4  bb           B               0                     0\n5  cc           C               0                     0\n6  cd           C               8                     1\n", "df['config_version_count'] = (df.drop_duplicates(['config_name','config_version'])\n                                .groupby('config_name')\n                                .cumcount())\ndf['config_version_count'] = df['config_version_count'].ffill().astype(int)\n"], "link": "https://stackoverflow.com/questions/54862986/how-to-use-groupby-and-cumcount-on-unique-names-in-a-pandas-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am doing some analysis on unstructured data in notebooks - which accounts for a column of information. I want to pull this sole column out and do natural language processing to see what keywords are most frequent and tokenization. When I apply my word tokenizer on the column for user reviews, the text I want to analyze: The row numbers are included with the text \"User Reviews\" column. Since some of the User Reviews contain the same numbers as the row numbers are, this is getting confusing for analysis, especially since I am applying tokenization and looking at term frequency. So the row starts at 1 in this below example, then the 2 is the next row, and then 3 and so on for 10K user reviews. Is there a way to do this? Do I need to to drop the row? I have looked up a few sources, here: https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/ https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c But still am struggling.", "q_apis": "get columns apply at at drop", "apis": "loc values", "code": ["text = df.loc[:, \"User Reviews\"].values\n"], "link": "https://stackoverflow.com/questions/50202307/only-select-columns-no-rows-in-python-notebooks"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "As per the following code, using panda, I am doing some analysis on one of the columns (HR): AT the end of the day, it will add a new column to the data ('Slope') That is fine and is working. The problem is that I want to redo the line (which is specified by **) for every other columns (not just HR) as well. in Other words,: Is there any way to do it automatically? I have around 100 columns so doing manually is not enticing. Thanks for your help", "q_apis": "get columns columns day add columns any columns", "apis": "index columns loc index index diff mean append", "code": ["aa = New_Data['index'].tolist()\naa = [0] + aa\navg = []\nfor col in df.columns:\n    for i in range(1,len(aa)):\n       ** val = raw_data.loc[(raw_data['index'] >= aa[i-1]) & (raw_data['index'] <= aa[i])[col].diff().mean()\n    avg.append(val)\nNew_Data['slope'] = avg\n"], "link": "https://stackoverflow.com/questions/61002413/automatic-analysis-on-multiple-columns-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame in pandas and I want to sort it by column. If I use like in the below code: I get the output in the 'id' column as: I expected: What is the best way to do this in pandas?", "q_apis": "get columns get", "apis": "loc index", "code": ["from natsort import index_natsorted, order_by_index\n\ndf2 = df.loc[order_by_index(df.index, index_natsorted(df['id']))]\n", "df1 = df['id'].str.split('_', expand=True)\ndf1[[0,2,3]] = df1[[0,2,3]].astype(int)\ndf1[1] = pd.to_datetime(df1[1])\n\ndf2 = df.loc[df1.sort_values([0,1,2,3]).index]\nprint (df2)\n                     id\n0   1075_2016-06-01_0_1\n5   1075_2016-06-01_1_1\n6   1075_2016-06-01_1_2\n1  1075_2016-06-01_10_1\n2  1075_2016-06-01_10_2\n3  1075_2016-06-01_11_1\n4  1075_2016-06-01_11_2\n", "f = lambda x: [int(x[0]), pd.to_datetime(x[1]), int(x[2]), int(x[3])]\ndf2 = df.iloc[df['id'].str.split('_').map(f).argsort()]\nprint (df2)\n                     id\n0   1075_2016-06-01_0_1\n5   1075_2016-06-01_1_1\n6   1075_2016-06-01_1_2\n1  1075_2016-06-01_10_1\n2  1075_2016-06-01_10_2\n3  1075_2016-06-01_11_1\n4  1075_2016-06-01_11_2\n"], "link": "https://stackoverflow.com/questions/57535099/sort-strings-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "We have this if else iteration with the goal to split a dataframe into several dataframes. The result of this iteration will vary, so we will not know how much dataframes we will get out of a dataframe. We want to save that several dataframe as text (.txt): And so on .... But, we want to save that several dataframes automatically, so that we don't need to write the code above for 100 times because of 100 splitted-dataframes. This is the example our dataframe df: and we would like to split the dataframe df to several df0, df1, df2 (notes: each column will be in their own dataframe, not in one dataframe): We tried this code: The problem with this code is that we cannot write several .txt files automatically, everything else works just fine. Can anybody figure it out?", "q_apis": "get columns get", "apis": "items join", "code": ["import os\nfor key, value in df_dict.items():\n    with open(f'D:/My_directory/{key}.txt', \"w\") as file:\n        file.write('\\n'.join(str(v) for v in value))\n"], "link": "https://stackoverflow.com/questions/60653833/write-several-txt-to-directory-automatically"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a simple json file and i have to convert it into a panda datarame and then to csv. Some sample records from the file are: Resulting dataframe: The code to convert it is given below and its working perfectly fine: I want to know that is there a more efficient solution? As i have to merge all the columns into one column because of ',' considered as a separator by pandas. May be converting the json directly to pandas dataframe without merging all the columns into one ? I will appreciate some help. Thanks", "q_apis": "get columns sample merge all columns all columns", "apis": "keys mode join items", "code": ["# load json file to a dictionary\nwith open('my_file.json') as f:\n    my_file_dictionary = json.load(f)    \n\n# save dictionary keys and value(text separated by space) to a csv\nwith open('outPutFile1.csv', mode='w', encoding='utf-8') as fp:\n    [fp.write('{0},{1}\\n'.format(key, ' '.join(value))) for key, value in my_file_dictionary.items()]\n"], "link": "https://stackoverflow.com/questions/64597318/more-efficient-way-of-converting-json-to-panda-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have been looking at different methods to export pandas dataframes into json files but I am not sure how to include other string 'constants' into the JSON. The purpose is to spit out a JSON file that can be read by chart.js. The format of the pandas dataframe for the example is: The format of the JSON required file is: i can export the pandas file as JSON using the inbuilt functions of pandas but i do not know how to separate the vectors and add the constant values seen above. My purpose is a json format which can be used in chart.js", "q_apis": "get columns at add values", "apis": "set_index index columns append values", "code": ["df = df.set_index('month')\nout = {'labels': df.index.tolist(), 'datasets': []}\nfor col in df.columns:\n    out['datasets'].append({\n        'label': col,\n        'data': df[col].values.tolist()\n    })\n"], "link": "https://stackoverflow.com/questions/64748089/how-to-convert-pandas-dataframe-to-specific-json-format"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'd like to identify all rows, where 4 from 5 columns are True i.e. So that.... How can I resolve this?", "q_apis": "get columns all where columns", "apis": "sum eq sum sum isin sum ge", "code": ["print (df[df.sum(axis=1).eq(4)])\n        a     b     c      d      e\nghi  True  True  True   True  False\nmnl  True  True  True  False   True\n", "print (df.sum(axis=1))\nabc    0\ndef    5\nghi    4\njkl    0\nmnl    4\ndtype: int64\n", "print (df[df.sum(axis=1).isin([4,5])])\n        a     b     c      d      e\ndef  True  True  True   True   True\nghi  True  True  True   True  False\nmnl  True  True  True  False   True\n", "print (df[df.sum(axis=1).ge(4)])\n        a     b     c      d      e\ndef  True  True  True   True   True\nghi  True  True  True   True  False\nmnl  True  True  True  False   True\n"], "link": "https://stackoverflow.com/questions/59948932/select-rows-in-a-dataframe-based-on-number-of-columns-equal-to-true"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "fairly new to and I am trying to create a calculated column on my using an If function. I've tried using the fields directly referenced from the , as well as assigning them to variables and converting them to series (as shown in my code below): f4 = (DataFrame extracted from my Postgres database using pscyopg2 and a SQL query) (379, 7) I'm receiving the following error: \"ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\" Through my research I've found that you can run into issues in Pandas when attempting to do calculations directly from the , but I'm having a hard time finding a solution, or information on which data type I should use instead. Essentially, I'm trying to create a calculated column on the f4 datafame that follows the logic above. Thanks!", "q_apis": "get columns DataFrame query value Series empty bool item any all time", "apis": "where where", "code": ["s = bnet-wv\nf4['watermark_adj'] = np.where(s<0, anet-s, s)\n", "s = bnet-wv\nf4['watermark_adj'] = s.where(s<0, anet-s)\n"], "link": "https://stackoverflow.com/questions/54248685/creating-a-calculated-column-with-an-if-statement-in-python-using-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Lets say i have 2 excel files each containing a column of names and dates Excel 1: Excel 2: I want to match each cell from column 1 to each cell in column 2 and then locate the biggest similarity. The following function will give a percentage value of how much two input match each other. SequenceMatcher code example: Output:0.92", "q_apis": "get columns names value", "apis": "max", "code": ["from difflib import SequenceMatcher\n\ncol_1 = ['potato','tomato', 'apple']\ncol_2 = ['tomatoe','potatao','appel']\n\ndef similar(a,b):\n    ratio = SequenceMatcher(None, a, b).ratio()\n    matches = a, b\n    return ratio, matches\n\nfor i in col_1:\n    print(max(similar(i,j) for j in col_2))\n"], "link": "https://stackoverflow.com/questions/62149590/how-to-iterate-through-2-columns-and-match-one-by-one"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm setting up a data pipeline for data from one of our tenants, who delivers data in Excel files: one workbook for each week, and each sheet in the workbook represents a day. We have no control over the format, but the process needs to be flexible enough to handle the variety of names in the workbooks and sheets. This also needs to be in Python, since we aren't allowed to execute macros or VBA (not my policy). I've tried using in a loop, but it will currently return the output as a dictionary of dataframes, and the function throws an error. I need to define a process that does the following: Pulls a list of workbooks from a directory, then Pulls a list of sheet names from that workbook, then Loops through each sheet and reads the data into an empty dataframe, then Repeats for each workbook, appending the results from each into a final, single dataframe. Eventually, this will be set up into a monthly script that will be pointed at a directory to import the latest data. I'm currently testing it on a few workbooks before I run the entire year's worth of data. Some of the print functions in the code are just for temp output checks. My current code, with the errors being thrown: Output: If I run it on the full list of workbooks: it throws the error: If I run it with just a single workbook using then it returns a dictionary of lists that looks like this: Trying to concatenate this dictionary with throws the error: I feel like this is close to working, but I'm missing some step to: Handle the list of files all at once, without having to process one at a time Transform the dictionary of dataframes correctly.", "q_apis": "get columns week day names names empty at year step all at at time", "apis": "DataFrame values append", "code": ["import glob\n\nall_data = pd.DataFrame()\npath = 'd:/projects/chassis/data/*.xlsx'\nfor f in glob.glob(path):\n    df = pd.read_excel(f, sheet_name=None, ignore_index=True, skiprows=6, usecols=8)\n    cdf = pd.concat(df.values())\n    all_data = all_data.append(cdf,ignore_index=True)\nprint(all_data)\n"], "link": "https://stackoverflow.com/questions/55508299/merge-multiple-sheets-from-multiple-excel-workbooks-into-a-single-pandas-datafra"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Okay, it's my real dataframe that i want to perform for reshaping on specific values with list. So i have this dataframe to reshape. I want to reshape it with specific list that i have before, here is my list. so, if string contain on column ['Keterangan'] match with item string in list, it will reshape specific column [Q2 2019] and [Q2 2018] to go down. so, here is which dataframe that i want. I have to try on some code to reshape it, move df.index to another column and switch column ['Keterangan'] to be index. and its work to make shift and swith index. And next, i want a perform last code for reshifting column, but , it occurs a problem.. it say like this. but, i have been checked a df['index'] type are int64. Why this column cant apply iloc? How to resolve the problem and gain df1 that i wish ? anyone can solve it?", "q_apis": "get columns values item index index shift index last index apply iloc", "apis": "join shift cumsum index index columns columns apply columns DataFrame iloc DataFrame iloc", "code": ["match = df['Keterangan'].str.fullmatch('|'.join(entry for entry in my_list))\ndf['shift'] = match.cumsum()\ndf['index'] = df.index\ncolumns = df.columns\ndf = df.apply(lambda row: print(row), axis='columns')\n", "Q2 2019    22686796.0\nQ2 2018    27421625.0\nshift             0.0\nindex             0.0\nName: Kas, dtype: float64\n\nQ2 2019    68409507.0\nQ2 2018    71159442.0\nshift             0.0\nindex             1.0\nName: Giro pada bank indonesia, dtype: float64\n\nQ2 2019    15675129.0\nQ2 2018    12584938.0\nshift             1.0\nindex             2.0\nName: Giro pada bank lain, dtype: float64\n...\n", "df = pd.DataFrame({'A': [1, 2], 'B': [1., 2.]})\nprint(df.iloc[0], 'w')\n\ndf = pd.DataFrame({'A': ['a', 'b'], 'B': [1., 2.]})\nprint(df.iloc[0])\n", "A    1.0\nB    1.0\nName: 0, dtype: float64\n\nA    a\nB    1\nName: 0, dtype: object\n"], "link": "https://stackoverflow.com/questions/64785356/reshifting-specific-column-based-on-row-if-string-row-match-with-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe as follows: Which looks like this: I want to do some multiindexing so that the Sample1 and Sample2 values are split by the colon and placed underneath as a sub-column name. However, I do not want these sub-column names to apply to the New Category column. Basically I want it to look like this: I really am stumped on how to do this. The multiindexing page of the pandas docs contains no example of multiindexing on selected columns only. This is making we wonder whether this is even possible.", "q_apis": "get columns values sub name sub names apply contains columns", "apis": "DataFrame values columns DataFrame columns applymap", "code": ["df_new_category = pd.DataFrame(\n    df[['New Category']].values,\n    columns=pd.MultiIndex.from_tuples([('New Category', '')])\n)\nsample_data_dfs = \\\n    [pd.DataFrame(list(df[col].str.split(':')),\n                  columns=pd.MultiIndex.from_product([[col], ['GT', 'GQ']]))\n     for col in ['Sample1', 'Sample2']]\n\npd.concat([df_new_category] + sample_data_dfs, axis=1)\n", "df[['Sample1', 'Sample2']].applymap(lambda s : s.split(':'))\n"], "link": "https://stackoverflow.com/questions/41168443/partial-multiindexing-with-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "SUMMARY of my problem: I have many DataFrames, all with the SAME POOL of columns (7 columns, for example COLUMN1:COLUMN7), BUT sometimes one or more columns are missing (i.e. a DataFrame might have COLUMN1:COLUMN3 + COLUMN6:COLUMN7, hence 4th and 5th column missing). Each DataFrame has columns arranged in different order every time (i.e. df1 has its order, df2 has another order, df3 yet another order and so on...). I want to arrange the columns in each DataFrame based on a list of columns that serves as a benchmark (in this case a list of columns from 1 to 7). The desired result would be for all the Dataframes to have the same column order based on this list, if a column is missing the order should be preserved (i.e. if column 4 and 5 are missing, the order of the columns should be: COL1, COL2, COL3, COL6, COL7). More detailed description: I have several DataFrames in my code that are produced by cleaning some datasets. Each one of these DataFrames has DIFFERENT NUMBER of columns and in DIFFERENT ORDER, BUT the columns are limited to this list: . Hence the columns can be 7 at most, from this list. Example: DataFrame1 DataFrame2 DataFrame3 DESIRED OUTPUT: I would like to order the columns based on the initial list , even if the number of columns varies. From the example above the DataFrames should become: DataFrame1 DataFrame2 DataFrame3 is there a way, a loop for example, to arrange the columns this way?", "q_apis": "get columns all columns columns columns DataFrame DataFrame columns time columns DataFrame columns columns all columns columns columns columns at columns columns columns", "apis": "reindex columns", "code": ["desired_order = ['id', 'title', 'type', 'category', 'secondary category', 'date', 'description']\n\ndf = df.reindex([i for i in desired_order if i in df.columns], axis=1)\n"], "link": "https://stackoverflow.com/questions/66229980/dataframes-columns-re-arrange-based-on-list-dataframes-have-different-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose my dataframe has 750 rows in a column and I want to convert that column to an image array of (20,20) numpy array. How to do that? EDIT1: I want to use the array for ax.contourf (x,y,z) as z. I got x,y by doing x,y=np.meshgrid(df3.x,df.y) now I want to convert another column to an (n,n) array to vary the colors inside the contours using the z parameter. EDIT2:x,y,z Data to plot contour plot", "q_apis": "get columns array array array now array plot plot", "apis": "get all xs values all xs all DataFrame index columns xs index columns columns append fillna append columns index all empty values sort_index index values sort_index columns xs values shape shape values", "code": ["df = pd.DataFrame(\n  columns=[\"z\", \"x\", \"y\"],\n  data=[\n    [-0.00222, 38, 46.1], [-0.00374, 30.5, 68.4], [0.001043, 36, 86.9], [0.003473, 52.9, 95.3], [0.001382, 50.6, 80.3], [-0.00486, 37, 92.7], [-0.00016, 29.8, 69.5], [0.001368, 34.7, 27.2], [-0.0016, 37.5, 63], [-0.00181, 45.5, 92], [0.003635, 53.6, 82.4], [-0.00363, 46.7, 91.1], [0.002253, 54.1, 97.8], [-0.00089, 43.8, 87], [-0.00115, 41, 62.7], [-0.001, 47.5, 23.2], [0.001332, 61.2, 4.7], [-0.00361, 45.9, 13.9], [0, 47.4, 32], [0, 54.1, 66.6], [0.000891, 53.2, 7.3], [-0.00133, 56.1, 15.8], [-0.00045, 43.6, 19.3], [0.004365, 68.8, 94.2], [-0.00355, 71.1, 76.1], [0, 71.3, 64.8], [-0.00415, 65.7, 59.4], [0.003881, 72.3, 47], [-0.00727, 54.7, 68.4], [0.001848, 63.3, 81.3], [0.002518, 69.7, 93.8], [-0.00252, 65.2, 84], [-0.0025, 55.8, 67.5], [0.004365, 68.2, 94.5], [0.008311, 76.4, 85.4], [0.000208, 35.4, 70.5], [-0.00021, 45, 92.7], [-0.00189, 32, 75.5], [0.000219, 32.3, 38.3], [0.001522, 33.4, 61.7], [0, 37.6, 37.1], [0.0016, 47.6, 32.5], [0.000595, 59.1, 7.1], [0, 63.1, 5], [0.00212, 55.3, 26.5], [0.002495, 64.7, 27.4], [0.00197, 79.4, 24.2], [0.006561, 75.2, 38.9], [0.011262, 82.3, 53.3], [0.019798, 88.7, 25.6], [-0.0198, 78.9, 45.2], [0.000602, 22.6, 74.7], [-0.00082, 29, 93.5], [0.002707, 43.8, 95], [0.000497, 27.7, 45.4], [-0.00131, 19.3, 28.7], [0.000152, 19.2, 38.3], [-0.00015, 23.5, 64.5], [0.001311, 31.7, 41.9], [0.004164, 54.8, 28.4], [-0.00185, 57.8, 32], [0.004771, 71.1, 69.3], [0.010728, 87.7, 92.3], [-0.0063, 81.6, 71.6], [0.079499, 84.6, 52.1], [-0.07294, 78.2, 52.1], [-0.00656, 80.6, 64.8], [0, 70.7, 67.2], [0.00541, 76.1, 75.5], [0.003548, 81.6, 93.2], [-0.00355, 74.9, 84.8], [-0.00016, 68.6, 60], [0.000164, 70, 17.2], [0.005518, 78.8, 16.3], [-0.00466, 64, 27.7], [0.000619, 66.5, 46], [-0.00552, 71.6, 77.9], [-0.00027, 72, 50.7], [0.002583, 46, 87.2], [0.007159, 69.6, 75.6], [0.005518, 77.1, 84.9], [-0.00729, 79.4, 92.4], [0.00197, 75.2, 75], [-0.00011, 74.7, 19.3], [0.011137, 90.9, 22.6], [0.006561, 77.8, 41.2], [-0.00186, 14.3, 38.6], [0.000497, 28, 61.2], [0.001303, 38.9, 55.7], [0.002049, 52.4, 41.8], [-0.00205, 41.9, 51.3], [-0.00055, 46.7, 70.2], [0.004365, 71.8, 94.5], [-0.00477, 65.8, 67.8], [0.00541, 75.5, 84.2], [-0.00541, 74.5, 72.2], [0.00541, 77.8, 85.7], [-0.00568, 72, 54.5], [0.001221, 46.9, 3.5], [0.003635, 58.2, 14.9], [0.000446, 45.2, 34], [0.004771, 68.4, 37.4], [0, 71.3, 55.1], [0, 67.6, 61.8], [0.022626, 94.2, 84.6], [-0.0026, 45.8, 69.8], [0.002565, 55.7, 74.9], [0.007159, 68, 76.1], [-0.00027, 71.1, 38.3], [0.019798, 91.3, 70.7], [0, 92.6, 92.4], [-0.01073, 74.6, 74.6], [0.005318, 87.2, 92.6], [-0.0063, 79.8, 70.5], [0.017925, 98.1, 78], [0.000668, 34.4, 91.2], [0, 36.2, 92.9], [0.000885, 38.2, 75.9], [-0.00094, 21.7, 47], [0.000939, 30.2, 31.3], [-0.00088, 37.9, 6.2], [-0.00016, 27.4, 36.7], [-0.00048, 39.8, 87.2], [0.005303, 63.2, 95.9], [0.001901, 61.8, 70.5], [0.000652, 22, 89.1], [0.003699, 33.6, 80.3], [0.005125, 60.3, 58.1], [0, 63.1, 38.5], [0, 65, 60.2], [0, 60.9, 39.1], [0.004152, 67.7, 27.1], [0.003441, 77, 9.7], [-0.00344, 74.6, 28], [0, 70.9, 49.7], [0.012118, 90.9, 23.8], [0.000366, 25.2, 93.3], [-0.00212, 58, 94.4], [-0.00225, 42.4, 93.5], [0.003635, 52, 85.7], [0.001332, 58.7, 94.5], [-0.00133, 53.9, 84.6], [-0.00388, 58.9, 45.1], [0.006956, 67, 48.2], [-0.007, 51.9, 67.8], [-0.00416, 39.1, 29.8], [-0.00253, 19.8, 44], [-0.00044, 45, 30.4], [-0.001, 45.9, 77.4], [0.002565, 53.1, 66.8], [-0.00416, 41.5, 35.2], [-0.00055, 46.9, 66], [0.000595, 64.1, 96], [0.001901, 61.2, 68.3], [-0.00252, 62.9, 9.4], [0, 73.4, 7.1], [0.025256, 94.3, 13.3], [-0.01084, 72.3, 16.4], [0.010991, 75.2, 56.3], [0.010999, 89.2, 87.5], [0.000164, 72.5, 80.4], [0.000107, 72, 69.6], [-0.00027, 73, 38.6], [0.002273, 56.7, 83.3], [-0.00035, 23.4, 77.6], [0.000353, 29.8, 87.5], [0.002707, 44, 95], [-0.00034, 34.3, 84.5], [-0.00122, 39.4, 59.7], [0.001219, 47.9, 77.1], [0.001001, 42.4, 58.1], [-0.00055, 43.9, 31.4], [0.000555, 45.1, 55.7], [0.003857, 58.6, 75.6], [0.002518, 67.5, 96.5], [-0.00059, 55.8, 73.7], [-0.00201, 49.5, 56.3], [-0.00078, 54, 17], [0.004152, 72, 66.5], [0.003441, 78.9, 91.4], [-0.00355, 70.4, 79.8], [-0.00027, 69.2, 61.7], [0.000271, 68.3, 36.2], [0.003441, 78.8, 11.2], [0, 82.8, 15.4], [-0.00098, 75.9, 33.5], [0, 75.2, 67.5], [0.012118, 90.5, 87], [0, 42.1, 10.4], [0.001808, 48.4, 31.4], [0.007266, 69.4, 26.6], [0.003548, 78.6, 6.2], [0.000989, 80.3, 28.1], [-0.00443, 74.8, 28.9], [0.001417, 10.9, 38.6], [0, 9.2, 46.3], [0.000497, 32.9, 49.4], [-0.0005, 8.6, 48.1], [0.000277, 28.1, 32.2], [-0.00232, 12.5, 23.3], [0.000991, 30.2, 3.5], [0.000818, 30.7, 18.7], [-0.00021, 43.8, 92.9], [0.000479, 39.6, 54.9], [0.00222, 48.8, 46], [-3.93E-05, 53.7, 29.9], [-0.00166, 35.1, 50.7], [-0.00116, 17.8, 45.1], [0.002461, 33.6, 44], [-0.00014, 46.2, 4], [0.001382, 54.5, 19.6], [3.93E-05, 54.5, 50.9], [0.001251, 73.6, 92.4], [0.000989, 75.2, 68.1], [-0.01036, 75.6, 94.4], [-0.00252, 66.5, 80.3], [0.005419, 69.7, 66.3], [0, 74.4, 74.4], [0, 70.9, 66.3], [0, 72.5, 72.5], [-0.00027, 67.6, 55.2], [0.016548, 90, 87.2], [0.067381, 86.9, 60.2], [-0.00656, 79.2, 71.6], [-0.0047, 73.7, 52.1], [0.005681, 78.1, 19.1], [0.006561, 78.2, 46.3], [-0.00656, 77.2, 69.3], [-0.0047, 72, 58.7], [-0.0045, 66, 67.4], [-0.00246, 53, 51.3], [0.007227, 66.9, 66.2], [0.003441, 79.8, 90.5], [-0.00016, 72, 54.5], [0.019798, 86.4, 29.5], [0.009515, 97.7, 25.6], [0.016818, 89.1, 76.8], [-0.01212, 80.7, 68.6], [0.013629, 85.6, 96.9], [-0.00729, 81, 96], [-0.00355, 71, 82.1], [-0.00011, 70.4, 24.5], [0.005518, 76.7, 14.9], [-0.00027, 71, 45.2], [0.005681, 79, 85.4], [-0.00568, 72.5, 53.6], [0, 72.5, 41.9], [0, 29.7, 66], [0, 31.7, 72.6], [0, 29.7, 64.2], [0.00222, 44.1, 44.9], [-0.0027, 37.8, 17.8], [0.000336, 44.3, 5], [-0.00034, 37.6, 16.4], [0.000336, 44.4, 12.1], [-0.00024, 29.9, 88.2], [0.000589, 23.2, 70.2], [0.001522, 33.9, 42.8], [-0.0006, 5.7, 45.7], [0.000602, 23, 37.1], [-0.00024, 25.9, 4.3], [-0.0042, 40.7, 68], [0.001154, 43.4, 87], [0.000891, 55.2, 93.5], [-0.00225, 42, 88.7], [-0.00138, 31.2, 74.7], [-0.00094, 16.8, 49.4], [0.002526, 41.4, 72.9], [0.000891, 55.9, 94.5], [-0.00133, 53, 75.3], [0.010668, 77.5, 95.7], [0.000989, 76.1, 68.1], [0.019798, 87.9, 30.8], [0.003771, 72.5, 96.5], [0.002253, 54.5, 93.2], [-0.00225, 43.3, 92.7], [-0.00271, 27.4, 91.7], [0.001051, 14.2, 54], [0.003464, 43.5, 67.1], [0.000555, 47.1, 57.5], [0.00201, 56.5, 72.5], [0.00187, 73.4, 95.6], [0.008311, 75.8, 77.9], [-0.00541, 67.2, 69.9], [-0.00027, 73.9, 54.6], [-0.01126, 71.8, 57], [0.004701, 81.4, 34.4], [0.000232, 9.2, 40.6], [0.000277, 26.3, 26.8], [-0.00075, 4.1, 51.5], [-0.00062, 8.5, 83.6], [0.002706, 46.6, 93.9], [0.003881, 68.7, 58.8], [0, 74.6, 47.5], [0, 70.7, 47.5], [-0.00149, 30.1, 90.6], [0.002097, 36.6, 65.4], [6.53E-05, 34.2, 30.3], [0.002044, 54.9, 4.1], [0, 54, 10.7], [0.004771, 68.9, 73.8], [0.00541, 82, 82.7], [-0.00098, 75.8, 64.2], [0, 79.5, 74.9], [-0.00552, 74.9, 75.9], [0.016944, 92.2, 85.5], [-0.00634, 70, 92.4], [0, 73.7, 96.2], [-0.00377, 61.8, 96.2], [0.00212, 53.7, 69], [0.002495, 59.7, 66.9], [0, 74, 83.4], [0.005518, 77.8, 86.3], [0.003548, 80.9, 93.6], [0, 79.1, 51.9], [-0.01126, 73.6, 55.5], [-0.00733, 75, 93.3], [-0.00344, 74.4, 71.3], [-0.00477, 66.3, 71], [0.004664, 68.1, 75.9], [0.000107, 69.9, 65.6], [-0.00027, 74.3, 55.7], [0, 66.7, 93], [0.002901, 68.1, 72.3], [0.00443, 75.2, 74.9], [0, 72.8, 92.3], [0.006342, 77.6, 95.6], [-0.00634, 70, 88.1], [-0.00016, 68.6, 57.8], [-0.00062, 66.4, 35.5], [0.000271, 73, 68.7], [-0.00388, 65.7, 53.7], [0.007592, 81.5, 11.3], [-0.01655, 74.3, 29.3], [-0.00388, 61.3, 53.9], [0.003881, 69, 49.9], [0.000271, 74.3, 64.5], [0.012118, 88.9, 86.1], [0.002565, 50.1, 66], [0.004326, 71.7, 7.1], [-0.0047, 71.3, 41.5], [0.004701, 79.3, 64.1], [-0.00843, 92.7, 76.2], [0.001253, 59.4, 80.3], [0, 56, 33.7], [0.000595, 65.4, 5.3], [0.001253, 61.5, 14.2], [0.001267, 65.2, 37.9], [0.003548, 77.4, 93.2], [-0.00355, 72.7, 85.4], [-0.00016, 70.7, 54.2], [0.026359, 91.4, 72.9], [-0.00729, 76.8, 6.1], [-0.00022, 29.6, 70.2], [0.001522, 36.5, 61.8], [0.011496, 67, 37.4], [-0.00477, 64.4, 26.8], [-0.00027, 74.5, 39.8], [0.019798, 90.3, 68.6], [0.000367, 14.6, 69], [0.001327, 28.8, 63.3], [0.001043, 37, 78.9], [0.000336, 42.2, 95.3], [-0.00189, 27.2, 85.1], [0.00222, 46.6, 56.6], [0.001328, 26.1, 93.8], [0.002707, 42.3, 92.3], [0.00011, 44.9, 38.5], [0.0045, 67.6, 38.8], [0, 62, 45.7], [0.007592, 75.3, 91.2], [0, 18.8, 55.4], [-0.00039, 13.4, 68.1], [-0.00014, 46.5, 89.7], [0, 42.4, 89.1], [0.002253, 53.8, 95], [0.005318, 85.7, 89.4], [-0.01212, 75.1, 66.3], [0.005681, 80.7, 14.6], [-0.00098, 76.1, 31.3], [-0.00027, 67.3, 56.9], [0.003712, 82.5, 87.8], [-0.01212, 80.2, 70.4], [-0.00541, 70.1, 74.4], [0.00541, 79.9, 85.8], [0, 75.2, 75.9], [-0.00098, 79.7, 72.9], [-0.00443, 72.1, 72.6], [0.00443, 76.6, 74.7], [-0.00443, 70.6, 73.5], [0.00443, 78.7, 65], [-0.0047, 72.7, 54.5], [0.003712, 82.8, 11.5], [-0.01655, 74.7, 29.8], [-0.00027, 73.3, 41.9], [0, 74.2, 57.6], [0.016548, 84.6, 86.1], [-0.01665, 74.3, 75], [0.004537, 80.3, 71.4], [-0.00116, 27.8, 47.3], [0, 39.1, 93.5], [0.003473, 53.5, 96], [-0.00122, 40.4, 91.1], [-0.00122, 34.4, 51], [0.002109, 52.5, 5.3], [0.00212, 52.7, 25.7], [0, 55.6, 36.7], [0.006996, 67.1, 43.9], [-0.00388, 62.7, 45.4], [0.001251, 74.1, 90.6], [0, 73.9, 63.3], [0, 70.7, 67.2], [-0.00027, 68.1, 44.3], [0, 65, 60.9], [0.004045, 73.5, 83.6], [0.00443, 81.6, 68.9], [1.33E-05, 22.2, 78], [0.010728, 85.8, 87.8], [-0.01665, 72.3, 78.6], [-0.00011, 72.6, 13.7], [0.00201, 51.7, 65.3], [-0.00256, 45.9, 68.8], [0.001848, 59.7, 78.6], [0.005419, 68.6, 73.2], [-0.00656, 78, 36.4], [0.000981, 80.4, 87.2], [-0.00541, 74.7, 71.3], [-0.00308, 87.4, 92.6], [-0.00532, 76.6, 85.8], [0.005318, 86.8, 93.2], [-0.01084, 72.8, 76.5], [0.004701, 81.8, 29.6], [0.006561, 81.9, 47.8], [-0.00089, 46.9, 79.8], [-0.00122, 39.3, 45.2], [0.00222, 46.3, 59.6], [0.001272, 55.6, 87.3], [0.002715, 64.9, 96.2], [-0.00125, 64, 91.4], [0.001253, 65.4, 78.9], [-0.00125, 63, 87.9], [-0.00059, 54.7, 73.4], [-0.00074, 54, 16.6], [0.001332, 60.6, 6.5], [0.000619, 64.9, 59.6], [0.001251, 72.3, 89.1], [0.011137, 85.4, 84.6], [0.001332, 66.6, 93.6], [-0.00252, 64.2, 86.3], [0.005312, 74.1, 77.6], [-0.00531, 66, 77.1], [0.002518, 70.1, 93.3], [-0.00252, 64.5, 83.6], [-0.00185, 54.7, 65.5], [-3.93E-05, 51.5, 35.2], [0.000595, 60.9, 4.7], [-0.00059, 58.3, 30.5], [-0.00415, 63, 52], [0.004664, 67.1, 77.4], [0.005518, 83, 87.3], [-0.0198, 79.4, 60.3], [0.202542, 97, 44.5], [0.000367, 10.1, 36.7], [-0.00037, 7.9, 49.3], [0, 29.8, 48.1], [0.001043, 37.5, 80], [0, 34.4, 54.2], [-0.00246, 24.3, 58.2], [0.002461, 35.9, 58.1], [-0.00048, 33.5, 86], [-0.00088, 41.6, 95.3], [0, 36, 89.6], [0.00305, 55.7, 56.9], [-0.0027, 37.1, 13.7], [0.003473, 54.3, 4.1], [0.000738, 56, 37.3], [3.93E-05, 53.8, 59.7], [0.00187, 74.6, 90.2], [0, 75.9, 66.3], [0.012118, 87.2, 83.7], [-0.01212, 75.1, 71.9], [0.012118, 87.2, 19.6], [-0.00541, 75, 29.9], [0.000164, 70.3, 75.8], [0.005518, 82.1, 86], [-0.00541, 74.2, 69.9], [-0.00027, 74.4, 60.8], [0.0842, 88.8, 62.3], [0.002518, 70.1, 91.4], [0.013107, 91.5, 85.1], [-0.00729, 81.8, 95.7], [-0.00355, 74.7, 80.6], [0.000107, 74.9, 66], [0, 72.1, 72.2], [0.000164, 69.7, 20.8], [0.000107, 71.4, 27.2], [-0.00477, 64.5, 32.5], [-0.00477, 65.4, 70.2], [0.004664, 70.2, 86], [0.003548, 80.7, 92.9], [-0.00355, 73.9, 78.5], [0.026359, 89.4, 68.7], [0, 92.4, 53.6], [0.061574, 84.9, 39.8], [0.011696, 76.6, 25.9], [-0.00027, 74.3, 57.9], [0.003631, 53.2, 12.2], [0, 59.8, 53.7], [0.004152, 70.6, 71.3], [0, 69.8, 64.7], [0.011262, 76.7, 53.4], [-0.00388, 66, 46.4], [0.020699, 86.3, 81], [0.011137, 90, 81], [-0.01114, 79.2, 75.2], [-0.00308, 84.5, 95.6], [-0.01084, 71.5, 86.4], [0.010836, 86.6, 94.4], [-0.01363, 73.5, 91.7], [0.002794, 69.6, 77.7], [0.005518, 76.5, 76.2], [0.004204, 54.4, 57], [0.003075, 60.3, 50.1], [-0.00427, 40.3, 54.9], [0.001219, 44.7, 84.3], [0.000891, 58.2, 94.5], [0, 56, 89.6], [0, 51.2, 88.7], [0.000738, 52.8, 65.4], [0, 57.8, 69.5], [0.002495, 61.5, 63.9], [0, 65, 69.3], [-0.00377, 65.2, 89.7], [-0.00185, 54.3, 64.1], [0, 51.6, 65.1], [0.001272, 50.8, 21.8], [0, 55, 33.5], [-0.00575, 33, 30.8], [-0.0006, 7.5, 51], [-0.00191, 7.1, 20.6], [0.000531, 2.9, 43.7], [0.001656, 45, 89.1], [-0.00125, 66.3, 89.3], [0, 65.4, 67.2], [0.016818, 85, 13.1], [-0.02636, 78.4, 36.8], [0.005681, 78.8, 78.2], [-0.00098, 79.8, 71.1], [0.006561, 75.7, 43], [0, 68.7, 46.4], [0.005681, 78.3, 19.7], [-0.01665, 71.7, 24.8], [0.003441, 80.7, 11.8], [-0.00355, 68.2, 20.2], [-0.00415, 65, 60.6], [0, 72.7, 57.2], [-0.00011, 67.9, 79.1], [0.000107, 71.6, 74.9], [-0.00011, 67.2, 78.6], [-0.0042, 37.3, 27.7], [-0.00054, 37.8, 14.3], [0.000545, 37.4, 36.1], [-0.00152, 32, 65.9], [0.000219, 29.8, 45.1], [-0.00022, 31.7, 63.6], [0.001303, 39.8, 40.7], [0.000823, 41, 20], [-0.00067, 29.7, 23.5], [0.003187, 44.3, 69.5], [0.00506, 64.5, 68.7], [0.002518, 74.1, 99.3], [0.00755, 80.7, 59.7], [-0.03587, 81, 62.6], [0.009374, 93.8, 5.3], [0.010363, 95.3, 6.5], [-0.01793, 82.2, 28.1], [0.000388, 20.7, 41.9], [-0.0019, 23.5, 24.7], [0.000814, 12.2, 51.8], [0.000277, 31.1, 66.2], [0.002144, 48.1, 72.3], [0.010836, 84.4, 94.1], [0.0045, 69.2, 56], [0.001303, 38.1, 45.4], [-0.0013, 25.8, 49.4], [-0.00062, 66.3, 25.6], [0, 57.4, 90.8], [0.003115, 61.3, 43.3], [-0.00066, 17.1, 43.1], [0.000662, 10.3, 56.7], [0.000497, 27.6, 59.3], [0.003187, 44.1, 72.2], [-0.00303, 39.6, 92.1], [-0.00149, 32.8, 90.9], [0.003473, 55, 96.2], [-0.00347, 39.5, 89.3], [0.000479, 33.9, 41], [0.002565, 52.4, 33.4], [0.001848, 62.5, 13.3], [0.000648, 62.2, 31], [0.004152, 67.1, 66.2], [0.00541, 81.6, 84.2], [0.0265, 95.9, 64.8], [0.001552, 37.7, 78.8], [0, 39, 87.2], [0.00853, 70.5, 96.2], [-0.00062, 9.6, 14.5], [0.001809, 26.4, 18.8], [0.000668, 40.1, 6.8], [3.93E-05, 55.3, 39.2], [0.004152, 68, 68.6], [0.00541, 77.5, 84.2], [0.000107, 72, 65.7], [0.00622, 61.8, 14.8], [-0.00065, 58.5, 13.9], [0.002518, 68.2, 7.9], [-0.00011, 72.2, 24.5], [0.016655, 85.7, 15.5], [-0.02636, 75.3, 29.6], [-0.00027, 72.4, 49.6], [0.000366, 29.9, 88.7], [0.009959, 72.8, 87.8], [-0.01212, 76.3, 67.8], [0.00443, 82.2, 27.4], [-3.93E-05, 57, 64.2], [-0.00201, 45.6, 46.3], [0.005125, 59.7, 47.6], [-0.00062, 63.7, 63.5], [0.004664, 73.9, 76.4], [-0.00098, 75.6, 27.7], [0.026359, 85, 32.2], [0.000948, 23.1, 90.8], [0.002915, 36, 31.7], [0.002044, 57, 10.4], [0.003473, 57.4, 96.5], [-0.00027, 67.8, 55.2], [0.005681, 75.2, 17], [-0.00088, 39.3, 9.5], [0.006188, 60.3, 4.7], [0.002901, 67.2, 26], [0.030789, 86.1, 74.3], [-0.0198, 76.8, 57.8], [-0.00252, 65.7, 91.5], [-0.00133, 58.3, 86.1], [0.000738, 55.8, 67.2], [0.002495, 58.9, 70.4], [0.003881, 71.7, 45.4], [-0.00263, 68.7, 97.5], [-1.33E-05, 6.4, 54.2], [0.001656, 46.7, 93.5], [-0.00377, 62.9, 8.2], [0.001267, 60, 61.5], [-0.00062, 65.4, 67.4], [0.001251, 71.4, 8.5], [0.000755, 21.1, 58.5], [-0.00075, 6, 52.4], [0.008524, 60.2, 66.2], [0, 66.4, 70.4], [0.004771, 67.7, 74.3], [0, 72.2, 62.9], [0, 72.2, 68.4], [0.040304, 98, 73.8],\n  ]\n)\ndf2 = df.pivot_table(\"z\", index=\"y\", columns=\"x\", aggfunc=\"sum\").fillna(0)\narray = df2.sort_index(ascending=False).values\n", "all_xs = all_ys = range(0, 1001) # get all discrete xs and ys values (on an integer basis)\nmissing_xs = list(set(all_xs) - set(df['x']*10)) #find all missing xs\nmissing_ys = list(set(all_ys) - set(df['y']*10)) #find all missing ys\ndummy_df = pd.DataFrame(0, index=missing_ys, columns=missing_xs) #construct a dummy dataframe with those xs and ys (filled with zeros)\ndf2.index *= 10 #convert your indexes and columns of the previous df2 to integers\ndf2.columns *= 10\ndf3 = df2.append(dummy_df).fillna(0) #append the dataframes, which will create the missing columns and index and fill all empty values with zeros\ndf3.sort_index(inplace=True, ascending=False) #resort the index (ie ys values)\ndf3.sort_index(axis=1, inplace=True, ascending=True) #resort the columns (ie xs values)\nprint(df3.shape) #make sure the shape is a square array\narray = df3.values #extract the numpy array\n"], "link": "https://stackoverflow.com/questions/65439602/how-to-convert-a-pandas-dataframe-column-to-an-image-array-i-e-a-numpy-array-wi"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe and I want to input to a slice of rows in a specific columns, where the input is a list of strings - so in each of the selected rows, I want the same list of strings. But when I try this, I get an error: It works fine when I do the same but instead of a list of strings I just have a single string. Why won't it accept a list as input?", "q_apis": "get columns columns where get", "apis": "update DataFrame index index iloc columns", "code": ["df.update(pd.DataFrame({'col':[['x', 'y', 'z', 'q'] for x in range(0,460)]},index=df.index[:460]))\n", "df.iloc[0:460, df.columns.get_indexer(['col'])[0]]=[['x', 'y', 'z', 'q'] for x in range(0,460)]\n"], "link": "https://stackoverflow.com/questions/62174545/setting-a-selection-of-rows-equal-to-a-list-of-strings"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to drop a few elements from the column through loop. After implementation of loop, it's not presenting output. Please refer this datafile:File: Please refer this code:", "q_apis": "get columns drop", "apis": "values filter isin values", "code": ["import pandas as pd\nimport calendar\n\n# read the file with the correct encoding\ndf = pd.read_csv('Environment_Temperature_change_E_All_Data_NOFLAG.csv', encoding='cp1252')\n\n# print the unique values in the Months column before filtering\nprint(df.Months.unique())\n\n[out]:\n['January' 'February' 'March' 'April' 'May' 'June' 'July' 'August'\n 'September' 'October' 'November' 'December' 'Dec\u2013Jan\u2013Feb' 'Mar\u2013Apr\u2013May'\n 'Jun\u2013Jul\u2013Aug' 'Sep\u2013Oct\u2013Nov' 'Meteorological year']\n\n# create a list of calendar month names\nmonth_names = list(calendar.month_name)[1:]\n\n# filter the dataframe with Boolean selection\nfiltered = df[df.Months.isin(month_names)]\n\n# show unique values in the Months column\nprint(filtered.Months.unique())\n\n[out]: \n['January' 'February' 'March' 'April' 'May' 'June' 'July' 'August'\n 'September' 'October' 'November' 'December']\n", "    Area Code         Area  Months Code               Months  Element Code             Element Unit\n22          2  Afghanistan         7012             December          7271  Temperature change   \ufffdC\n23          2  Afghanistan         7012             December          6078  Standard Deviation   \ufffdC\n24          2  Afghanistan         7016          Dec\ufffdJan\ufffdFeb          7271  Temperature change   \ufffdC\n25          2  Afghanistan         7016          Dec\ufffdJan\ufffdFeb          6078  Standard Deviation   \ufffdC\n26          2  Afghanistan         7017          Mar\ufffdApr\ufffdMay          7271  Temperature change   \ufffdC\n27          2  Afghanistan         7017          Mar\ufffdApr\ufffdMay          6078  Standard Deviation   \ufffdC\n28          2  Afghanistan         7018          Jun\ufffdJul\ufffdAug          7271  Temperature change   \ufffdC\n29          2  Afghanistan         7018          Jun\ufffdJul\ufffdAug          6078  Standard Deviation   \ufffdC\n30          2  Afghanistan         7019          Sep\ufffdOct\ufffdNov          7271  Temperature change   \ufffdC\n31          2  Afghanistan         7019          Sep\ufffdOct\ufffdNov          6078  Standard Deviation   \ufffdC\n32          2  Afghanistan         7020  Meteorological year          7271  Temperature change   \ufffdC\n33          2  Afghanistan         7020  Meteorological year          6078  Standard Deviation   \ufffdC\n34          3      Albania         7001              January          7271  Temperature change   \ufffdC\n", "    Area Code         Area  Months Code               Months  Element Code             Element Unit\n22          2  Afghanistan         7012             December          7271  Temperature change   \u00b0C\n23          2  Afghanistan         7012             December          6078  Standard Deviation   \u00b0C\n24          2  Afghanistan         7016          Dec\u2013Jan\u2013Feb          7271  Temperature change   \u00b0C\n25          2  Afghanistan         7016          Dec\u2013Jan\u2013Feb          6078  Standard Deviation   \u00b0C\n26          2  Afghanistan         7017          Mar\u2013Apr\u2013May          7271  Temperature change   \u00b0C\n27          2  Afghanistan         7017          Mar\u2013Apr\u2013May          6078  Standard Deviation   \u00b0C\n28          2  Afghanistan         7018          Jun\u2013Jul\u2013Aug          7271  Temperature change   \u00b0C\n29          2  Afghanistan         7018          Jun\u2013Jul\u2013Aug          6078  Standard Deviation   \u00b0C\n30          2  Afghanistan         7019          Sep\u2013Oct\u2013Nov          7271  Temperature change   \u00b0C\n31          2  Afghanistan         7019          Sep\u2013Oct\u2013Nov          6078  Standard Deviation   \u00b0C\n32          2  Afghanistan         7020  Meteorological year          7271  Temperature change   \u00b0C\n33          2  Afghanistan         7020  Meteorological year          6078  Standard Deviation   \u00b0C\n34          3      Albania         7001              January          7271  Temperature change   \u00b0C\n", "    Area Code         Area  Months Code    Months  Element Code             Element Unit\n22          2  Afghanistan         7012  December          7271  Temperature change   \u00b0C\n23          2  Afghanistan         7012  December          6078  Standard Deviation   \u00b0C\n34          3      Albania         7001   January          7271  Temperature change   \u00b0C\n35          3      Albania         7001   January          6078  Standard Deviation   \u00b0C\n36          3      Albania         7002  February          7271  Temperature change   \u00b0C\n37          3      Albania         7002  February          6078  Standard Deviation   \u00b0C\n38          3      Albania         7003     March          7271  Temperature change   \u00b0C\n39          3      Albania         7003     March          6078  Standard Deviation   \u00b0C\n40          3      Albania         7004     April          7271  Temperature change   \u00b0C\n41          3      Albania         7004     April          6078  Standard Deviation   \u00b0C\n42          3      Albania         7005       May          7271  Temperature change   \u00b0C\n43          3      Albania         7005       May          6078  Standard Deviation   \u00b0C\n44          3      Albania         7006      June          7271  Temperature change   \u00b0C\n"], "link": "https://stackoverflow.com/questions/67145310/how-to-remove-undesired-values-from-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a column in a dataframe called new_data. The dates are in the format 2019-08-01. I just want to change all the dates into just , etc.. I tried this looping through rows and replace, no luck I just the the dates column to say either , , etc... TypeError: cannot unpack non-iterable datetime.date object", "q_apis": "get columns all replace date", "apis": "DataFrame columns astype", "code": ["df = pd.DataFrame([['2018-01-01']], columns=['Date'])\ndf['Quarter'] = 'Q'+pd.to_datetime(df['Date']).dt.quarter.astype(str)\n", "    Date        Quarter\n0   2018-01-01  Q1\n"], "link": "https://stackoverflow.com/questions/57634283/changing-dates-into-quarter"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to compare two dataframes with time objects.", "q_apis": "get columns compare time", "apis": "apply apply", "code": ["df = df.apply(lambda x: x.dt.time)\n", "df = df.apply(lambda x: pd.to_timedelta(x.dt.strtime('%H:%M:%S')))\n"], "link": "https://stackoverflow.com/questions/64222477/how-do-you-convert-a-dataframe-of-datetime-to-time-only-basically-removing-the"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's say I have a DataFrame containing multiple rows with different phrases separated with commas like this: I want to order so that rows that have 'bird' in the column are on the first rows like this: How can I do this? Thanks in advance!", "q_apis": "get columns DataFrame first", "apis": "assign", "code": ["df = df.iloc[(~df['phrase'].str.contains('bird')).argsort()]\nprint (df)\n          phrase\n1      bird, cat\n2      cow, bird\n4           bird\n0  dog, cat, cow\n3       dog, cow\n", "a = df['phrase'].str.contains('bird')\nb = (~df['phrase'].str.contains('bird'))\nc = (~df['phrase'].str.contains('bird')).argsort()\nprint (df.assign(contains=a, invert=b, argsort=c))\n          phrase  contains  invert  argsort\n0  dog, cat, cow     False    True        1\n1      bird, cat      True   False        2\n2      cow, bird      True   False        4\n3       dog, cow     False    True        0\n4           bird      True   False        3\n"], "link": "https://stackoverflow.com/questions/56183057/order-columns-containing-a-string-first-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with Reddit data including an author and a subreddit field. I would like to get, per author, a distribution of how often he/she posts in each subreddit which can be compared to the distribution of other authors. That line gives me a multi-indexed pandas dataframe with author as the first index. Then all subreddits in which the author was active as second index and finally as values the fraction of their posts which were in that subreddit. That's good, but I would like to end up with distributions of equal length for each author. To do this I included all subreddits from the entire df (rather than only those visited by the author), in a dataframe with the authors as rows. Then I fill this with zeros, and then with the values from sub_visits. This for loop technically works for this. It is quite slow however, for just filling in values. I wonder if there is a better way to do this? Either creating the distribution, or filling in the values. Many thanks", "q_apis": "get columns get first index all second index values length all values values values", "apis": "groupby value_counts groupby count unstack fillna", "code": ["sub_visits = df.groupby('author').subreddit.value_counts()/df.groupby('author').subreddit.count()\nsub_visits = sub_visits.unstack(-1)\nsub_visits = sub_visits.fillna(0)\n\n"], "link": "https://stackoverflow.com/questions/62898126/getting-df-with-distribution-per-group-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to merge two dataframes. I create several of the below dataframe from reading files. What i need to do is pull the 'Depth' column and insert it into a new dataframe. I will then rename the column 'Depth' in the merged dataframe to the serial number of that part. Then repeat. sigData example The resulting dataframe after looping through all 'sigData' files should look like this: depthDF example I will do the same with 'Current' and 'Velocity'. The result should be three dataframes. One with the 'Depth' of all parts, one with 'Velocity' of all parts, and one with 'Current' of all parts. results in: and results in:", "q_apis": "get columns merge insert rename repeat all all all all", "apis": "merge", "code": ["depthDF = depthDF.merge(sigData[['Depth','Time']], on='Time', sort='True', how='right')\n"], "link": "https://stackoverflow.com/questions/61781811/python-dataframe-merging"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In a pandas dataframe, I want to create a new column that calculates the average of column values of 4th, 8th and 12th row before our present row. As shown in the table below, for row number 13 : Value in Existing column that is 4 rows before row 13 (row 9) = 4 Value in Existing column that is 8 rows before row 13 (row 5) = 6 Value in Existing column that is 12 rows before row 13 (row 1) = 2 Average of 4,6,2 is 4. Hence New Column = 4 at row number 13, for the remaining rows between 1-12, New Column = Nan I have more rows in my df, but I added only first 13 rows here for illustration. Row number Existing column New column 1 2 NaN 2 4 NaN 3 3 NaN 4 1 NaN 5 6 NaN 6 4 NaN 7 8 NaN 8 2 NaN 9 4 NaN 10 9 NaN 11 2 NaN 12 4 NaN 13 3 3", "q_apis": "get columns values at between first", "apis": "groupby apply shift shift shift", "code": ["df['New column'] = df.groupby((df['Row number'] - 1) // 13)['Existing column'].apply(lambda x: (x.shift(4) + x.shift(8) + x.shift(12)) / 3)\n", "(df['Row number'] - 1) // 13", "data = {'Row number' : np.arange(1, 40), 'Existing column': np.arange(11, 50) }\ndf = pd.DataFrame(data)\n\nprint(df)\n\n    Row number  Existing column\n0            1               11\n1            2               12\n2            3               13\n3            4               14\n4            5               15\n5            6               16\n6            7               17\n7            8               18\n8            9               19\n9           10               20\n10          11               21\n11          12               22\n12          13               23\n13          14               24\n14          15               25\n15          16               26\n16          17               27\n17          18               28\n18          19               29\n19          20               30\n20          21               31\n21          22               32\n22          23               33\n23          24               34\n24          25               35\n25          26               36\n26          27               37\n27          28               38\n28          29               39\n29          30               40\n30          31               41\n31          32               42\n32          33               43\n33          34               44\n34          35               45\n35          36               46\n36          37               47\n37          38               48\n38          39               49\n\ndf['New column'] = df.groupby((df['Row number'] - 1) // 13)['Existing column'].apply(lambda x: (x.shift(4) + x.shift(8) + x.shift(12)) / 3)\n\nprint(df)\n\n    Row number  Existing column  New column\n0            1               11         NaN\n1            2               12         NaN\n2            3               13         NaN\n3            4               14         NaN\n4            5               15         NaN\n5            6               16         NaN\n6            7               17         NaN\n7            8               18         NaN\n8            9               19         NaN\n9           10               20         NaN\n10          11               21         NaN\n11          12               22         NaN\n12          13               23        15.0\n13          14               24         NaN\n14          15               25         NaN\n15          16               26         NaN\n16          17               27         NaN\n17          18               28         NaN\n18          19               29         NaN\n19          20               30         NaN\n20          21               31         NaN\n21          22               32         NaN\n22          23               33         NaN\n23          24               34         NaN\n24          25               35         NaN\n25          26               36        28.0\n26          27               37         NaN\n27          28               38         NaN\n28          29               39         NaN\n29          30               40         NaN\n30          31               41         NaN\n31          32               42         NaN\n32          33               43         NaN\n33          34               44         NaN\n34          35               45         NaN\n35          36               46         NaN\n36          37               47         NaN\n37          38               48         NaN\n38          39               49        41.0\n"], "link": "https://stackoverflow.com/questions/67183056/for-a-column-in-pandas-dataframe-calculate-mean-of-column-values-in-previous-4t"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Good Afternoon, I want to compare dataframe \"new\" against dataframe \"old\" to get a new dataframe with data that only exists in \"new\" but not old. For example What I did at first (forgive me, I'm new to this) was: What I failed to realize, of course, is that there are values in 'old' that may not be in 'new' and per my code, those values would also show up in 'final' - which I don't want. If anyone can point me in the right direction, any help is always appreciated!", "q_apis": "get columns compare get at first values values right any", "apis": "DataFrame columns DataFrame columns isin rename columns", "code": ["df1 = pd.DataFrame(data=[1,2,3,4,5,7,8,9], columns=['New'])\ndf2 = pd.DataFrame(data=[1,2,3,5,8,9,0], columns=['Old'])\n\ndf1_unique = set(df1['New']) - set(df2['Old'])\nfinal_df= df1[df1['New'].isin(df1_unique)]\nfinal_df.rename(columns = {'New' : 'Desired Output'}, inplace=True)\nprint(final_df)\n"], "link": "https://stackoverflow.com/questions/58107652/comparing-one-dataframe-against-another-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have written a code which basically selects the first pdf from all folders and then extracts text data from all the pdfs. I am saving these extracted pdf text data to a dataframe. I also want to save the names of the pdf files to my dataframe, the problem is that, it keeps writing the recent pdf file name in my dataframe and ignores the rest of the pdf file names. Here's my code: I have tried creating a dataframe for capturing pdf file names in the if loop and later append it to text data, but that doesn't get the proper ouput(As it keeps creating a new dataframe everytime in the if loop). I have also tried creating an empty list first and then appending the filenames but that gives me a None in the output. Like this This gives me an output like The output that I want should look something like The output that I get is : Please help me understand how do I make it work in the correct way.", "q_apis": "get columns first all all names name names names append get empty first get", "apis": "first append append first append append DataFrame DataFrame", "code": ["# print file name of first PDF found in this folder\nprint('PDF-Names--', firstpdfs)\n# append file name to list of files\nfiles.append(firstpdfs)\n", "{'PDF NAME': firstpdfs, 'Text Data': [allmypdfs]}", "data_list = []\nfor folder_name in folders:\n    # TODO: first PDF files in the directory <folder_name> and return <file_name>\n    with open(file_name, 'rb') as fh:\n        # TODO: extract text from file and return as binary string in variable <text>\n    # store data in a dictionary and append to the list\n    data_list.append({'PDF NAME': file_name, 'Text Data': text})\n# convert list of dictionaries to pandas.DataFrame object\ndata = pd.DataFrame(data_list)\n"], "link": "https://stackoverflow.com/questions/68312958/problem-in-saving-filenames-in-my-list-using-if-loop-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Pandas CSV file and I would like to know how to create a Python user search function to find a row. Below is a sample of the CSV - I would like to create a function whereby it ask the user for the ICAO code, which is one of the columns, then it returns the whole row of information. For example if someone typed EHAM it would return all the information in that row (Position, ICAO, Airport, Country, Total Movements, Position Change in the last 24hrs) As a bonus but I am not sure it is possible, I would also love to show the 2 rows above and 2 rows below the requested search when displaying the results. So for example it would show not only EHAM, but also EDDF, EGKK (2 rows above) and also KBOS and KATL (2 rows below)", "q_apis": "get columns sample columns all last", "apis": "iloc index index", "code": ["icao = input(\"Enter the ICAO code: \")\nprint(df[df[\"ICAO\"] == icao]])\n", "icao = input(\"Enter the ICAO code: \")\nrequested = df[df[\"ICAO\"] == icao]]\nprint(df.iloc[requested.index[0] - 2 : requested.index[0] + 3, :])\n"], "link": "https://stackoverflow.com/questions/65308727/python-pandas-dataframe-user-search-function"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes as below. I want to add one column in dataframe . Using and number to select value in dataframe , such as in row zero date is 20130101, ranking is 3, select the third biggest number in dataframe in row zero( they have the same date) and return 0.24. How do i achieve the last dataframe output which i have added as the last one?", "q_apis": "get columns add select value date select date last last", "apis": "columns columns values sub values", "code": ["print (df)\n       date  part1  part2  part3  part4\n0  20130101  -0.17   0.67   0.70  0.240\n1  20130102  -1.03  -0.03  -3.00 -0.440\n2  20130103   1.59   1.95   1.50  1.335\n3  20130104  -0.05  -3.25  -0.25 -0.450\n4  20130105  -0.10  -0.30  -0.37 -0.570\n5  20130107   0.90   0.60   0.62  0.920 <-not matched date\n", "df1 = df2.merge(df, on='date', how='left')\nprint (df1)\n       date  ranking  part1  part2  part3  part4\n0  20130101        3  -0.17   0.67   0.70  0.240\n1  20130102        4  -1.03  -0.03  -3.00 -0.440\n2  20130103        1   1.59   1.95   1.50  1.335\n3  20130104        4  -0.05  -3.25  -0.25 -0.450\n4  20130105        2  -0.10  -0.30  -0.37 -0.570\n5  20130106        3    NaN    NaN    NaN    NaN\n", "cols = df1.columns.difference(df2.columns)\n\na = np.argsort(-df1[cols].values, axis=1)[np.arange(len(df1)), df1['ranking'].sub(1)]\ndf1['new'] = df1[cols].values[np.arange(len(df1)), a]\n", "df1 = df1.drop(cols, axis=1)\nprint (df1)\n       date  ranking   new\n0  20130101        3  0.24\n1  20130102        4 -3.00\n2  20130103        1  1.95\n3  20130104        4 -3.25\n4  20130105        2 -0.30\n5  20130106        3   NaN\n"], "link": "https://stackoverflow.com/questions/52290510/python-use-rank-number-to-select-value-between-two-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Is there a way to search for a string or substring in the column name and extract the entire column which name contains that particular string? My data: I want to search for \"total\" in the data frame and extract the entire column (the last column in this case) Thank you in advance!", "q_apis": "get columns name name contains last", "apis": "columns loc", "code": ["Description,Qty,Unit Cost (AED), Total Cost (AED), Amount (xyz)\nstring 1, 3, 3000, 9000, 9500\nstring 1, 3, 3000, 9000, 9500\nstring 1, 3, 3000, 9000, 9500\nstring 1, 3, 3000, 9000, 9500\nstring 1, 3, 3000, 9000, 9500\nstring 1, 3, 3000, 9000, 9500\n", "import pandas as pd\nimport re\n\ndf = pd.read_csv('test.csv')\nprint(df)\n\ncol = [name for name in df.columns if len(re.findall(r'\\b(?:total|amount)\\b', name.lower()))!=0]\n\nif len(col)!=0:\n    print(df.loc[:, col])\n"], "link": "https://stackoverflow.com/questions/65770804/search-for-string-in-column-name-and-return-the-entire-colum"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas data frame that looks something like this: As you may note, in the very last column it has a pattern of zeroes and ones. Is it possible to split this data frame into two sub-data frames? My desired output will be: df1: df2: will definitely not work, as it will just create two big dataframes; one with ones, the second one with zeroes. I am not interested in keeping data marked as zeroes. Thanks in advance! PS. In reality this dataframe is much bigger, so I am trying to create df1, df2, ... dfn", "q_apis": "get columns last sub second", "apis": "mask eq mask bool", "code": ["mask = df['result'].eq(1)\na = pd.factorize(df['result'].eq(0).cumsum()[mask])[0]\ndfs = dict(tuple(df[mask].groupby(a)))\nprint (dfs[0])\n   v1  v2  v3  result\n1  34  52   4       1\n2  32   4   5       1\n\nprint (dfs[1])\n   v1  v2  v3  result\n4   5  17   8       1\n5  11  25  23       1\n6   2  32  34       1\n", "mask = df['result'].eq(1)\nprint (mask)\n0    False\n1     True\n2     True\n3    False\n4     True\n5     True\n6     True\n7    False\nName: result, dtype: bool\n", "print (df['result'].eq(0).cumsum())\n0    1\n1    1\n2    1\n3    2\n4    2\n5    2\n6    2\n7    3\nName: result, dtype: int32\n", "print (df['result'].eq(0).cumsum()[mask])\n1    1\n2    1\n4    2\n5    2\n6    2\nName: result, dtype: int32\n", "a  = pd.factorize(df['result'].eq(0).cumsum()[mask])[0]\nprint (a)\n[0 0 1 1 1]\n", "dfs = dict(tuple(df[mask].groupby(a)))\nprint (dfs)\n{0:    v1  v2  v3  result\n1  34  52   4       1\n2  32   4   5       1, 1:    v1  v2  v3  result\n4   5  17   8       1\n5  11  25  23       1\n6   2  32  34       1}\n"], "link": "https://stackoverflow.com/questions/52290511/how-to-split-pandas-dataframe-using-periodic-values-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following data frame in Pandas... I want to perform the following code but I'm getting an error... I'm getting the following error... I checked the and is a int64. I can't figure out what is wrong with the code.", "q_apis": "get columns", "apis": "count apply count sum", "code": ["death_2013['percent_of_total'] = death_2013['count'].apply(\n    lambda x: (x / death_2013['count'].sum()))"], "link": "https://stackoverflow.com/questions/42602399/attributeerror-function-object-has-no-attribute-sum-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like below. The 'LATENCY' column has both numerical and 'NA' characters, that makes groupby() complex. I wanted to group the dataframe by 'DEVICE' value to the sum of 'LATENCY' - by bypassing 'NA' values. I should get the output as below.", "q_apis": "get columns groupby value sum values get", "apis": "groupby sum reset_index", "code": ["df['LATENCY'] = pd.to_numeric(df['LATENCY'], errors='coerce')\n\nres = df.groupby('DEVICE')['LATENCY'].sum().reset_index()\n\nprint(res)\n\n        DEVICE  LATENCY\n0    ab.fxx.in      6.0\n1  kddo.fxx.in      8.0\n"], "link": "https://stackoverflow.com/questions/50657545/pandas-dataframe-groupby-sum-while-ignoring-non-numerical-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm new to python Pandas. I faced a problem to find the difference for 2 lists within a Pandas DataFrame. Example Input with separator: Expected Output: What I want to do is similar to: But it returns an error: raise ValueError('Length of values does not match length of index',data,index,len(data),len(index)) Kindly advise", "q_apis": "get columns difference DataFrame values length index index index", "apis": "DataFrame DataFrame columns apply get apply", "code": ["import pandas as pd\n\n# creating DataFrame (can also be loaded from a file)\ndf = pd.DataFrame([[['A','B','C','D'], ['B','C']]], columns=['ColA','ColB'])\n\n# apply a lambda function to get the difference\ndf['ColC'] = df[['ColA','ColB']].apply(lambda x: [i for i in x[0] if i not in x[1]], axis=1)\n"], "link": "https://stackoverflow.com/questions/56986689/how-to-get-the-difference-of-2-lists-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hi I need to filter out dataframe based on latest file. Could you please help me how to do this? Example: In output I want- I need to filter this dataframe based on latest filename.", "q_apis": "get columns filter filter", "apis": "sort_values head", "code": ["out = df.sort_values(\n    by=\"filename\", key=lambda s: s.str.split(\"_\").str[-1], ascending=False\n).head(1)\nprint(out)\n", "                                            filename Is_Active Program Type\n0  SR0661000_risdiplam__Consolidated_Patient_Inpu...         A     Oncology\n"], "link": "https://stackoverflow.com/questions/67967795/how-to-filter-out-dataframe-based-on-latest-file-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have created a dataframe from csv which has values like this based on comma separated values I want to create sub columns like shown in the diagram. No of comma separated will remain same for all. For e.g. If one location value has 2 fields then all of them will have 2 fields. So I need to separate these values by comma and create sub columns based on that.", "q_apis": "get columns values values sub columns all value all values sub columns", "apis": "DataFrame astype rename columns keys columns", "code": ["dfs = [pd.DataFrame(df[c].astype(str).str.split(',').tolist())\n                   .rename(lambda x: f'Field{x + 1}', axis=1) for c in df.columns]      \ndf1 = pd.concat(dfs, axis=1, keys=df.columns)\n", "    position               location       \n    Field1 Field2 Field3   Field1 Field2\n0        1      0      1      1.2    2.2\n"], "link": "https://stackoverflow.com/questions/63395978/create-sub-column-in-csv-based-on-field-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I came across this problem at work today and I would like to know if there is an efficient way to do this. Basically I have a dataframe which look like this I also have a function that returns a tuple. (Make note that this is only a minimal example, my problem is different) I need to find a way to do something that would ideally look like this: Unfortunately, this syntax doesn't work and I can't wrap my head around another way to do it. The only similar problem is this , but the solution seems really 'hacky' and I am convinced there is a better way to do it. Thank you!!", "q_apis": "get columns at today head", "apis": "DataFrame apply DataFrame apply values", "code": ["compute = lambda x: (2*x, 3*x)\ndf[['b','c']]=pd.DataFrame(df.a.apply(compute).tolist()) #thanks harvpan\n#df[['b','c']]=pd.DataFrame(df.a.apply(compute).values.tolist())\nprint(df)\n", "   a  b  c\n0  1  2  3\n1  2  4  6\n2  3  6  9\n"], "link": "https://stackoverflow.com/questions/56775952/unpacking-a-tuple-on-multiple-columns-of-a-dataframe-from-series-apply"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame and only have the company name \"CC\" , i want to get all company names for the group name it belongs i.e ( AA,BB,CC,DD and EE data frame) condition : only company name are known rest of the data of columns gets change every week. I tried it gave me the group number, I can find the next dataframe by this group number but the challenge is that it changes every day, so how can solve this?", "q_apis": "get columns name get all names name name columns week day", "apis": "drop_duplicates set_index groupby apply isin", "code": ["co_grp = df.drop_duplicates('Company').set_index('Company')['GroupNumber']\ngrp_cos = df.groupby('GroupNumber')['Company'].apply(list)\n\nres = df[df['Company'].isin(grp_cos[co_grp['CC']])]\n\nprint(res)\n\n   S.no Company      City Vendor GroupNumber    Category\n0     1      AA     Delhi  Micro     9188-SC  BBRNC011TR\n1     2      BB     Delhi  Micro     9188-SC  BBRNC011TR\n2     3      CC  Banglore  Micro     9188-SC  BBRNC011TR\n3     4      DD  Banglore  Micro     9188-SC  BBRNC011TR\n4     5      EE    Mumbai  Micro     9188-SC  BBRNC011TR\n"], "link": "https://stackoverflow.com/questions/51728853/pandas-how-to-use-value-of-a-variable-while-making-a-new-data-frame-using-loc"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with 8 columns and ~0.8 million rows. I want to find the mode of every 50 rows of a specific column (e.g. Column 5) in a separate dataframe. My approach looks like this. But I get \"'int' object does not support item assignment\" error. My df looks like the below I have written the same function in R. And R returns the latest (last) mode value as a single output for every set of 50.", "q_apis": "get columns columns mode get item last mode value", "apis": "groupby apply mode", "code": ["df = df.groupby(np.arange(len(df)) // 50)['Col5'].apply(lambda x: x.mode())\n", "df = df.groupby(np.arange(len(df)) // 5)['Col5'].apply(lambda x: x.mode())\nprint (df)\n0  0    101038\n   1    101043\n1  0    101040\n2  0    101040\nName: Col5, dtype: int64\n", "df = df.groupby(np.arange(len(df)) // 5)['Col5'].apply(lambda x: x.mode().tolist())\nprint (df)\n0    [101038, 101043]\n1            [101040]\n2            [101040]\nName: Col5, dtype: object\n"], "link": "https://stackoverflow.com/questions/50246911/how-to-find-mode-of-every-n-50-rows-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to subset a dataframe based on a column with cumulative values (the column \"value\"). My dummy dataframe is: expected output: I have already tried: But it does not keep the rows with values lower than maximum. As far as I know, if you change n to higher values you will get nth highest values but the point is that I have no idea about the range between the first row and the highest value of value. Any help is highly appreciated. Omid.", "q_apis": "get columns values value values values get nth values between first value value", "apis": "groupby transform max ne duplicated", "code": ["max_m = (\n    df.groupby(['x', 'y', 'g1', 'g2'])['value']\n        .transform('max')\n        .ne(df['value'])\n)\ndup_m = ~df['value'].duplicated()\nfiltered_df = df[max_m | dup_m]\n", "       x      y g1  g2  value\n0  24.25  50.65  a   1     25\n1  24.25  50.65  a   1     28\n2  24.25  50.65  a   1     29\n5  24.25  50.65  b   1      3\n6  24.25  50.65  b   1      4\n7  24.25  50.65  b   1      5\n", "       x      y g1  g2  value\n0  24.25  50.65  a   1     25\n1  24.25  50.65  a   1     29  # Max\n2  24.25  50.65  a   1     25  # Duplicated but not Max\n3  24.25  50.65  a   1     28\n4  24.25  50.65  a   1     29  # Max (2)\n5  24.25  50.65  b   1      3\n6  24.25  50.65  b   1      4\n7  24.25  50.65  b   1      5\n8  24.25  50.65  b   1      5\n", "       x      y g1  g2  value\n0  24.25  50.65  a   1     25\n1  24.25  50.65  a   1     29  # First Max is kept\n2  24.25  50.65  a   1     25  # Duplicated but not Max (kept)\n3  24.25  50.65  a   1     28\n5  24.25  50.65  b   1      3\n6  24.25  50.65  b   1      4\n7  24.25  50.65  b   1      5\n"], "link": "https://stackoverflow.com/questions/67950804/how-to-subset-dataframe-from-first-row-to-the-highest-value-in-a-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i'm using a list of urls in a csv file to crawl and extract data from a html table. i want to stop going through the urls when 'style3' is not present in the table. I've created a function that will return false if it's not there, but i'm confused as to how to actually implement it. Any suggestions for a solution or directions to literature will help greatly as i've not been able to find anything on here to help me figure it out. i've included 1 url with 'style3' and 1 without. Thanks for any and all help. http://www.wvlabor.com/new_searches/contractor_RESULTS.cfm?wvnumber=WV057808&contractor_name=&dba=&city_name=&County=&Submit3=Search+Contractors http://www.wvlabor.com/new_searches/contractor_RESULTS.cfm?wvnumber=WV057924&contractor_name=&dba=&city_name=&County=&Submit3=Search+Contractors", "q_apis": "get columns stop any all", "apis": "mode any append last append to_csv index", "code": ["lst = []\nwith open('WV_urls.csv','r') as csvf: # Open file in read mode\n    urls = csv.reader(csvf)\n    for url in urls:\n        page = urlopen(url[0]).read()\n        df1, header = pd.read_html(page, header=0)\n        if license_exists(BS(page, \u2018html.parser\u2019)):\n            # if the license is present we don't want to parse any more urls.\n            # Note: we don't append this last result (should we?)\n            break\n        lst.append(df1)\n\ndf = pd.concat(lst)\ndf.to_csv('WV_Licenses_Daily.csv', index=False)\n"], "link": "https://stackoverflow.com/questions/52452491/stop-crawling-urls-if-class-does-not-exist-in-table-beautifulsoup-and-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I made a function that detects outliers of each columns in dataset, calling it over and over again is not necessary so i made a loop that iterates the function for each columns in dataset. It returns error , i do not know why this happens, it only errors when i use the loop but when calling it on its own with the column as the parameter like the error does not happen. How to fix this?", "q_apis": "get columns columns columns", "apis": "columns columns", "code": ["for column in avo_sales.columns[2:11]:\n    detectoutliers(column)\n", "for column in avo_sales.columns[2:11]:\n        detectoutliers(avo_sales[column])"], "link": "https://stackoverflow.com/questions/60635970/error-in-looping-function-to-detect-outliers-in-each-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a csv that contains 12 cols and 4 rows of data. As seen in the img I would like to divide each of those values by their area of which I have created an array, and then multiply by 100 to get a % and have these values in a new column. Image for array So for example, A2, A3, A4, will all be divided by 52,600 and then x100. My current df looks like this dataframe", "q_apis": "get columns contains values array get values array all", "apis": "columns round", "code": ["    import numpy as np\n    import pandas as pd\n\n    df = pd.read_excel(\"d:\\stack67477476.xlsx\")\n    area_arr = np.array([[52.6, 14.966, 7.702, 4.169, 3.71, 5.648, 6.785, 1.867, 5.268, 4.989, 1.659, 6.538]])\n    for ii, col in enumerate(df.columns):\n        if ii == 0:\n            continue\n        df[col + \"_Area\"] = round(df[col] / area_arr[0][ii - 1] * 100, 2)\n", "df\n   Label  Baseline   Sub_A   Sub_B   Sub_C   Sub_D   Sub_E   Sub_F   Sub_G   Sub_H   Sub_I  ...  Sub_A_Area  Sub_B_Area  Sub_C_Area  Sub_D_Area  Sub_E_Area  Sub_F_Area  Sub_G_Area  Sub_H_Area  Sub_I_Area  Sub_J_Area  Sub_K_Area\n0      0         0   15535    5128    8847   10784    5679   20481    8398   10012    5162  ...   103801.95    66580.11   212209.16   290673.85   100548.87   301857.04   449812.53   190053.15   103467.63   275527.43   380177.42\n1      1    159506  149454  157456  155680  154327  154671  146863  150761  150446  155335  ...   998623.55  2044352.12  3734228.83  4159757.41  2738509.21  2164524.69  8075040.17  2855846.62  3113549.81  9387040.39  1963949.22\n2      2    129087  111918  121515  122066  119557  123813  114746  123140  122156  125480  ...   747815.05  1577707.09  2927944.35  3222560.65  2192156.52  1691171.70  6595607.93  2318830.68  2515133.29  7608679.93  1653533.19\n3      3    137562  102318  114509  124641  127442  130324  123331  130392  130715  134528  ...   683669.65  1486743.70  2989709.76  3435094.34  2307436.26  1817700.81  6984038.56  2481302.20  2696492.28  8123206.75  1881890.49\n4      4     35901   26488   30836   33756   34549   34000   33269   34071   34151   35149  ...   176987.84   400363.54   809690.57   931239.89   601983.00   490331.61  1824906.27   648272.59   704529.97  2146473.78   531691.65\n\n[5 rows x 25 columns]\n", "    area_arr = np.array([52.6, 14.966, 7.702, 4.169, 3.71, 5.648, 6.785, 1.867, 5.268, 4.989, 1.659, 6.538])\n"], "link": "https://stackoverflow.com/questions/67477476/how-to-perform-calculations-on-csv-rows-and-cols-and-create-new-cols-using-numpy"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I try to calculate number of days until and since last and next holiday. My method of calculation it is like below: Nevertheless, I have en error like below: Why I have this erro ? I know that row is tuple, but i use in my code .iloc[0] and .iloc[-1] ? WHat can I do ?", "q_apis": "get columns days last iloc iloc", "apis": "sort_values assign shift shift head", "code": ["import pandas\nholidays = pandas.Series(pandas.to_datetime([\n        \"01.01.2013\", \"06.01.2013\", \"14.02.2013\",\"29.03.2013\",\n        \"31.03.2013\", \"01.04.2013\", \"01.05.2013\", \"03.05.2013\",\n        \"19.05.2013\", \"26.05.2013\", \"30.05.2013\", \"23.06.2013\",\n        \"15.07.2013\", \"27.10.2013\", \"01.11.2013\", \"11.11.2013\",\n        \"24.12.2013\", \"25.12.2013\", \"26.12.2013\", \"31.12.2013\",\n        \"01.01.2014\", \"06.01.2014\", \"14.02.2014\", \"30.03.2014\",\n        \"18.04.2014\", \"20.04.2014\", \"21.04.2014\", \"01.05.2014\",\n        \"03.05.2014\", \"03.05.2014\", \"26.05.2014\", \"08.06.2014\",\n        \"19.06.2014\", \"23.06.2014\", \"15.08.2014\", \"26.10.2014\",\n        \"01.11.2014\", \"11.11.2014\", \"24.12.2014\", \"25.12.2014\",\n        \"26.12.2014\", \"31.12.2014\",\n        \"01.01.2015\", \"06.01.2015\", \"14.02.2015\", \"29.03.2015\",\n        \"03.04.2015\", \"05.04.2015\", \"06.04.2015\", \"01.05.2015\",\n        \"03.05.2015\", \"24.05.2015\", \"26.05.2015\", \"04.06.2015\",\n        \"23.06.2015\", \"15.08.2015\", \"25.10.2015\", \"01.11.2015\",\n        \"11.11.2015\", \"24.12.2015\", \"25.12.2015\", \"26.12.2015\",\n        \"31.12.2015\"\n    ], dayfirst=True)\n)\n\nresults = (\n    holidays\n    .sort_values()\n    .to_frame('holiday')\n    .assign(\n        days_since_prev=lambda df: df['holiday'] - df['holiday'].shift(1),\n        days_until_next=lambda df: df['holiday'].shift(-1) - df['holiday'],\n    )\n)\n\nresults.head(10)\n", "     holiday days_since_prev days_until_next\n0 2013-01-01             NaT          5 days\n1 2013-01-06          5 days         39 days\n2 2013-02-14         39 days         43 days\n3 2013-03-29         43 days          2 days\n4 2013-03-31          2 days          1 days\n5 2013-04-01          1 days         30 days\n6 2013-05-01         30 days          2 days\n7 2013-05-03          2 days         16 days\n8 2013-05-19         16 days          7 days\n9 2013-05-26          7 days          4 days\n"], "link": "https://stackoverflow.com/questions/66382338/problem-with-tuple-indices-in-loop-in-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe containing unaggregated data, like so: As you can see, corresponding to each UniqueID, there are an arbitrary number of unique values for SrvDesc (HEF104 has 5 unique SrvDesc values, HEF198 has 3, etc.). What I'd like to do is perform some operation that will allow me to aggregate on UniqueID so that there is one row per UniqueID, and then any number of populated columns containing each of the values for SrvDesc for that given UniqueID: I've been looking into and , which seem very useful, but I'm not sure if they would allow me to accomplish exactly what I'm trying to do here. Thanks!", "q_apis": "get columns unique values unique values aggregate any columns values", "apis": "count groupby count count astype set_index count unstack count reset_index columns", "code": ["count UniqueID SrvDesc_1   SrvDesc_2 SrvDesc_3 SrvDesc_4   SrvDesc_5\n0       HEF104      Cash      Credit   Deposit    Ticket  Electronic\n1       HEF197     Check      Credit       NaN       NaN         NaN\n2       HEF198    Credit  Electronic     Check       NaN         NaN\n", "new_df.columns.name=None\nprint(new_df)\n\n      UniqueID SrvDesc_1   SrvDesc_2 SrvDesc_3 SrvDesc_4   SrvDesc_5\n0       HEF104      Cash      Credit   Deposit    Ticket  Electronic\n1       HEF197     Check      Credit       NaN       NaN         NaN\n2       HEF198    Credit  Electronic     Check       NaN         NaN\n", "df2['count']=df2.groupby('UniqueID').cumcount()+1\ndf2['count']='SrvDesc_'+df2['count'].astype('str')\nnew_df=df2.set_index(['UniqueID','count']).unstack('count',fill_value='')['SrvDesc'].reset_index()\nnew_df.columns.name=None\nprint(new_df)\n", "      UniqueID SrvDesc_1   SrvDesc_2 SrvDesc_3 SrvDesc_4   SrvDesc_5\n0       HEF104      Cash      Credit   Deposit    Ticket  Electronic\n1       HEF197     Check      Credit                                \n2       HEF198    Credit  Electronic     Check      \n"], "link": "https://stackoverflow.com/questions/58492804/pandas-aggregating-data-into-multiple-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames with three columns, with identical column names. I want to subtract the value of the third column where the values of the first, and second columns match. I've tried the following: This yields the following output. Observe that rows are sorted alphabetically on . How can I maintain the row order of the left hand operand? I.e. how can get the following output (or similar):", "q_apis": "get columns columns identical names value where values first second columns left get", "apis": "merge loc", "code": ["df3 = pd.merge(df1, df2, on=[\"month\", \"category\"], how=\"outer\")\ndf3.loc[:, \"difference\"] = df3[\"sum_x\"] - df3[\"sum_y\"]\n", "  month category  sum_x  sum_y  difference\n0   jan        j   10.0    9.5         0.5\n1   feb        f   20.0    NaN         NaN\n2   mar        m    NaN   30.0         NaN\n"], "link": "https://stackoverflow.com/questions/65613242/pandas-how-can-i-preserve-row-ordering-after-subtraction"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like below: How can I transform dataframe based on np.NaN values of Gender? I want the original dataframe df to be split into df1(Name,Age,Gender,Height,Date) which will have values of gender(first 3 rows of df) AND into which won't have Gender column (last 3 rows of df)", "q_apis": "get columns transform values values first last", "apis": "DataFrame columns notnull drop", "code": ["import pandas as pd\nimport numpy as np\n\n\ndata = [['lynda', 10,'F',125,'5/21/2018'],['tom', np.nan,'M',135,'7/21/2018'], ['nick', 15,'F',99,'6/21/2018'], ['juli', 14,np.nan,120,'1/21/2018'],['juli', 19,np.nan,140,'10/21/2018'],['juli', 18,np.nan,170,'9/21/2018']]\ndf = pd.DataFrame(data, columns = ['Name', 'Age','Gender','Height','Date'])\n\ndf2 = df[df['Gender'].notnull()].drop(\"Gender\", axis=1)\nprint(df2)\n", "    Name   Age  Height       Date\n0  lynda  10.0     125  5/21/2018\n1    tom   NaN     135  7/21/2018\n2   nick  15.0      99  6/21/2018\n"], "link": "https://stackoverflow.com/questions/54977459/pandas-splitting-a-dataframe-based-on-null-values-in-a-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How to compose each word in the dataframe into a sentence, and generate the next sentence after the period or a question mark? the original dataframe looks like this: the result I want to get looks like this: This is my dataframe: Is there any suggested algorithm to help this problem, thank you very much!", "q_apis": "get columns get any", "apis": "groupby apply bool shift fillna cumsum agg first last join", "code": ["import re\n\npattern = re.compile(r\"\\.|\\!|\\?$\")\n\ndf_out = df.groupby(\n    df.word.apply(lambda x: bool(pattern.search(x))).shift().fillna(0).cumsum()\n).agg({\"start_time\": \"first\", \"end_time\": \"last\", \"word\": \" \".join})\nprint(df_out)\n", "      start_time  end_time            word\nword                                      \n0            0.1       0.6        I AM OK.\n1            0.7       1.2  HOW ABOUT YOU?\n2            1.3       1.4             OK!\n"], "link": "https://stackoverflow.com/questions/68146715/how-to-compose-each-word-in-the-dataframe-into-a-sentence-and-generate-the-next"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm just getting up to speed on Pandas and cannot resolve one issue. I have a list of Counties in NY State. If the County is one of the 5 boroughs, I want to change the county name to New York, otherwise I leave it alone. The following gives the idea, but is not correct. EDIT - so if the counties in the County column of the first few rows were Albany, Allegheny, Bronx before the change, they would be Albany, Allegheny, New York after the change", "q_apis": "get columns name first", "apis": "where isin", "code": ["In [44]: c = ['c', 'g']\nIn [45]: df = pd.DataFrame({'county': list('abccdefggh')})\nIn [46]: df['county'] = df['county'].where(~df['county'].isin(c), 'N')\nIn [47]: df\nOut[47]:   county\n         0      a\n         1      b\n         2      N\n         3      N\n         4      d\n         5      e\n         6      f\n         7      N\n         8      N\n         9      h\n", "nypopdf['County'] = nypopdf['County'].where(~nypopdf['County'].isin(nyCounties), 'New York')\n", "nypopdf = pd.DataFrame({'County': ['Albany', 'Allegheny', 'Bronx']})\nnyCounties = [\"Kings\", \"Queens\", \"Bronx\", \"Richmond\", \"New York\"]\nprint(nypopdf)\n      County\n0     Albany\n1  Allegheny\n2      Bronx\nnypopdf['County'].where(~nypopdf['County'].isin(nyCounties), 'New York', inplace=True)\nprint(nypopdf)\n      County\n0     Albany\n1  Allegheny\n2   New York\n"], "link": "https://stackoverflow.com/questions/53023249/replace-certain-text-with-value-if-text-in-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "everyone, I've encountered some problems when I user panda.dataframework to display data and save to excel file: My goal is save dataframe to excel file, my code is following: And the results is as follows: But I want to keep the column order as same as I provide in dictionary: like this: Do you have any ideas how to solve this problem???", "q_apis": "get columns any", "apis": "DataFrame columns index to_excel index", "code": ["frame = pandas.DataFrame(data_final, columns=['Case index','Similarity', 'Case description'])\n\nwriter = pandas.ExcelWriter(rooter + '\\\\Final_Score_Result.xlsx')\nframe.to_excel(writer, 'Results', index = False)\nwriter.save()\n", "data_final = {'Case index':[1,2,3],\n          'Similarity':[6,7,8],\n          'Case description':['asa','def','gth']}\n\nframe = pandas.DataFrame(data_final, columns=['Case index','Similarity', 'Case description'])\nprint (frame)\n   Case index  Similarity Case description\n0           1           6              asa\n1           2           7              def\n2           3           8              gth\n"], "link": "https://stackoverflow.com/questions/49644188/the-column-order-is-broken-when-i-attempt-to-convert-dataframe-into-excel-file"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "PLEASE READ: I have looked at all the other answers related to this question and none of them solve my specific problem so please carry on reading below. I have the below code. what the code basically does is keeps the column and then concatenated the rest of the columns into one in order to be able to create a cosine matrix. the main point is the function that is suppose to take in a Title for imput and return the top 10 matches based on that title but what i get at the end is the error and i have no idea why.", "q_apis": "get columns at all columns take get at", "apis": "index loc index index size empty index index sort_values iloc index append index index iterrows plot", "code": ["idx = indices[indices == title].index[0]\n", "df.loc[df['Title']=='This is not a valid title'].index[0]\n", "IndexError: index 0 is out of bounds for axis 0 with size 0", "def recommendations(title, cosine_sim = cosine_sim):\n    #print(title)\n    # initializing the empty list of recommended movies\n    recommended_movies = []\n\n    if title not in indices:\n        raise KeyError(\"title is not in indices\")\n\n    # gettin the index of the movie that matches the title\n    idx = indices[indices == title].index[0]\n    print('idx is '+ idx)\n\n    # creating a Series with the similarity scores in descending order\n    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n\n    # getting the indexes of the 10 most similar movies\n    top_10_indexes = list(score_series.iloc[1:11].index)\n\n    # populating the list with the titles of the best 10 matching movies\n    for i in top_10_indexes:\n        recommended_movies.append(list(df.index)[i])\n\n    return recommended_movies\n", "\nfor index, row in df.iterrows():\n    plot = row['Plot']\n"], "link": "https://stackoverflow.com/questions/57145040/index-0-is-out-of-bounds-for-axis-0-with-size-0-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am new to Python and am looking for a simple solution. I have several .csv files with the same structure (number of columns and lines) in one folder. The path is: C:\\temp Now I want to read all these .csv files into a new dataframe, which I want to export later as a new .csv file. up to now i have read each .csv file by hand and saved it into a pandas dataframe. Here is an example: Then I used .append to merge the dataframes. Unfortunately this version always has the header with me, but I don't need it. So I deleted it afterwards by hand. Isn't there a faster version? I'm thinking of a for loop that opens all existing .csv files in the path and reads them line by line into a new dataframe and at the end of the loop makes a .csv file out of it? Unfortunately I have no experience with loops. I appreciate your help.", "q_apis": "get columns columns all now append merge all at", "apis": "append", "code": ["In [1]: import pandas as pd\n\nIn [2]: from io import StringIO\n\nIn [3]: df = pd.read_csv(StringIO(\"\"\"0 id Feature\n   ...: 1 1 12\n   ...: 2 2 13\n   ...: 3 3 14\n   ...: 4 4 15\n   ...: 5 5 16\n   ...: 6 7 17\n   ...: 7 8 15\n   ...: 8 9 12\n   ...: 9 10 13\n   ...: 10 11 23\"\"\"), sep=' ')\n\nIn [4]: df1 = pd.read_csv(StringIO(\"\"\"0   id  Feature\n   ...: 1   1   14\n   ...: 2   2   9\n   ...: 3   3   3\n   ...: 4   4   8\n   ...: 5   5   9\n   ...: 6   7   1\n   ...: 7   8   32\n   ...: 8   9   7\n   ...: 9   10   3\n   ...: 10   11   12\"\"\"), sep='   ')\n\nIn [10]: pd.concat([df, df1])\nOut[10]: \n    0  id  Feature\n0   1   1       12\n1   2   2       13\n2   3   3       14\n3   4   4       15\n4   5   5       16\n5   6   7       17\n6   7   8       15\n7   8   9       12\n8   9  10       13\n9  10  11       23\n0   1   1       14\n1   2   2        9\n2   3   3        3\n3   4   4        8\n4   5   5        9\n5   6   7        1\n6   7   8       32\n7   8   9        7\n8   9  10        3\n9  10  11       12\n\nIn [11]: %timeit pd.concat([df, df1])\n\n188 \u00b5s \u00b1 4.86 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nIn [14]: df.append(df1)\nOut[14]: \n    0  id  Feature\n0   1   1       12\n1   2   2       13\n2   3   3       14\n3   4   4       15\n4   5   5       16\n5   6   7       17\n6   7   8       15\n7   8   9       12\n8   9  10       13\n9  10  11       23\n0   1   1       14\n1   2   2        9\n2   3   3        3\n3   4   4        8\n4   5   5        9\n5   6   7        1\n6   7   8       32\n7   8   9        7\n8   9  10        3\n9  10  11       12\n\nIn [15]: %timeit df.append(df1)\n197 \u00b5s \u00b1 4.09 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n", "filename = ['1.csv', '2.csv']\n\ndfs = []\n\nfor name in filename:\n    dfs.append(pd.read_csv(name))\n\nnew_df = pd.concat(dfs)\n"], "link": "https://stackoverflow.com/questions/64322941/python-merge-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to determine and flag duplicate 'Sample' values in a dataframe using groupby with lambda: How do I make changes / apply the \"DuplicateSample\" to the source rdtRows data? I'm stumped :(", "q_apis": "get columns values groupby apply", "apis": "groupby transform size DataFrame size groupby transform size mean std duplicated mean std duplicated groupby transform mean std", "code": ["df['DuplicateSample'] = df.groupby('Sample')['Sample'].transform('size') > 1\n", "np.random.seed(2020)\nN = 100000\n\ndf = pd.DataFrame({'Sample': np.random.randint(100000, size=N)})\n\nIn [51]: %timeit df['DuplicateSample'] = df.groupby('Sample')['Sample'].transform('size') > 1\n17 ms \u00b1 50 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [52]: %timeit df['DuplicateSample1'] = df['Sample'].duplicated(keep=False)\n3.73 ms \u00b1 40 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n#Stef solution is unfortunately 2734times slowier like duplicated solution\nIn [53]: %timeit df['DuplicateSample2'] = df.groupby('Sample')['Sample'].transform(lambda x: len(x)>1)\n10.2 s \u00b1 517 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"], "link": "https://stackoverflow.com/questions/65314907/how-to-apply-changes-to-subset-dataframe-to-source-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to know if there is a way I can insert values into a new column of my dataframe based on using some sort of code which is similar to the function. My Df looks something like this: df['Count'] is the new empty column I have created where I would like to store the counts of how many times each customer uses the product that has been recorded per row in the 'Product_ID' column. Instead of doing a group-by, I was hoping to use the same df and only fill in the 'Count' column. I would like the df to look something like this: Would anyone happen to know how I can possibly do this? Thank you :)", "q_apis": "get columns insert values empty where product", "apis": "assign groupby transform count", "code": [">>> df.assign(new_count = df.groupby('Customer')['Product_ID'].transform('count')).drop('Count', axis=1)\n   ID Customer Connection Product_ID  new_count\n0  10     AMXX    Instant        AAB          2\n1  11     JKXX       Slow        AAB          1\n2  12     LKXX       Slow        HJA          1\n3  13     AMXX    Instant        AAB          2\n4  14     RFXX       Slow        WRQ          2\n5  15     RFXX    Instant        WRQ          2\n", "df = df.assign(Count = df.groupby('Customer')['Product_ID'].transform('count'))\n"], "link": "https://stackoverflow.com/questions/57678038/how-to-use-a-group-by-like-function-to-insert-values-into-a-new-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My dataframe has 3 buildings and the demand of electric vehicles in kW. Each building can only charge with max 11 kW, so if the demand is higher (eg. 13 kWh) then 13 - 11 = 2 must be added to the value of the next hour. To make it more clear, the dataframe looks like this, and I want transform it like this: I can't add the surplus to the last row, so the rest should be removed.", "q_apis": "get columns max value hour transform add last", "apis": "columns values where append DataFrame columns columns iloc iloc", "code": ["max_charge = 11\nto_add = np.zeros(len(df.columns), dtype=np.int64)\nmaxs = to_add + max_charge\nresult = []\n", "for r in df.values:\n    row = r + to_add\n    to_add = np.where(row > max_charge, row - max_charge, 0)\n    row = np.minimum(row, maxs)\n    result.append(row)\n", "df2 = pd.DataFrame(np.array(result), columns=df.columns)\ndf2.iloc[-1] = df.iloc[-1]\n", "In [6]: df\nOut[6]: \n   bd1  bd2  bd3\n0   11    9    7\n1   15    6   14\n2    8   12   13\n3    2    0    0\n4    8    7   12\n\nIn [7]: df2\nOut[7]: \n   bd1  bd2  bd3\n0   11    9    7\n1   11    6   11\n2   11   11   11\n3    3    1    5\n4    8    7   12\n"], "link": "https://stackoverflow.com/questions/51989046/substract-value-from-dataframe-under-certain-condition-and-add-the-rest-to-the-n"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Python #datas from API Actual Output Expected Output: Here i'm using pandas DataFrame to pass json data. My question is how would i convert Sales_Plan_Details(column) to json object before returning.", "q_apis": "get columns DataFrame", "apis": "DataFrame apply apply to_json", "code": ["import ast, json\n\ndf = pd.DataFrame(rows) \ndf['Sales_Plan_Details'] = df['Sales_Plan_Details'].apply(json.loads)\n#alternative solution\n#df['Sales_Plan_Details'] = df['Sales_Plan_Details'].apply(ast.literal_eval)\n\nj = df.to_json(orient='records')\nprint (j)\n[{\"Sales_Plan_Details\":[{\"Month\":\"2019-1\",\"Quantity\":10,\"Product_Gid\":3}],\n  \"customer_name\":\"ABI2\",\"employee_name\":\"ASU2\",\"location_name\":\"Cherai2\"},\n{\"Sales_Plan_Details\":[{\"Month\":\"2019-1\",\"Quantity\":10,\"Product_Gid\":3}],\n \"customer_name\":\"ABI\",\"employee_name\":\"ASU\",\"location_name\":\"Cherai\"}]\n", "rows= [{\n                    \"customer_name\": \"ABI2\",\n                    \"location_name\": \"Cherai2\",\n                    \"employee_name\": \"ASU2\",\n                    \"Sales_Plan_Details\": \"[{\\\"Month\\\": \\\"2019-1\\\", \\\"Quantity\\\": 10, \\\"Product_Gid\\\": 3}]\"\n\n    },\n{\n                \"customer_name\": \"ABI\",\n                \"location_name\": \"Cherai\",\n                \"employee_name\": \"ASU\",\n                \"Sales_Plan_Details\": \"[{\\\"Month\\\": \\\"2019-1\\\", \\\"Quantity\\\": 10, \\\"Product_Gid\\\": 3}]\"\n\n}]\n"], "link": "https://stackoverflow.com/questions/53845471/convert-column-into-json-using-dataframepython"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with id, value, missing values(this is a %age). I then want to have another column that has range that if the missing value is <=25 then it should return 1 <=50 return 2 <=75 return 3 <=80 return 4 What best way can i do this example of dataframe", "q_apis": "get columns value values value", "apis": "apply", "code": ["def RangeDefiner(val):\n    if val<=25:\n        return 1\n    elif val<=50:\n        return 2\n    elif val<=75:\n        return 3\n    elif val<=80:\n        return 4\n", "df['ranges']=df.apply(lambda x: RangeDefiner(x['missing value']), axis=1)\n", "     id      value    missing value ranges\n0   1245    11558522    34            2\n1   1323    12323552    56            3\n2   1784    13770958    80            4\n3   1557    18412280    5             1\n4   1456    13770958    76            4\n"], "link": "https://stackoverflow.com/questions/60451622/looping-through-a-dataframe-and-extracting-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "That's what I want to achieve I do have a Pandas Dataframe - e.g. this one: Based on 3 conditions the background-color of the cell should be different: cell <= 0 should be in red cell >= 100 should be in blue all other cells That's what I did to achieve that I wrote this function (based on the infos in Pandas documentation Pandas styling: it works fine for two conditions. That's my problem I tried different methods to add the third condition into the function above but I always got back an error. Could you please give me a hint how to add the condition inside the list? I didn't find an answer. Thx a lot.", "q_apis": "get columns all add add", "apis": "append append append style apply", "code": ["def highlight_number(row):\n    arr = []\n    for cell in row:\n        if  cell <= 0:\n            arr.append('background-color: red; color: white')\n        elif cell >= 100:\n            arr.append('background-color: blue; color: white')\n        else:\n            arr.append('background-color: white; color: black')\n    return arr\ndf.style.apply(highlight_number)\n"], "link": "https://stackoverflow.com/questions/64197958/how-can-i-chain-the-conditions-for-style-functions-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to copy the value of cells based on a filter of another cell to specific rows I would like to copy the cells \"Bircher\" and \"Carac\" from rows with the \"Flight Number\" LX2104 to the rows with \"Flight Number\" LX2105\". The values in \"STD Departure\" should stay unchanged", "q_apis": "get columns copy value filter copy values", "apis": "loc values loc values loc values", "code": ["df.loc[df['Flight Number'] == 'LX2104', 'Bircher'] = df[df['Flight Number'] == 'LX2105'].Bircher.values\ndf.loc[df['Flight Number'] == 'LX2104', 'Carac'] = df[df['Flight Number'] == 'LX2105'].Carac.values\n", "  Flight Number  STD Departure  Bircher  Carac\n0        LX2104              0     40.0     40\n1        LX2104              1     20.0     50\n2        LX2104              2     10.0     30\n3        LX2105              0     40.0     40\n4        LX2105              1     20.0     50\n5        LX2105              2     10.0     30\n6        LX2106              0     10.0     40\n7        LX2106              1     30.0     30\n8        LX2106              2     20.0     50\n", "df.loc[df['Flight Number'] == 'LX2104', ['Bircher', 'Carac']] = df[df['Flight Number'] == 'LX2105'][['Bircher', 'Carac']].values\n"], "link": "https://stackoverflow.com/questions/53942819/within-a-dataframe-copy-selected-cells-based-on-filter-critera-to-another-row-w"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe : Notice: The data is grouped by and the is data reported monthly (first of every month). The column is set so each consecutive reported date is a consecutive number in the series. The number of reported dates in each group are different. The interval of reported dates are different for each group (they don't start or end on the same date for each group). The problem: There is no reported data for some dates in the time series. Notice some dates are missing in each group. I want to add a row in each group for those missing dates and have the data reported in and columns as 'NaN'. Example of the dataframe I need: I know how to replace the blank cells with once the rows with missing dates are inserted, using the following code: I also know how to reset the index once the rows with missing dates are inserted, using the following code: However, I'm unsure how to locate the the missing dates in each group and insert the row for those (monthly reported) dates. Any help is appreciated.", "q_apis": "get columns first month date start date time add columns replace index insert", "apis": "reindex index min index max set_index groupby apply drop reset_index groupby", "code": ["df['date'] = pd.to_datetime(df['date'])\n\ndf = (df.set_index('date')\n        .groupby('Serial_no')\n        .apply(lambda x: x.asfreq('MS'))\n        .drop('Serial_no', axis=1))\ndf = df.reset_index()\ndf[\"Index\"] = df.groupby(\"Serial_no\").cumcount() + 1\nprint (df)\n    Serial_no       date  Index     x    y\n0           1 2014-01-01      1   2.0  3.0\n1           1 2014-02-01      2   NaN  NaN\n2           1 2014-03-01      3   3.0  3.0\n3           1 2014-04-01      4   6.0  2.0\n4           2 2011-03-01      1   5.1  1.3\n5           2 2011-04-01      2   5.8  0.6\n6           2 2011-05-01      3   6.5 -0.1\n7           2 2011-06-01      4   NaN  NaN\n8           2 2011-07-01      5   3.0  5.0\n9           3 2019-10-01      1   7.9 -1.5\n10          3 2019-11-01      2   8.6 -2.2\n11          3 2019-12-01      3   NaN  NaN\n12          3 2020-01-01      4  10.0 -3.6\n13          3 2020-02-01      5  10.7 -4.3\n14          3 2020-03-01      6   4.0  3.0\n", "df['date'] = pd.to_datetime(df['date'])\n\nf = lambda x: x.reindex(pd.date_range(x.index.min(), x.index.max(), freq='MS', name='date'))\ndf = df.set_index('date').groupby('Serial_no').apply(f).drop('Serial_no', axis=1)\ndf = df.reset_index()\ndf[\"Index\"] = df.groupby(\"Serial_no\").cumcount() + 1\n"], "link": "https://stackoverflow.com/questions/62690513/python-pandas-insert-rows-for-missing-dates-time-series-in-groupby-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "basically I would like to count number of the most frequent item grouped by 2 variables. I use this code: This code works, but does not work on columns that have Nan values, since NaN values are float and others are str. So this error is shown: I would like to omit NaN values and count mode for the rest. So str(x) is not a solution. And scipy.stats.mode(x, nan_policy='omit') does not work neither with an error: Could you please give me an advice how to deal with that. Thanks", "q_apis": "get columns count item columns values values values count mode mode", "apis": "copy groupby agg mode dropna copy groupby agg isnull all mode dropna", "code": ["dfgrouped = data[COLUMNS.copy()].groupby(['Var1','Var2']).agg(lambda x: stats.mode(x.dropna())[1])\n", "dfgrouped = (data[COLUMNS.copy()]\n              .groupby(['Var1','Var2'])\n              .agg(lambda x: None if x.isnull().all() else stats.mode(x.dropna())[1]))\n"], "link": "https://stackoverflow.com/questions/51132161/count-most-frequent-group-with-nan-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The data frame I am working with has three columns named , and based on three separate lists of \"best places to live\". Desired output: I want to return another column, series or groupby which shows the overall rank for each city after it takes into account position across all lists so would top the list and others would follow based on how close they are in terms of ranking to the top of each column. To clarify, Edingburgh is ranked 1st in and . It will look something like this: Basically I want to see the overall ranking for each city when all lists have been taken into account and learn how this can be achieved with Pandas. What have I tried? I was hoping there would be a simple way to rank using something like but do not see how I could use this with string values. Data", "q_apis": "get columns columns groupby rank all all rank values", "apis": "values index reset_index set_index index fillna reset_index groupby index apply reset_index drop index", "code": ["cities = pd.Index(np.unique(df.values))\nranks = pd.Series([1] * len(cities), index=cities)\n\nfor column in df:\n    ranks = ((ranks + df.reset_index().set_index(column)['index'])/2).fillna(ranks)\n\ncity_ranks = ranks.reset_index().groupby(0)['index'].apply(list).reset_index(drop=True)\ncity_ranks.index += 1\nprint(city_ranks)\n", "1                                    [Edinburgh]\n2                                         [Hart]\n3                     [London, Orkney, Solihull]\n4              [Bristol, Hertfordshire, Rutland]\n5          [Newcastle, Northumberland, Wychavon]\n6    [Manchester, South Lanarkshire, Winchester]\n7                [Berkshire, Glasgow, Wokingham]\n8                  [Darlington, Leeds, Waverley]\n9           [Cardiff, Craven, North Lanarkshire]\n"], "link": "https://stackoverflow.com/questions/52102125/rank-multiple-string-columns-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This is how df3(pandas dataframe) looks like: I want to put a filter for each value in df3['subtopic'] and return the resulting values to excel. For eg. the manual way of doing it is: This is how the output for this piece of code looks like: I would have to do this for each subtopic individually. Is there a way I can automate this? Even if the values are added to different excel sheets within 1 excel file, that's fine.", "q_apis": "get columns put filter value values values", "apis": "groupby to_excel", "code": ["writer = pd.ExcelWriter('a.xlsx', engine = 'xlsxwriter')\nfor n, g in df.groupby('subtopic'):\n    g.to_excel(writer, sheet_name=n)\nwriter.save()\nwriter.close()\n"], "link": "https://stackoverflow.com/questions/60309180/how-to-write-multiple-excel-sheets-from-a-particular-column-in-a-pandas-datafram"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I've recently run into a return from an API call where the return format is a list of dictionaries but the formatting causes the pd.DataFrame() to come out wrong. I get a return format of: When i run pd.DataFrames on this is turns into: I have tried using df.pivot(column='name', values='value') but that leads to a 5 x 5 df instead of just 1 row. Ideally I would like to get it where the column names= dictionary[name] and the row value=dictionary[value]: Any help and suggestions are greatly appreciated, thanks!", "q_apis": "get columns where DataFrame get pivot name values value get where names name value value", "apis": "DataFrame set_index T", "code": ["df = pd.DataFrame(api_result)\ndf = df.set_index(\"name\").T\n"], "link": "https://stackoverflow.com/questions/64159583/how-to-convert-a-dict-to-a-dataframe-with-the-values-as-headers-and-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a simple database consisting of 2 tables (say, Items and Users), where a column of the Users is their User_ID, a column of the Items is their Item_ID and another column of the Items is a foreign key to a User_ID, for instance: Imagine I want to denormalize this database, i.e. I'm adding the value of column Name from table Users into table Items for performance reasons when querying the data. My current solution is the following: That is, I'm adding the column as a Pandas Series constructed from a comprehension list, which uses .loc[] to retrieve the names of the users with a specific ID, and .iloc[0] to get the first element of the selection (which is the only one because user IDs are unique). But this solution is really slow for large sets of items. I did the following tests: For 1000 items and ~200K users: 20 seconds. For ~400K items and ~200K users: 2.5 hours. (and this is the real data size). Because this approach is column-wise, its execution time grows multiplicatively by the number of columns for which I'm doing this process, and gets too time-expensive. While I haven't tried using for loops to fill the new Series row by row, I expect that it should be much more costly. Are there other approaches that I'm ignoring? Is there a possible solution that takes a few minutes instead of a few hours?", "q_apis": "get columns where value Series loc names iloc get first unique items items seconds items size time columns time Series", "apis": "items merge", "code": ["items.merge(users[['User_ID', 'Name']], left_on='Its_User_ID', right_on='User_ID', how='left')\n"], "link": "https://stackoverflow.com/questions/52664433/efficiently-add-column-to-pandas-dataframe-with-values-from-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Is there a simple way to perform calculations for each fruit in turn, adding the newly created column to original df?", "q_apis": "get columns", "apis": "groupby diff fillna", "code": ["df['first_diff']=df[['concatted', 'score', 'fruit', 'status', 'date']].groupby('fruit')['score'].diff().fillna('')\n", "df['first_diff']=df[[x for x in df.columns]].groupby('fruit')['score'].diff().fillna('')\n\n     concatted  score  fruit status    date       first_diff\n0  apple_bana   0.50  apple   high  2010-02-20           \n1       apple   0.60  apple    low  2010-02-21        0.1\n2      banana   0.53   pear    low  2010-01-12   \n"], "link": "https://stackoverflow.com/questions/63123122/filter-dataframe-and-add-the-newly-created-columns-to-original-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Have a 4B row table in Oracle and a 30M row CSV, both tables share 2 columns on which I want to filter the large table using the smaller table. Due to security restrictions, I cannot load the 30M row CSV into Oracle and run a single join which would be ideal. I have also tried to use SAS Enterprise Guide for this process, but it seems to choke on the large join and fails to return before the connection to the Oracle table times out. Python seems to be a possible solution, but the 4B row table will not fit into memory, even when reducing to the 6 columns I need (6 strings each under 25 chars). Ideally, I'd like to do the following: The dataframe result_df will then be the set of all filtered rows from the 4B row Oracle table. Thanks for any assistance!", "q_apis": "get columns columns filter join join columns all any", "apis": "empty merge append", "code": ["csv_df = pd.read_csv(file_path)\nresult_df = (empty dataframe)\ndf_chunks = pd.read_sql(sql_query, con, chunksize = 10000000)\nchunk_list = []\nfor df_chunk in df_chunks:\n    result = pd.merge(df_chunk, csv_df, on=['XXX'])\n    chunk_list.append(result)\nresult_df = pd.concat(chunk_list)\n"], "link": "https://stackoverflow.com/questions/58936998/in-python-using-pandas-is-it-possible-to-read-4b-rows-chunkwise-and-filter-each"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have some pricing data for parts that updates monthly. It has been pulled into a pandas dataframe. Occasionally, a part won't get a price for a certain month, in which case I would like to replace it with that part's price from the previous month. In the event that the previous month also has a missing price for that part, I want to continue searching backwards until a valid price is found, in which case this price should propagate forward until a valid price is found. If no valid prices are found for that part, then I want this part to be dropped from the dataframe entirely. If the first number of months contain missing prices for a certain part, I would like to delete these rows so that the first record is always a valid price. Essentially I want to do a forward fill on the price column but taking part numbers into account. As an example, I would start with something like this: And end with this:", "q_apis": "get columns get month replace month month first delete first start", "apis": "loc groupby transform any sort_values assign groupby ffill dropna reset_index drop", "code": ["df.loc[lambda df: df.groupby('part')['price'].transform(np.any)]\\\n  .sort_values('date')\\\n  .assign(price=lambda df: df.groupby('part')['price'].ffill())\\\n  .dropna()\\\n  .reset_index(drop=True)\n", "    part    price   date\n0   1   88.37   2018-07-01\n1   3   264.02  2018-07-01\n2   1   88.37   2018-08-01\n3   3   264.02  2018-08-01\n4   1   88.37   2018-09-01\n5   3   212.70  2018-09-01\n6   1   67.32   2018-10-01\n7   3   167.34  2018-10-01\n8   1   67.32   2018-11-01\n9   3   167.34  2018-11-01\n10  1   67.32   2018-12-01\n11  3   99.16   2018-12-01\n"], "link": "https://stackoverflow.com/questions/54450186/replace-nans-for-month-n-with-the-value-for-month-n-1-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have the following DataFrame: And I want to find, for each city and year, what was the percentage change of value compared to the year before. My final dataframe would be: I tried to use a group in city and then use apply but it didn't work: It didn't work because I couldn't get the year and also because this way I was considereing that I had all years for all cities, but that is not true. EDIT: I'm not very concerned with efficiency, so any solution that solves the problem is valid for me.", "q_apis": "get columns DataFrame year value year apply get year all all any", "apis": "sort_values groupby pct_change where diff", "code": ["groups = df.sort_values('year').groupby(['city'])\n\ndf['pct_chg'] = (groups['value'].pct_change()\n                    .where(groups['year'].diff()==1)\n                )\n", "  city  year  value   pct_chg\n0    a  2013     10       NaN\n1    a  2014     12  0.200000\n2    a  2016     16       NaN\n3    b  2015     20       NaN\n4    b  2016     21  0.050000\n5    c  2013     11       NaN\n6    d  2016     15       NaN\n7    d  2017     13 -0.133333\n8    d  2018     16  0.230769\n"], "link": "https://stackoverflow.com/questions/67625491/pandas-percentage-change-using-group-by"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm working with the following dataset with hourly counts in columns. The dataframe has more than 1400 columns and 100 rows. My dataset looks like this: How can I convert this datatime to datetime such as this: I would like the average of all hours of the day to be in the column of the one day. The data type is: Thanks for your help!", "q_apis": "get columns columns columns all day day", "apis": "columns set_index groupby mean", "code": ["days = df.columns[1:].to_series().dt.normalize()\ndf.set_index('CITY').groupby(days, axis=1).mean()\n", "           2019-10-01  2019-12-01\nCITY                             \nWien        15.666667        14.0\nSalzburg    12.000000        14.0\nGraz        11.000000        10.0\nInnsbruck   11.333333        12.0\n"], "link": "https://stackoverflow.com/questions/59274422/python-pandas-mean-by-hour-of-day"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have a data set like: I want to distribute the values as evenly as possible between values of their surrounding s. For example the value 12 should take into consideration of their surrounding , and distribute them evenly until it touches the 2nd non- value's s. For example the 1st 12 should only take into consideration of his closest NaNs. The output should be: I was originally thinking about using smoothers, such as the interpolate function in Pandas. It does not have to be lossless, meaning that we can lose or get more than the sum in the progress. Are there any libraries that can perform this kind of distribution vs using a lossy smoother?", "q_apis": "get columns values between values value take value take interpolate get sum any", "apis": "DataFrame", "code": [">> series = pd.Series(x).interpolate(method='nearest').ffill().bfill()\n>> series.groupby(series).apply(lambda k: k/len(k))\n\n[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0]\n", "df = pd.DataFrame()\ndf[\"x\"] = x\n", ">>> df[\"inter\"] = df.x.interpolate(method='nearest').ffill().bfill()\n>>> df[\"inter\"] = df.groupby(\"inter\").inter.apply(lambda k: k/len(k))\n\n>>> df\n\n    x     inter\n0   NaN   2.0\n1   NaN   2.0\n2   NaN   2.0\n3   12.0  2.0\n4   NaN   2.0\n5   NaN   2.0\n6   NaN   2.0\n7   NaN   2.0\n8   10.0  2.0\n9   NaN   2.0\n10  NaN   2.0\n11  NaN   2.0\n12  NaN   2.0\n13  8.0   2.0\n14  NaN   2.0\n15  6.0   3.0\n16  NaN   3.0\n"], "link": "https://stackoverflow.com/questions/50240187/python-pandas-evenly-distribute-numeric-values-to-nearest-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This question has been asked multiple times. But my problem is bit different. I want to create a pandas dataframe with date range which includes start date and end date. The code I'm using is the following: Here is dynamic. So, it can be 1, 3, 4, 6 or 12. We also know does not include end dates. I also don't want to append the end date, as my frequency of period will be affected. So, if my date range is and . And the frequency is 3 months, then the series will end at . If I add the end date at the end, then there's 1 month difference between last date and end date. My question is how to create the date range series such that it includes extreme values and accommodate the dynamic frequencies. Thanks!", "q_apis": "get columns date start date date append date date at add date at month difference between last date date date values", "apis": "append", "code": ["start_date=pd.to_datetime(\"01-02-2020\", dayfirst=True)\nend_date=pd.to_datetime(\"01-12-2021\", dayfirst=True)\nperiod=3\nx=pd.date_range(start_date, end_date, freq=pd.DateOffset(months=period))\n\nif x[len(x)-1].date != end_date.date:\n    x=np.append(x.date, end_date.date())\n\nprint(x)\n\n[datetime.date(2020, 2, 1) datetime.date(2020, 5, 1)\n datetime.date(2020, 8, 1) datetime.date(2020, 11, 1)\n datetime.date(2021, 2, 1) datetime.date(2021, 5, 1)\n datetime.date(2021, 8, 1) datetime.date(2021, 11, 1)\n datetime.date(2021, 12, 1)]\n"], "link": "https://stackoverflow.com/questions/66535150/end-date-in-date-range"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I try to grab some stock data from a website. The german website onvista.de have all the information I need. Now I tried to get the stock data into a pandas dataframe. Like this: This works fine for other websites. But the onvista site has a nested 'span' element in the th element, which has text in it. How do I get rid of the span element in the th element, to get a proper dataframe, without the text? So I tried it with beautifulsoup to get rid of the 'span' element: the result looks like this: This is exactly what I want, only as a pandas dataframe. So please can someone tell me, how I can do this. Kind regards, Hoh", "q_apis": "get columns all get get get get", "apis": "get first pop DataFrame columns", "code": ["raw_data = [\n    ['Gewinn', '2020e', '2019e', '2018e', '2017', '2016', '2015', '2014'],\n    ['Gewinn pro Aktie in EUR', '-', '1,20', '0,89', '1,91', '2,11', '1,83', '4,65'],\n    ['KGV', '-', '12,52', '16,79', '6,95', '6,24', '7,06', '1,45'],\n    ['Gewinnwachstum', '-', '+45,18%', '-60,00%', '-9,47%', '+15,30%', '-60,64%', '+80,93%'],\n    ['PEG', '-', '-', '0,49', '-0,13', '-0,65', '0,46', '-0,02']\n]\n", "# get first list as headers\nheaders = raw_data.pop(0)\ndf_gewinn = DataFrame(raw_data, columns=headers)\n"], "link": "https://stackoverflow.com/questions/53620023/python-pandas-read-html-get-rid-of-nested-span-element-in-table"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this dataset that I want to transform, so I have just select a piece of how it looks like. So we have a column called Hospital which has those 4 rows which repeat until end of the dataframe. I want to transform so that all the data can only be saved on first row wich is called prelim_arm_1 and delete the rest of the 3 rows arms. Final dataset should look like this The dataset is huge with repeated rows arms but I want for each group of 4 rows, it should only save data on prelim_arm_1 and delete the other 3 row arms. so final table will only have prelim_arm_1 with data per group of 4 arms.", "q_apis": "get columns transform select repeat transform all first delete delete", "apis": "replace groupby eq cumsum first", "code": ["#if necessary\ndf_final = df_final.replace('NAN',np.nan)\n\ndf_final = df_final.groupby(df_final['Hospital'].eq('prelim_arm_1').cumsum()).first()\nprint(df_final)\n              Hospital Bug_Hosp code  cont    IPC NO_CT\nHospital                                               \n1         prelim_arm_1      133  G45  T256  567TY  5667\n2         prelim_arm_1      133  G45  T256  567Tu  3456\n", "print(df_final['Hospital'].eq('prelim_arm_1').cumsum())\n0    1\n1    1\n2    1\n3    1\n4    1\n5    2\n6    2\n7    2\n8    2\n9    2\nName: Hospital, dtype: int32\n"], "link": "https://stackoverflow.com/questions/60279618/move-data-from-row-to-another-row-within-a-group-of-specified-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm working with a large dataset. The following is an example, calculated with a smaller dataset. In this example i got the measurements of the pollution of 3 rivers for different timespans. Each year, the amount pollution of a river is measured at a measuring station downstream (\"pollution\"). It has already been calculated, in which year the river water was polluted upstream (\"year_of_upstream_pollution\"). My goal ist to create a new column [\"result_of_upstream_pollution\"], which contains the amount of pollution connected to the \"year_of_upstream_pollution\". For this, the data from the \"pollution\"-column has to be reassigned. Example: river_id = 1, year = 2000, year_of_upstream_pollution = 2002 value of the pollution-column in year 2002 = 20 Therefore: result_of_upstream_pollution = 20 The resulting column should look like this: My own approach: This results in the following ValueError: \"Length of values does not match length of index\" My explanation for this is, that the values in the \"year\"-column of \"dfr3\" are not unique, which leads to several numbers being assigned to each year and explains why: len(listr) = 28 I haven't been able to find a way around this error yet. Please keep in mind that the real dataset is much larger than this one. Any help would be much appreciated!", "q_apis": "get columns year at year contains year value year values length index values year unique year", "apis": "merge", "code": ["dfr1['result_of_upstream_pollution'] = dfr1.merge(dfr1, left_on=['river_id','year'],\n                                                  right_on=['river_id','year_of_upstream_pollution'], \n                                                  how='right')['pollution_x']\nprint(df)\n", "    result_of_upstream_pollution  \n0                           20.0  \n1                           20.0  \n2                           11.0  \n3                           11.0  \n4                           11.0  \n5                            NaN  \n6                           22.0  \n7                           20.0  \n8                           25.0  \n9                           18.0  \n10                           NaN  \n11                           NaN  \n12                          15.0  \n13                          15.0  \n14                          10.0  \n15                          26.0  \n16                          28.0  \n17                           NaN  \n"], "link": "https://stackoverflow.com/questions/63921651/python-merge-on-2-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What's the best way to insert new rows into an existing pandas DataFrame while maintaining column data types and, at the same time, giving user-defined fill values for columns that aren't specified? Here's an example: Assume that I want to add a new record passing just and . To maintain data types, I can copy rows from , modify values and then append to the copy, e.g. But that converts the column to an object. Here's a really hacky solution that doesn't feel like the \"right way\" to do this: I know I must be missing something.", "q_apis": "get columns insert DataFrame at time values columns add copy values append copy right", "apis": "append dtypes bool", "code": ["default = {'name': '', 'age': 0, 'weight': 0.0, 'has_children': False}\n\nrow = {'name': 'Cindy', 'age': 42}\n\ndf = df.append({**default, **row}, ignore_index=True)\n\nprint(df)\n\n   age  has_children   name  weight\n0   45          True    Bob   143.2\n1   40          True    Sue   130.2\n2   10         False    Tom    34.9\n3   42         False  Cindy     0.0\n\nprint(df.dtypes)\n\nage               int64\nhas_children       bool\nname             object\nweight          float64\ndtype: object\n"], "link": "https://stackoverflow.com/questions/50650850/insert-rows-into-pandas-dataframe-while-maintaining-column-data-types"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to create multiple good/bad files from original files from a directory. Im fairly new to Python, but have cobbled together the below, but it's not saving multiple files, just x1 \"good\" and x1 \"bad\" file. in the dir i have and . the output should be . Any help would be greatly appreciated. Thanks", "q_apis": "get columns", "apis": "isnull isnull astype drop append to_csv index merge drop to_csv index", "code": ["import pandas as pd\nfrom string import ascii_letters\nimport glob\nfrom pathlib import Path\n\n\nfiles = glob.glob('C:\\\\Users\\\\nickn\\\\OneDrive\\\\Documents\\\\Well\\\\*.csv')\n\n\nfor f in files:\n    filename = []\n    filename = Path(f)\n\n    #EDIT: we stay in loop and process each file one by one with following lines:\n\n    #Can not be null fields    \n    df = pd.read_csv(f)\n    emptyvals = []\n    emptyvals = df['First Name'].isnull() | df['Last Name'].isnull()\n    \n    #Bank Account Number is not 8 digits long\n    accountnolen = []\n    ac = []\n    accountnolen = df['AccNumLen'] = df['Bank Account Number'].astype(str).map(len)\n    ac =  df[(df['AccNumLen'] != 8)]\n    acd= ac.drop(['AccNumLen'],axis=1)\n    \n    #Create Exclusions\n    allexclusions = []\n    allexclusions = df[emptyvals].append(acd)\n    allexclusions.to_csv(filename.stem+\"bad.csv\",header =True,index=False)\n    \n    #GoodList\n    #for f in files:\n    #    filename = []\n    #    filename = Path(f)\n    origlist = df\n    df = pd.merge(origlist, allexclusions, how='outer', indicator=True)\n    cl =  df[(df['_merge'] == 'left_only')]\n    cld = cl.drop(['_merge','AccNumLen'],axis=1)\n    cld['Well ID'] = cld['Well ID'].str.rstrip(ascii_letters)\n    \n    cld.to_csv(filename.stem+'good.csv',header =True,index=False)\n"], "link": "https://stackoverflow.com/questions/63192446/to-csv-multiple-dataframes-from-loop-with-filename"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas data frame and I want to compare column data with the data in the next row. I can see see how this is possible with loops but is there a better way to do this with pandas? so for name - if the first two chars are the same as the first two chars in the name column of the next row. So is \"SH\" == \"DR\". Then I want to check if row 1 BS == row 2 BS... is sell == sell. if a pair that matches the requirements is found they should then be dropped and added to a new DF.", "q_apis": "get columns compare name first first name", "apis": "eq shift ne shift ne shift index index isin index index loc drop", "code": ["s = name2.eq(name2.shift(-1)) & name2.ne(name2.shift())\\\n    & df.BS.ne(df.BS.shift(-1))\n", "ind = df.index[df.index.isin(s[s].index.union(s[s].index + 1))]\ndf2 = df.loc[ind]\ndf.drop(ind, inplace=True)\n"], "link": "https://stackoverflow.com/questions/61004199/panda-dataframe-comparing-pairs-of-rows-and-if-they-pass-move-them-to-new-datafr"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Background:I have a DataFrame ('') containing two columns of interest, (weather on the planet Mars) and (the date the weather relates). Structure as follows: Objective:I am trying to write code that will determine the latest datestamp ( column) and print that row's corresponding column value.Sample rows:Here is a sample row: My code:Thus far, I have only been able to formulate some messy code that will return the latest dates in order, but it's pretty useless for my expected results: Any advice on how to reach the desired result would be much appreciated.", "q_apis": "get columns DataFrame columns date value sample", "apis": "loc max columns", "code": ["df_latest = weather_tweets.loc[weather_tweets.date == weather_tweets.date.max(),['weather','date']]\ndf_max.columns = ['latest_weather','latest_date']\n"], "link": "https://stackoverflow.com/questions/62970444/printing-row-based-on-datestamp-condition-of-another-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to build a 3,000 row long dataframe with code only (so far I import the document from XLS), following these rules: Top index: Flight Number: I would like to define the numbers in another simpler dataframe. The flight numbers stay the same 24 times (see departure times). Dataframe would look like this: STD Departure: From - to 23 (-, 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23) Leg Route: Similar to Flight Number this would also be defined in the dataframe (df above) Leg Flight Pair: Similar to Flight Number & Leg Route, this would also be defined in the dataframe (df above) Products: I would list the product names in a list This would be the beginning of the expected outcome", "q_apis": "get columns index product names", "apis": "DataFrame sort_values reset_index drop", "code": ["df = pd.DataFrame({\n    'Flight Number':['LX2104','LX2105','LX2320','LX2321','LX1232','LX1232'], \n    'Leg Route': ['GVA-AGP','GVA-AGP','GVA-AJA','GVA-AJA','GVA-ARN','GVA-ARN'],\n    'Leg Flight Pair': ['LX2104/2105','LX2104/2105','LX2320/2321','LX2320/2321','LX1232/1233','LX1232/1233']\n})\n", "n_flights = len(df)\ndf = pd.concat([df]*24).sort_values('Flight Number').reset_index(drop=True)\ndf['STD Departure'] = np.tile(np.arange(24), n_flights)\n", "cols = ['Prod1', 'Prod2']\nfor col in cols:\n  df[col] = 0\n", ">>> df.head()\n\nFlight Number   Leg Flight Pair     Leg Route   STD Departure   Prod1   Prod2\n0   LX1232      LX1232/1233         GVA-ARN             0           0       0\n1   LX1232      LX1232/1233         GVA-ARN             1           0       0\n2   LX1232      LX1232/1233         GVA-ARN             2           0       0\n3   LX1232      LX1232/1233         GVA-ARN             3           0       0\n4   LX1232      LX1232/1233         GVA-ARN             4           0       0\n"], "link": "https://stackoverflow.com/questions/54867922/how-to-quickly-create-dataframe-with-repeated-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am new to pandas. I have a dataset which looks like this: My goal Is to check if exists in , . And create a new dataframe where the structure would be the following: The problem which I have right now is that I don't know how to include Date_2, Hour_2, Date_3, Hour_3 or exclude them depending if the id_2 and id_3 is True or False. When I am creating my dataframe I simply add all each source of the information ( Date, Hour, id ) and I get large dataframe where I have Date_1-10, Hour_1-10, id_1-10. When I use method it filters the data correctly but it does not change if the hour and date from the same row is included or not. For example if id_1 exists in id_3 I would have True and its date and hour, if it does not exists I would have False and date with hour would be empty. At the moment when I use date and hour are not linked to the id_ value. Let me know if the problem is explained correctly. Thank you for your suggestions.", "q_apis": "get columns where right now add all get where hour date date hour date hour empty date hour value", "apis": "loc loc", "code": ["mask_id2 = df.id_1 == df.id_2\nmask_id3 = df.id_1 == df.id_3\n\ndf.id_2 = mask_id2\ndf.id_3 = mask_id3\n\ndf.loc[~mask_id2, ['Date_2', 'Hour_2']] = \"\"\ndf.loc[~mask_id3, ['Date_3', 'Hour_3']] = \"\"\n", "       Date_1  Hour_1     id_1      Date_2 Hour_2   id_2      Date_3 Hour_3   id_3\n0  2019-12-04       0      ABC  2019-12-04      1   True  2019-12-04      2   True\n1  2019-12-04       0     ABCD  2019-12-04      1   True  2019-12-04      2   True\n2  2019-12-04       0   ABCDEF                     False  2019-12-04      2   True\n3  2019-12-04       3  ABCDEFG  2019-12-04      1   True                     False\n"], "link": "https://stackoverflow.com/questions/59173464/adding-value-from-a-pandas-dataframe-only-if-other-value-from-the-same-row-is-tr"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have recently asked a question on applying select_dtypes for specific columns of a data frame. I have this data frame that has different dtypes on its columns (str and int in this case). I want to create different masks for strings and integers. And then I will apply stylings based on these masks. First let's define a function that will help me create my mask for different dtypes. (Thanks to @jpp) then our first mask will be: Second mask will be based on an interval of integers: But when I run the mask2 it gives me the error: How can I overcome this issue? Note: Stylings I would like to apply is like below:", "q_apis": "get columns select_dtypes columns dtypes columns apply mask dtypes first mask mask apply", "apis": "append append append style apply", "code": ["def styler(x):\n    res = []\n    for i in x:\n        try:\n            if 0 <= i <= 7:\n                res.append('background: red')\n            else:\n                res.append('')\n        except TypeError:\n            res.append('')\n    return res\n\nres = df.style.apply(styler, axis = 1)\n"], "link": "https://stackoverflow.com/questions/50987348/filtering-columns-that-has-different-dtype-cells"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe which can be generated using the code below I am trying to do the below operations on the above dataframe. Though the code works absolutely fine , the issue is when I use the . It's quick in sample dataframe but in real data with over 1 million records, it takes a while and just running for a long time I do to get the below output column so that I can reject records with count as . There is a logic involved behind dropping NA's. It's not just about dropping all NA's. If you would like to know about that then refer this post retain few NA's and drop rest of the NA's logic In real data one person might have more than 10000 rows. So a single dataframe has more than 1 million rows. Is there any other better and efficient way to do a or get the column?", "q_apis": "get columns sample time get count all drop any get", "apis": "groupby transform count ne copy astype sort_values astype sort_values count groupby transform count count ne mean std mean std", "code": ["df2['col2'] = [x.split(\"Date\")[0] for x in df2['colum']]\ndf2 = df2[df2.groupby(['subject_ID','col2'])['dates'].transform('count').ne(0)].copy()\n\ndf2['col3'] = df2['col2'].str.extract('(\\d+)', expand=True).astype(int)\ndf2 = df2.sort_values(by=['subject_ID','col3'])\nprint (df2)\n   subject_ID       colum                dates    col2  col3\n0           1  L1CreaDate  2016-10-30 00:00:00  L1Crea     1\n1           1      L1Crea                  2.3  L1Crea     1\n2           1  L2CreaDate  2016-10-30 00:00:00  L2Crea     2\n3           1      L2Crea                  2.5  L2Crea     2\n6           2  L1CreaDate  2016-10-30 00:00:00  L1Crea     1\n7           2      L1Crea                 12.3  L1Crea     1\n8           2  L2CreaDate  2016-10-30 00:00:00  L2Crea     2\n9           2      L2Crea                 12.3  L2Crea     2\n", "df2['col2'] = [x.split(\"Date\")[0] if x == x else np.nan for x in df2['colum']]\n", "def new(df2):\n    df2['col2'] = [x.split(\"Date\")[0] for x in df2['colum']]\n    df2 = df2[df2.groupby(['subject_ID','col2'])['dates'].transform('count').ne(0)].copy()\n    df2['col3'] = df2['col2'].str.extract('(\\d+)', expand=True).astype(int)\n    return df2.sort_values(by=['subject_ID','col3'])\n\n\ndef orig(df2):\n    df2['col2'] = df2['colum'].str.split(\"Date\").str[0]\n    df2['col3'] = df2['col2'].str.extract('(\\d+)', expand=True).astype(int)\n    df2 = df2.sort_values(by=['subject_ID','col3'])\n    df2['count'] = df2.groupby(['subject_ID','col2'])['dates'].transform(pd.Series.count)\n    return df2[df2['count'].ne(0)]\n\nIn [195]: %timeit (orig(df2))\n10.8 ms \u00b1 728 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [196]: %timeit (new(df2))\n6.11 ms \u00b1 144 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/57305101/efficient-way-to-use-pandas-group-by-on-a-million-records"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to take the input and keyword in the two tables from the database. So am using pandas to read both the tables and using the respective columns for splitting up of data and then write back the output in the same table in DB. My input: Original_Input My keyword table: So if the input(in table 1) has any of the name extension(this is in table 2) then it has to be split and put in as Core_input and Type_input columns where core input will contain the company names and type_input will contain the company type(from table 2 column 2) and it has to be checked with the priority. My output will be: My Code: Any help is appreciated. Edit:", "q_apis": "get columns take columns any name put columns where names", "apis": "head iterrows iloc sub replace iloc to_excel index", "code": ["import pandas as pd\nimport numpy\nimport re\n\ninput_df = pd.read_excel('input.xlsx',sheet_name='Sheet1')\nexts_df = pd.read_excel('exts.xlsx', sheet_name='Sheet1')\n\n# Check if correct data is loaded\nprint(input_df.head())\n\next_list = exts_df['Name_Extension']\ntype_list =exts_df['Company_Type']\n\nfor i, j in input_df.iterrows():\n    comp_name = input_df['Company Names'][i]\n    for idx, ex in enumerate(ext_list):\n        if re.search(rf'\\b{ex}\\b', comp_name,re.IGNORECASE):\n            comp_type = type_list[idx]\n            input_df['Type'].iloc[i] = comp_type\n            # Delete teh extension name from company name\n            updated_comp_name = re.sub(rf'\\b{str(ex).upper()}\\b','',str(comp_name).upper())\n            # Above regex is leaving space post word removal adding space from next word becomes 2 spaces\n            updated_comp_name = str(updated_comp_name).replace('  ',' ')\n            # Update the company name\n            input_df['Company Names'].iloc[i] = updated_comp_name\n\nprint(input_df)\ninput_df.to_excel('output.xlsx', index=False)\n"], "link": "https://stackoverflow.com/questions/60448654/how-to-split-the-input-based-by-comparing-two-dataframes-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset that contains a number of rows that are individuals from a country bearing an input category (1,2). Each unique row is present 5 times (5 times the same row, then 5 times the next row etc.). What I want to do is to create a new column in my df (let's say output) and to assign it another value (also 1 or 2) based on a conditional distribution. For instance, since for Algeria p1_1 (P of Output= 1 with input=1) = 2/5, I want to assign the output 1 to 2 of my rows (thus the output 2 to the 3 remaining row). Edited: here is the expected output :", "q_apis": "get columns contains unique assign value assign", "apis": "value_counts nunique iloc nunique sort_values reset_index drop melt sort_values reindex index mul values astype melt sort_values reindex index mul values assign set_axis sort_values values values", "code": ["n=5\n#\n#s = df['Country'].value_counts()\n#assert s.nunique() == 1\n#n = s.iloc[0] // df['Input category'].nunique()\n#print(n)\n##5\n", "df = df.sort_values(['Country', 'Input category']).reset_index(drop=True)\ndf2 = cond_prob.melt('Country').sort_values(['Country'])\ndf['Output Category'] = (df2.reindex(df2.index.repeat(df2['value'].mul(n)))['variable']\n                            .str.extract('(\\d+)')[0].values.astype(int))\n", "print(df)\n    Country  Input category  Output category\n0   Algeria               1                1\n1   Algeria               1                1\n2   Algeria               1                2\n3   Algeria               1                2\n4   Algeria               1                2\n5   Algeria               2                1\n6   Algeria               2                1\n7   Algeria               2                2\n8   Algeria               2                2\n9   Algeria               2                2\n10   France               1                1\n11   France               1                2\n12   France               1                2\n13   France               1                2\n14   France               1                2\n15   France               2                1\n16   France               2                1\n17   France               2                1\n18   France               2                2\n19   France               2                2\n20    Italy               1                1\n21    Italy               1                2\n22    Italy               1                2\n23    Italy               1                2\n24    Italy               1                2\n25    Italy               2                1\n26    Italy               2                1\n27    Italy               2                1\n28    Italy               2                1\n29    Italy               2                1\n", "df2 = cond_prob.melt('Country').sort_values('Country')\ndf2 = df2.reindex(df2.index.repeat(df2['value'].mul(5)))\nvalues = (df2.assign(**df2['variable'].str.split('_', expand=True)\n                                      .set_axis(['Output category', 'Input category'],\n                                                axis=1))\n             .sort_values(['Country', 'Input category']))['Output category'].str.extract('(\\d+)').values\ndf['Output category'] = values\nprint(df)\n"], "link": "https://stackoverflow.com/questions/62660182/how-can-i-assign-a-value-picked-from-a-list-to-a-df-column-based-on-a-probabilit"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "If we convert the dataframe to a dictionary, the duplicate entry (bob, age 20) is removed. Is there any possible way to produce a dictionary whose values are a list of dictionaries? Something that looks like this?", "q_apis": "get columns any values", "apis": "to_dict groupby groupby to_dict groupby apply to_dict to_dict", "code": ["{k: g.to_dict(orient='records') for k, g in df.groupby(level=0)}\n# {'bob': [{'age': 20, 'name': 'bob'}, {'age': 30, 'name': 'bob'}],\n#  'jim': [{'age': 25, 'name': 'jim'}]}\n", "for k, g in df.groupby(level=0):\n    print(g, end='\\n\\n')\n\n      age name\nname          \nbob    20  bob\nbob    30  bob\n\n      age name\nname          \njim    25  jim\n", "for k, g in df.groupby(level=0):\n    print(g.to_dict('r'))\n\n[{'age': 20, 'name': 'bob'}, {'age': 30, 'name': 'bob'}]\n[{'age': 25, 'name': 'jim'}]\n", "df.groupby(level=0).apply(lambda x: x.to_dict('r')).to_dict()\n# {'bob': [{'age': 20, 'name': 'bob'}, {'age': 30, 'name': 'bob'}],\n#  'jim': [{'age': 25, 'name': 'jim'}]}\n"], "link": "https://stackoverflow.com/questions/54137903/convert-pandas-dataframe-to-dict-and-preserve-duplicated-indexes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like following and another data frame of keyword like this What is want to do is to search from in of . And if tag is present, merge the kwx row of that tag with title. Here is what I have done. Split the title and search each tag in title, and return the first two match results. Output The next step i performed is to merge the and column with dataframe Output The output i want. Instead of confining to only first two match, i want all results and want dataframe in following shape Output PS: Because of space constraints here to make code pretty i dropped matchedName columns", "q_apis": "get columns merge first step merge first all shape columns", "apis": "all values assign merge explode drop columns groupby join agg add", "code": ["t=kwx.Tag.tolist()#puts all strings in Tag into a list\ndfx['term']=dfx.Title.str.split(' ')# Puts Title values into a list in a new colum term\ndfx['term']=dfx['term'].map(lambda x: [*{*x} & {*t}])#Leverages sets to find strings both in t and term\ndfx=dfx.assign(Tag=dfx.term)#creates a column called Tag\nnewdf=pd.merge(dfx.explode('Tag'),kwx).drop(columns=['Tag'])#Expands dfx to allow merging to kwx\nnewdf.groupby(['Title',newdf['term'].str.join(',')])['Area'].agg(list)#Groupby Title and term and add area to list\n  \n\n\nTitle                                term                 \nThese cats are very cute             cute,cats                     [nan, animal]\nchicken layes eggs full of proteins  proteins,chicken,eggs    [food, bird, food]\ndogs and horse is a loyal animal     horse,dogs                 [animal, animal]\nlion is the king of jungle           jungle,lion                 [place, animal]\n"], "link": "https://stackoverflow.com/questions/64422097/search-keywords-from-one-dataframe-in-another-and-merge-both-pandas-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have below dataframe called \"df\" and calculating the sum by unique id called \"Id\". Can anyone help me in optimizing the code i have tried.", "q_apis": "get columns sum unique", "apis": "groupby index mask count loc mask count count loc count update update count sum", "code": ["s = {}\nfor x , y in df.groupby(['Id','NCC']):\n    for i in y.index:\n        start_date = y['Date'][i] - timedelta(seconds=300)\n        end_date = y['Date'][i]\n\n        mask = (y['Date'] >= start_date) & (y['Date'] < end_date)\n        count = y.loc[mask]\n\n        count = count.loc[(y['Sys'] == 1)]\n\n        if len(count) == 0:\n            s.update({i : 0})\n        else:\n            s.update({i : count['Amount'].sum()})\n\ndf['New']=pd.Series(s)\n"], "link": "https://stackoverflow.com/questions/63090240/optimize-the-unique-id-with-in-certain-period"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the pandas DataFrame as shown below. and are both subsets of . I want to now create two new subsets of , let's call them and . should contain all values of that are in and . should contain values of that are not in and . What's the quickest way to do this? What I want and to look like (indices don't matter):", "q_apis": "get columns DataFrame now all values values indices", "apis": "apply isin apply product apply isin apply product", "code": ["s=pd.concat([df1,df2])\ndf[df.apply(tuple,1).isin(s.apply(tuple,1))]\nOut[77]: \n         date product  value\n2  2017-03-01   prod1    5.4\n3  2017-02-01   prod2    2.3\n5  2017-04-01   prod2    2.4\ndf[~df.apply(tuple,1).isin(s.apply(tuple,1))]\nOut[78]: \n         date product  value\n0  2017-01-01   prod1    5.1\n1  2017-02-01   prod1    5.2\n4  2017-03-01   prod2    2.2\n"], "link": "https://stackoverflow.com/questions/50141685/subset-pandas-dataframe-based-on-two-columns-in-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this pandas dataframe: and I plot a stacked bar chart using df.plot.bar(stacked=True) function Productions and consumptions are in % Is it possible to add percentages into bars?", "q_apis": "get columns plot plot add", "apis": "plot", "code": ["ax = df.plot.bar(stacked=True)\nlabels = [f'{i:.0%}' for i in df.to_numpy().flatten(order='F')]\n\nfor i, patch in enumerate(ax.patches):\n    x, y = patch.get_xy()\n    x += patch.get_width() / 2\n    y += patch.get_height() / 2\n    ax.annotate(labels[i], (x, y), ha='center', va='center', c='white')\n"], "link": "https://stackoverflow.com/questions/68107010/show-values-in-stacked-bar-chart-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In the below dataframe I want to count the number of purchases after a promo has been done for each product. So for banana, the promo is done on 1-5-2018 and I want to receive the total number of purchases after it (8 times). How do I best do that efficiently in python?", "q_apis": "get columns count product", "apis": "groupby product apply loc cummax mask sum product", "code": ["df.groupby('product')\\\n  .apply(lambda x: x.loc[x['Promo'].cummax().mask(x['Promo']==1,0) == 1,\n                         'Purchase'].sum())\n", "product\napple     0\nbanana    8\ndtype: int64\n"], "link": "https://stackoverflow.com/questions/50684086/count-purchases-after-occurrence-of-event-in-python-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Pandas dataframe that has multiple date columns. So, it indicates that the second row falls into March in the month category and 2015 in the year category. What I want to do is create a new dataframe that aggregates (let's do sum) the rows that fall into the same category. For example, if I want to aggregate by year by month, it will be like Any help?", "q_apis": "get columns date columns second month year sum aggregate year month", "apis": "groupby agg sum sum", "code": ["agg_col = \"Year\"\n\nnew_df = df.groupby(by=agg_col, as_index=False).agg({\"Count1\": \"sum\", \"Count2\": \"sum\"})\n"], "link": "https://stackoverflow.com/questions/66488448/how-can-i-aggregate-sum-mean-etc-values-and-create-a-new-pandas-dataframe-b"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There is one existing answer to this question here but it is wrong. In the example dataframe from the previous question, the US has the highest number of python users (10,110), yet in the graph it appears as though France has the highest instead. Can someone help me fix the solutions code? Data Frame Resulting Graph (incorrect) EXAMPLE DATAFRAME: INCORRECT CODE:", "q_apis": "get columns", "apis": "values values", "code": ["# the bars' heights\ndz = eg.values.ravel(order='F')\n", "eg.values.ravel()\n>> array([ 3222,   343,  2112, 10110,    89,  5432,   323,  1019,   678,\n         789,  7878,   467,   767,  8788,    40], dtype=int64)\n", "list(zip(xpos, ypos))\n>>[(0, 0),(1, 0),(2, 0),(0, 1),(1, 1),(2, 1),(0, 2),...]\n"], "link": "https://stackoverflow.com/questions/62166103/plotting-panda-dataframe-into-3d-bar-chart"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I Have two data frame df1 and df2. This is df1 This is df2 Based on the values from a column in df1['sector'], I want to get the name of column which has value 1 for the rows in df2.", "q_apis": "get columns values get name value", "apis": "merge drop rename columns", "code": ["df = pd.merge(df1, df2[['Name','result']], left_on='Sector', right_on='Name')\ndf = df.drop('Name_y', 1).rename(columns={'Name_x': 'Name'})\n", "In [102]: df\nOut[102]: \n       Name      Sector         result\n0  Company1          3D  Manufacturing\n1  Company2  Accounting        Finance\n2  Company3    Wireless  Entertainment\n"], "link": "https://stackoverflow.com/questions/68054548/i-have-a-dataframe-problem-in-python-how-to-map-two-different-data-frames"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Example data: I need to copy rows whose 'file' repeat 3 times, to get something like this:", "q_apis": "get columns copy repeat get", "apis": "groupby transform size groupby filter value_counts", "code": ["df = df1[df1.groupby('file')['file'].transform('size') == 3]\n", "print (df1.groupby('file')['file'].transform('size'))\n0    3\n1    3\n2    3\n3    3\n4    3\n5    3\n6    2\n7    2\n8    1\nName: file, dtype: int64\n", "df = df1.groupby('file').filter(lambda x: len(x) == 3)\n", "df = df1[df1['file'].map(df['file'].value_counts()) == 3]\n", "print (df)\n    file  prop1  prop2  prop3\n0  file1   True  False  False\n1  file1  False  False   True\n2  file1   True  False  False\n3  file2  False  False   True\n4  file2  False   True  False\n5  file2  False  False   True\n"], "link": "https://stackoverflow.com/questions/58305555/pandas-copy-rows-whose-names-are-repeated-n-times"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe as shown below: I have an input array that looks like this: For each of these values I want to check against a specific column of a dataframe and return a decision as shown below: Since only allows me to look for an individual value, I was wondering if there's some other direct way to determine the same something like: My end goal is not to do an absolute match () but rather a 'contains' logic that says return true even if it has those characters present in that cell of data frame. Note: I can of course use apply method to create my own function for the same logic such as: But I am more interested in the optimal algorithm for this and so prefer to use a native pandas function within pandas, or else the next most optimized custom solution.", "q_apis": "get columns array values value contains apply", "apis": "replace all bool astype", "code": ["df.a.str.replace(' ','').str.get_dummies(';')[['web','app']].all(1)\n0    False\n1     True\n2    False\n3    False\ndtype: bool\n", "elementList = ['app','web']\nfor eachValue in elementList:\n                    valueString += f'(?=.*{eachValue})'\ndf[header] = df[header].astype(str).str.lower() #To ensure case insenstivity and the dtype of the column is string\nresult = df[header].str.contains(valueString)\n"], "link": "https://stackoverflow.com/questions/68295234/check-if-a-pandas-dataframe-string-column-contains-all-the-elements-given-in-an"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a 2D histogram h1 with var1 on the x axis and var2 on the y axis, which I've plotted from a . I have normalised it as I want in c++ but now need to do the same in python and am struggling with how to get and set bin content. The idea is to remove the effect of having more events in one part of the distribution than in another and only leave the correlation between and . Working Code in c++: Attempt for code in python: Edit after help from @JohanC: Problem has been resolved. Make sure you don't have nan-s when normalising, because dealing with them is always a pain.", "q_apis": "get columns now get between", "apis": "hist hist sum size size size hist hist hist T hist hist sum hist columns hist hist sum sum hist hist hist", "code": ["hist *= norm / hist.sum(axis=0, keepdims=True)", "from matplotlib import pyplot as plt\nimport numpy as np\n\nN = 1000000\nvar1 = np.concatenate([np.random.uniform(0, 20, size=9 * N // 10), np.random.normal(10, 1, size=N // 10)])\nvar2 = var1 * 0.1 + np.random.normal(size=N)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))\n\nnorm = 10\nbinsX = 200\nbinsY = 100\nax1.hist2d(var1, var2, bins=(binsX, binsY), cmap='BuPu')\nax1.set_title('regular 2d histogram')\n\nhist, xedges, yedges = np.histogram2d(var1, var2, bins=(binsX, binsY))\nhist = hist.T\nwith np.errstate(divide='ignore', invalid='ignore'):  # suppress division by zero warnings\n    hist *= norm / hist.sum(axis=0, keepdims=True)\nax2.pcolormesh(xedges, yedges, hist, cmap='BuPu')\nax2.set_title('normalized columns')\nplt.show()\n", "hist *= norm / hist.sum(axis=0, keepdims=True)", "s[i, j] = sum([h[k,j] for k in range(0, N)])", "hist *= norm / s", "hist = np.nan_to_num(hist, nan=0, posinf=0, neginf=0)\n"], "link": "https://stackoverflow.com/questions/63415624/normalising-a-2d-histogram"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 3 dataframes which can be generated from the code shown below I would like to do two things a) Append all these 3 dataframes into one large dataframe When I attempted this using the below code, the output isn't as expected So, to resolve this, I understand we have to rename the column names which leads to objective b below b) Rename the column of these n dataframes to be uniform in a elegant way Please note that in real time I might have dataframe with different column names which I may not know in advance but the values in them will always be the same belonging to columns , and . But note there can be several other columns as well like , , etc Currently, I do this by manually reading the column names using below code How can I set the column names for all dataframe to be the same (,, and etc) irrespective of their original column values", "q_apis": "get columns all rename names time names values columns columns names names all values", "apis": "columns", "code": ["final_columns = ['ethn', 'gen', 'pers_id']\nprevious_columns = df2.columns()\nmapping = {previous_columns[i]: final_columns[i] for i in range(3)}  # 3 is arbitrary.\n"], "link": "https://stackoverflow.com/questions/57250943/append-dataframes-with-different-column-names-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've got a dataframe with one column filled with milliseconds that I've been able to convert somewhat into datetime format. The issue is that for two years worth of data, from 2017-2018, the time output remains at 1-1-1970. The output datetime looks like this: 27 1970-01-01 00:25:04.232399999 28 1970-01-01 00:25:04.232699999 29 1970-01-01 00:25:04.232999999 85264 1970-01-01 00:25:29.962799999 85265 1970-01-01 00:25:29.963099999 85266 1970-01-01 00:25:29.963399999 It seems to me that the milliseconds, which begin at 1504224299999 and end at 1529971499999, are getting added to the 10th hour of epoch and are not representing the true range that it should. This is my code so far... I'm not quite sure where I'm going wrong, so if anybody can tell me what I'm doing stupidly it'd be greatly appreciated.", "q_apis": "get columns time at at at hour where", "apis": "apply", "code": ["ms['close_time'] = ms['close_time'].apply( lambda x: datetime.datetime.fromtimestamp(x/1000) )\n"], "link": "https://stackoverflow.com/questions/51219069/issue-with-datetime-remaining-at-epoch"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to reassign multiple columns in DataFrame with modifications. The below is a simplified example. I use assign() method to add 1 to both 'col1' and 'col2'. However, the result is to add 1 only to 'col2' and copy the result to 'col1'. Can someone explain why this is happening, and also suggest a correct way to apply assign() to multiple columns?", "q_apis": "get columns columns DataFrame assign add add copy apply assign columns", "apis": "assign", "code": ["df.assign(**{c: df[c] + 1 for c in ['col1','col2']})\n"], "link": "https://stackoverflow.com/questions/60980861/pandas-dataframe-assign-doesnt-work-properly-for-multiple-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to join three dataframes of the following structure: The desired output for March should be: In a first step I tried to merge March and February but it does not yield the result for the two months. I tried to understand the hints on Performant cartesian product (CROSS JOIN) with pandas and pandas three-way joining multiple dataframes on columns but did not get any wiser. In R it can be achieved as a \"piped multiple join\" which I cannot seem to translate into Python. How do I get the output as wanted?", "q_apis": "get columns join first step merge product columns get any join get", "apis": "merge merge fillna", "code": ["tmp = pd.merge(january_df, february_df, on='ID')\nfinal_df = pd.merge(tmp, march_df, on='ID', how='right')[['January', 'February', 'March', 'Product_no', 'Label', 'ID']].fillna(0)\n\nprint(final_df)\n", "   January  February  March Product_no    Label    ID\n0      1.0       2.0      5         T1    Towel  1005\n1      0.0       0.0      1         E1  Earring  1006\n2      3.0       4.0      1         S1     Shoe  1002\n3      1.0       1.0      1         B3      Bag  1004\n4      0.0       0.0      1         L1   Lotion  1007\n5      4.0       3.0      1         B1     Ball  1000\n"], "link": "https://stackoverflow.com/questions/66619636/cross-join-of-three-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So i'm working on a dataframe which has a key-value pair as its value in columns. Is there a way to make the keys as column name while only keeping the value left in the column. Currently i have something like this:", "q_apis": "get columns value value columns keys name value left", "apis": "apply explode dropna DataFrame index index stack unstack", "code": ["obj = df.apply(lambda x: list(x), axis=1).explode().dropna()\ndfn = pd.DataFrame(obj.tolist(), index=obj.index)\ndfn.stack().unstack()\n", "#        1536235175000  1536235176000  1536236701000  1536239919000  \\\n# 0            26307.9            0.0         2630.0            NaN   \n# 1                NaN            NaN            NaN      1028127.0   \n# 2                NaN            NaN            NaN            NaN   \n\n#        1536239921000  1536242709000  1536242711000  \n# 0                NaN            NaN            NaN  \n# 1                0.0            NaN            NaN  \n# 2                NaN         2629.6            0.0\n"], "link": "https://stackoverflow.com/questions/65677566/i-have-dictionary-as-value-in-pandas-dataframe-columns-i-want-to-make-the-keys"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Consider a I have a column called 'test' of a dataframe. The column elements are like this: I want to make the each column elements of the dataframe as 2016-04-01. Based on this I have written a code which is working fine in spyder but when I am trying to apply it to Jupyter Notebook it is showing some error AttributeError: 'str' object has no attribute 'shape' My code is: Initially I have used the following code In both cases it is showing error. Kindly help so that I can use it in Jupyter Notebook. Note that dtypes is object for 'test'.", "q_apis": "get columns test apply shape dtypes test", "apis": "DataFrame", "code": ["df=pd.DataFrame({\n    'test':['201604', '201605']\n})\ndf\n", "test\n0   201604\n1   201605\n", "df.test = pd.to_datetime(df.test, format='%Y%m')\ndf\n", "    test\n0   2016-04-01\n1   2016-05-01\n"], "link": "https://stackoverflow.com/questions/67724567/error-str-object-has-no-attribute-shape-while-trying-to-covert-datetime-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am looking for a solution to the following problem. There's a DataFrame: I wish to retain rows in which, for example, value in column col1 belongs to a list [1, 2] while value in column col2 belongs to a list [2, 4]. This is what I thought would work However prints as an Empty DataFrame. On the other hand, this approach results in It would be expected to get a DataFrame with row1 in it. Needless to say I am relatively new to Python. Thanks a lot for your help.", "q_apis": "get columns DataFrame value value DataFrame get DataFrame", "apis": "apply", "code": ["df = pd.DataFrame(data=data[1:,1:].astype(int),\n                  index=data[1:,0],\n                  columns=data[0,1:])\n\ndf1 = df[df['col1'].isin([1,2]) & df['col2'].isin([2,4])]\n\nprint(df1)\n\n      col1  col2\nrow1     1     2\n", "cols = ['col1', 'col2']\ndf[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n"], "link": "https://stackoverflow.com/questions/50890405/python-dataframe-deleting-rows-with-column-values-belonging-to-lists-of-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to change value of to 0 if subtraction gives negative values I tried but don't know how to get index of x and also don't know if it possible with this approach", "q_apis": "get columns value values get index", "apis": "clip where", "code": ["df['points'] = (df['HOME_SCORE'] - df['prev_HOME_SCORE']).clip(0)\n", "df['points'] = np.where((df['HOME_SCORE'] - df['prev_HOME_SCORE']) < 0, 0,df['HOME_SCORE'] - df['prev_HOME_SCORE'])\n"], "link": "https://stackoverflow.com/questions/65255663/change-the-result-of-subtracting-two-columns-in-the-pandas-dataframe-if-we-get-a"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the named . And I am to create a new empty DataFrame with columns and and fill in with values from the original column with the same name and fill in with average values in given category from original . Filling in columns must be without using pandas functions, but with using for and while loops. Please, show me how to do it", "q_apis": "get columns empty DataFrame columns values name values columns", "apis": "append append append sum DataFrame", "code": ["directors_unique = []\n\nfor i in movies_converted.Director:\n    if i not in directors_unique:\n        directors_unique.append(i)\n\nlist_directors = []\nlist_ratings = []\nfor director in directors_unique:\n    list_directors.append(director)\n    list_ratings.append(sum(movies_converted[movies_converted[\"Director\"] == director][\"IMDB Rating\"])/len(movies_converted[movies_converted[\"Director\"] == director]))\n    director_rating = pd.DataFrame({\"Director\":list_directors, \"Average IMDB Rating\":list_ratings})\n"], "link": "https://stackoverflow.com/questions/60058939/create-new-dataframe-and-fill-in-its-columns-without-using-pandas-functions-but"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have dataframe of datetime index. I have a three lists of dates prescribing their condition. I want to compare each date of dataframe with three lists and assigns a string to the row. I want to compare each index in df with the above two lists and produce a new column indicating the condition of the day. My present code My present output is My code is not working properly.", "q_apis": "get columns index compare date compare index day", "apis": "index index loc loc", "code": ["bad = df['index'].between('2019-03-22', '2019-04-09')\ngood = df['index'].between('2019-04-10', '2019-04-29')\n", "df['test'] =  'normal_day'\ndf.loc[bad, 'test'] = 'bad_day'\ndf.loc[good, 'test'] = 'good_day'\n"], "link": "https://stackoverflow.com/questions/59886834/python-compare-a-dataframe-with-a-list-of-dates-and-assign-a-string-based-on-res"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have been trying to normalize a nested json but am struggling with is how to deal with NAN and empty fields as i am getting a => KeyError: 'address' when i am trying to normalize with The data that i am using the ouptput i want", "q_apis": "get columns normalize empty normalize", "apis": "last last last last last last last last fillna index join drop columns last last", "code": ["data = [{\n    \"id\": \"1\",\n    \"name\": \"test\",\n    \"last\": \"last\"\n\n}, {\n    \"id\": \"2\",\n    \"name\": \"test1\",\n    \"last\": \"last1\",\n    \"address\": []\n},\n{\n    \"id\": \"3\",\n    \"name\": \"test2\",\n    \"last\": \"last2\",\n    \"address\": [{\n        \"street\": \"street d2\",\n        \"no\": 2\n    }]\n}]\n", " id   name   last                          address\n0  1   test   last                              NaN\n1  2  test1  last1                               []\n2  3  test2  last2  [{'steet': 'streed2', 'no': 2}]\n", "id   name   last                        address\n0  1   test   last                            NaN\n1  2  test1  last1                            NaN\n2  3  test2  last2  {'steet': 'streed2', 'no': 2}\n", "df.address = df.address.fillna({i: {} for i in df.index})\n\n# use json_normalize\ndf = df.join(pd.json_normalize(df.address)).drop(columns=['address'])\nprint(df)\n", "  id   name   last     street   no\n0  1   test   last        NaN  NaN\n1  2  test1  last1        NaN  NaN\n2  3  test2  last2  street d2  2.0\n"], "link": "https://stackoverflow.com/questions/67784772/pandas-json-normalize-with-some-empty-record-path"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to Represent Single Row In HTML In Tabular Form. But getting Error KeyError at /input 'Company'. Am working With Django. KeyError at /input 'Company'", "q_apis": "get columns at at", "apis": "get", "code": ["d = {'k':'v'}\nval = d.get('k')\nif val is None:\n  print('not found')\nelse:\n  print('found')\n"], "link": "https://stackoverflow.com/questions/58065339/how-to-fix-error-keyerror-at-input-company-when-selecting-single-row-from"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that has column of repeating values/indexes and I want to group it by the 'Name' column but without performing any aggregation to it. I've looked at the function but from what I've searched, you are kind of forced to perform an aggregation. I've also tried and then do but it for some reason it returns a I feel like it's something obvious but I can't quite figure it out. This is my dataframe now: This is what I want:", "q_apis": "get columns values any at now", "apis": "index", "code": ["df['Name'] = df['Name'].mask(df['Name'].duplicated(), '')\nprint (df)\n  Name  Data1  Data2\n0    A    0.1    1.1\n1         0.2    1.2\n2         0.3    1.3\n3    B    0.6    1.6\n4         0.7    1.7\n5         0.8    1.8\n6    C    1.0    2.0\n7         1.1    2.1\n8         1.2    2.2\n", "df1 = df.set_index(['Name','Data1'])\nprint (df1)\n            Data2\nName Data1       \nA    0.1      1.1\n     0.2      1.2\n     0.3      1.3\nB    0.6      1.6\n     0.7      1.7\n     0.8      1.8\nC    1.0      2.0\n     1.1      2.1\n     1.2      2.2\n", "with pd.option_context('display.multi_sparse', False):\n    print (df1)\n\n            Data2\nName Data1       \nA    0.1      1.1\nA    0.2      1.2\nA    0.3      1.3\nB    0.6      1.6\nB    0.7      1.7\nB    0.8      1.8\nC    1.0      2.0\nC    1.1      2.1\nC    1.2      2.2\n", "print (df1.index.tolist())\n[('A', 0.1), ('A', 0.2), ('A', 0.3), \n ('B', 0.6), ('B', 0.7), ('B', 0.8), \n ('C', 1.0), ('C', 1.1), ('C', 1.2)]\n"], "link": "https://stackoverflow.com/questions/60034688/how-can-i-drop-repeating-values-in-a-column-while-keeping-the-data-for-its-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's say I have data like this: How can I use to replace NaN values with the average of the prior and the succeeding value if both of them are not NaN ? The result would look like this: Also, is there a way of calculating the average from the previous n and succeeding n values (if they are all not NaN) ?", "q_apis": "get columns replace values value values all", "apis": "shift shift fillna", "code": ["s1, s2 = df.shift(), df.shift(-1)\ndf = df.fillna((s1 + s2) / 2)\n", "   col1  col2\n0   5.0   1.0\n1   3.5   3.0\n2   2.0   NaN\n3   2.0   NaN\n4   5.0   5.0\n5   4.5   4.5\n6   4.0   4.0\n"], "link": "https://stackoverflow.com/questions/65447802/pandas-fillna-with-local-average-if-a-condition-is-met"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset I want to create a balanced panel so that I get the following: I tried the the following code: But I am not getting the desired output.", "q_apis": "get columns get", "apis": "set_index reindex reset_index", "code": ["idx = pd.MultiIndex.from_product(\n    [df.id.unique(), df.year.unique()], names=[\"id\", \"year\"]\n)\ndf = df.set_index([\"id\", \"year\"]).reindex(idx).reset_index()\nprint(df)\n", "   id  year  sales\n0   1  2000   10.0\n1   1  2001    NaN\n2   1  2002    NaN\n3   2  2000   10.0\n4   2  2001   20.0\n5   2  2002   30.0\n"], "link": "https://stackoverflow.com/questions/67679307/creating-a-balanced-panel-dataset"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe and only want to sort the first three columns in my dataframe in a specific order. The order of the rest of the columns does not matter. I have 40 columns in total. This thread had a solution but it doesn't seem to work for me: how to sort only some of the columns in a data frame in pandas? The solution in the link above recommends tackling this problem in three steps by using in the following way: In my case, the name of the three columns I want as first, second and third column are the following: , , and - in that specific order. Since can only take on one argument, I used for the first step. Yet the second step, gives me a . Any help?", "q_apis": "get columns first columns columns columns columns name columns first second take first step second step", "apis": "DataFrame columns columns reindex", "code": ["df = pd.DataFrame(np.random.randint(0,100,(10,6)), columns=[*'ABWXYZ'])\n\norderedCols = ['Z','Y','W']\nnonOrderedCols = [i for i in df.columns if i not in orderedCols]\n\nnonOrderedCols\n# ['A', 'B', 'X']\n\norderedCols\n# ['Z', 'Y', 'W']\n\ndf.reindex(orderedCols + nonOrderedCols, axis=1)\n", "    Z   Y   W   A   B   X\n0  29  61  65  17  95  59\n1  87   2  98   4  79  85\n2  53  45   2  77  49  81\n3  72  47  58  53  22  24\n4  74  66  50  93  29  71\n5  23  94  70  38  11  94\n6  54  60   7  29   3  33\n7  31   1  67  80  68  57\n8  99  50  79  28  49  52\n9  73  46  77  17  47  93\n"], "link": "https://stackoverflow.com/questions/59619480/python-trouble-sorting-only-some-columns-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to convert 3D array to Panel data in Pandas, but It is giving me error as \"TypeError: object() takes no parameters\" Output is:", "q_apis": "get columns array", "apis": "DataFrame diff shape columns columns index index index", "code": ["pd.DataFrame(diff.reshape(-1,df1.shape[1]), \n             columns=df1.columns,\n             index=pd.MultiIndex.from_product((df1.index, df1.index))\n            )\n", "B          c1   c3      c4   c6     c7\ni1 i1  0.0000  NaN  0.0000  NaN  0.000\n   i2     NaN  NaN     NaN  NaN    NaN\n   i3     NaN  NaN     NaN  NaN    NaN\n   i4  4.4877  NaN  2.1123  NaN    NaN\n   i5  0.2123  NaN  3.1123  NaN  9.123\ni2 i1     NaN  NaN     NaN  NaN    NaN\n   i2     NaN  NaN     NaN  0.0    NaN\n   i3     NaN  NaN     NaN  NaN    NaN\n   i4     NaN  NaN     NaN  NaN    NaN\n   i5     NaN  NaN     NaN  NaN    NaN\ni3 i1     NaN  NaN     NaN  NaN    NaN\n   i2     NaN  NaN     NaN  NaN    NaN\n   i3     NaN  0.0     NaN  NaN    NaN\n   i4     NaN  NaN     NaN  NaN    NaN\n   i5     NaN  NaN     NaN  NaN    NaN\ni4 i1  4.4877  NaN  2.1123  NaN    NaN\n   i2     NaN  NaN     NaN  NaN    NaN\n   i3     NaN  NaN     NaN  NaN    NaN\n   i4  0.0000  NaN  0.0000  NaN    NaN\n   i5  4.7000  NaN  1.0000  NaN    NaN\ni5 i1  0.2123  NaN  3.1123  NaN  9.123\n   i2     NaN  NaN     NaN  NaN    NaN\n   i3     NaN  NaN     NaN  NaN    NaN\n   i4  4.7000  NaN  1.0000  NaN    NaN\n   i5  0.0000  NaN  0.0000  NaN  0.000\n"], "link": "https://stackoverflow.com/questions/58139878/convert-3d-array-to-pandas-panel-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "First, skip the row of data if the columns have more than 2 columns that are empty. After this step, the rows with more than 2 columns missing value will be filtered out. Then, as some of the columns still have 1 or 2 columns are empty. So I will fill in the empty column with the mean value of that row. I can run the second step with my code below, however, I am not sure how to filter out the rows with more than 2 columns missing value. I have tried using but it deleted all the columns of the table. My code: My dataset: Country Name 2001 2002 2003 2004 Philippines 71 Malta 62 58 60 58 Singapore 60 56 Malaysia 58 57 55 Ireland 47 41 34 34 Georgia 38 41 24 38 Costa Rica", "q_apis": "get columns columns columns empty step columns value columns columns empty empty mean value second step filter columns value all columns", "apis": "dropna any", "code": ["hightech_export.dropna(axis=1, how='any', thresh=2, subset=None, inplace=False)\n"], "link": "https://stackoverflow.com/questions/58232062/skipping-the-row-if-there-are-more-than-2-fields-are-empty"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My csv file looks like below image. So I want to rename the column X using the adjacent column slice-0010-EDSR_x2. So the new column X name should be slice-0010-EDSR_x2_X And this column slice-0010-EDSR_x2 name should be slice-0010-EDSR_x2_Y . And cooresponding to all other columns Is this thing possible?", "q_apis": "get columns rename name name all columns", "apis": "DataFrame columns append columns", "code": ["df = pd.DataFrame(\n    {\n        'Contour': range(5),\n        'X': range(5, 10),\n        'slice-0010-EDSR_x2': range(10, 15),\n        'X_': range(5, 10),\n        'slice-0011-EDSR_x2': range(10, 15),        \n    }\n)\n", "col_names = df.columns.tolist()\nnew_col_names = []\n\nfor i_col, col in enumerate(col_names):\n    if i_col == 0:\n        new_col = col\n    elif col.startswith('X'):\n        new_col = col_names[i_col + 1] + '_X'\n    else:\n        new_col = col + '_Y'\n    \n    new_col_names.append(new_col)\n    \ndf.columns = new_col_names\nprint(df)\n", "   Contour  slice-0010-EDSR_x2_X  slice-0010-EDSR_x2_Y  slice-0011-EDSR_x2_X  \\\n0        0                     5                    10                     5   \n1        1                     6                    11                     6   \n2        2                     7                    12                     7   \n3        3                     8                    13                     8   \n4        4                     9                    14                     9   \n\n   slice-0011-EDSR_x2_Y  \n0                    10  \n1                    11  \n2                    12  \n3                    13  \n4                    14  \n"], "link": "https://stackoverflow.com/questions/64258113/renaming-the-columns-in-pd-dataframe-based-on-the-adjacent-column-name"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following pandas dataframe. I would like to get the following table by restructuring every three-column values. OR Is there any efficient way to do it? I tried to find similar examples here at the StackOverflow community but couldn't. If you have, you can point me. Any help is appreciated!", "q_apis": "get columns get values any at", "apis": "DataFrame add_prefix groupby apply reset_index", "code": ["f = lambda x: pd.DataFrame(np.reshape(x.to_numpy(),(-1,3))).add_prefix('Col_')\ndf.groupby('ID').apply(f).reset_index('ID')\n", "   ID Col_0 Col_1 Col_2\n0   1     A     B     C\n1   1     A     B     C\n2   1     A     B     C\n3   2     D     E     F\n4   2     D     E     F\n5   2     D     E     F\n"], "link": "https://stackoverflow.com/questions/60527174/restructuring-pandas-dataframe-based-on-number-of-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to iterate through multiple datasets with a calculation containing multiple conditions, but I receive an error message. I can iterate through multiple lists, but it doesn't seem to work for dataframes. I have divided a huge dataset (originally from an HDF5 file) into smaller datasets with the same columns, but for different building types. Now I want to iterate through each dataset to do the same calculation. The calculation includes multiple coditions too. If the dataframe (df) is either a OR b, then perform the following calculation for these datasets; or alternatively if the dataset is either c OR d, do the other calculation instead for those datasets. But, I receive the following ValueError message:", "q_apis": "get columns columns", "apis": "equals equals append equals equals append", "code": ["for df in (a,b,c,d):    \n    if df.equals(a) or df.equals(b):\n        k = 1\n        A.append(k)\n    elif df.equals(c) or df.equals(d):\n        k = 2\n        B.append(k) \n"], "link": "https://stackoverflow.com/questions/57360778/how-to-iterate-through-multiple-datasets-with-multiple-conditions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe called that looks similar to this (except the number of 'mat_deliv' columns goes up to mat_deliv_8, there are several hundred clients and a number of other columns between and - I have simplified it here). I want to create a new column called which counts the number of times appears in , , and . The values should look like this: I have tried the following code: But it does not produce a count, only a binary variable where = no cases of and = the presence of in at least one of the four columns. NB: this is a follow-up question to that asked here: Creating a column based on the presence of part of a string in multiple other columns", "q_apis": "get columns columns columns between values count where at columns columns", "apis": "loc stack sum loc stack eq any sum count all sum", "code": ["df['counts'] = (df.loc[:, \"mat_deliv_1\":\"mat_deliv_4\"]\n                  .fillna('')\n                  .agg(','.join, 1)\n                  .str.count('xxx'))\ndf\n  Client_ID  mat_deliv_1  mat_deliv_2 mat_deliv_3 mat_deliv_4  counts\n0  C1019876  xxx,yyy,zzz  aaa,bbb,xxx         xxx         ddd       3\n1  C1018765      yyy,zzz          xxx         xxx         NaN       2\n2  C1017654      yyy,xxx      aaa,bbb         ccc         ddd       1\n3  C1016543      aaa,bbb          ccc         NaN         NaN       0\n4  C1019876          yyy          NaN         NaN         NaN       0\n", "df['counts'] = (\n    df.loc[:, \"mat_deliv_1\":\"mat_deliv_4\"].stack().str.count('xxx').sum(level=0))\ndf\n  Client_ID  mat_deliv_1  mat_deliv_2 mat_deliv_3 mat_deliv_4  counts\n0  C1019876  xxx,yyy,zzz  aaa,bbb,xxx         xxx         ddd       3\n1  C1018765      yyy,zzz          xxx         xxx         NaN       2\n2  C1017654      yyy,xxx      aaa,bbb         ccc         ddd       1\n3  C1016543      aaa,bbb          ccc         NaN         NaN       0\n4  C1019876          yyy          NaN         NaN         NaN       0\n", "df['counts'] = (\n    df.loc[:, \"mat_deliv_1\":\"mat_deliv_4\"].stack().str.contains('xxx').sum(level=0))\n", "df['counts'] = (df.loc[:, \"mat_deliv_1\":\"mat_deliv_4\"]\n                  .stack()\n                  .str.split(',', expand=True)\n                  .eq('xxx')\n                  .any(1)  # change to `.sum(1)` to count all occurrences\n                  .sum(level=0))\n", "df['counts'] = [\n    ','.join(x).count('xxx') \n    for x in df.loc[:, \"mat_deliv_1\":\"mat_deliv_4\"].fillna('').values\n]\ndf\n  Client_ID  mat_deliv_1  mat_deliv_2 mat_deliv_3 mat_deliv_4  counts\n0  C1019876  xxx,yyy,zzz  aaa,bbb,xxx         xxx         ddd       3\n1  C1018765      yyy,zzz          xxx         xxx         NaN       2\n2  C1017654      yyy,xxx      aaa,bbb         ccc         ddd       1\n3  C1016543      aaa,bbb          ccc         NaN         NaN       0\n4  C1019876          yyy          NaN         NaN         NaN       0\n"], "link": "https://stackoverflow.com/questions/54098875/count-occurrences-of-a-string-in-multiple-string-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two Pandas data frames. df1 has columns ['a','b','c'] and df2 has columns ['a','c','d']. Now, I create a new data frame df3 with columns ['a', b','c','d']. I want to fill df3 with all the inputs from df1 and df2. For example, if I have x rows in df1, and y rows in df2, then I will have x+y rows in df3. Which Pandas function fills the new dataframe based on partial columns?", "q_apis": "get columns columns columns columns all columns", "apis": "DataFrame DataFrame", "code": ["df1 = pd.DataFrame({'a':[1, 2, 3], 'b':[2, 3, 4], 'd':['h', 'j', 'k']})\ndf2 = pd.DataFrame({'a':[5, 6, 7], 'b':[1, 1, 1], 'c':[2, 2, 2]})\n", "   a  b    c    d\n0  1  2  NaN    h\n1  2  3  NaN    j\n2  3  4  NaN    k\n0  5  1  2.0  NaN\n1  6  1  2.0  NaN\n2  7  1  2.0  NaN\n"], "link": "https://stackoverflow.com/questions/53946531/pandas-create-a-data-frame-based-on-two-other-sub-frames"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm wondering what I'm doing wrong while applying pd.to_numeric on multiple columns in dataframe Sample of dataframe: First I get rid of unwanted characters: And then I apply to_numeric: I'm not getting any errors and yet the result looks like this: BTW works when I transform given columns one by one though. I'd love to be able to convert given data at same time. Thank you.", "q_apis": "get columns to_numeric columns get apply to_numeric any transform columns at time", "apis": "apply", "code": ["cols = ['temp', 'feels', 'wind', 'gust', 'rain', 'humidity', 'cloud', 'pressure']\ndf_weather[cols] = df_weather[cols].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n"], "link": "https://stackoverflow.com/questions/65776284/after-applying-pd-to-numeric-on-multiple-columns-there-is-no-change-in-columns-d"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here's a simple DataFrame example: with the output I want to transpose it and rename the columns: so the output would be Transpose method is not quite the thing, what should I do then?", "q_apis": "get columns DataFrame transpose rename columns", "apis": "T columns reset_index drop", "code": ["df = df.T\ndf.columns = ['numbers', 'letters', 'words']\ndf = df.reset_index(drop = True)\n"], "link": "https://stackoverflow.com/questions/54118288/python-pandas-trasnpose-dataframe-and-rename-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to be able to change the colors of each bar of this stacked bar plot: The code currently is: The Dataframe has multiple columns with two rows for each column. How does the my_colors List have to like like to change the color for each part of the barplot?", "q_apis": "get columns plot columns", "apis": "DataFrame columns columns T plot columns columns T plot T", "code": ["import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\n\nd = {a: [random.randint(2, 5), random.randint(3, 7)] for a in list('abcdefghij')}\n\ndf = pd.DataFrame(d)\nmy_colors0 = [plt.cm.plasma(i / len(df.columns) / 2 + 0.5) for i in range(len(df.columns))]\nax = df.T[0].plot(kind='bar', color=my_colors0, width=0.7)\nmy_colors1 = [plt.cm.plasma(i / len(df.columns) / 2) for i in range(len(df.columns))]\nax = df.T[1].plot(kind='bar', bottom=df.T[0], color=my_colors1, width=0.7, ax=ax)\n\nplt.show()\n"], "link": "https://stackoverflow.com/questions/60073779/change-colors-in-stacked-barplot-from-dataframe-plot"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe contains misspelled words and abbreviations like this. I need to correcting the misspelled words and the Abvreviations I have tried with creating the dictionary such as: and applying this code The output is I have succeded correcting misspelled but not the abbreviation Please help.", "q_apis": "get columns contains", "apis": "DataFrame columns apply replace", "code": ["df = pd.DataFrame(['swtch', 'cola', 'FBI', 'smsng', 'BCA', 'MIB'], columns=['misspelled'])\nabbreviations = {\n    'FBI': 'Federal Bureau of Investigation',\n    'BCA': 'Bank Central Asia',\n    'MIB': 'Men In Black',\n    'cola': 'Coca Cola'\n}\n\nspell = SpellChecker()\ndf['fixed'] = df['misspelled'].apply(spell.correction).replace(abbreviations)\n", "  misspelled                            fixed\n0      swtch                           switch\n1       cola                        Coca Cola\n2        FBI  Federal Bureau of Investigation\n3      smsng                            among\n4        BCA                Bank Central Asia\n5        MIB                     Men In Black\n"], "link": "https://stackoverflow.com/questions/57443165/dealing-with-abbreviation-and-misspelled-words-in-dataframe-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My Excel file Contains Following Columns I need to filter out the row by present Date and Time. Means, it should display those rows where Today is between \"From Date\" and \"To Date\" AND Present time is Between \"From Time\" and \"To Time\" This is my code : The first filter is working perfectly, It is able to filter Date But second Filter for Time is not working. It is erroring out , where as the syntax is same for both. Please help.", "q_apis": "get columns filter where between time first filter filter second where", "apis": "astype astype loc pop pop", "code": ["df['from'] = pd.to_datetime(df['From Date'].astype(str) + ' ' + df['From Time'])\ndf['to'] = pd.to_datetime(df['To Date'].astype(str) + ' ' + df['To Time'])\n\nnow = pd.to_datetime('now')\ndf.loc[(now >= df.pop('from')) & (now <= df.pop('to'))]\n", "    From Date     To Date From Time   To Time\n1  2020-09-01  2020-09-02  12:00:00  13:00:00\n2  2020-08-31  2020-09-05  12:00:00  13:00:00\n"], "link": "https://stackoverflow.com/questions/63691975/filter-out-dataframe-for-preset-date-and-time"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I have three tables (A, A_to_B, B), one of which is a join table for many->many relationships ... I need to create a dataframe that contains an array of flags for each id in B (whether or not a transaction in the join table exists), for each row in A ... Very hard to explain, but below are the example tables ... And I want to end up with a dataframe that looks like this Hopefully that makes sense. Also, keep in mind that this has to be an efficient sqlite query as I am dealing with potentially tens of thousands of rows from each table. I have each table loaded into a separate dataframe, as follows (but of course that is by no means a constraint on the solution to this).", "q_apis": "get columns join contains array flags join query", "apis": "DataFrame DataFrame DataFrame dropna", "code": ["# input\ndf = pd.DataFrame({'A':[1,1,2], 'B':[2,3,3]})\ndfa = pd.DataFrame({'A':[1,2,3], 'tt':['f','b','z']})\ndfb = pd.DataFrame({'B':[1,2,3], 'tt':['fb','bb','zb']})\n\n# output\na = pd.Categorical(df['A'], categories=dfa['A'])\nb = pd.Categorical(df['B'], categories=dfb['B'])\npd.crosstab(a, b, dropna=False, rownames=['A'], colnames=['B'])\n"], "link": "https://stackoverflow.com/questions/58699913/pandas-dataframe-encoding-vector-from-manytomany-join-table-in-sqlite3"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am attempting to generate more variables for my dataset. My data is stored in multiple files, and using pandas I can only read a single file at once because of the memory limitations. Each csv file has the data for a single month and goes something like this: Now I am trying to generate more feature for each row based on the history of each sender and join these features to the dataframe. For example: As you can see from the desired dataframe above, the new variables are generated based on the sender's previous observations. What is the least computationally expensive way of generating such features? I will need to obtain information from all my monthly csv files to gather such data. There are over 200,000 unique senders, so it will take weeks to read the csv files and produce a dataframe and a csv file for every unique sender and merge this data with the monthly csv files. I am aware of dask and dask distributed, but I want to find out if there is a simpler way for me to implement what I am trying to do.", "q_apis": "get columns at month join all unique take unique merge", "apis": "to_parquet sort_values diff min cumsum diff groupby min reset_index merge astype", "code": ["import dask.dataframe as dd\ndf = dd.read_parquet('processed')\ndf.to_parquet('processed2', partition_on='Sender')\n", "def fun(df):\n    df = df.sort_values(\"Date\")\n    df[\"Day Since Prev Shipment\"] = df[\"Date\"].diff().dt.days\n    df[\"Day Since First Shipment\"](df[\"Date\"] - df[\"Date\"].min()).dt.days\n    df[\"Cumulative Quantity\"] = df[\"Quantity\"].cumsum() \n    df[\"Quantity difference\"] = df[\"Quantity\"].diff()\n    grp = df.groupby(\"Recipient\")[\"Date\"].min().reset_index(name=\"First Shipment\")\n    df = pd.merge(df, grp, how=\"left\", on=\"Recipient\")\n    df[\"First Shipment\"] = (df[\"Date\"]==df[\"First Shipment\"]).astype(\"int8\")\n    return df\n"], "link": "https://stackoverflow.com/questions/63263969/pandas-dataframe-generate-variables-using-previous-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Dataframe: I have this dataframe where 1,2,3 are columns and Main is set for index (set_index) Is there a way to tell pandas on dictionary conversion that I need nested dictionary. I.e. that for each Key 1 row-wise theres a list of values, inside that list of values there's a dictionary in which columns (1,2,3)-> are keys as well. All this one by one, row by row. Desired output: reproducible dataframe: no luck with this pseudo code approach. I tried with zip() as well doesn't seem to concatenate the list of dictionary to the column of the column with keys once I convert it to: I get really close to the answer however I don't know how to concetante or tell pandas that I need additonal column with bunch of keys (key1,key1,key1,key2,key2) -> with it's correspoding information ultimately the desired output in json:", "q_apis": "get columns where columns index set_index values values columns keys keys get keys", "apis": "DataFrame index values to_dict", "code": ["out = pd.DataFrame({'Main':df.index, 'values': df.to_dict(orient='records')})\n", "   Main                              values\n0  key1  {'1': 'ins', '2': 'tbr', '3': 'a'}\n1  key1  {'1': 'inb', '2': 'tba', '3': 'b'}\n2  key1  {'1': 'inr', '2': 'tbe', '3': 'c'}\n", "Main\nkey1    {'1': 'ins', '2': 'tbr', '3': 'a'}\nkey1    {'1': 'inb', '2': 'tba', '3': 'b'}\nkey1    {'1': 'inr', '2': 'tbe', '3': 'c'}\ndtype: object\n"], "link": "https://stackoverflow.com/questions/64992648/pandas-dataframe-nested-dictionary-keylistkeyvalue"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe (330 rows \u00d7 11 columns) that is indexed by an integer (age). I would like to downsample to dataframe to 97 rows. Can anyone think of an elegant and simple way to achieve this? Sample data: Thanks", "q_apis": "get columns columns", "apis": "iloc astype", "code": ["df.iloc[np.linspace(0, len(df) - 1, 97).astype(int)]\n", "df = pd.DataFrame(np.repeat(np.arange(33)[:, None], 3, axis=1))\n\n# Use 7 to guarantee a length 7 result.\ndf.iloc[np.linspace(0, len(df) - 1, 7).astype(int)]\n\n     0   1   2\n0    0   0   0\n5    5   5   5\n10  10  10  10\n16  16  16  16\n21  21  21  21\n26  26  26  26\n32  32  32  32\n"], "link": "https://stackoverflow.com/questions/49890625/resample-dataframe-with-an-integer-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset with a parameter of location. There are approximately 75 locations. Each location can have sub-locations. I needed to make plots for each location, so I broke the dataset into a dictionary of dataframes and worked on each value within the dictionary. Now I need to breakdown each value in the dictionary (the dataset belonging to the location) into datasets by sublocation. So if a location has 3 sublocation, I need 3 new dataframes. Using the following posts: PANDAS split dataframe to multiple by unique values rows Loop through a dictionary of dataframes When I look at the length of dfs2, I noticed that it only has 3 datasets. I know there are nearly 300 sublocations, so I need dfs2 to have key name of sublocation and value of all rows of 'd' with corresponding location and sublocation EDIT: I am attaching some sample data Sample Data. In the real data (it's sensitive can't post it) there are over 70locations and 300 sublocations the dictionary dfs has key M1 value: (all rows with location M1) now I need dfs2 with key 21M1 value: (all rows with sublocation 21M1) They should still be grouped by the location, which is why I was thinking about a 'sub dictionary' EDIT2: Following the advice of @Joe I used the fact that I can access each location using the dictionary I have. Using my original data I can make a list of unique sublocation values. Then use a loop to go through each dict value, and make a tmp dataframe where the sublocation matches some value in the unique list. I can use the temp dataframe to do my stats. I am adding the code too. Any chance this maybe flawed? EDIT 3: I am stuck with one last thing. I can't get the file to save in the proper directory. I made a new dictionary where key:values are subloc:d[d['subloc']== X]", "q_apis": "get columns sub value value unique values at length name value all sample value all now value all sub unique values value where value unique last get where values", "apis": "groupby groupby items append update items items", "code": ["dfs = dict(tuple(d.groupby('location')));\ndfss = dict(tuple(d.groupby('sublocation')));\ndd = {}\nfor key, value in dfs.items():\n    a = []\n    dee={}\n    for i in value['sublocation']:\n        if i in a:\n            pass\n        else:\n            a.append(str(i))\n    dee = {key:a}\n    dd.update(dee)\nfor key, value in dfss.items(): \n    try:\n        for k, v in dd.items():\n            if key in v:\n                dur=str(k)\n        else:\n             pass\n\n         #CODE FOR PLOTS\n         #SAVE PLOT           \n   except:\n         #SAVE PLOT\n         pass\n"], "link": "https://stackoverflow.com/questions/57754503/creating-a-new-set-of-dataframes-from-dictionary"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This question is based on a previous question I answered. The input looks like: I need to find every Buy-Sell sequence (ignoring extra Buy / Sell values out of sequence) and calculate the difference in Price. The desired output: My solution is verbose but seems to work: Is there a vectorised method? I'm concerned about code maintainability and performance over a large number of rows. Edit: Benchmarking code:", "q_apis": "get columns values difference", "apis": "shift eq cumsum groupby agg first last loc eq last sub first values mean std groupby groupby diff loc isin mean std", "code": ["0    0\n1    0\n2    1\n3    1\n4    1\n5    1\nName: Results, dtype: int32\n", "   Index  Results  Price  Diff\n0      0      Buy     10   NaN\n1      1     Sell     11   1.0\n2      2      Buy     12   NaN\n3      3  Neutral     13   NaN\n4      4      Buy     14   NaN\n5      5     Sell     15   3.0\n", "In [27]: df = pd.concat([df]*10**4, ignore_index=True)\n\nIn [28]: %%timeit\n    ...: a = df.Results.shift().eq('Sell').cumsum()\n    ...: agr = df.groupby(a).Price.agg(['first', 'last'])\n    ...: df.loc[df.Results.eq('Sell'), 'Diff'] = agr['last'].sub(agr['first']).values\n    ...:\n17.6 ms \u00b1 199 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [29]: %%timeit\n    ...: s=df.groupby('Results').cumcount()\n    ...: df['Diff']=df.Price.groupby(s).diff().loc[df.Results.isin(['Buy','Sell'])]\n    ...:\n3.71 s \u00b1 331 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"], "link": "https://stackoverflow.com/questions/52731563/vectorising-a-loop-based-on-the-order-of-values-in-a-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Considering the two dataframes below: I need to compare the column from with the same column from . If the value of appears within 's , the column must be added to . An example of desired output: However, I have no idea how to iterate over this column. Can anyone help? P.s.: I transform the column from to string because it is the column format in the real scenario, the result of reading a .csv file.", "q_apis": "get columns compare value transform", "apis": "merge assign explode assign astype", "code": ["df.merge(df2.assign(ID_ANTENNA=df2['ID_ANTENNA'].str.split(', '))\n            .explode('ID_ANTENNA')\n            .assign(ID_ANTENNA=lambda x: x['ID_ANTENNA'].astype(int)),\n         on='ID_ANTENNA', how='left')\n", "   ID_ANTENNA        LAT       LONG CITY_IDS\n0           1  -21.12699  -44.26144    SJDR1\n1           2  -21.12699  -44.26144    SJDR1\n2           3  -21.48194  -44.33039       LD\n3           4  -21.12699  -44.21979      SVM\n4           5  -21.11886  -44.16478       TR\n"], "link": "https://stackoverflow.com/questions/65038437/comparison-between-dataframes-check-if-values-of-a-column-of-one-of-the-datafra"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a log file (Text.TXT in this case): To read in this log file into pandas and ignore all the header info I would use up to line 16 like so: But this produces as it is skipping past where the data is starting. To make this work I've had to use it on line 11: My question is if the data doesn't start until row 17, in this case, why do I need to request a skiprows up to row 11?", "q_apis": "get columns all info where start", "apis": "T mode T T", "code": ["from io import StringIO\n\ntext='''# 1: 5\n# 3: x\n# F: 5.\n# ID: 001\n# No.: 2\n# No.: 4\n# Time: 20191216T122109\n# Value: \";\"\n# Time: 4\n# Time: \"\"\n# Time ms: \"\"\n# Date: \"\"\n# Time separator: \"T\"\n# J: 1000000\n# Silent: false\n# mode: true\nTimestamp;T;ID;P\n16T122109957;0;6;0006'''\n\ndf = pd.read_csv(StringIO(text),comment='#',sep=';')\ndf\n      Timestamp  T  ID  P\n0  16T122109957  0   6  6\n"], "link": "https://stackoverflow.com/questions/62342424/why-does-read-csv-skiprows-value-need-to-be-lower-than-it-should-be-in-this-case"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to read all data matrixes from an image and write to dataframe. I can print barcode number and location via pylibdmtx but I can't figure out how to store in dataframe 'msg' variable stored as list with 2 elements in this case and when I try to convert pandas Dataframe 'data' column is blank but 'rect' column is proper like above. Dataframe looks like below; How can I fill data column or any other method do you suggest? My second question is, this library seems very slow any suggestion how to make it quicker? Thanks in advance,", "q_apis": "get columns all any second any", "apis": "append", "code": ["import cv2\nimport matplotlib.pyplot as plt\nfrom pylibdmtx.pylibdmtx import decode\nimport pandas as pd \n", "image = cv2.imread('datamatrix.png')\ngray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\nplt.imshow(gray)\nret,thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\nmsg = decode(thresh)\nprint(msg)\n", "[Decoded(data=b'Stegosaurus', rect=Rect(left=5, top=6, width=96, height=95)), Decoded(data=b'Plesiosaurus', rect=Rect(left=298, top=6, width=95, height=95))]\n", "ls_msg = []\nfor item in msg:\n    ls_msg.append([item[0], item[1][0], item[1][1], item[1][2]])\n"], "link": "https://stackoverflow.com/questions/58428146/how-to-write-decoded-data-matrix-to-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My issue In SQL it is very easy to apply different aggregate functions to different columns, e.g. : In pandas, not so much. The solution provided here became deprecated: My solution The least worst solution I have managed to find, mostly based on other stack overflow questions I can no longer find, is something like the toy example at the bottom, where I: define a function with all the calculations I need calculate each column separately, then put them together in a dataframe apply the function as a lambda function: What I would like to improve: naming columns If you only have 2 or 3 columns to create, this solution is great. However, if you have many columns to calculate, naming them becomes fiddly and very error-prone: I have to create a list with the column names, and pass that list as the index of the dataframe created by the function. Now imagine I already have 12 columns and need to add 3 more; there's a chance I may make some confusion and add the corresponding column names in the wrong order. Compare this with SQL, where you assign the name right after defining the calculation - the difference is night and day. Is there a better way? E.g. a way to assign the name of the column at the same time I define the calculation? Why this is not a duplicate question The focus of the question is specifically on how to name the columns so as to minimise the risk of errors and confusion. There are somewhat similar questions based on now deprecated functionalities of pandas, or with answers which provide an automatic naming of the columns but, to my knowledge, no question which focuses on this very point. Toy example", "q_apis": "get columns apply aggregate columns stack at where all put apply columns columns columns names index columns add add names where assign name right difference day assign name at time name columns now columns", "apis": "DataFrame columns sum sum sum sum sum mean sum sum sum sum sum sum sum groupby apply", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(columns =['a','b','c','d'], data = np.random.rand(300,4))\ndf['city'] = np.repeat(['London','New York','Buenos Aires'], 100)\n\ndef func(x, df):\n    # func() gets called within a lambda function; x is the row, df is the entire table   \n    s_dict = {}\n\n    s_dict['sum of a'] = x['a'].sum()\n    s_dict['% of a'] = x['a'].sum() / df['a'].sum() if df['a'].sum() !=0 else np.nan\n    \n    s_dict['avg of b'] = x['b'].mean()\n    \n    s_dict['weighted avg of a, weighted by b'] = ( x['a'] * x['b']).sum() / x['b'].sum() if x['b'].sum() >0 else np.nan\n    \n    s_dict['sum of c'] = x['c'].sum()\n    s_dict['sum of d'] = x['d'].sum()\n    \n    return pd.Series( s_dict  ) \n\nout = df.groupby('city').apply(lambda x: func(x,df))\n"], "link": "https://stackoverflow.com/questions/66195952/pythonic-way-to-apply-different-aggregate-functions-to-different-columns-of-a-pa"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am new to Python, especially data handling. This is what I am trying to achieve- I run CIS test on several servers and produce a CSV file for each server (file name is the same as the server name). The output file from all servers is copied to a central server The output produced looks like below (Truncated output)- What I want is that output should look like- To merge these files, I have created another file with only Description field in it and two-column heading as below- I have written below code to do column-based merge- The output I am getting is- In my code, I am not sure how I get the file name as a header for the result and get rid of NaN. Also, is there a better way of achieving the output I am looking for without using dummy file(cis_report.csv)? Your help is much appreciated.", "q_apis": "get columns test name name all merge merge get name get", "apis": "all replace all all DataFrame append DataFrame pivot DataFrame to_csv", "code": ["import os\nimport pandas as pd\n\n# Get all file names in a directory\n# Use . to use current working directory or replace it with\n# e.g. r'C:\\Users\\Dames\\Desktop\\csv_files'\nfile_names = os.listdir('.')\n\n# Filter out all non .csv files\n# You can skip this if you know that only .csv files will be in that folder\ncsv_file_names = [fn for fn in file_names if fn[-4:] == '.csv']\n\n# This Loads a csv file into a dataframe and sets the Server column\ndef load_csv(file_name):\n    df = pd.read_csv(file_name)\n    df['Server'] = file_name.split('.')[0]\n    return df\n\n# Append all the csvfiles after being processed by load_csv\ndf = pd.DataFrame().append([load_csv(fn) for fn in csv_file_names])\n\n# Turn DataFrame into Pivot Table\ndf = df.pivot('Description', 'Server', 'Result')\n\n# Save DataFrame into CSV File\n# If this script runs multiple times make sure that the final.csv is saved elsewhere!\n# Or it will be read by the code above as an input file\ndf.to_csv('final.csv')\n", "Server                                     dc1pp1v01 dc1pp1v02 dc1pp2v01\nDescription\n1.1 Database Placement                        PASSED    PASSED    PASSED\n1.2 Use dedicated least privilaged account    PASSED    PASSED    PASSED\n1.3 Diable MySQL history                      PASSED    FAILED    PASSED\n", "Description,dc1pp1v01,dc1pp1v02,dc1pp2v01\n1.1 Database Placement,PASSED,PASSED,PASSED\n1.2 Use dedicated least privilaged account,PASSED,PASSED,PASSED\n1.3 Diable MySQL history,PASSED,FAILED,PASSED\n"], "link": "https://stackoverflow.com/questions/63939462/consolidating-column-data-from-number-of-csv-files-into-a-single-csv-file"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am using Python. I have a dictionary of Dataframes. Each dataframe has a name in the dictionary and I can reference it correctly no problem. I am trying to take that name and add it as a column across every row. I am having a rough time doing this.", "q_apis": "get columns name take name add time", "apis": "DataFrame DataFrame items", "code": ["import pandas as pd\n\nframes = {\n  'foo': pd.DataFrame({'a': [1, 2], 'b': [3, 4]}),\n  'bar': pd.DataFrame({'a': [9, 8], 'b': [7, 6]})\n}\n\nfor name, df in frames.items():\n  df['name'] = name\n  print(df, '\\n')\n", "   a  b name\n0  1  3  foo\n1  2  4  foo\n\n   a  b name\n0  9  7  bar\n1  8  6  bar\n"], "link": "https://stackoverflow.com/questions/53523186/dictionary-of-dataframes-select-name-of-the-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Pandas DF as below, and I'm struggling with printing it in a good looking format... Could someone please show me how to combine those two values from same column values? And I wish the result would be exactly as below:", "q_apis": "get columns combine values values", "apis": "groupby agg join to_string", "code": ["print(df.groupby('Animal')['Color'].agg(', '.join).to_string())\n\n\nAnimal\nBIRD           YELLOW\nCAT     BLACK, ORANGE\nDOG             WHITE\n"], "link": "https://stackoverflow.com/questions/63936631/pandas-combining-one-columns-values-with-same-index-into-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe, df, that has some columns of type float64, while the others are of object. Due to the mixed nature, I cannot use as the error happened with the columns whose type is float64 (what a misleading error message!) so I'd wish that I could do something like So my question is if there is any such filter expression that I can use with df.columns? I guess alternatively, less elegantly, I could do: I also would like to know why in the above code replacing '' with 'unknown' the code would work for certain cells but failed with a cell with the error of \"ValueError: Error parsing datetime string \"unknown\" at position 0\" Thanks a lot! Yu", "q_apis": "get columns columns columns any filter columns at", "apis": "loc dtypes loc dtypes fillna", "code": ["In [11]: df = pd.DataFrame([[1, 'a', 2.]])\n\nIn [12]: df\nOut[12]: \n   0  1  2\n0  1  a  2\n\nIn [13]: df.dtypes\nOut[13]: \n0      int64\n1     object\n2    float64\ndtype: object\n\nIn [14]: df.dtypes == object\nOut[14]: \n0    False\n1     True\n2    False\ndtype: bool\n", "In [15]: df.loc[:, df.dtypes == object]\nOut[15]: \n   1\n0  a\n", "In [16]: df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes == object].fillna('')\n"], "link": "https://stackoverflow.com/questions/21720022/find-all-columns-of-dataframe-in-pandas-whose-type-is-float-or-a-particular-typ"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There is an HDF file 'file.h5' and the key name of a pandas DataFrame (or a Series) saved into it is 'df'. How can one determine in what format (i.e. \u2018fixed\u2019 or \u2018table\u2019) was 'df' saved into the file? Thank you for your help!", "q_apis": "get columns name DataFrame Series", "apis": "info index info shape info info info", "code": [">>> print(h5_table.info())\n<class 'pandas.io.pytables.HDFStore'>\nFile path: /tmp/df_table.h5\n/df            frame_table  (typ->appendable,nrows->2,ncols->2,indexers->[index],dc->[])\n\n>>> print(h5_fixed.info())\n<class 'pandas.io.pytables.HDFStore'>\nFile path: /tmp/df_fixed.h5\n/df            frame        (shape->[2,2]) \n", "def get_hd5_format(path, key):\n    with pd.HDFStore(path) as store:\n        info = store.info()\n    return 'table' if 'typ->appendable' in next(k for k in info.splitlines()[2:] if k.startswith('/'+key)).split()[2] else 'fixed'\n", ">>> get_hd5_format('/tmp/df_table.h5', 'df')\n'table'\n>>> get_hd5_format('/tmp/df_fixed.h5', 'df')\n'fixed'\n"], "link": "https://stackoverflow.com/questions/50569465/determine-format-of-a-dataframe-in-pandas-hdf-file"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "All good so far, this is how I expected the dataframe and datetime values to be displayed. The problem comes when I save the dataframe to an excel file and then read it back into a dataframe, the datetime values get messed up. I'd like to evaluate to", "q_apis": "get columns values values get", "apis": "to_excel index", "code": ["df[\"datetime\"] = df[\"datetime\"].dt.strftime(\"%m/%d/%Y, %H:%M:%S\")\ndf.to_excel('price_data.xlsx', index=False)\n\nnew_df = pd.read_excel('price_data.xlsx')\nnew_df[\"datetime\"] = pd.to_datetime(new_df[\"datetime\"], format=\"%m/%d/%Y, %H:%M:%S\")\n"], "link": "https://stackoverflow.com/questions/68081938/pandas-datetime-values-messed-up-after-saving-df-to-excel-and-then-reading-back"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to update the dataframe with the values coming from another dataframe if some condition hold true. The indexes and the columns names of the dataframes does not match. How could it be done?", "q_apis": "get columns update values columns names", "apis": "reindex index loc", "code": ["s = df_new['profile'].reindex(df.index)\nabove_max = s >= 7\ndf.loc[above_max, 'val'] = s\n"], "link": "https://stackoverflow.com/questions/65091126/how-to-update-a-dataframe-with-values-from-another-dataframe-when-indexes-and-co"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two df's, one for user names and another for real names. I'd like to know how I can check if I have a real name in my first df using the data of the other, and then replace it. For example: My code should replace 'peterKing' and 'joe545' since these names appear in my df2. I tried using pd.contains, but I can only verify if a name appears or not. The output should be like this: Can someone help me with that? Thanks in advance!", "q_apis": "get columns names names name first replace replace names contains name", "apis": "loc", "code": ["for real_name in df2['realName'].to_list():\n  df1.loc[ df1['userName'].str.contains(real_name), 'userName' ] = real_name\n", "   userName\n0   peter\n1   john\n2   joe\n3   mary\n"], "link": "https://stackoverflow.com/questions/68076481/check-if-pandas-column-contains-text-in-another-dataframe-and-replace-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The development that I have to do in Python consists of taking an xml file with the tree built from the sig. shape: Xml file example:I put here xml file becuase of big it is The last tag (Signal) as seen in the image above in green is from which I have to extract the values \u200b\u200bof the Name and Value attributes; this can be repeated at the same level two or more times with different values \u200b\u200bin these attributes. The functions that read the xml would be the following: When taking the data with Pandas, putting it into a dataframe and grouping the data, I group them one by one and not all at once; this is the function I am using to group them and paint them in a graph with Matplotlib: This is the console result of this last function, which brings me all the data separately and obviously the graphs too: The problem is that I do not know if there is any way to join the attributes (Name and Value) of Signal to pass them to Pandas in the Dataframe and I graph everything together with names and values \u200b\u200bin the graph. And I did not graph each name and value separately as in the last image. I have tried with lists, tuples and dictionaries but I cannot unite the values, as if the label were a single entity and thus it was painted, separately. Next I share a failed attempt with lists by changing the levelChild7 and transformData function: The result is this below, which is the Name and Value attribute as something that I cannot find how to group, since they are separated by rows and by each tag that it finds in the XML document: Could someone guide me to know how to regroup the values \u200b\u200bof Name and Values \u200b\u200band put them in a single variable? Or tell me what is missing from my code? Thanks in advance.", "q_apis": "get columns shape put last values at values all at last all any join names values name value last values values put", "apis": "groupby count plot", "code": ["import xml.etree.ElementTree as ET\nimport pandas as pd\n\nfile_xml = ET.parse(\"Input.xml\")\n\ndata = [\n    {\"Name\": signal.attrib[\"Name\"],\n     \"Value\": signal.attrib[\"Value\"]\n    } for signal in file_xml.findall(\".//Signal\")\n]\n\nsignals_df = pd.DataFrame(data)\n\nsignals_df\n#            Name             Value\n# 0        Status             4 Run\n# 1   PhysicalRes       0 0,1 1,2 2\n# 2        Status             4 Run\n# 3        Status             1 Off\n# 4     GlblClkYr    0 2000,21 2021\n# 5        BrkTot        8191 Fault\n# 6           ACU  0 FrontRequester\n# 7           ACU           7 Radio\n# 8           ACU  0 FrontRequester\n# 9           ACU         4 Granted\n# 10   GlblClkDay           1 1-3 3\n\nsignals_df.groupby([\"Name\"]).count()\n#              Value\n# Name\n# ACU              4\n# BrkTot           1\n# GlblClkDay       1\n# GlblClkYr        1\n# PhysicalRes      1\n# Status           3\n", "data = [signal.attrib for signal in file_xml.findall(\".//Signal\")]\n\nsignals_df = pd.DataFrame(data)\n\nsignals_df\n#        Error    Hexval         Name             Value\n# 0   {x:Null}     0,1,2       Status             4 Run\n# 1   {x:Null}     0,1,2  PhysicalRes       0 0,1 1,2 2\n# 2   {x:Null}      0,15       Status             4 Run\n# 3   {x:Null}      0,15       Status             1 Off\n# 4   {x:Null}      0,15    GlblClkYr    0 2000,21 2021\n# 5   {x:Null}      1FFF       BrkTot        8191 Fault\n# 6   {x:Null}  {x:Null}          ACU  0 FrontRequester\n# 7   {x:Null}  {x:Null}          ACU           7 Radio\n# 8   {x:Null}  {x:Null}          ACU  0 FrontRequester\n# 9   {x:Null}  {x:Null}          ACU         4 Granted\n# 10  {x:Null}         1   GlblClkDay           1 1-3 3\n", "import xml.etree.ElementTree as ET\nimport pandas as pd\nimport matplotlib.pyplot as plt\n...\n\nsignals_df.groupby([\"Name\"]).count().plot(kind='bar', rot=0)\n\nplt.show()\nplt.clf()\nplt.close()\n"], "link": "https://stackoverflow.com/questions/67794684/merge-list-of-results-into-a-single-variable-with-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm reading from a CSV file with columns of various types. From my understanding, the format implies that it is a byte string and I have to it to a string formatted in ascii and remove as well as encodings like . However, when I try to decode, I get an error. I think what is going on is that when reading from the csv file, the column is set to data type, as such . So I checked the raw CSV file, and I saw . Is there a way to properly read this column as a byte string such that I could later call the decode function? I've also tried setting the for that specific column in the function and calling after reading the csv, but neither works. My last resort would be to manually remove the encodings with regex.", "q_apis": "get columns columns get last", "apis": "fillna apply", "code": ["from ast import literal_eval\n\ndf['columnname'] = df['columnname'].fillna(\"b''\").apply(lambda x: literal_eval(x).decode('utf-8'))\nprint(df)\n", "   index                       columnname\n0      1  Hi,\\r\\n\\r\\nI hope you are well.\n1      2                \u00a0Hello,\\r\\n\u00a0\\r\\n \n2      3          \\r\\n\\r\\n blah blah blah\n3      4                                 \n4      5                   blah blah blah\n"], "link": "https://stackoverflow.com/questions/64489221/string-formatted-as-utf-8-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Python newbie here, I will like to flag sporadic numbers that are obviously off from the rest of the row. In simple terms, flag numbers that seem not to belong to each row. Numbers in 100s and 100000s are considered 'off the rest' I am trying to do something exactly like this", "q_apis": "get columns", "apis": "style apply to_excel index", "code": ["def higlight_outliers(s):\n    # force to numeric and coerce string to NaN\n    s = pd.to_numeric(s, errors='coerce')\n    indexes = (s<1500)|(s>1000000)\n    return ['background-color: yellow' if v else '' for v in indexes]\n\nstyled = df.style.apply(higlight_outliers, axis=1)\n\nstyled.to_excel(\"flagged_data.xlsx\", index=False)\n"], "link": "https://stackoverflow.com/questions/66910807/how-to-flag-an-anomaly-in-a-data-frame-row-wise"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a large dataframe. It contains the columns 'Date', 'Time', 'Ticker', 'Open', 'High', 'Low', 'Close'. Edit.. I added 2 days worth of sample 25 01/03/18 8:15 PM USD/JPY 112.765 112.765 112.700 112.710 What I'm doing first is to find the highest (.idmax) value in the 'High' column for each date. Edit, 2 days worth of output The next step is where I get tripped up. I'd like to add one column, 'Open', for if 'Time' == 2:15, to the end of my dataframe. I have some code that finds this value when I create a new dataframe. I've tried to use a .merge function. It adds to my dataframe but as a separate row. I have also tried a concat function without any success. Ideally I'd have the following columns: 'Date' 'Time' 'Ticker' 'Open' 'High' 'Low' 'Close---- which I can get via the data dataframe. But to start I'd just like to add to the end the 'Open' value from the dataframe when the 'Time' == '2:15' Edit.. This would be the desired output print(result)", "q_apis": "get columns contains columns days sample first value date days step where get add value merge concat any columns get start add value", "apis": "loc rename columns merge set_index", "code": ["price_df = data.loc[data['Time'] == '2:15 PM', ['Date', 'Open']].rename(columns={'Open': 'value_at1415'})\nwith_cols_df = data.merge(price_df, on='Date', how='left')\n", "df['value_at1415'] = data['Date'].map(data[data['Time'] == '2:15 PM'].set_index('Date')['Open'])\n"], "link": "https://stackoverflow.com/questions/57513030/add-column-to-end-based-off-if-logic"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 dataframes: I want to take out values from df2 and append values of price_1, price_2, price_3, price_4 for each iteration of getting data from db (in df2) in df1 for matching df1.id = df2.id and df1.name = df2.name df1: df2(results form db): output: loop: I don't need the and columns from , I just need to compare if is equal in both df1 and df22. If yes, then take all the price_1/2/3/4", "q_apis": "get columns take values append values name name columns compare take all", "apis": "DataFrame groupby merge drop append", "code": ["df4 = pd.DataFrame()\ngrouped = df1.groupby('id')\n    \n    for i,groups in grouped:\n        df2 = sql(i) #goes to sql to fetch details for df1.id\n        sql_df = df2.name.unique()\n        dd = groups.name\n        if (set(sql_df) == set(sql_df) & set(dd)) & (set(dd) == set(sql_df) & set(dd)):\n            print (\"ID:\", i, \"Names Match: Y\")\n            df_temp = (df1[['id', 'name', 'tag']].merge(df2.drop('tag', axis=1), \n                                                        on = ['id', 'name']))\n            df4 = df4.append(df_temp, ignore_index = True)\n        else:\n            print(\"ID:\", i, \"Names Match: N\")\n"], "link": "https://stackoverflow.com/questions/63718450/compare-2-dfs-and-append-values-in-df1-based-on-query-in-df2"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Lets generate some dataframe: So let's look at amount of rows by category and subcategory: we got This is a dataframe of 1000 elements. There are 600 elements of cat1, 300 of cat2 and 100 of cat3. What I want is to reduce this dataframe from 1000 to let's say 60 rows so 1)each category has same amount of rows (20 in our case, which equals 60 / (number of categories)) 2) proportion of each subcategory in a category is kept 3) if we have small number of subcategory items it still stays in category (there is only one 'subX' in cat1, we need to keep it even if it's proportion was 1/600 for cat1). So when we create our new df I would like to receive something like this: In this case there are 21 element for cat1, but it is not a big deal, the main idea is that proportion of subcategories are saved and amount of rows is around targeted number 20.", "q_apis": "get columns at equals categories items", "apis": "nunique groupby size sum assign merge rename reset_index query drop columns groupby size", "code": ["# total (approximate) number of rows to keep\nn = 60\n\n# number of rows per category\nn_per_cat = n / df['category'].nunique()\n\n# number of rows per subcategory\ng_subcat = df.groupby(['category', 'subcategory'])\nz = g_subcat['category'].size()\nn_per_subcat = np.ceil(z / z.sum(level=0) * n_per_cat)\n\n# output\ndf_out = (df\n          .assign(i=g_subcat.cumcount())\n          .merge(n_per_subcat.rename('n').reset_index())\n          .query('i < n')\n          .drop(columns=['i', 'n']))\n\n# test\ndf_out.groupby(['category', 'subcategory']).size()\n", "category  subcategory\ncat1      sub1            2\n          sub2            4\n          sub3            4\n          sub4           10\n          subX            1\ncat2      sub1            2\n          sub2            4\n          sub3            4\n          sub4           10\ncat3      sub1            2\n          sub2            4\n          sub3            4\n          sub4           10\n"], "link": "https://stackoverflow.com/questions/66950099/pandas-sample-from-df-keeping-balance-of-groups"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My program has two loops. I generate a df in each looping. I want to append this result. For each iteration of inner loop, 1 row and 24 columns data is generated. For each iteration of outer loop, it generates 8 rows 24 columns data. I am having issues in appending in the right way so the final dataframe has 8 rows and 24 columns. My code: Present output: In the above, 8 rows is correct. But I got 192 columns, instead of 24. Here, 24 columns got repeated 8 times. That is the reason we see many NaNs here.", "q_apis": "get columns append columns columns right columns columns columns", "apis": "DataFrame columns DataFrame index append", "code": ["biglist1 = [item for sublist in biglist for item in sublist]", "# Before loop\ncolnames = ['y1', 'y2', 'y3']\ndf = pd.DataFrame(data=None, columns=colnames)\n", "biglist = []\nfor i in range (x1,...,x8):\n    for j in range ([y1,y2,y3],[y4,..]...[y22,y23,y24]):\n        tem_df = pd.DataFrame({'y1':[value1],'y2':[value2],'y3':[value3]},index=i)\n    biglist.append(pd.concat(tem_df),axis=1)\ndf = pd.concat(biglist)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/64774519/python-append-dataframe-generated-in-nested-loops"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "df For the df in the image, how do I find the number of instances for each column (eg:151) where the value of that column is <= 0 and value of \"WS\" column is > 0 (\"WS\" > 0 will be a constant \"AND\" with all the other columns - 151, 154, 152). So basically 151 <= 0 and WS > 0, 152 <= 0 and WS > 0.. and so on. How to write a for loop for this? The output needs to be stored in another df which would look something like (just random numbers)- 151 - 2368 152 - 3098 154 - 2301", "q_apis": "get columns where value value all columns", "apis": "columns index iterrows columns", "code": ["cols = df.columns[:-1] ##[151, 154, 152]\nout = pd.Series({c: 0 for c in cols})\nfor c in cols:\n    for index, row in df.iterrows():\n        if row[c] <= 0 and row['WS'] > 0:\n            out[c] = out[c] + 1\n", "cols = df.columns[:-1] ##[151, 154, 152]\nout = pd.Series()\nfor c in cols:\n    out[c] = len(df[(df[c] <= 0)&(df['WS'] > 0)])\n", "(df[c] <= 0)&(df['WS'] > 0)"], "link": "https://stackoverflow.com/questions/64002685/how-to-write-for-loop-with-multiple-conditions-for-a-dataframe-with-60-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have multiple text columns. I want to use bag of words for each text column, then create a new bag of words dataframe for each text column. This is what I have: I want 2 dataframes, one called answer1 and one called answer2, each with their own bag of words. But, I end up with one dataframe called \"a\" with only bag of words for answer2. Any ideas how to fix this?", "q_apis": "get columns columns", "apis": "append", "code": ["frame_list = [bow(text_df[a], a) for a in answer_list]\n", "frame_list = []\nfor answer in answer_list:\n    frame_list.append(bow(text_df[answer], answer))\n", "frame_table = {}\nfor idx, answer in enumerate(answer_list):\n    frame_table[\"answer\" + str(idx+1)] = (bow(text_df[answer], answer))\n"], "link": "https://stackoverflow.com/questions/66789066/loop-through-columns-and-create-multiple-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe which look like this: and another list that looks like this: I'm trying to write a test to check whatever all elements in the columns are in the list. If not - they should be inserted to another dataframe. The output should be: Any ideas ?", "q_apis": "get columns test all columns", "apis": "join assign apply join assign ne", "code": ["Allowed_fruits = ['Apple','Banana']\n\nf = lambda x: ','.join(y for y in x.split(',') if y not in Allowed_fruits)\ndf1 = df.assign(Fruits = df['Fruits'].apply(f))\n", "L = [','.join(y for y in x.split(',') if y not in Allowed_fruits) for x in df['Fruits']]\ndf1 = df.assign(Fruits = L)\n", "df1 = df1[df1['Fruits'].ne('')]\nprint (df1)\n     Name       Fruits\n2     Sam       Orange\n3  George  Kiwi,Cherry\n"], "link": "https://stackoverflow.com/questions/59734048/pandas-check-if-all-elemets-in-a-list-columns-are-in-another-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to assign a nested dict to a particular position, but it is'nt working. Here is what I wrote : but it gives the following output for the first iteration : However, what I expect is in the first row under current and consictively for the whole dataframe using row values. Note that, current and history were created using 'np.nan` to be populated later with nested dictionaries.", "q_apis": "get columns assign first first values", "apis": "loc", "code": ["df_this.loc[idx, 'current'] = [{row.cp_id : { row.products : {row.currency : row.tran_amnt}}}]\n"], "link": "https://stackoverflow.com/questions/59585993/value-assignment-to-a-particular-position-in-a-dataframe-is-not-working"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the below Data Frame: And I want to sum the total for columns A1 + A2 and for columns B1 +B2 The expected output is this: Sum_A = the sum of all the values in columns A1 and A2 Sum_B = the sum of all the values in columns B1 and B2 I'm working with pandas. Do you know how can I get the expected output", "q_apis": "get columns sum columns columns sum all values columns sum all values columns get", "apis": "sum melt groupby sum", "code": ["out = pd.wide_to_long(df, ['A', 'B'], i='id', j='val').sum()\n\nA    59\nB    66\ndtype: int64\n", "out = pd.DataFrame(out.values[None, :], columns= 'Sum_' + out.index)\n\n   Sum_A  Sum_B\n0     59     66\n", "out.to_frame().T.add_prefix('Sum_')\n\n   Sum_A  Sum_B\n0     59     66\n", "df = df.melt(id_vars='id')\nout = df.groupby(df['variable'].str[0])['value'].sum()\n\nvariable\nA    59\nB    66\nName: value, dtype: int64\n", "pd.DataFrame(out.values[None, :], columns='Sum_' + out.index)\n\nvariable  Sum_A  Sum_B\n0            59     66\n\n#OR\n\nout.to_frame().T.add_prefix('Sum_')\n\nvariable  Sum_A  Sum_B\nvalue        59     66\n"], "link": "https://stackoverflow.com/questions/64691856/sum-total-from-different-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that looks like this: what I'm trying to achieve is to multiply certain score values corresponding to specific products by a constant. I have the products target of this multiplication in a list like this: [1069104, 1069105] (this is just a simplified example, in reality it would be more than two products) and my goal is to obtain this: Multiply scores corresponding to products 1069104 and 1069105 by 10: I know that exists DataFrame.multiply but checking the examples it works for full columns, and I just one to change those specific values.", "q_apis": "get columns values DataFrame columns values", "apis": "loc product isin", "code": ["prod_list =  [1069104, 1069105]\ndf.loc[df['product'].isin(prod_list), 'score'] *= 10\n"], "link": "https://stackoverflow.com/questions/61346083/how-to-multiply-certain-values-of-a-column-by-a-constant"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 3 csv files I would like to read all those csv files and create one dataframe. However, I would like to read only two columns for each file and their names are bit different. I would like to create one final dataframe with two columns and (also named as in other csv files) I don't wish to read , or columns for other csv files. Basically use regex/pattern matching only to read columns while creating final dataframe I was trying something like below from SO post Can help me on how to use regex to select columns while reading csvs? I expect my final dataframe to have columns like as shown below you can see how only those 2 required columns from each csv file are selected and concatenated)", "q_apis": "get columns all columns names columns columns columns select columns columns columns", "apis": "rename columns filter filter filter columns columns columns filter rename columns columns rename columns append", "code": ["dfs = [pd.read_csv(f, sep=\";\").rename(columns={'Order Summary':'Summary'})[['person_id','Summary']]\n       for f in files] \n", "print (df.filter(regex='person_id|Summary$'))\n   person_id Summary\n0         11    Test\n1         11   Test1\n2         11   Test2\n\nprint (df1.filter(regex='person_id|Summary$'))\n  person_id Order Summary\n0         21           Tep\n1         22          Tst1\n2         51           Tt2\n\nprint (df2.filter(regex='person_id|Summary$'))\n   person_id Order Summary\n0         31           Tet\n1         31           Tt1\n2         41           Tt2\n", "print (df[df.columns.intersection(['person_id', 'Summary', 'Order Summary'])])\nprint (df1[df1.columns.intersection(['person_id', 'Summary', 'Order Summary'])])\nprint (df2[df2.columns.intersection(['person_id', 'Summary', 'Order Summary'])])\n \n        \n", "dfs = [pd.read_csv(f, sep=\";\").filter(regex='person_id|Summary$').rename(columns={'Order Summary':'Summary'})\n       for f in files] \n", "dfs = []       \nfor f in files:\n    df = pd.read_csv(f, sep=\";\")\n    df1 = df[df.columns.intersection(['person_id', 'Summary', 'Order Summary'])].rename(columns={'Order Summary':'Summary'})\n    dfs.append(df1)\n"], "link": "https://stackoverflow.com/questions/63987585/pandas-read-csv-using-wild-card-pattern-for-selecting-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a column in a dataframe with column 'url_product' that contains a list of dictionaries as below (showing first 4 rows as an example). Each dictionary contains url and product associated with that url. For each of these rows, I am looking to do the following Filter for only the dictionaries where URL contains '/product/' and parse out the number following the 'product/' (will call this product_id as easy reference). expected product_id of dictionary below = 11 For each of the dictionaries where URL contains '/product/' also count the number of 'products'. For the example below, that count would be 2 (giftcard, abcstore) For each row return the product_id that has the highest count and create a new column('top_product_id') in the dataframe to show this. If no single product_id has the highest count, leave as blank Expected outcome for the first 3 three rows after the steps above df.top_product_id a few points to explain the expected outcome Row[0]- expect 11 as product_id 11 has a count of 2 (giftcard, abcstore) while product_id 10 and 10414 only has 1 each. both the blog and article urls will be skipped as they do not contain '/product/' in the url Row[1]- expect the outcome to be blank as the two product URLs are attached to 1 product each and since there is no single url with the highest count, row would be blank Row[2]- expect 114 as product_id 114 has highest count of 2 (cellulosemask, mask) Row[3]- expect the outcome to be blank as there are no product URLs How would I create the new column ('top_product_id') in the dataframe with the expected outcome?", "q_apis": "get columns contains first contains product where contains product parse product where contains product count count count count first count product product product count count mask product", "apis": "DataFrame assign count product product dropna drop_duplicates count empty loc count idxmax apply", "code": ["def findID(data):\n    df1 = pd.DataFrame(data)\n    df1 = df1.assign(\n        count=df1['product'].str.split(', ').str.len(),\n        product_id=df1['url'].str.extract(r'.*/product/(\\d+)', expand=False)\n    ).dropna().drop_duplicates(subset=['count'], keep=False)\n\n    if df1.empty:\n        return '(Blank)'\n\n    return df1.loc[df1['count'].idxmax(), 'product_id']\n\n\ndf['top_product_id'] = df['url_product'].apply(findID)\n", "# print(df)\n                                         url_product top_product_id\n0  [{'url': 'https://www.abcstore.com/product/11-...             11\n1  [{'url': 'https://www.abcstore.com/product/7-n...        (Blank)\n2  [{'url': 'https://www.abcstore.com/product/25-...            144\n3  [{'url': 'https://www.abcstore.com/blog/top-sk...        (Blank)\n"], "link": "https://stackoverflow.com/questions/62463759/using-regex-to-parse-out-values-from-dictionaries-and-count-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Take the following multi-indexed dataframe: I need to create a to compute the difference of last for each , if this has increased on that month considering the past max value inside that , on previous months. The result should be like this: The first row with positive value on must show this value. I don't need negative max values. This is the rationale to calculate marginal values to pay some taxes.", "q_apis": "get columns difference last month max value first value value max values values", "apis": "groupby cummax ge ge loc groupby diff fillna mask bool", "code": ["c = df.groupby(level=0)['cum_value'].cummax()\nm = df['cum_value'].ge(c) & df['cum_value'].ge(0)\ndf['new_col'] = df.loc[m, 'cum_value'].groupby(level=0).diff()\ndf['new_col'] = df['new_col'].fillna(df['cum_value']).mask(~m, 0)\n", ">>> c\n\nindex_1  index_2\n0        2020-01    100.0\n         2020-02    100.0\n         2020-03    100.0\n         2020-04    150.0\n         2020-05    200.0\n1        2020-01     25.0\n         2020-02     50.0\n         2020-03     50.0\n         2020-04     50.0\n         2020-05    200.0\nName: cum_value, dtype: float64\n", ">>> m\n\nindex_1  index_2\n0        2020-01     True\n         2020-02    False\n         2020-03    False\n         2020-04     True\n         2020-05     True\n1        2020-01     True\n         2020-02     True\n         2020-03    False\n         2020-04     True\n         2020-05     True\nName: cum_value, dtype: bool\n", ">>> df.loc[m, 'cum_value']\n\nindex_1  index_2\n0        2020-01    100.0\n         2020-04    150.0\n         2020-05    200.0\n1        2020-01     25.0\n         2020-02     50.0\n         2020-04     50.0\n         2020-05    200.0\nName: cum_value, dtype: float64\n", ">>> df.loc[m, 'cum_value'].groupby(level=0).diff()\n\nindex_1  index_2\n0        2020-01      NaN\n         2020-04     50.0\n         2020-05     50.0\n1        2020-01      NaN\n         2020-02     25.0\n         2020-04      0.0\n         2020-05    150.0\nName: cum_value, dtype: float64\n", ">>> df\n                 cum_value  new_col\nindex_1 index_2                    \n0       2020-01      100.0    100.0\n        2020-02       50.0      0.0\n        2020-03      -50.0      0.0\n        2020-04      150.0     50.0\n        2020-05      200.0     50.0\n1       2020-01       25.0     25.0\n        2020-02       50.0     25.0\n        2020-03     -100.0      0.0\n        2020-04       50.0      0.0\n        2020-05      200.0    150.0\n"], "link": "https://stackoverflow.com/questions/66601219/how-to-make-a-new-column-based-on-difference-of-max-values-by-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to create a Pandas DataFrame filled with NaNs. During my research I found an answer: This code results in a DataFrame filled with NaNs of type \"object\". So they cannot be used later on for example with the method. Therefore, I created the DataFrame with this complicated code (inspired by this answer): This results in a DataFrame filled with NaN of type \"float\", so it can be used later on with . Is there a more elegant way to create the same result?", "q_apis": "get columns DataFrame DataFrame DataFrame DataFrame", "apis": "DataFrame index columns dtypes values", "code": [">>> import numpy as np\n>>> import pandas as pd\n>>> df = pd.DataFrame(np.nan, index=[0, 1, 2, 3], columns=['A', 'B'])\n\n>>> df.dtypes\nA    float64\nB    float64\ndtype: object\n\n>>> df.values\narray([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]])\n"], "link": "https://stackoverflow.com/questions/30053329/elegant-way-to-create-empty-pandas-dataframe-with-nan-of-type-float"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've got a tricky problem in pandas to solve. I was previously referred to this thread as a solution but it is not what I am looking for. Take this example dataframe with two columns: I first want to check each row in column 2 to see if that value exists in column 1. This is checking full and partial strings. I can check to see that I have a match of a partial or full string, which is good but not quite what I need. Here is what the dataframe looks like now: What I really want is the value from column 1 which the value in column 2 matched with. I have not been able to figure out how to associate them My desired result looks like this:", "q_apis": "get columns columns first value now value value", "apis": "append first append DataFrame columns compare", "code": ["def compare_cols(match_col, partial_col):\n    series = []\n    for partial_str in partial_col:\n        for match_str in match_col:\n            if partial_str in match_str:\n                series.append(match_str)\n                break  # matches to the first value found in match_col\n        else:  # for loop did not break = no match found\n                series.append(None)\n    return series\n\ndf = pd.DataFrame([['Mexico', 'Chile'], ['Nicaragua', 'Nica'], ['Colombia', 'Mex']], columns = [\"col1\", \"col2\"])\n\ndf['compare'] = compare_cols(match_col=df.col1, partial_col=df.col2)\n"], "link": "https://stackoverflow.com/questions/66369736/pandas-if-partial-string-match-exists-put-value-in-new-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have pandas dataframe which has the same column names. (column names are a,b,a,a,a) Below is example. Is there any way I can change column name only for 3rd column from the left by specifying column location? I found that there is a way to change column name by making a new list. But I wanted to see if there is any way I can specify column location and change the name. Below is what I want. Since I am new to programming, I would appreciate any of your help!", "q_apis": "get columns names names any name left name any name any", "apis": "columns values columns", "code": ["column_names = df.columns.values\ncolumn_names[2] = 'Changed'\ndf.columns = column_names\n"], "link": "https://stackoverflow.com/questions/38601014/python-i-have-pandas-dataframe-which-has-the-same-column-name-how-to-change-on"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe : that I need to split into multiple dataframes that will contain every 10 rows of , and every small dataframe I will write to separate file. so I decided create multilevel dataframe, and for this first assign the index to every 10 rows in my with this method: that throws out So, do you have an idea of how to fix it? Where is my method wrong? But if you have another approach to split my dataframe into multiple dataframes every of which contains 10 rows of , you are also welcome, cause this approach was just the first I thought about, but I'm not sure that it's the best one.", "q_apis": "get columns first assign index contains first", "apis": "DataFrame columns groupby index to_csv", "code": ["df = pd.DataFrame(data=np.random.rand(100, 3), columns=list('ABC'))\ngroups = df.groupby(np.arange(len(df.index))/10)\nfor (frameno, frame) in groups:\n    frame.to_csv(\"%s.csv\" % frameno)\n"], "link": "https://stackoverflow.com/questions/33922664/split-pandas-dataframe-into-multiple-dataframes-with-equal-numbers-of-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Consider a dataframe: Say, I want to select the 33% highest values in col1 (corresponding to 3 rows), but so that I have 1 in each of the values of col2 (A, B, C). In this case I want row index number 0, 2 and 5. I can of course solve this by iterating through the rows with a for loop and keeping track of the col2 values, but is there a faster/smarter way to do this?", "q_apis": "get columns select values values index values", "apis": "groupby index round items index groupby groupby head", "code": ["import math\nimport itertools\n\ndict1 = df.groupby(['col2'], sort=False).groups # returns the dict with groupname and index list\nindex_list  =[value[:round(len(value) * 0.33)] for key,value in dict1.items()] # taking 33% from each group\nflat_index_list = list(itertools.chain(*index_list)) # flatten list\nprint(flat_index_list) # will print [0,2,5]\n", "import math\n\nlen1 = int(len(df.index) * 0.33) #3 for 33%\n\nno_of_groups = df.groupby('col2', as_index = False).ngroups #3 groups\nk = math.ceil(len1 / no_of_groups) #how many elements to pick from each group k = 3/3 =1 here.\n\nfinal_df = df.groupby('col2', as_index = False, sort=False).head(k) #will pick top k elements from each group\n", "import math\nimport itertools\n\ndict1 = df.groupby(['col2'], sort=False).groups # returns the dict with groupname and index list\namount = [1,0,1]\nindex_list  =[value[:round(len(value) * (amount[index]/len(value)))] for index,(key,value) in enumerate(dict1.items())] # taking 33% from each group\nflat_index_list = list(itertools.chain(*index_list)) # flatten list\nprint(flat_index_list) # will print [0,5]\n"], "link": "https://stackoverflow.com/questions/67124523/selecting-top-x-of-a-column-in-a-dataframe-subject-to-another-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am attempting to interpolate between time points for all data in a pandas dataframe. My current data is in time increments of 0.04 seconds. I want it to be in increments of 0.01 seconds to match another data set. I realize I can use the DataFrame.interpolate() function to do this. However, I am stuck on how to insert 3 rows of NaN in-between every row of my dataframe in an efficient manner. I want df to transform from this: To something like this: Which I can then call on Which would yield something like this (I'm making up the numbers here): I attempted to use an iterrows technique by inserting the df_ins frame after every row. But my index was thrown off during the iteration. I also tried slicing df and concatenating the df slices and df_ins, but once again the indexes were thrown off by the loop. Does anyone have any recommendations on how to do this efficiently?", "q_apis": "get columns interpolate between time all time seconds seconds DataFrame interpolate insert between transform iterrows index any", "apis": "set_index resample ffill", "code": ["df[\"Time\"] = pd.to_timedelta(df[\"Time\"], unit=\"S\")\ndf.set_index(\"Time\").resample(\"0.01S\").ffill()\n", "                 Pulse   O2\nTime\n00:00:00            76   99\n00:00:00.010000     76   99\n00:00:00.020000     76   99\n00:00:00.030000     76   99\n00:00:00.040000     74  100\n00:00:00.050000     74  100\n00:00:00.060000     74  100\n00:00:00.070000     74  100\n00:00:00.080000     77   99\n00:00:00.090000     77   99\n00:00:00.100000     77   99\n00:00:00.110000     77   99\n00:00:00.120000     80   98\n", "                 Pulse      O2\nTime\n00:00:00         76.00   99.00\n00:00:00.010000  75.50   99.25\n00:00:00.020000  75.00   99.50\n00:00:00.030000  74.50   99.75\n00:00:00.040000  74.00  100.00\n00:00:00.050000  74.75   99.75\n00:00:00.060000  75.50   99.50\n00:00:00.070000  76.25   99.25\n00:00:00.080000  77.00   99.00\n00:00:00.090000  77.75   98.75\n00:00:00.100000  78.50   98.50\n00:00:00.110000  79.25   98.25\n00:00:00.120000  80.00   98.00\n"], "link": "https://stackoverflow.com/questions/65709568/expand-time-series-data-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this: I want to sum the row from col3 and col4 that belong to A or B and put the sum in the index 3 and 7 and then have an ouput data frame like this: Edited: Thank you @andrej it worket perfectly with the df like above, but in this one below: when I try your solution @andrej, it gives me this output: Is there a way to have the sum values only for index 3 and 7?", "q_apis": "get columns sum put sum index sum values index", "apis": "empty values replace groupby sum set_index update reset_index loc fillna groupby sum set_index update reset_index", "code": ["# if the empty values aren't NaNs already:\n# df = df.replace(\"\", np.nan) \n\nvals = df.groupby(\"col1\").sum()\ndf = df.set_index(\"col1\")\ndf.update(vals, overwrite=False)\nprint(df.reset_index())\n", "  col1   col2  col3  col4\n0    A    A_1   8.0   4.0\n1    A    A_2   2.0   7.0\n2    A    A_3   5.0   5.0\n3    A  A_sum  15.0  16.0\n4    B    B_1   9.0   8.0\n5    B    B_2   6.0   8.0\n6    B    B_3   5.0   2.0\n7    B  B_sum  20.0  18.0\n", "m = ~df.col2.str.contains(\"_sum\")\ndf.loc[m] = df[m].fillna(0)\n\nvals = df.groupby(\"col1\").sum()\ndf = df.set_index(\"col1\")\ndf.update(vals, overwrite=False)\nprint(df.reset_index())\n", "  col1   col2  col3  col4\n0    A    A_1   8.0   4.0\n1    A    A_2   0.0   7.0\n2    A    A_3   0.0   5.0\n3    A  A_sum   8.0  16.0\n4    B    B_1   9.0   8.0\n5    B    B_2   6.0   8.0\n6    B    B_3   5.0   0.0\n7    B  B_sum  20.0  16.0\n"], "link": "https://stackoverflow.com/questions/67493014/sum-dataframe-rows-and-put-the-result-in-the-next-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've two pandas data frames which have some rows in common. I want to identify the rows of df1 which are not in df2 (based on a condition like where df1.x = df2.x) and delete them from df1. Also keeping everything unchanged in df2.", "q_apis": "get columns where delete", "apis": "merge where drop", "code": ["df = pd.merge(df1, df2, how='left', indicator='Exist')\ndf['Exist'] = np.where(df.Exist == 'both', True, False)\ndf = df[df['Exist']==True].drop(['Exist','z'], axis=1)\n"], "link": "https://stackoverflow.com/questions/61954359/pandas-delete-rows-in-a-dataframe-that-are-not-in-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "After training a Keras model and using the in the function, how can we transform that 20% of the old dataframe into a new dataframe for testing?", "q_apis": "get columns transform", "apis": "iloc iloc", "code": ["idx = int(len(train_X) * 0.2)  # 0.2 is the value of validation split\n# if train_X and train_y are numpy arrays\nval_X = train_X[idx:]   \nval_y = train_y[idx:]\n\n# if train_X and train_y are pandas dataframes\nval_X = train_X.iloc[idx:]\nval_y = train_y.iloc[idx:]\n"], "link": "https://stackoverflow.com/questions/54027609/how-to-create-a-new-dataframe-after-the-validation-split"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have a count of candies I sold everyday since 2010. For each year (2010, 2011,2012...2019), how can I find the date when I sold the maximum candies using pandas? I've tried this and it gives me the max by year, but I want the date and count for each year. Thanks for the help!", "q_apis": "get columns count year date max year date count year", "apis": "groupby idxmax loc", "code": ["idx = df.groupby(lambda x: df['date'][x].year)[\"Count\"].idxmax()\n\nout = df.loc[idx]\n"], "link": "https://stackoverflow.com/questions/64865421/find-max-by-year-and-return-date-on-which-max-occurred-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset where I must read in the data as a 2d array using which creates a 2d array. I then use to remove the first two rows of the 2d array (my dataset only requires data from the 3rd row and below). I then assign the 2d array to a Pandas Dataframe using Now I am trying to split \"dataframe_1\" into 2 dataframes by column. I have 8 columns and I want 2 dataframes with 4 columns each. The issue arises due to the column names being A_first, A_second, A_third, A_fourth, A_first, A_second, A_third, A_fourth. I cannot use the pandas dataframe copy() function because there are duplicate column names. also does not work from what I understand because that requires the csv to be read dataframe from the beginning, but I created a dataframe by setting a 2d array. Any ideas on what to do?", "q_apis": "get columns where array array first array assign array columns columns names copy names array", "apis": "iloc iloc", "code": ["df_first_four_cols = dataframe_1.iloc[:,0:4]\ndf_second_four_cols = dataframe_1.iloc[:,4:]\n\n"], "link": "https://stackoverflow.com/questions/63654055/splitting-pandas-dataframe-by-column-when-duplicate-column-names-exist"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to build a Pandas DataFrame based on repeated API calls. I have filtered the JSON response object (which contains more data than I need) into several dicts. When I merge the dicts (using ) the order of keys is preserved. However, when I create a blank dataframe and use the append argument, the resulting dataframe sorts the column names (formerly dict keys) alphabetically even when the sort parameter is false. I would like to preserve the original order Here is an example row of the dataframe I am hoping to create: calling combo_dict returns: Which is the order I want these keys to remain in. However, calling returns a dataframe with all keys sorted in alphabetical order (blacklist_detections, continent, country, detection_rate, etc.) I am unsure whether the way I am adding each row to the dataframe or the way that I am merging the dicts is responsible for this behavior, but I would like it to remain unsorted. I do not understand why sort=False is not doing anything for me. Any help would be greatly appreciated!", "q_apis": "get columns DataFrame contains merge keys append names keys keys all keys", "apis": "get columns DataFrame columns empty columns append append", "code": ["combo_cols = list(combo_dict) # get the current order of the columns\ndf = pd.DataFrame(columns=combo_cols) # set empty df columns to match \ndf.append(combo_dict, ignore_index = True, sort = False) # append the dataframes\n"], "link": "https://stackoverflow.com/questions/57877791/how-to-get-pandas-to-stop-automatically-sorting-column-titles-in-new-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My data: It looks like: (The data must be in multi-index form) At first I tried the following calculation and it worked out perfectly: Then, for each city, mall, and category, I wanted to compare the price difference with egg. I wrote: This is when I got the error message: Why is this happening and what should I do? Expected outcome:", "q_apis": "get columns index first compare difference", "apis": "sub where index groupby transform first", "code": ["df['Price_diff'] = df['Price'].sub(df['Price'].where(df.index.get_level_values(2) == 'Egg').groupby(level=[0,1]).transform('first'))\ndf\n", "                    Price  Price_diff\nCity Mall Category                   \nNY   A    Milk          5        -5.0\n          Egg          10         0.0\nLA   B    Egg           4         0.0\n          Beef          9         5.0\nMIA  C    Egg           6         0.0\n          Orange       11         5.0\n"], "link": "https://stackoverflow.com/questions/65435671/calculating-in-dataframe-why-am-i-getting-keyerror-here"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two different dfs with the following columns: I want to find what percentage of values of present in For that, I thought I could construct a new df which will contain the same values and then its to . First I want to get this: Here is what I tried, but I dont want to use two loops", "q_apis": "get columns columns values values get", "apis": "DataFrame columns DataFrame columns", "code": [">>> df1 = pd.DataFrame([\"programming\", \"chess\", \"leadership\"], columns=[\"col1\"])\n>>> df2 = pd.DataFrame([\"programming\", \"python\", \"leadership\", \"abba\", \"games\"], columns=[\"col2\"])\n"], "link": "https://stackoverflow.com/questions/52343206/find-same-values-from-2-df-columns-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Using Pandas 1.0, I am trying to write an efficient program to calculate the running maximum for each observation of a given item in my dataset (each item is identified by the same ID). My program does the job at an exceedingly slow pace, due to the fact that I am using iterrows() and setting each high-water mark via the index. Having a very large dataset, this is not a viable solution. Output: Any suggestions about how to speed up this process?", "q_apis": "get columns item item at iterrows index", "apis": "values", "code": ["df['running_maximum'] = df.groupby('id').val.cummax()\n\nprint(df)\n\n    id  val  running_maximum\n0    1   10               10\n1    1   15               15\n2    1   10               15\n3    1    0               15\n4    1    5               15\n5    1   20               20\n6    1    0               20\n7    1   10               20\n8    2    5                5\n9    2   15               15\n10   2   10               15\n11   2   20               20\n12   2   25               25\n13   2   20               25\n14   2   30               30\n15   2   10               30\n", "df['running_maximum'] = np.maximum.accumulate(df.val.values.reshape(-1, 8), 1).ravel()\n"], "link": "https://stackoverflow.com/questions/60203634/how-to-efficiently-calculate-running-maxima-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe (for example) and wish to run If I want to define a colour for the bar whenever the index string contains and other colours for other samples, how would I do this?", "q_apis": "get columns index contains", "apis": "join keys index get fillna plot items loc index plot fillna", "code": ["colors = {'Sample_A': 'r', 'Sample_B': 'b', 'Sample_D': 'g'}\npat = (r'({})'.format('|'.join(colors.keys())))\nc = df.index.str.extract(pat,expand=False).map(colors.get).fillna('y')\n\ndf['Value'].plot(kind='bar', color=c)\n", "for k, v in colors.items():\n    df.loc[df.index.str.contains(k), 'color'] = v\nprint (df)\n            Value color\nSample_A_1      5     r\nSample_B        1     b\nSample_A_2      2     r\nSample_D        3     g\nSample_F        4   NaN\n\ndf['Value'].plot(kind='bar', color=df['color'].fillna('y'))\n"], "link": "https://stackoverflow.com/questions/54420479/defining-colour-if-index-contains-certain-string"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to convert a 8760x2 pandas DataFrame which has the following data: into a 365x25 DataFrame: I already made this: I get NaNs instead of Numbers. The date should also be checked, because sometimes I have the problem of a leap-year and time change (summertime, wintertime)... What is the best method to do this? Thanks in advance!", "q_apis": "get columns DataFrame DataFrame get date year time", "apis": "pivot index columns values", "code": ["df['Datetime'] = pd.to_datetime(df['Datetime'])\ndf['Date'] = df['Datetime'].dt.date\ndf['Hour'] = df['Datetime'].dt.hour\n\ndf.pivot(index='Date', columns='Hour', values='Value')\n"], "link": "https://stackoverflow.com/questions/57977164/how-do-i-convert-a-8760x2-dataframe-to-a-365x25-dataframe-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Is it possible to return the entirety of data not just part of which we are grouping by? I mean for example - I have a dataframe with 5 columns and one of those columns contains , the other one is and the last important one is . I grouped dataframe by - agg function I applied is . As a return i get correctly grouped dataframe with timestamp and distance - how can i add columns there. If I group it by as well then becomes duplicated - it has to stay unique. As a final result I need to get dataframe like this: timestamp name distance 2020-03-03 15:30:235 Billy 123 2020-03-03 15:30:435 Johny 111 But instead i get this: timestamp distance 2020-03-03 15:30:235 123 2020-03-03 15:30:435 111 Whole table has more than 700k rows so joining it back on gives me that amount of rows which my PC can't even handle. Here is my which gives me 2nd table: Here is what i tried to do in order to get inside the table:", "q_apis": "get columns mean columns columns contains last agg get timestamp add columns duplicated unique get timestamp name get timestamp get", "apis": "groupby transform min", "code": ["m = df.groupby('timestamp')['distance'].transform('min')\ndout = df[df.distance==m]\n"], "link": "https://stackoverflow.com/questions/67959557/group-by-to-return-entirety-of-the-data-not-just-what-i-am-grouping-by"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the next problem. I have a list with string values: And I have the dataframe with values: Please advice me on how to create a specific, aggregated dataframe with column values from list and with sum of the impressions and clicks columns if the word from list is met in keyword column. I've tried to iterate through dataframe with iterrows() method but it does not work for this situation.", "q_apis": "get columns values values values sum columns iterrows", "apis": "append sum values sum values DataFrame from_records columns", "code": ["b = []\nfor i in a:\n  b.append((a, checking_data[checking_data['keywords'].str.contains(a)][['impressions', 'clicks']].sum().values[0], \n               checking_data[checking_data['keywords'].str.contains(a)][['impressions', 'clicks']].sum().values[1]))\n\n\ngroupedOne_df = pd.DataFrame.from_records(b, columns = ['keywords', 'impressions', 'clicks'])\n\n"], "link": "https://stackoverflow.com/questions/62835143/aggregate-dataframe-base-on-list-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "A bit of pickle. I am trying to find a single row in a dataframe by searching for a specific string value, then replace the string value with zero. I am using the following code: where ID is some string. This returns one row within the dataframe as expected. However, I can't seem to modify the values of the selectRow in dataframe, as the selectRow is a separate dataframe at this point. I know I am missing something incredibly basic. Any help would be much appreciated! CN", "q_apis": "get columns value replace value where values at", "apis": "iloc loc iloc DataFrame columns", "code": ["ID = `eee`\ndf.iloc[:,0].loc[df.iloc[:,0]==ID] = 0\n", "        C1   C2\n0      aaa  bbb\n1      ccc  ddd\n2        0  fff\n3  ggg eee  hhh\n", "        C1   C2\n0      aaa  bbb\n1      ccc  ddd\n2        0  fff\n3  ggg eee  hhh\n", "        C1   C2\n0      aaa  bbb\n1      ccc  ddd\n2        0  fff\n3        0  hhh\n", "df = pd.DataFrame([['aaa','bbb'],['ccc','ddd'],['eee','fff'],['ggg eee','hhh']], columns=['C1','C2'])\nprint(df)\n", "        C1   C2\n0      aaa  bbb\n1      ccc  ddd\n2      eee  fff\n3  ggg eee  hhh\n"], "link": "https://stackoverflow.com/questions/58381980/pandas-search-for-dataframe-row-by-string-then-modify-without-header"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am very new to DataScience/Pandas in general. I mainly followed this and could get it to work using different classifiers. The \"helpers\" are functions I don't quite understand fully, but they work: I would like to have a \"manual\" test, such as entering x attributes and getting a prediction based on that. So for example, I hardcode a DataFrame like the following: How would I apply the same encoding? My code says but the manual ofc does not have a ? Please bear in mind I am a complete beginner, I searched the web a l ot, but I cannot come up with a proper source/tutorial that lets me test a single set. The full code can be found here.", "q_apis": "get columns get test DataFrame apply test", "apis": "rename columns rename le apply le apply all columns drop apply le", "code": ["import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\ndata = pd.read_csv('agaricus-lepiota.data.txt', header=None) #read data\ndata.rename(columns={0: 'y'}, inplace = True) #rename predict column (edible or not)\n\nle = LabelEncoder() # encoder to do label encoder\n\ndata = data.apply(lambda x: le.fit_transform(x)) #apply LE to all columns\n\nX = data.drop('y', 1) # X without predict column\ny = data['y'] #predict column\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nclf = GradientBoostingClassifier()#you can pass arguments\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test) #it is predict for objects in test\n\nprint(accuracy_score(y_test, y_pred)) #check accuracy\n", "manual = manual.apply(lambda x: le.fit_transform(x))\nclf.predict(manual)\n"], "link": "https://stackoverflow.com/questions/52343540/make-prediction-from-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How can I save a list that contains data-frame as every element in python? Also, I need this continuously (appending the list), so in every iteration of my loop, I add an element to the list. This new element contains a data-frame. This is running all day, and the size of the file is large, so It's important to do this efficiently (the size of final file should be small, and the process should be fast).", "q_apis": "get columns contains add contains all day size size", "apis": "to_csv", "code": ["dfs = [df1, df2, df3, df4] #here dfs is a list containing different dataframes\n\nfor i, df in enumerate(dfs):\n    df.to_csv(f'df{i}.csv')\n"], "link": "https://stackoverflow.com/questions/54906062/how-to-save-a-list-as-a-file-in-python-continuously-every-element-contains-a-d"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I have this dataframe(displayed only a part of it) name CEMS emit consent Ht CEMS-4 61 50 Ht CEMS-5 33.75 50 Ld CEMS-1 21.625 100 Sh CEMS-3 71.4 100 Now, what I have to do is find the mean of emit and then subtract it from consent of a particular CEMS What I am doing is mod = df.consent.iloc[0] eMean = df['emit'].mean() eMean = (\"%.2f\" % eMean) diff1 = eMean - mod diff = float(diff1)/float(mod) and I am getting this error diff1 = eMean - mod TypeError: ufunc 'subtract' did not contain a loop with signature matching types dtype('S21') dtype('S21') dtype('S21') Help me out in this please", "q_apis": "get columns name mean mod iloc mean mod diff mod mod dtype dtype dtype", "apis": "mod iloc astype mean mod diff mod", "code": ["eMean = (\"%.2f\" % eMean)", "mod = float(df.consent.iloc[0])\neMean = df['emit'].astype(float).mean()\nprint (\"%.2f\" % eMean)\ndiff1 = eMean - mod\ndiff = diff1 / mod\n"], "link": "https://stackoverflow.com/questions/52252477/pandas-find-a-min-value-in-a-specific-column-in-several-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames like as shown below What I would like to do is find the number of which are present in but not present in the . Please note that there can be duplicates in the but not in Basically I have to identify them as they are not valid (because they are missing in the parent dataframe which is ) I tried the below but it is for matching entries. How can I elegantly do it for entries because I have several millions of data (At least 10 million records and can go up to 15 million). I expect my output to be like as shown below", "q_apis": "get columns", "apis": "isin value_counts", "code": ["source_values = set(source_df.source_value.to_list())\nclient_df.source_value[lambda x: ~x.isin(source_values)].value_counts()\n\n#29SB    2\n#25FG    2\n#35DE    1\n#31DE    1\n#Name: source_value, dtype: int64\n"], "link": "https://stackoverflow.com/questions/66415293/how-to-merge-and-get-the-count-of-non-matching-entries-efficiently"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The funky way that you index into pandas dataframes to change values is difficult for me. I can never figure out if I'm changing the value of a dataframe element, or if I'm changing a copy of that value. I'm also new to python's syntax for operating on arrays, and struggle to turn loops over indexes (like in C++) into vector operations in python. My problem is that I wish to add a column of values to a dataframe based on values in other columns. Lets say I start with a dataframe like which returns I want to find the lesser of the dates BEFORE and AFTER and then make a new column called RELEVANT_DATE with the results. I can then drop BEFORE and AFTER. There are a zillion ways to do this but, for me, almost all of them don't work. The best I can do is this returning This approach is super slow. With a few million rows it takes too long to be useful. Can you provide a pythonic-style solution for this? Recall that I'm having trouble both with vectorizing these operations AND making sure they get set for real in the DataFrame.", "q_apis": "get columns index values value copy value add values values columns start drop all style get DataFrame", "apis": "min", "code": ["df.set_index('ID').min(axis=1).rename('RELEVANT DATE').reset_index()\n\n   ID RELEVANT DATE\n0  11    2015-02-24\n1  22    2015-02-27\n2  33    2015-02-25\n3  44    2015-02-26\n4  66    2015-01-24\n5  77    2015-02-25\n", "df['RELEVANT DATE'] = df[['BEFORE', 'AFTER']].min(1)\n"], "link": "https://stackoverflow.com/questions/61236542/pythonic-way-to-add-timestamps-to-pandas-dataframe-based-on-other-timestamps"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose there are two dataframes that share the same index but have different columns. Is it smarter to merge the two dataframes here or concat? This link inspired this question. Typically I have always used , but I am curious to what others use or think.", "q_apis": "get columns index columns merge concat", "apis": "merge", "code": ["df3_merge = df1.merge(df2,left_index = True, right_index = True)  \n", "df3_concat = pd.concat([df1,df2], axis = 1)\n"], "link": "https://stackoverflow.com/questions/60827293/when-is-it-smart-to-merge-vs-concat-two-pandas-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 3 dataframes. I need to convert them in one merged CSV separated by pipes '|'. And I need to sort them by Column1 after append. But, when I try to convert the final df to CSV, there comes exceeded pipes for null columns. How to avoid this? This is the output I have (note pipes'|'): I need this. Justo to introduce, I'll not work on this final data, I need to upload it to a specific database in the exact format I show below, but I need this without using regex (note pipes'|'). Is there a way to do so?", "q_apis": "get columns append columns", "apis": "join pipe sort_values columns to_csv index", "code": ["\"\\n\".join([l.rstrip(\"|\") for l in \n           pd.concat([df1,df2,df3]).pipe(lambda d: \n                                         d.sort_values(d.columns.tolist())).to_csv(sep=\"|\", index=False).split(\"\\n\")])\n"], "link": "https://stackoverflow.com/questions/67322247/remove-excess-of-pipes-in-csv-after-append-files"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to change this example to x-axis with dates to understand bokeh concepts. https://docs.bokeh.org/en/latest/docs/gallery/brewer.html So with above example I get x-axis and I can zoom in. How do I change this to date. Can I do something like , just traverse over dates list. I tried with commented code but it doesn't update minor lables and I can't zoom in. Does dates has to be in df` is And if has to be in I can add like below But still not sure how to plot these dates on x-axis.", "q_apis": "get columns get date update add plot", "apis": "DataFrame columns cumsum shift fillna shape shape values", "code": ["import pandas as pd\nimport numpy as np\nimport bokeh\nimport bokeh.plotting\n\nN = 4\ncats = 3\ndata = [[2,3,4], [2,2,3], [0,0,0], [1,2,3]]\ndf = pd.DataFrame(data, columns=['y0', 'y1', 'y2'])\n\nprint df\ndates = ['2016-06-01','2016-06-02','2016-06-03','2016-06-04']\ndt = [pd.datetime.strptime(x,'%Y-%m-%d') for x in dates]\ndef stacked(df):\n    df_top = df.cumsum(axis=1)\n    df_bottom = df_top.shift(axis=1).fillna({'y0': 0})[::-1]\n    df_stack = pd.concat([df_bottom, df_top], ignore_index=True)\n    return df_stack\n\nareas = stacked(df)\ncolors = bokeh.palettes.brewer['Spectral'][areas.shape[1]]\n\nx2 = np.hstack((dt[::-1], dt))\np = bokeh.plotting.figure(x_axis_type='datetime', y_range=(0, 10))\n\np.xaxis.formatter = bokeh.models.formatters.DatetimeTickFormatter(\n    days=[\"%Y-%m-%d\"])\n\np.grid.minor_grid_line_color = '#eeeeee'\n\np.patches([x2] * areas.shape[1], [areas[c].values for c in areas],\n          color=colors, alpha=0.8, line_color=None)\nbokeh.io.output_file('brewer.html', title='brewer.py example')\n\nbokeh.io.show(p)\n"], "link": "https://stackoverflow.com/questions/50689985/brewer-py-convert-x-axis-to-dates"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe containing people's order of the t-shirt. is the column name that contains each customer's order. I want to count how many times , , etc. appear for different design and sizes and then graph them on a bar chart. I tried groupby and count but it's only based on same item in each role, it doesn't count the individual word.", "q_apis": "get columns name contains count groupby count item count", "apis": "explode value_counts plot sum plot", "code": ["orders = df['Order'].str.split(', ').explode().value_counts()\n\norders.plot.bar()\n", "orders = df['Order'].str.get_dummies(', ').sum()\n\norders.plot.bar()\n"], "link": "https://stackoverflow.com/questions/65351256/counting-repeated-word-in-dataframe-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to locate duplicate rows in my dataframe. In reality, is , but I am using this toy example below for an MRE What I am trying to accomplish is observing a subset of the features, and if there are duplicate rows, to keep the first and then denote which pair is the duplicate. I have looked at the following posts: find duplicate rows in a pandas dataframe (I could not figure out how to replace in with my list of cols) Find all duplicate rows in a pandas dataframe I know has a call. So I tried implementing that and it sort of works: However, what I am trying to do is for a particular row, determine which rows are duplicates of it by saving those rows as combination. So while I'm able to extract the and for each duplicate, I have no ability to map it back to the original row for which it is a duplicate. An ideal dataset would look like: How can I take my duplicated values and map them back to their originals efficiently (understanding the size of my actual dataset)?", "q_apis": "get columns first at replace all map take duplicated values map size", "apis": "columns columns mask first duplicated aggregate assign apply groupby agg add join rename values first empty fillna mask columns duplicated duplicated loc index duplicated groupby apply duplicated duplicated where", "code": ["# Declare columns I am interested in\ncols = ['ft1', 'ft2', 'ft4', 'ft5']\n\n# Create a subset of my dataframe with only the columns I care about\nsub_df = df[cols]\n\n#mask for first dupes\nm = sub_df.duplicated()\n#create tuples, aggregate to list of tuples\ns = (df.assign(a = df[['id','label']].apply(tuple, 1))[m]\n       .groupby(cols)['a']\n       .agg(lambda x: dict(list(x))))\n\n#add new column\ndf = df.join(s.rename('duplicates'), on=cols)\n#repalce missing values and not first duplciates to empty strings\ndf['duplicates'] = df['duplicates'].fillna('').mask(m, '')\n", "print (df)\n\n        id  ft1  ft2  ft3  ft4  ft5   label  \\\n0   id_100    1    1   43    1    1    High   \n1   id_101    1    1   33    0    1  Medium   \n2   id_102    1    1   12    1    1     Low   \n3   id_103    1    1   46    1    0     Low   \n4   id_104    1    1   10    1    1    High   \n5   id_105    0    1   99    0    1     Low   \n6   id_106    0    0    0    0    0    High   \n7   id_107    1    1    6    0    1    High   \n8   id_108    1    1   29    1    1  Medium   \n9   id_109    1    0   27    0    0  Medium   \n10  id_110    0    1   32    0    1    High   \n\n                                           duplicates  \n0   {'id_102': 'Low', 'id_104': 'High', 'id_108': ...  \n1                                  {'id_107': 'High'}  \n2                                                      \n3                                                      \n4                                                      \n5                                  {'id_110': 'High'}  \n6                                                      \n7                                                      \n8                                                      \n9                                                      \n10                   \n", "# Declare columns I am interested in\ncols = ['ft1', 'ft2', 'ft4', 'ft5']\n\nm = ~df.duplicated(subset=cols)  & df.duplicated(subset=cols, keep=False)\n\ndef f(x):\n    x.loc[x.index[0], 'duplicated'] = [dict(x[['id','label']].to_numpy()[1:])]\n    return x\n\ndf = df.groupby(cols).apply(f)\ndf['duplicated'] = df['duplicated'].where(m, '')\n", "print (df)\n        id  ft1  ft2  ft3  ft4  ft5   label  \\\n0   id_100    1    1   43    1    1    High   \n1   id_101    1    1   33    0    1  Medium   \n2   id_102    1    1   12    1    1     Low   \n3   id_103    1    1   46    1    0     Low   \n4   id_104    1    1   10    1    1    High   \n5   id_105    0    1   99    0    1     Low   \n6   id_106    0    0    0    0    0    High   \n7   id_107    1    1    6    0    1    High   \n8   id_108    1    1   29    1    1  Medium   \n9   id_109    1    0   27    0    0  Medium   \n10  id_110    0    1   32    0    1    High   \n\n                                           duplicated  \n0   {'id_102': 'Low', 'id_104': 'High', 'id_108': ...  \n1                                  {'id_107': 'High'}  \n2                                                      \n3                                                      \n4                                                      \n5                                  {'id_110': 'High'}  \n6                                                      \n7                                                      \n8                                                      \n9                                                      \n10                                                     \n"], "link": "https://stackoverflow.com/questions/65109613/mapping-duplicate-rows-to-originals-with-dictionary-python-3-6"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a large dataframe that has two columns but with many rows, so this is just an example: and I also have three other dataframes that are of different sizes but they all have some rows from the text column in df1: what I would like to do is to merge the 'second','third' and 'fourth' column to df1 in a way that they would populate the empty column 'goal' in df1, I tried left merge multiple times for each dataframe but the output will appear in a different column. Is there a way of doing it at once and add them to the goal column? thank you", "q_apis": "get columns columns all merge second empty left merge at add", "apis": "set_index iloc", "code": ["m = pd.concat([df.set_index('text').iloc[:, 0] for df in (df2, df3, df4)])\ndf1['goal'] = df1['text'].map(m)\n", "# print(df1)\n                                  text     goal\n0             see you in five minutes.      num\n1                    she is my friend.   friend\n2  she goes to school in five minutes.      num \n3                     he is my friend.   friend\n4                       that is right.  correct\n5                         sky is blue.    color\n6                       sky is yellow.    color\n"], "link": "https://stackoverflow.com/questions/62835962/pandas-populate-an-empty-column-in-a-dataframe-with-values-from-multiple-datafr"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to apply different background colours to a column on a DataFrame according to the value found on another, equal length list. My data (this is a toy example) has the following structure: I am working on a test automation framework. At some point I need to read the values (the balance column) from a website and compare it to the values that I read from an excel. After I do so I append a True, or a False into a list. Thus if the first two read values are equal to the data on my spreadsheet but the third is wrong, my list would have this look: I have found how to apply an style to a row via this command: My problem is that I do not know how to iterate over the rows as well as the list with the booleans, on the line of code above applies the same condition to all the rows. I can provide further explanations if necessary.", "q_apis": "get columns apply DataFrame value length test values compare values append first values apply style all", "apis": "empty DataFrame DataFrame index index columns columns where style apply", "code": ["checkList =  [True, True, False]\n\ndef highlight(x):\n    c1 = 'background-color: red'\n    c2 = 'background-color: green' \n\n    #if necessary pass condition\n    #checkList = x['Balance'] < 300\n    #empty DataFrame of styles\n    df1 = pd.DataFrame('', index=x.index, columns=x.columns)\n    #set Balance column by condition in list (necessary same length like df)\n    df1['Balance'] = np.where(checkList, c1, c2)\n    return df1\n\n\ndf.style.apply(highlight, axis=None)\n"], "link": "https://stackoverflow.com/questions/61230022/how-to-apply-different-styles-with-a-condition-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like as shown below As you can see in the sample dataframe, the is the same even though and dates are different for the person. For example, , has logged in and out at 4 different timestamps. but he has got the same login_ids which is incorrect. Instead, I would like to generate a column where each person gets a new login_id but retains the information in their subsequent logins. So, we can know its a sequence I tried the below but it doesn't work well I expect my output to be like as shown below. You can see how and , the 1st login_id for each person is retained in their subsequent . We just add a sequence by adding and plus one based on number of rows. Please note I would like to apply this on a big data and the may not just be in real data. For ex, 1st login_id could even be etc kind of random number. In that case, the subsequent login id will be . Hope this helps. Whatever is the 1st for that subject, add , etc based on the number of rows that person has. Hope this helps", "q_apis": "get columns sample at where add apply add", "apis": "groupby apply min index", "code": ["cumcount = df.groupby(['person_id','login_id']).login_id.cumcount()\ndf.login_id = df.groupby(['person_id','login_id']).login_id.transform(\n    lambda x: x.shift().mul(100000).fillna(x.min())\n).add(cumcount)\n\n    person_id           login_date          logout_date  login_id\n# 0       101  2013-05-07 09:27:00  2013-05-12 09:27:00         1\n# 1       101  2013-09-08 11:21:00  2013-09-13 11:21:00    100001\n# 2       101  2014-06-06 08:00:00  2014-06-11 08:00:00    100002\n# 3       101  2014-06-06 05:00:00  2014-06-11 05:00:00    100003\n# 4       202  2011-12-11 10:00:00  2011-12-16 10:00:00         8\n# 5       202  2012-10-13 00:00:00  2012-10-18 00:00:00    800001\n# 6       202  2012-12-13 11:45:00  2012-12-18 11:45:00    800002\n", "cumcount = df.groupby(['person_id','login_id']).login_id.cumcount()\ndf.login_id = df.login_id.mul(100000).add(cumcount)\n\n#   person_id           login_date          logout_date  login_id\n# 0       101  2013-05-07 09:27:00  2013-05-12 09:27:00    100000\n# 1       101  2013-09-08 11:21:00  2013-09-13 11:21:00    100001\n# 2       101  2014-06-06 08:00:00  2014-06-11 08:00:00    100002\n# 3       101  2014-06-06 05:00:00  2014-06-11 05:00:00    100003\n# 4       202  2011-12-11 10:00:00  2011-12-16 10:00:00    800000\n# 5       202  2012-10-13 00:00:00  2012-10-18 00:00:00    800001\n# 6       202  2012-12-13 11:45:00  2012-12-18 11:45:00    800002\n", "df.login_id = df.groupby(['person_id','login_id']).login_id.apply(\n    lambda x: pd.Series([x.min()*100000+seq for seq in range(len(x))], x.index)\n)\n"], "link": "https://stackoverflow.com/questions/67042817/append-sequence-number-with-padded-zeroes-to-a-series-using-padas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So far I have exported the link to my notebook are parsed the phrase using beautiful soup: Then I tried to basically make a table that's only containing revenue (Telsa Quarterly Revenue) here (trying to omit Nan values): Then when I tried to print out the tail of the table, I just get this: | Date | Revenue | (only the headers) I think I might done something wrong when I made my table, but I can't be sure. Any help would be appreciated.", "q_apis": "get columns values tail get", "apis": "get attrs DataFrame columns replace replace append apply dropna", "code": ["import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nresponse = requests.get('https://www.macrotrends.net/stocks/charts/TSLA/tesla/revenue')\nsoup = BeautifulSoup(response.text, 'lxml')\n\nall_tables = soup.find_all('table', attrs={'class': 'historical_data_table table'})\n\ntesla_revenue = pd.DataFrame(columns=[\"Date\", \"Revenue\"])\n\nfor table in all_tables:\n    if table.find('th').getText().startswith(\"Tesla Quarterly Revenue\"):\n        for row in table.find_all(\"tr\"):\n            col = row.find_all(\"td\")  \n            if len(col) == 2: \n                date = col[0].text\n                revenue = col[1].text.replace('$', '').replace(',', '')\n                tesla_revenue = tesla_revenue.append({\"Date\": date, \"Revenue\": revenue}, ignore_index=True)\n\n#tesla_revenue = tesla_revenue.apply(pd.to_numeric, errors='coerce')\n#tesla_revenue = tesla_revenue.dropna()\n\nprint(tesla_revenue)\n"], "link": "https://stackoverflow.com/questions/66427465/trying-to-extract-a-table-from-webpage-using-beautifulsoup-table-inconsistent-w"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've run across some legacy code with data stored as a single-row . My intuition would be that working with a would be faster in this case - I don't know how they do optimization, but I know that they can and do so. Is my intuition correct? Or is there no significant difference for most actions? (to clarify - obviously the best practice would not be a single row DataFrame, but I'm asking about performance)", "q_apis": "get columns difference DataFrame", "apis": "DataFrame sum sum mean std mean std DataFrame sum sum mean std mean std", "code": ["s = pd.Series(np.random.randn(100))\ndf = pd.DataFrame(np.random.randn(1,100))\n%timeit s.sum()\n%timeit df.sum(axis=1)\n\n\n104 \u00b5s \u00b1 5.08 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n194 \u00b5s \u00b1 2.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n", "s = pd.Series(np.random.randn(10000))\ndf = pd.DataFrame(np.random.randn(1,10000))\n%timeit s.sum()\n%timeit df.sum(axis=1)\n\n149 \u00b5s \u00b1 10.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n253 \u00b5s \u00b1 36.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/56473593/is-a-dataframe-with-one-row-much-slower-to-work-with-than-a-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this dataframe. what i want is this: 1) find mean of every 'id'. 2) give the number of ids (length) which has mean >= 3. 3) give back all rows of dataframe (where mean of any id >= 3.", "q_apis": "get columns mean length mean all where mean any", "apis": "groupby filter mean", "code": ["df = df[df.groupby('id')['rate'].transform('mean') >=3]\nprint (df)\n  name  id  rate\n0    A   1   3.5\n1    D   1   4.5\n2    M   1   2.0\n3    T   2   5.0\n4    B   2   4.0\n9    L   5   5.0\n", "print (df.groupby('id')['rate'].transform('mean'))\n0    3.333333\n1    3.333333\n2    3.333333\n3    4.500000\n4    4.500000\n5    1.625000\n6    1.625000\n7    1.625000\n8    1.625000\n9    5.000000\nName: rate, dtype: float64\n", "df = df.groupby('id').filter(lambda x: x['rate'].mean() >=3)\n"], "link": "https://stackoverflow.com/questions/59191701/finding-mean-of-specific-column-and-keep-all-rows-that-have-specific-mean-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm working on a code that reads in 2 CSVs as dataframes (they share a common column) and merges them through comparison of that column. One CSV has about 94,000 rows, and the other has about 40,000 (there are duplicate rows of the common column in the larger dataset, and it is important that I keep these duplicates). Both datasets have the same column name on which they should merge, but I think that currently the merge is failing because one dataset has added characters that don't allow for comparison between the shared column. I've tried many different things to extract these characters, but nothing has worked so far. When I get the resultant merge dataframe, it's entirely empty. Assume I have two dataset likes this: Dataset 1: Note: Dataset 2's ID column is actually made up of strings, but the apostrophes do not appear in printing, and I wanted to illustrate this here. Dataset 2: How do I strip Dataset 1 of the apostrophes? Things I have tried so far are: 1) Converting shared column to strings (yields result above) import pandas as pd 2) Stripping the strings in df1 of the ' 3) Adding apostrophes to df2 4) Moving df1 to a Google sheets and using Power Tools to remove ' from this column (This worked, but I can't do it for my larger datasets) None of the code I tried was able to eliminate the apostrophes. I then merge like this: However, when I do this, I always get an empty dataset (as if the code couldn't find any common values), but with the correct column headers. What can I do to strip the apostrophes from these values?", "q_apis": "get columns name merge merge between get merge empty merge get empty any values values", "apis": "DataFrame replace", "code": ["data = {'A':[4, 4, 4], 'B': ['foo', 'bar', 'foo'], 'ID':[\"'111'\", \"'222'\", \"'333'\"]}\n# note I added apostrophes into the elements in ID\n\ndf1 = pd.DataFrame(data)\nfor x in df1.ID:\n    x = x.replace(\"'\", \"\")\n    x = int(x)\n    print (x) # x is now an int\n"], "link": "https://stackoverflow.com/questions/56529761/unable-to-extract-and-merge-two-datasets-sharing-a-common-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My training data: Desired output: All False conditions have been deleted and column 'b' has been added with the amended values. How can I get this desired output? I am aware of using with", "q_apis": "get columns values get", "apis": "groupby cumsum agg first sum first", "code": ["out = training_data.groupby(training_data['condition'].cumsum()).agg({'a':'first','b':'sum','condition':'first'})\nOut[271]: \n               a  b  condition\ncondition                     \n1          401.0  2       True\n2          410.0  2       True\n3          420.0  2       True\n4          425.0  8       True\n"], "link": "https://stackoverflow.com/questions/65439688/pandas-rows-multiple-rows-as-one-adding-specific-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I know it's quite straightforward to use to check if the column contains a certain substring. What if I want to do the other way around: check if the column's value is contained by a longer string? I did a search but couldn't find an answer. I thought this should be easy, like in pure python we could simply I tried to use but seems it's not designed for this purpose. Say I have a df looks like this: I want to query this df on if is contained by a string , it should return me the first two rows.", "q_apis": "get columns contains value query first", "apis": "loc apply", "code": ["longstring = 'appleorangefruits'\ndf.loc[df['col1'].apply(lambda x: x in longstring)]\n", "    col1    col2\n0   apple   one\n1   orange  two\n"], "link": "https://stackoverflow.com/questions/57512376/how-to-check-if-a-string-is-in-a-longer-string-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am working on a time series figure where the date progression is on the x axis and the corresponding value for that date is on the y axis. My code:", "q_apis": "get columns time where date value date", "apis": "DataFrame plot", "code": ["    # Import as Dataframe\ndf = pd.DataFrame({'date': ['2021-01-03', '2021-01-05', '2021-01-07', '2021-01-09'],\n        'value': [3, 1, 2, 10]})\ndf['date'] = pd.to_datetime(df['date'])\n# Draw Plot\ndef plot_df(df, x, y, title=\"\", xlabel='Days', ylabel='Tweets per day', dpi=100):\n    plt.figure(figsize=(16,5), dpi=dpi)\n    plt.plot(x, y, color='tab:blue')\n    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n    plt.show()\n\nplot_df(df, x=df['date'], y=df.value, title='Title')\n"], "link": "https://stackoverflow.com/questions/65649632/how-can-i-make-my-dataframe-recognize-dates"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a class that looks like this: I have a DataFrame with the following columns: . Now, I want to pass to the above function as the parameter . Currently, this is what I'm doing: And it gives me this error: AttributeError: 'Series' object has no attribute 'lower' What is the correct way to do this?", "q_apis": "get columns DataFrame columns Series", "apis": "apply apply", "code": ["# Apply to Email body column only\nmessages_df_inbox['Email Body'].apply(processor.to_lower)\n", "# Change your function to point to email body\nclass A:\n    def __int__(self):\n        pass\n    def to_lower(self,content_text):\n        return content_text['Email Body'].lower()\n\nmessages_df_inbox.apply(processor.to_lower, axis = 1)\n"], "link": "https://stackoverflow.com/questions/61738651/how-to-pass-row-content-as-string-argument-using-dataframe-apply"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Is it possible to force Pandas to include a specific set of ordered columns in a pivot table, irrespective of whether the underlying data warrants their presence? For example produces but what I want is to see columns for each of the 12 months, even if there is no data there: This is useful in cases where the data has an underlying natural ordering and rendering representation, such as a month-by-month calendar.", "q_apis": "get columns ordered columns pivot columns where month month", "apis": "reindex columns", "code": ["pv.reindex(columns=np.arange(1, 13), fill_value='')\n\nStart Month 1         2  3  4         5  6  7  8         9  10 11 12\nCategory                                                         \nA               ProjectA                           ProjectC      \nB                               ProjectB                         \n"], "link": "https://stackoverflow.com/questions/49178331/forcing-extra-columns-to-show-up-in-pandas-pivot-table"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have three DataFrames like this: I want layer these on top of each other to get a MultiIndex with groups as first index, items as second and weights as values. Eg. in 0,0 I have \"e\" (first index), \"ret\" (second index), and 241 (value), or in 2,3 I have \"g\", \"krt\" and 754. How can I accomplish this? I would like to do this without iterating over the datasets, and do it the \"Panda way\". This is not the same as question Combine multiple pandas DataFrames into a multi-index DataFrame, because I want it to look like this: and so on... Meaning that I put the dataframes on top of each other and create a three dimensional dataframe, \"looking through them\".", "q_apis": "get columns get MultiIndex groups first index items second values first index second index value index DataFrame put", "apis": "unstack items unstack unstack sort_values set_index", "code": ["out = pd.concat([groups.unstack(), items.unstack(), weights.unstack()], 1)\nout.sort_values(by=0).set_index([0, 1])\n", "         2\n0 1\nb qeb  234\ne ret  241\n  hnf   23\n  xfh  234\nf sef  234\n  cbv  276\n  bcr  256\ng sdf  343\n  vbd   35\n  krt  754\nh fhs   34\n  \u00f8jg   42\nq gry  561\nr sjd  654\n  dgf  345\ns awd  334\n"], "link": "https://stackoverflow.com/questions/52566588/layer-panda-dataframe-on-top-of-each-other"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames and with the following columns: For in matching in , I need to add from to row. is around 7 million rows and df2 is around 15k. I tried the code below but it takes too long. I was wondering if there's a better solution that could speed things up a bit and more memory efficient.", "q_apis": "get columns columns add", "apis": "DataFrame DataFrame merge", "code": ["import pandas as pd \ndf1 = pd.DataFrame({'a_id':[1, 2, 3, 4, 5], 'c': [\"foo\", \"bar\", \"baz\", \"qux\", \"foobar\"]})\ndf2 = pd.DataFrame({'a_id':[3, 4], 'e': [\"bar\", \"baz\"]})\ndf1 = df1.merge(df2, on=[\"a_id\"], how=\"left\")\n", "   a_id       c    e\n0     1     foo  NaN\n1     2     bar  NaN\n2     3     baz  bar\n3     4     qux  baz\n4     5  foobar  NaN\n\n"], "link": "https://stackoverflow.com/questions/65097508/efficient-mapping-of-very-large-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 4 data frames: each of them have a structure as follows: I want to create a new data frame such that it has aggregated values for each category in all the data frames. So, the new data frame should have values which are calculated using the formula :- Like this it should generate values for all the rows. Can someone please help me out.", "q_apis": "get columns values all values values all", "apis": "groupby values sum set_index values div reset_index groupby values sum groupby values sum div reset_index", "code": ["s = pd.concat([df1, df2, df3, df4]).groupby('category')['values'].sum()\n\nout = df1.set_index('category')['values'].div(s).reset_index(name='total')\n", "s = pd.concat([df1, df2, df3, df4]).groupby('category')['values'].sum()\ns1 = pd.concat([df1, df2]).groupby('category')['values'].sum()\n\nout = s1.div(s2).reset_index(name='new')\n"], "link": "https://stackoverflow.com/questions/58061001/how-to-iterate-over-multiple-dataframes-and-add-values-to-new-dataframe-in-pytho"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to get the 20 biggest 'MOV12' for 'DPTO', using python and pandas I have a csv (.del) with the following fields I have a csv with the following fields, considering that the 'DPTO' is a total of 12 and I have thousands of data for the moment and managed to obtain the moyor 'MOV12' I hope you give me suggestions to find the solution, thanks", "q_apis": "get columns get", "apis": "groupby apply nlargest", "code": ["import pandas as pd\ndf = pd.read_csv(\"c.del\", sep = ' ')\nresult=df.groupby('DPTO').apply(\n    lambda x: x.nlargest(20,'MOV12')\n)\n"], "link": "https://stackoverflow.com/questions/56365434/consultation-python-in-csv-20-major-movements-by-department"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Most of the info I found was not in python>pandas>dataframe hence the question. I want to transform an integer between 1 and 12 into an abbrieviated month name. I have a df which looks like: I want the df to look like this:", "q_apis": "get columns info transform between month name", "apis": "apply", "code": ["import calendar\ndf['Month'] = df['Month'].apply(lambda x: calendar.month_abbr[x])\n"], "link": "https://stackoverflow.com/questions/37625334/python-pandas-convert-month-int-to-month-name"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to represent a process flow diagram in a pandas dataframe. Let me give you an example let's say we've a process diagram like this - Proces Diagram:- Please have a look at the image. Now, I've created pandas dataframe based on the information in the image - process name Id To Id process 1 10 200 process 1 10 80 process 2 200 8 process 3 80 NAN process 4 8 70 process 5 70 NAN You guy's can generate the above dataframe by-> I want to add one more information column that is sequence like this - process name Id To Id Sequence Start 0 10 0 process 1 10 200 1 process 1 10 80 1 process 2 200 8 2 process 3 80 NAN 2 process 4 8 70 3 process 5 70 NAN 4 Here, I've created an additional row with process name start(sequence 0) which represent the start of my diagram( and ). The sequence algorithm then checks for 10 in id column and there are 2 matches. So ,it'll mark those rows as 1 and store there information. Again it'll pick 1 \"to ID\" from [200,80]. Let's say 80 then it'll repeat the same process and mark the row starting with 80 as sequence 2 and here the is NAN so the process will stop and it'll check for 200. Is there any easy way to add this sequence column information fast? any good algorithm?. I can extract the first row using set diff. like this - I need help from here onwards how can I achieve sequence column from here?", "q_apis": "get columns at name add name name start start repeat stop any add any first diff", "apis": "loc eq iloc agg", "code": ["# this has to be defined *after* df_2 initialisation\ndef count_prev(curr) -> int:\n    new = df_2.loc[df_2['To Id'].eq(curr), 'Id']\n    if(len(new)>0):\n        return count_prev(new.iloc[0]) + 1\n    return 1\n    \ndf_2['Sequence'] = df_2['Id'].agg(count_prev)\n", "  process name   Id  To Id  Sequence\n0    process 1   10  200.0         1\n1    process 1   10   80.0         1\n2    process 2  200    8.0         2\n3    process 3   80    NaN         2\n4    process 4    8   70.0         3\n5    process 5   70    NaN         4\n"], "link": "https://stackoverflow.com/questions/66719398/represent-a-process-flow-diagram-in-a-pandas-dataframe-with-sequence-information"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two pandas dataframes, which rows are in different orders but contain the same columns. My goal is to easily compare the two dataframes and confirm that they both contain the same rows. I have tried the \"equals\" function, but there seems to be something I am missing, because the results are not as expected: I would expect that the outcome returns True, because both dataframes contain the same rows, just in a different order, but it returns False.", "q_apis": "get columns columns compare equals", "apis": "sort_values columns reset_index drop sort_values columns reset_index drop equals", "code": ["df11 = df_1.sort_values(by=df_1.columns.tolist()).reset_index(drop=True)\ndf21 = df_2.sort_values(by=df_2.columns.tolist()).reset_index(drop=True)\nprint (df11.equals(df21))\nTrue\n"], "link": "https://stackoverflow.com/questions/55358268/compare-content-of-two-pandas-dataframes-even-if-the-rows-are-differently-ordere"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am looking for a way to reorder column in my Pivot Table Here rare my columns : output : I would like to reorder Level column and get in order instead. I have tried many topics but I cannot manage to do it. Thanks for your help", "q_apis": "get columns columns get", "apis": "pivot_table index columns values sum columns iloc columns astype columns", "code": ["df = dfpivot.pivot_table(index=['DEPARTMENT_NAME','LEVEL_NAME','NAME','CLTNAME'],\n                            columns='StaffitWeek', \n                            values='ASSIGN_TIME', \n                            aggfunc=np.sum,\n                            fill_value=0)\n", "df = df[sorted(df.columns, key=lambda x: int(x[1:]))]\n", "df = df.iloc[:, df.columns.str.extract('(\\d+)', expand=False).astype(int).argsort()]\n", "import natsort as ns\ndf = df[sorted(ns.natsorted(df.columns), key=lambda x: not x.isdigit())]\n"], "link": "https://stackoverflow.com/questions/54685527/reorder-column-in-pivot-table"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following data frame like (in reality with hundreds of rows) What I want to do is to get a nested dictionary that look like that: and so on. So in other words I want to determine the number of times winners won in the Outdoor and in the Indoor court. Thank you in advance!", "q_apis": "get columns get", "apis": "DataFrame pivot_table values columns index count to_dict", "code": ["import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'Location':['France','France','France','France'],\n                   'Court':['Outdoor','Indoor','Indoor','Indoor'],\n                   'Winner':['Flavio','Luca','Giovanni','Luca']})\ndf = pd.pivot_table(df,values='Location',columns='Winner',index='Court',aggfunc='count',fill_value=0)\na = df.to_dict()\nprint(a)\n", "{'Flavio': {'Indoor': 0, 'Outdoor': 1}, 'Giovanni': {'Indoor': 1, 'Outdoor': 0}, 'Luca': {'Indoor': 2, 'Outdoor': 0}}\n"], "link": "https://stackoverflow.com/questions/60319015/create-a-nested-dictionary-from-pd-dataframe-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to get row values is certain conditions are met here is the code My dataframe looks like so This is the error I get desired output How can I do this better??", "q_apis": "get columns get values get", "apis": "loc values", "code": ["phones = merged_df.loc[(merged_df['Facility Code'] == response) & (merged_df['group'] == group) & (merged_df['Optedout'] == optout)]['phone'].values\nprint(phones)\n"], "link": "https://stackoverflow.com/questions/61006281/for-loops-and-if-statements-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 dataframes I want to remove all the observations which is common in Map3 & Map4 from the Map3 data frame. Tried the following code :", "q_apis": "get columns all", "apis": "isin loc", "code": ["common = (Map3.partner_country + Map3.my_network).isin(Map4.partner_country + Map4.my_network)\nMap3 = Map3.loc[~common]\n"], "link": "https://stackoverflow.com/questions/61520155/remove-common-observations"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have addresses stored in \"address\" column in a store dataframe, I would like to create a new column with the following corrections on existing addresses: How should I move forward this? Expected output: Here is the R code to the same:", "q_apis": "get columns", "apis": "join keys replace get DataFrame", "code": ["pattern =  r'\\b(' + '|'.join(mappings.keys()) + r')\\b'\ndf['addr_mapped'] = df['address'].str.replace(pattern, lambda s: mappings.get(s.group()))\n", "df = pd.DataFrame({'address': ['101 ST LN', 'foo ST bar WAY', 'bar bar STE', 'W foo WAY STE', 'foo bar']})\n", "# print(df)\n\n          address         addr_mapped\n0       101 ST LN     101 STREET LANE\n1  foo ST bar WAY  foo STREET bar WAY\n2     bar bar STE       bar bar SUITE\n3   W foo WAY STE  WEST foo WAY SUITE\n4         foo bar             foo bar\n"], "link": "https://stackoverflow.com/questions/62579661/pandas-replace-multiple-characters-from-rows-in-a-datframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a large dataset where each line represents the value of a certain type (think a sensor) for a time interval (between start and end). It looks like this: I want to turn it into a daily time-indexed frame like this: (Note that we cannot make any assumption regarding the interval: they should be contiguous and non-overlapping but we cannot guarantee that) Based on these Stack Overflow answers [1] (DataFrame resample on date ranges) [2] (pandas: Aggregate based on start/end date), there seem to exist two methods: one around itertuples, one around melt (2 above used stack/unstack but it is similar to melt). Let us compare them for performance. With in Jupyter, method 1 takes ~8s and method 2 takes ~25s for the dataframe defined as example. This is way too slow as the real dataset that I am dealing with is much bigger than this. On that dataframe, method 1 takes ~20 minutes. Do you have any idea on how to make this faster?", "q_apis": "get columns where value time between start time any DataFrame resample date start date itertuples melt stack unstack melt compare any", "apis": "DataFrame from_records itertuples columns DataFrame from_records itertuples columns DataFrame from_records itertuples columns iloc DataFrame values values values take all assign groupby apply reset_index drop max DataFrame values values values values loc", "code": ["df_expand = pd.DataFrame.from_records(\n    (\n        (d, r.type, r.value) \n        for r in df.itertuples()\n        for d in pd.date_range(start=r.start, end=r.end, freq='D')\n    ),\n    columns=['day', 'type', 'row']\n)\n", "one_day = dt.timedelta(1)\ndf_expand = pd.DataFrame.from_records(\n    (\n        (r.start + i * one_day, r.type, r.value) \n        for r in df.itertuples()\n        for i in range(int((r.end-r.start)/one_day)+1)\n    ),\n    columns=['day', 'type', 'row']\n)\n", "one_day = dt.timedelta(1)\ndf_expand = pd.DataFrame.from_records(\n    (\n        (d, r.type, r.value) \n        for r in df.itertuples()\n        for d in np.arange(r.start.date(), r.end.date()+one_day, dtype='datetime64[D]')\n    ),\n    columns=['day', 'type', 'row']\n)\n", "def expand_group(g):\n    dur = g.dur.iloc[0] # how many days for each reading in this group?\n    return pd.DataFrame({\n        'day': (g.start.values[:,None] + np.timedelta64(1, 'D') * np.arange(dur)).ravel(),\n        'type': np.repeat(g.type.values, dur),\n        'value': np.repeat(g.value.values, dur),\n    })\n# take all readings with the same duration and process them together using vectorized code\ndf_expand = (\n    df.assign(dur=(df['end']-df['start']).dt.days + 1)\n    .groupby('dur').apply(expand_group)\n    .reset_index('dur', drop=True)\n)\n", "dur = (df['end']-df['start']).max().days + 1\ndf_expand = pd.DataFrame({\n    'day': (\n        df['start'].values[:,None] + np.timedelta64(1, 'D') * np.arange(dur)\n    ).ravel(),\n    'type': np.repeat(df['type'].values, dur),\n    'value': np.repeat(df['value'].values, dur),\n    'end': np.repeat(df['end'].values, dur),\n})\ndf_expand = df_expand.loc[df_expand['day']<=df_expand['end'], 'day':'value']\n"], "link": "https://stackoverflow.com/questions/50747359/performance-issue-turning-rows-with-start-end-into-a-dataframe-with-timeindex"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I started a scrape for the 2018 MLB pitchers. I have various categories that I would like to turn into a dataframe so I can print to excel. I would like to use pandas. Here is my code at the moment: I would like to create one dataframe with my scrapes. Does anyone know how to do this? I see an explanation on how to create a dataframe on https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html, however, I don't know how to apply it to my situation. Here is an example below: I want to use my data above, though. Not sure if I need to append my data. Thanks!", "q_apis": "get columns categories at DataFrame apply append", "apis": "columns DataFrame columns columns append append append loc columns DataFrame columns columns loc", "code": ["columns = (\"names\", \"age\", \"w\", \"l\", \"g\", \"gs\", \"ip\", \"hits\", \"runs\", \"bb\", \"so\")\ndf = pd.DataFrame(columns=columns)\n\nfor idx, pitcher_row in enumerate(tree.xpath('//table[contains(@class,\"stats_table\")]//tr[contains(@class,\"full_table\")]')):\n    tmp = []\n    tmp.append(pitcher_row.xpath('.//td[@data-stat=\"player\"]/a')[0].text)\n    tmp.append(pitcher_row.xpath('.//td[@data-stat=\"age\"]/text()')[0])\n    tmp.append(pitcher_row.xpath('.//td[@data-stat=\"W\"]/text()')[0])\n    ...\n\n    df.loc[idx] = tmp\n", "columns = (\"names\", \"age\", \"w\", \"l\", \"g\", \"gs\", \"ip\", \"hits\", \"runs\", \"bb\", \"so\")\ndf = pd.DataFrame(columns=columns)\n\nfor idx, pitcher_row in enumerate(tree.xpath('//table[contains(@class,\"stats_table\")]//tr[contains(@class,\"full_table\")]')):\n    names = pitcher_row.xpath('.//td[@data-stat=\"player\"]/a')[0].text\n    age = pitcher_row.xpath('.//td[@data-stat=\"age\"]/text()')[0]\n    w = pitcher_row.xpath('.//td[@data-stat=\"W\"]/text()')[0]\n    l = pitcher_row.xpath('.//td[@data-stat=\"L\"]/text()')[0]\n    g = pitcher_row.xpath('.//td[@data-stat=\"G\"]/text()')[0]\n    gs = pitcher_row.xpath('.//td[@data-stat=\"GS\"]/text()')[0]\n    ip = pitcher_row.xpath('.//td[@data-stat=\"IP\"]/text()')[0]\n    hits = pitcher_row.xpath('.//td[@data-stat=\"H\"]/text()')[0]\n    runs = pitcher_row.xpath('.//td[@data-stat=\"R\"]/text()')[0]\n    bb = pitcher_row.xpath('.//td[@data-stat=\"BB\"]/text()')[0]\n    so = pitcher_row.xpath('.//td[@data-stat=\"SO\"]/text()')[0]\n\n    df.loc[idx] = (names, age, w, l, g, gs, ip, hits, runs, bb, so)\n"], "link": "https://stackoverflow.com/questions/52463349/how-can-i-convert-multiple-xpaths-to-a-dataframe-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two DataFrames: Person_df \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Target_df \u00a0 \u00a0\u00a0 My aim is to create a third DataFrame based on the country of each person, which would look like this: Individual_df \u00a0 \u00a0 Basically I have to take a person from Person_df, match his/her country with country mentioned in Target_df and then assign each of this target to this person(and store in Individual_df).\u00a0 Problem is, I am new to python and can't really figure out how to carry out this comparison of country. I wrote the code below: \u00a0 \u00a0 I need help on couple of points here: 1) How really I can perform the comparison here for country of each person. 2) Even if I know how to compare, the code I wrote is not at efficient as I am doing loads of unnecessary iterations here. Any pointers how can I improve this? Thanks!", "q_apis": "get columns DataFrame take assign compare at", "apis": "merge astype groupby astype index iterrows values", "code": ["Individual_df = pd.merge(Person_df, Target_df2, on=['Country'], how='left')\nIndividual_df['TargetID'] = Individual_df['Name'] + df3['Emplid'].astype(str) + ((df3.groupby('Emplid').cumcount() + 1).astype(str).str.zfill(2))\nIndividual_df = Individual_df[['TargetID', 'Category', 'Target']]\nprint Individual_df\n", "   TargetID   Category                  Target\n0   DK12301  Marketing    Reduce spend by $xy.\n1   DK12302        R&D  Increase spend by $dd.\n2   DK12303      Infra    Reduce spend by $kn.\n3   JS45601  Marketing    Reduce spend by $xy.\n4   JS45602        R&D  Increase spend by $dd.\n5   JS45603      Infra    Reduce spend by $kn.\n6   RM78901  Marketing  Increase spend by $eg.\n7   RM78902        R&D  Increase spend by $cb.\n8   RM78903      Infra    Reduce spend by $mn.\n9   MS11101  Marketing  Increase spend by $eg.\n10  MS11102        R&D  Increase spend by $cb.\n11  MS11103      Infra    Reduce spend by $mn.\n12  SR22201  Marketing  Increase spend by $eg.\n13  SR22202        R&D  Increase spend by $cb.\n14  SR22203      Infra    Reduce spend by $mn.\n", "unique_countries=df1['Country'].unique().tolist()\n\nfor index, row in df2.iterrows():\n    if row['Country'] in unique_countries:\n        print row.values\n        //do operation\n"], "link": "https://stackoverflow.com/questions/50716573/how-to-compare-cell-values-of-two-different-dataframes-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here is the dataframe The new columns will be created as follows: For every other and and will take Here is how I have started but couldn't go any further. please help", "q_apis": "get columns columns take any", "apis": "where where", "code": ["s = df1['CurrentPlaytime']/3*10**4\n\nmask1 = (df1['EventCodeId'] == 1029) & (df1['EventCode'] == 'Goal')\nmask2 = (df1['EventCodeId'] == 2053) & (df1['EventCode'] == 'Away')\n\ndf1['new_col1'] = np.where(mask1, s, 0)\ndf1['new_col2'] = np.where(mask2, s, 0)\n"], "link": "https://stackoverflow.com/questions/50856859/create-a-new-pandas-columns-from-multiple-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to be able to search through my dataframe and skip cells that are blank. However, when i read in the DF it reads the blanks as \"nan\" DF1 I want to be able to filter through Address1, Street and Town. If there is text inside of those columns I want to add a \"|\" at the start but if there is no text inside of the column it skips that cell and doesn't add the \"|\" Desired Result", "q_apis": "get columns filter columns add at start add", "apis": "where notnull astype", "code": ["import numpy as np\nfor i in ['Address1','Street','Town']:\n    df[i] = np.where(df[i].notnull(),'|' + df[i].astype(str),'')\n", "print(df)\n\n   Name Address1       Street     Town  Postcode\n0  Will                        |London       nan\n1  Phil    |19.0  |Long Road                nan\n"], "link": "https://stackoverflow.com/questions/65406496/python-skip-empty-cells"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have pandas dataframe which consist of 10 columns. each row consist a step performed by a user to online. there are total of 10 columns so all 10 step process lets say first activity is booking a flight ticket so steps are login website-->give src dest time-->select seats-->pay--review so there are various permutations can happen at every step, I want to draw a directed graph out of all dataset. currently networkx supports only 2 columns in can someone tell me how to d it for more than two column directed graph", "q_apis": "get columns columns step columns all step first time select at step all columns", "apis": "DataFrame columns columns columns values columns empty columns columns columns values all", "code": ["# libraries\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Build your graph\n\ndf = pd.DataFrame(np.random.randn(2,4),columns=list('ABCD')) #Create a 4 column data frame\n\ncolumns = list(df.columns.values)# Get columns name\n\ng = nx.empty_graph(0, nx.DiGraph()) #initialize an empty graph\n\nfor i in range(len(columns)-1):\n    g.add_edges_from(zip(df[columns[i]], df[columns[i+1]])) #Create edge between 2 values, between all consecutive coumns\n\n# Plot it\nnx.draw(g, with_labels=True)\nplt.show()\n"], "link": "https://stackoverflow.com/questions/53389570/build-networkx-directed-graph-or-flow-chart-from-more-than-one-column-of-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe called which looks like this. in which the first column are indexes and the second one is the column I wanted to convert to integers. What I tried to do is: which did not work. Any ideas? Thanks", "q_apis": "get columns first second", "apis": "iloc iloc", "code": ["for i in range(len(df['code'])):\n    df['code'].iloc[i] = int(df['code'].iloc[i] )\n"], "link": "https://stackoverflow.com/questions/56159638/trying-to-convert-data-in-one-column-from-string-to-integer"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to add the the data of reference to data, so I use but it only creates the column with no value, how can I add the value?", "q_apis": "get columns add value add value", "apis": "columns values DataFrame DataFrame DataFrame DataFrame index columns", "code": ["for col in reference.columns:\n    data[col] = reference[col].values[0]\n", "import pandas as pd\ndata = pd.DataFrame({'id': [1, 2, 3, 4],\n                   'val1': ['A', 'B', 'C', 'D']})\nreference = pd.DataFrame({'id2': [1, 2, 3, 4],\n                   'val2': ['A', 'B', 'C', 'D']})\n", "   id val1  id2 val2\n0   1    A    1    A\n1   2    B    2    B\n2   3    C    3    C\n3   4    D    4    D\n", "data = pd.DataFrame({'id': [1, 2, 3, 4],\n                   'val1': ['A', 'B', 'C', 'D']})\nreference = pd.DataFrame({'id2': [1, 2, 3, 4],\n                   'val2': ['A', 'B', 'C', 'D']})\nreference.index=[3,4,5,6]\n\ndata[reference.columns]=reference\n", "   id val1  id2 val2\n0   1    A  NaN  NaN\n1   2    B  NaN  NaN\n2   3    C  NaN  NaN\n3   4    D  1.0    A\n"], "link": "https://stackoverflow.com/questions/50799633/cannot-add-multiple-column-with-values-in-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have one data frame containing two entirely different data sets. The data sets are separated by two rows of all NAN values. I have provided a sample of the data frame below. Row 14 is the last row of the first data set, and row 17 is the first row of the second data set. I would like to end up with two data frames where the first ends at row 14 above and the second starts at row 17 above. I have tried to split them like this: When I run the code, I get an error saying, \"cannot do slice indexing on class 'pandas.core.indexes.range.RangeIndex' with these indexers\"", "q_apis": "get columns all values sample last first first second where first at second at get RangeIndex", "apis": "isnull all shift groupby cumsum", "code": ["is_row_nan = df.isnull().all(1)\nis_two_row_nan = (is_row_nan & is_row_nan.shift(1))\n\ndfs = [g for _, g in df.groupby(is_two_row_nan.cumsum())]\n", "df = pd.DataFrame(np.random.choice((1, np.nan), (10, 2)))\n     0    1\n0  1.0  NaN\n1  NaN  1.0\n2  NaN  NaN\n3  NaN  NaN\n4  1.0  NaN\n5  NaN  NaN\n6  NaN  1.0\n7  1.0  NaN\n8  1.0  1.0\n9  NaN  1.0\n", "dfs[0]\n     0    1\n0  1.0  NaN\n1  NaN  1.0\n2  NaN  NaN\n\n\ndfs[1]\n\n     0    1\n3  NaN  NaN\n4  1.0  NaN\n5  NaN  NaN\n6  NaN  1.0\n7  1.0  NaN\n8  1.0  1.0\n9  NaN  1.0\n"], "link": "https://stackoverflow.com/questions/60173389/how-to-split-one-data-frame-into-two-after-two-rows-of-all-nan-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two pandas series for which I want to compare them visually by plotting them on top of each other. I already tried the following which yields the following picture: Now, I am aware of the keyword of but trying to apply it, requires me to to use the keywords and . I already tried to transform my data into a different dataframe like that so I can \"hue over\" ; but even then I have no idea what to put for the keyword (assuming ). Ignoring the keyword like that fails to hue anything:", "q_apis": "get columns compare apply transform put", "apis": "melt", "code": ["long_form = df.melt()\nlong_form['X'] = 1\n\nsns.stripplot(data=long_form, x='X', y='value', hue='variable')\n", "   variable  value  X\n0         0      1  1\n1         0      2  1\n2         0      3  1\n3         0      4  1\n4         0      5  1\n5         1      3  1\n6         1      3  1\n7         1      3  1\n8         1      3  1\n9         1      3  1\n"], "link": "https://stackoverflow.com/questions/58711333/hue-two-panda-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Problem How can you insert rows for missing YEARS, with imputed annual SALES. Progress The following code computes the sales differences. However, it is for one year, using the explicit iloc pointer technique. Original Data Goal The goal is to insert the yellow highlighted rows, where sales are imputed using straight-line steps between 1990 and 2000 sales figures.", "q_apis": "get columns insert year iloc insert where between", "apis": "groupby apply min max reset_index", "code": ["idx=df.groupby(['COUNTRY','STATE','BRANCH'])['YEAR'].\\\n       apply(lambda x : pd.Series(range(min(x),max(x)+1))).\\\n         reset_index(level=[0,1,2])\n"], "link": "https://stackoverflow.com/questions/55801028/inserting-missing-rows-with-imputed-values-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframes: Dataframe 1: Fruit Vegetable Mango Spinach Apple Kale Watermelon Squash Peach Zucchini Dataframe 2: Item Price/lb Mango 2 Spinach 1 Apple 4 Peach 2 Zucchini 1 I want to discard the rows from the dataframe 1 when both the columns are not present in the 'Item' series of dataframe 2 and I want to create the following dataframe3 based on dataframes 1 & 2: Fruit Vegetable Combination Price Mango Spinach 3 Peach Zucchini 3 The third column in dataframe 3 is the sum of the item prices from dataframe 2.", "q_apis": "get columns columns sum item", "apis": "isin isin reset_index melt index merge drop columns set_index index unstack transform assign xs sum", "code": ["(df1[(df1['Fruit'].isin(df2['Item'])) & (df1['Vegetable'].isin(df2['Item']))]\n    .reset_index()\n    .melt(id_vars = 'index',value_vars = ['Fruit','Vegetable'])\n    .merge(df2,left_on='value',right_on = 'Item')\n    .drop(columns = 'Item')\n    .set_index(['index','variable']).unstack(level = 1)\n    .transform(lambda g: g.assign(Combination_Price=g.xs('Price/lb',axis=1,level=0).sum(axis=1)))\n)\n", "            value               Price/lb           Combination_Price\nvariable    Fruit   Vegetable   Fruit   Vegetable   \nindex                   \n0           Mango   Spinach     2       1         3\n3           Peach   Zucchini    2       1         3\n"], "link": "https://stackoverflow.com/questions/66911494/how-to-filter-the-rows-of-a-dataframe-based-on-the-presence-of-the-column-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame similar to the below:, and I want to add a Streak column to it (see example below): The DataFrame is approximately 200k rows going from 2005 to 2020. Now, what I am trying to do is find the number of consecutive games the Home Team has won PRIOR to the date in in the Date column in the DataFrame. I have a solution, but it is too slow, see below: How can I speed this up?", "q_apis": "get columns DataFrame add DataFrame date DataFrame", "apis": "any diff diff diff diff cumsum shape", "code": ["x = np.array([\n    ['A', 'G', 'A'],\n    ['B', 'H', 'H'],\n    ['C', 'I', 'C'],\n    ['D', 'J', 'J'],\n    ['E', 'K', 'K'],\n    ['F', 'L', 'F'],\n    ['A', 'B', 'A'],\n    ['C', 'D', 'D'],\n    ['E', 'F', 'F'],\n    ['G', 'H', 'H'],\n    ['I', 'J', 'J'],\n    ['K', 'L', 'K'],\n    ['B', 'C', 'B'],\n    ['A', 'D', 'A'],\n    ['G', 'K', 'K'],\n    ['I', 'E', 'E'],\n    ['F', 'H', 'F'],\n    ['J', 'L', 'J']])\n", "A_played = np.flatnonzero((x[:, :2] == 'A').any(axis=1))\nA_won = x[A_played, -1] == 'A'\n", "streaks = np.diff(np.flatnonzero(np.diff(np.r_[False, A_won, False])))[::2]\n", "wins = np.r_[0, A_won, 0]  # Notice the int dtype here\nswitch_indices = np.flatnonzero(np.diff(wins)) + 1\nstreaks = np.diff(switch_indices)[::2]\nwins[switch_indices[1::2]] = -streaks\n", "streak_counts = np.cumsum(wins[:-2])\noutput = np.zeros((x.shape[0], 2), dtype=int)\n\n# Home streak\nhome_mask = x[A_played, 0] == 'A'\noutput[A_played[home_mask], 0] = streak_counts[home_mask]\n\n# Away streak\naway_mask = ~home_mask\noutput[A_played[away_mask], 1] = streak_counts[away_mask]\n", "def process_team(data, team, output):\n    played = np.flatnonzero((data[:, :2] == team).any(axis=1))\n    won = data[played, -1] == team\n    wins = np.r_[0, won, 0]\n    switch_indices = np.flatnonzero(np.diff(wins)) + 1\n    streaks = np.diff(switch_indices)[::2]\n    wins[switch_indices[1::2]] = -streaks\n    streak_counts = np.cumsum(wins[:-2])\n\n    home_mask = data[played, 0] == team\n    away_mask = ~home_mask\n\n    output[played[home_mask], 0] = streak_counts[home_mask]\n    output[played[away_mask], 1] = streak_counts[away_mask]\n\noutput = np.empty((x.shape[0], 2), dtype=int)\n\n# Assume every team has been home team at least once.\n# If not, x[:, :2].ravel() copies the data and np.unique(x[:, :2]) does too\nfor team in set(x[:, 0]):\n    process_team(x, team, output)\n"], "link": "https://stackoverflow.com/questions/63672218/efficiently-finding-consecutive-streaks-in-a-pandas-dataframe-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame with a column named title, I want to apply textdistance to check similarities between different titles and remove any rows with similar titles (based on a specific threshold). Is there away to do that directly, or I need to define a custom function and group similar titles togother before removing \"duplicates\" (titles that are similar)? A sample would look like this. The target is to drop one of the first two rows since they have a similar title under pub_title.", "q_apis": "get columns apply between any sample drop first", "apis": "index index index iterrows index index iloc iloc iloc iloc loc", "code": ["def remove_similar_titles(df):\ndf.index = range(len(df.index))\ndf['keep'] = 1\nfor index, target_row in df.iterrows():\n    target_title = target_row['pub_title']\n    for j in range(index+1, len(df.index)):\n            row = df.iloc[[j]]\n            title = row['pub_title'].iloc[0]\n            res = textdistance.jaro.similarity(target_title, title)\n            print(str(res) + ' --- ' + target_title + ' --- ' + title)\n            print(row['keep'].iloc[0])\n            if res > 0.85 and row['keep'].iloc[0] == 1:\n                df.loc[j, 'keep'] = 0\nreturn df\n"], "link": "https://stackoverflow.com/questions/66111317/pandas-filter-out-rows-according-to-titles-similarities"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like two transform two columns begin and end: into one column timestamp with an other column flag : But at the moment I can't find a solution to merge the two column begin and end into one. Thank you for your time !", "q_apis": "get columns transform columns timestamp at merge time", "apis": "stack rename_axis reset_index", "code": ["(df.stack()\n .rename_axis([None, 'Flag'])\n .reset_index(level=1, name='Timestamp'))\n", "    Flag                    Timestamp\n0    end  2019-10-21  07:48:28.272688\n1    end  2019-10-21  07:48:28.449916\n2  begin  2019-10-21  07:48:26.740378\n3  begin  2019-10-21  07:48:26.923764\n4    end  2019-10-21  07:48:41.689466\n5  begin  2019-10-21  07:48:37.306045\n6    end  2019-10-21  07:58:00.774449\n7  begin  2019-10-21  07:57:59.223986\n8    end  2019-10-21  08:32:37.004455\n9  begin  2019-10-21  08:32:35.755252\n"], "link": "https://stackoverflow.com/questions/60603500/how-to-merge-two-datetime-column-in-one-pandas-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a text file with 670,000 + lines need to process. Each line has the format of: I did some cleanning and transferred each line to a list: And my question is: How can I merge (x,y,t)tuples in different lists but have the common uid efficiently? For example: I have multiple lists And I want to transfer them into: Any help would be really appreciated.", "q_apis": "get columns merge", "apis": "items items append iteritems append", "code": ["import collections\n\ndef group_items(items):\n    grouped_dict = collections.defaultdict(list)\n    for item in items:\n        uid = item[0]\n        t = item[1]\n        grouped_dict[uid].append(t)\n\n    grouped_list = []\n    for uid, tuples in grouped_dict.iteritems():\n        grouped_list.append([uid] + tuples)\n\n    return grouped_list\n"], "link": "https://stackoverflow.com/questions/57508214/how-to-merge-elements-from-multiple-lists-with-same-id-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a , I want to and apply the following function, that pass each object as a df into the ; assign the boolean results back to the ,", "q_apis": "get columns apply assign", "apis": "merge groupby apply", "code": ["df.merge(df.groupby('cluster_id').apply(valid_row_dup).to_frame(), on='cluster_id')\n\n    cluster_id memo      0\n0            1    m  False\n1            1    n  False\n2            2    m  False\n3            2    m  False\n4            2    n  False\n5            3    m   True\n6            3    m   True\n7            3    m   True\n8            3    n   True\n9            4    m   True\n10           4    n   True\n11           4    n   True\n12           4    n   True\n"], "link": "https://stackoverflow.com/questions/54890589/pandas-how-to-apply-function-to-groupby-objects-with-argument"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hello I have a df such as : and I would like to count : Nb Groups with only \":\" = 1 (G3) Nb Groups with not only \":\" = 2(G1 and G4 ) Nb Groups without any \":\" = 1 (G2) does someone have na idea ? I guess I should sue a and do the sum of each in pandas ?", "q_apis": "get columns count any sum", "apis": "groupby all sum groupby any sum any all all groupby all sum", "code": ["om = df['COL1'].str.contains(':')\n\none = om.groupby(df['Groups']).all().sum() # 1\ntwo = om.groupby(df['Groups']).any().sum() - one # 2 \n# minus one because `any` counts all Trues too so we need \n# subtract groups with all Trues.\nthree = (~om).groupby(df['Groups']).all().sum() # 1\n"], "link": "https://stackoverflow.com/questions/64765989/count-sum-of-of-patterns-matches-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dictionary which contains around 10 dataframes. With key being the dataframe name What I am trying to do is create a new column called but by mapping it to a pandas series () is a pandas series as shown below. is the index name. I am trying to avoid the case sensitivity or make both lower. The below code throws error", "q_apis": "get columns contains name index name", "apis": "rename index index", "code": ["unit_dict = unit_dict.rename(str.lower)\ndataFramesDict[k]['unit'] = dataFramesDict[k]['Test'].str.lower().map(unit_dict)\n\n#alternative\nunit_dict.index = unit_dict.index.str.lower()\ndataFramesDict[k]['unit'] = dataFramesDict[k]['Test'].str.lower().map(unit_dict)\n"], "link": "https://stackoverflow.com/questions/57360203/ignore-case-while-mapping-dict-items-to-a-pandas-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to read several files from a directory into pandas and concatenate them into one big DataFrame. The directory consists of 7470 files. The runtime of the code exceeds more than an hour Is there a way to read bulk files into a pandas more efficiently? Sample Dataset Zipped Sample Dataset", "q_apis": "get columns DataFrame hour", "apis": "all merge all all all DataFrame info", "code": ["import os\nfrom datetime import datetime, timedelta\nfrom pandas import json_normalize\nimport pandas as pd\nimport numpy as np\nimport yaml\n\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nch.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n# yaml file path\nos.chdir('~/Downloads/all')\nyaml_file_list = os.listdir('.')\nyaml_file_list =  [i for i in yaml_file_list if i.endswith('yaml')]\n\nif not os.path.exists('output'):\n    os.mkdir('output')\n    \ndef yaml2json(file, cnt = 1):\n    file_json = f'output/{file}.json'\n    if os.path.exists(file_json):\n        return\n    # read yaml and convert to dict\n    with open(file,'r') as fh:\n        data = yaml.load(fh.read(), Loader=yaml.BaseLoader)\n    \n    # convert to json file\n    data_str = json.dumps(data, ensure_ascii=False) + '\\n'\n    with open(file_json, 'w') as fw:\n        fw.write(data_str)\n    logging.info(f'[{cnt}] {file_json}')\n\nfile = yaml_file_list[0]\nyaml2json(file)\n\n# muti-Process to handle file to json\nfrom concurrent.futures import ProcessPoolExecutor\n####################\nworkers = 8\npool_list = yaml_file_list\npool_func = yaml2json\n####################\ntotal_count = len(pool_list)\nwith ProcessPoolExecutor(max_workers=workers) as executor:\n    futures = [executor.submit(pool_func, param, total_count-n) \n                        for n, param in enumerate(pool_list)\n            ]\n  \n# 2020-12-29 14:29:19,648 - INFO - [7468] output/1163055.yaml.json   \n# 2020-12-29 14:32:07,597 - INFO - [6466] output/640941.yaml.json\n\n# macbook 15' 2015\n# 2.2 GHz Intel Core i7\n# 16 GB 1600 MHz DDR3\n# 1000 -> 14:29:19,648 -> 14:32:07,597 -> 3min\n# 7400 ~ 25min\n", "os.chdir('~/Downloads/all/output/')\n\n# merge file use bash cmd cat\n# !cat *.json > yaml-all-json\n# ipython\npycmd = lambda cmd: get_ipython().system(cmd)\ncmd = 'cat *.json > yaml-all-json'\n# pycmd(cmd)\n", "# read file\n# 1478 lines ->  4.37s\nfile = 'yaml-all-json'\ndf = pd.read_csv(file, sep='\\n', header=None)[0]\nobj = df.map(json.loads)\ndata_list = obj.tolist()\ndf_data = pd.DataFrame(data_list) # or use json_normalize to parse json data\n\ndf_data\n#   meta    info    innings\n# 0 {'data_version': '0.9', 'created': '2016-12-05...   {'dates': ['2016-11-24', '2016-11-25', '2016-1...   [{'1st innings': {'team': 'South Africa', 'dec...\n# 1 {'data_version': '0.9', 'created': '2016-12-21...   {'city': 'Brisbane', 'dates': ['2016-12-15', '...   [{'1st innings': {'team': 'Australia', 'delive...\n# 2 {'data_version': '0.9', 'created': '2016-10-21...   {'city': 'Port Moresby', 'dates': ['2016-10-16...   [{'1st innings': {'team': 'Papua New Guinea', ...\n# 3 {'data_version': '0.9', 'created': '2016-09-14...   {'city': 'Edinburgh', 'dates': ['2016-09-10'],...   [{'1st innings': {'team': 'Scotland', 'deliver...\n# 4 {'data_version': '0.9', 'created': '2016-09-12...   {'city': 'Londonderry', 'dates': ['2016-09-05'...   [{'1st innings': {'team': 'Hong Kong', 'delive..\n"], "link": "https://stackoverflow.com/questions/65487863/is-there-a-way-to-read-bulk-yaml-files-into-a-pandas-dataframe-more-efficien"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have query regarding for loops and adding one to an already working web scraper to run through a list of webpages. What I'm looking at it probably two or three simple lines of code. I appreciate this has probably been asked many times before and answered but I've been struggling to get some code to work for me for quite some time now. I'm relatively new to Python and looking to improve. Background info: I've written a web scraper using Python and Beautifulsoup which is successfully able to take a webpage from TransferMarkt.com and scrape all the required web links. The script is made up of two parts: In the first part, I am taking the webpage for a football league, e.g. The Premier League, and extract the webpage links for all the individual teams in the league table and put them in a list. In the second part of my script, I then take this list of individual teams and further extract information of each of the individual players for each team and then join this together to form one big pandas DataFrame of player information. My query is regarding how to add a for loop to the first part of this web scraper to not just extract the team links from one league webpage, but to extract links from a list of league webpages. Below I've included an example of a football league webpage, my web scraper code, and the output. Example: Example webpage to scrape (Premier League - code GB1): https://www.transfermarkt.co.uk/jumplist/startseite/wettbewerb/gb1/plus/?saison_id=2019 Code (part 1 of 2) - scrape individual team links from league webpage: Output: Extracted links from example webpage (20 links in total for example webpage, just showing 4): Using this list of teams - , I am then able to further extract information for all the players of each team with the following code. From this output I'm then able to create a pandas DataFrame of all players info: Code (part 2 of 2) - scrape individual player information using the team_links list: My question to you - adding a for loop to go through all the leagues: What I need to do is replace the variable assigned to an individual league code in the first part of my code, and instead use a for loop to go through the full list of league codes - . This list of league codes is as follows: I've read through several solutions but I'm struggling to get the loop to work and append the full list of team webpages at the correct part. I believe I'm now really close to completing my scraper and any advice on how to create this for loop would be much appreciated! Thanks in advance for your help!", "q_apis": "get columns query at get time now info take all first all put second take join DataFrame query add first all DataFrame all info all replace first codes codes get append at now any", "apis": "get get pad get items div join get get DataFrame columns append to_csv index", "code": ["import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0'\n}\n\nleagues = ['L1', 'GB1', 'IT1', 'FR1', 'ES1']\n\n\ndef main(url):\n    with requests.Session() as req:\n        links = []\n        for lea in leagues:\n            print(f\"Fetching Links from {lea}\")\n            r = req.get(url.format(lea), headers=headers)\n            soup = BeautifulSoup(r.content, 'html.parser')\n            link = [f\"{url[:31]}{item.next_element.get('href')}\" for item in soup.findAll(\n                \"td\", class_=\"hauptlink no-border-links hide-for-small hide-for-pad\")]\n            links.extend(link)\n\n        print(f\"Collected {len(links)} Links\")\n        goals = []\n        for num, link in enumerate(links):\n            print(f\"Extracting Page# {num +1}\")\n            r = req.get(link, headers=headers)\n            soup = BeautifulSoup(r.content, 'html.parser')\n            target = soup.find(\"table\", class_=\"items\")\n            pn = [pn.text for pn in target.select(\"div.rn_nummer\")]\n            pos = [pos.text for pos in target.findAll(\"td\", class_=False)]\n            name = [name.text for name in target.select(\"td.hide\")]\n            dob = [date.find_next(\n                \"td\").text for date in target.select(\"td.hide\")]\n            nat = [\" / \".join([a.get(\"alt\") for a in nat.find_all_next(\"td\")[1] if a.get(\"alt\")]) for nat in target.findAll(\n                \"td\", itemprop=\"athlete\")]\n            val = [val.get_text(strip=True)\n                   for val in target.select('td.rechts.hauptlink')]\n            goal = zip(pn, pos, name, dob, nat, val)\n            df = pd.DataFrame(goal, columns=[\n                              'position_number', 'position_description', 'name', 'dob', 'nationality', 'value'])\n            goals.append(df)\n\n        new = pd.concat(goals)\n        new.to_csv(\"data.csv\", index=False)\n\n\nmain(\"https://www.transfermarkt.co.uk/jumplist/startseite/wettbewerb/{}/plus/?saison_id=2019\")\n"], "link": "https://stackoverflow.com/questions/61216170/adding-a-for-loop-to-a-working-web-scraper-python-and-beautifulsoup"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas DataFrame which looks like this So I am trying to prompt the user to input a value and if the input exists within the Sub Region column, perform a particular task. I tried turning the 'Sub region' column to a list and iterate through it if it matches the user input That is not the output I had in mind. I believe there is an easier way to do this but can not seem to figure it out", "q_apis": "get columns DataFrame value", "apis": "sub index sub", "code": ["sub_region = input(\"Enter a sub region:\")\nif sub_region not in df.index.get_level_values('Sub Region'):\n    raise ValueError(\"You must enter a valid sub-region\")\n"], "link": "https://stackoverflow.com/questions/67921251/is-there-a-way-to-iterate-through-a-column-in-pandas-if-it-is-an-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Apologies for being a python noob (with only basic programming skills)! I have an excel file from which I need to export individual rows into single .text files, where the filename is that of a cell within a column. For example, I have an excel file like this: And I need the following: import the excel file, I've managed to do that; choose only rows where col1 contains 1; for each row, choose only col2, col3 to be printed into a text file where the data from col3 comes after a line break after the data from col2; name each file according to what's in ID. So far, this is what I have: This is probably simple, but I'm getting stuck in step 1 already :( can you help?", "q_apis": "get columns where where contains where name step", "apis": "DataFrame columns loc values iterrows", "code": ["vals = [(1, 0, 'cat', 34, 'k'), (2, 1, 'dog', 30, 'g')]\ndf = pd.DataFrame(vals, columns=['Id', 'col1', 'col2', 'col3', 'col4'])\n\nsubset = df.loc[df.col1 == 1] #subset for values equal to one in the col1\n\n#loop through \nfor i, r in subset.iterrows():\n    for_write = '{} \\n {}'.format(r.col2, r.col3)\n    print(for_write)\n    with open('{}.txt'.format(r.Id), 'w') as text_file:\n        text_file.write(for_write)\n"], "link": "https://stackoverflow.com/questions/51825300/python-print-rows-into-individual-text-files-with-columns-separated-by-linebrea"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas DataFrame of operations in my application: I would like to create another column with UserTeam. I know that Katie, Cristin and Steven have always been on the same team: So when I do I get: Now, I also know that: John moved from to on David moved from to on I know how to do it with and looping over the rows and hardcoding all rules but I don't think it's efficient. How do I do it in a vectorized way for a big number of users? Expected result:", "q_apis": "get columns DataFrame get all", "apis": "loc loc", "code": ["changes = [(\"John\", \"01-Jan-2016\", \"A\", \"C\"), (\"David\", \"12-Dec-2015\", \"B\", \"C\")]\n\nfor name, date, before, after in changes:\n    name_mask = df['UserName'] == name\n    df.loc[name_mask & (df['StartEdit'] < date), 'UserTeam'] = before\n    df.loc[name_mask & (df['StartEdit'] >= date), 'UserTeam'] = after\n"], "link": "https://stackoverflow.com/questions/50491303/efficient-way-of-conditional-value-setting"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a column in dataframe which contains string values as given below: I want to sort each word in a element alphabetically. Desired output: I tried doing this using below code: But this code doen not work.", "q_apis": "get columns contains values", "apis": "apply join join", "code": ["sortdf['col1'] = sortdf['col1'].apply(lambda x: ' '.join(sorted(x.split())))\n", "                      col1\n0            are hello you\n1           happenend what\n2          hello there you\n3  in is issue our program\n4       is name whatt your\n", "sortdf['col1'] = [' '.join(sorted(x)) for x in sortdf['col1'].str.split()]\n"], "link": "https://stackoverflow.com/questions/51449901/sort-words-alphabetically-in-each-row-of-a-dataframe-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe in pandas with shops and item ids columns. I'd like to assign to each couple (shop, item_id) a data range of a month. for example if shop is called 'Toy' and item_id 'ball', I have something like this: And I'd like to have something like this: How can I achieve this with pandas?", "q_apis": "get columns item columns assign month", "apis": "DataFrame values columns columns groupby", "code": ["df    \n  shop item_id\n0  Toy    ball\n1  Toy    book\n", "n = 5 # Number of days.\n", "df = pd.DataFrame(df.values.repeat(n, axis=0), columns=df.columns)\n", "days = pd.Series([\n    pd.DateOffset(days=x) for x in df.groupby(['shop', 'item_id']).cumcount()\n])\ndf['date'] = days + pd.to_datetime('2015-1-1')\nprint(df)\n\n  shop item_id       date\n0  Toy    ball 2015-01-01\n1  Toy    ball 2015-01-02\n2  Toy    ball 2015-01-03\n3  Toy    ball 2015-01-04\n4  Toy    ball 2015-01-05\n5  Toy    book 2015-01-01\n6  Toy    book 2015-01-02\n7  Toy    book 2015-01-03\n8  Toy    book 2015-01-04\n9  Toy    book 2015-01-05\n"], "link": "https://stackoverflow.com/questions/53793951/assigning-date-sequences-to-pandas-groupby-groups"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe below: Based on the \"value\" column, I want to have the top 50% value to be marked as 1, bottom 50% value marked as 0. Expecting to get result below: Thx in advance.", "q_apis": "get columns value value value get", "apis": "median astype quantile astype", "code": ["df['percent_mark'] = (df['value'] > df['value'].median()).astype(int)\n", "df['percent_mark'] = (df['value'] > df['value'].quantile(0.25)).astype(int)\n"], "link": "https://stackoverflow.com/questions/52876391/pandas-based-on-top-x-value-of-each-column-mark-as-new-number"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have got a dataframe of game releases and ratings I want to fill NaN values in user_score column with the mean of the same genre. If a game has sports genre and in that row user_score is NaN i want replace the null value with sport's average user rating.", "q_apis": "get columns values mean replace value", "apis": "groupby transform fillna mean", "code": ["name,platform,year_of_release,genre,na_sales,eu_sales,jp_sales,other_sales,critic_score,user_score,rating\nWii Sports,Wii,2006.0,Sports,41.36,28.96,3.77,8.45,76.0,8.0,E\nSuper Mario Bros.,NES,1985.0,Platform,29.08,3.58,6.81,0.77,,,\nMario Kart Wii,Wii,2008.0,Racing,15.68,12.76,3.79,3.29,82.0,8.3,E\nWii Sports Resort,Wii,2009.0,Sports,15.61,10.93,3.28,2.95,80.0,,E\nPokemon Red/Pokemon Blue,GB,1996.0,Role-Playing,11.27,8.89,10.22,1.0,,,\n", "df['user_score'] = df.groupby('genre')['user_score'].transform(lambda x: x.fillna(x.mean()))\n"], "link": "https://stackoverflow.com/questions/62775549/how-to-replace-nan-values-with-another-columns-mean-based-on-value-in-another-c"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am currently trying to generate new columns [Y, Z] based on the name of another column [X] of the same Dataframe. The problem is that I have not been able to obtain the name of the column and pass it as data to a new column. I am trying something like this: As an example, the dataframe would look something like this: But I am not very close to a solution and I've already tried in various ways... :( Please give me an idea where should I go? Thank you in advance for your time, I will be attentive to your answers! Regards!", "q_apis": "get columns columns name name where time", "apis": "set_index stack reset_index rename columns melt", "code": [">>> df\n     name  example_col\n0  shakir           33\n1   rafiq           37\n2     dev           36\n3   suraj           30\n", ">>> df.set_index('name').stack().reset_index().rename(columns={'level_1': 'new_col', 0:'value'})\n     name      new_col  value\n0  shakir  example_col     33\n1   rafiq  example_col     37\n2     dev  example_col     36\n3   suraj  example_col     30\n", ">>> pd.melt(df, id_vars=['name'], var_name='new_col')\n     name      new_col  value\n0  shakir  example_col     33\n1   rafiq  example_col     37\n2     dev  example_col     36\n3   suraj  example_col     30\n"], "link": "https://stackoverflow.com/questions/62779280/get-the-name-of-a-column-and-create-data-from-it-in-dataframe-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to investigate the cross correlation of two DataFrames. The code is given here: But I get this error: https://imgur.com/PIOXwND Any ideas?", "q_apis": "get columns get", "apis": "DataFrame DataFrame join corr", "code": ["# You will need to change your column names, like \ndf1 = pd.DataFrame({\"A\":[1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1]})\ndf2 = pd.DataFrame({\"B\":[7191, 7275, 9889, 9934, 9633, 9924, 9650, 9341, 8820, 8784, 8869]})\n\ndf1.join(df2).corr()\n", "            A           B\nA   1.000000    -0.174287\nB   -0.174287   1.000000\n"], "link": "https://stackoverflow.com/questions/60672052/valueerror-object-too-deep-for-desired-array-while-using-cross-correlation"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am working with a dataframe of Chess results like the following Now I'd like to (1) group by Opponent, (2) group by date (within opponent), (3) tabulate the count of each of the Results, (4) give the sequence of Results obtained. The first 3 can be obtained with , e.g. - a full example - What I would like is the same output as this last but with an added column showing the sequence of results (ordered by time) in that day's games between the two players, ordered by time. Ideally I'd like '1's as 'W', 0.5s as 'D', 0s as 'L' and a single long string in the column. Desired output: Please note that, in the original dataframe, it is NOT guaranteed that games/results are listed in time-order; and in the original dataframe, the datatype of every variable is and I'd like to keep it that way in the final output (e.g. should remain as '1', '0', '0.5' strings, not '1.0', '0.5, '0.0', s should finally be strings; only the actual result counts can and will presumably be integers). My thoughts: I thought of just ordering by time and then taking the column as a pandas Series. The problem is how to do this along with (i.e. after) the grouping by Opponent and Date.", "q_apis": "get columns date count first last ordered time day between ordered time time time Series", "apis": "pivot_table index columns size rename columns groupby apply join sort_values drop columns pivot_table index columns size rename columns groupby apply join", "code": ["   Opponent        Date      Time  Result\n0    Hikaru  2020.03.02  01:22:54     1.0\n1    Hikaru  2020.03.02  01:22:58     0.5\n2    Hikaru  2020.03.03  01:18:17     0.0\n3    Hikaru  2020.03.03  01:19:45     1.0\n4    Hikaru  2020.03.03  01:19:54     1.0\n5    Hikaru  2020.03.03  02:15:23     0.5\n6     Anish  2020.03.03  02:21:25     0.5\n7     Anish  2020.03.03  02:21:29     0.0\n8     Anish  2020.03.04  15:45:12     1.0\n9     Anish  2020.03.04  15:48:11     0.5\n10    Anish  2020.03.04  16:05:01     0.5\n", "df_out = df.pivot_table(\n    index=[\"Opponent\", \"Date\"],\n    columns=\"Result\",\n    aggfunc=\"size\",\n    fill_value=0,\n).rename(columns={0.0: \"0\", 1.0: \"1\"})\n\ndf_out[\"result_seq\"] = df.groupby([\"Opponent\", \"Date\"])[\"Result\"].apply(\n    lambda x: \"\".join({0: \"L\", 1: \"W\", 0.5: \"D\"}[v] for v in x)\n)\nprint(df_out)\n", "Result               0  0.5  1 result_seq\nOpponent Date                            \nAnish    2020.03.03  1    1  0         DL\n         2020.03.04  0    2  1        WDD\nHikaru   2020.03.02  0    1  1         WD\n         2020.03.03  1    1  2       LWWD\n", "df[\"tmp\"] = pd.to_datetime(df.Date + \" \" + df.Time)\ndf = df.sort_values(by=\"tmp\").drop(columns=\"tmp\")\n\ndf_out = df.pivot_table(\n    index=[\"Opponent\", \"Date\"],\n    columns=\"Result\",\n    aggfunc=\"size\",\n    fill_value=0,\n).rename(columns={0.0: \"0\", 1.0: \"1\"})\n\ndf_out[\"result_seq\"] = df.groupby([\"Opponent\", \"Date\"])[\"Result\"].apply(\n    lambda x: \"\".join({0: \"L\", 1: \"W\", 0.5: \"D\"}[v] for v in x)\n)\nprint(df_out)\n", "Result               0  0.5  1 result_seq\nOpponent Date                            \nAnish    2020.03.03  1    1  0         DL\n         2020.03.04  0    2  1        WDD\nHikaru   2020.03.02  0    1  1         WD\n         2020.03.03  1    1  2       WWLD\n"], "link": "https://stackoverflow.com/questions/67012319/collecting-series-from-pandas-groupby-object"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have created a data frame my dataset looks like, I want to create a new column which will take one value from min then one value from max. If there are cont. 2 values of min/max (as we can see that 12 and 13 are 2 values) I have to consider only one value (consider only 12 and then move to select min) In short, new column should have one min value row, then one max value row and so on. OUTPUT should be", "q_apis": "get columns take value min value max values min max values value select min min value max value", "apis": "replace min min where min shift isna max max where max shift isna dropna all min combine_first max", "code": ["df = df.replace('Nan', np.nan)\n\ndf['min'] = df['min'].where(df['min'].shift().isna())\ndf['max'] = df['max'].where(df['max'].shift().isna())\ndf = df.dropna(how='all')\ndf['combined'] = df['min'].combine_first(df['max'])\n", "print(df)\n\n\n   min   max  combined\n0  NaN  10.0      10.0\n1  4.0   NaN       4.0\n2  NaN  12.0      12.0\n4  5.0   NaN       5.0\n7  NaN   8.0       8.0\n"], "link": "https://stackoverflow.com/questions/67649149/combine-2-columns-of-dataframe-based-on-a-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have been trying to get a count on multiple columns using value_counts. Right now, I have it working on one column, but not multiple. EDIT: I needed a count of unique IDs previously, hence the count on 'id', but now I want a count of the services under 'id'. I'm editing the data below to more accurately explain the situation. If I try I get a KeyError on service. If I try I get the same error. I'm hoping to get something along the lines of: Am I using the wrong function?", "q_apis": "get columns get count columns value_counts now count unique count now count get get get", "apis": "DataFrame", "code": ["import pandas as pd\n\nd = {'id': [1, 2, 1], 'service': [3, 4, 3], 'name': ['Joe', 'Bob', 'Mark']}\n\ndf = pd.DataFrame(d)\n", "res = df.groupby(['id', 'service']).count()\n\n#             name\n# id service      \n# 1  3           2\n# 2  4           1\n", "df['id_service'] = list(zip(df.id, df.service))\nres = df['id_service'].value_counts()\n\n# (1, 3)    2\n# (2, 4)    1\n# Name: id_service, dtype: int64\n"], "link": "https://stackoverflow.com/questions/49367161/using-value-counts-to-find-unique-values-across-combination-of-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have already looked up another similar q&a on the subject but I can not figure out my issue so I appreciate any hint. I have three data frames stored in the dictonary and another dataframe when I want to gather data. I would like to add another columne ['rates'] in dfA by picking up the values from the coresponding dataframes (via dict. mapping) that match ['tenor']. So the expected outcome would be like this: I know I can get particular data in the dataframe with this line (for instance): So I try to embed it into apply() function with the following code: But unfortunately it returns: I do not understand why some rates are NaN. What am I missing here? Please help.", "q_apis": "get columns any add values get apply", "apis": "merge drop", "code": ["dfAux = pd.concat([df1, df2, df3])", "dfA = pd.merge(dfA, dfAux, how = 'left', on = ['tenor']).drop(['end_date'], axis = 1)"], "link": "https://stackoverflow.com/questions/60083354/pandas-pickup-and-merge-data-from-n-dataframes-into-one-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How to group a dataframe by column name parts and then plot the pairs with one command? It should be the following groups: regards", "q_apis": "get columns name plot groups", "apis": "columns columns T groupby columns sum", "code": ["df.columns = list(map(lambda x:x.split('_')[1],df.columns))<br>\ndf.T.groupby(by=df.columns).sum()\n"], "link": "https://stackoverflow.com/questions/53003239/pandas-groupby-column-name-part"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes and want to create a third one based on dynamic calculations. df1 (have) df2 (have) df3(want) Want to create df3 in an efficient way. Each record in df2 indicates how to calculate df3 (i.e. for prod A, subtract value time=1 and time=3 from df1 and name this column as value(df1)_new_name(df2), for prod B, subtract time=1 and time=2 etc.) Currently I am able to create this by going line by line through df2 and creating multiple subsets of df1 and eventually concatenating them but this is taking a long time given df1 can get very large", "q_apis": "get columns prod value time time name value prod time time time get", "apis": "drop melt prod merge groupby prod apply iloc iloc prod", "code": ["df2.drop('new_name',1).melt('prod',value_name='time').\\\n      merge(df1,how='left').groupby('prod').value.apply(lambda x : x.iloc[0]-x.iloc[1])\nOut[177]: \nprod\nA   -20\nB    -5\nName: value, dtype: int64\n", "df2.melt(['prod','new_name'],value_name='time').\\\n      merge(df1,how='left').groupby(['prod','new_name']).value.apply(lambda x : x.iloc[0]-x.iloc[1]).unstack()\nOut[205]: \nnew_name  'newval'  'newval2'\nprod                         \nA              -20        -10\nB               -5        -65\n"], "link": "https://stackoverflow.com/questions/56779137/python-pandas-create-dataframe-based-on-dynamic-calculation"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe let's say: I would like to apply a function to the age column. Something like this : The problem that I am facing is an infinite loop. There seems to be is not updating for each iteration. Could be because of not returning the value after calculation but I have no idea how to do it. The actual dataset is complex but I tried to simplified. Hope it is easy to understand.", "q_apis": "get columns apply value", "apis": "apply", "code": ["def process_row(r):\n    foo = row[\"Age\"]\n    while foo < row[\"Certain Value\"]:\n        foo  = foo * 2 \n    return foo\n", "df[\"New_age\"] = df.apply(processrow, axis=1)\ndf[\"Age\"] = df[\"New_age\"]\n\n\n\n     \n"], "link": "https://stackoverflow.com/questions/63590263/pandas-apply-a-function-to-a-column-while-the-condition-is-valid"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas Series S: I also have a Dataframe df A, B, C do change, but they are not relevant for this question. I would like to have an output dataframe as follows: Basically I want to add a column to df, called Value, where Value is whatever value corresponds to the Date in series S, that is in column PS of df. The pseudocode would be df[\"Value\"] = S[df[PS]] I don't want to bring the Date column/index from the series over.", "q_apis": "get columns Series add where value index", "apis": "merge DataFrame columns loc", "code": ["new_df = df.merge(pd.DataFrame(s, columns=['Value']), right_index=True, left_on='PS', how='outer')\n", "df.loc[:, 'Value'] = [s[ps] for ps in df['PS']]\n"], "link": "https://stackoverflow.com/questions/45807714/merging-dataframe-with-series-based-on-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "New to Python here. I hope my question isn't entirely redundant - if it is, let me know and chalk it up to my inexperience with StackOverflow. In any case, I'm working with the Titanic dataset from kaggle.com, and I'm looking to use a set of conditional statements to replace NaN 'values' throughout the Age column of the dataframe. Ultimately, I'd like to generate results based on the following conditions: 1) if age==NaN, and Title==(X or Y or Z), generate a random number in the 0-18 range 2) if age==NaN, and Title==(A or B or C), generate a random number in the 19-80 range Note: 'Title' is a column with the title of individual listed (i.e. Mr., Mrs., Lord, etc.) I found a similar situation here, but I haven't been able to adapt it to my case as it doesn't approach conditionality at all. Here is my most recent attempt (per. the replies as this update) Attempt 1 Result is no error, but no correction to NaN values in 'Age' column", "q_apis": "get columns any replace values at all update values", "apis": "isnull loc loc apply", "code": ["import numpy as np\nimport pandas as pd\n\nmask1 = (df.Age.isnull()) & (df.Title == 'Master')\ndf.loc[mask1, 'Age'] = df.loc[mask1, 'Age'].apply(lambda x: np.random.randint(0,18))\n", "list1 = ['Master', 'Sir', 'Mr']\nmask1 = (df.Age.isnull()) & (df.Title.isin(list1))\n"], "link": "https://stackoverflow.com/questions/49376077/conditional-replacement-of-nan"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame that is indexed from 1 to 100000 and I want to calculate the slope for every 12 steps. Is there any rolling window for that? I did the following, but it is not working. The column is created, but all of the values as .", "q_apis": "get columns any rolling all values", "apis": "add rolling apply plot plot", "code": ["# add the slope column\ndf['slope'] = df['Close'].rolling(window=days_back, min_periods=days_back).apply(get_slope, raw=True)\n\n# plot slope\ndf.plot(y='slope')\n"], "link": "https://stackoverflow.com/questions/67971902/rolling-window-calculation-is-added-to-the-dataframe-as-a-column-of-nan"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to add rows from DF2 to DF1, only if they don't already exist in DF1, based on colA. Basically, don't add the row into DF1 if the column value of ColA already exists. this is what I have so far: The reason doesn't work well for me is that I need the duplicates in df1 to remain.", "q_apis": "get columns add add value", "apis": "isin", "code": ["new_df = pd.concat([df1, df2[~df2['colA'].isin(df1['colA'])]], ignore_index=True)\n", "   colA  colB   colC\n0    10   100  100.0\n1    10    20   20.0\n2    20   300  300.0\n3    30    54   67.0\n4    40   400    NaN\n5    50   500    NaN\n"], "link": "https://stackoverflow.com/questions/66707412/how-do-i-concat-dataframes-without-duplicates-however-keeping-duplicates-in-the"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes with 3 columns each My output dataframe after joining the above two dataframes I am only getting the 2 row values but not the first row How can I deal with these missing values scenarios with joins, let me know if you are not able to understand the question", "q_apis": "get columns columns values first values", "apis": "merge", "code": ["import pandas as pd\nimport numpy as np\ndfA = pd.DataFrame({\"X\":[1,4],\n                   \"Y\":[2,5],\n                   \"Z\":['',6]})\nprint(dfA)\n\ndfB = pd.DataFrame({\"X\":[1,4],\n                   \"Y\":[2,5],\n                   \"C\":['',6]})\n\nprint(dfB)\n\ndfC=dfA.merge(dfB, left_on=['X','Y','Z'], right_on=['X','Y','C'], how='left')\n\nprint(dfC)\n", "   X  Y  Z\n0  1  2   \n1  4  5  6\n   X  Y  C\n0  1  2   \n1  4  5  6\n   X  Y  Z  C\n0  1  2      \n1  4  5  6  6\n", "import pandas as pd\nimport numpy as np\ndfA = pd.DataFrame({\"X\":[1,4],\n                   \"Y\":[2,5],\n                   \"Z\":[np.nan,6]})\nprint(dfA)\n", "   X  Y    Z\n0  1  2  NaN\n1  4  5  6.0\n", "dfB = pd.DataFrame({\"X\":[1,4],\n                   \"Y\":[2,5],\n                   \"C\":[np.nan,6]})\n\nprint(dfB)\n", "   X  Y    C\n0  1  2  NaN\n1  4  5  6.0\n", "dfC=dfA.merge(dfB, left_on=['X','Y','Z'], right_on=['X','Y','C'], how='left')\n\nprint(dfC)\n", "   X  Y    Z    C\n0  1  2  NaN  NaN\n1  4  5  6.0  6.0\n"], "link": "https://stackoverflow.com/questions/63154160/how-to-to-make-a-join-in-data-frame-if-two-columns-have-empty-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am using the csv library to read specific rows from several files I have. The problem I am having is saving those rows into a dataframe. I am getting an indexing error that I can't solve. The current version of the code finds the column names (which is on the third row) and then starts finding the data I need (which starts on the sixth row and continues until it hits a blank row). Finding the column names works fine, but when I try to append the data to it, I get the error: \"InvalidIndexError: Reindexing only valid with uniquely valued Index objects\" The code I currently have is as follows: EDIT: A sample of the data is here: I want the output to be a dataframe with row 3 as the column names and row 6 until a blank row as the data filling the columns. For example:", "q_apis": "get columns names names append get Index sample names columns", "apis": "append append values at DataFrame columns iloc reindex index drop", "code": ["    import csv\n    import pandas as pd\n    temp = []  #initialize array\n    with open('C:/Users/sword/Anaconda3/envs/exceltest/RF_SubjP02_Free_STATIC_TR01.csv', 'r') as csvfile:\n         csvreader = csv.reader(csvfile, delimiter=',')\n         for row in csvreader:\n             if csvreader.line_num == 3:  \n                temp.append(row)     #gets column names and saves to array  \n             if csvreader.line_num >= 6:\n                if row: \n                     temp.append(row)  # gets data values and saves to array\n                else: #stops at blank row\n                     break\n    df = pd.DataFrame(temp) #creates a dataframe from an array\n    df.columns = df.iloc[0]  #make top row the column names\n    df.reindex(df.index.drop(1))\n    print(df)\n"], "link": "https://stackoverflow.com/questions/53275142/writing-specific-csv-rows-to-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe (df) and wish to obtain the largest counts of \"NCT_ID\" (not unique values only, but every occurrence) with respect to columns \"COUNTRY\" and \"CONDITION\". So that for each country in \"COUNTRY\", I will have the n (set n = 2 for simplicity) most common conditions in \"CONDITION\", sorted by largest. The df has the following structure (All columns vary in values, including \"COUNTRY\", this is just a small subset): Which you can load as follows: So I am hoping for an end result that will look something like the following: This final df should show the n most common conditions, in the n countries with largest overall counts (of count of conditions combined). What I have done so far: Following https://stackoverflow.com/a/17679517/7445528, I have experimented with: But this does not get the right dataframe result. To see the entire project so far: https://github.com/Gustav-Rasmussen/AACT-Analysis/tree/master", "q_apis": "get columns unique values columns columns values count get right", "apis": "groupby apply reset_index sort_values", "code": ["new_df = df.groupby(['CONDITION', 'COUNTRY']).apply(len).reset_index(name='COUNTS')\n\nnew_df.sort_values(by='COUNTS', axis=0, inplace=True, ascending=False)\n"], "link": "https://stackoverflow.com/questions/62371391/get-max-count-of-dataframe-rows-with-respect-to-2-groupings"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe (df) of messages that appears similar the following: I want to count the amount of times each email appears from a specific list. My list being: I'm hoping to receive a dataframe/series/dictionary with a result like this: I'm tried several different things, but haven't succeeded. I thought I could try something like the for loop below (it returns a Syntax Error), but I cannot figure out the right way to write it. Should this type of task be accomplished with a for loop or are there out of the box pandas methods that could solve this easier?", "q_apis": "get columns count right", "apis": "stack isin value_counts set_index reset_index melt isin value_counts", "code": ["v = pd.concat([df.From, df.To.str.split(', ', expand=True)], axis=1).stack()\nv[v.isin(lst)].value_counts()\n\nstranger2@gmail.com    2\nperson1@gmail.com      2\nperson3@gmail.com      1\ndtype: int64\n", "v = (df.set_index('From')\n      .To.str.split(', ', expand=True)\n      .reset_index()\n      .melt()['value']\n)\nv[v.isin(lst)].value_counts()\n\nstranger2@gmail.com    2\nperson1@gmail.com      2\nperson3@gmail.com      1\nName: value, dtype: int64\n"], "link": "https://stackoverflow.com/questions/50596383/value-counts-for-specific-items-in-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a function in python that returns five arrays. I'd like to turn those arrays into a dataframe . I'll omit the function body for brevity, but this is what my code looks like: And then when I run , I get all this code returned for variables . So, my question is two-fold: What is the technical term for the specific output of ? Is it a list of arrays or tuple of arrays.. or something else? How can I turn these arrays into one dataframe? Is that even possible or are they different lengths? For the second part, I've tried adding the line in , but then I get the error; . Help greatly appreciated. I'm sorry if this is a horrendous question: I don't really even know where to start.", "q_apis": "get columns get all fold second get where start", "apis": "DataFrame", "code": ["def runmodel():\n   ## omitting code for brevity\n   result =  pd.DataFrame({\n    'Title for a': a,\n    'Title for b': b,\n    'Title for c': c,\n    'Title for d': d,\n    'Title for e': e,\n    })\n   return result\n"], "link": "https://stackoverflow.com/questions/56856504/save-tuple-of-arrays-as-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There are 2 CSV files. Each file has 700,000 rows. I should read one file line by line and find the same row from the other file. After then, make two files data as one file data. But, It takes about 1 minute just per 1,000 rows!! I don't know how to improve the performance. Here is my code : Which line can be changed? Is there any other way to do this process?", "q_apis": "get columns minute any", "apis": "merge", "code": ["result_df = pd.merge(left=file1_df, right=file2_df, on=['Name', 'Age'], how='inner')\n"], "link": "https://stackoverflow.com/questions/46373907/python-how-to-improve-the-dataframe-performance"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want read csv's from different sub-directories in my working directory to create a combined csv file. The combined csv should have a column containing the sub-directory name from which that particular csv was read from. This is what I tried. The problem is that, it adds sub-directories that does not have csvs' in them, which is wrong and problematic. What is the best way to this right.", "q_apis": "get columns sub sub name sub right", "apis": "append to_csv index", "code": ["import os\nimport glob\nimport pandas as pd\n\nall_filenames = [i for i in glob.glob(\"**/*.csv\", recursive=True)]\n\ndf_list = []\nfor f in all_filenames:\n    current_csv = f\n    data = pd.read_csv(current_csv)\n    data[\"sub_folder\"] = os.path.split(f)[0]    # <-- [0] is directory [1] is filename\n    df_list.append(data)\n\ncombined_df = pd.concat(df_list)\nprint(combined_df)\ncombined_df.to_csv(\"combined_csv.csv\", index=False)\n"], "link": "https://stackoverflow.com/questions/67350952/search-csv-from-subdirectory-and-add-folder-name-as-a-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes: df1: df2 I am trying to merge these data frames on and . I want to append df1 onto df2 and input the values if there is a match. If there is no match, NaN is fine. So, with that logic in mind, I have written a merge: Now, the resulting merge is almost exactly what I need. I'm getting the values, but they're overwriting existing columns. df1_plus_df2: If you see in , the values have overwritten the values in the previous 2 columns. I also did not get the third column from df1 to carry over. Overall, the weird thing here is I cannot understand why the columns get appended, but then the values overwrite existing df2 values. Here's the output I NEED, and I'm not sure what I'm doing wrong to not get it: df1_plus_df2: I would really appreciate some help. Thanks!", "q_apis": "get columns merge append values merge merge values columns values values columns get columns get values values get", "apis": "DataFrame DataFrame merge", "code": ["df1 = pd.DataFrame({\"ID CODE\": [11526.0, 11527.0], \"CD1\" : [\"1A\", \"1C\"], \"CD2\": [\"NWZ\", \"NWZ\"]})\ndf2 = pd.DataFrame({\"CD_Code\": [\"Mal3\", \"Mal2\"], \"CID_CODE\": [11529, 11526], \"OC_NME\": [\"6A\", \"6B\"], \"OC_CDE\": [\"Main Area\", \"Side Area\"]})\n\ndf3 = pd.merge(df2, df1, left_on=['CID_CODE'], right_on=['ID CODE'], how='left')\n", "    CD_Code CID_CODE    OC_NME      OC_CDE      ID CODE   CD1   CD2\n0   Mal3    11529       6A          Main Area   NaN       NaN   NaN\n1   Mal2    11526       6B          Side Area   11526.0   1A    NWZ\n"], "link": "https://stackoverflow.com/questions/59494970/pandas-merge-causing-data-to-go-to-wrong-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to delete the row if there is data missing in a certain column in the current row. This is what I wrote: But I receive the error: \"unreadable key error\"", "q_apis": "get columns delete", "apis": "drop index", "code": ["data = data.drop(data[data['A'] == ''].index)\n"], "link": "https://stackoverflow.com/questions/61278504/delete-a-row-in-pandas-dataframe-while-using-df-iterrows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to download multiple stocks dividend amounts and respective dates using yfianance package and trying to save it in a python Data Frame. I am using following code but what I get is : if I change the stock position in the stock_list as (AAPL first and MSFT last) I get this: I think the Data Frame set dates for the first stock and as it is not neccassry that the subsequent stock paid dividends on the same dates it is shown as NaN. I would appreciate any help to get all dividends in a given period for a long list of stocks. Thanks", "q_apis": "get columns get first last get first any get all", "apis": "DataFrame loc columns", "code": ["import yfinance as yf\nimport pandas as pd\n\ndata = pd.DataFrame()\nstock_list = ['AAPL', 'MSFT','GOOG']\n\nstart = '2019-10-1'\nend = '2020-10-30'\n\nfor i in stock_list:\n    series = yf.Ticker(i).dividends.loc[start:end]\n    data = pd.concat([data, series], axis=1)\ndata.columns = stock_list\n"], "link": "https://stackoverflow.com/questions/64814357/dividend-rates-and-dates-for-multiple-stocks-at-once-using-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This seems like it should be so much simpler yet here I am. I'm trying to add a row to a data frame (2 data frames to be exact) from another data frame, but I get the following error: My code Do I have to transform into another data frame? This seems horribly inefficient EDIT: I tried but that appended the data as a new column, example for :", "q_apis": "get columns add get transform", "apis": "DataFrame first columns first DataFrame columns", "code": ["X = pd.concat([X,pd.DataFrame([[data['first occurrence of \\'AB\\''],data['similarity to \\'AB\\'']]],columns=['first occurrence of \\'AB\\'','similarity to \\'AB\\''])], ignore_index=True)\ny = pd.concat([y,pd.DataFrame([data['Class']], columns=['Class'])], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/50643118/concatenating-a-row-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have following small dataframe: I use following code to plot these data as lines: And I get following figure: However, I want figure in black/white/gray colors. How can I change above code to get this color scheme? Different lines can be shown as of different linestyles.", "q_apis": "get columns plot get get", "apis": "DataFrame columns columns plot", "code": ["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom cycler import cycler\n\ndf = pd.DataFrame(np.array(range(20)).reshape(2,10), columns=list('ABCDEFGHIJ'))\n\nfix,ax = plt.subplots()\nax.set_prop_cycle(\n    cycler(color=[(.8,.8,.8), (.5,.5,.5), (.2,.2,.2)]) *\n    cycler(linestyle=['-', '--', '-.'])\n    ) #repeats after 9 columns\n\ndf.plot(ax=ax)\n"], "link": "https://stackoverflow.com/questions/62500859/how-to-have-black-white-gray-shade-lines-in-pandas-dataframe-plot"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe on which I do one hot encoding using method. Here is the sample code - Here is how I do one hot encoding The problem is, that when I get a record with an unknown categorical value, I dont know how to best handle it - If i again do on this new dataframe, then I get something like whereas my expected output is because category d was never seen before in the first place, so I want to neglect it by making all flags of columns as 0. How can I achieve this in pandas?", "q_apis": "get columns sample get value get first all flags columns", "apis": "reindex columns", "code": ["y_transformed = y_transformed.reindex(X_transformed.columns, axis=1, fill_value=0)\n", "  category  a  b  c\n0        a  1  0  0\n1        d  0  0  0\n"], "link": "https://stackoverflow.com/questions/63242667/how-to-handle-unknown-categorical-value-in-one-hot-encoding-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What works I managed to get data from a hmtl table via like so: Found the div id via Chrome dev tools: What does not work Now trying to get the data from a different URL, but without success. The URL is this: https://coinmarketcap.com/currencies/bitcoin/historical-data/?start=20130428&end=20190410 The table is in this div: My code is this: What am I missing? Edit Obviously there is no tag in this div I am interested in: Is that the cause of the error? If so, how else can I get the data that is within that div? Edit 2 I know that coinmarketcap.com has an API, but I rather prefer getting the data from their website.", "q_apis": "get columns get div get start div div get div", "apis": "attrs", "code": ["df_in_list = pd.read_html(URL, attrs = {'class': 'table'})", "df = df[['#', 'Name', 'Symbol', 'Market Cap', 'Price' ]]"], "link": "https://stackoverflow.com/questions/55620196/getting-html-table-via-pandas-read-html-wont-work"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here's a piece of the dataset I'm working on : What I'm trying to do is to extract the number of participations of each actor in a movie/tv-show. I created a couple of functions to separate the names of actors from the data : In order to count the number of participations of actors, I used a for loop as follows : But it just takes too much time since it iterates 32882 times. Iterative methods usually take a lot of time, is there any other efficient approach that could take less time in this case ?", "q_apis": "get columns names count time take time any take time", "apis": "explode value_counts mean std fillna mean std mean std", "code": ["In [8]: df\nOut[8]:\n      type  title           director                                               cast\n0  TV Show     3%                NaN  Jo\u00e3o Miguel, Bianca Comparato, Michel Gomes, R...\n1    Movie   7:19  Jorge Michel Grau  Demi\u00e1n Bichir, H\u00e9ctor Bonilla, Oscar Serrano, ...\n2    Movie  23:59       Gilbert Chan  Tedd Chan, Stella Chung, Henley Hii, Lawrence ...\n3    Movie      9        Shane Acker  Elijah Wood, John C. Reilly, Jennifer Connelly...\n4    Movie     21     Robert Luketic  Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...\n", "In [9]: from collections import Counter\n\nIn [10]: Counter(\n    ...:     actor.strip()\n    ...:     for cast in df['cast'].fillna('missing')\n    ...:     for actor in cast.split(',')\n    ...: )\nOut[10]:\nCounter({'Jo\u00e3o Miguel': 1,\n         'Bianca Comparato': 1,\n         'Michel Gomes': 1,\n         'Rodolfo Valente': 1,\n         'Vaneza Oliveira': 1,\n         'Rafael Lozano': 1,\n         'Viviane Porto': 1,\n         'Mel Fronckowiak': 1,\n         'Sergio Mamberti': 1,\n         'Zez\u00e9 Motta': 1,\n         'Celso Frateschi': 1,\n         'Demi\u00e1n Bichir': 1,\n         'H\u00e9ctor Bonilla': 1,\n         'Oscar Serrano': 1,\n         'Azalia Ortiz': 1,\n         'Octavio Michel': 1,\n         'Carmen Beato': 1,\n         'Tedd Chan': 1,\n         'Stella Chung': 1,\n         'Henley Hii': 1,\n         'Lawrence Koh': 1,\n         'Tommy Kuan': 1,\n         'Josh Lai': 1,\n         'Mark Lee': 1,\n         'Susan Leong': 1,\n         'Benjamin Lim': 1,\n         'Elijah Wood': 1,\n         'John C. Reilly': 1,\n         'Jennifer Connelly': 1,\n         'Christopher Plummer': 1,\n         'Crispin Glover': 1,\n         'Martin Landau': 1,\n         'Fred Tatasciore': 1,\n         'Alan Oppenheimer': 1,\n         'Tom Kane': 1,\n         'Jim Sturgess': 1,\n         'Kevin Spacey': 1,\n         'Kate Bosworth': 1,\n         'Aaron Yoo': 1,\n         'Liza Lapira': 1,\n         'Jacob Pitts': 1,\n         'Laurence Fishburne': 1,\n         'Jack McGee': 1,\n         'Josh Gad': 1,\n         'Sam Golzari': 1,\n         'Helen Carey': 1,\n         'Jack Gilpin': 1})\n", "In [17]: %timeit df['cast'].str.split(', ').explode('cast').value_counts()\n667 \u00b5s \u00b1 16.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nIn [18]: %%timeit\n    ...: Counter(\n    ...:     actor.strip()\n    ...:     for cast in df['cast'].fillna('missing')\n    ...:     for actor in cast.split(',')\n    ...: )\n    ...:\n    ...:\n146 \u00b5s \u00b1 2.76 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n", "In [28]: %%timeit\n    ...: Counter(\n    ...:     actor\n    ...:     for cast in df['cast']\n    ...:     for actor in cast.split(', ')\n    ...: )\n    ...:\n    ...:\n    ...:\n12.9 \u00b5s \u00b1 543 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n"], "link": "https://stackoverflow.com/questions/66917539/count-the-occurrences-of-many-elements-in-a-dataframe-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe and some columns. I want to sum column \"Gap\" where time is in some time slots. I want to sum gap column. I have time slots in dict like that. Now after summation, above dataframe should like that. I have many regions and 144 time slots from 00:00:00 to 23:59:49. I have tried this. But it doesn't work.", "q_apis": "get columns columns sum where time time sum time time", "apis": "values", "code": ["d = {'slot1': '00:00:00', 'slot2': '00:10:00', 'slot3': '00:20:00'}\nd1 = {v:k for k, v in d.items()}\n\ndf['time'] = pd.to_datetime(df['time']).dt.floor('10Min').dt.strftime('%H:%M:%S')\nprint (df)\n   region        date      time  gap\n0       1  2016-01-01  00:00:00    1\n1       1  2016-01-01  00:00:00    0\n2       1  2016-01-01  00:00:00    1\n3       1  2016-01-01  00:00:00    0\n4       1  2016-01-01  00:10:00    0\n5       1  2016-01-01  00:10:00    1\n6       1  2016-01-01  00:10:00    0\n7       1  2016-01-01  00:10:00    0\n", "regres = df.groupby(['region','date','time'], as_index=False)['gap'].sum()\nregres['time'] = regres['time'] + '/' + regres['time'].map(d1)\nprint (regres)\n   region        date            time  gap\n0       1  2016-01-01  00:00:00/slot1    2\n1       1  2016-01-01  00:10:00/slot2    1\n", "d = {'slot1': '00:00:00', 'slot2': '00:10:00', 'slot3': '00:20:00'}\nd1 = {v:k for k, v in d.items()}\n\ntimes = pd.to_datetime(df['time']).dt.floor('10Min')\ndf['time'] = times.dt.strftime('%H:%M:%S')\ndf['time1'] = times.add(pd.Timedelta('10Min')).dt.strftime('%H:%M:%S')\nprint (df)\n   region        date      time  gap     time1\n0       1  2016-01-01  00:00:00    1  00:10:00\n1       1  2016-01-01  00:00:00    0  00:10:00\n2       1  2016-01-01  00:00:00    1  00:10:00\n3       1  2016-01-01  00:00:00    0  00:10:00\n4       1  2016-01-01  00:10:00    0  00:20:00\n5       1  2016-01-01  00:10:00    1  00:20:00\n6       1  2016-01-01  00:10:00    0  00:20:00\n7       1  2016-01-01  00:10:00    0  00:20:00\n\nregres = df.groupby(['region','date','time','time1'], as_index=False)['gap'].sum()\nregres['time'] = regres.pop('time1') + '/' + regres['time'].map(d1)\nprint (regres)\n   region        date            time  gap\n0       1  2016-01-01  00:10:00/slot1    2\n1       1  2016-01-01  00:20:00/slot2    1\n", "df['time'] = pd.to_timedelta(df['time'])\n\nbins = pd.timedelta_range('00:00:00', '24:00:00', freq='10Min')\nlabels = np.array(['{}'.format(str(x)[-8:]) for x in bins])\nlabels = labels[:-1]\n\ndf['time1'] = pd.cut(df['time'], bins=bins, labels=labels)\ndf['time11'] = labels[np.searchsorted(bins, df['time'].values) - 1]\n"], "link": "https://stackoverflow.com/questions/53756516/sum-a-column-based-on-groupby-and-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Tags dataframe that has 2 columns \"Date\" & \"Tag\", there is multiple same dates and multiple of the same tag. I need to sort the table so that it shows a unique date alongside a unique tag and the count of how many times that tag occurred that month. Any ideas how I could do this? Please see below a screenshot of the current table.", "q_apis": "get columns columns unique date unique count month", "apis": "DataFrame sample groupby nunique", "code": ["import pandas as pd\ndf = pd.DataFrame({'Date': ['Nov','Nov','Dec'],\n                   'Name':[1,2,1]}) # sample dataframe\n", "df.groupby('Date')['Name'].nunique()\n    Date\nDec    1\nNov    2\n", "df['count'] = df.groupby('Date')['Name'].transform('nunique')\n      Date  Name  count\n0  Nov     1      2\n1  Nov     2      2\n2  Dec     1      1\n"], "link": "https://stackoverflow.com/questions/59033195/sorting-a-dataframe-by-month-and-grouping-by-a-count"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with three columns ID, Col1, and Col2. I want to group the df by ID and Col1 and remove all values of Col1 which are lower than the value in Col2. The end result should like this I can do it by iterating over the dataframe and splitting it into chunks, but there must be a simpler and faster way to this with Pandas functions.", "q_apis": "get columns columns all values value", "apis": "loc ge loc", "code": ["df1 = df.loc[df['Col1'].ge(df['Col2']), ['ID', 'Col1']]\n#alternative\n#df1 = df.loc[df['Col1'] >= df['Col2'], ['ID', 'Col1']]\n", "print (df1)\n\n  ID  Col1\n1  A     1\n2  A     2\n3  A     4\n6  B     2\n"], "link": "https://stackoverflow.com/questions/51498873/python-pandas-group-by-and-exclude-values-based-on-another-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i am looking to add a new column and multiply the number of lines in my df for a given series (ex..1,2,3,4) for a given input that looks like below output should be below is there any ways to achieve this in pandas", "q_apis": "get columns add any", "apis": "assign sort_values columns", "code": ["df1 = pd.concat([df.assign(Quater=i) for i in range(1, 5)])\ndf1 = df1.sort_values(by=df1.columns.tolist(), ignore_index=True)\n", "# print(df1)\n     Name  kills  saves  health  energy  Quater\n0   bruce      0     60      10      10       1\n1   bruce      0     60      10      10       2\n2   bruce      0     60      10      10       3\n3   bruce      0     60      10      10       4\n4   clark     25    100    1000      10       1\n5   clark     25    100    1000      10       2\n6   clark     25    100    1000      10       3\n7   clark     25    100    1000      10       4\n8   diana     15     80     100      10       1\n9   diana     15     80     100      10       2\n10  diana     15     80     100      10       3\n11  diana     15     80     100      10       4\n"], "link": "https://stackoverflow.com/questions/62532260/expand-rows-based-using-a-series-input-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Refer yellow highlighted cells: If K = LDE then look for FDE in column J (above LDE's row), in Result column return (D from LDE minus A from FDE) (ie 223-307 = -84) Refer green highlighted cells: 152-385 = -233 and so on. How to solve ? Data:", "q_apis": "get columns", "apis": "cumsum shift loc size loc iloc loc iloc get groupby apply loc", "code": ["import pandas as pd\n\n# define groups between two LDE\ndf['Group'] = (df['K'] == 'LDE').cumsum().shift(1, fill_value=0)\n\n# custom function to perform your subtraction\ndef f(x):\n    if x.loc[x['J'] == 'FDE', 'A'].size == 0:\n        return None\n    else:\n        return x.loc[x['K'] == 'LDE', 'D'].iloc[0] - x.loc[x['J'] == 'FDE', 'A'].iloc[0]\n\n# get list of numerical results\nresults = df.groupby('Group').apply(f).tolist()\n\n# input the list into the specified LDE rows\ndf.loc[df['K'] == 'LDE', 'Results'] = results\n", "df = pd.DataFrame([['03-01-2011', 523, 698, 284, 33, 416, 675, 300, 690, 314, '', '', 'FDM', ''], ['27-01-2011', 353, 1, 50, 547, 514, 957, 804, 490, 108, '', 'LDE', '', ''],\n                   ['28-01-2011', 307, 837, 656, 755, 792, 568, 119, 439, 943, 'FDE', '', '', ''], ['31-01-2011', 327, 409, 155, 358, 120, 401, 385, 965, 888, '', '', '', 'LDM'], ['01-02-2011', 686, 313, 714, 12, 140, 112, 589, 908, 605, '', '', 'FDM', ''], ['24-02-2011', 161, 846, 816, 223, 387, 566, 435, 567, 36, '', 'LDE', '', ''], ['25-02-2011', 889, 652, 190, 324, 947, 778, 575, 604, 314, 'FDE', '', '', ''], ['28-02-2011', 704, 33, 232, 630, 344, 796, 331, 409, 597, '', '', '', 'LDM'], ['01-03-2011', 592, 148, 974, 540, 848, 393, 505, 699, 315, '', '', 'FDM', ''], ['31-03-2011', 938, 768, 325, 756, 971, 644, 546, 238, 376, '', 'LDE', '', 'LDM'], ['01-04-2011', 385, 298, 654, 655, 2, 112, 960, 306, 477, 'FDE', '', 'FDM', ''], ['28-04-2011', 704, 516, 785, 152, 355, 348, 106, 611, 426, '', 'LDE', '', ''], ['29-04-2011', 753, 719, 776, 826, 756, 370, 660, 536, 903, 'FDE', '', '', 'LDM'], ['02-05-2011', 222, 28, 102, 363, 952, 860, 48, 976, 478, '', '', 'FDM', ''], ['26-05-2011', 361, 588, 866, 884, 809, 662, 801, 843, 668, '', 'LDE', '', '']],\n                  columns=['Date'] + list(map(chr, range(65, 78))))\n"], "link": "https://stackoverflow.com/questions/68195637/if-two-columns-satisfy-a-condition-return-calculated-value-from-other-columns-p"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to compare 2 dataframe rows to eachother. When I try to run my code I get this error: TypeError: string indices must be integers. this is my code: thanks in advance.", "q_apis": "get columns compare get indices", "apis": "index iterrows", "code": ["for index,item in data.iterrows():\n    if item[\"comment text\"] != item[\"original text\"]:\n        counter1 += 1\n"], "link": "https://stackoverflow.com/questions/60490366/typeerror-string-indices-must-be-integers-with-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to populate the values in a new column in a Pandas df by subtracting the value of two non-consecutive rows in a different column within the same df. I can do it, so long as the df does not have a column with dates in it. But if it does have a column with dates then pandas throws an error. Assume the following dataframe. This gives me a df that looks like this: I am trying to create a new column 'd' that, for each row, subtracts the value in column 'b' two rows below from the row in question. For instance, the value in row [0], column ['d'] would be calculated as df.loc[2]['b'] - df.loc[0]['b']. What I'm trying (which doesn't work) is: I can get this to work if I have no date in the df. But when I add a column with dates, it throws an error message saying I can't figure out why a date column causes the df to be unable to do math on columns with only int64 data. I've tried searching this site and just can't seem to solve the problem. Any help would be greatly appreciated.", "q_apis": "get columns values value value value loc loc get date add date columns", "apis": "shift", "code": ["df['d'] = df['b'].shift(-2) - df['b']\ndf\n", "        Date   a   b   c     d\n0 2020-01-02   1   2   3  53.0\n1 2020-01-05   4   5   6  94.0\n2 2020-06-10   7  55   9 -17.0\n3 2020-08-05  10  99  19 -89.0\n4 2020-09-01  27  38  29   NaN\n5 2020-10-29  39  10  72   NaN\n"], "link": "https://stackoverflow.com/questions/66481302/python-pandas-how-to-subtract-values-in-two-non-consecutive-rows-in-a-specific"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a huge set of data and a dummy is shown here Here, events is like a cycle . It can start with any event , stop at any event , but always ends with event-final. I get the sequence based on time. What I want to do here is keep only incomplete cycle records . For example , In above shown image , event-1 to event-final (first 4 rows) indicate that the cycle was complete , hence i need to remove all rows that are present before 'final-event' and I need only rows containing events after 'final-event' (in above pic case its event-2). There are about 20 events that can occur in any order for a particular ID . So what i want to do is just delete off all the rows before final-event . I have time column to get the sequence. My idea is to sort in descending order based on time and delete off rows after 'final-event'. But i am not sure how to do it in pandas. Can someone help in this? Also is there a better approach to this other than my idea with the given data? Edited to post the group by code (for @Joe Ferndz): So, this is what I tried. I sorted in descending order based on time and then groupby on ID . Then in the remove_cycle I find out the index of the time for which the event is 'event-final'. Then I return back only rows having greater time column value. This serves the purpose but it is slow.", "q_apis": "get columns start any stop at any get time first all any delete all time get time delete time groupby index time time value", "apis": "DataFrame T values groupby cumsum values eq astype groupby cumsum eq DataFrame T sort_values reset_index drop index all get last index iloc loc get all last loc", "code": ["import pandas as pd\ndf = pd.DataFrame({'ID':['001']*10 + ['002']*10,\n                   'Event':['event-1','event-2','event-3','event-final','event-1',\n                            'event-2','event-3','event-final','event-1','event-2',\n                            'event-1','event-2','event-3','event-final','event-1',\n                            'event-2','event-final','event-1','event-2','event-3'],\n                   'time':pd.date_range('2021-03-22 09:00:00', periods=20, freq=\"T\")\n                })\n\n#converting time to string format to match your data\ndf['time'] = df['time'].dt.strftime(\"%H:%M\")\n\n#checking for values of 'event-final' and reversing the dataframe to find groupby cumsum\n#A value of 0 indicates that its after 'event-final'\n#Picking those values will give you the desired results\n\nprint (df[df.Event.eq('event-final')[::-1].astype(int).groupby(df.ID).cumsum().eq(0)])\n\nprint (df)\n", "     ID    Event   time\n8   001  event-1  09:08\n9   001  event-2  09:09\n17  002  event-1  09:17\n18  002  event-2  09:18\n19  002  event-3  09:19\n", "     ID        Event   time\n0   001      event-1  09:00\n1   001      event-2  09:01\n2   001      event-3  09:02\n3   001  event-final  09:03\n4   001      event-1  09:04\n5   001      event-2  09:05\n6   001      event-3  09:06\n7   001  event-final  09:07\n8   001      event-1  09:08\n9   001      event-2  09:09\n10  002      event-1  09:10\n11  002      event-2  09:11\n12  002      event-3  09:12\n13  002  event-final  09:13\n14  002      event-1  09:14\n15  002      event-2  09:15\n16  002  event-final  09:16\n17  002      event-1  09:17\n18  002      event-2  09:18\n", "import pandas as pd\ndf = pd.DataFrame({'ID':['001']*10,\n                   'Event':['event-1','event-2','event-3','event-final','event-1',\n                            'event-2','event-3','event-final','event-1','event-2'],\n                   'time':pd.date_range('2021-03-22 09:00:00', periods=10, freq=\"T\")})\n\n#converting time to string format to match your data\n\ndf['time'] = df['time'].dt.strftime(\"%H:%M\")\n\n#sorting time in ascending order (assume this is within same day\n#if date goes beyond 24 hrs, then you should keep df['time'] in datetime format\n\ndf = df.sort_values(by='time').reset_index(drop=True)\n\nprint (df)\n\n#find out the index of all events that have `event-final`\n#and get only the last one using [-1]\n\nidx = df.index[df['Event']=='event-final'][-1]\n\n#using iloc or loc, you can get all records after the last `event-final` row\nprint (df.loc[idx+1:])\n", "    ID        Event   time\n0  001      event-1  09:00\n1  001      event-2  09:01\n2  001      event-3  09:02\n3  001  event-final  09:03\n4  001      event-1  09:04\n5  001      event-2  09:05\n6  001      event-3  09:06\n7  001  event-final  09:07\n8  001      event-1  09:08\n9  001      event-2  09:09\n", "    ID    Event   time\n8  001  event-1  09:08\n9  001  event-2  09:09\n"], "link": "https://stackoverflow.com/questions/66811577/filter-certain-rows-in-data-frame-based-on-time"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've got a dataframe containing country names & their percentage of energy output. I need to add a new column that assigns a 1 or 0, based on whether the country's energy output is above or below the median of energy output. Some dummy code is: the code returns ValueError: Wrong number of items passed 2, placement implies 1 I feel like this is an incredibly simple fix but I'm new to working with . Please help end my frustration", "q_apis": "get columns names add median items", "apis": "DataFrame median astype values median astype", "code": ["df = pd.DataFrame({\n    'name':['china', 'america', 'canada'],\n    'output': [33.2, 15.0, 5.0]\n})\n", "df['newcol'] = (df['output'] > df['output'].median()).astype(int)\n", "o = df['output'].values\ndf['newcol'] = (o > np.median(o)).astype(int)\n"], "link": "https://stackoverflow.com/questions/46230285/pandas-dataframe-assign-1-0-values-based-on-other-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I builded a dataframe to save the stocks consituent of a stock index over time with the following steps: 1)First, I download the raw data through a data provider and store in a dict 2)Transform into a dataframe to obtain: 3)Tranform into a boolean dataframe with: From there, I'have been trying to find a way to quickly update my table. To do so, I'd need to reconvert back constituent_bin to its original dictionary form, merge it with a new dictionart (for more recent dates) and restart the entire process. and I don't know how to reshape this long dataframe like constituent_pd to obtain a dic later. Thank you for any help!", "q_apis": "get columns index time update merge any", "apis": "stack max astype bool index", "code": ["print (constituent_pd)\n          col1    col2    col3\nindex                         \n1/1/92  stockA  stockB     NaN\n2/1/92  stockB  stockC  stockD\n", "print (pd.get_dummies(constituent_pd.stack()))\n             stockA  stockB  stockC  stockD\nindex                                      \n1/1/92 col1       1       0       0       0\n       col2       0       1       0       0\n2/1/92 col1       0       1       0       0\n       col2       0       0       1       0\n       col3       0       0       0       1\n\nprint (pd.get_dummies(constituent_pd.stack()).max(level=0))\n        stockA  stockB  stockC  stockD\nindex                                 \n1/1/92       1       1       0       0\n2/1/92       0       1       1       1\n", "constituent_bol = pd.get_dummies(constituent_pd.stack()).max(level=0).astype(bool)\nprint (constituent_bol)\n        stockA  stockB  stockC  stockD\nindex                                 \n1/1/92    True    True   False   False\n2/1/92   False    True    True    True\n", "step1 = constituent_bol.astype('int32')\nstep2 = step1[step1 == 1].stack().reset_index().drop(0,1)\nstep2 = step2.set_index(['index', step2.groupby('index').cumcount()])['level_1'].unstack()\nprint (step2)\n             0       1       2\nindex                         \n1/1/92  stockA  stockB     NaN\n2/1/92  stockB  stockC  stockD\n"], "link": "https://stackoverflow.com/questions/55140871/pandas-retrieve-original-dataframe-from-transformed-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe which has X, Y, and Z columns which represents coordinates of a points (each row represents one ponint). I want to draw vector from point 1 (row 1) to point 2 (row 2). I want to repeat such thing for entire dataframe. Here is how dataframe looks, The code I tried is here, it is giving mw vector but tail of the vector should be at point 1 (row 1) and head of the vector should be at point 2 (row 2) and so on. x, y, and z are the list containing each column.", "q_apis": "get columns columns repeat tail at head at", "apis": "T values shift T values shift shift shift", "code": ["import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(df.x, df.y, df.z, color = 'r', marker = 'o', alpha = 0.5)\n\nax.quiver(*df[:-1].T.values, *(df.shift(-1)-df)[:-1].T.values , length=1)\n# equivalent to\n#ax.quiver(df.x[:-1], df.y[:-1], df.z[:-1], \n#          (df.x.shift(-1)-df.x)[:-1], \n#          (df.y.shift(-1)-df.y)[:-1], \n#          (df.z.shift(-1)-df.z)[:-1], length = 1) \n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.show()\n"], "link": "https://stackoverflow.com/questions/53148758/plotting-vectors-from-dataframe-using-matplotlib"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The original dataframe is a table like this: I am looking for the smallest value in each column of a dataframe greater than zero. I was trying to use this example to answer my question. My code looks like: but still I get only zeros and my result looks like this: instead of this: I guess there might be a problem in data types but I'm not sure. I assumed would ignore zeros but it doesn't so I am confused why. And perhaps there's a more intelligent way to find what I need.", "q_apis": "get columns value get", "apis": "gt min first gt min where gt min values T copy min replace min DataFrame index columns index columns copy at div min T plot", "code": ["df = pd.DataFrame([[0, 0, 0],\n                   [0, 10, 0],\n                   [4, 0, 0],\n                   [1, 2, 3]],\n                  columns=['first', 'second', 'third'])\n", "df[df.gt(0)].min(0)\n\nfirst     1.0\nsecond    2.0\nthird     3.0\ndtype: float64\n", "def chris():\n    df1[df1.gt(0)].min(0)\n\ndef chris2():\n    df1.where(df1.gt(0)).min(0)\n\ndef wen():\n    a=df1.values.T\n    a = np.ma.masked_equal(a, 0.0, copy=False)\n    a.min(1)\n\ndef haleemur():\n    df1.replace(0, np.nan).min()\n", "from timeit import timeit\nimport matplotlib.pyplot as plt\n\nres = pd.DataFrame(\n       index=['chris', 'chris2', 'wen', 'haleemur'],\n       columns=[10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000],\n       dtype=float\n)\n\nfor f in res.index: \n    for c in res.columns:\n        df1 = df.copy()\n        df1 = pd.concat([df1]*c)\n        stmt = '{}()'.format(f)\n        setp = 'from __main__ import df1, {}'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=50)\n\nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(\"N\"); \nax.set_ylabel(\"time (relative)\");\n\nplt.show()\n"], "link": "https://stackoverflow.com/questions/51481884/min-value-in-each-column-of-a-data-frame-excluding-zeros"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I created a default dictionary from a large amount of data which has values as a list as previewed below. The default_dictionary values are represented as lists in the default dictionary. I have another data_dictionary that has the individual list values from the default_dictionary as keys. The ordering in the data_dictionary is (key_ID : [text_values], total, guser_ID). The data dictionary has the form : The second option (sum) in the values list is the number that I wish to use to compare the different keys. It is a sum amount. I would like the key_ID with the least sum to be shown first in a CSV file with the IDs that have greater sum showing next and so on as shown below. In words : (key_ID(least sum); key_ID ; sum for (least sum) key_ID ; sum for other key _Id ; shared text) So far, I was trying to use a dictionary to build the values and print as a csv using pandas but have not had much success. Any ideas would really help. This code provides every text with its own individual csv file of the key_IDs that share that text.", "q_apis": "get columns values values values keys second sum values compare keys sum sum first sum sum sum sum sum values", "apis": "items", "code": ["output_dict = {textval: sorted(\n                          [[key_ID, data_dictionary[key_ID][1]]\n                          for key_ID in default_dict[textval]\n                          if key_ID in data_dictionary],\n                        key=lambda x: x[1])\n               for textval in default_dict}\n\nfor textval, entries in output_dict.items():\n    list_for_output = entries if len(entries) == 1 else entries[1:]\n    for item in list_for_output:\n        print('%d ; %d ; %d ; %d ; %s' % (entries[0][0], item[0],\n        entries[0][1], item[1], list(textval)))\n"], "link": "https://stackoverflow.com/questions/53579575/how-can-i-get-corresponding-list-values-in-one-dictionary-from-another-python-di"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following data: The time to failure data is counted from a reference day 0 and I would like to turn it into the time since the previous failure: I tried using groupby and pivoting duplicate rows into new columns for the subtraction. However, I would like to do it in place to preserve other columns.", "q_apis": "get columns time day time groupby columns columns", "apis": "groupby diff fillna", "code": ["df['time_to_failure'] = (df.groupby('machine_id')\n                            ['time_to_failure'].diff(-1)\n                           .fillna(df['time_to_failure'])\n                        )\n", "   machine_id  time_to_failure\n0      430494            300.0\n1      430494            200.0\n2      430494            400.0\n3      430494            100.0\n4      430495            800.0\n5      430495            200.0\n"], "link": "https://stackoverflow.com/questions/64382404/how-to-find-the-difference-between-rows-of-cumulative-counts-while-retaining-col"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am new to Python and looking for help to multiply 2 dataframes over time. Any help to understand the error would be highly appreciated. First DataFrame (cov) Second DataFrame (w) the code: the error: I only show small extracts from the dataframes. The original cov dataframe is 123610 rows \u00d7 10 columns, the w dataframe 12361 rows \u00d7 10 columns. Expected output: Many thanks in advance!!", "q_apis": "get columns time DataFrame cov DataFrame cov columns columns", "apis": "cov values cov index nunique cov index nunique cov shape values values cov groupby apply loc dot values loc mean std values values mean std", "code": ["reshaped = cov.values.reshape(cov.index.levels[0].nunique(), cov.index.levels[1].nunique(), cov.shape[-1])\nnp.einsum('ik,ik->i', w.values, np.einsum('ijk,ik->ij', reshaped, w.values))\n", "%timeit cov.groupby(level='Date').apply(lambda g: w.loc[g.name].dot(g.values@(w.loc[g.name])))\n4.74 ms \u00b1 614 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit np.einsum('ik,ik->i', w.values, np.einsum('ijk,ik->ij', reshaped, w.values))\n35.6 \u00b5s \u00b1 5.19 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n", "cov = pd.DataFrame.from_dict({'NoDur': {('2018-12-27', 'NoDur'): 0.000109,\n  ('2018-12-27', 'Durbl'): 0.000112,\n  ('2018-12-27', 'Manuf'): 0.000118,\n  ('2018-12-28', 'NoDur'): 0.000109,\n  ('2018-12-28', 'Durbl'): 0.000113,\n  ('2018-12-28', 'Manuf'): 0.000117,\n  ('2018-12-31', 'NoDur'): 0.000109,\n  ('2018-12-31', 'Durbl'): 0.000113,\n  ('2018-12-31', 'Manuf'): 0.000118},\n 'Durbl': {('2018-12-27', 'NoDur'): 0.000112,\n  ('2018-12-27', 'Durbl'): 0.000339,\n  ('2018-12-27', 'Manuf'): 0.000238,\n  ('2018-12-28', 'NoDur'): 0.000113,\n  ('2018-12-28', 'Durbl'): 0.000339,\n  ('2018-12-28', 'Manuf'): 0.000239,\n  ('2018-12-31', 'NoDur'): 0.000113,\n  ('2018-12-31', 'Durbl'): 0.000339,\n  ('2018-12-31', 'Manuf'): 0.000239},\n 'Manuf': {('2018-12-27', 'NoDur'): 0.000118,\n  ('2018-12-27', 'Durbl'): 0.000238,\n  ('2018-12-27', 'Manuf'): 0.000246,\n  ('2018-12-28', 'NoDur'): 0.000117,\n  ('2018-12-28', 'Durbl'): 0.000239,\n  ('2018-12-28', 'Manuf'): 0.000242,\n  ('2018-12-31', 'NoDur'): 0.000118,\n  ('2018-12-31', 'Durbl'): 0.000239,\n  ('2018-12-31', 'Manuf'): 0.000245}})\n\n w = pd.DataFrame.from_dict({'NoDur': {'2018-12-27': -69.190732,\n  '2018-12-28': -113.83175,\n  '2018-12-31': -101.365016},\n 'Durbl': {'2018-12-27': -96.316224,\n  '2018-12-28': 30.426696,\n  '2018-12-31': -16.613136},\n 'Manuf': {'2018-12-27': -324.058486,\n  '2018-12-28': -410.055587,\n  '2018-12-31': -362.232014}})\n"], "link": "https://stackoverflow.com/questions/67087980/multiplying-multidimensional-multiindex-dataframe-with-single-index-dataframe-ov"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How can i assign string as newcolumn to pandas.assign Example: Above doesn't throw any error, but creates new column with name instead", "q_apis": "get columns assign assign any name", "apis": "assign apply assign", "code": ["newcolumn = 'myColumn'\n    if(newcolumn != ''):\n      sourcePandas = sourcePandas.assign(**{f'{newcolumn}': sourcePandas[row['PrimaryColumn']].apply(func)})\n", "# df --> dataframe\ndf.assign(**{'A': np.arange(7), 'B': np.arange(7) + 2})\n"], "link": "https://stackoverflow.com/questions/61850399/how-to-assign-new-column-as-string-to-pandas-assign"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm relatively new to python and have been using Pandas to manipulate scientific data. I have 79 datasets in CSV format of inconsistent satellite imagery of pixel values (lots of NaNs) that have been averaged to bi-monthly values (two months averaged together). The data is formatted similar to the the example data frame \"df\". The actual time series data extends from 1985-2020 with a screen shot at the bottom showing it's actual format for reference. I need to reformat the data so each row is just one year with each two month grouping as a column header. However, each dataset has two regions that need to be compared to each other. \"April-May region 1\" and \"April-May region 2\". The final data set would look something like this: I've tried using the following code, but I dont know how to include the region_2 data within the data frame. It also creates an index value and calls it \"grouping\" and shuffles the order of the bi-monthly grouping. Would it be better to create two separate data frames for each region? I also can't seem to find any posts that show how to do this.", "q_apis": "get columns values values time at year month index value any", "apis": "pivot index columns values reindex", "code": ["bimonths = ['F-M', 'A-M', 'J-J', 'A-S', 'O-N', 'D-J']\ndf.pivot(index='year', columns = 'grouping', values = ['region_1','region_2']).reindex(bimonths, axis=1, level=1)\n", "    region_1    region_2\ngrouping    F-M A-M J-J A-S O-N D-J F-M A-M J-J A-S O-N D-J\nyear                                                \n1985    NaN 0.264   0.339   0.321   0.305   NaN NaN 0.457   0.649   0.625   0.531   NaN\n1986    NaN 0.404   0.206   0.217   0.266   0.217   0.503   0.656   0.437   0.568   0.547   NaN\n"], "link": "https://stackoverflow.com/questions/65223566/pandas-reformatting-csv-data-from-single-column-into-multiple-new-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Code to create sample dataframe: Sample Dataframe visualized: I'm looking for a formula to combine Jan and Feb columns into one array, outputting in a New column this array. Expected output:", "q_apis": "get columns sample combine columns array array", "apis": "assign values assign apply assign mean std mean std mean std mean std mean std mean std", "code": ["df['New'] = [[x, y] for x, y in zip(df.Jan, df.Feb)]\ndf\n\n   Feb  Jan                               Mar    account         New\n0  200  150  [[0.332, 0.326], [0.058, 0.138]]  Jones LLC  [150, 200]\n1  210  200  [[0.234, 0.246], [0.234, 0.395]]   Alpha Co  [200, 210]\n2   90   50   [[0.084, 0.23], [0.745, 0.923]]   Blue Inc    [50, 90]\n", "df['New'] = df.apply(lambda x: [x['Jan'], x['Feb']], axis=1)    \ndf\n\n   Feb  Jan                               Mar    account         New\n0  200  150  [[0.332, 0.326], [0.058, 0.138]]  Jones LLC  [150, 200]\n1  210  200  [[0.234, 0.246], [0.234, 0.395]]   Alpha Co  [200, 210]\n2   90   50   [[0.084, 0.23], [0.745, 0.923]]   Blue Inc    [50, 90]\n", "%timeit df.assign(New=df[['Feb', 'Jan']].values.tolist())\n%timeit df.assign(New=df.apply(lambda x: [x['Jan'], x['Feb']], axis=1))\n%timeit df.assign(New=[[x, y] for x, y in zip(df.Jan, df.Feb)])\n\n2.76 ms \u00b1 596 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n152 ms \u00b1 9.47 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.59 ms \u00b1 13.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n", "5.95 ms \u00b1 527 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n1.53 s \u00b1 165 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n8.79 ms \u00b1 793 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/45020225/combine-numbers-from-two-columns-to-create-one-array"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a problem with merging together serval dataframes. I downloaded some historical trading data and saved it into a csv file. So now I want to read the data from the cvs file into several dataframes and extract some close prices. I created a function, called read_dataset that reads the data into a dataframe and returns that dataframe. In combination with a for loop I stored all Dataframes in a dict. Dict keys are the abbreviations of the currency (see coin_list dataframe). So after the dict currency_data is created I want to access and separate the dataframes contained in currency_data. Therefore I want to create a for loop to merge for example all close prices of the data frames into one dataframe. Has anyone an idea how I can achieve that? I can do this for two dataframes with the following code, but can't translate that into a for loop. Or is there a better way to create different dataframes from a cvs file and storing it in a dict? Thanks for your help!", "q_apis": "get columns now all keys merge all", "apis": "product join product columns values columns values columns filter product items DataFrame abs abs abs abs abs abs index columns", "code": ["master_df = pd.concat(currency_data, axis=1)\n\n# RENAME COLUMNS USING itertools.product\nall_cols = map(lambda x: \"_\".join(x), product(master_df.columns.levels[0].values,\n                                              master_df.columns.levels[1].values))    \nmaster_df.columns = all_cols\n\ndf_close = master_df.filter(regex='_close')\n", "import numpy as np\nimport pandas as pd\nfrom itertools import product\n\ncoins = { 'Bitcoin': 'BTC', 'Ethereum': 'ETH', 'Ripple': 'XRP', 'BitcoinCash': 'BCH', 'Litecoin':'LTC', 'EOS': 'EOS',\n          'Tronix': 'TRX', 'Stellar' : 'XLM', 'Neo' : 'NEO', 'Cardano': 'ADA', 'IOTA' : 'IOT', 'Monero': 'XMR'}\n\ncurrency_data = {}\nnp.random.seed(788)\n\nfor k, v in coins.items():    \n    currency_data[v] = pd.DataFrame({'open': abs(np.random.randn(50)),\n                                     'close': abs(np.random.randn(50)),\n                                     'high': abs(np.random.randn(50)),\n                                     'low': abs(np.random.randn(50)),\n                                     'volumefrom': abs(np.random.randn(50)) * 50,\n                                     'volumeto': abs(np.random.randn(50)) * 100},\n                                     index = pd.date_range(\"2018-01-01\", \"2018-02-19\", freq=\"D\"),\n                                     columns = ['open','close','low','high','volumefrom', 'volumeto'])\n", "print(df_close.head(10))\n\n#             ADA_close  BCH_close  BTC_close  EOS_close  ETH_close  IOT_close  LTC_close  NEO_close  TRX_close  XLM_close  XMR_close  XRP_close\n# 2018-01-01   0.650955   1.547163   0.796460   0.526820   0.191777   1.310333   0.322086   0.216098   1.231339   1.008557   1.452984   1.674484\n# 2018-01-02   0.115062   0.912895   0.163012   0.962510   0.486295   0.314905   0.345002   0.148462   0.487662   0.052015   0.461620   1.673353\n# 2018-01-03   1.001747   0.181435   0.439193   2.419863   0.856715   0.374709   0.277737   1.115768   0.068189   0.217582   0.501237   0.287705\n# 2018-01-04   0.850843   0.194079   0.187193   0.662573   0.480762   0.488702   0.039885   0.603018   0.555557   1.136274   0.804600   0.147496\n# 2018-01-05   1.195504   0.839676   0.997530   0.393851   0.606223   0.754789   1.723055   3.001308   1.601807   1.239889   0.384320   1.712975\n# 2018-01-06   0.694929   0.598245   0.412835   0.694578   1.416549   0.895094   1.266500   0.168239   1.133783   0.616416   0.836242   0.654971\n# 2018-01-07   0.274282   0.274834   0.760970   0.647609   2.189674   0.898377   0.932951   0.439612   1.252156   0.815973   0.051374   1.984519\n# 2018-01-08   0.294268   0.786343   0.548222   2.548036   1.313609   0.348784   0.091552   0.441314   0.908229   1.175537   1.213839   1.375724\n# 2018-01-09   1.383939   0.129143   0.650033   1.251369   1.064297   0.619202   1.275862   0.323824   0.083908   0.677591   0.774429   1.435533\n# 2018-01-10   0.426915   1.723191   0.008422   0.650916   1.431050   0.218723   0.292402   0.030168   1.169357   0.833438   1.048405   0.270780\n"], "link": "https://stackoverflow.com/questions/49731163/python-creating-dataframes-from-a-cvs-file-and-merging-these-dataframes-togeth"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to convert a formatted string into a pandas data frame. I am trying to use the method to do so but the result is that this whole string is placed inside one element in the .", "q_apis": "get columns", "apis": "DataFrame index columns", "code": ["\nimport numpy as np\nimport pandas as pd\n\ndef stringToDF(s):\n    array = s.split('],[')\n\n    # Adjust the constructor parameters based on your string\n    df = pd.DataFrame(data=array,    \n              #index=array[1:,0],    \n             #columns=array[0,1:]\n             ) \n\n    print(df)\n    return df\n\nstringToDF(s)\n\n"], "link": "https://stackoverflow.com/questions/59954492/convert-string-formatted-as-pandas-dataframe-into-an-actual-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Converting a pandas Series with Timestamps to strings is rather simple, e.g.: But how do you convert a large pandas Dataframe with all columns being dates. The above does not work on:", "q_apis": "get columns Series all columns", "apis": "apply", "code": ["dateSfromPandas = dfC.apply(lambda x: x.dt.strftime('%Y/%m/%d'))\n", "dfC = pd.DataFrame({'a': pd.date_range('2016-01-01', periods=10),\n                    'b': pd.date_range('2016-10-04', periods=10),\n                    'c': pd.date_range('2016-05-06', periods=10)})\nprint (dfC)\n           a          b          c\n0 2016-01-01 2016-10-04 2016-05-06\n1 2016-01-02 2016-10-05 2016-05-07\n2 2016-01-03 2016-10-06 2016-05-08\n3 2016-01-04 2016-10-07 2016-05-09\n4 2016-01-05 2016-10-08 2016-05-10\n5 2016-01-06 2016-10-09 2016-05-11\n6 2016-01-07 2016-10-10 2016-05-12\n7 2016-01-08 2016-10-11 2016-05-13\n8 2016-01-09 2016-10-12 2016-05-14\n9 2016-01-10 2016-10-13 2016-05-15\n\ndateSfromPandas = dfC.apply(lambda x: x.dt.strftime('%Y/%m/%d'))\nprint (dateSfromPandas)\n            a           b           c\n0  2016/01/01  2016/10/04  2016/05/06\n1  2016/01/02  2016/10/05  2016/05/07\n2  2016/01/03  2016/10/06  2016/05/08\n3  2016/01/04  2016/10/07  2016/05/09\n4  2016/01/05  2016/10/08  2016/05/10\n5  2016/01/06  2016/10/09  2016/05/11\n6  2016/01/07  2016/10/10  2016/05/12\n7  2016/01/08  2016/10/11  2016/05/13\n8  2016/01/09  2016/10/12  2016/05/14\n9  2016/01/10  2016/10/13  2016/05/15\n", "for col in dfC:\n    dfC[col] = dfC[col].dt.strftime('%Y/%m/%d')\nprint (dfC)\n            a           b           c\n0  2016/01/01  2016/10/04  2016/05/06\n1  2016/01/02  2016/10/05  2016/05/07\n2  2016/01/03  2016/10/06  2016/05/08\n3  2016/01/04  2016/10/07  2016/05/09\n4  2016/01/05  2016/10/08  2016/05/10\n5  2016/01/06  2016/10/09  2016/05/11\n6  2016/01/07  2016/10/10  2016/05/12\n7  2016/01/08  2016/10/11  2016/05/13\n8  2016/01/09  2016/10/12  2016/05/14\n9  2016/01/10  2016/10/13  2016/05/15\n"], "link": "https://stackoverflow.com/questions/45931946/convert-pandas-dataframe-with-timestamps-to-string"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 7 csv files that each contain the same # of columns and rows. I am trying to merge the data from these into 1 csv where each cell is the average of the 7 identical cells. (ex. new-csv(c3) = average(input-csv's(c3) Here is an example of what the inputs look like. The output should look identical (6 columns x 15 rows) except the values will be averaged in each cell. So far I have this code to load the csv files, and am reading about making them into a matrix but I don't see anything about merging and averaging by each cell, only row or column.", "q_apis": "get columns columns merge where identical identical columns values", "apis": "groupby mean", "code": ["import pandas as pd\nimport glob\n\nfiles = glob.glob('*.csv')\ndfs = (pd.read_csv(f, headers=None) for f in files)\npd.concat(dfs).groupby(level=0).mean()\n"], "link": "https://stackoverflow.com/questions/63886468/combining-multiple-csvs-by-averaging-each-cell-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I create a dataframe with MultiIndex And I get the following dataframe I also have a series that has same index of : I would like to assign this series to a new column, , only to the 'x' first index. It works if I use , but it returns a copy: and I obtain but I fail to assign it to the original data yields I get same result if I use assignment like this: But it yields NaN. How can I assign in this way?", "q_apis": "get columns MultiIndex get index assign first index copy assign get assign", "apis": "where index index get", "code": ["f['C4'] = df.index.get_level_values(1).map(series.get)\ndf.loc[df.index.get_level_values(0) != 'x', 'C4'] = np.nan\n\nprint(df)\n\n       C3     C4\nC1 C2           \nx  a   10  100.0\n   b   11    NaN\ny  a   12    NaN\n   b   13    NaN\nz  a   14    NaN\n   b   15    NaN\n", "df['C4'] = np.where(df.index.get_level_values(0) == 'x',\n                    df.index.get_level_values(1).map(series.get),\n                    np.nan)\n"], "link": "https://stackoverflow.com/questions/50992000/pandas-assign-series-to-new-column-to-multiindex"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My data is relating to NBA match results, my first data frame is: Sorry if the above is messy but basically its the monthly stats of all the teams. My second data frame contains match results for a given month as follows: Basically, I want to add the columns Team1OFFRTG and all the other stats for each team to compare one team's stats to the others in order to 'predict' the Team1Win variable. How can I combine the tables to create the final data frame necessary?", "q_apis": "get columns first all second contains month add columns all compare combine", "apis": "DataFrame add_prefix add_prefix DataFrame merge merge", "code": ["teams = pd.DataFrame({\"team\": [\"T0\", \"T1\", \"T2\", \"T3\", \"T4\", \"T5\"],\n                     \"A\": [\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]})\n\nt1_stats = teams.add_prefix(\"T1_\")\nt2_stats = teams.add_prefix(\"T2_\")\n\ngames = pd.DataFrame({\"gameID\": [\"G0\", \"G1\", \"G2\"],\n                      \"T1\": [ \"T0\", \"T2\", \"T4\"],\n                      \"T2\": [ \"T1\", \"T5\", \"T3\"],\n                      \"X\": [\"X0\", \"X1\", \"X2\"]})\n\ngames_with_team_stats = games.merge(t1_stats, \"left\", left_on=\"T1\", right_on=\"T1_team\").merge(t2_stats, left_on=\"T2\", right_on=\"T2_team\")\n"], "link": "https://stackoverflow.com/questions/60482532/i-have-2-pandas-dataframes-and-want-to-pull-values-from-one-dataframe-to-fill-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to combine 2 data frames columns into 1 but when I try to do it based on specific size the second data-frame column doesn't copy correctly. I have tried the code below as pasted below. File: month.csv File: year.csv These are the CURRENT results: Expected results are:", "q_apis": "get columns combine columns size second copy month year", "apis": "DataFrame DataFrame DataFrame reset_index drop reset_index drop", "code": ["newDF = pd.DataFrame()\nnewDF['date_y'] = dfY['date']\nprint(newDF)\n", "     date_y\n0 2014-05-23 04:00:00\n1 2015-12-21 04:00:00\n2 2016-05-03 04:00:00\n3 2017-12-20 04:00:00\n4 2018-06-14 04:00:00\n5 2019-06-25 04:00:00\n", "newDF = pd.DataFrame()\nnewDF['date_m'] = dfM['date'][len(dfM) - len(dfY):len(dfM)]\nprint(newDF)\n", "    date_m\n10 2019-01-31 04:00:00\n11 2019-02-05 04:00:00\n12 2019-03-29 04:00:00\n13 2019-04-30 04:00:00\n14 2019-05-03 04:00:00\n15 2019-06-03 04:00:00\n", "def readDataFile():\n    fileName = \"year.csv\"\n    dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n    dfY = pd.read_csv(fileName, parse_dates=['date'], date_parser=dateparse)\n\n    fileName = \"month.csv\"\n    dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n    dfM = pd.read_csv(fileName, parse_dates=['date'], date_parser=dateparse)\n\n\n    newDF = pd.DataFrame()\n    newDF['date_y'] = dfY['date']\n    newDF['year_y_n'] = dfY['Y_N']\n\n    # Changes made on this line.\n    newDF['date_m'] = dfM['date'][len(dfM) - len(dfY):len(dfM)].reset_index(drop=True)\n    newDF['month_y_n'] = dfM['Y_N'][len(dfM) - len(dfY):len(dfM)].reset_index(drop=True)\n\n    print(newDF)\nreadDataFile()\n", "date_y year_y_n              date_m month_y_n\n0 2014-05-23 04:00:00        Y 2019-01-31 04:00:00         N\n1 2015-12-21 04:00:00        N 2019-02-05 04:00:00         Y\n2 2016-05-03 04:00:00        Y 2019-03-29 04:00:00         N\n3 2017-12-20 04:00:00        N 2019-04-30 04:00:00         Y\n4 2018-06-14 04:00:00        N 2019-05-03 04:00:00         N\n5 2019-06-25 04:00:00        N 2019-06-03 04:00:00         Y\n"], "link": "https://stackoverflow.com/questions/56977225/combining-multiple-data-frame-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What is the best way to return the unique values of 'Col1' and 'Col2'? The desired output is", "q_apis": "get columns unique values", "apis": "values values", "code": [">>> pd.unique(df[['Col1', 'Col2']].values.ravel('K'))\narray(['Bob', 'Joe', 'Bill', 'Mary', 'Steve'], dtype=object)\n", ">>> np.unique(df[['Col1', 'Col2']].values)\narray(['Bill', 'Bob', 'Joe', 'Mary', 'Steve'], dtype=object)\n", ">>> df1 = pd.concat([df]*100000, ignore_index=True) # DataFrame with 500000 rows\n>>> %timeit np.unique(df1[['Col1', 'Col2']].values)\n1 loop, best of 3: 1.12 s per loop\n\n>>> %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel('K'))\n10 loops, best of 3: 38.9 ms per loop\n\n>>> %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel()) # ravel using C order\n10 loops, best of 3: 49.9 ms per loop\n"], "link": "https://stackoverflow.com/questions/26977076/pandas-unique-values-multiple-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe where there are two series, and each contains a number of lists. I would like to perform element-wise multiplication of each list in 'List A' with the corresponding list in 'List B'. The aim is to get the following output: However I am getting the following error:", "q_apis": "get columns where contains get", "apis": "apply", "code": ["df[\"new\"] = df.apply(lambda x: list(a*b for a,b in zip(x['List A'], x['List B'])), axis=1)\nprint(df)\n", "  ref     List A     List B           new\n0   A  [0, 1, 2]  [0, 1, 2]     [0, 1, 4]\n1   B  [2, 3, 4]  [2, 3, 4]    [4, 9, 16]\n2   C  [3, 4, 5]  [3, 4, 5]   [9, 16, 25]\n3   D  [4, 5, 6]  [4, 5, 6]  [16, 25, 36]\n"], "link": "https://stackoverflow.com/questions/63098764/element-wise-multiplication-of-a-series-of-two-lists-from-separate-pandas-datafr"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I don't know if this could be consider a join/merge/concatenate or something else but this is what is happening: I have two pandas dataframes (df1, df2) and I am trying to put them together so I could have something similar to the result below: index column A 0 item_a 1 item_b index column B 0 11 1 22 2 34 Desired Output: index column A column B 0 item_a 11 1 item_a 22 2 item_a 34 3 item_b 11 4 item_b 22 5 item_b 34 I tried most of the regular (, , etc.) and I still can't figure out what I was doing wrong? Does anyone have any idea of what I should do?", "q_apis": "get columns join merge put index index index any", "apis": "index assign merge assign drop columns reset_index", "code": ["   index  column A  column B\n0      0    item_a        11\n1      1    item_a        22\n2      2    item_a        34\n3      3    item_b        11\n4      4    item_b        22\n5      5    item_b        34\n", "(df1.assign(key=0)\n    .merge(df2.assign(key=0), on='key')\n    .drop(columns='key')\n    .reset_index())\n"], "link": "https://stackoverflow.com/questions/67343479/dataframe-join-on-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "For example I have the following data frame I want to randomly assign a \"Y\" or \"N\" to the A/B Test column based on 50% distribution. In other words I want to split the df and make sure 50% of the records have a \"Y\" and 50% a \"N\", but these values should be assigned randomly. Output DataFrame: Please help! Thanks", "q_apis": "get columns assign values DataFrame", "apis": "size", "code": ["df['A/B Test'] = np.random.choice(['Y','N'], size=len(df))\n"], "link": "https://stackoverflow.com/questions/62699584/randomly-assign-values-in-data-frame-based-on-a-weightage-percentage"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a df that looks like this: I want to update the column of the first instance of a email address. new df should look like this: I have tried creating statement to check the count of email address but not its not working:", "q_apis": "get columns update first count", "apis": "sort_values", "code": ["df\n  email  is_new col_n\n0   a@a       0     z\n1   a@a       1     x\n2   b@b       0     y\n", "df2 = pd.concat([df] * 2, ignore_index=True).sort_values('email')\n", "df2.groupby('email').cumcount()\n\n0    0\n1    1\n3    2\n4    3\n2    0\n5    1\ndtype: int64\n", "df2.groupby('email').cumcount().ne(0).astype(int)\n# df2.groupby('email').cumcount().astype(bool).astype(int)\n\n0    0\n1    1\n3    1\n4    1\n2    0\n5    1\ndtype: int64\n"], "link": "https://stackoverflow.com/questions/54116918/flagging-duplicate-values-in-a-column-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "how can I set up the column name as shown in the picture attached? I have joined multiple dfs but generates non-representative names for the columns. Screenshot", "q_apis": "get columns name names columns", "apis": "rename columns columns columns merge set_index", "code": ["new_dfs = [i.rename(columns = \ndict(zip(i.columns.difference(['date']),i.columns.difference(['date']) + f\"_dfno_{e}\"))) \n     for e,i in enumerate(dfs,3)]\n", "df_final = reduce(lambda left,right: pd.merge(left,right,on='date'), new_dfs )\ndf_final.set_index('date')\n"], "link": "https://stackoverflow.com/questions/65808605/how-to-set-name-of-columns-from-rows-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "So I have a csv with minute stock data for Microsoft. I am trying to find the low of each trading day. The code looks like: The output is: So the issue is that the lows are filled with NaN values, which I am asuming is because I am miss using groupby. I have also tried: The output to this is: The issue with this is that it is finding the low of of both Close and Open. Also there are only 31 total rows, though there should be more giving this is a dataset of all of 2020. I assuming in doing this I am grouping wrong because I looked at the close prices of each day for the first 31 days, and there is no way that these are the lows of each of those days. So the questions is how can I find the lows of each day, without affecting the Close columns, and avoiding the issues mentioned above?", "q_apis": "get columns minute day values groupby all at day first days days day columns", "apis": "index min loc index DataFrame", "code": ["unique_dates = list(set([str(date).split()[0] for date in df.index]))\n\nmin_values_daily = [min(df.loc[df.index==date].Close) for date in unique_dates] \n", "low_data = pd.DataFrame({\n     'date': unique_dates,\n     'low': min_values_daily\n})\n"], "link": "https://stackoverflow.com/questions/65565348/how-to-find-the-lows-of-a-stock-for-each-trading-day"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes: df1 (a row for every event that happens in the game) Date Game Event Type Player Time 02/28/10 USA vs Canada Faceoff Sidney Crosby 20:00 02/28/10 USA vs Canada Pass Drew Doughty 19:59 02/28/10 USA vs Canada Pass Scott Niedermayer 19:42 02/28/10 USA vs Canada Shot Sidney Crosby 18:57 02/28/10 USA vs Canada Takeaway Dany Heatley 18:49 02/28/10 USA vs Canada Shot Dany Heatley 18:02 02/28/10 USA vs Canada Shot Sidney Crosby 17:37 df2 Player Sidney Crosby Dany Heatley Scott Niedermayer Drew Doughty How do I create a new column in df2 that matches the Player column in each dataframe and counts each row where the Event Type in df1 is \"Shot\"? This is the output I would look for in this example: Player Shots Sidney Crosby 2 Dany Heatley 1 Scott Niedermayer 0 Drew Doughty 0 I'm new to Python, so I apologize if there's an easy answer that I'm missing. Thank you!", "q_apis": "get columns where", "apis": "loc value_counts reindex reindex values merge", "code": ["shots = df1.loc[df1['Event Type']=='shot', 'Player'].value_counts()\n\ndf2['shots'] = df2['Player'].map(shots)\n# or using reindex with `fill_value` option\n# shots.reindex(df2['Player'], fill_value=0).values\n", "df2.merge(pd.crosstab(df1['Player'], df1['Event Type']),\n          on='Player', how='left')\n"], "link": "https://stackoverflow.com/questions/66140256/how-do-i-create-a-new-column-based-on-matching-values-in-two-different-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to unstack() data in a Pandas dataframe, but I keep getting this error, and I'm not sure why. Here is my code so far with a sample of my data. My attempt to fix it was to remove all rows where voteId was not a number, which did not work with my actual dataset. This is happening both in an Anaconda notebook (where I am developing) and in my production env when I deploy the code. I could not figure out how to reproduce the error in my sample code... possibly due to a typecasting issue that doesnt exist when you instantiate the dataframe like I did in the sample? Full error message/stack trace:", "q_apis": "get columns unstack sample all where where sample sample stack", "apis": "axes axes", "code": ["def __init__(self)\n    print('BlockManager blocks')\n    pprint(self.blocks)\n    print('BlockManager axes')\n    pprint(self.axes)\n", "def __unstack_frame(self, ...)\n    from pprint import pprint\n    print('_unstack_frame level {} fill_value {} {}'.format(level, fill_value, type(obj)))\n    pprint(obj)\n"], "link": "https://stackoverflow.com/questions/49482356/assertionerror-gaps-in-blk-ref-locs-when-unstack-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe that shows latitude (y), longitude (x) and elevation (z). negative number means there is no elevation, i.e. no land. I want to change the elevation of any points that currently have no elevation if they are next to a point that does have elevation at the same y, but that y value is greater than the point at the same x of the next y up. Apologies for that explanation, I'm not sure that is the best way to describe it. But in this case: point (-2, 5) has elevation -999999. At that y value it is next to (-1, 5) with an elevation of 67 which is greater than the point at (x, y+1) = (-1, 6) with an elevation of 8. In this case I would like to change the elevation of (-2, 5) to that of (-2, 6) = 9. This: becomes: How do you go about manipulating dataframes like this?", "q_apis": "get columns any at value at describe value at", "apis": "filter loc get get index set_index assign add values values loc values loc values loc values get mask update mask update values where mask mask astype update update set_index index reset_index set_index index", "code": ["df = pd.read_clipboard()\n\n# filter table by relevant (negative) z locations\ndf_neg = df.loc[df.z < 0]\n\n# get coordinates of relevant locations\nlist_x, list_y = df_neg.x, df_neg.y\n\n# get lists of neighboring points relative to relevant locations\npoints_right = list(zip(list_x + 1, list_y))\npoints_topright = list(zip(list_x + 1, list_y + 1))\npoints_top = list(zip(list_x, list_y + 1))\n\n# set x, y index for convenient access and initialize adjusted col\ndf_idxd = df.set_index(['x', 'y']).assign(adjusted=0)\n\n# add values of neighboring points to the df_neg table\n# if one of the points in the points_... lists doesn't exist,\n# the values will be NaN and it won't bother us below\ndf_neg['right'] = df_idxd.loc[points_right].z.values\ndf_neg['topright'] = df_idxd.loc[points_topright].z.values\ndf_neg['top'] = df_idxd.loc[points_top].z.values\n\n# get mask which determines whether or not we update\nmask = (df_neg.right >= 0) & (df_neg.right > df_neg.topright)\n\n# update values in df_neg\ndf_neg['z'] = df_neg.z.where(~mask, df_neg.top)\ndf_neg['adjusted'] = mask.astype(int)\n\n# use df_neg to update the full table\ndf_idxd.update(df_neg.set_index(['x', 'y']))\n\n# restore original index\ndf_idxd.reset_index().set_index('index')\n", "         x    y         z  adjusted\nindex                              \n0.0   -5.0  5.0 -999999.0       0.0\n1.0   -4.0  5.0 -999999.0       0.0\n2.0   -3.0  5.0 -999999.0       0.0\n3.0   -2.0  5.0       9.0       1.0\n4.0   -1.0  5.0      67.0       0.0\n5.0    0.0  5.0      55.0       0.0\n6.0    1.0  5.0      49.0       0.0\n7.0    2.0  5.0       7.0       0.0\n8.0    3.0  5.0       6.0       0.0\n9.0    4.0  5.0       6.0       0.0\n10.0  -5.0  6.0      12.0       0.0\n11.0  -4.0  6.0      12.0       0.0\n12.0  -3.0  6.0      19.0       0.0\n13.0  -2.0  6.0       9.0       0.0\n14.0  -1.0  6.0       8.0       0.0\n15.0   0.0  6.0       9.0       0.0\n16.0   1.0  6.0       9.0       0.0\n17.0   2.0  6.0       7.0       0.0\n18.0   3.0  6.0       7.0       0.0\n19.0   4.0  6.0       7.0       0.0\n"], "link": "https://stackoverflow.com/questions/59118374/manipulate-pandas-dataframe-based-on-adjacent-coordinates"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have the first two columns of this dataframe: I want to create a new column named (demonstrated above as a comment), such that it is equal to: score value in that row score value of the day before in the same hour. Example: in row 4 is equal to: score in that row - score on the day before (30th) at , final value: . If the day before and same time do not exist, then the value of the score of that row is taken: E.g. in row 0, for time there is no , hence only the 2 is taken", "q_apis": "get columns first columns value value day hour day at value day time value time", "apis": "sub set_index", "code": ["df['datetime'] = pd.to_datetime(df['datetime'])\nbefore = df['datetime'] - pd.DateOffset(days=1)\n\ndf['difference'] = df['score'].sub(\n    before.map(df.set_index('datetime')['score']), fill_value=0)\n", ">>> before\n\n0   2017-08-29 07:00:00\n1   2017-08-29 08:00:00\n2   2017-08-30 07:00:00\n3   2017-08-30 08:00:00\n4   2017-08-28 21:00:00\n5   2017-08-27 21:00:00\nName: datetime, dtype: datetime64[ns]\n", ">>> before.map(df.set_index('datetime')['score'])\n\n0     NaN\n1     NaN\n2     2.0\n3    13.0\n4    44.0\n5     NaN\nName: datetime, dtype: float64\n", ">>> df\n\n   score            datetime  difference\n0      2 2017-08-30 07:00:00         2.0\n1     13 2017-08-30 08:00:00        13.0\n2     24 2017-08-31 07:00:00        22.0\n3     15 2017-08-31 08:00:00         2.0\n4     11 2017-08-29 21:00:00       -33.0\n5     44 2017-08-28 21:00:00        44.0\n"], "link": "https://stackoverflow.com/questions/67035947/create-new-column-conditional-on-day-and-hour-of-other-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am a newcomer to python. I want to implement a \"For\" loop on the elements of a dataframe, with an embedded \"if\" statement. Code: With the previous loop, I want to go through each item in the x dataframe and generate a new dataframe y based on the condition in the \"if\" statement. When I run the code, I get the following error message. Any help would be much appreciated.", "q_apis": "get columns item get", "apis": "bool", "code": ["x = pd.DataFrame([1,-2,3], columns=['a'])\n\ny = pd.DataFrame(np.where(x['a'] > 0, x['a'] * 2, 0), columns=['b'])\nprint (y)\n   b\n0  2\n1  0\n2  6\n", "print (x['a'] > 0)\n0     True\n1    False\n2     True\nName: a, dtype: bool\n", "print (np.where(x['a'] > 0, x['a'] * 2, 0))\n[2 0 6]\n", "x['new'] = np.where(x['a'] > 0, x['a'] * 2, 0)\nprint (x)\n   a  new\n0  1    2\n1 -2    0\n2  3    6\n"], "link": "https://stackoverflow.com/questions/52553757/a-for-loop-with-embedded-if-statement-to-update-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data set of 7 columns and 2557 rows in a pandas dataframe. I am trying to replace all negative values with 0 and all values above 192 with 192. I have succeeded in this, but the new dataframe I get is lacking the first row (date). I guess it is left out, because it isn't considered a numeric value? How do I get a new dataframe with the corrected data and still keeping the date column? I've tried out answers from this question: How to replace negative numbers in Pandas Data Frame by zero And written following code:", "q_apis": "get columns columns replace all values all values get first date left value get date replace", "apis": "select_dtypes columns clip", "code": ["print (df)\n       Date  Inlet 1  Inlet 2  Inlet 3  Inlet 4  Inlet 5      col\n0  01-01-13     -0.4   500.41     0.36     0.39     0.37  string1\n\ndf1 = df.select_dtypes(np.number)\ndf[df1.columns] = df1.clip(0, 192)\nprint (df)\n       Date  Inlet 1  Inlet 2  Inlet 3  Inlet 4  Inlet 5      col\n0  01-01-13      0.0    192.0     0.36     0.39     0.37  string1\n", "cols = df.select_dtypes(np.number).columns\ndf[cols] = df[cols].clip(0, 192)\n"], "link": "https://stackoverflow.com/questions/61057001/changing-values-in-pandas-dataframe-but-keeping-date-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am learning with python code and I have some issues: https://github.com/Slothfulwave612/Football-Analytics-Using-Python/blob/master/03.%20Analyzing%20Event%20Data/pass_map.py My dubt is really simple: I would like to apply a for expresion in order to apply the pass code to multiple football matches. import matplotlib.pyplot as plt import json from pandas.io.json import json_normalize from FCPython import createPitch ## computing pass accuracy pass_acc = (pass_comp / (pass_comp + pass_no)) * 100 pass_acc = str(round(pass_acc, 2)) ## adding text to the plot plt.text(20, 85, '{} pass map vs Real Madrid'.format(player_name), fontsize=15) plt.text(20, 82, 'Pass Accuracy: {}'.format(pass_acc), fontsize=15) ## handling labels handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) plt.legend(by_label.values(), by_label.keys(), loc='best', bbox_to_anchor=(0.9, 1, 0, 0),fontsize=12) ## editing the figure size and saving it fig.set_size_inches(12, 8) fig.savefig('{} passmap.png'.format(match_id), dpi=200) ## showing the plot plt.show() I only have edited the code in order to analayze multiple matches with a for expresion. P1TMP = [16205, 16131, 16265] for i in P1TMP: And the results: In The first image the result is almost perfect, but the Kind of passes\u00b4s filter is not working. enter image description here In the second image the passes are a mix of the passes of the first match and the second match. I only want the passes of the second match. enter image description here And in the third is the mix of the match n\u00ba1 +n\u00ba2 + n3\u00ba. I need the passes of the third : enter image description here Thanks in advance for your support. Best Regards", "q_apis": "get columns apply apply round plot map values keys loc size plot first filter second first second second", "apis": "get assign get all columns all values loc copy dropna loc all values loc copy dropna loc iterrows sum iloc apply at plot round plot values keys loc size plot", "code": ["import matplotlib.pyplot as plt\nimport json\nfrom pandas.io.json import json_normalize\nfrom FCPython import createPitch\n\n## Note Statsbomb data uses yards for their pitch dimensions\npitch_length_X = 120\npitch_width_Y = 80\n\n\n\n## match id for our El Clasico\nmatch_list = [16205, 16131, 16265]\nteamA = 'Barcelona'  #<--- adjusted here\n\nfor match_id in match_list:\n    ## calling the function to create a pitch map\n    ## yards is the unit for measurement and\n    ## gray will be the line color of the pitch map\n    (fig,ax) = createPitch(pitch_length_X, pitch_width_Y,'yards','gray') #< moved into for loop\n\n    player_name = 'Lionel Andr\u00e9s Messi Cuccittini'\n\n    ## this is the name of our event data file for\n    ## our required El Clasico\n    file_name = str(match_id) + '.json'\n\n    ## loading the required event data file\n    my_data = json.load(open('Statsbomb/data/events/' + file_name, 'r', encoding='utf-8'))\n\n\n    ## get the nested structure into a dataframe \n    ## store the dataframe in a dictionary with the match id as key\n    df = json_normalize(my_data, sep='_').assign(match_id = file_name[:-5])\n    teamB = [x for x in list(df['team_name'].unique()) if x != teamA ][0] #<--- get other team name\n\n    ## making the list of all column names\n    column = list(df.columns)\n\n    ## all the type names we have in our dataframe\n    all_type_name = list(df['type_name'].unique())\n\n    ## creating a data frame for pass\n    ## and then removing the null values\n    ## only listing the player_name in the dataframe\n    pass_df = df.loc[df['type_name'] == 'Pass', :].copy()\n    pass_df.dropna(inplace=True, axis=1)\n    pass_df = pass_df.loc[pass_df['player_name'] == player_name, :]\n\n    ## creating a data frame for ball receipt\n    ## removing all the null values\n    ## and only listing Barcelona players in the dataframe\n    breceipt_df = df.loc[df['type_name'] == 'Ball Receipt*', :].copy()\n    breceipt_df.dropna(inplace=True, axis=1)\n    breceipt_df = breceipt_df.loc[breceipt_df['team_name'] == 'Barcelona', :]\n\n    pass_comp, pass_no = 0, 0\n    ## pass_comp: completed pass\n    ## pass_no: unsuccessful pass\n\n    ## iterating through the pass dataframe\n    for row_num, passed in pass_df.iterrows():   \n\n        if passed['player_name'] == player_name:\n            ## for away side\n            x_loc = passed['location'][0]\n            y_loc = passed['location'][1]\n\n            pass_id = passed['id']\n            summed_result = sum(breceipt_df.iloc[:, 14].apply(lambda x: pass_id in x))\n\n            if summed_result > 0:\n                ## if pass made was successful\n                color = 'blue'\n                label = 'Successful'\n                pass_comp += 1\n            else:\n                ## if pass made was unsuccessful\n                color = 'red'\n                label = 'Unsuccessful'\n                pass_no += 1\n\n            ## plotting circle at the player's position\n            shot_circle = plt.Circle((pitch_length_X - x_loc, y_loc), radius=2, color=color, label=label)\n            shot_circle.set_alpha(alpha=0.2)\n            ax.add_patch(shot_circle)\n\n            ## parameters for making the arrow\n            pass_x = 120 - passed['pass_end_location'][0]\n            pass_y = passed['pass_end_location'][1] \n            dx = ((pitch_length_X - x_loc) - pass_x)\n            dy = y_loc - pass_y\n\n            ## making an arrow to display the pass\n            pass_arrow = plt.Arrow(pitch_length_X - x_loc, y_loc, -dx, -dy, width=1, color=color)\n\n            ## adding arrow to the plot\n            ax.add_patch(pass_arrow)\n\n    ## computing pass accuracy\n    pass_acc = (pass_comp / (pass_comp + pass_no)) * 100\n    pass_acc = str(round(pass_acc, 2))\n\n    ## adding text to the plot\n    plt.suptitle('{} pass map vs {}'.format(player_name, teamB), fontsize=15) #<-- make dynamic and change to suptitle\n    plt.title('Pass Accuracy: {}'.format(pass_acc), fontsize=15) #<-- change to title\n\n    ## handling labels\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    plt.legend(by_label.values(), by_label.keys(), loc='best', bbox_to_anchor=(0.9, 1, 0, 0), fontsize=12)\n\n    ## editing the figure size and saving it\n    fig.set_size_inches(12, 8)\n    fig.savefig('{} passmap.png'.format(match_id), dpi=200)  #<-- dynamic file name\n\n    ## showing the plot\n    plt.show()\n"], "link": "https://stackoverflow.com/questions/62029506/problem-with-friends-of-tracking-code-noob-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a df as I am looking for a pattern \"ABD followed by CDE without having event B in between them \" For example, The output of this df will be : This pattern can be followed multiple times for a single ID and I want find the list of all those IDs and their respective count (if possible).", "q_apis": "get columns between all count", "apis": "mask mask at mask astype sum groupby sum", "code": ["# Get the col in context and scale it to the three strings to form an ID array\na = df['Event']\nid_ar = (a=='ABD') + 2*(a=='B') + 3*(a=='CDE')\n\n# Mask of those specific strings and hence extract the corresponding masked df\nmask = id_ar>0\ndf1 = df[mask]\n\n# Get pattern col with 1s at places with the pattern found, 0s elsewhere\ndf1['Pattern'] = (np.convolve(id_ar[mask],[9,1],'same')==28).astype(int)\n\n# Groupby Id col and sum the pattern col for final output\nout = df1.groupby(['Id'])['Pattern'].sum()\n", "1*1 + 3*9 = 28", "In [377]: df\nOut[377]: \n   Id Event SeqNo\n0   1     A     1\n1   1     B     2\n2   1     C     3\n3   1   ABD     4\n4   1     B     5\n5   1     C     6\n6   1     A     7\n7   1   CDE     8\n8   1     D     9\n9   1     B    10\n10  1   ABD    11\n11  1     D    12\n12  1     B    13\n13  2     A     1\n14  2     B     2\n15  2     C     3\n16  2   ABD     4\n17  2     A     5\n18  2     C     6\n19  2     A     7\n20  2   CDE     8\n21  2     D     9\n22  2     B    10\n23  2   ABD    11\n24  2     D    12\n25  2     B    13\n26  2   CDE    14\n27  2     A    15\n", "In [380]: df1\nOut[380]: \n   Id Event SeqNo  Pattern\n1   1     B     2        0\n3   1   ABD     4        0\n4   1     B     5        0\n7   1   CDE     8        0\n9   1     B    10        0\n10  1   ABD    11        0\n12  1     B    13        0\n14  2     B     2        0\n16  2   ABD     4        0\n20  2   CDE     8        1\n22  2     B    10        0\n23  2   ABD    11        0\n25  2     B    13        0\n26  2   CDE    14        0\n", "In [381]: out\nOut[381]: \nId\n1    0\n2    1\nName: Pattern, dtype: int64\n"], "link": "https://stackoverflow.com/questions/54558981/looking-for-a-sequential-pattern-with-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to take an aggregated time-series and break it down to its component pieces. For example, I have a time series forecast, that I would like to break down between 3 buckets (red,blue,oj) Trying to multiply: I know this is related to indexing, but is there a way to do this without using for loops and doing a concat/join/merge? Edit: Resolved with answer below,", "q_apis": "get columns take time time between concat join merge", "apis": "index values values", "code": ["df[volume_breakdown.index] = df[\"Volume\"].values.reshape((-1, 1)) \\\n                                 * volume_breakdown.values\n", ">>> df\n            Volume    red   blue     oj\n2021-05-31     722  361.0  216.6  144.4\n2021-06-30     530  265.0  159.0  106.0\n2021-07-31     762  381.0  228.6  152.4\n2021-08-31     706  353.0  211.8  141.2\n2021-09-30     811  405.5  243.3  162.2\n2021-10-31     908  454.0  272.4  181.6\n2021-11-30     912  456.0  273.6  182.4\n2021-12-31     740  370.0  222.0  148.0\n2022-01-31     692  346.0  207.6  138.4\n2022-02-28     798  399.0  239.4  159.6\n2022-03-31     497  248.5  149.1   99.4\n2022-04-30     622  311.0  186.6  124.4\n2022-05-31     589  294.5  176.7  117.8\n"], "link": "https://stackoverflow.com/questions/67694969/python-dataframe-break-down-one-column-using-mapping"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Sorry If the headline isn't clear enough, i'll explain myself better with example: print are: I want to merge so the output dataframe will look like that: Meaning to add to dataframe2 the columns with the values correspond to the match between UniqueNum in dataframe1 to TestId in dataframe2. Thanks", "q_apis": "get columns merge add columns values between", "apis": "merge rename columns merge set_index merge drop", "code": ["d = {'UniqueNum':'TestId'}\ndf = dataframe2.merge(dataframe1.rename(columns=d), how='left', on='TestId')\n", "df = dataframe2.merge(dataframe1.set_index('UniqueNum'), \n                      how='left', \n                      left_on='TestId', \n                      right_index=True)\n", "df = dataframe2.merge(dataframe1, \n                      how='left', \n                      left_on='TestId', \n                      right_on='UniqueNum').drop('UniqueNum', axis=1)\n", "print (df)\n\n  TestId   C    D   B  A\n0     1a  22   13   3  2\n1     2b  46   88  88  6\n2     3c  47  233  23  7\n3     1a  22   22   3  2\n4     3c  46   46  23  7\n5     2b  47   47  88  6\n"], "link": "https://stackoverflow.com/questions/55236883/merge-dataframe-cols-and-a-row-to-specific-index-base-on-another-dataframe-colum"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to perform actions based on input from a config file. In the config, there will be specifications for a signal, a comparison, and a value. I'd like to translate that comparison string into a choice of inequality operator. Right now, this looks like In other applications, I was able to do something like in order to easily avoid having to repeat code over and over. How would I go about doing the same thing here?", "q_apis": "get columns value now repeat", "apis": "lt le eq gt ge ne", "code": ["import operator\n\nfuncs = {\n    '<': operator.lt,\n    '<=': operator.le,\n    '=': operator.eq,\n    '>': operator.gt,\n    '>=': operator.ge,\n    '!=': operator.ne\n}\n\ndef compute_mask(self, signal, comparator, value, df):\n    return funcs[comparator](df[signal], value)\n"], "link": "https://stackoverflow.com/questions/66125362/programmatically-picking-an-inequality-operator"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two columns in my dataframe which contain binary values. project_is_approved column has value 1 if the project is approved or else 0 numbers_in_summary_or_not: there was another column(named as \"summary\") which had text data based on which this column is constructed. if there was a number used in the text data in summary the corresponding row will have value 1 or else it will be 0 Now, I want to visualise my data based on these two columns: I think bar plots should work, I think I need 4 bar plots showing 4 features numbers_in_summary_or_not =0 and project_is_approved =0 : 1st barplot numbers_in_summary_or_not =0 and project_is_approved =1 : 2nd barplot numbers_in_summary_or_not =1 and project_is_approved =0 : 3rd barplot numbers_in_summary_or_not =1 and project_is_approved =1 : 4th barplot \u2013", "q_apis": "get columns columns values value value columns", "apis": "astype astype plot value_counts plot", "code": ["# a series to use for counting- formatted as cat1_cat2\nts = (df['numbers_in_summary_or_not'].astype(str) + '_' + df['project_is_approved'].astype(str))\n# and plot the counts of the groups like\nts.value_counts().plot(kind = 'bar')\n"], "link": "https://stackoverflow.com/questions/56501203/count-the-number-of-occurrences-each-combination-of-values-in-two-columns-of-a-p"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have below some IP addresses and I want to categorize them based on their last digits. An IPv4 address consists of four numbers: each of which contains one to three digits(0-255) with a single dot (.) separating each number or set of digits Now I want to refer the last digit of an IP address and if it was the related columns fulfilled with Odd and if it is it would fulfill with even. Expected Result:", "q_apis": "get columns last contains dot last columns", "apis": "DataFrame apply", "code": ["df = pd.DataFrame({\"IP Address\" : \n[\"192.168.1.1\",\n\"192.168.1.2\",\n\"192.168.152.200\",\n\"192.168.54.98\",\n\"192.168.98.93\"]})\n", "        IP Address\n0      192.168.1.1\n1      192.168.1.2\n2  192.168.152.200\n3    192.168.54.98\n4    192.168.98.93\n", "df['New-variable'] = df['IP Address'].apply(lambda x:\"Odd\" if int(x.split(\".\")[-1]) % 2 else  \"Even\")\n", "        IP Address New-variable\n0      192.168.1.1          Odd\n1      192.168.1.2         Even\n2  192.168.152.200         Even\n3    192.168.54.98         Even\n4    192.168.98.93          Odd\n"], "link": "https://stackoverflow.com/questions/65306045/detemining-odd-and-even-values-with-pandas-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to map a dataset to a blank CSV file with different headers, so I'm essentially trying to map data from one CSV file which has different headers to a new CSV with different amount of headers and called different things, the reason this question is different is since the column names aren't the same but there are no overlapping columns either. And I can't overwrite the data file with new headers since the data file has other columns with irrelevant data, I'm certain I'm overcomplicating this. I've seen this example code but how do I change this since this example is using a common header to join the data. Sample Data a.csv (blank format file, the format must match this file): b.csv: Expected output file:", "q_apis": "get columns map map names columns columns join", "apis": "columns rename columns all columns merge columns columns columns columns to_csv index", "code": ["# Read the csv files\ndfA = pd.read_csv(\"a.csv\")\ndfB = pd.read_csv(\"b.csv\")\n\n# Rename the columns of b.csv that should match the ones in a.csv\ndfB = dfB.rename(columns={'MEASUREMENT': 'HEIGHT', 'COUNTRY': 'LOCATION'})\n\n# Merge on all common columns\ndf = pd.merge(dfA, dfB, on=list(set(dfA.columns) & set(dfB.columns)), how='outer')\n\n# Only keep the columns that exists in a.csv\ndf = df[dfA.columns]\n\n# Save to a new csv\ndf.to_csv(\"output.csv\", index=False)\n"], "link": "https://stackoverflow.com/questions/60650432/merging-csv-files-with-different-headers-with-pandas-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am new to pandas, I am facing issue with adding df to a string. so I have a conditional string statement like this , where bikes are the column name in a data frame. Now I want to add df before 'bikes', How do I do it? I have used this code below but it is not working my code: this gives: but not adding the df Want I want is: Is there any way to do it?", "q_apis": "get columns where name add any", "apis": "eval replace bool", "code": ["df = pd.DataFrame({'bikes':[20,39,44]})\n\nx=\"'bikes'> 20\"\n\ndf = df.query(x.replace(\"'\",''))\nprint (df)\n   bikes\n1     39\n2     44\n", "df = df.query(\"bikes > 20\")\nprint (df)\n   bikes\n1     39\n2     44\n", "x=\"'bikes'> 20\"\n\nprint (pd.eval(\"df.\" + x.replace(\"'\",'')))\n0    False\n1     True\n2     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/66223793/how-to-df-to-a-conditional-string-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have an Input Dataframe that the following : I want to concatenate TEXT column if the consecutive rows of NAME column have the same value. Output Dataframe: Is using pandas shift, the best way to do this? Appreciate any help thanks", "q_apis": "get columns value shift any", "apis": "shift cumsum rename groupby agg join reset_index drop", "code": ["(df['Name'] != df['NAME'].shift()).cumsum().rename('group')\ndf.groupby(['NAME', grp], sort=False)['TEXT']\\\n  .agg(' '.join).reset_index().drop('group', axis=1)\n", "    NAME                                               TEXT\n0    Tim  Tim Wagner is a teacher. He is from Cleveland,...\n1  Frank                                Frank is a musician\n2   Tim                  He likes to travel with his family\n3  Frank  He is a performing artist who plays the cello....\n"], "link": "https://stackoverflow.com/questions/63853639/conditionally-merge-consecutive-rows-of-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to generate a new columns using following code I get the following error: ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). The datatype is category, why do I get this error, please?", "q_apis": "get columns columns get value DataFrame empty bool item any all get", "apis": "loc isin", "code": ["city_list = ['LHR','-1','-3','LGW','MAD','SIN','KUL','JFK','HKG','PVG','IST','SDA','GLA']\nplotdata['city'] = plotdata['LOCATION']\nplotdata.loc[~plotdata['city'].isin(city_list), 'city'] = 'other'\n"], "link": "https://stackoverflow.com/questions/66860555/error-the-truth-value-of-a-series-is-ambiguous"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have unique values in a column, but they all have strange codes, and I want to instead have a numeric counter to identify these values. Is there a better way to do this? Here is the dataframe:", "q_apis": "get columns unique values all codes values", "apis": "take DataFrame assign index combine get merge", "code": ["#take the unique Doc ID's in the column\nnew_df=pd.DataFrame({'Doc_ID':df['Doc_ID'].unique()})\n#assign a unique id \nnew_df['Doc_ID_index'] = new_df.index +1\n#combine with original df to get the whole df\npd.merge(df,new_df,on='Doc_ID')\n"], "link": "https://stackoverflow.com/questions/62589918/auto-increment-index-against-unique-column-values-in-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to set a value of a cell in a pandas DataFrame that has a MultiIndex. Is there a way similar to pd.DataFrame.at[index, column] that I can use on a MultiIndex DataFrame. In the DataFrame partial_corr, I'd like to be able to get/set the value of where the first entry in .at[] is for index \"i\", the second entry is for index \"k\", and the third entry is for the column...similar to the way a single index DataFrame can return the value of df.at[\"a\", \"b\"] where the first entry is the index and the second entry is the column.", "q_apis": "get columns value DataFrame MultiIndex DataFrame at index MultiIndex DataFrame DataFrame get value where first at index second index index DataFrame value at where first index second", "apis": "loc", "code": ["idx = pd.IndexSlice\npartial_corr.loc[idx['a', 'b'], idx['b']]\nOut[431]: nan\n"], "link": "https://stackoverflow.com/questions/49563833/is-there-an-equivalent-to-pd-dataframe-atindex-column-for-a-multiindex-datafr"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "First question on SO, very new to pandas and still a little shaky on the terminology: I'm trying to figure out the proper syntax/sequence of operations on a dataframe to be able to group by column B, find the max (or min) corresponding value for each group in column C, and retrieve the corresponding value for that in column A. Suppose this is my dataframe: Using returns: So far, so good. However, I'd like to figure out how to return this: I've gotten as far as , though that returns ... which is fine for this pretend dataframe, but doesn't quite help when working with a much larger one. Thanks very much!", "q_apis": "get columns max min value value", "apis": "groupby agg idxmax loc groupby agg idxmax max", "code": ["In [322]: df.loc[df.groupby('type').votes.agg('idxmax')]\nOut[322]: \n  name type  votes\n3  max  cat      9\n0  bob  dog     10\n", "import pandas as pd\ndf = pd.DataFrame({'name': ['bob', 'pete', 'fluffy', 'max'],\n                   'type': ['dog', 'cat', 'dog', 'cat'],\n                   'votes': [10, 8, 5, 9]}, \n                  index=list('AABB'))\nprint(df)\n#      name type  votes\n# A     bob  dog     10\n# A    pete  cat      8\n# B  fluffy  dog      5\n# B     max  cat      9\n", "print(df.groupby('type').votes.agg('idxmax'))\ntype\ncat    B\ndog    A\nName: votes, dtype: object\n", "print(df.loc[df.groupby('type').votes.agg('idxmax')])\n#      name type  votes\n# B  fluffy  dog      5\n# B     max  cat      9\n# A     bob  dog     10\n# A    pete  cat      8\n", "df = df.reset_index()\n#   index    name type  votes\n# 0     A     bob  dog     10\n# 1     A    pete  cat      8\n# 2     B  fluffy  dog      5\n# 3     B     max  cat      9\n", "print(df.groupby('type').votes.agg('idxmax'))\n# type\n# cat    3\n# dog    0\n# Name: votes, dtype: int64\n\nprint(df.loc[df.groupby('type').votes.agg('idxmax')])\n#   index name type  votes\n# 3     B  max  cat      9\n# 0     A  bob  dog     10\n"], "link": "https://stackoverflow.com/questions/30880511/pandas-groupby-category-rating-get-top-value-from-each-category"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe, where a given assignment is performed by a student in a date with a relative score . For each student, the dates are sorted in descending order. I would like to add two columns: and . Column should show the averaged score obtained by each student, where the average is calculated including all the scores obtained in the previous assignments. Column should contain the difference between the current score and the previous one (which is not a NaN). The final dataframe must thus look like this: I can get this in a cumbersome way, by defining the following two functions (which take care of the possible presence of NaN entries) and using apply/lambda: I need a more efficient way (perhaps using groupby) as this one is very slow on large dataframes.", "q_apis": "get columns where date add columns where all difference between get take apply groupby", "apis": "groupby apply dropna expanding mean apply dropna diff reset_index drop reset_index drop dropna expanding mean diff DataFrame groupby apply reset_index drop", "code": ["g = df.groupby(\"s_id\")[\"score\"]\ns1 = g.apply(lambda x: x.dropna().expanding().mean())\ns2 = g.apply(lambda x: x.dropna().diff())\n\ndf[\"mean_score\"] = s1.reset_index(level=0, drop=True)\ndf[\"diff_score\"] = s2.reset_index(level=0, drop=True)\nprint(df)\n", "def mean_and_diff(series):\n    s = series.dropna()\n    d = {\"mean_score\": s.expanding().mean(), \"diff_score\": s.diff()}\n    return pd.DataFrame(d)\ntmp = df.groupby(\"s_id\")[\"score\"].apply(mean_and_diff).reset_index(level=0, drop=True)\ndf[[\"mean_score\", \"diff_score\"]] = tmp[[\"mean_score\", \"diff_score\"]]\n", "  work_id s_id  score   work_date  mean_score  diff_score\n0      a3  p01    NaN  2020-05-01         NaN         NaN\n1      a2  p01   10.0  2020-06-10   10.000000         NaN\n2      a1  p01    5.0  2020-06-15    7.500000        -5.0\n3      a5  p02    5.0  2019-10-10    5.000000         NaN\n4      a7  p02   11.0  2020-03-01    8.000000         6.0\n5      a6  p02    NaN  2020-04-01         NaN         NaN\n6      a4  p02    4.0  2020-06-20    6.666667        -7.0\n"], "link": "https://stackoverflow.com/questions/63130301/calculating-mean-and-diff-conditional-to-date-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have selected the rows that are and aren't mentioning 'Korona', and counted them by date. Some of the dates don't have Korona True. The dataframe looks like: TABLE 1 Published_date Korona Count 242 2020-06-01 False 13 243 2020-06-01 True 3 244 2020-06-02 False 7 245 2020-06-02 True 1 246 2020-06-03 False 11 247 2020-06-04 False 8 248 2020-06-04 True 1 249 2020-06-05 False 10 250 2020-06-06 False 5 251 2020-06-07 False 5 252 2020-06-08 False 14 What I'm trying to do is remove duplicate date rows, but transform the value to another column. So for example this: Published_date Korona Count 242 2020-06-01 False 13 243 2020-06-01 True 3 Looks like this (after some renaming of columns and adding an Count-All column): TABLE 2 Published_date Count-NoKorona Count-Korona Count-All 152 2020-06-01 13 3 16 And I'm doing that with this code (found it on Python, Merging rows with same value in one column ) : Problem: For some reason after that line of code, my data gets mixed up. Before that line everything was fine. I had 782 Korona True rows (For a test I only took 'True' rows from the table 1, and summed up the Count of it, and it was correct --> 782). So 782/3443 True. After the code line I get a sum of 1011/3443. I'm guessing it takes some wrong values of dates, or it gets mixed up, but I don't know how to fix it, and the data table is too big to found the mistakes manualy to try and understand the problem better. I would be grateful for any help. (Also sorry if the question doesn't look okay, it's my first :D)", "q_apis": "get columns date date transform value columns value test get sum values any first", "apis": "pivot index pivot_table values index columns sum reset_index rename columns columns sum values sum", "code": ["d = ''' Published_date  Korona  Count\n242 2020-06-01  False   13\n243 2020-06-01  True    3\n244 2020-06-02  False   7\n245 2020-06-02  True    1\n246 2020-06-03  False   11\n247 2020-06-04  False   8\n248 2020-06-04  True    1\n249 2020-06-05  False   10\n250 2020-06-06  False   5\n251 2020-06-07  False   5\n252 2020-06-08  False   14'''\n\ndf = pd.read_csv(io.StringIO(d), sep='\\s+', engine='python')\n\n# pivot the data and reset the index\ndf1 = pd.pivot_table(df, values='Count', index=['Published_date'],\n                    columns=['Korona'], aggfunc=np.sum, fill_value=0).reset_index()\n# rename the columns to what you want\ndf1.columns = ['Published_date', 'Count-NoKorona', 'Count-Korona']\n# sum the values into a new column\ndf1['Count-All'] = df1[['Count-NoKorona', 'Count-Korona']].sum(axis=1)\n", "  Published_date  Count-NoKorona  Count-Korona  Count-All\n0     2020-06-01              13             3         16\n1     2020-06-02               7             1          8\n2     2020-06-03              11             0         11\n3     2020-06-04               8             1          9\n4     2020-06-05              10             0         10\n5     2020-06-06               5             0          5\n6     2020-06-07               5             0          5\n7     2020-06-08              14             0         14\n"], "link": "https://stackoverflow.com/questions/65429280/python-merging-rows-with-the-same-id-date-but-different-values-in-one-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a folder in which have lots of excel files, I want to add one more column which is in each one and save them into another folder . Here is what I have done. It works, but a little bit slow. So if you have quicker or others ideas, welcome to share. Thanks at advance.", "q_apis": "get columns add at", "apis": "update astype to_excel join index update", "code": ["import glob\nimport threading\nimport pandas as pd\n\nsrc = \"D:/test/src/*.xls*\"\ndst = \"D:/test/dst/\"\n\ndef update(excel_file):\n    df = pd.read_excel(excel_file)\n    df['date'] = \"2019-08-01\"\n    df[\"date\"] = df[\"date\"].astype(str)\n    df.to_excel(os.path.join(dst, os.path.basename(excel_file)), index=False)\n\nfor file in glob.glob(src):\n    threading.Thread(target=update, args=(file,)).start()\n"], "link": "https://stackoverflow.com/questions/57322054/loop-excel-files-add-one-more-column-and-save-them-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm learning python and am currently trying to parse out the longitude and latitude from a \"Location\" column and assign them to the 'lat' and 'lon' columns. I currently have the following code: The splitting portion of the code works. The problem is that this code copies the lat and lon from the last cell in the dataframe to all of the 'lat' and 'lon' rows. I want it to split the current row it is iterating through, assign the 'lat' and 'lon' values for that row, and then do the same on every subsequent row. I get that assigning dd['lat'] to the split value assigns it to the whole column, but I don't know how to assign to just the row currently being iterated over. Data sample upon request:", "q_apis": "get columns parse assign columns last all assign values get value assign sample", "apis": "index apply", "code": ["def getlatlong(x):\n    return pd.Series([x.split('\\n')[2].split(',')[0][1:], \n                      x.split('\\n')[2].split(',')[1][1:-1]],\n                      index = [\"lat\", \"lon\"])\n\ndf = pd.concat((df, df.Location.apply(getlatlong)), axis=1)\n"], "link": "https://stackoverflow.com/questions/50885024/get-latitude-longitude-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I like to think every design decision is made for a reason. A lot of pandas functions (e.g. , ) come with a parameter, . If you set it to , instead of returning a new dataframe, pandas modifies the dataframe, well, in place. No surprises here ;). However, I often find my self using in combination with lambda expression to do somewhat more complex operations on columns. Consider the following example: Say I have text data that needs to be pre-processed for sentiment analysis. I would use: And then adapt my column as follows: I recently noticed that .apply does not have an argument . Since this function is mostly used to update dataframes, why is such an argument not available? What would be a rationale behind this?", "q_apis": "get columns columns apply update", "apis": "apply", "code": ["   col1  col2\n0     1     3\n1     2     4\n", "s = df.apply(lambda x: x.col1 + x.col2, axis=1)\n"], "link": "https://stackoverflow.com/questions/60975864/why-does-pandas-not-come-with-an-option-to-use-apply-in-place"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "If I have this df dataframe with 41,000 rows contains thousands of words for example like this df: and I got the frequency of every word from df to df2 with this code: and my df2 looks like this: Then how to remove all the keywords in df which has counts below 5 times based on df2 so then df would look like this: my initial trial is to make list of keywords from df2 like this: and then simply remove all that word within ListKeywords from df with this code: then I got frustrated because I have 15,000 keywords which has words counts below 5 times in all rows. Meaning that I have to put that 15,000 keywords into the ListKeywords, which is insane. Anyone can help me out of this frustation? thank you", "q_apis": "get columns contains all all all put", "apis": "stack groupby transform size groupby agg join stack value_counts index apply join", "code": ["s = df['column1'].str.split(expand=True).stack()\n\n# Keep only words with frequency above specified threshold\ncutoff = 5\ns = s[s.groupby(s).transform('size') >= cutoff]\n\n# Alignment based on original Index\ndf['column1'] = s.groupby(level=0).agg(' '.join)\n", "                                            column1  column2\n0  better better rights rights rights rights rights     2015\n1                                     better rights     2016\n2                                            better     2015\n3                                            better     2014\n", "df2 = df['column1'].str.split(expand=True).stack().value_counts()\n\ncutoff = 5\nListKeywords = df2[df2 >= cutoff].index\n#Index(['rights', 'better'], dtype='object')\n\ndf['column1'].apply(lambda x: ' '.join([i for i in x.split(' ') if i in ListKeywords]))\n", "df = pd.DataFrame({'column1': ['better spotted better rights rights rights fresh fresh rights rights',\n                               'better rights reserved', 'better', 'better horse'],\n                   'column2': [2015, 2016, 2015, 2014]})\n"], "link": "https://stackoverflow.com/questions/59845910/remove-keywords-based-on-words-counts-fom-massive-python-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this I would like to select data from redshift city table using sql where city are included in city of the dataframe Can you help me to accomplish this query? Thank you", "q_apis": "get columns select where query", "apis": "join query query where", "code": ["# create a string for cities to use in sql, the way sql expects the string\nunique_cities = ','.join(\"'{0}'\".format(c) for c in list(df['city'].unique()))\n\n# output \n'London','Paris'\n\n#sql query would be\nquery = f\"select * from city where name in ({unique_cities})\"\n"], "link": "https://stackoverflow.com/questions/54458898/select-data-from-table-and-compare-with-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What is the best way to make a series of scatter plots using from a dataframe in Python? For example, if I have a dataframe that has some columns of interest, I find myself typically converting everything to arrays: The problem with converting everything to array before plotting is that it forces you to break out of dataframes. Consider these two use cases where having the full dataframe is essential to plotting: For example, what if you wanted to now look at all the values of for the corresponding values that you plotted in the call to , and color each point (or size) it by that value? You'd have to go back, pull out the non-na values of and check what their corresponding values. Is there a way to plot while preserving the dataframe? For example: Similarly, imagine that you wanted to filter or color each point differently depending on the values of some of its columns. E.g. what if you wanted to automatically plot the labels of the points that meet a certain cutoff on alongside them (where the labels are stored in another column of the df), or color these points differently, like people do with dataframes in R. For example: How can this be done? EDIT Reply to crewbum: You say that the best way is to plot each condition (like , ) separately. What if you have many conditions, e.g. you want to split up the scatters into 4 types of points or even more, plotting each in different shape/color. How can you elegantly apply condition a, b, c, etc. and make sure you then plot \"the rest\" (things not in any of these conditions) as the last step? Similarly in your example where you plot differently based on , what if there are NA values that break the association between ? For example if you want to plot all values based on their values, but some rows have an NA value in either or , forcing you to use first. So you would do: then you can plot using like you show -- plotting the scatter between using the values of . But will be missing some points that have values for but are NA for , and those still have to be plotted... so how would you basically plot \"the rest\" of the data, i.e. the points that are not in the filtered set ?", "q_apis": "get columns columns array where now at all values values size value values values plot filter values columns plot where plot shape apply plot any last step where plot values between plot all values values value first plot between values values plot", "apis": "plot where plot", "code": ["df = pd.DataFrame(np.random.randn(10,2), columns=['col1','col2'])\ndf['col3'] = np.arange(len(df))**2 * 100 + 100\n\nIn [5]: df\nOut[5]: \n       col1      col2  col3\n0 -1.000075 -0.759910   100\n1  0.510382  0.972615   200\n2  1.872067 -0.731010   500\n3  0.131612  1.075142  1000\n4  1.497820  0.237024  1700\n", "plt.scatter(df.col1, df.col2, s=df.col3)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=df.col3)\n", "colors = np.where(df.col3 > 300, 'r', 'k')\nplt.scatter(df.col1, df.col2, s=120, c=colors)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=120, c=colors)\n", "cond = df.col3 > 300\nsubset_a = df[cond].dropna()\nsubset_b = df[~cond].dropna()\nplt.scatter(subset_a.col1, subset_a.col2, s=120, c='b', label='col3 > 300')\nplt.scatter(subset_b.col1, subset_b.col2, s=60, c='r', label='col3 <= 300') \nplt.legend()\n", "df['subset'] = np.select([df.col3 < 150, df.col3 < 400, df.col3 < 600],\n                         [0, 1, 2], -1)\nfor color, label in zip('bgrm', [0, 1, 2, -1]):\n    subset = df[df.subset == label]\n    plt.scatter(subset.col1, subset.col2, s=120, c=color, label=str(label))\nplt.legend()\n"], "link": "https://stackoverflow.com/questions/14300137/making-matplotlib-scatter-plots-from-dataframes-in-pythons-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following , I like to and check how many unique values each group has, but I like to set when some cluster only contains empty/blank , and when some cluster contains one or more empty/blank , so the result will look like,", "q_apis": "get columns unique values contains empty contains empty", "apis": "ne groupby transform all bool groupby transform nunique", "code": ["s1=df.inv_id.ne('').groupby(df.cluster_id).transform('all')\ns1\nOut[432]: \n0    False\n1    False\n2     True\n3     True\n4    False\n5    False\nName: inv_id, dtype: bool\ns2=df.groupby('cluster_id')['inv_id'].transform('nunique') == 1 \n#df['same_inv_id']=s1&s2\n"], "link": "https://stackoverflow.com/questions/54789128/pandas-transform-nunique-on-groupby-object-dealing-with-nan-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's take this example: which works perfectly fine and generates the 2x2 dataframe Now, I have a string variable (let's call it mystring) formatted exactly like this: I am trying to run this code: and I get error: ValueError: DataFrame constructor not properly called! Can anyone help me please?", "q_apis": "get columns take get DataFrame", "apis": "DataFrame DataFrame", "code": ["import json\n\ns = '[[\"0.06308\", \"1291\"], [\"0.06346\", \"1990\"]]'\ndf = pd.DataFrame(json.loads(s))\n", "from ast import literal_eval\n\ns = '[[\"0.06308\", \"1291\"], [\"0.06346\", \"1990\"]]'\ndf = pd.DataFrame(literal_eval(s))\n"], "link": "https://stackoverflow.com/questions/66302174/python-create-a-dataframe-by-feeding-a-string-file-properly-formatted"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to merge/join 2 tables in pandas on the basis of a key, but some of which do not exist in the 2nd table . The 2 tables look somewhat like this: Now the key value A3 does not exist in the second table, still I need to join these 2 tables somewhat like: How do I achieve this?", "q_apis": "get columns merge join value second join", "apis": "merge fillna merge fillna", "code": ["df = pd.merge(df1, df2, how='left', on='ID').fillna({'ATTR12COND':0})\n", "df = pd.merge(df1, df2, how='left', on='ID').fillna(0)\n"], "link": "https://stackoverflow.com/questions/68312012/merge-pandas-with-fill-empty-cells"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Code which I run in Jupyter Notebook, This is the resulting DataFrame output which I get :- Could I Moderator tidy the output layout for me, if that is okay ? These are the two key parts of the Code I am filtering Rows by Date with :- and The First line of Code, is to filter the DataFrame Row Output by two days the 10th of the Month and the 15th. it correctly outputs the earliest days in the DataFrame Output first, i.e. 10 before 15, but not in the month order I want :- I want 10th June 2004 first then the 10th of July/s then the 15th of May's then the 15th of July Rows etc. How do I modify that line of Code, so that I can filter to get that order, without changing the index position of the Rows via code, which I know how to do ? I mean add something to either lines of Code, so that the Earlier month with an the earlier day, is shown 'favoured' before the later month with the same day ? i.e. 10-Jun-2004 is shown before 10-Jul-2004 , 15-May-2004 is shown before 15-Jul-2004 Rows then. But still dates with day 10 , showing before day 15 Rows. So the Rows shown, are in the Date Order Like this :- The Date output is from this line of Code :- Any help I could be given, would be much appreciated Best Regards Eddie Winch", "q_apis": "get columns DataFrame get filter DataFrame days days DataFrame first month first filter get index mean add month day month day day day", "apis": "DataFrame apply apply apply sort_values DataFrame sort_values", "code": ["import pandas as pd\ndata = [\n{\"Name\": \"Alice\", \"date\": \"15-May-2004\", \"Rating\": 55},\n{\"Name\": \"Bob\", \"date\": \"10-Jun-2004\", \"Rating\": 11},\n{\"Name\": \"Chanel\", \"date\": \"15-Aug-2004\", \"Rating\": 33},\n{\"Name\": \"Del\", \"date\": \"10-Jul-2004\", \"Rating\": 44},\n{\"Name\": \"Erin\", \"date\": \"15-Jul-2004\", \"Rating\": 22},\n]\ndf = pd.DataFrame(data)\ndf['date'] = pd.to_datetime(df['date'])\ndf['date_day'] = df.apply(lambda row: row.date.day, axis=1)\ndf['date_month'] = df.apply(lambda row: row.date.month, axis=1)\ndf['date_year'] = df.apply(lambda row: row.date.year, axis=1)\ndf = df.sort_values(by=[\"date_day\", \"date_month\"])\n", "    Name    date        Rating  date_day    date_month  date_year\n1   Bob     2004-06-10  11      10          6           2004\n3   Del     2004-07-10  44      10          7           2004\n0   Alice   2004-05-15  55      15          5           2004\n4   Erin    2004-07-15  22      15          7           2004\n2   Chanel  2004-08-15  33      15          8           2004\n", "df = pd.DataFrame(data)\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(by=\"date\", key=lambda col: 100 * col.dt.day + col.dt.month)\n"], "link": "https://stackoverflow.com/questions/63514270/filtering-rows-in-pandas-dataframe-by-certain-date-criteria"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am working with NFL play positional tracking data where there are multiple rows per play. Such I want to organize my data as such: x_train = [[a1,b1,c1,...],[a2,b2,c2,...],...,[an,bn,cn,...]] y_train = [y1,y2,...,yn] Where x_train holds tracking data from a play and y_train holds the outcome of the play. I saw examples of using imdb data for sentiment analysis with a Keras LSTM model and wanted to try the same with my tracking data. But, I am having issues formatting my x_train. The code above goes through my play data, gets a play then finds the tracking data for that play (isolatePlay). I want it to then add the play data to my train_x array. All of my previous attempts did not work and resulted in an array [a1,b1,c1,...,a2,b2,c2,...], simply adding the plays together into an array. I have tried a variety of methods such as using the append method but to no avail. How can I properly achieve my goal of formatting this data for Machine learning? Edit: Tried this: but still ended up with just one array with no separation for the different plays. I also tried: which threw an error trying on AttributeError: 'NoneType' object has no attribute 'append' Edit 2 (Solution): Making train_x into a list as suggested by the answer worked using Output was a list of data frames of various lengths", "q_apis": "get columns where add array array array append array append", "apis": "itertuples append count", "code": ["train_x = []\nfor  rows in plays.itertuples():\n     play = isolatePlay(week, getattr(rows, 'gameId'), getattr(rows, 'playId'))\n     train_x.append([play])\n     count=len(train_x)\n"], "link": "https://stackoverflow.com/questions/65012138/python-formatting-timeseries-data-for-machine-learning"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Currently trying to calculate a ratio for a dataset that looks something like this: This dataset is a pandas dataframe. My objective is to calculate the ratio of migration from a country to another. For example the ratio of migration from 'foo' to 'bar' over 'bar' to 'foo'. In this case it would be 123/222 = 0.55 In addition if possible grouping them together in either a single dataset or multiple subsets, for example, in the following fashion: How is this possible to accomplish using pandas, numpy, etc..? Been trying to group them like so (although I can't even begin to rationalize): Can't event think of possibilities due to my lack of knowledge in pandas operations.. Any advice would be helpful, even if there's an ugly workaround this issue. Thanks!", "q_apis": "get columns", "apis": "columns merge", "code": ["df.columns = ['Country_A', 'Country_B', 'A_to_B']\n\ndf1 = pd.merge(df, df, left_on=['Country_A', 'Country_B'], right_on=['Country_B', 'Country_A'])\ndf['ratio'] = df1['A_to_B_x'] / df1['A_to_B_y']\n"], "link": "https://stackoverflow.com/questions/59089549/pandas-dataframe-groupby-where-column-a-value-column-b-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a multiIndexed DataFrame. What I want to do is remove columns if any of the cell is NaN and then get the names of level 0 index. I used data.dropna(axis= 1, inplace= True) and got the following result. Now I used print(list(data.columns.levels[0])) but shows the following output. How do I only get ['A', 'B']? Thanks in advance.", "q_apis": "get columns DataFrame columns any get names index dropna columns levels get", "apis": "dropna columns columns columns", "code": ["data.dropna(axis= 1, inplace= True)\ndata.columns= data.columns.remove_unused_levels()\n", "print(data.columns.levels[0])\n#returns Index(['A', 'B'], dtype='object')\n"], "link": "https://stackoverflow.com/questions/64737173/get-names-of-multiindex-level-0-after-using-dropna"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am working on a requirement, there are 2 CSV as below - CSV1.csv and reference.csv I tried below code - But in the output dataframe I can see blank not sure what I am missing. In output complexity column i can see just [] for each row. I have tried to get exact match but i need to get all the possible combinations so I am using get_close_matches. How to pass possibility argument which is in dataframe in below code, I am not figure out the way to pass the possibility. I have tried few other possibilities like exact but that has not given result as expected as I am looking for the all possible combinations while comparing the columns with the string", "q_apis": "get columns get get all all columns", "apis": "get apply keys explode explode apply", "code": ["# Use n=1, so only tries to get 1 match\ndf1['Complexity'] = df1['Category'].apply(lambda x: difflib.get_close_matches(x, list(my_dict.keys()), n=1))\n# The output of get_close_matches is a list, we use explode to convert it to a string\ndf1 = df1.explode('Complexity')\n# We can now apply our map, to the *Complexity* column, \n# which is technically the best match *Category*, via get_close_matches\ndf1['Complexity'] = df1['Complexity'].map(my_dict)\n"], "link": "https://stackoverflow.com/questions/66857040/pandas-get-close-matches-returning-empty-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame with the following structure: I want to add a new column that selects for each row the column that corresponds to : I've been trying to extract via which returns only : Also trying to concatenate string with doesn't work this way:", "q_apis": "get columns DataFrame add", "apis": "shape columns get values get", "code": ["r, c = df.shape\nmapper = dict(zip(df.columns.str.get(-1), \n                  np.arange(c)))\n\ndf.values[np.arange(r), \n          df.variable.str.get(4).map(mapper)]\n"], "link": "https://stackoverflow.com/questions/57560677/pandas-string-operations-dynamically-select-column-depending-on-other-columns-c"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to use the pd.get_dummies() function to convert categorical features to numerical, but the problem is that I have a column with lists.This is the genre column by the way. I have tried all the answers on the stackoverflow which addressed this issue. Nothing works I want the output to be So that I can use the get_dummies to create the dummies. Please Help!", "q_apis": "get columns get_dummies all get_dummies", "apis": "DataFrame explode pivot columns values", "code": ["d = {\"genre\":[['Action', 'Adventure', 'Comedy', 'Drama'],  \n ['Action', 'Drama', 'Mystery', 'Sci-Fi', 'Space'],  \n ['Action', 'Sci-Fi', 'Adventure', 'Comedy'],  \n ['Action', 'Magic', 'Police', 'Supernatural'],    \n ['Adventure', 'Fantasy', 'Shounen', 'Supernatu']]}\n\ndf = pd.DataFrame(d)\npd.get_dummies(df.explode(\"genre\").pivot(columns=\"genre\", values=\"genre\"))\n"], "link": "https://stackoverflow.com/questions/58187309/how-to-remove-lists-in-a-column-of-a-pandas-data-frame-for-converting-categorica"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a challenge with using Lifelines for KM estimates. I have a variable column called worker type (Full Time, Part Time, etc) that I would like to group the KM estimates for, then output to a file. Here's a snippet: When I use the print function, I get each iteration of the KM estimate per ; however, when trying to export to a file, I only get the last estimate of worker type. I've read the lifelines docs, and seen the examples for the plotting of different levels, but not sure how to bridge that to exporting to .", "q_apis": "get columns get get last levels", "apis": "T to_csv first to_csv", "code": ["worker_types = df['Emp_Status'].unique() \nwith open('C:/Users/Downloads/test.csv', 'a') as fou:\n    for i, worker_type in enumerate(worker_types): \n        ix = df['Emp_Status'] == worker_type \n        kmf.fit(T[ix], C[ix]) \n        kmf.survival_function_['worker'] = worker_type \n        if i == 0:\n            kmf.survival_function_.to_csv(fou) # write header on first iteration\n        else:\n            kmf.survival_function_.to_csv(fou, header=False)\n"], "link": "https://stackoverflow.com/questions/38591748/getting-survival-function-estimates-group-by-attribute-level-in-lifelines"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Dataframe which looks like this: i want the output in such a way, that if Val1 is present in then in the output val1 will be diplayed otherwise it will take the Val2 in the output, so my output will be: i tred pd.Concat but it is not giving correct output", "q_apis": "get columns take", "apis": "where isna", "code": ["df['Output'] = np.where(df['Val1'].isna(), df['Val2'], df['Val1'])\n"], "link": "https://stackoverflow.com/questions/59045684/compare-the-column-of-table-and-put-result-in-another-table"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "A bit complicated to explain this one (see example table below for reference). I have a dataframe with a 'Date Received' column (datetime) I want to compare the 'Date Received' with the date in the Stage columns to see if it was on-time or late. The problem I have is that each document corresponds to a different stage, for example, file 26 might have a Stage 4 Date where as File 28 might be Stage 1. How do I get Python to search for the correct stage column and then compare with date received?", "q_apis": "get columns compare date columns time where get compare date", "apis": "melt apply loc isna loc fillna", "code": ["df1 = pd.melt(df,id_vars=['Filename','Date_Received'],var_name='Expected',value_name='Date')\n\n#df1[['Date_Received','Date']] = df1[['Date_Received','Date']].apply(pd.to_datetime)\n\nprint(df1)\n\n  Filename Date_Received          Expected       Date\n0   File_1    2021-01-01  Stage_1_Expected 2020-12-15\n1   File_2    2021-01-01  Stage_1_Expected        NaT\n2   File_1    2021-01-01  Stage_2_Expected        NaT\n3   File_2    2021-01-01  Stage_2_Expected 2021-05-01\n4   File_1    2021-01-01  Stage_3_Expected        NaT\n5   File_2    2021-01-01  Stage_3_Expected        NaT\n6   File_1    2021-01-01  Stage_4_Expected        NaT\n7   File_2    2021-01-01  Stage_4_Expected        NaT\n", "df1.loc[df1['Date'].isna(),'Status'] = 'Not Received'\ndf1.loc[df1['Date'] >= df1['Date_Received'], 'Status'] = 'On Time'\ndf1['Status'] = df1['Status'].fillna('Late')\n\nprint(df1)\n\n Filename Date_Received          Expected       Date        Status\n0   File_1    2021-01-01  Stage_1_Expected 2020-12-15          Late\n1   File_2    2021-01-01  Stage_1_Expected        NaT  Not Received\n2   File_1    2021-01-01  Stage_2_Expected        NaT  Not Received\n3   File_2    2021-01-01  Stage_2_Expected 2021-05-01       On Time\n4   File_1    2021-01-01  Stage_3_Expected        NaT  Not Received\n5   File_2    2021-01-01  Stage_3_Expected        NaT  Not Received\n6   File_1    2021-01-01  Stage_4_Expected        NaT  Not Received\n7   File_2    2021-01-01  Stage_4_Expected        NaT  Not Received\n"], "link": "https://stackoverflow.com/questions/66547454/comparing-one-column-against-multiple"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to group and sum-aggregate a specific column in my dataframe and then write this entire output to excel; however, when i check the excel file after using the below code, it only contains the one aggregated column as the output and does not include any of the other grouping. I am someone could help me correct the code or provide suggestions as to how to achieve this? Thanks in advance! Next, I use the below code to write it to excel but it does not write the output into excel as in the image above why is the group by output not the same when written to excel?", "q_apis": "get columns sum aggregate contains any", "apis": "reset_index to_excel index to_excel index", "code": ["writer = pd.ExcelWriter('Test.xlsx', engine = 'xlsxwriter')\nmy_df.reset_index().to_excel(writer,sheet_name = '1', index = False)\nwriter.save()\n", "writer = pd.ExcelWriter('Test.xlsx', engine = 'xlsxwriter')\nmy_df.to_excel(writer,sheet_name = '1', index = True)\nwriter.save()\n"], "link": "https://stackoverflow.com/questions/61398604/pandas-group-by-dataframe-outputs-only-the-aggregation-column-when-written-to-ex"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "OBS: I've spent a few hours searching in SO, Pandas docs and a few others websites, but couldnt understand where my code isnt working. My UDF: Important: column does not exist. I'm creating it right now in this function. column does not exist. I'm creating it right now in this function. exists and its a float and are previously defined This function is inside a loop in the main code (but this warning is raised since n=0) Warning raised I found a few articles and questions on web and also StackOverflow saying that using would solve the problem. I tried but with no success 1\u00ba try - Using loc I also tried to use loc each one each time actually, I tried a lot of possible combinations... Tried to use in and so on Now I have the same warning, twice, but a bit different: and and I also tried using copy. At first time this warning shown up, simple using solved the problem, I dont know why now its not working (I just loaded more data) 2\u00ba Try - Using copy() I tried to place in three places, with no sucess I have no more ideas, would appreciate a lot your support. ------- Minimun Reproducible Example -------- Main_testing.py calculoindice_support.py (module 01) getitemsid_support.py (module 02) Warning output:", "q_apis": "get columns where right now right now loc loc time copy first time now copy", "apis": "copy", "code": ["while n < len(product_ids):\n    df_item = df[df[nome_coluna] == item]\n\n    df_nf_aux = indice.indice(df_item, lb, ub)\n", "def indice(dfb, lb, ub):\n    dfb['isOutlier'] = ~dfb['valor_unitario'].between(lb, ub)\n", "def indice(dfb, lb, ub):\n    dfb = dfb.copy()\n    dfb['isOutlier'] = ~dfb['valor_unitario'].between(lb, ub)\n"], "link": "https://stackoverflow.com/questions/67376483/settingwithcopywarning-message-in-pandas-python-with-df-loc"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to get output from this code as excel or csv file. But as the output is in list i am not being able to. Is there a way to convert the list object to dataframe and get the value in excel or csv? At the moment this is the output i am getting when i print(l) but i need this value store in excel or csv as row wise, Output of the code", "q_apis": "get columns get get value value", "apis": "all iloc iloc append append abs append append append DataFrame columns columns columns DataFrame columns to_csv add to_csv T", "code": ["# Big matrix with all your lists\nm = []\n\nfor r in range (0,totalRow):\n    lst = df2.iloc[r,:]\n    x=df1.iloc[r,1]\n    cnt = 0\n    l = []\n\n    for i in lst:\n        if cnt==0:\n            l.append(i)\n            cnt=1\n            continue\n        elif (i-x)==0:\n            l.append(1) # condition for my program to work on excel files\n        elif abs(i-x) > 0.2:\n            l.append(0)\n        else:\n            l.append(i)\n\n    # Append your new list to the matrix\n    m.append(l)\n\n# Create the data frame\ndf = pd.DataFrame(m)\n\n# If you have the columns names you may create the df this way\n# If the columns names are the same of df1:\ncolumns_names = df1.columns\ndf = pd.DataFrame(m, columns = columns_names)\n\nfile_name = \"your_file_name.csv\"\ndf.to_csv(file_name, m)\n\n# You can add a separator like ',', '\\t', ';' ...\n# df.to_csv(file_name, m, sep = ',')\n", "import numpy as np\n\n# Create a matrix object\nm = np.matrix(m)\n\n# Transpose your matrix\nm = m.T\n"], "link": "https://stackoverflow.com/questions/51469336/how-do-i-get-an-output-from-this-code-using-excel"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have three dataframes: I want to do the following: How can I do this using a loop, instead of separately for each dataframe?", "q_apis": "get columns", "apis": "DataFrame DataFrame DataFrame", "code": ["import pandas as pd\ndf_a = pd.DataFrame({'X':[1,2,3]})\ndf_b = pd.DataFrame({'X':[1,2,3]})\ndf_c = pd.DataFrame({'X':[1,2,3]})\n\ndfs = [df_a, df_b, df_c]\nfor df in dfs:\n    df[\"X\"] = df.X + 1\n", "for c in [\"a\", \"b\", \"c\"]:\n    tmp = globals()[\"df_{:}\".format(c)]\n    tmp[\"X\"] = tmp.X + 1\n"], "link": "https://stackoverflow.com/questions/61615473/editing-dataframes-using-loop"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Have a column containing sentences in a standard format. I am trying to retrieve the rows where sentence contains particular key words. data is like this I am trying to pass words as list (list1) and i only need the rows which have both words. I tried contains but working only for one word.", "q_apis": "get columns where contains contains", "apis": "join", "code": ["list1 = ['Corrosion', 'Bonnet']\nregex = r'\\b(?:' + '|'.join(list1) + r')\\b'\ndf[df['data'].str.contains(regex, regex=True)]\n"], "link": "https://stackoverflow.com/questions/63861215/multiple-words-search-in-a-sentence-using-pandas-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Apologies if this has been asked before, but I looked extensively without results. I'd like to create a new column that maps several values of according to some rule, say a=[1,2,3] is 1, a = [4,5,6,7] is 2, a = [8,9,10] is 3. one-to-one mapping is clear to me, but what if I want to map by a list of values or a range? I tought along these lines...", "q_apis": "get columns values map values", "apis": "DataFrame columns values values", "code": ["df = pd.DataFrame(data=np.random.randint(1,10,10), columns=['a'])\n\ncriteria = [df['a'].between(1, 3), df['a'].between(4, 7), df['a'].between(8, 10)]\nvalues = [1, 2, 3]\n\ndf['b'] = np.select(criteria, values, 0)\n", "d = {range(1, 4): 1, range(4, 8): 2, range(8, 11): 3}\n\ndf['c'] = df['a'].apply(lambda x: next((v for k, v in d.items() if x in k), 0))\n\nprint(df)\n\n   a  b  c\n0  1  1  1\n1  7  2  2\n2  5  2  2\n3  1  1  1\n4  3  1  1\n5  5  2  2\n6  4  2  2\n7  4  2  2\n8  9  3  3\n9  3  1  1\n"], "link": "https://stackoverflow.com/questions/50098025/mapping-ranges-of-values-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data as: I would like to report the strong relationship in terms of corelation between column df1 and the other columns(df2,df3,df4 and df5) The output should look like this:", "q_apis": "get columns between columns", "apis": "corrwith pop abs gt bool items", "code": ["m = df.corrwith(df.pop('df1')).abs().gt(0.7)\nprint (m)\ndf2     True\ndf3    False\ndf4    False\ndf5     True\ndtype: bool\n", "for k, v in m.items():\n    if v:\n       print (f'df1 is strongly corelated to {k}')\n    else:\n       print (f'df1 is not strongly corelated to {k}')\n       \ndf1 is strongly corelated to df2\ndf1 is not strongly corelated to df3\ndf1 is not strongly corelated to df4\ndf1 is strongly corelated to df5\n"], "link": "https://stackoverflow.com/questions/68066525/how-to-report-the-corellation-of-one-column-in-relation-to-other-columns-in-pand"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following DataFrame , which can be created as follows: And which looks like this: I want to flag the rows where the condition is for the first time, such that the expected new column would be: How to create this column?", "q_apis": "get columns DataFrame where first time", "apis": "shift", "code": ["df['hit_first'] = df['is_hit'] & (~df['is_hit']).shift(1)\n"], "link": "https://stackoverflow.com/questions/55377130/flag-only-first-row-where-condition-is-met-in-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My dataframe is: I want to get the regions in which the two largest parties have a Vote difference of less than 50 every year. So the desired output is: These are two regions where the top two parties had a Vote difference of <50 every year. I tried to groupby using \"Election Year\" and \"Region\" and then sort the Votes in descending order. But I am unable to check if the difference between top two votes of each region in every year is less than 50. how can I get the desired output?", "q_apis": "get columns get difference year where difference year groupby difference between year get", "apis": "sort_values groupby diff", "code": [">>> df = df.sort_values(['Votes'])\n>>> votes_diff = df.groupby(['Election Year', 'Region'])['Votes'].diff()\n", ">>> df.join(votes_diff.rename('\u0394 votes')).sort_index()\n    Election Year  Votes Party Region  \u0394 votes\n0            2000     50     A      a      NaN\n1            2000    100     B      a     30.0\n2            2000     70     C      a     20.0\n3            2000     26     A      b      NaN\n4            2000    180     B      b     80.0\n5            2000    100     C      b     74.0\n6            2000    120     A      c     40.0\n7            2000     46     B      c      NaN\n8            2000     80     C      c     34.0\n9            2005    129     A      a     34.0\n10           2005     46     B      a      NaN\n11           2005     95     C      a     49.0\n12           2005     60     A      b     37.0\n13           2005     23     B      b      NaN\n14           2005     95     C      b     35.0\n15           2005     16     A      c      NaN\n16           2005     65     B      c     30.0\n17           2005     35     C      c     19.0\n", ">>> top_vote_diff = votes_diff.groupby([df['Election Year'], df['Region']]).last()\n>>> top_vote_diff\nElection Year  Region\n2000           a         30.0\n               b         80.0\n               c         40.0\n2005           a         34.0\n               b         35.0\n               c         30.0\nName: Votes, dtype: float64\n", ">>> criteria = top_vote_diff.lt(50).groupby('Region').all()\n>>> criteria\nRegion\na     True\nb    False\nc     True\nName: Votes, dtype: bool\n>>> pd.Series(criteria.index[criteria])\n0   a\n1   c\nName: Region, dtype: object\n"], "link": "https://stackoverflow.com/questions/67929137/getting-a-column-from-a-grouped-dataframe-having-certain-difference-of-two-large"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a url from where I want to extract the line having data as \"Underlying Stock: NCC 96.70 As on Jun 06, 2019 10:12:20 IST\" and extract the Symbol which is \"NCC\" and Underlying Price is \"96.70\" into a list.", "q_apis": "get columns where", "apis": "get", "code": ["from bs4 import BeautifulSoup\nimport requests\n\nurl = \"https://nseindia.com/live_market/dynaContent/live_watch/option_chain/optionKeys.jsp?symbolCode=917&symbol=NCC&symbol=ncc&instrument=OPTSTK&date=-&segmentLink=17&segmentLink=17\"\nres = requests.get(url)\nsoup = BeautifulSoup(res.text)\n\n# hacky way of finding and parsing the stock data\nsoup.get_text().split(\"Underlying Stock\")[1][2:10].split(\" \")\n"], "link": "https://stackoverflow.com/questions/56471058/get-value-from-web-link"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to identify doctors based on their title in a dataframe and create a new column to indicate if they are a doctor but I am struggling with my code.", "q_apis": "get columns", "apis": "DataFrame apply", "code": ["df = pd.DataFrame({'Title':['Dr', 'dr', 'Mr'],\n               'Name':['John', 'Jim', 'Jason']})\n\ndoctorcriteria = ['Dr', 'dr']\n\ndef doctor(x):\n    if x.Title in doctorcriteria:\n        return 'Doctor'\n    else: return 'Not a doctor'\n\ndf['IsDoctor'] = df.apply(doctor, axis=1)\n", "doctor_titles = {'Dr', 'dr'}\n\ndf['IsDoctor'] = df['Title'].map(lambda title: title in doctor_titles)\n"], "link": "https://stackoverflow.com/questions/60203871/creating-a-pandas-column-based-on-values-of-another-column-using-function"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe and would like to get the 'item_id' so the 'item_price' does not vary: For example here you should get 22154, 2552. So I tried: But I get:", "q_apis": "get columns get get get", "apis": "groupby transform nunique eq loc", "code": ["items = df.groupby('item_id')['item_price'].nunique()\nitems[items == 1].index.tolist()\n# [2552, 22154]\n", "m = df.groupby('item_id')['item_price'].transform('nunique').eq(1)\ndf.loc[m, 'item_id'].unique()\n# array([22154,  2552])\n"], "link": "https://stackoverflow.com/questions/64302338/filters-the-rows-of-a-dataframe-if-the-values-of-some-columns-have-changed"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to merge two dataframes based on some columns but getting empty dataframe. Can you please help me to get proper solution? Explain: df1: df1.info() df2: df2.info() I am trying to merger these two dataframes but getting empty dataframe. I am running below code: In output I am getting: Desirable Output:", "q_apis": "get columns merge columns empty get info info empty", "apis": "astype astype astype astype", "code": ["df1['jnj_id'] = df1['jnj_id'].astype(int)\ndf2['jnj_id'] = df2['jnj_id'].astype(int)\n", "df1['jnj_id'] = df1['jnj_id'].astype(float)\ndf2['jnj_id'] = df2['jnj_id'].astype(float)\n"], "link": "https://stackoverflow.com/questions/63841108/getting-empty-data-frame-while-merging-two-dataframe-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this problem which I've been trying to solve: I want the code to take this DataFrame and group multiple columns based on the most frequent number and sum the values on the last column. For example: I would like the code to show the result below: Notice that the most frequent value on the first rows is 1000, and this way I group the column 'A' so I get the sum 284 on the column 'D'. However, on the last rows, the most frequent number, which is 88, is not on column 'A', but in column 'C'. I am trying to sum the values on column 'D' by grouping column 'C' and get 360. I am not sure if I made myself clear. I tried to use the function , but it does not show the desired result aforementioned. Is there any pandas-style way of resolving this? Thanks in advance!", "q_apis": "get columns take DataFrame columns sum values last value first get sum last sum values get any style", "apis": "groupby transform count count count sum sum sum", "code": ["def get_count_sum(col, func):\n    return df.groupby(col).D.transform(func)\n    \nga = get_count_sum('A', 'count')\ngb = get_count_sum('B', 'count')\ngc = get_count_sum('C', 'count')\n\n\nconditions = [\n    ((ga > gb) & (ga > gc)),\n    ((gb > ga) & (gb > gc)),\n    ((gc > ga) & (gc > gb)),\n]\n\nchoices = [get_count_sum('A', 'sum'), \n           get_count_sum('B', 'sum'),\n           get_count_sum('C', 'sum')]\n\ndf['D'] = np.select(conditions, choices)\ndf\n", "    A       B   C   D\n0   1000    380 380 284\n1   1000    380 380 284\n2   1000    270 270 284\n3   1000    270 270 284\n4   1000    270 270 284\n5   200     45  88  360\n6   200     45  88  360\n7   500     45  88  360\n8   500     55  88  360\n"], "link": "https://stackoverflow.com/questions/67311140/how-can-i-group-multiple-columns-and-sum-the-last-one"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "input file contains the product and its price on a particular date output file should, combine all the days of month in one column and concatenate values with separated with comma (,) i tried to change column name with date format , from '1-jan-2020' to 'jan-2020' with and after df transpose we can use groupby. like there is option to group by and sum the values as :- is there something that can join values (string operation) with separate them with comma. click here to get sample data any direction is appreciated.", "q_apis": "get columns contains product date combine all days month values name date transpose groupby sum values join values get sample any", "apis": "sample set_index columns columns T groupby agg xs join filter notnull xs T", "code": ["inp = pd.read_excel(\"Stackoverflow sample.xlsx\")\n\ndf = inp.set_index(\"Product\")\ndf.columns = pd.to_datetime(df.columns)\n\nout = (\n    df\n    .T\n    .groupby(pd.Grouper(level=0, freq=\"MS\"))\n    .agg(lambda xs: \", \".join(map(str, filter(pd.notnull, xs))))\n    .T\n)\n"], "link": "https://stackoverflow.com/questions/67906741/combine-dataframe-column-with-similar-name-and-concate-values-with-separated-by"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two datframes that look like this I want to update the sales in for client1 with the sum of client1's sales in and client1's sales in where the posting_periods match. In other words The actual dataframes I'm working with are much larger, but these examples capture what I'm trying to accomplish. I came up with a very round-about way that not only didn't work, it wasn't very pythonic. The other challenge is the additional column in not in . I was hoping someone could suggest an alternative. Thank you!", "q_apis": "get columns update sum where round", "apis": "set_index", "code": ["idx_cols = ['posting_period', 'name']\ns = df2.set_index(idx_cols)['sales']\n", "print(df1)\n\n   posting_period     name  sales  profit\n0               1  client1   60.0    10.0\n1               1  client2  100.0    20.0\n2               2  client1  170.0    30.0\n"], "link": "https://stackoverflow.com/questions/51480105/updating-pandas-dataframe-with-value-equal-to-sum-of-same-df-and-another-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe and a dictionary I would like to use to compare with and if equal to add the value from to in to a new column that the result would be", "q_apis": "get columns compare add value", "apis": "DataFrame columns fillna", "code": ["import pandas as pd\n\ndata = {'A':['a','a',3,3,'c','c','d'],'B': [0, .5, 101, 105, 2000, 2500, 3000]}\ndf = pd.DataFrame(data, columns = ['A','B'])\n\nm = dict(zip(d['a'],d['b']))\ndf['C'] = df['B'] + df['A'].map(m).fillna(0)  # Fillna to avoid Not A Number (NaN\n\nprint(df)\n", "   A       B       C\n0  a     0.0     5.0\n1  a     0.5     5.5\n2  3   101.0   301.0\n3  3   105.0   305.0\n4  c  2000.0  5000.0\n5  c  2500.0  5500.0\n6  d  3000.0  3000.0\n"], "link": "https://stackoverflow.com/questions/51668261/how-to-associate-values-from-a-python-dict-to-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I currently have a Data Frame that looks like so when I read it in: Date Country A B C 01/01/2020 AFG 0 1 5 01/02/2020 AFG 2 5 0 01/03/2020 AFG 1 4 1 ... ... ... ... ... 01/01/2020 USA 2 3 7 01/02/2020 USA 4 5 6 I would like to transform it into the form below, whereby the country becomes the row's index, date replaces the columns, and the values of Column A go onto fill the date's respective value for each country. Country 01/01/2020 01/02/2020 01/03/2020 ... 04/25/2021 AFG 0 2 1 ... 5 USA 2 4 9 ... 15 I have tried to use group-by before but nothing appears to be working quite in the way shown above. Am I forgetting a command or is there some way this can be done?", "q_apis": "get columns transform index date columns values date value", "apis": "pivot_table index columns values rename_axis reset_index", "code": ["df = df.pivot_table(index='Country', columns='Date', values='A', fill_value=0).rename_axis(None, axis=1).reset_index()\n", "  Country  01/01/2020  01/02/2020  01/03/2020\n0     AFG           0           2           1\n1     USA           2           4           0\n"], "link": "https://stackoverflow.com/questions/67728773/flip-a-data-frame-in-pandas-and-keep-one-columns-values-as-the-new-rows-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Question How to drop rows with repeated values in a certain column and keep the first, only when they are next to each other? The pandas method is not an answer, as it drops all the duplicated rows, even when they are not next to each other. Code Example My desired output is shown as follows, with the variable As you can see above, only duplicates that are next to each other are deleted.", "q_apis": "get columns drop values first all duplicated", "apis": "shift shift", "code": ["example_df[example_df['Day'].shift(1) != example_df['Day']]\n\n    name    Day\n0   John    Monday\n2   Sarah   Tuesday\n4   Lala    Monday\n5   Mike    Tuesday\n", "example_df['Day_1_shifted'] = example_df['Day'].shift(1)\n\n    name    Day     Day_1_shifted\n0   John    Monday  NaN\n1   Mery    Monday  Monday\n2   Sarah   Tuesday Monday\n3   Jay     Tuesday Tuesday\n4   Lala    Monday  Tuesday\n5   Mike    Tuesday Monday\n"], "link": "https://stackoverflow.com/questions/65619971/how-to-delete-the-duplicates-and-keep-the-first-row-only-when-these-rows-are-nex"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to add a column 'is_drama' to df that will take the value True or False depending on whether the content belongs to the Dramas category on Netflix in the colums listed_in Can you tell me what the function would be and how I could add this new column to my dataframe? Thanks", "q_apis": "get columns add take value add", "apis": "apply apply", "code": ["   title            listed_in\n0     3%  TV Dramas, TV Shows\n1   7:19               Horror\n2  23:59       Dramas, Movies\n", "   title            listed_in  in_drama\n0     3%  TV Dramas, TV Shows      True\n1   7:19               Horror     False\n2  23:59       Dramas, Movies      True\n", "def fn(x):\n    return \"Drama\" in x\n\n\ndf[\"in_drama\"] = df[\"listed_in\"].apply(fn)\n\nprint(df)\n", "   title            listed_in  in_drama\n0     3%  TV Dramas, TV Shows      True\n1   7:19               Horror     False\n2  23:59       Dramas, Movies      True\n", "df[\"in_drama\"] = df[\"listed_in\"].apply(lambda x: \"Drama\" in x)\n"], "link": "https://stackoverflow.com/questions/67046702/add-a-column-to-a-dataframe-that-will-take-the-value-true-or-false-with-a-functi"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've been toying around with an idea for a program at work that automates our end of the month reports. Currently, it creates all the reports for us in Excel format and then we manually use Excel's subtotal feature to subtotal its columns and format the data into a table. My idea is to subtotal each of the columns by customer, like so: Patient Date Rx# Description Qty Price EXAMPLE, JOHN 2/1/2021 357649 Aspirin 30 6.99 EXAMPLE, JOHN 2/1/2021 357650 Drug 30 13.99 EXAMPLE, JOHN 2/1/2021 357651 Tylenol 30 7.99 EXAMPLE, JOHN Subtotal 28.97 EXAMPLE, SUSAN 2/12/2021 357652 Expensive Drug 30 51.99 EXAMPLE, SUSAN 2/12/2021 357653 Drug 30 13.99 EXAMPLE, SUSAN 2/12/2021 357654 Tylenol 30 7.99 EXAMPLE, SUSAN Subtotal 73.97 With the exisiting dataframe looking like: Patient Date Rx# Description Qty Price EXAMPLE, JOHN 2/1/2021 357649 Aspirin 30 6.99 EXAMPLE, JOHN 2/1/2021 357650 Drug 30 13.99 EXAMPLE, JOHN 2/1/2021 357651 Tylenol 30 7.99 EXAMPLE, SUSAN 2/12/2021 357652 Expensive Drug 30 51.99 EXAMPLE, SUSAN 2/12/2021 357653 Drug 30 13.99 EXAMPLE, SUSAN 2/12/2021 357654 Tylenol 30 7.99 Is this possible with groupby()? It seems to have an option to group by rows rather than columns. The bigger issue I see is inserting into the existing dataframe, as it seems that pandas is more designed for manipulating/performing operations on large datasets rather than inserting/adding information.", "q_apis": "get columns at month all columns columns groupby columns", "apis": "groupby agg sum sort_values", "code": ["# Calculate sums\ndf_subtotal = df.groupby('Patient', as_index=False)[['Price']].agg('sum')\n# Manipulate string Patient\ndf_subtotal['Patient'] = df_subtotal['Patient'] + ' subtotal'\n# Join dataframes\ndf_new = pd.concat([df, df_subtotal], axis=0, ignore_index=True)\n# Sort\ndf_new = df_new.sort_values(['Patient', 'Date'])\n"], "link": "https://stackoverflow.com/questions/66806957/efficient-subtotaling-of-columns-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to get matrix of TF-IDF features from the text stored in columns of a huge dataframe, loaded from a CSV file (which cannot fit in memory). I am trying to iterate over dataframe using chunks but it is returning generator objects which is not an expected variable type for the method TfidfVectorizer. I guess I am doing something wrong while writing a generator method shown below. Can anybody please advise how to modify the method above, or any other approach using dataframe. I would like to avoid creating separate text files for each row in the dataframe. Following is some dummy csv file data for recreating the scenario.", "q_apis": "get columns get columns any", "apis": "values", "code": ["def ChunkIterator(filename):\n    for chunk in pd.read_csv(csvfilename, chunksize=1):\n        for document in chunk['text_column'].values:\n            yield document\n", "import csv\ndef doc_generator(filepath, textcol=0, skipheader=True):\n    with open(filepath) as f:\n        reader = csv.reader(f)\n        if skipheader:\n            next(reader, None)\n        for row in reader:\n            yield row[textcol]\n", "In [1]: from sklearn.feature_extraction.text import TfidfVectorizer\n\nIn [2]: import csv\n   ...: def doc_generator(filepath, textcol=0, skipheader=True):\n   ...:     with open(filepath) as f:\n   ...:         reader = csv.reader(f)\n   ...:         if skipheader:\n   ...:             next(reader, None)\n   ...:         for row in reader:\n   ...:             yield row[textcol]\n   ...:\n\nIn [3]: vectorizer = TfidfVectorizer()\n\nIn [4]: result = vectorizer.fit_transform(doc_generator('testing.csv', textcol=1))\n\nIn [5]: result\nOut[5]:\n<4x9 sparse matrix of type '<class 'numpy.float64'>'\n    with 21 stored elements in Compressed Sparse Row format>\n\nIn [6]: result.todense()\nOut[6]:\nmatrix([[ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n          0.        ,  0.38408524,  0.        ,  0.38408524],\n        [ 0.        ,  0.6876236 ,  0.        ,  0.28108867,  0.        ,\n          0.53864762,  0.28108867,  0.        ,  0.28108867],\n        [ 0.51184851,  0.        ,  0.        ,  0.26710379,  0.51184851,\n          0.        ,  0.26710379,  0.51184851,  0.26710379],\n        [ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n          0.        ,  0.38408524,  0.        ,  0.38408524]])\n"], "link": "https://stackoverflow.com/questions/53754234/creating-a-tfidfvectorizer-over-a-text-column-of-huge-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 8 functions that I would like to run under one main() function. The process starts with importing from a file and creating a df and then doing some cleaning operations on that df under a new function. I have copied in the basic structure including the three starting functions and then a main() function. What I am unsure about is how to 'carry' the result of loader() to clean_data() and then the result of clean_data() to operation_one() in the right way. At the moment I get an error that df is not defined. Thank you for your help!", "q_apis": "get columns right get", "apis": "to_excel index", "code": ["import pandas as pd\nimport numpy as np\n\ndef loader():\n    df = pd.read_excel('file_example.xlsx')\n    return df\n\n\ndef clean_data(df):\n    del df['column_7']\n    return df\n\n\ndef operation_one(df):\n    del df['column_12']\n    return df\n\n\ndef main():\n    df = loader()\n    df = clean_data(df)\n    df = operation_one(df)\n\n    with pd.ExcelWriter(\"file.xlsx\") as writer:\n        df.to_excel(writer, sheet_name='test', index=False)\n\n    if __name__ == \"__main__\":\n        main()\n"], "link": "https://stackoverflow.com/questions/63331363/carrying-resulting-dataframes-through-different-functions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes with multi-indexes looking like this: df1 df2 The two are not the same size, and the values don't always overlap, but every index pair found in df1 is in df2. What I would like to do is update the observation col in the df1 with the values of in df2, wherever it matches. In other words, I would like to do the equivalent of an inner join based on the multi-index, and then overwrite the values in in df1 with those from df2. But is there a way to do this in one step, using /indexing? (This is structured as an index problem, but if there is a way to solve it using that would be fine too.) Desired output:", "q_apis": "get columns size values index update values join index values step index", "apis": "DataFrame index index", "code": ["df2 = pd.DataFrame({'observation': {('foo', '2017-04-16'): 'green',\n  ('bar', '2017-04-25'): 'red',\n  ('zap', '2017-04-16'): 'red',\n  ('zip', '2017-04-25'): 'blue',\n  ('zip', '2017-04-16'): 'white'},\n 'observation': {('zap', '2017-04-16'): 'yellow',\n  ('bar', '2017-04-27'): 'white',\n  ('foo', '2017-05-16'): 'black',\n  ('foo', '2017-04-25'): 'red',\n  ('zip', '2017-08-16'): 'red'}})\n\ndf['observation'] = df.index.map(dict(zip(df2.index, df2.observation)))\n", "               observation\nbar 2017-04-27       white\nfoo 2017-04-25         red\n    2017-05-16       black\nzap 2017-04-16      yellow\nzip 2017-08-16         red\n"], "link": "https://stackoverflow.com/questions/54218874/get-subset-of-pandas-df-where-multiple-columns-match-values-from-another-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe which has 4 columns I have to search for a substring in each column and return the complete dataframe in the search order for example if I get the substring in column row then my final would be having rows. For this I am using and it's working fine but one of the column consist each element in the column as list of strings like in column so is not working for column pls suggest how can I search in this column and maintain the order of complete dataframe.", "q_apis": "get columns columns get", "apis": "apply", "code": ["df1 = df[df['B'].apply(lambda x: 'cvb' in x)]\nprint (df1)\n       A              B      C       D\n0  asdfg  [asdfgh, cvb]  asdfg  nbcjsh\n", "df1 = df[df['B'].str.join(' ').str.contains('er')]\nprint (df1)\n       A                    B        C        D\n1  fghjk              [ertyu]   fghhjk    yrewf\n2   xcvb  [qwerr, hjklk, bnm]    cvbvb  gjfsjgf\n3  ertyu              [qwert]  ertyhhu   ertkkk\n", "df2 = (df[df.assign(B = df['B'].str.join(' '))\n           .apply(' '.join, axis=1)\n           .str.contains('g')]\n      )\nprint (df2)\n       A                    B       C        D\n0  asdfg        [asdfgh, cvb]   asdfg   nbcjsh\n1  fghjk              [ertyu]  fghhjk    yrewf\n2   xcvb  [qwerr, hjklk, bnm]   cvbvb  gjfsjgf\n"], "link": "https://stackoverflow.com/questions/61672366/search-string-in-dataframe-column-that-contains-lists-of-string-and-return-compl"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I do have a dataframe like this: I now want to get an output like this: So, it is the index of a multiindex dataframe which is ordered accoring to the of each group in . I currently do it like this: which gives the desired output. Is there a way to achieve the same output but without creating this intermediate column which is dropped later anyway?", "q_apis": "get columns now get index ordered", "apis": "set_index loc value_counts index", "code": ["df.set_index('Val').loc[df.Val.value_counts().index]\n\nOut[44]:\n    IDs\nVal\nfoo   a\nfoo   c\nfoo   g\nfoo   h\nbar   b\nbar   e\nbar   f\nabc   d\n"], "link": "https://stackoverflow.com/questions/58783908/how-to-create-a-multiindex-dataframe-that-is-sorted-according-to-group-size"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "A function returns a Pandas DataFrame. I try to create a new DataFrame with \"newFrame = myFunction()\". But the newFrame variable acts more like a reference than a distinct object. Can you help? In these snippets, FrameMaker.py updates a DataFrame object on a timer. It has a function get_frame that returns that DataFrame object. Multiple scripts call that function to get a copy of the DataFrame. GetFrameData.py is an example. It calls the get_frame function and assigns the returned value to a variable. It should then have its own copy of the DataFrame, and anything it does to its copy of the DataFrame should have no effect on any other copies. But it does cause an effect. GetFrameData.py drops columns from the DataFrame. The first time that runs, the \"values.drop\" line runs successfully. The second time it runs, it gives an error that the DataFrame doesn't have the columns in question. It's like the 'values' variable is a reference to the DataFrame in FrameMaker.py, not a distinct object in the GetFrameData stack. Kind of like how Strings function in most languages. How should I change my code to get a copy of the DataFrame object, not a reference?", "q_apis": "get columns DataFrame DataFrame DataFrame DataFrame get copy DataFrame value copy DataFrame copy DataFrame any columns DataFrame first time values drop second time DataFrame columns values DataFrame stack get copy DataFrame", "apis": "add DataFrame columns copy", "code": ["FrameMaker.py\n\ndef update_frame(myFrame):\n    #Code to remove old rows and add new rows to myFrame\n    return myFrame\n\n#Initialize the myFrame object\nmyFrame = update_frame(pd.DataFrame(columns = ['timestampx','length','dscp','srcip','destip']))\n\n#Code to run \"myFrame = update_frame(myFrame)\" on a timer.\n\ndef get_frame:\n    return myFrame.copy() # Copy the DF instead of returning the same reference\n"], "link": "https://stackoverflow.com/questions/67713683/pandas-and-confusion-over-passing-by-value-and-passing-by-reference"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this I want to group them by url and status and split a records by date, is it a more efficient way to do that? Here is a dataframe: 1000,20191109,active 1000,20191108,inactive 2000,20191109,active 2000,20191101,inactive 351,20191109,active 351,20191102,active 351,20191026,active 351,20191019,active 351,20191012,active 351,20191005,active 351,20190928,inactive 351,20190921,inactive 351,20190914,inactive 351,20190907,active 351,20190831,active 351,20190615,inactive 3000,20200101,active", "q_apis": "get columns date", "apis": "sort_values where shift shift astype sort_values where shift shift groupby cumsum replace ffill groupby agg min max", "code": ["import numpy as np\n\ndf.sort_values([\"url\", \"date_scraped\"], axis=0, ascending=True, inplace=True)\n\ndf[\"date_scraped_till\"]=np.where(df[\"url\"]==df[\"url\"].shift(-1), \n\ndf[\"date_scraped\"].shift(-1), np.nan).astype(np.int32)\n", "     url  date_scraped    status  date_scraped_till\n15   351      20190615  inactive           20190831\n14   351      20190831    active           20190907\n13   351      20190907    active           20190914\n12   351      20190914  inactive           20190921\n11   351      20190921  inactive           20190928\n10   351      20190928  inactive           20191005\n9    351      20191005    active           20191012\n8    351      20191012    active           20191019\n7    351      20191019    active           20191026\n6    351      20191026    active           20191102\n5    351      20191102    active           20191109\n4    351      20191109    active                  0\n1   1000      20191108  inactive           20191109\n0   1000      20191109    active                  0\n3   2000      20191101  inactive           20191109\n2   2000      20191109    active                  0\n16  3000      20200101    active                  0\n", "import numpy as np\n\ndf.sort_values([\"url\", \"date_scraped\"], axis=0, ascending=True, inplace=True)\n\ndf[\"test\"]=np.where((df[\"url\"]==df[\"url\"].shift(1)) & (df[\"status\"]==df[\"status\"].shift(1)), 0,1)\n\ndf[\"test\"]=df.groupby([\"url\", \"status\", \"test\"])[\"test\"].cumsum().replace(to_replace=0, method='ffill')\n\ndf_g = df.groupby(['url', 'status', 'test'])['date_scraped'].agg({min, max})\n", "                    max       min\nurl  status   test\n351  active   1     20190907  20190831\n              2     20191109  20191005\n     inactive 1     20190615  20190615\n              2     20190928  20190914\n1000 active   1     20191109  20191109\n     inactive 1     20191108  20191108\n2000 active   1     20191109  20191109\n     inactive 1     20191101  20191101\n3000 active   1     20200101  20200101\n"], "link": "https://stackoverflow.com/questions/59460167/python-dataframe-iterate-over-rows-compare-a-values-between-them-and-prepare"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have 10k rows of customers per employee name and sent emails, this is a basic sample of the dataframe: df Now I want to make a report for each customer to see if they already opened emails from different employees and notify employees with 0 Opened emails. What I did so far is: 1- Grouping the dataframe by customer and employee and used with : 2- 3- I want to Iterate over each customer and see if the of top > 0, then notify other employees with 0 that this customer is receiving emails. The issue that I cannot find the best way to access values and do the condition, this is what I tried but it seems not a good way with lots of errors Thank you", "q_apis": "get columns name sample values", "apis": "astype groupby transform sum groupby transform sum loc gt eq", "code": ["# convert opened emails to integer\ndf['Opened Emails'] = df['Opened Emails'].astype(int)\n\n# opened emails by customer\ncs = df.groupby('Customer')['Opened Emails'].transform('sum')\n\n# opened emails by customer and employee\necs = df.groupby(['Customer', 'Employee'])['Opened Emails'].transform('sum')\n\n# employee-customer pairs with 0 opened emails\n# while overall customer opened emails is greater than 0\ndf.loc[(cs.gt(0)) & (ecs.eq(0))]\n", "  Employee   Customer  Opened Emails\n1     John  CustomerA              0\n2   Silvia  CustomerA              0\n3     Sara  CustomerB              0\n"], "link": "https://stackoverflow.com/questions/67754772/iterate-and-sum-values-based-on-a-condition-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm working in Python with a dataframe by_year, which has columns payee, payment_date and amount. To calculate the sum of the amount totals for each month of the year, I ran the following line of code 12 times for each month Wondering if there's a more efficient way to write a block of code that would generate the amounts for every month Edit: Here's an excerpt of the data:", "q_apis": "get columns columns sum month year month month", "apis": "groupby sum", "code": ["by_year['payment_date'] = pd.to_datetime(by_year['payment_date'])\nyear = 2020\ndf = by_year[by_year['payment_date'].dt.year == year]\ndf.groupby([df['payment_date'].dt.month]).sum()\n"], "link": "https://stackoverflow.com/questions/65015998/is-there-a-python-function-that-can-generate-multiple-sums-so-i-can-avoid-multip"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes one large: myTradeFrame (7401x27) one small: specialsData (3x3) specialsData looks like this: and then the code is this: this line of code gives me an error even though the key columns are present in both dataframes. What am I missing? Basically, the NewColumn in myTradeFrame should have the values of the column \"maturity_max\" at the intersection coll_cusip and tran_type", "q_apis": "get columns columns values at intersection", "apis": "merge", "code": ["myTradeFrame = pd.merge(myTradeFrame, specialsData, how='left', on=[ 'coll_cusip', 'tran_type'],suffixes=('_left','_right'))\n"], "link": "https://stackoverflow.com/questions/52441715/left-join-and-valueerror-wrong-number-of-items-passed-55-placement-implies-1"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame that contains a numeric column and a list that contains strings as elements. I want to create a new column in that data frame where each number in the numeric column corresponds to the index of the list. Example: Expected output:", "q_apis": "get columns contains contains where index", "apis": "apply", "code": ["df['bar'] = [bar[x] for x in df['foo']]\n", "df['bar'] = df['foo'].apply(lambda x: bar[x])\nprint(df)\n", "   foo      bar\n0    0      cat\n1    0      cat\n2    1      dog\n3    1      dog\n4    2  chicken\n5    2  chicken\n"], "link": "https://stackoverflow.com/questions/60419678/create-a-new-column-from-a-pandas-dataframe-based-on-a-numeric-column-and-indice"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am working on text data having shape of (14640,16) using Pandas and Spacy for preprocessing but having issue in getting lemmetized form of text. Moreover, if I work with pandas series (i.e dataframe with one column) which contain only text column there are different issue with that also. Code:(Dataframe) Result After this I iterate over the column with parsed_tweets to get lemmetized data but get the error. Code: Error Code: (Pandas Series) Error Can someone help me with the errors? I tried other stackoverflow solution but can't get doc object of Spacy to iterate over it and get tokens and lemmetized tokens. What am I doing wrong?", "q_apis": "get columns shape get get Series get get", "apis": "get apply shape", "code": ["#you can directly get your lemmatized token by running list comprehension in your lambda function  \n\ndf['parsed_tweets'] = df['text'].apply(lambda x: [y.lemma_ for y in  nlp(x)])\n", "print(type(df['parsed_tweets'][0]))\n#op\nspacy.tokens.doc.Doc\n\n\nfor i in range(df.shape[0]):\n    for word in df['parsed_tweets'][i]:\n        print(word.lemma_)\n#op\nplay\nfootball\ni\nbe\nwork\nhard\n"], "link": "https://stackoverflow.com/questions/58843519/lemmetization-issue-using-spacy-in-pandas-series-and-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two Dataframes first one has my main data. I am taking one of the columns from the first dataframe and making another dataframe with string split. first value from the df1 is and second is now I want to create a column which should have values for row 0 and I am trying this loop: But I am getting an error:", "q_apis": "get columns first columns first first value second now values", "apis": "DataFrame", "code": ["import pandas as pd\n\ndf = pd.DataFrame({\"TABLE_NAME\":[\"T_STG_PRG_POS_NORM_FAREAST\",\n                                 \"T_STG_PRG_POS_NORM_EXEC_DBIT\"]})\n\ndf[\"SYSTEM_NAME\"] = df[\"TABLE_NAME\"].str.split(\"_\").str[-1]\n\n"], "link": "https://stackoverflow.com/questions/57674295/pandas-iterating-over-two-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe df1 with column name Acc Number as the first column and the data looks like: I need to make a new dataframe df2 that will have two columns first having the text part and the second having the numbers so the desired output is: How would I go about doing this? Thanks!", "q_apis": "get columns name first columns first second", "apis": "DataFrame columns columns", "code": ["import pandas as pd\n\ndata = ['ASC100.1',\n'MJT122',\n'ASC120.4',\n'XTY111']\n\ndf = pd.DataFrame(data=data, columns=['col'])\n\nresult = df.col.str.extract('([a-zA-Z]+)([^a-zA-Z]+)', expand=True)\nresult.columns = ['Text', 'Number']\nprint(result)\n", "  Text Number\n0  ASC  100.1\n1  MJT    122\n2  ASC  120.4\n3  XTY    111\n"], "link": "https://stackoverflow.com/questions/53290902/python-pandas-splitting-text-and-numbers-in-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have few doubts with subsetting grouping the data. My actual data format looks like this I need to convert this above data to this format Can anyone please suggest to get the data in this above format.", "q_apis": "get columns get", "apis": "pivot_table values index columns fillna reset_index", "code": ["import pandas as pd\n\nresult = pd.pivot_table(df, values=\"usage_count\", index=[\"userId\", \"userEmail\"], columns=\"month\").fillna(0).reset_index()\nprint(result)\n", "month      userId            userEmail  December  January\n0      aabzhlxycj  jakiyah@academy.com       2.0      2.0\n1      aacuvynjwq       jack@gmail.com       1.0      1.0\n2      aailjxciyk      maria@gmail.com       0.0      2.0\n"], "link": "https://stackoverflow.com/questions/60450235/grouping-data-based-on-month-wise-as-a-column-and-row-with-user-data-using-panda"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have these functions calculating the value of time strings in seconds. What is a shorter way to achieve this using built-in methods?", "q_apis": "get columns value time seconds", "apis": "sum eq eq sum eq eq apply apply sum eq eq", "code": ["in_seconds = {'hours': 60 * 60, 'minutes': 60, 'seconds': 1}\nsec = sum(int(num) * in_seconds[eq] for num, eq in re.findall(r'(\\d+)\\s?(hours|minutes|seconds)', \"54 minutes 45 seconds\"))\n", "import re\n\ndef sec(x):\n    in_seconds = {'hours': 60 * 60, 'minutes': 60, 'seconds': 1}\n    return sum(int(num) * in_seconds[eq] for num, eq in re.findall(r'(\\d+)\\s?(hours|minutes|seconds)', x))\n\n\ndf.Matching = df.Matching.apply(sec)\n", "in_seconds = {'hours': 60 * 60, 'minutes': 60, 'seconds': 1}\n\ndf.Matching = df.Matching.str.findall('(\\d+)\\s?(hours|minutes|seconds)').apply(lambda x: sum(int(num) * in_seconds[eq] for num, eq in x))\n"], "link": "https://stackoverflow.com/questions/62420048/what-is-a-shorter-way-to-rewrite-this-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame like: And I'd like to re-format it like: Any idea what would be an efficient way to do this? The dataset is small so it doesn't really have to be super efficient. Thank you in advance for your help.", "q_apis": "get columns", "apis": "groupby pivot_table index columns first columns columns reset_index", "code": ["df['col'] = df.groupby('person_ID').cumcount()\n\nout=df.pivot_table(index=['person_ID','first_name','last_name'], \n                   columns='col', aggfunc='first')\nout.columns = [f'{x}_{y}' for x,y in out.columns]\nout = out.reset_index()\n", "   person_ID first_name last_name  feature_1_0  feature_1_1  feature_2_0  feature_2_1  feature_3_0  feature_3_1\n0          1       John   Talbert            1            7            2            8            3            9\n1          2        Ted  Thompson            4           13            5           14            6           15\n"], "link": "https://stackoverflow.com/questions/66252565/how-to-merge-all-rows-in-a-pandas-data-frame-with-the-same-value-for-a-specific"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am using several functions and i am using them to print values but now I need further calculation. So, I need to convert the printing values in a special format. This is how my code looks: and its printing values like this: But now I want the values in RGB format and that should be like this: So then I would like to get the column as dataframe which will give output like this:", "q_apis": "get columns values now values values now values get", "apis": "append", "code": ["L = [(color.color.red, color.color.green, color.color.blue) for color in dominant_colors.colors]\n", "L = []\nfor color in dominant_colors.colors:\n    L.append((color.color.red, color.color.green, color.color.blue))\n"], "link": "https://stackoverflow.com/questions/63242702/how-to-use-for-loop-to-convert-the-values-into-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe which is structured as: How can I plot based on the ticker the versus ?", "q_apis": "get columns plot", "apis": "groupby axes keys axes plot", "code": ["grouped = df.groupby('ticker')\n\nncols=2\nnrows = int(np.ceil(grouped.ngroups/ncols))\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,4), sharey=True)\n\nfor (key, ax) in zip(grouped.groups.keys(), axes.flatten()):\n    grouped.get_group(key).plot(ax=ax)\n\nax.legend()\nplt.show()\n"], "link": "https://stackoverflow.com/questions/41494942/pandas-dataframe-groupby-plot"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a two dataframes: and . looks like this And And want to make something like this I tried this code, but it wont save data that match", "q_apis": "get columns", "apis": "isin isin where where isin isin where where columns isin where", "code": ["m1 = df_out['phone_number'].isin(df1['phone_number1'])\nm2 = df_out['phone_number'].isin(df1['phone_number2'])\ndf_out['df1_phone_number1'] = df_out['phone_number'].where(m1)\ndf_out['df1_phone_number2'] = df_out['phone_number'].where(m2)\nprint (df_out)\n   phone_number address   name  df1_phone_number1  df1_phone_number2\n1           123    add1  name1              123.0              123.0\n2        777777    add2  name2                NaN                NaN\n3           666    add3  name3                NaN              666.0\n4           555    add4  name4              555.0                NaN\n", "m1 = df_out['phone_number'].isin(df1['phone_number1'])\nm2 = df_out['phone_number'].isin(df1['phone_number2'])\ndf_out['df1_phone_number1'] = np.where(m1, df_out['phone_number'], None)\ndf_out['df1_phone_number2'] = np.where(m2, df_out['phone_number'], None)\nprint (df_out)\n   phone_number address   name df1_phone_number1 df1_phone_number2\n1           123    add1  name1               123               123\n2        777777    add2  name2              None              None\n3           666    add3  name3              None               666\n4           555    add4  name4               555              None\n", "for c in df1.columns:\n    m = df_out['phone_number'].isin(df1[c])\n    df_out[f'df1_{c}'] = np.where(m, df_out['phone_number'], None)\n"], "link": "https://stackoverflow.com/questions/59659247/merge-two-dataframes-based-on-two-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes, and : And I want to create a new column () in that is if there is a record in where or matches AND is less then or equal to . The result would look like this: Basically, I want to check, for each row in if there is a historical record for that . Hope the question makes sense!", "q_apis": "get columns where", "apis": "melt drop sort_values drop_duplicates merge assign drop sort_values assign notnull drop", "code": ["unique_id = (df_b.melt('date_b', value_name='ID')\n                .drop('variable',axis=1)\n                .sort_values('date_b')\n                .drop_duplicates('ID'))\n\n(df_a.merge(unique_id,\n           on='ID',\n           how='left'\n          )\n     .assign(V=lambda x: x.date_b <= x.date_a)\n     .drop('date_b',axis=1)\n)\n", "                      date_a ID      V\n0 2020-01-09 01:01:01.000001  a   True\n1 2020-01-04 01:01:01.000001  a  False\n2 2020-01-01 01:01:01.000001  c  False\n3 2020-01-06 01:01:01.000001  a   True\n", "# unique_id as above\n(pd.merge_asof(df_a.sort_values('date_a'), \n              unique_id, \n              left_on='date_a',\n              right_on='date_b', \n              by='ID')\n   .assign(V=lambda x: x.date_b.notnull())\n   .drop('date_b', axis=1)\n)\n", "                      date_a ID      V\n0 2020-01-01 01:01:01.000001  c  False\n1 2020-01-04 01:01:01.000001  a  False\n2 2020-01-06 01:01:01.000001  a   True\n3 2020-01-09 01:01:01.000001  a   True\n"], "link": "https://stackoverflow.com/questions/61483674/create-col-based-on-matching-historical-records-from-other-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a histogram with a bunch of binned data and I was wondering if it would be possible to say generate a table if I select a bar from the histogram and it would display the data as it is in the original dataframe.", "q_apis": "get columns select", "apis": "align rank rank rank align align align count count align rank rank rank align align align", "code": ["import altair as alt\nfrom vega_datasets import data\n\nsource = data.cars()\n\n# Brush for selection\nbrush = alt.selection(type='interval')\n\n# Scatter Plot\npoints = alt.Chart(source).mark_point().encode(\n    x='Horsepower:Q',\n    y='Miles_per_Gallon:Q',\n    color=alt.condition(brush, alt.value('steelblue'), alt.value('grey'))\n).add_selection(brush)\n\n# Base chart for data tables\nranked_text = alt.Chart(source).mark_text(align='right').encode(\n    y=alt.Y('row_number:O',axis=None)\n).transform_window(\n    row_number='row_number()'\n).transform_filter(\n    brush\n).transform_window(\n    rank='rank(row_number)'\n).transform_filter(\n    alt.datum.rank<16\n)\n\n# Data Tables\nhorsepower = ranked_text.encode(text='Horsepower:N').properties(title=alt.TitleParams(text='Horsepower', align='right'))\nmpg = ranked_text.encode(text='Miles_per_Gallon:N').properties(title=alt.TitleParams(text='MPG', align='right'))\norigin = ranked_text.encode(text='Origin:N').properties(title=alt.TitleParams(text='Origin', align='right'))\ntext = alt.hconcat(horsepower, mpg, origin) # Combine data tables\n\n# Build chart\nalt.hconcat(\n    points,\n    text\n).resolve_legend(\n    color=\"independent\"\n).configure_view(strokeWidth=0)\n", "import altair as alt\nfrom vega_datasets import data\n\n\nsource = data.cars()\n\n# Brush for selection\nbrush = alt.selection(type='single', encodings=['x'])\n\n# Histogram base\nhist_base = alt.Chart(source).mark_bar(color='grey').encode(\n    x=alt.X('Horsepower:Q', bin=True),\n    y='count()',\n).add_selection(brush)\n\n# Histogram selection\nhist_selection = alt.Chart(source).mark_bar().encode(\n    x=alt.X('Horsepower:Q', bin=True),\n    y='count()',\n).transform_filter(brush)\n\n# Base chart for data tables\nranked_text = alt.Chart(source).mark_text(align='right').encode(\n    y=alt.Y('row_number:O',axis=None)\n).transform_window(\n    row_number='row_number()'\n).transform_filter(\n    brush\n).transform_window(\n    rank='rank(row_number)'\n).transform_filter(\n    alt.datum.rank<16\n)\n\n# Data Tables\nhorsepower = ranked_text.encode(text='Horsepower:N').properties(title=alt.TitleParams(text='Horsepower', align='right'))\nmpg = ranked_text.encode(text='Miles_per_Gallon:N').properties(title=alt.TitleParams(text='MPG', align='right'))\norigin = ranked_text.encode(text='Origin:N').properties(title=alt.TitleParams(text='Origin', align='right'))\ntext = alt.hconcat(horsepower, mpg, origin) # Combine data tables\n\n# Build chart\nalt.hconcat(\n    hist_base+hist_selection,\n    text\n).resolve_legend(\n    color=\"independent\"\n).configure_view(strokeWidth=0)\n"], "link": "https://stackoverflow.com/questions/67997825/python-altair-generate-a-table-on-selection"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am new to ML and Data Science (recently graduated from Master's in Business Analytics) and learning as much as I can by myself now while looking for positions in Data Science / Business Analytics. I am working on a practice dataset with a goal of predicting which customers are likely to miss their scheduled appointment. One of the columns in my dataset is \"Neighbourhood\", which contains names of over 30 different neighborhoods. My dataset has 10,000 observations, and some neighborhood names only appear less than 50 times. I think that neighborhoods that appear less than 50 times in the dataset are too rare to be analyzed properly by machine learning models. Therefore, I want to remove the names of the neighborhoods from the \"Neighborhood\" column which appear in that column less than 50 times. I have been trying to write a code for this for several hours, but struggle to get it right. So far, I have gotten to the version below: I have also tried other versions of code to get rid of the rows in that categorical column, but I keep getting a similar error: I appreciate your help in advance, and thank you for sharing your knowledge and insights with me!", "q_apis": "get columns now columns contains names names names get right get", "apis": "value_counts loc isin index", "code": ["counts = my_df['Neighborhood'].value_counts()\nnew_df = my_df.loc[my_df['Neighborhood'].isin(counts.index[counts > 50])]\n"], "link": "https://stackoverflow.com/questions/60858422/how-to-remove-rows-from-a-categorical-variable-whose-value-counts-do-not-satisfy"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have dataframe similar to the one bellow I want to remove text and keep digits only from each coloumn in that Dataframe The expected output something like this So far I have tried this", "q_apis": "get columns", "apis": "DataFrame get rename columns T T T T", "code": ["df = pd.DataFrame({'a':['abc123abc', 'def456678'], 'b':['123a', 'b456']})\n", "    a           b\n0   abc123abc   123a\n1   def456678   b456\n", "    a       b\n0   123     123\n1   456678  456\n", "from pandas.io.json import json_normalize\nimport requests\nimport pandas as pd\n\nURL = 'https://wastemanagement.post-iot.lu/measurement/measurements?source=83512& pageSize=1000000000&dateFrom=2019-10-26&dateTo=2019-10-28'\nreq = requests.get(URL,auth=('xxxx', 'xxxx') )\ntext_data= req.text\njson_dict= json.loads(text_data)\ndf= json_normalize(json_dict['measurements'])\ndf = df_final.rename(columns={'source.id': 'source', 'battery.percent.value': 'battery', 'c8y_TemperatureMeasurement.T.value': 'Temperature Or T','c8y_DistanceMeasurement.distance.value':'Distance'})\ncols_to_keep =['source' ,'battery', 'Temperature Or T', 'time', 'Distance']\ndf_final = df[cols_to_keep] \n", "    source  battery Temperature Or T    time                        Distance\n0   83512   98.0    NaN                 2019-10-26T00:00:06.494Z    NaN\n1   83512   NaN     23.0                2019-10-26T00:00:06.538Z    NaN\n2   83512   NaN     NaN                 2019-10-26T00:00:06.577Z    21.0\n3   83512   98.0    NaN                 2019-10-26T00:30:06.702Z    NaN\n4   83512   NaN     23.0                2019-10-26T00:30:06.743Z    NaN\n"], "link": "https://stackoverflow.com/questions/58912848/removing-text-and-keep-digits-in-panda"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame which can contain columns with the same column name. Based on the value I want to rename the column name so there are no duplicates. I've tried a few things, but every time I try to iterate over the columns and rename them I end up with the column name. df.rename(columns=df.columns[i]: 'some_name'}) seems to use the column name as well. Let's say I have a dataframe; I would like to rename the column(s) named \"A\" based on the row value so that I get I tried something like this: But this also renames the first column 'A'. Is there another way to rename it based on location?", "q_apis": "get columns DataFrame columns name value rename name time columns rename name rename columns columns name rename value get first rename", "apis": "DataFrame DataFrame columns applymap any iloc columns DataFrame DataFrame applymap any columns iloc columns", "code": ["import pandas as pd\n\ndf = pd.concat([pd.DataFrame({\"A\": ['10kg'], \"B\": ['4']}), \n                pd.DataFrame({\"A\": ['4%']})], axis=1)\n\n\ndf.columns = [c + '_%'\n              if df.applymap(lambda x: '%' in x).any(axis=0).iloc[ic]\n              else c for ic, c in enumerate(df.columns)]\n", "import pandas as pd\n\ndf = pd.concat([pd.DataFrame({\"A\": ['10kg'], \"B\": ['4']}), \n                pd.DataFrame({\"A\": ['4%']})], axis=1)\n\nhas_percentage = df.applymap(lambda x: '%' in x).any(axis=0)\ndf.columns = [c + '_%' if has_percentage.iloc[ic]\n              else c for ic, c in enumerate(df.columns)]\n"], "link": "https://stackoverflow.com/questions/58861560/rename-column-with-same-column-name-based-on-values-in-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hi Friend I'm new here \ud83d\ude0a, Make a matrix from most repeated words in specific column and add to my data frame with names of selected column as label. What I have: What is my goal: I want to do: 1- Separate the string & count the words in specific column 2- Make a Zero-Matrix 3- The new matrix should be labelled with founded words in step 1 (my-problem) 4- Search every row, if the word has been founded then 1 else 0 The new data frame what I have as result: What I did: What I wanted: The Words in in step 3 should be label of new matrix instead 0 1 2 ...", "q_apis": "get columns add names count step step", "apis": "DataFrame explode value_counts reset_index rename index shape shape agg count DataFrame columns columns values count add values at insert insert insert rename all columns columns", "code": ["# 3- Countung the seprated words and the frequency of repetation\ndf_word_count=pd.DataFrame(df.A.str.split(' ').explode().value_counts()).reset_index().rename({'index':\"A\",\"A\":\"Count\"},axis=1)\ndisplay(df_word_count)\nlist_word_count=list(df_word_count[\"A\"])\nlen(list_word_count)\n", "# 4- Make a ZERO-Matrix \nallfeatures=np.zeros((df.shape[0],len(list_word_count)))\nallfeatures.shape\n\n# 5- Make a For-Loop \nfor i in range(len(list_word_count)):\n  allfeatures[:,i]=df['A'].agg(lambda x:x.split().count(list_word_count[i]))\n\n# 6- Concat the data\nComplete_data=pd.concat([df,pd.DataFrame(allfeatures)],axis=1)\ndisplay(Complete_data)\n", "# 7- change columns name from list\n#This creates a list of the words you wanted\n    l = list(df_word_count[\"A\"])\n# if you see this, it shows only the words you have in the column A\n# but the result dataset that you showed you wanted, you also had some columns #that had values such as word count, etc. So we need to add that. We do this by #inserting those values you want in the list, at the beginning\n    l.insert(0,\"char_count\")\n    l.insert(0,\"word_count\")\n    l.insert(0,\"A\")\n    \n# Finally, I rename all the columns with the names that I have in the list l\n    Complete_data.columns = l\n"], "link": "https://stackoverflow.com/questions/66102516/how-can-i-assign-the-words-from-a-specific-column-as-a-label-to-a-new-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this I have created the tab using the code Now I would like another column where I can also get the IDs of particular group, my ideal output would be : can anyone help me in this issue. Thank you in advance!", "q_apis": "get columns where get", "apis": "DataFrame columns apply join append append append apply", "code": ["df = pd.DataFrame([\n    ['1', 'Boy','','','',''],\n    ['1', 'Boy','1-1','play','',''],\n    ['1', 'Boy','1-1','play','1-1-21','car'],\n    ['1', 'Boy','1-1','play','1-1-1','online'],\n    ['2', 'Girl','','','',''],\n    ['2', 'Girl','','dance','','']], columns=['L_1','D_1','L_2','D_2','L_3','D_3']\n)\ndf['C_N'] = df[['D_1','D_2', 'D_3']].apply(lambda x: '|'.join(x), axis=1)\n\ndef get_data(x,y,z):\n    result = []\n    if x != '':\n        result.append(x)\n    if y != '':\n        result.append(y)\n    if z != '':\n        result.append(z)\n    return result[-1]\n\ndf['IDs'] = ''\ndf['IDs'] = df.apply(lambda row: get_data(row['L_1'], row['L_2'], row['L_3']), axis=1)\n"], "link": "https://stackoverflow.com/questions/67921612/create-id-column-in-dataframe-based-on-other-column-values-pandas-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have some excel files that are not always structured the same; so I'm reading the pandas dataframe with the headers=None parameter. I'm then doing some checks to get the header row index position. I have a list of mandatory columns I need to check against before passing the row index. My dataframe: I need to return since row 3 contains all of the items contained in my list. If any are missing return None. I've looked at but it seems to only return a dataframe of bools, I can't seem to figure out how to get only the index. Some things to note, the column positions may be out of order in the files so I need to be able to look through all columns dynamically with this check. Also the row can contain more than just the mandatory_cols, as long as it has all the mandatory columns I would want the index position. Thanks!", "q_apis": "get columns get index columns index contains all items any at get index all columns all columns index", "apis": "values index", "code": ["df[[set(mandatory_cols).issubset(x) for x in df.values.tolist()]].index\nOut[1098]: Int64Index([3], dtype='int64')\n"], "link": "https://stackoverflow.com/questions/49655995/how-do-i-get-the-index-of-a-row-in-pandas-if-the-row-contains-all-items-in-a-lis"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This question builds on a question already answered here - Pandas Cumulative Sum using Current Row as Condition To extend this question I am looking to find the maximum number of concurrent unique users, so I would need to ignore (or not count) any instances where the same user is connecting more than once during the period (row) in question. I have used the answer to the question referenced above to be able to count the maximum concurrent users at the time for each row but it doesn't take into account the user. One additional issue to point out here is that there may be overlaps in terms of a user log with itself. What I mean is the following is possible for a single user: I have created the code below to get the max concurrent users: I have tried including a check against the user name but I was only able to filter out users that were the same as the user in the current row, not duplicates for all concurrent users. This is an example of the data from the csv input: The code currently returns the following output csv: What I want to return is this, adjusting the concurrent count if the user has already been counted for the row in question, this output shows the user ady only counted once for each concurrent event calculation: Any help or ideas are much appreciated, thanks in advance.", "q_apis": "get columns unique count any where count at time take overlaps mean get max name filter all count", "apis": "index loc loc groupby DataFrame", "code": ["for i in df.index:\n    active_events[i] = len(df[(df[\"START_TIME\"]<=df.loc[i,\"START_TIME\"]) & (df[\"END_TIME\"]> df.loc[i,\"START_TIME\"])].groupby('USER_NAME'))\nlast_columns = pd.DataFrame({'CONCURRENT_EVENTS' : pd.Series(active_events)})\n", "   SESSION_ID          START_TIME            END_TIME USER_NAME  CONCURRENT_EVENTS\n0       45030 2020-03-29 14:37:00 2020-03-29 19:01:00       jkk                  1\n1       45033 2020-03-29 14:46:00 2020-03-29 16:23:00       ady                  2\n2       45035 2020-03-29 14:54:00 2020-03-29 18:27:00       ady                  2\n3       45036 2020-03-29 15:51:00 2020-03-29 17:34:00       drm                  3\n4       45040 2020-03-29 17:38:00 2020-03-29 22:07:00       ady                  2\n5       45042 2020-03-29 18:58:00 2020-03-29 20:25:00       djx                  3\n"], "link": "https://stackoverflow.com/questions/66121671/count-max-concurrent-unique-users-for-a-log-file"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to replaces the values from dictionary type to numeric value as the example below. How could I extract them and change the values in the DataFrame? eg.)", "q_apis": "get columns values value values DataFrame", "apis": "transform get", "code": ["df['544299'] = df['544299'].transform(lambda d: float(d.get('kWh', np.nan)))\n"], "link": "https://stackoverflow.com/questions/62112045/extracting-and-replacing-value-in-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to get the first and last names of people who work in HR department. This is my code it shows me error AttributeError: 'DataFrame' object has no attribute 'unique' I need the output to be like this", "q_apis": "get columns get first last names DataFrame unique", "apis": "loc drop_duplicates", "code": ["df.loc[df['Department'] == 'HR',\n       ['FirstName', 'LastName', 'Department']].drop_duplicates()\n"], "link": "https://stackoverflow.com/questions/66056422/get-names-based-on-another-column-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to find all rows where a certain value is present inside the column's list value. So imagine I have a set up like this: How can I get all rows where exists in the users column? Or... is the real problem that I should not have my data arranged like this, and I should instead explode that column to have a row for each user? What's the right way to approach this?", "q_apis": "get columns all where value value get all where explode right", "apis": "all where where", "code": ["\n# user_filter filters the dataframe to all the rows where\n# 'userID' is NOT in the 'users' column (the value of which\n# is a list type)\nuser_filter = df['users'].map(lambda u: userID not in u)\n\n# cuisine_filter filters the dataframe to only the rows\n# where 'cuisine' exists in the 'cuisines' column (the value\n# of which is a list type)\ncuisine_filter = df['cuisines'].map(lambda c: cuisine in c)\n\n# Display the result, filtering by the weight assigned\ndf[user_filter & cuisine_filter]\n\n"], "link": "https://stackoverflow.com/questions/60257829/python-pandas-find-rows-where-element-is-in-rows-array"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I make a function that accepts a dataframe as input: And returns a dataframe, where a certain delimiter number (in the example, it is 6) is the passed parameter: Here's what I got: How can I simplify the function and make it more versatile? How do I make the function faster? Thanks.", "q_apis": "get columns where", "apis": "where where", "code": ["sep = 6\ndf.array = df.array.apply(lambda a:\n    np.split(a, 1 + np.where(np.array(a) == sep)[0][:-1]))\n\ndf = df.set_index('string').explode('array').reset_index()\n\n#   string               array\n# 0    xxx  [1, 2, 3, 4, 5, 6]\n# 1    xxx        [1, 2, 3, 6]\n# 2    xxx                 [6]\n# 3    xxx     [2, 2, 3, 5, 6]\n# 4    yyy              [2, 6]\n# 5    yyy                 [6]\n", "a = [1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 6, 2, 2, 3, 5, 6]\nsep = 6\nnp.where(np.array(a) == sep)[0]\n\n# array([ 5,  9, 10, 15])\n", "np.split(a, np.where(np.array(a) == sep)[0])\n\n# [array([1, 2, 3, 4, 5]),\n#  array([6, 1, 2, 3]),\n#  array([6]),\n#  array([6, 2, 2, 3, 5]),\n#  array([6])]\n", "np.split(a, 1 + np.where(np.array(a) == sep)[0][:-1])\n\n# [array([1, 2, 3, 4, 5, 6]),\n#  array([1, 2, 3, 6]),\n#  array([6]),\n#  array([2, 2, 3, 5, 6])]\n"], "link": "https://stackoverflow.com/questions/67016378/converting-a-dataframe-with-a-line-separator"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I a writing a small python script to convert the excel into cvs, but there are few rows which I need to eliminate before my cvs: my current code is: So for example: here I don't want rows where census-included is NO, Nopes, and Error in my datasets.", "q_apis": "get columns where", "apis": "isin isin", "code": ["df = df[~df[\"census-included\"].isin([\"NO\", \"Nopes\", \"Error\"])]\n", "df =  df[~df[\"Unnamed:3\"].isin([\"NO\", \"Nopes\", \"Error\"])]\n"], "link": "https://stackoverflow.com/questions/63800580/pandas-skip-rows-on-cell-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I already have a dictionary of data frames, I would like to loop over each data frame of the dictionary and and group them based on the column named: Size and then store for each group of the data in a new data frames B. My problem is: for each iteration, B will be replaced by a newer data frame. I would like to have all the data frames for all possible groups. Anyone has any ideas on how to do that? Small example: I want to have this output: (see picture), with my code the data frames are overwritten with each iteration. So I have in the end three data frames instead of five.", "q_apis": "get columns all all groups any", "apis": "DataFrame DataFrame values groupby items DataFrame DataFrame items groupby append", "code": ["import pandas as pd\n\ndata = {'Name': ['Tom', 'nick', 'krish', 'jack', 'Kody', 'Kim'], 'Age': [20, 21, 19, 18, 6, 6],\n        'Size': ['M', 'M', 'L', 'S', 'S', 'M']}\ndata2 = {'Name': ['Jason', 'Damon', 'Ronda', 'Kylie', 'Ron', 'Harry'], 'Age': [20, 12, 11, 13, 6, 5],\n         'Size': ['L', 'M', 'L', 'M', 'L', 'L']}\ndf = pd.DataFrame(data)\ndf2 = pd.DataFrame(data2)\nA = {}\nA[0] = df\nA[1] = df2\nB = {}\nnew_df = pd.concat(A.values())\ngroups = new_df.groupby([\"Size\"])\nfor group in groups:\n    B[group[0]] = group[1]\n\nfor k, v in B.items():\n    print(f\"{k}: {v}\")\n", "L:     Name  Age Size\n2  krish   19    L\n0  Jason   20    L\n2  Ronda   11    L\n4    Ron    6    L\n5  Harry    5    L\nM:     Name  Age Size\n0    Tom   20    M\n1   nick   21    M\n5    Kim    6    M\n1  Damon   12    M\n3  Kylie   13    M\nS:    Name  Age Size\n3  jack   18    S\n4  Kody    6    S\n", "import pandas as pd\n\ndata = {'Name': ['Tom', 'nick', 'krish', 'jack', 'Kody', 'Kim'], 'Age': [20, 21, 19, 18, 6, 6],\n        'Size': ['M', 'M', 'L', 'S', 'S', 'M']}\ndata2 = {'Name': ['Jason', 'Damon', 'Ronda', 'Kylie', 'Ron', 'Harry'], 'Age': [20, 12, 11, 13, 6, 5],\n         'Size': ['L', 'M', 'L', 'M', 'L', 'L']}\ndf = pd.DataFrame(data)\ndf2 = pd.DataFrame(data2)\nA = {}\nA[0] = df\nA[1] = df2\nB = []\nfor key, value in A.items():\n    groups = value.groupby([\"Size\"])\n    for group in groups:\n        B.append(group[1])\n\nfor x in B:\n    print(x)\n", "    Name  Age Size\n2  krish   19    L\n   Name  Age Size\n0   Tom   20    M\n1  nick   21    M\n5   Kim    6    M\n   Name  Age Size\n3  jack   18    S\n4  Kody    6    S\n    Name  Age Size\n0  Jason   20    L\n2  Ronda   11    L\n4    Ron    6    L\n5  Harry    5    L\n    Name  Age Size\n1  Damon   12    M\n3  Kylie   13    M\n"], "link": "https://stackoverflow.com/questions/64250610/how-can-i-store-grouped-data-in-seperate-data-frames-in-a-nested-for-loop"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's say I have the following dataframes: and I can locate the rows in df_t1 that meet a certain condition using the code below: I can create a for loop that returns those same results: But when I try to use that code to attach those values to df_t2: df_t2 is unchanged", "q_apis": "get columns values", "apis": "DataFrame columns DataFrame columns loc", "code": ["df_t1 = pd.DataFrame([[\"AAA\", 1 ,2],[\"BBB\", 0, 3],[\"CCC\", 1, 2],[\"DDD\", 0, 0],[\"EEE\", 0, 3]], columns=list('ABC'))\n\ndf_t2 = pd.DataFrame([[\"XXX\", 4, 1],[\"YYY\", 5 ,6],[\"ZZZ\", 0, 3]], columns=list('ABC'))\n\nvalue_check = [2, 3]\n\nfor i in value_check:\n    condition = (df_t1['B'] <= i) & (df_t1['C'] > i)\n    row_to_add = df_t1.loc[condition]\n\n    df_t2 = pd.concat([df_t2, row_to_add], axis=0)\n"], "link": "https://stackoverflow.com/questions/54357005/how-to-append-a-dataframe-row-to-another-within-a-for-loop-using-loc"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have Pandas DataFrame in this form: How can I transform this into a new DataFrame with this form: I am beginning to use Seaborn and Plotly for plotting, and it seems like they prefer data to be formatted in the second way.", "q_apis": "get columns DataFrame transform DataFrame second", "apis": "set_index index set_index unstack set_index unstack reset_index columns index values rename columns columns", "code": ["df.set_index('Date')#Sets Date as index\ndf.set_index('Date').unstack()#Flips, melts the dataframe\nd=df.set_index('Date').unstack().reset_index()# resets the datframe and allocates columns, those in index become level_suffix and attained values become 0\nd.rename(columns={'level_0':'Name',0:'Score'})#renames columns\n"], "link": "https://stackoverflow.com/questions/64328033/how-can-i-transform-a-dataframe-so-that-the-headers-become-column-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 dataframes that have multiple columns in each of them. Both dataframes have the column \"SendID\" and \"SendDateTime\" as the only same columns. Both dataframes have \"SendID\" completely filled out. Df1 is missing 30,000 \"SendDateTime\". Df2 has all \"SendID\" and \"SendDateTime\" filled out. I want to get the missing \"SendDateTime\" in df1 from df2 by using the \"SendID\". Both dataframes have multiple columns that don't match up. df1 (7 columns) df2 (8 columns) Desired output in df1 (same 7 columns) so all missing \"SendDateTime\" are filled in using \"SendID\" in df2: I have tried: I have also tried: How can I do this correctly?", "q_apis": "get columns columns columns all get columns columns columns columns all", "apis": "drop_duplicates drop columns merge", "code": ["df2_t = df2[['SendID','SendDateTime']].drop_duplicates()\ndf1.drop(columns='SendDateTime').merge(df2_t, on='SendID')\n", "   SendID          Link             SendDateTime\n0   12345      Text Box   10/12/2019  8:00:00 AM\n1   12345  View Browser   10/12/2019  8:00:00 AM\n2   98765          News   11/19/2019  9:30:00 AM\n3   98765        Social   11/19/2019  9:30:00 AM\n4   45678          Shop  12/24/2019  11:00:00 AM\n5   45678        Button  12/24/2019  11:00:00 AM\n"], "link": "https://stackoverflow.com/questions/66787634/how-to-fill-only-missing-values-in-one-dataframe-column-with-values-from-another"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe documenting when a product was added and removed from basket. However, the column contains two sets of information for the color set and the shape set. See below: I would like to split out the two sets of information contained in into columns and and drop . so the previous df should look like: I attempted first splitting out the columns in a for loop and then aggregating with groupby: However this left me with a dataframe of only two columns and multi-level index that i wasn't sure how to unstack. Any help on this is greatly appreciated!", "q_apis": "get columns product contains shape columns drop first columns groupby left columns index unstack", "apis": "where pivot_table index columns values first reset_index", "code": ["df['key'] = np.where(df['set_name'].str.contains('COLOR'), 'color_set', 'shape_set')\n\ndf.pivot_table(\n  index=['eff_date', 'prod_id', 'change_type'],\n  columns='key',\n  values='set_name',\n  aggfunc='first'\n).reset_index()\n", "key  eff_date  prod_id change_type       color_set      shape_set\n0    20150414    20770         ADD  MONO COLOR SET  REC SHAPE SET\n1    20150429      132         ADD  MONO COLOR SET  REC SHAPE SET\n2    20150521      199         ADD  MONO COLOR SET  TET SHAPE SET\n3    20150521      199        DROP  MONO COLOR SET  REC SHAPE SET\n"], "link": "https://stackoverflow.com/questions/57418236/aggregating-rows-in-python-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm completely lost and need your help. I have N datasets each with m columns and x*N lines in a form of lists. x = amount of subjects N = amount of tasks param1 & param2 = parameters that are converging Each of the datasets consists of linear model parameters for each case for each subject This trend continues to df_N until we have x * N lines in the df_N. I need to stack all the df's in one, with some rules, so that df_final will still have x * N lines,but for each id and each task, if we have information about tasks before this task we will append them together, the order of the columns does not matter, the dtype of columns also does not matter, the NA values should not be 0. Information about task should be stacked by id of person The final result should look like this: HUGE thanks for your help! edit: sorry for long formatting, now looks like its done (this is for science).", "q_apis": "get columns columns stack all append columns dtype columns values now", "apis": "DataFrame columns set_index iterrows loc loc sort_index", "code": ["final_df = pd.DataFrame(columns=['id','df'])\nfinal_df.set_index(['id','df'], inplace=True)\nfor i, df in enumerate(datasets):\n    for _,row in df.iterrows():\n        final_df.loc[(int(row['id']),i+1), str(int(row['task'])) + 'param1'] = row['param1']\n        final_df.loc[(int(row['id']),i+1), str(int(row['task'])) + 'param2'] = row['param2']\n\nfinal_df.sort_index(inplace=True)\n", "       1param1  1param2  2param1  2param2  3param1  3param2\nid df\n1  1       1.0     0.50      NaN      NaN      NaN      NaN\n   2       1.2     0.40      3.2     1.10      NaN      NaN\n   3       1.1     0.35      3.1     1.05      2.2      0.7\n2  1       2.0     0.80      NaN      NaN      NaN      NaN\n   2       2.1     0.80      3.2     2.20      NaN      NaN\n   3       2.3     0.80      3.2     2.20      1.1      1.1\n"], "link": "https://stackoverflow.com/questions/59494412/how-to-stack-data-with-python-and-pandas-in-this-shape"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've tried using with but so far haven't succeeded. This is the DataFrame: How could I get a result like this? (Counting occurences of each result separately for each user) (Or it could be transposed, I don't mind) Also, what would be the efficient way to convert that to a DataFrame of percentages?", "q_apis": "get columns DataFrame get DataFrame", "apis": "pivot_table index columns values sum fillna astype", "code": ["df[\"_dummy\"]=1\ndf.pivot_table(index=\"user\", columns=\"game_result\", values=\"_dummy\", aggfunc=\"sum\").fillna(0).astype(\"int\")\n", "#df\n    user game_result\n0   john         win\n1    kim        draw\n2  alice        draw\n3   john       loose\n4    kim         win\n5   john       loose\n\n#pivot_table\ngame_result  draw  loose  win\nuser\nalice           1      0    0\njohn            0      2    1\nkim             1      0    1\n"], "link": "https://stackoverflow.com/questions/59617837/number-of-occurences-in-a-dataframe-grouped-by-other-column-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Pandas dataframe that I want to insert into a Teradata table. It appears as though python is trying to include the index as a column and it doesn't match the number of columns in Teradata, throwing this error: How can I get this to work?", "q_apis": "get columns insert index columns get", "apis": "to_records index", "code": ["with aos_td.default_session(username='my_username', password='my_password', \n                        system='blah.blah.com') as session:\n\nsession.executemany(\"\"\"SCHEMA.TABLE(col1,col2,col3,col4) \n                    VALUES (?,?,?,?)\"\"\", df2.to_records(index=False))\n"], "link": "https://stackoverflow.com/questions/50458188/write-to-teradata-with-python-aos-td"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Working with pandas dataframe in python3, I tried to call dataframe constructor on tuple of tuples. It resulted in an improper constructor call error. A quick reference to documentation of pandas.DataFrame revealed that data parameter can be initialized with numpy ndarray (structured or homogeneous), dict, or DataFrame, Dict can contain Series, arrays, constants, or list-like objects. I'm unable to reckon the reason for tuple of tuples being invalid and list of tuples being valid. I converted the tuple of tuples into list of tuples, and it saved my ass. I expected tuple of tuples to work, but list of tuples work, tuple of tuples don't.", "q_apis": "get columns DataFrame DataFrame Series", "apis": "ndim", "code": ["elif isinstance(data, (list, types.GeneratorType)):\n    if isinstance(data, types.GeneratorType):\n        data = list(data)\n    if len(data) > 0:\n        if is_list_like(data[0]) and getattr(data[0], 'ndim', 1) == 1:\n           # ....\n"], "link": "https://stackoverflow.com/questions/54489274/tuple-of-tuples-not-allowed-in-pandas-dataframe-constructor"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes, one containing IP addresses (), one containing IP networks (). The IP's and Networks are of the type and , which enables checking if an IP lies in the Network (). The dataframes look as follows: I want to merge/join with , adding the name of the network in which the IP lies per row. For this small instance, it should return the following: My actual dataframes are much larger, so id prefer to not use for-loops to maintain efficiency. How can I best achieve this? If this requires changing the datatypes, that's okay. Note: I've added code below to create the data for convenience.", "q_apis": "get columns merge join name", "apis": "apply apply size loc apply", "code": ["ip & netmask == network_address", "import numpy as np\nnet_masks = df_network.NETWORK.apply(lambda x: int(x.netmask)).to_numpy()\nnetwork_addresses = df_network.NETWORK.apply(lambda x: int(x.network_address)).to_numpy()\n\ndef get_first_network(ip):\n    is_in_network = int(ip) & net_masks == network_addresses\n    indices = np.argwhere(is_in_network)\n    if indices.size>0:\n        return df_network.loc[int(indices[0]), 'NETWORK_NAME' ]\n    else:\n        None\n\ndf_ip['network_name'] = df_ip.IP.apply(get_first_network)\n", "            IP network_name\n0  10.10.10.10      Subnet1\n1  10.10.20.10      Subnet2\n2  10.10.20.20      Subnet2\n"], "link": "https://stackoverflow.com/questions/58389793/merge-join-two-dataframes-one-with-ip-addresses-one-with-ip-networks"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a string column in a Data Frame that i want to extract a rate from that is the last occurrence of backslash. I want to get", "q_apis": "get columns last get", "apis": "last", "code": ["import re\nre.findall(r\"\\d+/\\d+\",\"After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP* \")\n"], "link": "https://stackoverflow.com/questions/65654853/a-regular-expression-to-find-second-occurence-in-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe 'data': I want to use the Computer names as column headers in a new df: There are duplicate values so that may be an issue EDIT: I tried the reshapes on the trivial example and it works. However it doesn't work on my actual data since it's complaining about duplicates. I will have to dig deeper", "q_apis": "get columns names values", "apis": "DataFrame columns pivot index columns values", "code": ["import pandas as pd\n\ndata = [[\"Joe\", \"MAC\", 10],\n        [\"Joe\", \"HP\", 5],\n        [\"Jackson\", \"MAC\", 20],\n        [\"Jackson\", \"HP\", 15]]\n\ndf = pd.DataFrame(data, columns=['Name','Computer','Data'])\ndf.pivot(index='Name', columns='Computer', values='Data')\n"], "link": "https://stackoverflow.com/questions/63494451/how-to-use-column-values-as-headers-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this (sample) And I want, group and while and the value I have this, becuase code need be a column A this return this But I want the two columns with the same together, no a section with values and another with values For example, this I think it is possible invert column index, but no idea", "q_apis": "get columns sample value columns values values index", "apis": "pivot_table index columns sum size swaplevel sort_index", "code": ["pd.pivot_table(\n     df,\n     index=['m'],\n     columns=['code'],\n     aggfunc={\"time\":['sum','size']},\n     fill_value=0\n).swaplevel(2,1,axis=1).sort_index(level=[0,1],axis=1)\n"], "link": "https://stackoverflow.com/questions/54889375/pandas-pivot-column-order-multiple-agg"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a huge CSV file with 2 relevant columns. Time and speed. The data got created while driving a car. Now, I want to compare some values of the speed column in order to conclude if the car is accelerating or getting slower and put it in a new dataframe \"accelerating\". For example:", "q_apis": "get columns columns compare values put", "apis": "DataFrame diff loc", "code": ["df = pd.DataFrame({'speed': [1.41, 5.341, 10.3412, 3.341, 456.432]})\n\ndf['accelerating'] = df['speed'].diff() > 0\n\nprint(df)\n\n      speed accelerating\n0    1.4100        False\n1    5.3410         True\n2   10.3412         True\n3    3.3410        False\n4  456.4320         True\n", "df_acc = df[df['accelerating']]\n\nprint(df_acc)\n\n      speed accelerating\n1    5.3410         True\n2   10.3412         True\n4  456.4320         True\n", "df_acc = df.loc[df['accelerating'] == True]"], "link": "https://stackoverflow.com/questions/50952675/how-to-compare-csv-column-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "When I iterate this code over, pandas keeps creating new index columns in my csv. So it ends up looking like this this (better visible in the screenshot): I know that indexing can be turned of with . However, what if I wanted to have the columns indexed? Any help would be appreciated. Thanks.", "q_apis": "get columns index columns columns", "apis": "DataFrame to_csv", "code": ["import pandas as pd\n\ntry:\n    credentials = pd.read_csv(\"credentials.csv\", index_col=0)\nexcept FileNotFoundError:\n    credentials = pd.DataFrame()\n...\n\ncredentials_updated.to_csv(\"credentials.csv\")\n"], "link": "https://stackoverflow.com/questions/64616422/pandas-for-python-creates-multiple-index-columns-when-saving-df-in-csv"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a scenario where I have to find the range of all the columns in a dataset which contains multiple columns with numeric value but one column has string values. Please find sample records from my data set below: The maximum and minimum of these columns are given by and ...respectively. To find the range of all the columns I can use the below code: But as the column 'species' has string values, the above code is throwing the below error: If the above error occurs, I want to print the value as the IOW, my expected output would be something like: How do I resolve this issue?", "q_apis": "get columns where all columns contains columns value values sample columns all columns values value", "apis": "select_dtypes apply max min select_dtypes max min append", "code": ["u = Iris.select_dtypes(include=[np.number])\n# U = u.apply(np.ptp, axis=0)\nU = u.max() - u.min()\n\nv = Iris.select_dtypes(include=[object])\nV = v.max() + ' - ' + v.min()\n\nU.append(V)\n\nsepal_length                   3.6\nsepal_width                    2.4\npetal_length                   5.9\npetal_width                    2.4\nspecies         virginica - setosa\ndtype: object\n"], "link": "https://stackoverflow.com/questions/53923282/find-the-range-of-all-columns-difference-between-maximum-and-minimum-while-gra"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a column in my dataset, which has the listening time of audible books. The data is stored like 10 hours and 43 minutes How to extract them and change it into minutes, in a python dataframe? I have used But this is not working correctly. Image of the dataset", "q_apis": "get columns time", "apis": "astype iloc iloc", "code": ["temp_df = audiob_adv['Listening Time'].str.extract(r'(\\d+)[^\\d]+(\\d+)').astype('int32')\naudiob_adv[\"Time\"] = temp_df.iloc[:,0]*60 + temp_df.iloc[:,1]\n"], "link": "https://stackoverflow.com/questions/64011163/how-to-extract-two-numbers-from-column-of-data-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to concatenate information to the main index of a multi-index. More specifically, i would like to concatenate the product description of the imported product within the main index column where only the product code is referenced. Consider the following code... Current Result: Consider that I have a separate dataframe with the following information... Desired Result: The core idea here is to make use of the extra white space that is created in the first column. in my real data, one imported product may go into more than 100 manufactured products, so instead of adding new columns to bring the imported product description, I would prefer to do it in the fashion requested herein, as my multi-index already has too many columns. While above I have just mentioned product description as an item of information to concatenate into the main index... in my real data there would be other information that I would like to do this with, including information resulting from the computations within python. Thanks in advance!!!", "q_apis": "get columns index index product product index where product first product columns product index columns product item index", "apis": "set_index pop index pop index get pop index pop", "code": ["s = mapper.set_index('Imported Product')['Product Description']\n\nnew_labels = pop.index.levels[0] + '-' + pop.index.levels[0].map(s.get)\npop.index.set_levels(new_labels, level=0, inplace=True)\n\nprint(pop)\n\nImported Product  Manufactured Product\nA-Widget1         x                       33871648\n                  y                       37253956\nB-Widget2         z                       18976457\n                  w                       19378102\nC-Widget3         s                       20851820\n                  q                       25145561\ndtype: int64\n"], "link": "https://stackoverflow.com/questions/53318411/concatenate-string-to-level-of-multiindex-mapped-from-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This is a small part of my huge dataframe: output: What I want to do is to count how many vehicles were involved per each index (index is indicating the accident). What is the easiest and fastest way to count by condition? For example, I can count number of drivers per each accident, and that way I will know how many vehicles I had in each accident. With this code I managed to do it: This part is still something I am trying to figure out: I also want to add another column named 'reference', where I can reference which casualty was in which vehicle or in the case of pedestrian, which vehicle hit the pedestrian. Can someone help out? I am still learning pandas. :/ DESIRED OUTPUT: EDIT Translation of the original data:", "q_apis": "get columns count index index count count add where", "apis": "groupby apply sum reset_index merge groupby apply astype cumsum reset_index drop loc merge", "code": ["df_n_of_veh = df.groupby(['Accident_Index']).apply(lambda x: sum(x['Casualty_Class'] == 'Driver')).to_frame(name='n_of_veh').reset_index()\ndf = df.merge(df_n_of_veh, how='left', on='Accident_Index') \n", "df_ref = df.groupby('Accident_Index').apply(lambda x: (x['Casualty_Class'] == 'Driver').astype(int).cumsum()).to_frame(name='reference').reset_index(level='Accident_Index').drop(['Accident_Index'], axis=1)\ndf_ref.loc[df_ref['reference'] == 0, 'reference'] = 1\ndf = df.merge(df_ref, left_index=True, right_index=True) \n"], "link": "https://stackoverflow.com/questions/62191495/create-a-new-column-in-pandas-dataframe-based-on-different-conditions"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I just plotted multiple horizontal bar charts that share the same y-axis. To elaborate, I have 4 dataframes, each representing a bar chart. I used these dataframes to plot 2 horizontal bar charts at the left and another 2 at the right. However, I do not know how to add the bar values for each horizontal bar as there are 4 dataframes, each containing different values. Below are my desired output, current code and graph Edit* Still trying to get the values ontop of the horizontal bar graph. It will be great if someone can lend a helping hand!", "q_apis": "get columns plot at left at right add values values get values", "apis": "axes axes axes axes", "code": ["for i, v in enumerate(df_male_1['single_value']):\n    axes[0].text(v + 4, i + 0.1, str(v))\nfor i, v in enumerate(df_male_2['single_value']):\n    axes[0].text(v + 4, i - 0.2, str(v))\n\nfor i, v in enumerate(df_female_1['single_value']):\n    axes[1].text(v + 1, i + 0.1, str(v))\nfor i, v in enumerate(df_female_2['single_value']):\n    axes[1].text(v + 1, i - 0.2, str(v))\n", "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata1 = {\n    'age': ['20-24 Years', '25-29 Years', '30-34 Years',\n            '35-39 Years', '40-44 Years', '45-49 Years'],\n    'single_value': [97, 75, 35, 19, 15, 13]\n}\n\ndata2 = {\n    'age': ['20-24 Years', '25-29 Years', '30-34 Years',\n            '35-39 Years', '40-44 Years', '45-49 Years'],\n    'single_value': [98, 79, 38, 16, 15, 13]\n}\n\ndata3 = {\n    'age': ['20-24 Years', '25-29 Years', '30-34 Years', \n            '35-39 Years', '40-44 Years', '45-49 Years'],\n    'single_value': [89, 52, 22, 16, 12, 13]\n}\n\ndata4 = {\n    'age': ['20-24 Years', '25-29 Years', '30-34 Years',\n            '35-39 Years', '40-44 Years', '45-49 Years'],\n    'single_value': [95, 64, 27, 18, 15, 13]\n}\n\ndf_male_1 = pd.DataFrame(data1)\ndf_male_2 = pd.DataFrame(data2)\ndf_female_1 = pd.DataFrame(data3)\ndf_female_2 = pd.DataFrame(data4)\n\nfig, axes = plt.subplots(ncols=2, sharey=True, figsize=(12,6))\naxes[0].barh(df_male_1['age'], df_male_1['single_value'],\n    align='edge', height=0.3, color='lightskyblue', zorder=10)\naxes[0].barh(df_male_2['age'], df_male_2['single_value'],\n    align='edge', height=-0.3, color='royalblue', zorder=10)\naxes[0].set(title='Age Group (Male)')\n\nfor i, v in enumerate(df_male_1['single_value']):\n    axes[0].text(v + 4, i + 0.1, str(v))\nfor i, v in enumerate(df_male_2['single_value']):\n    axes[0].text(v + 4, i - 0.2, str(v))\n\nfor i, v in enumerate(df_female_1['single_value']):\n    axes[1].text(v + 1, i + 0.1, str(v))\nfor i, v in enumerate(df_female_2['single_value']):\n    axes[1].text(v + 1, i - 0.2, str(v))\n\naxes[0].set_xlim([0, 105])\naxes[1].set_xlim([0, 105])\n\naxes[1].barh(df_female_1['age'], df_female_1['single_value'], \nalign='edge',height=0.3,color='navajowhite', zorder=10)\n\naxes[1].barh(df_female_2['age'], df_female_2['single_value'], align='edge', \nheight=-0.3,color='darkorange', zorder=10)\n\naxes[1].set(title='Age Group (Female)')\n\naxes[0].invert_xaxis()\naxes[0].set(yticks=df_male_1['age'])\naxes[0].yaxis.tick_right()\n\nfor ax in axes.flat:\n    ax.margins(0.03)\n    ax.grid(True)\n\nfig.tight_layout()\nfig.subplots_adjust(wspace=0.185, top=0.88)\nplt.show()\n"], "link": "https://stackoverflow.com/questions/54603692/trouble-adding-the-value-ontop-of-each-horizontal-barchart"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the below data set consisting of cards swiped and time when swiped. The output has to be total no of cards swiped month and year wise. Output I have tried using groupby but not able to reach the expected output. How do I include the month name?", "q_apis": "get columns time month year groupby month name", "apis": "groupby count reset_index sort_values", "code": ["df['month_'] = df['Date Time'].dt.strftime('%b')\ndf['year_'] = df['Date Time'].dt.strftime('%Y')\nnew_df = df.groupby([\"month_\", \"year_\"])[\"Card_No\"].count().reset_index().sort_values(\n    \"Card_No\", ascending=False)\nprint(new_df)\n    month_  year_   Card No\n2   Sep 2018    21\n1   Nov 2018    7\n0   Dec 2018    6\n", "    df['month_'] = df['Date Time'].dt.strftime('%m') # change %b to %m\n   df['year_'] = df['Date Time'].dt.strftime('%Y')\n   new_df = df.groupby([\"month_\", \"year_\"])[\"Card_No\"].count().reset_index().sort_values(\n    \"month_\")\n"], "link": "https://stackoverflow.com/questions/58039027/group-by-month-and-year-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm new to python and I'm trying to scrape a table from multiple pages of a website. After reading multiple websites and watching videos, I've managed to write a code that is capable of scraping a single page and saving it to excel. The urls for pagination is to simply change the page=x value at the end of the url. I've tried and failed to loop through multiple pages and create a dataframe. Single page scrape Scraping multiple pages Then create a dataframe by combining multiple dataframes created from each page. I don't know how to create multiple dataframes in a loop and combine them together.", "q_apis": "get columns value at combine", "apis": "get rename index columns append dropna to_excel columns index", "code": ["frames = []\nfor x in range (4):\n    res = requests.get(urlbase + str(x))\n    soup = BeautifulSoup(res.content,'lxml')\n    table = soup.find('table', id=\"offers_table\")\n    df = pd.read_html(str(table), header=1)\n    df[0].rename(index=str, columns={\"Unnamed: 0\": \"Full Desc\", \"Unnamed: 2\": \n        \"Detail\", \"Unnamed: 3\": \"Price\", \"Unnamed: 4\": \"Time\"}, inplace = True)\n    frames.append(df[0].dropna(thresh=3))\nres = pd.concat(frames)\nres.to_excel('new.xlsx', sheet_name='Page_2', columns= ['Detail','Price','Time'], index = False)\n"], "link": "https://stackoverflow.com/questions/54062559/creating-a-dataframe-looping-through-multiple-read-html-links"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have dataframe like this: I need a json in the format: Can I do that using inbuild pandas.to_json? I tried all orient option but couldn't find the one I needed", "q_apis": "get columns to_json all", "apis": "DataFrame to_json", "code": ["import pandas as pd\n\ndf = pd.DataFrame([\n    {\"dis\": \"RTU-1\", \"equip\": \"m\", \"siteRef\": \"r:153c-699a HQ\", \"installed\": \"d:2005-06-01\"},\n    {\"dis\": \"RTU-2\", \"equip\": \"m\", \"siteRef\": \"r:153c-699a HQ\", \"installed\": \"d:999-07-12\"}])\ndf.to_json(orient=\"records\")\n"], "link": "https://stackoverflow.com/questions/50123833/converting-dataframe-to-json-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following data for wind speed and wind direction taken over the course of a month in Salt Lake City. I want to group by the hour data were taken. For the data taken within that hour, I want to accomplish two things: (1) calculate mean wind speed (2) apply a function I have defined (\"yamatrino\") to all the wind_direction measurements taken within each hour. Below is the code I have written to (1) convert time data into a datetime format and (2) to create two columns with the mean wind speeds and yamatrino values for each hour of data. The error reads \"TYPE ERROR: cannot perform reduce with flexible type\" I am confused how to aggregate with more than one column of data.", "q_apis": "get columns month hour hour mean apply all hour time columns mean values hour aggregate", "apis": "groupby agg mean", "code": ["df.groupby(df['time'].dt.hour).agg(yamatrino_value = ('wind_direction', yamatrino), \n                                   hourly_velocity_mean = ('wind_speed', np.mean))\n"], "link": "https://stackoverflow.com/questions/63489568/how-do-i-run-multiple-functions-on-my-aggregated-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following data in my DataFrame I want to create another DataFrame by applying one of the filter ( Get all data which belongs to current month ). To achieve this I am using the following code : I think the filter is running properly , because when I print , I can see the data, but it is only returning column data, I want all of the columns/data from .", "q_apis": "get columns DataFrame DataFrame filter all month filter all columns", "apis": "astype", "code": ["    Name    date    address\n0   A   2021-07-03  X\n1   B   2021-07-03  Y\n2   C   2021-07-03  Z\n3   D   2021-08-01  M\n4   E   2021-08-01  N\n5   F   2021-08-01  O\n", "search = '2021-07'\nmonth_dataframe = df[df[\"date\"].str.contains(str(search))]\nmonth_dataframe\n", "df['date'] = pd.to_datetime(df.date)\nmonth_dataframe =  df[df.date.dt.month == pd.Timestamp('today').month]\nmonth_dataframe\n", "search = '2021-07'\nmonth_dataframe = df[df[\"date\"].astype(str).str.contains(str(search))]\nmonth_dataframe\n", "    Name    date    address\n0   A   2021-07-03  X\n1   B   2021-07-03  Y\n2   C   2021-07-03  Z\n"], "link": "https://stackoverflow.com/questions/68243003/python-pandas-findall-not-returning-all-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have found an answer that works for me to this Q and will post as well. How to take a list of dataframes and create a new df that has only the rows that share a common value in a non index column? Basically an intersection, but concat and merge wouldn't work for me for a number of reasons. I looked at the following and didn't get what i needed: Finding common rows (intersection) in two Pandas dataframes Pandas merge df error How to get intersection of dataframes based on column labels? Intersection of multiple pandas dataframes How to find intersection of dataframes based on multiple columns? Intersection of pandas dataframe with multiple columns How to do intersection of dataframes in pandas", "q_apis": "get columns take value index intersection concat merge at get intersection merge get intersection intersection columns columns intersection", "apis": "copy copy copy copy pop pop loc isin values insert", "code": ["import copy\n\ndfs_array = [ df1, df2, df3, df4, ... ]\n\ndef intersection_of_dfs(dfs_array,col='Ticker'):\n    if len(dfs_array) <= 1:\n        # if we only have 1 or 0 elements simply return the origial array\n        # no error is given, logic must be created for the return value.\n        return dfs_array\n    # does a shallow copy only.\n    dfs = copy.copy(dfs_array)\n    length = len(dfs) \n    while length > 1:\n        df1 = dfs.pop()\n        df2 = dfs.pop()\n        df0 = df1.loc[ df1[col].isin( df2[col].values ) ]\n        dfs.insert(0,df0)\n        length = len(dfs)\n    return dfs\n", ">     [   open_x_x  high_x_x  low_x_x  close_x_x  volume_x_x Ticker  ...  LowAboveShort_y_y  ShortAboveLong_y_y  Return_y_y  DayDiff_y_y \n> AboveBelow_y_y  ShortToLong_y_y\n>     0     52.60     52.68    52.24    52.4779        7632   AADR  ...            0.28214            1.087176    0.043298     2.600000             2.0         8.000000\n>     1     14.03     14.03    14.03    14.0300         359   AAMC  ...            0.17472            0.628733    0.202228     1.333333             7.0         2.600000\n>     2      2.15      2.15     1.72     1.9500       10095   AAME  ...           -0.20068            0.107564    0.114286     1.000000             1.0         0.636364\n>     \n>     [3 rows x 61 columns]]\n", ">     [             open   high    low    close  volume Ticker  Difference     LongMA   ShortMA  HighBelowShort  LowAboveShort \n> ShortAboveLong    Return   DayDiff  AboveBelow  ShortToLong\n>     timestamp                                                                                                                                                                           \n>     2019-12-12  52.60  52.68  52.24  52.4779    7632   AADR      0.1379  50.870684  51.95786         0.72214        0.28214        1.087176  0.043298  2.600000         2.0     8.000000\n>     2019-12-12  14.03  14.03  14.03  14.0300     359   AAMC     -0.0100  13.226547  13.85528         0.17472        0.17472        0.628733  0.202228  1.333333         7.0     2.600000\n>     2019-12-12   2.15   2.15   1.72   1.9500   10095   AAME      0.1900   1.813116   1.92068         0.22932       -0.20068        0.107564  0.114286  1.000000         1.0     0.636364]\n"], "link": "https://stackoverflow.com/questions/59338488/how-to-find-the-intersection-of-multiple-pandas-dataframes-on-a-non-index-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like as shown below With the help of this post, I am able to achieve part of what I wanted to. Now what I would like to do is a) Shift the dates backward (subtract) based on for each subject b) Shift the dates forward (add) based on for each subject c) Check whether the component remains the same between 3 columns , and So, I got the using the code below but not sure how can I use this and as date offset for each subject. I expect my output to be like as shown below", "q_apis": "get columns add between columns date", "apis": "get groupby min groupby transform min sub add all stack dropna groupby nunique where gt", "code": ["#get groupby min transformed to the length of the dataframe\nmin_pd_nd = df1.groupby('person_id')['pd','nd'].transform('min')\n\ndf1['Shift_backward'] = df1['dates'].sub(pd.to_timedelta(min_pd_nd['pd'],unit='d'))\ndf1['Shift_Forward'] = df1['dates'].add(pd.to_timedelta(min_pd_nd['nd'],unit='d'))\n#check if in a given row all year are unique \nc = (df1[['dates','Shift_backward','Shift_Forward']].stack(dropna=False).dt.year\n     .groupby(level=0).nunique())\ndf1['year_change'] = np.where(c.gt(1),'Yes','No')\n", "print(df1[['person_id','dates','Shift_backward','Shift_Forward','year_change']])\n\n   person_id      dates Shift_backward Shift_Forward year_change\n0         11 1961-12-30     1961-06-10    1961-12-31          No\n1         21 1967-05-29     1967-01-01    1967-12-31          No\n2         31 1957-02-03     1957-01-01    1957-12-31          No\n3         41 1959-07-27     1959-01-01    1959-12-31          No\n4         51 1971-01-13     1971-01-01    1971-12-31          No\n5         11 2017-07-23     2017-01-01    2017-07-24          No\n6         21 2017-05-29     2017-01-01    2017-12-31          No\n7         31 2015-02-03     2015-01-01    2015-12-31          No\n8         41        NaT            NaT           NaT          No\n9         51        NaT            NaT           NaT          No\n"], "link": "https://stackoverflow.com/questions/62442003/shift-dates-by-value-unique-for-each-subject"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Consider below two data frames (unequal length). I want to calculate a new column called in which is as below. Choose all the factor in whose dates are greater than the date in and multiple the by the same to arrive at So my final dataframe would look like Explanation: in is less than both and of . So multiply by factor However, date in is greater than and less than of . So only multiply by factor Most of what I have looked are related to 1. Merge two dataframe. 2. Compare two dataframe of equal length 3. Compare two dataframe of un-equal length However, the current issue is related to computing a value based on collected (factors multiplied) value of another dataframe - where the foreign keys will not be equal.", "q_apis": "get columns length all date at date length length value value where keys", "apis": "mask prod loc mask loc prod mul mask values mask mul prod loc mul mean std apply cumprod values mean std append product append prod items DataFrame from_dict index columns reset_index mean std", "code": ["mask = df1.date.to_numpy()[:,None] < df2.date.to_numpy() # `_.values` should be avoided instead use `_.to_numpy()`\nit = iter(mask)\n\ndef mul(x):\n    val = np.prod(df2.loc[next(it),'factor'])\n    return x*val\n\ndf1['factored_qty'] = df1['qty'].map(mul)\ndf1\n        date  qty  factored_qty\n0 2016-10-08    1             6\n1 2016-11-08    8            48\n2 2016-12-08    2             6\n3 2017-01-08    4            12\n", "mask = df1.date.to_numpy()[:,None] < df2.date.to_numpy()\nl = [np.prod(df2.loc[idx,'factor']) for idx in mask]\n     # df2.loc[idx,'factor'].prod()\ndf['factored_qty'] = df1.qty.mul(l)\n", "# My answer\nIn [163]: %%timeit\n     ...: mask = df1.date.to_numpy()[:,None] < df2.date.to_numpy() # `_.values` should be avoided instead use `_.to_num\n     ...: py()`\n     ...: it = iter(mask)\n     ...: \n     ...: def mul(x):\n     ...:     val = np.prod(df2.loc[next(it),'factor'])\n     ...:     return x*val\n     ...: df1['factored_qty'] = df1['qty'].map(mul)\n     ...:\n     ...:\n1.31 ms \u00b1 28.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n# Stef's answer.\nIn [164]: %%timeit\n     ...: df1['factored_qty'] = df1.apply(lambda x: df2[df2.date>x.date].factor.cumprod().values[-1] * x.qty,axis=1)\n     ...:\n     ...:\n3.65 ms \u00b1 194 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n#sammy's answer\nIn [180]: %%timeit\n     ...: d = defaultdict(list)\n     ...: #iterate through data and append factors that meet criteria\n     ...: for (date1, qty), (date2, factor) in product(df1.to_numpy(),df2.to_numpy()) : \n     ...:     if date1 < date2 :\n     ...:         d[(date1, qty)].append(factor)\n     ...: outcome = {k:[s,np.prod((s,*v))] for (k,s),v in d.items()}\n     ...: pd.DataFrame.from_dict(outcome, orient='index', columns=['qty','factored_qty']).reset_index()\n     ...:\n     ...:\n1.49 ms \u00b1 47.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/62371375/computing-values-of-a-column-in-dataframe-based-on-another-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame and I will like to multiply (or divide) every n indexes by a specific number from an array. A brief example is the following, where the letters are just numbers. df = DataFrame (or numpy array): I will like to obtain the following result: Result = Is it any way to solve this using or ? I am handling a huge DataFrame and I believe if I apply a for loop will take more time than needed. I know I have to use a function, but I cannot code one that does what I am looking for. Thanks.", "q_apis": "get columns DataFrame array where DataFrame array any DataFrame apply take time", "apis": "values columns values columns", "code": ["def dv(tbl):\n    return tbl.divide(df2.values, axis='columns')\n", "      0     1\n0  10.0  11.0\n1  12.0  13.0\n2  14.0  15.0\n3  16.0  17.0\n4  18.0  19.0\n5  20.0  21.0\n6  22.0  23.0\n7  24.0  25.0\n", "     0    1\n0  2.0  2.5\n1  3.0  3.5\n", "           0         1\n0   5.000000  4.400000\n1   4.000000  3.714286\n2   7.000000  6.000000\n3   5.333333  4.857143\n4   9.000000  7.600000\n5   6.666667  6.000000\n6  11.000000  9.200000\n7   8.000000  7.142857\n", "def ml(tbl):\n    return tbl.multiply(df2.values, axis='columns')\n"], "link": "https://stackoverflow.com/questions/55304606/multiply-or-divide-every-n-indexes-of-a-dataframe-by-an-constant-from-array"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am pulling out a value from a table, searching for the value based on matches in other columns. Right now, because there are hundreds of thousands of grid cells to go through, each call of the function takes a few seconds, but it adds up to hours. Is there a faster way to do this? Sample", "q_apis": "get columns value value columns now seconds", "apis": "loc mean std mean std values mean std values mean std", "code": ["data=pd.DataFrame({'column':[np.random.randint(30) for i in range(100000)],\n                'row':[np.random.randint(50) for i in range(100000)],\n                'value':[np.random.randint(100)+np.random.rand() for i in range(100000)],\n                 'test1':[np.random.choice(['X','Y']) for i in range(100000)],\n                'test2':[np.random.choice(['d','e','f','g','h','i']) for i in range(100000)]})\n\ndata.head()\n\n    column  row value       test1   test2\n0   4       30  88.367151   X       e\n1   7       10  92.482926   Y       d\n2   1       17  11.151060   Y       i\n3   27      10  78.707897   Y       g\n4   19      35  95.204207   Y       h\n", "%timeit data_1 = data.loc[(data['test1'] == 'X') & (data['column'] >=12) & (data['row'] > 22)]['value']\n13 ms \u00b1 538 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit data_1 = data[(data['test1'] == 'X') & (data['column'] >=12) & (data['row'] > 22)]['value']\n13.1 ms \u00b1 233 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "d1=data.values\n\n%timeit d1[(d1[:,3]=='X')&(d1[:,0]>=12)&(d1[:,1]>22)][:,2]\n8.37 ms \u00b1 161 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n", "%timeit d1=data.values;d1[(d1[:,3]=='X')&(d1[:,0]>=12)&(d1[:,1]>22)][:,2]\n20.6 ms \u00b1 624 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n"], "link": "https://stackoverflow.com/questions/54081969/speed-up-python-loc-function-search"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am relatively new to python and pandas. I am trying to replicate a battleship game. My goal is to locate the row and column that has 1 and storage that location as the Battleship location. I created a CSV file and it looks like this This is the code to read the created CSV file as a DataFrame. How do I use nested for loop and Pandas' iloc to locate the row and column that has 1(which is row 3 and column 1) and storage that location as the Battleship location? I found out that a nested for loop and pandas iloc to give the entire dataframe look something like this Do I have to put the if statement to locate the row and column?", "q_apis": "get columns DataFrame iloc iloc put", "apis": "where astype bool columns shape shape iloc columns", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('/content/pandas_tutorial/My Drive/pandas/myBattleshipmap.csv',)\nr, c = np.where(df.astype(bool))\n\nprint(r.tolist())\nprint(df.columns[c].tolist())\n", "for x in range(df.shape[0]):\n    for y in range(df.shape[1]):\n        if df.iloc[x, y] == 1:\n            print(f'Row: {x}, Col: {df.columns[y]}')\n"], "link": "https://stackoverflow.com/questions/63400324/how-to-use-nested-for-loop-and-pandass-iloc-to-locate-row-and-column-that-has-1"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe I read from a csv file with : I also have a dataframe with corrections How do I apply these corrections where date and location match to get the following? EDIT: I got two good answers and decided to go with . Here is how I did it 'non-destructively'.", "q_apis": "get columns apply where date get", "apis": "set_index set_index fillna", "code": ["df.set_index(['date','location'], inplace=True)\ndf1.set_index(['date','location'], inplace=True)\ndf['value1']=(df['value1']+df1['value']).fillna(df['value1'])\n"], "link": "https://stackoverflow.com/questions/64200427/adding-correction-column-to-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "df is as follows i want df['col1'] like below(rounded off to 0 decimal with '%' sign): col1 11% 56% 700% 118% 200% 102% 45% 68% My code is not working properly for some entries Like 700% changes to 7% 118.13 to %% some to %6% and for some entries it is working fine. Please help me with this!!!", "q_apis": "get columns", "apis": "DataFrame replace round index index index index", "code": ["import pandas as pd\n\nperc_df = pd.DataFrame(\n    {'col1' : ['65.94%', '761.19%', '17281.0191%', '9.4%', '14%'],\n     'col2' : ['a', 'b', 'c', 'd', 'e']\n})\n\n\nperc_df['col1'] = pd.to_numeric(perc_df['col1'].str.replace('%', ''))\nperc_df['col1'] = pd.Series([round(val, 2) for val in perc_df['col1']], index = perc_df.index)\nperc_df['col1'] = pd.Series([\"{0:.0f}%\".format(val) for val in perc_df['col1']], index = perc_df.index)\n"], "link": "https://stackoverflow.com/questions/53436708/how-to-convert-number-with-sign-to-roundnumber-with-sign"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "UPDATE: TomNash's answer solves the question as asked. However, attempting to use it in my real problem led to issues with quoted column names, issues when there was missing data, etc. To circumvent this I'm using CJR's suggestion in the comments to simply pickle my DataFrames. ORIGINAL QUESTION BELOW: I have a Panda's DataFrame in memory. I would like to be able to write it to file (using ), then use to read the results into a new DataFrame. I would like the original DataFrame and the new \"from file DataFrame\" to have identical data types. I've attempted to get this working by using the and arguments for both and . However, this doesn't seem to do the trick. I understand that for the argument can be used to force data types, but this isn't practical for my use case (lots of auto-generated files used for regression testing). Full example below. : Output from running : Column 1: As expected, the dtype is for both DataFrames. Column 2: Unexpected behavior. For the dtype is , while for the dtype is . Column 3: Expected behavior. has dtype while has dtype . As the csv module describes, \"Instructs the reader to convert all non-quoted fields to type float.\" The contents of are below. Notice that the second column is quoted, so I would expect to give me an object. :", "q_apis": "get columns names DataFrame DataFrame DataFrame DataFrame identical get dtype dtype dtype dtype dtype all second", "apis": "DataFrame info to_csv index replace info DataFrame info to_csv index replace info", "code": ["import pandas as pd\nfrom csv import QUOTE_NONNUMERIC, QUOTE_NONE\nimport sys\n\nprint('Python version information:')\nprint(sys.version)\nprint('Pandas version information:')\nprint(pd.__version__)\n\ndf1 = pd.DataFrame([['A', '100', 100], ['B', '200', 200]])\nprint('df1:')\nprint(df1.info())\n\ndf1.to_csv('tmp.csv', index=False, quoting=QUOTE_NONNUMERIC, quotechar='\"')\n\ndf2 = pd.read_csv('tmp.csv', quoting=QUOTE_NONE).replace('\"','', regex=True)\nprint('df2:')\nprint(df2.info())\n", "Python version information:\n3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]\nPandas version information:\n0.24.2\ndf1:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null object\n1    2 non-null object\n2    2 non-null int64\ndtypes: int64(1), object(2)\nmemory usage: 128.0+ bytes\nNone\ndf2:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null object\n1    2 non-null object\n2    2 non-null int64\ndtypes: int64(1), object(2)\nmemory usage: 128.0+ bytes\nNone\n", "import pandas as pd\nfrom csv import QUOTE_NONNUMERIC, QUOTE_NONE, QUOTE_MINIMAL\nimport sys\n\nprint('Python version information:')\nprint(sys.version)\nprint('Pandas version information:')\nprint(pd.__version__)\n\ndf1 = pd.DataFrame([['A', '100', 100.1], ['B', '200', 200.2]])\nprint('df1:')\nprint(df1.info())\n\ndf1.to_csv('tmp.csv', index=False, quoting=QUOTE_NONNUMERIC, quotechar='\"')\n\ndf2 = pd.read_csv('tmp.csv', quoting=QUOTE_NONE).replace('\"','', regex=True)\nprint('df2:')\nprint(df2.info())\n", "Python version information:\n3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]\nPandas version information:\n0.24.2\ndf1:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null object\n1    2 non-null object\n2    2 non-null float64\ndtypes: float64(1), object(2)\nmemory usage: 128.0+ bytes\nNone\ndf2:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null object\n1    2 non-null object\n2    2 non-null float64\ndtypes: float64(1), object(2)\nmemory usage: 128.0+ bytes\nNone\n"], "link": "https://stackoverflow.com/questions/56975273/how-to-get-consistent-dtypes-after-to-csv-and-read-csv"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How to select all columns that have header names starting with \"durations\" or \"shape\"? (instead of defining a long list of column names). I need to select these columns and substitute blank fields by 0.", "q_apis": "get columns select all columns names shape names select columns", "apis": "columns columns columns fillna iloc columns shape fillna", "code": ["df = data[data.columns[data.columns.str.startwith('durations') | data.columns.str.startwith('so')]]\ndf.fillna(0)\n", "df = data.iloc[:, data.columns.str.contains('durations.*'|'shape.*') ]\ndf.fillna(0)\n"], "link": "https://stackoverflow.com/questions/34096481/how-to-select-all-columns-that-start-with-durations-or-shape"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this 1000x6 dataframe. How do I add another two columns: column[latest person], column[latest date] to display the last name and Action Date / Time of it ? input: Desired output:", "q_apis": "get columns add columns date last name", "apis": "replace columns fillna ffill columns fillna ffill columns columns", "code": ["import numpy as np\n\ndf = df.replace(\"0\", np.nan)\n\npersons = df[df.columns[::2]].fillna(axis=1, method='ffill')\ndates = df[df.columns[1::2]].fillna(axis=1, method='ffill')\n\ndf['Latest person'] = persons[persons.columns[-1]]\ndf['Latest date'] = dates[dates.columns[-1]]\n"], "link": "https://stackoverflow.com/questions/51343512/return-value-of-last-dates-and-names-across-multi-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataset with x number of batches (batch sizes are different, i.e rows), now I create a new feature for each batch using the respective batch data. I want to automate this process, e.g.first create a new column then iterate over the batch id column until it has the same batch id, create new feature values and append the newly created column, then continue to next batch here is code for the manual method for single batch", "q_apis": "get columns now first values append", "apis": "empty sample all sample loc loc", "code": ["### First create an empty column \nsample[\"new feature\"] = np.nan \n\n### iterate through all unique id's \nfor id in sample[\"batch id\"].unique():\n    batch = samples.loc[samples[\"batch id\"] == id]\n    # do your computations \n    samples.loc[samples[\"batch id\"] == id, \"new feature\"] =  # your computed value \n"], "link": "https://stackoverflow.com/questions/65638043/iterate-over-pandas-column-and-create-new-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dictionary (df) of dataframes: I want to convert the data into a single dataframe and simultaneously set as additional column the key strings I first convert the dictionary into a list of dataframes: Then I can merge the dataframes into a single dataframe by: But I want to set as additional column the keys from the dictionary Expected result if like this: Thanks in advance", "q_apis": "get columns first merge keys", "apis": "DataFrame items loc", "code": ["final_df = pd.DataFrame()\nfor key, value in dictionary.items():\n     df = value\n     df.loc[:,'Country'] = key\n     final_df = pd.concat([df, final_df], 0)\nfinal_df\n"], "link": "https://stackoverflow.com/questions/57468692/unpack-a-dictionary-of-dataframes-and-set-key-as-column-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a large DataFrame with massive columns and rows. Each row is one sample. For example: Assume df is a large DataFrame, I would like to know how to conveniently check if there are any duplicate sample in this pandas DataFrame, then print the row index about duplicate samples. Thanks update: I want to check if row 1 (1,1,1) equal to row 2 (5,5,5) or row3 (7,7,7) , row4 (8,9,8) and so on. The duplication check is performing by row.", "q_apis": "get columns DataFrame columns sample DataFrame any sample DataFrame index update", "apis": "nunique shape bool duplicated bool iloc all bool", "code": ["df.nunique() == df.shape[0]\nA    False\nB    False\nC    False\nD    False\ndtype: bool\n", "df.duplicated()\n\n0    False\n1    False\n2    False\n3    False\n4    False\n5    False\ndtype: bool\n", "(df == df.iloc[0, :]).all(axis=1)\n0     True   #Only the row itself is True.\n1    False\n2    False\n3    False\n4    False\n5    False\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/68055467/how-to-check-if-there-are-any-duplicate-sample-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What's the easiest way to get a value in a Pandas 2D Dataframe with the row number and the column title as indicers (a combo of loc and iloc)?", "q_apis": "get columns get value loc iloc", "apis": "loc index iloc columns", "code": ["df = pd.DataFrame({'a':[1, 2, 3], 'b':[4, 5, 6]}, index=['i', 'j', 'k'])\ndf\n   a  b\ni  1  4\nj  2  5\nk  3  6\n", "df.loc[df.index[[0, 1]], 'a']\n\ni    1\nj    2\nName: a, dtype: int64\n", "df.iloc[[0, 1], df.columns.get_loc('a')]\n\ni    1\nj    2\nName: a, dtype: int64\n"], "link": "https://stackoverflow.com/questions/50318574/pandas-dataframe-2d-selection-with-row-number-and-column-label"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to split a dataframe in time intervals, given the time stamp of each row. For example if the date is 12/20/18 02:20:00, I want to be able to create a new column and label this as 12am-6am. I am trying to use .loc() to create this column, but splitting the time is proving to be a challenge to me. tried using between_time, changing my index to the time stamp, and I also used date_range as well as np.where(). or even using the Date column: The np.where() seems to compile but it does not give me the correct classification: For rows that satisfy the time, it labels as \"nothing\" My overall goal is to use group_by() to group my data frame into this intervals, so if there is an easier and faster solution using the timestamp column, Id love to read it. Thank you.", "q_apis": "get columns time time date loc time between_time index time date_range where where time timestamp", "apis": "ge le notnull ge le notnull", "code": ["(np.select([df.Timestamp.dt.hour.ge(6) & df.Timestamp.dt.hour.le(12) & df[col].notnull(), df.Timestamp.dt.hour.ge(12) & df.Timestamp.dt.hour.le(18) & df[col].notnull()], ['M', 'A'], default='E'))\n", "b = [6,12,18,24]\nl = ['M', 'A','E']\npd.cut(df.Timestamp.dt.hour, bins=b, labels=l, include_lowest=True)\n"], "link": "https://stackoverflow.com/questions/66767657/is-there-a-way-to-split-time-in-intervals-using-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe of size 2441x1441 (A), which is zero in the upper triangle - the diagnoal has values. I would like to multiply each column of this with a vector of length 2441 (B). The tricky part is, that I want the first non-zero value of A multiplied with the first value of B (and second value of A with second value of B and so on). This should happen for all columns of A and result in another dataframe, C. Here the result would be I have made a for loop, where I can iterate through each value However this is very slow, and I need to repeat this operation many times on different datasets. Is there a nice and pythonic way of vectorizing this?", "q_apis": "get columns size values length first value first value second value second value all columns where value repeat", "apis": "clip shape clip astype shape shape", "code": ["A = np.array([[1, 0, 0],\n              [3, 4, 0],\n              [6, 7, 8]])\n", "B = np.clip(np.fromfunction(lambda i, j: i-j+1, A.shape), 0, None)\n", "B = np.array([[1, 0, 0],\n              [2, 1, 0],\n              [3, 2, 1]])\n", "C = A * B\n", "C = np.array([[1,  0,  0],\n              [6,  4,  0],\n              [18, 14, 8]])\n", " B = np.tril(\n         np.fromfunction(\n             lambda i, j: b[np.clip((i-j).astype(int), 0, b.shape[0])],\n             A.shape))\n", " b = np.array([2, 3, 1, 4])\n", " B = np.array([[2, 0, 0],\n               [3, 2, 0],\n               [1, 3, 2]])\n"], "link": "https://stackoverflow.com/questions/53478441/multiply-vector-on-dataframe-vectorized"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i am trying to convert json to pandas dataframe using read_json, but it is always creating extra columns instead of rows json: code: result: i have tried different values for 'orient' arg yet it is the same how can i get dataframe in this manner", "q_apis": "get columns read_json columns values get", "apis": "items items DataFrame items index columns", "code": ["items = [next(iter(d.items())) for d in json]\npd.DataFrame.from_items(items, orient='index', columns=['Summary'])\n", "                                Summary\n1981121                            Tasa\n1981123  This fox only jumps on the top\n"], "link": "https://stackoverflow.com/questions/51569716/pandas-read-json-gives-multiple-columns-instead-of-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list of 4 dataframes each containing only 1 column ('CustomerID'). I would like to merge (inner join) them within a loop. This is what I've try for the moment: What I'm trying to do here is to merge the first dataframe (index 0) with the second (index 1), then delete the first dataframe in order that the dataframe of index 1 becomes the dataframe of index 0 and thus, I could iterate. I know this doesn't work as what I should merge from the second iteration should be the datframe from the new variable \"merged\" with the daframe of index 1. The 4 dataframes are a client database at diferent time (march 2019, april 2019, may 2019 etc.). The point is to analyse the client lifetime (how long did they stay client?, after how many days did they left? etc.) Could you please help me with that?", "q_apis": "get columns merge join merge first index second index delete first index index merge second index at time days left", "apis": "merge", "code": ["from functools import reduce\ndf_merge = reduce(lambda df_x, df_y: pd.merge(df_x, df_y, on='CustomerID'), all_df)\n"], "link": "https://stackoverflow.com/questions/59272087/automating-pandas-dataframe-join-from-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This is the data_frame That I am working on After running below code: The result obtained was below DataFrame; I want to to take two days high low and store it on the third day. For example, 01-08-2019 and 02-08-2019 high and low should be compared stored on 3rd day ie 05-08-2019. similarly, 02-08-2019 and 05-08-2019 should be compared to find high low and store it on the third day ie 06-08-2019. 05-08-2019 and 06-08-2019 should be compared to find high low and store it on the third day ie 07-08-2019. Expected data frame from my above explanation is as follows: Link to data", "q_apis": "get columns DataFrame take days day day day day", "apis": "count pop pop count DataFrame count DataFrame groupby index agg last max min append iloc append iloc append iloc DataFrame", "code": ["x1=[]\nx2=[]\nx3=[]\ncount=0\ndate=data_frame['Date'].tolist()\nhigh=data_frame['High'].tolist()\nlow=data_frame['Low'].tolist()\nhigh.pop()\nlow.pop()\nfor i in range (len(data_frame)-2):\n\n    if count==0:\n        temp=pd.DataFrame()\n        date.remove(date[0])\n        temp['Date']=date\n        temp['High']=high\n        temp['Low']=low\n        count=1\n    else:\n        temp=pd.DataFrame()\n        date.remove(date[0])\n        temp['Date']=date\n        high.remove(high[0])\n        low.remove(low[0])\n        temp['High']=high\n        temp['Low']=low\n        \n    TwoDay_High_Low=temp.groupby(temp.index // 2).agg({'Date': 'last','High':'max','Low':'min'})\n    x1.append(TwoDay_High_Low['Date'].iloc[0])\n    x2.append(TwoDay_High_Low['High'].iloc[0])\n    x3.append(TwoDay_High_Low['Low'].iloc[0])\n", "two_day=pd.DataFrame()\ntwo_day['Date']=x1\ntwo_day['High']=x2\ntwo_day['Low']=x3\ntwo_day\n"], "link": "https://stackoverflow.com/questions/64222808/extract-high-and-low-data-from-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to rearrange a pandas dataframe that looks like this: [![enter image description here][1]][1] into a dataframe that looks like this: [![enter image description here][2]][2] This is derived in a way that for each original row, a number of rows are created where the first two columns are unchanged, the third column is which of the next original columns this new column is from, and the fourth column is the corresponding float value (e.g. 20.33333). I don't think this is a pivot table, but I'm not sure how exactly to get this cleanly. Apologies if this question has been asked before, I can't seem to find what I'm looking for. Apologies also if my explanation or formatting were less than ideal! Thanks for your help.", "q_apis": "get columns where first columns columns value pivot get", "apis": "melt groupby size reset_index", "code": ["df1 = df.melt(id_vars=['CentroidID_O', 'CentroidID_D'], var_name='dt_15')\ndf2 = (df1.groupby(['CentroidID_O', 'CentroidID_D', 'dt_15'])\n          .size()\n          .reset_index(name='counts'))\n"], "link": "https://stackoverflow.com/questions/64725787/how-can-i-rearrange-a-pandas-dataframe-into-this-specific-configuration"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: However, after inserting a new row like: The date column (even though it doesn't have a label) gets transformed into: Instead of: (desired output) How can I add a new row with a certain date but without changing the whole dataframe, so I would get my desired output? Any help would be appreciated, thanks!", "q_apis": "get columns date add date get", "apis": "append DataFrame index", "code": ["row = {'value1': 40, 'value2': 40, 'value3': 40}\ndf.append(pd.DataFrame([row], index=[np.nan]))\n", "            value1  value2  value3\n2021-04-26      22      22      22\n2021-04-27      21      26      26\n2021-04-28      27      29      27\nNaN             40      40      40\n"], "link": "https://stackoverflow.com/questions/67348042/inserting-a-row-into-a-pandas-dataframe-in-a-certain-column-without-a-name"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to add information scraped from a website into columns. I have a dataset that looks like: and I would like to have a dataset which includes new columns: These new columns come from the this website: https://www.urlvoid.com/scan/bbc.co.uk. I would need to fill each column with its related information. For example: Unfortunately I am having some issue in creating new columns and filling them with the information scraped from the website. I might have more websites to check, not only bbc.co.uk. Please see below the code used. I am sure that there is a better (and less confused) approach to do that. I would really grateful if you could help me to figure it out. Thanks EDIT: As shown in the example above, to the already existing dataset including the three columns () I should add also the fields that come from scraping ( ). For each url, then, I should have information related to it (e.g. in the example). (the format is not good, but I think it could be enough to let you have an idea of the expected output). Updated code: Unfortunately there is something that I am doing wrong, as it is copying only the information from the first url checked on the website (i.e. bbc.co.uk) over all the rows under the new column.", "q_apis": "get columns add columns columns columns columns columns add first all", "apis": "DataFrame index transpose get append append append append DataFrame columns", "code": ["cols = ['Col1','Col2']\nrows = ['something','something else']\nmy_df= pd.DataFrame(rows,index=cols).transpose()\nmy_df\n", "for d in dat:\n    row = d.select('td')\n    my_df[row[0].text]=row[1].text\nmy_df\n", "    Col1       Col2       Website Address   Last Analysis   Blacklist Status    Domain Registration     Domain Information  IP Address  Reverse DNS     ASN     Server Location     Latitude\\Longitude  City    Region\n0   something   something else  Bbc.com     11 days ago  |  Rescan  0/35    1989-07-15 | 31 years ago   WHOIS Lookup | DNS Records | Ping   151.101.192.81   Find Websites  |  IPVoid  |  ...   Unknown     AS54113 FASTLY  (US) United States  37.751 / -97.822   Google Map   Unknown     Unknown\n", "urls = ['bbc.com', 'stackoverflow.com']\nares = []\nfor u in urls:\n    url = 'https://www.urlvoid.com/scan/'+u\n    r = requests.get(url)\n    ares.append(r)\nrows = []\ncols = []\nfor ar in ares:\n    soup = bs(ar.content, 'lxml')\n    tab = soup.select(\"table.table.table-custom.table-striped\")        \n    dat = tab[0].select('tr')\n    line= []\n    header=[]\n    for d in dat:\n        row = d.select('td')\n        line.append(row[1].text)\n        new_header = row[0].text\n        if not new_header in cols:\n            cols.append(new_header)\n\n    rows.append(line)\n\nmy_df = pd.DataFrame(rows,columns=cols)   \nmy_df\n", "Website Address     Last Analysis   Blacklist Status    Domain Registration     Domain Information  IP Address  Reverse DNS     ASN     Server Location     Latitude\\Longitude  City    Region\n0   Bbc.com     12 days ago  |  Rescan  0/35    1989-07-15 | 31 years ago   WHOIS Lookup | DNS Records | Ping   151.101.192.81   Find Websites  |  IPVoid  |  ...   Unknown     AS54113 FASTLY  (US) United States  37.751 / -97.822   Google Map   Unknown     Unknown\n1   Stackoverflow.com   5 minutes ago  |  Rescan    0/35    2003-12-26 | 17 years ago   WHOIS Lookup | DNS Records | Ping   151.101.1.69   Find Websites  |  IPVoid  |  Whois   Unknown     AS54113 FASTLY  (US) United States  37.751 / -97.822   Google Map   Unknown     Unknown\n"], "link": "https://stackoverflow.com/questions/61037401/creating-new-columns-by-scraping-information"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I currently have a dictionary with six keys, and the ideal ordering of those keys are as such: Right now each one of those keys corresponds to a dictionary of all possible values. The way I constructed this dictionary was through a Pandas reader (not sure if this matters, but there might be a way to resolve this through Pandas). The current structure of the dict is like so: I would like to turn it into an Ordered Dict of the form: I can't seem to figure out how to iterate over the key, value pairs in the right order to construct a list of tuples. Any thoughts?", "q_apis": "get columns keys keys now keys all values value right", "apis": "append", "code": ["from collections import OrderedDict\n\norder_of_keys = [\"id\", \"question\", \"choice1\", \"choice2\", \"choice3\", \"choice4\", \"solution\"]\n\ninput_dict = {'id': {0: 'CB_1', 1: 'CB_2'},\n              'question': {0: 'Who is Ghoulsbee Scroggins?', 1: 'Who is Ebeneezer Yakbain?'},\n              'choice1': {0: 'Cat', 1: 'A mathematician'},\n              'choice2': {0: 'Dog', 1: 'A mathematician'},\n              'choice3': {0: 'Ape', 1: 'A mathematician'},\n              'choice4': {0: 'Astrophysicist', 1: 'A mathematician'},\n              'solution': {0: 'Ape', 1: 'A mathematician'}}\n\nres = []\n\nfor key in input_dict['id']:\n    d = OrderedDict()\n    d['id'] = key\n    for k in order_of_keys[1:]:\n        d[k] = input_dict[k][key]\n    res.append(d)\n", "[OrderedDict([('id', 0),\n              ('question', 'Who is Ghoulsbee Scroggins?'),\n              ('choice1', 'Cat'),\n              ('choice2', 'Dog'),\n              ('choice3', 'Ape'),\n              ('choice4', 'Astrophysicist'),\n              ('solution', 'Ape')]),\n OrderedDict([('id', 1),\n              ('question', 'Who is Ebeneezer Yakbain?'),\n              ('choice1', 'A mathematician'),\n              ('choice2', 'A mathematician'),\n              ('choice3', 'A mathematician'),\n              ('choice4', 'A mathematician'),\n              ('solution', 'A mathematician')])]\n"], "link": "https://stackoverflow.com/questions/49866797/convert-dictionary-into-ordered-dictionary-using-specified-order"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given a small dataset as follows: I want to replace name columns's chinese character with , if it only contains one character. The expected result will like this: How could I do that in Pandas? Thanks.", "q_apis": "get columns replace name columns contains", "apis": "mask eq", "code": ["mask = df['name'].str.len().eq(1) & df['name'].str.contains(r'[\\u4e00-\\u9fff]')\n", "print (df)\n   id          name\n0   1           NaN\n1   2            \u4f60\u597d\n2   3          \u4f60\u597d\u5417\uff1f\n3   4           NaN\n4   5         hello\n5   6  how are you?\n6   7           how\n"], "link": "https://stackoverflow.com/questions/65352704/replace-only-one-chinese-character-rows-with-nan-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to wrangle some data to make a recommender system for an app. Of course, to do this I need a record of which users like which posts. I currently have that data in a JSON file that is formatted like this (numbers being post id, and letters being user ids): I'm trying to figure out how to get this into a pandas dataframe that would look like this: example format I've tried using a few online JSON to CSV converters out of laziness which unsurprisingly didn't bring it into a useable format for me. I've tried using \"print(json_normalize(data))\", as well which also did not work, and put each instance of a like into separate columns. Any advice?", "q_apis": "get columns get put columns", "apis": "keys DataFrame from_dict", "code": ["import pandas as pd\ndata = {\n       \"-1234\": {\n         \"abc\": \"abc\",\n         \"def\": \"def\",\n         \"ghi\": \"ghi\"\n    },\n       \"-5678\": {\n         \"jkl\": \"jkl\",\n         \"mno\": \"mno\"\n    }}\nformatted = [{'PostID': d, 'User Like': list(data[d].keys())} for d in data]\ndf = pd.DataFrame.from_dict(formatted)\n"], "link": "https://stackoverflow.com/questions/57234654/formatting-json-for-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Well, I need to check if number of rows >= 'x' and if true, delete first 'n' rows Currently I load csv to df, drop lines and load it back, but it's not very clean and efficient way to do this. Do you know any other? Here is algorithm in human language as I can see it: UPD: forgot to add, that file is growing every minute, and i need only like last 1.5k rows of it. The code is a part of a loop btw", "q_apis": "get columns delete first drop any add minute last", "apis": "count", "code": ["n = 25 # your line count.\nwith open('/path/to/your_file.csv') as f:\n    data = f.readlines()\n    lines = len(data)\n    if lines >= n:\n\n        df = pd.read_csv(data,skiprows=range((1, lines-1500)) #using range will keep your header.\n    else:\n        df = pd.read_csv(data)\n"], "link": "https://stackoverflow.com/questions/61773748/delete-first-n-rows-from-csv-file-and-keep-header"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose we have a scenario with multiple customers and each customer can purchase different products of different quantities. This is illustrated in the dataframe below - with multiple ids and each id may have different column values as shown below: From this we want to achieve a single row, so that we can view each customer's purchases on a single row. Updated dataframe should based on the following conditions: Each item in the items' column should have its own column similar to converting it into dummy variables. Therefore, item_140, item_160 etc. The content in item item_46 would be the item number 46. If the customer doesn't the item, it should be assigned a zero. No_Units and No_Purchases should split in relation to the associated item column - eg: No_Units_item_140,No_Units_item_160, No_Purchases_item_140, No_Purchases_item_160. NB: there are multiple columns in addition to the columns shown above. Output Dataframe: The first part of the code is to create the dummy variables: I have attempted to code the solution, however there are issue with speed,labels, merging and inserting original values into the dummy dataframe: I do know that dict() will speed this up considerably aswell.", "q_apis": "get columns values view item items item item item item columns columns first values", "apis": "pivot_table columns columns reset_index", "code": ["df = df.pivot_table(['No_Units','No_Purchases','item'],'id','item_ind',fill_value=0) \ndf.columns =[s1 + str(s2) for (s1,s2) in df.columns.tolist()] \ndf.reset_index(inplace=True) \n"], "link": "https://stackoverflow.com/questions/66476008/converting-a-dataframe-with-non-unique-id-and-column-values-into-a-single-rows-p"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need help comparing values in a pandas Dataframe which are indexed differently. I've read the Dataframe from a csv containing headers 'Time', 'Predicted', 'Engine'. 'Time' is a timeseries \"DD.MM.YYYY hh:mm:ss\" in 10 minute steps , 'Predicted' and 'Engine' take values 0 or 1. So it looks like this: I want to compare the Predicted value at [i] with the Engine value at [i+1]. This was my initial code (to clarify what I was aiming for), which resulted in ValueError: Can only compare identically-labeled Series objects Code: I now get why this isn't working but I can't find a solution to this problem on my own (yet) as I am fairly new to programming.", "q_apis": "get columns values minute take values compare value at value at compare Series now get", "apis": "shift", "code": ["df['Result'] = df['Predicted'].shift(1) == df['Engine']\n"], "link": "https://stackoverflow.com/questions/60989567/comparing-values-in-a-pandas-dataframe-which-are-indexed-differently"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have to calculate the value of S, which has the formula as: S = (25400/CN) \u2212 254 the CN value which I have to choose will depend on the amc_active condition viz 1, 2 and 3. if amc_active condition at 'index 0 or 1st row' is 1 then I have to choose the CN value from column cn1 i.e 47 and if the amc_active is 3, then I have to choose CN value as 95 from cn3 column in the 4th row and so on.. for doing so, I am using else if condition but getting the error as and from this the error is How to rectify this?", "q_apis": "get columns value value at index value value", "apis": "get values columns index", "code": ["# get the values of `cn*` columns\ncn_123 = df_col_all_merged[[\"cn1\", \"cn2\", \"cn3\"]].to_numpy()\n\n# index into it as \"(row_index, amc_active_value-1)\"\ncn = cn_123[np.arange(len(df_col_all_merged)),\n            df.amc_active-1]\n\n# perform the formula\ndf_col_all_merged[\"s_mm\"] = (25400/cn) - 254\n", "       cn1  cn2  cn3  amc_active        s_mm\n0       47   56   78           1  286.425532\n1       55   61   87           2  162.393443\n2       36   67   73           1  451.555556\n3       42   84   95           3   13.368421\n17410   42   84   96           3   10.583333\n17411   48   81   85           1  275.166667\n17412   55   59   82           1  207.818182\n17413   57   86   93           2   41.348837\n17414   36   87   91           2   37.954023\n", "# form the conditions & corresponding choices\nconditions = [df.amc_active.eq(1), df.amc_active.eq(2), df.amc_active.eq(3)]\nchoices = [df.cn1, df.cn2, df.cn3]\n\n# select so\ncn = np.select(conditions, choices)\n\n# formula\ndf_col_all_merged[\"s_mm\"] = (25400/cn) - 254\n"], "link": "https://stackoverflow.com/questions/67967473/selecting-value-of-column-based-on-the-values-in-another-column-then-applying-t"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have an two dataframes A and B. A is the original one but has some incorrect values. Dataframe B has only the values columns and has the correct values. Is there a way I can overwrite the values in A by those in B.", "q_apis": "get columns values values columns values values", "apis": "DataFrame columns DataFrame columns columns", "code": ["df_a = pd.DataFrame(np.ones((5, 4)))    # df_a is 4 columns of ones\ndf_b = pd.DataFrame(np.zeros((5, 2)))   # df_b in 2 columns of zeros\n\ndf_a[df_b.columns] = df_b\n\ndf_a\n", "     0    1    2    3\n0  0.0  0.0  1.0  1.0\n1  0.0  0.0  1.0  1.0\n2  0.0  0.0  1.0  1.0\n3  0.0  0.0  1.0  1.0\n4  0.0  0.0  1.0  1.0\n"], "link": "https://stackoverflow.com/questions/67390453/is-there-a-way-to-overwrite-values-of-a-dataframe-to-another-when-there-is-no-ke"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following code: However, I keep getting the error: ValueError: Can only compare identically-labeled Series objects I am confused as to why this is. The dataframe datamax2015 looks like this: The dataframe datamax looks like this: The columns seem to be the same in both dataframes. I tried resorting the indices in both these dataframes but this did not work.", "q_apis": "get columns compare Series columns indices", "apis": "columns", "code": ["datamax['datamax'] = datamax2015['Data_Value']\ndatamin['datamin'] = datamin2015['Data_Value']\nfilteredmax = datamax[datamax['datamax']>datamax['Data_Value']]\nfilteredmin = datamin[datamin['datamin']<datamin['Data_Value']]\n", "print(filteredmax)\nprint(filteredmin)\n\n\n\nDate           ID Element  Data_Value  datamax\n3    2005-01-04  USW00094889    TMAX          39     44.0\n5    2005-01-06  USW00094889    TMAX           0     33.0\n14   2005-01-15  USW00094889    TMAX         -33     -5.0\n15   2005-01-16  USW00094889    TMAX         -33      6.0\n16   2005-01-17  USW00094889    TMAX         -50     78.0\n17   2005-01-18  USW00094889    TMAX         -33     83.0\n18   2005-01-19  USW00094889    TMAX          11     67.0\n19   2005-01-20  USW00094889    TMAX          11     39.0\n20   2005-01-21  USW00094889    TMAX         -39     22.0\n21   2005-01-22  USW00094889    TMAX         -72     -5.0\n22   2005-01-23  USW00094889    TMAX         -44     11.0\n23   2005-01-24  USW00094889    TMAX          11     44.0\n24   2005-01-25  USW00094889    TMAX          28     33.0\n27   2005-01-28  USW00094889    TMAX         -11      6.0\n28   2005-01-29  USW00094889    TMAX          17     39.0\n31   2005-02-01  USW00094889    TMAX          17     39.0\n38   2005-02-08  USW00094889    TMAX          67     83.0\n39   2005-02-09  USW00094889    TMAX          39     83.0\n60   2005-03-02  USW00094889    TMAX         -11     17.0\n61   2005-03-03  USW00094889    TMAX          17     44.0\n67   2005-03-09  USW00094889    TMAX          -6     94.0\n68   2005-03-10  USW00094889    TMAX          61    100.0\n69   2005-03-11  USW00094889    TMAX          33    133.0\n70   2005-03-12  USW00094889    TMAX          28    128.0\n71   2005-03-13  USW00094889    TMAX          39    150.0\n72   2005-03-14  USW00094889    TMAX          39    128.0\n73   2005-03-15  USW00094889    TMAX          56    139.0\n74   2005-03-16  USW00094889    TMAX          78    206.0\n75   2005-03-17  USW00094889    TMAX          67    194.0\n76   2005-03-18  USW00094889    TMAX          50    117.0\n..          ...          ...     ...         ...      ...\n333  2005-11-30  USW00094889    TMAX          83    100.0\n334  2005-12-01  USW00094889    TMAX          11    111.0\n335  2005-12-02  USW00094889    TMAX           0    100.0\n336  2005-12-03  USW00094889    TMAX          -6     94.0\n337  2005-12-04  USW00094889    TMAX          22     94.0\n338  2005-12-05  USW00094889    TMAX          11     94.0\n339  2005-12-06  USW00094889    TMAX         -22    100.0\n340  2005-12-07  USW00094889    TMAX         -33    100.0\n341  2005-12-08  USW00094889    TMAX         -11    100.0\n342  2005-12-09  USW00094889    TMAX         -11    122.0\n343  2005-12-10  USW00094889    TMAX           0    150.0\n344  2005-12-11  USW00014853    TMAX          33    156.0\n345  2005-12-12  USW00094889    TMAX          22    200.0\n346  2005-12-13  USW00094889    TMAX          -6    194.0\n347  2005-12-14  USW00094889    TMAX          11    183.0\n348  2005-12-15  USW00094889    TMAX          17    161.0\n349  2005-12-16  USW00094889    TMAX          17    150.0\n350  2005-12-17  USW00094889    TMAX         -11    122.0\n351  2005-12-18  USW00094889    TMAX         -28     33.0\n352  2005-12-19  USW00094889    TMAX         -33     11.0\n353  2005-12-20  USW00094889    TMAX         -22     83.0\n354  2005-12-21  USW00094889    TMAX         -22    122.0\n355  2005-12-22  USW00094889    TMAX          28    133.0\n356  2005-12-23  USW00094889    TMAX          50    183.0\n357  2005-12-24  USW00094889    TMAX          61    172.0\n358  2005-12-25  USW00094889    TMAX          61    111.0\n359  2005-12-26  USW00094889    TMAX          50    117.0\n360  2005-12-27  USW00094889    TMAX          39     83.0\n362  2005-12-29  USW00094889    TMAX          56    100.0\n363  2005-12-30  USW00094889    TMAX          39     67.0\n\n[171 rows x 5 columns]\n           Date           ID Element  Data_Value  datamin\n0    2005-01-01  USC00200032    TMIN         -56   -133.0\n1    2005-01-02  USC00200032    TMIN         -56   -122.0\n2    2005-01-03  USC00200032    TMIN           0    -67.0\n3    2005-01-04  USC00200032    TMIN         -39    -88.0\n4    2005-01-05  USC00200032    TMIN         -94   -155.0\n5    2005-01-06  USC00200032    TMIN        -106   -182.0\n6    2005-01-07  USC00200032    TMIN        -111   -182.0\n7    2005-01-08  USC00200032    TMIN        -100   -211.0\n8    2005-01-09  USC00200032    TMIN         -67   -206.0\n9    2005-01-10  USC00200032    TMIN         -56   -206.0\n10   2005-01-11  USC00200032    TMIN         -22   -200.0\n11   2005-01-12  USC00200032    TMIN         -17   -117.0\n12   2005-01-13  USC00200032    TMIN         -83   -216.0\n13   2005-01-14  USC00200032    TMIN        -128   -244.0\n14   2005-01-15  USC00200032    TMIN        -144   -200.0\n15   2005-01-16  USC00200032    TMIN        -150   -167.0\n32   2005-02-02  USC00200032    TMIN        -167   -193.0\n33   2005-02-03  USC00200032    TMIN        -167   -238.0\n34   2005-02-04  USC00200032    TMIN        -156   -211.0\n35   2005-02-05  USC00200032    TMIN        -128   -277.0\n36   2005-02-06  USC00200032    TMIN         -89   -250.0\n37   2005-02-07  USC00200032    TMIN         -56   -122.0\n38   2005-02-08  USC00200032    TMIN         -33    -56.0\n39   2005-02-09  USC00200032    TMIN         -78   -116.0\n40   2005-02-10  USC00200032    TMIN        -111   -171.0\n41   2005-02-11  USC00200032    TMIN        -122   -150.0\n42   2005-02-12  USC00200032    TMIN         -83   -211.0\n43   2005-02-13  USC00200032    TMIN         -56   -266.0\n44   2005-02-14  USC00200032    TMIN         -28   -239.0\n45   2005-02-15  USC00200032    TMIN         -11   -260.0\n..          ...          ...     ...         ...      ...\n262  2005-09-20  USC00200032    TMIN          94     39.0\n263  2005-09-21  USC00200032    TMIN          78     56.0\n264  2005-09-22  USC00200032    TMIN          94     17.0\n266  2005-09-24  USC00200032    TMIN         111     56.0\n267  2005-09-25  USC00200032    TMIN          94     78.0\n275  2005-10-03  USC00200032    TMIN          50     44.0\n276  2005-10-04  USC00200032    TMIN         100     61.0\n277  2005-10-05  USC00200032    TMIN         100     94.0\n282  2005-10-10  USC00200032    TMIN          22      0.0\n284  2005-10-12  USC00200032    TMIN          67     28.0\n285  2005-10-13  USC00200032    TMIN          83     72.0\n286  2005-10-14  USC00200032    TMIN          56    -10.0\n287  2005-10-15  USC00200032    TMIN          56    -16.0\n288  2005-10-16  USC00200032    TMIN          -6    -10.0\n289  2005-10-17  USC00200032    TMIN         -33    -55.0\n290  2005-10-18  USC00200032    TMIN         -11    -55.0\n291  2005-10-19  USC00200032    TMIN          -6    -61.0\n292  2005-10-20  USC00200032    TMIN         -22    -39.0\n293  2005-10-21  USC00200032    TMIN           0    -28.0\n311  2005-11-08  USC00200032    TMIN         -11    -55.0\n312  2005-11-09  USC00200032    TMIN          22    -61.0\n313  2005-11-10  USC00200032    TMIN         -39    -44.0\n318  2005-11-15  USC00200032    TMIN         -17    -22.0\n325  2005-11-22  USC00200032    TMIN         -78   -116.0\n331  2005-11-28  USC00200032    TMIN           0    -39.0\n332  2005-11-29  USC00200032    TMIN         -39    -93.0\n333  2005-11-30  USC00200032    TMIN         -56    -89.0\n359  2005-12-26  USC00200032    TMIN         -11    -39.0\n361  2005-12-28  USC00200032    TMIN         -28    -39.0\n362  2005-12-29  USC00200032    TMIN         -11    -39.0\n"], "link": "https://stackoverflow.com/questions/62089133/valueerror-can-only-compare-identically-labeled-series-objects-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here's my data, I want to filter on latest version Here's what I want, because > > What I did is And then filter max, but I am seeking for better solution", "q_apis": "get columns filter filter max", "apis": "DataFrame columns sort_values drop_duplicates sort_index", "code": ["df = pd.DataFrame([[1,67,'one'], [1, 89, 'three'],\n               [2, 78,  'two'], [2, 70, 'one']], columns = ['Id', 'Score', 'Version' ])    \nd = {'one':1,'two':2, 'three':3}\ndf['vers'] = df['Version'].map(d)\ndf = df.sort_values('vers', ascending=False).drop_duplicates('Id').sort_index()\n", "   Id  Score Version  vers\n1   1     89   three     3\n2   2     78     two     2\n"], "link": "https://stackoverflow.com/questions/50443189/remove-duplicate-based-on-text-versioning"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How can I select rows from a based on values in some column in Pandas? In SQL, I would use: I tried to look at Pandas' documentation, but I did not immediately find the answer.", "q_apis": "get columns select values at", "apis": "loc", "code": ["df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\n", "df['column_name'] >= A & df['column_name'] <= B\n", "df['column_name'] >= (A & df['column_name']) <= B\n", "import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n", "     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n", "     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n", "       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n", "       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n"], "link": "https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two datadrames I want to inner join these df by the time diff is < 2 minutes, for example, row 1 in df A will join with row 1 in df B, for they're only 1 min diff. How can I set the condition to join these dataframes?", "q_apis": "get columns join time diff join min diff join", "apis": "sample sample index set_index set_index resample resample mean resample mean join drop all values join dropna all", "code": ["from io import StringIO\nimport pandas as pd\n\n# create sample data as in example given\ntextA = \"\"\"\ntime                  col1\n2020-10-31T16:30:30   10\n2020-10-31T16:40:30   40\n\"\"\"\n\ntextB = \"\"\"\ntime                  col2\n2020-10-31T16:31:30   10\n\"\"\"\n\n# put sample data in dataframes\ndfA = pd.read_csv(StringIO(textA), header=0, sep='\\s+')\ndfB = pd.read_csv(StringIO(textB), header=0, sep='\\s+')\ndfA['time'] = pd.to_datetime(dfA['time'])\ndfB['time'] = pd.to_datetime(dfB['time'])\n\n# for resampling the index needs to be a datetime\ndfA = dfA.set_index('time')\ndfB = dfB.set_index('time')\n\n# resample rows to every 2 minutes\ndfA = dfA.resample('2min').mean()\ndfB = dfB.resample('2min').mean()\n\n# join dataframes based on resampled time\n# and drop all rows that only have na values\ndfA.join(dfB).dropna(how='all')\n"], "link": "https://stackoverflow.com/questions/64643154/python-dataframe-inner-join-by-time-period"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "We are currently using 3 different dataframes to store product,performance and assortments data. The foreign key relationship is maintained between all the dimensions. I need to update the cost column in the performance by doing the below math operation I need this operation to be performed for each row of the performance dataframe. Please suggest any approach to make the calculations faster. The performance dataframe consists of 1 million records. Can we use any other approach rather than dataframe??", "q_apis": "get columns product between all update any any", "apis": "product DataFrame columns product product product DataFrame columns DataFrame columns product product product DataFrame columns product product product DataFrame columns DataFrame columns append mean product DataFrame columns product product product DataFrame columns DataFrame columns product append mean", "code": ["import pandas as pd\nimport math \nimport numpy as np\n\n\nproduct = pd.DataFrame(columns=['col3', 'sincol3'])\nproduct['col3'] = np.random.randn(1000000)\ns = pd.Series(product['col3'])\nproduct['sincol3'] = np.sin(s)\n\nperformance  = pd.DataFrame(columns=['col1', 'cost'])\nperformance['col1'] = np.random.randn(1000000)\n\nassortments  = pd.DataFrame(columns=['col2'])\nassortments['col2'] = np.random.randn(1000000)\n\nperformance['cost'] = performance['col1']+ product['sincol3'] + 2*assortments['col2']\n\nprint(performance)\n", "         col1      cost\n0       0.194011 -1.940931\n1       0.535375  1.891468\n", "import pandas as pd\nimport math \nimport numpy as np\nimport time\n\ndef cal():\n    performance['cost'] = performance['col1']+ product['sincol3'] + 2*assortments['col2']\n\n\nexecution_time = []\nproduct = pd.DataFrame(columns=['col3', 'sincol3'])\nproduct['col3'] = np.random.randn(1000000)\ns = pd.Series(product['col3'])\nproduct['sincol3'] = np.sin(s)\n\nperformance  = pd.DataFrame(columns=['col1', 'cost'])\nperformance['col1'] = np.random.randn(1000000)\n\nassortments  = pd.DataFrame(columns=['col2'])\nassortments['col2'] = np.random.randn(1000000)\n\nfor i in range(0,50):\n    start_time = time.time()\n    cal()\n    execution_time.append(time.time() - start_time)\n\nprint('average time taken:\\n')\nprint(np.mean(execution_time))\n", "import pandas as pd\nimport math \nimport numpy as np\nimport time\n\ndef cal():\n    execution_time = []\n    product = pd.DataFrame(columns=['col3', 'sincol3'])\n    product['col3'] = np.random.randn(1000000)\n    s = pd.Series(product['col3'])\n    product['sincol3'] = np.sin(s)\n\n    performance  = pd.DataFrame(columns=['col1', 'cost'])\n    performance['col1'] = np.random.randn(1000000)\n\n    assortments  = pd.DataFrame(columns=['col2'])\n    assortments['col2'] = np.random.randn(1000000)\n\n    performance['cost'] = performance['col1']+ product['sincol3'] + 2*assortments['col2']\n\nfor i in range(0,50):\n    start_time = time.time()\n    cal()\n    execution_time.append(time.time() - start_time)\n\nprint('average time taken:\\n')\nprint(np.mean(execution_time))\n"], "link": "https://stackoverflow.com/questions/58856251/multidimensional-computation-using-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's say I have an excel file with rows, I need to split and write into excel files with equivalent row number for each new file, except the last one since there is only one row left. This is code I have tried, but I get : Someone could help with this issue? Thanks a lot. Reference related: Split pandas dataframe into multiple dataframes with equal numbers of rows", "q_apis": "get columns last left get", "apis": "DataFrame columns groupby index", "code": ["df = pd.DataFrame(data=np.random.rand(101, 3), columns=list('ABC'))\ngroups = df.groupby(np.arange(len(df.index))//10)\nfor i, g in groups:\n    print (g)\n"], "link": "https://stackoverflow.com/questions/60000054/split-one-excel-file-into-multiple-with-specific-number-of-rows-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here is the data: Now I want to search through the dataframes and for two column values I want to get the corresponding two column values. Here is what I tried- but its getting the error", "q_apis": "get columns values get values", "apis": "set_index loc values", "code": ["df2 = df.set_index(['age', 'preTestScore'])\n\ncols = ['first_name', 'last_name']\nres = df2.loc[(42, 4), cols].values.tolist()\n\nprint(res)\n\n[['Jason', 'Miller']]\n"], "link": "https://stackoverflow.com/questions/51246259/parsing-through-multiple-columns-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to create a histogram to represent several elements of a dataframe. The dataframe is like follows: I am trying to make a histogram with Item 1 and Item 2 along the bottom, and the numbers representing the height (on the y axis) as a frequency (note that I do not want cumulative frequency). I have tried this: and I am getting results like this: Image showing the histogram (Actual result only differs from desired as per the number of elements in the dataframe) As oppose to the desired result, seen here: Image showing the desired result I have tried with orient='index' and without it, but the two are both incorrect and not what I am looking for.", "q_apis": "get columns index", "apis": "DataFrame plot", "code": [">>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n"], "link": "https://stackoverflow.com/questions/59687899/python-pandas-histogram-with-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to use numpy.where() to add a column to a pandas.DataFrame. I'd like to use NaN values for the rows where the condition is false (to indicate that these values are \"missing\"). Consider: Why does B show \"NaN\" but C shows \"nan\"? And why does DataFrame.isna() fail to detect the NaN values in C? Should I use something other than numpy.nan inside where? and both seem to work and can be detected by DataFrame.isna(), but I'm not sure these are the best choice. Thank you! Edit: As per @Tim Roberts and @DYZ, numpy.where returns an array of type string, so the str constructor is called on numpy.NaN. The values in column C are actually strings \"nan\". The question remains, however: what is the most elegant thing to do here? Should I use ? Or something else?", "q_apis": "get columns where add DataFrame values where values DataFrame isna values where DataFrame isna where array values", "apis": "where fillna", "code": ["df['C'] = numpy.where(df['A'] < 3, 'yes', None)\ndf['C'].fillna(np.nan, inplace=True)\n"], "link": "https://stackoverflow.com/questions/67477744/pandas-numpy-where-and-numpy-nan"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to pop MongoDB documents that consist array of JSON by using , usually it works well if MongoDB documents is only JSON, in array of JSON it produce too much column, so I can't pop to non-JSON format easily. for more detailed question I give explanation below Here's my data What I try to pop The output is The output that I expected is So, I can pop again", "q_apis": "get columns pop array array pop pop pop", "apis": "DataFrame pop values index index stack reset_index drop join rename DataFrame values DataFrame", "code": ["s = (pd.DataFrame(df.pop('locations').values.tolist(), index=df.index)\n       .stack()\n       .reset_index(level=1, drop=True))\n\ndf2 = df.join(s.rename('new'))\n", "df2 = pd.DataFrame({\n        \"Id\": np.repeat(df.Id.values, df.locations.str.len()),\n        \"new\": list(chain.from_iterable(df.locations))})\nprint (df2)\n   Id                                               new\n0   1  {'timestamp': '2018-05-28 15:00:00', 'lat': 0.0}\n1   1  {'timestamp': '2018-05-28 16:00:00', 'lat': 0.0}\n2   2  {'timestamp': '2018-05-28 10:00:00', 'lat': 0.0}\n3   2  {'timestamp': '2018-05-28 17:00:00', 'lat': 0.0}\n4   2  {'timestamp': '2018-05-28 18:00:00', 'lat': 0.0}\n", "df = pd.DataFrame({'Id':[1,2], \n                   'locations':[[{'timestamp': '2018-05-28 15:00:00', 'lat': 0.0}, {'timestamp': '2018-05-28 16:00:00', 'lat': 0.0}],\n                                [{'timestamp': '2018-05-28 10:00:00', 'lat': 0.0}, {'timestamp': '2018-05-28 17:00:00', 'lat': 0.0}, {'timestamp': '2018-05-28 18:00:00', 'lat': 0.0}]]})\nprint (df)\n\n\n   Id                                          locations\n0   1  [{'timestamp': '2018-05-28 15:00:00', 'lat': 0...\n1   2  [{'timestamp': '2018-05-28 10:00:00', 'lat': 0...\n"], "link": "https://stackoverflow.com/questions/51019323/how-to-explode-or-pop-array-of-json-twice"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm having trouble finding an elegant solution to this problem (there might not be one). I have the following example DataFrame: np.random.seed(0) df = pd.DataFrame(np.random.randn(10,10)).abs() I have the following zones map: zones = {\"A\": [0,1,2], \"B\": [3,4], \"C\": [5,6,7,8], \"D\": [9]} The zones show me groups of columns that I should examine together and for each row of df[columns] DataFrame, keep the top N items (NB: keep the top N items across the the row, i.e. cross-sectionally - see later), set the rest to zero. For example for zone \"A\" with N=2, I would examine the following DataFrame: and because N=2 I will keep the top N items: The entire output with the zone map above and N=2 will look like: The way I am attempting to solve this feels a bit slow. I loop over the zones, then I get a zone_df and then I loop over row, sorting each row and calling row.head(len(row) - N) to get the index and columns which need to be set to 0. I then use these values (in a dict) to set cells in zone_df to zero and then combine the zone_dfs.", "q_apis": "get columns DataFrame DataFrame abs map groups columns columns DataFrame items items DataFrame items map get head get index columns values combine", "apis": "values values shape", "code": ["def keeptopN_perkey(df, zones, N=2):\n    a = df.values\n    indx = zones.values()\n    r = np.arange(a.shape[0])[:,None]\n    for i in indx:\n        b = a[:,i]\n        L = np.maximum(len(i)-N,0)\n        if L>0:\n            idx = np.argpartition(b, L, axis=1)[:,:L] \n            # or np.argsort(b,axis=1)[:,:L]\n            b[r, idx] = 0\n        a[:,i] = b\n    return df\n", "In [303]: np.random.seed(0)\n     ...: N = 2\n     ...: df = pd.DataFrame(np.random.randint(11,99,(4,10)))\n     ...: zones = {\"A\": [0,1,2], \"B\": [3,4], \"C\": [5, 6,7,8], \"D\": [9]}\n     ...: \n\nIn [304]: df\nOut[304]: \n    0   1   2   3   4   5   6   7   8   9\n0  55  58  75  78  78  20  94  32  47  98\n1  81  23  69  76  50  98  57  92  48  36\n2  88  83  20  31  91  80  90  58  75  93\n3  60  40  30  30  25  50  43  76  20  68\n\nIn [305]: keeptopN_perkey(df, zones, N=2)\nOut[305]: \n    0   1   2   3   4   5   6   7   8   9\n0   0  58  75  78  78   0  94   0  47  98\n1  81   0  69  76  50  98   0  92   0  36\n2  88  83   0  31  91  80  90   0   0  93\n3  60  40   0  30  25  50   0  76   0  68\n", "def mask_n(df, n): # @piRSquared's helper func\n    v = np.zeros(df.shape, dtype=bool)\n    n = min(n, v.shape[1])\n    if v.shape[1] > n:\n        j = np.argpartition(-df.values, n, 1)[:, :n].ravel()\n        i = np.arange(v.shape[0]).repeat(n)\n        v[i, j] = True\n        return df.where(v, 0)\n    else:\n        return df\n\ndef piRSquared1(df, zones): # @piRSquared's soln1\n    zinv = {v: k for k in zones for v in zones[k]}\n    return df.groupby(zinv, 1).apply(mask_n, n=2)\n\ndef piRSquared2(df, zones): # @piRSquared's soln2\n    zinv = {v: k for k in zones for v in zones[k]}\n    return df.mask(df.groupby(zinv, 1).rank(axis=1, method='first', \n                   ascending=False) > 2, 0)\n\ndef COLDSPEED1(df, zones): # @COLDSPEED's soln\n    for z in zones:                   \n        df2 = df.iloc[:, zones[z]]\n        df.iloc[:, zones[z]] = \\\n                np.where(((-df2).rank(axis=1) - 1) >= 2, 0, df2.values)\n    return df\n\ndef s5s1(df, zones, N=2): # @s5s's soln\n    final = []\n    for zone_id, cols in zones.iteritems():\n        values = {}\n        d = df[cols]  # zone A\n        for i, row in d.iterrows():\n            if len(row) > N:\n                row.sort()\n                row[row.head(len(row) - N).index] = 0\n            values[i] = row\n        d = pd.DataFrame(values).T\n        final.append(d)\n\n    return pd.concat(final, axis=1)[df.columns]\n", "In [458]: # Setup\n     ...: ncols = 1000\n     ...: cuts = np.sort(np.random.choice(ncols, ncols//3, replace=0))\n     ...: indx_split = np.split(np.arange(ncols),cuts)\n     ...: zones = {i:p_i for i,p_i in enumerate(list(map(list,indx_split)))}\n     ...: df = pd.DataFrame(np.random.randint(11,99,(10,ncols)))\n     ...: N = 2\n     ...: \n     ...: df1 = df.copy()\n     ...: df2 = df.copy()\n     ...: df3 = df.copy()\n     ...: df4 = df.copy()\n     ...: df5 = df.copy()\n     ...: \n\nIn [459]: %timeit COLDSPEED1(df1, zones)\n     ...: %timeit piRSquared1(df2, zones)\n     ...: %timeit piRSquared2(df3, zones)\n     ...: %timeit s5s1(df4, zones)\n     ...: %timeit keeptopN_perkey(df5, zones)\n     ...: \n1 loop, best of 3: 324 ms per loop\n10 loops, best of 3: 116 ms per loop\n10 loops, best of 3: 81.6 ms per loop\n1 loop, best of 3: 1.47 s per loop\n100 loops, best of 3: 2.99 ms per loop\n"], "link": "https://stackoverflow.com/questions/46530720/keep-top-n-values-of-each-row-in-a-dataframe-within-groups-of-column-indices"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am having problems to use the proper pandas function to drop rows in dataframe of duplicate value of a key inside a dict in one of its column lugar. This is the data of the dataframe: As you can see, the lugar column has a dictionary and one of the keys is nombre in this case the value: Veraguas, Panam\u00e1 is duplicated, I will like to drop duplicates rows of dataframe and keep one row only per name for the rows that has the dict and key in lugar column. One approach I have tried is to create a new column with the value of the key and then run drop_duplicates but I am unable to get the value inside the column. but I am able to get it for the first row like this -> Huimanguillo, Tabasco, M\u00e9xico Is there a way to do this without looping the df doing it manually? I am really new to Python and Pandas. EDITED: Expected result I converted to csv to be able to delete in spreadsheet as I am unable to do it with pandas...", "q_apis": "get columns drop value keys value duplicated drop name value drop_duplicates get value get first delete", "apis": "loc duplicated isna T T T T T T T T T columns", "code": ["s = df['lugar'].str['nombre']\ndf.loc[~s.duplicated() | s.isna()]\n", "                  calculado  ... variable\n73683   2021-05-27 00:43:46  ...        T\n83767   2021-05-27 00:43:46  ...        T\n103395  2021-05-27 00:43:46  ...        T\n105314  2021-05-27 00:43:46  ...        T\n116555  2021-05-27 00:43:46  ...        T\n120764  2021-05-27 00:43:46  ...        T\n120892  2021-05-27 00:43:46  ...        T\n124269  2021-05-27 00:43:46  ...        T\n125707  2021-05-27 00:43:46  ...        T\n\n[9 rows x 7 columns]\n"], "link": "https://stackoverflow.com/questions/67714397/drop-duplicates-on-dataframe-on-value-inside-dictionary-in-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: that looks like this: I want to create a nested dict, with the columns \"color\" and \"object\" as the first key as a string (e.g. (Note: This is a syntactically incorrect tuple with intention. It should be in string format!), the group as the key in the second level and the value as the key of the second level. I.e. my desired output is: My approach would be to loop through the unique values of , and , but that seems cumbersome to me. Is there a more pythonic approach?", "q_apis": "get columns columns first second value second unique values", "apis": "set_index to_dict groupby join set_index to_dict groupby", "code": ["d = {str(k): v.set_index('group')['value'].to_dict() \n              for k, v in df.groupby(['color','object'])}\nprint (d)\n\n  {\n    \"('blue', 'coat')\": {\n        1: 5.4,\n        2: 7.1\n    },\n    \"('blue', 'hat')\": {\n        1: 1.2,\n        2: 3.5\n    },\n    \"('green', 'coat')\": {\n        1: 3.5,\n        2: 5.6\n    },\n    \"('green', 'hat')\": {\n        1: 6.4,\n        2: 1.8\n    }\n", "d = {f'({k[0]}, {k[1]})': v.set_index('group')['value'].to_dict() \n                 for k, v in df.groupby(['color','object'])}\n", "d = {f'({\", \".join(k)})': v.set_index('group')['value'].to_dict() \n                 for k, v in df.groupby(['color','object'])}\n", "print (d)\n\n{\n    '(blue, coat)': {\n        1: 5.4,\n        2: 7.1\n    },\n    '(blue, hat)': {\n        1: 1.2,\n        2: 3.5\n    },\n    '(green, coat)': {\n        1: 3.5,\n        2: 5.6\n    },\n    '(green, hat)': {\n        1: 6.4,\n        2: 1.8\n    }\n}\n"], "link": "https://stackoverflow.com/questions/64820196/most-pythonic-way-to-transform-dataframe-into-nested-custom-dict-with-tuple-styl"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "may I know how to convert text/string data to numbers for a column in Dataframe ? If the same text/string appear again, they should return the same number. Looking for a general way to convert since there are thousands of fruit in the world Example : Fruit Number (expected outcome) 1 Apple 1 2 Orange 2 3 Apple 1 4 Banana 3 5 Blackberries 4 6 Avocado 5 7 Grapes 6 8 Orange 2 9 Apple 1 10 Mango 7 . . . . . . . . .", "q_apis": "get columns", "apis": "DataFrame get index index values", "code": ["import pandas as pd \n\nfruitList={'name':[ \"Apple\",\"Orange\",\"Apple\",\"Banana\",\"Blackberries\",\"Avocado\",\"Grapes\",\"Orange\",\"Apple\",\"Mango\"] }\ndf = pd.DataFrame(fruitList) \n\n# get distinct fruit names\nunique=df.name.unique()\n# generating a dictionary based on Id of unique fruit names using list comprehension\ndict={ x:index+1 for index, x in enumerate(unique) }\n# assigning new column 'Id' values from the dictionary using the map function \ndf['Id']  = df[\"name\"].map(dict)\nprint(df)\n", "        name      Id\n0         Apple   1\n1        Orange   2\n2         Apple   1\n3        Banana   3\n4  Blackberries   4\n5       Avocado   5\n6        Grapes   6\n7        Orange   2\n8         Apple   1\n9         Mango   7\n"], "link": "https://stackoverflow.com/questions/59231867/convert-text-string-to-number-int-for-python-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a 3 column dataframe. Let's say my columns are \"doc\", \"word\", \"count\" and each row shows number of occurrences of a word in a document. I want to convert this dataframe to a matrix having rows as documents and columns as words so I do the following: What I get is a matrix having columns . However, what I want is to have another range for my columns, e.g. . This latter will end up some columns having all entries as 0 and this is what I want. How can I achieve this?", "q_apis": "get columns columns count columns get columns columns columns all", "apis": "shape max max DataFrame shape max DataFrame", "code": ["import pandas as pd\nfrom scipy.sparse import csr_matrix\n\nrows, cols, data = zip(*df.to_numpy())\nmat = csr_matrix((data, (rows, cols)), shape=(max(rows) + 1, max(cols) + 1))\nres = pd.DataFrame(data=mat.toarray())\nprint(res)\n", "    0  1  2  3  4  5  6  7\n0  10  0  0  0  5  0  0  2\n1   0  0  5  0  0  0  0  0\n", "rows, cols, data = zip(*df.to_numpy())\nmat = csr_matrix((data, (rows, cols)), shape=(max(rows) + 1, 10))\nres = pd.DataFrame(data=mat.toarray())\nprint(res)\n", "    0  1  2  3  4  5  6  7  8  9\n0  10  0  0  0  5  0  0  2  0  0\n1   0  0  5  0  0  0  0  0  0  0\n"], "link": "https://stackoverflow.com/questions/65224777/converting-3-column-dataframe-to-matrix-with-columns-defined-by-range"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two different data frames (df1&df2) and df1 has a few worksheets but the df2 has only one worksheet. Additionaly, I have a string which is 'checker = Airplane'. What I would like to do is to check if df1 has a worksheet named as 'Airplane'. If yes, I would like to copy df2 to the Airplane worksheet in the df1. I would be happy to hear some suggestions. Thanks!", "q_apis": "get columns copy", "apis": "compare append head to_excel index", "code": ["import pandas as pd\n\ndf1 = pd.read_excel('File1.xlsx',sheet_name='Airplane')\n# Read Lit of All the Sheets in the file 2\nall_sheets = pd.ExcelFile('File2.xlsx')\nworksheets = all_sheets.sheet_names\n\n# Loop and compare checker\nchecker = 'Airplane'\nfor wsh in worksheets:\n    print(wsh)\n    if wsh==checker:\n        df2 = pd.read_excel('File2.xlsx',sheet_name='Airplane')\n        # Append Data from File 2 Airplane Sheet\n        df1 = df1.append(df2)\n        # After Appending the dataframe lets check the output\n        print(df1.head(6))\n        # you can write the output on excel to\n        df1.to_excel('Output.xlsx',index=False)\n"], "link": "https://stackoverflow.com/questions/66639869/how-can-i-copy-a-df2-to-df1-if-the-name-of-the-worksheet-in-df1-matches-with-a-s"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a numpy array and I want to convert it into a dataframe. How do I convert it into a dataframe where the data will be like this:", "q_apis": "get columns array where", "apis": "shape DataFrame DataFrame T columns", "code": ["# Create list of column names with the format \"colN\" (from 1 to N)\ncol_names = ['col' + str(i) for i in np.arange(nparray.shape[0]) + 1]\n# Declare pandas.DataFrame object\ndf = pd.DataFrame(data=nparray.T, columns=col_names)\n"], "link": "https://stackoverflow.com/questions/65311352/convert-numpy-array-into-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two csv files/Pandas Dataframes organized like and I need to end up with a single dataframe that is essentially the second one with two additional boolean columns: and . So for the first new column the plan is to use to isolate the image codes and for the second to isolate the missing indexes. I'm struggling with how to 1. Join the missing indexes with their sheet ID to get the missing images codes' and 2. Join all this in the last dataframe. I'm not terrible at 2D Pandas but the function returns a MultiIndex object that I can't really understand how to operate. Thanks a lot,", "q_apis": "get columns second columns first codes second get codes all last at MultiIndex", "apis": "explode explode dropna dropna", "code": ["df = pd.read_csv(\"./First.csv\", names=[\"id\", \"issue\", \"obs\"])\ndf[\"crop\"] = df[\"issue\"].str.findall(\n    r\"(?P<crop>[\\w-]+)(?=(?:\\s*,\\s*[\\w-]+)*\\s+crop again)\"\n)\ndf[\"missing\"] = df[\"issue\"].str.findall(\n    r\"(?!Missing images?) (?P<missing>\\d{2})\"\n)\ndf = df.explode(\"crop\")\ndf = df.explode(\"missing\")\ndf[\"missing\"] = df[\"id\"] + \"-\" + df[\"missing\"]\ncrop = df[\"crop\"].dropna()\nmissing = df[\"missing\"].dropna()\n"], "link": "https://stackoverflow.com/questions/62938227/extract-strings-from-dataframe-join-with-previous-column-and-merge-with-another"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Pandas: select DF rows based on another DF is the closest answer I can find to my question, but I don't believe it quite solves it. Anyway, I am working with two very large pandas dataframes (so speed is a consideration), df_emails and df_trips, both of which are already sorted by CustID and then by date. df_emails includes the date we sent a customer an email and it looks like this: df_trips includes the dates a customer came to the store and how much they spent, and it looks like this: Basically, what I need to do is find the number of trips and total spend for each customer in between each email sent. If it is the last time an email is sent for a given customer, I need to find the total number of trips and total spend after the email, but before the end of the data (2018-04-01). So the final dataframe would look like this: Though I have tried my best to do this in a Python/Pandas friendly way, the only accurate solution I have been able to implement is through an np.where, shifting, and looping. The solution looks like this: However, a %%timeit has revealed that this takes 10.6ms on just the seven rows shown above, which makes this solution pretty much infeasible on my actual datasets of about 1,000,000 rows. Does anyone know a solution here that is faster and thus feasible?", "q_apis": "get columns select date date between last time where", "apis": "sort_values sort_values loc groupby agg count sum join sort_values", "code": ["df_emails = df_emails.sort_values(\"DateSent\")\ndf_trips = df_trips.sort_values(\"TripDate\")\ndf_lookup = pd.merge_asof(df_trips, df_emails, by=\"CustID\", left_on=\"TripDate\",right_on=\"DateSent\", direction=\"backward\")\n", "df_lookup = df_lookup.loc[:, [\"CustID\", \"DateSent\", \"TotalSpend\"]].groupby([\"CustID\", \"DateSent\"]).agg([\"count\",\"sum\"])\n", "df_merge = df_emails.join(df_lookup, on=[\"CustID\", \"DateSent\"]).sort_values(\"CustID\")\n", "   CustID   DateSent NextDateSent  (TotalSpend, count)  (TotalSpend, sum)\n0       2 2018-01-20   2018-02-19                  2.0              125.0\n1       2 2018-02-19   2018-03-31                  1.0              250.0\n2       2 2018-03-31          NaT                  NaN                NaN\n3       4 2018-01-10   2018-02-26                  NaN                NaN\n4       4 2018-02-26          NaT                  2.0              200.0\n5       5 2018-02-01   2018-02-07                  NaN                NaN\n6       5 2018-02-07          NaT                  NaN                NaN\n"], "link": "https://stackoverflow.com/questions/56906293/counting-number-of-occurrences-between-dates-given-an-id-value-from-another-da"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with datetime index: and I'm trying to add some new records but I get: I just want to add a 30 minutes interval record, my code is something like", "q_apis": "get columns index add get add", "apis": "append DataFrame index", "code": ["                   count    day hour    minute  label\ntime                    \n2018-06-07 00:25:00 207 7.0 0.0 25.0    177.0\n2018-06-07 00:55:00 187 7.0 0.0 55.0    150.0\n", "                     count  day  hour  label  minute\n2018-06-07 00:25:00  207.0  7.0   0.0  177.0    25.0\n2018-06-07 00:55:00  187.0  7.0   0.0  150.0    55.0\n2018-06-07 01:25:00    NaN  NaN   NaN    NaN     NaN\n", "data = {'hour':10} \ndf.append(pd.DataFrame(data, index=[last_date]))\n", "                     count  day  hour  label  minute\n2018-06-07 00:25:00  207.0  7.0   0.0  177.0    25.0\n2018-06-07 00:55:00  187.0  7.0   0.0  150.0    55.0\n2018-06-07 01:25:00    NaN  NaN   10.0    NaN     NaN\n"], "link": "https://stackoverflow.com/questions/50840769/insert-row-with-datetime-index-to-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a series of booleans extracted and I would like to filter a dataframe from this in Pandas but it is returning no results. Dataframe Here are the series: Now below, when I try to filter this, I am getting empty results: Lastly, when I just use one column, it seems to match and filter by row and not column. Any advice here on how to correct this? I thought that this should just populate BPHONE along the column axis and not the row axis. How would I be able to filter this way? The output wanted is the following: To explain this, rphone shows True, False, True, so only the True numbers should show. Under False it should not show, or show as NAN.", "q_apis": "get columns filter filter empty filter filter", "apis": "where", "code": ["df['BPHONE'] = pd.np.where(condition, df['BPHONE'], pd.np.nan)\n"], "link": "https://stackoverflow.com/questions/59974678/how-to-filter-by-boolean"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Following this Q&A, i have managed to concatenate several CSV files into one time-series dataframe, appending a column to add the name of CSV file from which each record came, like so: This seems to work fine, and- as you can see in this Jupyter Notebook -i get a dataframe whose shape has 5 columns. But then when i downsample this time series df from 15 minute intervals to an hourly mean, like so: I get a dataframe whose shape has only 4 columns. So it seems like this append function i have performed lacks persistence, and i need to make it persist. I have tried inserting the \"inplace=True\" arg into the append function itself (threw an error) and also after it (made no difference). If anyone can show me the way to make this appended column permanent, i'd be much obliged!", "q_apis": "get columns time add name get shape columns time minute mean get shape columns append append difference", "apis": "keys reset_index set_index groupby resample mean dropna", "code": ["capture_datetime_utc,fertilizer_level,light,soil_moisture_present,air_temperature_celsius\n2018-07-30 17:34:33,-1.0,1.28,12.13,26.42\n2018-07-30 17:49:33,-1.0,1.26,11.87,26.51\n2018-07-30 18:04:33,-1.0,1.26,11.47,26.37\n2018-07-30 18:19:33,-1.0,1.17,12.00,26.28\n2018-07-30 18:34:33,-1.0,0.94,11.47,25.34\n", "capture_datetime_utc,fertilizer_level,light,soil_moisture_present,air_temperature_celsius\n2018-08-28 07:50:23,-1.0,40.73,6.53,31.82\n2018-08-28 08:05:23,-1.0,47.13,6.65,33.65\n2018-08-28 08:20:23,-1.0,51.94,6.65,35.00\n2018-08-28 08:35:23,-1.0,57.46,6.65,36.55\n2018-08-28 08:50:23,-1.0,14.17,6.77,32.98\n", "all_files = ['sample1.csv','sample2.csv']\n\ndf = pd.concat([pd.read_csv(file_, sep=',', parse_dates=[\"capture_datetime_utc\"], index_col=\"capture_datetime_utc\") for file_ in all_files], keys=all_files)\n\ndf = df.reset_index().set_index('capture_datetime_utc').groupby('level_0').resample('H').mean().dropna()\n", "                                  fertilizer_level      light  \\\nlevel_0     capture_datetime_utc                                \nsample1.csv 2018-07-30 17:00:00               -1.0   1.270000   \n            2018-07-30 18:00:00               -1.0   1.123333   \nsample2.csv 2018-08-28 07:00:00               -1.0  40.730000   \n            2018-08-28 08:00:00               -1.0  42.675000   \n\n                                  soil_moisture_present  \\\nlevel_0     capture_datetime_utc                          \nsample1.csv 2018-07-30 17:00:00               12.000000   \n            2018-07-30 18:00:00               11.646667   \nsample2.csv 2018-08-28 07:00:00                6.530000   \n            2018-08-28 08:00:00                6.680000   \n\n                                  air_temperature_celsius  \nlevel_0     capture_datetime_utc                           \nsample1.csv 2018-07-30 17:00:00                 26.465000  \n            2018-07-30 18:00:00                 25.996667  \nsample2.csv 2018-08-28 07:00:00                 31.820000  \n            2018-08-28 08:00:00                 34.545000\n"], "link": "https://stackoverflow.com/questions/52103193/pandas-dataframe-append-function-does-not-persist"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe df1, Then I have a list of all the company names with just their tickers tickerdf, If I wanted to merge, on the company name I would do, But if I did that I would end up with all the Ticker values from tickerdf1 like this But, I want it to retain the values from the df1, basically only merge on the rows where there is no data on the ticker column, the output should look like this. Is it possible to only merge data on rows where the Ticker column is empty?", "q_apis": "get columns all names merge name all values values merge where merge where empty", "apis": "set_index fillna", "code": ["# Ticker by company\ns = df2.set_index('CompanyName')['Ticker']\n\ndf['Ticker'] = df['Ticker'].fillna(df['CompanyName'].map(s) )\n"], "link": "https://stackoverflow.com/questions/61658842/how-to-merge-only-on-rows-where-there-is-no-value-in-the-rows-of-a-certain-colum"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "View of abilities list / dataframe I would like to print the first skill in the abilities list for the first 10 rows. I have the first 10 rows, and have tried indexing the list with .str but I am having trouble. Thank you so much for your help. df.loc[0:10]['Abilities'] OUTPUT: 0 Overgrow,Chlorophyll 1 Overgrow,Chlorophyll 2 Overgrow,Chlorophyll 3 Sturdy 4 Blaze,Solar Power 5 Blaze,Solar Power 6 Blaze,Solar Power 7 Sand Force,Sheer Force,Intimidate 8 Water Compaction,Sand Veil 9 Torrent,Rain Dish 10 Torrent,Rain Dish Name: Abilities, dtype: object", "q_apis": "get columns first first first loc dtype", "apis": "loc values", "code": ["first_item_list = [df.loc[0:10]['Abilities'].values[i].split(',')[0] for i in range(10)]"], "link": "https://stackoverflow.com/questions/64019677/how-do-i-extract-the-first-word-in-a-list-within-a-dataframe-object"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have Data Frame like below: And I would like to select only these rows where col1 >= 2015-01-01 and col1 is NaN, how can I do that? I tierd to use: and it does not work.", "q_apis": "get columns select where", "apis": "isna", "code": ["df[(df[\"col1\"] >= \"2015-01-01\") & (df[\"col2\"].isna())]\n", "         col1 col2\n0  2015-01-02  NaN\n"], "link": "https://stackoverflow.com/questions/68000399/how-to-select-columns-based-on-date-in-one-column-and-nan-in-second-column-in-py"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a nested multi-index dataframe with 3 levels and I want to select the 1st and 3rd level without selecting the 2nd level. Is it possible to do that? My current code looks like this (updated). now I am getting TypeError: unhashable type: 'slice' for panda which I am not sure why. 'D' and 'E' are the columns that I want to select from the 3rd level.", "q_apis": "get columns index levels select now columns select", "apis": "DataFrame index columns sort_index sort_index T copy", "code": ["def mklbl(prefix,n):\n    return [\"%s%s\" % (prefix,i)  for i in range(n)]\n\nmiindex = pd.MultiIndex.from_product([mklbl('A',4),\n                                        mklbl('B',2),\n                                      mklbl('C',4),\n                                       mklbl('D',2)])\n\nmicolumns = pd.MultiIndex.from_tuples([('a','foo'),('a','bar'),\n                                        ('b','foo'),('b','bah')],\n                                       names=['lvl0', 'lvl1'])\n\n\ndfmi = pd.DataFrame(np.arange(len(miindex)*len(micolumns)).reshape((len(miindex),len(micolumns))),\n                    index=miindex,\n                     columns=micolumns).sort_index().sort_index(axis=1)\n\n\ndfmiT = dfmi.T.copy()\n\ndfmiT\n", "          A0                                    ...    A3                      \\\n          B0                             B1     ...    B0        B1             \n          C0     C1      C2      C3      C0     ...    C3        C0        C1   \n          D0 D1  D0  D1  D0  D1  D0  D1  D0  D1 ...    D0   D1   D0   D1   D0   \nlvl0 lvl1                                       ...                             \na    bar   1  5   9  13  17  21  25  29  33  37 ...   217  221  225  229  233   \n     foo   0  4   8  12  16  20  24  28  32  36 ...   216  220  224  228  232   \nb    bah   3  7  11  15  19  23  27  31  35  39 ...   219  223  227  231  235   \n     foo   2  6  10  14  18  22  26  30  34  38 ...   218  222  226  230  234   \n", "            A3               \n            B0        B1     \n            C2        C2     \n            D0   D1   D0   D1\nlvl0 lvl1                    \na    bar   209  213  241  245\n     foo   208  212  240  244\nb    bah   211  215  243  247\n     foo   210  214  242  246\n"], "link": "https://stackoverflow.com/questions/52060869/how-to-skip-a-column-level-in-a-multi-index-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to check if some records in some (not all) of the columns in a dataframe are null; to do this, I want to create T/F fields, which I will then need to group by. E.g. if I have a field 'x' then I want to create a 'x POPULATED' field, and so on. In my context, null means NaN, the string 'not available', or the string 'nan'. I have tried the code below, but it doesn't work - I get: My questions are: What am I doing wrong? Is there a better way to vectorise this? Even if there is, and I'm pretty sure there is, I'd still like to understand what I was doing wrong in my code. Code:", "q_apis": "get columns all columns T get", "apis": "isnull isin join add_suffix apply", "code": ["df1 = ~(df[['a','b']].isnull() | (df[['a','b']].isin(['not available','nan'])))\n\nprint (df1)\n       a      b\n0  False  False\n1   True  False\n2   True  False\n3   True   True\n4   True   True\n5   True   True\n6   True   True\n7   True   True\n8   True   True\n9   True   True\n", "df = df.join(df1.add_suffix('populated'))\nprint (df)\n     a              b                     c  apopulated  bpopulated\n0  NaN            NaN                   NaN       False       False\n1  1.0  not available  nothing to test here        True       False\n2  2.0            nan  nothing to test here        True       False\n3  3.0           test  nothing to test here        True        True\n4  4.0           test  nothing to test here        True        True\n5  5.0           test  nothing to test here        True        True\n6  6.0           test  nothing to test here        True        True\n7  7.0           test  nothing to test here        True        True\n8  8.0           test  nothing to test here        True        True\n9  9.0           test  nothing to test here        True        True\n", "for c in ['a','b']:\n    df[c + 'populated'] =  df.apply( lambda x: checknull(x[c]) , axis=1 )\n\nprint (df)\n     a              b                     c  apopulated  bpopulated\n0  NaN            NaN                   NaN       False       False\n1  1.0  not available  nothing to test here        True       False\n2  2.0            nan  nothing to test here        True       False\n3  3.0           test  nothing to test here        True        True\n4  4.0           test  nothing to test here        True        True\n5  5.0           test  nothing to test here        True        True\n6  6.0           test  nothing to test here        True        True\n7  7.0           test  nothing to test here        True        True\n8  8.0           test  nothing to test here        True        True\n9  9.0           test  nothing to test here        True        True\n"], "link": "https://stackoverflow.com/questions/47201270/pandas-checking-for-nulls-what-did-i-do-wrong-applying-this-function-by-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "As you can see, I have made a dataframe called df and have got the first 7 rows with specific columns that I specified. Right now though, I want to get every other row. like rows 1,3,5,7 or 2,4,6. The code I have done so far is also in blockquote. Also I want to know if this is similar to every three rows and so on. Thanks for the help.", "q_apis": "get columns first columns now get", "apis": "index", "code": ["n = number of rows", "new_df = df[df.index % 3 != 0]"], "link": "https://stackoverflow.com/questions/62380424/how-do-i-find-pandas-rows-with-a-pattern"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following Pandas DataFrame in Python: It looks as the following when you output it: I need to add 3 new columns, as column \"d\", column \"e\", and column \"f\". Values in each new column will be determined based on the values of column \"b\" and column \"c\". In a given row: If the value of column \"b\" is bigger than the value of column \"c\", columns [d, e, f] will have the values [1, 0, 0]. If the value of column \"b\" is equal to the value of column \"c\", columns [d, e, f] will have the values [0, 1, 0]. If the value of column \"b\" is smaller than the value of column \"c\", columns [d, e, f] will have the values [0, 0, 1]. After this operation, the DataFrame needs to look as the following: My original DataFrame is much bigger than the one in this example. Is there a good way of doing this in Python without looping through the DataFrame?", "q_apis": "get columns DataFrame add columns values value value columns values value value columns values value value columns values DataFrame DataFrame DataFrame", "apis": "where where where", "code": ["df[\"d\"] = np.where(df.b >  df.c, 1, 0)\ndf[\"e\"] = np.where(df.b == df.c, 1, 0)\ndf[\"f\"] = np.where(df.b <  df.c, 1, 0)\n"], "link": "https://stackoverflow.com/questions/63403084/how-to-conditionally-add-one-hot-vector-to-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset which has feature 'abdomcirc' that has multiple values per ChildID, like so: I want to calculate the range of values for a given a list of abdomcirc values per child id. So I want to get these results: So I first tried this: But I got this error ValueError: 'range' is not a valid function name for transform(name) So, as suggested in the answer to this question, I tried the following line: But I got this error: AttributeError: 'DataFrame' object has no attribute 'High' Not sure why I am getting this error. Any suggestion on how to successfully calculate the range of a group of values in a dataframe?", "q_apis": "get columns values values values get first name transform name DataFrame values", "apis": "groupby apply max min", "code": ["df.groupby(\"ChildID\").apply(lambda x: x['abdomcirc'].max() - x['abdomcirc'].min())\n"], "link": "https://stackoverflow.com/questions/63284582/calculating-the-range-of-values-in-a-pandas-dataframe-using-groupby-function"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames as follows: I would like to compare the two data frames (by checking where they match in the id, start, end columns) and create a matrix of size 2 x (number of items) for each id. The cells should contain the label corresponding to an item. In this example: I tried looking at the pandas documentation but didn't really find anything that could help me.", "q_apis": "get columns compare where start columns size items item at", "apis": "merge filter T groupby", "code": ["m = df1.merge(df2, on=['id', 'start', 'end'])\ndct = {f'M_id{k}': g.filter(like='label').to_numpy().T for k, g in m.groupby('id')}\n", ">>> dct['M_id1']\narray([['food', 'drink'], ['food', 'food']], dtype=object)\n\n>>> dct['M_id2']\narray([['food'], ['drink']], dtype=object)\n"], "link": "https://stackoverflow.com/questions/66063711/how-to-compare-two-dataframes-and-return-a-matrix-containing-values-where-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Im using jupyter notebooks, my current dataframe looks like the following: How can I group all players individually and average their polarity? Currently I have tried to use: But this will return a dataframe grouping all the mentions when together as well as separate, how best can I go about splitting the players up and then grouping them back together. An expected output would contain In other words how to group by each item in the lists in every row.", "q_apis": "get columns all all item", "apis": "explode index explode DataFrame values explode index join drop explode", "code": ["def unnesting(df, explode):\n    idx = df.index.repeat(df[explode[0]].str.len())\n    df1 = pd.concat([\n        pd.DataFrame({x: np.concatenate(df[x].values)}) for x in explode], axis=1)\n    df1.index = idx\n\n    return df1.join(df.drop(explode, 1), how='left')\n", "(unnesting(df, ['players_mentioned'])\n    .groupby('players_mentioned', as_index=False).mean())\n\n  players_mentioned  polarity\n0              Mane      0.12\n1             Salah      0.09\n"], "link": "https://stackoverflow.com/questions/55462635/pandas-dataframe-group-by-column-with-a-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe df which looks like this: Data i want to count the changes in the list, so we will compare within the list as well and with the previous ID list, my output should be like this: Output (A,B) there are 1 change from A to B. (B,C,D) B is already present as last element in the previous id, so there will be 2 changes from B to c and c to D. (E,F,G) since E is not the last element of previous list so there will be three changes from previous D to current E, E to F and F to G. for Id 4 there will be 0 changes as G is present in the last list as well. How can I create a the output column counting these changes.", "q_apis": "get columns count compare last last last", "apis": "shift fillna iterrows at", "code": ["df['Input-1'] = df['Input'].shift(1).fillna('A' )\ndf['Output'] = 0\n", "    Input Input-1  Output\nId\n1     A,B       A       0\n2   B,C,D     A,B       0\n3   E,F,G   B,C,D       0\n4       G   E,F,G       0\n", "for id_, row in df.iterrows():\n   df.at[id_, 'Output'] = len(set(row['Input-1'].split(',')[-1:]+row['Input'].split(',')))-1\n", "    Input Input-1  Output\nId\n1     A,B       A       1\n2   B,C,D     A,B       2\n3   E,F,G   B,C,D       3\n4       G   E,F,G       0\n"], "link": "https://stackoverflow.com/questions/58863472/count-changes-in-a-string-from-the-previous-row-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "For this question, let's take the following example. I have a dataframe which looks as follows (): As you can see the groups are by the column. I know that pandas has a , but what I wish to do is to group the interval rows, such that the column values are listed together. This would give an example output as follows: As you can see the desired end result is to have the columns grouped into a list for the interval groups, and the packet time is combined such that the value is for each interval group.", "q_apis": "get columns take groups values columns groups time value", "apis": "T apply columns groupby apply drop columns groupby apply columns", "code": ["PacketTime     [0.056078, 0.056106, 2.058089, 2.058115, 4.060...\nFrameLen                       [116.0, 66.0, 116.0, 66.0, 116.0]\nFrameCapLen                    [116.0, 66.0, 116.0, 66.0, 116.0]\nUnnamed: 3                             [nan, nan, nan, nan, nan]\nSpeed                             [25.0, 25.0, 25.0, 25.0, 25.0]\nDelay                                  [0.0, 0.0, 0.0, 0.0, 0.0]\nLoss                                   [0.0, 0.0, 0.0, 0.0, 0.0]\nInterval                               [0.0, 0.0, 2.0, 2.0, 4.0]\n", "\nimport pandas as pd\n\ndf = pd.read_excel('example.xlsx')\n\n\ndef to_list(df):\n    return df.T.apply(lambda x: x.to_list(), axis='columns')\n\n\ndf_other = df.groupby('Interval')\\\n            .apply(to_list)\\\n            .drop(columns='PacketTime')\n", "s_Interval = df.groupby('Interval')['PacketTime']\\\n            .apply(min_max)\nfinal_df = pd.concat([df_other,s_Interval], axis= 'columns')\n\n", "\nprint(final_df.to_markdown())\n|   Interval | FrameLen      | FrameCapLen   | Unnamed: 3   | Speed        | Delay      | Loss       | Interval   |   PacketTime |\n|-----------:|:--------------|:--------------|:-------------|:-------------|:-----------|:-----------|:-----------|-------------:|\n|          0 | [116.0, 66.0] | [116.0, 66.0] | [nan, nan]   | [25.0, 25.0] | [0.0, 0.0] | [0.0, 0.0] | [0.0, 0.0] |      2.8e-05 |\n|          2 | [116.0, 66.0] | [116.0, 66.0] | [nan, nan]   | [25.0, 25.0] | [0.0, 0.0] | [0.0, 0.0] | [2.0, 2.0] |      2.6e-05 |\n|          4 | [116.0]       | [116.0]       | [nan]        | [25.0]       | [0.0]      | [0.0]      | [4.0]      |      0       |\n\n\n\n\n"], "link": "https://stackoverflow.com/questions/62860340/convert-column-values-for-a-group-of-data-frame-rows-into-a-list-in-the-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm still learning to use numpy and pandas I've the following dataframe : arrival_time : Time at which the item is arriving (More than one item can arrive at the same time) id_col :unique id for an item. id assignment isn't related to arrival_time. col1,col2 : Other columns in the dataframe df : Now I'm using this code to get arrival_time_df i.e., df rows having same arrival_time Is it possible to improve this using pandas .apply() or .map() functions. The expected dictionary looks like this : Thanks in advance.", "q_apis": "get columns at item item at time unique item columns get apply map", "apis": "groupby", "code": ["{1:    arrival_time  id_col  col1 col2\n0             1       4     1    a\n1             1      10     7    r\n2             1       5     4    d, \n2:    arrival_time  id_col  col1 col2\n6             2       2    89    e\n9             2       9    30    d, \n3:    arrival_time  id_col  col1 col2\n3             3       1     6    t\n4             3       7     8    d, \n4:    arrival_time  id_col  col1 col2\n7             4       3     9    a, \n5:    arrival_time  id_col  col1 col2\n5             5       8     6    k, \n6:    arrival_time  id_col  col1 col2\n8             6       6    10    r}\n", "def m1(df):\n  time_unique = df.arrival_time.unique()\n  arrival_dict = dict()\n  for t in time_unique :\n    arrival_dict[t] = df[df.arrival_time == t]\n  return arrival_dict\n\ndef m2(df):\n  return dict(iter(df.groupby(df['arrival_time'])))\n\nin_ = [pd.concat([df]*n) for n in [1,10,100,1000]]\n"], "link": "https://stackoverflow.com/questions/63628402/improving-the-efficiency-of-code-by-replacing-for-loop-with-pandas-operations"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have some code which is working at bring back the max record for each id based on date, what I would like to do is have a new column in data2 called flag and have true if it meets this criteria.", "q_apis": "get columns at max date", "apis": "groupby transform max eq duplicated", "code": ["data2['new'] = data2.groupby('id').Date.transform('max').eq(data2.Date) & ~data2.Date.duplicated()\n"], "link": "https://stackoverflow.com/questions/66671230/flagging-records-that-meet-a-loc-groupby-query"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have got a dataframe like: How can I only select rows from 2020-01-13 to 2020-02-14 using a filter? I'm aware there are more questions like these but this dataframe doesn't have a name for the date column (e.g. the date is the index). Thank you!", "q_apis": "get columns select filter name date date index", "apis": "loc index index", "code": ["df.loc[(df.index >= '2020-01-13') & (df.index <= '2020-02-14')]\n"], "link": "https://stackoverflow.com/questions/66410368/select-rows-based-on-date-in-a-pandas-dataframe-in-python-date-is-not-a-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "supposed my dataset I want to add each person's first and last dates to a new column.. result what I want Is there a method to make this possible? I would appreciate it if you let me know. thank you for reading", "q_apis": "get columns add first last", "apis": "groupby agg first last join", "code": ["g = df.groupby('Name')['date']\ndf = df.assign(startdate = g.transform('first'), enddate = g.transform('last'))\nprint (df)\n  Name      date startdate   enddate\n0    A  15-01-01  15-01-01  16-03-04\n1    A  15-01-31  15-01-01  16-03-04\n2    A  16-02-02  15-01-01  16-03-04\n3    A  16-03-04  15-01-01  16-03-04\n4    B  17-04-05  17-04-05  17-07-09\n5    B  17-05-08  17-04-05  17-07-09\n6    B  17-07-09  17-04-05  17-07-09\n7    C  18-01-02  18-01-02  18-02-03\n8    C  18-02-03  18-01-02  18-02-03\n", "df['date'] = pd.to_datetime(df['date'], format='%y-%m-%d')\ng = df.groupby('Name')['date']\ndf = df.assign(startdate = g.transform('min'), enddate = g.transform('max'))\nprint (df)\n  Name       date  startdate    enddate\n0    A 2015-01-01 2015-01-01 2016-03-04\n1    A 2015-01-31 2015-01-01 2016-03-04\n2    A 2016-02-02 2015-01-01 2016-03-04\n3    A 2016-03-04 2015-01-01 2016-03-04\n4    B 2017-04-05 2017-04-05 2017-07-09\n5    B 2017-05-08 2017-04-05 2017-07-09\n6    B 2017-07-09 2017-04-05 2017-07-09\n7    C 2018-01-02 2018-01-02 2018-02-03\n8    C 2018-02-03 2018-01-02 2018-02-03\n", "df1 = df.groupby('Name').agg(startdate = ('date','first'), lastdate = ('date','last'))\ndf = df.join(df1, on='Name')\n"], "link": "https://stackoverflow.com/questions/62128244/create-column-of-start-date-and-end-date-from-given-date-record"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm quite new to using Pandas. I was tasked with making some changes to an existing script with zero documentation. I'm having a hard time deciphering what is happening in the second line: products is a pandas dataframe filled with some info form a CSV file. I understand that merge is used as kind of a SQL Join, but from what I've investigated, the structure of the merge statement (and the whole line actually) seems strange. I understand the on and how parameters, but I'm not sure what lookuptable is merging with, and what \"[['height', 'width', 'depth']].astype(int)\" does.", "q_apis": "get columns time second info merge merge astype", "apis": "size astype merge", "code": ["target_cols = ['height', 'width', 'depth']\nmerge_cols = ['unit', 'category', 'size']\n\nlookuptable[trg_cols] = lookuptable[trg_cols].astype(int)\n\nproducts = pd.merge(\n              products,\n              lookuptable[target_cols + merge_cols],\n            on = merge_cols,\n            how = 'left')\n              \n           \n"], "link": "https://stackoverflow.com/questions/66657913/pandas-dataframe-merge-statement-question"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a webscraped Twitter DataFrame that includes user location. The location variable looks like this: I would like to construct state dummies for all USA states by using a loop. I have managed to extract users from the USA using However the code would be too bulky I wrote this for every single state. I have a list of the states as strings. Also I am unable to use as there are different locations within the same state and each entry is a whole sentence. I would like the output to look something like this: Or the same with Boolean values.", "q_apis": "get columns DataFrame all values", "apis": "join sum clip join", "code": ["import pandas as pd\n\nstates = ['Texas', 'New York', 'Kentucky', 'Virginia']\npd.get_dummies(df.col1.str.extract('(' + '|'.join(x+',' for x in states)+ ')')[0].str.strip(','))\n\n   Kentucky  New York  Texas  Virginia\n0         0         0      1         0\n1         0         1      0         0\n2         0         0      0         0\n3         0         0      1         0\n4         0         0      0         1\n5         1         0      0         0\n", "pd.get_dummies(df.col1.str.extractall('(' + '|'.join(x+',' for x in states)+ ')')[0].str.strip(',')).sum(level=0).clip(upper=1)\n", "'State,( optional 5 digit Zip,) USA'", "states = ['Texas', 'New York', 'Kentucky', 'Virginia', 'California', 'Pennsylvania']\npat = '(' + '|'.join(x+',?(\\s\\d{5},)?\\sUSA' for x in states)+ ')'\n\ns = df.col1.str.extract(pat)[0].str.split(',').str[0]\n", "0           Texas\n1        New York\n2             NaN\n3           Texas\n4        Virginia\n5        Kentucky\n6    Pennsylvania\nName: 0, dtype: object\n", "                                          col1\n0  Crockett, Houston County, Texas, 75835, USA\n1                           NYC, New York, USA\n2                    Warszawa, mazowieckie, RP\n3                                   Texas, USA\n4         Virginia Beach, Virginia, 23451, USA\n5  Louisville, Jefferson County, Kentucky, USA\n6                California, Pennsylvania, USA\n"], "link": "https://stackoverflow.com/questions/54537792/constructing-a-dataframe-with-multiple-columns-based-on-str-conditions-using-a-l"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a df with and with present and absent as values. I'm trying to put status - for each name. If one of the has their status will be False Expected Output:", "q_apis": "get columns values put name", "apis": "all min eq groupby min bool", "code": ["# or `all()` instead of `min()`\ndf['status'].eq('present').groupby(df['name']).min()\n", "name\nanthony    False\ndonny      False\nmason       True\npaul        True\nName: status, dtype: bool\n"], "link": "https://stackoverflow.com/questions/64580205/python-dataframe-assign-values-to-a-column-based-on-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In the following data: Problem: I want to compare the ranges (start - end) from these two data frames. If the ranges overlap I want to transfer the and values from data02 to to a new column in the data01. I tried (using pandas): How can I improve this code? I am currently sticking to pandas, but if the problem is better addressed using dictionary I am open to it. But, please explain the process, I am open to learning rather than just getting an answer. Thanks, Desired output:", "q_apis": "get columns compare start values", "apis": "values values values last values values values where index fillna groupby agg join rename_axis replace assign", "code": ["s1 = data01.start.values\ne1 = data01.end.values\ns2 = data02.start.values\ne2 = data02['last'].values\n\noverlap = (\n    (s1[:, None] <= s2) & (e1[:, None] >= s2)\n) | (\n    (s1[:, None] <= e2) & (e1[:, None] >= e2)\n)\n\ng = data02.gene_id.values\nn = data02.gene_name.values\n\ni, j = np.where(overlap)\nidx_map = {i_: data01.index[i_] for i_ in pd.unique(i)}\n\ndef make_series(m):\n    s = pd.Series(m[j]).fillna('').groupby(i).agg(','.join)\n    return s.rename_axis(idx_map).replace('', np.nan)\n\ndata01.assign(\n    gene_id=make_series(g),\n    gene_name=make_series(n),\n)\n"], "link": "https://stackoverflow.com/questions/43475370/how-to-merge-two-pandas-dataframes-or-transfer-values-by-comparing-ranges-of-v"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The problem is I'm trying to generate a line graph using seaborn.lineplot() function, but I can't seem to find a way to generate a line graph like the one below: https://i.stack.imgur.com/zUKog.png My dataset has the following columns: Year, Month, Day, Units, Price per Unit, Sales. I've used the groupby function from pandas to sum the sales for each year in each month. Initially, I thought that using the code would automatically generate my desired graph, but it generated a confusing graph instead. Anyone who has a solution for me? even if it isn't by using seaborn but matplotlib.", "q_apis": "get columns stack columns groupby sum year month", "apis": "astype", "code": ["each_month['year'] = 'Y' + each_month['year'].astype(str)\nplt.figure(figsize=(10,6))\nsns.lineplot(x='month', y='sales', hue='year', data=each_month)\n"], "link": "https://stackoverflow.com/questions/65300109/generating-a-line-graph-using-seaborn-or-matplotlib-with-year-as-hue-month-as"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this following dataframe However, I want the dates of my dataframe to always range from 2013-02-04 8:00:00 to 2013-02-15 17:00:00 in windows of 5 minutes. The missing windows will always have 'doctets' 0 and 'Duration' 0. My desired output would look something like this: In other words, I want to fill the dataframe with the missing dates from my desired date range", "q_apis": "get columns date", "apis": "resample asfreq reindex min fillna astype", "code": ["df = df.resample('5min').asfreq().reindex(pd.date_range('2013-02-04 8:00:00', '2013-02-15 17:00:00', freq='5 min')).fillna(0).astype(int)\n", "df = df.merge(\n    pd.date_range(start='2013-02-04 8:00:00',\n                  end='2013-02-15 17:00:00',\n                  freq='5min',name='Real First Packet').to_frame()\\\n    .set_index('Real First Packet'),\n    left_index=True,\n    right_index=True,\n    how='right'\n ).fillna(0).astype(int)\n", "                    doctets Duration\nRealFirstPacket     \n2013-02-15 16:30:00 47255   6656\n2013-02-15 16:35:00 0   0\n2013-02-15 16:40:00 0   0\n2013-02-15 16:45:00 0   0\n2013-02-15 16:50:00 0   0\n2013-02-15 16:55:00 0   0\n2013-02-15 17:00:00 0   0\n"], "link": "https://stackoverflow.com/questions/67258867/adding-missing-windows-of-time-to-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a merged dataset-which now i need to filter. The merged dataset looks like this I need to filter this and get the row where color and color_y doesnt match, If there is atleast one row where the colors match then the whole row shouldnt be returned. In above example, none of the Jasmine rows should be returned, as 1 row matches between color and color_y. But Lily row has to be returned, Result dataframe should look like this. How do I achieve this ? Thank you !!", "q_apis": "get columns now filter filter get where where between", "apis": "groupby apply eq any head reset_index drop filter bool bool", "code": ["df = df.groupby('Flower').apply(\n    lambda g: None if g['Color'].eq(g['Color_y']).any() else g.head(1)\n).reset_index(drop=True).filter(regex='.*(?<!_y)$')\n", "  Flower      Id        city   Color\n0   Lily  2457MH  Washington  Purple\n", "    Flower      Id   city  Color Flower_y City_y Color_y\n0  Jasmine  1023LD  Hawai  White  Jasmine  Hawai   White\n1  Jasmine  1023LD  Hawai  White  Jasmine  Hawai  Yellow\n2  Jasmine  1023LD  Hawai  White  Jasmine  Hawai  Orange\n", "0     True\n1    False\n2    False\ndtype: bool\n", "0     True\n1    False\n2     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/67372499/how-to-filter-records-from-a-merged-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm using python3. I have a dictionary and a separate dictionary containing a descriptive string I am later joining simulations and condition_old to get a complete string. This results in output = I am then plotting data for each condition (e.g. slow). What I want to be able to do is to increase the number of values in the conditions e.g.: and return a dictionary for each: condition01 = condition02 = condition03 = What also needs to be considered is that the number of values in condition_new can vary, so I can't explicitly state 3 dictionary names to populate. Maybe a dictionary within a dictionary would be sufficient. In the end I want to create 3 separate plots based on condition01 condition02 condition03. Thanks", "q_apis": "get columns get values values names", "apis": "items", "code": ["simulations = {\n    'wk01' : '183',\n    'wk02' : '170',\n    'wk03' : '184',\n}\n\nconditions = {'slow', 'med', 'fast'}\n\nthedicts = dict()\nfor cond in conditions:\n    thedicts[cond] = {k: f'{d}-{cond}' for k, d in simulations.items()}\n"], "link": "https://stackoverflow.com/questions/65309273/python-dictionary-of-dictionaries"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to compare two different DataFrames to check the new_df symbol does it exists in old_df, if it doesnt exist in the old_df, I want to output to a list. Code looks like this: I want the output like this:", "q_apis": "get columns compare", "apis": "DataFrame DataFrame isin values values values", "code": ["old_df = pd.DataFrame({'symbol': ['A', 'B', 'C', 'D', 'E']})\nnew_df = pd.DataFrame({'symbol': ['C', 'A', 'B', 'F', 'H']})\n\nnew_df[~np.isin(new_df.values,old_df.values)].values\n"], "link": "https://stackoverflow.com/questions/66070238/is-there-a-function-to-compare-two-dataframes-and-output-the-different-elements"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with 10 columns and 160 rows. Column names are based on month and year for e.g Jun'17, July'17, Mar'18 etc. However in excel some columns are repeating like Jun'17 two times When I import them to pandas dataframe it renames duplicate columns to Jun'17 and Jun'17.1 This '.1' is extra and disturbing my whole calculation.", "q_apis": "get columns columns names month year columns columns", "apis": "rename columns", "code": ["df = df.rename(columns = {\"Jun'17.1\":\"Jun'17\"})\n"], "link": "https://stackoverflow.com/questions/53188883/pandas-dataframe-automatically-renames-duplicate-columns-name"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This is the sample dataset I'm converting this to json using to_json and i get this output What i'm trying to do is to get json a list of dict like this my code is here", "q_apis": "get columns sample to_json get get", "apis": "stack unstack reset_index to_json", "code": ["i = df.T.reset_index()\ni\n\n     datetime    a    b    c\n0  2017-12-24  1.5  0.0  1.5\n1  2017-12-31  0.5  1.5  0.0\n\ni.to_json()\n'[{\"datetime\":\"2017-12-24\",\"a\":1.5,\"b\":0.0,\"c\":1.5},{\"datetime\":\"2017-12-31\",\"a\":0.5,\"b\":1.5,\"c\":0.0}]'\n", "(df.stack()\n   .unstack(0)\n   .reset_index()\n   .to_json(orient='records'))\n\n'[{\"datetime\":\"2017-12-24\",\"a\":1.5,\"b\":0.0,\"c\":1.5},{\"datetime\":\"2017-12-31\",\"a\":0.5,\"b\":1.5,\"c\":0.0}]'\n"], "link": "https://stackoverflow.com/questions/48012197/pandas-dataframe-to-json-list-of-dict"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with a column ORDER_DATE. I'm trying to add a new column for FISCAL_YEAR which essentially has this type of criteria: If between 7/1/16 and 6/30/17 = FY2017 If between 7/1/17 and 6/30/18 = FY2018 The only way I could think to do it is a series of conditional statements, which I am certain is not the most efficient or elegant. While my solution works as intended, it takes forever and I'm hoping for suggestions on how to make it better.", "q_apis": "get columns add between between", "apis": "DataFrame assign", "code": ["df = pd.DataFrame({'date': pd.DatetimeIndex(start='2015-01', end='2016-12', freq='D')})\ndf = df.assign(fiscal_year=pd.PeriodIndex(df.date, freq='A-Jun'))\n"], "link": "https://stackoverflow.com/questions/52377432/add-category-column-based-on-date-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame with customers and categories they shop in, and I want to know if a customer is a cross-category shopper (there are only three categories). What is the best way to turn this: to this:", "q_apis": "get columns DataFrame categories categories", "apis": "bool pivot index columns values fillna nunique groupby transform size drop_duplicates set_index", "code": ["Customer\nboo    False\nfoo     True\nzoo    False\nName: Category, dtype: bool\n", "p = df.pivot(index='Customer', columns='Category', values='visited').fillna('N')\n\nCategory  A  B  C\nCustomer         \nboo       Y  N  N\nfoo       Y  Y  N\nzoo       N  N  Y\n", "# alternative approach to the nunique part from above\ndf['CrossCategory'] = df.groupby('Customer')['Category'].transform('size') > 1\n\n  Customer Category visited  CrossCategory\n0      foo        A       Y           True\n1      foo        B       Y           True\n2      boo        A       Y          False\n3      zoo        C       Y          False\n", "pd.concat([p, df[['Customer', 'CrossCategory']].drop_duplicates().set_index('Customer')], sort=False, axis=1)\n", "     A  B  C  CrossCategory\nboo  Y  N  N          False\nfoo  Y  Y  N           True\nzoo  N  N  Y          False\n"], "link": "https://stackoverflow.com/questions/53027105/create-a-column-based-on-values-of-multiple-rows-tied-to-a-key"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with more than 600 columns. I have given a sample dataframe with few columns here I would like to fetch the unique values from each column and output it in another dataframe These are the two approaches that I tried Is there anyway to do this at one go without loop? Please note that I expect to have unique values from each column and not unique row in dataframe As my data is over million records and columns are more than 600 any suggestions/solutions to improve would be helpful", "q_apis": "get columns columns sample columns unique values at unique values unique columns any", "apis": "drop_duplicates items", "code": ["print({k:v.drop_duplicates().tolist() for k,v in df_new.items()})\n", "{'bud_source_value': [1250000.0, 250000.0, nan], 'date2': ['12/31/2017', nan, '10/06/2015'], 'date3': ['12/31/2027', '11/25/2029', nan], 'hero_id': [2.0, 4.0, nan], 'heroine_id': [1.0, nan, 5.0], 'obs_date': ['12/31/2007', '11/25/2009', nan], 'person_id': [1, 2, 3], 'prod__source_value': [10000.0, 20000.0, nan]}\n"], "link": "https://stackoverflow.com/questions/57386914/efficient-and-elegant-way-to-fetch-unique-values-from-all-columns-big-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes (see here), which contain dates and times. The details for the first data frame are: The details for the second data frame are: I am trying to convert the times to a DateTime object so that I can then do a calculation to make the time relative to the first time instance (i.e. the earliest time would become 0, and then all others would be seconds after the start). When I try (from here): I get this error: When I try (from here): but it gives me this error: Any suggestions would be appreciated.", "q_apis": "get columns first second time first time time all seconds start get", "apis": "apply", "code": ["df['Date Time'] = df['Date'] + ' ' + df['Time']   \n", "df['Date Time'] = pd.to_datetime(df['Date Time'], format='%d/%m/%Y %H:%M:%S')\n", "df['Date Time'] = df['Date Time'].apply(pd.Timestamp)\n"], "link": "https://stackoverflow.com/questions/54461293/trying-to-convert-object-to-datetime-getting-typeerror"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that looks like this: and would like to split the original column into multiple columns, but group them in groups of 5, like this: I tried doing something like this, but I'm pretty sure there's a better way to do it... Anyone have any suggestions? Maybe some sort of a loop that divides by the max number or items in the original cell? Please and thank you!", "q_apis": "get columns columns groups any max items", "apis": "fillna add columns join groupby sum stack unstack rename columns add_prefix", "code": ["df3 = df1['Original'].str.split(',', expand=True).fillna('').add(',')\nlen_cols = len(df3.columns)\ndf1[['Name']].join(df3.groupby(np.arange(len_cols) // 5, axis=1)\n                      .sum()\n                      .stack()\n                      .str.rstrip(',')\n                      .unstack()\n                      .rename(columns=dict(zip(range(len_cols),\n                                               range(1, len_cols + 1))))\n                      .add_prefix('Group '))\n", "    Name        Group 1         Group 2 Group 3\n0  Row 1  A, B, C, D, E                        \n1  Row 2        A, B, C                        \n2  Row 3  A, B, C, D, E   F, G, H, I, J    K, L\n3  Row 4              A                        \n"], "link": "https://stackoverflow.com/questions/60765920/pandas-split-column-by-number-of-items-in-cell"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe read in from Excel, as below. The column labels are dates. Given today's date is 2020-04-13, I want to retrieve the C row values for next 7 days. Currently, I set the index and retrieve the values of C, when I print rows I get the output for all the dates and their values of C. I know that I should use . Can someone let me know how to capture the column of today's date () for the C row? I am beginner to python/pandas and am just learning the concepts of dataframes.", "q_apis": "get columns today date values days index values get all values today date", "apis": "columns all where equals", "code": ["import pandas as pd\nimport datetime\n\n\nINPUT_PATH = \"C:/temp/data.xlsx\"\n\npd_xls_obj = pd.ExcelFile(INPUT_PATH)\n\ndata = pd.read_excel(pd_xls_obj, sheet_name=\"Sheet1\")\n\ncolumn_headers = data.columns\n\n# loop though headers and check if todays date is equal to the column header\nfor column_header in column_headers:\n    # python would throw you an Attribute Error for all Headers which are not\n    # in the format datetime. In order to avoid that, we use a try - except\n    try:\n        # if the dates are equal, print an output\n        if column_header.date() == datetime.date.today():\n            # you can read the statement which puts together your result as\n            # follows:\n            # data[header_i_am_interested_in][where: data['Names'] equals 'C']\n            result = data[column_header][data['Names'] == 'C']\n            print(result)\n    except AttributeError:\n        pass\n"], "link": "https://stackoverflow.com/questions/61315759/retrieve-a-column-name-that-has-today-date-with-respect-to-the-specific-row-in-d"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe, which contains tool id and time. For the last date I have tool counter values, and I need to fill up missing counter values in the dataframe by substracting 1 from the counter for each time the id has been used at a particular date. Dataframe looks like this: Desired outcome would be like this: Please help me how can I do this. Thanks!", "q_apis": "get columns contains time last date values values time at date", "apis": "groupby ffill", "code": ["g = df_sort.groupby('id')\ndf_sort['counter'] = g['counter'].ffill() + g.cumcount()\n", "print(df_sort)\n\n    id  counter        date\n4   05    500.0  2020-02-04\n11  05    501.0  2020-02-03\n13  05    502.0  2020-02-03\n8   05    503.0  2020-02-02\n3   04    400.0  2020-02-04\n14  04    401.0  2020-02-03\n9   04    402.0  2020-02-02\n2   03    300.0  2020-02-04\n10  03    301.0  2020-02-03\n7   03    302.0  2020-02-02\n1   02    200.0  2020-02-04\n5   02    201.0  2020-02-02\n6   02    202.0  2020-02-02\n0   01    100.0  2020-02-04\n12  01    101.0  2020-02-03\n"], "link": "https://stackoverflow.com/questions/65157759/filling-missing-values-with-increasing-values-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two data frames with same column names but some columns may have different datatypes. How do i copy the {col:datatype} from reference dataframe and apply to the main dataframe", "q_apis": "get columns names columns copy apply", "apis": "columns astype dtypes", "code": ["for col in df2.columns:\n    try:\n        df2[col] = df2[col].astype(dtypes[col])\n    except (KeyError, ValueError):\n        pass\n"], "link": "https://stackoverflow.com/questions/66069899/how-do-i-copy-the-coldatatype-from-reference-dataframe-and-apply-to-the-main"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am reading a csv file as pandas dataframe like so: Here, the number of columns is not fixed but I want to read the first 80 columns only, that's why I am using . Now for files which have more than 80 columns this works fine, but for files having less than 80 columns, it's throwing this kind of error: Usecols do not match columns, columns expected but not found: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79] How can I fix it? Ideally, I would like to read first 80 columns (the last few columns will simply contain NaNs).", "q_apis": "get columns columns first columns columns columns columns columns first columns last columns", "apis": "iloc", "code": ["num_cols = 80\ndf = pd.read_csv(read_path, compression='zip', header=None, sep=',', names=range(100), low_memory=False)\ndf = df.iloc[: , :num_cols]\n"], "link": "https://stackoverflow.com/questions/67567640/pandas-read-specific-number-of-columns-irrespective-of-data-file"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with X rows and X columns. I would like to display only a part of the table which is sorted (case-insensitive). I.e., if I extract only a particular column with their count based on the unique rows say I get an output that looks like the following: But, I would like my output to be sorted case-insensitively. Like the following: How do I do that?", "q_apis": "get columns columns count unique get", "apis": "sort_index", "code": ["s = df.ROI.values_count()\n\ns = s.sort_index(key=lambda x: x.str.lower())\nprint (s)\nABDOMINAL CAVITY    48445\nAbdominal cavity    18282\nAccessory sinus     12088\nAdipose tissue       3048\nADRENAL GLAND       18889\nAdrenal gland           5\nName: ROI, dtype: int64\n"], "link": "https://stackoverflow.com/questions/66531033/case-insensitive-sorting-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "When comparing two dataframes and putting the result back into a dataframe: or in human readable form: or in human readable form: pandas outputs a dataframe with true or false values. Is it possible to change the name from 'true' or 'false' to 'lower' or 'higher'? I want to know if the outcome is higher, lower or equal, that is why I ask. If the output is not higher or lower, (true or false) then it must be equal.", "q_apis": "get columns values name", "apis": "DataFrame columns", "code": ["pd.DataFrame(np.where(dfA > dfB, 'higher', 'lower'), columns=['col'])\n\n      col\n0   lower\n1   lower\n2   lower\n3  higher\n", "m1 = dfA > dfB\nm2 = dfA < dfB  \npd.DataFrame(\n    np.where(m1, 'higher', np.where(m2, 'lower', 'equal')),\n    columns=['col']\n)\n", "pd.DataFrame(\n    np.select([m1, m2], ['higher', 'lower'], default='equal'),\n    columns=['col']\n)\n", "      col\n0   equal\n1   equal\n2   lower\n3  higher\n"], "link": "https://stackoverflow.com/questions/49922506/switch-boolean-output-to-string-in-pandas-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "original df desired output what had i tried, but this seem 'weird', by trying to remove punctuation one by one, then only convert to row is there a better way to convert the list to desired row and column? Thanks", "q_apis": "get columns", "apis": "pop join DataFrame index index", "code": ["L = [dict(y) for y in df.pop('description')]\ndf = df.join(pd.DataFrame(L, index=df.index))\nprint (df)\n   fruit      taste sweetness\n0  apple      sweet         5\n1  lemon       sour         0\n"], "link": "https://stackoverflow.com/questions/61726003/how-to-make-a-convert-set-in-list-to-row"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that looks like this: I've been trying to flip the signs on the column but only if is greater than This is supposed to be the end result:", "q_apis": "get columns", "apis": "loc", "code": ["df.loc[df['shortQuantity'] < df['longQuantity'],'averagePrice'] = df['averagePrice'] * -1\n"], "link": "https://stackoverflow.com/questions/62877547/pandas-flipping-sign-on-certain-rows-of-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame which has a following structure I want to convert above data frame into a list of tuples<String, List of List>, For example,", "q_apis": "get columns", "apis": "groupby columns apply to_dict", "code": ["(df.groupby('title')[df.columns[1:]]\n   .apply(lambda x: [tuple(y) for y in x.to_numpy()])\n   .to_dict())\n", "{'title1': [('value11', 'value12', 'value13', 'value14', 'value15'),\n  ('value21', 'value22', 'value23', 'value24', 'value25'),\n  ('value31', 'value32', 'value33', 'value34', 'value35')],\n 'title2': [('value1_1', 'value1_2', 'value1_3', 'value1_4', 'value1_5'),\n  ('value2_1', 'value2_2', 'value2_3', 'value2_4', 'value2_5'),\n  ('value3_1', 'value3_2', 'value3_3', 'value3_4', 'value3_5')]}\n"], "link": "https://stackoverflow.com/questions/67059690/how-to-convert-pandas-data-frame-into-list-of-tuplesstring-list-of-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "EDIT I had this dataframe: I had make a boolean indexing to create a new dataframe as the docs says. To check the difference in the length of the dataframe I make and the ouput is .That means that the boolean indexing gone right, doesn't it? But then, it raise when I'm traing to print True or False based on some conditions. This is the code: The output is If I make The output is I can't realized what's happening. Any suggest?", "q_apis": "get columns difference length right", "apis": "loc", "code": ["df2 = df.loc[df.a != 'some_string', :]\n"], "link": "https://stackoverflow.com/questions/63115451/error-using-a-dataframe-created-by-a-boolean-indexing"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to unroll a list within a column to add more rows for the purpose of feeding it into a swarmplot. Right now, I build a dictionary of lists: This dictionary is say 5 keys, each with a list of 500 floats. When I next create a dataframe: The result would look like: My question is how can I merge the columns 0-499, so that the new dataframe would be 2500 rows x 2 column with the id-column, and the numerical column. Other attempts: tried different ways of creating a dataframe from list of lists looked at merge/join, but this in general seemed to be geared towards \"combining\" separate dataframes and adding columns", "q_apis": "get columns add now keys merge columns at merge join columns", "apis": "DataFrame size DataFrame T reset_index rename columns index melt DataFrame melt drop DataFrame drop columns first sort_values reindex reset_index drop", "code": ["import numpy as np\nimport pandas as pd\n\n# recreate DataFrame from example\nclf_aucs = dict()\nfor id_ in range(5):\n    clf_aucs[f\"clf{id_}\"] = np.random.uniform(size=(500, ))\nclf_aucs_df = pd.DataFrame(clf_aucs).T.reset_index().rename(\n    columns={\"index\": \"ID\"})\n\n# melt DataFrame\nclf_aucs_df = pd.melt(clf_aucs_df, id_vars=\"ID\", value_name=\"Numerical_Column\")\n\n# drop what were the column names prior to reshaping the DataFrame\nclf_aucs_df.drop(columns=\"variable\", inplace=True)\n\n# sort first on ID and then on Numerical_Column\nclf_aucs_df.sort_values([\"ID\", \"Numerical_Column\"], inplace=True)\n\n# reindex from 0\nclf_aucs_df.reset_index(drop=True, inplace=True)\n", "     ID         0         1         2  ...       496       497       498       499\n0  clf0  0.647251  0.976586  0.675573  ...  0.911264  0.983211  0.685464  0.519285\n1  clf1  0.034560  0.340834  0.443456  ...  0.412356  0.968721  0.833882  0.634775\n2  clf2  0.723530  0.087285  0.014977  ...  0.563904  0.962543  0.860245  0.679423\n3  clf3  0.863781  0.609096  0.214915  ...  0.382548  0.798677  0.196336  0.673109\n4  clf4  0.185867  0.006018  0.635887  ...  0.622308  0.802546  0.771671  0.536761\n", "        ID  Numerical_Column\n0     clf0          0.000779\n1     clf0          0.001084\n2     clf0          0.001478\n3     clf0          0.004019\n4     clf0          0.004034\n...    ...               ...\n2495  clf4          0.996943\n2496  clf4          0.998093\n2497  clf4          0.998384\n2498  clf4          0.999620\n2499  clf4          0.999668\n"], "link": "https://stackoverflow.com/questions/57516069/merge-multiple-columns-as-new-rows-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to determine the reason why I get a failure when executing the pandas method. The DataFrame is valid, but it's very large (on the order of 1,000,000 records). Here is my code, where is my DataFrame: Right now, I am getting the exception being raised. Thanks in advance!", "q_apis": "get columns get DataFrame where DataFrame now", "apis": "to_dict", "code": ["import json\n\nwrite_file = \"/path/to/output.json\"\nholder_dictionary = predictions.to_dict(orient='records')\n\nwith open(write_file, 'w') as outfile:\n    json.dump(holder_dictionary, outfile)\n"], "link": "https://stackoverflow.com/questions/49545985/catch-pandas-df-to-json-exception"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to do multiple rounds of pivot_table to turn my flat data into something I can use for a project. Here is some sample data organized similarly to how it's coming out of the database. | | student_number | student_name | course_name | grade_level | storecode | grade | |---:|-----------------:|:---------------|:--------------|--------------:|:------------|:--------| | 0 | 123456 | Student A | Algebra I | 9 | S1 | A | | 1 | 123456 | Student A | Algebra I | 9 | S2 | B | | 2 | 123456 | Student A | Geometry | 10 | S1 | B | | 3 | 987654 | Student B | Algebra I | 9 | S1 | C | | 4 | 987654 | Student B | Geometry | 9 | S1 | B | Ultimately, we want the output to look something like this. Students can take multiple math classes in a year, and I would like to see each listed in its own set of columns. | 9 | | | 9 | | | 10 | | | 10 | | | | 1 | | | 2 | | | 1 | | | 2 | | | | course_name | S1 | S2 | course_name | S1 | S2 | course_name | S1 | S2 | course_name | S1 | S2 | |-----------------|---------------|-------------|----|------------------|----|----|-------------|----|----|-------------|----|----| | student_number | student_name | | | | | | | | | | | | | | 123456 | Student A | Algebra 1 | A | B | | | | Geometry | B | | | | | | 987654 | Student B | Algebra 1 | C | | Geometry | B | | | | | | | | Here is what I have so far: | | student_number | student_name | course_name | grade_level | storecode | grade | R | |---:|-----------------:|:---------------|:--------------|--------------:|:------------|:--------|----:| | 0 | 123456 | Student A | Algebra I | 9 | S1 | A | 1 | | 1 | 123456 | Student A | Algebra I | 9 | S2 | B | 1 | | 2 | 123456 | Student A | Geometry | 10 | S1 | B | 1 | | 3 | 987654 | Student B | Algebra I | 9 | S1 | C | 1 | | 4 | 987654 | Student B | Geometry | 9 | S1 | B | 2 | Here is where I perform my first pivot, and I like the shape of the result. | | student_number | student_name | course_name | grade_level | R | S1 | S2 | |---:|-----------------:|:---------------|:--------------|--------------:|----:|:-----|:-----| | 0 | 123456 | Student A | Algebra I | 9 | 1 | A | B | | 1 | 123456 | Student A | Geometry | 10 | 1 | B | | | 2 | 987654 | Student B | Algebra I | 9 | 1 | C | | | 3 | 987654 | Student B | Geometry | 9 | 2 | B | | This is the closest I've been able to get at my next step. It looks like I need to move the levels around and possible sort the columns | | storecode | S1 | | | S2 | | | course_name | | | | | grade_level | 9 | | 10 | 9 | | 10 | 9 | | 10 | | | R | 1 | 2 | 1 | 1 | 2 | 1 | 1 | 2 | 1 | |:--------------|:------------|:---|:---|:---|:---|:---|:---|:------------|:-----------|:-----------| | student_number| student_name| | | | | | | | | | | 123456 | Student A | A | | B | B | | | Algebra I | | Geometry | | 987654 | Student B | C | B | | | | | Algebra I | Geometry | |", "q_apis": "get columns pivot_table sample take year columns where first pivot shape get at step levels columns", "apis": "pivot_table index columns max fillna sort_index pivot index columns fillna sort_index", "code": ["(df\n .pivot_table(index=['student_number', 'student_name'], columns=['grade_level', 'R'], aggfunc=max)\n .fillna(\"\")\n [['course_name', 'S1', 'S2']]\n .sort_index(axis=1, level=[1,2], sort_remaining=False)\n)\n", "(df\n .pivot(index=['student_number', 'student_name'], columns=['grade_level', 'R'])\n .fillna(\"\")\n [['course_name', 'S1', 'S2']]\n .sort_index(axis=1, level=[1,2], sort_remaining=False)\n)\n"], "link": "https://stackoverflow.com/questions/65746739/multiple-levels-of-pivot-table-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose I have a dataframe that looks like this and I want to ensure all values within the column 'T' lie between 129 and 130. i.e. when a value is greater than 130, it should get subtracted until it lies between 129 and 130. Similarly, if the value is less then 129, I want to keep adding 1 til the value lies between 129 and 130. For some reason, the following code doesn't seem to work:", "q_apis": "get columns all values T between value get between value value between", "apis": "T T", "code": ["import numpy as np\ndf['T'] = df['T'] % 1 + 129\n"], "link": "https://stackoverflow.com/questions/50923383/ensure-all-values-in-a-dataframe-column-are-between-two-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The purpose of this script is to read a csv file then create a data frame out of it. The file contains forex historical data. The file has 7 columns Date, Time, Open, High, Low, Close and Volume, and around 600k rows. Here is a data sample: After scraping the date and time the script must make some date time calculation like month and day. Then, some technical analysis using TA-LIB library. With every new step the code produces a dataframe. All the new dataframes will be stored in a list. The last step is to concatenate all these dataframes into one final dataframe. Here is the code: Here is the error:", "q_apis": "get columns contains columns sample date time date time month day step last step all", "apis": "append append append", "code": ["    def dema(self):\n        self.df['DEMA'] = talib.DEMA(self.Close, timeperiod=30)\n        return self.dfs.append(self.df['dema'])\n\n    def ema(self):\n        self.df['EMA'] = talib.EMA(self.Close, timeperiod=30)\n        return self.dfs.append(self.df['ema'])\n\n    def ma(self):\n        self.df['MA'] = talib.MA(self.Close, timeperiod=30, matype=0)\n        return self.dfs.append(self.df['ma'])\n"], "link": "https://stackoverflow.com/questions/62080126/keyerror-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The following extremely simplified DataFrame represents a much larger DataFrame containing medical diagnoses: For machine learning, I need to randomly split this dataframe into three subframes in the following way: Where the split array specifies the fraction of the complete data that goes into each subframe, the data in the subframe needs to be mutually exclusive and the split array needs to sum to one. Aditionally, the fraction of positive diagnoses in each subset needs to be approximately the same. Answers to this question recommend using the pandas sample method or the train_test_split function from sklearn. But none of these solutions seem to generalize well to n splits and none provides a stratified split.", "q_apis": "get columns DataFrame DataFrame array array sum sample", "apis": "sample cumsum astype pop", "code": ["fractions = np.array([0.6, 0.2, 0.2])\n# shuffle your input\ndf = df.sample(frac=1) \n# split into 3 parts\ntrain, val, test = np.array_split(\n    df, (fractions[:-1].cumsum() * len(df)).astype(int))\n", "y = df.pop('diagnosis').to_frame()\nX = df\n", "X_train, X_test, y_train, y_test = train_test_split(\n        X, y,stratify=y, test_size=0.4)\n\nX_test, X_val, y_test, y_val = train_test_split(\n        X_test, y_test, stratify=y_test, test_size=0.5)\n"], "link": "https://stackoverflow.com/questions/50781562/stratified-splitting-of-pandas-dataframe-in-training-validation-and-test-set"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Python Pandas Dataframe with the data sample given below. The data has multiple unique ids as shown in column 2. I am trying to keep the data of unique / distinct ids which are greater than 20 in count. The new dataframe should have the data of unique_id which are greater than 20 in count and ignore the rest. For instance, if the appear 20 times in the data then keep the data of relevant columns otherwise ignore it. I tried: but it did not work.", "q_apis": "get columns sample unique unique count count columns", "apis": "value_counts index loc isin", "code": ["id_counts = df['unique_id'].value_counts()\nids_to_keep = id_counts[id_counts >= 20].index\ndf = df.loc[df['unique_id'].isin(ids_to_keep)]\n"], "link": "https://stackoverflow.com/questions/67255033/selecting-unique-id-s-count-greater-than-a-specified-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i am still a new student starting to learn python and programming. and here is my data I would like to merge them and see the result as", "q_apis": "get columns merge", "apis": "merge set_index drop_duplicates groupby sum", "code": ["cols = ['class1', 'class2']\n>>> pd.get_dummies(pd.merge(df, df2, how='outer').set_index(cols), prefix='', prefix_sep='')\\\n    .drop_duplicates()                                                         \\\n    .groupby(cols)                                                             \\\n    .sum()\n", "                 numid_d112   numid_d212    numid_d312    numid_d412\nclass1  class2              \ndoc1a   12       1            0             0             0\n        13       0            1             1             0\ndoc1b   11       0            0             0             1\n"], "link": "https://stackoverflow.com/questions/52900873/python-is-it-possible-to-map-data-and-make-a-vector-matrix"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas data frame with two columns one is temperature the other is time. I would like to make third and fourth columns called min and max. Each of these columns would be filled with nan's except where there is a local min or max, then it would have the value of that extrema. Here is a sample of what the data looks like, essentially I am trying to identify all the peaks and low points in the figure. Are there any built in tools with pandas that can accomplish this?", "q_apis": "get columns columns time columns min max columns where min max value sample all any", "apis": "min shift shift max shift shift sample xs xs append xs DataFrame xs columns min shift shift max shift shift index min index max plot", "code": ["df['min'] = df.data[(df.data.shift(1) > df.data) & (df.data.shift(-1) > df.data)]\ndf['max'] = df.data[(df.data.shift(1) < df.data) & (df.data.shift(-1) < df.data)]\n", "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Generate a noisy AR(1) sample\nnp.random.seed(0)\nrs = np.random.randn(200)\nxs = [0]\nfor r in rs:\n    xs.append(xs[-1]*0.9 + r)\ndf = pd.DataFrame(xs, columns=['data'])\n\n# Find local peaks\ndf['min'] = df.data[(df.data.shift(1) > df.data) & (df.data.shift(-1) > df.data)]\ndf['max'] = df.data[(df.data.shift(1) < df.data) & (df.data.shift(-1) < df.data)]\n\n# Plot results\nplt.scatter(df.index, df['min'], c='r')\nplt.scatter(df.index, df['max'], c='g')\ndf.data.plot()\n"], "link": "https://stackoverflow.com/questions/48023982/pandas-finding-local-max-and-min"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dictionary of lists (equal length), which I would like to convert to a dataframe such that each key in the dictionary represents one column (or series) in the dataframe, and the list of values corresponding to each key are converted into individual records in the column. Suppose the contents of dictionary are: I would like the contents of dataframe to be: I have tried solving this by first converting the dictionary to a dataframe, and then taking the transpose of the dataframe. This gives the following output: I am not sure how to proceed further.", "q_apis": "get columns length values first transpose", "apis": "DataFrame", "code": ["import pandas as pd\nsample_dict = {'col1':['abc','def pqr','w xyz'],'col2':['1a2b','lmno','qr st']}\nsample_df = pd.DataFrame(sample_dict)\nprint (sample_df)\n", "      col1   col2\n0      abc   1a2b\n1  def pqr   lmno\n2    w xyz  qr st\n"], "link": "https://stackoverflow.com/questions/59404567/pandas-how-to-convert-dictionary-to-transposed-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm practicing with Pandas, Lambda functions, and facing a difficult task. I have already got a \"formally\" correct solution but absolutely inefficient. This is the problem: I have a Pandas DataFrame which is something like this (the code to generate a sample of this at the end of this post): Expected output: a new column per each different type (A, C, G, T) which contains the number of unique ids which, in the last row they showed up in the table, have that type. A possible output is this (edited to match the desired outcome): To reach this goal, I have done the following (which is not working properly if your compare the output with the table showed above): Defined a temporary DataFrame which is storing the status of all possible ids (in this example there are a a maximum of 9): Defined an iterative loop which is checking which is the type of each row in and then is updating accordingly the status of the DataFrame: Here's the code: What I would like to understand, is if using Lambda functions or a different approach is it possible to get a quicker result which is also better in terms of performances. To generate a sample DataFrame like mine, you can use the following code (suggestions about how to tweak this are also welcome so I can learn more): The temporary DataFrame is defined as follows: Thank you for your precious time.", "q_apis": "get columns DataFrame sample at T contains unique last compare DataFrame all DataFrame get sample DataFrame DataFrame time", "apis": "reset_index pivot index index columns ffill apply value_counts reindex T fillna astype join", "code": ["state = df.reset_index().pivot(index=\"index\", columns=\"id\").ffill()\ncounts = state.apply(pd.value_counts, axis=1).reindex([\"A\", \"C\", \"G\", \"T\"], axis=1)\ncounts = counts.fillna(0).astype(int)\nout = df.join(counts)\n", "In [193]: out\nOut[193]: \n     id type  A  C  G  T\n0  1003    G  0  0  1  0\n1  1003    A  1  0  0  0\n2  1002    T  1  0  0  1\n3  1002    A  2  0  0  0\n4  1001    A  3  0  0  0\n5  1003    A  3  0  0  0\n6  1002    G  2  0  1  0\n7  1003    A  2  0  1  0\n8  1001    T  1  0  1  1\n9  1001    A  2  0  1  0\n"], "link": "https://stackoverflow.com/questions/51143508/state-counting-machine-with-python-and-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two different dataframes: I created the following plot using matplotlib: I was expecting 2 separate bars, one for each dataframe column, and but instead I got a stacked bar chart. How can I get a chart with bars next to one another similar to the chart here Matplotlib plot multiple bars in one graph ? I tried adjusting my code to the one in that example but I couldn't get it.", "q_apis": "get columns plot get plot get", "apis": "align align merge columns melt", "code": ["fig, ax = plt.subplots()\n\nax.bar(df_test1.age_group, df_test1.total_arrests, color = 'seagreen', width=0.4, align='edge')\nax.bar(df_test2.age_group, df_test2.total_arrests, color = 'lightgreen', width=-0.4, align='edge')\nax.set_xlabel('Age Group')\nax.set_ylabel('Number of Arrests')\nax.set_title('Arrests vs. Felony Arrests by Age Group')\nplt.xticks(rotation=0)\nplt.legend(['All Arressts', 'Felony Arrests'])\nax.yaxis.set_major_formatter(\nmatplotlib.ticker.FuncFormatter(lambda y,p: format(int(y), ','))\n)\n\nfor i,j in zip(df_test1.age_group, df_test1.total_arrests):\n    ax.annotate(format(j, ','), xy=(i,j))\n\nfor i,j in zip(df_test2.age_group, df_test2.total_arrests):\n    ax.annotate(format(j, ','), xy=(i,j), ha='right')\n\nplt.show()\n", "df = pd.merge(left=df_test1, right=df_test2, on='age_group')\n\ndf.columns=['age_group','all_arrests', 'felonies']\ndf = df.melt(id_vars=['age_group'], var_name='Type', value_name='Number')\n\nfig, ax = plt.subplots()\nsns.barplot(y='Number',x='age_group',hue='Type', data=df, hue_order=['felonies','all_arrests'])\n"], "link": "https://stackoverflow.com/questions/64914439/how-do-i-add-multiple-bar-graphs-from-a-pandas-dataframe-on-one-plot-in-matplotl"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe, formed of survey responses the column headers are scores and the number row beneath is counts of responses for those values How do I group these columns and retain the counts per the below", "q_apis": "get columns values columns", "apis": "groupby columns sum rename DataFrame", "code": ["df.groupby(df.columns >= 7, axis=1).sum().rename({True: '>=7', False: '<=6'}, axis=1)\n", "   <=6  >=7\n0  385  355\n", "data = {1: [100], 2: [50], 3: [25], 4: [50], 5: [100], 6: [60], 7: [80], 8: [75], 9: [100], 10: [50], 11: [50]}\ndf = pd.DataFrame(data)\n"], "link": "https://stackoverflow.com/questions/67991523/pandas-dataframe-grouping-columns-by-column-index-ranges-and-get-group-sums"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe of products that looks like this I want to group categories if they contain the same string (for example pc or charger) and send them to another dataframe like this Can i do this using pandas?", "q_apis": "get columns categories", "apis": "apply groupby sum", "code": [" df['Category'] = df[\"Category\"].apply(lambda x: x.split(\" \")[1])\n df1 = df.groupby(\"Category\").sum()\n", " Category   num_of_product\n charger    15\n pc         14\n"], "link": "https://stackoverflow.com/questions/62954672/organize-columns-in-dataframe-based-on-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to convert specific columns in my DataFrame to dtype: float. I tried this: But when I print this afterwards: I am still seeing this: Any ideas?", "q_apis": "get columns columns DataFrame dtype", "apis": "columns columns columns apply", "code": ["columns = ['DISTINCT_COUNT','MAX_COL_LENGTH', 'MIN_COL_LENGTH', 'NULL_COUNT']\ngrid[columns] = grid[columns].apply(pd.to_numeric, errors='ignore')\n"], "link": "https://stackoverflow.com/questions/50765631/datatypes-not-changing-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following excel sheet for an email campaign activity: I want to track each email_title activity per customer and arrange it by date. What I did so far with dataframe: I filtered the customer_email list to get the unique number of emails: Then filtered the activity of each customer_email for the email_title & Status: Now, what I want is to aggregate the status of each email_title per customer. The challenge is each customer has the same email_title, the first 3 rows show that email \"elevate your skills in 15 minutes\" was sent to email1@sample.com 2 times first - delivered and opened (this is one activity) in 1/1/20 then second - delivered and not opened in 2/1/2020 Now I want to aggregate each email_title for each customer per date How can I achieve this? I am not an expert in dataframes Thank you", "q_apis": "get columns date get unique aggregate first sample first second aggregate date", "apis": "DataFrame sample sample sample assign replace groupby agg last max sample sample", "code": ["df = pd.DataFrame({'date': {0: '1/1/2020', 1: '1/1/2020', 2: '2/1/2020'},\n 'customer_email': {0: 'email1@sample.com',\n  1: 'email1@sample.com',\n  2: 'email1@sample.com'},\n 'email_title': {0: 'Elevate your skills in 15 minutes',\n  1: 'Elevate your skills in 15 minutes',\n  2: 'Elevate your skills in 15 minutes'},\n 'status': {0: 'opened', 1: 'delivered', 2: 'delivered'}})\ndf\n", "(df.assign(**{'new status aggregrated per email title' : df['status'].replace('delivered', 'not opened')})\n   .groupby(['date', 'customer_email', 'email_title'], as_index=False)\n   .agg({'status' : 'last', 'new status aggregrated per email title' : 'max'}))\n", "       date     customer_email                        email_title     status  \\\n0  1/1/2020  email1@sample.com  Elevate your skills in 15 minutes  delivered   \n1  2/1/2020  email1@sample.com  Elevate your skills in 15 minutes  delivered   \n\n  new status aggregrated per email title  \n0                                 opened  \n1                             not opened  \n"], "link": "https://stackoverflow.com/questions/67511756/aggregate-arrange-results-in-dataframe-by-two-criteria"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Take this : I need to create a column named that will give an incremental value (1,2,3...) for each row, for unique values of and , and using a conditional to skip and values of . The result should be like this:", "q_apis": "get columns value unique values values", "apis": "where isin groupby replace keys groupby transform min where isin dropna all ne shift ne shift groupby cumsum", "code": ["s = (df.where(~df['operation'].isin(['transfer', 'fee']))\n       .groupby(['client_id', 'key'], sort=False).ngroup()\n       .replace(-1, np.NaN))  # ngroup makes NaN group keys -1.\n\ndf['pos_id'] = s - s.groupby(df['client_id']).transform('min') + 1\n", "s = (df.where(~df['operation'].isin(['transfer', 'fee']))\n       .dropna(how='all'))\n\ns = s['key'].ne(s['key'].shift()) | s['client_id'].ne(s['client_id'].shift())\ndf['pos_id'] = s.groupby(df['client_id']).cumsum()\n", "   client_id    key operation  pos_id\n0          0  0_382       buy     1.0\n1          0  0_382      sell     1.0\n2          0  0_356      sell     2.0\n3          1  1_365       buy     1.0\n4          1    NaN  transfer     NaN\n5          1  1_365       buy     1.0\n6          2    NaN       fee     NaN\n7          2  2_284       buy     1.0\n8          2  2_405       buy     2.0\n"], "link": "https://stackoverflow.com/questions/65946342/how-do-i-count-inside-a-dataframe-using-unique-values-and-conditional"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Good day to all! I need Your help. I have a DataFrame like: I want to leave only minimal values of marks for every person. For example: if Jack had his minimal grade \"4\" several times, I need to delete other rows where Jack got other grades and leave the ones where he got \"4\". The same logic should apply to others too. Here is an example of a DataFrame I want: Could you please advise me on how I should approach this?", "q_apis": "get columns day all DataFrame values delete where where apply DataFrame", "apis": "groupby apply min", "code": ["df.groupby('name').apply(lambda x: x[x['mark'] == x['mark'].min()])\n"], "link": "https://stackoverflow.com/questions/63853893/how-to-leave-only-minimal-values-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Suppose the data is ordered by date. How to exact the first row from each date. If the 'Volume' is 0, then take the next row until it's not 0. I have tried by using lambda, but it seems not working. If I have enough data and like to exact #2-5 rows (consecutive) from each date. Should I use range function?", "q_apis": "get columns ordered date first date take date", "apis": "query drop_duplicates", "code": ["df = df[df['Volume'].ne(0)].drop_duplicates('Date')\nprint (df)\n         Date Ticker  Volume\n0  2019-03-21   AAPL      10\n5  2019-03-25   GOOG      12\n7  2019-03-27    IBM      16\n9  2019-03-28    IBM       9\n", "df = df.query('Volume != 0').drop_duplicates('Date')\n", "df1 = df[df['Volume'].ne(0)]\nprint (df1)\n          Date Ticker  Volume\n0   2019-03-21   AAPL      10\n1   2019-03-21   GOOG       5\n5   2019-03-25   GOOG      12\n7   2019-03-27    IBM      16\n8   2019-03-27   GOOG      10\n9   2019-03-28    IBM       9\n11  2019-03-28   AAPL      10\n\ndf2 = df1.groupby('Date').nth(1)\nprint (df2)\n           Ticker  Volume\nDate                     \n2019-03-21   GOOG       5\n2019-03-27   GOOG      10\n2019-03-28   AAPL      10\n", "df2 = df1[df1.groupby('Date').cumcount().eq(1)]\nprint (df2)\n          Date Ticker  Volume\n1   2019-03-21   GOOG       5\n8   2019-03-27   GOOG      10\n11  2019-03-28   AAPL      10\n"], "link": "https://stackoverflow.com/questions/60301813/lookup-values-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "As you can see the calculations under column D follow a specific pattern i.e. prior value * (1+the rate%/365) so in cell D2 you have 100*(1+8%/365) D3 will be 100.021918*(1+8.06%/365) is there an easy way to do that in python as I don't want to use excel for that purpose....and I have daily data going back 30 years.", "q_apis": "get columns value", "apis": "append DataFrame", "code": ["cell_d = [100]\nrates = [0.08, 0.0806, 0.0812, 0.0813, 0.08]\nfor i, rate in enumerate(rates):\n    cell_d.append(cell_d[i] * (1 + rate/365))\npd.DataFrame({'rates': rates, 'cell_d': cell_d[1:]})\n"], "link": "https://stackoverflow.com/questions/58223317/dataframe-how-to-perform-calculations-please-see-the-attached-photo"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe: Col1 is the payment. and Col2 is the Project cost I basically want to match Col1 with the project in Col2. How can I get the following dataframe? Is there anything I can do in the pandas to get this result? Thanks a lot", "q_apis": "get columns get get", "apis": "append append append explode apply apply append append append explode apply apply drop columns", "code": ["# making list of project requirement and item value to iterate\nitem_cost = list(zip(payment['Col1'], payment['value']))\nrequirment = list(zip(project_cost['Col2'], project_cost['value']))\n\nd = {}\nfor project, cost in requirment:\n    item_used = []\n    try:\n        while cost > 0: \n            if item_cost[0][1] <= cost: # if item is having less cost than requirement\n                cost -= item_cost[0][1] # making requirement less by item cost\n                item_used.append((item_cost[0][1], item_cost[0][0]))\n                item_cost = item_cost[1:] # removing item from item_cost once used\n            else: \n                item_cost[0] = ((item_cost[0][0], item_cost[0][1] - cost))\n                item_used.append((cost, item_cost[0][0]))\n                cost = 0\n    except: # when item list will be finished exception will be called\n        item_used.append((cost,np.nan))\n    d[project] = item_used\n# d is dict having project as key and item used\nproject_cost['Col1'] = project_cost['Col2'].map(d)          \npr_cost = project_cost.explode(\"Col1\")            \npr_cost['value'] = pr_cost['Col1'].apply(lambda x:x[0])            \npr_cost['Col1'] = pr_cost['Col1'].apply(lambda x:x[1])      \n", "item_cost = list(zip(payment['Payment'], payment['Value']))\nrequirment = list(zip(project_cost['Project'], project_cost['Cost']))\nd = {}\nfor project, cost in requirment:\n    item_used = []\n    try:\n        while cost > 0: \n            if item_cost[0][1] <= cost: # if item is having less cost than requirement\n                cost -= item_cost[0][1] # making requirement less by item cost\n                item_used.append((item_cost[0][1], item_cost[0][0]))\n                item_cost = item_cost[1:] # removing item from item_cost once used\n            else: \n                item_cost[0] = ((item_cost[0][0], item_cost[0][1] - cost))\n                item_used.append((cost, item_cost[0][0]))\n                cost = 0\n    except: # when item list will be finished exception will be called\n        item_used.append((cost,np.nan))\n    d[project] = item_used\nproject_cost['bank_val'] = project_cost['Project'].map(d)          \npr_cost = project_cost.explode(\"bank_val\")            \npr_cost['value'] = pr_cost['bank_val'].apply(lambda x:x[0])            \npr_cost['bank'] = pr_cost['bank_val'].apply(lambda x:x[1]) \nprint(pr_cost.drop(columns = ['bank_val', 'Cost']))\n"], "link": "https://stackoverflow.com/questions/60294970/how-can-i-do-dataframe-subtraction"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am developing a function for calculating a number for a document based on evaluations on a dataset. I chose pandas since it seemed to be the most efficient way of using a big dataset. My columns are: citing (identifiers), cited (identifiers), creation (string YYYY-MM or YYYY). I need to add to a set all the identifiers of citing objects who meet the criteria of being created in year-1 or year-2. I found this cool trick to subset a Dataframe through indexing: I save the indexed Dataframe to a local variable ('citing') and then use the .loc[identifier]['creation'] to get the value of that row at column creation. Thing is, this can either return a series (more than one identifier) or a string (just one value, so directly the creation date). Since the value can either be in str(YYYY-MM) or str(YYYY) format I have to slice it with [:4] to make the actual comparison, plus. I tried to do a conditional block based on datatype but something must've gone wrong, because what I print with my DEBUG lines is this: DEBUG: 2014 is == to either 2015 or 2014 DEBUG: 2016 is == to either 2015 or 2014 DEBUG: 2018 is == to either 2015 or 2014 DEBUG: 2015 is == to either 2015 or 2014 i also tried to do a string comparison, turning the dates in str() and then comparing strings, unfortunately I got the same result this is really my first complex function in python, so some things may be obviously wrong or slow or inefficient, please be so kind as to spell them out for me so that I can improve my function! Thank you! EDIT: sample input as pandas dataframe: if the input were this Dataframe and the year 2018, the result set should only contain {1237} since it is the only one created in y-1 or y-2", "q_apis": "get columns columns add all year year loc get value at value date value first sample year", "apis": "astype astype isin index loc", "code": ["ix = df[\n    df.creation.astype(str).str[:4].astype(int).isin({year-1, year-2})\n  ].index\nidentifiers = set(df.loc[ix, 'citing'])\npub |= identifiers\n"], "link": "https://stackoverflow.com/questions/65429488/python-int-comparison-not-working-properly-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 same dataframes with different names (df_1 and df_2). Lets say the dataframes have 2 columns Category and Time. For eg. Category Time A 2020-02-02 05:05:05.0000 A 2020-02-02 06:06:06.0000 A 2020-02-02 07:07:07.0000 B 2020-02-02 05:05:05.0000 B 2020-02-02 06:06:06.0000 C 2020-02-02 05:05:05.0000 C 2020-02-02 06:06:06.0000 I want the following if conditions: if category of df_1 matches with category of df_2 then, in a new dataframe(with columns: category, starttime, endtime), In case of A category, I want to put the first datetime(2020-02-02 05:05:05.0000) in starttime and last datetime (2020-02-02 07:07:07.0000) in endtime column. Final Result new dataframe: Category Start Time EndTime A 2020-02-02 05:05:05.0000 2020-02-02 07:07:07.0000 B 2020-02-02 05:05:05.0000 2020-02-02 06:06:06.0000 C 2020-02-02 05:05:05.0000 2020-02-02 06:06:06.0000 How can I achieve this? Please help.", "q_apis": "get columns names columns columns put first last", "apis": "groupby agg min max groupby agg min max join apply min max rename columns min max groupby agg min max groupby agg min max join apply min max rename columns min max DataFrame min max values get groupby agg min max where min max min max min max max min astype rename columns min max rename columns min max", "code": ["pd.concat([df_1.groupby(\"CATEGORY\").agg([min, max]),\n           df_2.groupby(\"CATEGORY\").agg([min, max])], \n        join=\"inner\", axis=1).apply([min, max], axis=1)\n    .rename(columns={\"min\":\"START TIME\", \"max\":\"END TIME\"})\n", "grouped_1 = df_1.groupby(\"CATEGORY\").agg([min, max])\ngrouped_2 = df_2.groupby(\"CATEGORY\").agg([min, max])\n", "grouped_both = pd.concat([grouped_1, grouped_2], join=\"inner\", axis=1)\n", "final_df = grouped_both.apply([min, max], axis=1)\n    .rename(columns={\"min\":\"START TIME\", \"max\":\"END TIME\"})\n", "# Group the DataFrame by CATEGORY and keep the min and max values\n# We also need to get rid of the newly created MultiIndex level \"TIME\"\njoined_df = df_1.groupby(\"CATEGORY\").agg([min, max])[\"TIME\"]\n# Keep only rows where the min is different than the max\njoined_df = joined_df[joined_df[\"min\"]!= joined_df[\"max\"]]\n# Calculate the time deltas between min and max\n# then cast it to a number value of the minutes\njoined_df[\"DURATION\"] = (joined_df[ \"max\"]- joined_df[\"min\"]).astype('timedelta64[m]')\n# We rename the columns min and max\njoined_df = joined_df.rename(columns={\"min\":\"START TIME\", \"max\":\"END TIME\"})\n"], "link": "https://stackoverflow.com/questions/65203136/pandas-dataframe-python-how-to-compare-a-cell-with-another-cell-of-a-copied-da"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a simple DataFrame that looks like: I want to retrieve records which only have more than 5 vowels in their names. For this I made function: How can I use this function to extract records from the DataFrame? Please help me to understand how to use df.apply(map()) function on this Pandas Series and how to get the same using list comprehension if possible.", "q_apis": "get columns DataFrame names DataFrame apply map Series get", "apis": "count query count query", "code": ["m = df['Names'].str.lower().str.count(r'[aeiou]')\ndf = df.query('@m > 5')\n", "import re\n\nm = df['Names'].str.count(r'[aeiou]', flags = re.I)\ndf = df.query('@m > 5')\n", "          Names\n0   Alexi Laiho\n1  Jari Maenpaa\n"], "link": "https://stackoverflow.com/questions/57577133/conditional-extraction-of-data-from-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "If I had the following df: I want to group by the and columns, add up the , and also do a concatenation of the with a : What would be the correct way of approaching this? Side question: say if the was being read from a .csv and it had other unrelated columns, how do I do this calculation and then write to a new .csv along with the other columns (same schema as the one read)?", "q_apis": "get columns columns add columns columns", "apis": "groupby agg sum join first first groupby agg sum join", "code": ["df.groupby(['name', 'role'], as_index=False)\\\n.agg({'amount':'sum', 'desc':lambda x: ','.join(x)})\n\n\n    name    role    amount  desc\n0   a       x       1.0     f\n1   a       y       2.0     g\n2   b       y       7.0     h,j\n3   c       x       11.0    k,l\n4   c       y       6.0     p\n", "df.groupby(['name', 'role'], as_index=False).agg({'amount':'sum', 'desc':lambda x: ','.join(x), 'other1':'first', 'other2':'first'})\n", "df.groupby(['name', 'role', 'other1', 'other2'], as_index=False).agg({'amount':'sum', 'desc':lambda x: ','.join(x)})\n"], "link": "https://stackoverflow.com/questions/52546386/pandas-groupby-multiple-columns-concatenating-one-column-while-adding-another"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a bunch of csvs in a folder in this format: I am read them in as DF's and make a list of the DF's: What I am trying to do is reduce them down to one dataframe with no repetitions of \"chunk_id\". Instead, I would like to merge on this ID. I tried this: which just gives me a really wide dataframe with no entries. I haven't attempted the averaging yet, but I would like to end up with a dataframe that has exactly the same columns as the example above, but where each row in all the dataframes with the same \"chunk_id\" is merged but their \"diffs_avg\", \"timecodes\", \"chunk_completed\" and \"sd\" columns averaged. So, if I had read in the following dfs: DF1 DF2 RESULT: Reproducible DF:", "q_apis": "get columns merge columns where all columns", "apis": "all", "code": ["# First of all you should concat your csv's into one big dataframe:\ndf3 = pd.concat(csvs, axis=0, ignore_index=True)\n", "# First we concat df1 & df2 which is the appending of the CSV's\n# Note this is a simulation of your csv's\ndf3 = pd.concat([df1,df2], ignore_index=True)\n\nprint(df3)\n     chunk   timecodes  chunk_completed chunk_id  diffs_avg  sd\n0  [60 62]  [100, 200]              500    60-62          2   1\n1  [58 53]  [800, 900]             1000    58-53          4   6\n2  [60 62]  [200, 400]             1000    60-62          4   2\n3  [30 33]  [200, 700]              800    30-33          6   7\n", "df_grouped = df3.groupby('chunk_id').agg({'chunk_completed':'mean',\n                                          'diffs_avg':'mean',\n                                          'sd':'mean'}).reset_index()\n\nprint(df_grouped)\n  chunk_id  chunk_completed  diffs_avg   sd\n0    30-33              800          6  7.0\n1    58-53             1000          4  6.0\n2    60-62              750          3  1.5\n"], "link": "https://stackoverflow.com/questions/55354742/merge-dfs-on-str-id-and-take-averages"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have multiindex mapping rules, here's the rules I here's my dataframe, let say this is a dataframe, and want to do multi index mapping With , it can be use and . So, the code will be The output would be like this How to do this on pySpark datafarame", "q_apis": "get columns index", "apis": "values items values values", "code": ["from pyspark.sql import functions as F\n\nindex_dict = {\n    'Type A': ['Chicken', 'Beef', 'Goat'],\n    'Type B': ['Fish', 'Shrimp'],\n    'Type C': ['Chicken', 'Pork']\n}\n\nfor col_name, values in index_dict.items():\n    col = F.col('Menu').like('%'+values[0]+'%')\n    for value in values[1:]:\n        col2 = F.col('Menu').like('%'+value+'%')\n        col = col | col2\n    df = df.withColumn(col_name, col)\n", "df = df.withColumn('tmp', F.split(df['Menu'], ' '))"], "link": "https://stackoverflow.com/questions/51824082/multiindex-categorization-and-encoding-this-in-pyspark"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Im searching for a function that Returns the Position of an element in a dataframe. - there is duplicates in the dataframe amongst the values - dataframe About 10*2000 - the function will be applied on a dataframe using applymap() Example: get_position(2) is not clear as it could be either \"R1\" or \"R2\". I am wondering if there is another way that python knows which Position the element holds - possibly during the applymap() Operation Edit: df.rank(axis=1,pct=True) EDIT2: step1) step2) step3) step 4)", "q_apis": "get columns values applymap applymap rank step", "apis": "append append append append append append append append append append stack reset_index drop apply", "code": ["df = pd.DataFrame({\"R1\": [8,2,3], \"R2\": [2,3,4], \"R3\": [-3,4,-1]})\n\ni, c = np.where(df == 2)\nprint (i, c)\n[0 1] [1 0]\n\nprint (df.index.values[i])\n[0 1]\n\nprint (df.columns.values[c])\n['R2' 'R1']\n", "i, c = np.where(df == 2)\n\ndf1 = df.rank(axis=1,pct=True)\nprint (df1)\n         R1        R2        R3\n0  1.000000  0.666667  0.333333\n1  0.333333  0.666667  1.000000\n2  0.666667  1.000000  0.333333\n\nprint (df1.iloc[i, c])\n         R2        R1\n0  0.666667  1.000000\n1  0.666667  0.333333\n\nprint (df1.where(df == 2).dropna(how='all').dropna(how='all', axis=1))\n         R1        R2\n0       NaN  0.666667\n1  0.333333       NaN\n", "out = df1.stack()[df.stack() == 2].rename_axis(('idx','cols')).reset_index(name='val')\nprint (out)\n   idx cols       val\n0    0   R2  0.666667\n1    1   R1  0.333333\n", "P1=[]\nP2=[]\nP3=[]\nP4=[]\nP5=[]\nP6=[]\nP7=[]\nP8=[]\nP9=[]\nP10=[]\nP11=[]\n\ndef function103(x):\n\n    if 0.0 <= x[0] <= 0.1:\n        P1.append(get_column_name1(x))\n        return x\n    elif 0.1 < x[0] <= 0.2:\n        P2.append(get_column_name1(x))\n        return x\n    elif 0.2 < x[0] <= 0.3:\n        P3.append(get_column_name1(x))\n        return x\n    elif 0.3 < x[0] <= 0.4:\n        P4.append(get_column_name1(x))\n        return x\n    elif 0.4 < x[0] <= 0.5:\n        P5.append(get_column_name1(x))\n        return x\n    elif 0.5 < x[0] <= 0.6:\n        P6.append(get_column_name1(x))\n        return x\n    elif 0.6 < x[0] <= 0.7:\n        P7.append(get_column_name1(x))\n        return x\n    elif 0.7 < x[0] <= 0.8:\n        P8.append(get_column_name1(x))\n        return x\n    elif 0.8 < x[0] <= 0.9:\n        P9.append(get_column_name1(x))\n        return x\n    elif 0.9 < x[0] <= 1.0:\n        P10.append(get_column_name1(x))\n        return x\n    else:\n        return x\n", "a = df_rank.stack().reset_index(level=0, drop=True).to_frame().apply(function103, axis=1)\n", "print (P4)\n['R3', 'R1', 'R3']\n"], "link": "https://stackoverflow.com/questions/57523000/determine-position-of-element-in-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The DataFrame: col1 0 2 1 6 2 7 3 8 4 2 A function to create a new column being a cumulative sum (I'm aware of , this was just a simple test before doing more complex functions): col1 new 0 2 2 1 8 8 2 15 15 3 23 23 4 25 25 As you can see, for some reason gets modified despite never issuing a command to change it (?) I've tried: immediately doing in the function then only using in the rest of the function doing before calling the function with swapping with", "q_apis": "get columns DataFrame sum test", "apis": "copy values", "code": ["inputarr[i] = inputarr[i] + inputarr[i-1]\n", "def funcadd(inputarr):\n    c_inputarr = inputarr.copy()\n    for i in range(1, len(inputarr)):\n        c_inputarr[i] = c_inputarr[i] + c_inputarr[i-1]\n    return c_inputarr\n\ndf['new'] = funcadd(df['col1'].values)\n", "col1    new\n0   2   2\n1   6   8\n2   7   15\n3   8   23\n4   2   25\n"], "link": "https://stackoverflow.com/questions/68044077/why-is-this-creation-of-a-new-pandas-column-causing-a-change-in-the-original-col"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with 100s of columns and 1000s of rows but the basic structure is I would like to add two new columns one would be and id which would be equal to the first value in each row the second would be a count of the values in each row. It would look like this. To be clear all rows will always have the same value. Any help in figuring out a way to do this would be greatly appreciated. Thanks", "q_apis": "get columns columns add columns first value second count values all value", "apis": "bfill iloc fillna count drop columns notnull sum columns columns", "code": ["df['id'] = df.bfill(axis=1).iloc[:, 0].fillna('All NANs')\ndf['count'] = df.drop(columns=[\"id\"]).notnull().sum(axis=1)\n", "df = df[list(df.columns[-2:]) + list(df.columns[:-2])]\n"], "link": "https://stackoverflow.com/questions/60385781/pandas-dataframe-add-columns-based-on-existing-data"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a hard time to randomly select rows from a dataframe. In general, choosing one row is not a problem using . I assume that replacement=True. However, I need to randomly select an hour and as output, recieve the 4 rows of each quarter. The dataframe to choose from is the following (1132 rows): My desired output is something like this: Assuming the random generator has \"selected\" , the output would be Depending on the number (N) of random samples, the length of the resulting dataframe should be 4xN. Is it possible to randomly select an dayhour directly from the dataframe and repeat this 1000 times? I am afraid that using an extra dataframe to select an hour and then looking the corresponding values up in the original dataframe will be too time consuming. I am confident that this should be doable in Python, but I couldn`t find any tips on this. Thanks for any help!", "q_apis": "get columns time select select hour quarter length select repeat select hour values time any any", "apis": "index size index index loc", "code": ["N = 1000\nvals = pd.to_datetime(np.random.choice(df.index,size=N)).floor('H')\nhours = df.index.floor('H')\n\nfor i in vals:\n    print (df[hours == i])\n", "df1 = pd.concat([df[hours == i] for i in vals])\n", "idx = np.concatenate([df.index[hours == i] for i in vals])\ndf1 = df.loc[idx]\n"], "link": "https://stackoverflow.com/questions/57556657/randomly-select-hour-from-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following Pandas series: I want to convert these language codes into their original names, for example en >> English ar >> Arabic I looked up this question but it didn't help. If there are any packages required, please provide a source of how to install them using pip if possible.", "q_apis": "get columns codes names any", "apis": "apply get apply get", "code": ["#pip install iso-639\nfrom iso639 import languages\ndf['lang'] = df['lang'].apply(lambda x: languages.get(alpha2=x).name)\n", "       lang  count\n0   English  32269\n1    French   2438\n2   Italian   1529\n3  Japanese   1350\n4    German   1080\n5     Latin      1\n6  Javanese      1\n7    Samoan      1\n8  Galician      1\n9   Maltese      1\n\n", "from iso639 import languages\ndf['original_language'] = df['original_language'].apply(lambda x: languages.get(alpha2=x).name)\n"], "link": "https://stackoverflow.com/questions/67340773/how-to-convert-languages-iso639-1-codes-to-language-names-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Background: I have two Pandas DataFrames: DF1 represents known road segments with >= 7% truck traffic. DF2 represents all road segments in the study area. Columns: is 'standard route identifier', is 'mile point start', is 'mile point end', and is 'truck traffic percentage'. Task: For each row in , the task is to check each record in to find two concurrent items: Find matching records. Within the matching records, find the rows in where and values are present within the and values. Then assign the value to the correct row in following the above condition. An example follows below. Sample DF1: Sample DF2: Expected result: For the row where , and : on row 0 () falls within row 5 of and of row 0 () falls within row 4. Therefore, both rows 4 and 5 are assigned a value of 7%. Similarly, where (row 7) this row would be assigned a of 7% because row 3 of contains and values of where . Sample DF3 Output: Disclaimer: This is a confusing problem - please ask for more info where needed and I will try and clarify.", "q_apis": "get columns all start items where values values assign value where value where contains values where info where", "apis": "reset_index merge loc", "code": ["DF2 = DF2.reset_index()\ndf = pd.merge(DF2, DF1, on='SRI', how='left', suffixes=('', '_range'))\n", "    index          SRI  MP_START  MP_END  MP_START_range  MP_END_range TRUCK_PCT\n0       0   17031021__      0.00   0.100             NaN           NaN       NaN\n1       1   03291236__      0.00   0.050             NaN           NaN       NaN\n2       2  200006165__      0.00   0.120             NaN           NaN       NaN\n3       3   00000009__     52.36  52.394           51.30        52.260        7%\n4       4   00000009__     51.78  52.360           51.30        52.260        7%\n5       5   00000009__     49.09  51.780           51.30        52.260        7%\n6       6   00000009__     48.76  48.760           51.30        52.260        7%\n7       7   00000015__     15.45  15.480           14.41        14.710       12%\n8       7   00000015__     15.45  15.480           14.71        15.450        8%\n9       7   00000015__     15.45  15.480           15.45        15.724        7%\n10      7   00000015__     15.45  15.480           15.72        16.250        8%\n11      7   00000015__     15.45  15.480           16.25        16.310       10%\n12      7   00000015__     15.45  15.480           16.31        16.700       10%\n13      7   00000015__     15.45  15.480           16.70        16.890       11%\n", "cond_start = (df['MP_START'] >= df['MP_START_range']) & (df['MP_START'] < df['MP_END_range'])\ncond_end = (df['MP_END'] >= df['MP_START_range']) & (df['MP_END'] < df['MP_END_range'])\ndf.loc[~(cond_start | cond_end), 'TRUCK_PCT'] = np.nan\n", "               SRI  MP_START  MP_END TRUCK_PCT\nindex                                         \n0       17031021__      0.00   0.100          \n1       03291236__      0.00   0.050          \n2      200006165__      0.00   0.120          \n3       00000009__     52.36  52.394          \n4       00000009__     51.78  52.360        7%\n5       00000009__     49.09  51.780        7%\n6       00000009__     48.76  48.760          \n7       00000015__     15.45  15.480        7%\n"], "link": "https://stackoverflow.com/questions/66997534/checking-segment-length-in-dataframe-1-against-multiple-segment-instances-in-dat"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to plot column number 0 vs all the other columns with a colour map. I have written in following with For loop. However, all the graphs appear separately and not on the same graph. Below, is the code I wrote, How to get all the plot of the same graph?", "q_apis": "get columns plot all columns map all get all plot", "apis": "columns columns columns plot columns columns columns columns plot columns", "code": ["from matplotlib import pyplot as plt\nimport pandas as pd\n\ncsv_file1 = pd.read_csv(r'file path')\n\nax = plt.gca()\ncolumns = csv_file1.columns\nfor col in columns[1:]:\n    csv_file1.plot.scatter(x=columns[0], y=col, c=col, colormap='viridis',\n        ax=ax, colorbar=False)\nplt.show()\n", "colors = [[plt.cm.viridis(i / len(columns))] for i in range(len(columns) - 1)]\nfor col, color in zip(columns[1:], colors):\n    csv_file1.plot.scatter(x=columns[0], y=col, c=color, ax=ax, colorbar=False)\nplt.show()\n"], "link": "https://stackoverflow.com/questions/59989956/plotting-multiple-columns-of-data-frame-with-color-map-on-the-same-graph"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have my dataframe - So basically I want the categories should be created as individual column i.e A, B, & C and each column should contain the counts of them. What I want as my output - What I was trying - please help me to get my desired output in python.", "q_apis": "get columns categories get", "apis": "rename_axis columns reset_index rename_axis columns reset_index replace reset_index pivot_table index groupby transform join columns values index count rename_axis columns reset_index", "code": ["df1 = pd.crosstab(df['emp_id'], df['category']).rename_axis(\n    columns=None).reset_index()\n", "  emp_id  A  B  C\n0    033  0  0  1\n1     12  2  0  0\n2   2233  1  0  0\n3    441  0  0  3\n4   6676  1  0  1\n5     91  0  1  0\n", "df = pd.crosstab(df['emp_id'],  df['category']).rename_axis(\n    columns=None).reset_index().replace(0, '')\n", "  emp_id  A  B  C\n0    033        1\n1     12  2      \n2   2233  1      \n3    441        3\n4   6676  1     1\n5     91     1   \n", "df = (\n    df.reset_index()\n    .pivot_table(\n        index=['emp_id', df.groupby('emp_id')['year'].transform(', '.join)],\n        columns='category',\n        values='index',\n        aggfunc='count',\n        fill_value=0)\n    .rename_axis(columns=None)\n    .reset_index()\n)\n", "  emp_id              year  A  B  C\n0    033              FY16  0  0  1\n1     12        FY18, FY14  2  0  0\n2   2233              FY21  1  0  0\n3    441  FY20, FY17, FY12  0  0  3\n4   6676        FY19, FY10  1  0  1\n5     91              FY15  0  1  0\n"], "link": "https://stackoverflow.com/questions/68139935/create-pivot-table-and-get-count-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to work on value from a pandas dataframe that are identified to be the same based on two columns in a dataframe. I could not find an efficient way for this purpose. Please see the following explanation of the code. The dataframe df below shows branch information that are connected between the first node in a and the second node in b. The first two rows of df shows branch that are connected with: (Nod1, Nod2) and (Node2, Nod1). These two are regarded the same branches, and I want to add the value in c for both of them: 0.15+0.152. I thought that I can do this by making a pairing of these two by using zip and work with these pairs (doesn't matter the sequence of node). However I could not come to a good way for this purpose without a loop. is there anyway to achieve my purpose? e.g., checking only the entry 'uniq' column but disregarding the sequence of node in it to acquire the value c.", "q_apis": "get columns value columns between first second first add value value", "apis": "get where apply groupby sum groupby sum", "code": ["# get 'a' and 'b' into a state where we can group them\ndf['ab'] = df.apply(lambda _: str(sorted([_['a'], _['b']])), axis=1)\n\n# groupby this new column and sum on c\ndf.groupby('ab')['c'].sum()\n", "ab\n['Nod1', 'Nod2']      0.302\n['Node1', 'Node5']    0.240\n['Node3', 'Node4']    0.230\nName: c, dtype: float64\n"], "link": "https://stackoverflow.com/questions/58271404/identifying-similar-data-in-a-multiple-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have few static key columns EmployeeId,type and few columns coming from first FOR loop. While in the second FOR loop if i have a specific key then only values should be appended to the existing data frame columns else whatever the columns getting fetched from first for loop should remain same. First For Loop Output: After Second For Loop i have below output: As per code if i have Employee tag available , i have got above 2 records but i may have few json files without Employee tag then output should remain same as per First Loop Output. But i am getting 0 records as per my code. Please help me if my way of coding is wrong. Really sorry -- If the way of asking question is not clear , as i am new to python . Please find the code in the below hyper link Please find below code If Employee Tag is not available i am getting 0 records as output but i am expecting 1 record as per output of first FOR loop. If the \"Employee tag\" is available then i am expecting 2 records along with my static columns \"EmployeeId\",\"type\",\"KeyColumn\",\"Start\",\"End\", else if the tag is not available then all the static columns \"EmployeeId\",\"type\",\"KeyColumn\",\"Start\",\"End\", and remaining columns as blanks enter link description here", "q_apis": "get columns columns columns first second values columns columns first first columns all columns columns", "apis": "DataFrame max items keys append DataFrame keys drop_duplicates", "code": ["df = pd.DataFrame()\n\nnum = max([len(v) for k,v in json_file['data'][0]['data1'].items()])\nfor i in range(num):\n    temp = {}\n    temp['Empid'] = json_file['data'][0]['Empid']\n    temp['Empname'] = json_file['data'][0]['Empname']\n    for key in json_file['data'][0]['data1'].keys():\n        if key not in temp:\n            temp[key] = []\n        try:\n            for j in range(len(json_file['data'][0]['data1'][key])):\n                temp[key].append(json_file['data'][0]['data1'][key][j]['relative']['id']) \n        except:\n            temp[key] = None                    \n    temp_df = pd.DataFrame([temp])\n    df = pd.concat([df, temp_df],ignore_index=True)\nfor i in json_file['data'][0]['data1'].keys():\n    df[i] = pd.Series([x for y in df[i].tolist() for x in y]).drop_duplicates()\n", "  Empid Empname    XXXX   YYYYY\n0  1234     ABC  Naveen   Kumar\n1  1234     ABC     NaN  Rajesh\n"], "link": "https://stackoverflow.com/questions/56389351/how-to-parse-multi-index-values-and-create-a-csv-file-while-parsing-json-data-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to \"convert\" the information of the dataframe rows 0 to 15 and columns col1 to col 16 into an image (16x16) I'm reading the dataframe from a .txt file: After creating an empty matrix , I want to iterate over the dataframe to transfer the columns information. For that, I thought using but I'm having problems in filling the brackets. Can you provide any advice? Thanks.", "q_apis": "get columns columns empty columns any", "apis": "iloc filter iloc", "code": ["#import numpy as np\nnp.resize(df.iloc[:16, -16:].to_numpy(), (16, 16, 3))\n", "np.resize(df.filter(regex='col').iloc[:16, :].to_numpy(), (16, 16, 3))\n"], "link": "https://stackoverflow.com/questions/64757025/transfer-some-dataframe-information-to-a-new-matrix"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The following data frame is used as input: The exercise requires you to compute the mean of the column for each moment in time (). However, the current observation should not be included in the mean. For instance, the first observation (index=0) gets since there are no observations apart from the one we're calculating the mean for. The second observation (index=1) gets 1 since 1/1 = 1 (0 from the second observation is not included). The third observation (index=2) gets 0.5 since (1+0)/2=0.5. My code provides a correct answer (in terms of numbers) but is not elegant. I wonder whether you can complete the exercise with something different. Is it possible to use the or and then method? My solution: Full disclaimer: The exercise was a part of a recruitment process a month ago. The recruitment is now closed and I can't submit code anymore.", "q_apis": "get columns mean time mean first index mean second index second index month now closed", "apis": "copy rolling mean", "code": ["from pandas.api.indexers import BaseIndexer\n\nclass CustomIndexer(BaseIndexer):\n    \n    def get_window_bounds(self, num_values, min_periods, center, closed):\n        \n        start = np.zeros(num_values, dtype='int64')\n        end = np.arange(0, num_values, dtype='int64')\n        \n        return start, end  \n    \nindexer = CustomIndexer(window_size=0)\n\ndf_expanded = df.copy(deep=True)\n\ndf_expanded = df_expanded.rolling(indexer).mean()\n"], "link": "https://stackoverflow.com/questions/63764458/pandas-aggregate-values-with-a-variable-length-rolling-window"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a csv file with data from multiple sensors, like this: And I want to transform that on a pandas df with a column for each sensor, with datetime as index, like this: There's an easy way to do that on pandas?", "q_apis": "get columns transform index", "apis": "set_index apply loc", "code": ["df.set_index(df.apply(lambda row: datetime(int(row[\"YY\"]), int(row[\"mm\"]), int(row[\"dd\"]), int(row[\"HH\"]), int(row[\"MM\"])), axis=1)).loc[:, [\"sensor1value\", \"sensor2value\", \"sensor7value\"]]\n"], "link": "https://stackoverflow.com/questions/64883751/grouping-rows-with-same-date-pandas-df"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to make unit-test that deals with csv files using python framework. I want to test such cases as columns names match, values in columns match, etc. I know that there are more convenient libraries for it, like and , but I can use only in my project. Guess I'm using wrong methods, and send data in the wrong format. Please advise how to do it better way. db.csv example: Here is code example: Errors: Any help is appreciated.", "q_apis": "get columns test test columns names values columns", "apis": "columns all", "code": ["import pandas as pd\nimport unittest\n\ncolnames = [\"TIMESTAMP\", \" TYPE\", \" VALUE\", \" YEAR\", \" FILE\", \" SHEET\"]\nyears = set([2018, 2019, 2020])\n\n\nclass DfTests(unittest.TestCase):\n    def setUp(self):\n        try:\n            data = pd.read_csv(\"data.csv\", sep=\",\")\n            self.fixture = data\n        except IOError as e:\n            print(e)\n\n    def test_colnames(self):\n        self.assertListEqual(list(self.fixture.columns), colnames)\n\n    def test_timestamp_format(self):\n        ts = self.fixture[\"TIMESTAMP\"]\n        # You need to check for every row in the dataframe\n        [self.assertRegex(i, r\"\\d{2}-\\d{2}-\\d{4}\") for i in ts]\n\n    def test_years(self):\n        df_years = self.fixture[\" YEAR\"]\n        self.assertTrue(all([i in years for i in df_years]))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"], "link": "https://stackoverflow.com/questions/64242014/testing-pandas-dataframe-with-unittest-framework"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I tried to define a function to append all the the data I have into one dataframe. The output appears to be an empty dataframe. Empty DataFrame; Columns: []; Index: []. The code works if consider the variables frame and df as lists and appends up to a large list. But I want a dataframe with all the data under the same column heads. What am I doing wrong?", "q_apis": "get columns append all empty DataFrame Index all", "apis": "DataFrame DataFrame columns append", "code": ["frame = pd.DataFrame()\nfor i in range (1995,2020):\n  file_name = f\"{i}\"\n  df = pd.read_csv(BytesIO(uploaded[\"%s.csv\"%file_name]))\n  df = pd.DataFrame(data, columns= ['DATE','ARANGALI'])\n  frame = frame.append(df)\nprint(frame)\n"], "link": "https://stackoverflow.com/questions/66672668/problem-with-appending-values-into-a-empty-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have below dataframe structure as a sample. I want to obtain a column where it calculates the percentile of the \"price column\" based on the value of the \"percentile\" column, using a rolling n period lookback. Is it possible? I tried using some kind of a lambda function and use the .apply syntax but couldn't get it to work. Thanks!!", "q_apis": "get columns sample where value rolling apply get", "apis": "DataFrame rolling mean", "code": ["df = pd.DataFrame({'B': [0, 1, 2, 2, 4]})\ndf['rolling_mean'] = df['B'].rolling(2).mean()\n"], "link": "https://stackoverflow.com/questions/58862991/dataframe-percentile-using-a-rolling-value-from-another-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes like below. I using pandas and numpy to compare differences. I am using below code for merging My intention to output compare both df and any missing items on both side should come to output. Actual output below: but problem is i want to display Key from both dataframes, but if you see below output its showing only once, i.e i need Key_y also to be part of output. Expected output: I wanted to display Key from both", "q_apis": "get columns compare compare any items", "apis": "merge add_suffix add_suffix diff where eq", "code": ["df = pd.merge(\n    df_b.add_suffix('_x'), df_a.add_suffix('_y'), \n    left_on='Key_x', right_on='Key_y', how='outer')\n\ndf['diff'] = np.where(df['Value_x'].eq(df['Value_y']), 'No', 'Yes')\n", "# print(df)\n          Key_x      Value_x         Key_y    Value_y diff\n0    data_owner         John    data_owner       John   No\n1  locationcode        local  locationcode      local   No\n2          Unit        sales          Unit      sales   No\n3   application    autosales   application  autosales   No\n4    department  frontoffice           NaN        NaN  Yes\n"], "link": "https://stackoverflow.com/questions/62449902/merging-two-data-frames-not-getting-key-column-on-both-dataframes-with-full-oute"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "here is my code: here i am matching my csv file columns with holder list, how to create a missing column from my csv file in my df but in the correct order as my holder list? output file looks like this: |S00045792|||SIRSI|SIRSI||.STREET.Block 3 Street 1|.CITY/STATE.Mishref||.EMAIL.s00045792@test.com|||||||||||||ARAB|||MAIN|CHECKEDOUT|STAFF||||1234||||||||||||||Fatema||Al-Mutawa||||||||||||||||||||| input file: so if the 'title' column is missing from my csv file, how to create it in the df in the exact same order? thanks", "q_apis": "get columns columns test", "apis": "columns", "code": ["for col_name in holder:\n    if col_name not in df.columns:\n        df[col_name] = ''\n"], "link": "https://stackoverflow.com/questions/64691669/how-to-create-a-missing-column-in-a-dataframe-and-place-it-in-the-correct-order"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to to find a solution how I could change my pandas dataframe. I have a dataset with news headlines. There are multiple headlines per day. I would like to have the date(day) as rows and each headline gets assigned to a new column per day. In other words I would like to combine all the headline data for each date. Instead of having a separate headline for each day. Some sort of pandas custom aggregator could do the job, but I'm struggling to come up with one. I was able to group the data by date but now all the headlines per day are in the same column. and not in separate columns. (see picture 2) I have been looking for a solution for a while now but without any luck. I attached 3 pictures. The first picture shows what my df looked like originally. The third picture shows an example how I would like the df to look like.", "q_apis": "get columns day date day day combine all date day date now all day columns now any first", "apis": "groupby agg join groupby apply apply reset_index columns rename columns", "code": ["df.groupby(['Date'], as_index = True).agg({'headline': ','.join})['headline'].str.split(',', expand=True)\n", "    0   1   2\nDate            \nd1  h1  h2  h3\nd2  h4  h5  None\n", "    Date    0   1   2\n0   d1      h1  h2  h3\n1   d2      h4  h5  NaN\n", "df2 = df.groupby('Date', as_index=True)['headline'].apply(list).apply(pd.Series).reset_index()\nnew_col_names = {n:f'Top{n}' for n in range(len(df2.columns))}\ndf2.rename(columns = new_col_names, inplace = True)\ndf2\n", "Date    Top0    Top1    Top2\n0 d1    h1      h2      h3\n1 d2    h4      h5      NaN\n"], "link": "https://stackoverflow.com/questions/64776596/sort-pandas-by-date-custom-aggregator-combine-all-the-data-for-each-date"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to return the percentage rounded to one decimal place from a database using pandas. my code consists of: which returns: 17.3713601914639 The correct answer is 17.4 so I just need to round it up If I use : I get: How can I round it up one decimal place?", "q_apis": "get columns round get round", "apis": "loc shape loc shape round", "code": ["number = df.loc[((df['education-num'] < 13)|(df['education'] == 'Prof-school')) & (df['salary'] == '>50K')].shape[0]/ df.loc[(df['education-num'] < 13)| (df['education'] == 'Prof-school')].shape[0]*100\n\nrounded = round(number,1)\n"], "link": "https://stackoverflow.com/questions/66398942/how-do-i-round-off-a-float-object"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Movie Dataframe I have a DataFrame that contains movie information and I'm trying to filter the rows so that if the list of dictionaries contains 'name' == 'specified genre' it will display movies containing that genre. I have tried using a list comprehension however I end up with an error: TypeError: string indices must be integers", "q_apis": "get columns DataFrame contains filter contains name indices", "apis": "DataFrame DataFrame to_dict", "code": ["import pandas as pd\n\ndf = pd.DataFrame({\"abc\": [1,2,3], \"def\": [4,5,6]})\nfor d in df:\n    print(d)\n", "df = pd.DataFrame({\"genre\": [\"something\", \"soemthing else\"], \"abc\": [\"movie1\", \"movie2\"]})\n\nmovies = df.to_dict(\"records\")\n\n[m[\"abc\"] for m in movies if m[\"genre\"] == \"something\"]\n"], "link": "https://stackoverflow.com/questions/64253315/filtering-a-pandas-dataframe-through-a-list-dictionary"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given a pandas dataframe containing possible NaN values scattered here and there: Question: How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?", "q_apis": "get columns values columns values get names", "apis": "columns isna any isnull any bool columns isnull any", "code": ["In [71]: df\nOut[71]:\n     a    b  c\n0  NaN  7.0  0\n1  0.0  NaN  4\n2  2.0  NaN  4\n3  1.0  7.0  0\n4  1.0  3.0  9\n5  7.0  4.0  9\n6  2.0  6.0  9\n7  9.0  6.0  4\n8  3.0  0.0  9\n9  9.0  0.0  1\n\nIn [72]: df.isna().any()\nOut[72]:\na     True\nb     True\nc    False\ndtype: bool\n", "In [74]: df.columns[df.isna().any()].tolist()\nOut[74]: ['a', 'b']\n", "In [73]: df.loc[:, df.isna().any()]\nOut[73]:\n     a    b\n0  NaN  7.0\n1  0.0  NaN\n2  2.0  NaN\n3  1.0  7.0\n4  1.0  3.0\n5  7.0  4.0\n6  2.0  6.0\n7  9.0  6.0\n8  3.0  0.0\n9  9.0  0.0\n", "In [97]: df\nOut[97]:\n     a    b  c\n0  NaN  7.0  0\n1  0.0  NaN  4\n2  2.0  NaN  4\n3  1.0  7.0  0\n4  1.0  3.0  9\n5  7.0  4.0  9\n6  2.0  6.0  9\n7  9.0  6.0  4\n8  3.0  0.0  9\n9  9.0  0.0  1\n\nIn [98]: pd.isnull(df).sum() > 0\nOut[98]:\na     True\nb     True\nc    False\ndtype: bool\n", "In [5]: df.isnull().any()\nOut[5]:\na     True\nb     True\nc    False\ndtype: bool\n\nIn [7]: df.columns[df.isnull().any()].tolist()\nOut[7]: ['a', 'b']\n", "In [31]: df.loc[:, df.isnull().any()]\nOut[31]:\n     a    b\n0  NaN  7.0\n1  0.0  NaN\n2  2.0  NaN\n3  1.0  7.0\n4  1.0  3.0\n5  7.0  4.0\n6  2.0  6.0\n7  9.0  6.0\n8  3.0  0.0\n9  9.0  0.0\n"], "link": "https://stackoverflow.com/questions/36226083/how-to-find-which-columns-contain-any-nan-value-in-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following df: Sorry that I can't make the headers appear but they are as in this image: I'm stuck in this step and I'll really appreciate your help! I need a code that for each subject ID looks at the Description column, if DTI, T1 and FLAIR are present in a single Visit take that visit and delete the rest, if they are present in multiple Visits take the Visit with the minimum value and delete the rest. If DTI, T1 and FLAIR are not present in a single visit delete also those rows. What I need is for each Subject ID get the minimum Visit value that has the three values in Description (DTI, T1 and FLAIR) and delete the rest My output would look something like this: Thank you!", "q_apis": "get columns step at take delete take value delete delete get value values delete", "apis": "reset_index groupby transform groupby transform min eq set_index append", "code": ["# Remove Description and Visit from MultiIndex\nnew_df = df.reset_index(['Visit', 'Description'])\n# Create Set of Values to Check against\ncheck_values = {'DTI', 'FLAIR', 'T1'}\n# Create Boolean Index\nm = (\n        new_df.groupby(level=[0, 1])['Description'].transform(\n            lambda g: set(g) == check_values and len(g) == len(check_values)\n        )\n        & new_df.groupby(level=0)['Visit'].transform('min').eq(new_df['Visit'])\n)\n\n# Filter Dataframe with Index and Fix MultiIndex\nnew_df = new_df[m].set_index(['Visit', 'Description'], append=True)\n", "set(g) == check_values and len(g) == len(check_values)\n", "                                         Modality  Phase\nSubject ID Study Date Visit Description                 \n002_S_0413 6/21/2017  1     DTI                 1      1\n                            FLAIR               1      1\n                            T1                  1      1\n002_S_1261 3/15/2017  1     DTI                 1      1\n                            FLAIR               1      1\n                            T1                  1      1\n"], "link": "https://stackoverflow.com/questions/67747201/filter-a-dataframe-meeting-some-criteria"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There are duplicated transactions in a bank dataframe(DF). ID is customer IDs. Duplicated transaction is a multi-swipe, where a vendor accidentally charges a customer's card multiple times within a short time span (2 minutes here). I want to add a column to my dataframe, which recognizes the duplicated transactions (dollar amount of same customer ID should be the same, and transaction date time should be less than 2 minutes). Please consider the first transaction to be \"normal\".", "q_apis": "get columns duplicated where time add duplicated date time first", "apis": "sort_values groupby diff lt", "code": ["df['Duplicated?'] = (df.sort_values(['transactionDateTime'])\n                       .groupby(['ID', 'Dollar'], sort=False)['transactionDateTime']\n                       .diff()\n                       .dt.total_seconds()\n                       .lt(120))\ndf\n\n     ID  Dollar transactionDateTime  Duplicated?\n0   111       1 2016-01-08 19:04:50        False\n1   111       3 2016-01-29 19:03:55        False\n2   111       1 2016-01-08 19:05:50         True\n3   111     100 2016-01-08 20:08:50        False\n4   222      25 2016-01-08 19:04:50        False\n5   222       8 2016-02-08 19:04:50        False\n6   222      25 2016-03-08 19:04:50        False\n7   333       9 2016-01-08 19:04:50         True\n8   333      20 2016-03-08 19:05:53        False\n9   333       9 2016-01-08 19:03:20         True\n10  333       9 2016-01-08 19:02:15        False\n11  111     100 2016-02-08 20:08:50        False\n"], "link": "https://stackoverflow.com/questions/56517004/mark-duplicates-based-on-time-difference-between-successive-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here's the thing, I have the dataset below where date is the index: And I want to transform it in this other dataset: First I thought about using pandas.pivot_table to do something, but that would just provide a different layout grouped by some column, which is not exactly what I want. Later, I thought about using pandasql and apply 'case when', but that wouldn't work because I would have to type dozens of lines of code. So I'm stuck here.", "q_apis": "get columns where date index transform pivot_table apply", "apis": "DataFrame shift", "code": ["new_df = pd.DataFrame({f\"value_t{i}\": df['value'].shift(i) for i in range(len(df))})\n", "{column name: column data, ...}"], "link": "https://stackoverflow.com/questions/64124732/whats-the-most-efficient-way-to-convert-a-time-series-data-into-a-cross-section"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to reshape a table in pandas . I have a table of the form: I want to turn it to this shape: To explain, the initial data has pop and numbers by country, state for a date range. Now I want to convert into a number of columns of time series for each level (country, country-state) How do I do this?", "q_apis": "get columns shape pop date columns time", "apis": "pop pop iloc pop apply apply merge", "code": ["    date country state  population  num_cars\n0   1990     Xxx   Aaa         100        15\n1   2010     Xxx   Aaa         120        18\n2   1990     Xxx   Bbb          80         9\n3   2010     Xxx   Bbb          88        11\n4   1990     Xxx   Ccc          75         6\n5   2010     Xxx   Ccc          82         8\n6   1990     Yyy   Ggg          40         5\n7   2010     Yyy   Ggg          50         6\n8   1990     Yyy   Hhh          30         3\n9   2010     Yyy   Hhh          38         4\n10  1990     Yyy   Jjj          29         3\n11  2010     Yyy   Jjj          35         4\n", "def reformat(grp, col):\n    pop = grp[col]\n    pop.name = grp.date.iloc[0]\n    return pop\n", "df1 = gr.apply(reformat, col='population')\ndf2 = gr.apply(reformat, col='num_cars')\n", "pd.merge(df1, df2, left_index=True, right_index=True,\n    suffixes=('_pop', '_cars'))\n", "country Xxx_pop         Yyy_pop         Xxx_cars         Yyy_cars        \nstate       Aaa Bbb Ccc     Ggg Hhh Jjj      Aaa Bbb Ccc      Ggg Hhh Jjj\ndate                                                                     \n1990        100  80  75      40  30  29       15   9   6        5   3   3\n2010        120  88  82      50  38  35       18  11   8        6   4   4\n"], "link": "https://stackoverflow.com/questions/58124558/reshaping-table-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Basically, I got a table like the following: Let's say this is a standard table and I will transform that into a Pandas DF, using the groupby function just like that: After the dataframe is created I want to delete all the rows where frequencies of all other sports than Soccer are greater than Soccer frequency. So I need to run following conditions: Identify where Soccer is present; and then If so, identify if there is any other sport present; and finally Delete rows where sport is any other than Soccer and its frequency is greater than the Soccer frequency associated to that name (used in the function). So, the output would be something like: Thank you for your support", "q_apis": "get columns transform groupby delete all where all where any where any name", "apis": "assign groupby transform mean query iloc", "code": ["pd.concat(\n    [\n        value.assign(temp=lambda x: x.loc[x.Sport == \"Soccer\", \"Frequency\"])\n        .bfill()\n        .ffill()\n        .query(\"Frequency <= temp\")\n        .drop('temp', axis = 1)\n        for key, value in df.groupby(\"Name\").__iter__()\n    ]\n)\n\n\n    Name    Sport   Frequency   \n7   John    Soccer     1        \n0   Jonas   Soccer     3        \n3   Mathew  Soccer     2        \n4   Mathew  Tennis     1        \n", "sport_dtype = pd.api.types.CategoricalDtype(categories=df.Sport.unique(), ordered=True)\ndf = df.astype({\"Sport\": sport_dtype})\n\n(\n    df.sort_values([\"Name\", \"Sport\"], ascending=[False, True])\n    .assign(temp=lambda x: x.loc[x.Sport == \"Soccer\", \"Frequency\"])\n    .ffill()\n    .query(\"Frequency <= temp\")\n    .drop('temp', axis = 1)\n)\n\n    Name    Sport   Frequency   \n3   Mathew  Soccer      2       \n4   Mathew  Tennis      1       \n0   Jonas   Soccer     3        \n7   John    Soccer     1        \n", "index = (\n    df.assign(temp=lambda x: x.loc[x.Sport == \"Soccer\", \"Frequency\"])\n    .groupby(\"Name\")\n    .pipe(lambda x: x.ffill().bfill())\n    .query(\"Frequency <= temp\")\n    .index\n)\n\ndf.loc[index]\n\n    Name    Sport   Frequency\n0   Jonas   Soccer  3\n3   Mathew  Soccer  2\n4   Mathew  Tennis  1\n7   John    Soccer  1\n", "(df.assign(temp=df.Sport == \"Soccer\",\n           temp2=lambda x: x.groupby(\"Name\").temp.transform(\"mean\"),\n           )\n   .query('Sport==\"Soccer\" or temp2>=0.5')\n   .iloc[:, :3]\n)\n"], "link": "https://stackoverflow.com/questions/63236768/how-to-delete-row-in-pandas-dataframe-using-2-colums-as-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This question is referring to the previous post The solutions proposed worked very well for a smaller data set, here I'm manipulating with 7 .txt files with a total memory of 750 MB. Which shouldn't be too big, so I must be doing something wrong in the process. This is how one of my dataframes (df1) look like - head: And tail: I followed a suggestion and dropped duplicates: etc. Similarly df2 has , df3 etc. The solution is modified according to one of the answers from the previous post. The aim is to create a new, merged DataFrame with all (of each dfX) as additional columns to the depth, profile and other 3 ones, so I tried something like this: The current error is: ValueError: cannot handle a non-unique multi-index! What am I doing wrong?", "q_apis": "get columns head tail DataFrame all columns unique index", "apis": "assign groupby set_index reset_index", "code": ["grp_cols = ['depth', 'name_profile', 'year', 'month', 'day']\n\ndfs = [(df.assign(grp_count = df.groupby(grp_cols).cumcount())\n          .set_index(grp_cols + ['grp_count'])\n       ) for df in [df1, df2, df3, df4, df5, df6, df7]]\n\ndf_merged = pd.concat(dfs, axis=1).reset_index()\n\nprint(df_merged)\n"], "link": "https://stackoverflow.com/questions/55656207/merge-multiple-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "For multiple files in a folder, I hope to loop all files ends with and merge as one excel file, here I give two examples: first.csv second.csv ... My desired output is like this, merging them based on : I have edited the following code, but obviously there are some parts about convert and merge are incorrect: Please help me. Thank you.", "q_apis": "get columns all merge first second merge", "apis": "DataFrame columns columns replace dropna dropna astype empty copy merge", "code": ["import numpy as np\nimport pandas as pd\nimport glob\nfrom pandas.tseries.offsets import MonthEnd\n\ndfs = pd.DataFrame()\nfor file_name in glob.glob(\"*.csv\"):\n    df = pd.read_csv(file_name, engine='python', skiprows=2, encoding='utf-8')\n    df.columns = df.columns.str.lower().str.replace('dates', 'date')\n    df = df.dropna()\n    df = df.dropna(axis = 1)\n    df['date'] = pd.to_datetime(df['date'].astype(str), format='%Y.%m') + MonthEnd(1)\n    if dfs.empty:\n        dfs = df.copy()\n    else:\n        dfs = dfs.merge(df, on='date', how=\"outer\")\n"], "link": "https://stackoverflow.com/questions/58537324/loop-excel-files-and-merge-based-on-one-common-column-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to create a column from a another column in dataframe : with code based on these posts: post1 post2 but incurred error: What is the best way to create a column from column ? Update: Tried which returned then tried which returned Why is this happening?", "q_apis": "get columns", "apis": "apply", "code": ["df['Weekday'] = pd.to_datetime(df['Date']).apply(lambda x : x.weekday())\n"], "link": "https://stackoverflow.com/questions/61002631/convert-dataframe-column-to-string-to-apply-strftime"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "There is a dataframe let's say: I would like to apply a function to the price column. Something like this : So basically, if of a product is lower than a certain value, the function has to update the until net profit margin is greater than the certain value. The problem that I am facing is an infinite loop. There seems to be is not updating for each iteration. Could be because of not returning the value after calculation but I have no idea how to do it. The actual dataset is complex but I tried to simplified. Hope it is easy to understand. Here some additional details: Note: update_columns() function uses current columns and adds new results to the end of the dataframe as new columns. Most of the time many retailers put their prices by hand. I am trying to prevent if someone puts very low numbers, I will calculate a new price according to a rate and correct it. So they don't lose money", "q_apis": "get columns apply product value update value value columns columns time put", "apis": "apply astype pipe", "code": ["def new_func(df):\n     return df['Price'] * df['Cost'] \n\n\ndef update_columns(df):\n  df[\"New Amount\"] = df.apply(new_func,axis=1).astype(float)\n  print(\"Columns updated succesfully!\")\n  return df\n\n\ndef update_price(df):\n   \n    df['Price'] = df['Price']*df['Certain Value'] + df['Cost']\n    # This function updates the profit margin and cost depended on new price.\n    df = df.pipe(update_columns) \n    return df\n", "mask =  df[\"New Net Profit Margin\"] < df[\"Certain Value\"]\ndf1 = df[mask].copy().pipe(update_price)\nprint (df1)\n  Product  New Net Profit Margin  Certain Value  Cost  Price  New Amount\n1       B                     12             40     5    685      3425.0\n2       C                     13             20     6    246      1476.0\n", "df = df.reindex(df1.columns, axis=1)\nprint (df)\n  Product  New Net Profit Margin  Certain Value  Cost  Price  New Amount\n0       A                     50             10    10     20         NaN\n1       B                     12             40     5     17         NaN\n2       C                     13             20     6     12         NaN\n", "df.update(df1)\nprint (df)\n  Product  New Net Profit Margin  Certain Value  Cost  Price  New Amount\n0       A                   50.0           10.0  10.0   20.0         NaN\n1       B                   12.0           40.0   5.0  685.0      3425.0\n2       C                   13.0           20.0   6.0  246.0      1476.0\n"], "link": "https://stackoverflow.com/questions/63591027/pandas-apply-a-function-to-a-column-while-the-condition-is-valid-deeper-versio"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Let's suppose we have such dataframe: is unique key I want to make values to become columns split or to cross-tabulate on them and finally to looks something like this: First naive approach didn't work here well: This code failed to run with How get it work?", "q_apis": "get columns unique values columns get", "apis": "pivot_table index columns max swaplevel sort_index groupby max unstack swaplevel sort_index", "code": ["df = (df.pivot_table(index='key', columns='col', aggfunc=np.max)\n       .swaplevel(0,1,axis=1)\n       .sort_index(axis=1))\n", "df = (df.groupby(['key', 'col'])\n        .max()\n        .unstack()\n        .swaplevel(0,1,axis=1)\n        .sort_index(axis=1))\n", "print (df)\ncol           A                             B                             C  \\\n           val1      val2      val3      val1      val2      val3      val1   \nkey                                                                           \nfour  -0.225967  0.362041  0.040915 -1.227718 -0.879248 -1.279912 -1.577218   \none   -0.187167  1.530731 -1.112116 -0.871077 -2.099876 -0.069297 -0.351971   \nthree -0.165375 -0.378049 -0.390724  0.484519 -0.408990 -1.496042  0.590083   \ntwo    1.923084 -0.688284  1.702659 -0.159921  0.635245  0.623821 -1.503893   \n\ncol                        \n           val2      val3  \nkey                        \nfour  -1.135872  0.645371  \none    2.347472  0.129252  \nthree  0.402825  0.883710  \ntwo   -0.132847  0.179476  \n"], "link": "https://stackoverflow.com/questions/53764465/how-to-transpose-pandas-dataframe-to-cross-tabulate-dataframe-keeping-all-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a text file where all columns are merged into a single column and 'rows' are separated by two long rows of '-'. It looks like this: expected output for this example is is a dataframe with just 3 rows. dataframe columns: Hash (str), Author (str), Message (str), Reviewers (str), Reviewed By (str), Test Plan (str), Commit Date (timestamp), Modified path (array(str))", "q_apis": "get columns where all columns columns timestamp array", "apis": "DataFrame sub", "code": ["pd.DataFrame([ collections.OrderedDict(\n    { m.group('key').strip(): re.sub(r'\\n', ' ', m.group('val').strip())\n        for m in re.finditer(\n            r'^(?P<key>[^:\\n]+):\\s*(?P<val>.+?(?:\\n[^:\\n]+)*)$', chunk, re.M)})\n    for chunk in re.split(r'(?:\\n\\-+)+\\n', txt) ])\n"], "link": "https://stackoverflow.com/questions/57383067/parsing-non-csv-text-file-into-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataset which I need to group by on single field and aggregate on multiple fields. As part of aggregate, I need to concatenate a string column values in a sorted order conditionally. Input: Output expected: I have the following code: The issue is with string concatenation, I want to concatenate TABLE values where cat_a=1 and also sorted order. Currently I am getting A;B;C for minute 00:00 but expect only A;B where cat_a=1 Is there a way to add condition to the join function? P.S: I am new to python, I did see similar questions but I want specifically to add condition inside an agg function", "q_apis": "get columns aggregate aggregate values values where minute where add join add agg", "apis": "groupby agg first agg max sum sum sum sum merge copy groupby agg join apply groupby agg max sum join sum sum sum apply sub", "code": ["df_table_acc=df.groupby(['SYSTIME'],as_index=False).agg(    #Remove TABLE from first agg\n            {'TT' : 'max','REC' : 'sum', 'cat_a': 'sum', 'cat_b': 'sum', 'cat_c': 'sum'})\ndf_table_acc = pd.merge(df_table_acc, df[df['cat_a']>0].copy().groupby(['SYSTIME'],as_index=False).agg(\n            {'TABLE':';'.join}),how='left',on='SYSTIME')\n", "import re\ndf['TABLE'] = df.apply(lambda x: x['TABLE'] if x['cat_a']>0 else '', axis=1)\ndf_table_acc=df.groupby(['SYSTIME'],as_index=False).agg(\n            {'TT' : 'max','REC' : 'sum','TABLE': ';'.join, \n             'cat_a': 'sum', 'cat_b': 'sum', 'cat_c': 'sum'})\ndf_table_acc.TABLE = df_table_acc.TABLE.apply(lambda x: re.sub(';+',';',x).strip(';'))\n#Quick explanation: the re part avoids having repeat \";\" eg: \"A;;C;D;;G\" -> \"A;C;D;G\"\n#The strip removes outside strings eg: \";A;B;\" -> \"A;B\"\n"], "link": "https://stackoverflow.com/questions/59422882/conditionally-concatenate-strings-within-a-groupby-aggregate-function"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following dataframe consisting of the number of posts made by a user in a duration of 2 weeks (days from -7 to 7). I want to create another dataframe that should have the mean number of posts made per day. I have written the following code but it returns me a series with 1 column instead of a Dataframe. The required Dataframe should have 2 separate columns for and . Part of Dataframe (df) CODE (To obtain mean posts per day) Code Output This output is correct, the only problem is that it is a series. The expected output should have 2 separate columns for and as shown. Expected Output", "q_apis": "get columns days mean day columns mean day columns", "apis": "mean", "code": ["df1 = df.iloc[:,2:].mean().rename_axis('day').reset_index(name='mean')\nprint (df1)\n   day      mean\n0   -7  0.000000\n1   -6  0.333333\n2   -5  0.333333\n3   -4  0.333333\n4   -3  0.666667\n5   -2  0.000000\n6   -1  1.000000\n7    0  0.666667\n8    1  0.000000\n9    2  0.333333\n10   3  0.666667\n11   4  0.666667\n12   5  0.333333\n13   6  1.000000\n14   7  0.333333\n", "sns.lineplot(data=df1, x = 'day', y = 'mean', err_style=\"bars\",ci=68)\n"], "link": "https://stackoverflow.com/questions/64312503/how-to-aggregate-values-of-a-dataframe-by-mean-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My input data is this- Which when printed appears as this: I am trying to get something like this - I am not sure how to get here. I did try the groupby function which got me to this- I am not sure how to proceed after this, any hints would be of great help.", "q_apis": "get columns get get groupby any", "apis": "set_index bool max reset_index assign groupby first unstack reset_index rename_axis", "code": ["df = (pd.get_dummies(df.set_index(['City','CityID'])['Name'], dtype=bool)\n        .max(level=[0,1])\n        .reset_index())\nprint (df)\n       City CityID  Alice   Bob  Jake  Steve\n0   Seattle      1   True  True  True   True\n1  Portland      2  False  True  True  False\n", "df = (df.assign(a=True)\n        .groupby(['City','CityID', 'Name'], sort=False)['a']\n        .first()\n        .unstack(fill_value=False)\n        .reset_index()\n        .rename_axis(None, axis=1))\nprint (df)\n       City CityID  Alice   Bob  Jake  Steve\n0   Seattle      1   True  True  True   True\n1  Portland      2  False  True  True  False\n"], "link": "https://stackoverflow.com/questions/59224854/converting-pandas-groupby-to-a-dataframe-having-columns-with-boolean-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to set the entire rows to a value from a vector, if a condition in on column is met. I want the result to be like this: I tried this but it gives me error: // m.", "q_apis": "get columns value", "apis": "loc select_dtypes columns", "code": ["df.loc[df['one']=='b', ['two', 'three']] = vector[df['one']=='b']\nprint(df)\n  one  two  three\n0   a    1      1\n1   a    1      1\n2   b    4      4\n", "m = df['one']=='b'\ndf.loc[m, df.select_dtypes(np.number).columns] = vector[m]\n"], "link": "https://stackoverflow.com/questions/58268462/select-rows-based-on-condition-and-set-values-from-a-vector"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Having the following running code: I am getting the dataframe below. I want to get the count of the total rows, with 1 or more NaN, which in my case is 4, on rows - .", "q_apis": "get columns get count", "apis": "isna isna any bool", "code": ["print (df.isna())\n    TIME  FUNDS   x**2\n0  False   True   True\n1  False  False  False\n2  False  False   True\n3  False   True  False\n4  False   True  False\n5  False  False  False\n", "print (df.isna().any(axis=1))\n0     True\n1    False\n2     True\n3     True\n4     True\n5    False\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/60096370/count-number-of-rows-with-nan-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with 20 columns, and 3 of those columns (always the same) may contain one or more of these strings [\"fraction\", \"fractional\", \"1/x\", \"one fifth\"]. I want to add a new column that says whether or not each row is \"fractional\" (in other words, contains one of those words). This column could have Y or N to indicate this. I've tried to do it with , like so: however, I'm trying to understand if there is a more / efficient way to do so. Something like: as described in this SO question, but using a list and different headers.", "q_apis": "get columns columns columns add contains", "apis": "DataFrame columns join", "code": ["import pandas as pd\nimport numpy as np\n\nkeywords = [\"fraction\", \"fractional\", \"1/x\", \"one fifth\", \"Fraction\"]\nnp.random.seed(45)\ndf = pd.DataFrame(np.random.choice(keywords+list('abcdefghijklm'), (4,3)),\n                  columns=['HeaderX', 'HeaderY', 'HeaderZ'])\n", "pat = '|'.join(keywords)\n\ndf['Fractional?'] = np.logical_or.reduce([df[col].str.contains(pat) \n                                          for col in ['HeaderX', 'HeaderY', 'HeaderZ']])\n", "      HeaderX    HeaderY   HeaderZ  Fractional?\n0           g  one fifth  fraction         True\n1   one fifth   Fraction         k         True\n2  fractional          j         d         True\n3           j          d         h        False\n"], "link": "https://stackoverflow.com/questions/62347069/pandas-create-column-conditioned-on-row-containing-a-string-in-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to add a single cell to df but before column names(see image below/Example). Is it possible via pandas? Had two ideas, to merge/concat/append two df vertically, of which one would only be a single cell or to a single cell, but no luck. Example", "q_apis": "get columns add names merge concat append", "apis": "DataFrame get reindex columns", "code": ["mux = pd.MultiIndex.from_product([['Panel Schedule - Manufacturing'], \n                                  ['Construction Type', 'Panel Type']])\ndf = (pd.DataFrame({'Construction Type': [combo2.get()],\n                   'Panel Type': [int(upis_tickness)]})\n        .reindex(columns=mux, level=1))\n", "mux = pd.MultiIndex.from_product([['Panel Schedule - Manufacturing'], \n                                  ['Construction Type', 'Panel Type']])\ndf = (pd.DataFrame({'Construction Type': [1,2],'Panel Type': [5,9]})\n        .reindex(columns=mux, level=1))\n\nprint (df)\n\n  Panel Schedule - Manufacturing           \n               Construction Type Panel Type\n0                              1          5\n1                              2          9\n", "df = pd.DataFrame({'Construction Type': [1,2],'Panel Type': [5,9]})\n\nwriter = pd.ExcelWriter('pandas_simple.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Sheet1', startrow = 1, index=False)\nworkbook  = writer.book\nworksheet = writer.sheets['Sheet1']\n\ntext = 'Panel Schedule - Manufacturing'\nworksheet.write(0, 0, text)\n\nwriter.save()\n"], "link": "https://stackoverflow.com/questions/59769529/pandas-add-single-cell-vertically"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this dataframe: Where stands for root publication, are comments. For each category I want to count the number of root publications () and number of comments (). I use for counting the unique values: I thought I could put as a condition inside the dataframe but it didn't work: How do I get the total number of occurences for depth=0 and depth>0? I want to put it in a table like this:", "q_apis": "get columns count unique values put get put", "apis": "DataFrame", "code": ["df = (cat_df['depth'].ne(0)\n                     .groupby(cat_df['category'])\n                     .value_counts()\n                     .unstack(fill_value=0)\n                     .rename(columns={0:'depth=0', 1:'depth>0'})\n                     .assign(total=lambda x: x.sum(axis=1))\n                     .reindex(columns=['total','depth=0','depth>0']))\n\nprint (df)\ndepth     total  depth=0  depth>0\ncategory                         \nfood          3        1        2\nschool        3        2        1\nsport         1        0        1\n", "cat_df = pd.DataFrame({'category': ['food', 'food', 'sport', 'food', 'school', 'school', 'school'], 'depth': [0.0, 1.0, 1.0, 3.0, 0.0, 0.0, 1.0], 'num_of_likes': [10, 10, 10, 20, 20, 20, 20]})\n", "print (cat_df)\n  category  depth  num_of_likes\n0     food    0.0            10\n1     food    1.0            10\n2    sport    1.0            10\n3     food    3.0            20\n4   school    0.0            20\n5   school    0.0            20\n6   school    1.0            20\n\ndf = (cat_df['depth'].ne(0)\n                     .groupby([cat_df['num_of_likes'], cat_df['category']])\n                     .value_counts()\n                     .unstack(fill_value=0)\n                     .rename(columns={0:'depth=0', 1:'depth>0'})\n                     .assign(total=lambda x: x.sum(axis=1))\n                     .reindex(columns=['total','depth=0','depth>0'])\n                     .reset_index()\n                     .rename_axis(None, axis=1)\n)\n\nprint (df)\n   num_of_likes category  total  depth=0  depth>0\n0            10     food      2        1        1\n1            10    sport      1        0        1\n2            20     food      1        0        1\n3            20   school      3        2        1\n", "s = cat_df.groupby('category')['num_of_likes'].sum()\nprint (s)\ncategory\nfood      40\nschool    60\nsport     10\nName: num_of_likes, dtype: int64\n\ndf = (cat_df['depth'].ne(0)\n                     .groupby(cat_df['category'])\n                     .value_counts()\n                     .unstack(fill_value=0)\n                     .rename(columns={0:'depth=0', 1:'depth>0'})\n                     .assign(total=lambda x: x.sum(axis=1))\n                     .reindex(columns=['total','depth=0','depth>0'])\n                     .reset_index()\n                     .rename_axis(None, axis=1)\n                     .assign(num_of_likes=lambda x: x['category'].map(s))\n)\nprint (df)\n  category  total  depth=0  depth>0  num_of_likes\n0     food      3        1        2            40\n1   school      3        2        1            60\n2    sport      1        0        1            10\n"], "link": "https://stackoverflow.com/questions/49878814/count-total-number-of-occurrences-in-pandas-dataframe-with-a-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas data frame. In one of the columns ('Q8') of this data frame, some of the rows are empty. I would like to replace these empty cells with a string ('ss'). I want to do this replacement with a condition. This condition is that if the string in column ('Q7') is ('I am a student') and the cell in this row at column ('Q8') is empty, replace the empty cell of column ('Q8') with 'ss'. This is the code which I wrote for it: but it can not find any np.nan from the first if!!", "q_apis": "get columns columns empty replace empty at empty replace empty any first", "apis": "loc isna loc loc fillna", "code": ["df.loc[(df['Q7'] == 'I am a student') & (df['Q8'].isna()), 'Q8'] = 'ss'\n", "df.loc[df['Q7'] == 'I am a student', 'Q8'] = df.loc[df['Q7'] == 'I am a student', 'Q8'].fillna('ss')\n"], "link": "https://stackoverflow.com/questions/55034586/replace-nan-or-blank-with-string-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame that looks like this: And I want to create a third columns that should look like this: So basically if 'Not Known' is present in both and , then the value should be 'cruise'. But if there is a string other than 'Not Known' in the first two columns, then should be filled with that string, either 'kayak' or 'ship'. What's the most elegant way to do this? I've seen several options such as , creating a function, and/or logic, and I'd like to know what a true pythonista would do. Here's my code so far:", "q_apis": "get columns columns value first columns", "apis": "replace ffill iloc fillna", "code": ["df['boat_type_final'] = (df.replace('Not Known',np.nan)\n                           .ffill(axis=1)\n                           .iloc[:, -1]\n                           .fillna('cruise'))\nprint (df)\n   boat_type boat_type_2 boat_type_final\n0  Not Known   Not Known          cruise\n1  Not Known       kayak           kayak\n2       ship   Not Known            ship\n3  Not Known   Not Known          cruise\n4       ship   Not Known            ship\n", "print (df.replace('Not Known',np.nan))\n  boat_type boat_type_2\n0       NaN         NaN\n1       NaN       kayak\n2      ship         NaN\n3       NaN         NaN\n4      ship         NaN\n", "print (df.replace('Not Known',np.nan).ffill(axis=1))\n  boat_type boat_type_2\n0       NaN         NaN\n1       NaN       kayak\n2      ship        ship\n3       NaN         NaN\n4      ship        ship\n", "print (df.replace('Not Known',np.nan).ffill(axis=1).iloc[:, -1])\n0      NaN\n1    kayak\n2     ship\n3      NaN\n4     ship\nName: boat_type_2, dtype: object\n", "print (df.replace('Not Known',np.nan).ffill(axis=1).iloc[:, -1].fillna('cruise'))\n0    cruise\n1     kayak\n2      ship\n3    cruise\n4      ship\nName: boat_type_2, dtype: object\n", "m1 = df['boat_type'] == 'ship'\nm2 = df['boat_type_2'] == 'kayak'\n\ndf['boat_type_final'] = np.select([m1, m2], ['ship','kayak'], default='cruise')\nprint (df)\n   boat_type boat_type_2 boat_type_final\n0  Not Known   Not Known          cruise\n1  Not Known       kayak           kayak\n2       ship   Not Known            ship\n3  Not Known   Not Known          cruise\n4       ship   Not Known            ship\n"], "link": "https://stackoverflow.com/questions/51515724/create-pandas-data-frame-column-based-on-strings-from-two-other-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Here's my dataset, call them The expected value excludes , first expected value is called as :", "q_apis": "get columns value first value", "apis": "iloc mask iloc eq bfill iloc", "code": ["df['Grade'] = df.iloc[:, 2:].mask(df.iloc[:, 2:].eq('Other')).bfill(1).iloc[:, 0]\n"], "link": "https://stackoverflow.com/questions/54090610/how-do-i-get-first-non-other-on-multiple-column-label"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a problem that I have been trying to wrap my head around for most of the day, so far in vain. I want to: take a series from a multi(column) index modify it with a mathematical operator, or an .apply() function return it to the dataframe. I can do 1 and 2, but for some reason i can't properly return it to the dataframe. I would also be happy to construct a new dataframe from the series. By this point, I am willing to do many things to solve it. Example data looks like this: Foo Bar Baz day night day night day night a a b b a a b b a a b b a a b b a a b b a a b b 12 5 33 50 12 5 33 50 12 5 33 50 12 5 33 50 12 5 33 50 12 5 33 50 id 1 0 2 1 0 12 0 18 0 9 0 3 0 2 0 0 0 0 15 0 0 3 0 0 0 2 1 4 0 0 0 0 0 0 0 2 7 0 0 2 0 11 1 19 0 0 0 6 1 0 3 0 0 0 3 20 1 0 14 0 3 2 0 3 0 0 0 1 0 0 16 0 1 6 0 4 0 3 2 0 0 10 0 0 0 2 0 12 0 0 20 4 5 0 0 0 1 0 0 0 5 16 0 0 0 7 2 0 0 0 0 0 0 18 1 0 0 0 0 5 0 5 13 0 0 6 0 0 10 6 1 0 9 13 0 3 8 0 4 8 0 0 11 14 0 0 21 0 0 0 7 7 0 0 0 0 19 0 0 1 17 0 0 0 0 22 0 2 0 0 0 0 2 0 17 I can get the data by some variant of: Now I want to do something along the lines of: and have the dataframe updated with the new data. It is to do iterative editing later on. But for some reason nothing changes my dataframe. I've tried slicing it and updating in all the different ways I can think of. I've tried the code in jezrael's answer to jaydeepsb here: Pandas update values in a multi-index dataframe Which tought me to update a slice with a value, but so far updating a slice with a series eludes me. If anyone can help me, I thank you greatly. Also for not making me throw my computer out of the window in frustration :D", "q_apis": "get columns head day take index apply day day day get all update values index update value", "apis": "index update loc", "code": ["idx = pd.IndexSlice  # for multi-index slicing\n\ndf.update(df.loc[[2], idx[\"Foo\", \"day\", :, :]] * (60*24))\n"], "link": "https://stackoverflow.com/questions/67812077/modify-slice-of-multiindex-with-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a .txt file with a header, which I'd like to remove. The file looks like this: Which returns: I can grab the header: which returns: and then split into distinct columns: which returns: Now you see that the header still appears as the first line in my dataframe here. I'm unsure of how to remove it. .iloc is not available, and I often see this approach, but this only works on an RDD: So which alternatives are available?", "q_apis": "get columns columns first iloc", "apis": "get first filter filter", "code": ["df=spark.read.option(\"header\",\"true\").csv(\"path\")\ndf.show(10,False)\n#+----------------------------------------------------+\n#|Entry  Per  Account     Description                 |\n#+----------------------------------------------------+\n#| 16524  01  3930621977  TXNPUES                     |\n#|191675  01  2368183100  OUNHQEX                     |\n#|191667  01  3714468136  GHAKASC                     |\n#|191673  01  2632703881  PAHFSAP                     |\n#| 80495  01  2766389794  XDZANTV                     |\n#| 80507  01  4609266335  BWWYEZL                     |\n#| 80509  01  1092717420  QJYPKVO                     |\n#| 80497  01  3386366766  SOQLCMU                     |\n#|191669  01  5905893739  FYIWNKA                     |\n#|191671  01  2749355876  CBMJTLP                     |\n#+----------------------------------------------------+\n", "#can't read header\ndf=spark.read.text(\"path\")\n#get the header\nheader=df.first()[0]\n#filter the header out from data\ndf.filter(~col(\"value\").contains(header)).show(10,False)\n#+----------------------------------------------------+\n#|value                                               |\n#+----------------------------------------------------+\n#| 16524  01  3930621977  TXNPUES                     |\n#|191675  01  2368183100  OUNHQEX                     |\n#|191667  01  3714468136  GHAKASC                     |\n#|191673  01  2632703881  PAHFSAP                     |\n#| 80495  01  2766389794  XDZANTV                     |\n#| 80507  01  4609266335  BWWYEZL                     |\n#| 80509  01  1092717420  QJYPKVO                     |\n#| 80497  01  3386366766  SOQLCMU                     |\n#|191669  01  5905893739  FYIWNKA                     |\n#|191671  01  2749355876  CBMJTLP                     |\n#+----------------------------------------------------+\n", "sorted_df = df.select(\n    df.value.substr( 1,  6).alias('Entry'      ),\n    df.value.substr( 8,  3).alias('Per'        ),\n    df.value.substr(12, 11).alias('GL Account' ),\n    df.value.substr(24, 11).alias('Description'),\n)\n\nsorted_df.show()\nsorted_df.printSchema()\n", "#get header into a variable\nheader=spark.sparkContext.textFile(\"path\").first()\n\n#.textfile and filter out the header\nspark.sparkContext.textFile(\"path\").\\\nfilter(lambda l :not str(l).startswith(header)).\\\nmap(lambda x:x.split()).map(lambda x:(str(x[0].strip()),str(x[1].strip()),str(x[2].strip()),str(x[3].strip()))).\\\ntoDF([\"Entry\",\"Per\",\"Account\",\"Description\"]).\\\nshow()\n#+------+---+----------+-----------+\n#| Entry|Per|   Account|Description|\n#+------+---+----------+-----------+\n#| 16524| 01|3930621977|    TXNPUES|\n#|191675| 01|2368183100|    OUNHQEX|\n#|191667| 01|3714468136|    GHAKASC|\n#|191673| 01|2632703881|    PAHFSAP|\n#| 80495| 01|2766389794|    XDZANTV|\n#| 80507| 01|4609266335|    BWWYEZL|\n#| 80509| 01|1092717420|    QJYPKVO|\n#| 80497| 01|3386366766|    SOQLCMU|\n#|191669| 01|5905893739|    FYIWNKA|\n#|191671| 01|2749355876|    CBMJTLP|\n#+------+---+----------+-----------+\n"], "link": "https://stackoverflow.com/questions/61781152/pyspark-remove-first-row-from-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following CSV data frame imported using pandas (the numeric values are distances) I have a second data frame imported using (the numeric values are supply quantities) The column is matching between the two data frames. I created a list of forests from using: Result: And a list of destinations in using: Result: The list of tuples (arcs) was created using: Result: I want to create a dictionary of the in (that refers to the in ) and quantity values in . The dictionary should look like this: The quantity values in should be linked with the in by referring to the list or the key in . Can anyone suggest the best way to formulate this dictionary? This is only a small set of I(10) and J(4) in a combined matrix. My methods have to be applicable to very large datasets with over 10 million I*J combinations. Help would be much appreciated!", "q_apis": "get columns values second values between values values", "apis": "loc loc", "code": ["dQ = {}\nfor forest in I:\n    dQ[forest] = df2.loc[forest]['Supply']\nprint(dQ)\n", "{'Coupe 1': 600, 'Coupe 2': 100, 'Coupe 3': 900, 'Coupe 4': 300, 'Coupe 5': 300, 'Coupe 6': 400, 'Coupe 7': 900, 'Coupe 8': 700, 'Coupe 9': 500, 'Coupe 10': 300}\n", "dQ = {forest: df2.loc[forest]['Supply'] for forest in I}\n"], "link": "https://stackoverflow.com/questions/62442282/create-dictionary-based-on-matrix-data-frame-index-and-a-second-data-frame-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am asking you help for a part of my Python script I am struggling with: I have a dataframe with 4 columns : this dataframe contains thousands of lines. I am trying to delete lines from this dataframe if it's 'keyword' is in a list (for ex: list = {'Action', 'About} Here is the code line I made : but I get this error : I do not really understand what it stands for ... How can I resolve it ?or how should I process to get the result I want ? Thank's for helping.", "q_apis": "get columns columns contains delete get get", "apis": "drop apply index drop apply index", "code": ["df.drop( df[ df['keyword'].apply(lambda x: x in list) ].index, inplace=True)\n", "def func(x):\n    return x in list\n\ndf.drop( df[ df['keyword'].apply(func)].index, inplace=True)\n"], "link": "https://stackoverflow.com/questions/67722996/delete-elements-of-a-dataframe-if-they-are-in-a-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to read multiple files at once.I have data in two files as below: data: data1: I tried a couple of answers as per this link. Below is my code: When I ran this code it gives me output as below: But when I display I need output as below and in different columns: So how can I do it??", "q_apis": "get columns at columns", "apis": "join sample append append first columns to_csv index", "code": ["import glob\nimport os\nimport pandas as pd\nfrom functools import partial\nmapfunc = partial(pd.read_csv, header=None)\ndf = pd.concat(map(mapfunc, glob.glob(os.path.join(\" \", \"/home/cloudera/Desktop/sample/*\"))))\n", "              0         1\n0  123.22.21.11       sid\n1  112.112.11.1      john\n2  110.11.23.23     jenny\n3  122.23.21.13     ankit\n0  145.123.11.1   Joaquin\n", "# Initialize Variables\nfpath = \"C:/Users/5188048/Desktop/example/\"\ninterval = 5\nfilenames = []\n\n# loop through files in directory\nfor i, j in enumerate(os.listdir(fpath)):\n\n    # append filenames to list, initialized previously\n    filenames.append(j)\n\n    # for every interval'th file, perform this...\n    if (i+1)%interval==0:\n\n        # use first file to initialize dataframe\n        temp_df = pd.read_csv(fpath+filenames[0], header=None)\n\n        # loop through remaining files\n        for file in filenames[1:]:\n\n            # concatenate additional files to dataframe\n            temp_df = pd.concat([temp_df, pd.read_csv(fpath+file, header=None)], ignore_index=True)\n\n        # do your manipulation here, example reset column names\n        temp_df.columns = ['IP_Address', 'Name']\n\n        # Generate outfile variable name & path\n        out_file = fpath+'out_file_' + str(int((i+1)/interval)) + '.csv'\n\n        # write outfile to csv\n        temp_df.to_csv(out_file, index=False)\n\n        # reset variable\n        filenames = []\n\n    else:\n\n        pass\n"], "link": "https://stackoverflow.com/questions/55731824/reading-multiple-files-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame like this, And I have a dictionary like this, Now I want to keep maximum 2 values in col 2 based on the dictionary. So the final dataframe would look like, I could able to do it using a for loop, mapping with the dictionary but I am looking for some pandas shortcuts to do it more efficiently.", "q_apis": "get columns values", "apis": "nlargest apply nlargest get", "code": ["from heapq import nlargest\n\ndf[\"col2\"] = df[\"col2\"].apply(lambda x: nlargest(2, x, key=d.get))\nprint(df)\n", "  col1    col2\n0    A  [x, y]\n1    B     [z]\n2    C  [q, p]\n3    D  [q, t]\n"], "link": "https://stackoverflow.com/questions/66059938/map-a-pandas-list-type-column-with-a-dictionary-for-two-highest-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes like as shown below I already tried handling outliers using and but they seem to be influenced by data distribution and still give me outliers in the data. So, what I would like to do is impose the and conditions to the data. Meaning, if you look at the dataframe, you will get the and limits for a specific paramter like or . Any value which violates these condition, has to be replaced with the default (if it violates criteria) and (if it violates criteria) value as shown below. Please note my real data has more than million rows and 70 columns. Any scalable approach is helpful.but yes, I have the limits for all these columns in a dataframe like I expect my output to be like as shown below. you can see the are replaced by default and values", "q_apis": "get columns at get value value columns all columns values", "apis": "set_index columns clip loc loc", "code": ["df_limit = df_limit.set_index('reading')\n\ndf_data['temp'] = df_data['temp'].clip(df_limit.loc['min_temp','min_val'],\n                                       df_limit.loc['min_temp','max_val'])\ndf_data['heart_rate'] = df_data['heart_rate'].clip(df_limit.loc['min_heart_rate','min_val'], \n                                                   df_limit.loc['min_heart_rate','max_val'])\nprint (df_data)\n   temp  heart_rate\n0    30          24\n1    36          24\n2    34          28\n3    38          31\n4    39          25\n5    39          32\n", "df_limit = df_limit.set_index('reading')\n\nfor c in df_data.columns:\n    df_data[c] = df_data[c].clip(df_limit.loc[f'min_{c}','min_val'],\n                                 df_limit.loc[f'min_{c}','max_val'])\n"], "link": "https://stackoverflow.com/questions/59496645/how-to-handle-outliers-by-imposing-limits-using-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've been playing around with this but I can't even get the simplest case to work so I'm going to ask for assistance. I have a large dataframe and I'm trying to add four new columns to it. The values for each column is dependent on the data in the row as per the if statements below. Here is what I'd sketched out so far: Suffice to say, this doesn't get me anywhere. I'm used to using to create columns but that won't work in this case as in order to calculate before, I need to calculate after in the same row. Ie. each row needs to be filled sequentially. That's why I'd tried to approach this as a function that takes in the row and calculates the values for the four new columns. I'd appreciate any assistance or similar examples to get unstuck here. Thanks. EDIT: I've taken the advice from HakunaMaData and added the Helper column to the df to ensure I apply the first if statement as I intended. I had initially thought was going to work here but it won't because I can't shift the whole dataframe when applying along a row, right? Is there another way I can approach this? The intended output I'm looking for is:", "q_apis": "get columns get add columns values get columns values columns any get apply first shift right", "apis": "DataFrame columns assign at", "code": ["d = {'Signal': [0,1,1,0],\n   'Win': [False,True,False,False],\n   'Odds': [1.1, 1.2, 1.3, 1.4],\n   'before': [0]*4,\n   'stake':[0]*4,\n   'result':[0]*4,\n   'after':[0]*4\n}\n", "df = pd.DataFrame(d)\n\ndef function(df, start, stake_size):\n\n   '''\n   takes in three arguments: a dataframe, a start number as int and \n   stake_size as int\n   the function fills up before, stake, result, after columns row by row \n   using the IF statements below\n   '''\n\n   global after #Create a global variable to track the value in the previous row\n\n   if df.name == 0: \n       df['before'] = start\n   else: \n        df['before'] = after \n\n   if df['Signal'] == 0:\n       df['stake'] = 0\n       df['result'] = 0\n   elif df['Signal'] == 1:\n       df['stake'] = df['before'] * (stake_size/100)\n\n   if (df['Signal'] == 1 & df['Win'] == True):\n       df['result'] = (df['stake'] * df['odds']) - df['stake']\n   else:\n       df['result'] = df['stake'] * -1\n\n   df['after'] = df['before'] + df['result']\n\n   after = df['after'] #assign the value to the global variable at the end\n\n   return df\n"], "link": "https://stackoverflow.com/questions/54092491/apply-a-function-to-pandas-dataframe-row-by-row-axis-0-to-create-four-new-co"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to apply to a dataframe a function that has more than one argument, of which two need to be assigned to the dataframe's rows, and one is a variable (a simple number). A variation from a similar thread works for the rows: (all functions are oversimplified compared to my original ones) But how can I apply the following variation, where my function takes in an additional argument (a fixed variable)? I seem to find no way to pass it inside the apply method: Thanks for your help!", "q_apis": "get columns apply all apply where apply", "apis": "apply", "code": ["def DummyFunction2(row, threshold):\n    return row['a']*row['b']*threshold\n\ndf['Dummy2'] = df.apply(DummyFunction2, threshold=2, axis=1)\n"], "link": "https://stackoverflow.com/questions/50071669/pandas-apply-a-function-with-columns-and-a-variable-as-argument"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like this: I want to have all information in one column like this: can anyone help write a function please !", "q_apis": "get columns all", "apis": "melt loc index drop reset_index drop", "code": ["df1 = df.melt('ID', var_name='Materials')\ndf1 = df1.loc[df1.index.repeat(df1['value'])].drop('value', axis=1).reset_index(drop=True)\nprint (df1)\n   ID  Materials\n0  24  Material1\n1  12  Material1\n2  12   Materia2\n3  25  Material3\n4  25  Material3\n", "df1 = df.melt('ID', var_name='Materials')\n\ndf0 = df[['ID']].drop_duplicates()\nprint (df0)\n   ID\n0  14\n1  24\n2  12\n3  25\n\ndf2 = df1.loc[df1.index.repeat(df1['value'])].drop('value', axis=1).reset_index(drop=True)\n\ndf2 = df0.merge(df2, on='ID', how='left')\nprint (df2)\n   ID  Materials\n0  14        NaN\n1  24  Material1\n2  12  Material1\n3  12   Materia2\n4  25  Material3\n5  25  Material3\n"], "link": "https://stackoverflow.com/questions/61224628/how-to-write-the-fucntion-that-transfrom-the-columns-of-my-dataframe-to-a-single"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas data frame as follows after attempting to flatten it: I'm trying to persist this to the database; however, I don't see Symbols in the df.columns. In order to save the df to a following format: Any suggestions on how to achieve this? My database has a composite key on Symbols, date columns. Thank You.", "q_apis": "get columns columns date columns", "apis": "DataFrame iloc unstack droplevel rename_axis columns reset_index index", "code": ["df = (pd.DataFrame(web.DataReader(stocks, 'yahoo', day, day)\n      .iloc[0])\n      .unstack(level=0)\n      .droplevel(level=0, axis=1)\n      .rename_axis(columns=None) # Gets rid of the \"Attributes\"\n      .reset_index()             # Puts \"Symbols\" as an actual column, not as the index\n)\n"], "link": "https://stackoverflow.com/questions/64456503/pandas-flatten-hierarchical-multi-index"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a simple dataframe that looks like this. I want to change the numeric index to month-dates but not in chronologic order. I want it to be Jan,Feb,Mar,Apr,Nov,Dec. How could I make the index into these custom months so it skips the months I do not want?", "q_apis": "get columns index month index", "apis": "DataFrame isin isin", "code": ["np.random.seed(2018)\ndf = pd.DataFrame({'A':np.random.rand(186)})\n#print (df)\n\n#365 days\ndates = pd.date_range('01-01-2015', '31-12-2015')\ndates = dates[dates.month.isin([1,2,3,4,11,12])].strftime('%b-%d')\nprint (len(dates))\n181\n\n#366 days\ndates = pd.date_range('01-01-2000', '31-12-2000')\ndates = dates[dates.month.isin([1,2,3,4,11,12])].strftime('%b-%d')\nprint (len(dates))\n182\n", "#filter DaatFrame for same length\ndf = df.iloc[:len(dates)]\n#assign dates\ndf.index = dates\nprint (df.head(10))\n               A\nJan-01  0.882349\nJan-02  0.104328\nJan-03  0.907009\nJan-04  0.306399\nJan-05  0.446409\nJan-06  0.589985\nJan-07  0.837111\nJan-08  0.697801\nJan-09  0.802803\nJan-10  0.107215\n"], "link": "https://stackoverflow.com/questions/51304723/python-pandas-dataframe-setting-the-index-as-datetime-with-custom-months"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame like this: And I need to get the last for each customer. I feel there is must be something like and , but I haven't still figured out here. Help, please :)", "q_apis": "get columns DataFrame get last", "apis": "groupby max", "code": ["Customer_id\n1    2020.6.5\n2    2020.6.3\n3    2020.6.4\nName: Date, dtype: object\n", "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\nnew_df = df.groupby(\"Customer_id\")[\"Date\"].max()\nprint(new_df)\n", "Customer_id\n1   2020-06-05\n2   2020-06-03\n3   2020-06-04\nName: Date, dtype: datetime64[ns]\n"], "link": "https://stackoverflow.com/questions/62339302/last-day-of-customer-activity-python-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like the following. If the value in the column (HW Category) is \"19. Out of Warranty, Expired\" then I wanted to empty the value in the column (IO Stat). How can we achieve this result? Actual Dataframe: Expected Result:", "q_apis": "get columns value empty value", "apis": "loc", "code": ["df.loc[df['HW Category'] == '19. Out of Warranty, Expired', 'IO Stat'] = ''\n", "print(df)\n\n\n\n     IO Stat                   HW Category\n0             19. Out of Warranty, Expired\n1   Disabled                        In Use\n2   Disabled                        In Use\n3   Disabled                        Onsite\n4   Disabled                           NaN\n5   Disabled                           NaN\n6   Disabled                           NaN\n7             19. Out of Warranty, Expired\n8   Disabled                           NaN\n9   Disabled                           NaN\n10  Disabled                        In Use\n11  Disabled                        In Use\n12  Disabled                        Onsite\n13  Disabled                           NaN\n14  Disabled                           NaN\n15  Disabled                           NaN\n16            19. Out of Warranty, Expired\n17  Disabled                           NaN\n18            19. Out of Warranty, Expired\n\n"], "link": "https://stackoverflow.com/questions/67482818/deleting-values-in-the-column-based-upon-condition-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I was trying to add a new column by giving multiple strings contain conditions using and function. By this way, I can have the final result I want. But, the code is very lengthy. Are there any good ways to reimplement this using pandas function? This output will be, if those strings contain in 'sr_description' then give a , else to Maybe store the multiple string conditions in a list then read and apply them to a function. Edit: Sample Data:", "q_apis": "get columns add any apply", "apis": "join all join DataFrame columns", "code": ["import re\nimport pandas as pd\nimport numpy as np\n\n# list of the strings we want to check for\ncheck_strs = ['gross to net', 'gross up', 'net to gross', 'gross-to-net', 'gross-up', 'net-to-gross', 'gross 2 net',\n             'net 2 gross', 'gross net', 'net gross', 'memo code']\n\n# From the re.escape() docs: Escape special characters in pattern. \n# This is useful if you want to match an arbitrary literal string that may have regular expression metacharacters in it.\ncheck_strs_esc = [re.escape(curr_val) for curr_val in check_strs]\n\n# join all the escaped strings as a single regex\ncheck_strs_re = '|'.join(check_strs_esc)\n\ntest_col_1 = ['something with gross up.', 'without those words.', np.NaN, 'or with Net to gross', 'if not then we give a \"0\"']\ndf_1 = pd.DataFrame(data=test_col_1, columns=['sr_description'])\n\ndf_1['contains_str'] = df_1['sr_description'].str.contains(check_strs_re, case=False, na=False)\n\nprint(df_1)\n", "              sr_description  contains_str\n0   something with gross up.          True\n1       without those words.         False\n2                        NaN         False\n3       or with Net to gross          True\n4  if not then we give a \"0\"         False\n"], "link": "https://stackoverflow.com/questions/58700943/how-to-add-a-new-column-with-multiple-string-contain-conditions-in-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "i created this new data frame called unitown after merging two data frames with index 'State' and 'RegionName'. Below is how unitown looks like: enter image description here from the pic you can see it has column named in the format of Year and Quarter. However when I try it gives me the following error: I have tried and below is part of the output: I am not sure why it gives such error given '2000Q1' is clearly one of the column names. Can anyone please help me on this? Thanks a lot!", "q_apis": "get columns index names", "apis": "columns columns", "code": ["df.columns = [str(col) for col in df.columns]\n"], "link": "https://stackoverflow.com/questions/62087635/cannot-select-specific-column-after-merging-it-with-another-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like in this one: for each row, I need (both) the 'n' (in this case two) highest values and the corresponding column in descending order:", "q_apis": "get columns values", "apis": "nlargest values nlargest reindex columns values combine apply values", "code": ["import numpy as np\n\n# Apply Decorate-Sort row-wise to our df, and slice the top-n columns within each row...\n\nsort_decr2_topn = lambda row, nlargest=2:\n    sorted(pd.Series(zip(df.columns, row)), key=lambda cv: -cv[1]) [:nlargest]\n\ntmp = df.apply(sort_decr2_topn, axis=1)\n\n0    [(b, 4), (a, 1)]\n1    [(b, 6), (c, 4)]\n2    [(c, 8), (a, 1)]\n\n# then your result (as a pandas DataFrame) is...\nnp.array(tmp)\narray([[('b', 4), ('a', 1)],\n       [('b', 6), ('c', 4)],\n       [('c', 8), ('a', 1)]], dtype=object)\n# ... or as a list of rows is\ntmp.values.tolist()\n#... and you can insert the row-indices 0,1,2 with \nzip(tmp.index, tmp.values.tolist())\n[(0, [('b', 4), ('a', 1), ('c', 0)]), (1, [('b', 6), ('c', 4), ('a', 2)]), (2, [('c', 8), ('a', 1), ('b', 0)])]\n", "import numpy as np\n\nnlargest = 2\ntopnlocs = np.argsort(-df.values, axis=1)[:, 0:nlargest]\n# ... now you can use topnlocs to reindex both into df.columns, and df.values, then reformat/combine them somehow\n# however it's painful trying to apply that NumPy array of indices back to df or df.values,\n"], "link": "https://stackoverflow.com/questions/40433682/get-both-the-top-n-values-and-the-names-of-columns-they-occur-in-within-each-ro"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Incorporating with excel, I'm looking for a solution that would copy a specific element to another element depending if isOrganization is true. Using pandas df['isOrganization'] = df['Code'].str.endswith('000') statement, I managed to list true and false result with print function. If the column isOrganization is true, then the row that is true should be copied from column E and F to column B and C. Else: the row should be copied from column E and F to column D and E I.E. : This copies the entire column # dfo[['B', 'C']] = df[['E', 'F']] but I would only like to copy single row from the column. Attempt: This is printed for isOrganization output: This is printed for pf(column) output: First few element under E and F:", "q_apis": "get columns copy copy", "apis": "mask loc mask loc mask loc mask loc mask to_excel index columns", "code": ["df = pd.read_excel('input.xls')\n\nmask = df['Code'].str.endswith('000', na=False)\n\ndf.loc[mask, ['B', 'C']] = df.loc[mask,['F', 'G']].to_numpy()\n\ndf.loc[~mask, ['D', 'E']] = df.loc[~mask, ['F', 'G']].to_numpy()\n\ndf.to_excel('output.xlsx', index=False)\n", "df = pd.DataFrame({\n        'Code':['code000','code001','code002'] * 2,\n         'B':[4,5,4,5,5,4],\n         'C':[7,8,9,4,2,3],\n         'D':[1,3,5,7,1,0],\n         'E':[5,3,6,9,2,4],\n         'F':[7,8] * 3,\n         'G':[1,0] * 3\n})\n", "print (df.columns)\nIndex(['Code', 'B', 'C', 'D', 'E', 'F', 'G'], dtype='object')\n", "mask = df['Code'].str.endswith('000', na=False)\n\ndf.loc[mask, ['B', 'C']] = df.loc[mask,['F', 'G']].to_numpy()\n\ndf.loc[~mask, ['D', 'E']] = df.loc[~mask, ['F', 'G']].to_numpy()\n\nprint (df)\n      Code  B  C  D  E  F  G\n0  code000  7  1  1  5  7  1\n1  code001  5  8  8  0  8  0\n2  code002  4  9  7  1  7  1\n3  code000  8  0  7  9  8  0\n4  code001  5  2  7  1  7  1\n5  code002  4  3  8  0  8  0\n"], "link": "https://stackoverflow.com/questions/65087895/python-how-do-i-output-a-element-to-a-specific-column-and-rows-depending-on-the"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to convert an array of strings in arrays of integers associating its ids in a dataframe column. That's because I need to map a list of home rooms per id like the next shows: That's the JSON I have to map: That's the dataframe I have: And that's the dataframe I need: Any solution? Thanks in advance.", "q_apis": "get columns array map map", "apis": "set_index explode replace replace groupby index agg reset_index", "code": ["mapper = pd.read_json(jsonstr).set_index('name')['id']\ndf_out = df.explode('home_rooms').replace('dinig room', 'dining room') #fix typo with replace\ndf_out['home_rooms'] = df_out['home_rooms'].map(mapper)\ndf_out.groupby('index').agg(list).reset_index()\n", "   index home_rooms\n0      0  [1, 2, 4]\n1      1  [3, 6, 5]\n2      2  [7, 9, 8]\n3      3  [1, 2, 4]\n4      4  [3, 6, 5]\n5      5  [7, 9, 8]\n6      6  [1, 2, 4]\n7      7  [3, 6, 5]\n8      8  [7, 9, 8]\n"], "link": "https://stackoverflow.com/questions/62580035/convert-array-of-strings-in-arrays-of-integers-in-a-dataframe-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am right now working on a database and I would like to go from XML to a Pandas DataFrame and I've been stuck for a long time now. I have no idea how to solve this problem. When I'm trying to read Data, there's only a dataframe with a value 0 for country and 0 for numberStudent. I don't understand what is wrong. I have been looking for answer already on this forum but I'm still stuck. Also, I am not sure I am doing right. I would like to find the 0-th ans 17-th tag \"cell\" in each parent tag \"row\". Is it right to do use two times \"in\" in one declaration for the second for?", "q_apis": "get columns right now DataFrame time now value right right second", "apis": "append append DataFrame DataFrame", "code": ["country = []\nnumberStudent = []\nfor row in root.findall('row'):\n    i=0\n    for cell in row.findall('cell'):\n        if i==0:\n            country.append(cell.text)\n        if i==17:\n            numberStudent.append(cell.text)\n        i=i+1\ndata=pd.DataFrame({'country': country, 'number of student': numberStudent})\n", "country = [cell.text for cell in root.findall('.//row/cell[1]')]\nnumberStudent = [cell.text for cell in root.findall('.//row/cell[18]')]\ndata=pd.DataFrame({'country': country, 'number of student': numberStudent})\n"], "link": "https://stackoverflow.com/questions/40497725/python-and-pandas-xml-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've DataFrame with 4 columns and want to merge the first 3 columns in a new DataFrame. The data is identical, the order is irrelevant and any duplicates must remain. Desired DataFrame How do I get this done?", "q_apis": "get columns DataFrame columns merge first columns DataFrame identical any DataFrame get", "apis": "values DataFrame", "code": ["a = df.values\npd.DataFrame({'col_a': np.ravel(a[:, :3]), 'col_b': np.repeat(a[:, 3], 3)})\n", "  col_a col_b\n0   tom    10\n1  nick    10\n2  john    10\n3   bob    15\n4  jane    15\n5  nick    15\n"], "link": "https://stackoverflow.com/questions/66615706/join-columns-in-a-single-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to filter a DataFrame, and add values to a list if they meet certain criteria. The problem I'm getting is I end up appending <class 'pandas.core.series.Series'> to my list, rather than the value itself. So I get a collection of entries like this in my list: whereas I really just want the number '32.0'. Advice on how to extract the number would be appreciated.", "q_apis": "get columns filter DataFrame add values Series value get", "apis": "append at", "code": ["list_.append( df['Sum of Hours'][(df['Class'] == condition_a) & (df['Name'] == condition_b)].at[0, 'Sum of Hours'] )\n"], "link": "https://stackoverflow.com/questions/66399318/filtering-a-dataframe-but-cant-access-the-number-in-a-pandas-series"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe like: Col1 Col2 0.54523 0.992324 0.24223 0.274336 0.94234 0.245435 And I want only to have the specific values , rounding the original dataframe to the nearest possible number on the specific values. So, I want to become: Col1 Col2 0.5 1.0 0.25 0.25 1.0 0.25 How to do this in python (preferably in a pandas dataframe)?", "q_apis": "get columns values values", "apis": "abs values", "code": ["c = ['Col1', 'Col2']\n\na = np.array([0.25, 0.5, 1.0])\nidx = np.abs(df[c].values[:, :, None] - a).argmin(axis=-1)\ndf[c] = a[idx]\n", "   Col1  Col2\n0  0.50  1.00\n1  0.25  0.25\n2  1.00  0.25\n"], "link": "https://stackoverflow.com/questions/65978233/round-floats-in-a-dataframe-to-nearest-specific-float-values-rounding-to-a-non"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have 2 Dataframes: I am trying to search for for each \"Bill_Number\" in df_Billed to see if it is present if df_Received. Ideally, if it is present, I would like to take the difference between the dates between df_Billed and df_Received for that particular bill number (to see how many days it took to get paid). If the billing number is not present in df_Received, I would like to simply return the all rows for that billing number in df_Billed.", "q_apis": "get columns take difference between between days get all", "apis": "apply isnull abs", "code": ["df_Billed['result']=df_Billed.apply(lambda x:x.Date_x if pd.isnull(x.Date_y) \n                    else abs(pd.to_datetime(x.Date_x)-pd.to_datetime(x.Date_y)).days, \n                    axis=1)\n"], "link": "https://stackoverflow.com/questions/60955095/searching-for-values-from-one-dataframe-in-another-and-returning-information-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have this dataframe: I want to perform multiple linear regression with multiple independent variables (A1 & A2) with this dataframe, but I'm confused on how to utilize this dataframe within the formula: This doesn't work because I can only give one independent variable, do I have to make multiple dataframes?", "q_apis": "get columns", "apis": "filter columns columns join", "code": ["result_1 = sm.ols(formula=\"A1 ~ B + C + D\", data=df).fit()\nresult_2 = sm.ols(formula=\"A2 ~ B + C + D\", data=df).fit()\n", "indep_vars = df.filter(regex=\"^A\").columns\ndependents = df.columns.difference(indep_vars)\n\nresults = [sm.ols(formula=f\"{indep} ~ {' + '.join(dependents)}\", data=df).fit()\n           for indep in indep_vars]\n"], "link": "https://stackoverflow.com/questions/67941067/how-to-run-ols-regression-on-pandas-dataframe-with-multiple-indepenent-variables"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have created a dataframe by reading in two different datasets as csv files. At the minute, the data frame prints out something like this: Although the actual dataframe has over 200 rows. Is there a way, for example, for the letter B, to change the value in the column named Value 3 from 6 to 8?", "q_apis": "get columns minute value", "apis": "loc eq eq set_index at", "code": ["df.loc[df['Value 3'].eq(6) & df['Name'].eq('B'), 'Value 3'] = 8\n", "df = df.set_index('Name')\ndf.at['B', 'Value 3'] = 8\n"], "link": "https://stackoverflow.com/questions/53824342/replacing-a-certain-value-in-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a bunch of CSV files, each one named as the date it was collected, ie. : I would like to create a single pandas DataFrame containing the data from all the CSVs, with a new date column listing the date that data is from. As a toy example: Current, single CSV (eg. ): Desired result (combined DataFrame): What is the best way to achieve this in pandas? I have tried a couple methods using and , with no luck.", "q_apis": "get columns date DataFrame all date date DataFrame", "apis": "get all values add append all", "code": ["import glob\ncsvfiles = []\ncsvfiles = glob.glob(\"/path/to/folder/*.csv\")\nprint(csvfiles)\n", "list_df = []\nfor csvfile in csvfiles:\n    #read csv file to df\n    df = pd.read_csv(csvfile)\n    #get the filename ex: 2020-03-19\n    csv_name = csvfile.split('/')[-1].split('.')[0]\n    #create a new column with all values are filename ex: 2020-03-19\n    df['Date'] = csv_name\n    #add df to a list\n    list_df.append(df)\n#concat all the df in the list\nfinal_df = pd.concat(list_df)\n"], "link": "https://stackoverflow.com/questions/60923806/combining-separate-daily-csvs-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with me which has an ID column and description column with START and STOP values represented by the ID's. Every START-STOP pair is denoted by an ID and it is incremented to 1 on next appearance of the pair. I need to increment the ID by 1 right after the STOP element has occured (update the NaN's of course) and this same ID should continue till I find the next STOP. Also how to take care of the first START-STOP pair since the main problem focuses on covering STOP-STOP event? Also, any number of events or segments can be there inside the START-STOP or a STOP-STOP pair I would like to have like this in the end This kind of needs to be applied for hundreds of thousands of rows and not a bunch of rows shown as sample. Kindly help me on this! Thanks in advance :)", "q_apis": "get columns values right update take first any sample", "apis": "cumsum loc loc fillna groupby bfill astype", "code": ["# create a group-tag by every STOP\ncond = df.SEG_DESC == 'STOP'\ndf['tag'] = cond.cumsum()\ndf.loc[cond, 'tag'] = df.loc[cond, 'tag'] - 1\n\n# for every tag-group use back fillna\ndf['ID_START_STOP'] = df.groupby('tag')['ID_START_STOP'].bfill().astype(int)\n"], "link": "https://stackoverflow.com/questions/66059546/fill-in-nan-values-in-dataframe-intervals-according-to-conditions-of-a-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "What is a efficient way to remove duplicated rows from a pandas dataframe where I would like always to keep the first value that is not NAN. Example: Should give: does that job but it is ridiculously slow. For a dataframe of shape it required 40 minutes to remove the duplicates, for another table of shape it took 5 hours 20 minutes. (datetime index, all columns integer/float). Note that the data might contain only few duplicated values. In another question , they suggest to use , but this does not handle NANs in the desired way. It doesn't really matter, if I choose , , or whatever, as long as it is fast. Is there a faster way than or is there a problem with my data that's making it slow.", "q_apis": "get columns duplicated where first value shape shape index all columns duplicated values", "apis": "shape resample min first shape", "code": ["step = 20000\nprint (df_sample.shape)\n%timeit x = pd.concat([df_sample[s:s+step].resample('min').first() for s in range(0,df_sample.shape[0],step)],axis=0)\n"], "link": "https://stackoverflow.com/questions/49485160/faster-way-to-remove-duplicated-indices-from-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Given a DataFrame with an ID column and corresponding values column, how can I aggregate (let's say sum) the values within blocks of repeating IDs? Example DF: Note that there's only two unique IDs, so a simple won't work. Also, the IDs don't alternate/repeat in a regular manner. What I came up with was to recreate the index, to represent the blocks of changed IDs: This re-creation of the index feels sort-of not how you'd do this in . What did I miss? Is there a better way to do this?", "q_apis": "get columns DataFrame values aggregate sum values unique repeat index index", "apis": "groupby ne shift cumsum agg first sum", "code": ["print (df['id'].ne(df['id'].shift()).cumsum())\n0     1\n1     1\n2     1\n3     1\n4     1\n5     2\n6     2\n7     2\n8     3\n9     3\n10    4\n11    5\n12    6\n13    6\n14    6\nName: id, dtype: int32\n\ndf1 = (df.groupby([df['id'].ne(df['id'].shift()).cumsum(), 'id'])['v'].sum()\n          .reset_index(level=0, drop=True)\n          .reset_index())\nprint (df1)\n  id    v\n0  a  5.0\n1  b  3.0\n2  a  2.0\n3  b  1.0\n4  a  1.0\n5  b  3.0\n", "df1 = (df.groupby(df['id'].ne(df['id'].shift()).cumsum(), as_index=False)\n         .agg({'id':'first', 'v':'sum'}))\n"], "link": "https://stackoverflow.com/questions/62167354/pandas-dataframe-aggregate-values-within-blocks-of-repeating-ids"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe which I extend to include values for all increments in 2 columns. Therefor NaN values are introduced, as expected and desired. However, when I use pivot on this dataframe I'll get a row and column for NaN. Can I prevent this when doing the pivot? If not, how can I drop a column named NaN? Trying to drop it by calling [NaN],[nan] or ['NaN'] doesn't work. Dropping the columns and rows where all values are NaN is not working in this case as the column headings and indexes are used for a seaborn heatmap plot, so eventhough all cell values are NaN it is still useful to have it as the index and key values are not NaN Sample code; Sample DataFrame before pivot: After pivot: Desired output:", "q_apis": "get columns values all columns values pivot get pivot drop drop columns where all values plot all values index values DataFrame pivot pivot", "apis": "pivot sort_values drop drop pivot sort_values reindex index index columns columns", "code": ["data = (df.pivot(\"Y\",\"X\",\"Z\")\n         .sort_values(by=['Y'],ascending=False)\n         .drop(np.nan, axis=1)\n         .drop(np.nan))\n", "data = df.pivot(\"Y\",\"X\",\"Z\").sort_values(by=['Y'],ascending=False)\n\ndata = data.reindex(index=data.index.difference([np.nan]),\n                    columns=data.columns.difference([np.nan]))\n"], "link": "https://stackoverflow.com/questions/53300244/prevent-nan-to-become-index-and-column-in-dataframe-pivot"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This is tricky problem and I am banging my head from a long time. I have the following data frame. parts data frame is for reference, I have another piece of code that will provide a list for related parts and primary part for each store. #For Store A -> TV : ['remote','antenna','speaker'] ; Store B -> Cell :['display','touchpad'] and my expected dataframe is: I have code that is good for the execution for the whole df at once. But due to other business rules this will be a slice of data. meaning 2 & 3 will be omitted so, .iloc value may be different for some records. So if you subset df on <=10 days and if is working for you then it will work for me. If any more information is required please let me know. I know it is very complicated and is actually a brain teaser.", "q_apis": "get columns head time at iloc value days any", "apis": "DataFrame DataFrame DataFrame index stack reset_index columns merge items loc astype groupby max to_dict where astype fillna drop drop_duplicates", "code": ["dct = {'Store': ('A','A','A','A','A','A','B','B','B','C','C','C'),\n       'code_num':('INC101','INC102','INC103','INC104','INC105','INC106','INC201','INC202','INC203','INC301','INC302','INC303'),\n       'days':('4','18','9','15','3','6','10','5','3','1','8','5'),\n       'products': ('remote','antenna','remote,antenna','TV','display','TV','display,touchpad','speaker','Cell','display','speaker','antenna')\n}\n\ndf = pd.DataFrame(dct)\npts = {'Primary': ('TV','TV','TV','Cell','Cell'),\n         'Related' :('remote','antenna','speaker','display','touchpad')\n    \n}\n\nparts = pd.DataFrame(pts)\nstore = {'A':'TV','B':'Cell'}\n", "new_df = pd.DataFrame(df.products.str.split(',').tolist(), index=df.code_num).stack()\nnew_df = new_df.reset_index([0, 'code_num'])\nnew_df.columns = ['code_num', 'Prod_seperated']\nnew_df = new_df.merge(df, on='code_num', how='left')\n", "store_prod = {}\nfor k,v in store.items():\n    store_prod[k] = k+'_'+v\nnew_df['prod_store'] = new_df['Store'].map(store_prod)\nnew_df['p_store'] = new_df['Store'].map(store)\nnew_df['main_ind'] = ' '\nnew_df.loc[(new_df['prod_store']==new_df['Store']+'_'+new_df['Prod_seperated'])&(new_df['days'].astype('int')<10),'main_ind']=new_df['code_num']\nrefer_dic = new_df.groupby('Store')['main_ind'].max().to_dict()\nnew_df['prod_subproducts'] = new_df['Prod_seperated'].map(parts_df_dict)\nnew_df['refer']  = np.where((new_df['p_store']==new_df['prod_subproducts'])&(new_df['days'].astype('int')<=10),new_df['Store'].map(refer_dic),np.nan) \n\nnew_df['refer'].fillna(new_df['main_ind'],inplace=True)\nnew_df.drop(['Prod_seperated','prod_store','p_store','main_ind','prod_subproducts'],axis=1,inplace=True)\nnew_df.drop_duplicates(inplace=True)\n"], "link": "https://stackoverflow.com/questions/63823069/pandas-copy-column-element-and-apply-to-another-column-based-on-related-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a DataFrame with an index, and then a reference to other indexes, and an organization. For example: Output: What I need to do is count on each row how many other rows where that row's index occurs as the refIndex from the same org. So I end up with a DataFrame like: I am new to Python and Pandas, so please excuse if this is obvious to you. I have been struggling all day with trying groupbys, functions, for loops inside for loops, merges.", "q_apis": "get columns DataFrame index count where index DataFrame all day", "apis": "values T count sum index count", "code": ["i, r, o = df.values.T\ndf['count'] = np.sum((i[:, None] == r) & (o[:, None] == o), axis=1)\n", ">>> (i[:, None] == r)\n\narray([[False,  True, False, False],\n       [False, False,  True, False],\n       [ True, False, False,  True],\n       [False, False, False, False]])\n", ">>> (o[:, None] == o)\n\narray([[ True,  True, False, False],\n       [ True,  True, False, False],\n       [False, False,  True,  True],\n       [False, False,  True,  True]])\n", ">>> (i[:, None] == r) & (o[:, None] == o)\n\narray([[False,  True, False, False],\n       [False, False, False, False],\n       [False, False, False,  True],\n       [False, False, False, False]])\n", ">>> df\n\n   index  refIndex   org  count\n0      1         3  org1      1\n1      2         1  org1      0\n2      3         2  org2      0\n"], "link": "https://stackoverflow.com/questions/66778962/in-pandas-count-rows-that-match-multiple-criteria-based-on-values-within-the-sa"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "Hello I have a df such as So basicaly I would like to keep for each the lowest BUT if the lowest in contains no \"\" pattern, and there is other values with a \"\" then I keep that one. For instance in : here the lowest one is : but it does not contain any \"\" and do contain one and is the second lowest, so I keep that one. another exemple : here there is only content without \"\" pattern, so I keep the lowest one : Then the expected output for all COL1 should be : So far I know how to sort column and keep the first one (with lowest values) but I do not know how to take into accoutn the fast that priority is given to values with a \"\" pattern...", "q_apis": "get columns contains values any second all first values take values", "apis": "where sort_values drop_duplicates first", "code": ["df[\"has_underscore\"] = np.where(df.COL2.str.contains(\"_\"),  1, 2)\ndf = df.sort_values([\"has_underscore\", \"VALUE\"])\nres = df.drop_duplicates(subset = \"COL1\", keep=\"first\")\n", "    COL1         COL2  VALUE  has_underscore\n8  SP4_1  YP_00321334     10               1\n1  SP1_1  YP_00321331     33               1\n3  SP2_1      NP_8821     89               1\n5  SP3_1    AJZ7313.1     10               2\n4  SP2_3   KJZ73167.1     90               2\n"], "link": "https://stackoverflow.com/questions/63689384/sort-and-keep-first-but-with-pattern-conditions-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "It's a follow up to my previous question here: Finding the index of rows based on a sequence of values in a column of pandas DataFrame I want to get a list of tuples that has index of very bad, followed with the the index of first occurrence of 'bad': Here's the data frame: How can I get a tuple of such combinations? [(2,4), (10,16), (17,18)]", "q_apis": "get columns index values DataFrame get index index first get", "apis": "isin eq shift eq get index values shift index items", "code": ["# filters only rows with bad and very bad\nm = df[df['status'].isin(['bad','very bad'])] \n\n# check id current row is very bad and next row is bad\nc = m['status'].eq('very bad') & m['status'].shift(-1).eq('bad')\n\n# if true return next row as true too and get only index values\nidx = m[c|c.shift()].index\n\n# convert every 2 items into a tuple\nres = [*zip(idx[::2],idx[1::2])]\n\n", "[(2, 4), (10, 16), (17, 18)]\n"], "link": "https://stackoverflow.com/questions/61736020/generating-a-tuple-of-indexes-based-on-a-sequence-of-values-in-a-pandas-datafram"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am new to pandas, I am trying to use group by and create a list of in a new column. I have 3 columns in my Dataframe and I created a 4th column(New_List) to create a list from another column like below: using the below code: new_df = df.join(pd.Series(df.groupby(by='NO_ACCOUNTS').apply(lambda x: list(x.Bucket)), name=\"list_of_b\"), on='NO_ACCOUNTS') I am looking to get the desired output with 3 columns:", "q_apis": "get columns columns join Series groupby apply name get columns", "apis": "groupby agg sum", "code": ["out = (df.groupby(\"Account_Number\", sort=False, as_index=False)\n         .agg(Number_Transactions=(\"Number_Transactions\", \"sum\"),\n              New_List=(\"Bucket\", list)))\n", ">>> out\n\n  Account_Number  Number_Transactions    New_List\n0            ABA                  155       [APP]\n1            ABC                 1352       [APP]\n2            AAA                   95  [APP, API]\n"], "link": "https://stackoverflow.com/questions/67917271/create-a-column-of-list-from-another-column-and-display-only-unique-values-in-pa"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe with and columns. tells the nth order by a user. I want to select users who have done certain number of orders. Sample DataFrame: Output: Output should be because they have done 3 orders but 2 have done only 2 orders. I am trying: This gives me boolean series but how to select index with only True values?", "q_apis": "get columns columns nth select DataFrame select index values", "apis": "groupby max query groupby max loc", "code": ["df.groupby(['user_id'],as_index=False)['order_number'].max().query(\"order_number==3\")\n#or\ndf.groupby(['user_id'],as_index=False)['order_number'].max().loc[\n                                   lambda x: x['order_number']==3]\n", "   user_id  order_number\n0        1             3\n2        3             3\n"], "link": "https://stackoverflow.com/questions/59548468/select-users-with-specific-number-of-orders-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame that looks like this: I want to create a third column with 3 possible classes based on conditions of the other 2 columns. I tried writing a function below, but it's not working - I don't get a return when I call df.head() after calling the function. The other answers surrounding this topic don't seem to cover 3 classes for the new column, but I am a novice so just might not get it.", "q_apis": "get columns columns get head get", "apis": "sub shift gt diff lt gt assign sub shift fillna gt fillna diff lt gt assign", "code": ["c1 = df.Value.sub(df.Value.shift()).gt(0.1)\nc2 = df.Value.diff().lt(0.1) & df.TransactionId.gt(0)\n", "df.assign(ChargerState=np.select([c1, c2], ['Charging', 'Not Charging'], 'Vacant'))\n", "                     TransactionId   Value ChargerState\nTimestamp\n2018-01-07 22:00:00         633025  674.87       Vacant\n2018-01-07 22:15:00         633025  676.11     Charging\n2018-01-07 22:30:00         633025  677.06     Charging\n", "c1 = df.Value.sub(df.Value.shift().fillna(0)).gt(0.1)    # Notice the fillna\nc2 = df.Value.diff().lt(0.1) & df.TransactionId.gt(0)\n\ndf.assign(ChargerState=np.select([c1, c2], ['Charging', 'Not Charging'], 'Vacant'))\n", "                     TransactionId   Value ChargerState\nTimestamp\n2018-01-07 22:00:00         633025  674.87     Charging\n2018-01-07 22:15:00         633025  676.11     Charging\n2018-01-07 22:30:00         633025  677.06     Charging\n"], "link": "https://stackoverflow.com/questions/52373764/how-do-i-create-a-new-column-in-a-dataframe-based-on-conditions-of-other-columns"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that looks like this: If the given value is , I want to check if there's a gap bigger than the given value in a row. For example, in the 2nd row, there's a gap between InLevel_02(11.5) and InLevel_01(10.5), which is 11. In the 5th row, the gaps are 10 and 9.5, between InLevel_02(10.5) and InLevel_01(9.0). The result of this job would look like this: I tried converting the dataframe into an array(using .to_records) and comparing each value with its next value using loops, but the code gets too complicated when there are more than 1 level between two values and I'd like to know if there are more efficient ways to do this.", "q_apis": "get columns value value between between array to_records value value between values", "apis": "astype where shift iloc values", "code": ["t = 0.5\n# df = df.astype(float) # if it isn't already\nrows, cols = np.where(df - df.shift(-1, axis = 1) > t)\n# (array([1, 2, 3, 4]), array([1, 4, 1, 1]))\n", "v = [np.arange(*df.iloc[r,[c+1, c]].values, step=t)[1:] for r, c in zip(rows, cols)]\n# [array([11.]), array([10.5]), array([11.]), array([ 9.5, 10. ])]\n", "from itertools import chain\nfrom collections import Counter\n\nx = list(chain.from_iterable(v.values))\n#[11.0, 10.5, 11.0, 9.5, 10.0]\npd.Series(Counter(x), name = 'count')\n\n11.0    2\n10.5    1\n9.5     1\n10.0    1\nName: count, dtype: int64\n"], "link": "https://stackoverflow.com/questions/54072319/how-to-compare-dataframe-columns-using-a-given-value"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two dataframes: I also have a column consisting of \"Y\" and \"N\": I want to create a 3rd dataframe that returns df1 if df0['Split'] = 'Y' and returns df2 if df0['Split'] = 'N'. I'd like to keep the shape of the original two dataframes if possible. I thought I could do something such as the following: In reality, I have a lot more columns than A through C. Appreciate your help.", "q_apis": "get columns shape columns", "apis": "mask mask mask sort_index", "code": ["mask = df0['Split'] == 'Y'\nres = pd.concat([df1[mask], df2[~mask]]).sort_index()\n"], "link": "https://stackoverflow.com/questions/54178834/create-third-dataframe-based-on-condition-applied-to-two-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following df: Now I have a number like 8405 and I want to know the Place or whole Row which has this number under . I also tried with classes but it was hard to get all Numbers of all Objects because I want to be able to call all PLZ in a list and also check, if I have any number, to which Place it belongs. Maybe there is an obvious better way and I just don't know it.", "q_apis": "get columns get all all all any", "apis": "agg apply agg", "code": ["df[df['PLZ'].agg(lambda x:8405 in x)]\n#you can also use apply() in place of agg\n", "    Place       PLZ                                             shortName   Parzellen\n0   Winterthur  [8400, 8401, 8402, 8404, 8405, 8406, 8407, 840...   WIN     []\n"], "link": "https://stackoverflow.com/questions/68055535/check-for-value-in-pandas-dataframe-cell-which-has-a-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have following dataframe: I want to keep only those rows which have 'local' value, that appears >100 times in the df. I have tried: And none have worked. What am I missing here?", "q_apis": "get columns value", "apis": "value_counts isin index", "code": ["counts = df[\"local\"].value_counts() > 100\ndf[df['local'].isin(counts[counts].index )]\n"], "link": "https://stackoverflow.com/questions/65618330/how-to-select-rows-based-on-a-column-being-true-for-one-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "With the following dataframe as an example : I would like to add a new column called with the indices of columns and/or provided they satisfy 2 conditions: Must be greater than 5 or must be greater than or equal to 1 The resulting dataframe would be: I can get True or False values for my first condition with but I cannot get it to work with my condition 2 which is based on another column in my dataframe. Getting the indices where I get True in a new column is yet another story. I imagine something with apply and get_loc or index but I cannot get it to work no matter how I try.", "q_apis": "get columns add indices columns get values first get indices where get apply get_loc index get", "apis": "gt div ge dot columns", "code": ["m = df[['A', 'C']].gt(5) & df[['A', 'C']].div(df['Base'], axis=0).ge(1)\ndf['indices'] = m.dot(m.columns + ',').str.rstrip(',')\n", "  Sample  Base    A   C indices\n0      X     2    0   0        \n1      Y    10    5  10       C\n2      Z     3  100   7     A,C\n"], "link": "https://stackoverflow.com/questions/64305938/get-indices-of-columns-satisfying-multiple-conditions-in-new-column-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I get that this is a common question but there are so many variations on it in pandas that I couldn't find a solution to my problem. I have two DataFrames - one for and one for and would like to create two new df's using both as inputs. Based on my single Unique column - the comparison between the two df's can have one of three outputs. The Unique Key is in both and The Unique Key is in but not The Unique Key is not but is in The first df I would like to merge the df's such that it outputs #1 & #2, with the values in taking precedence AND adding any additional columns from . The second output I simply want to output #3. Using some examples: today yesterday output1 (#1 & #2) output2 (#3) A few extra things: I have been trying to use for #1, I would really like to not have and duplicate columns when merging. I tried for #3 and I think it works? Any help is much appreciated. Thanks.", "q_apis": "get columns get between first merge values any columns second today columns", "apis": "isin isin append replace replace", "code": ["case1 = yesterday[yesterday['unique'].isin(today['unique'])] \n\ncase2 = today[~today['unique'].isin(yesterday['unique'])]\n\noutput_1_2 = case1.append(case2, ignore_index = True)\noutput_1_2.replace(np.nan, \"\", regex = True) # to replace NaN\n"], "link": "https://stackoverflow.com/questions/51490578/pandas-merging-and-comparing-two-dataframes-one-unique-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This value from excel sheet I written below code This code print two rows I need to extract sometime three rows depend upon excel sheet value how to give index instead of two?", "q_apis": "get columns value value index", "apis": "groupby mask eq ffill", "code": ["d = {x : y for x , y in df.groupby(df.Id.mask(df.Id.eq('')).ffill())}\n"], "link": "https://stackoverflow.com/questions/64656350/how-to-divide-the-dataframes-based-first-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "We have a SQL table where we store all the software that boots at start time across our computer fleet: The column is an foreign key integer that points to the IP address of the machine with the autorun row. Where an autorun is defined as a , how can we use Pandas Dataframes to count how many machines have a particular autorun? The return we would like to generate is a panda dataframe with all the autoruns listed (minus and ) with a new column called that contains the number of machines this autorun was discovered on. Thank in very much in advance for your assistance. Kind Regards, Alexander", "q_apis": "get columns where all at start time count all contains", "apis": "copy drop drop groupby columns size reset_index count sort_values count", "code": ["res = panda.copy(deep=True)\nres = res.drop('id', 1)\nres = res.drop('machine_id', 1)\nres = res.groupby(res.columns.tolist()).size().reset_index(name=\"count\")\nreturn res.sort_values('count')\n"], "link": "https://stackoverflow.com/questions/41561727/pandas-dataframe-special-counting"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a big pandas Dataframe, which essentially has a structure like the following one: Each of the 'rows' is an array of numbers. e.g.: I would like to calculate the correlation coefficient (np.corrcoef) between all combination of rows, e. g.: I want to obtain a DataFrame in the end that will hold all the correlation coefficients (CC) for all combinations. I can't figure out how to vectorize the code. My original dataframe is pretty huge, wherefore I would be grateful for any advice how to speed up the code. Thanks!", "q_apis": "get columns array between all DataFrame all all any", "apis": "iloc values iloc values", "code": ["row_one = df.iloc[0, :].values\nrow_two = df.iloc[1, :].values\nnp.corrcoef(row_one,row_two)\n", "df = pd.DataFrame(np.random.randint(0,10,size=(3, 3)), columns=list('ABC'))\n\n  0         1         2\n0  1.000000 -0.479317 -0.921551\n1 -0.479317  1.000000  0.782467\n2 -0.921551  0.782467  1.000000\n", "row_one = df.iloc[0, :].values\nrow_two = df.iloc[1, :].values\nnp.corrcoef(row_one,row_two)\n\narray([[ 1.        , -0.47931716],\n       [-0.47931716,  1.        ]])\n"], "link": "https://stackoverflow.com/questions/53973442/vectorize-code-in-big-pandas-dataframe-where-each-row-should-be-treated-as-a-nu"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am using this dataset: https://gist.github.com/JafferWilson/2c468679fa66c04c08a0ca624ca92d8f What I want to achieve is: Check the day is Monday or Friday If so, remove the first 2 hours values from Monday and the last 2 hours values from Friday from the Dataframe in pandas I have tried to load the values from the csv using the pandas dataframe as: But I don't know how to sort the data in the fashion I want. I tried using the and , but they are of no use to me. Please help me to get the data in the form I want it.", "q_apis": "get columns day first values last values values get", "apis": "loc", "code": ["df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y.%m.%d %H:%M:%S\")\n\ndf = df.loc[~((df[\"date\"].dt.weekday_name == \"Monday\") # Excludes Mondays before 2am\n            & (df[\"date\"].dt.hour < 2)) \n\n          & ~((df[\"date\"].dt.weekday_name == \"Friday\") # Excludes Fridays after 10pm\n            & (df[\"date\"].dt.hour >= 22))]\n"], "link": "https://stackoverflow.com/questions/51709244/remove-rows-where-datetime-is-a-specific-day-between-specific-times"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I would like to group the ids by Type column and apply a function on the grouped stocks that returns the first row where the Value column of the grouped stock is not NaN and copies it into a separate data frame. I got the following so far: The code above should work for the first id, but I am struggling to apply this to all ids in the data frame. Does anyone know how to do this?", "q_apis": "get columns apply first where first apply all", "apis": "mask isna groupby first", "code": ["df2['Value'] = pd.to_numeric(df2['Value'], errors='coerce')\ndf2.mask(df2['Value'].isna()).groupby('Type', as_index=False).first()\n", "   Type        Date  Value\n0   1.0  05.12.1998  100.0\n1   2.0  06.12.1998   20.0\n"], "link": "https://stackoverflow.com/questions/64263420/groupby-apply-function-and-combine-results-in-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following Dataframe. However it could be any data frame in that format. How can I perform an ANOVA to give me the F and p values for this without explicitly specifying the groups by text? In other words, is there a code that will automatically detect the groups and run an ANOVA so that it will work on any dataframe in that structure, not just this one?", "q_apis": "get columns any values groups groups any", "apis": "loc loc", "code": ["from scipy.stats import f_oneway\n\ngrps = [d['Weight'] for _, d in df.groupby('Group')]\n\nF, p = f_oneway(*grps)\n\nprint(F, p)\n4.846087862380136 0.0159099583256229\n", "from itertools import combinations\nfrom scipy.stats import f_oneway\n\ncombs = list(combinations(df['Group'].unique(), 2))\nfor g1, g2 in combs:\n    a = f_oneway(df.loc[df['Group'] == g1, 'Weight'], \n                 df.loc[df['Group'] == g2, 'Weight'])\n    print(f'For groups {g1} & {g2} the F-value is: {a[0]}, the p-value is: {a[1]}')\n", "For groups A & B the F-value is: 1.4191012973623165, the p-value is: 0.24902316597300575\nFor groups A & C the F-value is: 4.554043294351827, the p-value is: 0.04685138491157386\nFor groups B & C the F-value is: 9.0606932332992, the p-value is: 0.007518426118219876\n"], "link": "https://stackoverflow.com/questions/61058846/code-to-do-anova-on-different-dataframes-where-groups-can-change"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data frame with 20 columns and in each of them there is 0 or 1. I want to have a new column with 1 if in any of my 20 columns is 1 and with 0 otherwise. I tried to do it like this: but my df is quite big (~ 5 000 000 rows) and it lasts long time. Is there any faster solution?", "q_apis": "get columns columns any columns time any", "apis": "any values astype bool DataFrame size columns any values astype bool mean std values max astype bool mean std any values mean std values sum astype bool mean std", "code": ["L = ['s2', 's3', 's4', 's5','s6', 's7', \n     'k1', 'k2', 'k3', 'k4','k5', 'k6', 'k7', \n     'n1', 'n2', 'n3', 'n4','n5', 'n6', 'n7']\n\ndf['new_column'] = np.any(df[L].values.astype(bool), axis=1)\n", "np.random.seed(2019)\n\nL = ['s2', 's3', 's4', 's5','s6', 's7', \n     'k1', 'k2', 'k3', 'k4','k5', 'k6', 'k7', \n     'n1', 'n2', 'n3', 'n4','n5', 'n6', 'n7']\n\nN = 5000000 \ndf = pd.DataFrame(np.random.choice([0,1], p=(.8,.2), size=(N, len(L))), columns= L)\n#print (df)\n\nIn [311]: %timeit df['new_column'] = np.any(df[L].values.astype(bool), axis=1)\n544 ms \u00b1 18.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [312]: %timeit df['new_column'] = df[L].values.max(axis=1).astype(bool)\n504 ms \u00b1 16.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [313]: %timeit df['new_column'] = np.any(df[L].values, axis=1)\n546 ms \u00b1 36.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [315]: %timeit df['new_column'] = df[L].values.sum(axis=1).astype(bool)\n428 ms \u00b1 11 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"], "link": "https://stackoverflow.com/questions/55221129/faster-way-to-aggregate-20-columns-to-one-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a numpy array which I wish to filter by datetime. I have current functionality to compare an input datetime ( and ) to the dataframe like so: However, I'd like to add functionality for start and end to be specified as a \"day of year\", where the year is irrelevant to the comparison - if the day is later than 1st October then exclude it, be it 2001 or 2021. I am currently converting an integer value into datetime via: Which gives a default year of 1900, which will become part of the comparison.", "q_apis": "get columns array filter compare add start day year where year day value year", "apis": "index all index values", "code": ["dates = <ndarray of dates>\ns = pd.Series(dates, index=dates).dt.strftime('%m-%d')\n\n# Select between Oct 1 and Dec 31 of all years\ncond = ('10-01' <= s) & (s <= '12-31')\nselected = s[cond].index.values\n"], "link": "https://stackoverflow.com/questions/57510866/compare-numpy-array-and-datetime-without-year"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm working on a dataset in which I have various string column with different values and want to apply the . Here's the sample dataset: So, here I need to apply one-hot encoding on like that I have avrious other columns, which I have created a list of these cols as and here's how I'm applying the : But it's not affecting the dataset, when I print the it's still the same, what's wrong here?", "q_apis": "get columns values apply sample apply columns", "apis": "columns", "code": ["df = pd.get_dummies(df, columns=['pfv_cat'])\n\n      v_4         v5      s_5   vt_5       ex_5            pfv  pfv_cat_Clothes pfv_cat_Shoes\n0    0-50  StoreSale  Clothes  8-apr  above 100    FatimaStore                0             1\n1    0-50  StoreSale  Clothes  8-apr       0-50  DiscountWorld                1             0\n2  51-100  CleanShop  Clothes  4-dec     51-100    BetterUncle                0             1\n"], "link": "https://stackoverflow.com/questions/58297448/apply-one-hot-encoding-on-a-dataframe-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe as follows I want to keep a list of names prior to each new addition/ removal. Suppose originally I have then on index 0, it should be index 1, index 2, index 3, index 4, i.e. my desired result is In essence, this is what I am trying to do, which becomes My code is the following: However I run into Would really appreciate a solution to this issue. I really don't get why the code isn't working... Thanks", "q_apis": "get columns names index index index index index get", "apis": "replace fillna append append", "code": ["#for replace NaNs\ndf = df.fillna('')\n\noriginal = ['AAPL', 'AAL', 'MPWR', 'TRMB']\n\n#looping by df simplier way\nfor a, r in df[['added','removed']].to_numpy():\n    if a != '' and a in original:  \n        original.remove(a)\n    elif r != '' and r not in original:\n        original.append(r)\n        #alternative instead append\n        #original = original + [r] \n\nprint (original)\n['AAPL', 'AAL', 'FTI', 'CXO', 'TIF']\n"], "link": "https://stackoverflow.com/questions/67015808/updating-a-list"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to make a cluster of the following pandas data frame and trying to give the names. E.g - \"Personal Info\" is cluster name and it consist of (PERSON,LOCATION,PHONE_NUMBER,EMAIL_ADDRESS,PASSPORT,SSN, DRIVER_LICENSE) and also addition of there Counts. which will be 460. Clusters: for reference I am providing clusters structure Input data: Output:", "q_apis": "get columns names name", "apis": "info items groupby sum rename_axis rename reset_index count", "code": ["d = {'personal_info': ['PERSON','LOCATION','PHONE_NUMBER','EMAIL_ADDRESS','PASSPORT','SSN','DRIVER_LICENSE'],\n    'finance':['CREDIT_CARD','BANK_NUMBER','ITIN','IBAN_CODE'],\n    'info': ['NHS'],\n    'network':['IP_ADDRESS','DOMAIN_NAME'],\n    'others':['CRYPTO','DATE_TIME','NRP']\n    }\n\nd_inv = {x:k for k, v in d.items() for x in v}\n\n(df['Counts'].groupby(df['PII'].map(d_inv)).sum()\n   .rename_axis('Cluster names')       # rename to match output\n   .reset_index(name='Total count')\n)\n", "   Cluster names  Total count\n0        finance           97\n1           info            0\n2        network          140\n3         others           86\n4  personal_info          460\n"], "link": "https://stackoverflow.com/questions/65582869/how-to-make-clusters-of-pandas-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm doing some natural language processing, and I have a MultiIndexed DataFrame that looks something like this (except there are actually about 3,000 rows): What I want to do is to count how many times each n-gram appears in each month (hence the first index, \"Period\"). Doing that is rather straightforward, if time-consuming (and because each cell in the \"N-grams\" column is a list, I'm not sure much could be done to speed it up). I create a new DataFrame to hold the counts, using this code: The logic is pretty simple: if the n-gram in question has been seen already (there's a column for it in the \"freqs\"), increment the count by 1. If it hasn't been seen, create a new column of 0's for that n-gram, and then increment as normal. In the vast majority of cases, this works fine, but for a tiny fraction of n-grams, I get this error when the loop hits the increment line: (Sorry for the lack of a proper stack trace--I'm doing this in a Zeppelin Notebook, and Zeppelin doesn't give a proper stack trace.) A little more debugging showed that, in these cases, the creation of the new column fails silently (that is, it doesn't work, but it doesn't return an exception, either). It might be worth noting that in an earlier version of the code, I was using \"loc\" to assign directly to a cell in a newly created column, rather than creating the column first, like this: I changed this because it caused problems by assigning NaN's for that n-gram to all the other periods, but the direct assignment choked on exactly the same n-grams as with the new code. By wrapping the increment line in a try/except block, I've discovered that the error is extremely rare: it occurs for about 20 out of a total of over 100,000 n-grams in the corpus. Here are some examples: Most of the 20 include digits, but at least one is entirely letters (two words separated by a space--it's not in the list above, because I re-ran the script while typing up this question, and didn't get all the way to that point), and plenty of digits-only n-grams don't cause problems. Most of the problematic ones involve years, which, on the face of it, might suggest some sort of confusion with the DataFrame's DatetimeIndex (given that a DatetimeIndex accepts partial matches), but that wouldn't explain the non-dates, especially the ones beginning with letters. Despite the unlikelihood of the DatetimeIndex conflict, I tried a different method of creating each new column (as suggested by an answer to Adding new column to existing DataFrame in Python pandas), using \"loc\" to avoid any confusion between rows and columns: ...but that meets with exactly the same fate as my original code that created each new column implicitly by assigning to a non-existent column: Next, I tried the DataFrame.assign method (suggested in the same answer cited above, though I needed to add a workaround suggested by an answer to pandas assign with new column name as string): Alas, that produces exactly the same error. Does anyone have any insights on why this might be happening? Given the rarity, I suppose I could just ignore the problematic n-grams, but it would be good to understand what's going on.", "q_apis": "get columns DataFrame count month first index Period time DataFrame count get stack stack loc assign first all at get all DataFrame DatetimeIndex DatetimeIndex DatetimeIndex DataFrame loc any between columns DataFrame assign add assign name any", "apis": "index DataFrame loc DataFrame index index iterrows append value_counts sum append sum fillna astype", "code": ["period_index = ngrams.index.unique(level = \"Period\")\nfreqs = DataFrame()\n\nfor period in period_index:\n    period_ngrams = ngrams.loc[period]\n    period_freqs = DataFrame(index = period_ngrams.index)\n    for i, doc in period_ngrams.iterrows():\n        period_freqs = period_freqs.append(Series(doc[\"N-grams\"]). \\\n                           value_counts(sort = False), ignore_index = True)\n    period_sums = period_freqs.sum()\n    period_sums.name = period\n    freqs = freqs.append(period_sums)\n    print \"Processed period \" + str(period) + \".\"\n\nfreqs[\"Totals\"] = freqs.sum(axis = 1)\nfreqs = freqs.fillna(0).astype(int)\n"], "link": "https://stackoverflow.com/questions/53532140/puzzling-keyerror-when-assigning-to-a-new-columns-in-a-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My dataframe, df: The nulls are represented as s in the dataframe. How can I check if all the values in each row of Col_B are null? This is my : However, I get a truth value is ambiguous even though I have a \".all()\" there. Any way to get around this issue?", "q_apis": "get columns all values get value all get", "apis": "apply apply all", "code": ["df['one_bad'] = df['Col_B'].apply(lambda x: np.nan in x)\n", "df['all_bad'] = df['Col_B'].apply(lambda x: np.isnan(x).all())\n"], "link": "https://stackoverflow.com/questions/64123798/how-to-check-for-null-elements-in-a-list-in-a-dataframe-in-np-where"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe with multiple columns. One of the columns is which contains float values or NaNs: I run a function over excluding missing values () like this: Imagine the result is a numpy.ndarray like this: Notice that this array does not have same length than the original column . I need a solution to add the array as a column to . For each row where is , the new column gets too. Desired output would look like this:", "q_apis": "get columns columns columns contains values values array length add array where", "apis": "loc mask", "code": ["from sklearn.preprocessing import StandardScaler\n\nmask = df['Col1'].notnull()\ndf.loc[mask, 'Ref'] = StandardScaler().fit_transform(df.loc[mask, ['Col1']])\nprint (df)\n   No  Col1       Ref\n0  12  10.0 -0.327089\n1  23   NaN       NaN\n2  34   5.0 -1.027992\n3  45   NaN       NaN\n4  54  22.0  1.355081\n", "print (StandardScaler().fit_transform(df.loc[mask, ['Col1']]))\n[[-0.32708852]\n [-1.02799249]\n [ 1.35508101]]\n"], "link": "https://stackoverflow.com/questions/50856752/python-add-a-numpy-array-as-column-to-a-pandas-dataframe-with-different-length"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe in which I want to loop over with following condition: I also want to include, if the above condition is not met, then the values of P1 column should be equal to 0. Dataframe image link is given below:", "q_apis": "get columns values", "apis": "apply", "code": ["df['P1'] = df.apply(lambda row: 1 if row['P1'] >= 2 * row['T1'] else 0, axis=1)\n"], "link": "https://stackoverflow.com/questions/61939244/how-to-loop-in-dataframe-with-a-specific-condition"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a 2 dimentional associative array as below: But, now i wish to save it in .csv file. So, that its final format should be like this: I have tried the below code: But it complains with the below error:", "q_apis": "get columns array now", "apis": "loc", "code": ["       0_0_0  0_0_1  0_0_2  0_0_3\n0_0_0     10      0      1      0\n0_0_1      0      0      5      0\n0_0_2      0      0      0      0\n0_0_3      0      7      4      0\n", "for fromState in stateList:\n    for toState in stateList:\n        dt.loc[dt[\"States\"] == fromState,toState] = transitionCounter[fromState][toState]\n"], "link": "https://stackoverflow.com/questions/58568725/how-to-save-a-2d-associative-array-in-csv-without-loops"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm trying to convert hours to an categoric format, the column is like this with hundreds of entries. And i want the output like this: I'm using this code to categorize the time: But i keep having this error: Ok, i get it. The format is wrong. As i checked the format of the column is an object: But everytime i try to convert it, i get a new error: 1th. 2th 3th The fact is that i only want the hour to be converted. The date is in another column. Here is a print of the dataset to help understand. How can i do it? Thanks in advance, i'm really breaking my head -----EDIT--- By using the answer given, i get this:", "q_apis": "get columns time get get hour date head get", "apis": "add replace", "code": ["df['hr_animals'] = ((pd.to_timedelta(df['hr_animals'], errors='coerce')\n                      // pd.Timedelta('4H') )\n                      .add(1)\n                      .replace({1: 'Dawn',\n                      2: 'Early Morning',\n                      3: 'Morning',\n                      4: 'Noon',\n                      5: 'Evening',\n                      6: 'Night'})\n                   )\n", "  hr_animals\n0      Night\n1    Morning\n2      Night\n3       Dawn\n4        NaN\n5       Noon\n6        NaN\n", "df['hr_animals'] = pd.cut(pd.to_timedelta(df['hr_animals'], errors='coerce'),\n                          bins = pd.to_timedelta(np.arange(0,25,4), unit='H'),\n                          labels=['Dawn','Early Morning', 'Morning', \n                                  'Noon', 'Evening', 'Night']\n                   )\n"], "link": "https://stackoverflow.com/questions/64671385/format-problem-categorizing-time-in-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My code works fine in gives me some values but for example in my csv I have some values between 35000-65000. Somehow the output from that list is zero that means my code can't see those values. My values type looks like this Thats the output that I get.", "q_apis": "get columns values values between values values get", "apis": "notnull", "code": ["import pandas as pd\n\ndf = pd.read_csv('/Users/gfidarov/Desktop/daylite/export_daylite_v0.2.csv')\ndf = df[pd.to_numeric(df['\u0418\u0442\u043e\u0433'], errors='coerce').notnull()]\n", "#print(df)\ndf1 = df[df['\u0418\u0442\u043e\u0433'] > 60000]\n\ndf5 = df[df['\u0418\u0442\u043e\u0433'].between(40565, 60000)]\n\ndf2 = df[df['\u0418\u0442\u043e\u0433'].between(5000, 35000)]\n\ndf3 = df[df['\u0418\u0442\u043e\u0433'].between(500, 5000)]\n\ndf4 = df[df['\u0418\u0442\u043e\u0433'].between([0, 500)]\n"], "link": "https://stackoverflow.com/questions/59470762/i-am-trying-to-find-all-rows-that-have-values-between-0-500-500-5000-5000-3500"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I've a dataframe DF1: group the rows by common row in \"YEAR\" column and add all the data of that column. I tried to check with this: The Expected Output is like: DF2; Thank You For Your Time :)", "q_apis": "get columns add all", "apis": "groupby sum reset_index drop", "code": ["DF2 = DF1.groupby('YEAR' , as_index = False).sum().reset_index(drop = True)\n"], "link": "https://stackoverflow.com/questions/68096611/how-to-add-all-the-data-of-the-dataframe-by-identifying-the-common-column-in-dat"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am following this post to create a number of columns that are true/false based on if a substring is present in another column. Prior to using the code in the above post, I look at a field called which has values such as or . Unfortunately, the data is a comma-delimited string instead of a list but no problem, in one line, I can get the list of 25 unique values. Once I get the list of unique values, I want to make a new column for each value so a , , etc. columns. I want these columns to be true/false based on if the header is a substring of the original LANGUAGES column. Following the post, I use . However, when I check the values of the columns (the value counts in the last for loop), all the values come up false. How can I create a true/false column based on the column's header being a substring of another column? My Code: Edit: There was a request for a raw data input and expected output. Here is what a column looks like: This is the expected output:", "q_apis": "get columns columns at values get unique values get unique values value columns columns values columns value last all values", "apis": "replace explode drop_duplicates first index replace", "code": ["s  = df['LANGUAGES'].str.replace(\"'\",'').str.split(',').explode().to_frame()\n\ncols = s['LANGUAGES'].drop_duplicates(keep='first').tolist()\n\ndf2 = pd.concat([df, pd.crosstab(s.index, s[\"LANGUAGES\"])[cols]], axis=1).replace(\n    {1: True, 0: False}\n)\nprint(df2)\n         LANGUAGES   ENG    OTH    RUS    CZE    SPA\n0  'ENG, OTH, RUS'  True   True   True  False  False\n1            'ENG'  True  False  False  False  False\n2  'ENG, CZE, SPA'  True  False  False   True   True\n"], "link": "https://stackoverflow.com/questions/60978254/pandas-create-true-false-column-if-column-header-is-substring-of-another-colum"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have been trying to convert a pandas dataframe into a numpy array, carrying over the dtypes and header names for ease of reference. I need to do this as the processing on pandas is WAY too slow, numpy is 10 fold quicker. I have this code from SO that gives me what I need apart from that the result does not look like a standard numpy array - i.e. it does not show the columns numbers in the shape. Am I missing something or is there another way of doing this? I have many df's to convert and their dtypes and column names vary so I need this automated approach. I also need it to be efficient due to the large number of df's.", "q_apis": "get columns array dtypes names fold array columns shape dtypes names", "apis": "shape", "code": ["import numpy as np\n\ndf = pd.DataFrame(np.random.rand(10,3),columns=['Acol','Ccol','Bcol'])\n\nres = df.to_records(index=False)\n\n# rec.array([(0.12448699852020828, 0.7621451848466592, 0.0958529943831431),\n#  (0.14534869167076214, 0.695297214355628, 0.3753874117495527),\n#  (0.09890006207909052, 0.46364777245941025, 0.10216301104094272),\n#  (0.3467673672203968, 0.4264108141950761, 0.1475998692158026),\n#  (0.9272619907467186, 0.3116253419608288, 0.5681628329642517),\n#  (0.34509767424461246, 0.5533523959180552, 0.02145207648054681),\n#  (0.7982313824847291, 0.563383955627413, 0.35286630304880684),\n#  (0.9574060540226251, 0.21296949881671157, 0.8882413119348652),\n#  (0.0892793829627454, 0.6157843461905468, 0.8310360916075473),\n#  (0.4691016244437851, 0.7007146447236033, 0.6672404967622088)], \n#           dtype=[('Acol', '<f8'), ('Ccol', '<f8'), ('Bcol', '<f8')])\n", "res.view(np.float64).reshape(len(res), -1).shape  # (10, 3)\n"], "link": "https://stackoverflow.com/questions/49734441/converting-pandas-dataframe-to-numpy-array-with-headers-and-dtypes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have tried a lot to sort DataFrame column on my own way. But could not be able to correctly do it. So refer given code and let me know what is the additional syntax to do the job. This code does not give desire output. I need the Dataframe sorted as per below.", "q_apis": "get columns DataFrame", "apis": "sort_values unstack astype agg sort_values astype agg reindex unstack astype agg sort_values index mean std sort_values unstack astype agg mean std sort_values astype agg mean std", "code": [">>> df.reindex(\n        df['TC'].str.extractall('(\\d+)')\n                .unstack().astype(int)\n                .agg(tuple, 1).sort_values()\n                .index\n    )\n\n       TC Case\n0   1-1.1    A\n1   1-1.2    B\n4   1-2.1    E\n5   1-2.1    F\n6   1-2.2    G\n9   1-3.1    J\n2  1-10.1    C\n3  1-10.2    D\n7  1-20.1    H\n8  1-20.2    I\n", ">>> df.sort_values('TC', \n        key=lambda ser:\n           ser.str.extractall('(\\d+)')\n              .unstack()\n              .astype(int).agg(tuple, 1)\n    )\n", ">>> df.sort_values('TC', \n         key=lambda series:\n             series.str.split(r'\\D+', expand=True)\n                   .astype(int).agg(tuple,1)\n    )\n", ">>> %timeit df.reindex(df['TC'].str.extractall('(\\d+)').unstack().astype(int).agg(tuple, 1).sort_values().index)\n2.95 ms \u00b1 40.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n>>> %timeit df.sort_values('TC', key=lambda ser: ser.str.extractall('(\\d+)').unstack().astype(int).agg(tuple, 1))\n2.91 ms \u00b1 32.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n>>> %timeit df.sort_values('TC', key=lambda series:series.str.split(r'\\D+', expand=True).astype(int).agg(tuple,1))\n1.6 ms \u00b1 5.88 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n"], "link": "https://stackoverflow.com/questions/66090077/sort-pandas-dataframe-by-customize-way"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am extremely new to Python. I have a huge dataframe that contains two variables in list format. It has a dimension of 1416631 x 2. I am trying to extract the first element of the list to create another variable. However, the current code has been running for over an hour to no avail. Here is a snippet of the dataframe with two variables, and (which is currently empty): This is what I want it to look like (2 variables: and ): Here is my code: This code works on a smaller dataframe, but doesn't stop running on my larger dataframe. Please advise. Thanks", "q_apis": "get columns contains first hour empty stop", "apis": "DataFrame apply values", "code": ["import pandas as pd\n\ndf = pd.DataFrame({'col': [[1, 'Aged', 'Adult', 'Child'],\n                           [53, 'Humans', 'Kidney Injury'],\n                           [22, 'Diagnostic Imaging', 'Aged']]})\n\ndf['PMID'], df['col'] = list(zip(*df['col'].apply(lambda x: (x[:1][0], x[1:])).values))\n\n#                           col  PMID\n# 0        [Aged, Adult, Child]     1\n# 1     [Humans, Kidney Injury]    53\n# 2  [Diagnostic Imaging, Aged]    22\n"], "link": "https://stackoverflow.com/questions/48896522/optimize-extracting-list-from-dataframe-to-create-new-variable-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to make operations on an existing dataframe's column, based on other columns of the dataframe. For example: Here, I am trying to assign the value 0 to each element of the column that satifies the conditions in the statement. However I get the following errror : truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()", "q_apis": "get columns columns assign value get value Series empty bool item any all", "apis": "where", "code": ["import numpy as np\n\ndf['Value'] = np.where(((df['col1']== 'this is') & (df['col2'] == 30) & (df['col3'] <= '2020-01-01')), 0, df['Value'])\n"], "link": "https://stackoverflow.com/questions/62268620/error-while-assigning-conditions-to-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am running into an issue where I can't get my bar chart to show up in descending order of a column grouped by region. I have tried to order the values and then group by and plot on a bar chart. The graph still plots the values out in alphabetical order by region.", "q_apis": "get columns where get values plot values", "apis": "groupby mean sort_values plot", "code": ["# Manipulate the dataframe\ndf1 = df1.groupby('region').mean().sort_values(['AveragePrice'],ascending=False)\n\n# Plot the results\ndf1.plot(kind='bar', figsize=(15,5))\n"], "link": "https://stackoverflow.com/questions/55908012/sorting-bar-chart-values-that-have-been-grouped-in-descending-order"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "someone would help me optimize my solution of loading data from json files using json normalize and pd concat? My 5k json files like: I have to load data from 'state' and I must have the date (taken from json file name) on each city. My solution is Loading 1000 jsons takes a long time, let alone 5000. Is there any way to optimize my solution?", "q_apis": "get columns normalize concat date name time any", "apis": "add append append where DataFrame", "code": ["import pandas as pd\nfrom pathlib import Path\nimport json\n\n# each dict in states corresponds to a row\nstates = []\n\n# you can glob directly on pathlib.Path objects\nfor file in Path(\"my files_directory\").glob(\"*.json\"):\n\n    # load json data\n    with open(file) as jsonf:\n        data = json.load(jsonf)\n\n    # add the date from the filename stem to each dict, and append to list\n    for result in data:\n        for state in result[\"state\"]:\n            state[\"date\"] = file.stem\n            states.append(state)\n\n# create a df where each row corresponds to each dict in states\ndf = pd.DataFrame(states)\n"], "link": "https://stackoverflow.com/questions/59383661/loading-json-files-using-json-normalize-pd-concat"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "How do I correlate two pandas dataframes, find a single r value for all values? I don't want to correlate columns or rows, but all scalar values. One dataframe is the x axis, and the other dataframe is the y axis. I downloaded identically structured csv files here: https://www.gapminder.org/data/ The tables have years for columns, countries for rows, with numerical values for the indicator that each table reports. For instance, I want to see how the Political Participation Indicator (gapminder calls it an index, but I don't want to confuse it with a dataframe index) correlates overall with the Government Functioning Indicator, by year and country. You can correlate by column or row: But, I want a single r value that compares every field in one table with every corresponding field in the other table. Essentially, I want the r value of this scatterplot: (The example code won't color the plot like this, but plots the same points.) The second part of the question would be how to do this with tables that aren't exactly identical in structure. Every table (dataframe) I want to compare has country records and year columns, but not all of them have the same countries or years. In the example above, they do. How do I get a single r value for only the shared rows and columns of the dataframes?", "q_apis": "get columns value all values columns all values columns values index index year value value plot second identical compare year columns all get value columns", "apis": "DataFrame index columns DataFrame index columns DataFrame index columns stack stack stack keys", "code": ["import pandas as pd\n\nset1 = pd.DataFrame({1980:[4, 11, 0], 1981:[5, 10, 2], 1982:[0, 3, 1]},\n    index=pd.Index(['USA', 'UK', 'Iran'], name='country'))\nset1.columns.name = 'year'\nset1\n", "year     1980  1981  1982\ncountry                  \nUSA         4     5     0\nUK         11    10     3\nIran        0     2     1\n", "set2 = pd.DataFrame({1981:[2, 1, 10], 1982:[15, 1, 12], 1983:[10, 13, 1]},\n    index=pd.Index(['USA', 'UK', 'Turkey'], name='country'))\nset2.columns.name = 'year'\nset2\n", "year     1981  1982  1983\ncountry                  \nUSA         2    15    10\nUK          1     1    13\nTurkey     10    12     1\n", "set3 = pd.DataFrame({1980:[12, 11, 4], 1982:[9, 8, 11]},\n    index=pd.Index(['USA', 'UK', 'Turkey'], name='country'))\nset3.columns.name = 'year'\n", "df = pd.concat([set1.stack('year'), set2.stack('year'), set3.stack('year')],\n    keys=['set1', 'set2', 'set3'], names=['set'], axis=1)\ndf\n", "set           set1  set2  set3\ncountry year                  \nIran    1980   0.0   NaN   NaN\n        1981   2.0   NaN   NaN\n        1982   1.0   NaN   NaN\nTurkey  1980   NaN   NaN   4.0\n        1981   NaN  10.0   NaN\n        1982   NaN  12.0  11.0\n        1983   NaN   1.0   NaN\nUK      1980  11.0   NaN  11.0\n        1981  10.0   1.0   NaN\n        1982   3.0   1.0   8.0\n        1983   NaN  13.0   NaN\nUSA     1980   4.0   NaN  12.0\n        1981   5.0   2.0   NaN\n        1982   0.0  15.0   9.0\n        1983   NaN  10.0   NaN\n", "set       set1      set2      set3\nset                               \nset1  1.000000 -0.723632  0.509902\nset2 -0.723632  1.000000  0.606891\nset3  0.509902  0.606891  1.000000\n"], "link": "https://stackoverflow.com/questions/64216845/how-to-correlate-scalar-values-of-two-pandas-dataframes"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "The code under [6] does what I want. My question, however, is: is there a better solution? That is, more concise and/or more elegant.", "q_apis": "get columns", "apis": "loc any", "code": ["df.loc[:, df.any()]\n\n       a      d\n0   True  False\n1  False   True\n2   True  False\n", "       a      d\n0   True  False\n1  False   True\n2   True  False\n"], "link": "https://stackoverflow.com/questions/65452608/best-solution-for-selecting-the-columns-that-contain-at-least-one-true-value-in"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm having hard times trying to update data from a dataframe on specifics indexes. Example DataFrame: What I'm trying to do is: having an range of indexes, I want to update the data from an specific column of the dataset by a list of values. E.g.: Change all the values from the \"author\" column from index 100 to 200 by the values present in a list with the same size (100). I've been thinking of using something like iterrows() but I know that's a anti-pattern solution. So what would be the best approach?", "q_apis": "get columns update DataFrame update values all values index values size iterrows", "apis": "iloc", "code": ["names = ['a', 'b', 'c']\ndf.iloc[1:4, 0] = names  # +1 to the length (e.g. 100:201)\n\nprint(df)\n\n   author  episode_number episode_title quote\n0  Monica             1.0      Roommate    Hi\n1       a             1.0      Roommate    Ho\n2       b             1.0      Roommate    Ha\n3       c             1.0      Roommate    He\n4  Monica             1.0      Roommate    Hu\n"], "link": "https://stackoverflow.com/questions/61530422/change-values-of-rows-by-a-list-data-with-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a Dataframe which has a Datetime as Index and a column named \"Holiday\" which is an Flag with 1 or 0. So if the datetimeindex is a holiday, the Holiday column has 1 in it and if not so 0. I need a new column that says whether a given datetimeindex is the first day after a holiday or not.The new column should just look if its previous day has the flag \"HOLIDAY\" set to 1 and then set its flag to 1, otherwise 0. EDIT Doing: Has the Output: if you check the first timestamp for 2014-01-02 the DayAfter flag is set right. But the other flags are 0. Thats wrong.", "q_apis": "get columns Index first day day first timestamp right flags", "apis": "index add", "code": ["days = pd.Series(df[df.Holiday == 1].index).add(pd.DateOffset(1)).dt.date.unique()\n", "df['DayAfter'] = np.where(pd.Series(df.index).dt.date.isin(days),1,0)\n\n\n\n                 Holiday  AnyNumber  DayAfter\nDatum\n2014-01-01 20:00:00        1          9         0\n2014-01-01 20:30:00        1          2         0\n2014-01-01 21:00:00        1          3         0\n2014-01-01 21:30:00        1          3         0\n2014-01-01 22:00:00        1          6         0\n2014-01-01 22:30:00        1          1         0\n2014-01-01 23:00:00        1          1         0\n2014-01-01 23:30:00        1          1         0\n2014-01-02 00:00:00        0          1         1\n2014-01-02 00:30:00        0          2         1\n2014-01-02 01:00:00        0          1         1\n2014-01-02 01:30:00        0          1         1\n"], "link": "https://stackoverflow.com/questions/49833339/pandas-dataframe-new-column-which-checks-previous-day"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I'm still working on a pandas project with data from all the gym signups at my local school. I'm attempting to display a bar graph by (x = df.index.values, y = the count of values), the index consists of an array that looks like this This data represents every occurrence of a sign-up on a Monday, I'm now trying to graph this and I use the [\"Time\"] column as the x value on a bar plot. As I expected, the graph doesn't sort the values by I attempted to use regex to find the values that end in \"AM\" or \"PM\" and placed them in opposite arrays, but that has lead me down a dark path. Does anyone have a suggestion to sorting these values? Should I convert them into a datetime and then apply df.sort.values()? Here is the code that has gotten me to display this graph: Lastly, here is some an example of my original data! I hope I provided as much information as possible, thanks for reading this and being apart of the project which i'm having so much fun developing!", "q_apis": "get columns all at index values count values index array now value plot values values values apply values", "apis": "assign sort_values", "code": ["df =(\n    df.assign(\n        time = lambda x: pd.to_datetime(x['time']).dt.strftime('%H:%M:%S')\n    ).sort_values('time')\n)\n"], "link": "https://stackoverflow.com/questions/64374580/sorting-a-dataframe-index-by-ascending-time-values"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a very large dataframe (about 1.1M rows) and I am trying to sample it. I have a list of indexes (about 70,000 indexes) that I want to select from the entire dataframe. This is what Ive tried so far but all these methods are taking way too much time: Method 1 - Using pandas : Method 2 : I tried to write all the sampled lines to another csv. Can anyone please suggest a better method? Or how I can modify this to make it faster? Thanks", "q_apis": "get columns sample select all time all", "apis": "DataFrame size columns to_csv info DataFrame columns columns dtypes iloc", "code": ["import pandas as pd\nimport numpy as np\n\n\nindices = [1, 2, 3, 10, 20, 30, 67, 78, 900, 2176, 78776]\n\ndf = pd.DataFrame(np.random.randint(0, 100, size=(1000000, 4)), columns=list(\"ABCD\"))\ndf.to_csv(\"test.csv\", header=False)\ndf.info()\n", "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 4 columns):\nA    1000000 non-null int32\nB    1000000 non-null int32\nC    1000000 non-null int32\nD    1000000 non-null int32\ndtypes: int32(4)\nmemory usage: 15.3 MB\n", "idxs = pd.Index(indices)   \nsubset = df.iloc[idxs, :]\nprint(subset)\n", "pred = lambda x: x not in indices\ndata = pd.read_csv(\"test.csv\", skiprows=pred, index_col=0, names=\"ABCD\")\nprint(data)\n", "        A   B   C   D\n1      74  95  28   4\n2      87   3  49  94\n3      53  54  34  97\n10     58  41  48  15\n20     86  20  92  11\n30     36  59  22   5\n67     49  23  86  63\n78     98  63  60  75\n900    26  11  71  85\n2176   12  73  58  91\n78776  42  30  97  96\n"], "link": "https://stackoverflow.com/questions/39677183/quickly-sampling-large-number-of-rows-from-large-dataframes-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to drop rows which has int values in patient column of the dataframe. Here is my code: But I am getting How to get it right?", "q_apis": "get columns drop values get right", "apis": "fillna fillna loc fillna loc", "code": ["m=df['patient'].str.isdigit().fillna(False)\n#OR(use anyone since both are doing the same thing)\nm=df['patient'].str.isnumeric().fillna(False)\n#Finally:\ndf=df[~m]\n#OR\ndf.loc[~m]\n", "m=df['patient'].str.isalpha().fillna(False)\n#Finally:\ndf=df[m]\n#OR\ndf=df.loc[m]\n"], "link": "https://stackoverflow.com/questions/68271402/getting-keyerror-61-while-iterating-through-rows"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have data similar to this: Here, I have used the python function ffill() to fill the last finite value to the next nan cells.That's why the quantity is same for all the dates under a flag. Count is just the number of rows where flag is same. Now, I need to split the quantity column like this: So here 6 is divided as 2+2+1+1 because 6 needs to be divided in 4 rows. And similarly, 5 as 1+1+1+1+1 because I have more number of rows(9) than the value(5). So I can evenly distribute 5 as 1's in starting 5 rows. How can I do this in python?", "q_apis": "get columns ffill last value all where value", "apis": "groupby head count values iterrows loc", "code": ["def get_split(x, n):\n    if x < n:\n        return [1]*x + [0]*(n-x)\n    q = x//n\n    r = x%n\n    c = n-r\n    if r == 0:\n        return [q]*n\n    else:\n        return [q+1 if i>=c else q for i in range(n)]\n", "df['new qty'] = 0\ngroups = df.groupby('flag')\nfor key, grp in groups:  \n    x, n = grp.head(1)[['qty', 'count']].values[0]   \n    splits = sorted(get_split(x, n), reverse=True)   \n    j=0\n    for i, row in grp.iterrows():    \n        df.loc[i, 'new qty'] = splits[j]\n        j+=1\n\nprint(df)\n", "         Date Desciption  qty  flag  count  new qty\n0  2019-09-18          A    6     3      4        2\n1  2019-09-19          A    6     3      4        2\n2  2019-09-20          A    6     3      4        1\n3  2019-09-21          A    6     3      4        1\n4  2019-09-22          A    5     7      9        1\n5  2019-09-23          A    5     7      9        1\n6  2019-09-24          A    5     7      9        1\n7  2019-09-25          A    5     7      9        1\n8  2019-09-26          A    5     7      9        1\n9  2019-09-27          A    5     7      9        0\n10 2019-09-28          A    5     7      9        0\n11 2019-09-29          A    5     7      9        0\n12 2019-09-30          A    5     7      9        0\n"], "link": "https://stackoverflow.com/questions/62459194/how-to-split-an-integer-in-n-parts-each-integer"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe ranges from 2016 to 2019, shows the production numbers of a commodity in everyday. I have set the time as index and have converted it to date_time format. Now, I would like to find the average daily production over these years, But in a way that is aware of the weekdays. So, since we have 52 weeks in a year, the output indexes will be: 1st Monday, 1st Tuesday,...., 52nd Saturday, 52nd Sunday. For each index, it should find the mean of the produced numbers over the years 2016-2019. I guess that I should use something such as groupby month and groupby dayofweek I do not know how to implement it. Could you please guide me?", "q_apis": "get columns time index year index mean groupby month groupby dayofweek", "apis": "groupby mean reset_index", "code": ["def helper(my_date_time):\n    week_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday']\n    week_number = my_date_time.isocalendar()[1]\n    week_day = week_list[my_date_time.isocalendar()[2] - 1]\n    return (week_day + str(week_number))\n", "grouped_df = df.groupby(\"week_number_day\")\nmean_df = grouped_df.mean()\nmean_df = mean_df.reset_index()\nprint(mean_df)\n"], "link": "https://stackoverflow.com/questions/66660915/pandas-mean-of-the-data-within-the-same-day-of-week"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe (df) of the form- I need to convert this to an edge list i.e. a dataframe of the form: EDIT Note that the new dataframe has rows equal to the total number of possible pairwise combinations. Also, to compute the 'Weight' column, we simply find the intersection between the two lists. For instance, for B&C, the elements share two colors: Blue and Yellow. Therefore, the 'Weight' for the corresponding row is 2. What is the fastest way to do this? The original dataframe contains about 28,000 elements.", "q_apis": "get columns intersection between contains", "apis": "DataFrame index apply", "code": ["from itertools import combinations\n\ndf = pd.DataFrame({\n        'Col1': [['Green','Red','Purple'], \n                 ['Red', 'Yellow', 'Blue'], \n                 ['Brown', 'Green', 'Yellow', 'Blue']]\n     }, index=['A', 'B', 'C'])\n\ndf['Col1'] = df['Col1'].apply(set)    \ndf\n\n                           Col1\nA          {Purple, Red, Green}\nB           {Red, Blue, Yellow}\nC  {Green, Yellow, Blue, Brown}\n", "df1 = pd.DataFrame(\n    data=list(combinations(df.index.tolist(), 2)), \n    columns=['Src', 'Dst'])\n\ndf1\n\n  Src Dst\n0   A   B\n1   A   C\n2   B   C\n", "df1['Weights'] = df1.apply(lambda x: len(\n    df.loc[x['Src']]['Col1'].intersection(df.loc[x['Dst']]['Col1'])), axis=1)\ndf1\n\n  Src Dst  Weights\n0   A   B        1\n1   A   C        1\n2   B   C        2\n"], "link": "https://stackoverflow.com/questions/44992103/how-to-create-an-edge-list-from-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a dataframe that has field names placed in every field: I want to remove the extra field names on all data points in the data frame. The result should be: I can remove the extra field name by iterating through every data point of the data frame: This is very slow and its not pythonic. How can I remove the extra field names with high performance?", "q_apis": "get columns names names all name names", "apis": "apply index columns apply", "code": ["df = df.apply(lambda x: x.str.split('=').str[-1])\n\nprint(df)\n\n  index     name ngram field slop\n0     1  unknown    00  body    0\n1     2  unknown    01  body    0\n2     3  unknown    02  body    0\n", "d = dict(zip(df, df.columns.map(len)))\ndf = df.apply(lambda x: x.str[d[x.name]+1:])\n"], "link": "https://stackoverflow.com/questions/52865504/remove-extra-characters-in-all-data-points-in-a-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "This may be a trivial question. I have the following dataframe that the columns contain lists. The values in of the list represent time intervals. I am trying to convert all these elements into a time format. I am trying to get something like this:", "q_apis": "get columns columns values time all time get", "apis": "join columns apply join columns apply", "code": ["functime = lambda x: '-'.join([t[:-2] + ':' + t[-2:] for t in x])\n\nfor col in df.columns:\n    df[col] = df[col].apply(functime)\n", "print(df)\n         time1        time2\n0  20:00-23:00  22:00-24:00\n1  10:00-11:00    8:00-9:00\n", "def functime2(x):\n    val = '-'.join([t[:-2] + ':' + t[-2:] for t in x])\n\n    return val\n\nfor col in df.columns:\n    df[col] = df[col].apply(functime2)\n", "         time1        time2\n0  20:00-23:00  22:00-24:00\n1  10:00-11:00    8:00-9:00\n"], "link": "https://stackoverflow.com/questions/56433503/using-lambda-or-apply-on-multiple-columns-of-a-dataframe-with-lists-contents"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a as follows: I wanted to use the ID as an index if it's value is common across all columns, or fill with NA for the columns with missing ID values. I tried the following one liner: It results in making a list of values in as row values. At the end, I would like to have a dataframe which looks like this: With as index and all other columns as columns. Fill the missing values filled with . In other words, if the is common accross the columns, then fill the value in otherwise fill with", "q_apis": "get columns index value all columns columns values values values index all columns columns values columns value", "apis": "DataFrame columns columns DataFrame iloc iloc pivot index columns values", "code": ["# Bring your frame into normalized format\ndf2 = pd.DataFrame()\nfor i in range(len(df.columns)//2):\n    key = df.columns[2*i+1]\n    dfx = pd.DataFrame()\n    dfx['ID'] = df.iloc[:,2*i]\n    dfx['Key'] = key\n    dfx['Value'] = df.iloc[:,2*i+1]\n    df2=pd.concat([df2,dfx], sort=False)\n\n# Pivot\ndf_res = df2.pivot(index='ID', columns='Key', values='Value')\n"], "link": "https://stackoverflow.com/questions/57559071/setting-index-using-commonly-occurring-column-value-as-index-of-the-data-frame"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to downsample a pandas dataframe in order to reduce granularity. In example, I want to reduce this dataframe: to this (downsampling to obtain a 2x2 dataframe using mean): Is there a builtin way or efficient way to do it or I have to write it on my own? Thanks", "q_apis": "get columns mean", "apis": "index columns index columns index columns groupby index columns", "code": ["In [11]: df.groupby(lambda x: x//2).mean()\nOut[11]:\n     0    1  2    3\n0  1.5  3.0  3  3.5\n1  2.5  1.5  2  2.5\n", "In [12]: df.groupby(lambda x: x//2).mean().groupby(lambda y: y//2, axis=1).mean()\nOut[12]:\n      0     1\n0  2.25  3.25\n1  2.00  2.25\n", "In [21]: df = pd.DataFrame(np.random.randn(100, 100))\n\nIn [22]: %timeit df.groupby(lambda x: x//2).mean().groupby(lambda y: y//2, axis=1).mean()\n1000 loops, best of 3: 1.64 ms per loop\n\nIn [23]: %timeit viktor()\n1 loops, best of 3: 822 ms per loop\n", "In [31]: df = pd.DataFrame(np.random.randn(1000, 1000))\n\nIn [32]: %timeit df.groupby(lambda x: x//2).mean().groupby(lambda y: y//2, axis=1).mean()\n10 loops, best of 3: 42.9 ms per loop\n\nIn [33]: %timeit viktor()\n# crashes\n", "df_index, df_cols, df.index, df.columns = df.index, df.columns, np.arange(len(df.index)), np.arange(len(df.columns))\nres = df.groupby(...\nres.index, res.columns = df_index[::2], df_cols[::2]\n"], "link": "https://stackoverflow.com/questions/18825412/how-to-downsample-a-pandas-dataframe-by-2x2-averaging-kernel"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "we are using this df: We need to Plot the TenYearRisk of disease over the age distribution. output should be like this: I think we need to use pivot_table but dont know how to split the columns to yes and no. I got only this: and the output is:", "q_apis": "get columns pivot_table columns", "apis": "groupby mean DataFrame", "code": ["age                                                                                                                     \n32    0.000000                                                                                                          \n33    0.000000                                                                                                                      \n34    0.000000                                                                                                          \n35    0.047619\n....               \n", "hd = heart_df.groupby('age').mean().TenYearCHD\nnHd = 1. - hd\npd.DataFrame({'Yes':hd,'No':nHd})     \n", "      Yes        No\nage\n32   0.000000  1.000000\n33   0.000000  1.000000\n34   0.000000  1.000000\n35   0.047619  0.952381\n36   0.035714  0.964286                                                                             \n"], "link": "https://stackoverflow.com/questions/65399433/python-how-to-extract-from-a-df-a-column-that-will-be-the-index-and-that-vluese"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following csv in this format: As you can see, in the column, a leading zero is needed for the milliseconds part. should optimally be converted to . Moreover, I need to combine and into a single index column. So far here is my attempt: Everything is good except for the milliseconds part. How can I solve this? EDIT: My apologies, I realised I need to add a leading zero to ALL UNITS of time, not just milliseconds. E.g. should become", "q_apis": "get columns combine index add time", "apis": "apply", "code": ["from datetime import datetime\nimport pandas as pd\n\ndf = pd.read_csv(\"data.csv\", parse_dates=[[\"Date\", \"Time\"]])\nprint(df)\n\n\ndef parse_time(datetime_str):\n    date_str, time_str = datetime_str.split(\" \")\n    hour, minute, sec, msec = time_str.split(\":\")\n    day, month, year = date_str.split(\"/\")\n    return datetime(\n        int(year), int(month), int(day), int(hour), int(minute), int(sec), int(msec) * 1000\n    )\n\n\ndf[\"Date_Time\"] = df[\"Date_Time\"].apply(parse_time)\nprint(df)\n", "               Date_Time  Dummy\n0  22/7/2020 11:5:49:774    123\n1  22/7/2020 11:5:49:868    123\n2  22/7/2020 11:5:50:24     123\n3  22/7/2020 11:5:50:196    123\n4  22/7/2020 11:5:50:414    123\n5  22/7/2020 11:5:50:730    123\n", "                Date_Time  Dummy\n0 2020-07-22 11:05:49.774    123\n1 2020-07-22 11:05:49.868    123\n2 2020-07-22 11:05:50.024    123\n3 2020-07-22 11:05:50.196    123\n4 2020-07-22 11:05:50.414    123\n5 2020-07-22 11:05:50.730    123\n"], "link": "https://stackoverflow.com/questions/63040429/add-leading-zeroes-to-the-minute-part-of-a-datetime-column"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "In a pandas dataframe, I want to search row by row for multiple string values. If the row contains a string value then the function will add/print for that row, into an empty column at the end of the df 1 or 0 based upon There have been multiple tutorials on how to select rows of a Pandas DataFrame that match a (partial) string. For Example: I'm pulling the above example from this website: https://davidhamann.de/2017/06/26/pandas-select-elements-by-string/ How would I do a multi-value search of the entire row for: 'int', 'tos', '198'? Then print into a column next discontinued, a column int that would have 1 or 0 based upon whether the row contained that keyword.", "q_apis": "get columns values contains value add empty at select DataFrame select value", "apis": "iterrows", "code": ["def check_all_for(column_name, search_terms):\n    df[column_name] = ''\n    for row in df.iterrows():\n        flag = 0\n        for element in row:\n            for search_term in search_terms:\n                if search_term in (str(element)).lower():\n                    flag = 1\n        row[column_name] = flag\n"], "link": "https://stackoverflow.com/questions/50845987/search-for-multiple-string-values-of-entire-row-of-dataframe-in-python-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following , is of with format; is of string with or format if non-digit characters removed; How to convert into , and set a flag to if is within +/- 180 days of not considering that it does not have the year and any date format that it converted to; the result should looks like,", "q_apis": "get columns days year any date", "apis": "replace le", "code": ["s=pd.to_datetime(df['date_string'].str.replace(r'\\D+', ''),format='%d%m')\n#df.doc_date=pd.to_datetime(df.doc_date) convert to datetime if not already datetime\ndf['withith_180'] = (df.doc_date.dt.dayofyear-s.dt.dayofyear).le(180)\n", "    doc_date date_string  withith_180\n0 2019-06-03      WW0306         True\n1 2019-06-07      EH0706         True\n"], "link": "https://stackoverflow.com/questions/56635186/pandas-how-to-calculate-delta-only-given-month-and-day"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I am trying to convert a list of strings to its ascii and place each character in columns in a dataframe. I have 30M such strings and I am running into memory issues with the code I'm running. For example: would like to get the following dataframe: What I have tried: Error: Not sure if relevant but is actually a column from another dataframe with . Also, the longest string is almost 255 characters long. I know 30M x 1000 is a big number. Any way I can get around this issue?", "q_apis": "get columns columns get get", "apis": "max apply astype info DataFrame columns columns dtypes size apply info DataFrame columns columns dtypes", "code": ["import pandas as pd\nimport numpy as np\nstrings = ['a','asd','1234','ewq']\n\n# Find padding length\nmaxlen = max(len(s) for s in strings)\n\n# Use 8 bit integer with pandas sparse data type, compressing zeros\ndt = pd.SparseDtype(np.int8, 0)\n\n# Create the sparse dataframe from a pandas Series for each integer ord value, padded with zeros\n# NOTE: This compresses the dataframe after creation. I couldn't find the right way to compress\n# each series as the dataframe is built\n\nsdf = stringsSeries.apply(lambda s: pd.Series((ord(c) for c in s.ljust(maxlen,chr(0))))).astype(dt)\nprint(f\"Memory used: {sdf.info()}\")\n\n# <class 'pandas.core.frame.DataFrame'>\n# RangeIndex: 4 entries, 0 to 3\n# Data columns (total 4 columns):\n# 0    4 non-null Sparse[int8, 0]\n# 1    4 non-null Sparse[int8, 0]\n# 2    4 non-null Sparse[int8, 0]\n# 3    4 non-null Sparse[int8, 0]\n# dtypes: Sparse[int8, 0](4)\n# memory usage: 135.0 bytes\n# Memory used: None\n\n# The original uncompressed size\ndf = stringsSeries.apply(lambda s: pd.Series((ord(c) for c in s.ljust(maxlen,chr(0)))))\nprint(f\"Memory used: {df.info()}\")\n\n# <class 'pandas.core.frame.DataFrame'>\n# RangeIndex: 4 entries, 0 to 3\n# Data columns (total 4 columns):\n# 0    4 non-null int64\n# 1    4 non-null int64\n# 2    4 non-null int64\n# 3    4 non-null int64\n# dtypes: int64(4)\n# memory usage: 208.0 bytes\n# Memory used: None\n"], "link": "https://stackoverflow.com/questions/57280596/given-a-column-with-string-data-create-a-dataframe-with-ascii-equivalent-of-each"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I try to get my head around this problem. I have three dataframes and I would like to merge (concatenate?) two of these dataframes based on values inside a third one. Here are the dataframes: df1: df2: Columns in df1 and df2 are different but their relationship is in df3. df3: I would like to merge the data in df1 and df2 but keep the same columns as in d1 (as b1, b2, b3 are referenced with a1, a2, a3, a4 and a5). Here is df4, the desired dataframe I want. df4: many thanks in advance,", "q_apis": "get columns get head merge values merge columns", "apis": "melt index merge drop index drop pivot_table index index columns values reset_index", "code": ["df2_melt = df2.melt([\"index\", \"fields\"], var_name=\"product2\")\n", "merged = pd.merge(df2_melt, df3.drop(\"index\", axis=1), on=\"product2\")\\\n    .drop(\"product2\", axis=1)\n", "new_rows = pd.pivot_table(merged, index=[\"index\", \"fields\"],\n                          columns=\"product1\", values=\"value\")\\\n    .reset_index()\n", "product1    index       fields  a1      a2      a3      a4      a5\n0           2018-06-01  price   1.1     2.1     3.1     4.1     5.1\n1           2018-06-01  amount  15.0    25.0    35.0    45.0    55.0\n2           2018-06-01  clients 1.0     1.0     2.0     2.0     3.0\n3           2018-06-02  price   1.2     2.2     3.2     4.2     5.2\n4           2018-06-02  amount  16.0    26.0    36.0    46.0    56.0\n5           2018-06-02  clients 1.0     1.0     2.0     2.0     3.0\n6           2018-06-03  price   1.3     2.3     3.3     4.3     5.3\n7           2018-06-03  amount  17.0    27.0    37.0    47.0    57.0\n8           2018-06-03  clients 1.0     1.0     2.0     2.0     3.0\n"], "link": "https://stackoverflow.com/questions/50907980/adding-row-values-to-a-dataframe-based-on-matching-column-labels"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have the following DataFrame: Looking like this: I want the null values to be filled with: \"Average(All the preceding values before null, first non-null succeeding value after null)\" Note:If the first succeeding value after null is also Null, then the code should look for the first succeeding value which is not null . Example: Row 2 of column A should be filled with Average(1550,1540) = 1545 Here \"All Preceding value before null\" = 1550, \"First non null succeeding value after null\" = 1540 Similarly, row 3 of column A should be filled with Average(1550,1545,1540) = 1545 Here all the preceding value before the null are 1550 and 1545(1545 is what we found in the above step) First non null succeeding value after null is again 1540 . It goes on like that and row 9 of column A should be filled with Average(All the values before the null, 1590) 1590 is now the first non null succeeding value after null. So at the end my desired output in Column A looks like this: Similarly i wanted my null values to be filled for all the other columns as well. Since i am new to python I dont know how to write a code for this. Any help on the code is much appreciated.", "q_apis": "get columns DataFrame values values first value first value first value value value all value step value values now first value at values all columns", "apis": "any isna loc index isna dropna all index loc isna expanding mean loc", "code": ["def foo(row):\n    if any(row.isna()):\n        next_non_null = df.loc[df.index>row.name, row.isna()].dropna(how='all').index[0]\n        df.loc[row.name, row.isna()] = df.expanding().mean().loc[next_non_null, :]\n", "             A          B     C          D         E         F      G     H\n1   1550.000000  41.000000  9.41  22.600000  4.740000  3.200000  11.64  2.23\n2   1545.000000  42.000000  9.41  22.450000  4.790000  3.160000  11.64  2.23\n3   1545.000000  42.000000  9.41  22.450000  4.790000  3.160000  11.64  2.23\n4   1545.000000  42.000000  9.41  22.450000  4.790000  3.160000  11.64  2.23\n5   1545.000000  42.000000  9.41  22.450000  4.790000  3.160000  11.64  2.23\n6   1545.000000  42.000000  9.41  22.450000  4.790000  3.160000  11.64  2.23\n7   1545.000000  42.000000  9.41  22.450000  4.790000  3.160000  11.64  2.23\n8   1540.000000  43.000000  9.41  22.300000  4.840000  3.120000  11.64  2.23\n9   1550.000000  41.666667  9.41  22.588889  4.784444  3.142222  11.64  2.23\n10  1550.000000  41.666667  9.41  22.588889  4.784444  3.142222  11.64  2.23\n11  1550.000000  41.666667  9.41  22.588889  4.784444  3.142222  11.64  2.23\n12  1550.000000  41.666667  9.41  22.588889  4.784444  3.142222  11.64  2.23\n13  1590.000000  39.000000  9.41  23.700000  4.740000  3.000000  11.64  2.23\n14  1549.285714  41.619048  9.41  22.582540  4.781270  3.146349  11.64  2.23\n15  1540.000000  41.000000  9.41  22.500000  4.740000  3.200000  11.64  2.23\n"], "link": "https://stackoverflow.com/questions/62940115/how-can-i-fill-the-null-values-in-a-datafame-with-a-specific-condition-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I want to append data from column and a default phrase at the same time into a Pandas DataFrame, has many columns, not only id_sin & extra... I tried this: This is the expected result: But I am getting an error, please help... thanks in advance!", "q_apis": "get columns append at time DataFrame columns", "apis": "DataFrame DataFrame", "code": ["import pandas as pd\ndb = pd.DataFrame({'id_sin':['s123','s124','s125','s126'],\n               'extra':['abc','def','ghi','jkl'],\n               ...\n})\n\ndf = pd.DataFrame()\ndf['id_sin'] = db[['id_sin']]\ndf['Phrase'] = 'Default Phrase'\n"], "link": "https://stackoverflow.com/questions/55694403/append-column-and-default-data-into-new-pandas-dataframe"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a nparray shows below. Now I want to get a new dataframe or nparray that contains coordinates of each value. For example: How to achieve it? Thank you very much!", "q_apis": "get columns get contains value", "apis": "DataFrame explode values assign reset_index drop", "code": ["vals, ixs = ndix_unique(a)\ndf = pd.DataFrame({'c':vals, 'xy':ixs}).explode('xy')\nx, y = zip(*df.xy.values.tolist())\ndf = df[['c']].assign(x=x, y=y).reset_index(drop=True)\n", "print(df)\n      c    x  y\n0     A    0  1\n1     A    6  1\n2     B    0  2\n3     B    8  1\n4     B   12  1\n5     B    4  2\n6     C    5  1\n7     C    9  2\n....\n"], "link": "https://stackoverflow.com/questions/62001117/how-to-get-coordinates-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "My dataframe is like this- It goes on like this to the month 2015-12. So you can imagine all the data. I want to plot a continuous graph with the as the x-axis and the as the y-axis. How to best represent this using ? I would also like to know for my knowledge if there's a way to print as on the x-axis and so on. Probably a lambda function or something while plotting.", "q_apis": "get columns month all plot", "apis": "plot", "code": ["from datetime import datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\ndf = <set_your_data_frame_here>\n\nmyDates = pd.to_datetime(df['Month'])\nmyValues = df['Energy_MWh']\nfig, ax = plt.subplots()\nax.plot(myDates,myValues)\n\nmyFmt = DateFormatter(\"%b-%Y\")\nax.xaxis.set_major_formatter(myFmt)\n\n## Rotate date labels automatically\nfig.autofmt_xdate()\nplt.show()\n"], "link": "https://stackoverflow.com/questions/53576942/represent-a-continuous-graph-using-matplotlib-and-pandas"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "it is my first question and I would like to know a smart way of doing the following: I have a large dataset that looks something like this: identifier name group period product gross_sales net_sales expense 1 nameone groupone q1 baloons 20000 10000 0 1 nameone groupone q1 cartoons 2000 10000 0 1 nameone groupone q2 cartoons 20000 10000 0 2 nametwo groupone q1 baloons 1 1000 0 3 namethree grouptwo q4 cartoons 0 0 0 1 nameone groupone q1 expense 0 -1000 1000 I would like to distribute the expense and the gross sales of each identifier that has an expense in the product column ([product == expense]) in its sales entries ([volume > 0 and (product != expense)]) using gross_sales to apportion proportionately by product. The DF would look something like this at the end: identifier name group period product gross_sales net_sales expense 1 nameone groupone q1 baloons 20000 9500 500 1 nameone groupone q1 cartoons 20000 9500 500 1 nameone groupone q2 cartoons 20000 10000 0 2 nametwo groupone q1 baloons 20000 1000 0 3 namethree grouptwo q4 cartoons 0 0 0 Thank you ! :D A solution previously presented by @Andrej Kesely pointed me to: And I got it working, but it is dividing the expense equally among the products and I need it to be proportionately. I then tried : And although it worked, it was not properly, the sum of all expenses did not totaled the amount pre transform. Thank you!!", "q_apis": "get columns first name product product product product product at name product sum all transform", "apis": "product groupby first agg groupby transform get product product groupby first agg groupby transform sum get product", "code": ["m = df[\"product\"] == \"expense\"\nexpenses = df[m].groupby([\"name\", \"group\"])[\"expense\"].first().agg(dict)\n\ndf[\"expense\"] = (\n    df[~m]\n    .groupby([\"name\", \"group\"])[\"volume\"]\n    .transform(lambda x: expenses.get(x.name, np.nan) / len(x))\n)\nprint(df[~m])\n", "   identifier       name     group period   product  volume  net_sales  expense\n0           1    nameone  groupone     q1   baloons      10      10000    500.0\n1           1    nameone  groupone     q1  cartoons       1      10000    500.0\n2           2    nametwo  groupone     q1   baloons       1       1000      NaN\n3           3  namethree  grouptwo     q4  cartoons       0          0      NaN\n", "m = df[\"product\"] == \"expense\"\nexpenses = df[m].groupby([\"identifier\", \"period\"])[\"expense\"].first().agg(dict)\n\ndf[\"expense\"] = (\n    df[~m]\n    .groupby([\"identifier\", \"period\"])[\"gross_sales\"]\n    .transform(\n        lambda x: [(gs / x.sum()) * expenses.get(x.name, np.nan) for gs in x]\n    )\n)\nprint(df[~m])\n", "   identifier       name     group period   product  gross_sales  net_sales     expense\n0           1    nameone  groupone     q1   baloons        20000      10000  909.090909\n1           1    nameone  groupone     q1  cartoons         2000      10000   90.909091\n2           1    nameone  groupone     q2  cartoons        20000      10000         NaN\n3           2    nametwo  groupone     q1   baloons            1       1000         NaN\n4           3  namethree  grouptwo     q4  cartoons            0          0         NaN\n"], "link": "https://stackoverflow.com/questions/67235018/tidying-data-how-to-create-a-column-distributing-values-of-specific-rows-into"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I need to delete all empty sheets from a workbook, and these sheets have headers, so they are not completely empty. Just row 2 and on will be empty and I need to delete these sheets. I currently have this: This works fine for completely empty sheets but my sheets have headers. Any idea how I can manipulate this to work how I need it to? Thanks in advance, any guidance is appreciated!", "q_apis": "get columns delete all empty empty empty delete empty any", "apis": "empty dropna all dropna all to_excel index", "code": ["def DeleteEmptyColumnsAndRows(filename):\n    import pandas as pd\n    full_path = filename\n    df_with_header = pd.read_excel(full_path, header=None, sheet_name=None)\n    df_without_header = pd.read_excel(full_path, header=0, sheet_name=None)\n\n\n    # engine can be openpyxl if we need .xlsx ext\n    writer = pd.ExcelWriter(new_loc, engine='xlwt') \n    for key in df_with_header:\n        if not df_without_header[key].empty:\n            print(df_with_header[key])\n            sheet = df_with_header[key].dropna(how=\"all\").dropna(1, how=\"all\")\n            sheet.to_excel(writer, key, index=False, header=False)\n    writer.save()\n"], "link": "https://stackoverflow.com/questions/65357979/how-to-delete-empty-sheets-that-have-a-header-row-from-excel-workbook-in-python"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a list of dataframes (lst_dfs) of which i want to subset columns using a list of partial column names (lst). The list of the columns needs to be applied with startswith as sometimes the end of the identifier is different. the index of the list of dataframes matches the index of the list of names. Its easy to apply with one dataframe, but not with this list/in a loop. The expected output would be a dictionary containing a list of two dataframes with the subsetted columns but its returning empty. I think my level of iteration is incorrect (amongst other things?). any help is very appreciated. thanks so much! two data framees that i put in to a list", "q_apis": "get columns columns names columns index index names apply columns empty any put", "apis": "append filter join", "code": ["from collections import defaultdict\ndat = defaultdict(list)\n\nfor i, df in enumerate(lst_dfs):\n    for elem in lst:\n        dat[i].append(df.filter(regex='^('+'|'.join(elem)+')', axis=1))\n", ">>> dat[0]\n[   am.1  abn.1  b1c  b1d\n0     1      1    1    1\n1     1      1    1    1\n2     1      1    1    1\n3     1      1    1    1\n4     1      1    1    1,    am.1  b1c  b1d\n0     1    1    1\n1     1    1    1\n2     1    1    1\n3     1    1    1\n4     1    1    1]\n"], "link": "https://stackoverflow.com/questions/67990656/python-loop-through-list-of-dataframes-and-list-of-lists"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a that looks something like this: col1 col2 col3 80% 10% SP 90% 0% SP 90% 10% SP 70% SP 20% 90% SP 0% As you can see, the values have a sign appended onto them, I could usually remove this by using a function and using however, I cannot do this because the columns currently contain strings such as which throws an error when doing this Any ideas as to how to do this?", "q_apis": "get columns values columns", "apis": "applymap", "code": ["df.applymap(lambda x : float(x.strip('%')) / 100 if x.endswith('%') else x)"], "link": "https://stackoverflow.com/questions/65598546/i-want-to-remove-a-from-entries-in-a-column-to-create-floats-without-removin"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a data structure like this: Each key in the dictionaries has values of type integer, string or list of strings (not all keys are in all dicts present), each dictionary represents a row in a table; all rows are given as the list of dictionaries. How can I easily import this into Pandas? I tried but here I get an \"ValueError: arrays must all be same length\" error.", "q_apis": "get columns values all keys all all get all length", "apis": "mean DataFrame", "code": ["data = [{\n  \"name\": \"leopard\",\n  \"character\": \"mean\",\n  \"skills\": [\"sprinting\", \"hiding\"],\n  \"pattern\": \"striped\",\n },\n {\n  \"name\": \"antilope\",\n  \"character\": \"good\",\n  \"skills\": [\"running\"],\n }]\ndf = pd.DataFrame(data)\nprint(df)\n", "  character      name  pattern               skills\n0      mean   leopard  striped  [sprinting, hiding]\n1      good  antilope      NaN            [running]\n"], "link": "https://stackoverflow.com/questions/55235648/how-to-create-a-pandas-dataframe-from-row-based-list-of-dictionaries"}
{"id": 660, "q": "Python Dataframe get the NaN columns for each row", "d": "I have two Data Frames as below. I want to check two columns, named and in these data frames, and if they match, then merge them together. What I mean is that I want to read the DF1 data (store id and store name) into DF2 data (because the DF2 is much bigger) if these two column are matched. I wrote the code below : but it gives me error , can any one help me? I use the pd.concat, but it does not work again.", "q_apis": "get columns columns merge mean name any concat", "apis": "astype astype astype astype", "code": ["df1['lat'] = df1.lat.astype(float)\ndf2['lat'] = df2.lon.astype(float)\n", "df1['lat'] = df1.lat.astype(str)\ndf2['lat'] = df2.lon.astype(str)\n"], "link": "https://stackoverflow.com/questions/66714742/data-type-error-while-merging-pandas-dataframes"}
