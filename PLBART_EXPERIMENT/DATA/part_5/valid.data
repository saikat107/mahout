{"id": 194, "q": "pandas series add previous row if diff negative", "d": "I have a df that contains some revenue values and I want to interpolate the values to the dates that are not included in the index. To do so, I am finding the difference between rows and interpolating: I have this in a function and it is looped over thousands of such calculations (each one creating such a df). This works for most cases, but there are a few where the 'checkout till' resets and thus the diff is negative: The above code will give out negative interpolating values, so I am wondering whether there is a quick way to take that into account when it happens, without putting too much toll on the execution time because it's called thousands of times. The end result for the revenue df (before the interpolation is carried out) should be: So basically if there is a 'reset', the diff should be added to the value in the row above. And that will happen for all rows below. I hope this makes sense. I am struggling to find a way of doing it which is not costly computationally. Thanks in advance.", "q_apis": "add diff contains values interpolate values index difference between where diff values take time diff value all", "io": " revenue 2015-10-19 203.0 2016-04-03 271.0 2016-06-13 301.0 2016-06-13 0.0 2016-09-27 30.0 2017-03-14 77.0 2017-09-19 128.0 2018-09-19 0.0 2018-03-19 10.0 2019-03-22 287.0 2020-03-20 398.0 <s> revenue 2015-10-19 203.0 2016-04-03 271.0 2016-06-13 301.0 2016-09-27 331.0 2017-03-14 378.0 2017-09-19 429.0 2018-03-19 439.0 2019-03-22 716.0 2020-03-20 827.0 ", "apis": "reset_index difference loc values values get index values values at loc sum index drop_duplicates set_index index index name", "code": ["import pandas as pd\nimport numpy as np\n\ndf.reset_index(inplace=True)\n\n# 1. compute difference\ndf[\"rev_diff\"] = 0.0\ndf.loc[1:, \"rev_diff\"] = df[\"revenue\"].values[1:] - df[\"revenue\"].values[:-1]\n\n# get breakpoint locations\nbreakpoints = df[df[\"rev_diff\"] < 0].index.values\n\n# 2. accumulate the values to be added\ndf[\"rev_add\"] = 0.0\nfor idx in breakpoints:\n    add_value = df.at[idx-1, \"revenue\"]\n    df.loc[idx:, \"rev_add\"] += add_value  # accumulate\n\n# 3. sum up\ndf[\"rev_new\"] = df[\"revenue\"] + df[\"rev_add\"]\n\n# 4. remove duplicate rows\ndf_new = df[[\"index\", \"rev_new\"]].drop_duplicates().set_index(\"index\")\ndf_new.index.name = None\n"], "link": "https://stackoverflow.com/questions/64404294/pandas-series-add-previous-row-if-diff-negative"}
{"id": 446, "q": "Plot partial dependency with model built from pandas data frame", "d": "I have a model that is trained from a pandas dataframe. It can predict dataframe input without problem: However, when I use the exact data and model to plot the partial dependency graph, I have the following error: The code I used was: I understand that I can convert X_train to numpy.ndarray before training the model, and it solves the problem. However, as the actual classifier is very large and it took a long time to train already, I would like to re-use the classifier that was trained with pandas dataframe. Is there a way to do that? Thank you very much! Edit the OP to include some sample data: X_train.head(10): y_train.head(10):", "q_apis": "plot time sample head head", "io": " a b c d e 0 34 226830 5249738 409 1186.78850 1 36 38940 8210911 76 2326.72880 2 36 38940 8210911 76 2326.72880 3 34 761188 5074516 698 370.27365 4 36 1097060 9072727 296 576.91693 5 36 1097060 9072727 296 576.91693 6 25 62240 881740 102 194.59651 7 25 62240 881740 102 194.59651 8 25 62240 881740 102 194.59651 9 28 65484 1391620 105 259.25095 <s> 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 ", "apis": "values value plot columns values", "code": ["# store keyword argument default values\ntmpdefaults = XGBClassifier.predict_proba.__defaults__\n# change default value of validate_features to False\nXGBClassifier.predict_proba.__defaults__ = (None, False)\n\n# plot\nplot_partial_dependence(estimator=clf, X=X_train, features=[0, 1], feature_names=X_train.columns.tolist())\nplt.show()\n\n# reset default keyword argument values to original\nXGBClassifier.predict_proba.__defaults = tmpdefaults\n"], "link": "https://stackoverflow.com/questions/57262708/plot-partial-dependency-with-model-built-from-pandas-data-frame"}
{"id": 423, "q": "Filtering rows of a dataframe based on values in columns", "d": "I want to filter the rows of a dataframe that contains values less than ,say 10. gives, Expected: Any suggestions on how to obtain expected result?", "q_apis": "values columns filter contains values", "io": "A B C D 0 5.0 NaN NaN NaN 1 NaN NaN NaN NaN 2 0.0 NaN 6.0 NaN 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN 5 6.0 NaN NaN NaN 6 NaN NaN NaN NaN 7 NaN NaN NaN 7.0 8 NaN NaN NaN NaN 9 NaN NaN NaN NaN <s> 0 5 57 87 95 2 0 80 6 82 5 6 33 74 75 7 71 44 60 7 ", "apis": "DataFrame size columns any dtype bool", "code": ["np.random.seed(21)\ndf = pd.DataFrame(np.random.randint(0,100,size=(10, 4)), columns=list('ABCD'))\n", "print (df < 10)\n       A      B      C      D\n0  False  False  False   True\n1  False  False  False  False\n2  False  False  False  False\n3  False  False  False  False\n4  False  False  False  False\n5   True  False  False  False\n6  False  False  False  False\n7   True  False  False  False\n8  False  False  False  False\n9   True  False  False  False\n\nprint ((df < 10).any(axis=1))\n0     True\n1    False\n2    False\n3    False\n4    False\n5     True\n6    False\n7     True\n8    False\n9     True\ndtype: bool\n"], "link": "https://stackoverflow.com/questions/58128354/filtering-rows-of-a-dataframe-based-on-values-in-columns"}
{"id": 238, "q": "Drop consecutive duplicates in Pandas dataframe if repeated more than n times", "d": "Building off the question/solution here, I'm trying to set a parameter that will only remove consecutive duplicates if the same value occurs 5 (or more) times consecutively... I'm able to apply the solution in the linked post which uses to check if the previous (or a specified value in the past or future by adjusting the shift periods parameter) equals the current value, but how could I adjust this to check several consecutive values simultaneously? Suppose a dataframe that looks like this: I'm trying to achieve this: Where we lose rows 4,5,6,7 because we found five consecutive 3's in the y column. But keep rows 1,2 because it we only find two consecutive 2's in the y column. Similarly, keep rows 8,9,10,11 because we only find four consecutive 4's in the y column.", "q_apis": "value apply value shift equals value values", "io": "x y 1 2 2 2 3 3 4 3 5 3 6 3 7 3 8 4 9 4 10 4 11 4 12 2 <s> x y 1 2 2 2 3 3 8 4 9 4 10 4 11 4 12 2 ", "apis": "diff ne cumsum groupby transform size duplicated", "code": ["thresh = 5\ns = df['y'].diff().ne(0).cumsum()\n\nsmall_size = s.groupby(s).transform('size') < thresh\nfirst_rows = ~s.duplicated()\n\ndf[small_size | first_rows]\n"], "link": "https://stackoverflow.com/questions/63182136/drop-consecutive-duplicates-in-pandas-dataframe-if-repeated-more-than-n-times"}
{"id": 422, "q": "What is the Right Syntax When Using .notnull() in Pandas?", "d": "I want to use on several columns of a dataframe to eliminate the rows which contain \"NaN\" values. Let say I have the following : I tried to use this syntax but it does not work? do you know what I am doing wrong? I get this Error: What should I do to get the following output? Any idea?", "q_apis": "notnull columns values get get", "io": " A B C 0 1 1 1 1 1 NaN 1 2 1 NaN NaN 3 NaN 1 1 <s> A B C 0 1 1 1 ", "apis": "dropna any", "code": ["print (df.dropna(subset=['A', 'B', 'C'], how='any'))\n"], "link": "https://stackoverflow.com/questions/38702332/what-is-the-right-syntax-when-using-notnull-in-pandas"}
{"id": 355, "q": "Name of the dataframe from which the minimum value is taken", "d": "I have 3 data-frames as below. & With the code , I get a dateframe which has the min value of each cell of these 3 dataframes Now, my question is there a way to get a dataframe which shows from which dataframe these individual values have come from? The expected out put is as below Is this even possible in Pandas?", "q_apis": "value get min value get values put", "io": "val val2 val3 1 1 2 11 3 3 23 3 3 24 24 3 25 3 3 3 3 3 <s> val val2 val3 df1 df2 df3 df1 df3 df2 df2 df3 df2 df2 df1 df2 df2 df3 df2 df3 df3 df1,df2 ", "apis": "add stack argmin astype array dtype", "code": ["np.char.add(\"df\", (np.stack(dfs, 0).argmin(0)+1).astype(str))\n\narray([['df1', 'df2', 'df3'],\n       ['df1', 'df3', 'df2'],\n       ['df2', 'df3', 'df2'],\n       ['df2', 'df1', 'df2'],\n       ['df2', 'df3', 'df2'],\n       ['df3', 'df3', 'df1']], dtype='<U23')\n"], "link": "https://stackoverflow.com/questions/60087855/name-of-the-dataframe-from-which-the-minimum-value-is-taken"}
{"id": 33, "q": "Replicate dataframe n number of times and increment another column by 1", "d": "I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.", "q_apis": "columns index values", "io": "S/No Column1 Column2 Column3 1 123 abc 2.20 1 234 bcd 1.19 1 345 cde 1.22 <s> S/No Column1 Column2 Column3 1 123 abc 2.20 1 234 bcd 1.19 1 345 cde 1.22 2 123 abc 2.20 2 234 bcd 1.19 2 345 cde 1.22 3 123 abc 2.20 3 234 bcd 1.19 3 345 cde 1.22 ", "apis": "copy concat", "code": ["df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n"], "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1"}
{"id": 271, "q": "How to select a row by value-set it as the header row-drop all the rows up to that value in python?", "d": "I have a dataframe such as this: What I would like to do is make row 4 the header and drop the rows above it. The key is that the source data is being read from an excel and the format of the report could change, so it may all of the sudden, start looking like this: I know I could do this: However, I need to keep all of this independent of the order of the dataframe because that could change. So, I need to select a row with a specific value, make that row the header and then delete/drop all the rows that lead up to that row that I selected if they are left in the dataframe (probably dependent on how the code is written). Everything that I've looked up seems to assume the data columns/rows won't change from load to load. I hope I worded this well enough to make sense.....", "q_apis": "select value drop all value drop all start all select value delete drop all left columns", "io": "0 1 2 3 4 1 a b c d 2 a b c d 3 a b c d 4 gab fob upo tem <s> 0 1 2 3 4 1 a b c d 2 gab fob upo tem 3 a b c d 4 a b c d ", "apis": "read_excel all idxmax columns loc iloc", "code": ["df = pd.read_excel('file.xlsx', header=None)\nheaders = [2, 'gab','fob','upo','tem']\n\nstarts = (df==headers).all(1).idxmax()\n\ndf.columns=df.loc[starts]\ndf = df.iloc[starts+1:]\n"], "link": "https://stackoverflow.com/questions/62664248/how-to-select-a-row-by-value-set-it-as-the-header-row-drop-all-the-rows-up-to-th"}
{"id": 161, "q": "Create pd.DataFrame from dictionary with multi-dimensional array", "d": "I've the following dictionary: I want to convert it to a , expecting this: How can I do that? I'm trying But it obviously doesn't work!", "q_apis": "DataFrame array", "io": "dictA = {'A': [[1, 2, 3], [1, 2, 3], [1, 2, 3]], 'B': [[4, 4, 4], [4, 4, 4],], 'C': [[4, 6, 0]] } <s> id ColA ColB ColC 0 1 4 4 1 2 4 6 2 3 4 0 3 1 4 4 2 4 5 3 4 6 1 7 2 8 3 ", "apis": "all length DataFrame Series items DataFrame Series items DataFrame Series items replace DataFrame Series items replace", "code": [" ValueError: arrays must all be same length\n", "df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dictA.items()]))\nprint(df)\n", "df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()]))\nprint(df)\n", "df = pd.DataFrame(dict([(k, pd.Series([a for b in v for a in b])) for k, v in dictA.items()])).replace(np.nan, '')\nprint(df)\n", "df = pd.DataFrame(dict([(f'Col{k}', pd.Series([a for b in v for a in b])) for k,v in dictA.items()])).replace(np.nan, '')\nprint(df)\n"], "link": "https://stackoverflow.com/questions/64876318/create-pd-dataframe-from-dictionary-with-multi-dimensional-array"}
{"id": 363, "q": "Split string column based on delimiter and convert it to dict in Pandas without loop", "d": "I have below dataframe My desired result is I have tried below method This is giving me desired result but I am looking for a more efficient way to convert clm3 to dict which does not involve looping through each row.", "q_apis": "", "io": "clm1, clm2, clm3 10, a, clm4=1|clm5=5 11, b, clm4=2 <s> clm1, clm2, clm4, clm5 10, a, 1, 5 11, b, 2, Nan ", "apis": "stack reset_index drop concat groupby index sum unstack drop", "code": ["s = (\n    df[\"clm3\"]\n    .str.split(\"|\", expand=True)\n    .stack()\n    .str.split(\"=\", expand=True)\n    .reset_index(level=1, drop=True)\n)\n\nfinal = pd.concat([df, s.groupby([s.index, s[0]])[1].sum().unstack()], axis=1).drop(\n    \"clm3\", axis=1\n)\n"], "link": "https://stackoverflow.com/questions/59829680/split-string-column-based-on-delimiter-and-convert-it-to-dict-in-pandas-without"}
{"id": 11, "q": "Why does it add .0 to the value while converting Dataframe columns to JSON", "d": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ?", "q_apis": "add value columns DataFrame get", "io": "A B C D 2 6 5 8.0 6 11 2 3.6 1 5 7 5.2 <s> {\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0} {\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6} {\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2} ", "apis": "iloc dtype", "code": ["df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"], "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json"}
{"id": 535, "q": "Pandas dataframe to sparse representation", "d": "I have a dense pandas dataframe. I would like to get a sparse dataframe out of it where each value of the original dataframe would be the column of a 1 in the resulting sparse dataframe. Example: Original df: Sparse df: I do not care if in case of collision it is a 1 or the number of collision I will then pass this df to sklearn.linear_model.LogisticRegression fit function (I am not sure which kind of sparse matrix would be accepted here) What would be the appropriate approach ? I can create it by hand (iterating over the row) but the dataframe is quite big so I am trying to find an efficient way of doing it. Thanks", "q_apis": "get where value", "io": " a b 0 5 3 1 2 6 <s> (0,3): 1 (0,5): 1 (1,2): 1 (1,6): 1 ", "apis": "index columns indices values indices indices indices", "code": ["nrow = len(df.index)\nncol = len(df.columns)\nindices = df.values.flatten()\ndata = np.full_like(indices, 1)\nnelement = len(indices)\nindptr = range(0, nelement+ncol, ncol)\nresult = csr_matrix((data, indices, indptr))\n"], "link": "https://stackoverflow.com/questions/53673872/pandas-dataframe-to-sparse-representation"}
{"id": 502, "q": "Pandas - Ignoring empty values when concatenating grouped rows", "d": "I'm trying to group a dataframe based on a column value, and I want to concatenate (join) the values in the other columns. I'm doing something like - But, this gives me some values where the columns has no values. So the result looks like How can I get rid of these nan values in the column? Also, is there a way to get a third column that has the number of values present in column. For eg. for the above, Edit: Sample Data - Thanks! :)", "q_apis": "empty values value join values columns values where columns values get values get values", "io": "K Code K0016, K0068, nan, nan, A0046 nan, nan, nan <s> UC LO Number K Code C001 C001.1 K0068 C001 C001.2 K0372 C002 C002.1 C002 C002.3 K0032 C002 C002.5 ", "apis": "replace groupby agg join join count assign count groupby agg join join count count", "code": ["df=df.replace({'nan':np.nan})\n\n\ndf_combined.groupby('UC').agg({'LO Number': ', '.join,\n                                             'K Code': [lambda x : ', '.join(y for y in x if y==y),'count']})\n", "df_combined.assign(count=df_combined['K Code']).\n         groupby('UC').agg({'LO Number': ', '.join,\n                           'K Code': lambda x : ', '.join(y for y in x if y==y),\n                            'count':'count'})\n"], "link": "https://stackoverflow.com/questions/55214456/pandas-ignoring-empty-values-when-concatenating-grouped-rows"}
{"id": 162, "q": "Pandas: slice Dataframe according to values of a column", "d": "I have to slice my Dataframe according to values (imported from a txt) that occur in one of my Dataframe' s column. This is what I have: This is what I need: drop rows whenever value in col2 is not among values in mytxt.txt Expected result must be: I tried: But it doesn' t work. Help would be very appreciated, thanks!", "q_apis": "values values drop value values", "io": ">df col1 col2 a 1 b 2 c 3 d 4 >'mytxt.txt' 2 3 <s> >df col1 col2 b 2 c 3 ", "apis": "values read_csv squeeze values values", "code": ["values = pd.read_csv('mytxt.txt', header=None, squeeze=True)\nvalues = set(values.tolist())\n"], "link": "https://stackoverflow.com/questions/64862482/pandas-slice-dataframe-according-to-values-of-a-column"}
{"id": 587, "q": "Mean of only two consecutive rows with conditional statements", "d": "After having searched for similar questions I found out with this and this questions. Unfortunately neither of them works with me. The first works on all the columns, the second does not work on my column of and and returns error (I also have not understood it completely). Here's a description of the problem: I am working with a dataframe of ~54k rows. Here's an example of 24 values: is the solar hour angle in radians. It ranges from -pi/2 to +pi/2 for the hours 00:00 and 24:00 respectively. At midday its value is 0. is the hour angle to which the sunset occurs. Due to the symmetry of the sun-earth system, . These values are constant along one day, but change for every day. The column is a result of a conditional expression: when then it's day and further calculations can be made. In order to do further calculations I need to associate for each hour the midpoint of the time span that the measure covers. So, for example, the midday measure was recorded at 12:00 but in order to represent all of that hour I want to have the hour angle of 12:30. Therefore I need a where represents the index. But here a new problem arises: if the sunset occurs, let's say, at 6:40 am then the midpoint hour has to be calculated like this: Thus the hourly radian angle will correspond to 6:50am. I created the column to help perform this task, but unfortunately I can't really use it. Thank you. EDIT: The solution proposed by @Mabel Villaba is not correct, for the column only has sunrise and sunset values. A coorect column would be: I hope that it is clear enough EDIT2: Thank you again, but the values are still not correct: the mean values are still calculated wrongly. I have calculated manually the correct values, I will post them here: EDIT3: I think the column obtained thanks to the boolean mask might be misleading. In fact: the sunrise always occurs between two rows, let them be called and , whom belong to and respectively. The same happens with the sunset, but with and . What happens is that the correct of is calculated as: but hase a attribute in the column. For the sunset it's the opposite: occurring between and it is calculated as: and has a attribute.", "q_apis": "first all columns second values hour value hour values day day day hour time at all hour hour where index at hour values values mean values values mask between between", "io": " omegam ... 7 -1.530775 8 -1.359855 9 -1.098058 ... 13 -0.05256705 ... 19 1.47992 ... <s> omegam(row3) = (omega3 + omegass)/2 ", "apis": "shift shift mask diff bfill date mask date mask apply to_datetime date hour shift date mean mean", "code": ["df['omega1'] = .5*((df['omega'] + df['omegasr'].shift(-1)))   \ndf['omega2'] = .5*((df['omega'].shift(1) + df['omegass']))\n", "df['mask'] =  (df['isday'] * 1).diff().bfill()\n\n>> df[['date','mask', 'isday']]\n\n                     date  mask  isday\n0    2012-03-27 00:00:00    0.0  False\n1    2012-03-27 01:00:00    0.0  False\n2    2012-03-27 02:00:00    0.0  False\n3    2012-03-27 03:00:00    0.0  False\n4    2012-03-27 04:00:00    0.0  False\n5    2012-03-27 05:00:00    0.0  False\n6    2012-03-27 06:00:00    0.0  False\n7    2012-03-27 07:00:00    1.0   True\n8    2012-03-27 08:00:00    0.0   True\n9    2012-03-27 09:00:00    0.0   True\n10   2012-03-27 10:00:00    0.0   True\n11   2012-03-27 11:00:00    0.0   True\n12   2012-03-27 12:00:00    0.0   True\n13   2012-03-27 13:00:00    0.0   True\n14   2012-03-27 14:00:00    0.0   True\n15   2012-03-27 15:00:00    0.0   True\n16   2012-03-27 16:00:00    0.0   True\n17   2012-03-27 17:00:00    0.0   True\n18   2012-03-27 18:00:00    0.0   True\n19   2012-03-27 19:00:00   -1.0  False\n20   2012-03-27 20:00:00    0.0  False\n21   2012-03-27 21:00:00    0.0  False\n22   2012-03-27 22:00:00    0.0  False\n23   2012-03-27 23:00:00    0.0  False\n", "df['new_omega'] = df.apply(lambda x: x['omegasr'] if pd.to_datetime(x['date']).hour < 12 else -x['omegasr'], axis=1).shift(-1)\n\n>> df\n\n                     date   omegasr     omega   omegass  isday  new_omega\n\n1    2012-03-27 00:00:00  -1.570796 -3.323350  1.570796  False  -1.570796\n2    2012-03-27 01:00:00  -1.570796 -3.061551  1.570796  False  -1.570796\n3    2012-03-27 02:00:00  -1.570796 -2.799752  1.570796  False  -1.570796\n4    2012-03-27 03:00:00  -1.570796 -2.537952  1.570796  False  -1.570796\n5    2012-03-27 04:00:00  -1.570796 -2.276153  1.570796  False  -1.570796\n6    2012-03-27 05:00:00  -1.570796 -2.014353  1.570796  False  -1.570796\n7    2012-03-27 06:00:00  -1.570796 -1.752554  1.570796  False  -1.570796\n8    2012-03-27 07:00:00  -1.570796 -1.490755  1.570796   True  -1.570796\n9    2012-03-27 08:00:00  -1.570796 -1.228955  1.570796   True  -1.570796\n10   2012-03-27 09:00:00  -1.570796 -0.967156  1.570796   True  -1.570796\n11   2012-03-27 10:00:00  -1.570796 -0.705356  1.570796   True  -1.570796\n12   2012-03-27 11:00:00  -1.570796 -0.443557  1.570796   True   1.570796\n13   2012-03-27 12:00:00  -1.570796 -0.181758  1.570796   True   1.570796\n14   2012-03-27 13:00:00  -1.570796  0.080042  1.570796   True   1.570796\n15   2012-03-27 14:00:00  -1.570796  0.341841  1.570796   True   1.570796\n16   2012-03-27 15:00:00  -1.570796  0.603640  1.570796   True   1.570796\n17   2012-03-27 16:00:00  -1.570796  0.865440  1.570796   True   1.570796\n18   2012-03-27 17:00:00  -1.570796  1.127239  1.570796   True   1.570796\n19   2012-03-27 18:00:00  -1.570796  1.389039  1.570796   True   1.570796\n20   2012-03-27 19:00:00  -1.570796  1.650838  1.570796  False   1.570796\n21   2012-03-27 20:00:00  -1.570796  1.912637  1.570796  False   1.570796\n22   2012-03-27 21:00:00  -1.570796  2.174437  1.570796  False   1.570796\n23   2012-03-27 22:00:00  -1.570796  2.436236  1.570796  False   1.570796\n24   2012-03-27 23:00:00  -1.570796  2.698036  1.570796  False        NaN\n", "omegam[i] = (omegasr[i],omega[i+1]).mean() #sunrise\nomegam[i] = (omega[i],omegass[i+1]).mean() #sunset\n"], "link": "https://stackoverflow.com/questions/52312211/mean-of-only-two-consecutive-rows-with-conditional-statements"}
{"id": 247, "q": "How do I sum values in a column that match a given condition using pandas?", "d": "Suppose I have a column like so: I want to sum up the values for where , for example. This would give me . How do I do this in pandas?", "q_apis": "sum values sum values where", "io": "a b 1 5 1 7 2 3 1 3 2 5 <s> 5 + 7 + 3 = 15", "apis": "loc sum loc sum query sum query sum", "code": [">>> df.loc[df['a'] == 1, 'b'].sum()\n15\n", "df.loc[(df['a'] == 1) & (df['c'] == 2), 'b'].sum()\n", ">>> df.query(\"a == 1\")['b'].sum()\n15\n", "df.query(\"a == 1 and c == 2\")['b'].sum()\n"], "link": "https://stackoverflow.com/questions/28236305/how-do-i-sum-values-in-a-column-that-match-a-given-condition-using-pandas"}
{"id": 416, "q": "list to pandas dataframe - Python", "d": "I have the following list: I have created a using the following code: Now I have a new list: I would like to arrange the in a way that the output looks like this (MANUAL EDIT) and that whenever there is a third or foruth colum... the data gets arranged in the same way. I have tried to convert the list to a but since I am new with python I am not getting the desired output but only errors due to invalid shapes. -- EDIT -- Once I have the dataframe created, I want to plot it using . However, the way the data is shown is not what I would like. I am comming from so I am not sure if this is because of the data structure used in the . Is is it that I need one measurement in each row? My idea would be to have the , , in the x-axis (it's a temporal series). In the y-axis the range of values (so that is ok in that plot) and the differnet lines should be showing the evolution of , , , etc.", "q_apis": "plot values plot", "io": "list = [-0.14626096918979603, 0.017925919395027533, 0.41265398151061766] <s> list2 = [-0.14626096918979603, 0.017925919395027533, 0.41265398151061766, -0.8538301985671065, 0.08182534201640915, 0.40291331836021105] ", "apis": "DataFrame index ceil DataFrame columns ceil columns columns columns loc", "code": ["df = pd.DataFrame(index = ['var1', 'var2', 'var3'])\n\nn_cols = int(np.ceil(len(list2) / len(df)))\nfor ii in range(n_cols):\n    L = list2[ii * len(df) : (ii + 1) * len(df)]\n    df['col_{}'.format(ii)] = L\n", "len(list2) % len(df) != 0", "len(df) - (len(list2) % len(df))", "df = pd.DataFrame(columns = ['var1', 'var2', 'var3'])\nn_rows = int(np.ceil(len(list2) / len(df.columns)))\nfor ii in range(n_rows):\n    L = list2[ii * len(df.columns) : (ii + 1) * len(df.columns)]\n    df.loc['col_{}'.format(ii)] = L\n"], "link": "https://stackoverflow.com/questions/58319979/list-to-pandas-dataframe-python"}
{"id": 192, "q": "How to get the n most frequent or top values per column in python pandas?", "d": "my dataframe looks like: for top 2 most frequent values per column (n=2), the output should be: Thank you", "q_apis": "get values values", "io": "df: A B 0 a g 1 f g 2 a g 3 a d 4 h d 5 f a <s> top_df: A B 0 a g 1 f d ", "apis": "apply Series value_counts index", "code": ["n = 2\ndf.apply(lambda x: pd.Series(x.value_counts().index[:n]))\n"], "link": "https://stackoverflow.com/questions/64445512/how-to-get-the-n-most-frequent-or-top-values-per-column-in-python-pandas"}
{"id": 631, "q": "Create extra columns in pandas time-indexed DataFrame", "d": "I currenty have a Datetime-Indexed dataframe with three columns: I would like to create three extra columns that hold the values indexed one hour from the current index to end up with something like this: I have already defined a function that creates a dataframe with the columns 'Glucosa1', 'Insulina1', 'Carbs1' but it is very poorly performing and I would like to make it run faster. I profile the time used by different functions on my code using the following: This outputs a time of 8.331165 seconds (on average) for the function nn_format_df() compared to similar functions (which iterate over the rows of the dataframe) wicht output 0.366158 seconds . After creating a new dataframe calling my function on the original I merge them to get the desired dataframe. The function: To sum it up: I would appreciate any comments on how to improve the function so that it doesn't take so long. A better, more Pythonic or pandas-y way of getting the desired dataframe is welcome. I am new to pandas and I understand my implementation of the function is a completely na\u00efve approach.", "q_apis": "columns time DataFrame columns columns values hour index columns time time seconds seconds merge get sum any take", "io": " Glucosa Insulina Carbs Hour 2018-05-16 06:43:00 156.0 7.0 65.0 2018-05-16 07:43:00 170.0 0.0 65.0 2018-05-16 08:45:00 185.0 2.0 0.0 2018-05-16 09:45:00 150.0 0.0 0.0 2018-05-16 10:45:00 80.0 0.0 0.0 ... <s> Glucosa Insulina Carbs Glucosa1 Insulina1 Carbs1 Hour 2018-05-16 06:43:00 156.0 7.0 65.0 170.0 0.0 65.0 2018-05-16 07:43:00 170.0 0.0 65.0 185.0 2.0 0.0 2018-05-16 08:45:00 185.0 2.0 0.0 150.0 0.0 0.0 2018-05-16 09:45:00 150.0 0.0 0.0 80.0 0.0 0.0 2018-05-16 10:45:00 80.0 0.0 0.0 ... ... ... ... ", "apis": "concat shift rename columns columns", "code": ["import pandas as pd\npd.concat([df, df.shift(-1).rename(columns=dict((elem, elem+'1') for elem in df.columns))], axis=1)\n"], "link": "https://stackoverflow.com/questions/50545075/create-extra-columns-in-pandas-time-indexed-dataframe"}
{"id": 80, "q": "Select only those rows from a Dataframe where certain columns with suffix have values not equal to zero", "d": "I want to select only those rows from a dataframe where certain columns with suffix have values not equal to zero. Also the number of columns is more so I need a generalised solution. eg: The dataframe would be : The desired output is : (because of 2 in CA_DIFF and 1 in BC_DIFF) This works with using multiple conditions but what if the number of DIFF columns are more? Like 20? Can someone provide a general solution? Thanks.", "q_apis": "where columns values select where columns values columns columns", "io": " ID M_NEW M_OLD M_DIFF CA_NEW CA_OLD CA_DIFF BC_NEW BC_OLD BC_DIFF 0 1 10 10 0 10 10 0 10 10 0 1 2 12 12 0 12 12 0 12 12 0 2 3 14 14 0 16 14 2 14 14 0 3 4 16 16 0 16 16 0 16 16 0 4 5 18 18 0 18 18 0 18 17 1 <s> ID M_NEW M_OLD M_DIFF CA_NEW CA_OLD CA_DIFF BC_NEW BC_OLD BC_DIFF 0 3 14 14 0 16 14 2 14 14 0 1 5 18 18 0 18 18 0 18 17 1 ", "apis": "get all columns columns columns columns contains any value columns transform any", "code": ["\n...\n# get all columns with X_DIFF\ncolumns = df.columns[df.columns.str.contains('_DIFF')]\n\n# check if any has value greater than 0\ndf[df[columns].transform(lambda x: x > 0).any(axis=1)]\n\n"], "link": "https://stackoverflow.com/questions/66507107/select-only-those-rows-from-a-dataframe-where-certain-columns-with-suffix-have-v"}
{"id": 646, "q": "How to append multiple columns into two?", "d": "The data I am working with is in a large set of columns, with related values - for example: In order to join this data with another data set, I need to append these values into one column, preserving the column data, as well as the data: I've tried using and , but am so far unable to get the required result .. which pandas function should I be using here?", "q_apis": "append columns columns values join append values get", "io": "| YearQ | Area A | Area B | Area C | +--------+--------+--------+--------+ | 2017Q1 | 1234.0 | 9252.0 | 3421.0 | | 2017Q2 | 1245.0 | 9368.0 | 3321.0 | | 2017Q3 | 1350.0 | 9440.0 | 3225.0 | | 2017Q4 | 1333.0 | 9501.0 | 3625.0 | <s> | YearQ | Area | Value | +--------+--------+---------+ | 2017Q1 | Area A | 1234.0 | | 2017Q1 | Area B | 9252.0 | | 2017Q1 | Area C | 3421.0 | | 2017Q2 | Area A | 1245.0 | | 2017Q2 | Area B | 9368.0 | | 2017Q2 | Area C | 3321.0 | ", "apis": "melt sort_values", "code": ["df = df.melt('YearQ', var_name='Area', value_name='Value').sort_values(['YearQ','Area'])\n"], "link": "https://stackoverflow.com/questions/49717720/how-to-append-multiple-columns-into-two"}
{"id": 400, "q": "Dropping rows that have string cointained in other rows in pandas", "d": "Given dataframe in this form: I want to drop the rows that have value from column present in another string before the hyphen () in other rows So based on this I would drop value since there are , etc. Expected output:", "q_apis": "drop value drop value", "io": " ID A 130 Yes 130-1 Yes 130-2 Yes 200 No 201 No 201-10 No 201-101 Yes 201-22 Yes 300 No <s> ID A 130-1 Yes 130-2 Yes 200 No 201-10 No 201-101 Yes 201-22 Yes 300 No ", "apis": "duplicated duplicated", "code": ["s = df['ID'].str.split('-').str[0]\nm = s.duplicated(keep=False) ^ s.duplicated()\n\ndf[~m]\n"], "link": "https://stackoverflow.com/questions/58821180/dropping-rows-that-have-string-cointained-in-other-rows-in-pandas"}
{"id": 79, "q": "how to convert lists/array entries in a column to one row with different columns for each entry", "d": "I have a dataframe where one column called has its entries as lists of numbers over 1064 rows. So each row contains 6 to 7 columns with the column where over each row it contains a list of numbers. I want to take this list, and spread it over the columns till while the number of columns is equal to . Here's an example: That's the first entry of the column let's say I want to spread it over the dataframe in a way where it would be formatted like the following: And most importantly I want to iterate this process over the 1064 rows. Thank you for your help!", "q_apis": "array columns where contains columns where contains take columns columns first where", "io": "print(df1[0])>>> [-0.03980884 -0.18056028 0.11624704 0.08659928 0.02749503 -0.23401791 0.10772136 -0.32243717 -0.09397306 -0.08458275 0.11873401 0.10531124 0.11620065 -0.1100786 -0.27929837 -0.06915713 -0.11539902 0.26890758 -0.16375561 0.00525901 0.01196074 0.15442082 0.10281886 -0.15471214 -0.22901823 0.11486725 -0.05937155 -0.00580112 -0.25958595 -0.27098128 -0.03174639 -0.20656739 -0.13286862 -0.07104845 -0.04765386 -0.08396237 0.14032942 -0.15563552 -0.17417437 0.02441286 0.06222694 -0.08691377 0.08214904 -0.08121296 -0.079873 0.06362587 0.06934057 0.07980402 -0.08373277 -0.08293616 -0.07830499 -0.08762348 0.07899728 -0.04922628 -0.02680833 -0.0853695 -0.03179847 0.00792945 0.02782207] <s> col0 col1 col2 col3 ......col59 -0.03980884 -0.18056028 0.11624704 0.08659928 ......0.02782207 ", "apis": "DataFrame columns DataFrame values add_prefix", "code": ["import numpy as np \ndf = pd.DataFrame(columns = ['features']) \ndf['features'] = list(np.random.random((6,3))) + list(np.random.random((6,4)))\nres = pd.DataFrame(df['features'].values.tolist()).add_prefix('col')\n"], "link": "https://stackoverflow.com/questions/66515491/how-to-convert-lists-array-entries-in-a-column-to-one-row-with-different-columns"}
{"id": 504, "q": "Transforming pandas dataframe, where column entries are column headers", "d": "My dataset has 12 columns, X1-X6 and Y1-Y6. The variables X and Y match to each other - the first record means: 80 parts of A, 10 parts of C, 2 parts of J and 8 parts of K (each row has 100 total). I would like to be able to transform my dataset into a dataset in which the entries in columns X1-X6 are now the headers. See before and after datasets below. My dataset (before): The dataset I'd like to transform to:", "q_apis": "where columns first transform columns now transform", "io": " X1 X2 X3 X4 X5 X6 Y1 Y2 Y3 Y4 Y5 Y6 0 A C J K NaN NaN 80.0 10.0 2.0 8.0 NaN NaN 1 F N O NaN NaN NaN 2.0 25.0 73.0 NaN NaN NaN 2 A H J M NaN NaN 70.0 6.0 15.0 9.0 NaN NaN 3 B I K P NaN NaN 0.5 1.5 2.0 96.0 NaN NaN 4 A B F H O P 83.0 4.0 9.0 2.0 1.0 1.0 5 A B F G NaN NaN 1.0 16.0 9.0 74.0 NaN NaN 6 A B D F L NaN 95.0 2.0 1.0 1.0 1.0 NaN 7 B F H P NaN NaN 0.2 0.4 0.4 99.0 NaN NaN 8 A D F L NaN NaN 35.0 12.0 30.0 23.0 NaN NaN 9 A B F I O NaN 95.0 0.3 0.1 1.6 3.0 NaN 10 B E G NaN NaN NaN 10.0 31.0 59.0 NaN NaN NaN 11 A F G L NaN NaN 24.0 6.0 67.0 3.0 NaN NaN 12 A C I NaN NaN NaN 65.0 30.0 5.0 NaN NaN NaN 13 A F G L NaN NaN 55.0 6.0 4.0 35.0 NaN NaN 14 A F J K L NaN 22.0 3.0 12.0 0.8 62.2 NaN 15 B F I P NaN NaN 0.6 1.2 0.2 98.0 NaN NaN 16 A B F H O NaN 27.0 6.0 46.0 13.0 8.0 NaN <s> A B C D E F G H I J K L M \\ 0 80.0 NaN 10.0 NaN NaN NaN NaN NaN NaN 2.0 8.0 NaN NaN 1 NaN NaN NaN NaN NaN 2.0 NaN NaN NaN NaN NaN NaN NaN 2 70.0 NaN NaN NaN NaN NaN NaN 6.0 NaN 15.0 NaN NaN 9.0 3 NaN 0.5 NaN NaN NaN NaN NaN NaN 1.5 NaN 2.0 NaN NaN 4 83.0 4.0 NaN NaN NaN 9.0 NaN 2.0 NaN NaN NaN NaN NaN 5 1.0 16.0 NaN NaN NaN 9.0 74.0 NaN NaN NaN NaN NaN NaN 6 95.0 2.0 NaN 1.0 NaN 1.0 NaN NaN NaN NaN NaN 1.0 NaN 7 NaN 0.2 NaN NaN NaN 0.4 NaN 0.4 NaN NaN NaN NaN NaN 8 35.0 NaN NaN 12.0 NaN 30.0 NaN NaN NaN NaN NaN 23.0 NaN 9 95.0 0.3 NaN NaN NaN 0.1 NaN NaN 1.6 NaN NaN NaN NaN 10 NaN 10.0 NaN NaN 31.0 NaN 59.0 NaN NaN NaN NaN NaN NaN 11 24.0 NaN NaN NaN NaN 6.0 67.0 NaN NaN NaN NaN 3.0 NaN 12 65.0 NaN 30.0 NaN NaN NaN NaN NaN 5.0 NaN NaN NaN NaN 13 55.0 NaN NaN NaN NaN 6.0 4.0 NaN NaN NaN NaN 35.0 NaN 14 22.0 NaN NaN NaN NaN 3.0 NaN NaN NaN 12.0 0.8 62.2 NaN 15 NaN 0.6 NaN NaN NaN 1.2 NaN NaN 0.2 NaN NaN NaN NaN 16 27.0 6.0 NaN NaN NaN 46.0 NaN 13.0 NaN NaN NaN NaN NaN N O P 0 NaN NaN NaN 1 25.0 73.0 NaN 2 NaN NaN NaN 3 NaN NaN 96.0 4 NaN 1.0 1.0 5 NaN NaN NaN 6 NaN NaN NaN 7 NaN NaN 99.0 8 NaN NaN NaN 9 NaN 3.0 NaN 10 NaN NaN NaN 11 NaN NaN NaN 12 NaN NaN NaN 13 NaN NaN NaN 14 NaN NaN NaN 15 NaN NaN 98.0 16 NaN 8.0 NaN ", "apis": "apply DataFrame", "code": ["data = list(df.apply(lambda x: {x['X'+ str(i)]: x['Y'+str(i)] for i in range(1,7)\n                                if x['X'+str(i)]!= 'NaN'}, axis=1))\n\nresul = pd.DataFrame(data)\nprint(resul)\n"], "link": "https://stackoverflow.com/questions/54944513/transforming-pandas-dataframe-where-column-entries-are-column-headers"}
{"id": 211, "q": "Concat list of dataframes containing empty dataframes", "d": "I have this list of dataframes and some of them are Empty. I do not want to discard them as it would change my number of rows when I concatenate it. I want to convert them into NA values so that these 5 dataframes are converted to 5 rows. List of dataframes: Code: Output: Desired Output:", "q_apis": "empty values", "io": "0 102,000,000.00 2,000,000.00 1,400,000.00 0.00 0 60,900,000.00 1,300,000.00 0.00 0.00 <s> 0 102,000,000.00 2,000,000.00 1,400,000.00 0.00 0 NA NA NA NA 0 NA NA NA NA 0 NA NA NA NA 0 60,900,000.00 1,300,000.00 0.00 0.00 ", "apis": "DataFrame columns empty", "code": ["template = pd.DataFrame(data = [[pd.NA, pd.NA, pd.NA, pd.NA]], columns = [0,1,2,3])\ndataframes= [i if not i.empty else template for i in dataframes]\n"], "link": "https://stackoverflow.com/questions/63970182/concat-list-of-dataframes-containing-empty-dataframes"}
{"id": 627, "q": "Add new column in Pandas DataFrame Python", "d": "I have dataframe in Pandas for example: Now if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be: Any idea on how to achieve this?", "q_apis": "DataFrame add value", "io": "Col1 Col2 A 1 B 2 C 3 <s> Col1 Col2 Col3 A 1 1 B 2 0 C 3 0 ", "apis": "astype map loc loc", "code": ["df['Col3'] = (df['Col2'] <= 1).astype(int)\n", "df['Col3'] = df['Col2'].map(lambda x: 42 if x > 1 else 55)\n", "df['Col3'] = 0\ncondition = df['Col2'] > 1\ndf.loc[condition, 'Col3'] = 42\ndf.loc[~condition, 'Col3'] = 55\n"], "link": "https://stackoverflow.com/questions/18942506/add-new-column-in-pandas-dataframe-python"}
{"id": 182, "q": "Selectively adding values of columns in a dataframe", "d": "I have a pandas data frame like this I want to add all the values in the given columns like this: So basically I want to check if the column names fall in a five year range of the corresponding values in the column 'YEAR_OPENED' and create a new column with the sum of all the values. How should I proceed?", "q_apis": "values columns add all values columns names year values sum all values", "io": "YEAR_OPENED 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 1999 1 0 0 0 1 0 0 0 1 0 2000 1 1 2 0 3 0 0 0 0 0 2001 0 0 0 4 0 0 0 0 0 0 <s> YEAR_OPENED CLOSED_IN_5_YEARS 1999 2 2000 7 2001 4 ", "apis": "set_index apply sum index name name values", "code": ["df['CLOSED_IN_5_YEARS'] = df.set_index('YEAR_OPENED').apply(\n        lambda x: sum(i for i, c in zip(x, x.index) if x.name <= int(c) <= x.name + 5), axis=1\n    ).values\n\nprint(df)\n"], "link": "https://stackoverflow.com/questions/64583504/selectively-adding-values-of-columns-in-a-dataframe"}
{"id": 158, "q": "How to insert values of a dataframe to others dataframe", "d": "This is the first dataframe: and in the other side I have other dataframes (CSV files) that have the same content. Here is two exemple: I want to add new column to each dataframe and to add each element of the first dataset to the others. Expected output: NB: I read CSV files as pandas so here the multiple dataframe are multiple csv files and I'm inserting new colum to each file and this column contain an element from the first dataframe.df1. This is my code it add the last element of df1 to all other df like that:", "q_apis": "insert values first add add first first add last all", "io": " df3= df4= A B C A B C 1 2 3 1 2 3 4 5 6 4 5 6 <s> df3= df4= A B C D A B C D 1 2 3 a 1 2 3 b 4 5 6 a 4 5 6 b ", "apis": "read_csv append read_csv to_csv index", "code": ["list1=[]\ndf=pd.read_csv('C:/dataframe_1.csv', sep=',')\nfor elem in df['Name']:\n    list1.append(elem)\n\nos.chdir('C:/New')\nextension = 'csv'\nall_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n\nfor i, file in enumerate(all_filenames):\n    df1 = pd.read_csv(file, sep=',')\n    df1['new_column'] = list1[i]\n    \ndf1.to_csv(file, index=False, na_rep='NaN')\n"], "link": "https://stackoverflow.com/questions/64958453/how-to-insert-values-of-a-dataframe-to-others-dataframe"}
{"id": 511, "q": "Add a fix value to a dataframe (accumulating to future ones)", "d": "I am trying to simulate inventory level during the next 6 months: 1- I have the expected accumulated demand for each day of next 6 months. So, with no reorder, my balance would be more negative everyday. 2- My idea is: Everytime the inventory level is lower than 3000, I would send an order to buy 10000, and after 3 days, my level would increase again: How is the best way to add this value into all the future values ? I started doing like this : But it just adds to the actual row, not for the accumulated future ones.", "q_apis": "value day days add value all values", "io": " ds saldo 0 2019-01-01 10200.839819 1 2019-01-02 5219.412952 2 2019-01-03 3.161876 3 2019-01-04 -5507.506201 4 2019-01-05 -10730.291221 5 2019-01-06 -14406.833593 6 2019-01-07 -17781.500396 7 2019-01-08 -21545.503098 8 2019-01-09 -25394.427708 <s> print(row['ds'], row['saldo']) 9 2019-01-10 -29277.647817 ", "apis": "index index", "code": ["forecast_data['saldo'][index:] += forecast_data['saldo'][index:] + 1000\n"], "link": "https://stackoverflow.com/questions/54783721/add-a-fix-value-to-a-dataframe-accumulating-to-future-ones"}
{"id": 559, "q": "How to do a calculation only at some rows of my dataframe?", "d": "Let's say I have a dataframe with only two columns and 20 rows, where all values from the first column are equal to 10, and all values from the second row are random percentage numbers. Now, I want to multiply the first column with the percentage values of the second column +1, but only at some intervals, and copy the last value to the next row. E.g. I want to do this multiplication operation from row 5 to 10. The problem Is that I don't know to start and end the calculation in arbitrary spots based on the df's index. Example input data: Which produces: An output I would like to get, is where the first row go thorugh a comulative multiplication only at sow rows, like this: Thank you!", "q_apis": "at columns where all values first all values second first values second at copy last value start index get where first at", "io": " A B 0 10 0.07 1 10 0.02 2 10 0.05 3 10 0.00 4 10 0.01 5 10 0.09 6 10 0.00 7 10 0.02 8 10 0.03 9 10 0.05 10 10 0.05 11 10 0.03 12 10 0.01 13 10 0.09 14 10 0.06 15 10 0.07 16 10 0.01 17 10 0.01 18 10 0.01 19 10 0.07 <s> C B 0 10 0.07 1 10 0.02 2 10 0.05 3 10 0.00 4 10 0.01 5 10,9 0.09 6 10,9 0.00 7 11,11 0.02 8 11,45 0.03 9 12,02 0.05 10 12,62 0.05 11 12,62 0.03 12 12,62 0.01 13 12,62 0.09 14 12,62 0.06 15 12,62 0.07 16 12,62 0.01 17 12,62 0.01 18 12,62 0.01 19 12,62 0.07 ", "apis": "start start cumprod reindex index fillna ffill", "code": ["start = 5\nend = 10\n\ndf['C'] = ((1+df.B)[start:end+1].cumprod().reindex(df.index[:end+1]).fillna(1)*df.A).ffill()\n"], "link": "https://stackoverflow.com/questions/53357204/how-to-do-a-calculation-only-at-some-rows-of-my-dataframe"}
{"id": 134, "q": "Fill null values in a column using percent change from a second column while grouping by a third column", "d": "I have a dataframe that looks like this: I want to fill in the gaps in the column by applying the same percent change as was calculated. However I also need to group using the column. I should end up with something like this: I only want to replace values that are null. Notice the 10 in row seven \"resets\" the forward fill. Without having to group, I could simply get the percent change in and multiply the previous row's cell by the current row's percent change cell wherever is not null. I was thinking that I could order the dataframe using , but then I would still have to worry about the edge case of when values change.", "q_apis": "values second replace values get values", "io": "grp val run a 5 10 b 10 1 a NaN 8 a NaN 4 b NaN 5 b NaN 4 a 10 6 a NaN 6 <s> grp val run a 5 10 b 10 1 a 4 8 a 2 4 b 50 5 b 40 4 a 10 6 a 10 6 ", "apis": "notna groupby cumsum groupby shift cumprod ffill transform first ffill cumprod transform first", "code": ["# identify the na blocks and group by `grp` and these blocks\nna_blocks = df['val'].notna().groupby(df['grp']).cumsum()    \ng = df.groupby(['grp', na_blocks])\n\n# \"pct change\" on run\ndf['x'] = df['run'] / g['run'].shift(fill_value=1)\n\n# cumprod() for cumulative change\n# `ffill` and `transform('first')` behave the same \n# since we are grouping on non-nan following by consecutive nan's\ndf['val'] = g['val'].ffill() * g['x'].cumprod() / g['run'].transform('first')\n"], "link": "https://stackoverflow.com/questions/65515347/fill-null-values-in-a-column-using-percent-change-from-a-second-column-while-gro"}
{"id": 268, "q": "Split the data frame based on consecutive row values differences", "d": "I have a data frame like this, Now I want to create multiple data frames from above when the col1 difference of two consecutive rows are more than 1. So the result data frames will look like, I can do this using for loop and storing the indices but this will increase execution time, looking for some pandas shortcuts or pythonic way to do this most efficiently.", "q_apis": "values difference indices time", "io": "df col1 col2 col3 1 2 3 2 5 6 7 8 9 10 11 12 11 12 13 13 14 15 14 15 16 <s> df1 col1 col2 col3 1 2 3 2 5 6 df2 col1 col2 col3 7 8 9 df3 col1 col2 col3 10 11 12 11 12 13 df4 col1 col2 col3 13 14 15 14 15 16 ", "apis": "assign difference diff diff diff gt cumsum difference", "code": ["df.assign(difference=(diff:=df.col1.diff()), \n          condition=(gt1:=diff.gt(1)), \n          grouper=gt1.cumsum())\n\n   col1  col2  col3  difference  condition  grouper\n0     1     2     3         NaN      False        0\n1     2     5     6         1.0      False        0\n2     7     8     9         5.0       True        1\n3    10    11    12         3.0       True        2\n4    11    12    13         1.0      False        2\n5    13    14    15         2.0       True        3\n6    14    15    16         1.0      False        3\n"], "link": "https://stackoverflow.com/questions/62721565/split-the-data-frame-based-on-consecutive-row-values-differences"}
{"id": 420, "q": "Grouping panda by time interval + aggregate function", "d": "Let's say I have a panda like that: And I need convert it in a format as following: The first column corresponds to each 5 seconds interval starting at 2010-01-01 04:10:00:000. The second column is the max of all the grouped rows. The third column is the first of all the grouped rows. How can I get that?", "q_apis": "time aggregate first seconds at second max all first all get", "io": "2010-01-01 04:10:00:025 69 2010-01-01 04:10:01:669 1 2010-01-01 04:10:03:027 3 2010-01-01 04:10:04:003 8 2010-01-01 04:10:05:987 10 2010-01-01 04:10:06:330 99 2010-01-01 04:10:08:369 55 2010-01-01 04:10:09:987 5000 2010-01-01 04:10:11:148 13 <s> 2010-01-01 04:10:00:000 69 69 2010-01-01 04:10:05:000 5000 10 2010-01-01 04:10:10:000 13 13 ", "apis": "first to_datetime groupby Grouper freq agg max first reset_index", "code": ["# use this line if your first column is not datetime type yet.\n# df['col1'] = pd.to_datetime(df['col1'], format='%Y-%m-%d %H:%M:%S:%f')\n\ndf.groupby(pd.Grouper(key='col1', freq='5s'))['col2'].agg(['max', 'first']).reset_index()\n"], "link": "https://stackoverflow.com/questions/58210576/grouping-panda-by-time-interval-aggregate-function"}
{"id": 278, "q": "Python dataframe get index start and end of successive values", "d": "Let's say I have this dataframe : I want to store the start and end indexes of each value (even repeated ones) in the dataframe as well as the value corresponding. So that I would get a result like this for example : I tried this (for the value 2 for example here) : But this returns only first and last result each time.", "q_apis": "get index start values start value value get value first last time", "io": " 0 0 1 1 1 2 1 3 2 4 2 5 3 6 3 7 1 8 1 <s> Value | Start | End ---------------------------- 1 | 0 | 2 2 | 3 | 4 3 | 5 | 6 1 | 7 | 8 ", "apis": "diff ne index index shift loc reset_index drop assign rename columns", "code": ["starts_bool = df.diff().ne(0)[0]\nstarts = df.index[starts_bool]\nends = df.index[starts_bool.shift(-1, fill_value=True)]\n\nresult = (df.loc[starts]\n            .reset_index(drop=True)\n            .assign(Start=starts, End=ends)\n            .rename({0: 'Value'}, axis='columns')\n          )\n"], "link": "https://stackoverflow.com/questions/62361446/python-dataframe-get-index-start-and-end-of-successive-values"}
{"id": 73, "q": "Find average of every column in a dataframe, grouped by column, exluding one value", "d": "I have a Dataframe like the one presented below: What I want is to and find the average of each label. So far I have this which works just fine and get the results as follows: The only thing I haven't yet found is how to exclude everything that is labeled as . Is there a way to do that?", "q_apis": "value get", "io": " CPU Memory Disk Label 0 21 28 29 0 1 46 53 55 1 2 48 45 49 2 3 48 52 50 3 4 51 54 55 4 5 45 50 56 5 6 50 83 44 -1 <s> Label CPU Memory Disk -1 46.441176 53.882353 54.176471 0 48.500000 58.500000 60.750000 1 45.000000 51.000000 60.000000 2 54.000000 49.000000 56.000000 3 55.000000 71.500000 67.500000 4 53.000000 70.000000 71.000000 5 21.333333 30.000000 30.666667 ", "apis": "loc groupby mean", "code": ["# Exclude rows with Label=-1\ndataset = dataset.loc[dataset['Label'] != -1]\n\n# Group by on filtered result\ndataset.groupby('Label')['CPU', 'Memory', 'Disk'].mean()\n"], "link": "https://stackoverflow.com/questions/54669058/find-average-of-every-column-in-a-dataframe-grouped-by-column-exluding-one-val"}
{"id": 128, "q": "Loop which every iteration will replace one column from a dataframe with zeros", "d": "I want to perform Sensitivity Analysis for classification models in Python. So I want to check how lack of every column will affect the metrics. I prepared function which returns metrics from original test set. I want perform the same but for X_test which with every iteration will have one column values replaced with 0. For example if this would be X_test: I would like to check those metrics for: and so on. And my question is to edit above code (or suggest something else) which help me to achieve it. Later I would like to have this outcome in Pandas DataFrame but it's enough to show me up to dictionary state.", "q_apis": "replace test values DataFrame", "io": " A B C D E 5 7 11 12 6 11 32 11 13 6 <s> A B C D E 0 7 11 12 6 0 32 11 13 6 A B C D E 5 0 11 12 6 11 0 11 13 6 A B C D E 5 7 0 12 6 11 32 0 13 6 ", "apis": "columns copy", "code": ["for c in df.columns:\n    newDF = df.copy(deep=True)\n    newDF[c] = 0\n    # Here you opperate with the new DF in this instance\n"], "link": "https://stackoverflow.com/questions/65668962/loop-which-every-iteration-will-replace-one-column-from-a-dataframe-with-zeros"}
{"id": 92, "q": "How to stack/append all columns into one column in Pandas?", "d": "I am working with a DF similar to this one: I want the values of all columns to be stacked into the first column Desired Output:", "q_apis": "stack append all columns values all columns first", "io": " A B C 0 1 4 7 1 2 5 8 2 3 6 9 <s> A 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 ", "apis": "melt drop rename value", "code": ["import pandas as pd\ndf.melt().drop('variable',axis=1).rename({'value':'A'},axis=1)\n"], "link": "https://stackoverflow.com/questions/66266188/how-to-stack-append-all-columns-into-one-column-in-pandas"}
{"id": 169, "q": "Process multiple csv files on pandas", "d": "I have got three different .csv files which contain the grades for students in three different assignment. I would like to read them with pandas and calculate the average for each student. The template for each file is: For the second assignment: Desired output for the two doc for instance: the same for the last doc. I want to read these docs separately and for each student id to average the Mark. Now, my code for reading one file is the following: How can I process all three files simultaneously and extract the average grade?", "q_apis": "second last all", "io": "Student id, Mark, extra fields, ... 4358975489, 9, ... ... 2345234523, 10, ... ... 7634565323, 7, ... ... 7653563366, 7, ... ... ... ..., ... ... <s> Student id, Mark, extra fields, ... 4358975489, 6, ... ... 2345234523, 8, ... ... 7634565323, 4, ... ... 7653563366, 5, ... ... ... ..., ... ... ", "apis": "append groupby mean groupby mean", "code": ["df = df1.append([df2, df3]).groupby('Student id', as_index=False).mean()\n", "In [1223]: df = [i.groupby('Student_id', as_index=False).mean() for i in df_list]\n"], "link": "https://stackoverflow.com/questions/64753531/process-multiple-csv-files-on-pandas"}
{"id": 16, "q": "set some rule to groupby in pandas", "d": "I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have \"dup by\" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated.", "q_apis": "groupby groupby groupby all max all columns groupby", "io": "1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403 2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403 3|F08210020403|FO||dup by F08210020403 4|F08210020403|FO||dup by F08210020403 5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403 6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403 7|F08210020403|FO||dup by F08210020403 8|F08210020403|FO||dup by F08210020403 <s> 1|F08210020403|GO|2014-05-17 00:00:00|yes 2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403 3|F08210020403|FO||dup by F08210020403 4|F08210020403|FO||dup by F08210020403 5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403 6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403 7|F08210020403|FO||dup by F08210020403 8|F08210020403|FO||dup by F08210020403 ", "apis": "contains contains to_datetime groupby max groupby where contains map where astype where where astype loc duplicated time drop columns", "code": ["c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n"], "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas"}
{"id": 538, "q": "Find a shared time (overlap) in time columns in python", "d": "I'm trying to find a shared time where two datetime columns representing a start and an end time overlap with other records. for example if we have these two columns: I want to check the overlap between them, the output would be: is there an efficient way to achieve it?", "q_apis": "time time columns time where columns start time columns between", "io": "Start End 2016-08-22 20:20:00 2016-08-22 20:30:00 2016-08-22 20:55:00 2016-08-22 21:53:00 2016-08-22 21:38:00 2016-08-22 21:58:00 <s> Start End Overlap 2016-08-22 20:20:00 2016-08-22 20:30:00 NaN 2016-08-22 20:55:00 2016-08-22 21:53:00 2016-08-22 21:38:00 2016-08-22 21:38:00 2016-08-22 21:58:00 2016-08-22 21:38:00 ", "apis": "loc index name min total_seconds itertuples index values size loc loc index name min total_seconds itertuples index values size", "code": ["def common_row(x):\n    rows = df.loc[df.index != x.name,:]\n    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() > 0 for \n             y in rows.itertuples()]\n    shared = rows.index[s].values\n    if shared.size > 0:\n        return df.loc[shared[0], 'Start']\n", "def common_row(x):\n    rows = df.loc[df.index != x.name,:]\n    s = [min(x.End -y.Start, y.End - x.Start).total_seconds() > 0 for \n             y in rows.itertuples()]\n    shared = rows.index[s].values\n    if shared.size > 0:\n        return int(shared[0])\n"], "link": "https://stackoverflow.com/questions/53976903/find-a-shared-time-overlap-in-time-columns-in-python"}
{"id": 122, "q": "combining real and imag columns in dataframe into complex number to obtain magnitude using np.abs", "d": "I have a data frame that has complex numbers split into a real and an imaginary column. I want to add a column (2, actually, one for each channel) to the dataframe that computes the log magnitude: If I try this: I get error: \"TypeError: cannot convert the series to <class 'float'>\", because I think cmath.complex cannot work on an array. So I then experimented using loc to pick out the first element of ch1_real, for example, to then work out how use it to accomplish what I'm trying to do, but couldn't figure out how to do it: This produces a KeyError. Brute forcing it works, but, I believe it is more legible to use np.abs to get the magnitude, plus I'm more interested in understanding how dataframes and indexing dataframes work and why what I initially attempted does not work. btw, what is the difference between df.ch1_real and df['ch1_real'] ? When do I use one vs. the other? Edit: more attempts at solution I tried using apply, since my understanding is that it \"applies\" the function passed to it to each row (by default): but this generates the same TypeError, since I think the issue is that complex cannot work on Series. Perhaps if I cast the series to float? After reading this post, I tried using pd.to_numeric to convert a series to type float: to no avail.", "q_apis": "columns abs add get array loc first abs get difference between at apply Series to_numeric", "io": " ` ch1_real ch1_imag ch2_real ch2_imag ch1_phase ch2_phase distance 79 0.011960 -0.003418 0.005127 -0.019530 -15.95 -75.290 0.0 78 -0.009766 -0.005371 -0.015870 0.010010 -151.20 147.800 1.0 343 0.002197 0.010990 0.003662 -0.013180 78.69 -74.480 2.0 80 -0.002686 0.010740 0.011960 0.013430 104.00 48.300 3.0 341 -0.007080 0.009033 0.016600 -0.000977 128.10 -3.366 4.0 <s> df['ch1_log_mag'] = 20 * np.log10(np.sqrt(df.ch1_real**2+ df.ch1_imag**2)) ", "apis": "abs apply abs", "code": ["df['ch1_log_mag'] = 20 * np.log10((df.ch1_real + 1j * df.ch1_imag).abs())\n", "20 * np.log10(df.apply(lambda x: complex(x.ch1_real, x.ch1_imag), 1).abs())"], "link": "https://stackoverflow.com/questions/65738704/combining-real-and-imag-columns-in-dataframe-into-complex-number-to-obtain-magni"}
{"id": 510, "q": "pd.DataFrame prints output in a single column", "d": "I have a file which I'm trying to convert into data frame using pandas. It is inside a loop and returns me the output as shown below. Here is the code I'm using: Here the tbl file is not tab seperated and to create a tab separation, I've to convert it into list. Here is the reult I'm getting: The expected output is like this: Any help will be highly appreciated.", "q_apis": "DataFrame", "io": "0 B 1 244 2 S 3 0 0 0 B 1 245 2 A 3 0 <s> 0 B 244 S 0 0 B 245 A 0 ", "apis": "DataFrame columns concat DataFrame T DataFrame to_csv", "code": ["import pandas as pd\nimport csv\ndf=pd.DataFrame()\nwith open('File.tbl', 'r') as  f:\n    P=list(f)\n    del P[0]\nfor o in P:\n    M=o.split()\n    B= M[:4]             #selecting specific columns only\n    df = pd.concat([df,pd.DataFrame(B).T])    #converting into DataFrame\ndf.to_csv('para.csv', sep=',')\n"], "link": "https://stackoverflow.com/questions/54777760/pd-dataframe-prints-output-in-a-single-column"}
{"id": 330, "q": "Python pandas.DataFrame: Make whole row NaN according to condition", "d": "I want to make the whole row NaN according to a condition, based on a column. For example, if , I want to make the whole row NaN. Unprocessed data frame looks like this: Make whole row NaN, if : Thank you.", "q_apis": "DataFrame", "io": " A B 0 1 4 1 3 5 2 4 6 3 8 7 <s> A B 0 1.0 4.0 1 3.0 5.0 2 NaN NaN 3 NaN NaN ", "apis": "loc loc", "code": ["df.loc[df.B > 5, :] = np.nan", "df.loc[df.B > 5, :] = np.nan"], "link": "https://stackoverflow.com/questions/46488530/python-pandas-dataframe-make-whole-row-nan-according-to-condition"}
{"id": 117, "q": "Pandas: Create New Column Based on Conditions of Multiple Columns", "d": "I have the following dataset: In the cell, \u2018key\u2019 is the date when someone is hospitalized and \u2018value\u2019 is the number of days. I need to create a new column for hospitalization ('Yes' or 'No'). The condition to be 'yes': The column [AAA or BBB] as well as the column [CCC or DDD] both should have filled-in dates. The date in the column [CCC or DDD] should be the next day of the date in the column [AAA or BBB]. For example, if [AAA or BBB] has a date of January 01, 2020. For 'yes', the date in [CCC or DDD] should be January 02, 2020. Desired output: I have tried the following code, but this captures if the dates are present but doesn't capture the timestamp. Any suggestions would be appreciated. Thanks!", "q_apis": "date value days date day date date date timestamp", "io": " ID AAA BBB CCC DDD 1234 {'2015-01-01': 1} {'2016-01-01': 1, {'2015-01-02': 1} {'2016-01-02': 1} '2016-02-15': 2} 1235 {'2017-11-05': 1, {'2018-01-05': 1} NaN {'2017-01-06': 1} '2018-06-05': 1} <s> ID AAA BBB CCC DDD Hospitalized 1234 {'2015-01-01': 1} {'2016-01-01': 1, {'2015-01-02': 1} {'2016-01-02': 1} Yes '2016-02-15': 2} 1235 {'2017-11-05': 1, {'2018-01-05': 1} NaN NaN No '2018-06-05': 1} 1236 {'2017-11-05': 1, {'2018-01-05': 1} NaN {'2018-01-06': 1} Yes '2018-06-05': 1} ", "apis": "DataFrame columns map parse keys keys map parse keys keys days where notna apply", "code": ["df = pd.DataFrame([[1234, {'2015-01-01': 1}, {'2016-01-01': 1, '2016-02-15': 2}, {'2015-01-02': 1}, {'2016-01-02': 1}], [1235, {'2017-11-05': 1,'2018-06-05': 1}, {'2018-01-05': 1}, np.nan, np.nan]], columns= ['ID', 'AAA', 'BBB', 'CCC', 'DDD'])\n", "import itertools\nfrom dateutil import parser\nimport datetime\ndef func(x):\n    A_B_dates = list(map(parser.parse,list(itertools.chain(*[x['AAA'].keys()] + [x['BBB'].keys()]))))\n    C_D_dates = list(map(parser.parse,list(itertools.chain(*[x['CCC'].keys()] + [x['DDD'].keys()]))))\n    for date1 in A_B_dates:\n        if date1+datetime.timedelta(days=1) in C_D_dates:\n            return 'yes'\n    return 'no'\n\ndf = df.where(df.notna(), lambda x: [{}])    \ndf['Hospitalised'] = df.apply(func, axis=1)\n"], "link": "https://stackoverflow.com/questions/65793450/pandas-create-new-column-based-on-conditions-of-multiple-columns"}
{"id": 102, "q": "Remove double quotes in Pandas", "d": "I have the following file: Which I read by: Then I print and see: Instead, I would like to see: How to remove the double quotes?", "q_apis": "", "io": " \"j\" \"x\" y 0 \"0\" \"1\" 5 1 \"1\" \"2\" 6 2 \"2\" \"3\" 7 3 \"3\" \"4\" 8 4 \"4\" \"5\" 3 5 \"5\" \"5\" 4 <s> j x y 0 0 1 5 1 1 2 6 2 2 3 7 3 3 4 8 4 4 5 3 5 5 5 4 ", "apis": "replace read_csv test rename columns", "code": ["rm_quote = lambda x: x.replace('\"', '')\n\ndf = pd.read_csv('test.csv', delimiter='; ', engine='python', \n     converters={'\\\"j\\\"': rm_quote, \n                 '\\\"x\\\"': rm_quote})\n\ndf = df.rename(columns=rm_quote)\n"], "link": "https://stackoverflow.com/questions/44615807/remove-double-quotes-in-pandas"}
{"id": 656, "q": "Compare 2 consecutive cells in a dataframe", "d": "I have a dataframe (more than 150 rows and 16 columns) with like this: What I would like is to have only the last numbers per column before the 0 in a following row: I started to try with , but then I got stucked Can anyone help in this direction or with any other solution? Thanks in advance", "q_apis": "columns last any", "io": " a001 a002 a003 a004 a005 Year Week 2017 1 0 1 1 3 0 2 1 2 2 4 0 3 2 0 3 5 0 4 0 0 4 0 0 5 0 1 5 0 0 6 0 2 6 1 0 7 0 0 7 2 0 8 1 0 0 3 0 9 2 0 0 0 0 10 3 2 0 0 0 <s> a001 a002 a003 a004 a005 Year Week 2017 1 0 0 0 0 0 2 0 0 0 0 0 3 0 2 0 0 0 4 2 0 0 5 0 5 0 0 0 0 0 6 0 0 0 0 0 7 0 2 0 0 0 8 0 0 7 0 0 9 0 0 0 3 0 10 0 0 0 0 0 ", "apis": "where shift shift fillna astype", "code": ["df1 = df.where((df != 0) & (df.shift(-1) == 0)).shift().fillna(0).astype(int)\n"], "link": "https://stackoverflow.com/questions/49358261/compare-2-consecutive-cells-in-a-dataframe"}
{"id": 120, "q": "Add multiple DataFrame series to new series in same DataFrame", "d": "I have a dataset in a .csv which I imported into a DataFrame using pandas, organized in the following manner (obviously not real numbers): What I want to achieve is to save the data in two extra columns G and H in the same DataFrame so I get the following: Where I would like to keep the same index for all data (so B belongs to A, D to C, F to E etc.). As you can see, the original dataset has some missing values, so I would also like to skip these if they are in there. Now, I have looked into pandas append and concat, however I do not see how I can achieve what I wanted, especially with skipping the empty values (presumbly via or some other function?).", "q_apis": "DataFrame DataFrame DataFrame columns DataFrame get index all values append concat empty values", "io": " A B C D E F 0 20 4 24 8 28 1 21 5 25 NA NA NA NA 6 26 10 30 3 23 NA NA 11 31 <s> A B C D E F G H 0 20 1 21 ... ... 11 31 ", "apis": "read_excel", "code": ["df = pd.read_excel('sample_excel.xlsx', engine='openpyxl') df\n"], "link": "https://stackoverflow.com/questions/65749219/add-multiple-dataframe-series-to-new-series-in-same-dataframe"}
{"id": 552, "q": "Outputting pandas series to txt file", "d": "I have a pandas series object that look like this: What is the best way to go about outputting this to a txt file (e.g. output.txt) such that the format look like this? The values on the far left are the userId's and the other values are the movieId's. Here is the code that generated the above: I tried implementing @emmet02 solution but got this error, I do not understand why I got it though: Any advice is appreciated, please let me know if you need any more information or clarification.", "q_apis": "values left values any", "io": "userId 1 3072 1196 838 2278 1259 2 648 475 1 151 1035 3 457 150 300 21 339 4 1035 7153 953 4993 2571 5 260 671 1210 2628 7153 6 4993 1210 2291 589 1196 7 150 457 111 246 25 8 1221 8132 30749 44191 1721 9 296 377 2858 3578 3256 10 2762 377 2858 1617 858 11 527 593 2396 318 1258 12 3578 2683 2762 2571 2580 13 7153 150 5952 35836 2028 14 1197 2580 2712 2762 1968 15 1245 1090 1080 2529 1261 16 296 2324 4993 7153 1203 17 1208 1234 6796 55820 1060 18 1377 1 1073 1356 592 19 778 1173 272 3022 909 20 329 534 377 73 272 21 608 904 903 1204 111 22 1221 1136 1258 4973 48516 23 1214 1200 1148 2761 2791 24 593 318 162 480 733 25 314 969 25 85 766 26 293 253 4878 46578 64614 27 1193 2716 24 2959 2841 28 318 260 58559 8961 4226 29 318 260 1196 2959 50 30 1077 1136 1230 1203 3481 642 123 593 750 1212 50 643 750 671 1663 2427 5618 644 780 3114 1584 11 62 645 912 2858 1617 1035 903 646 608 527 21 2710 1704 647 1196 720 5060 2599 594 648 46578 50 745 1223 5995 649 318 300 110 529 246 650 733 110 151 318 364 651 1240 1210 541 589 1247 652 4993 296 95510 122900 736 653 858 1225 1961 25 36 654 333 1221 3039 1610 4011 655 318 47 6377 527 2028 656 527 1193 1073 1265 73 657 527 349 454 357 97 658 457 590 480 589 329 659 474 508 1 288 477 660 904 1197 1247 858 1221 661 780 1527 3 1376 5481 662 110 590 50 593 733 663 2028 919 527 2791 110 664 1201 64839 1228 122886 1203 665 1197 858 7153 1221 6539 666 318 300 161 500 337 667 527 260 318 593 223 668 161 527 151 110 300 669 50 2858 4993 318 2628 670 296 5952 508 272 1196 671 1210 1200 7153 593 110 <s> User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5 User-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5 ", "apis": "DataFrame index T columns index", "code": ["df = pd.DataFrame.from_items(zip(series.index, series.str.split(' '))).T\n", "df.columns = ['movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']\n", "df['customer_id'] = df.index\ndf = df[['customer_id', 'movie-id1', 'movie-id2', 'movie-id3', 'movie-id4', 'movie-id5']]\n"], "link": "https://stackoverflow.com/questions/48831802/outputting-pandas-series-to-txt-file"}
{"id": 655, "q": "Python Pandas delete every row after specific value in cell", "d": "I have a Pandas Dataframe that looks as follows: I want to delete every row after the first in the column. The result should look like this:", "q_apis": "delete value delete first", "io": " streak 0 1.0 1 2.0 2 0.0 3 1.0 4 2.0 5 0.0 6 0.0 <s> streak 0 1.0 1 2.0 ", "apis": "iloc eq values argmax", "code": ["print (df)\n   streak\na     1.0\nb     2.0\nc     0.0\nd     1.0\ne     2.0\nf     0.0\ng     0.0\n\ndf = df.iloc[:df['streak'].eq(0).values.argmax()]\nprint (df)\n   streak\na     1.0\nb     2.0\n"], "link": "https://stackoverflow.com/questions/49352824/python-pandas-delete-every-row-after-specific-value-in-cell"}
{"id": 127, "q": "Pandas: Add an empty row after every index in a MultiIndex dataframe", "d": "Consider below : How can I add an empty row after each index? Expected output:", "q_apis": "empty index MultiIndex add empty index", "io": " IA1 IA2 IA3 Name Subject Abc DS 45 43 34 DMS 43 23 45 ADA 32 46 36 Bcd BA 45 35 37 EAD 23 45 12 DS 23 35 43 Cdf EAD 34 33 23 ADA 12 34 25 <s> IA1 IA2 IA3 Name Subject Abc DS 45 43 34 DMS 43 23 45 ADA 32 46 36 Bcd BA 45 35 37 EAD 23 45 12 DS 23 35 43 Cdf EAD 34 33 23 ADA 12 34 25 ", "apis": "loc append DataFrame columns columns index name", "code": ["def f(x):\n    x.loc[('', ''), :] = ''\n    return x\n", "def f(x):\n    return x.append(pd.DataFrame('', columns=df.columns, index=[(x.name, '')]))\n"], "link": "https://stackoverflow.com/questions/65679439/pandas-add-an-empty-row-after-every-index-in-a-multiindex-dataframe"}
{"id": 32, "q": "Creating a table in pandas from json", "d": "I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance!", "q_apis": "", "io": "0 website 0 1 NaN 1 2 NaN <s> 0 website 0 1 https://bitcoin.org/ 1 2 https://litecoin.org/ ", "apis": "DataFrame loc apply to_frame DataFrame loc to_frame", "code": ["(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n", "(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n"], "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json"}
{"id": 176, "q": "Create Dataframe column that increases count based on another columns value", "d": "I need to add a column to my dataframe that will add a number each time a value in another column surpasses a limit. Example below: Original DF: Desired DF: Where value in ColB surpasses 10, ColC count = count +1 thank you for the help!", "q_apis": "count columns value add add time value value count count", "io": "ColA ColB 4 1.4 10 0.5 1 2.3 3 12.2 8.8 8.5 2 5.2 0.6 0.33 9 3 4 144 33 8 <s> ColA ColB ColC 4 1.4 1 10 0.5 1 1 2.3 1 3 12.2 2 8.8 8.5 2 2 5.2 2 0.6 0.33 2 9 3 2 4 144 3 33 8 3 ", "apis": "cumsum", "code": ["df['ColC'] = (df['ColB'] > 10).cumsum() + 1\nprint(df)\n"], "link": "https://stackoverflow.com/questions/64651683/create-dataframe-column-that-increases-count-based-on-another-columns-value"}
{"id": 565, "q": "Interpolation of a dataframe with immediate data appearing before and after it - Pandas", "d": "Let's say I've a dataframe like this - I want the dataframe to be transormed to - It takes the average of the immediate data available before and after a NaN and updates the missing/NaN value accordingly.", "q_apis": "value", "io": "ID Weight Height 1 80.0 180.0 2 60.0 170.0 3 NaN NaN 4 NaN NaN 5 82.0 185.0 <s> ID Weight Height 1 80.0 180.0 2 60.0 170.0 3 71.0 177.5 4 76.5 181.25 5 82.0 185.0 ", "apis": "interpolate interpolate", "code": ["df['Weight'], df['Height'] = df.Weight.interpolate(), df.Height.interpolate()\n"], "link": "https://stackoverflow.com/questions/53184917/interpolation-of-a-dataframe-with-immediate-data-appearing-before-and-after-it"}
{"id": 283, "q": "Trying to create a dataframe from a list of tuples, and a tuple", "d": "As the title states I'm tyring to create a df using a list of tuples and a tuple. So what I currently have is and my result is: Where as I want something like: Any and help is appreciated!", "q_apis": "", "io": " A B 0 1 4 1 2 5 2 3 6 <s> 0 1 0 A 1,2,3 1 B 4,5,6 ", "apis": "join map items DataFrame from_dict index reset_index", "code": ["d1 = {k: ','.join([*map(str,v)]) for k,v in d.items()}\npd.DataFrame.from_dict(d1,orient='index').reset_index()\n"], "link": "https://stackoverflow.com/questions/62200280/trying-to-create-a-dataframe-from-a-list-of-tuples-and-a-tuple"}
{"id": 402, "q": "How to update the Index of df with a new Index?", "d": "I am currently having one df which has an incomplete Index. like this: I have the complete . Would like to how to supplement the complete Index into the incomplete df. The can be \"\", the purpose is for me to identify which case I am currently missing. It's the first time I ask question on stackover, apology for the poor formatting.", "q_apis": "update Index Index Index Index first time", "io": "Idx bar baz zoo 001 A 1 x 003 B 2 y 005 C 3 z 007 A 4 q 008 B 5 w 009 C 6 t <s> Idx bar baz zoo 001 A 1 x 002 nan nan nan 003 B 2 y 004 nan nan nan 005 C 3 z 006 nan nan nan 007 A 4 q 008 B 5 w 009 C 6 t 010 nan nan nan ", "apis": "DataFrame index reindex", "code": ["df = pd.DataFrame({'bar':['A','B','C','A','B','C'],'baz':[1,2,3,4,5,6],'zoo':['x','y','z','q','w','t']}, index = ['001','003','005','007','008','009']) #your original df\nfull_index = ['001','002','003','004','005','006','007','008','009','010']  \ndf = df.reindex(full_index)\n"], "link": "https://stackoverflow.com/questions/58811263/how-to-update-the-index-of-df-with-a-new-index"}
{"id": 460, "q": "How to I convert a pandas dataframe row into multiple rows", "d": "I have a pandas dataframe that has one row per object. Within that object, there are subobjects. I want to create a dataframe which contains one row per subobject. I've read stuff on melt but can't begin to work out how to use it for what I want to do. I want to go from to", "q_apis": "contains melt", "io": "ObjectID Sub1_ID Sub1_Var1 Sub1_Var2 Sub1_Var3 Sub2_ID Sub2_Var1 Sub2_Var2 Sub2_Var3 1 98398 3 10 9 19231 6 7 5 2 87868 8 5 4 3 4579 5 6 6 24833 6 2 2 4 2514 1 6 9 <s> ObjectID Sub_ID Var1 Var2 Var3 1 98398 3 10 9 1 19231 6 7 5 2 87868 8 5 4 3 4579 5 6 6 3 24833 6 2 2 4 2514 1 6 9 ", "apis": "set_index columns MultiIndex from_arrays columns stack reset_index drop", "code": ["df1 = df.set_index('ObjectID')\n\ndf1.columns = pd.MultiIndex.from_arrays(zip(*df1.columns.str.split('_')))\n\ndf1.stack(0).reset_index().drop('level_1', axis=1)\n"], "link": "https://stackoverflow.com/questions/56776539/how-to-i-convert-a-pandas-dataframe-row-into-multiple-rows"}
{"id": 49, "q": "Summation of operation in dataframe", "d": "I want to implement a function that does the operation that you can see in the image: But i not sure how to implement the Summation for the moment i doing something like that: And the problem is in the summation. If someone can help me. For example, i have two dataframes like that: Where for the abs operation we will obtain this: And for the sum: Finally we do the summation: where and", "q_apis": "abs sum where", "io": " A B 0 5 14 1 4 2 <s> A B 0 11 38 1 8 36 ", "apis": "sub abs sum sum add sum sum", "code": ["result = (\n    dataframe1.sub(dataframe2).abs().sum().sum()\n    / dataframe1.add(dataframe2).sum().sum()\n) * 100\nprint(result)\n"], "link": "https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe"}
{"id": 520, "q": "How to drop a percentage of rows where the column value is NaN", "d": "Let's say I have a where a certain column has 50% missing values. How can I drop let's say 10% of the rows which are missing values in respect to the column? Basically how can I reduce the percentage of missing values of a column from 50% to 40%? Input (50% of values are missing (6/12)): Output (40% of values are missing (4/10)): We dropped the last 2 NaN rows with ID's of 8 and 10", "q_apis": "drop where value where values drop values values values values last", "io": " 0 0 1.0 1 1.0 2 NaN 3 NaN 4 NaN 5 1.0 6 NaN 7 1.0 8 NaN 9 1.0 10 NaN 11 1.0 <s> 0 0 1.0 1 1.0 2 NaN 3 NaN 4 NaN 5 1.0 6 NaN 7 1.0 9 1.0 11 1.0 ", "apis": "drop DataFrame put", "code": ["df.drop(nan_indices[:int(len(nan_indices) * 0.2)])   #to create a new DataFrame, if you want to modify the original one, put inplace=True\n"], "link": "https://stackoverflow.com/questions/54629663/how-to-drop-a-percentage-of-rows-where-the-column-value-is-nan"}
{"id": 625, "q": "Merge dictionaries to dataframe get_dummies", "d": "In a dictionary with information about a string in a text file, where keys are the strings and values are the names of the files. 'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'} Another dictionary contains information about the best match for the strings from the text. The third dictionary contains information about the percentage of occurrence of the string in the text. Now my aims are to use the information of these different dictionaries to generate a dataframe like this: To this, I started the script but I am stuck: This block was removed from script: And substituted to: But the final result was not very exact: But the expected table is: Where the string in file4 where not detected. Blosk removed Here relies the problem But the only result that I get is this: As I can understand, pd.getdummies groups the field 'file' from my df_origin and uses 'text' to check their presence. How can I redirect the command to plot the columns 'percent' in my df_origin dataframe?", "q_apis": "get_dummies where keys values names contains contains where get groups plot columns", "io": "Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'} <s> Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'} ", "apis": "", "code": ["       fly jump  run swim\nfile1   60   90  NaN   30\nfile2   30   70   40   60\nfile3   90   10   90   70\nfile4  NaN  NaN  NaN  NaN\n"], "link": "https://stackoverflow.com/questions/50644315/merge-dictionaries-to-dataframe-get-dummies"}
