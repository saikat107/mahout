{"id": 421, "q": "Filter duplicate rows of a pandas DataFrame", "d": "I'm trying to filter the rows of a pandas DataFrame based on some conditions and I'm having difficulties with it. The DataFrame is like so: The selection I would like to apply is the following: For all cus_id that appear more than once (i.e. for all duplicates cus_id), keep only the ones where cus_group is equal to 1. Caution: If a cus_id appears more than once but it only belongs to group 0, we keep all instances of this customer. Visually, the resulting DataFrame I want is like so: As you can see for cus_id = 5555, even though it does appear twice, we keep both records since it only belongs to group 0. I have tried a few things using the duplicated() method but with no success. Any additional help is would be appreciated. EDIT: The solution provided by jezrael works perfectly for the example above. I have noticed that in the real DataFrame I'm using there are cases where customers are linked to group. For example: Using the solution of jezrael those customers are dropped. Is there a quick fix to keep ALL (duplicates included) such cases in the final DataFrame? Visually (after filtering):", "q_apis": "DataFrame filter DataFrame DataFrame apply all all where all DataFrame duplicated DataFrame where DataFrame", "io": " cus_id cus_group 0 1111 1 1 2222 1 2 3333 0 3 4444 1 4 4444 1 5 5555 0 6 5555 0 <s> cus_id cus_group 0 1111 1.0 1 2222 1.0 2 3333 0.0 3 4444 1.0 4 4444 1.0 5 5555 0.0 6 5555 0.0 7 6666 NaN 8 7777 NaN 9 7777 NaN ", "apis": "eq groupby transform all eq groupby transform nunique eq eq", "code": ["df = df[df['cus_group'].eq(0).groupby(df['cus_id']).transform('all') | df['cus_group'].eq(1)]\n", "df = df[df.groupby('cus_id')['cus_group'].transform('nunique').eq(1) | df['cus_group'].eq(1)]\n"], "link": "https://stackoverflow.com/questions/58198249/filter-duplicate-rows-of-a-pandas-dataframe"}
{"id": 648, "q": "Merge 2 CSV files with mapped values in another file separated by comma", "d": "here is my problem: I have tow csv files as follows: Book1.csv Book2.csv I want merge above files and get an output file as this: The code I am using right now is: what I get from this is : All the Attributes and Products are merged correctly. But what I want is merge Attibutes into one string and separate by comma (not line by line). How do I do this? Thank you in advance!", "q_apis": "values merge get right now get merge", "io": "Id Product 0 aaaa 1 bbbb 2 cccc 3 dddd <s> Id Attribute 0 aaad 0 sssd 1 fffd 1 gggd 1 cccd 2 bbbd 3 hhhd 3 bbbd ", "apis": "groupby apply join map", "code": ["g = df2.groupby('Id')['Attribute'].apply(', '.join)\ndf1['Attributes'] = df1['Id'].map(g)\n"], "link": "https://stackoverflow.com/questions/49274575/merge-2-csv-files-with-mapped-values-in-another-file-separated-by-comma"}
{"id": 61, "q": "Splitting by indices: I want to split the train + test from the data whose indices have been given. How shall I get train/test df?", "d": "for example= df is the data with features. I want to split the train + test from the data whose indices have been given. How shall I get train/test df. where train.txt is where in this dataframe indices are given. How should I get the training data from those indices? Contents in data_train.txt(there are 10000 of data in which train indices are given in this txt file) I want these indices for training data with feature:- like final train should look like this (see the index):", "q_apis": "indices test indices get test test indices get test where where indices get indices indices indices index", "io": "df= 0 2 0.3 0.5 0.5 1 4 0.5 0.7 0.4 2 2 0.5 0.1 0.4 3 4 0.4 0.1 0.3 4 2 0.3 0.1 0.5 <s> 0 2 0.3 0.5 0.5 2 2 0.5 0.1 0.4 4 2 0.3 0.1 0.5 ", "apis": "index iloc index loc isin", "code": ["#if you're trying to match the index of the df itself\ntrain_df = df.iloc[train_indices]\n#if you're trying to match column 0, which might be important \n#if it's not aligned to the index\ntrain_df =  df.loc[df[0].isin(train_indices)]\n"], "link": "https://stackoverflow.com/questions/66977794/splitting-by-indices-i-want-to-split-the-train-test-from-the-data-whose-indic"}
{"id": 178, "q": "How to modify numercial values in a column of mixed data types in a pandas dataframe?", "d": "I have a pandas dataframe in pyhton that looks like this (my actual dataframe is MUCH bigger than this): How can I perform some operations on the numerical values of specific columns. For example, multiply the numerical values of col_2 by 10 to get something like this: Although it looks like a simple task I couldn't find a solution for it anywhere on internet. Thanks in advance.", "q_apis": "values values columns values get", "io": " col_1 col_2 0 0.8 0.1 1 nope 0.6 2 0.4 0.7 3 nope nope <s> col_1 col_2 0 0.8 1 1 nope 6 2 0.4 7 3 nope nope ", "apis": "to_numeric", "code": ["In [141]: df.col_2 = pd.to_numeric(df.col_2, errors='coerce')\n"], "link": "https://stackoverflow.com/questions/64640182/how-to-modify-numercial-values-in-a-column-of-mixed-data-types-in-a-pandas-dataf"}
{"id": 29, "q": "Python dataframe create index column based on other id column", "d": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:", "q_apis": "index", "io": "ID Price 000afb96ded6677c 1514.5 000afb96ded6677c 13.0 000afb96ded6677c 611.0 000afb96ded6677c 723.0 000afb96ded6677c 2065.0 ffea14e87a4e1269 2286.0 ffea14e87a4e1269 1150.0 ffea14e87a4e1269 80.0 fff455057ad492da 650.0 fff5fc66c1fd66c2 450.0 <s> ID Price ID 2 000afb96ded6677c 1514.5 1 000afb96ded6677c 13.0 1 000afb96ded6677c 611.0 1 000afb96ded6677c 723.0 1 000afb96ded6677c 2065.0 1 ffea14e87a4e1269 2286.0 2 ffea14e87a4e1269 1150.0 2 ffea14e87a4e1269 80.0 2 fff455057ad492da 650.0 3 fff5fc66c1fd66c2 450.0 4 ", "apis": "groupby ngroup", "code": ["df['ID_2'] = df.groupby('ID').ngroup() + 1\n"], "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column"}
{"id": 542, "q": "Dataframe summary math based on condition from another dataframe?", "d": "I have what amounts to 3D data but can't install the Pandas recommended xarray package. df_values df_condition I know I can get the average of all values in like this. Question... \ud83d\udc47 What is the simplest way to find the where ?", "q_apis": "get all values where", "io": " | a b c ----------------- 0 | 5 9 2 1 | 6 9 5 2 | 1 6 8 <s> | a b c ----------------- 0 | y y y 1 | y n y 2 | n n y ", "apis": "concat concat values eq where values eq stack mean", "code": ["# Pandas 0.23.0, NumPy 1.14.3\nn = 10**5\ndf_values = pd.concat([df_values]*n, ignore_index=True)\ndf_condition = pd.concat([df_condition]*n, ignore_index=True)\n\n%timeit np.nanmean(df_values.values[df_condition.eq('y')])       # 32 ms\n%timeit np.nanmean(df_values.where(df_condition == 'y').values)  # 88 ms\n%timeit df_values[df_condition.eq('y')].stack().mean()           # 107 ms\n"], "link": "https://stackoverflow.com/questions/53953121/dataframe-summary-math-based-on-condition-from-another-dataframe"}
{"id": 135, "q": "Replace NaN Values with the Means of other Cols based on Condition", "d": "I have the following Pandas DataFrame I am writing the following function: I want to to replace the missing values present in columns with labels in the list . The value to be replaced is computed as the mean of the non missing values of the corresponding group. Groups are formed based on the values in the columns with labels in the list . When is applied to the above dataframe with arguments, it should yield: this is because the record on line 4 belongs to the group that has a mean of (1+3)/2 = 2. I tried using but it is giving me the error", "q_apis": "DataFrame replace values columns value mean values values columns mean", "io": " Col1 Col2 Col3 0 A c 1.0 1 A c 3.0 2 B c 5.0 3 A d 6.0 4 A c NaN <s> Col1 Col2 Col3 0 A c 1.0 1 A c 3.0 2 B c 5.0 3 A d 6.0 4 A c 2.0 ", "apis": "groupby transform mean fillna to_dict", "code": ["def replace_missing_with_conditional_mean(df, condition_cols, cols):\n    s = df.groupby(condition_cols)[cols].transform('mean')\n    return df.fillna(s.to_dict('series'))\n\n\nres = replace_missing_with_conditional_mean(df, ['Col1', 'Col2'], ['Col3'])\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65483406/replace-nan-values-with-the-means-of-other-cols-based-on-condition"}
{"id": 444, "q": "Drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe", "d": "I want to remove all rows (or take all rows without) a question mark symbol in any column. I also want to change the elements to float type. Input: Output: Preferably using pandas dataframe operations.", "q_apis": "value any all take all any", "io": "X Y Z 0 1 ? 1 2 3 ? ? 4 4 4 4 ? 2 5 <s> X Y Z 1 2 3 4 4 4 ", "apis": "all any", "code": ["df = df[(df != '?').all(axis=1)]\n", "df = df[~(df == '?').any(axis=1)]\n"], "link": "https://stackoverflow.com/questions/35682719/drop-rows-with-a-question-mark-value-in-any-column-in-a-pandas-dataframe"}
{"id": 373, "q": "Replace negatives with zeros in a dataframe column of lists", "d": "I have a dataframe containing two columns. The first column is the date index. Each row of the second column is a list of 60 numbers that include negative values. I want to replace all negative values in this column with zeros. Here is the complete data for the first two rows: Currently, my solution is to convert the column of lists into a separate df of 60 columns. I can then convert the negatives into zeros in this df. Although this does the job, the .apply() operation is slow (taking 1.3 minutes for a df with 400,000 rows). Could someone please offer a more efficient (faster) alternative?", "q_apis": "columns first date index second values replace all values first columns apply", "io": " Spc 1976-10-31 15:00:00 [0.0124, 0.0096, 0.0325, 0.1562, 0.4494, 0.738...-1., -1., -1., -1.] 1976-11-01 03:00:00 [0.0254, 0.0299, 0.0273, 0.1229, 0.596, 0.9833...-1., -1., -1., -1.] 1976-11-01 15:00:00 [0.0226, 0.0236, 0.0269, 0.085, 0.4163, 0.8011...-1., -1., -1., -1.] 1976-11-02 03:00:00 [0.0132, 0.0154, 0.0172, 0.1336, 0.4743, 0.694...-1., -1., -1., -1.] 1976-11-02 15:00:00 [0.0124, 0.0169, 0.028, 0.5028, 1.4503, 1.6055...-1., -1., -1., -1.] : : : : : : : : : : 2017-05-20 04:00:00 [5.374061e-13, 1.2720002e-06, 0.00052255474, 0...2.8157034e-03, 1.4578120e-03] 2017-05-20 04:30:00 [1.2021946e-12, 3.3477074e-06, 0.0014435094, 0...5.88221522e-03, 3.44922021e-03] 2017-05-20 05:00:00 [1.2236685e-13, 5.018357e-07, 0.00023753957, 0...2.28277827e-03, 1.07194704e-03] 2017-05-20 05:30:00 [3.5527579e-13, 1.1004944e-06, 0.0005480177, 0...2.0632602e-03, 1.6171171e-03] 2017-05-20 06:00:00 [4.968573e-13, 1.4969078e-06, 0.00065009575, 0...1.21051911e-03, 1.18123344e-03] <s> 1976-10-31 15:00:00 [ 0.0013, 0.0016, 0.007, 0.03, 0.0803, 0.2318, 0.5842, 0.8401, 0.6, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -1., -1., -1., -1., -1., -1., -1., -1., -1. ] 1976-11-01 03:00:00 [ 0.0022, 0.004, 0.0104, 0.0512, 0.1112, 0.2227, 0.5263, 0.7085, 0.4, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -1., -1., -1., -1., -1., -1., -1., -1., -1. ] ", "apis": "copy apply Series to_numpy copy map copy array array clip min copy stack to_numpy clip min copy array clip min", "code": ["def func_1(df_in):\n    df_in = df_in.copy()\n    temp = df_in['col_2'].apply(pd.Series)\n    temp[temp < 0] = 0\n    df_in['col_2'] = temp.to_numpy().tolist()\n    return df_in\n", "def func_2(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = df_in['col_2'].map(lambda l: [0 if elem < 0 else elem for elem in l])\n    return df_in\n", "def func_3(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array([np.array(elem) for elem in df_in['col_2']]).clip(min=0).tolist()\n    return df_in\n", "def func_4(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.stack(df_in['col_2'].to_numpy()).clip(min=0).tolist()\n    return df_in\n", "def func_5(df_in):\n    df_in = df_in.copy()\n    df_in['col_2'] = np.array(df_in['col_2'].tolist()).clip(min=0).tolist()\n    return df_in\n"], "link": "https://stackoverflow.com/questions/59594392/replace-negatives-with-zeros-in-a-dataframe-column-of-lists"}
{"id": 560, "q": "creating a pandas dataframe based on cell content of two other dataframes", "d": "I have wo dataframes with the same number of rows and columns. I would like to create a third dataframe based on these two dataframes that has the same dimensions as the other two dataframes. Each cell in the third dataframe should be the result by a function applied to the corresponding cell values in df1 and df2 respectively. i.e. if I have then df3 should be like this I have a way to do this that I do not think is very pythonic nor appropriate for large dataframes and would like to know if there is an efficient way to do such a thing? The function I wish to apply is: It can be used to produce a single scalar value OR an array of values. In my use case above the input to the function would be two scalar values. So smape(1, 5) = 0.66.", "q_apis": "columns values apply value array values values", "io": "df1 = | 1 | 2 | | 3 | 4 | df2 = | 5 | 6 | | 7 | 8 | <s> df3 = | func(1, 5) | func(2, 6) | | func(3, 7) | func(4, 8) | ", "apis": "abs abs DataFrame where eq eq", "code": ["def smape3(df1, df2):\n    return (df2 - df1).abs() / (df2 + df1).abs()\n\ndf = pd.DataFrame(np.where(df1.eq(0) & df2.eq(0), 0, smape3(df1, df2)))\n"], "link": "https://stackoverflow.com/questions/53327932/creating-a-pandas-dataframe-based-on-cell-content-of-two-other-dataframes"}
{"id": 566, "q": "functools reduce In-Place modifies original dataframe", "d": "I currently facing the issue that \"functools.reduce(operator.iadd,...)\" alters the original input. E.g. I have a simple dataframe df = pd.DataFrame([[['A', 'B']], [['C', 'D']]]) Applying the iadd operator leads to following result: Now, the original df changed to Also copying the df using df.copy(deep=True) beforehand does not help. Has anyone an idea to overcome this issue? THX, Lazloo", "q_apis": "DataFrame copy", "io": " 0 0 [A, B] 1 [C, D] <s> 0 0 [A, B, C, D] 1 [C, D] ", "apis": "sum", "code": ["In [39]: df[0].sum()\nOut[39]: ['A', 'B', 'C', 'D']\n"], "link": "https://stackoverflow.com/questions/53149552/functools-reduce-in-place-modifies-original-dataframe"}
{"id": 146, "q": "How do I apply a function to the groupby sub-groups that depends on multiple columns?", "d": "Take the following data frame and groupby object. How would I apply to the groupby object , multiplying each element of and together and then taking the sum. So for this example, for the group and for the group. So my desired output for the groupby object is:", "q_apis": "apply groupby sub groups columns groupby apply groupby sum groupby", "io": "2*3 + 4*5 = 26 <s> a f 0 1 26 2 2 30 ", "apis": "DataFrame columns groupby sum", "code": ["df = pd.DataFrame([[1, 2, 3],[1, 4, 5],[2, 5, 6]], columns=['a', 'b', 'c'])\ndf['f'] = df['c'] * df['b']\nres = df.groupby('a', as_index=False)['f'].sum()\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65178805/how-do-i-apply-a-function-to-the-groupby-sub-groups-that-depends-on-multiple-col"}
{"id": 267, "q": "Rename Columns in a Pandas Dataframe with values form dictionary", "d": "I have a pandas data frame read from an excel file. Note: the column names remain the same but the position of the column might vary in the excel file. df I have a list of dictionaries that should be used to change the column names, which is as below field_map I could convert the column keys for each row in the DataFrame separately in this way and using the for further operations. This method is taking too long when my file is large. I want to change the column headers of the data Frame before processing the entries further, this will reduce a lot of processing time for me. Kindly help me with this. I'm expecting the data frame to be something like this Expected df Thanks in Advance", "q_apis": "values names names keys DataFrame time", "io": " colA colB colC ... 0 val11 val12 val13 ... 1 val21 val22 val23 ... ... ... ... <s> tab1 tab2 tab3 ... 0 val11 val12 val13 ... 1 val21 val22 val23 ... ... ... ... ", "apis": "rename columns rename columns", "code": ["df = df.rename(columns={\"colA\":\"tab1\", \"colB\":\"tab2\", \"colB\":\"tab3\"})\n", "col_rename_dict = {el[\"file_field\"]:el[\"table_field\"] for el in field_map}\ndf = df.rename(columns=col_rename_dict)\n"], "link": "https://stackoverflow.com/questions/62811834/rename-columns-in-a-pandas-dataframe-with-values-form-dictionary"}
{"id": 602, "q": "pandas: assign random numbers in given range to equal column values", "d": "I am working with a large dataset, and one of the columns has very long integers, like below: What is important here is not the actual number in Column_2, but when those numbers are the same while Column_1 is different. I would like to reassign the values of Column_2 randomly from a range of smaller numbers, say (1, 999). My issue is figuring a way to describe in a lambda function that each equal value in Column_2 needs the same random number.", "q_apis": "assign values columns values describe value", "io": " Column_1 Column_2 1 A 12345123451 2 B 12345123451 3 C 12345123451 4 D 23456789234 5 E 23456789234 6 F 34567893456 <s> Column_1 Column_2 1 A 120 2 B 120 3 C 120 4 D 54 5 E 54 6 F 567 ", "apis": "array", "code": [">>> nums\narray([274, 842, 860])\n"], "link": "https://stackoverflow.com/questions/51735106/pandas-assign-random-numbers-in-given-range-to-equal-column-values"}
{"id": 312, "q": "Count how many cells are between the last value in the dataframe and the end of the row", "d": "I'm using the pandas library in Python. I have a data frame: Is it possible to create a new column that is a count of the number of cells that are empty between the end of the row and the last value above zero? Example data frame below:", "q_apis": "between last value count empty between last value", "io": " 0 1 2 3 4 0 0 0 0 1 0 1 0 0 0 0 1 2 0 0 1 0 0 3 1 0 0 0 0 4 0 0 1 0 0 5 0 1 0 0 0 6 1 0 0 1 1 <s> 0 1 2 3 4 Value 0 0 0 0 1 0 1 1 0 0 0 0 1 0 2 0 0 1 0 0 2 3 1 0 0 0 0 4 4 0 0 1 0 0 2 5 0 1 0 0 0 3 6 1 0 0 1 1 0 ", "apis": "value apply iloc argmax where iloc argmax", "code": ["df['value'] = df.apply(lambda x: (x.iloc[::-1] == 1).argmax(),1)\n\n##OR\n", "df['Value'] = np.where(df.iloc[:,::-1] == 1,True,False).argmax(1)\n"], "link": "https://stackoverflow.com/questions/61138851/count-how-many-cells-are-between-the-last-value-in-the-dataframe-and-the-end-of"}
{"id": 31, "q": "Sort pandas df subset of rows (within a group) by specific column", "d": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation?", "q_apis": "sub", "io": " A B C D E z k s 7 d z k s 6 l x t r 2 e x t r 1 x u c r 8 f u c r 9 h y t s 5 l y t s 2 o <s> A B C D E z k s 6 l z k s 7 d x t r 1 x x t r 2 e u c r 8 f u c r 9 h y t s 2 o y t s 5 l ", "apis": "sort_values", "code": ["df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"], "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column"}
{"id": 78, "q": "How to manipulate data cell by cell in pandas df?", "d": "Let the sample df (df1) be, We can achieve df2 or final data-frame by manipulating the data of df1 in the following manner, Step 1: Remove all positive numbers including zeros After Step 1 the sample data should look like, Step 2: If A row is a negative number and B is blank, then remove the -ve number of A row Step 3: If A row is blank and B is a negative number, then keep the -ve number of B row After Steps 1,2 and 3 are done, Step 4: If both A and B of are negative then, For each A and B row of , check the left-side (LHS) value (for a given month) of the same A and B row of Step 4.1: If either of the LHS values of A or B is a -ve number, then delete the current row value of B and keep the current row value of A After Step 4.1, the sample data should look like this, Step 4.2: If the LHS value of A and B is blank, then keep the current row value of B and delete the current row value of A Sample data after Step 4.2 should look like, Since we see two negative numbers still, we perform Step 4.1 again and then the final data-frame or df2 will look like, How may I achieve the above using pandas? I was able to achieve till Step 1 but have no idea as to how to proceed further. Any help would be greatly appreciated. This is the approach that I took, Small Test data: df1, df2 (expected output), Test data: df1 df2 (expected output) , Note: I have implemented my code on the basis of the Test data provided. The sample data is merely to focus on the columns that are supposed to be manipulated.", "q_apis": "sample all sample left value month values delete value value sample value value delete value sample columns", "io": "{'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [0, 10, 0, 0], 'Mar-21': [0, 0, 70, 70], 'Apr-21': [-10, -10, -8, 60], 'May-21': [-30, -60, -10, 40], 'Jun-21': [-20, 9, -40, -20], 'Jul-21': [30, -10, 0, -20], 'Aug-21': [-30, -20, 0, -20], 'Sep-21': [0, -15, 0, -20], 'Oct-21': [0, -15, 0, -20]} <s> {'column1': ['ABC', 'ABC', 'CDF', 'CDF'], 'column4': ['A', 'B', 'A', 'B'], 'Feb-21': [nan, nan, nan, nan], 'Mar-21': [nan, nan, nan, nan], 'Apr-21': [nan, -10.0, nan, nan], 'May-21': [-30.0, nan, nan, nan], 'Jun-21': [nan, nan, nan, -20.0], 'Jul-21': [nan, -10.0, nan, -20.0], 'Aug-21': [-30.0, nan, nan, -20.0], 'Sep-21': [nan, -15.0, nan, -20.0], 'Oct-21': [nan, -15.0, nan, -20.0]} ", "apis": "DataFrame unique index reset_index drop index loc index shift loc index columns index where fillna set_index drop index T values isnull values index loc T groupby index apply reset_index drop set_index drop index T isnull values values index loc loc T groupby index apply reset_index drop set_index drop index T columns get_loc columns get_loc step values values index index get_indexer iloc notnull any index index get_indexer iloc step values values index index get_indexer iloc isnull all index index get_indexer iloc step values values index index get_indexer iloc notnull any index index get_indexer iloc T groupby index apply reset_index drop", "code": ["df = pd.DataFrame(  ...  )\n\n#Compute the unique index for each pair of rows\ndf.reset_index(drop=False, inplace=True)\nix = df.index\nix = ix[ix%2==0]\ndf.loc[ix, 'index'] = df.shift(-1).loc[ix, 'index']\n\n#step1 :\ncols = [x for x in df.columns.tolist() if not x.startswith('column') and x != \"index\"]\ndf[cols] = df[cols].where(df[cols] < 0, np.nan)\n\n\ncols_index = [\"column4\", \"column1\", \"column2\", \"column3\", \"column5\", \"column6\", \"column7\"]\ndf[cols_index] = df[cols_index].fillna(-1)\n\n#step2 :\ndef step2(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            (df.A<=0).values\n            & df.B.isnull().values\n         ].index\n    df.loc[ix, \"A\"] = np.nan\n    return df.T\ndf = df.groupby('index').apply(step2)\nprint(df)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n\n#step3 :\ndef step3(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    ix = df[\n            df.A.isnull().values \n            & (df.B>=0).values\n            ].index\n    df.loc[ix, \"B\"] = df.loc[ix, \"A\"]\n    return df.T\ndf = df.groupby('index').apply(step3)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n\n#step4 :\ndef step4(df):\n    df = df.set_index(cols_index).drop('index', axis=1).T\n    a_pos = df.columns.get_loc('A')\n    b_pos = df.columns.get_loc('B')\n    \n    #step 4.1\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n    \n    #step 4.2\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].isnull().all(axis=1)    \n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, a_pos] = np.nan\n    \n    #step 4.1 (again)\n    ix = df[\n            (df.A<0).values\n            & (df.B<0).values\n            ].index\n    if len(ix):\n        ix = df.index.get_indexer(ix)\n        left_pos = ix-1\n        condition_left = df.iloc[left_pos].notnull().any(axis=1)\n        ix = condition_left[condition_left].index\n        ix = df.index.get_indexer(ix)\n        df.iloc[ix+1, b_pos] = np.nan\n        \n    return df.T\n\ndf = df.groupby('index').apply(step4)\ndf.reset_index(drop=False, inplace=True)\nprint(df)\nprint('-'*50)\n"], "link": "https://stackoverflow.com/questions/66422275/how-to-manipulate-data-cell-by-cell-in-pandas-df"}
{"id": 432, "q": "pandas how to drop rows when all float columns are NaN", "d": "I have the following df With the following dtypes Is there a way to drop rows only when ALL float columns are NaN? output: I can't do it with df.dropna(subset=['ID1','ID2','ID3','ID4']) because my real df has several dynamic floating columns. Thanks", "q_apis": "drop all columns dtypes drop columns dropna columns", "io": " AAA BBB CCC DDD ID1 ID2 ID3 ID4 0 txt txt txt txt 10 NaN 12 NaN 1 txt txt txt txt 10 NaN 12 13 2 txt txt txt txt NaN NaN NaN NaN <s> AAA BBB CCC DDD ID1 ID2 ID3 ID4 0 txt txt txt txt 10 NaN 12 NaN 1 txt txt txt txt 10 NaN 12 13 ", "apis": "dropna select_dtypes columns all dropna select_dtypes columns all", "code": ["df1 = df.dropna(subset=df.select_dtypes(float).columns, how='all')\n#for return same dataframe \n#df.dropna(subset=df.select_dtypes(float).columns, how='all', inplace=True)\n"], "link": "https://stackoverflow.com/questions/57842073/pandas-how-to-drop-rows-when-all-float-columns-are-nan"}
{"id": 105, "q": "Create DF Columns Based on Second DDF", "d": "I have 2 dataframes with different columns: I would like to add the missing columns for the 2 dataframes - so each one will have each own columns + the other DFs columns (without column \"number\"). And the new columns will have initial number for our choice (let's say 0). So the final output: What's the best way to achieve this result? I've got messed up with getting the columns and trying to create new ones. Thank!", "q_apis": "columns add columns columns columns columns columns", "io": "DF A - DF B - number | a | b | c |||| a | c | d | e | f 1 | 12 | 13 | 15 |||| 22 | 33 | 44 | 55 | 77 <s> DF A - number | a | b | c | d | e | f 1 | 12 | 13 | 15 | 0 | 0 | 0 DF B - a | b | c | d | e | f 22 | 0 | 33 | 44 | 55 | 77 ", "apis": "columns to_list columns to_list columns columns", "code": ["all_columns = list(set(A.columns.to_list() + B.columns.to_list()))\n", "col_missing_from_A = [col for col in all_columns if col not in A.columns]\ncol_missing_from_B = [col for col in all_columns if col not in B.columns]\n"], "link": "https://stackoverflow.com/questions/66075388/create-df-columns-based-on-second-ddf"}
{"id": 613, "q": "Convert list of dictionaries to dataframe with one column for keys and one for values", "d": "Let's suppose I have the following list: Which I want to convert it to a panda dataframe that have two columns: one for the keys, and one for the values. To do so, I have tried to use and also , but, in both cases, I get a dataframe like: Is there any way to specify what I want? By doing research I could only find the way I am describing above.", "q_apis": "keys values columns keys values get any", "io": "list1 = [{'a': 1}, {'b': 2}, {'c': 3}] <s> a b c 0 1.0 NaN NaN 1 NaN 2.0 NaN 2 NaN NaN 3.0 ", "apis": "items", "code": ["print ([(i, j) for a in list1 for i, j in a.items()])\n\n[('a', 1), ('b', 2), ('c', 3)]\n"], "link": "https://stackoverflow.com/questions/51186619/convert-list-of-dictionaries-to-dataframe-with-one-column-for-keys-and-one-for-v"}
{"id": 652, "q": "Merging/Combining Dataframes in Pandas", "d": "I have a df1, example: ,and a df2, example: The column and row 'C' is common in both dataframes. I would like to combine these dataframes such that I get, Is there an easy way to do this? pd.concat and pd.append do not seem to work. Thanks! Edit: df1.combine_first(df2) works (thanks @jezarel), but can we keep the original ordering?", "q_apis": "combine get concat append combine_first", "io": " C E D C 2 3 E 1 D 2 <s> B A C D E B 1 A 1 C 2 2 3 D 1 E 2 ", "apis": "columns append columns unique index append index unique combine_first reindex index columns", "code": ["c = df1.columns.append(df2.columns).unique()\ni = df1.index.append(df2.index).unique()\n\ndf = df1.combine_first(df2).reindex(index=i, columns=c)\n"], "link": "https://stackoverflow.com/questions/49493720/merging-combining-dataframes-in-pandas"}
{"id": 93, "q": "Reverse columns of dataframe based on the column name", "d": "I have a dataframe: I would like to reverse the columns that their column names have their 1st and 2nd letters reversed and their 3rd and 4th as-is. i.e. 1st col: 1000 \u2192 2nd col: 0100 3rd col: 0010 \u2192 5th col: 1110 4th col: 0001 \u2192 6th col: 1101 7th col: 1011 \u2192 8th col: 0111 I would like to have a dataframe like this: This is what I have for the reversion:", "q_apis": "columns name columns names", "io": " '1000' '0100' '0010' '0001' '1110' '1101' '1011' '0111' 0 0 1 2 3 4 5 6 7 1 00 11 22 33 44 55 66 77 <s> '0100' '1000' '1110' '1101' '0010' '0001' '1011' '0111' 0 1 0 4 5 2 3 7 6 1 11 00 44 55 22 33 77 66 ", "apis": "columns columns columns array bool columns", "code": ["df.columns = df.columns.str.strip(\"'\")\ncols = df.columns\n\narr = np.array([[bool(int(y)) for y in x] for x in df.columns])\nprint (arr)\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n"], "link": "https://stackoverflow.com/questions/66255574/reverse-columns-of-dataframe-based-on-the-column-name"}
{"id": 641, "q": "Cross reference list of ids to index", "d": "I have grouped together a list of ids that are associated with a certain value and placed all these lists of ids into a dataframe. It looks like this: (with index = id) I want to iterate through these lists and cross reference them to the id index where phase equals either a 2 or 3, then just keep the ids that match within the original list (or if not possible, create a new column with modified lists). Something like this below: If possible I'd like to do this within the dataframe object as there are multiple features/dependencies for each row. Any tips on how to go about this? My actual data: And the dtypes: And the good_ids output:", "q_apis": "index value all index index where equals dtypes", "io": " phase list_ids id a1 1 [a1,a2,c3] a2 3 [a1,b2,c3] b1 3 [a2,b2] b2 2 [b1,b2,c1] b3 3 [b2,c1] c1 1 [a1,a2,c3] c2 1 [a1,b1,c4] c3 2 [c1,c2,c4] c4 1 [c1,c2] <s> phase ids Study_id ACP-103-006 2.0 [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... ACP-103-008 2.0 [ACP-103-006, ACP-103-020, ACP-103-019, ACP-10... ACP-103-010 2.0 [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10... ACP-103-012 3.0 [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10... ACP-103-014 3.0 [ACP-103-042, ACP-103-034, ACP-103-014, ACP-10... ", "apis": "isin index", "code": ["good_ids = set(df[df[\"phase\"].isin([2,3])].index)\nprint(good_ids)\n#{'a2', 'b1', 'b2', 'b3', 'c3'}\n"], "link": "https://stackoverflow.com/questions/49885060/cross-reference-list-of-ids-to-index"}
{"id": 406, "q": "Convert standard date format to string splitting by point in Python", "d": "One have one column which has the following format: How can I convert it to format six digits format? I have tried with the following code to but only get : But my desired output: Thank you.", "q_apis": "date get", "io": "0 2019/5/20 22:49:29 1 2019/5/20 23:18:23 2 2019/3/8 9:11:35 3 2019/3/8 9:19:58 4 2019/5/20 22:57:12 5 2019/3/8 9:06:41 <s> 0 19.05.20 1 19.05.20 2 19.03.08 3 19.03.08 4 19.05.20 5 19.03.08 ", "apis": "date date strftime date date to_datetime date strftime", "code": ["df['date'] = df['date'].dt.strftime('%y.%m.%d')\n\n# if your date is not datetime type:\n# df['date'] = pd.to_datetime(df['date']).dt.strftime('%y.%m.%d')\n"], "link": "https://stackoverflow.com/questions/58653299/convert-standard-date-format-to-string-splitting-by-point-in-python"}
{"id": 221, "q": "pandas drop diffrent rows with differing column values", "d": "I have DataFrame that looks like Input: I would like to remove rows from \"col1\" that share a common value in \"col2\" except values that are the same i.e. letter \"e\". I would like it to be where only one value in \"col1\" can = a unique one in \"col2\" The expected output would look something like... Output: What would be the process of doing this?", "q_apis": "drop values DataFrame value values where value unique", "io": " col1 col2 col3 0 a 1 1 1 b 3 2 2 c 3 3 3 d 2 4 4 e 6 5 5 e 6 6 <s> col1 col2 col3 0 a 1 1 3 d 2 4 4 e 6 5 5 e 6 6 ", "apis": "duplicated duplicated", "code": ["df[np.logical_or(~df.duplicated('col2', keep = False),df.duplicated('col1', keep = False)) ]\n"], "link": "https://stackoverflow.com/questions/63674555/pandas-drop-diffrent-rows-with-differing-column-values"}
{"id": 382, "q": "Pandas: How to return the rows where col value is greater than &#39;x&#39; in rolling window", "d": "I have a large df and I am trying find all rows where the value in a specific column is above a given number but within a window of say 3 rows and returning only the rows with the highest value over the given number. If I wanted to do this with the above example for column D, where the value must be above 11, the output would be. What would be the best way to go about this? I've tried: but can't find a way to include the greater than condition. Any help is appreciatted. Thanks!", "q_apis": "where value rolling all where value value where value", "io": "A B C D E 1 5 9 10 15 2 4 7 12 16 3 3 5 10 18 4 2 3 15 17 5 1 1 10 14 6 5 9 17 13 7 4 7 10 14 8 3 5 19 19 9 2 3 10 18 10 4 7 5 14 11 3 5 6 19 12 2 3 7 18 <s> A B C D E 2 4 7 12 16 6 5 9 17 13 8 3 5 19 19. ", "apis": "floor index groupby apply loc max max", "code": ["threshold = 11\nwindow = 3\ndf['r'] = np.floor(df.index / window)\nprint(df.groupby('r').apply(lambda x : (x.loc[x['D'] == x['D'].max() ,:]) if x['D'].max() > threshold else None))\n"], "link": "https://stackoverflow.com/questions/59371013/pandas-how-to-return-the-rows-where-col-value-is-greater-than-x-in-rolling-wi"}
{"id": 170, "q": "Pandas remove characters from index", "d": "I have the following dataframe: I want to remove the '-' character with the upper value in the index so I end up with the following dataframe: How do I do this?", "q_apis": "index value index", "io": " A 0-1.5 1 1.5-3.3 2 3.3-5.4 3 5.4-7.9 4 <s> A 0 1 1.5 2 3.3 3 5.4 4 ", "apis": "rename", "code": ["df = df.rename(lambda x: x.split('-')[0])\n"], "link": "https://stackoverflow.com/questions/64747375/pandas-remove-characters-from-index"}
{"id": 493, "q": "How to print rows if a list of values appear in any column of pandas dataframe", "d": "How to print rows if values appear in any column of pandas dataframe I would like to print all rows of a dataframe where I find some values from a list of values in any of the columns. The dataframe follows this structure: First: I have a Series of values with size 3 that I get from a combinatory of 6 different values. Second: I have a dataframe with 2143 rows. I want to check if in any of these rows, I have those three values in any sort of order in the columns. Gave me this: I just tried the query command and this is what I've got: df_ordered.query('_1 == 2 & _2 == 12') Now, I want to expand the same thing, but I want to look at all those columns and find any of those values. I also didn't know how to plug those series into a loop to find the values into the query statement. EDIT: I tried the command, but I have no ideia how to expand it to the 6 columns I have.", "q_apis": "values any values any all where values values any columns Series values size get values any values any columns query query at all columns any values values query columns", "io": "1476 13/03/2013 4 10 26 37 47 57 1475 09/03/2013 12 13 37 44 48 51 1474 06/03/2013 1 2 3 11 28 43 1473 02/03/2013 2 12 33 57 58 60 1472 27/02/2013 12 18 23 25 45 50 1471 23/02/2013 10 25 33 36 40 58 1470 20/02/2013 2 34 36 38 51 55 1469 16/02/2013 4 13 35 54 56 58 1468 13/02/2013 1 2 10 19 20 37 1467 09/02/2013 23 24 26 41 52 53 1466 06/02/2013 4 6 13 34 37 51 1465 02/02/2013 6 11 16 26 44 53 1464 30/01/2013 2 24 32 50 54 59 1463 26/01/2013 13 22 28 29 40 48 1462 23/01/2013 5 9 25 27 38 40 1461 19/01/2013 31 36 44 47 49 54 1460 16/01/2013 4 14 27 38 50 52 1459 12/01/2013 2 6 30 34 35 52 1458 09/01/2013 2 4 16 33 44 51 1457 05/01/2013 15 16 34 42 46 59 1456 02/01/2013 6 8 14 26 36 40 1455 31/12/2012 14 32 33 36 41 52 1454 22/12/2012 4 27 29 41 48 52 1453 20/12/2012 6 13 25 32 47 57 <s> 0 [(2, 12, 35), (2, 12, 51), (2, 12, 57), (2, 12... 1 [(12, 35, 51), (12, 35, 57), (12, 35, 58), (12... 2 [(35, 51, 57), (35, 51, 58), (35, 57, 58)] 3 [(51, 57, 58)] ", "apis": "array mask array values any mask", "code": ["import numpy as np\nfrom itertools import combinations\n\ninputlist = [2,12,35,51,57,58]\ncombined = np.array(list(combinations(inputlist, 3)))\n\nmask = (np.array([set(row).issuperset(c) for row in df.values for c in combined])\n        .reshape(len(df), -1).any(1))\n\nprint(df[mask])\n"], "link": "https://stackoverflow.com/questions/55777300/how-to-print-rows-if-a-list-of-values-appear-in-any-column-of-pandas-dataframe"}
{"id": 439, "q": "In a dataframe how can I count a specific value and then select the value with the highest count to create another dataframe?", "d": "I am looking for a way to select specific rows of data from a dataframe. Here is an example of the dataframe. I am looking for an output frame like this: Note, ID 006DE4E3 is not in the output because there the counts of the value was equal. Thank You!", "q_apis": "count value select value count select value", "io": "Id \\ Value 0 002D85EF 5 1 002D85EF 1 2 002D85EF 5 3 00557D1B 1 4 00557D1B 1 5 00557D1B 5 6 0063EAFB 5 7 0063EAFB 5 8 0063EAFB 5 9 006DE4E3 1 10 006DE4E3 5 11 006DE4E3 1 12 006DE4E3 5 <s> Id \\ Value 0 002D85EF 5 1 00557D1B 1 2 0063EAFB 5 ", "apis": "size DataFrame columns value groupby count nunique iloc groupby apply value dropna", "code": ["import numpy as np\nimport pandas as pd\na = np.random.randint(5, high=10, size=(20, 1))\nb = np.random.choice(['a', 'b', 'c', 'd'], 20)[:, None]\nc = pd.DataFrame(np.hstack([b,a]), columns=['id', 'value'])\n\n\ndef first_or_none(grp, col_name):\n    cnts = grp.groupby(col_name).count()\n    if len(cnts) == len(cnts.nunique()):\n        return None\n    else:\n        return grp.iloc[0]\n\nc.groupby(['id']).apply(first_or_none, 'value').dropna()\n"], "link": "https://stackoverflow.com/questions/57481676/in-a-dataframe-how-can-i-count-a-specific-value-and-then-select-the-value-with-t"}
{"id": 57, "q": "How to replace &#39;Zero&#39; by &#39;One&#39; for particular row in data frame", "d": "I've this dataframe:df1 I would like to Find the minimum value of last two entry of Variance row. I would like to last two entries and finding minimum , like in variance last two entries are 474.0 and 1101.0 and that should be added in Nan place. Output look like I've tried this code:", "q_apis": "replace value last last last", "io": "Variance 160244.0 37745.0 42003.0 15082.0 13695.0 89.0 474.0 1101.0 NaN -0.0 <s> Variance 160244.0 37745.0 42003.0 15082.0 13695.0 89.0 474.0 1101.0 474.0 -0.0 ", "apis": "iloc iloc min index get_loc iloc iloc min loc columns loc columns min", "code": ["df1.iloc[-2, -2] = df1.iloc[-2, -4:-2].min()\n", "pos = df1.index.get_loc('Variance')\ndf1.iloc[pos, -2] = df1.iloc[pos, -4:-2].min()\n", "df1.loc['Variance', df1.columns[-2]] = df1.loc['Variance', df1.columns[-4:-2]].min()\n"], "link": "https://stackoverflow.com/questions/67053308/how-to-replace-zero-by-one-for-particular-row-in-data-frame"}
{"id": 331, "q": "Need help getting the frequency of each number in a pandas dataframe", "d": "I am trying to find a simple way of converting a pandas dataframe into another dataframe with frequency of each feature. I'll provide an example of what I'm trying to do below Current dataframe example (feature labels are just index values here): Dataframe I would like to convert this to: As you can see, the column label corresponds to the possible numbers within the dataframe and each frequency of that number per row is put into that specific feature for the row in question. Is there a simple way to do this with python? I have a large dataframe that I am trying to transform into a dataframe of frequencies for feature selection. If any more information is needed I will update my post.", "q_apis": "index values put transform any update", "io": " 0 1 2 3 4 ... n 0 2 3 1 4 2 ~ 1 4 3 4 3 2 ~ 2 2 3 2 3 2 ~ 3 1 3 0 3 2 ~ ... m ~ ~ ~ ~ ~ ~ <s> 0 1 2 3 4 ... n 0 0 1 2 1 1 ~ 1 0 0 1 2 2 ~ 2 0 0 3 2 0 ~ 3 1 1 1 2 0 ~ ... m ~ ~ ~ ~ ~ ~ ", "apis": "T melt crosstab value", "code": ["df2 = df.T.melt()\npd.crosstab(df2['variable'], df2['value'])\n"], "link": "https://stackoverflow.com/questions/60728016/need-help-getting-the-frequency-of-each-number-in-a-pandas-dataframe"}
{"id": 224, "q": "Round in pandas column scientific numbers", "d": "Hello I have a df such as : I knwo how to roun the digit in the COL2 by using but if I do the same for COL1 it only displays 0 how can I get instead: in fact the big issue is here: it displays : and I would like to keep only 3 number after the coma", "q_apis": "get", "io": "COL1 COL2 0.005554 0.35200000000000004 5.622e-11 0.267 0.006999999999999999 0.307 2.129e-14 0.469 2.604e-14 0.39 1.395e-60 0.27899999999999997 8.589999999999998e-74 0.29600000000000004 1.025e-42 0.4270000000000001 <s> COL1 COL2 0.005 0.352 5.622e-11 0.267 0.007 0.307 2.129e-14 0.469 2.604e-14 0.39 1.395e-60 0.279 8.560e-74 0.296 1.025e-42 0.427 ", "apis": "", "code": ["pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"], "link": "https://stackoverflow.com/questions/63616688/round-in-pandas-column-scientific-numbers"}
{"id": 555, "q": "Modify and flatten values from Pandas dataframe", "d": "Here is the dataframe I am working with: dtypes gives this: You can get a sample of the data by click on the link below: https://ufile.io/x534q What I would like to do now is to get rid of the header, the first column (0 to 6) and to flatten the rest of values so that the end result looks like this: Could you please help me? Thanks in advance.", "q_apis": "values dtypes get sample now get first values", "io": " 0 0 380.143752 1 379.942595 2 379.589472 3 379.816187 4 379.622086 5 379.299071 6 379.559615 <s> 380.143752 379.942595 379.589472 379.816187 379.622086 379.299071 379.559615 ", "apis": "to_csv mode index any", "code": ["newdf.to_csv(file,mode='w', sep=',',header=False,index=False) # you can use any separator here 'tabs' if you don't want commas.\n"], "link": "https://stackoverflow.com/questions/53592186/modify-and-flatten-values-from-pandas-dataframe"}
{"id": 581, "q": "Split pandas dataframe into multiple dataframes based on null columns", "d": "I have a pandas dataframe as follows: Is there a simple way to split the dataframe into multiple dataframes based on non-null values?", "q_apis": "columns values", "io": " a b c 0 1.0 NaN NaN 1 NaN 7.0 5.0 2 3.0 8.0 3.0 3 4.0 9.0 2.0 4 5.0 0.0 NaN <s> a 0 1.0 b c 1 7.0 5.0 a b c 2 3.0 8.0 3.0 3 4.0 9.0 2.0 a b 4 5.0 0.0 ", "apis": "dropna groupby isnull dot columns", "code": ["d = {y : x.dropna(1) for y, x in df.groupby(df.isnull().dot(df.columns))}\n"], "link": "https://stackoverflow.com/questions/52502179/split-pandas-dataframe-into-multiple-dataframes-based-on-null-columns"}
{"id": 434, "q": "pandas Integers to negative integer powers are not allowed", "d": "I have a , I want to create a new column based on the following calculation: but I got the following error, I am wondering how to get around this, so the result will look like,", "q_apis": "get", "io": "decimal_places amount 2 10 3 100 1 1000 <s> decimal_places amount converted_amount 2 10 10 3 100 10 1 1000 10000 ", "apis": "astype assign fillna", "code": ["df = df.astype(float)\ndf = (df.assign(\n      converted_amount=lambda x: x.amount * 10 ** (2 - x.decimal_places.fillna(2))))\n"], "link": "https://stackoverflow.com/questions/57726939/pandas-integers-to-negative-integer-powers-are-not-allowed"}
{"id": 229, "q": "How to convert a pandas dataframe column from string to an array of floats?", "d": "I have a dataframe where a column is an array of floats. When I am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows: I want to convert this long character string into an array of floats like this: Is there a way to do that?", "q_apis": "array where array array", "io": "'[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0]', '[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0]',.... <s> [4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0], [13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0],... ", "apis": "apply", "code": ["from ast import literal_eval    \ndf[\"a\"] = df[\"a\"].apply(lambda x: literal_eval(x))\n"], "link": "https://stackoverflow.com/questions/63479071/how-to-convert-a-pandas-dataframe-column-from-string-to-an-array-of-floats"}
{"id": 442, "q": "Pandas concatenate levels in multiindex", "d": "I do have following excel file: I would like to create following dataframe: What I tried: The new dataframe: This approach works but is kind of tedious: Which gives me: Is there a simpler solution available ?", "q_apis": "levels", "io": "{0: {0: nan, 1: nan, 2: nan, 3: 'A', 4: 'A', 5: 'B', 6: 'B', 7: 'C', 8: 'C'}, 1: {0: nan, 1: nan, 2: nan, 3: 1.0, 4: 2.0, 5: 1.0, 6: 2.0, 7: 1.0, 8: 2.0}, 2: {0: 'AA1', 1: 'a', 2: 'ng/mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 3: {0: 'AA2', 1: 'a', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 4: {0: 'BB1', 1: 'b', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 5: {0: 'BB2', 1: 'b', 2: 'mL', 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 6: {0: 'CC1', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}, 7: {0: 'CC2', 1: 'c', 2: nan, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}} <s> AA1 AA2 CB1 BB2 CC1 CC2 a a b b c c ng/mL N/A N/A mL N/A N/A 0 1 A 1 1 1 1 1 1 1 2 1 1 1 1 1 1 B 1 1 1 1 1 1 1 2 1 1 1 1 1 1 C 1 1 1 1 1 1 1 2 1 1 1 1 1 1 ", "apis": "read_excel set_index columns rename_axis read_excel set_index columns rename_axis columns get_level_values columns get_level_values columns get_level_values where columns to_series", "code": ["df = pd.read_excel('file.xlsx', header=[0,1,2])\n\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n", "df = pd.read_excel('file.xlsx', header=[0,1,2])\ndf = df.set_index(df.columns[:2].tolist()).rename_axis((None, None))\n\nlv1 = df.columns.get_level_values(0)\nlv2 = df.columns.get_level_values(1)\nlv3 = df.columns.get_level_values(2)\nlv3 = lv3.where(~lv3.str.startswith('Unnamed'),'N/A')\n\ndf.columns = [lv1, lv2.to_series() + ' ' + lv3]\n"], "link": "https://stackoverflow.com/questions/57411679/pandas-concatenate-levels-in-multiindex"}
{"id": 81, "q": "Repeating single DataFrame with changing DateTimeIndex", "d": "Let's say I have very simple DataFrame like this: Output: I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this: It's still the same DataFrame, repeating again and again, and year is incrementally changed. I could do something like this (example for 3 years): I have mainly two questions: Is there a way how to do this in a single command? What is the best way how to deal with leap-year?", "q_apis": "DataFrame DataFrame take DataFrame append DataFrame year index DataFrame year year", "io": " A B C D 2010-01-31 6 0 8 10 2010-02-28 7 8 10 3 2010-03-31 10 5 8 10 2010-04-30 4 4 9 7 2010-05-31 2 3 0 11 2010-06-30 8 7 10 8 2010-07-31 11 9 0 4 2010-08-31 0 3 8 6 2010-09-30 4 6 7 9 2010-10-31 1 0 11 9 2010-11-30 5 4 8 4 2010-12-31 1 4 5 1 <s> A B C D 2010-01-31 6 0 8 10 2010-02-28 7 8 10 3 2010-03-31 10 5 8 10 2010-04-30 4 4 9 7 2010-05-31 2 3 0 11 2010-06-30 8 7 10 8 2010-07-31 11 9 0 4 2010-08-31 0 3 8 6 2010-09-30 4 6 7 9 2010-10-31 1 0 11 9 2010-11-30 5 4 8 4 2010-12-31 1 4 5 1 2011-01-31 6 0 8 10 2011-02-28 7 8 10 3 2011-03-31 10 5 8 10 2011-04-30 4 4 9 7 2011-05-31 2 3 0 11 2011-06-30 8 7 10 8 2011-07-31 11 9 0 4 2011-08-31 0 3 8 6 2011-09-30 4 6 7 9 2011-10-31 1 0 11 9 2011-11-30 5 4 8 4 2011-12-31 1 4 5 1 2012-01-31 6 0 8 10 2012-02-28 7 8 10 3 2012-03-31 10 5 8 10 2012-04-30 4 4 9 7 2012-05-31 2 3 0 11 2012-06-30 8 7 10 8 2012-07-31 11 9 0 4 2012-08-31 0 3 8 6 2012-09-30 4 6 7 9 2012-10-31 1 0 11 9 2012-11-30 5 4 8 4 2012-12-31 1 4 5 1 ", "apis": "concat set_index index year", "code": ["data_new = pd.concat([\n    df.set_index(df.index + pd.DateOffset(year=x)) for x in [2010, 2011, 2012]])\n"], "link": "https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex"}
{"id": 394, "q": "How to find the correlation between a group of values in a pandas dataframe column", "d": "I have a dataframe df: I want to find the pearson correlation coefficient value between and for every So the result should look like this: update: Must make sure all columns of variables are or", "q_apis": "between values value between update all columns", "io": "ID Var1 Var2 1 1.2 4 1 2.1 6 1 3.0 7 2 1.3 8 2 2.1 9 2 3.2 13 <s> ID Corr_Coef 1 0.98198 2 0.97073 ", "apis": "groupby corr eq reset_index drop rename reset_index", "code": ["df_out = df.groupby('ID').corr()\n(df_out[~df_out['Var1'].eq(1)]\n          .reset_index(1, drop=True)['Var1']\n          .rename('Corr_Coef')\n          .reset_index())\n"], "link": "https://stackoverflow.com/questions/45064916/how-to-find-the-correlation-between-a-group-of-values-in-a-pandas-dataframe-colu"}
{"id": 659, "q": "Python Dataframe get the NaN columns for each row", "d": "I have a pandas dataframe which look like the following: I would like to add a column that gives me something like a summary of Null values. So I need a command which gives me for every row which columns are NULL. Something like this: I could not find anything which satisfies my need on the internet.", "q_apis": "get columns add values columns", "io": " a b c NaN 2 165 NaN 9 NaN NaN NaN NaN 15 15 NaN 5 NaN 11 <s> a b c Summary NaN 2 165 a NaN 9 NaN a + c NaN NaN NaN a + b + c 15 15 NaN c 5 NaN 11 b ", "apis": "DataFrame columns apply join", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[np.nan, 2, 165], [np.nan, 9, np.nan], [np.nan, np.nan, np.nan],\n                   [15, 15, np.nan], [5, np.nan, 11]], columns=['a', 'b', 'c'])\n\ndf['Errors'] = df.apply(lambda row: ' + '.join(i for i in ['a', 'b', 'c'] if np.isnan(row[i])), axis=1)\n"], "link": "https://stackoverflow.com/questions/48563186/python-dataframe-get-the-nan-columns-for-each-row"}
{"id": 585, "q": "How to get the top frequency elements after grouping by columns?", "d": "I have a DataFrame named , and I want to count the top frequency elements in column , and on different . As you see, the of both and is . For , appears the most in column , and , appears the second most. So for and , the most frequency element is , and the second most is .", "q_apis": "get columns DataFrame count second second", "io": "df id app_0 app_1 app_2 sex 0 1 a b c 0 1 2 b c b 0 2 3 c d a 1 3 4 d NaN a 1 <s> df id app_0 app_1 app_2 sex top_1 top_2 0 1 a b c 0 b c 1 2 b c b 0 b c 2 3 c d a 1 a d 3 4 d NaN a 1 a d ", "apis": "stack value_counts Series index index index", "code": ["def f(x):\n    s = x.stack().value_counts()\n    return pd.Series([s.index[0], s.index[1]], index=['top_1','top_2'])\n"], "link": "https://stackoverflow.com/questions/52439222/how-to-get-the-top-frequency-elements-after-grouping-by-columns"}
{"id": 71, "q": "add a string prefix to each value in a string column using Pandas", "d": "I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly). I already figured out how to kind-of do this and I am currently using: This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)? In case this is yet unclear, I would like to turn: into:", "q_apis": "add value append start value any where", "io": " col 1 a 2 0 <s> col 1 stra 2 str0 ", "apis": "astype", "code": ["df['col'] = 'str' + df['col'].astype(str)\n"], "link": "https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas"}
{"id": 589, "q": "Rearranging python data frame index and columns", "d": "I want to convert this dataframe (note that 'ABC' is the index name): to this dataframe: What's the best way to perform this function?", "q_apis": "index columns index name", "io": " t1 t2 t3 ABC gp 7 11 26 fp 6 14 23 pm 3 -1 7 wm 2 -2 9 <s> s1 tx gp fp pm wm 0 ABC t1 7 6 3 2 1 ABC t2 11 14 -1 -2 2 ABC t3 26 23 7 9 ", "apis": "T reset_index rename columns index", "code": ["df = df.T\ndf.reset_index(inplace=True)\ndf.rename(columns={\"index\":\"tx\"}, inplace=True)\ndf[\"s1\"] = \"ABC\"\n"], "link": "https://stackoverflow.com/questions/52155125/rearranging-python-data-frame-index-and-columns"}
{"id": 323, "q": "Pandas: Reshaping dataframe", "d": "I have a panda's related question. My dataframe looks something like this: I want to transform it into something like: I thought of something like adding a sub_id column that is enumerated cyclically by a, b and c and then do an unstack of the frame. Is there an easier/smarter solution? Thanks a lot! Tim", "q_apis": "transform unstack", "io": " id val1 val2 0 1 0 1 1 1 1 0 2 1 0 0 3 2 1 1 4 2 1 1 5 2 1 0 6 3 0 0 7 3 0 1 8 3 1 1 9 4 1 0 10 4 0 1 11 4 0 0 <s> a b c id a0 a1 b0 b1 c0 c1 1 0 1 1 0 0 0 2 1 1 1 1 1 0 3 0 0 1 1 1 1 4 1 0 0 1 0 0 ", "apis": "set_index groupby apply Series values columns MultiIndex from_tuples", "code": ["res = df.set_index('id').groupby('id').apply(\n    lambda grp: pd.Series(grp.values.flatten()))\n", "res.columns = pd.MultiIndex.from_tuples(\n    [(x, x + y) for x in list('abc') for y in list('01')])\n"], "link": "https://stackoverflow.com/questions/60968943/pandas-reshaping-dataframe"}
{"id": 208, "q": "data frame to file.txt python", "d": "I have this dataframe I want to save it as a text file with this format I tried this code but is not working:", "q_apis": "", "io": " X Y Z Value 0 18 55 1 70 1 18 55 2 67 2 18 57 2 75 3 18 58 1 35 4 19 54 2 70 <s> X Y Z Value 18 55 1 70 18 55 2 67 18 57 2 75 18 58 1 35 19 54 2 70 ", "apis": "values to_csv index mode", "code": ["np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter=\"\\t\", header=\"X\\tY\\tZ\\tValue\")  \n", "a.to_csv('xgboost.txt', header=True, index=False, sep='\\t', mode='a')\n"], "link": "https://stackoverflow.com/questions/41428539/data-frame-to-file-txt-python"}
{"id": 630, "q": "Merge dataframes including extreme values", "d": "I have 2 data frames, df1 and df2: I want to merge dataframes but at the same time including the first and/or last value of the set in column A. This is an example of the desired outcome: I'm trying to use but that only slice the portion of data frames that coincides. Someone have an idea to deal with this? thanks!", "q_apis": "values merge at time first last value", "io": "df1 Out[66]: A B 0 1 11 1 1 2 2 1 32 3 1 42 4 1 54 5 1 66 6 2 16 7 2 23 8 3 13 9 3 24 10 3 35 11 3 46 12 3 51 13 4 12 14 4 28 15 4 39 16 4 49 df2 Out[80]: B 0 32 1 42 2 13 3 24 4 35 5 39 6 49 <s> df3 Out[93]: A B 0 1 2 1 1 32 2 1 42 3 1 54 4 3 13 5 3 24 6 3 35 7 3 46 8 4 28 9 4 39 10 4 49 ", "apis": "merge left eval groupby apply rolling max astype bool", "code": ["df[df.merge(df2, on='B', how='left', indicator='Ind').eval('Found=Ind == \"both\"')\n     .groupby('A')['Found']\n     .apply(lambda x: x.rolling(3, center=True, min_periods=2).max()).astype(bool)]\n"], "link": "https://stackoverflow.com/questions/50592595/merge-dataframes-including-extreme-values"}
{"id": 113, "q": "How can I use split() in a string when broadcasting a dataframe&#39;s column?", "d": "Take the following dataframe: Result: I need to create a 3rd column (broadcasting), using a condition on , and splitting the string on . This is ok to do: Result: But I need to specify dynamic indexes to split the string on , instead of (5, 8). When I try to run the following code it does not work, because is treated as a : I'm spending a huge time trying to solve this without needing to iterate the dataframe.", "q_apis": "time", "io": " col_1 col_2 0 0 here 123 1 1 here 456 <s> col_1 col_2 col_3 0 0 here 123 NaN 1 1 here 456 456 ", "apis": "values", "code": ["df['col_3']=[y.split(' ')[1] if x==1 else float('nan') for x,y in df[['col_1','col_2']].values]\n"], "link": "https://stackoverflow.com/questions/65893903/how-can-i-use-split-in-a-string-when-broadcasting-a-dataframes-column"}
{"id": 415, "q": "using a dictionary to modify the dfs values", "d": "I have a df like this: Then I have a dictionary with some keys (which correspond to the index names of the df) and values (column names): I would like to use the dictionary to check that those column names that do not appear in the dict values , are set to zero to generate this output: How could I use the dictionary to generate the desired output?", "q_apis": "values keys index names values names names values", "io": " xx yy zz A 6 5 2 B 4 4 5 B 5 6 7 C 6 6 6 C 7 7 7 <s> xx yy zz A 6 0 0 B 0 4 5 B 0 6 7 C 6 0 6 C 7 0 7 ", "apis": "mask DataFrame values index keys stack reset_index drop get_dummies groupby sum astype bool", "code": ["mask = (pd.DataFrame(d.values(), index=d.keys())\n          .stack()\n          .reset_index(level=1, drop=True)\n          .str.get_dummies()\n          .groupby(level=0).sum()\n          .astype(bool)\n        )\n"], "link": "https://stackoverflow.com/questions/58402008/using-a-dictionary-to-modify-the-dfs-values"}
{"id": 404, "q": "How can I remove columns of pandas dataframe conditional on last row values?", "d": "Given a data-frame like: I would like to remove the columns in which the value of the last row is less than (<) a constant X, say X = 25. In this example It would remove the column B only and the output would be: Thanks", "q_apis": "columns last values columns value last", "io": " A B C 2019-11-02 120 25 11 2019-11-03 119 28 15 2019-11-04 115 23 18 2019-11-05 119 30 20 2019-11-06 121 32 25 2019-11-07 117 24 30 <s> A C 2019-11-02 120 11 2019-11-03 119 15 2019-11-04 115 18 2019-11-05 119 20 2019-11-06 121 25 2019-11-07 117 30 ", "apis": "drop columns columns iloc lt select last iloc dtype columns value iloc lt dtype bool select columns iloc lt Index dtype", "code": ["df = df.drop(columns = df.columns[df.iloc[-1].lt(25)])\n", "# select last row\n\ndf.iloc[-1]\n\nA    117\nB     24\nC     30\nName: 2019-11-07, dtype: int64\n", "# check which columns have value < 25:\n\ndf.iloc[-1].lt(25)\n\nA    False\nB     True\nC    False\nName: 2019-11-07, dtype: bool\n", "# select those column(s) with boolean indexing:\n\ndf.columns[df.iloc[-1].lt(25)]\n\nIndex(['B'], dtype='object')\n"], "link": "https://stackoverflow.com/questions/58758098/how-can-i-remove-columns-of-pandas-dataframe-conditional-on-last-row-values"}
{"id": 255, "q": "Dask equivalent to pandas.DataFrame.update", "d": "I have a few functions that are using method, and I'm trying to move into using instead for the datasets, but the Dask Pandas API doesn't have the method implemented. Is there an alternative way to get the same result in ? Here are the methods I have using : Forward fills data with last known value input output Replaces values in a dataframe with values from another dataframe based on an id/index column input df1 df2 output", "q_apis": "DataFrame update get last value values values index", "io": "id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 .... 1 10 20 0 40 0 50 2 10 30 30 0 0 50 . . <s> id .. .. ..(some cols) 1/1/20 1/2/20 1/3/20 1/4/20 1/5/20 1/6/20 .... 1 10 20 20 40 40 50 2 10 30 30 30 30 50 . . ", "apis": "name name set_index to_dict copy map name name rename columns where notnull merge left drop name name drop rename columns merge left", "code": ["import pandas as pd\nimport dask.dataframe as dd\n\ndef replace_names(df1, # can be pandas or dask dataframe\n                  df2, # this should be pandas.\n                  idxCol='id',\n                  srcCol='name',\n                  dstCol='name'):\n    diz = df2[[idxCol, srcCol]].set_index(idxCol).to_dict()[srcCol]\n    out = df1.copy()\n    out[dstCol] = out[idxCol].map(diz)\n    return out\n", "def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    if srcCol == dstCol:\n        df2 = df2.rename(columns={srcCol:f\"{srcCol}_new\"})\n        srcCol = f\"{srcCol}_new\"\n    \n    def map_replace(x, srcCol, dstCol):\n        x[dstCol] = np.where(x[srcCol].notnull(),\n                             x[srcCol],\n                             x[dstCol])\n        return x\n    \n    df = dd.merge(df1, df2, on=idxCol, how=\"left\")\n    df = df.map_partitions(lambda x: map_replace(x, srcCol, dstCol))\n    df = df.drop(srcCol, axis=1)\n    return df\n\ndf = replace_names_dask(df1, df2)\n", "def replace_names_dask(df1, df2,\n                       idxCol='id',\n                       srcCol='name',\n                       dstCol='name'):\n    df1 = df1.drop(dstCol, axis=1)\n    df2 = df2.rename(columns={srcCol: dstCol})\n    df = dd.merge(df1, df2, on=idxCol, how=\"left\")\n    return df\n\ndf = replace_names_dask(df1, df2)\n"], "link": "https://stackoverflow.com/questions/62900970/dask-equivalent-to-pandas-dataframe-update"}
{"id": 583, "q": "Pandas Dataframe data are same or new?", "d": "In Python, Pandas dataframes are used : dataframe_1 : dataframe_2 : Here, dataframe_2 contains AB20, AB10 and AB17 same as dataframe_1 in random order. How to check which elements in dataframe_2 are new and which are same as dataframe_1 ???", "q_apis": "contains", "io": " id 0 AB17 1 AB18 2 AB19 3 AB20 4 AB10 <s> id 0 AB20 1 AB10 2 AB17 3 AB21 4 AB09 ", "apis": "mask isin mask dtype bool loc mask diff loc mask unique values loc mask unique diff loc mask unique diff", "code": ["mask = dataframe_2['id'].isin(dataframe_1['id'])\nprint (mask)\n0     True\n1     True\n2     True\n3    False\n4    False\nName: id, dtype: bool\n\nsame = dataframe_2.loc[mask, 'id'].tolist()\ndiff = dataframe_2.loc[~mask, 'id'].tolist()\n\n#if want unique values\n#same = dataframe_2.loc[mask, 'id'].unique().tolist()\n#diff = dataframe_2.loc[~mask, 'id'].unique().tolist()\n\nprint (same)\n['AB20', 'AB10', 'AB17']\n\nprint (diff)\n['AB21', 'AB09']\n"], "link": "https://stackoverflow.com/questions/52491327/pandas-dataframe-data-are-same-or-new"}
{"id": 46, "q": "Pandas Replace NaN with blank/empty string", "d": "I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so:", "q_apis": "empty values empty", "io": " 1 2 3 0 a NaN read 1 b l unread 2 c NaN read <s> 1 2 3 0 a \"\" read 1 b l unread 2 c \"\" read ", "apis": "replace", "code": ["import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"], "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string"}
{"id": 22, "q": "Splitting a dataframe into separate CSV files", "d": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header.", "q_apis": "value value", "io": "+---------+---------+ | Column1 | Column2 | +---------+---------+ | 1 | 93644 | | 2 | 63246 | | 3 | 47790 | | 3 | 39644 | | 3 | 32585 | | 1 | 19593 | | 1 | 12707 | | 2 | 53480 | +---------+---------+ <s> +---+-------+----------------+ | 1 | 19593 | NewColumnValue | | 1 | 93644 | NewColumnValue | | 1 | 12707 | NewColumnValue | +---+-------+----------------+ +---+-------+-----------------+ | 2 | 63246 | NewColumnValue | | 2 | 53480 | NewColumnValue | +---+-------+-----------------+ +---+-------+-----------------+ | 3 | 47790 | NewColumnValue | | 3 | 39644 | NewColumnValue | | 3 | 32585 | NewColumnValue | +---+-------+-----------------+ ", "apis": "groupby to_csv", "code": ["for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"], "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files"}
{"id": 203, "q": "Pandas : new column with index of unique values of another column", "d": "My dataframe: Expected new dataframe:", "q_apis": "index unique values", "io": "ID Name_Identify ColumnA ColumnB ColumnC 1 POM-OPP D43 D03 D59 2 MIAN-ERP D80 D74 E34 3 POM-OPP E97 B56 A01 4 POM-OPP A66 D04 C34 5 DONP28 B55 A42 A80 6 MIAN-ERP E97 D59 C34 <s> ID Name_Identify ColumnA ColumnB ColumnC NEW_ID 1 POM-OPP D43 D03 D59 1 2 MIAN-ERP D80 D74 E34 2 3 POM-OPP E97 B56 A01 1 4 POM-OPP A66 D04 C34 1 5 DONP28 B55 A42 A80 3 6 MIAN-ERP E97 D59 C34 2 ", "apis": "unique start map unique start map", "code": ["convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\ndf[\"NEW_ID\"] = df.Name_Identify.map(convert)\n", "In[24]: convert = {k: v for v, k in enumerate(df.Name_Identify.unique(), start=1)}\nIn[25]: convert\n", "In[26]: df[\"NEW_ID\"] = df.Name_Identify.map(convert)\nIn[27]: df\n"], "link": "https://stackoverflow.com/questions/64168703/pandas-new-column-with-index-of-unique-values-of-another-column"}
{"id": 450, "q": "Pandas list of tuples to MultiIndex", "d": "I have a that looks like this: I need to return a that looks like this: What is the best approach to this?", "q_apis": "MultiIndex", "io": " id t_l 0 100 [('a', 1), ('b', 2)] 1 151 [('x', 4), ('y', 3)] <s> id f g 0 100 'a' 1 1 'b' 2 2 151 'x' 4 3 'y' 3 ", "apis": "DataFrame index repeat columns reset_index", "code": ["from itertools import chain\npd.DataFrame(chain.from_iterable(df.t_l), index=np.repeat(df.id, df.t_l.str.len()), \\\n                                          columns=['f', 'g']).reset_index()\n"], "link": "https://stackoverflow.com/questions/57050068/pandas-list-of-tuples-to-multiindex"}
{"id": 500, "q": "Pandas filter rows based on condition, but always retain the first row", "d": "I would like to drop some rows that meets certain conditions but I do not want to drop the first row even if the first row meets that criteria. I tried dropping rows by using the df.drop function but it will erase the first row if the first row meets that condition. I do not want that. Data looks something like this: I want to do it in a way that if a row has a value of 3 in column2 then drop it. And I want the new data to be like this (after dropping but keeping the first one even though the first row had a value of 3 in column 2):", "q_apis": "filter first drop drop first first drop first first value drop first first value", "io": "Column1 Column2 Column3 1 3 A 2 1 B 3 3 C 4 1 D 5 1 E 6 3 F <s> Column1 Column2 Column3 1 3 A 2 1 B 4 1 D 5 1 E ", "apis": "index", "code": ["<ORIGINAL CONDITION> or <CONDITION TO KEEP FIRST ROW>", "(df['Column2'] == 3) & (df.index != 0)"], "link": "https://stackoverflow.com/questions/55388727/pandas-filter-rows-based-on-condition-but-always-retain-the-first-row"}
{"id": 393, "q": "Phyton: How to get the average of the n largest values for each column grouped by id", "d": "I'm trying to get the mean for each column while grouped by id. But I don't get it to work as I want to. The data: What I got so far: I got those two tries. But they are both just for one column and I don't know how to do it for more then just one.: What I want: Idealy, I would like to have a dataframe as follows: So that each row contains the mean values for the 100 biggest values for EACH column grouped by id.", "q_apis": "get values get mean get contains mean values values", "io": "ID Property3 Property2 Property3 1 10.2 ... ... 1 20.1 1 51.9 1 15.8 1 12.5 ... 1203 104.4 1203 11.5 1203 19.4 1203 23.1 <s> ID Property3 Property2 Property3 1 37.8 5.6 2.3 2 33.0 1.5 10.4 3 34.9 91.5 10.3 4 33.0 10.3 14.3 ", "apis": "groupby agg nlargest mean reset_index", "code": ["df = (data.groupby('ID')['Property1','Property2','Property3']\n          .agg(lambda grp: grp.nlargest(100).mean())\n          .reset_index())\n"], "link": "https://stackoverflow.com/questions/59067194/phyton-how-to-get-the-average-of-the-n-largest-values-for-each-column-grouped-b"}
{"id": 86, "q": "Changing Value of adjacent column based on value of of another column", "d": "I have following dataframe: I want to change value in column A1 to NaN whenever corresponding value in column A2 is No or NA. Same for B1. Note: NA here is a string objects not NaN.", "q_apis": "value value value", "io": " A1 A2 B1 B2 0 10 20 20 NA 1 20 40 30 No 2 50 No 50 10 3 40 NA 50 20 <s> A1 A2 B1 B2 0 10 20 NaN NA 1 20 40 NaN No 2 NaN No 50 10 3 NaN NA 50 20 ", "apis": "loc isna eq mask isna eq", "code": ["df.loc[df.A2.isna() | df.A2.eq('No'), 'A1'] = np.nan\n", "df['A1'] = df['A1'].mask(df.A2.isna() | df.A2.eq('No'))\n"], "link": "https://stackoverflow.com/questions/66383901/changing-value-of-adjacent-column-based-on-value-of-of-another-column"}
{"id": 647, "q": "Dynamically accessing subset of pandas dataf rame, perform calculation and write to new data frame", "d": "I have a very large data frame from which I would like to pull a subsample, perform some calculation and then write these results into a new data frame. For the sample, please consider: returning this: Now I would like \"extract\" always 3 rows, rolling from the beginning and calculate the averages (as an example, other calculations would work too) of each column: the result data frame is then How can I do that?", "q_apis": "sample rolling", "io": " a b c d e 0 1 9 0 3 0 1 5 4 1 0 3 2 9 3 6 3 5 3 6 2 5 9 7 4 9 0 7 9 5 <s> df_1 a b c d e 0 1 9 0 3 0 1 5 4 1 0 3 2 9 3 6 3 5 df_2 a b c d e 1 5 4 1 0 3 2 9 3 6 3 5 3 6 2 5 9 7 df_3 a b c d e 2 9 3 6 3 5 3 6 2 5 9 7 4 9 0 7 9 5 ", "apis": "rolling mean iloc", "code": ["N = 3\ndf = df.rolling(N).mean().iloc[N-1:]\n"], "link": "https://stackoverflow.com/questions/49715104/dynamically-accessing-subset-of-pandas-dataf-rame-perform-calculation-and-write"}
{"id": 364, "q": "Using np.split_array and then saving each split into dataframes", "d": "Appending data to a dataframe but changing rows after certain # of columns The above is my previous post, where I attempted to convert 1800 row x 1 column dataframe into 300 row x 6 column dataframe through: I would then would like to further split the dataframe into six chunks. I was thinking about using np split like: This line would be added right after (I know the lines won't work if split is applied). For example: The starting data table would look like: and so on (please note that the numbers are just random for this post, and for testing, you can use any floating numbers, these are essentially p-values). The rows are in groups of 50 rows and hence why I would like to separate the 300x6 df into 6 df of 50x6. Because of the data size, I wasn't able to insert all of it and had to express the table as above, but for the actual testing, you can probably generate random values with 300x6 shape df (not counting the headers). what I want is: and so on. I am not sure how I would iterate over each split from then save as separate dataframes. Any help or suggestions would be appreciated.", "q_apis": "columns where right any values groups size insert all values shape", "io": "col1 col2 col3 col4 col5 col6 1 0.658 0.1067 0.777 0.459 0.3307 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 . . . . 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 . . . <s> [df1] col1 col2 col3 col4 col5 col6 1 0.658 0.1067 0.777 0.459 0.3307 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 1 0.622 0.4178 0.3158 0.7674 0.7426 [df2] col1 col2 col3 col4 col5 col6 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 0.123 1 0.1222 0.111 0.123 0.1234 ", "apis": "DataFrame apply name groupby get_group set_index groups columns shape first head head head values array values last tail tail values array values", "code": ["import numpy as np\nimport pandas as pd\n\ndata = np.random.rand(300,6)\ndf = pd.DataFrame(data)\n\ndf[\"label\"] = df.apply(lambda x: x.name//50, axis=1)\ngb = df.groupby(\"label\")\ndf_list = [gb.get_group(x).set_index(\"label\") for x in gb.groups]\n", "for x in df_list: # each dataframe should have 50 rows and 6 columns\n    assert x.shape == (50, 6)\n", "# print first dataframe head (rows should be same as head printed above)\ndf_list[0].head(3) # and access the values/numpy array by df_list[0].values\n", "# print last section (rows should be same as tail printed above)\ndf_list[5].tail(3) # and access the values/numpy array by df_list[5].values\n"], "link": "https://stackoverflow.com/questions/59819257/using-np-split-array-and-then-saving-each-split-into-dataframes"}
{"id": 490, "q": "Predicting Values in Movie Recommendations", "d": "I've been trying to create a recommendation system using the movielens dataset in python. My goal is to determine the similarity between users and then output the top five recommended movies for each user in this format: The data I am using for now is this ratings dataset. Here is the code so far: I am trying to implement the prediction function. I want to predict the missing values and add them to c1. I am trying to implement this. The formula as well as an example of how it should be used is in the picture. As you can see it uses the similarity scores of the most similar users. The output of similarity looks like this: For example here is user1's similarity: I need help using these similarities in the prediction function to predict missing movie ratings. If that is solved I will then have to find the top 5 recommended movies for each user and output them in the format above. I currently need help with the prediction function. Any advice helps. Please let me know if you need any more information or clarification. Thank you for reading", "q_apis": "between now values add any", "io": "User-id1 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5 User-id2 movie-id1 movie-id2 movie-id3 movie-id4 movie-id5 <s> [(34, 0.19269904365720053) (196, 0.19187531680008307) (538, 0.14932027335788825) (67, 0.14093020024386654) (419, 0.11034407313683092) (319, 0.10055810007385564)] ", "apis": "index sum sum columns read_csv pivot columns index values DataFrame T fillna index columns columns columns fillna apply join mid mid fillna name nlargest iloc nlargest index apply join mid mid fillna name nlargest iloc nlargest index", "code": ["import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import scale\n\n\ndef predict(l):\n    # finds the userIds corresponding to the top 5 similarities\n    # calculate the prediction according to the formula\n    return (df[l.index] * l).sum(axis=1) / l.sum()\n\n\n# use userID as columns for convinience when interpretering the forumla\ndf = pd.read_csv('ratings.csv').pivot(columns='userId',\n                                                index='movieId',\n                                                values='rating')\n\nsimilarity = pd.DataFrame(cosine_similarity(\n    scale(df.T.fillna(-1000))),\n    index=df.columns,\n    columns=df.columns)\n# iterate each column (userID),\n# for each userID find the highest five similarities\n# and use to calculate the prediction for that user,\n# use fillna so that original ratings dont change\n\nres = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in col.fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\nprint(res)\n", "res = df.apply(lambda col: ' '.join('{}'.format(mid) for mid in (0 * col).fillna(\n    predict(similarity[col.name].nlargest(6).iloc[1:])).nlargest(5).index))\n"], "link": "https://stackoverflow.com/questions/48821092/predicting-values-in-movie-recommendations"}
{"id": 366, "q": "Save in DataFrame unique values for every column", "d": "If I have a data (df) like this: With the next fuction: It returns something like: \u00bfHow can I save the return of the fuction in a DataFrame?, I would like to see it like this: Thanks you !", "q_apis": "DataFrame unique values DataFrame", "io": "X1 X2 X3 A A C B A C C B C <s> X1 X2 X3 A A C B B C ", "apis": "apply Series unique fillna apply drop_duplicates reset_index drop fillna", "code": ["df1 = df.apply(lambda x: pd.Series(pd.unique(x))).fillna('')\n", "df1 = df.apply(lambda x: x.drop_duplicates().reset_index(drop=True)).fillna('')\n"], "link": "https://stackoverflow.com/questions/59767001/save-in-dataframe-unique-values-for-every-column"}
{"id": 582, "q": "Compare each of the column values and return final value based on conditions", "d": "I currently have a dataframe which looks like this: What I want to do is apply some condition to the column values and return the final result in a new column. The condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4] I tried to define a function to append the final results but wasnt really getting anywhere...any thoughts? The desired outcome would look something like: where col4 is the new column created. Thanks", "q_apis": "values value apply values assign values where first append any where", "io": "col1 col2 col3 1 2 3 2 3 NaN 3 4 NaN 2 NaN NaN 0 2 NaN <s> col1 col2 col3 col4 1 2 3 2 2 3 NaN 2 3 4 NaN 3 2 NaN NaN 2 0 2 NaN 2 ", "apis": "apply", "code": ["def func(x,l=[2,1,3,0,4,5]):\n    for j in l:\n      if(j in x):\n         return j\n\ndf['new'] = df.apply(lambda x: func(list(x)),axis =1)\n"], "link": "https://stackoverflow.com/questions/52492961/compare-each-of-the-column-values-and-return-final-value-based-on-conditions"}
{"id": 609, "q": "How to find which row items are appearing most in a pandas dataframe", "d": "I have a dataframe something like this : How to find which row is appearing the most number of times and unique items count? Here this is appearing most times in rows . I tried ,but it is giving me 100+ rules if my data is big. .NB : My real data is not and . This is mock data.", "q_apis": "items unique items count", "io": " a b c d e f ------------------------ 0 0 0 1 1 0 1 1 1 0 1 1 0 0 2 0 0 1 1 0 1 3 1 0 1 0 0 0 4 0 0 1 1 0 1 5 0 1 1 0 0 0 6 1 0 1 0 1 1 7 0 0 1 1 0 1 8 1 0 1 1 1 0 9 0 0 1 1 0 1 <s> 0 0 1 1 0 1", "apis": "groupby columns columns transform size index max Int64Index dtype", "code": ["s = df.groupby(df.columns.tolist())[df.columns[0]].transform('size')\nidx = s.index[s == s.max()]\nprint (idx)\nInt64Index([0, 2, 4, 7, 9], dtype='int64')\n"], "link": "https://stackoverflow.com/questions/51357253/how-to-find-which-row-items-are-appearing-most-in-a-pandas-dataframe"}
{"id": 132, "q": "Count the number of specific values in multiple columns pandas", "d": "I have a data frame: I want to count the number of times 'BUY' appears in each row. Intended result: I have tried the following but it simply gives 0 for all the rows: Note that BUY can only appear in B, C, D, E columns. I tried to find the solution online but shockingly found none. Little help will be appreciated. THANKS!", "q_apis": "values columns count all columns", "io": "A B C D E 12 4.5 6.1 BUY NaN 12 BUY BUY 5.6 NaN BUY 4.5 6.1 BUY NaN 12 4.5 6.1 0 NaN <s> A B C D E score 12 4.5 6.1 BUY NaN 1 12 BUY BUY 5.6 NaN 2 15 4.5 6.1 BUY NaN 1 12 4.5 6.1 0 NaN 0 ", "apis": "apply count", "code": ["df['score'] = df.apply(lambda x: x.tolist().count('BUY'), axis=1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/65550028/count-the-number-of-specific-values-in-multiple-columns-pandas"}
{"id": 124, "q": "How to concat two or more data frames with different columns names in pandas", "d": "I have hundreds csv files and I need join it to one file. I have it all load as pandas dataframes. Sample dataframes: I need this output: or How can I do that? Thanks EDIT: I have cca 500 csv files, this is my code to make one file from them:", "q_apis": "concat columns names join all", "io": " a x y z 0 e1 4 7 1 e1 5 8 2 e1 6 9 3 e2 13 16 100 4 e2 14 17 101 5 e2 15 18 102 <s> a x y z 0 e1 4 7 na 1 e1 5 8 na 2 e1 6 9 na 3 e2 13 16 100 4 e2 14 17 101 5 e2 15 18 102 ", "apis": "DataFrame DataFrame append hist DataFrame read_csv append to_csv index", "code": ["df1 = pd.DataFrame({'a':['e1','e1','e1'],'x':[4,5,6],'y':[7,8,9]})\ndf2 = pd.DataFrame({'a':['e2','e2','e2'],'x':[13,14,15],'y':[16,17,18], 'z':[100,101,102]})\nnewdf = df1.append(df2, ignore_index=True)\n", "import glob\nimport pandas as pd\n\npath = r'C:/Users/Miro/data hist'\nall_files = glob.glob(path + \"/*.csv\")\n\nli = pd.DataFrame()\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, sep='delimiter', header=None)\n    li = li.append(df, ignore_index=True)\n\n\nli.to_csv( \"full.csv\", index=False, encoding='utf-8-sig')\n"], "link": "https://stackoverflow.com/questions/65694203/how-to-concat-two-or-more-data-frames-with-different-columns-names-in-pandas"}
