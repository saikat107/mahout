{"id": 256, "q": "Extract data from certain columns and generate new rows", "d": "I have a pandas dataframe The output I want will be: Is there any convenient way I can use?", "q_apis": "columns any", "io": "COL1 COL2 COL3 COL4 A B C [{COL5: D1, COL6: E1, COL7: F1}, {COL5: D2, COL6: E2, COL7: F2}, {COL5: D3, COL6: E3, COL7: F3}, ... {COL5: D10, COL6: E10, COL7: F10}] <s> COL1 COL2 COL3 COL5 COL6 COL7 A B C D1 E1 F1 A B C D2 E2 F2 A B C D3 E3 F3 ... A B C D10 E10 F10 ", "apis": "explode reset_index drop iloc join DataFrame", "code": ["u = df.explode('COL4').reset_index(drop=True)\nu.iloc[:, :-1].join(pd.DataFrame(u['COL4'].tolist()))\n"], "link": "https://stackoverflow.com/questions/62923547/extract-data-from-certain-columns-and-generate-new-rows"}
{"id": 414, "q": "Pandas Adding Row with All Values Zero", "d": "If I have a following dataframe: How can i add a row end of the dataframe with all values \"0 (Zero)\"? Desired Output is; Could you please help me about this?", "q_apis": "add all values", "io": " A B C D E 1 1 2 0 1 0 2 0 0 0 1 -1 3 1 1 3 -5 2 4 -3 4 2 6 0 5 2 4 1 9 -1 6 1 2 2 4 1 <s> A B C D E 1 1 2 0 1 0 2 0 0 0 1 -1 3 1 1 3 -5 2 4 -3 4 2 6 0 5 2 4 1 9 -1 6 1 2 2 4 1 7 0 0 0 0 0 ", "apis": "append Series index columns", "code": ["df = df.append(pd.Series(0, index=df.columns), ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/58434018/pandas-adding-row-with-all-values-zero"}
{"id": 515, "q": "Convert the last non-zero value to 0 for each row in a pandas DataFrame", "d": "I'm trying to modify my data frame in a way that the last variable of a label encoded feature is converted to 0. For example, I have this data frame, top row being the labels and the first column as the index: Columns 1-10 are the ones that have been encoded. What I want to convert this data frame to, without changing anything else is: So the last values occurring in each row should be converted to 0. I was thinking of using the last_valid_index method, but that would take in the other remaining columns and change that as well, which I don't want. Any help is appreciated", "q_apis": "last value DataFrame last first index last values last_valid_index take columns", "io": "df 1 2 3 4 5 6 7 8 9 10 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 <s> 1 2 3 4 5 6 7 8 9 10 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ", "apis": "iloc columns iloc cumsum le fillna", "code": ["u = df.iloc[:, :10]\ndf[u.columns] = u[~u.iloc[:, ::-1].cumsum(1).le(1)].fillna(0, downcast='infer')\n"], "link": "https://stackoverflow.com/questions/54737891/convert-the-last-non-zero-value-to-0-for-each-row-in-a-pandas-dataframe"}
{"id": 573, "q": "Calculating grid values given the distance in python", "d": "I have a cell grid of big dimensions. Each cell has an ID (), cell value () and coordinates in actual measures (, ). This is how first 10 rows/cells look like Neighbouring cells of cell in the can be determined as (, , , , , ). For example: of 5 has neighbours - 4,6,504,505,506. (these are the ID of rows in the upper table - ). What I am trying to is: For the chosen value/row in , I would like to know all neighbours in the chosen distance from and sum all their values. I tried to apply this solution (link), but I don't know how to incorporate the distance parameter. The cell value can be taken with , but the steps before this are a bit tricky for me. Can you give me any advice? EDIT: Using the solution from Thomas and having df called : I'd like to add another column and use the values from columns But it doesn't work. If I add a number instead of a reference to row it works like charm. But how can I use values from p3 column automatically in function? SOLVED: It worked with:", "q_apis": "values value first value all sum all values apply value any add values columns add values", "io": " p1 p2 p3 X Y 0 0 0.0 0.0 0 0 1 1 0.0 0.0 100 0 2 2 0.0 12.0 200 0 3 3 0.0 0.0 300 0 4 4 0.0 70.0 400 0 5 5 0.0 40.0 500 0 6 6 0.0 20.0 600 0 7 7 0.0 0.0 700 0 8 8 0.0 0.0 800 0 9 9 0.0 0.0 900 0 <s> p3 0 45 1 580 2 12000 3 12531 4 22456 ", "apis": "DataFrame filter filter iloc sum sum", "code": ["import numpy as np\nimport pandas\n\n# Generating toy data\nN = 10\ndata = pandas.DataFrame({'p3': np.random.randn(N)})\nprint(data)\n\n# Finding neighbours\nget_candidates = lambda i: [i-500+1, i-500-1, i-1, i+1, i+500+1, i+500-1]\nfilter = lambda neighbors, N: [n for n in neighbors if 0<=n<N]\nget_neighbors = lambda i, N: filter(get_candidates(i), N)\n\nprint(\"Neighbors of 5: {}\".format(get_neighbors(5, len(data))))\n\n# Summing p3 on neighbors\ndef sum_neighbors(data, i, col='p3'):\n  return data.iloc[get_neighbors(i, len(data))][col].sum()\n\nprint(\"p3 sum on neighbors of 5: {}\".format(sum_neighbors(data, 5)))\n"], "link": "https://stackoverflow.com/questions/52873547/calculating-grid-values-given-the-distance-in-python"}
{"id": 340, "q": "Pandas - Groupby or Cut multi dataframes to bins", "d": "I'm having a dataframe with starting axis points and their end points like this I'm drawing a heatmap. I need each \"zone\" of that map has a line which shows the average distance and angle of lines which have x/y from that zone and their x_end/y_end. It looks like this My bins is I've already plotted a heatmap I'm looking for something like this", "q_apis": "map", "io": "x y x_end y_end distance 14.14 30.450 31.71 41.265 20.631750 -27.02 55.650 -33.60 63.000 9.865034 -19.25 70.665 -28.98 80.115 13.563753 16.45 59.115 9.94 41.895 18.409468 <s> xbins = np.linspace(-35, 35, 11) ybins = np.linspace(0, 105, 22) ", "apis": "drop groupby cut cut mean dropna all add_prefix", "code": ["(df.drop(['x','y'], axis=1)\n  .groupby([pd.cut(df.x, xbins),\n            pd.cut(df.y, ybins)],\n          )\n   .mean()\n   .dropna(how='all')\n   .add_prefix('Average_')\n)\n"], "link": "https://stackoverflow.com/questions/60492857/pandas-groupby-or-cut-multi-dataframes-to-bins"}
{"id": 143, "q": "Efficiently relocate elements conditionally in a panda.Dataframe", "d": "I am trying to sort the values of my data.frame in the following way: It is working, however it is very slow for my +40k rows. How can I do this more efficiently and more elegantly? I would prefer a solution that directly manipulates the original df, if possible. Example data: Desired output:", "q_apis": "values", "io": "x1 p1 x2 p2 1 0.4 2 0.6 2 0.2 1 0.8 <s> x1 p1 x2 p2 1 0.4 2 0.6 1 0.8 2 0.2 ", "apis": "loc loc values", "code": ["check = df[\"x1\"] > df[\"x2\"]\ndf.loc[check, [\"x2\", \"x1\", \"p2\", \"p1\"]] = df.loc[check, [\"x1\", \"x2\", \"p1\", \"p2\"]].values\n"], "link": "https://stackoverflow.com/questions/65222196/efficiently-relocate-elements-conditionally-in-a-panda-dataframe"}
{"id": 317, "q": "Issue in applying str.contains across multiple columns in Python", "d": "Dataframe: Desired output: What I have tried: (this doesn't delete all rows with alpha-numeric vales) I want to delete all alphanumeric rows, and have only the rows containing numbers alone. Col1 and Col2 has decimal points, but Col3 has only whole numbers. I have tried few other similar threads, but it didn't work. Thanks for the help!!", "q_apis": "contains columns delete all delete all", "io": "col1 col2 col3 132jh.2ad3 34.2 65 298.487 9879.87 1kjh8kjn0 98.47 79.8 90 8763.3 7hkj7kjb.k23l 67 69.3 3765.9 3510 <s> col1 col2 col3 98.47 79.8 90 69.3 3765.9 3510 ", "apis": "apply contains flags any loc apply contains flags any", "code": ["df[~df.apply(lambda row: row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n", "df[~df.loc[:, 'col1':'col3'].apply(lambda row:\n    row.str.contains(r'[A-Z]', flags=re.I).any(), axis=1)]\n"], "link": "https://stackoverflow.com/questions/61016701/issue-in-applying-str-contains-across-multiple-columns-in-python"}
{"id": 649, "q": "Python pandas: how to fast process the value in columns", "d": "Hi there is a dataframe like the following dataframes df1. The data type is string. I want to get the dataframe like the following dataframe. The eye data is divided into eye_x, eye_y, the other columns is the same, the data type is float. Until now I know how to get the (x, y) value together with the following code:", "q_apis": "value columns get columns now get value", "io": " eye_x eye_y nose_x nose_y mouse_x mouse_y ear_x ear_y 34 35 45 66 45 64 78 87 35 38 75 76 95 37 38 79 64 43 85 66 65 45 87 45 <s> eye nose mouse ear (34, 35) (45,66) (45,64) (78,87) (35, 38) (75,76) (95,37) (38,79) (64, 43) (85,66) (65,45) (87,45) ", "apis": "stack iloc columns unstack swaplevel columns columns map join", "code": ["v = df.stack().str.split('_', expand=True).iloc[:, :-1]\nv.columns = ['x', 'y']\n\nv = v.unstack().swaplevel(0, 1, axis=1)\nv.columns = v.columns.map('_'.join)\n"], "link": "https://stackoverflow.com/questions/49525337/python-pandas-how-to-fast-process-the-value-in-columns"}
{"id": 257, "q": "Pandas: Multiline data schema to single line", "d": "I have a big (huge) dataset that have the next schema: and for many reasons, one of them the to reduce the size, I want to transform it to the next schema: I tried grouping by ['dt', 'id'] and then iterating over each group to build the new rows but it is too slow. I'm not figuring out a way without iterating over every original row. Any idea?", "q_apis": "size transform", "io": "dt | id | val_t | val 1 | 1 | 1 | 123 1 | 1 | 2 | 145 1 | 1 | 3 | 234 1 | 2 | 1 | 234 1 | 2 | 2 | 433 1 | 2 | 3 | 453 .................. N | X | 1 | 652 N | X | 2 | 543 N | X | 3 | 324 <s> dt | id | val_1 | val_2 | val_3 1 | 1 | 123 | 145 | 234 1 | 2 | 234 | 433 | 453 .................. N | X | 652 | 543 | 324 ", "apis": "pop astype set_index unstack columns columns droplevel rename_axis columns reset_index", "code": ["df['temp'] = 'val_'\ndf['val_t'] = df.pop('temp') + df['val_t'].astype(str)\ndf = df.set_index(['dt', 'id', 'val_t']).unstack()\ndf.columns = df.columns.droplevel()\ndf = df.rename_axis(columns=None).reset_index()\n"], "link": "https://stackoverflow.com/questions/62164809/pandas-multiline-data-schema-to-single-line"}
{"id": 371, "q": "Pandas - split dataframe according to sorted sequence in columns", "d": "I have a pandas dataframe with this type of structure: Basically, I sort the data frame according to the values of val1 and val2 beforehand, so I know I'll have two ascending sequences afterwards. What I want is to split this df in two new dfs, according to the two sequences, which in my example would be this: I have checked this question and this, but I don't know the number of values/rows beforehand... I've also checked another question, so I thought about using split with a regular expression. But I only know the sequences will be ascending, there's no guarantee that the values will be continuous, so it doesn't work as expected. Is this possible to achieve? I appreciate in advance any help!", "q_apis": "columns values values values any", "io": "df Val1 Val2 Col1 Col2 1 1 0 3 1 2 2 4 2 1 2 3 3 2 2 5 1 2 3 4 2 1 3 1 3 4 2 1 <s> df1 Val1 Val2 Col1 Col2 1 1 0 3 1 2 2 4 2 1 2 3 3 2 2 5 df2 Val1 Val2 Col1 Col2 1 2 3 4 2 1 3 1 3 4 2 1 ", "apis": "shift groupby cumsum", "code": ["m = df['Val1'].shift() > df['Val1']\ndfs = [df for _, df in df.groupby(m.cumsum())]\n"], "link": "https://stackoverflow.com/questions/59645274/pandas-split-dataframe-according-to-sorted-sequence-in-columns"}
{"id": 521, "q": "Faster way to update a column in a pandas data frame based on the value of another column", "d": "I have a pandas data frame with columns = [A, B, C, D, ...I, Z]. There are around ~80000 rows in the dataframe, and columns A, B, C, D, ..., I have value 0 for all these rows. Z has a value between [0, 9]. What I am trying to do is update the value of the x'th column for all rows in the data frame, where x is the current value of Z. If value of x is 0, then ignore. The data frame looks like - This is what I have so far. This is way too slow, and causes the script to stop executing midway. Is there a faster or better way to do it? I tried looking at np.where and np.apply, but I am not able to figure out the syntax. This is what I tried using np.apply - The desired output for the above sample is -", "q_apis": "update value columns columns value all value between update value all where value value stop at where apply apply sample", "io": " A B C D ... Z 0 0 0 0 0 ... 9 1 0 0 0 0 ... 1 2 0 0 0 0 ... 2 3 0 0 0 0 ... 3 <s> A B C D ... Z 0 0 0 0 0 ... 9 1 0 1 0 0 ... 1 2 0 0 1 0 ... 2 3 0 0 0 1 ... 3 ", "apis": "array dtype array DataFrame concat get_dummies drop", "code": ["In [276]: cols[df['Z']]\nOut[276]: array(['temp', 'B', 'C', 'D', 'B', 'F', 'E'], dtype='<U3')\n", "import numpy as np\nimport pandas as pd\ncols = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'temp'])\ndf = pd.DataFrame({'Z':[9,1,2,3,1,5,4]})\n\ndf = pd.concat([pd.get_dummies(cols[df['Z']]), df['Z']], axis=1)\ndf = df.drop('temp', axis=1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/54640734/faster-way-to-update-a-column-in-a-pandas-data-frame-based-on-the-value-of-anoth"}
{"id": 354, "q": "Fill the values with each column combination with some default values in pandas data frame", "d": "I have a dataframe like this, Now CD is not present with col1 1908 and 1909 values, FR not present with 1908 and 1909 values and PR not present wth 1907. Now I want to create rows with col2 values which are not with all col1 values with col3 values as 0. So final dataframe will look like, I could do this using a for loop with every possible col2 values and comparing with every col1 group. But I am looking for shortcuts to do it most efficiently.", "q_apis": "values values values values values all values values values", "io": "df col1 col2 col3 1907 CD 49 1907 FR 33 1907 SA 34 1908 PR 1 1908 SA 37 1909 PR 16 1909 SA 38 <s> df col1 col2 col3 1907 CD 49 1907 FR 33 1907 SA 34 1907 PR 0 1908 CD 0 1908 FR 0 1908 PR 1 1908 SA 37 1908 CD 0 1908 FR 0 1909 PR 16 1909 SA 38 ", "apis": "MultiIndex from_product unique unique names set_index reindex reset_index", "code": ["mux = pd.MultiIndex.from_product([df['col1'].unique(), \n                                  df['col2'].unique()], names=['col1','col2'])\ndf = df.set_index(['col1','col2']).reindex(mux, fill_value=0).reset_index()\n"], "link": "https://stackoverflow.com/questions/60095370/fill-the-values-with-each-column-combination-with-some-default-values-in-pandas"}
{"id": 376, "q": "add column to groups on dataframe pandas", "d": "I have a dataframe: How can I add a new column to dataframe relative to the id column? for example:", "q_apis": "add groups add", "io": " id | x | y 1 | 0.3 | 0.4 1 | 0.2 | 0.5 2 | 0.1 | 0.6 2 | 0.9 | 0.1 3 | 0.8 | 0.2 3 | 0.7 | 0.3 <s> id | x | y | color 1 | 0.3 | 0.4 | 'green' 1 | 0.2 | 0.5 | 'green' 2 | 0.1 | 0.6 | 'black' 2 | 0.9 | 0.1 | 'black' 3 | 0.8 | 0.2 | 'red' 3 | 0.7 | 0.3 | 'red' ", "apis": "unique map", "code": ["d={x:random_color() for x in df.id.unique()}\ndf['color']=df['id'].map(d)\n"], "link": "https://stackoverflow.com/questions/59533480/add-column-to-groups-on-dataframe-pandas"}
{"id": 531, "q": "How to delete continuous four digits from a column value in pandas dataframe", "d": "I have a data frame like this: I want to remove where the digits occurring for exact four times The result will look like: What is the best way to do it using python", "q_apis": "delete value where", "io": "col1 col2 col3 A 12134 tea2014 2 B 2013 coffee 1 1 C green 2015 tea 4 <s> col1 col2 col3 A 12134 tea 2 B coffee 1 1 C green tea 4 ", "apis": "any", "code": ["(\n    ?<!   # negative lookbehind \n    \\d    # any single digit\n)\n\\d{4}     # match exactly 4 digits\n(\n    ?!    # negative lookahead\n    \\d\n)\n"], "link": "https://stackoverflow.com/questions/54020869/how-to-delete-continuous-four-digits-from-a-column-value-in-pandas-dataframe"}
{"id": 184, "q": "Pandas loop to numpy . Numpy count occurrences of string as nonzero in array", "d": "Suppose I have the following dataframe with element types in brackets When using pandas loops I use the following code. If : I am trying to use NumPy and array methods for efficiency. I have tried translating the method but no luck. Expected output", "q_apis": "count array array", "io": " Column1(int) Column2(str) Column3(str) 0 2 02 34 1 2 34 02 2 2 80 85 3 2 91 09 4 2 09 34 <s> Column1(int) Column2(str) Column3(str) Column4(int) 0 2 02 34 1 1 2 34 02 2 2 2 80 85 0 3 2 91 09 0 4 2 09 34 1 ", "apis": "map value_counts fillna where eq", "code": ["s = df['Column2'].map(df['Column3'].value_counts()).fillna(0)\ndf['Column4'] = np.where(df['Column1'].eq(2), s, 'F')\n"], "link": "https://stackoverflow.com/questions/64526205/pandas-loop-to-numpy-numpy-count-occurrences-of-string-as-nonzero-in-array"}
{"id": 653, "q": "Select consecultive times in a dataframe", "d": "I have a data frame where I would like to select the consecutive timestamp. I mean times that happen one after the other, in this case, that happened 15 minutes consecutively. For example, I would select only those from the above dataframe I have This is just an example but I am dealing with many timestamps. How can I do this with python commands?", "q_apis": "where select timestamp mean select", "io": "2017-07-19 17:45:00+02:00 16 2017-07-23 02:45:00+02:00 23 2017-07-25 14:15:00+02:00 23 2017-07-27 07:00:00+02:00 25 2017-07-28 09:30:00+02:00 22 2017-07-28 18:00:00+02:00 17 2017-07-29 04:00:00+02:00 28 2017-07-29 04:15:00+02:00 19 2017-07-29 11:30:00+02:00 20 2017-07-30 09:00:00+02:00 11 2017-08-03 02:45:00+02:00 22 2017-08-04 06:45:00+02:00 27 2017-08-06 01:45:00+02:00 21 2017-08-08 19:30:00+02:00 27 2017-08-08 19:45:00+02:00 27 2017-08-08 20:00:00+02:00 15 2017-08-08 21:45:00+02:00 25 <s> 2017-07-29 04:00:00+02:00 28 2017-07-29 04:15:00+02:00 19 2017-08-08 19:30:00+02:00 27 2017-08-08 19:45:00+02:00 27 2017-08-08 20:00:00+02:00 15 ", "apis": "time diff total_seconds eq time dtype bool", "code": ["In [205]: df.time.diff().dt.total_seconds().eq(900)\nOut[205]:\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7      True\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14     True\n15     True\n16    False\nName: time, dtype: bool\n"], "link": "https://stackoverflow.com/questions/49367519/select-consecultive-times-in-a-dataframe"}
{"id": 336, "q": "Transpose pandas dataframe", "d": "How do I convert a list of lists to a panda dataframe? it is not in the form of coloumns but instead in the form of rows. for example: I want it to be shown as rows and not coloumns. currently it shows somethign like this I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists? This is how I want the output to look like with other coloumns also filled in. However. won't help.", "q_apis": "all", "io": " B P F I FP BP 2 M 3 1 I L 0 64 73 76 64 61 32 36 94 81 49 94 48 1 57 58 69 46 34 66 15 24 20 49 25 98 2 99 61 73 69 21 33 78 31 16 11 77 71 3 41 1 55 34 97 64 98 9 42 77 95 41 4 36 50 54 27 74 0 8 59 27 54 6 90 5 74 72 75 30 62 42 90 26 13 49 74 9 6 41 92 11 38 24 48 34 74 50 10 42 9 7 77 9 77 63 23 5 50 66 49 5 66 98 8 90 66 97 16 39 55 38 4 33 52 64 5 9 18 14 62 87 54 38 29 10 66 18 15 86 10 60 89 57 28 18 68 11 29 94 34 37 59 11 78 67 93 18 14 28 64 11 77 79 94 66 <s> B P F I FP BP 2 M 3 1 I L 0 64 1 73 1 76 2 64 3 61 4 32 5 36 6 94 7 81 8 49 9 94 10 48 ", "apis": "DataFrame T columns", "code": ["import numpy\n\ndf = pandas.DataFrame(numpy.asarray(data[x]).T.tolist(),\n                      columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])\n"], "link": "https://stackoverflow.com/questions/24412510/transpose-pandas-dataframe"}
{"id": 514, "q": "Conditional mean and sum of previous N rows in pandas dataframe", "d": "Concerned is this exemplary pandas dataframe: Whenever is , I wish to calculate sum and mean of the last 3 (starting from current) valid measurements. Measurements are considered valid, if the column is . So let's clarify using the two examples in the above dataframe: : Indices should be used. Expected : Indices should be used. Expected I have tried and creating new, shifted columns, but was not successful. See the following excerpt from my tests (which should directly run): Any help or solution is greatly appreciated. Thanks and Cheers! EDIT: Clarification: This is the resulting dataframe I expect: EDIT2: Another clarification: I did indeed not miscalculate, but rather I did not make my intentions as clear as I could have. Here's another try using the same dataframe: Let's first look at the column: We find the first in index 3 (green rectangle). So index 3 is the point, where we start looking. There is no valid measurement at index 3 (Column is ; red rectangle). So, we start to go further back in time, until we have accumulated three lines, where is . This happens for indices 2,1 and 0. For these three indices, we calculate the sum and mean of the column (blue rectangle): SUM: 2.0 + 4.0 + 3.0 = 9.0 MEAN: (2.0 + 4.0 + 3.0) / 3 = 3.0 Now we start the next iteration of this little algorithm: Look again for the next in the column. We find it at index 7 (green rectangle). There is also a valid measuremnt at index 7, so we include it this time. For our calculation, we use indices 7,6 and 5 (green rectangle), and thus get: SUM: 1.0 + 2.0 + 3.0 = 6.0 MEAN: (1.0 + 2.0 + 3.0) / 3 = 2.0 I hope, this sheds more light on this little problem.", "q_apis": "mean sum sum mean last columns first at first index index where start at index start time where indices indices sum mean start at index at index time indices get", "io": "Sum = 9.0, Mean = 3.0 <s> Sum = 6.0, Mean = 2.0", "apis": "rolling mean rolling sum loc mean sum rolling agg mean sum loc mean sum loc mean sum shift mean sum", "code": ["df['RollM'] = df.Measurement.rolling(window=3,min_periods=0).mean()\n\ndf['RollS'] = df.Measurement.rolling(window=3,min_periods=0).sum()\n", "df.loc[df.Trigger == False,['RollS','RollM']] = np.nan\n", "   Measurement  Trigger  Valid     RollM  RollS\n0          2.0    False   True       NaN    NaN\n1          4.0    False   True       NaN    NaN\n2          3.0    False   True       NaN    NaN\n3          0.0     True  False  2.333333    7.0\n4        100.0    False   True       NaN    NaN\n5          3.0    False   True       NaN    NaN\n6          2.0    False   True       NaN    NaN\n7          1.0     True   True  2.000000    6.0\n", "df['mean'],df['sum'] = np.nan,np.nan\n\nroller = df.Measurement.rolling(window=3,min_periods=0).agg(['mean','sum'])\n\ndf.loc[(df.Trigger == True) & (df.Valid == True),['mean','sum']] = roller\n\ndf.loc[(df.Trigger == True) & (df.Valid == False),['mean','sum']] = roller.shift(1)\n", "  Measurement  Trigger  Valid  mean  sum\n0          2.0    False   True   NaN  NaN\n1          4.0    False   True   NaN  NaN\n2          3.0    False   True   NaN  NaN\n3          0.0     True  False   3.0  9.0\n4        100.0    False   True   NaN  NaN\n5          3.0    False   True   NaN  NaN\n6          2.0    False   True   NaN  NaN\n7          1.0     True   True   2.0  6.0\n"], "link": "https://stackoverflow.com/questions/46723310/conditional-mean-and-sum-of-previous-n-rows-in-pandas-dataframe"}
{"id": 470, "q": "How do you stack two Pandas Dataframe columns on top of each other?", "d": "Is there a library function or correct way of stacking two Pandas data frame columns on top of each other? For example make 4 columns into 2: to The documentation for Pandas Data Frames that I read for the most part only deal with concatenating rows and doing row manipulation, but I'm sure there has to be a way to do what I described and I am sure it's very simple. Any help would be great.", "q_apis": "stack columns columns columns", "io": "a1 b1 a2 b2 1 2 3 4 5 6 7 8 <s> c d 1 2 5 6 3 4 7 8 ", "apis": "DataFrame columns iloc iloc columns columns concat", "code": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.arange(1, 9).reshape((2, 4)),\n        columns=[\"a1\", \"b1\", \"a2\", \"b2\"])\n\npart1 = df.iloc[:,0:2]\npart2 = df.iloc[:,2:4]\n\nnew_columns = [\"c\", \"d\"]\npart1.columns = new_columns\npart2.columns = new_columns\n\nprint pd.concat([part1, part2], ignore_index=True)\n"], "link": "https://stackoverflow.com/questions/27513890/how-do-you-stack-two-pandas-dataframe-columns-on-top-of-each-other"}
{"id": 215, "q": "How to split dataframe at headers that are in a row", "d": "I've got a page I'm scraping and most of the tables are in the format Heading --info. I can iterate through most of the tables and create separate dataframes for all the various information using pandas.read_html. However, there are some where they've combined information into one table with subheadings that I want to be separate dataframes with the text of that row as the heading (appending a list). Is there an easy way to split this dataframe - It will always be heading followed by associated rows, new heading followed by new associated rows. eg. Should be It'd be nice if people would just create web pages that made sense with the data but that's not the case here. I've tried iterrows but cannot seem to come up with a good way to create what I want. Help would be very much appreciated!", "q_apis": "at info all read_html where iterrows", "io": " Col1 Col2 0 thing 1 1 2 2 2 3 3 thing2 4 1 2 5 2 3 6 3 4 <s> thing 1 1 1 2 2 2 thing2 4 1 2 5 2 3 6 3 4 ", "apis": "reset_index drop where applymap empty rename columns iloc drop index", "code": ["import numpy as np\n\n\nres = [x.reset_index(drop=True) for x in np.split(df, np.where(df.applymap(lambda x: x == ''))[0]) if not x.empty]\nfor x in res:\n    x = x.rename(columns=x.iloc[0]).drop(x.index[0])\n    print(x)\n"], "link": "https://stackoverflow.com/questions/63799400/how-to-split-dataframe-at-headers-that-are-in-a-row"}
{"id": 381, "q": "Why doesn&#39;t pandas reindex() operate in-place?", "d": "From the reindex docs: Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Therefore, I thought that I would get a reordered by setting in place (!). It appears, however, that I do get a copy and need to assign it to the original object again. I don't want to assign it back, if I can avoid it (the reason comes from this other question). This is what I am doing: Outs: Reindex gives me the correct output, but I'd need to assign it back to the original object, which is what I wanted to avoid by using : The desired output after that line is: Why is not working in place? Is it possible to do that at all? Working with python 3.5.3, pandas 0.23.3", "q_apis": "reindex reindex DataFrame index value index index copy get get copy assign assign assign at all", "io": " a b c d e 0 0.234296 0.011235 0.664617 0.983243 0.177639 1 0.378308 0.659315 0.949093 0.872945 0.383024 2 0.976728 0.419274 0.993282 0.668539 0.970228 3 0.322936 0.555642 0.862659 0.134570 0.675897 4 0.167638 0.578831 0.141339 0.232592 0.976057 <s> e d c b a 0 0.177639 0.983243 0.664617 0.011235 0.234296 1 0.383024 0.872945 0.949093 0.659315 0.378308 2 0.970228 0.668539 0.993282 0.419274 0.976728 3 0.675897 0.134570 0.862659 0.555642 0.322936 4 0.976057 0.232592 0.141339 0.578831 0.167638 ", "apis": "reindex reindex index copy reindex index copy", "code": ["df = df.reindex(['e', 'd', 'c', 'b', 'a'], axis=1)  \n", "id(df)\n# 4839372504\n\nid(df.reindex(df.index, copy=False)) # same object returned \n# 4839372504\n\nid(df.reindex(df.index, copy=True))  # new object created - ids are different\n# 4839371608  \n"], "link": "https://stackoverflow.com/questions/56462088/why-doesnt-pandas-reindex-operate-in-place"}
{"id": 185, "q": "How to create a new column based on matching ID&#39;s and string&#39;s in names of other columns in the same data frame?", "d": "I have tried to find a solution online but I cannot. I have a dataframe with 10 separate id columns, and 10 separate corresponding value columns for each ID. A brief example is shown below Example: I want to create a new column that takes the value column from the corresponding match of ID's between the 'shooter_id' and any of the 'player_id' columns like below: I have really been struggling to make this work, I am not sure if I need to merge within itself, for loop through the dataframe, or .apply.. any insight would be very helpful! Thank you!", "q_apis": "names columns columns value columns value between any columns merge apply any", "io": "player_id_1 player_1_x player_id_2 player_2_x shooter_id 300 10 301 12 301 299 11 300 13 299 <s> player_id_1 player_1_x player_id_2 player_2_x shooter_id shooter_x 300 10 301 12 301 12 299 11 300 13 299 11 ", "apis": "filter eq idxmax replace add lookup index", "code": ["c = df.filter(like='player_id')\\\n      .eq(df['shooter_id'], axis=0)\\\n      .idxmax(1).str.replace('_id', '').add('_x')\n\ndf['shooter_x'] = df.lookup(df.index, c)\n"], "link": "https://stackoverflow.com/questions/64524795/how-to-create-a-new-column-based-on-matching-ids-and-strings-in-names-of-other"}
{"id": 110, "q": "Conditional Row shift in Pandas", "d": "I'm attempting to shift a row based on whether or not another column is not null. There's inconsistent spacing in the Description column so I can't do a .shift() Here's the original data And this is what I want my result to be Here's the code that I used from Align data in one column with another row, based on the last time some condition was true However when I run it, no errors and no changes in the dataframe.", "q_apis": "shift shift shift last time", "io": "Permit Number A Description 1234 NaN NaN NaN NaN NaN NaN NaN foo 3456 NaN NaN NaN NaN bar <s> Permit Number A Description 1234 NaN foo NaN NaN NaN NaN NaN NaN 3456 NaN bar NaN NaN NaN ", "apis": "mask notnull notnull isnull assign groupby mask cumsum transform iloc where", "code": ["mask = df['Description'].notnull()\nfmask = (df['Permit Number'].notnull() & df['Description'].isnull())\ndf = df.assign(Description=df.groupby(mask[::-1].cumsum())['Description'].transform(lambda x: x.iloc[-1]).where(fmask))\n"], "link": "https://stackoverflow.com/questions/65834585/conditional-row-shift-in-pandas"}
{"id": 138, "q": "Pandas advanced problem : For each row, get complex info from another dataframe", "d": "Problem I have a dataframe : And I have another dataframe , with products like this : What I want is to add to a column, that will sum the Prices of the last products of each type known at the current date (with ). An example is the best way to explain : For the first row of The last A-Product known at this date bought by is this one : (since the 4th one is older, and the 5th one has a > ) The last B-Product known at this date bought by is this one : So the row in , after transformation, will look like that ( being , prices of the 2 products of interest) : The full after transformation will then be : As you can see, there are multiple possibilities : Buyers didn't necessary buy all products (see , who only bought A-products) Sometimes, no product is known at (see row 3 of the new , in 2015, we don't know any product bought by ) Sometimes, only one product is known at , and the value is the one of the product (see row 3 of the new , in 2015, we only have one product for , which is a B-product bought in 2014, whose price is ) What I did : I found a solution to this problem, but it's taking too much time to be used, since my dataframe is huge. For that, I iterate using iterrows on rows of , I then select the products linked to the Buyer, having on , then get the older grouping by and getting the max date, then I finally sum all my products prices. The fact I solve the problem iterating on each row (with a for iterrows), extracting for each row of a part of that I work on to finally get my sum, makes it really long. I'm almost sure there's a better way to solve the problem, with pandas functions ( for example), but I couldn't find the way. I've been searching a lot. Thanks in advance for your help Edit after Dani's answer Thanks a lot for your answer. It looks really good, I accepted it since you spent a lot of time on it. Execution is still pretty long, since I didn't specify something. In fact, are not shared through buyers : each buyers has its own multiple products types. The true way to see this is like this : As you can understand, product types are not shared through different buyers (in fact, it can happen, but in really rare situations that we won't consider here) The problem remains the same, since you want to sum prices, you'll add the prices of last occurences of johndoe-ID2 and johndoe-ID3 to have the same final result row But as you now understand, there are actually more than , so the step \"get unique product types\" from your answer, that looked pretty fast on the initial problem, actually takes a lot of time. Sorry for being unclear on this point, I didn't think of a possibility of creating a new df based on product types.", "q_apis": "get info add sum last at date first last at date last at date all product at any product product at value product product product time iterrows select get max date sum all iterrows get sum time product sum add last now step get unique product time product", "io": "3 A 2019-01-01 johndoe 600 <s> 7 B 2016-11-15 johndoe 300 ", "apis": "get unique product unique DataFrame explode merge date product merge merge_asof sort_values sort_values merge merge fillna sum date merge groupby sum rename columns get unique product unique DataFrame explode get unique product groupby apply DataFrame map explode merge date product merge merge_asof sort_values sort_values merge merge fillna sum date merge groupby sum rename columns", "code": ["# get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\nprint(res)\n", "# get unique product types\nproduct_types = list(df_prod['Product-Type'].unique())\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = [product_types for _ in range(len(df))]\ndf_with_prod = df.explode('Product-Type')\n", "# get unique product types\nproduct_types = df_prod.groupby('Buyer')['Product-Type'].apply(lambda x: list(set(x)))\n\n# create a new DataFrame with a row for each Product-Type for each Client_ID\ndf['Product-Type'] = df['Client_ID'].map(product_types)\ndf_with_prod = df.explode('Product-Type')\n\n# merge only the closest date by each client and product type\nmerge = pd.merge_asof(df_with_prod.sort_values(['Date', 'Client_ID']),\n                      df_prod.sort_values(['Product-Date', 'Buyer']),\n                      left_on='Date',\n                      right_on='Product-Date',\n                      left_by=['Client_ID', 'Product-Type'], right_by=['Buyer', 'Product-Type'])\n\n# fill na in prices\nmerge['Price'] = merge['Price'].fillna(0)\n\n# sum Price by client and date\nres = merge.groupby(['Client_ID', 'Date'], as_index=False)['Price'].sum().rename(columns={'Price' : 'LastProdSum'})\n\nprint(res)\n"], "link": "https://stackoverflow.com/questions/65392984/pandas-advanced-problem-for-each-row-get-complex-info-from-another-dataframe"}
{"id": 97, "q": "Dataframe - Pandas - How ploting same columns of two dataframe for visualising the differences", "d": "There are two dataframes and I would like to have a plot that shows the both price columns on X axis and sum on the Y axis to see how are the difference between these two dataframes. I tried the below but does nothing. What is the best way to show the differences between the two patterns of the price in these two dataframes? I thought of something like this, but if there is a better way to show the differences please mention it.", "q_apis": "columns plot columns sum difference between between", "io": "df1 +-----+-----+-------+ | | id | price | +-----+-----+-------+ | 1 | 1 | 5 | +-----+-----+-------+ | 2 | 2 | 12 | +-----+-----+-------+ | 3 | 3 | 34 | +-----+-----+-------+ | 4 | 4 | 62 | +-----+-----+-------+ | ... | ... | ... | +-----+-----+-------+ | 125 | 125 | 90 | +-----+-----+-------+ <s> df2 +-----+-----+-------+ | | id | price | +-----+-----+-------+ | 1 | 1 | 14 | +-----+-----+-------+ | 2 | 2 | 15 | +-----+-----+-------+ | 3 | 3 | 45 | +-----+-----+-------+ | 4 | 4 | 62 | +-----+-----+-------+ | ... | ... | ... | +-----+-----+-------+ | 125 | 125 | 31 | +-----+-----+-------+ ", "apis": "DataFrame round DataFrame round plot first plot second merge abs", "code": ["import pandas as pd \nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\n\ndf1 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\ndf2 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\n\nplt.plot(df1['col2'], label='first')\nplt.plot(df2['col2'], label='second')\nplt.legend()\n", "df3 = pd.merge(df1,df2, on='col1')\nsns.lineplot(x='col2_x', y='col2_y', data=df3)\n", "df3['dif'] = abs(df3['col2_x'] - df3['col2_y'])\n\nsns.lineplot(x='col1', y='dif', data=df3)\nplt.xticks(df3['col1'])\n"], "link": "https://stackoverflow.com/questions/66194236/dataframe-pandas-how-ploting-same-columns-of-two-dataframe-for-visualising-t"}
{"id": 410, "q": "Python Pandas Get Values According to If/Else", "d": "My input dataframe; I want to count whether any difference or not among \"Order\" and Need values with below code; I want to that something like this; If (WarehouseStock-StoreStock) >= Need: Else Desired Outputs are; Count 3 Could you please help me about this?", "q_apis": "count any difference values", "io": "Order Need WarehouseStock StoreStock 1 3 74 5 0 4 44 44 0 0 44 44 6 12 44 44 0 6 644 44 6 6 44 44 <s> Order Need WarehouseStock StoreStock 1 3 74 5 6 12 44 44 0 6 644 44 ", "apis": "where ge between between", "code": ["import pandas as pd\nimport numpy as np\n\ns = df['Need'] - df['Order']\nind = np.where((df['WarehouseStock'] - df['StoreStock']).ge(df['Need']), ~s.between(-1, 1), ~s.between(-5 , 5))\n"], "link": "https://stackoverflow.com/questions/58522174/python-pandas-get-values-according-to-if-else"}
{"id": 130, "q": "How to fill multiple list by 0&#39;s in Pandas data frame?", "d": "I have Pandas data frame and I am trying to add 0's in those lists where numbers are missing. In the below data frame, the max length of the list is 4 which is in the 3rd position. accordingly, I will add 0's to the remaining lists. Input: Output:", "q_apis": "add where max length add", "io": " Lists 0 [158, 202] 1 [609, 405] 2 [544, 20] 3 [90, 346, 130, 202] 4 [6] <s> Lists 0 [158, 202, 0, 0] 1 [609, 405, 0, 0] 2 [544, 20, 0, 0] 3 [90, 346, 130, 202] 4 [6, 0, 0, 0] ", "apis": "max apply apply", "code": ["maxlen = df['Lists'].str.len().max() #as suggested by Anky, better than an apply since vectorised\nf = lambda x: x + ([0] * (maxlen - len(x)))\n\ndf['Padded'] = df['Lists'].apply(f)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/65601285/how-to-fill-multiple-list-by-0s-in-pandas-data-frame"}
{"id": 302, "q": "selecting rows on pandas dataframe based on conditions", "d": "I have the following code: that creates a dataframe stored in variable 'df'. It consists of 2 columns (column1 and column2) filled with random 0s and 1s. This is the output I got when I ran the program (If you try to run it you won't get exactly the same result because of the randomint generation). I would like to create a filter on column2, showing only the clusters of data when there are three or more 1s in a row. The output would be something like this: I have left a space between the clusters for visual clarity, but the real output would not have the empty spaces in the dataframe. I would like to do it in the following way. Thank you", "q_apis": "columns get filter left between empty", "io": " column1 column2 0 0 1 1 1 0 2 0 1 3 1 1 4 0 1 5 1 1 6 0 1 7 1 1 8 1 0 9 0 1 10 0 0 11 1 1 12 1 1 13 0 1 14 0 0 15 0 1 16 1 1 17 1 1 18 0 1 19 1 0 20 0 0 21 1 0 22 0 1 23 1 0 24 1 1 25 0 0 26 1 1 27 1 0 28 0 1 29 1 0 <s> column1 column2 2 0 1 3 1 1 4 0 1 5 1 1 6 0 1 7 1 1 11 1 1 12 1 1 13 0 1 15 0 1 16 1 1 17 1 1 18 0 1 ", "apis": "ne shift cumsum groupby transform size ge eq loc eq loc mul map value_counts ge mean std groupby transform size ge mean std", "code": ["n = 3\nblocks = df['column2'].ne(df['column2'].shift()).cumsum()\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\nm2 = df['column2'].eq(1)\ndf_filtered = df.loc[m1 & m2]\n# Alternative without df['column2'].eq(1)\n#df_filtered = df.loc[m1.mul(df['column2'])]\nprint(df_filtered)\n", "%%timeit\nm1 = blocks.map(blocks.value_counts().ge(n))\n1.41 ms \u00b1 122 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n\n%%timeit\nm1 = (df.groupby(blocks)['column2']\n        .transform('size').ge(n))\n2.12 ms \u00b1 226 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n"], "link": "https://stackoverflow.com/questions/61545023/selecting-rows-on-pandas-dataframe-based-on-conditions"}
{"id": 123, "q": "Transform a pandas dataframe: need for a more efficient solution", "d": "I have a dataframe indexed by dates from a certain period. My columns are predictions about the value of a variable by the end of a given year. My original dataframe looks something like this: where NaN means that the prediction does not exist for that given year. Since I am working with 20+ years and most predictions are for the next 2-3 years, my real dataframe has 20+ columns mostly containing values. For instance, the column for the year 2005 has predictions made in 2003-2005, but in the range 2006-2020 it's all . I would like to transform my dataframe to something like this: where represents the prediction made for the . This way, I would have a dataframe with only 4 columns (Y_0, Y_1, Y_2, Y_3). I actually achieved this, but in what I think it is a very inefficient way: For a dataframe with only 1000 rows, this is taking almost 3 seconds to run. Can anyone think of a better solution?", "q_apis": "columns value year where year columns values year all transform where columns seconds", "io": " 2016 2017 2018 2016-01-01 0.0 1 NaN 2016-07-01 1.0 1 4.1 2017-01-01 NaN 5 3.0 2017-07-01 NaN 2 2.0 <s> Y_0 Y_1 Y_2 2016-01-01 0 1 NaN 2016-07-01 1 1 4.1 2017-01,01 5 3 NaN 2017-07-01 2 2 NaN ", "apis": "year to_datetime date year year pivot index date columns values dropna all add_prefix", "code": ["df['year'] = pd.to_datetime(df['date']).dt.year\ndf['dt'] = df['prediction_year'] - df['year']\ndf = df.pivot(index = 'date', columns='dt', values='prediction').dropna(axis = 1, how = 'all').add_prefix('Y_')\n"], "link": "https://stackoverflow.com/questions/65721916/transform-a-pandas-dataframe-need-for-a-more-efficient-solution"}
{"id": 651, "q": "Insert characters at multiple positions in a dataframe column", "d": "I have a table with a lot of rows, and i need to change all the first \" \" space for \"h\" the second space for \"m\" and add at last char \"s\" I have made this code, but for my use is getting a slow. With this code I change my data frame from: To This: Is there any function that i can do this? I tried using df.replace, but it replaces all \" \" in the table... thanks for now", "q_apis": "at all first second add at last any replace all now", "io": " DEC RA +26 01 09.559 11 50 10.4747 +26 01 10.770 11 50 10.2641 +26 01 11.980 11 50 10.0534 +26 01 13.191 11 50 09.8428 <s> DEC RA 0 +26d01m09.559s 11h50m10.4747s 1 +26d01m10.770s 11h50m10.2641s 2 +26d01m11.980s 11h50m10.0534s 3 +26d01m13.191s 11h50m09.8428s ", "apis": "", "code": ["def split_combine(v, letters=list('dms')):\n    v = v.str.split(expand=True)\n    return (\n       v[0] + letters[0] \n     + v[1] + letters[1] \n     + v[2] + letters[2]\n    )\n"], "link": "https://stackoverflow.com/questions/49499352/insert-characters-at-multiple-positions-in-a-dataframe-column"}
{"id": 263, "q": "Pandas: creating values in a column based on the previous value in that column", "d": "Quick example: Before: After So the formula here is . I started with adding a Value column where each cell is 0. I then have looked a shift() but that uses the value in the previous row from the start of the command/function. So it will always use 0 as the value for Value. Is there a way of doing this without using something like iterrows() or a for loop ?", "q_apis": "values value where shift value start value iterrows", "io": "In Out 1 5 10 0 2 3 <s> In Out Value 1 5 -4 10 0 6 2 3 5 ", "apis": "cumsum", "code": ["df['Value'] = (df['In'] - df['Out']).cumsum()\n"], "link": "https://stackoverflow.com/questions/62862213/pandas-creating-values-in-a-column-based-on-the-previous-value-in-that-column"}
{"id": 76, "q": "Copy the last seen non empty value of a column based on a condition in most efficient way in Pandas/Python", "d": "I need to copy and paste the previos non-empty value of a column based on a condition. I need to do it in the most efficient way because the number of rows is a couple of millions. Using for loop will be computationally costly. So it will be highly appreciated if somebody can help me in this regard. Based on the condition, whenever the Col_A will have any value (not null) 10.2.6.1 in this example, the last seen value in Col_B (51,61 respectively) will be paste on that corresponding row where the Col_A value is not null. And the dataset should look like this: I tried with this code below but it's not working:", "q_apis": "last empty value copy empty value any value last value where value", "io": "|Col_A |Col_B | |--------|--------| |10.2.6.1| NaN | | NaN | 51 | | NaN | NaN | |10.2.6.1| NaN | | NaN | 64 | | NaN | NaN | | NaN | NaN | |10.2.6.1| NaN | <s> |Col_A |Col_B | |--------|--------| |10.2.6.1| NaN | | NaN | 51 | | NaN | NaN | |10.2.6.1| 51 | | NaN | 64 | | NaN | NaN | | NaN | NaN | |10.2.6.1| 64 | ", "apis": "loc notnull loc notnull drop columns ffill dropna", "code": ["df.loc[df['Col_A'].notnull(), 'Col_B'] = df.loc[df['Col_A'].notnull(), 'Col_C']\ndf = df.drop(columns=['Col_C'])\n", "df['Col_B'] = df['Col_B'].ffill()\ndf = df.dropna()\n"], "link": "https://stackoverflow.com/questions/66523605/copy-the-last-seen-non-empty-value-of-a-column-based-on-a-condition-in-most-effi"}
{"id": 136, "q": "why having std for 1 column and others are nan?", "d": "i have DataFrame looks something like this but with shape (345,5) like this and i want to get the std for the numeric columns ONLY with my manually std function and save in dictionary, the probelm is i am getting this result for the first column only: and here is my code:", "q_apis": "std DataFrame shape get std columns std first", "io": "|something1| something2| numbers1| number2 |number3| |----------|------------|----------|---------|-------| | A | str | 45 | nan |nan | |B | str2 | 6 | nan | nan | | c | str3 | 34 | 67 | 45 | |D | str4 | 56 | 45 | 23 | <s> {'number1': 18.59267328815305, 'number2': nan, 'number3': nan, 'number4': nan} ", "apis": "std dropna values columns", "code": ["std = {column:std_func(df[column].dropna().values) for column in df.columns}\n"], "link": "https://stackoverflow.com/questions/65451097/why-having-std-for-1-column-and-others-are-nan"}
{"id": 177, "q": "Calculate similarity between rows of a dataframe (count values in common)", "d": "I want to calculate similarity between the rows of my dataframe. I have some columns with informations about some people. One row is one person. It looks like that : I want to count for each row the number of values in common with the other rows divided by the number of columns if at least 3 columns are completed. For example, between the row with the index 1 and the row with the index 2, there are 4 variables in common. So, my similarity will be 4/5 (id doesn't count) = 80% of similarity. My result has to be a similarity matrix, because after that I want to find the rows with a similarity higher than 0.6 to build a new dataframe. It could be something like that : Because the results are duplicated, half of that would be enough : I'm looking for a function that will automate that but I couldn't find. Does something like that exist? Thanks for reading, any advice or idea will be welcomed.", "q_apis": "between count values between columns count values columns at columns between index index count duplicated any", "io": " print(similarity) 0 1 2 3 4 0 1 0 0 0 0.2 1 0.2 1 0.8 0.2 0 2 0 0.8 1 0.2 0 3 0 0.2 0.2 1 0 4 0.2 0 0 0 1 <s> print(similarity) 0 1 2 3 4 0 0 0 0 0.2 1 0.8 0.2 0 2 0.2 0 3 0 4 ", "apis": "DataFrame set_index mean", "code": ["from scipy.spatial.distance import pdist, squareform\npd.DataFrame(1 - squareform(pdist(df.set_index('id'), lambda u,v: (u != v).mean())))\n"], "link": "https://stackoverflow.com/questions/64646490/calculate-similarity-between-rows-of-a-dataframe-count-values-in-common"}
{"id": 292, "q": "Reverse the order of elements by group", "d": "Say I have a DataFrame like this: which looks like this I would like to reverse its elements within each group, where column determines the group. So, the desired output would be How can I do this?", "q_apis": "DataFrame where", "io": " a b 0 1 1 1 1 2 2 1 3 3 1 4 4 2 5 5 2 6 6 2 7 7 2 8 <s> a b 0 1 4 1 1 3 2 1 2 3 1 1 4 2 8 5 2 7 6 2 6 7 2 5 ", "apis": "groupby apply iloc reset_index drop", "code": ["(\n    df.groupby('a', sort=False)\n    .apply(lambda x: x.iloc[::-1])\n    .reset_index(drop=True)\n)\n"], "link": "https://stackoverflow.com/questions/61969815/reverse-the-order-of-elements-by-group"}
{"id": 489, "q": "How to concatenate pandas dataframe date and different time formats to single timestamp?", "d": "I have two columns in a data frame as outlined below. Notice how some of the is in , some is in format. When running... ...I can get in a consumable (for my purposes) format (e.g. ). But when running... ... is added to the times and is not being applied to all rows. How do I concatenate the date and times (which include multiple time formats) in a single timestamp? Edit1: @Wen-Ben 's solution got me here: Then to concatenate EVENT_DATE and EVENT_TIME, I found this (which works): ...results in: Next I want to get this into ISO8601 format. So I found this (which works): ...results in: HERES MY NEW PROBLEM: Running still shows the concatenated versions (e.g. ) instead of the ISO version (e.g.) How do I get the ISO8601 column \"added\" to the dataframe? Ideally, I want it to take the place of . I want it as a transformation of the data, not tacked on as a new column.", "q_apis": "date time timestamp columns get all date time timestamp get get take", "io": "1 19:53:00 11 14:30:00 15 16:30:00 <s> 1 1999-07-28 19:53:00 11 2001-07-28 14:30:00 15 2002-06-07 16:30:00 ", "apis": "to_datetime to_datetime fillna", "code": ["s1=pd.to_datetime(df['EVENT_TIME'], format='%H.%M.%S', errors='coerce')\ns2=pd.to_datetime(df['EVENT_TIME'], format='%I:%M:%S %p', errors='coerce')\ndf['EVENT_TIME']=s1.fillna(s2)\n"], "link": "https://stackoverflow.com/questions/55859947/how-to-concatenate-pandas-dataframe-date-and-different-time-formats-to-single-ti"}
{"id": 513, "q": "Grouping columns by unique values in Python", "d": "I have a data set with two columns and I need to change it from this format: to this I need every unique value in the first column to be on its own row. I am a beginner with Python and beyond reading in my text file, I'm at a loss for how to proceed.", "q_apis": "columns unique values columns unique value first at", "io": "10 1 10 5 10 3 11 5 11 4 12 6 12 2 <s> 10 1 5 3 11 5 4 12 6 2 ", "apis": "DataFrame", "code": ["import pandas as pd\n\ndf = pd.DataFrame({'A':[10,10,10,11,11,12,12],'B':[1,5,3,5,4,6,2]})\nprint(df)\n"], "link": "https://stackoverflow.com/questions/44606413/grouping-columns-by-unique-values-in-python"}
{"id": 343, "q": "Pandas: How to remove non-alphanumeric columns in Series", "d": "A Pandas' Series can contain invalid values: I want to produce a clean Series keeping only the columns that contain a numeric value or a non-empty non-space-only alphanumeric string: should be dropped because it is an empty string; because ; and because space-only strings. The expected result: How can I filter the columns that contain numeric or valid alphanumeric? returns for , instead of the True I would expect. changes 's to string and later considers it a valid string. of course drops only (). I don't see so many other possibilities listed at https://pandas.pydata.org/pandas-docs/stable/reference/series.html As a workaround I can loop on the items() checking type and content, and create a new Series from the values I want to keep, but this approach is inefficient (and ugly): Is there any boolean filter that can help me to single out the good columns?", "q_apis": "columns Series Series values Series columns value empty empty filter columns at items Series values any filter columns", "io": "a b c d e f g 1 \"\" \"a3\" np.nan \"\\n\" \"6\" \" \" <s> a c f 1 \"a3\" \"6\" ", "apis": "astype notna dtype", "code": ["row = row[row.astype(str).str.isalnum() & row.notna()]\nprint (row)\na     1\nc    a3\nf     6\nName: 0, dtype: object\n"], "link": "https://stackoverflow.com/questions/60394977/pandas-how-to-remove-non-alphanumeric-columns-in-series"}
{"id": 433, "q": "Subtract previous row value from the current row value in a Pandas column", "d": "I have a pandas column with the name 'values' containing respective values . I want to subtract the each value from the next value so that I get the following format: I've tried to solve this using a for loop that loops over all the data-frame but this method was time consuming. Is there a straightforward method the dataframe functions to solve this problem quickly? The output we desire is the following:", "q_apis": "value value name values values value value get all time", "io": "10 15 36 95 99 <s> 10 5 21 59 4", "apis": "Series diff fillna", "code": ["import pandas as pd\n\ns = pd.Series([11,15,22,27,36,69,77])\ns.diff().fillna(s)\n"], "link": "https://stackoverflow.com/questions/57801048/subtract-previous-row-value-from-the-current-row-value-in-a-pandas-column"}
{"id": 368, "q": "Is there a way to replace each cell value in a dataframe with the column name, row value in the first column and the value itself?", "d": "I have a matrix in excel which I am reading in as a pandas dataframe in python I want to be able to concatenate the column name, cell values from the first column and the current value of the cell for all cells in columns greater than col1. I essentially want the following output: I could not figure out a way to do this in python.", "q_apis": "replace value name value first value name values first value all columns", "io": " col1 col2 col3 C_0 a f C_1 b g C_2 c h C_3 d i C_4 e j <s> col1 col2 col3 C_0 col2_C_0_a col3_C_0_f C_1 col2_C_1_b col3_C_1_g C_2 col2_C_2_c col3_C_2_h C_3 col2_C_3_d col3_C_3_i C_4 col2_C_4_e col3_C_4_j ", "apis": "astype set_index radd index radd columns reset_index", "code": ["m = df.astype(str).set_index('col1')\nm.radd(m.index+'_',axis=0).radd(m.columns + '_').reset_index()\n"], "link": "https://stackoverflow.com/questions/59754548/is-there-a-way-to-replace-each-cell-value-in-a-dataframe-with-the-column-name-r"}
{"id": 612, "q": "How to select specific column items as list from pandas dataframe?", "d": "I have a dataframe like this : How to convert it into this form (All zeros appearing are not considered) : And finally into a set of items like :", "q_apis": "select items items", "io": " A B C D --------------- 0 A 0 C D 1 A 0 C D 2 0 B C 0 3 A 0 0 D 4 0 B C 0 5 A 0 0 0 <s> A B C D E ---------------------- 0 A 0 C D [A,C,D] 1 A 0 C D [A,C,D] 2 0 A C 0 [A,C] 3 A 0 0 D [A,D] 4 0 A C 0 [A,C] 5 A 0 0 0 [A] ", "apis": "values", "code": ["s = [set([y for y in x if y != '0']) for x in df.values.tolist()]\nprint (s)\n[{'A', 'D', 'C'}, {'A', 'D', 'C'}, {'C', 'B'}, {'A', 'D'}, {'C', 'B'}, {'A'}]\n"], "link": "https://stackoverflow.com/questions/51259229/how-to-select-specific-column-items-as-list-from-pandas-dataframe"}
{"id": 452, "q": "Error when trying to .insert() into dataframe", "d": "I have some code that takes a csv file, that finds the min/max each day, then tells me what time that happens. I also have 2 variables to find the percentage for both max/min. This is currently the output for the dataframe Then I have 2 varables for the % of High/Lows.. (Just ph shown) I've tried to do an .insert(), but recieve this error. TypeError: insert() takes from 4 to 5 positional arguments but 6 were given This was my code I would like the output to show the % in columns 3 &4", "q_apis": "insert min max day time max min insert insert columns", "io": ">Out High Low 10:00 6.0 10.0 10:05 10.0 3.0 10:10 1.0 7.0 10:15 1.0 NaN 10:20 4.0 4.0 10:25 4.0 4.0 10:30 5.0 1.0 10:35 5.0 6.0 10:40 3.0 2.0 10:45 4.0 5.0 10:50 4.0 1.0 10:55 3.0 4.0 11:00 4.0 5.0 > <s> >Out High Low % High %Low 10:00 6.0 10.0 .015306 10:05 10.0 3.0 .025510 10:10 1.0 7.0 .002551 10:15 1.0 NaN .002551 10:20 4.0 4.0 .010204 10:25 4.0 4.0 .010204 > ", "apis": "insert insert", "code": ["#adding % to end of dataframe\nresult[\"High %\"] = ph\nresult[\"Low %\"] = pl\n", "result.insert(2, \"High %\", ph)\nresult.insert(3, \"Low %\", pl)\n"], "link": "https://stackoverflow.com/questions/56976142/error-when-trying-to-insert-into-dataframe"}
{"id": 365, "q": "Setting subset of a pandas DataFrame by a DataFrame if a value matches", "d": "I think the easiest way to explain what I am trying to do is by showing an example: Given a DataFrame and a subset of a second DataFrame : I want to replace the NaN's from the second DataFrame by the first, BUT at the place where the ID matches, as I am not sure that the selected data will always be in the same order or if all IDs will be included. I know I could do it with a for and if loop, but I am wondering if there is a faster way. If an ID form the second DataFrame is not included in the first DataFrame the values should just stay as NaN's. Any help is highly appreciated.", "q_apis": "DataFrame DataFrame value DataFrame second DataFrame replace second DataFrame first at where all second DataFrame first DataFrame values", "io": " V_set V_reset I_set I_reset HRS LRS ID 0 0.599417 -0.658417 0.000021 -0.000606 84562.252849 1097.226787 1383.0 1 0.595250 -0.684708 0.000023 -0.000617 43234.544776 1144.445368 1384.0 2 0.621229 -0.710812 0.000026 -0.000625 51719.718749 1216.609759 1385.0 3 0.625292 -0.720104 0.000029 -0.000625 40827.993527 1209.966052 1386.0 4 0.634563 -0.735937 0.000029 -0.000641 46881.785573 1219.497465 1387.0 ... ... ... ... ... ... ... 1066 0.167521 0.000000 0.000581 0.000000 720.116614 708.098519 2811.0 1067 0.167360 0.000000 0.000581 0.000000 718.165882 708.284487 2812.0 1068 0.172812 0.000000 0.000278 0.000000 715.302620 708.167571 2813.0 1069 0.167729 0.000000 0.000581 0.000000 716.096291 708.333064 2814.0 1070 0.173037 0.000000 0.000278 0.000000 715.474310 707.980273 2815.0 <s> V_set V_reset I_set I_reset HRS LRS ID 1383 NaN NaN NaN NaN NaN NaN 1383.0 1384 NaN NaN NaN NaN NaN NaN 1384.0 1385 NaN NaN NaN NaN NaN NaN 1385.0 1386 NaN NaN NaN NaN NaN NaN 1386.0 1387 NaN NaN NaN NaN NaN NaN 1387.0 ... ... ... ... ... ... ... 2811 NaN NaN NaN NaN NaN NaN 2811.0 2812 NaN NaN NaN NaN NaN NaN 2812.0 2813 NaN NaN NaN NaN NaN NaN 2813.0 2814 NaN NaN NaN NaN NaN NaN 2814.0 2815 NaN NaN NaN NaN NaN NaN 2815.0 ", "apis": "set_index columns fillna map", "code": ["df.set_index('ID',inplace=True)\nfor column in df.columns:\n    df2[column] = df2[column].fillna(df2['ID'].map(df[column]))\n"], "link": "https://stackoverflow.com/questions/59786133/setting-subset-of-a-pandas-dataframe-by-a-dataframe-if-a-value-matches"}
{"id": 529, "q": "Select columns in panda&#39;s dataframe that have an integer header", "d": "I have a dataframe in pandas that looks like this: What I want to do is select specific columns from this data frame. But when I try the following code (the df_matrix being the dataframe displayed at the top) : It does not work and from what I can tell is because it is an integer. I tried to force it with str(100) but gave the same error as before: Does anyone know how to get around this? Thanks! EDIT 1: After trying to use it worked as expecte. Btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use: and the output will be:", "q_apis": "columns select columns at get select columns at time", "io": " 100 200 300 400 0 1 1 0 1 1 1 1 1 0 <s> 100 300 0 1 0 1 1 1 ", "apis": "columns columns", "code": ["df.columns = [str(x) for x in df.columns]"], "link": "https://stackoverflow.com/questions/54333620/select-columns-in-pandas-dataframe-that-have-an-integer-header"}
{"id": 436, "q": "Identical dictionaries in DataFrame all changing at once", "d": "I'm working on a project and currently experiencing an issue with populating some dictionaries within a DataFrame. The problem is more complicated but essentially the main issue can be simplified as follows: I have a DataFrame of dictionaries, all of which initially empty, say When I attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one I'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a': I'm assuming this behaviour is caused by using in my DataFrame initialization, but I'm not familiar enough with Python to understand why. Is Python creating one dictionary and passing references to it to populate the DataFrame? If so, how could I initialise it to create individual dictionaries? I found that I can create a deep copy of each dictionary before processing them, i.e. , but I'm curious if there is a way of doing it without resorting to that.", "q_apis": "DataFrame all at DataFrame DataFrame all empty add value item at all identical add value DataFrame DataFrame copy", "io": " 0 1 0 {} {} 1 {} {} 2 {} {} <s> 0 1 0 {'char': 'a'} {'char': 'a'} 1 {'char': 'a'} {'char': 'a'} 2 {'char': 'a'} {'char': 'a'} ", "apis": "DataFrame index DataFrame", "code": ["example = pd.DataFrame([[{} for _ in range(2)] for _ in range(3)], index=range(0, 3))", "example = pd.DataFrame([[{} for _ in range(n)] for _ in range(m)])"], "link": "https://stackoverflow.com/questions/57589538/identical-dictionaries-in-dataframe-all-changing-at-once"}
{"id": 48, "q": "How to add a value to a new column by referencing the values in a column", "d": "I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this?", "q_apis": "add value values value names at first value value value first", "io": "id reason x1 x2 x3 x4 x5 1 x1 100 15 10 20 25 2 x1 15 16 14 10 10 3 x4 10 50 40 30 25 4 x3 12 15 60 5 1 5 x1 80 15 10 20 25 6 x1 15 19 84 10 10 7 x4 90 40 90 30 25 8 x4 12 85 60 50 10 <s> id reason x1 x2 x3 x4 x5 xy 1 x1 100 15 10 20 25 100 2 x1 15 16 14 10 10 15 3 x4 10 50 40 30 25 30 4 x3 12 15 60 5 1 60 5 x1 80 15 10 20 25 80 6 x1 15 19 84 10 10 15 7 x4 90 40 90 30 25 30 8 x4 12 85 60 50 10 50 ", "apis": "apply", "code": ["df[\"xy\"] = df.apply(lambda x: x[x[\"reason\"]], axis=1)\nprint(df)\n"], "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column"}
{"id": 44, "q": "Python pandas dataframe apply result of function to multiple columns where NaN", "d": "I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result:", "q_apis": "apply columns where columns values value values get apply mask get", "io": " x y z 0 a 1.0 2.0 1 b 1.0 2.0 2 c 1.0 2.0 3 d NaN NaN 4 e NaN NaN 5 f NaN NaN <s> x y z 0 a 1.0 2.0 1 b 1.0 2.0 2 c 1.0 2.0 3 d a1 a2 4 e b2 b1 5 f c3 c4 ", "apis": "fillna apply set_axis", "code": ["out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n"], "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan"}
{"id": 522, "q": "How to create a join in Dataframe based on the other dataframe?", "d": "I have 2 dataframes. One containing student batch details and another one with points. I want to join 2 dataframes. Dataframe1 contains Dataframe2 contains I am trying to map the mark in the same dataset for each student. I tried below code but it is replacing the values one by one.", "q_apis": "join join contains contains map values", "io": "+-------+-------+-------+--+ | s1 | s2 | s3 | | +-------+-------+-------+--+ | Stud1 | Stud2 | Stud3 | | | Stud2 | Stud4 | Stud1 | | | Stud1 | Stud3 | Stud4 | | +-------+-------+-------+--+ <s> +-------+-------+-------+----+----+----+ | Stud1 | Stud2 | Stud3 | 90 | 80 | 95 | | Stud2 | Stud4 | Stud1 | 80 | 55 | 90 | | Stud1 | Stud3 | Stud4 | 90 | 95 | 55 | +-------+-------+-------+----+----+----+ ", "apis": "set_index join replace add_prefix join apply map add_prefix", "code": ["s = dfnamepoints.set_index('Name')['Point']\ndf = df3.join(df3.replace(s).add_prefix('new_'))\n", "df = df3.join(df3.apply(lambda x: x.map(s)).add_prefix('new_'))\n"], "link": "https://stackoverflow.com/questions/54607098/how-to-create-a-join-in-dataframe-based-on-the-other-dataframe"}
{"id": 139, "q": "How to apply a function to every element in a dataframe?", "d": "This is probably a very basic question but I can't find the answer in other questions. I have two lists that I have used to create a 2D dataframe, let's say: Which gives: I would like to go through all elements in the dataframe and use the values of and as inputs to some function, , that I have written. For example, in the 2rd row, 1st column (using zero indexing) position I have , so in this position I would like to apply and not . I think I should be able to use or somehow but I can't figure it out!", "q_apis": "apply all values apply", "io": " 10.0 15.0 20.0 25.0 0.00 NaN NaN NaN NaN 0.25 NaN NaN NaN NaN 0.50 NaN NaN NaN NaN 0.75 NaN NaN NaN NaN 1.00 NaN NaN NaN NaN 1.25 NaN NaN NaN NaN 1.50 NaN NaN NaN NaN 1.75 NaN NaN NaN NaN 2.00 NaN NaN NaN NaN <s> (X, Y) = (0.5, 15.0)", "apis": "name index name index DataFrame index columns apply name index", "code": ["import numpy as np\nimport pandas as pd \n\ndef foo(name, index):\n    return name - index\n\nx = np.arange(0, 2.01, 0.25)\ny = np.arange(10, 30, 5.0) \n\ndf = pd.DataFrame(index = x, columns = y)\n\ndf.apply(lambda x: foo(x.name, x.index))\n"], "link": "https://stackoverflow.com/questions/65356414/how-to-apply-a-function-to-every-element-in-a-dataframe"}
{"id": 386, "q": "Groupby &amp; Sum from occurance of a particular value till the occurance of another particular value or the same value", "d": "I have a dataframe as below. I want to 'user' & 'eve' and 'Ses' till 100/200 & from 100 to 200. Also, return the value of column 'Name' where 100/200 occurs. If after an hundred, there is no 100 or 200 (like last row in group a & 123 or a & 456), ignore it. The expected output for the above input df is a df below.", "q_apis": "value value value value where last", "io": "User eve Ses ID Name a 123 1 10 a a 123 2 11 a a 123 3 12 a a 123 4 13 a a 123 3 100 xyz a 123 6 10 a a 456 1 11 a a 456 2 12 a a 456 3 13 a a 456 4 40 a a 456 1 100 mno a 456 14 10 a a 456 7 20 a a 456 8 30 a a 456 12 200 pqr a 456 10 10 a b 123 1 20 a b 123 2 30 a b 123 3 40 a b 123 4 50 a b 123 1 70 a b 123 6 100 abc b 888 1 20 a b 888 1 200 jkl b 888 3 10 a b 888 4 20 a b 888 5 30 a b 888 1 100 rrr b 888 7 50 a b 888 8 70 a <s> User eve Ses Name a 123 13 xyz a 456 11 mno a 456 41 pqr b 123 17 abc b 888 2 jkl b 888 13 rrr ", "apis": "isin mask where groupby bfill notnull groupby groupby shift cumsum agg sum last", "code": ["# valid IDs\ndf['valids'] = df['ID'].isin([100,200])\n\n# mask the trailing non-hundred ids\nheads = (df['ID'].where(df['valids'])\n             .groupby([df['User'],df['eve']])\n             .bfill().notnull()\n        )\ndf = df[heads]\n\n# groupby and output:\n(df.groupby(['User','eve', df['valids'].shift(fill_value=0).cumsum()],\n           as_index=False)\n   .agg({'Ses':'sum', 'Name':'last'})\n)\n"], "link": "https://stackoverflow.com/questions/59281644/groupby-sum-from-occurance-of-a-particular-value-till-the-occurance-of-another"}
{"id": 356, "q": "multiply two columns from two different pandas dataframes", "d": "I have a pandas.DataFrame. I have another pandas.DataFrame I want to apply df2['Price_factor'] to df1['Price'] column. I tried my code but it didn't work. Thank you for your help in advance.", "q_apis": "columns DataFrame DataFrame apply", "io": " df1 Year Class Price EL 2024 PC1 $243 Base 2025 PC1 $215 Base 2024 PC1 $217 EL_1 2025 PC1 $255 EL_1 2024 PC2 $217 Base 2025 PC2 $232 Base 2024 PC2 $265 EL_1 2025 PC2 $215 EL_1 <s> df2 Year Price_factor 2024 1 2025 0.98 ", "apis": "map set_index astype", "code": ["df1['Price_factor'] = df1['Year'].map(df2.set_index('Year')['Price_factor'])\ndf1['Price_adjusted']= df1['Price'].str.strip('$').astype(int) * df1['Price_factor']\ndf1\n"], "link": "https://stackoverflow.com/questions/60065499/multiply-two-columns-from-two-different-pandas-dataframes"}
{"id": 481, "q": "What is the most efficient way to create a dictionary of two pandas Dataframe columns?", "d": "What is the most efficient way to organise the following pandas Dataframe: data = into a dictionary like ?", "q_apis": "columns", "io": "Position Letter 1 a 2 b 3 c 4 d 5 e <s> alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']", "apis": "Series values index to_dict", "code": ["In [9]: pd.Series(df.Letter.values,index=df.Position).to_dict()\nOut[9]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}\n"], "link": "https://stackoverflow.com/questions/17426292/what-is-the-most-efficient-way-to-create-a-dictionary-of-two-pandas-dataframe-co"}
{"id": 575, "q": "Creating a function to perform grouping and sorting based on columns in Pandas dataframe and Labeling", "d": "I am wanting to group the data into two groups based on the Col2 group. However the first match should be assigned one value and the rest of the matches should be assigned a different value. Rahlf helped me to get a function created and then do However, I need two modifications to the function. Instead of the val, it will take the corresponding value from the Col 4 and then return one value (like 'low' to the first match within a group (based on the sorted col1) and then say 'low_red' for the rest of the matches in the group. So my question is how can I modify the function to do that? My Input: Expected Output:", "q_apis": "columns groups first value value get take value value first", "io": " Col1 Col2 Col3 Col4 100 m1 1 4 200 m2 7 5 120 m1 4 4 240 m2 8 5 300 m3 5 4 330 m3 2 4 350 m3 11 4 200 m4 9 4 <s> Col1 Col2 Col3 Col4 Col 5 100 m1 1 4 low 200 m2 7 5 med 120 m1 4 4 low_red 240 m2 8 5 med_red 300 m3 5 4 high 330 m3 2 4 high_red 350 m3 11 4 high_red 200 m4 9 4 high ", "apis": "iloc index value iloc iloc value value shape shape any iloc sort_values groupby transform", "code": ["def my_function(group):\n\n    val = df.iloc[group.index]['Col4']\n\n    value = deeper_logic(group.iloc[0], val.iloc[0], group)\n\n    return [value if i==0 else value + '_red' for i in range(group.shape[0])]\n\ndef deeper_logic(x, val, group):\n\n    if group.shape[0]==1:\n        if x>val:\n            return 'high'\n        else:\n            return 'low'\n\n    if x>val and any(i<=val for i in group.iloc[1:]):\n        return 'high'\n    elif x>val:\n        return 'med'\n    elif x<=val:\n        return 'low'\n    else:\n        return np.nan\n\ndf['Col5'] = df.sort_values(['Col2','Col1']).groupby('Col2')['Col3'].transform(my_function)\n"], "link": "https://stackoverflow.com/questions/52744334/creating-a-function-to-perform-grouping-and-sorting-based-on-columns-in-pandas-d"}
{"id": 597, "q": "creating list from dataframe", "d": "I am a newbie to python. I am trying iterate over rows of individual columns of a dataframe in python. I am trying to create an adjacency list using the first two columns of the dataframe taken from csv data (which has 3 columns). The following is the code to iterate over the dataframe and create a dictionary for adjacency list: and the following is the output I am getting: I see that I am not getting the entire list when I use the constructor. Hence I am not able to loop over the entire data. Could anyone tell me where I am going wrong? To summarize, Here is the input data: the output that I am expecting:", "q_apis": "columns first columns columns where", "io": "A,B,C 933,4139,20100313073721718 933,6597069777240,20100920094243187 933,10995116284808,20110102064341955 933,32985348833579,20120907011130195 933,32985348838375,20120717080449463 1129,1242,20100202163844119 1129,2199023262543,20100331220757321 1129,6597069771886,20100724111548162 1129,6597069776731,20100804033836982 <s> 933: [4139,6597069777240, 10995116284808, 32985348833579, 32985348838375] 1129: [1242, 2199023262543, 6597069771886, 6597069776731] ", "apis": "columns names groupby apply to_dict columns iloc groupby iloc apply to_dict", "code": ["#selecting by columns names\nd = df1.groupby('A')['B'].apply(list).to_dict()\n\n#seelcting columns by positions\nd = df1.iloc[:, 1].groupby(df1.iloc[:, 0]).apply(list).to_dict()\n"], "link": "https://stackoverflow.com/questions/51872125/creating-list-from-dataframe"}
{"id": 621, "q": "How do I sum time series data by day in Python? resample.sum() has no effect", "d": "I am new to Python. How do I sum data based on date and plot the result? I have a Series object with data like: I have the following code: This gives me the following line(?) graph: It's a start; now I want to sum the doses by date. However, this code fails to effect any change: The resulting plot is the same. What is wrong? I have also tried , , , but there is no change in the plot. Is even the correct function? I understand resampling to be sampling from the data, e.g. randomly taking one point per day, whereas I want to sum each day's values. Namely, I'm hoping for some result (based on the above data) like:", "q_apis": "sum time day resample sum sum date plot Series start now sum date any plot plot day sum day values", "io": "2017-11-03 07:30:00 NaN 2017-11-03 09:18:00 NaN 2017-11-03 10:00:00 NaN 2017-11-03 11:08:00 NaN 2017-11-03 14:39:00 NaN 2017-11-03 14:53:00 NaN 2017-11-03 15:00:00 NaN 2017-11-03 16:00:00 NaN 2017-11-03 17:03:00 NaN 2017-11-03 17:42:00 800.0 2017-11-04 07:27:00 600.0 2017-11-04 10:10:00 NaN 2017-11-04 11:48:00 NaN 2017-11-04 12:58:00 500.0 2017-11-04 13:40:00 NaN 2017-11-04 15:15:00 NaN 2017-11-04 16:21:00 NaN 2017-11-04 17:37:00 500.0 2017-11-04 21:37:00 NaN 2017-11-05 03:00:00 NaN 2017-11-05 06:30:00 NaN 2017-11-05 07:19:00 NaN 2017-11-05 08:31:00 200.0 2017-11-05 09:31:00 500.0 2017-11-05 12:03:00 NaN 2017-11-05 12:25:00 200.0 2017-11-05 13:11:00 500.0 2017-11-05 16:31:00 NaN 2017-11-05 19:00:00 500.0 2017-11-06 08:08:00 NaN <s> 2017-11-03 800 2017-11-04 1600 2017-11-05 1900 2017-11-06 NaN ", "apis": "read_csv plot date to_datetime loc plot Series loc values index name plot combine day resample sum plot", "code": ["import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')\n# plot bar graph of date and painkiller amount\ntimes = pd.to_datetime(df.loc[:,'Time'])\n\n# raw plot of data\nts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,\n               name = 'Painkiller over Time')\nfig1 = ts.plot()\n\n# combine data by day\ntest2 = ts.resample('D').sum()\nfig2 = test2.plot()\n"], "link": "https://stackoverflow.com/questions/50983386/how-do-i-sum-time-series-data-by-day-in-python-resample-sum-has-no-effect"}
{"id": 47, "q": "How can I add a new line in pandas dataframe based in a condition?", "d": "I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame.", "q_apis": "add first value second values value add add last last add DataFrame", "io": "[1] [2] [3] 1 30 2 1 30 1 1 30 3 1 90 3 1 370 3 1 430 3 1 705 3 1 805 3 1 880 2 1 905 3 1 1005 3 1 1170 3 1 1230 3 1 1970 3 1 2030 3 1 2970 3 1 3030 3 1 3970 3 1 4030 3 1 4423 3 1 4539 3 1 4575 3 1 4630 2 1 4635 3 1 4671 3 1 4787 3 1 4957 3 1 5057 3 1 5270 3 1 5330 3 1 5970 3 1 6030 3 1 6970 3 1 7030 3 1 7970 3 1 8030 3 1 8158 3 1 8257 3 1 8332 2 1 8357 3 1 8457 3 1 8970 3 1 9030 3 1 9970 3 1 10030 3 1 10970 3 1 11030 3 1 11470 3 1 11530 3 1 11853 3 1 11953 3 <s> 1 90 3 1 190 0 1 370 3 ", "apis": "diff loc Series iloc iloc index index index explode where apply astype mask diff mask mask idxmax loc Series iloc iloc index index index explode where apply astype", "code": ["m = df[\"[2]\"].diff() > 100\n\ndf.loc[m, \"[2]\"] = pd.Series(\n    [\n        [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(\"[2]\")\ndf[\"[3]\"] = np.where(\n    df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n)\ndf[\"[2]\"] = df[\"[2]\"].astype(int)\nprint(df)\n", "mask = df[\"[2]\"].diff() > 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, \"[2]\"] = pd.Series(\n        [\n            [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(\"[2]\")\n    df[\"[3]\"] = np.where(\n        df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n    )\n    df[\"[2]\"] = df[\"[2]\"].astype(int)\n    print(df)\n"], "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition"}
{"id": 359, "q": "Python: how to add a column to a pandas dataframe between two columns?", "d": "I would like to add a column to a dataframe between two columns in number labeled columns dataframe. In the following dataframe the first column corresponds to the index while the first row to the name of the columns. I have that I want to put between the columns and , so", "q_apis": "add between columns add between columns columns first index first name columns put between columns", "io": "df 0 0 1 2 3 4 5 1 6 7 4 5 2 1 2 0 3 1 3 3 4 3 9 8 4 3 6 2 <s> df 0 0 1 2 3 4 5 6 1 6 7 4 5 2 2 1 2 0 3 1 3 3 3 4 3 9 8 4 3 6 5 2 ", "apis": "insert", "code": ["df = df.insert(4, 'new_col_name', tmp)"], "link": "https://stackoverflow.com/questions/36437849/python-how-to-add-a-column-to-a-pandas-dataframe-between-two-columns"}
{"id": 458, "q": "How to append value_counts() output to the original df?", "d": "So I have the following : when I running this line: I get the following output: I was wonder how I could append this output to the original to make it look like this: Thank you very much for your help in advance.", "q_apis": "append value_counts get append", "io": " Open High Low Close 0 0.001268 0.001277 0.001266 0.001271 1 0.001268 0.001269 0.001265 0.001266 2 0.001265 0.001265 0.001242 0.001254 3 0.001253 0.001271 0.001244 0.001251 4 0.001253 0.001259 0.001249 0.001257 5 0.001257 0.001260 0.001241 0.001248 <s> 0.001253 2 0.001268 2 0.001265 1 0.001257 1 ", "apis": "value_counts merge", "code": ["s = df[\"Open\"].value_counts()\n\ndf.merge(s, left_on='Open', right_index=True,\n         suffixes=['','_count'])\n"], "link": "https://stackoverflow.com/questions/56813691/how-to-append-value-counts-output-to-the-original-df"}
{"id": 571, "q": "Pandas variable shifting within groups", "d": "I have a dataframe: I want to create a new field val2 such that each value in val2 is the value in val2 shifted by Lag number of rows. The tricky part here is that the shift should happen within the groups defined in field c1, such that the output looks something like I have been trying with along the lines of to no avail and getting a \"The truth value of a Series is ambiguous.\" error. Appreciate any help. Thanks!", "q_apis": "groups value value shift groups value Series any", "io": "c1 Lag Val1 A 3 10 A 1 5 A 2 20 A 2 15 A 1 10 B 1 25 B 2 10 <s> c1 Lag Val1 Val2 A 3 10 15 A 1 5 20 A 2 20 10 A 2 15 NaN A 1 10 NaN B 1 25 10 B 2 10 NaN ", "apis": "columns rename columns drop columns copy index index index index reset_index merge reset_index index left drop columns index", "code": ["# Copy and keep only the columns that are relevant\ndf2 = df.rename(columns={'Val1': 'Val2'}).drop(columns='Lag').copy()\n\n# Shift the index\ndf.index = df.index+df.Lag\n\n# Merge, requiring match on shifted index and within group.\ndf.reset_index().merge(df2.reset_index(), on=['index', 'c1'], how='left').drop(columns='index')\n"], "link": "https://stackoverflow.com/questions/52940317/pandas-variable-shifting-within-groups"}
{"id": 291, "q": "How can I create a new dataframe by subtracting the first column from every other column?", "d": "df1 is my original dataframe. I want to create another dataframe by substracting column a from every other column (taking the difference between column a and all other columns). df2 is the outcome. I would appreciate your help.", "q_apis": "first difference between all columns", "io": " a b c d e 0 1 3 5 7 9 1 1 3 5 7 9 2 1 3 5 7 9 <s> b c d e 0 2 4 6 8 1 2 4 6 8 2 2 4 6 8 ", "apis": "loc apply", "code": ["df = df1.loc[:,'b':].apply(lambda x: x-df1['a'])\nprint(df)\n"], "link": "https://stackoverflow.com/questions/61969568/how-can-i-create-a-new-dataframe-by-subtracting-the-first-column-from-every-othe"}
{"id": 407, "q": "Write pandas dataframe to_csv in columns with trailing zeros", "d": "I have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns). The complicating factor is I also want each column to be rounded to different number of decimals (some need much higher accuracy). To reproduce: Current result for out.txt: Desired:", "q_apis": "to_csv columns to_csv pad columns", "io": "0 1.0 3.0 5.0 1 1.5 3.455 5.45454 <s> 0 1.0 3.000 5.00000 1 1.5 3.455 5.45454 ", "apis": "to_string", "code": ["with open('out.txt', 'w') as file:\n    file.writelines(df_rounded.to_string(header=False))\n"], "link": "https://stackoverflow.com/questions/58649009/write-pandas-dataframe-to-csv-in-columns-with-trailing-zeros"}
{"id": 190, "q": "How to find the average of each cell in multiple csv&#39;s", "d": "I have several excel files with data in them in a format similar to this There are 3 csv's total, and I need to create a new csv with the average of each cell. So would be as follows So far I have the files imported but I am not sure how to proceed.", "q_apis": "", "io": "csv1 csv1 a b c a b c x 1 2 3 x 3 2 1 y 4 5 6 y 6 5 4 <s> a b c x (3+1)/2) (2+2)/2 (3+1)/2 y (6+4)/2 etc. ", "apis": "", "code": ["import numpy as np\ncsv1 = np.genfromtxt('my_file1.csv', delimiter=',')\ncsv2 = np.genfromtxt('my_file2.csv', delimiter=',')\nnp.savetxt(\"foo.csv\", (csv1+csv2)/2, delimiter=\",\")    \n"], "link": "https://stackoverflow.com/questions/64444765/how-to-find-the-average-of-each-cell-in-multiple-csvs"}
{"id": 281, "q": "Remove quotation marks and brackets from Pandas DataFrame .csv file after performing a GroupBy with MultiIndex", "d": "I'm new to pandas so apologies if my explanations of things are wrong. I have a data frame created as follows: Then I perform a weighted mean, using the indices, using code from the second top answer here. The output on the console looks like this: where (x,y) are the indices that I have grouped by and the number at the end is the weighted mean. When I export to a .csv file, I get a file that looks like this: This is not what I want. I would like to get a .csv file that looks like this: I've tried using reset.index() but this does not work. I want to remove the brackets, quotation marks and the rogue ,0 at the start of the .csv file. How can I do this? Many thanks in advance.", "q_apis": "DataFrame MultiIndex mean indices second where indices at mean get get index at start", "io": " (1, 2) 3 (4, 5) 6 (7, 8) 9 <s> ,0 \"(1, 2)\",3 \"(4, 5)\",6 \"(7, 8)\",9 ", "apis": "groupby index names apply rename reset_index to_csv index", "code": ["df2 = df.groupby(level=df.index.names).apply(lambda x: np.average(x.name3, weights=x.name4))\n\n# save the df2 to csv file\ndf2.rename('avg').reset_index().to_csv('data.csv', index=False)\n"], "link": "https://stackoverflow.com/questions/62302113/remove-quotation-marks-and-brackets-from-pandas-dataframe-csv-file-after-perfor"}
{"id": 536, "q": "Merge order with items in columns", "d": "I have a dataset with all the order, customer and orderitem information. I wandt to expand my orderitems in new columns, but without losing the information about the customer And the result should be somehow: I tried", "q_apis": "items columns all columns", "io": "CustomerId OrderId Item 1 1 CD 1 1 DVD 2 2 CD <s> CustomerId OrderId CD DVD 1 1 1 1 2 2 1 0 ", "apis": "set_index get_dummies max reset_index", "code": ["(df.set_index(['CustomerId', 'OrderId'])\n   .Item.str.get_dummies()\n   .max(level=[0, 1])\n   .reset_index())\n"], "link": "https://stackoverflow.com/questions/54037912/merge-order-with-items-in-columns"}
{"id": 633, "q": "Python pandas: apply on separated values", "d": "How can I sum values in dataframe that a separated by semicolon? Got: Need:", "q_apis": "apply values sum values", "io": " col1 col2 2018-03-05 2.1 8 2018-03-06 8 3.1;2 2018-03-07 1;1 8;1 <s> col1 col2 2018-03-05 2.1 8 2018-03-06 8 5.1 2018-03-07 2 9 ", "apis": "apply astype sum", "code": ["df = df.apply(lambda x: x.str.split(';', expand=True).astype(float).sum(axis=1))\n"], "link": "https://stackoverflow.com/questions/50409452/python-pandas-apply-on-separated-values"}
{"id": 378, "q": "Calculate percentage of similar values in pandas dataframe", "d": "I have one dataframe , with two columns : Script (with text) and Speaker And I have the following list : With the following code, I obtain this dataframe : Which line can I add in my code to obtain, for each line of my dataframe , a percentage value of all lines spoken by speaker, in order to have the following dataframe :", "q_apis": "values columns add value all", "io": "Speaker a b c Speaker 1 2 1 1 Speaker 2 2 0 0 Speaker 3 0 1 0 <s> Speaker a b c Speaker 1 50% 25% 25% Speaker 2 100% 0 0 Speaker 3 0 100% 0 ", "apis": "set_index join join get_dummies sum", "code": ["out = (df.set_index('Speaker')['Script'].str.findall('|'.join(L))\n         .str.join('|')\n         .str.get_dummies()\n         .sum(level=0))\n"], "link": "https://stackoverflow.com/questions/59502905/calculate-percentage-of-similar-values-in-pandas-dataframe"}
