{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a484c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stackapi import StackAPI\n",
    "\n",
    "SITE = StackAPI('stackoverflow')\n",
    "\n",
    "qids = [\n",
    "    11881165, 11941492, 13647222, 18172851, 49583055, 49592930, 49572546, 13261175, \n",
    "    13793321, 14085517, 11418192, 49567723, 13261691, 13659881, 13807758, 34365578, \n",
    "    10982266, 11811392, 49581206, 12065885, 13576164, 14023037, 53762029, 21982987, \n",
    "    39656670, 23321300\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cb8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "autopandas_questions = SITE.fetch('questions/{ids}', ids=qids, filter='withbody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a7e053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['backoff', 'has_more', 'page', 'quota_max', 'quota_remaining', 'total', 'items'])\n"
     ]
    }
   ],
   "source": [
    "print(autopandas_questions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e829b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tags', 'owner', 'is_answered', 'view_count', 'accepted_answer_id', 'answer_count', 'score', 'last_activity_date', 'creation_date', 'last_edit_date', 'question_id', 'content_license', 'link', 'title', 'body'])\n"
     ]
    }
   ],
   "source": [
    "ap_questions = autopandas_questions['items']\n",
    "print(ap_questions[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bb879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "dict_keys(['owner', 'is_accepted', 'score', 'last_activity_date', 'last_edit_date', 'creation_date', 'answer_id', 'question_id', 'content_license', 'body'])\n"
     ]
    }
   ],
   "source": [
    "accepted_answer_ids = [a['accepted_answer_id'] for a in ap_questions if 'accepted_answer_id' in a.keys()]\n",
    "print(len(accepted_answer_ids))\n",
    "answers = SITE.fetch('answers/{ids}', ids=accepted_answer_ids, filter='withbody')[\"items\"]\n",
    "print(answers[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58b99aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def min_word_filter(texts, min_word=3):\n",
    "    filtered_texts = []\n",
    "    for t in texts:\n",
    "        if len(nltk.wordpunct_tokenize(t)) >= min_word:\n",
    "            filtered_texts.append(t)\n",
    "            pass\n",
    "        pass\n",
    "    return filtered_texts\n",
    "\n",
    "def extract_code(text, filters):\n",
    "    soup = bs(text)\n",
    "    all_code = [code.text for code in soup.find_all('code')]\n",
    "    for f in filters:\n",
    "        all_code = f(all_code)\n",
    "    return all_code\n",
    "    pass\n",
    "\n",
    "def add_answer_to_code(questions, answer):\n",
    "    for idx, q in enumerate(questions):\n",
    "        if q[\"question_id\"] == answer[\"question_id\"]:\n",
    "            codes =  extract_code(answer[\"body\"], filters=[min_word_filter])\n",
    "            questions[idx][\"code\"] = codes\n",
    "            break\n",
    "            pass\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "for a in answers:\n",
    "    add_answer_to_code(ap_questions, a)\n",
    "    pass\n",
    "\n",
    "for q in ap_questions:\n",
    "    if \"code\" not in q.keys():\n",
    "        q[\"code\"] = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cded1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description(html):\n",
    "    soup = bs(html)\n",
    "    [code.extract() for code in soup.find_all('code')]\n",
    "    text = re.sub(\"[ \\n\\t]+\", \" \", soup.text)\n",
    "    return text\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4d86aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = []\n",
    "\n",
    "for example_id, q in enumerate(ap_questions):\n",
    "    ques_id = q[\"question_id\"]\n",
    "    qtitle = q[\"title\"]\n",
    "    qdesc = extract_description(q[\"body\"])\n",
    "    test_examples.append(\n",
    "        {\n",
    "            'id': ques_id,\n",
    "            'query': qtitle.strip().lower() + \" \" + qdesc.strip().lower(),\n",
    "            \"apis\": [],\n",
    "            'link': q['link'],\n",
    "            \"example\": q[\"code\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9fe4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "fp = open(\"dpr_exp/autopandas_test.json\", \"w\")\n",
    "json.dump(test_examples, fp, indent=4)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9ba6085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"id\": 18172851,\n",
      "        \"query\": \"deleting dataframe row in pandas based on column value i have the following dataframe: i need to remove the rows where is equal to . what's the most efficient way to do this?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value\",\n",
      "        \"example\": [\n",
      "            \"df = df[df.line_race != 0]\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 21982987,\n",
      "        \"query\": \"mean per group in a data.frame i have a and i need to calculate the mean per group (i.e. per , below). my desired output is like below, where the values for and are the group means. please disregard the value, i have made it up for the example.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/21982987/mean-per-group-in-a-data-frame\",\n",
      "        \"example\": [\n",
      "            \"d <- read.table(text=\\n'Name     Month  Rate1     Rate2\\nAira       1      12        23\\nAira       2      18        73\\nAira       3      19        45\\nBen        1      53        19\\nBen        2      22        87\\nBen        3      19        45\\nCat        1      22        87\\nCat        2      67        43\\nCat        3      45        32', header=TRUE)\\n\\naggregate(d[, 3:4], list(d$Name), mean)\\n\\n  Group.1    Rate1    Rate2\\n1    Aira 16.33333 47.00000\\n2     Ben 31.33333 50.33333\\n3     Cat 44.66667 54.00000\\n\",\n",
      "            \"d$Name\",\n",
      "            \"aggregate(. ~ Name, d[-2], mean)\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 13807758,\n",
      "        \"query\": \"how to delete a row in a pandas dataframe and relabel the index? i am reading a file into a pandas dataframe that may have invalid (i.e. nan) rows. this is sequential data, so i have row_id+1 refer to row_id. when i use frame.dropna(), i get the desired structure, but the index labels stay as they were originally assigned. how can the index labels get reassigned 0 to n-1 where n is the number of rows after dropna()?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/13807758/how-to-delete-a-row-in-a-pandas-dataframe-and-relabel-the-index\",\n",
      "        \"example\": [\n",
      "            \"pandas.DataFrame.reset_index()\",\n",
      "            \"drop=True\",\n",
      "            \"In [14]: df = pd.DataFrame(np.random.randn(5,4))\\n\\nIn [15]: df.ix[::3] = np.nan\\n\\nIn [16]: df\\nOut[16]:\\n          0         1         2         3\\n0       NaN       NaN       NaN       NaN\\n1  1.895803  0.532464  1.879883 -1.802606\\n2  0.078928  0.053323  0.672579 -1.188414\\n3       NaN       NaN       NaN       NaN\\n4 -0.766554 -0.419646 -0.606505 -0.162188\\n\\nIn [17]: df = df.dropna()\\n\\nIn [18]: df.reset_index(drop=True)\\nOut[18]:\\n          0         1         2         3\\n0  1.895803  0.532464  1.879883 -1.802606\\n1  0.078928  0.053323  0.672579 -1.188414\\n2 -0.766554 -0.419646 -0.606505 -0.162188\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 12065885,\n",
      "        \"query\": \"filter dataframe rows if value in column is in a set list of values i have a python pandas dataframe : i can filter the rows whose stock id is like this: and i want to get all the rows of some stocks together, such as . that means i want a syntax like this: since pandas not accept above command, how to achieve the target?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/12065885/filter-dataframe-rows-if-value-in-column-is-in-a-set-list-of-values\",\n",
      "        \"example\": [\n",
      "            \"rpt[rpt['STK_ID'].isin(stk_list)]\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 13261175,\n",
      "        \"query\": \"pandas pivot dataframe to 3d data there seem to be a lot of possibilities to pivot flat table data into a 3d array but i'm somehow not finding one that works: suppose i have some data with columns=['name', 'type', 'date', 'value']. when i try to pivot via i get am i reading docs from dev pandas maybe? it seems like this is the usage described there. i am running 0.8 pandas. i guess, i'm wondering if i have a multiindex ['x', 'y', 'z'] series, is there a pandas way to put that in a panel? i can use groupby and get the job done, but then this is almost like what i would do in numpy to assemble an n-d array. seems like a fairly generic operation so i would imagine it might be implemented already.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/13261175/pandas-pivot-dataframe-to-3d-data\",\n",
      "        \"example\": [\n",
      "            \"pandas.tools.pivot.pivot_table(your_dataframe, values='value', index='name', columns=['type', 'date'], aggfunc='sum')\\n\",\n",
      "            \"df = DataFrame({'name': ['A', 'B', 'A', 'B'], 'type': [1, 1, 2, 2], 'date': ['2012-01-01', '2012-01-01', '2012-02-01', '2012-02-01'],  'value': [1, 2, 3, 4]})\\n\\npt = df.pivot_table(values='value', index='name', columns=['type', 'date'])\\np = df.pivot('name', 'type')\\n\",\n",
      "            \"type           1           2\\ndate  2012-01-01  2012-02-01\\nname                        \\nA              1           3\\nB              2           4\\n\",\n",
      "            \"          date              value   \\ntype           1           2      1  2\\nname                                  \\nA     2012-01-01  2012-02-01      1  3\\nB     2012-01-01  2012-02-01      2  4\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 11418192,\n",
      "        \"query\": \"pandas: complex filter on rows of dataframe i would like to filter rows by a function of each row, e.g. or for another more complex, contrived example, how can i do so?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/11418192/pandas-complex-filter-on-rows-of-dataframe\",\n",
      "        \"example\": [\n",
      "            \"DataFrame.apply\",\n",
      "            \"In [3]: df = pandas.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'])\\n\\nIn [4]: df\\nOut[4]: \\n          a         b         c\\n0 -0.001968 -1.877945 -1.515674\\n1 -0.540628  0.793913 -0.983315\\n2 -1.313574  1.946410  0.826350\\n3  0.015763 -0.267860 -2.228350\\n4  0.563111  1.195459  0.343168\\n\\nIn [6]: df[df.apply(lambda x: x['b'] > x['c'], axis=1)]\\nOut[6]: \\n          a         b         c\\n1 -0.540628  0.793913 -0.983315\\n2 -1.313574  1.946410  0.826350\\n3  0.015763 -0.267860 -2.228350\\n4  0.563111  1.195459  0.343168\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 13261691,\n",
      "        \"query\": \"understanding multiindex so i have a sample data set like this in csv:- executing the following:- how do i compress the first index (\\\"team\\\") further so that i don't have duplicate values? to become:-\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/13261691/understanding-multiindex\",\n",
      "        \"example\": [\n",
      "            \"df = read_csv(\\\"data/Workbook1.csv\\\")\\n\\ndf\\n\\n     name team      date  score\\n0    John    A    3/9/12    100\\n1    John    B    3/9/12     99\\n2    Jane    B    4/9/12    102\\n3   Peter    A    9/9/12    103\\n4   Josie    C   11/9/12    111\\n5  Rachel    A  30/10/12     98\\n6    Kate    B  31/10/12    103\\n7   David    C   1/11/12    104\\n\\ndf2 = df.pivot('team', 'name').stack()\\n\\ndf2\\n\\n                 date  score\\nteam name                   \\nA    John      3/9/12    100\\n     Peter     9/9/12    103\\n     Rachel  30/10/12     98\\nB    Jane      4/9/12    102\\n     John      3/9/12     99\\n     Kate    31/10/12    103\\nC    David    1/11/12    104\\n     Josie    11/9/12    111\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 13659881,\n",
      "        \"query\": \"count by unique pair of columns in pandas i'm trying to figure out how to count by number of rows per unique pair of columns (ip, useragent), e.g. to produce: ideas?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/13659881/count-by-unique-pair-of-columns-in-pandas\",\n",
      "        \"example\": [\n",
      "            \"d.groupby(['ip', 'useragent']).size()\\n\",\n",
      "            \"ip          useragent               \\n192.168.0.1 a           2\\n            b           1\\n192.168.0.2 b           1\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 13576164,\n",
      "        \"query\": \"pandas merge and retain the index a similar question was asked in how to keep index when using pandas merge, but it will not work with multiindexes, i.e, how can one make the merge while preserving the multiindex in the left dataframe?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/13576164/pandas-merge-and-retain-the-index\",\n",
      "        \"example\": [\n",
      "            \"In [255]: a = a.reset_index()\\n\\nIn [256]: a\\nOut[256]: \\n   id1 id2  col1  to_merge_on\\n0    1   a     1            2\\n1    1   b     3            4\\n2    2   a     1            2\\n3    2   b     3            4\\n\\nIn [271]: c = pd.merge(a, b, how=\\\"left\\\")\\n\\nIn [272]: c\\nOut[272]: \\n   id1 id2  col1  to_merge_on  col2\\n0    1   a     1            2   NaN\\n1    2   a     1            2   NaN\\n2    2   b     3            3     2\\n3    1   b     3            4   NaN\\n\\nIn [273]: c = c.set_index(['id1','id2'])\\n\\nIn [274]: c\\nOut[274]: \\n         col1  to_merge_on  col2\\nid1 id2                         \\n1   a       1            2   NaN\\n2   a       1            2   NaN\\n    b       3            3     2\\n1   b       3            4   NaN\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 11941492,\n",
      "        \"query\": \"selecting rows from a pandas dataframe with a compound (hierarchical) index i'm suspicious that this is trivial, but i yet to discover the incantation that will let me select rows from a pandas dataframe based on the values of a hierarchical key. so, for example, imagine we have the following dataframe: df looks as we would expect: if df were not indexed on group1 i could do the following: but that fails on this dataframe with an index. so maybe i should think of this like a pandas series with a hierarchical index: nope. that fails as well. so how do i select out all the rows where: group1 == 'a' group1 == 'a' & group2 == 'c' group2 == 'c' group1 in ['a','b','c']\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/11941492/selecting-rows-from-a-pandas-dataframe-with-a-compound-hierarchical-index\",\n",
      "        \"example\": [\n",
      "            \"In [5]: df.xs('a', level=0)\\nOut[5]: \\n        value1  value2\\ngroup2                \\nc          1.1     7.1\\nc          2.0     8.0\\nd          3.0     9.0\\n\\nIn [6]: df.xs('c', level='group2')\\nOut[6]: \\n        value1  value2\\ngroup1                \\na          1.1     7.1\\na          2.0     8.0\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 13793321,\n",
      "        \"query\": \"joining table/dataframes with common column in python i have two dataframes: as you can see both dataframes have as a common column. i want to join these two dataframes by matching . my current code is: , but this is giving an error.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/13793321/joining-table-dataframes-with-common-column-in-python\",\n",
      "        \"example\": [\n",
      "            \"df1.merge(df2, on='Date_Time')\\n\",\n",
      "            \"import pandas as pd\\ndf1 = pd.DataFrame([[1, 2, 3]])\\ndf2 = pd.DataFrame([[1, 7, 8],[4, 9, 9]], columns=[0, 3, 4])\\n\\nIn [4]: df1\\nOut[4]: \\n   0  1  2\\n0  1  2  3\\n\\nIn [5]: df2\\nOut[5]: \\n   0  3  4\\n0  1  7  8\\n1  4  9  9\\n\\nIn [6]: df1.merge(df2, on=0)\\nOut[6]: \\n   0  1  2  3  4\\n0  1  2  3  7  8\\n\\nIn [7]: df1.merge(df2, on=0, how='outer')\\nOut[7]: \\n   0   1   2  3  4\\n0  1   2   3  7  8\\n1  4 NaN NaN  9  9\\n\",\n",
      "            \"In [8]: df1.join(df2, on=0)\\n# error!\\nException: columns overlap: array([0], dtype=int64)\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 53762029,\n",
      "        \"query\": \"pandas groupby and cumsum on a column i have a data set which looks like the following i want to group by the month, year and speciality and get cumulative sum on 'doc_id count' column. these are the following i tried: none of them are returning the proper cumulative sum. any solution can help. the expected output should be: for each year, month and speciality i want the cumsum of 'doc_id count'\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/53762029/pandas-groupby-and-cumsum-on-a-column\",\n",
      "        \"example\": []\n",
      "    },\n",
      "    {\n",
      "        \"id\": 39656670,\n",
      "        \"query\": \"another way to do pivot table in r i have data set like below: then there is a code count mean of certain variables: and the output is: then i need to perform an output like below: i tried to do like this: the output is same. my question: is there anyway to perform the same output with the different way? this is my a course but my submission was rejected. i do not know why but i had ask the about this. please advise\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/39656670/another-way-to-do-pivot-table-in-r\",\n",
      "        \"example\": [\n",
      "            \"library(dplyr)\\nlibrary(tidyr)\\nworldcup %>% \\n       gather(Var, Val, Time:Saves) %>% \\n       filter(Var!= \\\"Shots\\\") %>%\\n       group_by(Var) %>% \\n       summarise(Mean = mean(Val))\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 49592930,\n",
      "        \"query\": \"inserting row in pandas dataframe based on time using datetime index i have two dataframes as show below: the length is equal to 55037 , and the other one the length is equal to 27519, the two dataframe above are groupby with different time step, now i would like to match the time step in the beginning and end automatic. for example, the \\\"2014-05-21 09:00:00\\\" is lack in df1, how can i write a function, put the df1 and it would check if the beginning and end matched df2, then return a new df1 which in the beginning and end equal to df2 and the value equal to zero. so in the end the new df1 would as show below, and length equal to 55038 (make sure it is two times large than df2) anyone have idea ? thanks in adavance!\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/49592930/inserting-row-in-pandas-dataframe-based-on-time-using-datetime-index\",\n",
      "        \"example\": [\n",
      "            \"df2.index\",\n",
      "            \"df = df1.combine_first(df2)\\nprint (df)\\n                     value\\n2014-05-21 09:00:00    0.0 <- value not exist in df1, used df2 value\\n2014-05-21 09:30:00    0.0\\n2014-05-21 10:00:00   10.0 <-value exist in both df, used df1 value\\n2014-05-21 10:30:00    3.0\\n2017-07-10 21:00:00    1.6\\n2017-07-10 22:00:00   32.1\\n2017-07-10 22:30:00   18.3\\n2017-07-10 23:00:00    7.6\\n2017-07-10 23:30:00    0.0\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 49583055,\n",
      "        \"query\": \"how to sort only certain rows within a group in pandas? i have a dataframe that is grouped by . for each group, there is one row that has a flag that identifies that its the first instance . i ultimately want to sort each group by admit date, however, i need the row with to be the first row of that group regardless of its date. then i want to sort the remaining rows based on admit date. sample : expected : my approach does not account for the 'first' column being first within the group. this has been stumping me all day.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/49583055/how-to-sort-only-certain-rows-within-a-group-in-pandas\",\n",
      "        \"example\": [\n",
      "            \"df.sort_values(by=['ID', 'first', 'admit'], \\n               ascending = [True, False, True],\\n               inplace = True)\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 49581206,\n",
      "        \"query\": \"how can i get the percentage between two subcolumn on pandas multinindex? i have this, i would like the percent of yes column, as follows,\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/49581206/how-can-i-get-the-percentage-between-two-subcolumn-on-pandas-multinindex\",\n",
      "        \"example\": []\n",
      "    },\n",
      "    {\n",
      "        \"id\": 49572546,\n",
      "        \"query\": \"assign values in pandas column based on another column i have two dataframes like the ones that are shown below: a and b i want to merge the two datasets and replace the zeros in a with the values in b based on the timestamp column and have a new a dataframe like the one shown below:\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/49572546/assign-values-in-pandas-column-based-on-another-column\",\n",
      "        \"example\": [\n",
      "            \"#convert columns to indices if necessary\\n#A = A.set_index('Timestamp')\\n#B = B.set_index('Timestamp')\\n\\ndf = B.mask(B == 0).combine_first(A)\\n#alternative\\n#df = B.replace({0:np.nan}).combine_first(A)\\nprint (df)\\n           C1  C2  C3\\nTimestamp            \\n1           0   0   0\\n2           0   0   0\\n3          v1  v2  v3\\n4          v4  v5  v6\\n5           0   0   0\\n6           0   0   0\\n7           0   0   0\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 49567723,\n",
      "        \"query\": \"how can i compare different values with same ids of different dataframe pandas i have two different dataframe named df1 and df2, with same id column, but some ids have the same count and some ids have the different counts, so i want to get the data for same ids with the different count values, and both dataframes have the different indexes following is my df1 my df2 is how can i do that?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/49567723/how-can-i-compare-different-values-with-same-ids-of-different-dataframe-pandas\",\n",
      "        \"example\": [\n",
      "            \"df1.merge(df2, on='id').query('valueA != valueB')\\n\",\n",
      "            \"    id  valueA  valueB\\n0  255    1141    1231\\n1   91    1130    1170\\n2  347     830     870\\n5  159     715     734\\n8  225     638     644\\n\",\n",
      "            \"df_out = df1.merge(df2, on='id')\\ndf_out[df_out['valueA'] != df_out['valueB']]\\n\",\n",
      "            \"df1 = df1.set_index('id')\\ndf2 = df2.set_index('id')\\n\\ndf_diff = df1['valueA'] - df2['valueB']\\n\\ndf_diff = df_diff[df_diff.notnull() & df_diff.ne(0)]\\n\\npd.concat([df1.reindex(df_diff.index), df2.reindex(df_diff.index)], axis=1)\\n\",\n",
      "            \"     valueA  valueB\\nid                 \\n91     1130    1170\\n159     715     734\\n225     638     644\\n255    1141    1231\\n347     830     870\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 11881165,\n",
      "        \"query\": \"slice pandas dataframe by row i am working with survey data loaded from an h5-file as through the pandas package. within this , all rows are the results of a single survey, whereas the columns are the answers for all questions within a single survey. i am aiming to reduce this dataset to a smaller including only the rows with a certain depicted answer on a certain question, i.e. with all the same value in this column. i am able to determine the index values of all rows with this condition, but i can't find how to delete this rows or make a new df with these rows only.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/11881165/slice-pandas-dataframe-by-row\",\n",
      "        \"example\": [\n",
      "            \"In [36]: df\\nOut[36]:\\n   A  B  C  D\\na  0  2  6  0\\nb  6  1  5  2\\nc  0  2  6  0\\nd  9  3  2  2\\n\\nIn [37]: rows\\nOut[37]: ['a', 'c']\\n\\nIn [38]: df.drop(rows)\\nOut[38]:\\n   A  B  C  D\\nb  6  1  5  2\\nd  9  3  2  2\\n\\nIn [39]: df[~((df.A == 0) & (df.B == 2) & (df.C == 6) & (df.D == 0))]\\nOut[39]:\\n   A  B  C  D\\nb  6  1  5  2\\nd  9  3  2  2\\n\\nIn [40]: df.ix[rows]\\nOut[40]:\\n   A  B  C  D\\na  0  2  6  0\\nc  0  2  6  0\\n\\nIn [41]: df[((df.A == 0) & (df.B == 2) & (df.C == 6) & (df.D == 0))]\\nOut[41]:\\n   A  B  C  D\\na  0  2  6  0\\nc  0  2  6  0\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 11811392,\n",
      "        \"query\": \"how to generate a list from a pandas dataframe with the column name and column values? i have a pandas dataframe object that looks like this: i'd like to generate a list of lists objects where the first item is the column label and the remaining list values are the column data values: how can i do this? thanks for the help.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/11811392/how-to-generate-a-list-from-a-pandas-dataframe-with-the-column-name-and-column-v\",\n",
      "        \"example\": [\n",
      "            \"list(dt.T.itertuples())\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 10982266,\n",
      "        \"query\": \"how to aggregate duplicate timestamps with pandas? i am working on python ( specifically) to analyze a dataset. (python is too awesome, the power of open source is amazing). i am having trouble with a specific part of my dataset. i have the following data set, and it goes on ... i am using pandas to load the data. after this, i would like to be able to do the following, take a volume weighted average of the time there are duplicates. i.e. since there are two asks at time 08:01:16, i would like to calculate the average price based on volume which would be (58.4*60 + 58*60)/(60+60) and an average of the volume on the volume column which would be (60+60)/2.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/10982266/how-to-aggregate-duplicate-timestamps-with-pandas\",\n",
      "        \"example\": [\n",
      "            \"In [28]: a = pd.read_csv('aa.csv')\\n\\nIn [29]: a\\nOut[29]: \\n       time contract ticker    expiry  strike quote  price  volume\\n0  08:01:08        C    PXA  20100101    4000     A   57.8      60\\n1  08:01:11        C    PXA  20100101    4000     A   58.4      60\\n2  08:01:12        C    PXA  20100101    4000     A   58.0      60\\n3  08:01:16        C    PXA  20100101    4000     A   58.4      60\\n4  08:01:16        C    PXA  20100101    4000     A   58.0      60\\n5  08:01:21        C    PXA  20100101    4000     A   58.4      60\\n6  08:01:21        C    PXA  20100101    4000     A   58.0      60\\n\\nIn [30]: pd.DataFrame([{'time': k,\\n                        'price': (v.price * v.volume).sum() / v.volume.sum(),\\n                        'volume': v.volume.mean()}\\n                       for k,v in a.groupby(['time'])],\\n                      columns=['time', 'price', 'volume'])\\nOut[30]: \\n       time  price  volume\\n0  08:01:08   57.8      60\\n1  08:01:11   58.4      60\\n2  08:01:12   58.0      60\\n3  08:01:16   58.2      60\\n4  08:01:21   58.2      60\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 14085517,\n",
      "        \"query\": \"pandas sorting pivot_table or grouping dataframe? i have a problem. i did this: my goal is to have each 'segm1' sorted by 'distribuzione ponderata'. e.g. in 'norm' subset the first row should be \\\"bonta' malat b\\\" with the higher level of 'distribuzione ponderata'. i was able to achieve partially the result using groupby method but without being able to set multiple columns. someone can help me please?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/14085517/pandas-sorting-pivot-table-or-grouping-dataframe\",\n",
      "        \"example\": [\n",
      "            \"import io\\nimport pandas as pd\\nimport numpy as np\\n\\ntext = '''\\\\\\nSEGM1\\\\tDESC\\\\tDistribuzione Ponderata\\\\tRotazioni a volume\\nAD\\\\tACCADINAROLO\\\\t74.040\\\\t140249.693409\\nAD\\\\tZYMIL AMALAT Z\\\\t90.085\\\\t321529.053570\\nFUN\\\\tSPECIALMALAT S\\\\t88.650\\\\t120711.182177\\nNORM\\\\tSTD INNAROLO\\\\t49.790\\\\t162259.216710\\nNORM\\\\tSTD P.NAROLO\\\\t52.125\\\\t1252174.695695\\nNORM\\\\tSTD PLNAROLO\\\\t54.230\\\\t213257.829615\\nNORM\\\\tBONTA' MALAT B\\\\t79.280\\\\t520454.366419\\nNORM\\\\tDA STD RILGARD\\\\t35.290\\\\t554927.497875\\nNORM\\\\tOVANE VT.MANTO\\\\t15.040\\\\t466232.639628\\nNORM\\\\tWEIGHT MALAT W\\\\t79.170\\\\t118628.572692\\n'''\\n\\ndf = pd.read_csv(io.BytesIO(text), delimiter = '\\\\t',\\n                 index_col = (0,1),)\\n\\nkey1 = df.index.labels[0]\\nkey2 = df['Distribuzione Ponderata'].rank(ascending=False)\\nsorter = np.lexsort((key2, key1))\\n\\nsorted_df = df.take(sorter)\\nprint(sorted_df)\\n\",\n",
      "            \"                      Distribuzione Ponderata  Rotazioni a volume\\nSEGM1 DESC                                                       \\nAD    ZYMIL AMALAT Z                   90.085       321529.053570\\n      ACCADINAROLO                     74.040       140249.693409\\nFUN   SPECIALMALAT S                   88.650       120711.182177\\nNORM  BONTA' MALAT B                   79.280       520454.366419\\n      WEIGHT MALAT W                   79.170       118628.572692\\n      STD PLNAROLO                     54.230       213257.829615\\n      STD P.NAROLO                     52.125      1252174.695695\\n      STD INNAROLO                     49.790       162259.216710\\n      DA STD RILGARD                   35.290       554927.497875\\n      OVANE VT.MANTO                   15.040       466232.639628\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 23321300,\n",
      "        \"query\": \"efficient method to filter and add based on certain conditions (3 conditions in this case) i have a data frame which looks like this i have a data frame which looks like this i am currently doing it { } i would like to add the 'd' coloumn and take the average by counting the number of rows without counting 0. for example the first row is (200+300)/2 = 250. currently i am building a list that stores the 'd' coloumn but ideally i want it in the format above. for example first row would look like this is a very inefficient way to do this work. the code takes a long time to run in a loop. so any help is appreciated that makes it run faster. the original data frame has about a million rows.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/23321300/efficient-method-to-filter-and-add-based-on-certain-conditions-3-conditions-in\",\n",
      "        \"example\": [\n",
      "            \"aggregate(d ~ a + b + c, data = df, sum)\\n#   a b c   d\\n# 1 1 1 1 500\\n# 2 1 3 1   0\\n# 3 1 1 2 600\\n# 4 1 2 3 300\\n\",\n",
      "            \"data.table\",\n",
      "            \"library(dplyr)\\ndf %>%\\n  group_by(a, b, c) %>%\\n  summarise(\\n    sum_d = sum(d))\\n\\n# Source: local data frame [4 x 4]\\n# Groups: a, b\\n# \\n#   a b c sum_d\\n# 1 1 1 1   500\\n# 2 1 1 2   600\\n# 3 1 2 3   300\\n# 4 1 3 1     0\\n\",\n",
      "            \"aggregate(d ~ a + b + c, data = df, function(x) mean(x[x > 0]))\\n#   a b c   d\\n# 1 1 1 1 250\\n# 2 1 3 1 NaN\\n# 3 1 1 2 600\\n# 4 1 2 3 150\\n\\ndf %>%\\n  filter(d != 0) %>%\\n  group_by(a, b, c) %>%\\n  summarise(\\n    mean_d = mean(d))\\n\\n#   a b c mean_d\\n# 1 1 1 1    250\\n# 2 1 1 2    600\\n# 3 1 2 3    150\\n\",\n",
      "            \"df$d[df$d == 0] <- NA\\ndf %>%\\n  group_by(a, b, c) %>%\\n  summarise(\\n    mean_d = mean(d, na.rm = TRUE))\\n\\n#   a b c mean_d\\n# 1 1 1 1    250\\n# 2 1 1 2    600\\n# 3 1 2 3    150\\n# 4 1 3 1    NaN\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 34365578,\n",
      "        \"query\": \"dplyr filtering based on two variables i want to use to determine which observations in a dataframe meet the following condition: within each , the combined total of for observations where is greater than the combined total of observations where here's the toy dataframe: so far i've determined that i should be using some combination of ,, and but i can't seem to wrap my head around a good way to do it. here's what i've come up with so far: what next steps should i take? ultimately the analysis should return \\\"a\\\", because it's the only where is greater for the observations than for the observations.\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/34365578/dplyr-filtering-based-on-two-variables\",\n",
      "        \"example\": [\n",
      "            \"> library(tidyr)\\n> df %>% group_by(Group, Var1) %>%\\n+    summarise(Total = sum(Var2)) %>%\\n+    spread(Var1,Total) %>%\\n+    filter(good>bad)\\nSource: local data frame [1 x 3]\\n\\n  Group bad good\\n1     A   9   23\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 14023037,\n",
      "        \"query\": \"reshape a dataframe with pandas how can i reshape this dataframe with pandas ... into the following format? edit: a1 in the last line of the second table is supposed to be a2. as the data is paired (e.g. \\\"before\\\" and \\\"after\\\") i need the columns to be aligned without 'nas'. does not work because does not result in an unique index. i could create an additional column which would result in being unique. is that the only way to go?\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/14023037/reshape-a-dataframe-with-pandas\",\n",
      "        \"example\": [\n",
      "            \"In [234]: tmp = DataFrame(\\n    {'id':[1,2,3,4,5,6], \\n     'col1':['A1','A1','A1','A1','A2','A2'],\\n     'col2':['B1','B1','B2','B2','B1','B2'],\\n     'col3':['before','after','before','after','before','after'],\\n     'value':[20,13,11,21,18,22]},\\n    columns=['id','col1','col2','col3','value'])\\n\",\n",
      "            \"In [236]: pivoted = pd.pivot_table(tmp, values='value',\\n                                        rows=['col1','col2'],\\n                                        cols=['col3'])\\nIn [237]: pivoted\\nOut[237]:\\ncol3       after  before\\ncol1 col2\\nA1   B1       13      20\\n     B2       21      11\\nA2   B1      NaN      18\\n     B2       22     NaN\\n\",\n",
      "            \"In [238]: pivoted = pivoted.fillna(method='bfill').dropna()\\nOut[238]:\\ncol3       after  before\\ncol1 col2\\nA1   B1       13      20\\n     B2       21      11\\nA2   B1       22      18\\n\\nIn [245]: pivoted.reset_index()\\nOut[245]:\\ncol3 col1 col2  after  before\\n0      A1   B1     13      20\\n1      A1   B2     21      11\\n2      A2   B1     22      18\\n\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 13647222,\n",
      "        \"query\": \"plotting data lines from pandas dataframe, identified by keys in one column instead by different columns i'm running a simulation which gives me a csv file corresponding in structure to the following pandas data frame: given this , how can i plot the pairs for each series? that is, i would like to get the image which would be easy to get (just if i could get my dataframe into the format but i don't see how i would do the transformation \\u2192 or obtain the plot from (which is what i get from my simulation) in any other way. i could change my simulation code to write the corresponding data into in my case 16 separate table columns instead of one and one column, but that 16 is the value for most simulations, some use not all of those series and in the future i may need to split those further, so this does not look like the best solution to me. to generate both example dataframes, i went in the other direction and did\",\n",
      "        \"apis\": [],\n",
      "        \"link\": \"https://stackoverflow.com/questions/13647222/plotting-data-lines-from-pandas-dataframe-identified-by-keys-in-one-column-inst\",\n",
      "        \"example\": [\n",
      "            \"In [5]: df2 = DataFrame(dict(\\n                 (L, df[df['series'] == L]['value'].values)\\n                       for L in df['series'].unique()))\\n\\nIn [6]: df2\\nOut[6]: \\n   A  B  C\\n0  0  0  5\\n1  1  0  4\\n2  2  1  3\\n3  3  2  2\\n4  4  4  1\\n\",\n",
      "            \"'step'\",\n",
      "            \"In [7]: df2.index.name = 'step'\\nOut[7]: \\n      A  B  C\\nstep         \\n0     0  0  5\\n1     1  0  4\\n2     2  1  3\\n3     3  2  2\\n4     4  4  1\\n\\nIn [8]: df2.plot()\\n\"\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(test_examples, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9d794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
