{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "post_data = json.load(open('ten_thousand_questions/questions_with_bodies.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['items', 'has_more', 'quota_max', 'quota_remaining'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_answers = []\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "post_data['items'][0]['is_answered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9de75a29c7342a1a472e7aa74b42958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(post_data['items']):\n",
    "    if 'accepted_answer_id' in item:\n",
    "        accepted_answers.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accepted_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stackapi import StackAPI\n",
    "\n",
    "SITE = StackAPI('stackoverflow')\n",
    "aids = [i['accepted_answer_id'] for i in accepted_answers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backoff': 0, 'has_more': False, 'page': 1, 'quota_max': 300, 'quota_remaining': 295, 'total': 0, 'items': [{'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 3, 'last_activity_date': 1625407195, 'creation_date': 1625407195, 'answer_id': 68245456, 'question_id': 68245443, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>try:</p>\\n<pre><code>df['counter']=df.index//3\\n</code></pre>\\n<p>OR</p>\\n<p>If you have a custom index then you can use:</p>\\n<pre><code>df['counter']=[x//3 for x in range(len(df))]\\n</code></pre>\\n<p>output of <code>df</code>:</p>\\n<pre><code>    key     team    counter\\n0   K0      a       0\\n1   K1      d       0\\n2   K2      w       0\\n3   K3      a       1\\n4   K4      c       1\\n5   K5      s       1\\n6   K6      x       2\\n7   K7      d       2\\n8   K8      a       2\\n9   K9      f       3\\n10  K10     e       3\\n11  K11     r       3\\n</code></pre>\\n\"}, {'owner': {'reputation': 4635, 'user_id': 2106934, 'user_type': 'registered', 'accept_rate': 89, 'profile_image': 'https://www.gravatar.com/avatar/7d29cd79756208cfd1a7c814c477ddd6?s=128&d=identicon&r=PG', 'display_name': 'Utsav', 'link': 'https://stackoverflow.com/users/2106934/utsav'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625395151, 'last_edit_date': 1625395151, 'creation_date': 1625389123, 'answer_id': 68243106, 'question_id': 68243003, 'content_license': 'CC BY-SA 4.0', 'body': \"<p><strong>Input df</strong></p>\\n<pre><code>    Name    date    address\\n0   A   2021-07-03  X\\n1   B   2021-07-03  Y\\n2   C   2021-07-03  Z\\n3   D   2021-08-01  M\\n4   E   2021-08-01  N\\n5   F   2021-08-01  O\\n</code></pre>\\n<p><strong>If date col is object/string type and we want to keep it as is</strong></p>\\n<pre><code>search = '2021-07'\\nmonth_dataframe = df[df[&quot;date&quot;].str.contains(str(search))]\\nmonth_dataframe\\n</code></pre>\\n<p><strong>If date col is object/str type and we are okay with converting it to datetime</strong></p>\\n<p>Benefit in this case is we don't need to define variable <code>search = '2021-07'</code> and this solution will work for every(current) month.</p>\\n<pre><code>df['date'] = pd.to_datetime(df.date)\\nmonth_dataframe =  df[df.date.dt.month == pd.Timestamp('today').month]\\nmonth_dataframe\\n</code></pre>\\n<p><strong>If date col is datetime type</strong></p>\\n<p>(Just an option, not the best way)</p>\\n<pre><code>search = '2021-07'\\nmonth_dataframe = df[df[&quot;date&quot;].astype(str).str.contains(str(search))]\\nmonth_dataframe\\n</code></pre>\\n<p><strong>Output</strong></p>\\n<pre><code>    Name    date    address\\n0   A   2021-07-03  X\\n1   B   2021-07-03  Y\\n2   C   2021-07-03  Z\\n</code></pre>\\n\"}, {'owner': {'reputation': 12124, 'user_id': 9332187, 'user_type': 'registered', 'profile_image': 'https://lh3.googleusercontent.com/-I_opr6uwXKQ/AAAAAAAAAAI/AAAAAAAAADw/ZJMBGmRMJCM/photo.jpg?sz=128', 'display_name': 'Mustafa AydÄ±n', 'link': 'https://stackoverflow.com/users/9332187/mustafa-ayd%c4%b1n'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625393691, 'creation_date': 1625393691, 'answer_id': 68243655, 'question_id': 68243538, 'content_license': 'CC BY-SA 4.0', 'body': '<p>You can use <code>loc</code> to get the locations where <code>id</code> column equals to <code>&quot;Na&quot;</code> and put your list in there:</p>\\n<pre><code>df.loc[df.id.eq(&quot;Na&quot;), &quot;id&quot;] = correct_id\\n</code></pre>\\n<p>If you want them integers at the end, you can use <code>astype</code>:</p>\\n<pre><code>df.id = df.id.astype(int)\\n</code></pre>\\n<p>to get</p>\\n<pre><code>&gt;&gt;&gt; df\\n\\n           movieName    id  Year\\n0            birdman  1987  2010\\n1  avengers: endgame  4563  2020\\n2           deadpool  3452  2012\\n3      The bird King  1230  2018\\n4            Bla bla  6547  2013\\n5          Lion King  9384  2020\\n</code></pre>\\n'}, {'owner': {'reputation': 6318, 'user_id': 6660638, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/04d9ccc9f5df2357d1edc8c265493eef?s=128&d=identicon&r=PG&f=1', 'display_name': 'Epsi95', 'link': 'https://stackoverflow.com/users/6660638/epsi95'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625393625, 'creation_date': 1625393625, 'answer_id': 68243644, 'question_id': 68243543, 'content_license': 'CC BY-SA 4.0', 'body': '<p>You can try <code>regex</code></p>\\n<pre class=\"lang-py prettyprint-override\"><code>import re\\nimport pandas as pd\\n\\ns = &quot;Key=xxxx, age=11, key=yyyy , age=22,Key=zzzz, age=01, key=qqqq, age=21,Key=wwwww, age=91, key=pppp, age=22&quot;\\n\\ndf = pd.DataFrame(zip(re.findall(r\\'Key=([^,\\\\s]+)\\', s, re.IGNORECASE), re.findall(r\\'age=([^,\\\\s]+)\\', s, re.IGNORECASE)),\\n                 columns=[\\'key\\', \\'age\\'])\\n\\ndf\\n</code></pre>\\n<pre><code>     key    age\\n0   xxxx    11\\n1   yyyy    22\\n2   zzzz    01\\n3   qqqq    21\\n4   wwwww   91\\n5   pppp    22\\n</code></pre>\\n'}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625389695, 'creation_date': 1625389695, 'answer_id': 68243166, 'question_id': 68243146, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>via <code>mask()</code>:</p>\\n<pre><code>df['id']=df['id'].mask(df['id'].eq(0),df['ref'])\\n</code></pre>\\n<p>OR</p>\\n<p>via numpy's <code>where()</code>:</p>\\n<pre><code>#import numpy as np\\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\\n</code></pre>\\n\"}, {'owner': {'reputation': 37498, 'user_id': 7212686, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://i.stack.imgur.com/Ey8ll.png?s=128&g=1', 'display_name': 'azro', 'link': 'https://stackoverflow.com/users/7212686/azro'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625387071, 'creation_date': 1625387071, 'answer_id': 68242911, 'question_id': 68242870, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>As you do <code>count = </code>, you erase the previous value, you want to sum up the different counts</p>\\n<pre><code>def func(stringans):\\n    count = 0\\n    for x in ai_tech:\\n        count += stringans.count(x)\\n    return count\\n\\n# with sum and generator \\ndef func(stringans):\\n    return sum(stringans.count(x) for x in ai_tech)\\n</code></pre>\\n<p>Fixing some typos in <code>ai_tech</code> and setting all to <code>.lower()</code> gives <code>2,1,0</code> in the counter col, the last row has no value in common</p>\\n<pre><code>import pandas as pd\\n\\nai_tech = [&quot;natural language processing&quot;, &quot;nlp&quot;, &quot;A I &quot;, &quot;Artificial intelligence&quot;,\\n           &quot;stemming&quot;, &quot;lemmatization&quot;, &quot;information extraction&quot;,\\n           &quot;text mining&quot;, &quot;text analytics&quot;, &quot;data - mining&quot;]\\n\\ndf = pd.DataFrame([[&quot;1. More details  A I   Artificial Intelligence&quot;], [&quot;2. NLP works very well these days&quot;],\\n                   [&quot;3. receiving information at the right time&quot;]], columns=[&quot;text&quot;])\\n\\ndef func(stringans):\\n    return sum(stringans.lower().count(x.lower()) for x in ai_tech)\\n\\ndf['counter'] = df['text'].apply(func)\\nprint(df)\\n\\n# ------------------\\n                                             text  counter\\n0  1. More details  A I   Artificial Intelligence        2\\n1               2. NLP works very well these days        1\\n2      3. receiving information at the right time        0\\n</code></pre>\\n\"}, {'owner': {'reputation': 10379, 'user_id': 6633975, 'user_type': 'registered', 'profile_image': 'https://lh6.googleusercontent.com/-_Ta3cjF1uhk/AAAAAAAAAAI/AAAAAAAAAk8/V1id8gyx0PE/photo.jpg?sz=128', 'display_name': 'Sayali Sonawane', 'link': 'https://stackoverflow.com/users/6633975/sayali-sonawane'}, 'is_accepted': True, 'score': 15, 'last_activity_date': 1625380793, 'last_edit_date': 1625380793, 'creation_date': 1473325350, 'answer_id': 39386767, 'question_id': 39386458, 'content_license': 'CC BY-SA 4.0', 'body': '<p>As you are trying to process 85GB CSV file, if you will try to read all the data by breaking it into chunks and converting it into dataframe then it will hit memory limit for sure. You can try to solve this problem by using different approach. In this case, you can use filtering operations on your data. For example, if there are 600 columns in your dataset and you are interested only in 50 columns. Try to read only 50 columns from the file. This way you will save lot of memory. Process your rows as you read them. If you need to filter the data first, use a generator function. <code>yield</code> makes a function a generator function, which means it won\\'t do any work until you start looping over it.</p>\\n<p>For more information regarding generator function:\\n<a href=\"https://stackoverflow.com/questions/17444679/reading-a-huge-csv-in-python\">Reading a huge .csv file</a></p>\\n<p>For efficient filtering refer: <a href=\"https://codereview.stackexchange.com/questions/88885/efficiently-filter-a-large-100gb-csv-file-v3\">https://codereview.stackexchange.com/questions/88885/efficiently-filter-a-large-100gb-csv-file-v3</a></p>\\n<p>For processing smaller dataset:</p>\\n<p><strong>Approach 1: To convert reader object to dataframe directly:</strong></p>\\n<pre><code>full_data = pd.concat(TextFileReader, ignore_index=True)\\n</code></pre>\\n<p>It is necessary to add parameter <a href=\"http://pandas.pydata.org/pandas-docs/stable/merging.html#ignoring-indexes-on-the-concatenation-axis\" rel=\"nofollow noreferrer\">ignore index</a> to function concat, because avoiding duplicity of indexes.</p>\\n<p><strong>Approach 2:</strong> <strong>Use Iterator or get_chunk to convert it into dataframe.</strong></p>\\n<p>By specifying a chunksize to read_csv,return value will be an iterable object of type TextFileReader.</p>\\n<pre><code>df=TextFileReader.get_chunk(3)\\n\\nfor chunk in TextFileReader:\\n    print(chunk)\\n</code></pre>\\n<p>Source : <a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking\" rel=\"nofollow noreferrer\">http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking</a></p>\\n<p><code>df= pd.DataFrame(TextFileReader.get_chunk(1))</code></p>\\n<p>This will convert one chunk to dataframe.</p>\\n<p><strong>Checking total number of chunks in TextFileReader</strong></p>\\n<pre class=\"lang-py prettyprint-override\"><code>for chunk_number, chunk in enumerate(TextFileReader):\\n    # some code here, if needed\\n    pass\\n\\nprint(&quot;Total number of chunks is&quot;, chunk_number+1)\\n</code></pre>\\n<p>If file size is bigger,I won\\'t recommend second approach. For example, if csv file consist of 100000 records then chunksize=5 will create 20,000 chunks.</p>\\n'}, {'owner': {'reputation': 40864, 'user_id': 12833166, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/p1a2a.jpg?s=128&g=1', 'display_name': 'Shubham Sharma', 'link': 'https://stackoverflow.com/users/12833166/shubham-sharma'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625379551, 'creation_date': 1625379551, 'answer_id': 68242225, 'question_id': 68240448, 'content_license': 'CC BY-SA 4.0', 'body': \"<h3>Simple pandas solution</h3>\\n<pre><code>s = df.stack()\\ns[s.str.contains(r'\\\\d?,\\\\d+', na=False)] = np.hstack(values)\\nout = s.unstack().reindex(df.columns, axis=1)\\n</code></pre>\\n<h3>Explanation</h3>\\n<ul>\\n<li><code>stack</code> the dataframe to reshape. Note: Stacking operation also drops the <code>NaN</code> values by default.</li>\\n</ul>\\n<pre><code>&gt;&gt;&gt; df.stack()\\n\\n0  Unnamed: 1    Ore\\n   2Gi           ,30\\n   3Ve           ,46\\n   4Sa           ,50\\n   ...\\n   9Me           ,07\\n   0Gi           ,11\\n   1Ve           ,59\\n1  Unnamed: 1    Ore\\n   6Lu           ,53\\ndtype: object\\n</code></pre>\\n<ul>\\n<li>Match the regular expression pattern (<code>\\\\d?,\\\\d+</code>) against the stacked frame using <code>str.contains</code>, this essentially creates a boolean mask</li>\\n</ul>\\n<pre><code>&gt;&gt;&gt; s.str.contains(r'\\\\d?,\\\\d+', na=False)\\n\\n0  Unnamed: 1    False\\n   2Gi            True\\n   3Ve            True\\n   4Sa            True\\n   ...\\n   9Me            True\\n   0Gi            True\\n   1Ve            True\\n1  Unnamed: 1    False\\n   6Lu            True\\ndtype: bool\\n</code></pre>\\n<ul>\\n<li>Using <code>hstack</code> flatten the <code>values</code> then assign these values to the matched strings in the stacked frame</li>\\n</ul>\\n<pre><code>&gt;&gt;&gt; s[s.str.contains(r'\\\\d?,\\\\d+', na=False)] = np.hstack(values)\\n&gt;&gt;&gt; s\\n\\n0  Unnamed: 1     Ore\\n   2Gi           6,30\\n   3Ve           5,46\\n   4Sa           4,50\\n   ...\\n   9Me           5,07\\n   0Gi           6,11\\n   1Ve           2,59\\n1  Unnamed: 1     Ore\\n   6Lu           2,53\\ndtype: object\\n</code></pre>\\n<ul>\\n<li>Now <code>unstack</code> to reshape back into a dataframe and <code>reindex</code> the columns</li>\\n</ul>\\n<pre><code>&gt;&gt;&gt; s.unstack().reindex(df.columns, axis=1)\\n\\n   Unnamed: 0 Unnamed: 1  1Me   2Gi   3Ve   4Sa  5Do   6Lu   7Ma   8Me   9Gi   0Ve   1Sa   2Do  3Lu   4Ma   5Me   6Gi   7Ve   8Sa  9Do   0Lu   1Ma   2Me   3Gi   4Ve   5Sa  6Do   7Lu   8Ma   9Me   0Gi   1Ve  Unnamed: 2\\n0         NaN        Ore  NaN  6,30  5,46  4,50  NaN   NaN  5,20  7,48  5,41  2,07  3,52  3,11  NaN  4,53  4,51  5,14  4,28  3,33  NaN  5,32  3,10  5,03  4,44  4,39  5,04  NaN  5,26  7,15  5,07  6,11  2,59         NaN\\n1         NaN        Ore  NaN   NaN   NaN   NaN  NaN  2,53   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   NaN         NaN\\n</code></pre>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625376700, 'last_edit_date': 1625376700, 'creation_date': 1625376087, 'answer_id': 68241939, 'question_id': 68241764, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>try via <code>map()</code> and <code>groupby()</code>:</p>\\n<pre><code>df2['Word Count']=df2['Character'].map(df1.groupby('Character')['Word Count'].sum())\\n#you can also use replace() method in place of map()\\n</code></pre>\\n<p>output of <code>df2</code>:</p>\\n<pre><code>    Character       Line Count  Word Count\\n0   Leslie Knope    81          71\\n1   Child           1           72\\n</code></pre>\\n\"}, {'owner': {'reputation': 571, 'user_id': 16317892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/Y41Fa.png?s=128&g=1', 'display_name': 'Hammurabi', 'link': 'https://stackoverflow.com/users/16317892/hammurabi'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625358586, 'creation_date': 1625358586, 'answer_id': 68240855, 'question_id': 68240743, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Your dataframe has the dates as an index, so reset index to pull them out into a column.  If the index was previously named something other than 'index', use that name instead of 'index' if you choose to rename it.</p>\\n<pre><code>df = pd.DataFrame([[2], [4]], columns=['order'])\\n\\ndf = df.reset_index()\\ndf = df.rename(columns={'index':'date'})\\nprint(df)\\n#    date  order\\n# 0     0      2\\n# 1     1      4\\n\\n</code></pre>\\n\"}, {'owner': {'reputation': 174150, 'user_id': 4983450, 'user_type': 'registered', 'accept_rate': 74, 'profile_image': 'https://i.stack.imgur.com/NR2ko.jpg?s=128&g=1', 'display_name': 'Psidom', 'link': 'https://stackoverflow.com/users/4983450/psidom'}, 'is_accepted': True, 'score': 3, 'last_activity_date': 1625355542, 'creation_date': 1625355542, 'answer_id': 68240672, 'question_id': 68240645, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html\" rel=\"nofollow noreferrer\"><code>wide_to_long</code></a>:</p>\\n<pre><code>pd.wide_to_long(\\n  df, \\n  [\\'Quan1\\', \\'Quan2\\'], \\n  i=[\\'Year\\', \\'code\\', \\'Country\\'], \\n  j=\\'month\\', \\n  suffix=\\'\\\\w+\\'\\n).reset_index()\\n\\n#   Year  code Country month  Quan1  Quan2\\n#0  2020  8123   Japan   jan    500     26\\n#1  2020  8123   Japan   feb    400     28\\n#2  2020  8123  Taiwan   jan    450    245\\n#3  2020  8123  Taiwan   feb   4500     87\\n</code></pre>\\n'}, {'owner': {'reputation': 26, 'user_id': 4601680, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/771780b47cde3d98b163116fd825d2c1?s=128&d=identicon&r=PG&f=1', 'display_name': 'Vishwa', 'link': 'https://stackoverflow.com/users/4601680/vishwa'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625330400, 'creation_date': 1625330400, 'answer_id': 68238169, 'question_id': 68230652, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Use group by and then filter on those countries something like below:</p>\\n<pre><code>t=df.groupby(['location-name']).count().reset_index()\\ndf_filtr=df[df['location-name'].isin(t[t['location-id']&gt;1]['location-name'])]\\n\\n</code></pre>\\n<p>You can sort by countries to find the correct entry</p>\\n\"}, {'owner': {'reputation': 40864, 'user_id': 12833166, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/p1a2a.jpg?s=128&g=1', 'display_name': 'Shubham Sharma', 'link': 'https://stackoverflow.com/users/12833166/shubham-sharma'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625323360, 'creation_date': 1625323360, 'answer_id': 68237244, 'question_id': 68237180, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>We can use <code>value_counts</code> after flattening the columns <code>tokens</code> and <code>labels</code> using <code>hstack</code></p>\\n<pre><code>t = np.hstack(df['tokens'])\\nl = np.hstack(df['labels'])\\n</code></pre>\\n<p>Count of each label</p>\\n<pre><code>pd.value_counts(l)\\n\\nB    3\\nA    3\\nD    1\\nC    1\\ndtype: int64\\n</code></pre>\\n<p>Count tokens under each label</p>\\n<pre><code>pd.DataFrame(zip(l, t)).value_counts()\\n\\n0  1   \\nA  Hi      2\\n   fine    1\\nB  am      1\\n   how     1\\n   say     1\\nC  I       1\\nD  Ila     1\\ndtype: int64\\n</code></pre>\\n\"}, {'owner': {'reputation': 9015, 'user_id': 8228558, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/fd59fcdaa6fef158fce4b6d372998521?s=128&d=identicon&r=PG&f=1', 'display_name': 'IoaTzimas', 'link': 'https://stackoverflow.com/users/8228558/ioatzimas'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625323218, 'creation_date': 1625323218, 'answer_id': 68237224, 'question_id': 68237066, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>You can merge the datasets:</p>\\n<pre><code>df = Dataset_1.merge(Dataset_2, left_on='company_id', right_on='org_number', how='left')\\n\\ndf['lower_year']=np.where(df.yr&lt;=df.year, 1, 0)\\n\\ndf = df[(df.lower_year==1) | (pd.isna(df.year))]\\n\\ndf.reset_index(drop=True, inplace=True)\\n\\ndf.drop(['org_number', 'year'], axis=1)\\n\\nprint(df)\\n</code></pre>\\n<p>Output:</p>\\n<pre><code>  company_id    yr  lower_year\\n0        111  2012           1\\n1        111  2014           1\\n2        223  2020           0\\n3        444  1843           1\\n</code></pre>\\n<p>Let me know if you need explanation about the code</p>\\n\"}, {'owner': {'reputation': 37498, 'user_id': 7212686, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://i.stack.imgur.com/Ey8ll.png?s=128&g=1', 'display_name': 'azro', 'link': 'https://stackoverflow.com/users/7212686/azro'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625320222, 'creation_date': 1625320222, 'answer_id': 68236863, 'question_id': 68236800, 'content_license': 'CC BY-SA 4.0', 'body': '<p>To <a href=\"https://stackoverflow.com/questions/28457149/how-to-map-a-function-using-multiple-columns-in-pandas\">map a function using multiple columns</a> you may use <code>DataFrame.apply(,axis=1)</code></p>\\n<pre><code>df = pd.DataFrame([[None, &quot;A&quot;, &quot;Galaxy Puzzle&quot;, &quot;TRUE&quot;],\\n                   [None, &quot;B&quot;, &quot;Car Toy&quot;, &quot;TRUE&quot;],\\n                   [&quot;4EB3D322-7A25-4A67-9053-28955383DD1F&quot;, &quot;B&quot;, &quot;Car Toy&quot;, &quot;FALSE&quot;]],\\n                  columns=[&quot;ID&quot;, &quot;Type&quot;, &quot;Name&quot;, &quot;Enabled&quot;])\\n\\nlookup = [{&quot;ID&quot;: &quot;70B52DA6-F099-4D01-BBD0-03EA97292C26&quot;, &quot;Type&quot;: &quot;A&quot;, &quot;Name&quot;: &quot;Galaxy Puzzle&quot;},\\n          {&quot;ID&quot;: &quot;442B1598-20CF-4425-8A28-0438FBF77C46&quot;, &quot;Type&quot;: &quot;B&quot;, &quot;Name&quot;: &quot;Car&quot;}]\\n\\nlookup = {(entity[&quot;Name&quot;], entity[\\'Type\\']): entity[&quot;ID&quot;] for entity in lookup}\\n\\ndf.loc[df[&quot;ID&quot;].isnull(), &quot;ID&quot;] = df[df[&quot;ID&quot;].isnull()] \\\\\\n    .apply(lambda row: lookup.get((row[\\'Name\\'], row[\\'Type\\'])), axis=1)\\nprint(df)\\n</code></pre>\\n<p>Giving the following (I changed a type in the <code>lookup</code> so you see it is well handled)</p>\\n<pre><code>                                     ID Type           Name Enabled\\n0  70B52DA6-F099-4D01-BBD0-03EA97292C26    A  Galaxy Puzzle    TRUE\\n1                                  None    B        Car Toy    TRUE\\n2  4EB3D322-7A25-4A67-9053-28955383DD1F    B        Car Toy   FALSE\\n</code></pre>\\n'}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 3, 'last_activity_date': 1625314202, 'last_edit_date': 1625314202, 'creation_date': 1625313915, 'answer_id': 68236099, 'question_id': 68236070, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>try via <code>agg()</code>,<code>to_frame()</code> and <code>reset_index()</code>:</p>\\n<pre><code>out=(df.agg(' '.join)\\n        .to_frame('text')\\n        .reset_index(drop=True))\\n</code></pre>\\n<p>output of out:</p>\\n<pre><code>    text\\n0   Hi how are you I am fine I love you I hate you\\n</code></pre>\\n\"}, {'owner': {'reputation': 40864, 'user_id': 12833166, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/p1a2a.jpg?s=128&g=1', 'display_name': 'Shubham Sharma', 'link': 'https://stackoverflow.com/users/12833166/shubham-sharma'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625310430, 'creation_date': 1625310430, 'answer_id': 68235670, 'question_id': 68235334, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Create triplets of <code>Node</code>, <code>ComponentId</code> and <code>Component</code> by enumerating over the connected component list, then create a new dataframe from these triplets and <code>merge</code> it with the given dataframe on <code>Node</code></p>\\n<pre><code>df = pd.DataFrame([(n, i, c) for i,c in enumerate(Gcc, 1) for n in c], \\n                        columns=['Node', 'ComponentID', 'Components'])\\n\\ndata = data.merge(df, on='Node')\\n</code></pre>\\n<p>Alternatively you can use <code>map</code> instead of <code>merge</code> to individually create <code>ComponentID</code> and <code>Components</code> columns</p>\\n<pre><code>d = dict(enumerate(Gcc, 1))\\ndata['ComponentID'] = data['Node'].map({n:i for i,c in d.items() for n in c})\\ndata['Components']  = data['ComponentID'].map(d)\\n</code></pre>\\n<hr />\\n<pre><code>print(data)\\n\\n   Node  degree  ComponentID    Components\\n1     1       2            1  {0, 1, 2, 3}\\n2     2       2            1  {0, 1, 2, 3}\\n5    11       2            2  {10, 11, 12}\\n0     0       1            1  {0, 1, 2, 3}\\n3     3       1            1  {0, 1, 2, 3}\\n4    10       1            2  {10, 11, 12}\\n6    12       1            2  {10, 11, 12}\\n</code></pre>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 3, 'last_activity_date': 1625305645, 'last_edit_date': 1625305645, 'creation_date': 1625304033, 'answer_id': 68234871, 'question_id': 68234845, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>try via <code>pivot_table()</code>:</p>\\n<pre><code>out=df.pivot_table('Amount','ID','Type',fill_value=0,aggfunc=['sum','count'])\\n</code></pre>\\n<p>Then:</p>\\n<pre><code>out=(out.swaplevel(axis=1)\\n        .rename(columns=lambda x:'_FREQ' if x=='count' else '',level=1)\\n        .rename_axis(columns=[None,None])\\n        .reset_index())\\n</code></pre>\\n<p>Finally:</p>\\n<pre><code>out.columns=out.columns.map(''.join)\\n</code></pre>\\n<p>output of <code>out</code>:</p>\\n<pre><code>    ID  CR  DB  CR_FREQ     DB_FREQ\\n0   1   63  40  2           1\\n1   2   99  0   1           0\\n</code></pre>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625293175, 'last_edit_date': 1625293175, 'creation_date': 1625292189, 'answer_id': 68233650, 'question_id': 68233487, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Try:</p>\\n<pre><code>d1=df.groupby('subject_id').filter(lambda x: x['test_id'].count() == 0)\\nd2=df.groupby('test_id').filter(lambda x: x['test_name'].nunique() &gt; 1)\\nd3=df[df['invalid_condition']==1]\\n#your conditions\\n\\ndata_inconsistencies_df = pd.DataFrame([[(len(d1)*100/len(df)),(len(d2)*100/len(df)),(len(d3)*100/len(df))]],columns = ['subject_with_no_test_info','test_id_diff_names','invalid_condition'])\\n#created dataframe to show percentages\\n\\nd2=d2.drop_duplicates('test_id',keep='last')\\n          #^your sub condition\\n          #(We don't drop  101, because we keep = first of duplicate test ids)\\nto_drop=pd.concat([d1,d2,d3]).index\\n#concatinating 3 dataframes to grab the index of the rows which are going to drop\\n</code></pre>\\n<p>Finally:</p>\\n<pre><code>df=df.drop(to_drop)\\n#dropping those indexes\\n</code></pre>\\n<p>Output of <code>df</code>:</p>\\n<pre><code>  subject_id    test_id     test_name   invalid_condition\\n0   101         21.0            A           0\\n3   201         24.0            D           0\\n</code></pre>\\n<p>Output of <code>data_inconsistencies_df</code>:</p>\\n<pre><code>  subject_with_no_test_info     test_id_diff_names  invalid_condition\\n0          20.0                         40.0            20.0\\n</code></pre>\\n\"}, {'owner': {'reputation': 16385, 'user_id': 15497888, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/W1on0.jpg?s=128&g=1', 'display_name': 'Henry Ecker', 'link': 'https://stackoverflow.com/users/15497888/henry-ecker'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625280514, 'last_edit_date': 1625280514, 'creation_date': 1625279880, 'answer_id': 68232816, 'question_id': 68232782, 'content_license': 'CC BY-SA 4.0', 'body': '<p>One option is to <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>stack</code></a> <code>df1</code> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html\" rel=\"nofollow noreferrer\"><code>reindex</code></a> from <code>well</code>:</p>\\n<pre><code>df3 = df1.stack()\\ndf3.index = df3.index.map(lambda s: \\'\\'.join(map(str, s)))\\ndf3 = df3.reindex(df2[\\'well\\']).reset_index(name=\\'Value\\')\\n</code></pre>\\n<p><code>df3</code>:</p>\\n<pre><code>  Well  Value\\n0   A1    237\\n1   A2    543\\n2   A3    300\\n3   B1    435\\n4   B2    313\\n5   B3    150\\n</code></pre>\\n<p>DataFrames used:</p>\\n<pre><code>import pandas as pd\\n\\ndf1 = pd.DataFrame({\\n    1: {\\'A\\': 237, \\'B\\': 435}, 2: {\\'A\\': 543, \\'B\\': 313},\\n    3: {\\'A\\': 300, \\'B\\': 150}, 4: {\\'A\\': 256, \\'B\\': 635},\\n    5: {\\'A\\': 343, \\'B\\': 847}, 6: {\\'A\\': 122, \\'B\\': 321}\\n})\\n\\ndf2 = pd.DataFrame({\\'well\\': [\\'A1\\', \\'A2\\', \\'A3\\', \\'B1\\', \\'B2\\', \\'B3\\']})\\n</code></pre>\\n<hr />\\n<p>Explanations:</p>\\n<ol>\\n<li>Stack produces a series:</li>\\n</ol>\\n<pre><code>df3 = df1.stack()\\n</code></pre>\\n<p><code>df3</code>:</p>\\n<pre><code>A  1    237\\n   2    543\\n   3    300\\n   4    256\\n   5    343\\n   6    122\\nB  1    435\\n   2    313\\n   3    150\\n   4    635\\n   5    847\\n   6    321\\ndtype: int64\\n</code></pre>\\n<hr />\\n<ol start=\"2\">\\n<li>Collapse the MultiIndex into a single index with <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Index.map.html\" rel=\"nofollow noreferrer\"><code>Index.map</code></a>:</li>\\n</ol>\\n<pre><code>df3.index = df3.index.map(lambda s: \\'\\'.join(map(str, s)))\\n</code></pre>\\n<p><code>df3</code>:</p>\\n<pre><code>A1    237\\nA2    543\\nA3    300\\nA4    256\\nA5    343\\nA6    122\\nB1    435\\nB2    313\\nB3    150\\nB4    635\\nB5    847\\nB6    321\\ndtype: int64\\n</code></pre>\\n<hr />\\n<ol start=\"3\">\\n<li>Grab <code>well</code> from <code>df2</code> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html\" rel=\"nofollow noreferrer\"><code>reindex</code></a>:</li>\\n</ol>\\n<pre><code>df3 = df3.reindex(df2[\\'well\\'])\\n</code></pre>\\n<pre><code>Well\\nA1    237\\nA2    543\\nA3    300\\nB1    435\\nB2    313\\nB3    150\\ndtype: int64\\n</code></pre>\\n<p>*If well is a <code>list</code> as it appears in the OP turn it into a <code>Series</code> with the name <code>well</code> and <code>reindex</code> from that instead:</p>\\n<pre><code>well = [\\'A1\\', \\'A2\\', \\'A3\\', \\'B1\\', \\'B2\\', \\'B3\\']\\ndf3 = df3.reindex(pd.Series(well, name=\\'well\\'))\\n</code></pre>\\n<p>Or <code>reindex</code> directly from the <code>list</code> but then the axis will need renamed before resetting the index:</p>\\n<pre><code>well = [\\'A1\\', \\'A2\\', \\'A3\\', \\'B1\\', \\'B2\\', \\'B3\\']\\ndf3 = df3.reindex(well).rename_axis(index=\\'Well\\')\\n</code></pre>\\n<hr />\\n<ol start=\"4\">\\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reset_index.html\" rel=\"nofollow noreferrer\"><code>reset_index</code></a> to convert to a DataFrame (and give the <code>Value</code> column a name):</li>\\n</ol>\\n<pre><code>df3 = df3.reindex(df2[\\'well\\']).reset_index(name=\\'Value\\')\\n</code></pre>\\n<p><code>df3</code>:</p>\\n<pre><code>  Well  Value\\n0   A1    237\\n1   A2    543\\n2   A3    300\\n3   B1    435\\n4   B2    313\\n5   B3    150\\n</code></pre>\\n'}, {'owner': {'reputation': 238634, 'user_id': 14122, 'user_type': 'registered', 'accept_rate': 71, 'profile_image': 'https://www.gravatar.com/avatar/5e2861b08f37fa306fbf5384994af688?s=128&d=identicon&r=PG', 'display_name': 'Charles Duffy', 'link': 'https://stackoverflow.com/users/14122/charles-duffy'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625269471, 'last_edit_date': 1625269471, 'creation_date': 1625268494, 'answer_id': 68232150, 'question_id': 68232120, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Instead of having <code>Short1</code> and <code>Short5</code> be separate variables, have there just be <em>one</em> <code>Shorts</code> dictionary, and have <code>1</code>, <code>5</code>, etc be keys. Thus:</p>\\n<pre><code>MySet = [1, 5, 15, 30, 60, 240, 360, 720, 1440, 10080]\\nShorts = {}\\nLongs = {}\\nMACDs = {}\\nSignals = {}\\n\\nfor val in MySet:\\n  Shorts[val] = df.Close.ewm(span=12 * val, adjust=False).mean()\\n  Longs[val] = df.Close.ewm(span=26 * val, adjust=False).mean()\\n  MACDs[val] = Shorts[val] - Longs[val]\\n  Signals[val] = MACDs[val].ewm(span=9 * val, adjust=False).mean()\\n\\n  df[f'MACD{val}'] = MACDs[val]\\n  df[f'Signal Line{val}'] = Signals[val]\\n</code></pre>\\n\"}, {'owner': {'reputation': 319, 'user_id': 9472575, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/1hhcj.jpg?s=128&g=1', 'display_name': 'anilewe', 'link': 'https://stackoverflow.com/users/9472575/anilewe'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625268280, 'creation_date': 1625268280, 'answer_id': 68232133, 'question_id': 68231600, 'content_license': 'CC BY-SA 4.0', 'body': '<p>The <strong>column</strong> parameter of <code>pd.Dataframe()</code> function doesn\\'t set column names in result dataframe, but selects columns from the original file.</p>\\n<p>See <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\" rel=\"nofollow noreferrer\">pandas documentation</a> :</p>\\n<blockquote>\\n<p>Column labels to use for resulting frame when data does not have them, defaulting to RangeIndex(0, 1, 2, â¦, n). If data contains column labels, will perform column selection instead.</p>\\n</blockquote>\\n<p>So you shouldn\\'t provide <code>column</code> parameter and after the file is read, rename columns of the dataframe:</p>\\n<pre><code>df = pd.DataFrame(data)\\ndf.columns = [\\'subreddit_group\\', \\'links/caption\\', \\'def\\']\\n</code></pre>\\n'}, {'owner': {'reputation': 7962, 'user_id': 15239951, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/976f07be84907aad663ce8c1c51cf5c4?s=128&d=identicon&r=PG', 'display_name': 'Corralien', 'link': 'https://stackoverflow.com/users/15239951/corralien'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625268072, 'creation_date': 1625268072, 'answer_id': 68232113, 'question_id': 68231389, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>A straightforward way with boolean mask:</p>\\n<pre><code>dt = df.select_dtypes('datetime')\\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\\n\\ndf.loc[:, dt.columns.tolist()] = dt\\n</code></pre>\\n<pre><code>&gt;&gt;&gt; df\\n    Col0                Col1                Col2 Col3 Col4\\n0  1.txt 2021-06-23 15:04:30                 NaT  NaT  NaT\\n1  2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30  NaT  NaT\\n</code></pre>\\n\"}, {'owner': {'reputation': 16385, 'user_id': 15497888, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/W1on0.jpg?s=128&g=1', 'display_name': 'Henry Ecker', 'link': 'https://stackoverflow.com/users/15497888/henry-ecker'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625259165, 'creation_date': 1625259165, 'answer_id': 68231157, 'question_id': 68231104, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html#pandas-index-isin\" rel=\"nofollow noreferrer\"><code>Index.isin</code></a> on the level 1 values of columns then select with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</p>\\n<pre><code>filtered_df = df.loc[:, df.columns.isin([\\'A\\', \\'B\\'], level=1)]\\n</code></pre>\\n<p><code>filtered_df</code>:</p>\\n<pre><code>   d1      d2    \\n    A   B   A   B\\n0   1   2   5   6\\n1   9  10  13  14\\n2  17  18  21  22\\n</code></pre>\\n<hr />\\n<p>Sample Data Used:</p>\\n<pre><code>import numpy as np\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    np.arange(1, 25).reshape((-1, 8)),\\n    columns=pd.MultiIndex.from_product(([\\'d1\\', \\'d2\\'], list(\\'ABCD\\')))\\n)\\n</code></pre>\\n<pre><code>   d1              d2            \\n    A   B   C   D   A   B   C   D\\n0   1   2   3   4   5   6   7   8\\n1   9  10  11  12  13  14  15  16\\n2  17  18  19  20  21  22  23  24\\n</code></pre>\\n'}, {'owner': {'reputation': 14907, 'user_id': 6366770, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/6if5V.png?s=128&g=1', 'display_name': 'David Erickson', 'link': 'https://stackoverflow.com/users/6366770/david-erickson'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625257821, 'creation_date': 1625257821, 'answer_id': 68230973, 'question_id': 68230916, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>You can achieve this dynamically creating a dictionary with list comprehension like this:</p>\\n<pre><code>df.groupby(['group', 'code'], as_index=False).agg({col : 'sum' for col in df.columns[3:]}\\n</code></pre>\\n<p>If <code>item_no</code> is your index, then change <code>df.columns[3:]</code> to <code>df.columns[2:]</code></p>\\n\"}, {'owner': {'reputation': 44, 'user_id': 16345229, 'user_type': 'registered', 'profile_image': 'https://lh3.googleusercontent.com/a-/AOh14GgA1i7mZ_KMk_nVs3qqqri8PWdHyNhUR577H8FWqg=k-s128', 'display_name': 'Peerasak Intarapaiboon', 'link': 'https://stackoverflow.com/users/16345229/peerasak-intarapaiboon'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625256943, 'last_edit_date': 1625256943, 'creation_date': 1625255786, 'answer_id': 68230654, 'question_id': 68228154, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>By following your code, I found that the data type of both 'start_date' and 'end_date' is Series (NOT Timestamp like df['week']). Check by:</p>\\n<pre><code>type(df['week'][0]), type(start_date)\\n</code></pre>\\n<p>Then they cannot be compared.\\nYou may try this code:<br />\\n<code>id = start_date.index[0]</code></p>\\n<p><code>start_date = pd.to_datetime(start_date[id])</code></p>\\n<p>Note: &quot;id&quot; stores the index (based on your data, it is 199.)</p>\\n\"}, {'owner': {'reputation': 34, 'user_id': 8824980, 'user_type': 'registered', 'profile_image': 'https://lh3.googleusercontent.com/-w33TTt2eQdQ/AAAAAAAAAAI/AAAAAAAAK5Y/eeHQaA8sxaI/photo.jpg?sz=128', 'display_name': 'Fernandino Tavares', 'link': 'https://stackoverflow.com/users/8824980/fernandino-tavares'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625251196, 'creation_date': 1625251196, 'answer_id': 68229969, 'question_id': 68229806, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Your mistake is on this string <code>df1[df1['id']==id]['col0']</code> when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value.</p>\\n<p>To solve this issue is very very very simple, you just have to call the first item at the Series object like this: <code>df1[df1['id']==id]['col0'][0]</code></p>\\n<p>Your code with the ajustment must look like this</p>\\n<pre><code>import pandas as pd\\n\\nid=1\\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\\n\\ndf2.insert(0, &quot;id&quot;, id)\\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'][0])\\n\\nprint(df2)\\n</code></pre>\\n<p>Then your new df2 is like this:</p>\\n<pre><code>   id  col0  col1  col2\\n0   1     3    13    23\\n1   1     3    14    24\\n2   1     3    15    25\\n</code></pre>\\n\"}, {'owner': {'reputation': 14907, 'user_id': 6366770, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/6if5V.png?s=128&g=1', 'display_name': 'David Erickson', 'link': 'https://stackoverflow.com/users/6366770/david-erickson'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625248355, 'last_edit_date': 1625248355, 'creation_date': 1624981294, 'answer_id': 68181888, 'question_id': 68181818, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Use <code>groupby</code> (on the first column) + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html\" rel=\"nofollow noreferrer\"><code>cumcount</code></a>, add 1 (since we start counting at zero), and multiply by 10:</p>\\n<pre><code>df[\\'newcol\\'] = (df.groupby(\\'col1\\').cumcount() + 1) * 10\\n\\n    col1 col2 col3  newcol\\n0  data1   s1   k1      10\\n1  data1   s2   k2      20\\n2  data2   s4   k4      10\\n3  data2   s5   k5      20\\n4  data3   s6   k6      10\\n5  data3   s7   k7      20\\n6  data1   s8   k8      30\\n7  data1   s9   k9      40\\n</code></pre>\\n<p>EDIT (After Question Update). You have to merge in the original database dataframe, so that you can know where to start counting with <code>(df.groupby(\\'col1\\')[\\'newcol\\'].transform(\\'first\\')</code> and then add it to my first solution:</p>\\n<pre><code>df = df.merge(db_df, on=[\\'col1\\', \\'col2\\', \\'col3\\'], how=\\'left\\')\\ndf[\\'newcol\\'] = df[\\'newcol\\'].fillna(0).astype(int)\\ndf[\\'newcol\\'] = (df.groupby(\\'col1\\')[\\'newcol\\'].transform(\\'max\\') \\n             + (df.groupby(\\'col1\\').cumcount()+ 1) * 10)\\ndf\\n</code></pre>\\n'}, {'owner': {'reputation': 962, 'user_id': 10650144, 'user_type': 'registered', 'profile_image': 'https://lh3.googleusercontent.com/-ERqZfgct_8c/AAAAAAAAAAI/AAAAAAAAAAc/kXi0yNa0i6k/photo.jpg?sz=128', 'display_name': 'fusion', 'link': 'https://stackoverflow.com/users/10650144/fusion'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625247643, 'creation_date': 1625247643, 'answer_id': 68229348, 'question_id': 68229245, 'content_license': 'CC BY-SA 4.0', 'body': '<p>You can do a linear fit first then filter out the data that is outside of a certain threshold.\\nSample code below:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\\n\\ndf = pd.DataFrame({\\'ip\\':[10,20,30,40],\\'op\\':[105,195,500,410]})\\n\\n# do a linear fit on ip and op\\nf = np.polyfit(df.ip,df.op,1)\\n\\nfl = np.poly1d(f)\\n\\n# you will have to determine this threshold in some way\\nthreshold = 100\\n\\noutput = df[(df.op - fl(df.ip)).abs()&lt;threshold]\\n\\n</code></pre>\\n'}, {'owner': {'reputation': 10469, 'user_id': 9441404, 'user_type': 'registered', 'profile_image': 'https://graph.facebook.com/10212649426843269/picture?type=large', 'display_name': 'Rob Raymond', 'link': 'https://stackoverflow.com/users/9441404/rob-raymond'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625246745, 'creation_date': 1625246745, 'answer_id': 68229173, 'question_id': 68228682, 'content_license': 'CC BY-SA 4.0', 'body': '<ul>\\n<li>use <code>itertools.combinations</code> to get all combinations of values</li>\\n<li>then <code>apply()</code> <code>fuzz.ratio()</code></li>\\n<li>analyse results and select rows that don\\'t have a strong match to another combination</li>\\n</ul>\\n<pre><code>import pandas as pd\\nimport io\\nimport itertools\\nfrom fuzzywuzzy import fuzz\\n\\ndf = pd.read_csv(\\n    io.StringIO(\\n        &quot;&quot;&quot;    Page_no\\n0   Hello\\n2   Hey\\n3   Helloo\\n4   Heyy\\n5   Hellooo&quot;&quot;&quot;\\n    ),\\n    sep=&quot;\\\\s+&quot;,\\n)\\n\\n# find combinations that have greater than 80 match\\ndfx = pd.DataFrame(itertools.combinations(df[&quot;Page_no&quot;].values, 2)).assign(\\n    ratio=lambda d: d.apply(lambda t: fuzz.ratio(t[0], t[1]), axis=1)\\n).loc[lambda d: d[&quot;ratio&quot;].gt(80)]\\n\\n# exclude rows that have big match to another row...\\ndf.loc[~df[&quot;Page_no&quot;].isin(dfx[1])]\\n\\n</code></pre>\\n<div class=\"s-table-container\">\\n<table class=\"s-table\">\\n<thead>\\n<tr>\\n<th style=\"text-align: right;\"></th>\\n<th style=\"text-align: left;\">Page_no</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td style=\"text-align: right;\">0</td>\\n<td style=\"text-align: left;\">Hello</td>\\n</tr>\\n<tr>\\n<td style=\"text-align: right;\">2</td>\\n<td style=\"text-align: left;\">Hey</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>'}, {'owner': {'reputation': 12124, 'user_id': 9332187, 'user_type': 'registered', 'profile_image': 'https://lh3.googleusercontent.com/-I_opr6uwXKQ/AAAAAAAAAAI/AAAAAAAAADw/ZJMBGmRMJCM/photo.jpg?sz=128', 'display_name': 'Mustafa AydÄ±n', 'link': 'https://stackoverflow.com/users/9332187/mustafa-ayd%c4%b1n'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625242352, 'last_edit_date': 1625242352, 'creation_date': 1625242191, 'answer_id': 68228377, 'question_id': 68228284, 'content_license': 'CC BY-SA 4.0', 'body': '<p>You can use a list comprehension:</p>\\n<pre><code>df_len = len(df)\\npercent_duplicates = [df[col].duplicated(keep=False).sum() * 100 / df_len\\n                      for col in df]\\n</code></pre>\\n<p>or with <code>apply</code>:</p>\\n<pre><code>percent_duplicates = df.apply(lambda col:\\n                              col.duplicated(keep=False).sum() * 100 / df_len)\\n</code></pre>\\n<p>where we pass <code>keep=False</code> so that all the duplicates are marked as <code>True</code>,</p>\\n<p>to get</p>\\n<pre><code>&gt;&gt;&gt; pd.DataFrame({&quot;column_name&quot;: df.columns,\\n                  &quot;percent_duplicates&quot;: percent_duplicates})\\n\\n  column_name  percent_duplicates\\n0  subject_id                40.0\\n1     test_id                40.0\\n</code></pre>\\n'}, {'owner': {'reputation': 1706, 'user_id': 4687565, 'user_type': 'registered', 'accept_rate': 88, 'profile_image': 'https://www.gravatar.com/avatar/bcf4ed000ba052745a0667d5b5ebea0c?s=128&d=identicon&r=PG&f=1', 'display_name': 'Dimitry', 'link': 'https://stackoverflow.com/users/4687565/dimitry'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625240386, 'creation_date': 1625240386, 'answer_id': 68227972, 'question_id': 68061201, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Only because the question is well written and it seemed like a nice puzzle, here's some magic.</p>\\n<p>Potentially you'll have to store a lot of data, so you need to compress the frame as much as possible and do several passes through the base. If the database contains not primitive objects, convert those into integers, if you do multiprocessing, the dataframe will be copied into subprocesses, so keeping it contents small helps.</p>\\n<p>The runtime depends on the length of the dataframe but also on the number of unique stores, unique products and the size of a chunk of pairs to count. Spreading the work to many subprocesses can speed up things but there is constant cost to all the functions which will accumulate. For example, pandas' own methods will run faster on a single ten thousand rows dataframe than on a dozen of thousand row frames. And when you're running nested calls on sub dataframes of unpredictable size things get complicated. You'll probably have to experiment a bit to find a chunksize with optimal speed\\\\memory usage.</p>\\n<p>Test runtimes with smaller numbers first. Including less shops and products. That being said, this is not a quick task. On high end machine it completes in about ten minutes.</p>\\n<pre><code>import pandas as pd, numpy as np\\ndf = pd.DataFrame({\\n  'store':np.random.randint(0,int(2e4),int(5e6)),\\n  'product':np.random.randint(0,int(5e4),int(5e6))\\n  }).sort_values('store')\\n\\nproducts = df['product'].unique()\\nN, chunksize, Ntop = len(products), int(1e4), 200\\ndtype = np.min_scalar_type(max(products.max(),N))\\ndf = df.astype(dtype)\\n\\ndef store_cats(df):\\n    df = df.astype('category')\\n    cats = [df[x].cat.categories for x in df.columns]\\n    for col in df.columns:\\n        df[col] = df[col].cat.codes\\n    return df, cats    \\ndef restore_cats(summary,cats):\\n    for col in ['product_x','product_y']:\\n        summary[col] = pandas.Categorical.from_codes(summary[col], cats)\\n\\ndef subsets(n = chunksize):\\n    n = int(n)\\n    res = [frozenset(products[i:i+n]) for i in range(0,N,n)]\\n    info = 'In total there will be {:.1E} pairs, per pass {:.1E} will be checked, thats up to around {} mb per pass, {} passes'\\n    print(info.format((N**2),(n*N),(n*N*3*8/1e6),len(res)))\\n    return res\\n\\ndef count(df,subset):\\n    res = df.merge(df,on = 'store')\\\\\\n        .query('(product_x &lt; product_y) and product_x in @subset')\\\\\\n        .groupby(['product_x','product_y'])\\\\\\n        .count()\\\\\\n        .astype(dtype)\\\\\\n        .reset_index()\\n    return res \\ndef one_pass(gr,subset):\\n    per_group = gr.apply(count,subset)\\n    total_counts = per_group.sort_values(['product_x','product_y'])\\\\\\n        .groupby(['product_x','product_y'])\\\\\\n        .agg('sum')\\\\\\n        .sort_values('store',ascending=False)[:Ntop]\\\\\\n        .copy().reset_index()\\n    return total_counts\\ndef merge_passes(dfs):\\n    res = pd.concat(dfs,ignore_index=True)\\n    res = res.append(res.rename(columns={'product_x':'product_y','product_y':'product_x'}),ignore_index=True)\\n    res = res.sort_values('store',ascending=False)[:Ntop]\\n    return res\\n\\nfrom concurrent.futures import as_completed, ProcessPoolExecutor as Pool\\n\\ngr = df.groupby('store',as_index = False)\\ndef worker(subset):\\n    return one_pass(gr,subset)\\ndef run_progress(max_workers=2,chunksize=chunksize):\\n    from tqdm.auto import tqdm \\n    with Pool(max_workers = max_workers) as p:\\n        futures = [p.submit(worker,subset) for subset in subsets(chunksize)]\\n        summaries = [x.result() for x in tqdm(as_completed(futures),total=len(futures))]\\n        return merge_passes(summaries)\\n</code></pre>\\n\"}, {'owner': {'reputation': 40864, 'user_id': 12833166, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/p1a2a.jpg?s=128&g=1', 'display_name': 'Shubham Sharma', 'link': 'https://stackoverflow.com/users/12833166/shubham-sharma'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625240203, 'creation_date': 1625240203, 'answer_id': 68227920, 'question_id': 68226605, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>We can iterate over the column names in the given list, then for each column check the given conditions and create the corresponding flag columns</p>\\n<pre><code>cols = ['subject_id', 'test_id']\\n\\nfor c in cols:\\n    df[c + '_missing'] = df[c].isna()\\n    df[c + '_duplicated'] = df[c].duplicated(keep=False)\\n    df[c + '_numeric'] = pd.to_numeric(df[c], errors='coerce') % 1 == 0\\n</code></pre>\\n<hr />\\n<pre><code>print(df)\\n\\n   subject_id               test_id  subject_id_missing  subject_id_duplicated  subject_id_numeric  test_id_missing  test_id_duplicated  test_id_numeric\\n0         101         A1:123,A25668               False                  False                True            False               False             True\\n1         102  B1:TEST,B2456,B3#123               False                  False                True            False               False             True\\n2         103                B3:456               False                  False                True            False               False             True\\n3         201         B3:678,C1:345               False                  False                True            False               False             True\\n4         202             C2:367,C3               False                  False                True            False               False             True\\n</code></pre>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625238502, 'last_edit_date': 1625238502, 'creation_date': 1625238132, 'answer_id': 68227419, 'question_id': 68227378, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>try via <code>DataFrame()</code> method and <code>apply()</code>:</p>\\n<pre><code>out=pd.DataFrame(df['data_list'].tolist()).apply(pd.Series.explode)\\n#OR(you can also use agg() method in place of apply() method)\\nout=pd.DataFrame(df['data_list'].tolist()).agg(pd.Series.explode)\\n</code></pre>\\n<p>Finally:</p>\\n<pre><code>out.columns=['token','ext_t','symbol','name']\\n</code></pre>\\n<p>Now If you print <code>out</code> you will get your expected output</p>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625236839, 'last_edit_date': 1625236839, 'creation_date': 1625233853, 'answer_id': 68226341, 'question_id': 68226202, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>you can try:</p>\\n<pre><code>m=(df['day'].isin([7,14,21,28])) &amp; (df['countriesAndTerritories'].isin(['USA','Germany']))\\n#If the names are exact 'USA' and 'Germany'\\n#OR\\nm=(df['day'].isin([7,14,21,28])) &amp; (df['countriesAndTerritories'].str.contains('USA|Germany',case=False))\\n#IF the names are in irregular case i.e some are in uppercase and some are in lowercase\\n</code></pre>\\n<p>Finally:</p>\\n<pre><code>df[m]\\n#OR\\ndf.loc[m]\\n</code></pre>\\n\"}, {'owner': {'reputation': 2236, 'user_id': 3526116, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/e9a7854e15da93050a28c70ccb3490a2?s=128&d=identicon&r=PG&f=1', 'display_name': 'crayxt', 'link': 'https://stackoverflow.com/users/3526116/crayxt'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625235729, 'creation_date': 1625235729, 'answer_id': 68226812, 'question_id': 68226692, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Is this expected output? Based on your</p>\\n<blockquote>\\n<p>Basically, if the ID and season are the same update the existing\\nrecord, and if they're different add a new record.</p>\\n</blockquote>\\n<p>We concatenate two dataframes, group by <code>ID</code> and <code>Year</code> and leave the last (thus, coming from <code>df2</code>) element in each group.</p>\\n<pre><code>&gt;&gt;&gt; pd.concat([df1, df2]).groupby([&quot;ID&quot;, &quot;Year&quot;], as_index=False).last()\\n    ID  Year       Name balance\\n0  112  2020  Johnstown    $321\\n1  112  2021  Johnstown    $321\\n2  121  2020    Jackson    $254\\n3  321  2020     Oregon    $216\\n</code></pre>\\n\"}, {'owner': {'reputation': 640603, 'user_id': 2901002, 'user_type': 'registered', 'accept_rate': 97, 'profile_image': 'https://i.stack.imgur.com/hMDvl.jpg?s=128&g=1', 'display_name': 'jezrael', 'link': 'https://stackoverflow.com/users/2901002/jezrael'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625230890, 'last_edit_date': 1625230890, 'creation_date': 1625230800, 'answer_id': 68225663, 'question_id': 68225629, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html\" rel=\"nofollow noreferrer\"><code>Series.str.extract</code></a> if need values before first <code>:</code> with <code>^</code> for match start of string and <code>.*</code> for any value before <code>:</code>:</p>\\n<pre><code>df[\\'new_test_id\\'] = df[\\'new_test\\'].str.extract(&quot;^(.*):&quot;)\\nprint (df)\\n   person_id               test_id new_test new_test_id\\n0        101         A1:123,A25668   A1:123          A1\\n0        101         A1:123,A25668   A25668         NaN\\n1        102  B1:TEST,B2456,B3#123  B1:TEST          B1\\n1        102  B1:TEST,B2456,B3#123    B2456         NaN\\n1        102  B1:TEST,B2456,B3#123   B3#123         NaN\\n2        103                B3:456   B3:456          B3\\n3        201         B3:678,C1:345   B3:678          B3\\n3        201         B3:678,C1:345   C1:345          C1\\n4        202             C2:367,C3   C2:367          C2\\n4        202             C2:367,C3       C3         NaN\\n</code></pre>\\n<p>Your solution is with selectting by <code>str[0]</code>, but need also set <code>NaN</code>s if no match <code>:</code>:</p>\\n<pre><code>df[\\'new_test_id\\'] = df[\\'new_test\\'].str.split(&quot;:&quot;).str[0].where(df[\\'new_test\\'].str.contains(&quot;:&quot;))\\n</code></pre>\\n'}, {'owner': {'reputation': 640603, 'user_id': 2901002, 'user_type': 'registered', 'accept_rate': 97, 'profile_image': 'https://i.stack.imgur.com/hMDvl.jpg?s=128&g=1', 'display_name': 'jezrael', 'link': 'https://stackoverflow.com/users/2901002/jezrael'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625229974, 'creation_date': 1625229974, 'answer_id': 68225485, 'question_id': 68225470, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>If need replace <code>NaN</code>s in all columns (and all columns are numeric):</p>\\n<pre><code>df = df.fillna(df.mean())\\nprint (df)\\n   one  two  three\\n0  1.0  2.5      1\\n1  2.0  2.0      2\\n2  3.0  3.0      3\\n3  2.0  2.5      4\\n</code></pre>\\n<p>If need specify columns by names in list:</p>\\n<pre><code>c = ['one','two']\\ndf[c] = df[c].fillna(df[c].mean())\\n</code></pre>\\n<p>Or if need replace only numeric columns:</p>\\n<pre><code>c = df.select_dtypes(np.number).columns\\ndf[c] = df[c].fillna(df[c].mean())\\n</code></pre>\\n\"}, {'owner': {'reputation': 9163, 'user_id': 349948, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/e06d0e631af3fe3d713e76f2957fb05f?s=128&d=identicon&r=PG', 'display_name': 'Corley Brigman', 'link': 'https://stackoverflow.com/users/349948/corley-brigman'}, 'is_accepted': True, 'score': 110, 'last_activity_date': 1625229777, 'last_edit_date': 1625229777, 'creation_date': 1500303485, 'answer_id': 45147491, 'question_id': 45147100, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>From the <code>dropna</code> docstring:</p>\\n<h5>Drop the columns where all elements are NaN:</h5>\\n<pre><code>df.dropna(axis=1, how='all')\\n\\n\\n   A    B    D\\n0  NaN  2.0  0\\n1  3.0  4.0  1\\n2  NaN  NaN  5\\n</code></pre>\\n\"}, {'owner': {'reputation': 90518, 'user_id': 10035985, 'user_type': 'registered', 'profile_image': 'https://lh4.googleusercontent.com/-1WgJ_2yA-78/AAAAAAAAAAI/AAAAAAAAAOA/0CBOlYqYe7M/photo.jpg?sz=128', 'display_name': 'Andrej Kesely', 'link': 'https://stackoverflow.com/users/10035985/andrej-kesely'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625226378, 'last_edit_date': 1625226378, 'creation_date': 1625140603, 'answer_id': 68209735, 'question_id': 68209418, 'content_license': 'CC BY-SA 4.0', 'body': '<p>I hope I\\'ve understood your question right. This example will substract necessary value (&quot;reset&quot;) when cumulative sum of sale is greater than 5 and IsSuccess==True:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>df[&quot;SumSale&quot;] = df[&quot;Sale&quot;].cumsum()\\n\\n# &quot;reset&quot; when SumSale&gt;5 and IsSuccess==True\\nm = df[&quot;SumSale&quot;].gt(5) &amp; df[&quot;IsSuccess&quot;].eq(True)\\ndf.loc[m, &quot;to_remove&quot;] = df[&quot;SumSale&quot;]\\ndf[&quot;to_remove&quot;] = df[&quot;to_remove&quot;].ffill().shift().fillna(0)\\ndf[&quot;SumSale&quot;] -= df[&quot;to_remove&quot;]\\n\\ndf = df.drop(columns=&quot;to_remove&quot;)\\n\\nprint(df)\\n</code></pre>\\n<p>Prints:</p>\\n<pre class=\"lang-none prettyprint-override\"><code>   Sale  IsSuccess  SumSale\\n0     1      False      1.0\\n1     2       True      3.0\\n2     3      False      6.0\\n3     2      False      8.0\\n4     4       True     12.0\\n5     3      False      3.0\\n6     5       True      8.0\\n7     5      False      5.0\\n</code></pre>\\n<hr />\\n<p>EDIT:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>def fn():\\n    sale, success = yield\\n    cum = sale\\n    while True:\\n        sale, success = yield cum\\n        if success and cum &gt; 5:\\n            cum = sale\\n        else:\\n            cum += sale\\n\\n\\ns = fn()\\nnext(s)\\ndf[&quot;ss&quot;] = df[&quot;IsSuccess&quot;].shift()\\ndf[&quot;SumSale&quot;] = df.apply(lambda x: s.send((x[&quot;Sale&quot;], x[&quot;ss&quot;])), axis=1)\\ndf = df.drop(columns=&quot;ss&quot;)\\nprint(df)\\n</code></pre>\\n<p>Prints:</p>\\n<pre class=\"lang-none prettyprint-override\"><code>   Sale  IsSuccess  SumSaleExpected  SumSale\\n0    10      False               10       10\\n1     2       True               12       12\\n2     2      False                2        2\\n3     1      False                3        3\\n4     3       True                6        6\\n5     2      False                2        2\\n6     1       True                3        3\\n7     3      False                6        6\\n8     5      False               11       11\\n9     5      False               16       16\\n</code></pre>\\n'}, {'owner': {'reputation': 90518, 'user_id': 10035985, 'user_type': 'registered', 'profile_image': 'https://lh4.googleusercontent.com/-1WgJ_2yA-78/AAAAAAAAAAI/AAAAAAAAAOA/0CBOlYqYe7M/photo.jpg?sz=128', 'display_name': 'Andrej Kesely', 'link': 'https://stackoverflow.com/users/10035985/andrej-kesely'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625221035, 'creation_date': 1625221035, 'answer_id': 68223460, 'question_id': 68223193, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Try:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>def get_tree(df, parent=0):\\n    out = []\\n    for _, row in df[df.parent_id == parent].iterrows():\\n        out.append({&quot;id&quot;: row[&quot;id&quot;], &quot;title&quot;: row[&quot;title&quot;]})\\n\\n        subs = df[df.parent_id == row[&quot;id&quot;]]\\n        if len(subs) &gt; 0:\\n            out[-1][&quot;subs&quot;] = get_tree(df, parent=row[&quot;id&quot;])\\n\\n    return out\\n\\n\\nprint(get_tree(df))\\n</code></pre>\\n<p>Prints:</p>\\n<pre class=\"lang-json prettyprint-override\"><code>[\\n    {\\n        &quot;id&quot;: 1,\\n        &quot;title&quot;: &quot;Math&quot;,\\n        &quot;subs&quot;: [\\n            {\\n                &quot;id&quot;: 2,\\n                &quot;title&quot;: &quot;Algebra&quot;,\\n                &quot;subs&quot;: [\\n                    {\\n                        &quot;id&quot;: 3,\\n                        &quot;title&quot;: &quot;Polynomials&quot;,\\n                        &quot;subs&quot;: [{&quot;id&quot;: 4, &quot;title&quot;: &quot;sum of polynomials&quot;}],\\n                    }\\n                ],\\n            }\\n        ],\\n    },\\n    {\\n        &quot;id&quot;: 5,\\n        &quot;title&quot;: &quot;Physics&quot;,\\n        &quot;subs&quot;: [\\n            {\\n                &quot;id&quot;: 6,\\n                &quot;title&quot;: &quot;Mechanics&quot;,\\n                &quot;subs&quot;: [\\n                    {&quot;id&quot;: 7, &quot;title&quot;: &quot;Kinematics &quot;},\\n                    {&quot;id&quot;: 8, &quot;title&quot;: &quot;Dynamics&quot;},\\n                ],\\n            }\\n        ],\\n    },\\n]\\n</code></pre>\\n'}, {'owner': {'reputation': 242, 'user_id': 6232483, 'user_type': 'registered', 'profile_image': 'https://graph.facebook.com/10101422670608505/picture?type=large', 'display_name': 'Clay Shwery', 'link': 'https://stackoverflow.com/users/6232483/clay-shwery'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625218923, 'last_edit_date': 1625218923, 'creation_date': 1625217735, 'answer_id': 68222693, 'question_id': 68222462, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Here\\'s the top-level description from the pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>pandas.DataFrame.groupby</code></a> docs:</p>\\n<p>A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.</p>\\n<p>In this context, you are using the values of the column \\'level\\' to split the dataframe.</p>\\n<p>You then select only the column &quot;attempt&quot; and apply the mean function, the values of which are then combined back together.</p>\\n<p>So in english, your results are the mean value for all records of each \\'level\\'. That is, in the example below, the mean attempt value for all records of level 1 is 0.75144</p>\\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\\nimport io\\nimport requests\\nresp = requests.get(\\'https://raw.githubusercontent.com/whitehatjr/Data-Analysis-by-visualisation/master/data.csv\\')\\ndf = pd.read_csv(io.BytesIO(resp.content))\\ndf.groupby(&quot;level&quot;)[&quot;attempt&quot;].mean().reset_index()\\n</code></pre>\\n<p>output:</p>\\n<pre><code>     level   attempt\\n0  Level 1  0.751445\\n1  Level 2  0.863281\\n2  Level 3  0.698113\\n3  Level 4  0.734694\\n</code></pre>\\n'}, {'owner': {'reputation': 7962, 'user_id': 15239951, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/976f07be84907aad663ce8c1c51cf5c4?s=128&d=identicon&r=PG', 'display_name': 'Corralien', 'link': 'https://stackoverflow.com/users/15239951/corralien'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625217708, 'creation_date': 1625217708, 'answer_id': 68222684, 'question_id': 68222155, 'content_license': 'CC BY-SA 4.0', 'body': \"<pre><code>&gt;&gt;&gt; categories\\n['ATIDS', 'BasicCrane', 'LLP', 'Beam Sensor', 'CLPS', 'SPR']\\n\\n&gt;&gt;&gt; pd.merge(pd.DataFrame({'Cat': categories}), df, how='outer')\\n           Cat    UR3   VR1    VR   VR3\\n0        ATIDS  137.0  99.0  40.0  84.0\\n1   BasicCrane    2.0   8.0   3.0   1.0\\n2          LLP    NaN   NaN   NaN   NaN\\n3  Beam Sensor   27.0  12.0  13.0  14.0\\n4         CLPS    1.0   NaN   NaN   1.0\\n5          SPR    NaN   NaN   NaN   NaN\\n</code></pre>\\n\"}, {'owner': {'reputation': 4512, 'user_id': 11380795, 'user_type': 'registered', 'profile_image': 'https://lh5.googleusercontent.com/-b9TNUDV-Dx0/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcKn5vSc80O8oLuWjrCS77xcVPq7w/mo/photo.jpg?sz=128', 'display_name': 'RJ Adriaansen', 'link': 'https://stackoverflow.com/users/11380795/rj-adriaansen'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625213729, 'creation_date': 1625213729, 'answer_id': 68221821, 'question_id': 68219578, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>This runs in 4 seconds for me:</p>\\n<pre><code>import requests\\nimport pandas as pd\\n\\nheaders = {\\n    'Connection': 'keep-alive',\\n    'Cache-Control': 'max-age=0',\\n    'sec-ch-ua': '&quot; Not;A Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;91&quot;, &quot;Chromium&quot;;v=&quot;91&quot;',\\n    'sec-ch-ua-mobile': '?0',\\n    'Upgrade-Insecure-Requests': '1',\\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\\n    'Sec-Fetch-Site': 'cross-site',\\n    'Sec-Fetch-Mode': 'navigate',\\n    'Sec-Fetch-User': '?1',\\n    'Sec-Fetch-Dest': 'document',\\n    'Accept-Language': 'nl-NL,nl;q=0.9,en-US;q=0.8,en;q=0.7',\\n}\\n\\nresponse = requests.get('https://www1.nseindia.com/products/content/derivatives/equities/fo_underlyinglist.htm', headers=headers)\\ndf = pd.read_html(response.content)\\n</code></pre>\\n<p>The table can be found under <code>df[0]</code>.</p>\\n\"}, {'owner': {'reputation': 640603, 'user_id': 2901002, 'user_type': 'registered', 'accept_rate': 97, 'profile_image': 'https://i.stack.imgur.com/hMDvl.jpg?s=128&g=1', 'display_name': 'jezrael', 'link': 'https://stackoverflow.com/users/2901002/jezrael'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625208028, 'last_edit_date': 1625208028, 'creation_date': 1624971509, 'answer_id': 68179161, 'question_id': 68179089, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Use:</p>\\n<pre><code>up_edge_x = x[edge_or_not &gt; 0]\\nup_edge_y = y[edge_or_not &gt; 0]\\n\\ndown_edge_x = x[edge_or_not &lt; 0]\\ndown_edge_y = y[edge_or_not &lt; 0]\\n\\nall_edges_x = x[edge_or_not != 0]\\nall_edges_y = y[edge_or_not != 0]\\n</code></pre>\\n<p>Create <code>Series</code> by ranges with index by <code>up_edge_x, down_edge_x</code> first:</p>\\n<pre><code>up_edge = pd.Series(range(len(up_edge_x)), index=up_edge_x, name='pos')\\ndown_edge = pd.Series(range(len(down_edge_x)), index=down_edge_x, name='pos')\\nprint (up_edge)\\n0.9394    0\\n0.8955    1\\nName: pos, dtype: int64\\n\\nprint (down_edge)\\n0.8574    0\\n0.9196    1\\n0.9388    2\\n0.9602    3\\nName: pos, dtype: int64\\n</code></pre>\\n<p>Then join together:</p>\\n<pre><code>pos = pd.concat([up_edge, down_edge])\\nprint (pos)\\n0.9394    0\\n0.8955    1\\n0.8574    0\\n0.9196    1\\n0.9388    2\\n0.9602    3\\nName: pos, dtype: int64\\n</code></pre>\\n<p>And last map new column:</p>\\n<pre><code>all_edges = pd.DataFrame({'y':all_edges_y,\\n                          'edge':edge_or_not[edge_or_not != 0].to_numpy(), \\n                          'pos': pd.Index(all_edges_x).map(pos)},\\n                          index=all_edges_x)\\n\\n\\nprint (all_edges)\\n            y  edge  pos\\n0.9394  0.884     1    0\\n0.8574  0.880    -1    0\\n0.8955  0.861     1    1\\n0.9196  0.817    -1    1\\n0.9388  0.771    -1    2\\n0.9602  0.727    -1    3\\n</code></pre>\\n\"}, {'owner': {'reputation': 640603, 'user_id': 2901002, 'user_type': 'registered', 'accept_rate': 97, 'profile_image': 'https://i.stack.imgur.com/hMDvl.jpg?s=128&g=1', 'display_name': 'jezrael', 'link': 'https://stackoverflow.com/users/2901002/jezrael'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625202740, 'creation_date': 1625202740, 'answer_id': 68219905, 'question_id': 68219878, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>If need set all <code>Id</code> if contain at least one row of data with only missing values to string <code>None</code> use:</p>\\n<pre><code>garage_cat_columns = ['GarageType','GarageYrBlt','GarageQual','GarageCond','GarageFinish']\\n\\nno_garage = testing.loc[testing[garage_cat_columns].isnull().all(axis=1), 'Id'].tolist()\\ntesting.loc[df['Id'].isin(no_garage), garage_cat_columns] = 'None'\\n</code></pre>\\n<p>If need set only rows with all <code>None</code>s like Nonetype:</p>\\n<pre><code>mask = testing[garage_cat_columns].isnull().all(axis=1)\\ntesting.loc[mask, garage_cat_columns] = 'None'\\n</code></pre>\\n\"}, {'owner': {'reputation': 262690, 'user_id': 7964527, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://i.stack.imgur.com/HPW8R.jpg?s=128&g=1', 'display_name': 'BENY', 'link': 'https://stackoverflow.com/users/7964527/beny'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625191080, 'creation_date': 1625191080, 'answer_id': 68218751, 'question_id': 68218610, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Try with <code>apply</code> <code>np.mean</code></p>\\n<pre><code>df.groupby('group')['embedding'].apply(np.mean).reset_index()\\n  group   embedding\\n0     a  [0.5, 0.5]\\n1     b  [0.5, 0.5]\\n</code></pre>\\n\"}, {'owner': {'reputation': 14907, 'user_id': 6366770, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/6if5V.png?s=128&g=1', 'display_name': 'David Erickson', 'link': 'https://stackoverflow.com/users/6366770/david-erickson'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625179028, 'last_edit_date': 1625179028, 'creation_date': 1625178347, 'answer_id': 68217580, 'question_id': 68217404, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>use <code>np.where</code> when you have one condition that creates exactly two mutually exclusive groups:</p>\\n<pre><code>dataframe['DATE1'] = dt.datetime.now()\\ndataframe['DATE2'] = dt.datetime.now()\\ndataframe['DATE3'] = np.where(dataframe['FLOAT']&gt;0,dataframe['DATE1'],dataframe['DATE2'])\\ndataframe\\n</code></pre>\\n<p>Regarding the error with <code>np.select</code>. Initially store the date as an object datatype with <code>str()</code> and then convert <code>to_datetime()</code> later. Per the error, there is an issue with the datetime data type in the np.select statement</p>\\n<pre><code>dataframe['DATE1'] = str(dt.datetime.now())\\ndataframe['DATE2'] = str(dt.datetime.now())\\ndataframe['DATE3'] = pd.to_datetime(np.select([(dataframe['FLOAT']&gt;0),(dataframe['FLOAT']&lt;=0)],\\n                                              [dataframe['DATE1'],dataframe['DATE2']]))\\ndataframe\\n</code></pre>\\n\"}, {'owner': {'reputation': 3046, 'user_id': 7233155, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/97ca17a34b5f7b188c1bcb230a4701e2?s=128&d=identicon&r=PG&f=1', 'display_name': 'Attack68', 'link': 'https://stackoverflow.com/users/7233155/attack68'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625173534, 'creation_date': 1625173534, 'answer_id': 68216942, 'question_id': 68179824, 'content_license': 'CC BY-SA 4.0', 'body': '<p>In the latest version of pandas 1.3.0 <code>set_na_rep()</code> is deprecated and you should use <code>format(na_rep=&quot;xx&quot;)</code> instead.</p>\\n<p>If the values are truly missing, i.e. they are <code>pd.nan</code> and  <code>pd.isna(val)</code> returns <code>True</code> then the value will be substitututed for the display value of <code>na_rep</code>.</p>\\n<p>On the other hand if you have previously performed some manipulation and the value is a string representation of \\'NAN\\', then this is not a missing value (it is a string), and the <code>na_rep</code> argument will have no effect.</p>\\n<p>Actually you can see the relevant issue to this here (<a href=\"https://github.com/pandas-dev/pandas/pull/40060\" rel=\"nofollow noreferrer\">https://github.com/pandas-dev/pandas/pull/40060</a>) and the ultimate solution (<a href=\"https://github.com/pandas-dev/pandas/pull/40134\" rel=\"nofollow noreferrer\">https://github.com/pandas-dev/pandas/pull/40134</a>)</p>\\n'}, {'owner': {'reputation': 10689, 'user_id': 15070697, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/954fb328192b6541652eb8895bc9db19?s=128&d=identicon&r=PG&f=1', 'display_name': 'SeaBean', 'link': 'https://stackoverflow.com/users/15070697/seabean'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625171608, 'creation_date': 1625171608, 'answer_id': 68216605, 'question_id': 68208793, 'content_license': 'CC BY-SA 4.0', 'body': '<p>For unsigned integer data in range 0..255, you can reduce the memory storage from default <code>int64</code> (8 bytes) to use <code>uint8</code> (1 byte).  You can refer to <a href=\"https://www.educative.io/edpresso/reduce-the-memory-usage-when-loading-a-file-in-pandas\" rel=\"nofollow noreferrer\">this article</a> for an example where the memory usage is substantially reduced from 1.5MB to 332KB (around one fifth).</p>\\n<p>For Categorical type, as Pandas stores categorical columns as objects, this storage is not optimal. One of the reason is that it creates a list of pointers to the memory address of each value of your column.  Refer to <a href=\"https://vincentteyssier.medium.com/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e\" rel=\"nofollow noreferrer\">this article</a> for more information.</p>\\n<p>To use <code>uint8</code>, either you can do it when you input your data, e.g. during <code>pd.read_csv</code> call, you specify the dtype of input columns with <code>uint8</code> type. (See the <a href=\"https://www.educative.io/edpresso/reduce-the-memory-usage-when-loading-a-file-in-pandas\" rel=\"nofollow noreferrer\">first article</a> for an example).  If you already have your data loaded and you want to convert the dataframe columns to use <code>uint8</code>, you can use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html\" rel=\"nofollow noreferrer\"><code>Series.astype()</code></a> or <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html#\" rel=\"nofollow noreferrer\"><code>DataFrame.astype()</code></a> function with syntax like <code>.astype(\\'uint8\\')</code>.</p>\\n'}, {'owner': {'reputation': 2449, 'user_id': 9857631, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/c0befb2a32e9e8e9593994cfff5dbcfc?s=128&d=identicon&r=PG&f=1', 'display_name': 'not_speshal', 'link': 'https://stackoverflow.com/users/9857631/not-speshal'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625169678, 'creation_date': 1625169678, 'answer_id': 68216272, 'question_id': 68216193, 'content_license': 'CC BY-SA 4.0', 'body': '<p>You can do one of two things:</p>\\n<pre><code>df[&quot;date1&quot;]= pd.to_datetime(df[&quot;date1&quot;], format=&quot;%Y-%m-%d %H:%M:%S.%f UTC&quot;)\\ndf[&quot;date2&quot;]= pd.to_datetime(df[&quot;date2&quot;], format=&quot;%Y-%m-%d %H:%M:%S.%f&quot;)\\n&gt;&gt;&gt; df\\n  col1                   date1                   date2\\n0   10 2021-06-13 12:08:52.311 2021-03-29 12:44:33.468\\n1   36 2019-12-07 12:18:02.311 2011-10-15 10:14:32.118\\n</code></pre>\\n<p>Or:</p>\\n<pre><code>df[&quot;date1&quot;]= pd.to_datetime(df[&quot;date1&quot;].str.replace(&quot; UTC&quot;, &quot;&quot;))\\ndf[&quot;date2&quot;]= pd.to_datetime(df[&quot;date2&quot;])\\n&gt;&gt;&gt; df\\n col1                   date1                   date2\\n0   10 2021-06-13 12:08:52.311 2021-03-29 12:44:33.468\\n1   36 2019-12-07 12:18:02.311 2011-10-15 10:14:32.118\\n</code></pre>\\n'}, {'owner': {'reputation': 90518, 'user_id': 10035985, 'user_type': 'registered', 'profile_image': 'https://lh4.googleusercontent.com/-1WgJ_2yA-78/AAAAAAAAAAI/AAAAAAAAAOA/0CBOlYqYe7M/photo.jpg?sz=128', 'display_name': 'Andrej Kesely', 'link': 'https://stackoverflow.com/users/10035985/andrej-kesely'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625167034, 'creation_date': 1625167034, 'answer_id': 68215819, 'question_id': 68215642, 'content_license': 'CC BY-SA 4.0', 'body': '<p>You can use boolean indexing:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>mask = df1[&quot;col2&quot;].isin(df2[&quot;col1&quot;])\\nprint(df1[mask])\\n</code></pre>\\n<p>Prints:</p>\\n<pre class=\"lang-none prettyprint-override\"><code>   col1  col2  col3\\n1     4    55     6\\n2     1     8    88\\n</code></pre>\\n'}, {'owner': {'reputation': 117554, 'user_id': 6361531, 'user_type': 'registered', 'accept_rate': 80, 'profile_image': 'https://i.stack.imgur.com/BRRGS.png?s=128&g=1', 'display_name': 'Scott Boston', 'link': 'https://stackoverflow.com/users/6361531/scott-boston'}, 'is_accepted': True, 'score': 5, 'last_activity_date': 1625162265, 'creation_date': 1625162265, 'answer_id': 68214906, 'question_id': 68214806, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Use <code>pd.wide_to_long</code>:</p>\\n<pre><code>pd.wide_to_long(df,['Key', 'Val'],['City', 'State', 'Name'],'No').reset_index()\\n</code></pre>\\n<p>Output:</p>\\n<pre><code>       City    State      Name  No     Key Val\\n0   Houston    Texas      Aria   1   test1  28\\n1   Houston    Texas      Aria   2   test4  82\\n2   Houston    Texas      Aria   3   test7   4\\n3   Houston    Texas      Aria   4  test10  97\\n4   Houston    Texas      Aria   5  test13   4\\n5    Austin    Texas  Penelope   1   test2   4\\n6    Austin    Texas  Penelope   2   test5  45\\n7    Austin    Texas  Penelope   3   test8  76\\n8    Austin    Texas  Penelope   4  test11  66\\n9    Austin    Texas  Penelope   5  test14  10\\n10   Hoover  Alabama      Niko   1   test3   7\\n11   Hoover  Alabama      Niko   2   test6  76\\n12   Hoover  Alabama      Niko   3   test9   9\\n13   Hoover  Alabama      Niko   4  test12  10\\n14   Hoover  Alabama      Niko   5  test15    \\n</code></pre>\\n<p>You are trying to simultaneously melt two columns.  pd.wide_to_long handles this situation.</p>\\n\"}, {'owner': {'reputation': 12124, 'user_id': 9332187, 'user_type': 'registered', 'profile_image': 'https://lh3.googleusercontent.com/-I_opr6uwXKQ/AAAAAAAAAAI/AAAAAAAAADw/ZJMBGmRMJCM/photo.jpg?sz=128', 'display_name': 'Mustafa AydÄ±n', 'link': 'https://stackoverflow.com/users/9332187/mustafa-ayd%c4%b1n'}, 'is_accepted': True, 'score': 3, 'last_activity_date': 1625161702, 'creation_date': 1625161702, 'answer_id': 68214798, 'question_id': 68214659, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>For each row, you can check if the entry is equal to the maximum of that row; this will form a boolean frame. Then you can <code>dot</code> product it with the column names to choose those columns' names that gave <code>True</code> for the rows:</p>\\n<pre><code>is_max = df.eq(df.max(axis=1), axis=0)\\nresult = is_max.dot(df.columns + &quot; &quot;)\\n</code></pre>\\n<p>where <code>axis=1</code> of <code>max</code> says take the maximum of each row and <code>axis=0</code> of <code>eq</code> says align the argument (i.e., <code>df.max(axis=1)</code>) to compare row-wise i.e., broadcast so),</p>\\n<p>to get</p>\\n<pre><code>&gt;&gt;&gt; is_max\\n\\n   column_1  column_2\\n0     False      True\\n1      True     False\\n2      True      True\\n\\n&gt;&gt;&gt; result\\n\\n0             column_2\\n1             column_1\\n2    column_1 column_2\\n</code></pre>\\n\"}, {'owner': {'reputation': 470, 'user_id': 7118625, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/7cd71c7004da0471a5efccd97b3a9c11?s=128&d=identicon&r=PG&f=1', 'display_name': 'Alon Gadot', 'link': 'https://stackoverflow.com/users/7118625/alon-gadot'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625161463, 'creation_date': 1625161463, 'answer_id': 68214760, 'question_id': 68214700, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>The most efficient way would be as follows:</p>\\n<pre><code>df['Codes'] = df['Codes'].str.split()\\n</code></pre>\\n<p>Its usually a good idea to favor built in functions when using pandas. These will usually have very efficient implementations in cython.</p>\\n<p>A more flexible, but slower solution would be to apply an arbitrary python function -</p>\\n<pre><code>df['Codes'] = df['Codes'].apply(lambda x: x.split())\\n</code></pre>\\n\"}, {'owner': {'reputation': 7962, 'user_id': 15239951, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/976f07be84907aad663ce8c1c51cf5c4?s=128&d=identicon&r=PG', 'display_name': 'Corralien', 'link': 'https://stackoverflow.com/users/15239951/corralien'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625157900, 'last_edit_date': 1625157900, 'creation_date': 1625156432, 'answer_id': 68213724, 'question_id': 68213612, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>You can use <code>df.rolling</code> after grouping by <code>ID</code>:</p>\\n<pre><code>out = df.groupby('ID').rolling(2).mean() \\\\\\n        .dropna(how='all').reset_index(level=1, drop=True)\\n\\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\\n</code></pre>\\n<pre><code>&gt;&gt;&gt; out\\n       Val1  Val2\\nid0_1  10.5  19.5\\nid1_1   3.0   3.0\\nid1_2   1.5   2.5\\n</code></pre>\\n\"}, {'owner': {'reputation': 90518, 'user_id': 10035985, 'user_type': 'registered', 'profile_image': 'https://lh4.googleusercontent.com/-1WgJ_2yA-78/AAAAAAAAAAI/AAAAAAAAAOA/0CBOlYqYe7M/photo.jpg?sz=128', 'display_name': 'Andrej Kesely', 'link': 'https://stackoverflow.com/users/10035985/andrej-kesely'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625155680, 'creation_date': 1625155680, 'answer_id': 68213559, 'question_id': 68213482, 'content_license': 'CC BY-SA 4.0', 'body': '<p>If you have this dataframe:</p>\\n<pre class=\"lang-none prettyprint-override\"><code>                      title  tag1  tag2  tag3\\n0  This is title tag1, tag2   NaN   NaN   NaN\\n1  This is title tag3, tag2   NaN   NaN   NaN\\n2        This is title tag3   NaN   NaN   NaN\\n</code></pre>\\n<p>Then you can do:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>for tag in df.columns[1:]:\\n    df[tag] = df[&quot;title&quot;].str.contains(tag)\\n\\nprint(df)\\n</code></pre>\\n<p>Prints:</p>\\n<pre class=\"lang-none prettyprint-override\"><code>                      title   tag1   tag2   tag3\\n0  This is title tag1, tag2   True   True  False\\n1  This is title tag3, tag2  False   True   True\\n2        This is title tag3  False  False   True\\n</code></pre>\\n'}, {'owner': {'reputation': 13104, 'user_id': 15438033, 'user_type': 'registered', 'profile_image': 'https://lh5.googleusercontent.com/-g-MHtD-r-HA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucl0F9JbTp_s5TnQbSvyrydlRgWblQ/s96-c/photo.jpg?sz=128', 'display_name': 'Nk03', 'link': 'https://stackoverflow.com/users/15438033/nk03'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625154962, 'last_edit_date': 1625154962, 'creation_date': 1625154651, 'answer_id': 68213324, 'question_id': 68213292, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Here's one way:</p>\\n<pre><code>df = (\\n    df.pivot_table(\\n        index='Primary ID',\\n        columns=df.groupby('Primary ID').cumcount().add(1),\\n        values='Secondary ID'\\n    ).add_prefix('Secondary').reset_index()\\n)\\n</code></pre>\\n<p>Alternative:</p>\\n<pre><code>df = df.assign(t=df.groupby('Primary ID').cumcount().add(\\n    1)).set_index(['Primary ID',  't']).unstack(-1)\\n</code></pre>\\n<h4>OUTPUT:</h4>\\n<pre><code>   Primary ID  Secondary1  Secondary2  Secondary3\\n0           1    234234.0    435234.0     22233.0\\n1           2    334342.0    543236.0    134623.0\\n2           3   8475623.0   3928484.0         NaN\\n3           4   3723429.0         NaN         NaN\\n4           5   3945857.0  11112233.0   9878976.0\\n</code></pre>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 3, 'last_activity_date': 1625154040, 'last_edit_date': 1625154040, 'creation_date': 1625153309, 'answer_id': 68213002, 'question_id': 68212970, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Try via <code>bfill()</code>:</p>\\n<pre><code>xdf['A']=xdf.bfill(1)['A']\\n</code></pre>\\n<p>output of <code>df</code>:</p>\\n<pre><code>    A       B       C\\n0   10.0    15.0    NaN\\n1   20.0    NaN     NaN\\n2   30.0    30.0    35.0\\n3   40.0    NaN     40.0\\n</code></pre>\\n<p><strong>Update:</strong></p>\\n<p>if there were additional columns (like D, E) not needed to fillna then select the subset of df and backword fill on axis 1:</p>\\n<pre><code>xdf['A']=xdf[['A','B','C']].bfill(1)['A']\\n</code></pre>\\n\"}, {'owner': {'reputation': 16385, 'user_id': 15497888, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/W1on0.jpg?s=128&g=1', 'display_name': 'Henry Ecker', 'link': 'https://stackoverflow.com/users/15497888/henry-ecker'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625149939, 'last_edit_date': 1625149939, 'creation_date': 1625149527, 'answer_id': 68211988, 'question_id': 68211888, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Let\\'s try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> then build out a <code>dict</code>:</p>\\n<pre><code>dfs = {group_name: df_\\n       for group_name, df_ in pd.concat([df1, df2]).groupby(\\'NAME\\')}\\n</code></pre>\\n<p><code>dfs</code>:</p>\\n<pre><code>{\\'player1\\':       NAME  VAL1  VAL2  VAL3\\n0  player1     3     5     7,\\n \\'player2\\':       NAME  VAL1  VAL2  VAL3\\n1  player2     2     6     8\\n0  player2     5     7     7,\\n \\'player3\\':       NAME  VAL1  VAL2  VAL3\\n2  player3     3     6     7\\n1  player3     2     6     8,\\n \\'player5\\':       NAME  VAL1  VAL2  VAL3\\n2  player5     3     6     7}\\n</code></pre>\\n<p>Each player\\'s DataFrame can then be accessed like:</p>\\n<p><code>dfs[\\'player1\\']</code>:</p>\\n<pre><code>      NAME  VAL1  VAL2  VAL3\\n0  player1     3     5     7\\n</code></pre>\\n<hr />\\n<p>Or as a <code>list</code>:</p>\\n<pre><code>dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby(\\'NAME\\')]\\n</code></pre>\\n<p><code>dfs</code>:</p>\\n<pre><code>[      NAME  VAL1  VAL2  VAL3\\n0  player1     3     5     7,\\n       NAME  VAL1  VAL2  VAL3\\n1  player2     2     6     8\\n0  player2     5     7     7,\\n       NAME  VAL1  VAL2  VAL3\\n2  player3     3     6     7\\n1  player3     2     6     8,\\n       NAME  VAL1  VAL2  VAL3\\n2  player5     3     6     7]\\n</code></pre>\\n<p>Each player\\'s DataFrame can then be accessed like:</p>\\n<p><code>dfs[1]</code>:</p>\\n<pre><code>      NAME  VAL1  VAL2  VAL3\\n1  player2     2     6     8\\n0  player2     5     7     7\\n</code></pre>\\n'}, {'owner': {'reputation': 7962, 'user_id': 15239951, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/976f07be84907aad663ce8c1c51cf5c4?s=128&d=identicon&r=PG', 'display_name': 'Corralien', 'link': 'https://stackoverflow.com/users/15239951/corralien'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625145684, 'creation_date': 1625145684, 'answer_id': 68211012, 'question_id': 68210857, 'content_license': 'CC BY-SA 4.0', 'body': '<p>If you have an older version, you can use:</p>\\n<pre><code>df1.columns.name = None  # alternative to rename_axis\\ndf1 = df1.reset_index()\\n</code></pre>\\n<pre><code>&gt;&gt;&gt; df1\\n   cola  colb  Average  Count  Total\\n0     1  Val1      100     10   1000\\n1     2  Val2       10      5    100\\n</code></pre>\\n'}, {'owner': {'reputation': 13104, 'user_id': 15438033, 'user_type': 'registered', 'profile_image': 'https://lh5.googleusercontent.com/-g-MHtD-r-HA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucl0F9JbTp_s5TnQbSvyrydlRgWblQ/s96-c/photo.jpg?sz=128', 'display_name': 'Nk03', 'link': 'https://stackoverflow.com/users/15438033/nk03'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625144198, 'creation_date': 1625144198, 'answer_id': 68210635, 'question_id': 68210073, 'content_license': 'CC BY-SA 4.0', 'body': '<p>you can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.combine_first.html\" rel=\"nofollow noreferrer\"><code>combine first</code></a></p>\\n<pre><code>df[\\'C\\'] = df.A.combine_first(df.B)\\n</code></pre>\\n<h5>OUTPUT:</h5>\\n<pre><code>      A      B      C\\n0    Cat    NaN    Cat\\n1  Mouse  Mouse  Mouse\\n2   Fish    NaN   Fish\\n3    NaN    NaN    NaN\\n4    NaN    Dog    Dog\\n</code></pre>\\n'}, {'owner': {'reputation': 90518, 'user_id': 10035985, 'user_type': 'registered', 'profile_image': 'https://lh4.googleusercontent.com/-1WgJ_2yA-78/AAAAAAAAAAI/AAAAAAAAAOA/0CBOlYqYe7M/photo.jpg?sz=128', 'display_name': 'Andrej Kesely', 'link': 'https://stackoverflow.com/users/10035985/andrej-kesely'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625142582, 'creation_date': 1625142582, 'answer_id': 68210215, 'question_id': 68210024, 'content_license': 'CC BY-SA 4.0', 'body': '<p>To skip the first <code>n</code> rows, <code>skiprows=</code> parameter:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.read_csv(\\n    &quot;movies.txt&quot;,\\n    sep=r&quot;\\\\s*\\\\|\\\\s*&quot;,\\n    comment=&quot;+&quot;,\\n    usecols=range(1, 13),\\n    skiprows=4,            # &lt;-- add skiprows here\\n    engine=&quot;python&quot;,\\n)\\n</code></pre>\\n'}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625141262, 'last_edit_date': 1625141262, 'creation_date': 1625140678, 'answer_id': 68209755, 'question_id': 68209684, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Try:</p>\\n<pre><code>out=df[df['name'].eq('four').shift(-1,fill_value=False)]\\n</code></pre>\\n<p><strong>OR</strong>(If there is particular reason of using <code>groupby()</code>)</p>\\n<pre><code>out=(df.groupby('in_id')['name']\\n   .agg(lambda x: x[x.eq('four').shift(-1,fill_value=False)])\\n   .reset_index())\\n#you can also use apply() in place of agg() method\\n</code></pre>\\n<p>Now If you print <code>out</code> you will get your desired output</p>\\n\"}, {'owner': {'reputation': 90518, 'user_id': 10035985, 'user_type': 'registered', 'profile_image': 'https://lh4.googleusercontent.com/-1WgJ_2yA-78/AAAAAAAAAAI/AAAAAAAAAOA/0CBOlYqYe7M/photo.jpg?sz=128', 'display_name': 'Andrej Kesely', 'link': 'https://stackoverflow.com/users/10035985/andrej-kesely'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625139520, 'creation_date': 1625139520, 'answer_id': 68209482, 'question_id': 68209241, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Try:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.read_csv(\\n    &quot;name_of_your_file.txt&quot;,\\n    sep=r&quot;\\\\s*\\\\|\\\\s*&quot;,\\n    comment=&quot;+&quot;,\\n    usecols=range(1, 5),\\n    engine=&quot;python&quot;,\\n)\\nprint(df)\\n</code></pre>\\n'}, {'owner': {'reputation': 99693, 'user_id': 565635, 'user_type': 'registered', 'accept_rate': 89, 'profile_image': 'https://www.gravatar.com/avatar/7b4b3e7c9ac68b7d2c93ad02d0b9c79d?s=128&d=identicon&r=PG', 'display_name': 'orlp', 'link': 'https://stackoverflow.com/users/565635/orlp'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625138855, 'creation_date': 1625138855, 'answer_id': 68209340, 'question_id': 68209126, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>This works:</p>\\n<pre><code>top20 = df.ge(df.quantile(0.8, axis=1), axis=0)\\ntop_cols = top20.apply(lambda x: x.index[x], axis=1)\\n</code></pre>\\n<p>Example result:</p>\\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame(data=np.random.randint(1, 5, (5, 10)),\\n                      columns=[f&quot;i{i}&quot; for i in range(10)])\\n&gt;&gt;&gt; df\\n   i0  i1  i2  i3  i4  i5  i6  i7  i8  i9\\n0   4   4   2   4   2   4   4   3   2   4\\n1   2   3   4   2   2   3   1   1   4   1\\n2   3   2   2   1   2   2   3   1   2   4\\n3   3   1   1   3   3   4   2   2   1   3\\n4   3   2   3   2   4   1   4   2   4   2\\n\\n&gt;&gt;&gt; top20 = df.ge(df.quantile(0.8, axis=1), axis=0)\\n&gt;&gt;&gt; top20\\n      i0     i1     i2     i3     i4     i5     i6     i7     i8     i9\\n0   True   True  False   True  False   True   True  False  False   True\\n1  False  False   True  False  False  False  False  False   True  False\\n2   True  False  False  False  False  False   True  False  False   True\\n3   True  False  False   True   True   True  False  False  False   True\\n4  False  False  False  False   True  False   True  False   True  False\\n\\n&gt;&gt;&gt; top20.apply(lambda x: x.index[x], axis=1)\\n0    Index(['i0', 'i1', 'i3', 'i5', 'i6', 'i9'], dt...\\n1                  Index(['i2', 'i8'], dtype='object')\\n2            Index(['i0', 'i6', 'i9'], dtype='object')\\n3    Index(['i0', 'i3', 'i4', 'i5', 'i9'], dtype='o...\\n4            Index(['i4', 'i6', 'i8'], dtype='object')\\ndtype: object\\n</code></pre>\\n\"}, {'owner': {'reputation': 5772, 'user_id': 11971785, 'user_type': 'registered', 'profile_image': 'https://lh5.googleusercontent.com/-Tm3RpH5aT9s/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfqnZceaddibCeBqwQbb0SmiR6prw/photo.jpg?sz=128', 'display_name': 'Andreas', 'link': 'https://stackoverflow.com/users/11971785/andreas'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625138043, 'last_edit_date': 1625138043, 'creation_date': 1625136154, 'answer_id': 68208717, 'question_id': 68208644, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>You can groupby and count the entries per group, since you want a series returned with the same length as the original, you can use <code>.transform()</code> which will do exactly that:</p>\\n<pre><code>df['Count'] = df.groupby(['X'])['Modules'].transform('count')\\n\\n\\n   X   Y Modules  Count\\n0  1   1       A      3\\n1  1   1       B      3\\n2  1  45       C      3\\n3  2  13       A      2\\n4  2  12       B      2\\n5  3  18       A      3\\n6  3  16       B      3\\n7  3  15       D      3\\n</code></pre>\\n\"}, {'owner': {'reputation': 13104, 'user_id': 15438033, 'user_type': 'registered', 'profile_image': 'https://lh5.googleusercontent.com/-g-MHtD-r-HA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucl0F9JbTp_s5TnQbSvyrydlRgWblQ/s96-c/photo.jpg?sz=128', 'display_name': 'Nk03', 'link': 'https://stackoverflow.com/users/15438033/nk03'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625137961, 'creation_date': 1625137961, 'answer_id': 68209128, 'question_id': 68209093, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Here's one way via <code>set_index</code> / <code>stack</code> / <code>unstack</code>:</p>\\n<pre><code>df = df.set_index(df.columns[:-3].to_list()).stack().unstack(-2).reset_index()\\n</code></pre>\\n<p>OUTPUT:</p>\\n<pre><code>print(df.head(10))\\n\\ncol_G  col_A  col_B col_C  col_D  col_E col_F level_6   a   b   c   d   e   f\\n0          1     20     A     10  Alpha     X   col_H  97  96  59  71  98  19\\n1          1     20     A     10  Alpha     X   col_I   0  55   8  26  70  85\\n2          1     20     A     10  Alpha     X   col_J  90  87  16  70  68  39\\n3          1     20     A     10  Alpha     Y   col_H  65  69   9  92  35  11\\n4          1     20     A     10  Alpha     Y   col_I  28  47  56  83  43  81\\n5          1     20     A     10  Alpha     Y   col_J   8  29  89  47  84  32\\n6          1     20     A     10  Alpha     Z   col_H  97  12  95  17  34  47\\n7          1     20     A     10  Alpha     Z   col_I  78  64  51  81  15  62\\n8          1     20     A     10  Alpha     Z   col_J  47  83  58   3  90  11\\n9          1     20     A     10   Beta     X   col_H  19   8  92  69  65  32\\n</code></pre>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625135681, 'last_edit_date': 1625135681, 'creation_date': 1625134680, 'answer_id': 68208394, 'question_id': 68208254, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>IIUC:</p>\\n<p>Try via <code>mask()</code> and <code>bfill()</code>:</p>\\n<pre><code>df['id_new']=df['id_new'].mask(df['id_new'].isin(df['id_old'])).bfill()\\n</code></pre>\\n<p><strong>Explaination:</strong></p>\\n<p>Checking If values of 'id_new' is in 'id_old' via <code>isin()</code></p>\\n<p>So <code>isin()</code> method is giving us a boolean series so we are passing that series to <code>mask()</code> method so basically where where condition matches <code>mask()</code> method put <code>NaN</code> since we are chaining it on 'id_new' column so it will give <code>NaN</code> where the value in mask is True and where it is False it will give the values of 'id_new' column</p>\\n<p>Finally backword filling values via <code>bfill()</code> method</p>\\n\"}, {'owner': {'reputation': 9015, 'user_id': 8228558, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/fd59fcdaa6fef158fce4b6d372998521?s=128&d=identicon&r=PG&f=1', 'display_name': 'IoaTzimas', 'link': 'https://stackoverflow.com/users/8228558/ioatzimas'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625135591, 'creation_date': 1625135591, 'answer_id': 68208587, 'question_id': 68208345, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>You are on a good path. You can continue like this:</p>\\n<pre><code>grp_by_series=grp_by_series.reset_index()\\n\\nres=df[['VehicleType', 'Colour']].merge(grp_by_series, how='left')\\n\\ndf['Frequency'] =  res[0]\\n\\nprint(df)\\n</code></pre>\\n<p>Output:</p>\\n<pre><code>  VehicleType  Colour  Year  Frequency\\n0       Truck   Green  2002          1\\n1         Car   Green  2014          2\\n2       Truck   Black  1975          1\\n3         Car  Yellow  1987          1\\n4         Car   Green  1987          2\\n</code></pre>\\n\"}, {'owner': {'reputation': 12941, 'user_id': 14289892, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/mM8Ee.jpg?s=128&g=1', 'display_name': 'Anurag Dabas', 'link': 'https://stackoverflow.com/users/14289892/anurag-dabas'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625133229, 'creation_date': 1625133229, 'answer_id': 68208055, 'question_id': 68208016, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Try via <code>groupby()</code> ,<code>agg()</code> and <code>to_dict()</code> method:</p>\\n<pre><code>dct=df.groupby('uniprot_id')['GO_id'].agg(list).to_dict()\\n</code></pre>\\n<p>output of <code>dct</code>:</p>\\n<pre><code>{'A0A009KHZ9': ['GO:0006097', 'GO:0006099', 'GO:0006099'],\\n 'A0A009KJV3': ['GO:0006412', 'GO:0006417', 'GO:0006412'],\\n 'A0A009KXK5': ['GO:0022900', 'GO:0006457'],\\n 'A0A009LQ34': ['GO:0046690'],\\n 'A0A009YU38': ['GO:0015074']}\\n</code></pre>\\n\"}, {'owner': {'reputation': 40864, 'user_id': 12833166, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/p1a2a.jpg?s=128&g=1', 'display_name': 'Shubham Sharma', 'link': 'https://stackoverflow.com/users/12833166/shubham-sharma'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625125750, 'creation_date': 1625125750, 'answer_id': 68206278, 'question_id': 68206182, 'content_license': 'CC BY-SA 4.0', 'body': '<p><code>Sort</code> the values by column <code>b</code> then <code>group</code> the dataframe and aggregate using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.head.html\" rel=\"nofollow noreferrer\"><code>head(n)</code></a> to select the first <code>n</code> rows of each group</p>\\n<pre><code>df.sort_values(\\'b\\', ascending=False).groupby(\\'a\\').head(2)\\n</code></pre>\\n<hr />\\n<pre><code>   a  b\\n3  1  4\\n0  1  3\\n6  2  2\\n7  2  2\\n</code></pre>\\n'}, {'owner': {'reputation': 602, 'user_id': 9607072, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/vzv9T.jpg?s=128&g=1', 'display_name': 'Kevin', 'link': 'https://stackoverflow.com/users/9607072/kevin'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625125110, 'creation_date': 1625125110, 'answer_id': 68206146, 'question_id': 68206032, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>You can filter the desired columns by specifying the list of column names inside the <code>[]</code> brackets like so:</p>\\n<pre><code>coal = df[dic1['coal']].sum(axis=1)\\ngas = df[dic1['gas']].sum(axis=1)\\n</code></pre>\\n<p>Then you can combine the two outputs into one dataframe:</p>\\n<pre><code>sol = pd.concat([coal, gas], axis=1)\\n</code></pre>\\n<p>finally we will change the names of the columns:</p>\\n<pre><code>sol.columns = dic1.keys()\\n</code></pre>\\n<p>output:</p>\\n<pre><code>print(sol)\\n&gt;&gt;&gt;    coal  gas\\n&gt;&gt;&gt; 0    42  237\\n&gt;&gt;&gt; 1    50  333\\n&gt;&gt;&gt; 2   102  142\\n</code></pre>\\n\"}, {'owner': {'reputation': 4286, 'user_id': 6006383, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/4a8f6cbb571399d66269fc48a922d2c1?s=128&d=identicon&r=PG&f=1', 'display_name': 'Ferris', 'link': 'https://stackoverflow.com/users/6006383/ferris'}, 'is_accepted': False, 'score': 1, 'last_activity_date': 1625122015, 'last_edit_date': 1625122015, 'creation_date': 1625119202, 'answer_id': 68205038, 'question_id': 68203974, 'content_license': 'CC BY-SA 4.0', 'body': \"<ol>\\n<li>find the target SiteLocation</li>\\n<li>then fill the target SiteLocation with 'Found', other with 'Not Found'</li>\\n</ol>\\n<pre><code>cond1 = df['Boolean'] == 'true'\\ncond2 = df['Active?'] == 'Pie-active'\\nsite_list = set(df.loc[cond1, 'SiteLocation']) - set(df.loc[cond2, 'SiteLocation'])\\ndf['Found?'] = np.where(df['SiteLocation'].isin(site_list), \\n                        'Found', 'Not Found')\\n</code></pre>\\n\"}, {'owner': {'reputation': 4635, 'user_id': 2106934, 'user_type': 'registered', 'accept_rate': 89, 'profile_image': 'https://www.gravatar.com/avatar/7d29cd79756208cfd1a7c814c477ddd6?s=128&d=identicon&r=PG', 'display_name': 'Utsav', 'link': 'https://stackoverflow.com/users/2106934/utsav'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625120583, 'last_edit_date': 1625120583, 'creation_date': 1625120261, 'answer_id': 68205200, 'question_id': 68205087, 'content_license': 'CC BY-SA 4.0', 'body': '<pre><code>df1 = df.groupby((df[\\'B\\'].shift() != df[\\'B\\']).cumsum()).mean().reset_index(drop=True)\\ndf1 = df1[df1[\\'B\\'] == 1].astype(int).reset_index(drop=True)\\ndf1\\n</code></pre>\\n<p><strong>Output</strong></p>\\n<pre><code>    A   B\\n0   2   1\\n1   3   1\\n</code></pre>\\n<p><strong>Explanation</strong></p>\\n<p>We are checking if each row\\'s value of B is not equal to next value using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html\" rel=\"nofollow noreferrer\">pd.shift</a>, if so then we are grouping those values and calculating its mean and assigning it to new dataframe <code>df1</code>.</p>\\n<p>Since we have mean of groups of all consecutive 0s and 1s, so we are then filtering only values of <code>B==1</code>.</p>\\n'}, {'owner': {'reputation': 1114, 'user_id': 6393476, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/5e8xE.png?s=128&g=1', 'display_name': 'EnriqueBet', 'link': 'https://stackoverflow.com/users/6393476/enriquebet'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625110433, 'creation_date': 1625110433, 'answer_id': 68203969, 'question_id': 68203921, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Just you need to split the sentence every time you see the &quot;&gt;&gt;&quot; element:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>sentences = []\\nsentence = &quot;&quot;\\nfor item in word:\\n    if &quot;&gt;&gt;&quot; == item:\\n        if sentence:\\n            sentences.append(sentence)\\n        sentence = &quot;&quot;\\n        continue\\n    sentence += item\\n</code></pre>\\n<p>That should do it, in order to integrate this solution to your dataframe you might need to create a method for this algorithm</p>\\n'}, {'owner': {'reputation': 16385, 'user_id': 15497888, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/W1on0.jpg?s=128&g=1', 'display_name': 'Henry Ecker', 'link': 'https://stackoverflow.com/users/15497888/henry-ecker'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625106154, 'creation_date': 1625106154, 'answer_id': 68203525, 'question_id': 68203377, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Try with <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.fliplr.html#numpy-fliplr\" rel=\"nofollow noreferrer\"><code>fliplr</code></a>:</p>\\n<pre><code># Get numpy structure\\nx = df.loc[:, \\'Sup_1 ID\\':].to_numpy()\\n# flip left to right\\na = np.fliplr(x)\\n# Overwrite not NaN values in x with not NaN in a\\nx[~np.isnan(x)] = a[~np.isnan(a)]\\n# Update DataFrame\\ndf.loc[:, \\'Sup_1 ID\\':] = x\\n</code></pre>\\n<p><code>df</code>:</p>\\n<pre><code>   Emp_ID  Sup_1 ID  Sup_2 ID  Sup_3 ID  Sup_4 ID\\n0     123     789.0     678.0     456.0     234.0\\n1     234     789.0     678.0     456.0       NaN\\n2     456     789.0     678.0       NaN       NaN\\n3     678     789.0       NaN       NaN       NaN\\n4     789       NaN       NaN       NaN       NaN\\n</code></pre>\\n<hr />\\n<p>DataFrame Constructor and imports:</p>\\n<pre><code>import numpy as np\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\\n    \\'Emp_ID\\': [123, 234, 456, 678, 789],\\n    \\'Sup_1 ID\\': [234.0, 456.0, 678.0, 789.0, np.nan],\\n    \\'Sup_2 ID\\': [456.0, 678.0, 789.0, np.nan, np.nan],\\n    \\'Sup_3 ID\\': [678.0, 789.0, np.nan, np.nan, np.nan],\\n    \\'Sup_4 ID\\': [789.0, np.nan, np.nan, np.nan, np.nan]\\n})\\n</code></pre>\\n'}, {'owner': {'reputation': 14965, 'user_id': 765395, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/Xc817.png?s=128&g=1', 'display_name': 'Robin Mackenzie', 'link': 'https://stackoverflow.com/users/765395/robin-mackenzie'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625106100, 'creation_date': 1625106100, 'answer_id': 68203512, 'question_id': 68203385, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>See comments in the code for details on the steps:</p>\\n<pre><code>import io\\nimport pandas as pd\\n\\n# data as a string\\ntext = '''date     ctr     code          \\n12-May   CN      1111/abc/12-e\\n12-May   CN      1112/abc/wds/12-e\\n12-May   CN      1113/abc/12-e'''\\n\\n# your original data frame\\ndf = pd.read_csv(io.StringIO(text), sep=r'\\\\s+')\\n\\n# split code to new columns (0-based)\\ndf2 = df.code.str.split(&quot;/&quot;, expand=True)\\n\\n# rename new columns from 0-base to 1-based\\ndf2 = df2.rename(columns=lambda x: f&quot;code{int(x)+1}&quot;)\\n\\n# join with original dataframe\\ndf2 = df.join(df2)\\n\\n# drop original code column\\ndf2.drop(columns=['code'], inplace=True)\\n\\n# test\\nprint(df2)\\n</code></pre>\\n<p>Outputs:</p>\\n<pre><code>     date ctr code1 code2 code3 code4\\n0  12-May  CN  1111   abc  12-e  None\\n1  12-May  CN  1112   abc   wds  12-e\\n2  12-May  CN  1113   abc  12-e  None\\n</code></pre>\\n\"}, {'owner': {'reputation': 910, 'user_id': 11004559, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/4a49114ec2a40d6bc06d8b26dbe87688?s=128&d=identicon&r=PG&f=1', 'display_name': 'Xu Qiushi', 'link': 'https://stackoverflow.com/users/11004559/xu-qiushi'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625104571, 'creation_date': 1625104571, 'answer_id': 68203360, 'question_id': 68197524, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>First of all, let's solve this problem.</p>\\n<pre><code>raw = {\\n    &quot;20211229&quot;: {\\n        &quot;00101&quot;: [\\n            &quot;ë¹ê³ 101-1&quot;,\\n            &quot;ë¹ê³ 101-2&quot;,\\n            &quot;ë¹ê³ 101-3&quot;,\\n            {0: [&quot; UT1213K&quot;, &quot;1&quot;, &quot;11.00&quot;, &quot;11&quot;, &quot;2&quot;]},\\n        ]\\n    },\\n    &quot;20211230&quot;: {\\n        &quot;00102&quot;: [\\n            &quot;ë¹ê³ 102-1&quot;,\\n            &quot;ë¹ê³ 102-2&quot;,\\n            &quot;ë¹ê³ 102-3&quot;,\\n            {0: [&quot;B 001&quot;, &quot;2&quot;, &quot;22.00&quot;, &quot;44&quot;, &quot;5&quot;]},\\n        ]\\n    },\\n    &quot;20211231&quot;: {\\n        &quot;00103&quot;: [\\n            &quot;ë¹ê³ 103-1&quot;,\\n            &quot;ë¹ê³ 103-2&quot;,\\n            &quot;ë¹ê³ 103-3&quot;,\\n            {\\n                0: [&quot;B 004&quot;, &quot;10&quot;, &quot;66.00&quot;, &quot;660&quot;, &quot;66&quot;],\\n                1: [&quot;B 005&quot;, &quot;20&quot;, &quot;77.00&quot;, &quot;1540&quot;, &quot;154&quot;],\\n                2: [&quot;B 006&quot;, &quot;30&quot;, &quot;88.00&quot;, &quot;2640&quot;, &quot;264&quot;],\\n                3: [&quot;B 007&quot;, &quot;40&quot;, &quot;99.00&quot;, &quot;3960&quot;, &quot;396&quot;],\\n            },\\n        ],\\n        &quot;00104&quot;: [\\n            &quot;ë¹ê³ 1&quot;,\\n            &quot;ë¹ê³ 2&quot;,\\n            &quot;&quot;,\\n            {\\n                0: [&quot;B 003&quot;, &quot;3&quot;, &quot;33.00&quot;, &quot;99&quot;, &quot;10&quot;],\\n                1: [&quot;B 004&quot;, &quot;4&quot;, &quot;44.00&quot;, &quot;176&quot;, &quot;18&quot;],\\n            },\\n        ],\\n    },\\n}\\n\\nformatted_dict = []\\nfor first_level_key, first_level_value in raw.items():\\n    for second_level_key, second_level_value in first_level_value.items():\\n        third_level_tmp_list = []\\n        for third_level_value in second_level_value:\\n            if isinstance(third_level_value, str):\\n                third_level_tmp_list.append(third_level_value)\\n        third_level_tmp_dict = dict(\\n            zip([&quot;remark1&quot;, &quot;remark2&quot;, &quot;remark3&quot;], third_level_tmp_list)\\n        )\\n        for third_level_value in second_level_value:\\n            if isinstance(third_level_value, dict):\\n                for (\\n                    fourth_level_key,\\n                    fourth_level_value,\\n                ) in third_level_value.items():\\n                    new_record = {}\\n                    new_record.update(\\n                        {\\n                            &quot;date&quot;: first_level_key,\\n                            &quot;customer_code&quot;: second_level_key,\\n                            &quot;item&quot;: fourth_level_key,\\n                        }\\n                    )\\n                    new_record.update(\\n                        dict(\\n                            zip(\\n                                [\\n                                    &quot;item_code&quot;,\\n                                    &quot;qty&quot;,\\n                                    &quot;unit_price&quot;,\\n                                    &quot;supply_price&quot;,\\n                                    &quot;tax_amount&quot;,\\n                                ],\\n                                fourth_level_value,\\n                            )\\n                        )\\n                    )\\n                    new_record.update(third_level_tmp_dict)\\n                    formatted_dict.append(new_record)\\n\\nprint(formatted_dict)\\nresult = pd.DataFrame(formatted_dict).set_index(\\n    [&quot;date&quot;, &quot;customer_code&quot;, &quot;remark1&quot;, &quot;remark2&quot;, &quot;remark3&quot;]\\n)[[&quot;item&quot;, &quot;item_code&quot;, &quot;qty&quot;, &quot;unit_price&quot;, &quot;supply_price&quot;, &quot;tax_amount&quot;]]\\npd.set_option(&quot;display.max_columns&quot;, 500)\\npd.set_option(&quot;display.width&quot;, 1000)\\nprint(result)\\n</code></pre>\\n<p>Second, I suggest you do not construct raw data like that. I suggest you make your raw data like this.</p>\\n<pre><code>[\\n    {\\n        &quot;date&quot;: &quot;20211229&quot;,\\n        &quot;customer_code&quot;: &quot;00101&quot;,\\n        &quot;item&quot;: 0,\\n        &quot;item_code&quot;: &quot; UT1213K&quot;,\\n        &quot;qty&quot;: &quot;1&quot;,\\n        &quot;unit_price&quot;: &quot;11.00&quot;,\\n        &quot;supply_price&quot;: &quot;11&quot;,\\n        &quot;tax_amount&quot;: &quot;2&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 101-1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 101-2&quot;,\\n        &quot;remark3&quot;: &quot;ë¹ê³ 101-3&quot;,\\n    },\\n    {\\n        &quot;date&quot;: &quot;20211230&quot;,\\n        &quot;customer_code&quot;: &quot;00102&quot;,\\n        &quot;item&quot;: 0,\\n        &quot;item_code&quot;: &quot;B 001&quot;,\\n        &quot;qty&quot;: &quot;2&quot;,\\n        &quot;unit_price&quot;: &quot;22.00&quot;,\\n        &quot;supply_price&quot;: &quot;44&quot;,\\n        &quot;tax_amount&quot;: &quot;5&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 102-1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 102-2&quot;,\\n        &quot;remark3&quot;: &quot;ë¹ê³ 102-3&quot;,\\n    },\\n    {\\n        &quot;date&quot;: &quot;20211231&quot;,\\n        &quot;customer_code&quot;: &quot;00103&quot;,\\n        &quot;item&quot;: 0,\\n        &quot;item_code&quot;: &quot;B 004&quot;,\\n        &quot;qty&quot;: &quot;10&quot;,\\n        &quot;unit_price&quot;: &quot;66.00&quot;,\\n        &quot;supply_price&quot;: &quot;660&quot;,\\n        &quot;tax_amount&quot;: &quot;66&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 103-1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 103-2&quot;,\\n        &quot;remark3&quot;: &quot;ë¹ê³ 103-3&quot;,\\n    },\\n    {\\n        &quot;date&quot;: &quot;20211231&quot;,\\n        &quot;customer_code&quot;: &quot;00103&quot;,\\n        &quot;item&quot;: 1,\\n        &quot;item_code&quot;: &quot;B 005&quot;,\\n        &quot;qty&quot;: &quot;20&quot;,\\n        &quot;unit_price&quot;: &quot;77.00&quot;,\\n        &quot;supply_price&quot;: &quot;1540&quot;,\\n        &quot;tax_amount&quot;: &quot;154&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 103-1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 103-2&quot;,\\n        &quot;remark3&quot;: &quot;ë¹ê³ 103-3&quot;,\\n    },\\n    {\\n        &quot;date&quot;: &quot;20211231&quot;,\\n        &quot;customer_code&quot;: &quot;00103&quot;,\\n        &quot;item&quot;: 2,\\n        &quot;item_code&quot;: &quot;B 006&quot;,\\n        &quot;qty&quot;: &quot;30&quot;,\\n        &quot;unit_price&quot;: &quot;88.00&quot;,\\n        &quot;supply_price&quot;: &quot;2640&quot;,\\n        &quot;tax_amount&quot;: &quot;264&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 103-1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 103-2&quot;,\\n        &quot;remark3&quot;: &quot;ë¹ê³ 103-3&quot;,\\n    },\\n    {\\n        &quot;date&quot;: &quot;20211231&quot;,\\n        &quot;customer_code&quot;: &quot;00103&quot;,\\n        &quot;item&quot;: 3,\\n        &quot;item_code&quot;: &quot;B 007&quot;,\\n        &quot;qty&quot;: &quot;40&quot;,\\n        &quot;unit_price&quot;: &quot;99.00&quot;,\\n        &quot;supply_price&quot;: &quot;3960&quot;,\\n        &quot;tax_amount&quot;: &quot;396&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 103-1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 103-2&quot;,\\n        &quot;remark3&quot;: &quot;ë¹ê³ 103-3&quot;,\\n    },\\n    {\\n        &quot;date&quot;: &quot;20211231&quot;,\\n        &quot;customer_code&quot;: &quot;00104&quot;,\\n        &quot;item&quot;: 0,\\n        &quot;item_code&quot;: &quot;B 003&quot;,\\n        &quot;qty&quot;: &quot;3&quot;,\\n        &quot;unit_price&quot;: &quot;33.00&quot;,\\n        &quot;supply_price&quot;: &quot;99&quot;,\\n        &quot;tax_amount&quot;: &quot;10&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 2&quot;,\\n        &quot;remark3&quot;: &quot;&quot;,\\n    },\\n    {\\n        &quot;date&quot;: &quot;20211231&quot;,\\n        &quot;customer_code&quot;: &quot;00104&quot;,\\n        &quot;item&quot;: 1,\\n        &quot;item_code&quot;: &quot;B 004&quot;,\\n        &quot;qty&quot;: &quot;4&quot;,\\n        &quot;unit_price&quot;: &quot;44.00&quot;,\\n        &quot;supply_price&quot;: &quot;176&quot;,\\n        &quot;tax_amount&quot;: &quot;18&quot;,\\n        &quot;remark1&quot;: &quot;ë¹ê³ 1&quot;,\\n        &quot;remark2&quot;: &quot;ë¹ê³ 2&quot;,\\n        &quot;remark3&quot;: &quot;&quot;,\\n    },\\n]\\n</code></pre>\\n<p>If your raw data like this, You could just get your result simply like this.</p>\\n<pre><code>result = pd.DataFrame(raw).set_index(\\n    [&quot;date&quot;, &quot;customer_code&quot;, &quot;remark1&quot;, &quot;remark2&quot;, &quot;remark3&quot;]\\n)[[&quot;item&quot;, &quot;item_code&quot;, &quot;qty&quot;, &quot;unit_price&quot;, &quot;supply_price&quot;, &quot;tax_amount&quot;]]\\n</code></pre>\\n\"}, {'owner': {'reputation': 74685, 'user_id': 2538939, 'user_type': 'registered', 'accept_rate': 50, 'profile_image': 'https://www.gravatar.com/avatar/876f7ddaf27a16c17a62b8a9705b45f1?s=128&d=identicon&r=PG&f=1', 'display_name': 'Code Different', 'link': 'https://stackoverflow.com/users/2538939/code-different'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1625104488, 'creation_date': 1625104488, 'answer_id': 68203353, 'question_id': 68203313, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Yes you can:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>differential = df[\\'Price2\\'] - df[\\'Price\\']\\nticker = df.loc[differential.idxmax(), \\'Ticker\\']\\n</code></pre>\\n<p>But seeing that you are working with stock prices, absolute price differential has little meaning. A $10 differential means more to a $136 stock (like Apple) than to a $3400 stock (like Amazon) and it\\'s a rounding error on a $418k stock (Berkshire Hathaway). A better measure is to use differential percentage:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>differential = df[\\'Price2\\'] / df[\\'Price\\'] - 1\\nticker = df.loc[differential.idxmax(), \\'Ticker\\']\\n</code></pre>\\n'}, {'owner': {'reputation': 16, 'user_id': 13213655, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/3c9e97530cc9282fe40a86f50087b8f4?s=128&d=identicon&r=PG&f=1', 'display_name': 'Precel', 'link': 'https://stackoverflow.com/users/13213655/precel'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625098240, 'creation_date': 1625098240, 'answer_id': 68202868, 'question_id': 68202268, 'content_license': 'CC BY-SA 4.0', 'body': '<p>I found a similar question here: <a href=\"https://stackoverflow.com/questions/32249960/in-python-pandas-start-row-index-from-1-instead-of-zero-without-creating-additi\">In Python pandas, start row index from 1 instead of zero without creating additional column</a></p>\\n<p>For your question, it would be as simple as adding the following line:</p>\\n<pre><code>df[&quot;Field&quot;] = np.arange(1, len(df) + 1)\\n</code></pre>\\n'}, {'owner': {'reputation': 240, 'user_id': 15448022, 'user_type': 'registered', 'profile_image': 'https://lh3.googleusercontent.com/-MzW4a3wzzuA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclYe8-QKx-IsdFXk04ohEJX8Phq_g/s96-c/photo.jpg?sz=128', 'display_name': 'aj7amigo', 'link': 'https://stackoverflow.com/users/15448022/aj7amigo'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625096262, 'last_edit_date': 1625096262, 'creation_date': 1625095574, 'answer_id': 68202582, 'question_id': 68202480, 'content_license': 'CC BY-SA 4.0', 'body': \"<pre><code>cols = [&quot;Month&quot;, &quot;Type_failure&quot;]\\ngrouped_df1 = df.groupby(cols).size()\\ngrouped_df1.unstack()[['Roof fall']].stack()\\n# to get as a new dataframe, use below code\\n# df2 = pd.DataFrame(grouped_df1.unstack()[['Roof fall']].stack()).rename(columns={0: &quot;count&quot;})\\n</code></pre>\\n\"}, {'owner': {'reputation': 7962, 'user_id': 15239951, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/976f07be84907aad663ce8c1c51cf5c4?s=128&d=identicon&r=PG', 'display_name': 'Corralien', 'link': 'https://stackoverflow.com/users/15239951/corralien'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625086968, 'last_edit_date': 1625086968, 'creation_date': 1625086643, 'answer_id': 68201541, 'question_id': 68199111, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>I used a more lightweight dataframe:</p>\\n<pre><code>&gt;&gt;&gt; df\\n          id           grades\\n0    smithsm     [1, 9, 2, 6]  # &lt;- 9\\n1   mullenjb  [1, 5, 8, 4, 7]\\n2    swainrl        [4, 2, 9]  # &lt;- 9\\n3  rankinsns           [5, 2]\\n4  carlsonrm  [7, 4, 6, 3, 2]  # &lt;- 3\\n5     ragomv        [6, 1, 5]\\n6    smithdl  [2, 9, 6, 7, 3]  # &lt;- 3 &amp; 9\\n7  kappleraj        [9, 5, 8]  # &lt;- 9\\n8   iresonss  [8, 6, 7, 5, 4]\\n9  conklincc           [8, 6]\\n</code></pre>\\n<p>How to find grade list [3, 9]?</p>\\n<p>Explode your column <code>grades</code> and find the grade is in the grade list.</p>\\n<pre><code>&gt;&gt;&gt; df.loc[df['grades'].explode().isin([3, 9]).groupby(level=0).any()\\n          id           grades\\n0    smithsm     [1, 9, 2, 6]\\n2    swainrl        [4, 2, 9]\\n4  carlsonrm  [7, 4, 6, 3, 2]\\n6    smithdl  [2, 9, 6, 7, 3]\\n7  kappleraj        [9, 5, 8]\\n</code></pre>\\n<p>Same as:</p>\\n<pre>\\n>>> df.loc[df['grades'].explode() \\\\\\n      <b>.apply(lambda x: x in [3, 9])</b> \\\\\\n      .groupby(level=0).any()]`\\n</pre>\\n\"}, {'owner': {'reputation': 242, 'user_id': 6232483, 'user_type': 'registered', 'profile_image': 'https://graph.facebook.com/10101422670608505/picture?type=large', 'display_name': 'Clay Shwery', 'link': 'https://stackoverflow.com/users/6232483/clay-shwery'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1625085048, 'creation_date': 1625085048, 'answer_id': 68201283, 'question_id': 68200658, 'content_license': 'CC BY-SA 4.0', 'body': '<p>With the clarified requirements in the comments, here is a working solution, that can be run and returns the expected result.</p>\\n<p>It should be very efficient as well.</p>\\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\\nimport io\\n\\ndf_txt = &quot;&quot;&quot;\\nId  Calls   Distance  AirportSize\\n0   HVN19   3.727263    2\\n1   HVN19   3.727263    1\\n2   HVN19   11.485452   2\\n3   CCA839  2.094717    2\\n4   CCA839  2.094717    1\\n5   CCA839  6.622537    2\\n6   CES219  1.751279    1\\n7   CES219  5.436940    4\\n8   CES219  6.950773    4\\n9   ETH704  2.976954    4\\n10  ETH704  3.844980    4\\n11  ETH704  5.452634    4\\n&quot;&quot;&quot;\\n    \\ndf1 = pd.read_fwf(io.StringIO(df_txt)).set_index(\\'Id\\').rename(columns = {\\'Calls\\':\\'Callsign\\'})\\n\\n\\n# Need to sort or to ensure following will always work\\ndf1.sort_values([\\'Callsign\\',\\'Distance\\'])\\n# calc distance between each subsequent ap - fillna as 0 for closest ap\\ndf1[\\'dist_diff\\'] = df1.groupby(\\'Callsign\\')[\\'Distance\\'].diff().fillna(0)\\ndf1[\\'dist_diff\\'] = df1.groupby(\\'Callsign\\')[\\'dist_diff\\'].cumsum()\\n\\n# Keep only smallest qualifiying airport for each callsign, tiebreak with Distance\\ndf1[df1[\\'dist_diff\\']&lt;1].sort_values([\\'Callsign\\',\\'AirportSize\\',\\'Distance\\']).drop_duplicates(\\'Callsign\\').drop(\\'dist_diff\\',axis=1)\\n</code></pre>\\n<p>output</p>\\n<pre><code>    Callsign    Distance    AirportSize\\nId          \\n4   CCA839  2.094717    1\\n6   CES219  1.751279    1\\n9   ETH704  2.976954    4\\n1   HVN19   3.727263    1\\n</code></pre>\\n'}, {'owner': {'reputation': 6581, 'user_id': 7465462, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/GjL5q.jpg?s=128&g=1', 'display_name': 'Ric S', 'link': 'https://stackoverflow.com/users/7465462/ric-s'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1625061259, 'creation_date': 1625061259, 'answer_id': 68196162, 'question_id': 68195637, 'content_license': 'CC BY-SA 4.0', 'body': '<p>I found a quite tricky solution that works.</p>\\n<pre><code>import pandas as pd\\n\\n# define groups between two LDE\\ndf[\\'Group\\'] = (df[\\'K\\'] == \\'LDE\\').cumsum().shift(1, fill_value=0)\\n\\n# custom function to perform your subtraction\\ndef f(x):\\n    if x.loc[x[\\'J\\'] == \\'FDE\\', \\'A\\'].size == 0:\\n        return None\\n    else:\\n        return x.loc[x[\\'K\\'] == \\'LDE\\', \\'D\\'].iloc[0] - x.loc[x[\\'J\\'] == \\'FDE\\', \\'A\\'].iloc[0]\\n\\n# get list of numerical results\\nresults = df.groupby(\\'Group\\').apply(f).tolist()\\n\\n# input the list into the specified LDE rows\\ndf.loc[df[\\'K\\'] == \\'LDE\\', \\'Results\\'] = results\\n</code></pre>\\n<hr />\\n<p>Results</p>\\n<p><a href=\"https://i.stack.imgur.com/GWnod.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/GWnod.png\" alt=\"enter image description here\" /></a></p>\\n<hr />\\n<p>Starting data</p>\\n<pre><code>df = pd.DataFrame([[\\'03-01-2011\\', 523, 698, 284, 33, 416, 675, 300, 690, 314, \\'\\', \\'\\', \\'FDM\\', \\'\\'], [\\'27-01-2011\\', 353, 1, 50, 547, 514, 957, 804, 490, 108, \\'\\', \\'LDE\\', \\'\\', \\'\\'],\\n                   [\\'28-01-2011\\', 307, 837, 656, 755, 792, 568, 119, 439, 943, \\'FDE\\', \\'\\', \\'\\', \\'\\'], [\\'31-01-2011\\', 327, 409, 155, 358, 120, 401, 385, 965, 888, \\'\\', \\'\\', \\'\\', \\'LDM\\'], [\\'01-02-2011\\', 686, 313, 714, 12, 140, 112, 589, 908, 605, \\'\\', \\'\\', \\'FDM\\', \\'\\'], [\\'24-02-2011\\', 161, 846, 816, 223, 387, 566, 435, 567, 36, \\'\\', \\'LDE\\', \\'\\', \\'\\'], [\\'25-02-2011\\', 889, 652, 190, 324, 947, 778, 575, 604, 314, \\'FDE\\', \\'\\', \\'\\', \\'\\'], [\\'28-02-2011\\', 704, 33, 232, 630, 344, 796, 331, 409, 597, \\'\\', \\'\\', \\'\\', \\'LDM\\'], [\\'01-03-2011\\', 592, 148, 974, 540, 848, 393, 505, 699, 315, \\'\\', \\'\\', \\'FDM\\', \\'\\'], [\\'31-03-2011\\', 938, 768, 325, 756, 971, 644, 546, 238, 376, \\'\\', \\'LDE\\', \\'\\', \\'LDM\\'], [\\'01-04-2011\\', 385, 298, 654, 655, 2, 112, 960, 306, 477, \\'FDE\\', \\'\\', \\'FDM\\', \\'\\'], [\\'28-04-2011\\', 704, 516, 785, 152, 355, 348, 106, 611, 426, \\'\\', \\'LDE\\', \\'\\', \\'\\'], [\\'29-04-2011\\', 753, 719, 776, 826, 756, 370, 660, 536, 903, \\'FDE\\', \\'\\', \\'\\', \\'LDM\\'], [\\'02-05-2011\\', 222, 28, 102, 363, 952, 860, 48, 976, 478, \\'\\', \\'\\', \\'FDM\\', \\'\\'], [\\'26-05-2011\\', 361, 588, 866, 884, 809, 662, 801, 843, 668, \\'\\', \\'LDE\\', \\'\\', \\'\\']],\\n                  columns=[\\'Date\\'] + list(map(chr, range(65, 78))))\\n</code></pre>\\n'}, {'owner': {'reputation': 3049, 'user_id': 14311263, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/6GaBJ.png?s=128&g=1', 'display_name': 'Timus', 'link': 'https://stackoverflow.com/users/14311263/timus'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1624986065, 'creation_date': 1624986065, 'answer_id': 68182904, 'question_id': 68178902, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Does this do what you want:</p>\\n<pre><code>fillData.set_index('time', drop=True, inplace=True)\\ncondition = fillData.fill.eq(1)\\nfillData['500 milli'] = (condition.rolling(pd.Timedelta('500ms'))\\n                                  .agg(any)\\n                                  .astype(int))\\nfillData['6 minutes'] = (condition.rolling(pd.Timedelta('6m'))\\n                                  .agg(any)\\n                                  .astype(int))\\nfillData['6 minutes'][fillData['500 milli'].eq(1)] = 0\\nfillData.reset_index(drop=False, inplace=True)\\n</code></pre>\\n<p>I'm not sure how <code>fillData</code> is sorted. My assumption is that the sorting is ascending (in time). Otherwise you have to reverse it.</p>\\n\"}, {'owner': {'reputation': 36100, 'user_id': 5168011, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/eb70c32e1e533e7c0cb2f9317a6b2aa3?s=128&d=identicon&r=PG&f=1', 'display_name': 'Guy', 'link': 'https://stackoverflow.com/users/5168011/guy'}, 'is_accepted': True, 'score': 0, 'last_activity_date': 1624794367, 'creation_date': 1624794367, 'answer_id': 68150825, 'question_id': 68150589, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>You can create a function to calculate the score and apply it to every <code>timeSpent</code></p>\\n<pre><code>def get_score(num):\\n    if num &lt;= 2500: return 1\\n    if num &gt;= 5500: return 0\\n    x = 1\\n    for _ in range((num - 2500) // 100):\\n        x *= 0.8\\n    return x\\n\\ndf = pd.DataFrame({'question': [a, b, c, d, e], 'timeSpent': [5354, 2344, 2555, 5200, 3567]})\\ndf['Score'] = df.timeSpent.apply(lambda x: get_score(x))\\n</code></pre>\\n<p>Output:</p>\\n<pre><code>  question  timeSpent     Score\\n0        a       5354  0.001934\\n1        b       2344  1.000000\\n2        c       2555  1.000000\\n3        d       5200  0.002418\\n4        e       3567  0.107374\\n</code></pre>\\n\"}, {'owner': {'reputation': 2236, 'user_id': 3526116, 'user_type': 'registered', 'accept_rate': 100, 'profile_image': 'https://www.gravatar.com/avatar/e9a7854e15da93050a28c70ccb3490a2?s=128&d=identicon&r=PG&f=1', 'display_name': 'crayxt', 'link': 'https://stackoverflow.com/users/3526116/crayxt'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1623760637, 'creation_date': 1623760637, 'answer_id': 67986413, 'question_id': 67986112, 'content_license': 'CC BY-SA 4.0', 'body': '<p>The format of <code>df</code> seems weird (data points in columns, not rows).</p>\\n<p>Below is not the cleanest solution at all:</p>\\n<pre><code>import numpy as np\\n\\nlookup_df = df1.set_index([&quot;Name&quot;, &quot;Segment&quot;, &quot;Axis&quot;]).T\\n\\ndef find_interp(row):\\n    try:\\n        res = np.interp([row[&quot;x&quot;]], lookup_df[(row[&quot;Name&quot;], row[&quot;Segment&quot;], &quot;x&quot;)], lookup_df[(row[&quot;Name&quot;], row[&quot;Segment&quot;], &quot;y&quot;)])\\n    except:\\n        res = [np.nan]\\n    return res[0]\\n\\n\\n&gt;&gt;&gt; df2[&quot;y&quot;] = df2.apply(find_interp, axis=1)\\n&gt;&gt;&gt; df2\\n      Name  Segment    x     y\\n0   Amazon        1  1.0  0.40\\n1   Amazon        2  2.3  1.15\\n2  Netflix        1  4.1   NaN\\n3  Netflix        2  5.5   NaN\\n</code></pre>\\n'}, {'owner': {'reputation': 65304, 'user_id': 9840637, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/QqY9S.png?s=128&g=1', 'display_name': 'anky', 'link': 'https://stackoverflow.com/users/9840637/anky'}, 'is_accepted': True, 'score': 10, 'last_activity_date': 1615400752, 'last_edit_date': 1615400752, 'creation_date': 1615399384, 'answer_id': 66570407, 'question_id': 66570375, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>If the seperator is <code>' '</code> ,you can try <code>series.str.count</code> , else you can replace the <code>sep</code></p>\\n<pre><code>n=3\\ndf[df['answer'].str.count(' ').gt(n-1)]\\n</code></pre>\\n<p>To include Multiple spaces <code>#credits @piRSquared</code></p>\\n<pre><code>df['answer'].str.count('\\\\s+').gt(2)\\n</code></pre>\\n<p>Or using list comprehension:</p>\\n<pre><code>n= 3\\ndf[[len(i.split())&gt;n for i in df['answer']]] #should be faster than above\\n</code></pre>\\n<hr />\\n<pre><code>                    answer some_number\\n0  hello how are you doing         1.0\\n2          bye bye bye bye         0.0\\n5     Who let the dogs out         0.0\\n6            1 + 1 + 1 + 2         1.0\\n</code></pre>\\n\"}, {'owner': {'reputation': 1615, 'user_id': 5852001, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/44af1a3008b639742fa883da3df467f4?s=128&d=identicon&r=PG', 'display_name': 'Mike', 'link': 'https://stackoverflow.com/users/5852001/mike'}, 'is_accepted': True, 'score': 127, 'last_activity_date': 1607811441, 'last_edit_date': 1607811441, 'creation_date': 1458115059, 'answer_id': 36029761, 'question_id': 36028759, 'content_license': 'CC BY-SA 4.0', 'body': '<p>Despite sqlite being part of the Python Standard Library and is a nice and easy interface to SQLite databases, the Pandas tutorial <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#reading-tables\" rel=\"noreferrer\">states</a>:</p>\\n<blockquote>\\n<p>Note In order to use read_sql_table(), you must have the SQLAlchemy\\noptional dependency installed.</p>\\n</blockquote>\\n<p>But Pandas still supports sqlite3 access if you want to avoid installing SQLAlchemy:</p>\\n<pre><code>import sqlite3\\nimport pandas as pd\\n# Create your connection.\\ncnx = sqlite3.connect(\\'file.db\\')\\n\\ndf = pd.read_sql_query(&quot;SELECT * FROM table_name&quot;, cnx)\\n</code></pre>\\n<p>As stated <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sqlite-fallback\" rel=\"noreferrer\">here</a>, but you need to know the name of the used table in advance.</p>\\n'}, {'owner': {'reputation': 70886, 'user_id': 1426056, 'user_type': 'registered', 'accept_rate': 87, 'profile_image': 'https://www.gravatar.com/avatar/3ac815b6fca201737db888a28efd7248?s=128&d=identicon&r=PG', 'display_name': 'waitingkuo', 'link': 'https://stackoverflow.com/users/1426056/waitingkuo'}, 'is_accepted': True, 'score': 3762, 'last_activity_date': 1607675908, 'last_edit_date': 1607675908, 'creation_date': 1368169678, 'answer_id': 16476974, 'question_id': 16476924, 'content_license': 'CC BY-SA 4.0', 'body': '<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html#pandas-dataframe-iterrows\" rel=\"noreferrer\"><code>DataFrame.iterrows</code></a> is a generator which yields both the index and row (as a Series):</p>\\n<pre><code>import pandas as pd\\n\\ndf = pd.DataFrame({\\'c1\\': [10, 11, 12], \\'c2\\': [100, 110, 120]})\\n\\nfor index, row in df.iterrows():\\n    print(row[\\'c1\\'], row[\\'c2\\'])\\n</code></pre>\\n\\n<pre><code>10 100\\n11 110\\n12 120\\n</code></pre>\\n'}, {'owner': {'reputation': 6994, 'user_id': 8467558, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/tqOGB.jpg?s=128&g=1', 'display_name': 'MaFF', 'link': 'https://stackoverflow.com/users/8467558/maff'}, 'is_accepted': True, 'score': 113, 'last_activity_date': 1601895928, 'last_edit_date': 1601895928, 'creation_date': 1520695194, 'answer_id': 49210679, 'question_id': 49198068, 'content_license': 'CC BY-SA 4.0', 'body': '<p>The column must be a <code>datetime</code> dtype, as from using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\" rel=\"noreferrer\"><code>pd.to_datetime</code></a>, then you can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.tz_localize.html\" rel=\"noreferrer\"><code>tz_localize</code></a> to change the time zone, a naÃ¯ve timestamp corresponds to time zone <code>None</code>:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>testdata[\\'time\\'].dt.tz_localize(None)\\n</code></pre>\\n<p>Unless the column is an index (<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html\" rel=\"noreferrer\"><code>DatetimeIndex</code></a>), the <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor\" rel=\"noreferrer\"><code>.dt</code> accessor</a> must be used to access <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\" rel=\"noreferrer\">pandas datetime functions</a>.</p>\\n'}, {'owner': {'reputation': 3919, 'user_id': 3301113, 'user_type': 'registered', 'profile_image': 'https://www.gravatar.com/avatar/93a8377316d825afcc391cebcb768349?s=128&d=identicon&r=PG&f=1', 'display_name': 'fmarm', 'link': 'https://stackoverflow.com/users/3301113/fmarm'}, 'is_accepted': True, 'score': 2, 'last_activity_date': 1587517882, 'creation_date': 1587517882, 'answer_id': 61355712, 'question_id': 61355655, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>In two steps:</p>\\n\\n<p>First use <code>map</code> to create a new column with the values associated with the countries</p>\\n\\n<pre><code>df['country_value'] = df.countries.map(howToSortDict)\\n</code></pre>\\n\\n<p>Then <code>sort</code> and <code>drop</code> the new column</p>\\n\\n<pre><code>df.sort_values('country_value').drop('country_value',axis=1)\\n\\n#    countries   amount\\n#1   Russia      2000\\n#2   China       3000\\n#0   Brazil      1000\\n</code></pre>\\n\"}, {'owner': {'reputation': 719478, 'user_id': 190597, 'user_type': 'registered', 'accept_rate': 88, 'profile_image': 'https://www.gravatar.com/avatar/aabc98d5c6482ca0e1405ec97710f30a?s=128&d=identicon&r=PG&f=1', 'display_name': 'unutbu', 'link': 'https://stackoverflow.com/users/190597/unutbu'}, 'is_accepted': True, 'score': 856, 'last_activity_date': 1578606471, 'last_edit_date': 1578606471, 'creation_date': 1384196595, 'answer_id': 19913845, 'question_id': 19913659, 'content_license': 'CC BY-SA 4.0', 'body': '<p><strong>If you only have two choices to select from:</strong></p>\\n\\n<pre><code>df[\\'color\\'] = np.where(df[\\'Set\\']==\\'Z\\', \\'green\\', \\'red\\')\\n</code></pre>\\n\\n<p>For example,</p>\\n\\n<pre><code>import pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame({\\'Type\\':list(\\'ABBC\\'), \\'Set\\':list(\\'ZZXY\\')})\\ndf[\\'color\\'] = np.where(df[\\'Set\\']==\\'Z\\', \\'green\\', \\'red\\')\\nprint(df)\\n</code></pre>\\n\\n<p>yields</p>\\n\\n<pre><code>  Set Type  color\\n0   Z    A  green\\n1   Z    B  green\\n2   X    B    red\\n3   Y    C    red\\n</code></pre>\\n\\n<hr>\\n\\n<p><strong>If you have more than two conditions then use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html\" rel=\"noreferrer\"><code>np.select</code></a></strong>. For example, if you want <code>color</code> to be </p>\\n\\n<ul>\\n<li><code>yellow</code> when <code>(df[\\'Set\\'] == \\'Z\\') &amp; (df[\\'Type\\'] == \\'A\\')</code></li>\\n<li>otherwise <code>blue</code> when <code>(df[\\'Set\\'] == \\'Z\\') &amp; (df[\\'Type\\'] == \\'B\\')</code> </li>\\n<li>otherwise <code>purple</code> when <code>(df[\\'Type\\'] == \\'B\\')</code></li>\\n<li>otherwise <code>black</code>,</li>\\n</ul>\\n\\n<p>then use</p>\\n\\n<pre><code>df = pd.DataFrame({\\'Type\\':list(\\'ABBC\\'), \\'Set\\':list(\\'ZZXY\\')})\\nconditions = [\\n    (df[\\'Set\\'] == \\'Z\\') &amp; (df[\\'Type\\'] == \\'A\\'),\\n    (df[\\'Set\\'] == \\'Z\\') &amp; (df[\\'Type\\'] == \\'B\\'),\\n    (df[\\'Type\\'] == \\'B\\')]\\nchoices = [\\'yellow\\', \\'blue\\', \\'purple\\']\\ndf[\\'color\\'] = np.select(conditions, choices, default=\\'black\\')\\nprint(df)\\n</code></pre>\\n\\n<p>which yields</p>\\n\\n<pre><code>  Set Type   color\\n0   Z    A  yellow\\n1   Z    B    blue\\n2   X    B  purple\\n3   Y    C   black\\n</code></pre>\\n'}, {'owner': {'reputation': 280766, 'user_id': 4909087, 'user_type': 'registered', 'accept_rate': 97, 'profile_image': 'https://i.stack.imgur.com/LmD3e.png?s=128&g=1', 'display_name': 'cs95', 'link': 'https://stackoverflow.com/users/4909087/cs95'}, 'is_accepted': True, 'score': 68, 'last_activity_date': 1576272552, 'last_edit_date': 1592644375, 'creation_date': 1576272552, 'answer_id': 59330040, 'question_id': 45846765, 'content_license': 'CC BY-SA 4.0', 'body': '<h1><strong>pandas &gt;= 0.25</strong></h1>\\n<p>Assuming all columns have the same number of lists, you can call <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html\" rel=\"noreferrer\"><strong><code>Series.explode</code></strong></a> on each column.</p>\\n<pre><code>df.set_index([\\'A\\']).apply(pd.Series.explode).reset_index()\\n\\n    A   B   C   D   E\\n0  x1  v1  c1  d1  e1\\n1  x1  v2  c2  d2  e2\\n2  x2  v3  c3  d3  e3\\n3  x2  v4  c4  d4  e4\\n4  x3  v5  c5  d5  e5\\n5  x3  v6  c6  d6  e6\\n6  x4  v7  c7  d7  e7\\n7  x4  v8  c8  d8  e8\\n</code></pre>\\n<p>The idea is to set as the index all columns that must <strong>NOT</strong> be exploded first, then reset the index after.</p>\\n<hr />\\n<p>It\\'s also <strong>faster</strong>.</p>\\n<pre><code>%timeit df.set_index([\\'A\\']).apply(pd.Series.explode).reset_index()\\n%%timeit\\n(df.set_index(\\'A\\')\\n   .apply(lambda x: x.apply(pd.Series).stack())\\n   .reset_index()\\n   .drop(\\'level_1\\', 1))\\n\\n\\n2.22 ms Â± 98.6 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\\n9.14 ms Â± 329 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\\n</code></pre>\\n'}, {'owner': {'reputation': 9267, 'user_id': 5612363, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/sTz0z.jpg?s=128&g=1', 'display_name': 'Anwarvic', 'link': 'https://stackoverflow.com/users/5612363/anwarvic'}, 'is_accepted': True, 'score': 1, 'last_activity_date': 1573722398, 'last_edit_date': 1573722398, 'creation_date': 1573720163, 'answer_id': 58852371, 'question_id': 58841995, 'content_license': 'CC BY-SA 4.0', 'body': '<p>According to the Spacy Documentation of <code>Doc</code> object <a href=\"https://spacy.io/api/doc\" rel=\"nofollow noreferrer\">here</a>, the <code>__len__</code> operator gets \"the number of tokens in the document.\".</p>\\n\\n<p>The last text in your data is: </p>\\n\\n<pre><code>&gt;&gt;&gt; df[\\'text\\'].values[-1]\\n@AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?\\n</code></pre>\\n\\n<p>After running the <code>nlp.pipe()</code> method, this sentence will be tokenized into 32 tokens which what you\\'re asking for. To verfiy that, try runn the following code after <code>len(text)</code> and will get the exact result:</p>\\n\\n<pre><code>&gt;&gt;&gt; last_tokens = [token for token in text]\\n&gt;&gt;&gt; last_tokens\\n[@AmericanAir, we, have, 8, ppl, so, we, need, 2, know, how, many, seats, are, on, the, next, flight, ., Plz, put, us, on, standby, for, 4, people, on, the, next, flight, ?]\\n\\n&gt;&gt;&gt; len(last_tokens)\\n32\\n</code></pre>\\n\\n<h2>EDIT</h2>\\n\\n<p>You can iterate over the tokens of each <code>doc</code> returned from the pipeline like so:</p>\\n\\n<pre><code>nlp = spacy.load(\"en_core_web_sm\")\\nfor text in nlp.pipe(iter(df[\\'text\\']), batch_size = 1000, n_threads=-1):\\n    for token in text:\\n        print(token)\\n    print(\\'\\\\n\\')\\n</code></pre>\\n'}, {'owner': {'reputation': 280766, 'user_id': 4909087, 'user_type': 'registered', 'accept_rate': 97, 'profile_image': 'https://i.stack.imgur.com/LmD3e.png?s=128&g=1', 'display_name': 'cs95', 'link': 'https://stackoverflow.com/users/4909087/cs95'}, 'is_accepted': True, 'score': 12, 'last_activity_date': 1529984524, 'last_edit_date': 1529984524, 'creation_date': 1529980242, 'answer_id': 51034079, 'question_id': 51034054, 'content_license': 'CC BY-SA 4.0', 'body': \"<p>Shift the mask up by 1.</p>\\n\\n<pre><code>df[(df['Value'] &lt; 15).shift(-1).fillna(False)]\\n\\n   Row  Value\\n1    2     25\\n</code></pre>\\n\\n<hr>\\n\\n<p>More generally, if you're trying to find all rows greater than 15, whose next row is lesser than 15, you can compute two separate masks and AND them:</p>\\n\\n<pre><code>df[(df['Value'].shift(-1) &lt; 15) &amp; (df['Value'] &gt; 15)]\\n\\n   Row  Value\\n1    2     25\\n</code></pre>\\n\"}, {'owner': {'reputation': 26171, 'user_id': 5003756, 'user_type': 'registered', 'profile_image': 'https://i.stack.imgur.com/8BUMb.jpg?s=128&g=1', 'display_name': 'James', 'link': 'https://stackoverflow.com/users/5003756/james'}, 'is_accepted': True, 'score': 38, 'last_activity_date': 1479157007, 'creation_date': 1479157007, 'answer_id': 40597684, 'question_id': 40596518, 'content_license': 'CC BY-SA 3.0', 'body': \"<p>You can write the table straight into a <code>.docx</code> file using the <code>python-docx</code> library.</p>\\n\\n<p>If you are using the Conda or installed Python using Anaconda, you can run the command from the command line:</p>\\n\\n<pre><code>conda install python-docx --channel conda-forge\\n</code></pre>\\n\\n<p>Or to pip install from the command line:</p>\\n\\n<pre><code>pip install python-docx\\n</code></pre>\\n\\n<p>After that is installed, we can use it to open the file, add a table, and then populate the table's cell text with the data frame data.</p>\\n\\n<pre><code>import docx\\nimport pandas as pd\\n\\n# i am not sure how you are getting your data, but you said it is a\\n# pandas data frame\\ndf = pd.DataFrame(data)\\n\\n# open an existing document\\ndoc = docx.Document('./test.docx')\\n\\n# add a table to the end and create a reference variable\\n# extra row is so we can add the header row\\nt = doc.add_table(df.shape[0]+1, df.shape[1])\\n\\n# add the header rows.\\nfor j in range(df.shape[-1]):\\n    t.cell(0,j).text = df.columns[j]\\n\\n# add the rest of the data frame\\nfor i in range(df.shape[0]):\\n    for j in range(df.shape[-1]):\\n        t.cell(i+1,j).text = str(df.values[i,j])\\n\\n# save the doc\\ndoc.save('./test.docx')\\n</code></pre>\\n\"}, {'owner': {'reputation': 24108, 'user_id': 3877338, 'user_type': 'registered', 'accept_rate': 93, 'profile_image': 'https://www.gravatar.com/avatar/300539394aa55b738b51e6769cca9b3e?s=128&d=identicon&r=PG&f=1', 'display_name': 'JohnE', 'link': 'https://stackoverflow.com/users/3877338/johne'}, 'is_accepted': True, 'score': 6, 'last_activity_date': 1479058800, 'last_edit_date': 1479058800, 'creation_date': 1478996537, 'answer_id': 40569207, 'question_id': 40568438, 'content_license': 'CC BY-SA 3.0', 'body': \"<p>Sample data (note that you posted an image which can't be used by potential answerers without retyping, so I'm making a simple example in its place):</p>\\n\\n<pre><code>df=pd.DataFrame({ 'id':[1,1,1,1,2,2,2,2],\\n                   'a':range(8), 'b':range(8,0,-1) })\\n</code></pre>\\n\\n<p>The key to this is just using <code>idxmax</code> and <code>idxmin</code> and then futzing with the indexes so that you can merge things in a readable way.  Here's the whole answer and you may wish to examine intermediate dataframes to see how this is working.</p>\\n\\n<pre><code>df_max = df.groupby('id').idxmax()\\ndf_max['type'] = 'max'\\ndf_min = df.groupby('id').idxmin()\\ndf_min['type'] = 'min'\\n\\ndf2 = df_max.append(df_min).set_index('type',append=True).stack().rename('index')\\n\\ndf3 = pd.concat([ df2.reset_index().drop('id',axis=1).set_index('index'), \\n                  df.loc[df2.values] ], axis=1 )\\n\\ndf3.set_index(['id','level_2','type']).sort_index()\\n\\n                 a  b\\nid level_2 type      \\n1  a       max   3  5\\n           min   0  8\\n   b       max   0  8\\n           min   3  5\\n2  a       max   7  1\\n           min   4  4\\n   b       max   4  4\\n           min   7  1\\n</code></pre>\\n\\n<p>Note in particular that df2 looks like this:</p>\\n\\n<pre><code>id  type   \\n1   max   a    3\\n          b    0\\n2   max   a    7\\n          b    4\\n1   min   a    0\\n          b    3\\n2   min   a    4\\n          b    7\\n</code></pre>\\n\\n<p>The last column there holds the index values in <code>df</code> that were derived with <code>idxmax</code> &amp; <code>idxmin</code>.  So basically all the information you need is in <code>df2</code>.  The rest of it is just a matter of merging back with <code>df</code> and making it more readable.</p>\\n\"}, {'owner': {'reputation': 174150, 'user_id': 4983450, 'user_type': 'registered', 'accept_rate': 74, 'profile_image': 'https://i.stack.imgur.com/NR2ko.jpg?s=128&g=1', 'display_name': 'Psidom', 'link': 'https://stackoverflow.com/users/4983450/psidom'}, 'is_accepted': True, 'score': 11, 'last_activity_date': 1478553763, 'creation_date': 1478553763, 'answer_id': 40474890, 'question_id': 40474799, 'content_license': 'CC BY-SA 3.0', 'body': \"<p>You can sort each row of the data frame before dropping the duplicates:</p>\\n\\n<pre><code>data.apply(lambda r: sorted(r), axis = 1).drop_duplicates()\\n\\n#   A    B\\n#0  0   50\\n#1  10  22\\n#2  11  35\\n#3  5   21\\n</code></pre>\\n\\n<p>If you prefer the result to be sorted by column <code>A</code>:</p>\\n\\n<pre><code>data.apply(lambda r: sorted(r), axis = 1).drop_duplicates().sort_values('A')\\n\\n#   A    B\\n#0  0   50\\n#3  5   21\\n#1  10  22\\n#2  11  35\\n</code></pre>\\n\"}]}\n"
     ]
    }
   ],
   "source": [
    "answers = SITE.fetch('answers/{ids}', ids=aids[:100], filter='withbody')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answers = answers['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "final_answers.extend(SITE.fetch('answers/{ids}', ids=aids[100:200], filter='withbody')['items'])\n",
    "print(len(final_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "final_answers.extend(SITE.fetch('answers/{ids}', ids=aids[200:300], filter='withbody')['items'])\n",
    "print(len(final_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_answer = {\n",
    "    answer['answer_id']: answer for answer in final_answers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(accepted_answers)):\n",
    "    accepted_answers[idx]['answer_body'] = id_to_answer[accepted_answers[idx]['accepted_answer_id']]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = [i['question_id'] for i in accepted_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "question_items = SITE.fetch('questions/{ids}', ids=qids[:100], filter='withbody')['items']\n",
    "question_items.extend(SITE.fetch('questions/{ids}', ids=qids[100:200], filter='withbody')['items'])\n",
    "question_items.extend(SITE.fetch('questions/{ids}', ids=qids[200:300], filter='withbody')['items'])\n",
    "print(len(question_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_body = {\n",
    "    q['question_id']: q['body'] for q in question_items\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(accepted_answers)):\n",
    "    accepted_answers[idx]['question_body'] = id_to_body[accepted_answers[idx]['question_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Mining Pandas APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "urls = json.load(open(\"pandas_apis.json\"))\n",
    "data_frame_url = urls[\"df\"]\n",
    "\n",
    "def get_api_list(_url):\n",
    "    page = requests.get(_url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    codes = soup.find_all(\"code\", {\"class\": \"xref py py-obj docutils literal notranslate\"})\n",
    "    apis = []\n",
    "    for code in codes:\n",
    "        api_name = code.find(\"span\").text\n",
    "        if \".\" in api_name:\n",
    "            name = api_name[api_name.index(\".\") + 1:]\n",
    "            apis.append(name)\n",
    "            pass\n",
    "        else:\n",
    "            apis.append(api_name)\n",
    "        pass\n",
    "    return apis\n",
    "\n",
    "data_frame_apis = get_api_list(data_frame_url)\n",
    "# print(data_frame_apis)\n",
    "\n",
    "list_of_apis = []\n",
    "complete_api_sets = []\n",
    "\n",
    "for key in urls:\n",
    "    api_from_url = get_api_list(urls[key])\n",
    "    list_of_apis.append({\n",
    "        \"url_key\": key,\n",
    "        \"url\": urls[key],\n",
    "        \"apis\": api_from_url\n",
    "    })\n",
    "    complete_api_sets.extend(api_from_url)\n",
    "    pass\n",
    "\n",
    "# print(\"Dataframe APIs only\")\n",
    "# print(data_frame_apis)\n",
    "# print(\"=\" * 100)\n",
    "# print(\"Pandas APIS\")\n",
    "complete_api_sets = list(set(complete_api_sets))\n",
    "# print(complete_api_sets)\n",
    "# print(\"=\" * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello() False\n",
      "concat(dsaf) True\n",
      "index() True\n",
      "dasfasd False\n"
     ]
    }
   ],
   "source": [
    "# print(data_frame_apis)\n",
    "\n",
    "def dataframe_api_exists(line):\n",
    "    for api in data_frame_apis:\n",
    "        if api in line:\n",
    "            return True\n",
    "        pass\n",
    "    return  False\n",
    "\n",
    "def api_exists(line):\n",
    "    for api in complete_api_sets:\n",
    "        if api in line:\n",
    "            return True\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "lines = [\"hello()\", \"concat(dsaf)\", \"index()\", \"dasfasd\"]\n",
    "for l in lines:\n",
    "    print(l, dataframe_api_exists(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import keyword\n",
    "\n",
    "keywords = keyword.kwlist\n",
    "\n",
    "def keyword_exists(line):\n",
    "    for kw in keywords:\n",
    "        if (kw + \" \") in line or (\" \" + kw) in line:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def min_word_filter(texts, min_word=5):\n",
    "    filtered_texts = []\n",
    "    for t in texts:\n",
    "        if len(t.split()) >= min_word:\n",
    "            filtered_texts.append(t)\n",
    "            pass\n",
    "        pass\n",
    "    return filtered_texts\n",
    "\n",
    "def extract_code(text, filters):\n",
    "    soup = bs(text)\n",
    "    all_code = [code.text for code in soup.find_all('code')]\n",
    "    for f in filters:\n",
    "        all_code = f(all_code)\n",
    "    return all_code\n",
    "    pass\n",
    "\n",
    "def data_frame_exists(code):\n",
    "    lines = [l.strip() for l in code.split(\"\\n\")]\n",
    "    data_frame_re = \"[0-9]+[, \\t]+[.]*\"\n",
    "    matches = []\n",
    "    for l in lines:\n",
    "        if len(re.findall(data_frame_re, l)) > 0 \\\n",
    "            and not keyword_exists(l) \\\n",
    "            and not dataframe_api_exists(l) \\\n",
    "            and not api_exists(l):\n",
    "            matches.append(True)\n",
    "        else:\n",
    "            matches.append(False)\n",
    "    return any(matches)\n",
    "\n",
    "def code_exists(code):\n",
    "    lines = [l.strip() for l in code.split(\"\\n\")]\n",
    "    matches = []\n",
    "    for l in lines:\n",
    "        if dataframe_api_exists(l) or keyword_exists(l) or api_exists(l):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def print_formatted_code(code):\n",
    "    lines = code.split('\\n')\n",
    "    lines = ['\\t\\t' + l for l in  lines]\n",
    "    print('\\n'.join(lines))\n",
    "    \n",
    "def extract_description(html):\n",
    "    soup = bs(html)\n",
    "    [code.extract() for code in soup.find_all('code')]\n",
    "    text = re.sub(\"[ \\n\\t]+\", \" \", soup.text)\n",
    "    return text\n",
    "    pass\n",
    "    \n",
    "taken_answers = []\n",
    "\n",
    "for a in accepted_answers:\n",
    "    title = a['title']\n",
    "    code_from_body = extract_code(a['question_body'], filters=[min_word_filter])\n",
    "    code_from_answer = extract_code(a['answer_body'], filters=[min_word_filter])\n",
    "    data_frames = []\n",
    "    for cid, c in enumerate(code_from_body):\n",
    "        is_df = data_frame_exists(c)\n",
    "        is_code = code_exists(c)\n",
    "        if is_df and not is_code:\n",
    "            data_frames.append(c)\n",
    "        \n",
    "    taken_code = []\n",
    "    for cid, c in enumerate(code_from_answer):\n",
    "        is_df = data_frame_exists(c)\n",
    "        is_pandas_code = api_exists(c)\n",
    "        if is_pandas_code and not is_df:\n",
    "            taken_code.append(c)\n",
    "    if len(data_frames) == 2 and len(taken_code) > 0:\n",
    "        a[\"formatted_input\"] = {\n",
    "            \"qid\": a['question_id'],\n",
    "            'link': a['link'],\n",
    "            \"question\": {\n",
    "                \"title\": a[\"title\"],\n",
    "                \"ques_desc\" : extract_description(a['question_body'])\n",
    "            },\n",
    "            \"io\": data_frames,\n",
    "            \"answer\" : {\n",
    "                \"ans_desc\" : extract_description(a['answer_body']),\n",
    "                \"code\": taken_code\n",
    "            }\n",
    "        }\n",
    "        taken_answers.append(a)\n",
    "\n",
    "print(len(taken_answers))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"qid\": 68243146\n",
      "\"link\": https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas\n",
      "\"question\": {\n",
      "\t\"title\": replace zero with value of an other column using pandas\n",
      "\t\"desc\": I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\t    ref   Name   id  Score\n",
      "\t\t  8400   John    0     12\n",
      "\t\t  3840  Peter  414      0\n",
      "\t\t  7400  David  612     64\n",
      "\t\t  5200  Karen    0      0\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\t   ref    Name   id   Score\n",
      "\t\t  8400   John  8400     12\n",
      "\t\t  3840  Peter  414      0\n",
      "\t\t  7400  David  612     64\n",
      "\t\t  5200  Karen 5200      0\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s via : OR via numpy's : \n",
      "\t\"code-snippets\": [\n",
      "\t\t#import numpy as np\n",
      "\t\tdf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68231389\n",
      "\"link\": https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas\n",
      "\"question\": {\n",
      "\t\"title\": Compare two columns that contains timestamps in pandas\n",
      "\t\"desc\": Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\t  Col0       Col1                    Col2                   Col3                   Col4\n",
      "\t\t   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n",
      "\t\t   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\t  Col0       Col1                    Col2               Col3                   Col4\n",
      "\t\t   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n",
      "\t\t   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s A straightforward way with boolean mask: \n",
      "\t\"code-snippets\": [\n",
      "\t\tdt = df.select_dtypes('datetime')\n",
      "\t\tdt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n",
      "\t\t\n",
      "\t\tdf.loc[:, dt.columns.tolist()] = dt\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68231104\n",
      "\"link\": https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe\n",
      "\"question\": {\n",
      "\t\"title\": Extract part of a 3 D dataframe\n",
      "\t\"desc\": I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\t     d1        d2            d3\n",
      "\t\t   A B C D...   A B C D...   A B C D..\n",
      "\t\t0  \n",
      "\t\t1\n",
      "\t\t2\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\t    d1    d2    d3\n",
      "\t\t  A  B   A  B   A  B\n",
      "\t\t0\n",
      "\t\t1\n",
      "\t\t2\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s Use on the level 1 values of columns then select with : : Sample Data Used: \n",
      "\t\"code-snippets\": [\n",
      "\t\tfiltered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t\timport numpy as np\n",
      "\t\timport pandas as pd\n",
      "\t\t\n",
      "\t\tdf = pd.DataFrame(\n",
      "\t\t    np.arange(1, 25).reshape((-1, 8)),\n",
      "\t\t    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n",
      "\t\t)\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68229806\n",
      "\"link\": https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe\n",
      "\"question\": {\n",
      "\t\"title\": Insert values from variable and DataFrame into another DataFrame\n",
      "\t\"desc\": On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\t   id  col0  col1  col2\n",
      "\t\t0   1   3.0    13    23\n",
      "\t\t1   1   NaN    14    24\n",
      "\t\t2   1   NaN    15    25\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\t   id  col0  col1  col2\n",
      "\t\t0   1   3.0    13    23\n",
      "\t\t1   1   3.0    14    24\n",
      "\t\t2   1   3.0    15    25\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: \n",
      "\t\"code-snippets\": [\n",
      "\t\timport pandas as pd\n",
      "\t\t\n",
      "\t\tid=1\n",
      "\t\tdf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\n",
      "\t\tdf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n",
      "\t\t\n",
      "\t\tdf2.insert(0, \"id\", id)\n",
      "\t\tdf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n",
      "\t\t\n",
      "\t\tprint(df2)\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68213612\n",
      "\"link\": https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun\n",
      "\"question\": {\n",
      "\t\"title\": How to combine rows in a dataframe in a pairwise fashion while applying some function\n",
      "\t\"desc\": I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\tID    Val1    Val2\n",
      "\t\tid0     10      20\n",
      "\t\tid0     11      19\n",
      "\t\tid1      5       5\n",
      "\t\tid1      1       1\n",
      "\t\tid1      2       4\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\tID      Val1    Val2\n",
      "\t\tid0_1   10.5    19.5\n",
      "\t\tid1_1   3       3\n",
      "\t\tid1_2   1.5     2.5\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s You can use after grouping by : \n",
      "\t\"code-snippets\": [\n",
      "\t\tout = df.groupby('ID').rolling(2).mean() \\\n",
      "\t\t        .dropna(how='all').reset_index(level=1, drop=True)\n",
      "\t\t\n",
      "\t\tout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68211888\n",
      "\"link\": https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base\n",
      "\"question\": {\n",
      "\t\"title\": Loop through multiple small Pandas dataframes and create summary dataframes based on a single column\n",
      "\t\"desc\": I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\tNAME     VAL1  VAL2  VAL3\n",
      "\t\tplayer1  3     5     7\n",
      "\t\tplayer2  2     6     8\n",
      "\t\tplayer3  3     6     7\n",
      "\t\t\n",
      "\t\tNAME     VAL1  VAL2  VAL3\n",
      "\t\tplayer2  5     7     7\n",
      "\t\tplayer3  2     6     8\n",
      "\t\tplayer5  3     6     7\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\tNAME     VAL1  VAL2  VAL3\n",
      "\t\tplayer1  3     5     7\n",
      "\t\t\n",
      "\t\tNAME     VAL1  VAL2  VAL3\n",
      "\t\tplayer2  2     6     8\n",
      "\t\tplayer2  5     7     7\n",
      "\t\t\n",
      "\t\tNAME     VAL1  VAL2  VAL3\n",
      "\t\tplayer3  3     6     7\n",
      "\t\tplayer3  2     6     8\n",
      "\t\t\n",
      "\t\tNAME     VAL1  VAL2  VAL3\n",
      "\t\tplayer5  3     6     7\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : \n",
      "\t\"code-snippets\": [\n",
      "\t\tdfs = {group_name: df_\n",
      "\t\t       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t\tdfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68193558\n",
      "\"link\": https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values\n",
      "\"question\": {\n",
      "\t\"title\": pandas group many columns to one column where every cell is a list of values\n",
      "\t\"desc\": I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\tdf = \n",
      "\t\tc1 c2 c3 c4 c5\n",
      "\t\t1.  2. 3. 1. 5\n",
      "\t\t8.  2. 1. 3. 8\n",
      "\t\t4.  9. 1  2. 3\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\tdf = \n",
      "\t\t    l\n",
      "\t\t[1,2,3,1,5]\n",
      "\t\t[8,2,1,3,8]\n",
      "\t\t[4,9,1,2,3]\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s Try: \n",
      "\t\"code-snippets\": [\n",
      "\t\t#best way:\n",
      "\t\tdf['l']=df.values.tolist()\n",
      "\t\t#OR\n",
      "\t\tdf['l']=df.to_numpy().tolist()\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t#another way:\n",
      "\t\tdf['l']=df.agg(list,1)\n",
      "\t\t#OR\n",
      "\t\tdf['l']=df.apply(list,1)\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68193521\n",
      "\"link\": https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame\n",
      "\"question\": {\n",
      "\t\"title\": Concatenate values and column names in a data frame to create a new data frame\n",
      "\t\"desc\": I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\t  Value col1 col2 col3\n",
      "\t\t0     a   aa   ab   ac\n",
      "\t\t1     b   ba   bb   bc\n",
      "\t\t2     c   ca   cb   cc\n",
      "\t\t3     d   da   db   dc\n",
      "\t\t4     e   ea   eb   ec\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\t      Value Col 1\n",
      "\t\t0   a_Col 1    aa\n",
      "\t\t1   a_Col 2    ab\n",
      "\t\t2   a_Col 3    ac\n",
      "\t\t3   b_Col 1    ba\n",
      "\t\t4   b_Col 2    bb\n",
      "\t\t5   b_Col 3    bc\n",
      "\t\t6   c_Col 1    ca\n",
      "\t\t7   c_Col 2    cb\n",
      "\t\t8   c_Col 3    cc\n",
      "\t\t9   d_Col 1    da\n",
      "\t\t10  d_Col 2    db\n",
      "\t\t11  d_Col 3    dc\n",
      "\t\t12  e_Col 1    ea\n",
      "\t\t13  e_Col 2    eb\n",
      "\t\t14  e_Col 3    ec\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s Try: Prints: Optionally, you can sort values afterwards: \n",
      "\t\"code-snippets\": [\n",
      "\t\tx = df.melt(\"Value\", value_name=\"Col 1\")\n",
      "\t\tx.Value += \"_\" + x.variable\n",
      "\t\tx = x.drop(columns=\"variable\")\n",
      "\t\tprint(x)\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68174614\n",
      "\"link\": https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json\n",
      "\"question\": {\n",
      "\t\"title\": Why does it add .0 to the value while converting Dataframe columns to JSON\n",
      "\t\"desc\": I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\tA   B   C   D\n",
      "\t\t2   6   5   8.0\n",
      "\t\t6   11  2   3.6 \n",
      "\t\t1   5   7   5.2\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\t{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n",
      "\t\t{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n",
      "\t\t{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: \n",
      "\t\"code-snippets\": [\n",
      "\t\tdf.iloc[0]\n",
      "\t\tA    2.0\n",
      "\t\tB    6.0\n",
      "\t\tC    5.0\n",
      "\t\tD    8.0\n",
      "\t\tName: 0, dtype: float64\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68174113\n",
      "\"link\": https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction\n",
      "\"question\": {\n",
      "\t\"title\": Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries\n",
      "\t\"desc\": I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\tbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\tids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: \n",
      "\t\"code-snippets\": [\n",
      "\t\tdef reclass(group, name):\n",
      "\t\t    bins = bins_dic[name]\n",
      "\t\t    ids = ids_dic[name]\n",
      "\t\t    return pd.cut(group, bins, labels=ids)\n",
      "\t\t    \n",
      "\t\tdf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 68150020\n",
      "\"link\": https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector\n",
      "\"question\": {\n",
      "\t\"title\": Getting first/second/third... value in row of numpy array after nan using vectorization\n",
      "\t\"desc\": I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\tf(offset=0)\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t| 0  | 1  |\n",
      "\t\t| -- | -- |\n",
      "\t\t| 1  | 25 |\n",
      "\t\t| 2  | 29 |\n",
      "\t\t| 3  | 33 |\n",
      "\t\t| 4  | 31 |\n",
      "\t\t| 5  | 30 |\n",
      "\t\t| 6  | 35 |\n",
      "\t\t| 7  | 31 |\n",
      "\t\t| 8  | 33 |\n",
      "\t\t| 9  | 26 |\n",
      "\t\t| 10 | 27 |\n",
      "\t\t| 11 | 35 |\n",
      "\t\t| 12 | 33 |\n",
      "\t\t| 13 | 28 |\n",
      "\t\t| 14 | 25 |\n",
      "\t\t| 15 | 25 |\n",
      "\t\t| 16 | 26 |\n",
      "\t\t| 17 | 34 |\n",
      "\t\t| 18 | 28 |\n",
      "\t\t| 19 | 34 |\n",
      "\t\t| 20 | 28 |\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\tf(offset=1)\n",
      "\t\t\n",
      "\t\t| 0  | 1   |\n",
      "\t\t| -- | --- |\n",
      "\t\t| 1  | nan |\n",
      "\t\t| 2  | nan |\n",
      "\t\t| 3  | nan |\n",
      "\t\t| 4  | 35  |\n",
      "\t\t| 5  | 34  |\n",
      "\t\t| 6  | 34  |\n",
      "\t\t| 7  | 26  |\n",
      "\t\t| 8  | 25  |\n",
      "\t\t| 9  | 31  |\n",
      "\t\t| 10 | 26  |\n",
      "\t\t| 11 | 25  |\n",
      "\t\t| 12 | 35  |\n",
      "\t\t| 13 | 25  |\n",
      "\t\t| 14 | 25  |\n",
      "\t\t| 15 | 26  |\n",
      "\t\t| 16 | 31  |\n",
      "\t\t| 17 | 29  |\n",
      "\t\t| 18 | 29  |\n",
      "\t\t| 19 | 26  |\n",
      "\t\t| 20 | 30  |\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster \n",
      "\t\"code-snippets\": [\n",
      "\t\tdef first_valid(arr, offset=0):\n",
      "\t\t    m = ~np.isnan(arr)\n",
      "\t\t    i =  m.argmax(axis=1) + offset\n",
      "\t\t    iy = np.clip(i, 0, arr.shape[1] - 1)\n",
      "\t\t\n",
      "\t\t    vals = arr[np.r_[:arr.shape[0]], iy]\n",
      "\t\t    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n",
      "\t\t    return vals\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t\tdef first_valid(df, offset=0):\n",
      "\t\t    return df.stack().groupby(level=0)\\\n",
      "\t\t                     .nth(offset).reindex(df.index)\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t\t# Sample dataframe for testing purpose\n",
      "\t\tdf_test = pd.concat([df] * 10000, ignore_index=True)\n",
      "\t\t\n",
      "\t\t%%timeit # Numpy approach\n",
      "\t\t_ = first_valid(df_test.to_numpy(), 1)\n",
      "\t\t# 6.9 ms Â± 212 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t%%timeit # Pandas approach\n",
      "\t\t_ = first_valid(df_test, 1)\n",
      "\t\t# 90 ms Â± 867 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t%%timeit # OP's approach\n",
      "\t\t_ = f(df_test, 1)\n",
      "\t\t# 2.03 s Â± 183 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\"qid\": 57033657\n",
      "\"link\": https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe\n",
      "\"question\": {\n",
      "\t\"title\": How to Extract Month Name and Year from Date column of DataFrame\n",
      "\t\"desc\": I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. \n",
      "}\n",
      "\"io\": {\n",
      "\t\"Frame-1\": \n",
      "\t\t45    2018-01-01\n",
      "\t\t73    2018-02-08\n",
      "\t\t74    2018-02-08\n",
      "\t\t75    2018-02-08\n",
      "\t\t76    2018-02-08\n",
      "\t\t\n",
      "\t\"Frame-2\":\n",
      "\t\t45    Jan-2018\n",
      "\t\t73    Feb-2018\n",
      "\t\t74    Feb-2018\n",
      "\t\t75    Feb-2018\n",
      "\t\t76    Feb-2018\n",
      "\t\t\n",
      "}\n",
      "\"answer\": {\n",
      "\t\"desc\": %s Cast you date from object to actual datetime and use dt to access what you need. \n",
      "\t\"code-snippets\": [\n",
      "\t\timport pandas as pd\n",
      "\t\t\n",
      "\t\tdf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n",
      "\t\t\n",
      "\t\tdf['Date'] = pd.to_datetime(df['Date'])\n",
      "\t\t\n",
      "\t\t# You can format your date as you wish\n",
      "\t\tdf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n",
      "\t\t\n",
      "\t\t# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n",
      "\t\t\n",
      "\t\tprint(df['Mon_Year'])\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t----------------------------------------------------------------------\n",
      "\t]\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "for a in taken_answers:\n",
    "    fmt_input = a[\"formatted_input\"]\n",
    "    print(\"\\\"qid\\\": %s\" % fmt_input[\"qid\"])\n",
    "    print(\"\\\"link\\\": %s\" % fmt_input[\"link\"])\n",
    "    print(\"\\\"question\\\": {\")\n",
    "    print(\"\\t\\\"title\\\": %s\" % fmt_input[\"question\"][\"title\"])\n",
    "    print(\"\\t\\\"desc\\\": %s\" % fmt_input[\"question\"][\"ques_desc\"])\n",
    "    print(\"}\")\n",
    "    print(\"\\\"io\\\": {\")\n",
    "    print(\"\\t\\\"Frame-1\\\": \")\n",
    "    print_formatted_code(fmt_input[\"io\"][0])\n",
    "    print(\"\\t\\\"Frame-2\\\":\")\n",
    "    print_formatted_code(fmt_input[\"io\"][1])\n",
    "    print(\"}\")\n",
    "    print(\"\\\"answer\\\": {\")\n",
    "    print(\"\\t\\\"desc\\\": %s\", fmt_input[\"answer\"][\"ans_desc\"])\n",
    "    print(\"\\t\\\"code-snippets\\\": [\")\n",
    "    for t in fmt_input[\"answer\"][\"code\"]:\n",
    "        print_formatted_code(t)\n",
    "        print(\"\\t\\t\" + (\"-\" * 70))\n",
    "    print(\"\\t]\")\n",
    "    print(\"}\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers_file = open(\"outputs/all_accepted_answers_with_all_details.json\", 'w')\n",
    "json.dump(obj=accepted_answers, fp=all_answers_file, indent=4)\n",
    "all_answers_file.close()\n",
    "\n",
    "taken_answer_file = open(\"outputs/taken_answers_with_all_details.json\", \"w\")\n",
    "json.dump(obj=taken_answers, fp=taken_answer_file, indent=4)\n",
    "taken_answer_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# questions = []\n",
    "# for pi in range(1, 21):\n",
    "#     d = SITE.fetch(\n",
    "#             \"search/advanced\", \n",
    "#             q=\"pandas,dataframe\", \n",
    "#             accepted=True, \n",
    "#             tagged=\"pandas;dataframe;python\", \n",
    "#             page=pi, \n",
    "#             filter=\"withbody\"\n",
    "#         )['items']\n",
    "#     for itm in d:\n",
    "#         if 'accepted_answer_id' in itm.keys() and 'pandas' in itm['tags'] and 'dataframe' in itm['tags']:\n",
    "#             questions.append(itm)\n",
    "#     print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs('ten_thousand_questions', exist_ok=True)\n",
    "# ten_thousand_questions = open('ten_thousand_questions/questions_with_bodies.json', 'w')\n",
    "# json.dump(questions, fp=ten_thousand_questions, indent=4)\n",
    "# ten_thousand_questions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
