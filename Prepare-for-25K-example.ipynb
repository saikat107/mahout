{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "urls = json.load(open(\"pandas_apis.json\"))\n",
    "data_frame_url = urls[\"df\"]\n",
    "\n",
    "def get_api_list(_url):\n",
    "    page = requests.get(_url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    codes = soup.find_all(\"code\", {\"class\": \"xref py py-obj docutils literal notranslate\"})\n",
    "    apis = []\n",
    "    for code in codes:\n",
    "        api_name = code.find(\"span\").text\n",
    "        if \".\" in api_name:\n",
    "            name = api_name[api_name.index(\".\") + 1:]\n",
    "            apis.append(name)\n",
    "            pass\n",
    "        else:\n",
    "            apis.append(api_name)\n",
    "        pass\n",
    "    return apis\n",
    "\n",
    "data_frame_apis = get_api_list(data_frame_url)\n",
    "# print(data_frame_apis)\n",
    "\n",
    "list_of_apis = []\n",
    "complete_api_sets = []\n",
    "\n",
    "for key in urls:\n",
    "    api_from_url = get_api_list(urls[key])\n",
    "    list_of_apis.append({\n",
    "        \"url_key\": key,\n",
    "        \"url\": urls[key],\n",
    "        \"apis\": api_from_url\n",
    "    })\n",
    "    complete_api_sets.extend(api_from_url)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pandas.pydata.org/docs/reference/frame.html\n"
     ]
    }
   ],
   "source": [
    "print(data_frame_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame\n",
      "index\n",
      "columns\n",
      "dtypes\n",
      "info\n",
      "select_dtypes\n",
      "values\n",
      "axes\n",
      "ndim\n",
      "size\n",
      "shape\n",
      "memory_usage\n",
      "empty\n",
      "set_flags\n",
      "astype\n",
      "convert_dtypes\n",
      "infer_objects\n",
      "copy\n",
      "bool\n",
      "head\n",
      "at\n",
      "iat\n",
      "loc\n",
      "iloc\n",
      "insert\n",
      "__iter__\n",
      "items\n",
      "iteritems\n",
      "keys\n",
      "iterrows\n",
      "itertuples\n",
      "lookup\n",
      "pop\n",
      "tail\n",
      "xs\n",
      "get\n",
      "isin\n",
      "where\n",
      "mask\n",
      "query\n",
      "add\n",
      "sub\n",
      "mul\n",
      "div\n",
      "truediv\n",
      "floordiv\n",
      "mod\n",
      "pow\n",
      "dot\n",
      "radd\n",
      "rsub\n",
      "rmul\n",
      "rdiv\n",
      "rtruediv\n",
      "rfloordiv\n",
      "rmod\n",
      "rpow\n",
      "lt\n",
      "gt\n",
      "le\n",
      "ge\n",
      "ne\n",
      "eq\n",
      "combine\n",
      "combine_first\n",
      "apply\n",
      "applymap\n",
      "pipe\n",
      "agg\n",
      "aggregate\n",
      "transform\n",
      "groupby\n",
      "rolling\n",
      "expanding\n",
      "ewm\n",
      "abs\n",
      "all\n",
      "any\n",
      "clip\n",
      "corr\n",
      "corrwith\n",
      "count\n",
      "cov\n",
      "cummax\n",
      "cummin\n",
      "cumprod\n",
      "cumsum\n",
      "describe\n",
      "diff\n",
      "eval\n",
      "kurt\n",
      "kurtosis\n",
      "mad\n",
      "max\n",
      "mean\n",
      "median\n",
      "min\n",
      "mode\n",
      "pct_change\n",
      "prod\n",
      "product\n",
      "quantile\n",
      "rank\n",
      "round\n",
      "sem\n",
      "skew\n",
      "sum\n",
      "std\n",
      "var\n",
      "nunique\n",
      "value_counts\n",
      "add_prefix\n",
      "add_suffix\n",
      "align\n",
      "at_time\n",
      "between_time\n",
      "drop\n",
      "drop_duplicates\n",
      "duplicated\n",
      "equals\n",
      "filter\n",
      "first\n",
      "head\n",
      "idxmax\n",
      "idxmin\n",
      "last\n",
      "reindex\n",
      "reindex_like\n",
      "rename\n",
      "rename_axis\n",
      "reset_index\n",
      "sample\n",
      "set_axis\n",
      "set_index\n",
      "tail\n",
      "take\n",
      "truncate\n",
      "backfill\n",
      "bfill\n",
      "dropna\n",
      "ffill\n",
      "fillna\n",
      "interpolate\n",
      "isna\n",
      "isnull\n",
      "notna\n",
      "notnull\n",
      "pad\n",
      "replace\n",
      "droplevel\n",
      "pivot\n",
      "pivot_table\n",
      "reorder_levels\n",
      "sort_values\n",
      "sort_index\n",
      "nlargest\n",
      "nsmallest\n",
      "swaplevel\n",
      "stack\n",
      "unstack\n",
      "swapaxes\n",
      "melt\n",
      "explode\n",
      "squeeze\n",
      "to_xarray\n",
      "T\n",
      "transpose\n",
      "append\n",
      "assign\n",
      "compare\n",
      "join\n",
      "merge\n",
      "update\n",
      "asfreq\n",
      "asof\n",
      "shift\n",
      "slice_shift\n",
      "tshift\n",
      "first_valid_index\n",
      "last_valid_index\n",
      "resample\n",
      "to_period\n",
      "to_timestamp\n",
      "tz_convert\n",
      "tz_localize\n",
      "Flags\n",
      "attrs\n",
      "plot\n",
      "plot.area\n",
      "plot.bar\n",
      "plot.barh\n",
      "plot.box\n",
      "plot.density\n",
      "plot.hexbin\n",
      "plot.hist\n",
      "plot.kde\n",
      "plot.line\n",
      "plot.pie\n",
      "plot.scatter\n",
      "boxplot\n",
      "hist\n",
      "sparse.density\n",
      "sparse.from_spmatrix\n",
      "sparse.to_coo\n",
      "sparse.to_dense\n",
      "from_dict\n",
      "from_records\n",
      "to_parquet\n",
      "to_pickle\n",
      "to_csv\n",
      "to_hdf\n",
      "to_sql\n",
      "to_dict\n",
      "to_excel\n",
      "to_json\n",
      "to_html\n",
      "to_feather\n",
      "to_latex\n",
      "to_stata\n",
      "to_gbq\n",
      "to_records\n",
      "to_string\n",
      "to_clipboard\n",
      "to_markdown\n",
      "style\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(data_frame_apis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_apli_list = []\n",
    "for apis in list_of_apis:\n",
    "    full_apli_list.extend(apis['apis'])\n",
    "    \n",
    "full_apli_list = [api.strip() for api in full_apli_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 58 4 5 6 0\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import tokenize \n",
    "from io import BytesIO\n",
    "import tokenize\n",
    "print(tokenize.ENCODING, tokenize.NL, tokenize.NEWLINE, tokenize.INDENT, tokenize.DEDENT, tokenize.ENDMARKER)\n",
    "\n",
    "my_code = \"\"\"\n",
    "'''\n",
    "adsfsdf\n",
    "'''\n",
    "def python_code_tokenize(full_code_text):\n",
    "    g = tokenize.tokenize(BytesIO(full_code_text.encode('utf-8')).readline)\n",
    "    tokens = []\n",
    "    prev_token = None\n",
    "    try:\n",
    "        for x in g:\n",
    "            if x.type == tokenize.ENDMARKER:  # End Marker\n",
    "                continue\n",
    "            # if x.type == tokenize.COMMENT:\n",
    "            #     continue\n",
    "            elif x.type == tokenize.NEWLINE:\n",
    "                tokens.append('NEW_LINE')\n",
    "            elif x.type == tokenize.INDENT:\n",
    "                tokens.append(\"INDENT\")\n",
    "            elif x.type == tokenize.DEDENT:\n",
    "                tokens.append('DEDENT')\n",
    "            elif x.type == tokenize.STRING:  # String\n",
    "                s = x.string.strip()\n",
    "                if s.startswith('\\\"\\\"\\\"') or s.startswith(\"'''\"):\n",
    "                    if prev_token is not None and (prev_token == '=' or prev_token == '(' or prev_token == ','):\n",
    "                        tokens.append(s)\n",
    "                    continue\n",
    "                tokens.append(s)\n",
    "                pass\n",
    "            elif x.type == tokenize.NL:\n",
    "                continue\n",
    "            elif x.type < 57:\n",
    "                tokens.append(x.string)\n",
    "            prev_token = x.string.strip()\n",
    "    except:\n",
    "        return []\n",
    "        pass\n",
    "return tokens\n",
    "\"\"\"\n",
    "def python_code_tokenize(full_code_text):\n",
    "        '''\n",
    "        :param full_code_text:\n",
    "        :return:\n",
    "        '''\n",
    "        g = tokenize.tokenize(BytesIO(full_code_text.encode('utf-8')).readline)\n",
    "        tokens = []\n",
    "        prev_token = None\n",
    "        for x in g:\n",
    "            if x.type == tokenize.ENDMARKER:  # End Marker\n",
    "                continue\n",
    "            elif x.type == tokenize.NEWLINE:\n",
    "                tokens.append('NEW_LINE')\n",
    "            elif x.type == tokenize.INDENT:\n",
    "                tokens.append('INDENT')\n",
    "            elif x.type == tokenize.DEDENT:\n",
    "                tokens.append('DEDENT')\n",
    "            elif x.type == tokenize.STRING:  # String\n",
    "                s = x.string.strip()\n",
    "                if s.startswith('\"\"\"') or s.startswith(\"'''\"):\n",
    "                    if prev_token is not None and (prev_token == '=' or prev_token == '(' or prev_token == ','):\n",
    "                        tokens.append(x.string)\n",
    "                    continue\n",
    "                tokens.append(x.string)\n",
    "                pass\n",
    "            elif x.string == '\\n':\n",
    "                continue\n",
    "            elif x.type < 57:\n",
    "                tokens.append(x.string)\n",
    "            prev_token = x.string.strip()\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def detokenize_python_code(tokens):\n",
    "    indent_count = 0\n",
    "    converted_tokens = []\n",
    "    l = len(tokens)\n",
    "    i = 0\n",
    "    while i < l:\n",
    "        t = tokens[i]\n",
    "        if t == \"NEW_LINE\":\n",
    "            converted_tokens.append(\"\\n\")\n",
    "            for _ in range(indent_count):\n",
    "                converted_tokens.append(\"  \")\n",
    "        elif t == \"INDENT\":\n",
    "            converted_tokens.append(\"  \")\n",
    "            indent_count += 1\n",
    "        elif t == \"DEDENT\":\n",
    "            if converted_tokens[-1] == \"  \":\n",
    "                converted_tokens = converted_tokens[:-1]\n",
    "            indent_count -= 1\n",
    "        elif t == \".\":\n",
    "            if i+1 < l:\n",
    "                token = converted_tokens[-1] + \".\" + tokens[i+1]\n",
    "                converted_tokens = converted_tokens[:-1]\n",
    "                converted_tokens.append(token)\n",
    "                i += 1\n",
    "            else:\n",
    "                converted_tokens.append(\".\")\n",
    "        else:\n",
    "            converted_tokens.append(t)\n",
    "        i += 1\n",
    "    return \" \".join(converted_tokens)\n",
    "\n",
    "import keyword\n",
    "\n",
    "def get_names(tokenized_code):\n",
    "    tokens = tokenized_code.split()\n",
    "    org_code = detokenize_python_code(tokens)\n",
    "    names = []\n",
    "    g = tokenize.tokenize(BytesIO(org_code.encode('utf-8')).readline)\n",
    "    for x in g:\n",
    "        if x.type == tokenize.NAME:\n",
    "            st = x.string.strip()\n",
    "            if st not in keyword.kwlist:\n",
    "                names.append(st)\n",
    "    return \" <s> \".join(sorted(list(set(names))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00b90dd02824cd6903e3a04af6dbdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae646f788d542428953ca81f6eb10b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7416f622a18c4d7e98d82f3e15221999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "659 659\n",
      "659\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "examples = []\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "fail_count = 0\n",
    "example_id = 1\n",
    "id_to_link = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    example_file = f\"25_K_Examples/part-{i}-output/taken_answers_with_all_details.json\"\n",
    "    data = json.load(open(example_file))\n",
    "    for e in tqdm(data):\n",
    "        try:\n",
    "            ques_id = e['question_id']\n",
    "            qtitle = e['formatted_input']['question']['title']\n",
    "            qdesc = e['formatted_input']['question']['ques_desc']\n",
    "            io = \" <s> \".join(e['formatted_input']['io'])\n",
    "            codes = e['formatted_input']['answer']['code']\n",
    "            apis = []\n",
    "            for c in codes:\n",
    "                tokens = nltk.wordpunct_tokenize(c)\n",
    "                for token in tokens:\n",
    "                    token = token.strip()\n",
    "                    if token in data_frame_apis:\n",
    "                        apis.append(token)\n",
    "            api_seq = \" \".join(apis)\n",
    "            \n",
    "            question_apis = []\n",
    "            q_tokens = nltk.wordpunct_tokenize(qtitle)\n",
    "            q_tokens.extend(nltk.wordpunct_tokenize(qdesc))\n",
    "            for q_t in q_tokens:\n",
    "                q_t = q_t.strip()\n",
    "                if q_t in full_apli_list:\n",
    "                    question_apis.append(q_t)\n",
    "            examples.append({\n",
    "                'qid': ques_id,\n",
    "                'id': example_id,\n",
    "                'q': qtitle.strip(),\n",
    "                'd': qdesc.strip(),\n",
    "                'q_apis': \" \".join(question_apis),\n",
    "                \"io\":  re.sub(\"[ \\t\\n]+\", \" \", io),\n",
    "                \"apis\": api_seq,\n",
    "                'code': codes,\n",
    "                'link': e['link']\n",
    "            })\n",
    "            id_to_link[example_id] = e['link']\n",
    "            example_id += 1\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            fail_count += 1\n",
    "            \n",
    "print(fail_count)\n",
    "print(len(examples), len(id_to_link))\n",
    "test_examples = examples\n",
    "\n",
    "taken_question_ids = list(set([q['qid'] for q in examples]))\n",
    "print(len(taken_question_ids))\n",
    "print(len(data_frame_apis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello() False\n",
      "concat(dsaf) True\n",
      "index() True\n",
      "dasfasd False\n"
     ]
    }
   ],
   "source": [
    "def dataframe_api_exists(line):\n",
    "    for api in data_frame_apis:\n",
    "        if api in line:\n",
    "            return True\n",
    "        pass\n",
    "    return  False\n",
    "\n",
    "def api_exists(line):\n",
    "    for api in complete_api_sets:\n",
    "        if api in line:\n",
    "            return True\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "lines = [\"hello()\", \"concat(dsaf)\", \"index()\", \"dasfasd\"]\n",
    "for l in lines:\n",
    "    print(l, dataframe_api_exists(l))\n",
    "    \n",
    "import re \n",
    "import keyword\n",
    "\n",
    "keywords = keyword.kwlist\n",
    "\n",
    "def keyword_exists(line):\n",
    "    for kw in keywords:\n",
    "        if (kw + \" \") in line or (\" \" + kw) in line:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def min_word_filter(texts, min_word=5):\n",
    "    filtered_texts = []\n",
    "    for t in texts:\n",
    "        if len(t.split()) >= min_word:\n",
    "            filtered_texts.append(t)\n",
    "            pass\n",
    "        pass\n",
    "    return filtered_texts\n",
    "\n",
    "def extract_code(text, filters):\n",
    "    soup = bs(text)\n",
    "    all_code = [code.text for code in soup.find_all('code')]\n",
    "    for f in filters:\n",
    "        all_code = f(all_code)\n",
    "    return all_code\n",
    "    pass\n",
    "\n",
    "def data_frame_exists(code):\n",
    "    lines = [l.strip() for l in code.split(\"\\n\")]\n",
    "    data_frame_re = \"[0-9]+[, \\t]+[.]*\"\n",
    "    matches = []\n",
    "    for l in lines:\n",
    "        if len(re.findall(data_frame_re, l)) > 0 \\\n",
    "            and not keyword_exists(l) \\\n",
    "            and not dataframe_api_exists(l) \\\n",
    "            and not api_exists(l):\n",
    "            matches.append(True)\n",
    "        else:\n",
    "            matches.append(False)\n",
    "    return any(matches)\n",
    "\n",
    "def code_exists(code):\n",
    "    lines = [l.strip() for l in code.split(\"\\n\")]\n",
    "    matches = []\n",
    "    for l in lines:\n",
    "        if dataframe_api_exists(l) or keyword_exists(l) or api_exists(l):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def print_formatted_code(code):\n",
    "    lines = code.split('\\n')\n",
    "    lines = ['\\t\\t' + l for l in  lines]\n",
    "    print('\\n'.join(lines))\n",
    "    \n",
    "def extract_description(html):\n",
    "    soup = bs(html)\n",
    "    [code.extract() for code in soup.find_all('code')]\n",
    "    text = re.sub(\"[ \\n\\t]+\", \" \", soup.text)\n",
    "    return text\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4413118eacba4bef9e78b8e5f4906b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b1b5e1571f4f7e9ff71ce9c29b7e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca77578262644ed85f31aceeb2960d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12424\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "train_examples = []\n",
    "\n",
    "for part in range(1, 4):\n",
    "    data = json.load(open(f\"25_K_Examples/part-{part}-output/all_accepted_answers_with_all_details.json\"))\n",
    "    for point in tqdm(data):\n",
    "        if point['question_id'] in taken_question_ids:\n",
    "            continue\n",
    "        title = point['title']\n",
    "        question = point['question_body']\n",
    "        answer = point['answer_body'] \n",
    "        code_from_body = extract_code(question, filters=[min_word_filter])\n",
    "        code_from_answer = extract_code(answer, filters=[min_word_filter])\n",
    "        qdesc = extract_description(question)\n",
    "        taken_code = []\n",
    "        for cid, c in enumerate(code_from_answer):\n",
    "            is_df = data_frame_exists(c)\n",
    "            is_pandas_code = api_exists(c)\n",
    "            if is_pandas_code and not is_df:\n",
    "                taken_code.append(c)\n",
    "                \n",
    "        apis = []\n",
    "        for c in taken_code:\n",
    "            tokens = nltk.wordpunct_tokenize(c)\n",
    "            for token in tokens:\n",
    "                token = token.strip()\n",
    "                if token in data_frame_apis:\n",
    "                    apis.append(token)\n",
    "        if len(apis) <= 0 :\n",
    "            continue\n",
    "        api_seq = \" \".join(apis)\n",
    "            \n",
    "        question_apis = []\n",
    "        q_tokens = nltk.wordpunct_tokenize(qtitle)\n",
    "        q_tokens.extend(nltk.wordpunct_tokenize(qdesc))\n",
    "        for q_t in q_tokens:\n",
    "            q_t = q_t.strip()\n",
    "            if q_t in full_apli_list:\n",
    "                question_apis.append(q_t)\n",
    "        train_examples.append({\n",
    "            'id': example_id,\n",
    "            'q': qtitle.strip(),\n",
    "            'd': qdesc.strip(),\n",
    "            'q_apis': \" \".join(question_apis),\n",
    "            \"apis\": api_seq,\n",
    "            'code': code_from_answer,\n",
    "            'link': point['link']\n",
    "        })    \n",
    "    \n",
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "\n",
    "np.random.shuffle(train_examples)\n",
    "\n",
    "num_train = int(0.9 * len(train_examples))\n",
    "\n",
    "valid_examples = train_examples[num_train:]\n",
    "train_examples = train_examples[:num_train]\n",
    "\n",
    "partitions = [(train_examples, valid_examples, examples)]\n",
    "\n",
    "def write_examples(examples, d, part):\n",
    "    _file = open(os.path.join(d, f\"{part}.data\"), 'w')\n",
    "    for ex in examples:\n",
    "        _file.write(json.dumps(ex) + \"\\n\")\n",
    "    _file.close()\n",
    "\n",
    "for i, (tr, val, te) in enumerate(partitions):\n",
    "    data_dir = f\"PLBART_EXPERIMENT/DATA/part_{i}\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    write_examples(tr, data_dir, 'train')\n",
    "    write_examples(val, data_dir, 'valid')\n",
    "    write_examples(te, data_dir, 'test')\n",
    "    \n",
    "fp = open(\"PLBART_EXPERIMENT/DATA/id_to_link.json\", 'w')\n",
    "json.dump(id_to_link, fp, indent=4)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24ce26154344a4d86841114bcd387c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/10062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b91d6c40b347b1a1dce296cfc437b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/10062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30e1e4a1b4844d99674cce34084c71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/10062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e418c9bad84641ace0f5fd8fdcd7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/10062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d23db58b72b42489ad50db450e04af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b541dc38d9e147ee87d0d225346c9edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13146f5b79bb4526a8b581e7459cae0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59540b653a94c1ca0a1157dcd053081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5b3e4818934468907d0611b7aadc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08445d86e4b34dcba95a3d66bd72f7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a2385e4f56434c9b700113483090fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635be786f0614e95ad69941e344aaef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import sentencepiece as spm\n",
    "\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def python_code_tokenize(full_code_text):\n",
    "    '''\n",
    "    :param full_code_text:\n",
    "    :return:\n",
    "    '''\n",
    "    g = tokenize.tokenize(BytesIO(full_code_text.encode('utf-8')).readline)\n",
    "    tokens = []\n",
    "    prev_token = None\n",
    "    try:\n",
    "        for x in g:\n",
    "            if x.type == tokenize.ENDMARKER:  # End Marker\n",
    "                continue\n",
    "            # if x.type == tokenize.COMMENT:\n",
    "            #     continue\n",
    "            elif x.type == tokenize.NEWLINE:\n",
    "                tokens.append('NEW_LINE')\n",
    "            elif x.type == tokenize.INDENT:\n",
    "                tokens.append('INDENT')\n",
    "            elif x.type == tokenize.DEDENT:\n",
    "                tokens.append('DEDENT')\n",
    "            elif x.type == tokenize.STRING:  # String\n",
    "                s = x.string.strip()\n",
    "                if s.startswith('\"\"\"') or s.startswith(\"'''\"):\n",
    "                    if prev_token is not None and (prev_token == '=' or prev_token == '(' or prev_token == ','):\n",
    "                        tokens.append(x.string)\n",
    "                    continue\n",
    "                tokens.append(x.string)\n",
    "                pass\n",
    "            elif x.string == '\\n':\n",
    "                continue\n",
    "            elif x.type < 57:\n",
    "                tokens.append(x.string)\n",
    "            prev_token = x.string.strip()\n",
    "    except:\n",
    "        return []\n",
    "        pass\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class MultiprocessingEncoder(object):\n",
    "    def __init__(self, model_file):\n",
    "        self.model_file = model_file\n",
    "\n",
    "    def initializer(self):\n",
    "        global sp\n",
    "        sp = spm.SentencePieceProcessor(model_file=self.model_file)\n",
    "\n",
    "    def _encode(self, line):\n",
    "        global sp\n",
    "        return sp.encode(line, out_type=str)\n",
    "\n",
    "    def _decode(self, tokens):\n",
    "        global sp\n",
    "        return sp.decode(tokens)\n",
    "\n",
    "    def encode(self, example):\n",
    "        assert isinstance(example, dict)\n",
    "        assert 'src' in example and 'tgt' in example\n",
    "        if len(example['src']) == 0 :\n",
    "            return None\n",
    "        if len(example['tgt']) == 0 :\n",
    "            return None\n",
    "        src_tokens = self._encode(example['src'])\n",
    "        tgt_tokens = example['tgt'].split()\n",
    "        return {'src': \" \".join(src_tokens), 'tgt': \" \".join(tgt_tokens), 'link': example['link']}\n",
    "\n",
    "\n",
    "def load_data(input_file, src_fields, tgt_field):\n",
    "    data = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            assert tgt_field in ex\n",
    "            srcs = []\n",
    "            for sf in src_fields:\n",
    "                assert sf in ex\n",
    "                src = ex[sf]\n",
    "                if isinstance(src, list):\n",
    "                    src = \" \".join(src)\n",
    "                src = src.strip()\n",
    "                if len(src) > 0:\n",
    "                    srcs.append(src)\n",
    "            src = \" <s> \".join(srcs)\n",
    "            tgt = ex[tgt_field]\n",
    "            if isinstance(tgt, list):\n",
    "                tgt = \" \".join(tgt)\n",
    "            tgt = tgt.replace('\\n', ' ').strip()\n",
    "            link = ex['link']\n",
    "            data.append({'src': src, 'tgt': tgt, 'link': link})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def process(\n",
    "    model_file, input_file, src_fields, tgt_field, \n",
    "    output_dir, part, pref, src_lang=\"en_XX\", tgt_lang=\"py\", workers=1\n",
    "):\n",
    "    dataset = load_data(input_file, src_fields, tgt_field)\n",
    "    encoder = MultiprocessingEncoder(model_file)\n",
    "    pool = Pool(workers, initializer=encoder.initializer)\n",
    "    processed_dataset = []\n",
    "    with tqdm(total=len(dataset), desc='Processing') as pbar:\n",
    "        for i, ex in enumerate(pool.imap(encoder.encode, dataset, 100)):\n",
    "            pbar.update()\n",
    "            processed_dataset.append(ex)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    out_src = os.path.join(output_dir, '{}_{}.spm.{}'.format(pref, part, src_lang))\n",
    "    out_tgt = os.path.join(output_dir, '{}_{}.spm.{}'.format(pref, part, tgt_lang))\n",
    "    out_link = os.path.join(output_dir, '{}.link'.format(part))\n",
    "    with open(out_src, 'w', encoding='utf-8') as src_writer, \\\n",
    "            open(out_tgt, 'w', encoding='utf-8') as tgt_writer, \\\n",
    "            open(out_link, 'w', encoding='utf-8') as link_writer:\n",
    "        for ex in processed_dataset:\n",
    "            if ex is not None:\n",
    "                src_writer.write(ex['src'] + '\\n')\n",
    "                tgt_writer.write(ex['tgt'] + '\\n')\n",
    "                link_writer.write(ex['link'] + '\\n')\n",
    "\n",
    "                \n",
    "# examples.append({\n",
    "#                 'id': example_id,\n",
    "#                 'q': qtitle.strip(),\n",
    "#                 'd': qdesc.strip(),\n",
    "#                 'q_apis': \" \".join(question_apis),\n",
    "#                 \"io\":  re.sub(\"[ \\t\\n]+\", \" \", io),\n",
    "#                 \"apis\": api_seq,\n",
    "#                 'code': codes,\n",
    "#                 'link': e['link']\n",
    "#             })\n",
    "                \n",
    "for i in range(1):\n",
    "    data_dir = f\"PLBART_EXPERIMENT/DATA/part_{i}\"\n",
    "    for part in ['train', 'valid', 'test']:\n",
    "        process(\n",
    "            input_file=os.path.join(data_dir, f\"{part}.data\"),\n",
    "            src_fields=[\"q\"],\n",
    "            tgt_field=\"apis\",\n",
    "            output_dir=data_dir,\n",
    "            pref=f\"query\",\n",
    "            part=part,\n",
    "            model_file=\"PLBART_EXPERIMENT/pretrained/sentencepiece.bpe.model\"\n",
    "        )\n",
    "        \n",
    "        process(\n",
    "            input_file=os.path.join(data_dir, f\"{part}.data\"),\n",
    "            src_fields=[\"q\", \"d\"],\n",
    "            tgt_field=\"apis\",\n",
    "            output_dir=data_dir,\n",
    "            pref=f\"query_desc\",\n",
    "            part=part,\n",
    "            model_file=\"PLBART_EXPERIMENT/pretrained/sentencepiece.bpe.model\"\n",
    "        )\n",
    "        \n",
    "        process(\n",
    "            input_file=os.path.join(data_dir, f\"{part}.data\"),\n",
    "            src_fields=[\"q\", \"q_apis\"],\n",
    "            tgt_field=\"apis\",\n",
    "            output_dir=data_dir,\n",
    "            pref=f\"query_names\",\n",
    "            part=part,\n",
    "            model_file=\"PLBART_EXPERIMENT/pretrained/sentencepiece.bpe.model\"\n",
    "        )\n",
    "        \n",
    "        process(\n",
    "            input_file=os.path.join(data_dir, f\"{part}.data\"),\n",
    "            src_fields=[\"q\", \"d\", \"q_apis\"],\n",
    "            tgt_field=\"apis\",\n",
    "            output_dir=data_dir,\n",
    "            pref=f\"query_desc_io_names\",\n",
    "            part=part,\n",
    "            model_file=\"PLBART_EXPERIMENT/pretrained/sentencepiece.bpe.model\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n"
     ]
    }
   ],
   "source": [
    "data_frame_apis = list(set(data_frame_apis))\n",
    "print(len(data_frame_apis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = open(\"PLBART_EXPERIMENT/pretrained/checkpoints/tgt_dict.txt\", \"w\")\n",
    "target_vocab.write(\"\\n\".join([a.strip() + \" 100\" for a in data_frame_apis]))\n",
    "target_vocab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
